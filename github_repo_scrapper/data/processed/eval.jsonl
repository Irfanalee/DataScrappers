{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `text_to_audio.py`:\n\n```python\nDEFAULT_VOCODER_ID = \"microsoft/speecht5_hifigan\"\n\n\nChatType = list[dict[str, str]]\n\n\n# Copied from transformers.pipelines.text_generation.Chat\nclass Chat:\n    \"\"\"This class is intended to just be used internally in this pipeline and not exposed to users. We convert chats\n    to this format because the rest of the pipeline code tends to assume that lists of messages are\n    actually a batch of samples rather than messages in the same conversation.\"\"\"\n\n    def __init__(self, messages: dict):\n        for message in messages:\n            if not (\"role\" in message and \"content\" in message):\n                raise ValueError(\"When passing chat dicts as input, each dict must have a 'role' and 'content' key.\")\n        self.messages = messages\n```"}, {"role": "assistant", "content": "thanks ! I've put in `utils/chat_template_utils.py` for now"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42326, "url": "https://github.com/huggingface/transformers/pull/42326#discussion_r2550900748"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `processing_audioflamingo3.py`:\n\n```python\n# coding=utf-8\n# Copyright 2025 NVIDIA CORPORATION and the HuggingFace Inc. team. All rights\n# reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nProcessor class for AudioFlamingo3.\n\"\"\"\n\nfrom typing import Optional, Union\n\nimport numpy as np\n\nfrom ...audio_utils import AudioInput, make_list_of_audio\nfrom ...feature_extraction_utils import BatchFeature\nfrom ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\nfrom ...tokenization_utils_base import TextInput\nfrom ...utils import logging\n\n\nlogger = logging.get_logger(__name__)\n\nMAX_AUDIO_LEN = 10 * 60  # 10 minutes\n\n\nclass AudioFlamingo3ProcessorKwargs(ProcessingKwargs, total=False):\n    _defaults = {\n        \"text_kwargs\": {\n            \"padding\": True,\n            \"padding_side\": \"left\",\n        },\n        \"audio_kwargs\": {\n            \"sound_token\": \"<sound>\",  # Placeholder token used in text for audio expansion.\n            \"return_attention_mask\": True,\n            \"padding\": \"max_length\",\n        },\n        \"common_kwargs\": {\"return_tensors\": \"pt\"},\n    }\n\n\nclass AudioFlamingo3Processor(ProcessorMixin):\n    r\"\"\"\n    Constructs an AudioFlamingo3 processor which wraps an AudioFlamingo3 feature extractor and an AudioFlamingo3\n    tokenizer into a single processor.\n\n    [`AudioFlamingo3Processor`] offers all the functionalities of [`WhisperFeatureExtractor`] and\n    [`Qwen2TokenizerFast`]. See the [`~AudioFlamingo3Processor.__call__`] for more information.\n\n    Args:\n        feature_extractor ([`WhisperFeatureExtractor`]):\n            The feature extractor is a required input.\n        tokenizer ([`Qwen2TokenizerFast`]):\n            The tokenizer is a required input.\n    \"\"\"\n\n    attributes = [\"feature_extractor\", \"tokenizer\"]\n    feature_extractor_class = \"WhisperFeatureExtractor\"\n    tokenizer_class = \"Qwen2TokenizerFast\"\n\n    def __init__(self, feature_extractor, tokenizer):\n        super().__init__(feature_extractor, tokenizer)\n\n    def __call__(\n        self,\n        text: Union[TextInput, list[TextInput]],\n        audio: Optional[AudioInput] = None,\n```"}, {"role": "assistant", "content": "(Note about one of my changes) your paper mentioned possibility of follow-up/clarification prompts where there is no audio input -> so I made audio optional"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 40290, "url": "https://github.com/huggingface/transformers/pull/40290#discussion_r2368753697"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_commands.py`:\n\n```python\nself.assertEqual(2, self.call('genspider', self.project_name))\n        assert not exists(join(self.proj_mod_path, 'spiders', '%s.py' % self.project_name))\n\n    def test_same_name_as_existing_spider(self):\n        self.call('genspider', 'example', 'example.com')\n        self.assertEqual(0, self.call('genspider', 'example', 'example.com'))\n```"}, {"role": "assistant", "content": "This only checks that the command finishes correctly with an exit code 0. Is there a way we could also test that the file is not overridden? Checking that the printed message appears in the command's output should be enough I think."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4623, "url": "https://github.com/scrapy/scrapy/pull/4623#discussion_r438793602"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `parsing.py`:\n\n```python\nelse:\n            normalized: object\n            # Constant strings may be indented across newlines, if they are\n            # docstrings; fold spaces after newlines when comparing. Similarly,\n            # trailing and leading space may be removed.\n            if (\n                isinstance(node, ast.Constant)\n                and field == \"value\"\n                and isinstance(value, str)\n            ):\n                # Constant strings may be indented across newlines, if they are\n                # docstrings; fold spaces after newlines when comparing. Similarly,\n                # trailing and leading space may be removed.\n                normalized = _normalize(\"\\n\", value)\n            elif field == \"type_comment\" and isinstance(value, str):\n                # Trailing spaces in type comments are removed.\n```"}, {"role": "assistant", "content": "Interesting that we apparently do not remove trailing tabs, only spaces. That might be a bug?\r\n\r\nIn any case, this change looks good, thanks!"}], "_meta": {"repo": "psf/black", "pr_number": 3773, "url": "https://github.com/psf/black/pull/3773#discussion_r1257558777"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `cookies.py`:\n\n```python\n\"\"\"Deletes a cookie given a name. Wraps cookielib.CookieJar's remove_cookie_by_name().\"\"\"\n        remove_cookie_by_name(self, name)\n\n    def set_cookie(self, cookie, *args, **kwargs):\n        if cookie.value.startswith('\"') and cookie.value.endswith('\"'):\n            cookie.value = cookie.value.strip('\\\\\"')\n```"}, {"role": "assistant", "content": "alternatively, use `cookie.value = cookie.value.replace('\\\\\"', '')` if we want to keep the double quotes, removing the `/`'s"}], "_meta": {"repo": "psf/requests", "pr_number": 1440, "url": "https://github.com/psf/requests/pull/1440#discussion_r4969856"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_models.py`:\n\n```python\nitems = parse_qsl(value)\n        elif isinstance(value, QueryParams):\n            items = value.multi_items()\n        elif isinstance(value, list):\n        elif type(value) in [list, tuple]:\n```"}, {"role": "assistant", "content": "I'm also in favor of using the more generic `Sequence`, makes the docs easier to change as well."}], "_meta": {"repo": "encode/httpx", "pr_number": 1426, "url": "https://github.com/encode/httpx/pull/1426#discussion_r541186266"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_ggml.py`:\n\n```python\ntext = tokenizer(self.example_text, return_tensors=\"pt\")\n        out = model.generate(**text, max_new_tokens=10)\n\n        EXPECTED_TEXT = \"Hello, I am a 20 year old male\"\n        EXPECTED_TEXT = \"Helloab, I am a 1000\"\n```"}, {"role": "assistant", "content": "i don't think it's reasonable, we need take a look"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 40331, "url": "https://github.com/huggingface/transformers/pull/40331#discussion_r2294103079"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `mode.py`:\n\n```python\nfeatures_and_magics,\n        ]\n        return \".\".join(parts)\n\n    def __hash__(self) -> int:\n```"}, {"role": "assistant", "content": "This is how `Mode` would be modified :) if we use dataclass to generate hash:-\r\n\r\n```\r\n-\r\n+(frozen=True)\r\n class Mode:\r\n-    target_versions: set[TargetVersion] = field(default_factory=set)\r\n+    target_versions: frozenset[TargetVersion] = field(default_factory=frozenset)\r\n     line_length: int = DEFAULT_LINE_LENGTH\r\n     string_normalization: bool = True\r\n     is_pyi: bool = False\r\n     is_ipynb: bool = False\r\n     skip_source_first_line: bool = False\r\n     magic_trailing_comma: bool = True\r\n-    python_cell_magics: set[str] = field(default_factory=set)\r\n+    python_cell_magics: frozenset[str] = field(default_factory=frozenset)\r\n     preview: bool = False\r\n     unstable: bool = False\r\n-    enabled_features: set[Preview] = field(default_factory=set)\r\n+    enabled_features: frozenset[Preview] = field(default_factory=frozenset)\r\n+\r\n+    def __post_init__(self):\r\n+        object.__setattr__(self, 'target_versions', frozenset(self.target_versions))\r\n+        object.__setattr__(self, 'python_cell_magics', frozenset(self.python_cell_magics))\r\n+        object.__setattr__(self, 'enabled_features', frozenset(self.enabled_features))\r\n```\r\n\r\nRest of the class remains as is."}], "_meta": {"repo": "psf/black", "pr_number": 4645, "url": "https://github.com/psf/black/pull/4645#discussion_r2227158747"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_generate_schema.py`:\n\n```python\n):\n            schema = existing_schema\n        elif (validators := getattr(obj, '__get_validators__', None)) is not None:\n            warn(\n                '`__get_validators__` is deprecated and will be removed, use `__get_pydantic_core_schema__` instead.',\n                PydanticDeprecatedSince20,\n            )\n            from pydantic.v1 import BaseModel as BaseModelV1\n\n            if issubclass(obj, BaseModelV1):\n```"}, {"role": "assistant", "content": "Will leave it out for now, and can add this back if there's an issue \ud83d\udc4d"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10431, "url": "https://github.com/pydantic/pydantic/pull/10431#discussion_r1765037097"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `sqlparser.py`:\n\n```python\ntry:\n        sqlalchemy_engine = hook.get_sqlalchemy_engine()\n    except ImportError as e:\n        if \"sqlalchemy\" in str(e).lower() or \"No module named 'sqlalchemy\" in str(e):\n            raise AirflowOptionalProviderFeatureException(\n```"}, {"role": "assistant", "content": "Yes, I don't think we need another except here for ImportError, the generic one we have below will do just fine"}], "_meta": {"repo": "apache/airflow", "pr_number": 59921, "url": "https://github.com/apache/airflow/pull/59921#discussion_r2652921013"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `canvas.py`:\n\n```python\nother = maybe_unroll_group(other)\n            if isinstance(self, _chain):\n                # chain | group() -> chain\n                sig = self.clone()\n                sig.tasks.append(other)\n                return sig\n                return _chain(\n                    seq_concat_item([\n                        reduce(\n```"}, {"role": "assistant", "content": "With `reduce` I'm assigning tasks from `link_error` of current chain to individual tasks with respect to their order.\r\nThis code equals to:\r\n\r\n    new_tasks = []\r\n    for task in self.tasks:\r\n      new_task = task.clone()\r\n      new_tasks.append(new_task)\r\n      for error_handler in self._with_list_option('link_error'):\r\n        new_task.on_error(error_handler)\r\n    return _chain(seq_concat_item(new_tasks, other), app=self._app)`"}], "_meta": {"repo": "celery/celery", "pr_number": 4240, "url": "https://github.com/celery/celery/pull/4240#discussion_r138437103"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `shared_memory.py`:\n\n```python\nIn order to ensure proper cleanup of resources, unlink should be\n        called once (and only once) across all processes which have access\n        to the shared memory block.\"\"\"\n```"}, {"role": "assistant", "content": "`unlink` is essentially a file delete so it should only happen once. Maybe I lost track of the bigger picture. I will reread my changes."}], "_meta": {"repo": "python/cpython", "pr_number": 110778, "url": "https://github.com/python/cpython/pull/110778#discussion_r1359932324"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `__init__.py`:\n\n```python\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nfrom __future__ import annotations\n```"}, {"role": "assistant", "content": "Good catch! It's no need at all, I added them by accident."}], "_meta": {"repo": "apache/airflow", "pr_number": 59805, "url": "https://github.com/apache/airflow/pull/59805#discussion_r2662062732"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `scheduler_job_runner.py`:\n\n```python\n.where(~DM.is_paused)\n                .where(TI.state == TaskInstanceState.SCHEDULED)\n                .where(DM.bundle_name.is_not(None))\n                .join(\n                    dr_task_concurrency_subquery,\n                    TI.run_id == dr_task_concurrency_subquery.c.run_id,\n                    isouter=True,\n                )\n                .where(func.coalesce(dr_task_concurrency_subquery.c.dr_count, 0) < DM.max_active_tasks)\n                .options(selectinload(TI.dag_model))\n                .order_by(-TI.priority_weight, DR.logical_date, TI.map_index)\n            )\n\n            # Create a subquery with row numbers partitioned by run_id.\n            #\n            # run_id    | task_id | priority_weight | row_num\n            # ----------|---------|-----------------|--------\n            # dag1_dr1  | task1   | 100             | 1\n            # dag1_dr1  | task22  | 90              | 2\n            # dag1_dr1  | task5   | 80              | 3\n            # dag1_dr1  | task13  | 70              | 4\n            # dag2_dr1  | task3   | 95              | 1\n            # dag2_dr1  | task1   | 85              | 2\n            # dag2_dr1  | task5   | 75              | 3\n            ranked_query = (\n                query.add_columns(\n                    func.row_number()\n                    .over(\n                        partition_by=TI.run_id,\n                        order_by=[-TI.priority_weight, DR.logical_date, TI.map_index],\n                    )\n                    .label(\"row_num\"),\n                    DM.max_active_tasks.label(\"dr_max_active_tasks\"),\n                    # Create columns for the order_by checks here for sqlite.\n                    TI.priority_weight.label(\"priority_weight_for_ordering\"),\n                    DR.logical_date.label(\"logical_date_for_ordering\"),\n                    TI.map_index.label(\"map_index_for_ordering\"),\n                )\n            ).subquery()\n\n            # Select only rows where row_number <= max_active_tasks.\n            query = (\n                select(TI)\n                .select_from(ranked_query)\n                .join(\n                    TI,\n                    (TI.dag_id == ranked_query.c.dag_id)\n                    & (TI.task_id == ranked_query.c.task_id)\n                    & (TI.run_id == ranked_query.c.run_id)\n                    & (TI.map_index == ranked_query.c.map_index),\n                )\n                .where(ranked_query.c.row_num <= ranked_query.c.dr_max_active_tasks)\n                # Add the order_by columns from the ranked query for sqlite.\n                .order_by(\n                    -ranked_query.c.priority_weight_for_ordering,\n                    ranked_query.c.logical_date_for_ordering,\n                    ranked_query.c.map_index_for_ordering,\n                )\n            )\n```"}, {"role": "assistant", "content": "Similar thing here: The window only partitions on `run_id`, which collides across DAGs. Once one DAG consumes its `max_active_tasks`, every other DAG with the same `run_id` is filtered out by `row_num <= dr_max_active_tasks`, even if they have zero active tasks. \n\nBetter would be to partition by both `TI.dag_id` and `TI.run_id` (or the dag run PK) before applying the limit so each DAG run keeps its own quota."}], "_meta": {"repo": "apache/airflow", "pr_number": 54103, "url": "https://github.com/apache/airflow/pull/54103#discussion_r2479694425"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `translate.py`:\n\n```python\nif old_translation:\n        prompt_segments.extend(\n            [\n                \"There's an existing previous translation for this content that is probably outdated with old content or old instructions.\",\n                \"There is an existing previous translation for the original English content, that may be outdated.\",\n                \"Update the translation only where necessary:\",\n                \"- If the original English content has changed, reflect that in the translation.\",\n                \"- If the original English content has added parts, also add these parts to the translation.\",\n                \"- If the original English content has removed parts, also remove them from the translation, unless you were instructed earlier to not do that in specific cases.\",\n                \"- If parts of the original English content have changed, also change those parts in the translation.\",\n```"}, {"role": "assistant", "content": "As I understand it ...\r\n\r\n```\r\nIf the original English content has added parts (...)\r\nIf the original English content has removed parts (...)\r\nIf parts of the original English content have changed (...)\r\n```\r\n\r\n... means _\"compared to the existing translation\"_ and not _\"compared to the previous version of the English document\"_. Do you agree? If so, then I do not see an issue, do you?\r\n\r\nNotice, the shorter version did not add a part to a sentence, in a situation when retranslating the test file (this sentence). It was stubborn, even when I retranslated again. After I changed the prompt, it detected the addition.\r\n\r\nRegarding comment markers, I also thought about this one, and the problem you mentioned (when the addition may become invalid), and I found no good solution, which is why stuck to just allow added `abbr`s, which are removed when the sentence gets removed.\r\n\r\nBTW, I will make another complete retranslation before merging, to check if it works with the updated prompts."}], "_meta": {"repo": "fastapi/fastapi", "pr_number": 14015, "url": "https://github.com/fastapi/fastapi/pull/14015#discussion_r2323501046"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `stream.py`:\n\n```python\nfrom twisted.internet.error import ConnectionClosed\nfrom twisted.python.failure import Failure\nfrom twisted.web.client import ResponseFailed\nfrom typing import Dict\nfrom typing import List, Optional, Tuple, TYPE_CHECKING\n```"}, {"role": "assistant", "content": "And it should definitely not be split into 2 lines :slightly_smiling_face:"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4610, "url": "https://github.com/scrapy/scrapy/pull/4610#discussion_r478350996"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_http_request.py`:\n\n```python\nresponse, formcss=\"input[name='abc']\")\n\n    def test_from_response_valid_form_methods(self):\n        body = \"\"\"<form action=\"post.php\" method=\"%s\">\n            <input type=\"hidden\" name=\"one\" value=\"1\">\n            </form>\"\"\"\n```"}, {"role": "assistant", "content": "I think in this case the point is to avoid repeating the string, so I would switch to `.format()` instead of f-string."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5246, "url": "https://github.com/scrapy/scrapy/pull/5246#discussion_r716080918"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_computed_fields.py`:\n\n```python\nassert m.model_dump() == {'x': 2, '_double': 4}\n\n\n@pytest.mark.skipif(sys.version_info < (3, 9), reason='@computed_field @classmethod @property only works in 3.9+')\n@pytest.mark.skipif(\n    sys.version_info < (3, 9) or sys.version_info >= (3, 13),\n    reason='@computed_field @classmethod @property only works in 3.9-3.12',\n```"}, {"role": "assistant", "content": "I didn't find an easy way - I suppose you could create a custom decorator that basically handles the `classmethod` stuff with `__wrapped__` as suggested here:\r\n\r\nhttps://github.com/python/cpython/pull/92379/files#diff-78f24041d66ab8ed2ae1aee94bcd42396d27af833cb96a3c506294c6d6dce82d"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 9743, "url": "https://github.com/pydantic/pydantic/pull/9743#discussion_r1653395326"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `models.py`:\n\n```python\nyield part\n            await self.close()\n\n    async def next(self) -> \"Response\":\n    async def anext(self) -> \"Response\":\n        \"\"\"\n        Get the next response from a redirect response.\n        \"\"\"\n        if not self.is_redirect:\n            raise NotRedirectResponse()\n        assert self.call_next is not None\n        assert inspect.iscoroutinefunction(self.call_next)\n```"}, {"role": "assistant", "content": "Yep, that'd fix it *for now* \u2014 but we might bump into this again when looking at `.next()`. Anyway, will remove this check for now so we can move on."}], "_meta": {"repo": "encode/httpx", "pr_number": 676, "url": "https://github.com/encode/httpx/pull/676#discussion_r360660547"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `utils.py`:\n\n```python\n**repo_loading_kwargs,\n                )\n            except OSError:\n                # `self` already has a generation config created from model config so we can simply raise a warning\n                logger.info(\n                    \"Generation config file not found, using a generation config created from the model config.\"\n                )\n```"}, {"role": "assistant", "content": "oh, didn't notice! Nah, I think we don't need a warning, it's a common situation when no generation config is saved"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42922, "url": "https://github.com/huggingface/transformers/pull/42922#discussion_r2630514091"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `trainer.py`:\n\n```python\nignore_keys_for_eval=ignore_keys_for_eval,\n            )\n\n    def get_cp_size(self) -> int:\n        \"\"\"Get the context parallel size\"\"\"\n        if getattr(self.accelerator, \"parallelism_config\", None) is None:\n            return 1\n        else:\n            pc = self.accelerator.parallelism_config\n            return pc.cp_size\n```"}, {"role": "assistant", "content": "in which case we should rework how it's calculated, otherwise we are confusing sp with cp"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41832, "url": "https://github.com/huggingface/transformers/pull/41832#discussion_r2539147267"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_modeling_deepseek_v2.py`:\n\n```python\ntest_all_params_have_gradient = False\n    model_tester_class = DeepseekV2ModelTester\n    model_split_percents = [0.5, 0.7, 0.8]\n    has_experts = True\n```"}, {"role": "assistant", "content": "we could check \"experts\" in \"\".join(config.dictg())"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42697, "url": "https://github.com/huggingface/transformers/pull/42697#discussion_r2626422444"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `genspider.py`:\n\n```python\ntry:\n            spidercls = self.crawler_process.spider_loader.load(name)\n        except KeyError:\n            pass\n            if not self.settings.get('NEWSPIDER_MODULE'):\n                if exists(name + \".py\"):\n                    print(\"File %r already exists in the current directory\" % (name + \".py\"))\n                    return\n            else:\n                spiders_module = import_module(self.settings['NEWSPIDER_MODULE'])\n                spiders_dir = dirname(spiders_module.__file__)\n                spiders_dir_abs = abspath(spiders_dir)\n                if exists(join(spiders_dir_abs, name + \".py\")):\n                    print(\"Spider with filename %r already exists in directory:\" % (name + \".py\"))\n                    print(\"  %s\" % spiders_dir)\n```"}, {"role": "assistant", "content": ":lipstick:  I would simplify both messages to \"%r already exists\", where %r is the absolute path of the destination file."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4623, "url": "https://github.com/scrapy/scrapy/pull/4623#discussion_r439863160"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `item.py`:\n\n```python\nfrom collections.abc import MutableMapping\n\nfrom scrapy.item import BaseItem\n\n\ndef _is_dataclass_instance(obj):\n    \"\"\"\n    Return True if *obj* is a dataclass object, False otherwise.\n    Taken from https://docs.python.org/3/library/dataclasses.html#dataclasses.is_dataclass.\n    \"\"\"\n    try:\n        from dataclasses import is_dataclass\n    except ImportError:\n        return False\n    else:\n        return is_dataclass(obj) and not isinstance(obj, type)\n\n\ndef is_item_like(obj):\n    \"\"\"\n    Return True if *obj* is considered a Scrapy *item*, False otherwise.\n\n    An object is considered an *item* if it is:\n    - a scrapy.item.BaseItem or dict instance (or any subclass)\n    - a dataclass object\n    \"\"\"\n    return isinstance(obj, (BaseItem, dict)) or _is_dataclass_instance(obj)\n\n\nclass ItemAdapter(MutableMapping):\n```"}, {"role": "assistant", "content": "My main concern is the handling of `{'fields': '\u2026'}` in non-`Item` objects. I\u2019m not sure it is safe to assume that if the attribute exists we can treat the object as a Scrapy `Item`. Best case scenario: a puzzling `TypeError` exception is raised. Worst case scenario: things work in unintended ways."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 3881, "url": "https://github.com/scrapy/scrapy/pull/3881#discussion_r415834019"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_client.py`:\n\n```python\ndef _init_proxy_transport(\n        self,\n        proxy: Proxy,\n        verify: VerifyTypes = True,\n        cert: CertTypes | None = None,\n        ssl_context: typing.Optional[ssl.SSLContext] = None,\n```"}, {"role": "assistant", "content": "```suggestion\r\n        ssl_context: ssl.SSLContext | None = None,\r\n```"}], "_meta": {"repo": "encode/httpx", "pr_number": 3022, "url": "https://github.com/encode/httpx/pull/3022#discussion_r1790218211"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `crawler.py`:\n\n```python\nreactor_class: str = self.settings[\"TWISTED_REACTOR\"]\n        event_loop: str = self.settings[\"ASYNCIO_EVENT_LOOP\"]\n        logged_reactor_info: bool = False\n\n        if self._init_reactor:\n            # this needs to be done after the spider settings are merged,\n            # but before something imports twisted.internet.reactor\n            if reactor_class:\n                install_reactor(reactor_class, event_loop)\n            else:\n                from twisted.internet import reactor  # noqa: F401\n            log_reactor_info()\n\n            if not logged_reactor_info:\n                log_reactor_info()\n                logged_reactor_info = True\n\n        if reactor_class:\n            verify_installed_reactor(reactor_class)\n            if is_asyncio_reactor_installed() and event_loop:\n                verify_installed_asyncio_event_loop(event_loop)\n\n            log_reactor_info()\n```"}, {"role": "assistant", "content": "\ud83d\udcaf  , I updated the code using your logic make more sense and is more clear, please take a look."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6657, "url": "https://github.com/scrapy/scrapy/pull/6657#discussion_r1944482651"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_downloaderslotssettings.py`:\n\n```python\nimport time\n\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.http import Request\n\nfrom tests.mockserver import MockServer\nfrom tests.spiders import MetaSpider\n\nfrom twisted.internet import defer\nfrom twisted.trial.unittest import TestCase\n\n\nclass DownloaderSlotsSettingsTestSpider(MetaSpider):\n\n    name = 'downloader_slots'\n\n    custom_settings = {\n        \"DOWNLOAD_DELAY\": 1,\n        \"RANDOMIZE_DOWNLOAD_DELAY\": False,\n        \"DOWNLOAD_SLOTS\": {\n            'quotes.toscrape.com': {\n                'concurrency': 1,\n                'delay': 1.5,\n                'randomize_delay': False\n            },\n            'books.toscrape.com': {\n                'delay': 2,\n                'randomize_delay': False\n            }\n        }\n    }\n\n    def start_requests(self):\n        self.times = {None: []}\n\n        slots = list(self.custom_settings.get('DOWNLOAD_SLOTS', {}).keys()) + [None]\n\n        for slot in slots:\n            url = self.mockserver.url(f\"/?downloader_slot={slot}\")\n            self.times[slot] = []\n            yield Request(url, callback=self.parse, meta={'download_slot': slot})\n\n    def parse(self, response):\n        slot = response.meta.get('download_slot', None)\n        self.times[slot].append(time.time())\n        url = self.mockserver.url(f\"/?downloader_slot={slot}&req=2\")\n        yield Request(url, callback=self.not_parse, meta={'download_slot': slot})\n\n    def not_parse(self, response):\n        slot = response.meta.get('download_slot', None)\n        self.times[slot].append(time.time())\n\n\nclass CrawlTestCase(TestCase):\n\n    def setUp(self):\n        self.mockserver = MockServer()\n        self.mockserver.__enter__()\n        self.runner = CrawlerRunner()\n\n    def tearDown(self):\n        self.mockserver.__exit__(None, None, None)\n\n    @defer.inlineCallbacks\n    def test_delay(self):\n        crawler = CrawlerRunner().create_crawler(DownloaderSlotsSettingsTestSpider)\n        yield crawler.crawl(mockserver=self.mockserver)\n        slots = crawler.engine.downloader.slots\n        times = crawler.spider.times\n        tolerance = 0.3\n\n        delays_real = {k: v[1] - v[0] for k, v in times.items()}\n        error_delta = {k: 1 - delays_real[k] / v.delay for k, v in slots.items()}\n```"}, {"role": "assistant", "content": "> The main question what is acceptable total time for this test?\r\n\r\nThe minimum value that still makes the test reliable, both in the sense that it does not break randomly, and that it indeed validates what it is meant to validate. If we need 30 seconds for that, so be it. But if it can be done in 5 seconds, that would be better."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5328, "url": "https://github.com/scrapy/scrapy/pull/5328#discussion_r1027725321"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `backfill.py`:\n\n```python\n) -> list[DagRunInfo]:\n    infos = dag.iter_dagrun_infos_between(from_date, to_date)\n    now = timezone.utcnow()\n    dagrun_info_list = [x for x in infos if x.data_interval.end < now]\n    dagrun_info_list = []\n    for x in infos:\n        # todo: AIP-76 update for partitioned dags\n        if x.data_interval and x.data_interval.end < now:\n            dagrun_info_list.append(x)\n```"}, {"role": "assistant", "content": "```suggestion\n    dagrun_info_list = [\n        x\n        for x in infos\n        # todo: AIP-76 update for partitioned dags\n        if x.data_interval and x.data_interval.end < now\n    ]\n```\n\ncomprehension still looks better IMO \ud83e\udd14"}], "_meta": {"repo": "apache/airflow", "pr_number": 59115, "url": "https://github.com/apache/airflow/pull/59115#discussion_r2697696657"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `tokenization_auto.py`:\n\n```python\n(\"align\", \"BertTokenizer\" if is_tokenizers_available() else None),\n        (\"arcee\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n        (\"aria\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n        (\"audioflamingo3\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n```"}, {"role": "assistant", "content": "nit but for v5 we drop the \"Fast\", we can update this to `Qwen2Tokenizer` ! == the Fast one \ud83d\ude0a \n\n(same for updating ParakeetTokenizerFast to drop the \"fast\" (from file name too)"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42667, "url": "https://github.com/huggingface/transformers/pull/42667#discussion_r2611323140"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_utils.py`:\n\n```python\ndef make_inputs_require_grads(module, input, output):\n            output.requires_grad_(True)\n\n        self._require_grads_hook = self.get_input_embeddings().register_forward_hook(make_inputs_require_grads)\n        hooks = []\n        seen_modules = set()\n\n        for module in self.modules():\n            if not (isinstance(module, PreTrainedModel) and hasattr(module, \"get_input_embeddings\")):\n                continue\n\n            input_embeddings = module.get_input_embeddings()\n\n            if input_embeddings is None:\n                continue\n\n            embedding_id = id(input_embeddings)\n            if embedding_id in seen_modules:\n                continue\n\n            seen_modules.add(embedding_id)\n            hooks.append(input_embeddings.register_forward_hook(make_inputs_require_grads))\n\n        self._require_grads_hooks = hooks\n        if hooks:\n            # for BC\n            self._require_grads_hook = hooks[0]\n```"}, {"role": "assistant", "content": "aren't we ignoring all hooks except for the first one in this case, i.e. when we disable it will disable the text model and will not disable vision model?"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41993, "url": "https://github.com/huggingface/transformers/pull/41993#discussion_r2527443756"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_generics.py`:\n\n```python\n}\n\n\ndef test_mix_default_and_constraints() -> None:\n    T = TypingExtensionsTypeVar('T', str, int, default=str)\ndef test_serialize_typevars_default_and_bound_with_user_model() -> None:\n    class MyErrorDetails(BaseModel):\n        bar: str\n\n    class ExtendedMyErrorDetails(MyErrorDetails):\n        foo: str\n\n    class MoreExtendedMyErrorDetails(ExtendedMyErrorDetails):\n        suu: str\n\n    msg = 'Pydantic does not support mixing more than one of TypeVar bounds, constraints and defaults'\n    with pytest.raises(NotImplementedError, match=msg):\n    type_var = TypingExtensionsTypeVar('type_var', bound=MyErrorDetails, default=ExtendedMyErrorDetails)\n\n        class _(BaseModel, Generic[T]):\n            x: T\n    class Error(BaseModel, Generic[type_var]):\n        message: str\n        details: type_var\n```"}, {"role": "assistant", "content": "```suggestion\r\n    class Error(BaseModel, Generic[T]):\r\n        message: str\r\n        details: T\r\n```"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10789, "url": "https://github.com/pydantic/pydantic/pull/10789#discussion_r1843474416"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_canvas.py`:\n\n```python\ndef test_when_sig(self):\n        s = self.add.s()\n        assert maybe_signature(s, app=self.app) is s\n\n\nclass test_merge_dictionaries(CanvasCase):\n\n    def test_docstring_example(self):\n        d1 = {'dict': {'a': 1}, 'list': [1, 2], 'tuple': (1, 2)}\n        d2 = {'dict': {'b': 2}, 'list': [3, 4], 'set': {'a', 'b'}}\n        _merge_dictionaries(d1, d2)\n        assert d1 == {\n            'dict': {'a': 1, 'b': 2},\n            'list': [1, 2, 3, 4],\n            'tuple': (1, 2),\n            'set': {'a', 'b'}\n        }\n\n    @pytest.mark.parametrize('d1,d2,expected_result', [\n```"}, {"role": "assistant", "content": "can you guys please review this specifically?\r\nThere are lots of \"None\" symbols there so >2 eyes reviewing it would be awesome.\r\nThanks \ud83d\ude4f"}], "_meta": {"repo": "celery/celery", "pr_number": 7942, "url": "https://github.com/celery/celery/pull/7942#discussion_r1039712073"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `parse.py`:\n\n```python\nself._dead_ilabels: Set[int] = set()\n        self._start_point = self.parser.stack\n        self._points = {ilabel: copy.deepcopy(self._start_point) for ilabel in ilabels}\n        self._points = {ilabel: stack_copy(self._start_point) for ilabel in ilabels}\n\n    @property\n    def ilabels(self) -> Set[int]:\n        return self._dead_ilabels.symmetric_difference(self._ilabels)\n\n    @contextmanager\n    def switch_to(self, ilabel: int) -> Iterator[None]:\n        self.parser.stack = self._points[ilabel]\n        with self.patch():\n            self.parser.stack = self._points[ilabel]\n            try:\n                yield\n            except ParseError:\n                self._dead_ilabels.add(ilabel)\n            finally:\n                self.parser.stack = self._start_point\n\n    @contextmanager\n    def patch(self) -> Iterator[None]:\n        \"\"\"\n        Patch basic state operations (push/pop/shift) with node-level\n```"}, {"role": "assistant", "content": "Should be reworded to remove mention of \"patching\""}], "_meta": {"repo": "psf/black", "pr_number": 2728, "url": "https://github.com/psf/black/pull/2728#discussion_r781419887"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `asyncio_enabled_reactor_different_loop.py`:\n\n```python\n\"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n    }\n)\nprocess.crawl(NoRequestsSpider)\nd = process.crawl(NoRequestsSpider)\nd.addErrback(lambda failure: failure.printTraceback())\n```"}, {"role": "assistant", "content": "The issue here is that we're not following https://docs.scrapy.org/en/latest/topics/practices.html#run-scrapy-from-a-script in the tests because it's problematic. So, it may be problematic for our users as well."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6038, "url": "https://github.com/scrapy/scrapy/pull/6038#discussion_r1324530155"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `bitsandbytes.py`:\n\n```python\ntarget_key, value = tuple(input_dict.items())[0]\n        value = value[0] if isinstance(value, list) else value\n\n        full_name = target_key\n        # update param name to get the weights instead of the quantized stats\n        target_key = self.hf_quantizer.get_param_name(target_key)\n        module, _ = get_module_from_name(model, target_key)\n\n        if not self.hf_quantizer.pre_quantized:\n            # Support models using `Conv1D` in place of `nn.Linear` (e.g. openai-community/gpt2) by transposing the weight matrix prior to quantization.\n            # Since weights are saved in the correct \"orientation\", we skip transposing when loading.\n            if issubclass(module.source_cls, Conv1D):\n                value = value.T\n            old_value = model.get_parameter_or_buffer(target_key)\n            new_value = bnb.nn.Params4bit(value, requires_grad=False, **old_value.__dict__).to(value.device)\n            # remove missing keys that were create when initializing Params4bit\n            for key in new_value.quant_state.as_dict(packed=True).keys():\n                missing_keys.discard(f\"{full_name}.{key}\")\n            module._is_hf_initialized = True\n            return {target_key: new_value}\n        else:\n            module_name = target_key.rsplit(\".\", 1)[0]\n            # Save the states for later quantization when they are all gathered\n            if not hasattr(self.hf_quantizer, \"param_quant_stats\"):\n                self.hf_quantizer.param_quant_stats = defaultdict(dict)\n            self.hf_quantizer.param_quant_stats[module_name].update({full_name: value})\n            missing_keys.discard(full_name)\n            # We are ready for quantization in this case (note, the +1 is for the weight itself)\n            if len(self.hf_quantizer.param_quant_stats[module_name]) == len(self.hf_quantizer.bnb_keys) + 1:\n                weight = self.hf_quantizer.param_quant_stats[module_name].pop(f\"{module_name}.weight\")\n                new_value = bnb.nn.Params4bit.from_prequantized(\n                    data=weight,\n                    quantized_stats=self.hf_quantizer.param_quant_stats[module_name],\n                    requires_grad=False,\n                    device=value.device,\n                    module=module,\n                )\n                module._is_hf_initialized = True\n                del self.hf_quantizer.param_quant_stats[module_name]\n                return {target_key: new_value}\n            return {}\n        # Support models using `Conv1D` in place of `nn.Linear` (e.g. openai-community/gpt2) by transposing the weight matrix prior to quantization.\n        # Since weights are saved in the correct \"orientation\", we skip transposing when loading.\n        if issubclass(module.source_cls, Conv1D):\n            value = value.T\n\n        old_value = model.get_parameter_or_buffer(target_key)\n        new_value = bnb.nn.Params4bit(value, requires_grad=False, **old_value.__dict__).to(value.device)\n        module._is_hf_initialized = True\n```"}, {"role": "assistant", "content": "just to be careful, i'm not sure how `_is_hf_initialized` works but i'm sure it will try to initialize somehow"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42426, "url": "https://github.com/huggingface/transformers/pull/42426#discussion_r2565723099"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `entry_points.py`:\n\n```python\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nfrom __future__ import annotations\n\nimport functools\nimport logging\nimport sys\nfrom collections import defaultdict\nfrom collections.abc import Iterator\n\nif sys.version_info >= (3, 12):\n    from importlib import metadata\nelse:\n    import importlib_metadata as metadata\n\nlog = logging.getLogger(__name__)\n\nEPnD = tuple[metadata.EntryPoint, metadata.Distribution]\n\n\"\"\"Re-export entry_points utilities from shared library for backward compatibility.\"\"\"\n\n@functools.cache\ndef _get_grouped_entry_points() -> dict[str, list[EPnD]]:\n    mapping: dict[str, list[EPnD]] = defaultdict(list)\n    for dist in metadata.distributions():\n        try:\n            for e in dist.entry_points:\n                mapping[e.group].append((e, dist))\n        except Exception as e:\n            log.warning(\"Error when retrieving package metadata (skipping it): %s, %s\", dist, e)\n    return mapping\n\n\ndef entry_points_with_dist(group: str) -> Iterator[EPnD]:\n    \"\"\"\n    Retrieve entry points of the given group.\n\n    This is like the ``entry_points()`` function from ``importlib.metadata``,\n    except it also returns the distribution the entry point was loaded from.\n\n    Note that this may return multiple distributions to the same package if they\n    are loaded from different ``sys.path`` entries. The caller site should\n    implement appropriate deduplication logic if needed.\nfrom __future__ import annotations\n\n    :param group: Filter results to only this entrypoint group\n    :return: Generator of (EntryPoint, Distribution) objects for the specified groups\n    \"\"\"\n    return iter(_get_grouped_entry_points()[group])\nfrom airflow._shared.module_loading.entry_points import (\n    EPnD as EPnD,\n    _get_grouped_entry_points as _get_grouped_entry_points,\n    entry_points_with_dist as entry_points_with_dist,\n    metadata as metadata,\n)\n```"}, {"role": "assistant", "content": "Here you go: https://github.com/apache/airflow/pull/60061"}], "_meta": {"repo": "apache/airflow", "pr_number": 59971, "url": "https://github.com/apache/airflow/pull/59971#discussion_r2658219297"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `networks.py`:\n\n```python\nimport email_validator\n    except ImportError as e:\n        raise ImportError('email-validator is not installed, run `pip install pydantic[email]`') from e\n    if getattr(email_validator, '__version__', '').partition('.')[0] == '2':\n        return\n    else:\n        input_email = 'testaddr@example.tld'\n        valid_email = email_validator.validate_email(input_email, check_deliverability=False)\n        if not hasattr(valid_email, 'normalized'):\n            raise ImportError('email-validator version >= 2.0 required')\n```"}, {"role": "assistant", "content": ",\r\n\r\nThe check works with `2.0.0`:\r\n\r\n```py\r\nimport email_validator\r\nfrom importlib.metadata import version\r\n\r\ndef import_email_validator() -> None:\r\n    global email_validator\r\n    try:\r\n        import email_validator\r\n    except ImportError as e:\r\n        raise ImportError('email-validator is not installed, run `pip install pydantic[email]`') from e\r\n\r\n    print(version(\"email-validator\"))\r\n    #> 2.0.0\r\n    \r\n    if not version(\"email-validator\").partition('.')[0] == '2':\r\n        raise ImportError('email-validator version >= 2.0 required, run pip install -U email-validator')\r\n\r\nimport_email_validator()\r\n```\r\n\r\nThis doesn't raise an error. I don't get an error if I'm using 2.1 either. I do get an error if I'm using < v2"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 6033, "url": "https://github.com/pydantic/pydantic/pull/6033#discussion_r1406777742"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `frozen_field.py`:\n\n```python\nfoo.a = 2\n# MYPY: error: Property \"a\" defined in \"Foo\" is read-only  [misc]\n\n\nclass Parent(BaseModel):\n    parent_attr: str = Field(exclude=True)\n\n\n# We don't want to freeze `parent_attr` in the plugin:\n```"}, {"role": "assistant", "content": "```suggestion\r\n# `parent_attr` is writable, mypy should error when overriding with a read-only property\r\n```"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 12290, "url": "https://github.com/pydantic/pydantic/pull/12290#discussion_r2391286422"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_typing_extra.py`:\n\n```python\ndef is_annotated(ann_type: Any) -> bool:\n    from ._utils import lenient_issubclass\n\n    origin = get_origin(ann_type)\n    return origin is not None and lenient_issubclass(origin, Annotated)\n    return (origin is not None) and (origin is Annotated)\n```"}, {"role": "assistant", "content": "```suggestion\r\n    return origin is Annotated\r\n```"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 9743, "url": "https://github.com/pydantic/pydantic/pull/9743#discussion_r1653380170"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_decoders.py`:\n\n```python\nraise DecodingError(str(exc)) from exc\n\n\nclass ZStandardDecoder(ContentDecoder):\n    \"\"\"\n    Handle 'zstd' RFC 8878 decoding.\n\n    Requires `pip install zstandard`.\n    Can be installed as a dependency of httpx using `pip install httpx[zstd]`.\n    \"\"\"\n\n    def __init__(self) -> None:\n        if zstd is False:  # pragma: no cover\n            raise ImportError(\n                \"Using 'ZStandardDecoder', ...\"\n                \"Make sure to install httpx using `pip install httpx[zstd]`.\"\n            ) from None\n\n        self.decompressor = zstd.ZstdDecompressor().decompressobj()\n\n    def decode(self, data: bytes) -> bytes:\n        try:\n            data_parts = [self.decompressor.decompress(data)]\n            while self.decompressor.eof and self.decompressor.unused_data:\n                unused_data = self.decompressor.unused_data\n                self.decompressor = zstd.ZstdDecompressor().decompressobj()\n                data_parts.append(self.decompressor.decompress(unused_data))\n```"}, {"role": "assistant", "content": "I'm not exactly sure what happens, but if I try to use `read_across_frames` I get decoding errors. Also, please note I've copied this logic from urllib3 so it is \"Best Practice\" :-) or at least proven to be working.\r\n\r\nI would be open for suggestions of course!"}], "_meta": {"repo": "encode/httpx", "pr_number": 3139, "url": "https://github.com/encode/httpx/pull/3139#discussion_r1518821040"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `comments2.py`:\n\n```python\nsyms.simple_stmt,\n        [\n            Node(statement, result),\n            Leaf(token.NEWLINE, '\\n'),  # FIXME: \\r\\n?\n            Leaf(token.NEWLINE, '\\n')  # FIXME: \\r\\n?\n```"}, {"role": "assistant", "content": "What caused this change? Black should put a trailing comma here."}], "_meta": {"repo": "psf/black", "pr_number": 826, "url": "https://github.com/psf/black/pull/826#discussion_r282331164"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `context.py`:\n\n```python\ntask_instance: RuntimeTaskInstanceProtocol\n    task_instance_key_str: str\n    # `templates_dict` is only set in PythonOperator\n    templates_dict: dict[str, Any] | None\n    templates_dict: NotRequired[dict[str, Any] | None]\n    test_mode: bool\n    ti: RuntimeTaskInstanceProtocol\n    # triggering_asset_events: Mapping[str, Collection[AssetEvent | AssetEventPydantic]]\n    triggering_asset_events: Any\n    try_number: int | None\n    try_number: NotRequired[int | None]\n```"}, {"role": "assistant", "content": "I didn't even think it should be none. By the time it gets to a worker, try number will always be a positive integer."}], "_meta": {"repo": "apache/airflow", "pr_number": 54421, "url": "https://github.com/apache/airflow/pull/54421#discussion_r2381514070"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `check_docstrings.py`:\n\n```python\n\"YolosImageProcessor\",\n    \"YolosModel\",\n    \"YosoConfig\",\n    \"YoutuConfig\",\n```"}, {"role": "assistant", "content": "Please run `make fix-repo` (without having this added to any exceptions) --> you will see it will update a few strings like I showed above, namely `initializer_range` and `embedding_initializer_range` - you might need to left tab them to have the same spacing again as before."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43166, "url": "https://github.com/huggingface/transformers/pull/43166#discussion_r2728541986"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `middleware.py`:\n\n```python\ndef __init__(self, *middlewares):\n        self.middlewares = middlewares\n        self.methods = defaultdict(deque)\n        self.methods: dict[str, deque[Callable]] = defaultdict(deque)\n```"}, {"role": "assistant", "content": "That's really strange! Python 3.6:\r\n\r\n```\r\n>>> def foo(x: dict[str, str]): pass\r\n...\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: 'type' object is not subscriptable\r\n```"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5077, "url": "https://github.com/scrapy/scrapy/pull/5077#discussion_r608783817"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_redirects.py`:\n\n```python\n@pytest.mark.usefixtures(\"async_environment\")\nasync def test_cannot_redirect_streaming_body():\n    client = AsyncClient(transport=MockTransport())\n    client = AsyncClient(transport=AsyncMockTransport())\n    url = \"https://example.org/redirect_body\"\n\n    async def streaming_body():\n        yield b\"Example request body\"\n        yield b\"Example request body\"  # pragma: nocover\n```"}, {"role": "assistant", "content": "Had to add this since we need a streaming body for this test, although is never consumed."}], "_meta": {"repo": "encode/httpx", "pr_number": 1012, "url": "https://github.com/encode/httpx/pull/1012#discussion_r433184602"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_result.py`:\n\n```python\nimport platform\nimport traceback\nfrom contextlib import contextmanager\nfrom unittest import mock\nfrom unittest.mock import Mock, call, patch\nfrom zoneinfo import ZoneInfo\n```"}, {"role": "assistant", "content": "> FAILED t/unit/tasks/test_result.py::test_AsyncResult::test_date_done[utc-2024-08-24T00:00:00+00:00] - zoneinfo._common.ZoneInfoNotFoundError: 'No time zone found with key utc'"}], "_meta": {"repo": "celery/celery", "pr_number": 8385, "url": "https://github.com/celery/celery/pull/8385#discussion_r1730378612"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_black.py`:\n\n```python\nos.unlink(tmp_file)\n        self.assertFormatEqual(expected, actual)\n\n    @patch(\"black.dump_to_file\", dump_to_stderr)\n    def test_one_empty_line(self) -> None:\n        for nl in [\"\\n\", \"\\r\\n\"]:\n            source = expected = nl\n            actual = fs(source)\n            self.assertFormatEqual(expected, actual)\n            black.assert_equivalent(source, actual)\n            black.assert_stable(source, actual, DEFAULT_MODE)\n\n    def test_one_empty_line_ff(self) -> None:\n        for nl in [\"\\n\", \"\\r\\n\"]:\n            expected = nl\n            tmp_file = Path(black.dump_to_file(nl))\n            try:\n                self.assertFalse(ff(tmp_file, write_back=black.WriteBack.YES))\n                with open(tmp_file, \"rb\") as f:\n                    actual = f.read().decode(\"utf8\")\n            finally:\n                os.unlink(tmp_file)\n            self.assertFormatEqual(expected, actual)\n```"}, {"role": "assistant", "content": "The tests failing on Windows seem to confirm that writing a newline actually writes a `\\r\\n` instead of `\\n`.\r\n\r\n```\r\nExpected tree:\r\nfile_input\r\n  ENDMARKER '\\n' ''\r\n/file_input\r\nActual tree:\r\nfile_input\r\n  ENDMARKER '\\r\\n' ''\r\n/file_input\r\n```\r\n\r\nI'm getting this on Windows:\r\n\r\n```python\r\n>>> with open(\"temp.py\", \"w\") as f:\r\n...     f.write(\"\\n\")\r\n...\r\n1\r\n>>>\r\n>>> with open(\"temp.py\", \"rb\") as f:\r\n...     print(f.read())\r\n...\r\nb'\\r\\n'                \r\n```"}], "_meta": {"repo": "psf/black", "pr_number": 3348, "url": "https://github.com/psf/black/pull/3348#discussion_r1002233432"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `json_schema.py`:\n\n```python\n\"\"\"\n        json_schema: JsonSchemaValue = {'type': 'object'}\n\n        keys_schema = self.generate_inner(schema['keys_schema']).copy() if 'keys_schema' in schema else {}\n        ks = schema['keys_schema'] if 'keys_schema' in schema else None\n```"}, {"role": "assistant", "content": "```suggestion\r\n        keys_core_schema = schema.get('keys_schema')\r\n```"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10478, "url": "https://github.com/pydantic/pydantic/pull/10478#discussion_r1775335287"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_llava.py`:\n\n```python\nif vision_feature_select_strategy not in [\"default\", \"full\"]:\n            raise ValueError(f\"Unexpected select feature strategy: {self.config.vision_feature_select_strategy}\")\n\n        kwargs = {k: v for k, v in kwargs.items() if v is not None}\n```"}, {"role": "assistant", "content": "this is done for `image_sizes` afair because we support two models from the same class, Llava and Pixtral. Can you check if it works with slow integration tests ?"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42564, "url": "https://github.com/huggingface/transformers/pull/42564#discussion_r2639494253"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `feedexport.py`:\n\n```python\nclass FeedSlot:\n    def __init__(\n        self,\n        file,\n        exporter,\n```"}, {"role": "assistant", "content": "We decided that as this only changes the `__init__` signature and the current usage of this class is to receive it in the `feed_slot_closed` signal handler, it's fine to just change this."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5847, "url": "https://github.com/scrapy/scrapy/pull/5847#discussion_r1238688165"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `mail.py`:\n\n```python\n# Import twisted.mail here because it is not available in python3\n        from twisted.internet import reactor\n        from twisted.mail.smtp import ESMTPSenderFactory\n        msg = BytesIO(msg)\n        from twisted import version as twisted_version\n```"}, {"role": "assistant", "content": "I did it to conform to the code that was already there. You think it's better to move them all up?"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5406, "url": "https://github.com/scrapy/scrapy/pull/5406#discussion_r810668132"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_feedexport.py`:\n\n```python\nmaxDiff = None\n\n    def test_init(self):\n        settings_dict = {\n            'FEED_URI': 'file:///tmp/foobar',\n            'FEED_STORAGES': {\n                'file': FileFeedStorageWithoutFeedOptions\n            },\n        }\n        crawler = get_crawler(settings_dict=settings_dict)\n        feed_exporter = FeedExporter.from_crawler(crawler)\n        with tempfile.NamedTemporaryFile() as temp:\n            settings_dict = {\n                'FEED_URI': f'file:///{temp.name}',\n```"}, {"role": "assistant", "content": "```suggestion\r\n                'FEED_URI': f'file://{temp.name}',\r\n```"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5198, "url": "https://github.com/scrapy/scrapy/pull/5198#discussion_r683291676"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `image_processing_auto.py`:\n\n```python\nimage_processor.save_pretrained(\"image-processor-test\")\n    image_processor_config = get_image_processor_config(\"image-processor-test\")\n    ```\"\"\"\n    resolved_config_file = cached_file(\n        pretrained_model_name_or_path,\n        IMAGE_PROCESSOR_NAME,\n        cache_dir=cache_dir,\n        force_download=force_download,\n        proxies=proxies,\n        token=token,\n        revision=revision,\n        local_files_only=local_files_only,\n        _raise_exceptions_for_gated_repo=False,\n        _raise_exceptions_for_missing_entries=False,\n        _raise_exceptions_for_connection_errors=False,\n    )\n    if resolved_config_file is None:\n    resolved_config_files = [\n        resolved_file\n        for filename in [IMAGE_PROCESSOR_NAME, PROCESSOR_NAME]\n        if (\n            resolved_file := cached_file(\n                pretrained_model_name_or_path,\n                filename=filename,\n                cache_dir=cache_dir,\n                force_download=force_download,\n                proxies=proxies,\n                token=token,\n                revision=revision,\n                local_files_only=local_files_only,\n                _raise_exceptions_for_gated_repo=False,\n                _raise_exceptions_for_missing_entries=False,\n                _raise_exceptions_for_connection_errors=False,\n            )\n        )\n        is not None\n    ]\n    # An empty list if none of the possible files is found in the repo\n    if not resolved_config_files:\n        logger.info(\n            \"Could not locate the image processor configuration file, will try to use the model config instead.\"\n        )\n        return {}\n\n    resolved_config_file = resolved_config_files[0]\n```"}, {"role": "assistant", "content": "good point, I haven't noticed that we're prioritizing `IMAGE_PROCESSOR` here. Imo the priority should be as follows if we find more than one file on the hub:\r\n\r\n`PROCESSOR -> IMAGE_PROCESSOR` and for videos `PROCESSOR -> VIDEO_PROCESSOR -> IMAGE_PROCESSOR`\r\n\r\nDo you think this priority makes sense? I will update accordingly"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42085, "url": "https://github.com/huggingface/transformers/pull/42085#discussion_r2503515197"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_zamba.py`:\n\n```python\nlayers.append(ZambaHybridLayer(ZambaAttentionDecoderLayer(config), linear, mamba))\n                if self._tied_weights_keys is None:\n                    self._tied_weights_keys = {\n                        rf\"layers.(?![{layer_id}])\\d+.shared_transf\": f\"layers.{layer_id}.shared_transf\"\n                        rf\"layers.(?!{layer_id}\\.)\\d+.shared_transf\": f\"layers.{layer_id}.shared_transf\"\n```"}, {"role": "assistant", "content": "Yea honestly how was it supposed to work without the dot :sweat_smile:"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43623, "url": "https://github.com/huggingface/transformers/pull/43623#discussion_r2746907676"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_validate_call.py`:\n\n```python\nfrom ._config import ConfigWrapper\nfrom ._namespace_utils import MappingNamespace, NsResolver, ns_for_function\n\n# Note: This does not play very well with type checkers. For example,\n# `a: LambdaType = lambda x: x` will raise a type error by Pyright.\nValidateCallSupportedTypes = Union[\n    LambdaType,\n    FunctionType,\n    MethodType,\n    BuiltinFunctionType,\n    BuiltinMethodType,\n    functools.partial,\n]\n\nVALIDATE_CALL_SUPPORTED_TYPES = get_args(ValidateCallSupportedTypes)\n```"}, {"role": "assistant", "content": "We could remove the `ValidateCallSupportedTypes` type alias and just define `VALIDATE_CALL_SUPPORTED_TYPES`, and replace occurrences of the type alias with `Callable[..., Any]`. I agree `get_args` is not pretty but we use this trick in some places already :)"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10627, "url": "https://github.com/pydantic/pydantic/pull/10627#discussion_r1801490331"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `postprocessing.py`:\n\n```python\nclass GzipPlugin:\n    \"\"\"\n    Compresses received data with with :py:mod:`gzip` module.\n```"}, {"role": "assistant", "content": "The plugins should be about compressing in a specific file format.\r\n\r\n```suggestion\r\n    Compresses received data using `gzip <https://en.wikipedia.org/wiki/Gzip>`_.\r\n```\r\n\r\nThe use of a Python module from the standard library is an implementation detail, and should not be covered in the documentation. Even if we link to the Python documentation about parameters, we could technically reimplement the plugin in the future without relying on the standard library module, while supporting the same option names and values (hence it being OK to link to that documentation for option reference)."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5190, "url": "https://github.com/scrapy/scrapy/pull/5190#discussion_r663675054"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `poolmanager.py`:\n\n```python\nif headers:\n            headers_.update(headers)\n\n        return headers_\n\n    def urlopen(self, method, url, **kw):\n    def urlopen(self, method, url, redirect=True, **kw):\n        \"Same as HTTP(S)ConnectionPool.urlopen, ``url`` must be absolute.\"\n        kw['assert_same_host'] = False\n        kw['headers'] = self._set_proxy_headers(url, headers=kw.get('headers'))\n        return self.proxy_pool.urlopen(method, url, **kw)\n        u = parse_url(url)\n\n        if u.scheme == \"http\":\n            # It's too late to set proxy headers on per-request basis for\n            # tunnelled HTTPS connections, should use\n            # constructor's proxy_headers instead.\n            kw['headers'] = self._set_proxy_headers(url, kw.get('headers',\n                                                                self.headers))\n            kw['headers'].update(self.proxy_headers)\n```"}, {"role": "assistant", "content": "> That network has a very large number of forks. For performance reasons, we don't show all of the available forked repositories in the Compare drop-down. We're looking into ways of improving this, but in the meantime you can generate a comparison by manually entering the URL. Try this one, for example:\n> \n> https://github.com/Lukasa/requests/compare/schlamar:master...master"}], "_meta": {"repo": "psf/requests", "pr_number": 1515, "url": "https://github.com/psf/requests/pull/1515#discussion_r5772110"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `components.py`:\n\n```python\nreturn self\n\n    def start(self, w):\n        pass\n        # ensure the kombu hub's poller is created (accessing the\n        # property ensures this)\n```"}, {"role": "assistant", "content": "[nitpick] The comment states \"ensure the kombu hub's poller is created (accessing the property ensures this)\" but the code assigns the result to a throwaway variable `_`. This is a valid pattern, but could we add clarity by explaining *why* the poller needs to be initialized here?\n\nSince `asynloop()` no longer calls `hub.reset()` during shutdown (removed in this PR), this initialization becomes critical to ensure the poller is ready before the event loop starts. This rationale would help future maintainers understand why this seemingly no-op line is essential.\n\nSuggested enhancement:\n```python\ndef start(self, w):\n    # Ensure the kombu hub's poller is initialized before the event loop starts.\n    # Since asynloop() no longer resets the hub on exit (to preserve timers\n    # during shutdown), we must initialize the poller upfront.\n    _ = w.hub.poller\n```\n```suggestion\n        # Ensure the kombu hub's poller is initialized before the event loop starts.\n        # Since asynloop() no longer resets the hub on exit (to preserve timers\n        # during shutdown), we must initialize the poller upfront.\n```"}], "_meta": {"repo": "celery/celery", "pr_number": 9986, "url": "https://github.com/celery/celery/pull/9986#discussion_r2554637875"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `crypto.py`:\n\n```python\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nfrom __future__ import annotations\n\nimport logging\nfrom functools import cache\nfrom typing import Protocol\n\nlog = logging.getLogger(__name__)\n\n\nclass FernetProtocol(Protocol):\n    \"\"\"This class is only used for TypeChecking (for IDEs, mypy, etc).\"\"\"\n\n    is_encrypted: bool\n\n    def decrypt(self, msg: bytes | str, ttl: int | None = None) -> bytes:\n        \"\"\"Decrypt with Fernet.\"\"\"\n        ...\n\n    def encrypt(self, msg: bytes) -> bytes:\n        \"\"\"Encrypt with Fernet.\"\"\"\n        ...\n\n\nclass _NullFernet:\n    \"\"\"\n    A \"Null\" encryptor class that doesn't encrypt or decrypt but that presents a similar interface to Fernet.\n\n    The purpose of this is to make the rest of the code not have to know the\n    difference, and to only display the message once, not 20 times when\n    `airflow db migrate` is run.\n    \"\"\"\n\n    is_encrypted = False\n\n    def decrypt(self, msg: bytes | str, ttl: int | None = None) -> bytes:\n        \"\"\"Decrypt with Fernet.\"\"\"\n        if isinstance(msg, bytes):\n            return msg\n        if isinstance(msg, str):\n            return msg.encode(\"utf-8\")\n        raise ValueError(f\"Expected bytes or str, got {type(msg)}\")\n\n    def encrypt(self, msg: bytes) -> bytes:\n        \"\"\"Encrypt with Fernet.\"\"\"\n        return msg\n\n\nclass _RealFernet:\n    \"\"\"\n    A wrapper around the real Fernet to set is_encrypted to True.\n\n    This class is only used internally to avoid changing the interface of\n    the get_fernet function.\n    \"\"\"\n\n    from cryptography.fernet import Fernet, MultiFernet\n\n    is_encrypted = True\n\n    def __init__(self, fernet: MultiFernet):\n        self._fernet = fernet\n\n    def decrypt(self, msg: bytes | str, ttl: int | None = None) -> bytes:\n        \"\"\"Decrypt with Fernet.\"\"\"\n        return self._fernet.decrypt(msg, ttl)\n\n    def encrypt(self, msg: bytes) -> bytes:\n        \"\"\"Encrypt with Fernet.\"\"\"\n        return self._fernet.encrypt(msg)\n\n    def rotate(self, msg: bytes | str) -> bytes:\n```"}, {"role": "assistant", "content": "I think its a separate discussion about having to delete the NullFernet and using fixture instead as I see from linked PRs. To reach a status quo I documented it instead: adding unit tests for crypto"}], "_meta": {"repo": "apache/airflow", "pr_number": 60771, "url": "https://github.com/apache/airflow/pull/60771#discussion_r2706970976"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `generate_schema.py`:\n\n```python\nimport json\nfrom typing import IO, Any\n\nimport click\n\nimport black\n\n\ndef generate_schema_from_click(\n    cmd: click.Command,\n) -> dict[str, Any]:\n    result: dict[str, dict[str, Any]] = {}\n    for param in cmd.params:\n        if not isinstance(param, click.Option) or param.is_eager:\n            continue\n\n        assert param.name\n        name = param.name.replace(\"_\", \"-\")\n\n        result[name] = {}\n\n        match param.type:\n            case click.types.IntParamType():\n                result[name][\"type\"] = \"integer\"\n            case click.types.StringParamType() | click.types.Path():\n                result[name][\"type\"] = \"string\"\n            case click.types.Choice(choices=choices):\n                result[name][\"enum\"] = choices\n            case click.types.BoolParamType():\n                result[name][\"type\"] = \"boolean\"\n            case _:\n                msg = f\"{param.type!r} not a known type for {param}\"\n                raise TypeError(msg)\n\n        if param.multiple:\n            result[name] = {\"type\": \"array\", \"items\": result[name]}\n\n        result[name][\"description\"] = param.help\n\n        if param.default is not None and not param.multiple:\n            result[name][\"default\"] = param.default\n\n    return result\n\n\n@click.command(context_settings={\"help_option_names\": [\"-h\", \"--help\"]})\n@click.option(\"--schemastore\", is_flag=True, help=\"SchemaStore format\")\n@click.option(\"--outfile\", type=click.File(mode=\"w\"), help=\"Write to file\")\ndef main(schemastore: bool, outfile: IO[str]) -> None:\n    properties = generate_schema_from_click(black.main)\n    del properties[\"line-ranges\"]\n\n    schema: dict[str, Any] = {\n        \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n        \"$id\": (\n            \"https://github.com/psf/black/blob/main/black/resources/black.schema.json\"\n        ),\n        \"$comment\": \"black table in pyproject.toml\",\n        \"type\": \"object\",\n        \"additionalProperties\": False,\n        \"properties\": properties,\n    }\n\n    if schemastore:\n        schema[\"$id\"] = (\"https://json.schemastore.org/partial-black.json\",)\n        schema[\"properties\"][\"enable-unstable-feature\"][\"items\"] = {\"type\": \"string\"}\n```"}, {"role": "assistant", "content": "```suggestion\r\n        # The precise list of unstable features may change frequently, so don't bother putting it in SchemaStore\r\n        schema[\"properties\"][\"enable-unstable-feature\"][\"items\"] = {\"type\": \"string\"}\r\n```"}], "_meta": {"repo": "psf/black", "pr_number": 4181, "url": "https://github.com/psf/black/pull/4181#discussion_r1468511652"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modular_nanochat.py`:\n\n```python\n# coding=utf-8\n# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\nfrom collections.abc import Callable\nfrom typing import Optional, Union\n\nimport torch\nimport torch.nn as nn\n\nfrom ...activations import ACT2FN\nfrom ...cache_utils import Cache, DynamicCache\nfrom ...generation import GenerationMixin\nfrom ...masking_utils import create_causal_mask\nfrom ...modeling_layers import GradientCheckpointingLayer\nfrom ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\nfrom ...modeling_utils import ALL_ATTENTION_FUNCTIONS\nfrom ...processing_utils import Unpack\nfrom ...utils import TransformersKwargs, auto_docstring, can_return_tuple\nfrom ...utils.generic import check_model_inputs\nfrom ..llama.modeling_llama import LlamaPreTrainedModel, LlamaRotaryEmbedding, apply_rotary_pos_emb, eager_attention_forward\nfrom ..llama4.modeling_llama4 import Llama4TextL2Norm\nfrom .configuration_nanochat import NanoChatConfig\n\n\nclass NanoChatRMSNorm(Llama4TextL2Norm):\n    \"\"\"\n    NanoChatRMSNorm inherits from Llama4TextL2Norm (weight-less RMS normalization).\n    Overrides __init__ to match NanoChat's API with hidden_size parameter.\n    \"\"\"\n\n    def __init__(self, hidden_size, eps=1e-6):\n        super().__init__(eps=eps)\n        self.hidden_size = hidden_size\n```"}, {"role": "assistant", "content": "Let's change this in the inits of the other modules instead. It's weird to have hidden_size when it's unused"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41634, "url": "https://github.com/huggingface/transformers/pull/41634#discussion_r2444862080"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_utils.py`:\n\n```python\nself.config._attn_implementation_internal = self._check_and_adjust_attn_implementation(\n            self.config._attn_implementation, is_init_check=True\n        )\n        if self.can_generate():\n            self.generation_config = GenerationConfig.from_model_config(config)\n```"}, {"role": "assistant", "content": "It the same line which I deleted below and tbh no reason to change the order. Most prob there was a reason at first, but then I changed my mind \ud83d\ude04"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41695, "url": "https://github.com/huggingface/transformers/pull/41695#discussion_r2534735529"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `base.py`:\n\n```python\nclass RawURL:\n    def __init__(self, scheme: bytes, host: bytes, port: int, target: bytes) -> None:\n    def __init__(\n        self, scheme: bytes, host: bytes, port: Optional[int], target: bytes\n    ) -> None:\n        self.scheme = scheme\n        self.host = host\n        self.port = port\n        self.target = target\n\n    @property\n    def origin(self) -> Origin:\n        return Origin(self.scheme, self.host, self.port)\n        default_port = {b\"http\": 80, b\"https\": 443}[self.scheme]\n        return Origin(self.scheme, self.host, self.port or default_port)\n\n    def __str__(self) -> str:\n        scheme = self.scheme.decode(\"ascii\")\n        host = self.host.decode(\"ascii\")\n        port_str = \"f:{self.port}\" if self.port else \"\"\n```"}, {"role": "assistant", "content": "```suggestion\r\n        port_str = f\":{self.port}\" if self.port else \"\"\r\n```"}], "_meta": {"repo": "encode/httpx", "pr_number": 1765, "url": "https://github.com/encode/httpx/pull/1765#discussion_r676015127"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `httpobj.py`:\n\n```python\n\"\"\"Helper functions for scrapy.http objects (Request, Response)\"\"\"\n\nimport weakref\nfrom urllib.parse import urlparse\nfrom weakref import WeakKeyDictionary\n\n\n_urlparse_cache = weakref.WeakKeyDictionary()\n_urlparse_cache: WeakKeyDictionary = WeakKeyDictionary()\n```"}, {"role": "assistant", "content": "Would it be possible to type annotate keys/values here, and for other caches (`_fingerprint_cache`, etc.)? It is a chance to make type annotations a bit more useful for reading (and probably using) the code; otherwise I suspect we'll never get to this - mypy error is silenced, all is good :)"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4895, "url": "https://github.com/scrapy/scrapy/pull/4895#discussion_r529919791"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `core_model_loading.py`:\n\n```python\nhf_quantizer,\n                        )\n\n                # Cleanup the tensors\n                # Cleanup the tensors that were gathered internally in the mapping\n                mapping.reset()\n\n            except SkipLayer:\n                continue\n\n    # Keep the current weight conversion mapping for later saving (in case it was coming directly from the user)\n    model._weight_conversions = weight_mapping\n    thread_pool.shutdown(wait=False)\n    if thread_pool is not None:\n        thread_pool.shutdown(wait=False)\n```"}, {"role": "assistant", "content": "should be a finally as well if someone does a ctrl  + C while loading"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42619, "url": "https://github.com/huggingface/transformers/pull/42619#discussion_r2589339833"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `serialized_dag.py`:\n\n```python\nreturn [cls._sort_serialized_dag_dict(i) for i in serialized_dag]\n        return serialized_dag\n\n    @classmethod\n    def _generate_deadline_uuids(cls, dag_data: dict[str, Any]) -> dict[str, dict]:\n        \"\"\"\n        Generate UUIDs for DeadlineAlerts and replace dicts with list[UUID] in dag_data.\n\n        This modifies dag_data in place, replacing deadline alert definitions with UUID strings.\n        Called before SerializedDagModel creation to ensure UUIDs are included in the hash.\n\n        :param dag_data: The serialized DAG data dictionary\n        :return: Mapping of UUID strings to deadline alert data dicts\n        \"\"\"\n        uuid_mapping: dict[str, dict] = {}\n\n        dag_deadline_data = dag_data.get(\"dag\", {}).get(\"deadline\")\n        if not dag_deadline_data:\n            return uuid_mapping\n\n        for deadline_alert in dag_deadline_data:\n            deadline_data = deadline_alert.get(Encoding.VAR, deadline_alert)\n\n            deadline_uuid = str(uuid6.uuid7())\n            uuid_mapping[deadline_uuid] = deadline_data\n\n        dag_data[\"dag\"][\"deadline\"] = list(uuid_mapping.keys())\n\n        return uuid_mapping\n\n    @classmethod\n    def _create_deadline_alert_records(\n        cls,\n        serialized_dag: SerializedDagModel,\n        uuid_mapping: dict[str, dict],\n    ) -> None:\n        \"\"\"\n        Create DeadlineAlert records in the database and appends them to serialized_dag.\n\n        :param serialized_dag: The SerializedDagModel instance (not yet flushed)\n        :param uuid_mapping: Mapping of UUID strings to deadline alert data dicts\n        \"\"\"\n        if not uuid_mapping:\n            return\n\n        for uuid_str, deadline_data in uuid_mapping.items():\n            alert = DeadlineAlertModel(\n                id=uuid_str,\n                reference=deadline_data[DeadlineAlertFields.REFERENCE],\n                interval=deadline_data[DeadlineAlertFields.INTERVAL],\n                callback_def=deadline_data[DeadlineAlertFields.CALLBACK],\n            )\n            serialized_dag.deadline_alerts.append(alert)\n\n    @classmethod\n    def _get_existing_deadline_uuids(\n```"}, {"role": "assistant", "content": "I don't see this being called anywhere here. Am I missing something?"}], "_meta": {"repo": "apache/airflow", "pr_number": 58248, "url": "https://github.com/apache/airflow/pull/58248#discussion_r2608609874"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `text_to_audio.py`:\n\n```python\nelse:\n            waveform_key = \"waveform\"\n\n        # We directly get the waveform\n        if self.no_processor:\n            if isinstance(audio, dict):\n                waveform = audio[waveform_key]\n            elif isinstance(audio, tuple):\n                waveform = audio[0]\n            else:\n                waveform = audio\n        # Or we need to postprocess to get the waveform\n        if isinstance(audio, dict):\n            waveform = audio[waveform_key]\n        elif isinstance(audio, tuple):\n            waveform = audio[0]\n        else:\n            waveform = self.processor.decode(audio)\n```"}, {"role": "assistant", "content": "Reopening because I want to verify that Dia works with this current version - I'm pretty sure we need the processor to decode for Dia which is why I wrote the initial long message on how we plan to standardize\r\n- Everything handled by the model, audio tokenizer within it already\r\n- Separate model / tokenizer, processor handles encoding/decoding into codebooks/waveform"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42326, "url": "https://github.com/huggingface/transformers/pull/42326#discussion_r2557052519"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `processing_ernie4_5_vl.py`:\n\n```python\n# coding=utf-8\n# Copyright 2025 Baidu and HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom typing import Optional, Union\n\nimport numpy as np\n\nfrom ...image_processing_utils import BatchFeature\nfrom ...image_utils import ImageInput\nfrom ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\nfrom ...tokenization_utils_base import PreTokenizedInput, TextInput\nfrom ...video_utils import VideoInput\n\n\nclass Ernie4_5_VLProcessorKwargs(ProcessingKwargs, total=False):\n    _defaults = {\n        \"text_kwargs\": {\n            \"padding\": False,\n            \"return_token_type_ids\": False,\n            \"return_mm_token_type_ids\": True,\n        },\n    }\n\n\nclass Ernie4_5_VLProcessor(ProcessorMixin):\n    r\"\"\"\n    Constructs a Ernie 4.5 VL processor which wraps a Ernie 4.5 VL image processor and a Llama tokenizer into a single processor.\n    [`Ernie4_5_VLProcessor`] offers all the functionalities of [`Ernie4_5_VLImageProcessor`] and [`LlamaTokenizerFast`]. See the\n    [`~Ernie4_5_VLProcessor.__call__`] and [`~Ernie4_5_VLProcessor.decode`] for more information.\n    Args:\n        image_processor ([`Ernie4_5_VLImageProcessor`], *optional*):\n            The image processor is a required input.\n        tokenizer ([`LlamaTokenizerFast`], *optional*):\n            The tokenizer is a required input.\n        video_processor ([`Ernie4_5_VLVideoProcessor`], *optional*):\n            The video processor is a required input.\n        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n            in a chat into a tokenizable string.\n    \"\"\"\n\n    attributes = [\"image_processor\", \"tokenizer\", \"video_processor\"]\n    image_processor_class = \"AutoImageProcessor\"\n    video_processor_class = \"AutoVideoProcessor\"\n\n    tokenizer_class = (None, \"LlamaTokenizerFast\")\n```"}, {"role": "assistant", "content": "cc  to take a look after Ernie-VL is merged, if you get a chance\r\n\r\nIMO we should let models use auto-class as long as the auto-map has the correct classes. In Ernie the auto-tokenizer mapping has no slow tokenizer defined, so I'd expect it to catch up and load a fast tokenizer"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 39585, "url": "https://github.com/huggingface/transformers/pull/39585#discussion_r2522642786"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `__init__.py`:\n\n```python\ndef __init__(self, item=None, selector=None, response=None, parent=None, **context):\n        if selector is None and response is not None:\n            selector = self.default_selector_class(response)\n            selector = None if not (isinstance(response, HtmlResponse) or isinstance(response, XmlResponse)) else self.default_selector_class(response)\n```"}, {"role": "assistant", "content": "The problem I see with this approach is that it seems more restrictive that it needs to be. It does not account for custom response classes inheriting from `TextResponse`, which would currently provide an HTML-based selector, but after this change would get no selector. So this change has the potential of breaking existing code.\r\n\r\nAt the moment, all a response object needs to have to get a valid Scrapy selector is to have a `text` attribute. I feel we should not impose any requirement on top of that.\r\n\r\nFurthermore, if in the future the selector class of Scrapy gains support for an alternative logic, e.g. reading the `body` attribute instead of `text` (BSON support, to follow the upcoming JSON support?), we would need to change the code here as well to make things work. Because of that, I\u2019m inclined to suggest that, instead of checking anything in `response` here, we should always pass it to the default selector class, and upon an AttributeError exception (or some new, custom exception?) set `selector` to `None`."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5269, "url": "https://github.com/scrapy/scrapy/pull/5269#discussion_r725596236"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `comms.py`:\n\n```python\nerr_decoder: TypeAdapter[ErrorResponse] = attrs.field(factory=lambda: TypeAdapter(ToTask), repr=False)\n\n    # Threading lock for sync operations\n    _thread_lock: threading.Lock = attrs.field(factory=threading.Lock, repr=False)\n    # Async lock for async operations\n    _async_lock: asyncio.Lock = attrs.field(factory=asyncio.Lock, repr=False)\n```"}, {"role": "assistant", "content": "This is worth consolidating with the async lock on Trigger since `TriggerCommsDecoder` inherits this\n\nhttps://github.com/apache/airflow/blob/0b341e6b92040360f86f5697da119c58a91aa4c2/airflow-core/src/airflow/jobs/triggerer_job_runner.py#L783"}], "_meta": {"repo": "apache/airflow", "pr_number": 59087, "url": "https://github.com/apache/airflow/pull/59087#discussion_r2672640943"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `versions.py`:\n\n```python\nfrom __future__ import annotations\n\nimport platform\nimport sys\nfrom importlib.metadata import version\nfrom warnings import warn\n\nimport cryptography\nimport cssselect\nimport lxml.etree\nimport parsel\nimport twisted\nimport w3lib\n\nimport scrapy\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.settings.default_settings import LOG_VERSIONS\nfrom scrapy.utils.ssl import get_openssl_version\n\n_DEFAULT_SOFTWARE = [\"Scrapy\"] + LOG_VERSIONS\n\n\ndef _version(item):\n    lowercase_item = item.lower()\n    if lowercase_item == \"libxml2\":\n        return \".\".join(map(str, lxml.etree.LIBXML_VERSION))\n    if lowercase_item == \"platform\":\n        return platform.platform()\n    if lowercase_item == \"pyopenssl\":\n        return get_openssl_version()\n    if lowercase_item == \"python\":\n        return sys.version.replace(\"\\n\", \"- \")\n    return version(item)\n\n\ndef get_versions(\n    software: list | None = None,\n) -> list[tuple[str, str]]:\n    software = software or _DEFAULT_SOFTWARE\n    return [(item, _version(item)) for item in software]\n\n\ndef scrapy_components_versions() -> list[tuple[str, str]]:\n    lxml_version = \".\".join(map(str, lxml.etree.LXML_VERSION))\n    libxml2_version = \".\".join(map(str, lxml.etree.LIBXML_VERSION))\n\n    return [\n        (\"Scrapy\", scrapy.__version__),\n        (\"lxml\", lxml_version),\n        (\"libxml2\", libxml2_version),\n        (\"cssselect\", cssselect.__version__),\n        (\"parsel\", parsel.__version__),\n        (\"w3lib\", w3lib.__version__),\n        (\"Twisted\", twisted.version.short()),\n        (\"Python\", sys.version.replace(\"\\n\", \"- \")),\n        (\"pyOpenSSL\", get_openssl_version()),\n        (\"cryptography\", cryptography.__version__),\n        (\"Platform\", platform.platform()),\n    ]\n    warn(\n        (\n            \"scrapy.utils.versions.scrapy_components_versions is deprecated, \"\n            \"use scrapy.utils.versions.get_versions instead.\"\n```"}, {"role": "assistant", "content": "```suggestion\r\n            \"scrapy.utils.versions.scrapy_components_versions() is deprecated, \"\r\n            \"use scrapy.utils.versions.get_versions() instead.\"\r\n```"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6582, "url": "https://github.com/scrapy/scrapy/pull/6582#discussion_r1886674988"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `__init__.py`:\n\n```python\n\" functionality in the next major release.\"\n    ),\n)\n@click.option(\n    \"--unstable\",\n    is_flag=True,\n    help=(\n        \"Enable potentially disruptive style changes that have known bugs or are not\"\n        \" currently expected to make it into the stable style Black's next major\"\n        \" release. Implies --preview.\"\n    ),\n)\n@click.option(\n    \"--enable-unstable-feature\",\n    type=click.Choice([v.name for v in UNSTABLE_FEATURES]),\n```"}, {"role": "assistant", "content": "It could be worth it to be more permissive here, so folks can set `--enable-unstable-feature hug_parens` and forget about it for a year. But I think that much bother is probably fine for preview..."}], "_meta": {"repo": "psf/black", "pr_number": 4096, "url": "https://github.com/psf/black/pull/4096#discussion_r1466942632"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `tokenization_auto.py`:\n\n```python\ndef tokenizer_class_from_name(class_name: str) -> Union[type[Any], None]:\n    if class_name == \"BloomTokenizer\":\n        return TokenizersBackend\n```"}, {"role": "assistant", "content": "Not sure about this, the tokenizer should be fixed in a separate pr, and team members are already working on an other fix"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41567, "url": "https://github.com/huggingface/transformers/pull/41567#discussion_r2602742421"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_content.py`:\n\n```python\nassert isinstance(stream, httpx.SyncByteStream)\n    assert isinstance(stream, httpx.AsyncByteStream)\n\n    sync_content = stream.read()\n    async_content = await stream.aread()\n```"}, {"role": "assistant", "content": "We're matching the style of the other test cases now.\r\nThese `.read()`/`.aread()` operations were only added to this test case so that we'd have coverage of those code lines."}], "_meta": {"repo": "encode/httpx", "pr_number": 2407, "url": "https://github.com/encode/httpx/pull/2407#discussion_r994626502"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `dependency_versions_table.py`:\n\n```python\n\"opentelemetry-api\": \"opentelemetry-api\",\n    \"opentelemetry-exporter-otlp\": \"opentelemetry-exporter-otlp\",\n    \"opentelemetry-sdk\": \"opentelemetry-sdk\",\n    \"sudachipy\": \"sudachipy>=0.6.6\",\n```"}, {"role": "assistant", "content": "this file was updated by `make fix-repo` - not sure why it changed the order, I will revert, run `make fix-repo` and see if it keeps it unchanged"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43490, "url": "https://github.com/huggingface/transformers/pull/43490#discussion_r2745344151"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `hardware_metrics.py`:\n\n```python\nimport json\nimport logging\nimport subprocess\nimport sys\nimport threading\nimport time\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom logging import Logger\nfrom multiprocessing import Pipe, Process\nfrom multiprocessing.connection import Connection\n\nfrom transformers.utils.import_utils import is_cuda_platform, is_rocm_platform\n\n\nif is_cuda_platform():\n    import gpustat\n```"}, {"role": "assistant", "content": "Makes sense. I\u2019ll test it locally and switch to it if everything works as expected."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43552, "url": "https://github.com/huggingface/transformers/pull/43552#discussion_r2736776428"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_tutorial001.py`:\n\n```python\nimport importlib\n\nimport pytest\nfrom fastapi.testclient import TestClient\nfrom pytest import MonkeyPatch\n\nfrom ...utils import needs_pydanticv2\nfrom ...utils import needs_pydanticv1, needs_pydanticv2\n\n\n@pytest.fixture(\n    params=[\n        pytest.param(\"tutorial001\", marks=needs_pydanticv2),\n        pytest.param(\"tutorial001_pv1\", marks=needs_pydanticv1),\n    ],\n)\ndef get_app(request: pytest.FixtureRequest):\n    def app_wrapper():\n        mod = importlib.import_module(f\"docs_src.settings.{request.param}\")\n        return mod.app\n\n    return app_wrapper\n```"}, {"role": "assistant", "content": "I think that it is better to monkeypatch `get_app()`, instead of test_settings:\r\n\r\n```python\r\n\r\n.fixture(\r\n    name=\"app\",\r\n    params=[\r\n        pytest.param(\"tutorial001\", marks=needs_pydanticv2),\r\n        pytest.param(\"tutorial001_pv1\", marks=needs_pydanticv1),\r\n    ],\r\n)\r\ndef get_app(request: pytest.FixtureRequest, monkeypatch: MonkeyPatch):\r\n    monkeypatch.setenv(\"ADMIN_EMAIL\", \"admin.com\")\r\n    mod = importlib.import_module(f\"docs_src.settings.{request.param}\")\r\n    return mod.app\r\n\r\n\r\ndef test_settings(app):\r\n    client = TestClient(app)\r\n    response = client.get(\"/info\")\r\n    assert response.status_code == 200, response.text\r\n    assert response.json() == {\r\n        \"app_name\": \"Awesome API\",\r\n        \"admin_email\": \"admin.com\",\r\n        \"items_per_user\": 50,\r\n    }\r\n\r\n```"}], "_meta": {"repo": "fastapi/fastapi", "pr_number": 13505, "url": "https://github.com/fastapi/fastapi/pull/13505#discussion_r2005481524"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `return_annotation_brackets.py`:\n\n```python\ndef foo(a: int, b: int, c: int,) -> int:\n    return 2\n\n# Deeply nested brackets\n# with *interesting* spacing\n```"}, {"role": "assistant", "content": "You missed an opportunity to write ASCII art using weirdly spaced parentheses"}], "_meta": {"repo": "psf/black", "pr_number": 2990, "url": "https://github.com/psf/black/pull/2990#discussion_r841128433"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `trainer.py`:\n\n```python\nReturn:\n            `torch.Tensor`: The tensor with training loss on this batch.\n        \"\"\"\n        model.train()\n        if hasattr(self.optimizer, \"train\") and callable(self.optimizer.train):\n            self.optimizer.train()\n        # Prepare buffers for context parallelism\n\n        inputs = self._prepare_inputs(inputs)\n        if is_sagemaker_mp_enabled():\n            loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps)\n            return loss_mb.reduce_mean().detach().to(self.args.device)\n        cp_context, inputs = self._prepare_cp(model, inputs)\n```"}, {"role": "assistant", "content": "can we think of another name instead of _prepare_cp ? We have the same function in accelerate so it makes things slightly confusing"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 40205, "url": "https://github.com/huggingface/transformers/pull/40205#discussion_r2291488536"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_tokenization_blenderbot.py`:\n\n```python\ndef test_3B_tokenization_same_as_parlai_rust_tokenizer(self):\n        assert self.rust_tokenizer_3b.add_prefix_space\n        assert self.rust_tokenizer_3b([\" Sam\", \"Sam\"]).input_ids == [[5502, 2], [5502, 2]]\n\n    @require_jinja\n    def test_tokenization_for_chat(self):\n```"}, {"role": "assistant", "content": "It's extremely old - it's not related to this PR really, but these tests come from before chat templates, and we just patched them to support chat templates after chat templates were added. They only exist for a few models, and I think we don't want to keep them, because it's not clear what they test that the main chat template tests don't."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41626, "url": "https://github.com/huggingface/transformers/pull/41626#discussion_r2478846898"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `base.py`:\n\n```python\nmodel = model_class.from_pretrained(model, **kwargs)\n                # Stop loading on the first successful load.\n                break\n            except (OSError, ValueError, TypeError, RuntimeError):\n            except (OSError, ValueError, TypeError, RuntimeError) as e:\n                # `from_pretrained` may raise a `TypeError` or `RuntimeError` when the requested `dtype`\n                # is not supported on the execution device (e.g. bf16 on a consumer GPU). We capture those so\n                # we can transparently retry the load in float32 before surfacing an error to the user.\n                fallback_tried = False\n\n                # Check if it's a safetensors-related error and retry without safetensors\n                error_msg = str(e).lower()\n                if isinstance(e, OSError) and \"safetensors\" in error_msg:\n```"}, {"role": "assistant", "content": "I don\u2019t think so. If it\u2019s a safetensors problem, we would either successfully load the model with `safetensors=False`, or otherwise continue to the next class. In the case where we check for a dtype-related error, we can always be sure that fallback_tried=False. \r\n\r\nTo be more strict, the error message contains this : `\"use use_safetensors=False to load the original model.\"` I can do something like `and \"use use_safetensors=False to load the original model.\" in error_msg`"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43178, "url": "https://github.com/huggingface/transformers/pull/43178#discussion_r2686656226"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `engine.py`:\n\n```python\n)\n        return scheduler_cls\n\n    @inlineCallbacks\n    def start(self) -> Generator[Deferred[Any], Any, None]:\n    @deferred_f_from_coro_f\n    async def start(self, _start_request_processing=True) -> None:\n```"}, {"role": "assistant", "content": "About the `_start_request_processing` parameter:\r\n\r\nStarting `spider.start()`  and `scheduler.next_request()` processing before `engine.start()` was problematic, because `engine.start()` sets `engine.running = True`, and before that, `engine.needs_backout()` returns `True`. This was one of the causes of the behavior differences in reactors, the asyncio one would happen to start request processing late enough for `engine.running` to be `True` already, but the default reactor would not.\r\n\r\nSo I concluded that request processing should be delayed until the engine starts.\r\n\r\nSo I moved the corresponding lines from `engine.open_spider()` to a separate, public method (should it be private?), and made `engine.start()` call it by default after setting `engine.running` to `True`. But I added a private parameter to allow disabling that, since the shell needs to delay that further.\r\n\r\nI wonder if this change could pave the way to make the shell open the spider before starting the engine, in line with `scrapy crawl`, but I don\u2019t want to go down that rabbit hole now, not even as part of the follow-up PR."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6729, "url": "https://github.com/scrapy/scrapy/pull/6729#discussion_r2014920524"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `engine.py`:\n\n```python\nsignals.request_dropped, request=request, spider=spider\n            )\n\n    def download(self, request: Request, spider: Optional[Spider] = None) -> Deferred:\n    def download(self, request: Request) -> Deferred:\n        \"\"\"Return a Deferred which fires with a Response as result, only downloader middlewares are applied\"\"\"\n        if spider is not None:\n            warnings.warn(\n                \"Passing a 'spider' argument to ExecutionEngine.download is deprecated\",\n                category=ScrapyDeprecationWarning,\n                stacklevel=2,\n            )\n            if spider is not self.spider:\n                logger.warning(\n                    \"The spider '%s' does not match the open spider\", spider.name\n                )\n        if self.spider is None:\n            raise RuntimeError(f\"No open spider to crawl: {request}\")\n        return self._download(request, spider).addBoth(\n            self._downloaded, request, spider\n        return self._download(request, self.spider).addBoth(\n            self._downloaded, request, self.spider\n```"}, {"role": "assistant", "content": "Maybe remove this parameter altogether from `_download` function? Since we have this logic inside the function?\r\n\r\nhttps://github.com/scrapy/scrapy/blob/9f9a2292e08cb0944e1e88e64cef1f4b17486ec8/scrapy/core/engine.py#L344-L345"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5996, "url": "https://github.com/scrapy/scrapy/pull/5996#discussion_r1282010952"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `cookies.py`:\n\n```python\nexpires = time.time() + morsel[\"max-age\"]\n    elif morsel['expires']:\n        expires = morsel['expires']\n        if type(expires) == type(\"\"):\n```"}, {"role": "assistant", "content": "That would be what I would prefer. **But** don't submit a new PR. Simply tack extra commits onto this branch to fix that.\n\nFor testing, we could use mock here but the library doesn't already use it and while we're not as concerned about testing dependencies, it is fine to create the Morsel. It communicates the intent of the function. I would just ask that you break that one test up into 3. You already have the body of that function broken into three paragraphs, they're three different logical tests and should be isolated in three different functions to represent that."}], "_meta": {"repo": "psf/requests", "pr_number": 1772, "url": "https://github.com/psf/requests/pull/1772#discussion_r8050016"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `base.py`:\n\n```python\noptions, route_name or name, args, kwargs, task_type)\n\n        driver_type = self.producer_pool.connections.connection.transport.driver_type\n        is_native_delayed_delivery = detect_quorum_queues(self, driver_type)[0]\n        if is_native_delayed_delivery and options['queue'].exchange.type != 'direct':\n            if eta:\n                if isinstance(eta, str):\n                    eta = isoparse(eta)\n                countdown = (maybe_make_aware(eta) - self.now()).total_seconds()\n\n            if countdown:\n                if countdown > 0:\n                    routing_key = calculate_routing_key(int(countdown), options[\"queue\"].routing_key)\n                    exchange = Exchange(\n                        'celery_delayed_27',\n                        type='topic',\n                    )\n                    del options['queue']\n                    options['routing_key'] = routing_key\n                    options['exchange'] = exchange\n        elif is_native_delayed_delivery and options['queue'].exchange.type == 'direct':\n            logger.warning(\n                'Direct exchanges are not supported with native delayed delivery.\\n'\n                f'{options[\"queue\"].exchange.name} is a direct exchange but should be a topic exchange or '\n                'a fanout exchange in order for native delayed delivery to work properly.\\n'\n                'If quorum queues are used, this task may block the worker process until the ETA arrives.'\n            )\n\n        if (eta or countdown) and detect_quorum_queues(self, driver_type)[0]:\n```"}, {"role": "assistant", "content": "This is wrong suggesstion, `quorum_queues_detected` function is used only once in this module, no need to keep in variable for that"}], "_meta": {"repo": "celery/celery", "pr_number": 9686, "url": "https://github.com/celery/celery/pull/9686#discussion_r2072889465"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `urllength.py`:\n\n```python\nclass UrlLengthMiddleware:\n\n    def __init__(self, maxlength):\n    def __init__(self, maxlength, stats):\n        self.maxlength = maxlength\n        self.stats = stats\n\n    @classmethod\n    def from_settings(cls, settings):\n    def from_crawler(cls, crawler):\n```"}, {"role": "assistant", "content": "we might not inject `stats` instance to a mw, but instead access it by `spider.crawler.stats` so no need to change signatures, and backward compatibility will be kept. \r\n\r\nIs that fine?"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5036, "url": "https://github.com/scrapy/scrapy/pull/5036#discussion_r593074824"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `continuous_api.py`:\n\n```python\nelse:\n                raise ValueError(f\"Request {state.request_id} is in an unexpected state: {state.status}\")\n\n        # If some requests need to be forked, we do it now\n        while self.scheduler._requests_to_fork:\n            state = self.scheduler._requests_to_fork.pop()\n            num_children = state.num_children\n            state.num_children = 0\n            for i in range(num_children):\n                # FIXME: if fork cant be done, create a new pending request without forking\n                new_request = self.cache.fork_request(state, f\"{state.request_id}__child#{i}\")\n                self.scheduler.active_requests[new_request.request_id] = new_request\n```"}, {"role": "assistant", "content": "same here, we should make that async IMO (new status \"FORKING\" -> wait until forked? IDK but we need to bench a tad bit"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42921, "url": "https://github.com/huggingface/transformers/pull/42921#discussion_r2627124477"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `engine.py`:\n\n```python\nCalled when a spider gets idle, i.e. when there are no remaining requests to download or schedule.\n        It can be called multiple times. If a handler for the spider_idle signal raises a DontCloseSpider\n        exception, the spider is not closed until the next loop and this function is guaranteed to be called\n        (at least) once again.\n        (at least) once again. A handler can raise CloseSpider to provide a custom closing reason.\n        \"\"\"\n        assert self.spider is not None  # typing\n        res = self.signals.send_catch_log(signals.spider_idle, spider=self.spider, dont_log=DontCloseSpider)\n        if any(isinstance(x, Failure) and isinstance(x.value, DontCloseSpider) for _, x in res):\n        expected_ex = (DontCloseSpider, CloseSpider)\n        res = self.signals.send_catch_log(signals.spider_idle, spider=self.spider, dont_log=expected_ex)\n        detected_ex = {ex: x.value\n                       for _, x in res for ex in expected_ex\n                       if isinstance(x, Failure) and isinstance(x.value, ex)}\n```"}, {"role": "assistant", "content": ":lipstick: \r\n\r\n```suggestion\r\n        detected_ex = {\r\n            ex: x.value\r\n            for _, x in res\r\n            for ex in expected_ex\r\n            if isinstance(x, Failure) and isinstance(x.value, ex)\r\n        }\r\n```"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5191, "url": "https://github.com/scrapy/scrapy/pull/5191#discussion_r665996239"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `convert_ministral3_weights_to_hf.py`:\n\n```python\nif new_text_config_kwargs[\"sliding_window\"] is not None:\n        new_text_config_kwargs[\"sliding_window\"] = int(new_text_config_kwargs[\"sliding_window\"])\n\n    new_text_config = Ministral3Config(**new_text_config_kwargs)\n    def get_maybe_quant_config() -> dict:\n        kwargs = {}\n        if original_config.get(\"quantization\", {}).get(\"qformat_weight\") == \"fp8_e4m3\":\n            assert original_config[\"quantization\"][\"qscheme_act\"] == \"TENSOR\"\n            quantization_config = {\n                \"activation_scheme\": \"static\",\n                \"modules_to_not_convert\": [\"model.vision_tower\", \"model.multi_modal_projector\", \"lm_head\"],\n                \"quant_method\": \"fp8\",\n                \"weight_block_size\": None,\n            }\n            kwargs[\"quantization_config\"] = AutoQuantizationConfig.from_dict(quantization_config)\n        return kwargs\n\n    # No vision\n    if original_vision_config is None:\n        new_text_config = Ministral3Config(**new_text_config_kwargs, **get_maybe_quant_config())\n```"}, {"role": "assistant", "content": "let's just add the quantization config in `new_config` and not potentially in the `new_text_config`. In the `config.json` that you converted, everything looks good"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42715, "url": "https://github.com/huggingface/transformers/pull/42715#discussion_r2607410409"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `default.py`:\n\n```python\nclass HTTPTransport(BaseTransport):\n    def __init__(\n        self,\n        verify: VerifyTypes = True,\n        cert: CertTypes | None = None,\n        ssl_context: ssl.SSLContext | None = None,\n        http1: bool = True,\n        http2: bool = False,\n        limits: Limits = DEFAULT_LIMITS,\n        trust_env: bool = True,\n        proxy: ProxyTypes | None = None,\n        uds: str | None = None,\n        local_address: str | None = None,\n        retries: int = 0,\n        socket_options: typing.Iterable[SOCKET_OPTION] | None = None,\n        # Deprecated...\n        verify: typing.Any = None,\n        cert: typing.Any = None,\n    ) -> None:\n        ssl_context = create_ssl_context(verify=verify, cert=cert, trust_env=trust_env)\n        proxy = Proxy(url=proxy) if isinstance(proxy, (str, URL)) else proxy\n        if verify is not None or cert is not None:  # pragma: nocover\n            # Deprecated...\n            ssl_context = create_ssl_context(verify, cert)\n        else:\n            ssl_context = ssl_context or SSLContext()\n```"}, {"role": "assistant", "content": "perhaps, raising error when used together?\r\n```python\r\nhttpx.get(\"https://example.com/\", verify=False, ssl_context=httpx..SSLContext())\r\n```"}], "_meta": {"repo": "encode/httpx", "pr_number": 3370, "url": "https://github.com/encode/httpx/pull/3370#discussion_r1819097875"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_typing_extra.py`:\n\n```python\nvalue, globalns, localns\n            )\n    except TypeError as e:\n        if not (isinstance(value, typing.ForwardRef) and is_backport_fixable_error(e)):\n            raise\n        try:\n            from eval_type_backport import eval_type_backport\n        except ImportError:\n            raise TypeError(\n                f'You have a type annotation {value.__forward_arg__!r} '\n                f'which makes use of newer typing features than are supported in your version of Python. '\n                f'To handle this error, you should either remove the use of new syntax '\n                f'or install the `eval_type_backport` package.'\n            ) from e\n\n        return eval_type_backport(value, globalns, localns, try_default=False)\n        if isinstance(value, typing.ForwardRef):\n```"}, {"role": "assistant", "content": "Ah gotcha. I'm ok with this approach then, but let's see what  thinks, as he's pretty well versed in the `eval_type_backport` space."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10030, "url": "https://github.com/pydantic/pydantic/pull/10030#discussion_r1701888470"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `pqueues.py`:\n\n```python\ndef push(self, request: Request) -> None:\n        slot = self._downloader_interface.get_slot_key(request)\n        # Handle None slot by converting to string\n        slot = str(slot)\n```"}, {"role": "assistant", "content": "This will treat None as a valid slot. Since `DownloaderAwarePriorityQueue` manages requests by slot, it will simply create a slot named \"None\", which the downloader will accept as valid.\r\nAn alternative would be to treat None as a fallback, using the default hostname-based slot key instead of creating a literal \"None\" slot"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6940, "url": "https://github.com/scrapy/scrapy/pull/6940#discussion_r2217405376"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `configuration_modernbert.py`:\n\n```python\nWhether to use bias in the classifier.\n        classifier_activation (`str`, *optional*, defaults to `\"gelu\"`):\n            The activation function for the classifier.\n        deterministic_flash_attn (`bool`, *optional*, defaults to `False`):\n            Whether to use deterministic flash attention. If `False`, inference will be faster but not deterministic.\n```"}, {"role": "assistant", "content": "Hmm, maybe we can set the env variable based on this instead? Ignore in modeling but adjust this based on if this attribute is set"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43030, "url": "https://github.com/huggingface/transformers/pull/43030#discussion_r2673607149"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `genspider.py`:\n\n```python\ntry:\n            spidercls = self.crawler_process.spider_loader.load(name)\n        except KeyError:\n            pass\n            if not self.settings.get('NEWSPIDER_MODULE'):\n                if exists(name + \".py\"):\n                    print(\"%r already exists\" % (abspath(name + \".py\")))\n                    return\n            else:\n                spiders_module = import_module(self.settings['NEWSPIDER_MODULE'])\n                spiders_dir = dirname(spiders_module.__file__)\n                spiders_dir_abs = abspath(spiders_dir)\n                if exists(join(spiders_dir_abs, name + \".py\")):\n                    print(\"%r already exists\" % (join(spiders_dir_abs, (name + \".py\"))))\n                    return\n```"}, {"role": "assistant", "content": "Looking at lines 84-88, I suspect this implementation will fail even if `--force` is used, which is probably undesired."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4623, "url": "https://github.com/scrapy/scrapy/pull/4623#discussion_r440178468"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_utils.py`:\n\n```python\n# Buffers are not initialized on the meta device, so we still need this check to avoid overwriting them\n            if param.device == torch.device(\"meta\"):\n                value = torch.empty_like(param, dtype=dtype, device=\"cpu\")\n                if not is_quantized or not hf_quantizer.param_needs_quantization(self, key):\n                    _load_parameter_into_model(self, key, value)\n```"}, {"role": "assistant", "content": "We removed `create_quantized_param` as we didn't need to anymore since we have the quantize ops xD. Let's see in another how to deal with that in another PR cc  but I don't think this urgent."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42818, "url": "https://github.com/huggingface/transformers/pull/42818#discussion_r2615100879"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `processing_grounding_dino.py`:\n\n```python\nself,\n        images: ImageInput = None,\n        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n```"}, {"role": "assistant", "content": "FYI the call method now looks uniformly like this:\r\n```\r\nimages: ImageInput = None,\r\ntext: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\r\naudio=None,\r\nvideos=None,\r\n**kwargs: Unpack[GroundingDinoProcessorKwargs],\r\n```\r\nI don't see the video keyword argument here so that would need to be added. cc"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 31964, "url": "https://github.com/huggingface/transformers/pull/31964#discussion_r1677539286"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_modeling_common.py`:\n\n```python\nm.gradient_checkpointing, f\"Module {n} does not have gradient_checkpointing set to False\"\n                    )\n\n    def test_enable_input_require_grads(self):\n        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n        for model_class in self.all_model_classes:\n            model = model_class(copy.deepcopy(config))\n            if not hasattr(model, \"get_input_embeddings\"):\n                continue\n            try:\n                model.enable_input_require_grads()\n            except NotImplementedError as error:\n                self.fail(f\"enable_input_require_grads raised NotImplementedError for {model_class.__name__}: {error}\")\n            finally:\n                model.disable_input_require_grads()\n\n    def test_enable_input_require_grads_with_gradient_checkpointing(self):\n        if not getattr(self.model_tester, \"is_training\", False):\n            self.skipTest(reason=\"ModelTester is not configured to run training tests\")\n```"}, {"role": "assistant", "content": "If this one is `True` by default for all models sg"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42558, "url": "https://github.com/huggingface/transformers/pull/42558#discussion_r2597809763"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `stats.py`:\n\n```python\ntype.__setattr__(cls, \"instance\", None)\n\n        if is_statsd_datadog_enabled:\n            from airflow.observability.metrics import datadog_logger\n            from airflow.sdk._shared.observability.metrics import datadog_logger\n```"}, {"role": "assistant", "content": "Oh wait thats the shared library. This won't work."}], "_meta": {"repo": "apache/airflow", "pr_number": 60698, "url": "https://github.com/apache/airflow/pull/60698#discussion_r2701022442"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modular_youtu.py`:\n\n```python\nimport torch\nfrom torch import nn\n\nfrom ... import initialization as init\nfrom ...utils import logging\nfrom ..deepseek_v3.modeling_deepseek_v3 import DeepseekV3Attention\nfrom ..llama.modeling_llama import (\n    LlamaDecoderLayer,\n    LlamaForCausalLM,\n    LlamaMLP,\n    LlamaModel,\n    LlamaPreTrainedModel,\n    LlamaRMSNorm,\n    LlamaRotaryEmbedding,\n)\nfrom .configuration_youtu import YoutuConfig\n\n\nlogger = logging.get_logger(__name__)\n\n\nclass YoutuRMSNorm(LlamaRMSNorm):\n    pass\n\n\nclass YoutuRotaryEmbedding(LlamaRotaryEmbedding):\n    pass\n\n\nclass YoutuMLP(LlamaMLP):\n    def __init__(self, config):\n        super().__init__(config)\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n\n\nclass YoutuMLAttention(DeepseekV3Attention):\n    pass\n\n\nclass YoutuDecoderLayer(LlamaDecoderLayer):\n    def __init__(self, config: YoutuConfig, layer_idx: int):\n        nn.Module.__init__(self)\n        self.hidden_size = config.hidden_size\n\n        self.self_attn = YoutuMLAttention(config=config, layer_idx=layer_idx)\n\n        self.mlp = YoutuMLP(config)\n\n        self.input_layernorm = YoutuRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = YoutuRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n\nclass YoutuPreTrainedModel(LlamaPreTrainedModel):\n    @torch.no_grad()\n    def _init_weights(self, module):\n        super()._init_weights(module)\n        std = getattr(self.config, \"initializer_range\", 0.02)\n        embed_std = getattr(self.config, \"embedding_initializer_range\", 2 * std)\n        if isinstance(module, nn.Linear):\n            init.normal_(module.weight, mean=0.0, std=std)\n            if module.bias is not None:\n                init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            init.normal_(module.weight, mean=0.0, std=embed_std)\n            if module.padding_idx is not None:\n                init.zeros_(module.weight.data[module.padding_idx])\n```"}, {"role": "assistant", "content": "Yes, we design to use different std for embedding layers"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43166, "url": "https://github.com/huggingface/transformers/pull/43166#discussion_r2726511674"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_pipelines_image_to_text.py`:\n\n```python\noutputs = pipe(image, prompt=prompt)\n        self.assertTrue(outputs[0][\"generated_text\"].startswith(prompt))\n        for batch_size in (1, 2):\n```"}, {"role": "assistant", "content": "This should be its own separate test - rather than adding onto `test_small_model_pt_conditional` e.g. something like `test_consistent_batching_behaviour` and should test that the output is well defined both with and without the `batch_size` kwarg."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 29342, "url": "https://github.com/huggingface/transformers/pull/29342#discussion_r1515907713"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_utils.py`:\n\n```python\nif dtype is not None:\n                        module.compute_dtype = dtype\n                    module.cuda(device)\n            # Clear cached dtype if dtype conversion occurred\n            if dtype_present_in_args and hasattr(self, \"_dtype\"):\n                self._dtype = None\n```"}, {"role": "assistant", "content": "From what I have seen, only 4 models have this attribute, it might be worthwhile to just overload the `to` method for those models rather than adding `if`s in the code common to all models"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43178, "url": "https://github.com/huggingface/transformers/pull/43178#discussion_r2685555425"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `data_factory.py`:\n\n```python\nhook = AzureDataFactoryAsyncHook(azure_data_factory_conn_id=self.azure_data_factory_conn_id)\n        executed_after_token_refresh = False\n        try:\n            while True:\n                try:\n                    pipeline_status = await hook.get_adf_pipeline_run_status(\n                        run_id=self.run_id,\n                        resource_group_name=self.resource_group_name,\n                        factory_name=self.factory_name,\n                    )\n                    executed_after_token_refresh = False\n                    if pipeline_status == AzureDataFactoryPipelineRunStatus.FAILED:\n                        yield TriggerEvent(\n                            {\"status\": \"error\", \"message\": f\"Pipeline run {self.run_id} has Failed.\"}\n            async with hook:\n                while True:\n                    try:\n                        pipeline_status = await hook.get_adf_pipeline_run_status(\n                            run_id=self.run_id,\n                            resource_group_name=self.resource_group_name,\n                            factory_name=self.factory_name,\n                        )\n                        return\n                    elif pipeline_status == AzureDataFactoryPipelineRunStatus.CANCELLED:\n                        msg = f\"Pipeline run {self.run_id} has been Cancelled.\"\n                        yield TriggerEvent({\"status\": \"error\", \"message\": msg})\n                        return\n                    elif pipeline_status == AzureDataFactoryPipelineRunStatus.SUCCEEDED:\n                        msg = f\"Pipeline run {self.run_id} has been Succeeded.\"\n                        yield TriggerEvent({\"status\": \"success\", \"message\": msg})\n                        return\n                    await asyncio.sleep(self.poke_interval)\n                except ServiceRequestError:\n                    # conn might expire during long running pipeline.\n                    # If exception is caught, it tries to refresh connection once.\n                    # If it still doesn't fix the issue,\n                    # than the execute_after_token_refresh would still be False\n                    # and an exception will be raised\n                    if executed_after_token_refresh:\n                        await hook.refresh_conn()\n                        executed_after_token_refresh = False\n                    else:\n                        raise\n                        if pipeline_status == AzureDataFactoryPipelineRunStatus.FAILED:\n```"}, {"role": "assistant", "content": "Maybe write a helper method which constructs the TriggerEvent base on the pipeline_status so then you could yield the resulting event from it and encapsulating the if/else in a dedicated method.\r\n\r\n```\r\ndef trigger_event_from_pipline_status(self, pipeline_status: AzureDataFactoryPipelineRunStatus) -> TriggerEvent:\r\n    if pipeline_status == AzureDataFactoryPipelineRunStatus.FAILED:\r\n        return TriggerEvent(\r\n            {\"status\": \"error\", \"message\": f\"Pipeline run {self.run_id} has Failed.\"}\r\n        )\r\n    if pipeline_status == AzureDataFactoryPipelineRunStatus.CANCELLED:\r\n        msg = f\"Pipeline run {self.run_id} has been Cancelled.\"\r\n        return TriggerEvent({\"status\": \"error\", \"message\": msg})\r\n    if pipeline_status == AzureDataFactoryPipelineRunStatus.SUCCEEDED:\r\n        msg = f\"Pipeline run {self.run_id} has been Succeeded.\"\r\n        return TriggerEvent({\"status\": \"success\", \"message\": msg})\r\n        \r\n...\r\n\r\n        event = self.trigger_event_from_pipline_status(pipeline_status)\r\n        yield event\r\n```"}], "_meta": {"repo": "apache/airflow", "pr_number": 60650, "url": "https://github.com/apache/airflow/pull/60650#discussion_r2704780853"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `__init__.py`:\n\n```python\nreturn value\n\n    def load_item(self):\n        item = self.item\n        adapter = ItemAdapter(self.item)\n        for field_name in tuple(self._values):\n            value = self.get_output_value(field_name)\n            if value is not None:\n                item[field_name] = value\n\n        return item\n                adapter[field_name] = value\n        return adapter.item\n```"}, {"role": "assistant", "content": "It's the same object, and I'd argue that `return adapter.item` is a little bit more clear in the sense that in order to understand `return self.item` you need to know that `ItemAdapter` modifies the item in-place, which you might not know a priori."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 3881, "url": "https://github.com/scrapy/scrapy/pull/3881#discussion_r426944757"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `dataclasses.py`:\n\n```python\norder: Determines if comparison magic methods should be generated, such as `__lt__`, but not `__eq__`.\n        unsafe_hash: Determines if a `__hash__` method should be included in the class, as in `dataclasses.dataclass`.\n        frozen: Determines if the generated class should be a 'frozen' `dataclass`, which does not allow its\n            attributes to be modified after it has been initialized.\n            attributes to be modified after it has been initialized. If not set, the value from the provided `config` argument will be used (and will default to `False` otherwise).\n```"}, {"role": "assistant", "content": "Not sure what you mean - this comment is still incorrect..."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10082, "url": "https://github.com/pydantic/pydantic/pull/10082#discussion_r1713916253"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `python2.py`:\n\n```python\nprint >> sys.stderr , \"Warning:\" ,\nprint >> sys.stderr , \"this is a blast from the past.\"\nprint >> sys.stderr , \"Look, a repr:\", `sys`\nprint >> sys.stderr , U\"Even these prefixes are normalized!\"\n\n\ndef function((_globals, _locals)):\n    exec ur\"print 'hi from exec!'\" in _globals, _locals\n    exec \"print 'hi from exec!'\" in _globals, _locals\n```"}, {"role": "assistant", "content": "Sorry that comment was in reference to `ru` not `rb`.\r\n\r\nIn summary:\r\n- Python 3 allows `rb`, `br`, but not `ur` or `ru`\r\n- Python 2 allows `br` and `ur` but not `rb` or `ru`"}], "_meta": {"repo": "psf/black", "pr_number": 2297, "url": "https://github.com/psf/black/pull/2297#discussion_r643258684"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `scheduler_job_runner.py`:\n\n```python\n.limit(1)\n        )\n\n        query = session.query(Log).where(\n            Log.task_id == ti.task_id,\n            Log.dag_id == ti.dag_id,\n            Log.run_id == ti.run_id,\n            Log.map_index == ti.map_index,\n            Log.try_number == ti.try_number,\n            Log.event == TASK_STUCK_IN_QUEUED_RESCHEDULE_EVENT,\n        statement = (\n            select(func.count())\n            .select_from(Log)\n```"}, {"role": "assistant", "content": "Yes, you are right! This is the simplest and best approach.\nAnd if we follow that approach, we might need to use `.subquery()`, which will make it more complex\n\nThanks a lot, I learned a lot from this discussion."}], "_meta": {"repo": "apache/airflow", "pr_number": 59918, "url": "https://github.com/apache/airflow/pull/59918#discussion_r2700693841"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_trainer.py`:\n\n```python\nset_seed(42)\n\n        model_name = \"roneneldan/TinyStories-33M\"\n        dataset_name = \"wikitext\"\n        dataset_name = \"Salesforce/wikitext\"\n```"}, {"role": "assistant", "content": "\"Canonical\" datasets are deprecated now, so I think this one was just broken, yes."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43530, "url": "https://github.com/huggingface/transformers/pull/43530#discussion_r2732848410"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `mockserver.py`:\n\n```python\nfrom six.moves.urllib.parse import urlencode\nfrom subprocess import Popen, PIPE\n\nfrom OpenSSL import SSL\n```"}, {"role": "assistant", "content": "a nitpick: this import should be with twisted imports according to pep8 (3 groups: stdlib imports, third-party imports, internal imports)"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 3442, "url": "https://github.com/scrapy/scrapy/pull/3442#discussion_r223100938"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_processor_grounding_dino.py`:\n\n```python\nwith open(self.image_processor_file, \"w\", encoding=\"utf-8\") as fp:\n            json.dump(image_processor_map, fp)\n\n        image_processor = GroundingDinoImageProcessor()\n        tokenizer = BertTokenizer.from_pretrained(self.from_pretrained_id)\n\n        processor = GroundingDinoProcessor(image_processor, tokenizer)\n\n        processor.save_pretrained(self.tmpdirname)\n\n        self.batch_size = 7\n        self.num_queries = 5\n        self.embed_dim = 5\n        self.seq_length = 5\n\n    def get_component(self, attribute, **kwargs):\n        assert attribute in self.processor_class.attributes\n        component_class_name = getattr(self.processor_class, f\"{attribute}_class\")\n        if isinstance(component_class_name, tuple):\n            component_class_name = component_class_name[0]\n\n        component_class = processor_class_from_name(component_class_name)\n        component = component_class.from_pretrained(self.tmpdirname, **kwargs)  # noqa\n\n        return component\n```"}, {"role": "assistant", "content": "Seems to be identical to the general `get_component` from Mixin"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 31964, "url": "https://github.com/huggingface/transformers/pull/31964#discussion_r1704158219"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_feedexport.py`:\n\n```python\n's3://mybucket/export.csv',\n            'access_key',\n            'secret_key',\n            'custom-acl'\n            acl='custom-acl'\n```"}, {"role": "assistant", "content": "This type of change should no longer be needed after fixing backward compatibility."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5210, "url": "https://github.com/scrapy/scrapy/pull/5210#discussion_r676580658"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_generate_schema.py`:\n\n```python\nif origin is not None:\n                dataclass = origin\n\n            config = getattr(dataclass, '__pydantic_config__', ConfigDict())\n            # if (plain) dataclass doesn't have config, we use the parent's config, hence a default of `None`\n```"}, {"role": "assistant", "content": "```suggestion\r\n            # if (plain) dataclass doesn't have config, we use the parent's config, hence a default of `None`\r\n            # (Pydantic dataclasses have an empty dict config by default).\r\n```"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10928, "url": "https://github.com/pydantic/pydantic/pull/10928#discussion_r1852452898"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `finegrained_fp8.py`:\n\n```python\nqinput, scale = act_quant(input, self.block_size[1])\n                elif self.activation_scheme == \"static\":\n                    scale = self.activation_scale.to(torch.float32)\n                    qinput = (input / scale).to(torch.float8_e4m3fn)\n                    qinput = input / scale\n                else:\n                    raise NotImplementedError(\"Not supported\")\n                output = w8a8_block_fp8_matmul_triton(\n                    qinput,\n                    weight,\n                    scale,\n                    scale_inv,\n                    self.block_size,\n                    output_dtype=input.dtype,\n                )\n\n                if self.activation_scheme == \"static\":\n                    # TODO: fix that so we don't have to upcast to bfloat16\n                    output = (\n                        F.linear(qinput.to(torch.bfloat16), weight.to(torch.bfloat16), self.bias) * scale * scale_inv\n                    )\n```"}, {"role": "assistant", "content": "```suggestion\r\n                if self.activation_scheme == \"static\":\r\n                    # TODO: fix that so we don't have to upcast to bfloat16\r\n                    output = (\r\n                        F.linear(qinput.to(input.dtype), weight.to(input.dtype), self.bias) * scale * scale_inv\r\n                    )\r\n```"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42775, "url": "https://github.com/huggingface/transformers/pull/42775#discussion_r2607262255"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modular_deepseek_v2.py`:\n\n```python\nsuper().__init__(config)\n        self.num_experts = config.n_routed_experts\n\n    def forward(\n```"}, {"role": "assistant", "content": "I don't understand how the change breaks FP8 compatibility, though? If you look at the modeling file, it only updates one slice"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43288, "url": "https://github.com/huggingface/transformers/pull/43288#discussion_r2694739979"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_config.py`:\n\n```python\nclass SSLContext(ssl.SSLContext):\n    DEFAULT_CA_BUNDLE_PATH = Path(certifi.where())\n```"}, {"role": "assistant", "content": "Calling `certifi.where()` at the point the certifi certificates are loaded seems reasonable to me."}], "_meta": {"repo": "encode/httpx", "pr_number": 3335, "url": "https://github.com/encode/httpx/pull/3335#discussion_r1795109143"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `engine.py`:\n\n```python\nself.spider: Optional[Spider] = None\n        self.running = False\n        self.paused = False\n        self.scheduler_cls = load_object(crawler.settings[\"SCHEDULER\"])\n        self.scheduler_cls = self._get_scheduler_class(crawler.settings)\n        downloader_cls = load_object(self.settings['DOWNLOADER'])\n        self.downloader = downloader_cls(crawler)\n        self.scraper = Scraper(crawler)\n        self._spider_closed_callback = spider_closed_callback\n\n    def _get_scheduler_class(self, settings: BaseSettings) -> type:\n        from scrapy.core.scheduler import BaseScheduler\n        scheduler_cls = load_object(settings[\"SCHEDULER\"])\n        if not issubclass(scheduler_cls, BaseScheduler):\n            raise TypeError(\n                f\"The provided scheduler class ({settings['SCHEDULER']})\"\n                \" does not fully implement the scheduler interface\"\n            )\n```"}, {"role": "assistant", "content": "Cool! Today I Learn that can be done. I should have had a closer look at the metaclass implementation."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 3559, "url": "https://github.com/scrapy/scrapy/pull/3559#discussion_r617221246"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `strategy.py`:\n\n```python\nlimit_task = consumer._limit_task\n    limit_post_eta = consumer._limit_post_eta\n    Request = symbol_by_name(task.Request)\n    Req = create_request_cls(Request, task, consumer.pool, hostname, eventer, app=app)\n    Req = create_request_cls(Request, task, consumer.pool, hostname, eventer,\n                             app=app)\n\n    revoked_tasks = consumer.controller.state.revoked\n    eta_task_limit = getattr(app.conf, 'worker_eta_task_limit', None)\n\n    class ETATaskTracker:\n        \"\"\"Track ETA tasks to enforce limits.\"\"\"\n        def __init__(self):\n            self.count = 0\n\n        def check_limit(self, req):\n            if eta_task_limit is not None and self.count >= eta_task_limit:\n                info(\"ETA task limit reached (%s/%s), rejecting task %s\",\n                     self.count, eta_task_limit, req.id)\n                req.reject(requeue=True)\n                return True\n\n            self.count += 1\n            return False\n\n        def create_callback(self, req, original_callback):\n            def eta_callback(*args):\n                self.count = max(0, self.count - 1)\n                return original_callback(*args)\n            eta_callback.__name__ = 'eta_callback'\n            return eta_callback\n\n    eta_tracker = ETATaskTracker()\n    # Apply ETA task limit to QoS prefetch count if configured\n    if eta_task_limit is not None:\n        # Set a maximum prefetch count to limit ETA tasks\n        original_prefetch = consumer.qos.value\n        consumer.qos.value = min(original_prefetch, eta_task_limit)\n        consumer.qos.set(consumer.qos.value)\n```"}, {"role": "assistant", "content": "You're absolutely right, I had limited knowledge of the Kombu implementation details. My approach is flawed since increment_eventually bypasses the limit I set during initialisation.\r\nLet me go through the Kombu QoS implementation and come back with a better solution that handles this at the right architectural layer. Meanwhile, I am converting this PR as a draft."}], "_meta": {"repo": "celery/celery", "pr_number": 9853, "url": "https://github.com/celery/celery/pull/9853#discussion_r2265125876"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `worker.py`:\n\n```python\n# Cancel all unacked requests and allow the worker to terminate naturally\n    worker.consumer.cancel_all_unacked_requests()\n\n    from celery.worker import state\n    state.should_terminate = True\n\n    # Stop the pool to allow successful tasks call on_success()\n```"}, {"role": "assistant", "content": "[nitpick] The comment \"Stop the pool to allow successful tasks call on_success()\" could be clarified. Successful tasks have already called `on_success()` by this point (which is how they were preserved in `cancel_all_unacked_requests()`). The `pool.stop()` call actually waits for the pool to finish processing and allows any remaining task completion callbacks to finish. Consider updating the comment to: \"Stop the pool and wait for any remaining task completion to finish\" or similar.\n```suggestion\n    # Stop the pool and wait for any remaining task completion to finish\n```"}], "_meta": {"repo": "celery/celery", "pr_number": 9678, "url": "https://github.com/celery/celery/pull/9678#discussion_r2531973294"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `item.py`:\n\n```python\nfrom collections.abc import MutableMapping\n\nfrom scrapy.item import BaseItem\n\n\ndef _is_dataclass_instance(obj):\n    \"\"\"\n    Return True if *obj* is a dataclass object, False otherwise.\n    Taken from https://docs.python.org/3/library/dataclasses.html#dataclasses.is_dataclass.\n    \"\"\"\n    try:\n        from dataclasses import is_dataclass\n    except ImportError:\n        return False\n    else:\n        return is_dataclass(obj) and not isinstance(obj, type)\n\n\ndef is_item_like(obj):\n    \"\"\"\n    Return True if *obj* is considered a Scrapy *item*, False otherwise.\n\n    An object is considered an *item* if it is:\n    - a scrapy.item.BaseItem or dict instance (or any subclass)\n    - a dataclass object\n    \"\"\"\n    return isinstance(obj, (BaseItem, dict)) or _is_dataclass_instance(obj)\n\n\nclass ItemAdapter(MutableMapping):\n```"}, {"role": "assistant", "content": "Sounds fine to me, I removed meta for MutableMappings already, I'll remove MutableMappings altogether now, it'd be easy to add it back if we decide to."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 3881, "url": "https://github.com/scrapy/scrapy/pull/3881#discussion_r416846569"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_downloader_handlers.py`:\n\n```python\ndef _test(response):\n            self.assertEqual(response.status, 200)\n            self.assertEqual(response.url, request.url)\n            self.assertEqual(response.body, b'http://example.com')\n            self.assertTrue(\n                response.body == b'http://example.com'  # HTTP/1.x\n                or response.body == b'/'  # HTTP/2\n            )\n```"}, {"role": "assistant", "content": "What about keeping the old check, but turning `b'http://example.com'` into a class-level variable to be overridden in HTTP/2 tests? That way the check is more strict, and will fail if the protocol-version-specific expectations are not met (e.g. we suddenly start returning `/` for HTTP/1.x)."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4610, "url": "https://github.com/scrapy/scrapy/pull/4610#discussion_r467843413"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_image_processing_glpn.py`:\n\n```python\nself.assertTrue(hasattr(image_processing, \"do_rescale\"))\n\n    def test_call_pil(self):\n        # Initialize image_processing\n        image_processing = self.image_processing_class(**self.image_processor_dict)\n        # create random PIL images\n        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n        for image in image_inputs:\n            self.assertIsInstance(image, Image.Image)\n\n        # Test not batched input (GLPNImageProcessor doesn't support batching)\n        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n        self.assertTrue(tuple(encoded_images.shape) == (1, *expected_output_image_shape))\n\n    def test_call_numpy(self):\n        # Initialize image_processing\n        image_processing = self.image_processing_class(**self.image_processor_dict)\n        # create random numpy tensors\n        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n        for image in image_inputs:\n            self.assertIsInstance(image, np.ndarray)\n\n        # Test not batched input (GLPNImageProcessor doesn't support batching)\n        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n        self.assertTrue(tuple(encoded_images.shape) == (1, *expected_output_image_shape))\n\n    def test_call_pytorch(self):\n        # Initialize image_processing\n        image_processing = self.image_processing_class(**self.image_processor_dict)\n        # create random PyTorch tensors\n        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n        for image in image_inputs:\n            self.assertIsInstance(image, torch.Tensor)\n\n        # Test not batched input (GLPNImageProcessor doesn't support batching)\n        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n        self.assertTrue(tuple(encoded_images.shape) == (1, *expected_output_image_shape))\n\n    def test_call_numpy_4_channels(self):\n        # Initialize image_processing\n        image_processing = self.image_processing_class(**self.image_processor_dict)\n        # create random numpy tensors\n```"}, {"role": "assistant", "content": "Agreed comments are not very informative but no need to remove them all imo"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41725, "url": "https://github.com/huggingface/transformers/pull/41725#discussion_r2445121125"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `black.py`:\n\n```python\nreturn \"#\" + content\n\n\ndef _line_has_trailing_comma(l: Line) -> bool:\n```"}, {"role": "assistant", "content": "This turned into `is_collection_with_optional_trailing_comma` as a property on Line.\r\n\r\n(Standalone tests for `is_collection_with_optional_trailing_comma` should be coming soon, but I wanted to get feedback from Travis on the rest of the CI status...)"}], "_meta": {"repo": "psf/black", "pr_number": 826, "url": "https://github.com/psf/black/pull/826#discussion_r282119903"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `masking_utils.py`:\n\n```python\nor (\n                padding_mask.all()\n                if not _is_torch_xpu_available or query_length == 1\n                else padding_mask[:, :query_length].all()\n                else (padding_mask[:, :query_length].all() and not padding_mask[:, query_length:].any())\n```"}, {"role": "assistant", "content": "Can we add a comment here for what this entails or rather why we need this check explicitly"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42052, "url": "https://github.com/huggingface/transformers/pull/42052#discussion_r2510857422"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_form_default.py`:\n\n```python\nfrom typing import Optional\n\nfrom fastapi import FastAPI, File, Form\nfrom starlette.testclient import TestClient\nfrom typing_extensions import Annotated\n\napp = FastAPI()\nclient = TestClient(app)\n\n\n@app.post(\"/urlencoded\")\nasync def post_url_encoded(age: Annotated[Optional[int], Form()] = None):\n    return age\n\n\ndef test_form_default_url_encoded():\n    response = client.post(\"/urlencoded\", data={\"age\": \"\"})\n    assert response.status_code == 200\n    assert response.text == \"null\"\n```"}, {"role": "assistant", "content": "This looks odd to me.\r\nIf you pass a string to an integer field, I would expect a `4xx` error code."}], "_meta": {"repo": "fastapi/fastapi", "pr_number": 13537, "url": "https://github.com/fastapi/fastapi/pull/13537#discussion_r2076095083"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_qwen3.py`:\n\n```python\nreturn q_embed, k_embed\n\n\ndef apply_rotary_kernel(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n    \"\"\"\n    Rotary kernel implementation wrapper\n    Adapts rotary kernels implementation to match HuggingFace apply_rotary_pos_emb signature\n    \"\"\"\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n\n    q_rotated = q.clone()\n    k_rotated = k.clone()\n\n    # Get half dimension for rotation\n    half_dim = q.shape[-1] // 2\n    q1 = q_rotated[..., :half_dim]\n    q2 = q_rotated[..., half_dim:]\n    k1 = k_rotated[..., :half_dim]\n    k2 = k_rotated[..., half_dim:]\n    if cos.shape[-1] != half_dim:\n        # Trim cos/sin to match half_dim\n        cos = cos[..., :half_dim]\n        sin = sin[..., :half_dim]\n\n    # Apply rotary embedding using our kernel\n    rotary_kernel.apply_rotary(q1, q2, cos, sin, q1, q2, False)\n```"}, {"role": "assistant", "content": "Yes, on Intel XPU, one single rotary op needs 0.22 ms, and it drops to 0.1 ms after applying this patch. above 2x speedup."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41147, "url": "https://github.com/huggingface/transformers/pull/41147#discussion_r2380626693"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `feedexport.py`:\n\n```python\nreturn True\n        logger.error(\"Unknown feed format: %(format)s\", {'format': format})\n\n    def _settings_are_valid(self, uri):\n    def _settings_are_valid(self):\n        \"\"\"\n        If FEED_STORAGE_BATCH_ITEM_COUNT setting or FEEDS.batch_item_count is specified uri has to contain\n        If FEED_EXPORT_BATCH_ITEM_COUNT setting or FEEDS.batch_item_count is specified uri has to contain\n        %(batch_time)s or %(batch_id)s to distinguish different files of partial output\n        \"\"\"\n        for uri_template, values in self.feeds.items():\n            if values['batch_item_count'] and not any(s in uri_template for s in ['%(batch_time)s', '%(batch_id)s']):\n            if values['batch_item_count'] and not re.findall(r'(%\\(batch_time\\)s|(%\\(batch_id\\)0\\d*d))', uri_template):\n```"}, {"role": "assistant", "content": "What about just looking for `%(asdf)`, and hope that the user did not break the formatting syntax? Either we do that, or we implement something that finds all syntax variants.\r\n\r\nAlso, `findall` probably does not make sense here, given the preceding `not`; finding a single match should suffice in this scenario."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4434, "url": "https://github.com/scrapy/scrapy/pull/4434#discussion_r448916047"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_fields.py`:\n\n```python\n# to make sure the decorators have already been built for this exact class\n        decorators: DecoratorInfos = cls.__dict__['__pydantic_decorators__']\n        if ann_name in decorators.computed_fields:\n            raise ValueError(\"you can't override a field with a computed field\")\n            raise TypeError(\n                f'Field \"{ann_name}\" of class \"{cls.__name__}\" overrides symbol of same name in \"{cls.__bases__[0].__name__}\".\\n'\n```"}, {"role": "assistant", "content": "```suggestion\r\n                f'Field {ann_name!r} of class {cls.__name__!r} overrides symbol of same name in {cls.__bases__[0].__name__!r}. '\r\n```\r\na couple notes:\r\n- The parent symbol might not be in `cls.__bases__[0]` but in another base class, and I don't think there's a simple way to actually know which base class it is.\r\n- Probably we should avoid explicit newlines in exception messages"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 11459, "url": "https://github.com/pydantic/pydantic/pull/11459#discussion_r1961882131"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `UP045_py39.py`:\n\n```python\n\"\"\"\nRegression test for https://github.com/astral-sh/ruff/issues/20096\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Optional, cast, TypeAlias\n\n\nx: Optional[str]             # UP045\nx: \"Optional[str]\"           # UP045\ncast(\"Optional[str]\", None)  # UP045\ncast(Optional[str], None)    # okay, str | None is a runtime error\nx: TypeAlias = \"Optional[str]\"  # UP045\nx: TypeAlias = Optional[str]  # okay\n```"}, {"role": "assistant", "content": "I think I'll just close this for now, unless you think it's worth landing without looking into the (arguably separate) issue with concatenated annotations. This is a pretty niche issue in any case."}], "_meta": {"repo": "astral-sh/ruff", "pr_number": 22320, "url": "https://github.com/astral-sh/ruff/pull/22320#discussion_r2713264436"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_models.py`:\n\n```python\nCookies(cookies).set_cookie_header(self)\n\n        if stream is None:\n            headers, stream = encode_request(content, data, files, json)\n            headers, stream = encode_request(\n                content=content,\n                data=data,\n                files=files,\n                json=json,\n                content_type=self.headers.get(\"content-type\"),\n            )\n```"}, {"role": "assistant", "content": "Okay I liked it once I wrote it, I think given that `encode_request` already takes a `boundary` parameter (which only makes sense in the context of multipart requests) moving the call to where you are suggesting is not a big deal"}], "_meta": {"repo": "encode/httpx", "pr_number": 2278, "url": "https://github.com/encode/httpx/pull/2278#discussion_r945185780"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `scraper.py`:\n\n```python\nif isinstance(output, Failure):\n            ex = output.value\n            if isinstance(ex, DropItem):\n                if ex.log_level is None:\n                    ex.log_level = spider.crawler.settings.get(\n                        \"DEFAULT_DROPITEM_LOG_LEVEL\", \"WARNING\"\n                    )\n                else:\n                    ex.log_level = ex.log_level.upper()\n```"}, {"role": "assistant", "content": "If you remove the `severity = getattr(logging, log_level.upper())` line, and set `\"level\": log_level,` in the resulting dictionary, doesn\u2019t that work?"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6608, "url": "https://github.com/scrapy/scrapy/pull/6608#discussion_r1912881131"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_triggerer_job.py`:\n\n```python\ntrigger_instance.cancel()\n        await runner.cleanup_finished_triggers()\n\n    @pytest.mark.asyncio\n    @patch(\"airflow.sdk.execution_time.task_runner.SUPERVISOR_COMMS\", create=True)\n    async def test_sync_state_to_supervisor(self, supervisor_builder):\n        trigger_runner = TriggerRunner()\n        trigger_runner.comms_decoder = AsyncMock(spec=TriggerCommsDecoder)\n        trigger_runner.comms_decoder.asend.side_effect = [\n            messages.TriggerStateSync(to_create=[], to_cancel=set()),\n        ]\n        trigger_runner.events.append((1, TriggerEvent(payload={\"status\": \"SUCCESS\"})))\n        trigger_runner.events.append((2, TriggerEvent(payload={\"status\": \"FAILED\"})))\n        trigger_runner.events.append((3, TriggerEvent(payload={\"status\": \"SUCCESS\", \"data\": object()})))\n\n        await trigger_runner.sync_state_to_supervisor(finished_ids=[])\n\n        assert trigger_runner.comms_decoder.asend.call_count == 1\n        assert len(trigger_runner.comms_decoder.asend.call_args_list[0].args[0].events) == 2\n```"}, {"role": "assistant", "content": "This is fixed now in the unit test, now it will check 3 events first and only 2 after sanitize function"}], "_meta": {"repo": "apache/airflow", "pr_number": 60152, "url": "https://github.com/apache/airflow/pull/60152#discussion_r2705901725"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `cache_utils.py`:\n\n```python\ni.e. `mode=\"reduce-overhead\"` is known to fail). But it will in general work correctly, and prefill should\n        not be compiled anyway for performances!\n        \"\"\"\n        self.max_batch_size, self.num_heads, _, self.head_dim = key_states.shape\n        self.dtype, self.device = key_states.dtype, key_states.device\n        self.max_batch_size, self.num_heads = key_states.shape[:2]\n        self.v_head_dim = value_states.shape[-1]\n        self.k_head_dim = key_states.shape[-1]\n```"}, {"role": "assistant", "content": "had to add value_states to the signature since some models like deepseek have different k/v head_dim"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42697, "url": "https://github.com/huggingface/transformers/pull/42697#discussion_r2660880703"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `decorators.py`:\n\n```python\ndef valid_method_info(self, info: ValidationInfo) -> Self: ...\n\n\nclass BeforeFieldValidator(BaseModel):\n    \"\"\"Same tests should apply to `mode='plain'`.\"\"\"\n\n    @field_validator('foo', mode='before')\n    def no_classmethod(self, value: Any) -> Any:\n        \"\"\"TODO this shouldn't be valid, the decorator should only work on classmethods.\n\n        We might want to do the same type checking as wrap model validators.\n        \"\"\"\n\n    @field_validator('foo', mode='before')\n    @classmethod\n    def valid_classmethod(cls, value: Any) -> Any: ...\n\n    @field_validator('foo', mode='before')  # pyright: ignore[reportArgumentType]\n    @classmethod\n    def invalid_with_info(cls, value: Any, info: int) -> Any: ...\n\n    @field_validator('foo', mode='before', input_type=int)  # `input_type` allowed here.\n    @classmethod\n    def valid_with_info(cls, value: Any, info: ValidationInfo) -> Any: ...\n```"}, {"role": "assistant", "content": "Because in reality any value can be passed in, so the type hint won't reflect the actual possible values. I prefer to enforce that users use `Any` for the `value` type annotation, so that they make sure to actually check the allowed values. If we were to encourage users to annotate `value` to the allowed types:\r\n\r\n```python\r\n('foo', mode='before') \r\n    \r\n    def valid_with_info(cls, value: int, info: ValidationInfo) -> Any: ...\r\n```\r\n\r\nUsers will likely forget to do the proper runtime checks on `value` and assume it is already an `int`. This is even more likely if they do something like:\r\n\r\n```python\r\n('foo', mode='before') \r\n    \r\n    def valid_with_info(cls, value: int, info: ValidationInfo) -> Any:\r\n        if not isinstance(value, int):  # This should be done by users\r\n            raise ValidationError(...)  # Type checker error, marked as unreachable code\r\n```"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10094, "url": "https://github.com/pydantic/pydantic/pull/10094#discussion_r1719979051"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `configuration_bart.py`:\n\n```python\n)\n\n        # ensure backward compatibility for BART CNN models\n        if self.forced_bos_token_id is None and kwargs.get(\"force_bos_token_to_be_generated\", False):\n```"}, {"role": "assistant", "content": "In general. \"model.config\" used to contain generation params until this PR, i.e. we could do `model.config.do_sample` and get a default value. From now on, it will throw attribute error unless the model's config actually had a \"do_sample\" saved in json file"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41695, "url": "https://github.com/huggingface/transformers/pull/41695#discussion_r2534742977"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `log.py`:\n\n```python\nand adapts it into a tuple of positional arguments for logger.log calls,\n    handling backward compatibility as well.\n    \"\"\"\n    if not {\"level\", \"msg\", \"args\"} <= set(logkws):\n        warnings.warn(\"Missing keys in LogFormatter method\", ScrapyDeprecationWarning)\n\n    if \"format\" in logkws:\n        warnings.warn(\n            \"`format` key in LogFormatter methods has been \"\n            \"deprecated, use `msg` instead\",\n            ScrapyDeprecationWarning,\n        )\n\n    level = logkws.get(\"level\", logging.INFO)\n    message = logkws.get(\"format\", logkws.get(\"msg\"))\n    message = logkws.get(\"msg\", \"\")\n    assert message is not None\n```"}, {"role": "assistant", "content": "By using `or` operator in case we have the key `msg` but the value is null we fallback to empty string, since the default empty string value using `get()` is only when the dict don\u2019t have the key not for the key being null."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6116, "url": "https://github.com/scrapy/scrapy/pull/6116#discussion_r1366740910"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modular_qwen3_next.py`:\n\n```python\nclass Qwen3NextAttention(Qwen3MoeAttention):\n    def __init__(self, config: Qwen3NextConfig, layer_idx: int):\n        super().__init__(config, layer_idx)\n        # Initialize nn.Module (skip Qwen3MoeAttention.__init__ to avoid loading rotary kernel)\n        nn.Module.__init__(self)\n        self.config = config\n        self.layer_idx = layer_idx\n        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n        self.scaling = self.head_dim**-0.5\n        self.attention_dropout = config.attention_dropout\n        self.is_causal = True\n\n        self.q_proj = nn.Linear(\n            config.hidden_size, config.num_attention_heads * self.head_dim * 2, bias=config.attention_bias\n        )\n        del self.sliding_window\n        self.k_proj = nn.Linear(\n            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n        )\n        self.v_proj = nn.Linear(\n            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n        )\n        self.o_proj = nn.Linear(\n            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n        )\n        self.q_norm = Qwen3NextRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n        self.k_norm = Qwen3NextRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n```"}, {"role": "assistant", "content": "seems unrelated too, we should only load the kernel and use it with very minimal changes to the modeling files"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41147, "url": "https://github.com/huggingface/transformers/pull/41147#discussion_r2468770617"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `cli_config.py`:\n\n```python\nActionCommand(\n        name=\"import\",\n        help=\"Import connections from a file. \"\n        \"This feature is compatible with airflow CLI `airflow connections export a.json` command. \"\n        \"Export it from `airflow CLI` and import it securely via this command.\",\n        \"Export functionality is only available via the local Airflow CLI command 'airflow connections export'. \"\n```"}, {"role": "assistant", "content": "This seems a bit long (see the screenshot). I think it should be much shorter (\"Import connections from a file exported with local CLI.\")"}], "_meta": {"repo": "apache/airflow", "pr_number": 59850, "url": "https://github.com/apache/airflow/pull/59850#discussion_r2649083646"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `comment_after_escaped_newline.py`:\n\n```python\npass\n\n\ndef bobtwo():  # some comment here\ndef bobtwo():\n```"}, {"role": "assistant", "content": "Actually, this falls under the exception where we can change the formatting of code that hasn't already been Black-formatted. Normally I'm hesitant to use that exception, but this pattern seems unlikely to appear in any real-world code."}], "_meta": {"repo": "psf/black", "pr_number": 4343, "url": "https://github.com/psf/black/pull/4343#discussion_r1590003594"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_worker.py`:\n\n```python\n\"Retrying to establish a connection to the message broker after a connection \"\n        \"loss has been disabled (app.conf.broker_connection_retry_on_startup=False). \"\n        \"Shutting down...\") != -1, output\n\n\ndef test_django_fixup_direct_worker(caplog):\n    \"\"\"Test Django fixup by directly instantiating Celery worker without subprocess.\"\"\"\n    import logging\n\n    import django\n\n    # Set logging level to capture debug messages\n    caplog.set_level(logging.DEBUG)\n\n    # Configure Django settings\n    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 't.integration.test_django_settings')\n    django.setup()\n\n    # Create Celery app with Django integration\n    app = Celery('test_django_direct')\n    app.config_from_object('django.conf:settings', namespace='CELERY')\n    app.autodiscover_tasks()\n\n    # Test that we can access worker configuration without recursion errors\n    # This should trigger the Django fixup initialization\n    worker = app.Worker(\n        pool='solo',\n        concurrency=1,\n        loglevel='debug'\n    )\n\n    # Accessing pool_cls should not cause AttributeError\n    pool_cls = worker.pool_cls\n    assert pool_cls is not None\n\n    # Verify pool_cls has __module__ attribute (should be a class, not a string)\n    assert hasattr(pool_cls, '__module__'), \\\n        f\"pool_cls should be a class with __module__, got {type(pool_cls)}: {pool_cls}\"\n\n    # Capture and check logs\n    log_output = caplog.text\n\n    # Verify no recursion-related errors in logs\n    assert \"RecursionError\" not in log_output, f\"RecursionError found in logs:\\n{log_output}\"\n    assert \"maximum recursion depth exceeded\" not in log_output, \\\n        f\"Recursion depth error found in logs:\\n{log_output}\"\n\n    assert \"AttributeError: 'str' object has no attribute '__module__'.\" not in log_output, \\\n        f\"AttributeError found in logs:\\n{log_output}\"\n```"}, {"role": "assistant", "content": "I replaced os.environ with monkeypatch to prevent the issues mentioned in the comment"}], "_meta": {"repo": "celery/celery", "pr_number": 10045, "url": "https://github.com/celery/celery/pull/10045#discussion_r2657688025"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modular_qwen2_5_omni.py`:\n\n```python\nattention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n        return attention_mask\n\n    @check_model_inputs(tie_last_hidden_states=False)\n```"}, {"role": "assistant", "content": "See https://github.com/huggingface/transformers/pull/42564#discussion_r2695194529, we might have a regression otherwise."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42564, "url": "https://github.com/huggingface/transformers/pull/42564#discussion_r2704324009"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `vptq.py`:\n\n```python\n# limitations under the License.\n\"VPTQ (Vector Post-Training Quantization) integration file\"\n\nimport torch.nn as nn\nfrom accelerate import init_empty_weights\nfrom vptq import VQuantLinear\nfrom ..quantizers.quantizers_utils import should_convert_module\nfrom ..utils import is_accelerate_available, is_torch_available, logging\n\n\ndef replace_with_vptq_linear(\n    model,\n    quantization_config=None,\n    modules_to_not_convert=None,\n    current_key_name=None,\n    has_been_replaced=False,\n):\nif is_accelerate_available():\n    from accelerate import init_empty_weights\n\nif is_torch_available():\n    import torch.nn as nn\n\nlogger = logging.get_logger(__name__)\n\n\ndef replace_with_vptq_linear(model, modules_to_not_convert: list[str] | None = None, quantization_config=None):\n    \"\"\"\n    Public method that recursively replaces the Linear layers of the given model with VPTQ quantized layers.\n    `accelerate` is needed to use this method. Returns the converted model and a boolean that indicates if the\n    conversion has been successful or not.\n    Public method that recursively replaces the Linear layers of the given model with SPQR quantized layers.\n```"}, {"role": "assistant", "content": "```suggestion\n    Public method that replaces the Linear layers of the given model with SPQR quantized layers.\n```"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42711, "url": "https://github.com/huggingface/transformers/pull/42711#discussion_r2602577845"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `httpcompression.py`:\n\n```python\nreturn response\n\n    def _handle_encoding(self, body, content_encoding):\n        content_to_decode = body\n\n        separator = b\",\"\n        decoded_body = \"\"\n        for index, encodings in reversed(list(enumerate(content_encoding))):\n            if separator in encodings:\n                decoded_body = \"\"\n```"}, {"role": "assistant", "content": "I think this line is unnecessary, and would break scenarios with multiple headers where some of those headers include comma-separated encodings."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6063, "url": "https://github.com/scrapy/scrapy/pull/6063#discussion_r1339608030"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_generate_schema.py`:\n\n```python\nreturn_type_schema = self._apply_field_serializers(\n            return_type_schema,\n            filter_field_decorator_info_by_field(field_serializers.values(), d.cls_var_name),\n            computed_field=True,\n        )\n```"}, {"role": "assistant", "content": "I believe you can remove the `computed_field` argument from the `_apply_field_serializers` function as well.\r\n\r\nhttps://github.com/pydantic/pydantic/blob/a6dc87285f93f90c2d5c298ee7c52f5d7e878194/pydantic/_internal/_generate_schema.py#L2196C9-L2201"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10390, "url": "https://github.com/pydantic/pydantic/pull/10390#discussion_r1758886279"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_fields.py`:\n\n```python\nreturn True\n\n\ndef rebuild_model_fields(\n    cls: type[BaseModel],\n    *,\n    ns_resolver: NsResolver,\n    typevars_map: dict[TypeVar, Any],\n    raise_errors: bool = True,\n) -> dict[str, FieldInfo]:\n    \"\"\"Rebuild the (already present) model fields by trying to reevaluate annotations.\n\n    This function should be called whenever a model with incomplete fields is encountered.\n\n    Note:\n        This function *doesn't* mutate the model fields in place, as it can be called during\n        schema generation, where you don't want to mutate other model's fields.\n    \"\"\"\n    FieldInfo_ = import_cached_field_info()\n\n    rebuilt_fields: dict[str, FieldInfo] = {}\n```"}, {"role": "assistant", "content": "Understandable, but downside - this is going to increate memory usage. Could you benchmark this with memray and see what the difference is like?"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 11388, "url": "https://github.com/pydantic/pydantic/pull/11388#discussion_r1949422871"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `main.py`:\n\n```python\nreturn super().dict(\n                include=include,\n                exclude=exclude,\n                by_alias=by_alias,\n                by_alias=by_alias or False,\n```"}, {"role": "assistant", "content": "With this trick, we avoid the type when `by_alias` is `None`. :sweat_smile:"}], "_meta": {"repo": "tiangolo/sqlmodel", "pr_number": 1340, "url": "https://github.com/fastapi/sqlmodel/pull/1340#discussion_r2061541310"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_spidermiddleware.py`:\n\n```python\nclass ProcessSpiderOutputCoroutineMiddleware:\n    async def process_spider_output(self, response, result, spider):\n        results = []\n        for r in result:\n            results.append(r)\n        return results\n        return result\n```"}, {"role": "assistant", "content": "(Well, not a copy, as it's not necessarily a list)"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6581, "url": "https://github.com/scrapy/scrapy/pull/6581#discussion_r1883607300"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `sessions.py`:\n\n```python\nreturn r\n\n    def merge_environment_settings(self, url, proxies, stream, verify, cert):\n        \"\"\"Checks the environment and merges it with some settings.\"\"\"\n        # Gather clues from the surrounding environment.\n        if self.trust_env:\n            # Set environment's proxies.\n            env_proxies = get_environ_proxies(url) or {}\n            for (k, v) in env_proxies.items():\n                proxies.setdefault(k, v)\n\n            # Look for configuration.\n            if verify is True or verify is None:\n                verify = os.environ.get('REQUESTS_CA_BUNDLE')\n\n            # Curl compatibility.\n            if verify is True or verify is None:\n                verify = os.environ.get('CURL_CA_BUNDLE')\n```"}, {"role": "assistant", "content": "While we're here, these two conditional blocks make me feel a bit uncomfortable. Maybe combine the two? Something like:\n\n``` python\nif verify is True or verify is None:\n    verify = os.environ.get('REQUESTS_CA_BUNDLE') or os.environ.get('CURL_CA_BUNDLE')\n```\n\nObviously there's a line length issue here, but I feel like it's a little cleaner. We'd need to update the commenting too.\n\nI'm not hugely attached to it, what do you think?"}], "_meta": {"repo": "psf/requests", "pr_number": 2178, "url": "https://github.com/psf/requests/pull/2178#discussion_r16632859"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `my-script.py`:\n\n```python\nfrom __future__ import annotations\n```"}, {"role": "assistant", "content": "beat you to it ;) https://github.com/astral-sh/ruff/pull/21751"}], "_meta": {"repo": "astral-sh/ruff", "pr_number": 21741, "url": "https://github.com/astral-sh/ruff/pull/21741#discussion_r2582521122"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_sys.py`:\n\n```python\nself.assertEqual(sys.getsizeof(True, -1), size('') + self.longdigit)\n\n    def test_objecttypes(self):\n        import _datetime\n```"}, {"role": "assistant", "content": "> And since it is optional, skip the corresponding test if the import fails.\r\n\r\n`_datetime` is now a built-in module, it's not really optional."}], "_meta": {"repo": "python/cpython", "pr_number": 144003, "url": "https://github.com/python/cpython/pull/144003#discussion_r2712794851"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `decorators.py`:\n\n```python\ndef valid_method_info(self, info: ValidationInfo) -> Self: ...\n\n\nclass BeforeFieldValidator(BaseModel):\n    \"\"\"Same tests should apply to `mode='plain'`.\"\"\"\n\n    @field_validator('foo', mode='before')\n    def no_classmethod(self, value: Any) -> Any:\n        \"\"\"TODO this shouldn't be valid, the decorator should only work on classmethods.\n\n        We might want to do the same type checking as wrap model validators.\n        \"\"\"\n\n    @field_validator('foo', mode='before')\n    @classmethod\n    def valid_classmethod(cls, value: Any) -> Any: ...\n\n    @field_validator('foo', mode='before')  # pyright: ignore[reportArgumentType]\n    @classmethod\n    def invalid_with_info(cls, value: Any, info: int) -> Any: ...\n\n    @field_validator('foo', mode='before', input_type=int)  # `input_type` allowed here.\n    @classmethod\n    def valid_with_info(cls, value: Any, info: ValidationInfo) -> Any: ...\n```"}, {"role": "assistant", "content": "You mention in the PR description:\r\n\r\n> want to enforce the validator functions to check for any type (by using typing.Any)\r\n\r\nCould you elaborate on this?"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10094, "url": "https://github.com/pydantic/pydantic/pull/10094#discussion_r1719889316"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_main.py`:\n\n```python\nassert '\"TestModel\" object has no field \"b\"' in exc_info.value.args[0]\n\n\ndef test_immutable_with_hashable_fields_are_hashable():\n    class TestModel(BaseModel):\n        a: int = 10\n\n        class Config:\n            allow_mutation = False\n\n    m = TestModel()\n    assert m.__hash__ is not None\n    assert isinstance(hash(m), int)\n\n\ndef test_immutable_with_unhashable_fields_are_not_hashable():\n    class TestModel(BaseModel):\n        a: int = 10\n        y: List[int] = [1, 2, 3]\n\n        class Config:\n            allow_mutation = False\n\n    m = TestModel()\n    with pytest.raises(TypeError) as exc_info:\n        hash(m)\n    assert \"unhashable type: 'list'\" in exc_info.value.args[0]\n```"}, {"role": "assistant", "content": "IMO would be nice for the exception message to be more like \"unhashable type: 'TestModel' contains unhashable type: 'list'  but I wouldn't say it's a strong opinion.  Food for thought."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 1881, "url": "https://github.com/pydantic/pydantic/pull/1881#discussion_r490416671"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_vision_encoder_decoder.py`:\n\n```python\n# encoder outputs might need to be projected to different dimension for decoder\n        if (\n            self.encoder.config.hidden_size != self.decoder.config.hidden_size\n            and getattr(self.decoder.config, \"cross_attention_hidden_size\", None) is None\n            and self.decoder.config.cross_attention_hidden_size is None\n```"}, {"role": "assistant", "content": "```suggestion\n            and getattr(self.decoder.config, \"cross_attention_hidden_size\", None) is None\n```"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42845, "url": "https://github.com/huggingface/transformers/pull/42845#discussion_r2716928528"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `hitl.py`:\n\n```python\nself.multiple = multiple\n\n        self.params: ParamsDict = params if isinstance(params, ParamsDict) else ParamsDict(params or {})\n        if hasattr(self.params, \"filter_params_by_source\"):\n            # Params that exist only in Dag level does not make sense to appear in HITLOperator\n            self.params = ParamsDict.filter_params_by_source(self.params, source=\"task\")\n```"}, {"role": "assistant", "content": "It _seems_ we don't need to check the attribute as we had already cast the `self.params` as `ParamsDict`, then we could directly call the `filter_params_by_source` method?\n\n```suggestion\n        self.params = ParamsDict.filter_params_by_source(self.params, source=\"task\")\n```"}], "_meta": {"repo": "apache/airflow", "pr_number": 58615, "url": "https://github.com/apache/airflow/pull/58615#discussion_r2567316711"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `mode.py`:\n\n```python\nstr(int(self.is_ipynb)),\n            str(int(self.magic_trailing_comma)),\n            str(int(self.experimental_string_processing)),\n            \",\".join(sorted(self.python_cell_magics)),\n```"}, {"role": "assistant", "content": "Of course.  I've done that and resolved the merge conflicts."}], "_meta": {"repo": "psf/black", "pr_number": 2744, "url": "https://github.com/psf/black/pull/2744#discussion_r788436436"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_commands.py`:\n\n```python\ndef parse(self, response):\n        return {'test': 'value'}\n\"\"\",\n                           args=('-s', 'DNSCACHE_ENABLED=False'))\n        print(log)\n\"\"\"\n        log = self.get_log(dnscache_spider, args=('-s', 'DNSCACHE_ENABLED=False'))\n        self.assertNotIn(\"DNSLookupError\", log)\n        self.assertIn(\"INFO: Spider opened\", log)\n\n        if sys.platform == 'win32':\n            log = self.get_log(dnscache_spider, name='myspider,pyw', args=('-s', 'DNSCACHE_ENABLED=False'))\n            self.assertNotIn(\"DNSLookupError\", log)\n            self.assertIn(\"INFO: Spider opened\", log)\n\n    def test_runspider_log_short_names(self):\n        log1 = self.get_log(self.debug_log_spider,\n                            args=('-s', 'LOG_SHORT_NAMES=1'))\n        print(log1)\n        self.assertIn(\"[myspider] DEBUG: It Works!\", log1)\n        self.assertIn(\"[scrapy]\", log1)\n        self.assertNotIn(\"[scrapy.core.engine]\", log1)\n\n        log2 = self.get_log(self.debug_log_spider,\n                            args=('-s', 'LOG_SHORT_NAMES=0'))\n        print(log2)\n```"}, {"role": "assistant", "content": "I don't think we need them, they can be useful when finding why a test fails but in that case you can add them locally"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4646, "url": "https://github.com/scrapy/scrapy/pull/4646#discussion_r446839094"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_utils.py`:\n\n```python\n\"\"\"Test that `decoder_input_ids` can be used to condition the generation in vision-to-text models\"\"\"\n        pixel_values = floats_tensor((2, 3, 30, 30))\n        conditioning_input = torch.tensor([[10], [10]])  # this should be the 2nd output token, after the BOS token\n        model = AutoModelForVision2Seq.from_pretrained(\n        model = AutoModelForImageTextToText.from_pretrained(\n```"}, {"role": "assistant", "content": "the mapping was deprecated in favor of `AutoModelForImageTextToText`, so just making sure we don't use deprecated stuff in tests"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42922, "url": "https://github.com/huggingface/transformers/pull/42922#discussion_r2630516257"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `core_model_loading.py`:\n\n```python\nGLOBAL_WORKERS = min(16, (os.cpu_count() or 8) * 2)  # NVMe: 8-16; HDD/NFS: 2-4\n\n\nclass LoadedParameter(torch.nn.Parameter):\n    r\"\"\"\n    Because `transformers` initialized the missing keys we need to make sure\n    we can skip the ones that are actually loaded. Now we could force something, but\n    we want people to have an intuitive API usage, thus they can keep the well know API, and\n    just define their custom `_init_weight`, as long as they don't use `module.xxx.data`.\n\n    We added a check for this in `make fixup` to force people to use it.\n    After the `missing` weights are initialized, LoadedParameters become just nn.Parameters.\n# Factory function to create LoadedParameter subclasses dynamically\ndef get_loaded_parameter_class(base_cls):\n    \"\"\"\n\n    def __new__(cls, data=None, requires_grad=True):\n        inst = super().__new__(cls, data, requires_grad)\n        inst._is_hf_initialized = False\n        return inst\n\n    def __repr__(self):\n        return f\"LoadedParameter(_is_hf_initialized={self._is_hf_initialized}, data={self.data}\"\n    # block .data assignment when flagged\n    @property\n    def data(self):\n        return super().data\n\n    @data.setter\n    def data(self, new):\n        if not getattr(self, \"_is_hf_initialized\", False):\n            super(LoadedParameter, LoadedParameter).data.__set__(self, new)  # delegate to base\n        # else: skip or warn\n\n    # shadow common in-place init methods\n    def _guard(self, fn, *a, **k):\n        if getattr(self, \"_is_hf_initialized\", False):\n    base_cls: an nn.Parameter subclass (or nn.Parameter)\n    Returns a new class that combines the base_cls with LoadedParameterMixin\n    \"\"\"\n    class LoadedParam(base_cls):\n        _inplace_methods = [\n                'add_', 'mul_', 'clamp_', 'zero_', 'fill_', 'normal_', 'uniform_',\n                'copy_', 'erfinv_', 'log_'\n            ]\n        def __new__(cls, from_existing, **kwargs):\n            inst = super().__new__(cls, from_existing.data, from_existing.requires_grad, **from_existing.__dict__)\n            inst._original_param = from_existing\n            # Explicitly override all in-place methods per instance\n            for method_name in inst._inplace_methods:\n                setattr(inst, method_name, MethodType(inst._skip, inst))\n\n            return inst\n\n        def _skip(self, *args, **kwargs):\n            \"\"\"Helper to skip in-place operations.\"\"\"\n```"}, {"role": "assistant", "content": "simplified a bit to accomodate subclass of nn.parameters cc . Also if we are using this class, it means the param is initialized as you said so let's simplify everything"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42043, "url": "https://github.com/huggingface/transformers/pull/42043#discussion_r2504367329"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_dependencies.py`:\n\n```python\nif not os.environ.get('_SCRAPY_PINNED', None):\n            self.skipTest('Not in a pinned environment')\n\n        tox_config_file_path = os.path.join(\n            os.path.dirname(__file__),\n            '..',\n            'tox.ini',\n        )\n        tox_config_file_path = Path(__file__).parent / '..' / 'tox.ini'\n```"}, {"role": "assistant", "content": "The old code was like this so I didn't change it. `'..'` and `.parent` are slightly different if there are symbolic links. It'd probably be okay to change it here."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5682, "url": "https://github.com/scrapy/scrapy/pull/5682#discussion_r1032656058"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `video_utils.py`:\n\n```python\nvideos (`VideoInput`):\n            Video inputs to turn into a list of videos.\n    \"\"\"\n    if not valid_videos:\n    # Early exit for deeply nested list of image frame paths. We shouldn't flatten them\n    try:\n        if isinstance(videos[0][0][0], str):\n            return [image_paths for sublist in videos for image_paths in sublist]\n    except (IndexError, TypeError):\n        pass\n\n    if isinstance(videos, str) or is_valid_video(videos):\n        return convert_pil_frames_to_video([videos])\n    # only one frame passed, thus we unsqueeze time dim\n    elif is_valid_image(videos):\n        return [np.array(videos)[None, ...]]\n    elif not isinstance(videos, list):\n        raise ValueError(\n            f\"Invalid video input. Expected either a list of video frames or an input of 4 or 5 dimensions, but got\"\n            f\" type {type(videos)}.\"\n        )\n\n    if is_batched_video(videos):\n        pass\n    elif is_valid_video(videos):\n        videos = [videos]\n    # only one frame passed, thus we unsqueeze time dim\n    elif is_valid_image(videos):\n        videos = [np.array(videos)[None, ...]]\n    # nested batch so we need to unflatten\n    elif isinstance(videos[0], (list, tuple)) and is_valid_video(videos[0][0]):\n        videos = [video for sublist in videos for video in sublist]\n    return convert_pil_frames_to_video(videos)\n    # Recursively flatten any nested structure\n    flat_videos_list = []\n    for item in videos:\n        if isinstance(item, str) or is_valid_video(item):\n            flat_videos_list.append(item)\n        elif isinstance(item, list):\n            flat_videos_list.extend(make_batched_videos(item))\n\n    flat_videos_list = convert_pil_frames_to_video(flat_videos_list)\n    return flat_videos_list\n\n\ndef make_batched_metadata(videos: VideoInput, video_metadata: Union[VideoMetadata, dict]):\n    if video_metadata is None:\n        # Create default metadata with 24fps\n        video_metadata = [\n            {\n                \"total_num_frames\": len(video),\n                \"fps\": 24,\n                \"duration\": len(video) / 24,\n                \"frames_indices\": list(range(len(video))),\n            }\n            for video in videos\n        ]\n```"}, {"role": "assistant", "content": "Any time when user doesn't pass metadata. If we are decoding the video ourselves, it will be overwritten afterwards. In all other cases, we will use the default values (e.g input was decoded video or list of image frames)\r\n\r\nThe metadata is then needed when sampling frames, if the user wants to sample from an already decoded video. I decided to not raise any warnings, because not all sampling methods require correct `metadata`"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 39600, "url": "https://github.com/huggingface/transformers/pull/39600#discussion_r2259936990"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `executorch.py`:\n\n```python\n\"batch_size\": batch_size,\n                \"max_cache_len\": max_cache_length,\n            },\n            eos_token_id=model.config.eos_token_id,\n```"}, {"role": "assistant", "content": "I am not sure why `Seq2SeqLMExportableModule` doesn't use something similar to other place like `generation_config = model.generation_config`.\r\n(maybe  or  could say sth here)\r\n\r\nFor the change here, I am also not sure `model.config` is the best thing to use, or it should use `model.generation_config` instead.\r\n\r\nAnd one more thing: if we change here, would other tests will be affected. I guess no as I only see only T5 test file use `Seq2SeqLMExportableModule`."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43374, "url": "https://github.com/huggingface/transformers/pull/43374#discussion_r2713171377"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_ipynb.py`:\n\n```python\nfrom black import NothingChanged, format_cell, format_ipynb_string\nimport os\nfrom tests.util import DEFAULT_MODE\nimport pytest\nimport subprocess\n\npytestmark = pytest.mark.jupyter\n\n\ndef test_noop() -> None:\n    src = 'foo = \"a\"'\n    with pytest.raises(NothingChanged):\n        format_cell(src, mode=DEFAULT_MODE)\n\n\ndef test_trailing_semicolon() -> None:\n    src = 'foo = \"a\" ;'\n    result = format_cell(src, mode=DEFAULT_MODE)\n    expected = 'foo = \"a\";'\n    assert result == expected\n\n\ndef test_trailing_semicolon_with_comment() -> None:\n    src = 'foo = \"a\" ;  # bar'\n    result = format_cell(src, mode=DEFAULT_MODE)\n    expected = 'foo = \"a\";  # bar'\n    assert result == expected\n\n\ndef test_trailing_semicolon_noop() -> None:\n    src = 'foo = \"a\";'\n    with pytest.raises(NothingChanged):\n        format_cell(src, mode=DEFAULT_MODE)\n\n\ndef test_cell_magic() -> None:\n    src = \"%%time\\nfoo =bar\"\n    result = format_cell(src, mode=DEFAULT_MODE)\n    expected = \"%%time\\nfoo = bar\"\n    assert result == expected\n\n\ndef test_cell_magic_noop() -> None:\n    src = \"%%time\\n2 + 2\"\n    with pytest.raises(NothingChanged):\n        format_cell(src, mode=DEFAULT_MODE)\n\n\n@pytest.mark.parametrize(\n    \"src, expected\",\n    (\n        pytest.param(\"ls =!ls\", \"ls = !ls\", id=\"System assignment\"),\n```"}, {"role": "assistant", "content": "What if the input already contains `get_ipython().system` or similar? The output should still be `get_ipython().system`, not something transformed back into magics."}], "_meta": {"repo": "psf/black", "pr_number": 2357, "url": "https://github.com/psf/black/pull/2357#discussion_r667262868"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_forward_ref.py`:\n\n```python\nfunc_val(a=1)\n\n\n@pytest.mark.xfail(\n    reason='In `GenerateSchema`, only the current class module is taken into account. '\n    'This is similar to `test_uses_the_correct_globals_to_resolve_model_forward_refs`.'\n)\ndef test_uses_the_correct_globals_to_resolve_forward_refs_on_serializers(create_module):\n    # Note: unlike `test_uses_the_correct_globals_to_resolve_model_forward_refs`,\n    # we use the globals of the underlying func to resolve the return type.\n    @create_module\n    def module_1():\n        from pydantic import BaseModel, field_serializer  # or model_serializer\n        from pydantic import BaseModel, field_serializer  # or model_serializer, computed_field\n```"}, {"role": "assistant", "content": "```suggestion\r\n        from pydantic import BaseModel, field_serializer  # or model_serializer\r\n```"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10929, "url": "https://github.com/pydantic/pydantic/pull/10929#discussion_r1852553697"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `sample.py`:\n\n```python\ncache_frames=True, stats=self.collect_stats\n            )\n        else:\n            unwinder = _remote_debugging.RemoteUnwinder(\n                self.pid, only_active_thread=bool(self.all_threads), mode=self.mode, native=native, gc=gc,\n                opcodes=opcodes, skip_non_matching_threads=skip_non_matching_threads,\n                cache_frames=True, stats=self.collect_stats\n            )\n            # FIX: Properly handle all_threads vs only_active_thread parameters\n            if self.all_threads:\n                unwinder = _remote_debugging.RemoteUnwinder(\n                    self.pid, all_threads=self.all_threads, mode=self.mode, native=native, gc=gc,\n                    opcodes=opcodes, skip_non_matching_threads=skip_non_matching_threads,\n                    cache_frames=True, stats=self.collect_stats\n                )\n            else:\n                unwinder = _remote_debugging.RemoteUnwinder(\n                    self.pid, only_active_thread=bool(self.all_threads), mode=self.mode, native=native, gc=gc,\n                    opcodes=opcodes, skip_non_matching_threads=skip_non_matching_threads,\n                    cache_frames=True, stats=self.collect_stats\n```"}, {"role": "assistant", "content": "This code is almost the same as above. You should do something like:\n\n```py\nkwargs = {}\nif self.all_threads:\n    kwargs['all_threads' = self.all_threads\nunwinder = _remote_debugging.RemoteUnwinder(..., **kwargs)\n```"}], "_meta": {"repo": "python/cpython", "pr_number": 143426, "url": "https://github.com/python/cpython/pull/143426#discussion_r2672937067"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `configuration_llama4.py`:\n\n```python\nDimensionality of the embeddings and hidden states.\n        intermediate_size (`int`, *optional*, defaults to 8192):\n            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n        intermediate_size_mlp (`int`, *optional*, defaults to 16384): TODO\n        intermediate_size_mlp (`int`, *optional*, defaults to 16384):\n            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the MoE MLP. Increasing this\n            dimension increases the capacity of individual experts to memorize patterns, but linearly scales parameter count.\n```"}, {"role": "assistant", "content": "```suggestion\n            Intermediate size of dense MLP layers. Larger value increases FFN capacity and compute.\n```"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43460, "url": "https://github.com/huggingface/transformers/pull/43460#discussion_r2728379510"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `processing_utils.py`:\n\n```python\nclass methods and docstrings.\n\n    Attributes:\n        annotations (`AnnotationType` or `List[AnnotationType]`, *optional*):\n            List of annotations associated with the image or batch of images.\n        return_segmentation_masks (`bool`, *optional*):\n            Whether to return segmentation masks.\n        masks_path (`str` or `pathlib.Path`, *optional*):\n            Path to the directory containing the segmentation masks.\n        do_resize (`bool`, *optional*):\n```"}, {"role": "assistant", "content": "Yeah this is still on draft! (I will ping again the PR request from github)"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 31964, "url": "https://github.com/huggingface/transformers/pull/31964#discussion_r1689641442"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_gguf_pytorch_utils.py`:\n\n```python\n# to add this patch to ensure things work correctly on our side.\n    if \"llama\" in architecture and \"mistral\" in model_name:\n        updated_architecture = \"mistral\"\n    elif \"t5\" in architecture or \"t5encoder\" in architecture:\n        parsed_parameters[\"config\"][\"tie_word_embeddings\"] = False\n        parsed_parameters[\"config\"][\"is_gated_act\"] = True\n```"}, {"role": "assistant", "content": "Thanks for the explanation. Can you add a comment to explain this ? This is probably something that we need to fix in the future !"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 33389, "url": "https://github.com/huggingface/transformers/pull/33389#discussion_r1786420452"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_api.py`:\n\n```python\nproxy: ProxyTypes | None = None,\n    timeout: TimeoutTypes = DEFAULT_TIMEOUT_CONFIG,\n    follow_redirects: bool = False,\n    verify: VerifyTypes = True,\n    cert: CertTypes | None = None,\n    ssl_context: typing.Optional[ssl.SSLContext] = None,\n```"}, {"role": "assistant", "content": "Do we need DEFAULT_SSL_CONTEXT instead of None?\r\nI can remember there are memory leak issues for high count instantiate of Client. Personally, I fixed this by using global ssl context varible used for all httpx.Client instances.\r\nref: https://github.com/encode/httpx/discussions/2785#discussioncomment-6539886"}], "_meta": {"repo": "encode/httpx", "pr_number": 3022, "url": "https://github.com/encode/httpx/pull/3022#discussion_r1787817691"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_feedexport.py`:\n\n```python\nself.assertEqual(row['expected'], data[feed_options['format']])\n\n\nclass FeedPostProcessedExportsTest(FeedExportTestBase):\n    __test__ = True\n\n    items = [{'foo': 'bar'}]\n    expected = b'foo\\r\\nbar\\r\\n'\n\n    class MyPlugin1:\n        def __init__(self, file, feed_options):\n            self.file = file\n            self.feed_options = feed_options\n            self.char = self.feed_options.get('plugin1_char', b'')\n\n        def write(self, data):\n            return self.file.write(bytes(data) + self.char)\n```"}, {"role": "assistant", "content": "> Does it work if you use 2 separate write calls?\r\n\r\nYup it works that way. It also works like this: `self.file.write(self.char + data)` weirdly.\r\n\r\n> If we go this route, it might be good to indicate that the input data may be a memoryview, and support that officially.\r\n\r\nBy that you mean we should include in the documentation to warn the users that input could be a `memoryview` while using `PickleItemExporter`?"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5190, "url": "https://github.com/scrapy/scrapy/pull/5190#discussion_r677492156"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_known_annotated_metadata.py`:\n\n```python\n)\n            continue\n        elif isinstance(annotation, (at.Predicate, at.Not)):\n            predicate_name = f'{annotation.func.__qualname__}' if hasattr(annotation.func, '__qualname__') else ''\n            predicate_name = f'{annotation.func.__qualname__!r} ' if hasattr(annotation.func, '__qualname__') else ''\n\n            def val_func(v: Any) -> Any:\n```"}, {"role": "assistant", "content": "For performance we could consider formatting the error message before the `def val_func`, so it's reused on each error. Or could have it be cached in some way.\n\nSimilarly might want to have the `at.Predicate` / `at.Not` branches define two separate `val_func`?"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 12289, "url": "https://github.com/pydantic/pydantic/pull/12289#discussion_r2401609542"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_proxies.py`:\n\n```python\nassert transport == client._transport\n    else:\n        assert transport.proxy_origin == url_to_origin(expected)\n\n\n@pytest.mark.parametrize(\n    [\"proxies\", \"expected_scheme\"],\n    [\n        ({\"http\": \"http://127.0.0.1\"}, [\"http://\"]),\n        ({\"https\": \"http://127.0.0.1\"}, [\"https://\"]),\n        ({\"all\": \"http://127.0.0.1\"}, [\"all://\"]),\n    ],\n)\ndef test_for_deprecated_proxy_params(proxies, expected_scheme):\n    with pytest.deprecated_call() as block:\n        httpx.AsyncClient(proxies=proxies)\n\n    warning_message = str(block.pop(DeprecationWarning))\n\n    for scheme in expected_scheme:\n        assert scheme in warning_message\n```"}, {"role": "assistant", "content": "Yeah I think that was walking through the *characters* in the string. :)"}], "_meta": {"repo": "encode/httpx", "pr_number": 1127, "url": "https://github.com/encode/httpx/pull/1127#discussion_r465890018"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_decoders.py`:\n\n```python\nSee: https://stackoverflow.com/questions/1838699\n    \"\"\"\n\n    def __init__(self) -> None:\n    def __init__(self, request: \"Request\") -> None:\n        self.request = request\n```"}, {"role": "assistant", "content": "? (Applicable to all other decoders here)\r\n\r\n```suggestion\r\n        super().__init__(request)\r\n```"}], "_meta": {"repo": "encode/httpx", "pr_number": 1095, "url": "https://github.com/encode/httpx/pull/1095#discussion_r463129186"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_buffer.py`:\n\n```python\nwith self.assertRaises(SystemError):\n            obj.__buffer__(inspect.BufferFlags.WRITE)\n\n    @support.cpython_only\n    @unittest.skipIf(_testcapi is None, \"requires _testcapi\")\n    def test_bytearray_alignment(self):\n        # gh-140557: pointer alignment of buffers including empty allocation\n        # should be at least to `size_t`.\n        align = struct.calcsize(\"N\")\n        cases = [\n            bytearray(),\n            bytearray(1),\n            bytearray(b\"0123456789abcdef\"),\n            bytearray(16),\n        ]\n        ptrs = [_testcapi.buffer_pointer_as_int(array) for array in cases]\n        self.assertEqual([ptr % align for ptr in ptrs], [0]*len(ptrs))\n\n    @support.cpython_only\n    @unittest.skipIf(_testcapi is None, \"requires _testcapi\")\n    def test_array_alignment(self):\n        # gh-140557: pointer alignment of buffers including empty allocation\n        # should be at least to `size_t`.\n        align = struct.calcsize(\"N\")\n```"}, {"role": "assistant", "content": "I haven't fully paged in to this (awesome!) workstream yet, but I wanted to note that I'm *incredibly* inspired by the unstable rust `core::simd` feature, and I maintain both the https://docs rs/re2 and https://docs.rs/vectrorscan-async (hyperscan) rust wrapper crates.\r\n\r\nWith regards to SIMD, I have two ideas:\r\n1. Reflecting the FFI constraints into SIMD-compatible memory for rust `core::simd` could be a great investigation for rust's external types proposal: https://github.com/rust-lang/rust/issues/43467#issuecomment-3741642799\r\n2. I have begun work that searches for SIMD instructions in the cpython configure script: https://discuss.python.org/t/add-zero-copy-conversion-of-bytearray-to-bytes-by-providing-bytes/79164/70\r\n    - I would be very interested to discuss this sort of thing further. In that post I mention two specific use cases (one of them being url quoting in pip) which would produce very noticeable real-world speedups in foundational python tooling like pip.\r\n\r\nI'm also right now doing a cleanroom implementation of zstd in rust (I can explain my concerns with ruzstd) and making use of alignment for SIMD to improve over the zstd C implementation (currently working on a ring buffer component), which I very much intend to plug into a python native module.\r\n\r\nThis comment is in full support of this PR and this workstream. Please let me know if I can follow up and learn more about this! Thanks! \\^\\_\\^"}], "_meta": {"repo": "python/cpython", "pr_number": 140559, "url": "https://github.com/python/cpython/pull/140559#discussion_r2694056220"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `is_simple_lookup_for_doublestar_expression.py`:\n\n```python\n# flags: --preview\nm2 = None if not isinstance(dist, Normal) else m** 2 + s * 2\nm3 = None if not isinstance(dist, Normal) else m**2 + s * 2\nm4 = None if not isinstance(dist, Normal) else s**2 + s * 2\n```"}, {"role": "assistant", "content": "You are right, they are testing the same thing I'll remove the duplicate test."}], "_meta": {"repo": "psf/black", "pr_number": 4154, "url": "https://github.com/psf/black/pull/4154#discussion_r1477632836"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `__init__.py`:\n\n```python\nelif p.is_dir():\n            if exclude is None:\n                exclude = re_compile_maybe_verbose(DEFAULT_EXCLUDES)\n                gitignore = get_gitignore(root)\n                root_gitignore = get_gitignore(root)\n                p_gitignore = get_gitignore(p)\n                # No need to use p's gitignore if it is identical to root's gitignore\n                # (i.e. root and p point to the same directory).\n```"}, {"role": "assistant", "content": "Should this be a separate issue/PR? If its an undefined behavior, then it should be documented properly I think"}], "_meta": {"repo": "psf/black", "pr_number": 3338, "url": "https://github.com/psf/black/pull/3338#discussion_r1012088079"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `processing_siglip.py`:\n\n```python\nfrom ...processing_utils import ProcessorMixin\n\n\ndef _is_siglip2_checkpoint(processor) -> bool:\n    \"\"\"\n    SigLIP2 checkpoints currently ship a SiglipConfig (model_type='siglip') and thus load SiglipProcessor.\n    We detect SigLIP2 primarily via the image processor type/module, with a tokenizer\n    name/path fallback.\n    \"\"\"\n    # SigLIP2 uses Siglip2ImageProcessor / Siglip2ImageProcessorFast\n    image_processor = getattr(processor, \"image_processor\", None)\n    if image_processor is not None:\n        mod = getattr(image_processor, \"__module__\", \"\") or \"\"\n```"}, {"role": "assistant", "content": "Good point ,  processor-only fixes won\u2019t cover text-only usage via AutoTokenizer.\r\n\r\n SigLIP2 checkpoints ship:\r\n\t\u2022\ta SiglipConfig (model_type=\"siglip\") \u2192 resolves to SiglipProcessor\r\n\t\u2022\ttokenizer_class=\"GemmaTokenizer\" in tokenizer_config.json \u2192 resolves to GemmaTokenizer\r\n\t\r\nsiglip2 tokenizer_config.json\r\n```\r\n\"model_max_length\": 1000000000000000019884624838656,\r\n  \"pad_token\": \"<pad>\",\r\n  \"padding_side\": \"right\",\r\n  \"processor_class\": \"SiglipProcessor\",\r\n  \"sp_model_kwargs\": {},\r\n  \"spaces_between_special_tokens\": false,\r\n  \"tokenizer_class\": \"GemmaTokenizer\",\r\n  \"unk_token\": \"<unk>\",\r\n  \"use_default_system_prompt\": false\r\n ```\r\n \r\nThat means SigLIP2-specific text behavior (lowercasing + fixed-length padding to 64) is currently only applied when users go through the processor path. Text-only flows correctly end up using Gemma\u2019s tokenizer, which is expected and correct for Gemma models, but not sufficient for SigLIP2.\r\n\r\nWe shouldn\u2019t bake SigLIP2 behavior into GemmaTokenizer, since that would silently affect all Gemma-based checkpoints. The clean fix is a dedicated Siglip2Tokenizer wrapper subclassing GemmaTokenizer that applies SigLIP2-specific defaults, and can be explicitly selected (or wired via metadata later).\r\n\r\nI\u2019ll add a Siglip2Tokenizer + unit test validating lowercasing and default padding/truncation. I believe that AutoTokenizer won\u2019t pick it up automatically until model metadata is updated, but this PR adds the correct tokenizer abstraction on the library side."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43101, "url": "https://github.com/huggingface/transformers/pull/43101#discussion_r2673264484"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_llava_next.py`:\n\n```python\nfeature_lens = torch.tensor(feature_lens, dtype=torch.long, device=image_features[0].device)\n        return new_image_features, feature_lens\n\n    @check_model_inputs(tie_last_hidden_states=False)\n    @auto_docstring(\n        custom_intro=\"Obtains image last hidden states from the vision tower and apply multimodal projection.\"\n    )\n    def get_image_features(\n        self,\n        pixel_values: torch.FloatTensor,\n        image_sizes: torch.Tensor,\n        vision_feature_layer: int | list[int] | None = None,\n        vision_feature_select_strategy: str | None = None,\n    ):\n        \"\"\"\n        Obtains image last hidden states from the vision tower and apply multimodal projection.\n\n        Args:\n            pixel_values (`torch.FloatTensor]` of shape `(batch_size, num_patches, channels, height, width)`)\n               The tensors corresponding to the input images.\n            image_sizes (`torch.Tensor` of shape `(num_images, 2)`)\n                Actual image size of each images (H, W).\n            vision_feature_layer (`Union[int, list[int]]`, *optional*):\n                The index of the layer to select the vision feature. If multiple indices are provided,\n                the vision feature of the corresponding indices will be concatenated to form the\n                vision features.\n            vision_feature_select_strategy (`str`, *optional*):\n                The feature selection strategy used to select the vision feature from the vision backbone.\n                Can be one of `\"default\"` or `\"full\"`\n        Returns:\n            image_features (list[`torch.Tensor`]): List of image feature tensor, each contains all the visual feature of all patches\n            and are of shape `(num_patches, image_length, embed_dim)`).\n        output_hidden_states: bool | None = None,\n        **kwargs: Unpack[TransformersKwargs],\n    ) -> tuple | BaseModelOutputWithPooling:\n        r\"\"\"\n        pixel_values (`torch.FloatTensor]` of shape `(batch_size, num_patches, channels, height, width)`)\n            The tensors corresponding to the input images.\n        image_sizes (`torch.Tensor` of shape `(num_images, 2)`)\n            Actual image size of each images (H, W).\n        vision_feature_layer (`Union[int, list[int]]`, *optional*):\n            The index of the layer to select the vision feature. If multiple indices are provided,\n            the vision feature of the corresponding indices will be concatenated to form the\n            vision features.\n        vision_feature_select_strategy (`str`, *optional*):\n            The feature selection strategy used to select the vision feature from the vision backbone.\n            Can be one of `\"default\"` or `\"full\"`\n```"}, {"role": "assistant", "content": "Keeping the `vision_...` kwargs so users see them in their IDEs and API Reference, even though I can now technically remove them due to the `check_model_inputs` & having them work via `kwargs` exclusively."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42564, "url": "https://github.com/huggingface/transformers/pull/42564#discussion_r2720579271"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `tokenization_auto.py`:\n\n```python\n(\n            \"llama\",\n            (\n                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n                None,\n                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n```"}, {"role": "assistant", "content": "```suggestion\r\n                \"LlamaTokenizer\" if is_tokenizers_available() else None,\r\n```"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 40936, "url": "https://github.com/huggingface/transformers/pull/40936#discussion_r2428501826"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `linegen.py`:\n\n```python\n# We could enforce triple quotes at this point.\n            quote = quote_char * quote_len\n            leaf.value = prefix + quote + docstring + quote\n\n            if Preview.long_docstring_quotes_on_newline in self.mode:\n                leaf.value = prefix + quote + docstring + quote\n            else:\n```"}, {"role": "assistant", "content": "I assume you want the old tests as well so I've kept them in"}], "_meta": {"repo": "psf/black", "pr_number": 3044, "url": "https://github.com/psf/black/pull/3044#discussion_r865380356"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_api.py`:\n\n```python\nRequestData,\n    RequestFiles,\n    TimeoutTypes,\n    URLTypes,\n    VerifyTypes,\n)\nfrom ._urls import URL\n```"}, {"role": "assistant", "content": "I'd only use that if it was required to avoid a cyclical dependency."}], "_meta": {"repo": "encode/httpx", "pr_number": 3245, "url": "https://github.com/encode/httpx/pull/3245#discussion_r1688184272"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_decoders.py`:\n\n```python\ndef decode(self, data: bytes) -> str:\n        if self.decoder is None:\n            self.decoder = codecs.getincrementaldecoder(\"utf-8\")(errors=\"strict\")\n            attempt_utf_8 = codecs.getincrementaldecoder(\"utf-8\")(errors=\"strict\")\n            try:\n                return self.decoder.decode(data)\n                attempt_utf_8.decode(data)\n            except UnicodeDecodeError:\n                self.decoder = codecs.getincrementaldecoder(\"cp1251\")(errors=\"ignore\")\n                return self.decoder.decode(data)\n                self.decoder = codecs.getincrementaldecoder(\"cp1252\")(errors=\"replace\")\n            else:\n                self.decoder = codecs.getincrementaldecoder(\"utf-8\")(errors=\"replace\")\n```"}, {"role": "assistant", "content": "We need `strict` to raise an error if it doesn't appear to decode as UTF-8, but once we've made the decision we use `errors=\"replace\"` for the most robust behaviour possible. So eg. if we've got a streaming response that initially appears to be UTF-8, but later has some non-UTF-8 bytes, then we're not raising a hard error on accessing `.text`.\r\n\r\n(We'd like it to have a failure mode that is as graceful as possible.)"}], "_meta": {"repo": "encode/httpx", "pr_number": 1269, "url": "https://github.com/encode/httpx/pull/1269#discussion_r485716138"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `requests.py`:\n\n```python\nif self.status != RequestStatus.DECODING:\n            return False\n\n        # If we're recording timestamps, add the token ID and timestamp to the list\n```"}, {"role": "assistant", "content": "You're only adding a timestamp here, no token id, Mr GPT"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42408, "url": "https://github.com/huggingface/transformers/pull/42408#discussion_r2561733072"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_opt.py`:\n\n```python\nself.assertEqual(uop_names.count(\"_RETURN_VALUE\"), 15)\n\n        self.assertEqual(uop_names.count(\"_CHECK_STACK_SPACE\"), 0)\n        self.assertEqual(uop_names.count(\"_CHECK_STACK_SPACE_OPERAND\"), 1)\n        largest_stack = (\n            _testinternalcapi.get_co_framesize(dummy6.__code__) +\n            _testinternalcapi.get_co_framesize(dummy5.__code__) +\n            _testinternalcapi.get_co_framesize(dummy2.__code__) +\n            _testinternalcapi.get_co_framesize(dummy1.__code__) +\n            _testinternalcapi.get_co_framesize(dummy0.__code__)\n        )\n        self.assertIn(\n            (\"_CHECK_STACK_SPACE_OPERAND\", largest_stack), uops_and_operands\n        )\n        self.assertEqual(uop_names.count(\"_CHECK_STACK_SPACE_OPERAND\"), 15)\n\n    @unittest.skip(\"gh-139109 WIP\")\n```"}, {"role": "assistant", "content": "Is it still necessary to retain these tests? I thinks these tests primarily test the logic on converting multiple `_CHECK_STACK_SPACEs` into a single `_CHECK_STACK_SPACE_OPERAND`."}], "_meta": {"repo": "python/cpython", "pr_number": 144394, "url": "https://github.com/python/cpython/pull/144394#discussion_r2754604472"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_model_construction.py`:\n\n```python\nif isinstance(parent_namespace, dict):\n                parent_namespace = unpack_lenient_weakvaluedict(parent_namespace)\n\n            # This is also set in `complete_model_class()`, after schema gen because they are recreated.\n            # We set them here as well for backwards compatibility:\n            cls.__pydantic_computed_fields__ = {\n                k: v.info for k, v in cls.__pydantic_decorators__.computed_fields.items()\n            }\n```"}, {"role": "assistant", "content": "Users can inspect computed fields on models that are not fully built yet, so we need them even though they are going to change after core schema build (the impact is limited compared to normal fields, as it is only the `return_type` property that changes -- see the comment in `complete_model_class()`)."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 11388, "url": "https://github.com/pydantic/pydantic/pull/11388#discussion_r1949905556"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_pointers.py`:\n\n```python\nptr.set_type(c_int)\n        self.assertIs(ptr._type_, c_int)\n\nclass TestPointerStringProto(unittest.TestCase):\n    def test_pointer_string_proto_argtypes_error(self):\n        with self.assertWarns(DeprecationWarning):\n            BadType = ctypes.POINTER(\"BugTrigger\")\n\n        if os.name == \"nt\":\n            libc = ctypes.WinDLL(\"kernel32.dll\")\n            func = libc.GetCurrentProcessId\n        else:\n            libc = ctypes.CDLL(None)\n            func = libc.getpid\n\n        func.argtypes = (BadType,)\n```"}, {"role": "assistant", "content": "You can  use the Python C API:\n\n```suggestion\n        func = ctypes.pythonapi.Py_GetVersion\n        func.argtypes = (BadType,)\n```"}], "_meta": {"repo": "python/cpython", "pr_number": 144108, "url": "https://github.com/python/cpython/pull/144108#discussion_r2722097649"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_modeling_gpt_oss.py`:\n\n```python\nmodel_split_percents = [0.5, 0.6]\n    model_tester_class = GptOssModelTester\n\n    @require_kernels\n    @require_flash_attn\n    @pytest.mark.flash_attn_test\n    @require_torch_gpu\n    def test_initialization_raises_error_for_flash_attn(self):\n        \"\"\"\n        Tests that initializing the model with unsupported Flash Attention implementations raises a ValueError,\n        but allows the specific vllm kernel.\n        \"\"\"\n\n        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n\n        config._attn_implementation = \"flash_attention_2\"\n        with self.assertRaisesRegex(ValueError, \"GPT-OSS model does not support\"):\n            model = GptOssModel(config)\n\n        config._attn_implementation = \"kernels-community/vllm-flash-attn3\"\n        model = GptOssModel(config).to(device=torch_device, dtype=torch.bfloat16)\n```"}, {"role": "assistant", "content": "And here added the dtype casting to half precision (otherwise it won't run because FA only works with half precision)"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42736, "url": "https://github.com/huggingface/transformers/pull/42736#discussion_r2626448219"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `dataclasses.py`:\n\n```python\noriginal_cls = cls\n\n        # we warn on conflicting config specifications, but only if the class doesn't have a dataclass base\n        # because a dataclass base might provide a __pydantic_config__ attribute that we don't want to warn about\n        has_dataclass_base = cls.__bases__ != (object,)\n        if config and (getattr(cls, '__pydantic_config__', None) and not has_dataclass_base):\n```"}, {"role": "assistant", "content": "```suggestion\r\n        if not has_dataclass_base and config is not None and hasattr(cls, '__pydantic_config__'):\r\n```"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10406, "url": "https://github.com/pydantic/pydantic/pull/10406#discussion_r1759132218"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `display.py`:\n\n```python\n\"\"\"\n\nimport sys\nfrom platform import version\nfrom packaging.version import parse\n```"}, {"role": "assistant", "content": ":lipstick: A-Z\r\n```suggestion\r\nfrom packaging.version import parse\r\nfrom platform import version\r\n```"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4403, "url": "https://github.com/scrapy/scrapy/pull/4403#discussion_r399683893"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `fmtonoff5.py`:\n\n```python\n# Regression test for https://github.com/psf/black/issues/3129.\nsetup(\n    entry_points={\n        # fmt: off\n        \"console_scripts\": [\n            \"foo-bar\"\n            \"=foo.bar.:main\",\n        # fmt: on\n        ]\n    },\n)\n\n\n# Regression test for https://github.com/psf/black/issues/3129.\nrun(\n    # fmt: off\n    [\n        \"ls\",\n        \"-la\",\n    ]\n    # fmt: on\n    + path\n    ,\n```"}, {"role": "assistant", "content": "Hmmm, this is concerning. I looked into it and the problem is that the `+` and `path` leaves are being ignored too. Fixing that with a hacky \"if `child` is a Leaf (so doesn't contain children) and contains a `# fmt: on`, then return\" works, but it breaks this example:\r\n```python\r\nsetup(\r\n    entry_points={\r\n        # fmt: off\r\n        \"console_scripts\": [\r\n            \"foo-bar\"\r\n            \"=foo.bar.:main\",\r\n        # fmt: on\r\n        ]\r\n    },\r\n)\r\n```\r\n... since `]` should be ignored too. Unfortunately looking at the CST structure, I can't see an easy way of differentiating these two cases :/  \r\n\r\nI'd prefer to just reject this (i.e. error out / ignore the formatting directive) as an invalid use of `# fmt: off/on` but then the problem is how do you detect invalid uses of `# fmt: off/on` as it seems non-trivial :("}], "_meta": {"repo": "psf/black", "pr_number": 3158, "url": "https://github.com/psf/black/pull/3158#discussion_r916295108"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `models.py`:\n\n```python\nreturn content\n\n    def json(self, **kwargs):\n        r\"\"\"Returns the json-encoded content of a response, if any.\n        r\"\"\"Returns the json-decoded dict of a response, if any.\n```"}, {"role": "assistant", "content": "```suggestion\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n\n           This may return a dictionary, list, etc. depending on what is in the response.\n```"}], "_meta": {"repo": "psf/requests", "pr_number": 6529, "url": "https://github.com/psf/requests/pull/6529#discussion_r1950711358"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `cli_parser.py`:\n\n```python\nlog = logging.getLogger(__name__)\n\n\nfor executor_name in ExecutorLoader.get_executor_names(validate_teams=False):\n```"}, {"role": "assistant", "content": "Good point!\r\n\r\n> Possibly one way of doing it is simple mapping of the executors -> provider (for built-in executors we have mapping from the executor to provider package and checking if the provider has \"cli\" defined in provider info? Also likely we should raise an error if provider has an executor defined with cli commands AND cli command defined in provider info.\r\n\r\nYes, that is the exact same idea for me. It could be done in ProviderManager, and I will add the same validation for AuthManager as well."}], "_meta": {"repo": "apache/airflow", "pr_number": 59805, "url": "https://github.com/apache/airflow/pull/59805#discussion_r2649708589"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `models.py`:\n\n```python\nself.is_closed = True\n            self.is_stream_consumed = True\n            self._raw_content = content or b\"\"\n            self._elapsed = datetime.timedelta(0)\n```"}, {"role": "assistant", "content": "Ah that'd be better yup. As it *happens* we'll always use `stream=...` for actual network responses, it's really only the \"mock response instance\" case that we use `content=...` for, but yes it makes sense to account for the time properly in that case too."}], "_meta": {"repo": "encode/httpx", "pr_number": 692, "url": "https://github.com/encode/httpx/pull/692#discussion_r361860550"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `dag_stability_checker.py`:\n\n```python\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nfrom __future__ import annotations\n\nimport ast\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom pathlib import Path\n\n\n@dataclass\nclass RuntimeVaryingValueWarning:\n    \"\"\"Warning information for runtime-varying value detection.\"\"\"\n\n    line: int\n    col: int\n    code: str\n    message: str\n\n\nclass WarningContext(str, Enum):\n    \"\"\"Context types for warnings.\"\"\"\n\n    TASK_CONSTRUCTOR = \"Task constructor\"\n    DAG_CONSTRUCTOR = \"Dag constructor\"\n\n\nRUNTIME_VARYING_CALLS = [\n```"}, {"role": "assistant", "content": "We can move this list to the top so that we can easily find where to extend"}], "_meta": {"repo": "apache/airflow", "pr_number": 59430, "url": "https://github.com/apache/airflow/pull/59430#discussion_r2660935240"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_prefork_shutdown.py`:\n\n```python\n\"\"\"Integration tests for prefork pool shutdown behaviour.\n\nThese tests verify that the prefork pool gracefully shuts down and maintains\nheartbeats during the shutdown process, preventing connection loss during\nworker drain.\n\"\"\"\n\nfrom time import sleep\n\nimport pytest\n\nfrom celery.contrib.testing.worker import start_worker\nfrom celery.worker import state\n\nfrom .tasks import sleeping\n\nTEST_HEARTBEAT = 2\nLONG_TASK_DURATION = 10  # Exceeds 4-second timeout (2 * 2-second heartbeat)\n```"}, {"role": "assistant", "content": "[nitpick] Incorrect timeout comment: The comment states \"Exceeds 4-second timeout (2 * 2-second heartbeat)\", but this misrepresents AMQP heartbeat behavior. The broker typically closes connections after missing 2 consecutive heartbeat intervals, which would be approximately 2 * heartbeat seconds. With `TEST_HEARTBEAT = 2`, the connection timeout would be around 4 seconds, but the task duration is 10 seconds, which is more than twice the timeout. The comment should clarify that it's the connection timeout, not just a simple multiplication.\n```suggestion\nLONG_TASK_DURATION = 10  # Exceeds AMQP connection timeout (~4 seconds: broker closes after missing 2 consecutive 2-second heartbeats)\n```"}], "_meta": {"repo": "celery/celery", "pr_number": 9986, "url": "https://github.com/celery/celery/pull/9986#discussion_r2533829830"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `sample.py`:\n\n```python\ncache_frames=True, stats=self.collect_stats\n            )\n        else:\n            unwinder = _remote_debugging.RemoteUnwinder(\n                self.pid, only_active_thread=bool(self.all_threads), mode=self.mode, native=native, gc=gc,\n                opcodes=opcodes, skip_non_matching_threads=skip_non_matching_threads,\n                cache_frames=True, stats=self.collect_stats\n            )\n            # FIX: Properly handle all_threads vs only_active_thread parameters\n```"}, {"role": "assistant", "content": "I attempted to revert the changes to `_new_unwinder` as requested, but I found that they are actually required for the tests to pass.\r\n\r\nIf I revert to the original code, `test_profiling` fails because `idle_worker` threads are missing from the stats.\r\n\r\n**The Traceback:**\r\n```text\r\ntest test_profiling failed -- Traceback (most recent call last):\r\n  File \"/workspaces/cpython/Lib/test/test_profiling/test_sampling_profiler/test_modes.py\", line 197, in test_cpu_mode_integration_filtering\r\n    self.assertIn(\"idle_worker\", wall_mode_output)\r\nAssertionError: 'idle_worker' not found in 'Captured 401 samples...\r\n```\r\nThe Logic Error: The issue lies in the original code: `only_active_thread=bool(self.all_threads)`\r\n\r\nWhen `self.all_threads` is True, this sets `only_active_thread=True.` This effectively hides idle threads, which contradicts the intention of all_threads and causes the test failure shown above.\r\n\r\nIt seems my main fix (correcting _FREE_THREADED_BUILD) unmasked this existing logic bug, as the code now enters this path where it previously didn't.\r\n\r\nGiven this dependency, is it okay to keep the _new_unwinder fix in this PR?"}], "_meta": {"repo": "python/cpython", "pr_number": 143426, "url": "https://github.com/python/cpython/pull/143426#discussion_r2672910430"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `reqser.py`:\n\n```python\n\"\"\"\nHelper functions for serializing (and deserializing) requests.\n\"\"\"\nimport inspect\nimport warnings\nfrom typing import Optional\n\nfrom scrapy.http import Request\nfrom scrapy.utils.python import to_unicode\nfrom scrapy.utils.misc import load_object\nimport scrapy\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.utils.request import request_from_dict as _from_dict\n\n\ndef request_to_dict(request, spider=None):\n    \"\"\"Convert Request object to a dict.\nwarnings.warn(\n    (\"Module scrapy.utils.reqser is deprecated, please use scrapy.Request.to_dict\"\n```"}, {"role": "assistant", "content": "```suggestion\r\n    (\"Module scrapy.utils.reqser is deprecated, please use request.to_dict method\"\r\n```\r\n\r\nWhat do you think about ^^? It is a bit more confusing, but a bit more precise as well; e.g. if you're serializing a custom request, you shouldn't be using scrapy.Request.to_dict."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5130, "url": "https://github.com/scrapy/scrapy/pull/5130#discussion_r628913393"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_black.py`:\n\n```python\none.write_text(\"print('hello')\", encoding=\"utf-8\")\n            two = (workspace / \"two.py\").resolve()\n            two.write_text(\"print('hello')\", encoding=\"utf-8\")\n            black.write_cache({}, [one], mode)\n            cache = black.Cache.read(mode)\n            cache.write([one])\n            invokeBlack([str(workspace)])\n            assert one.read_text(encoding=\"utf-8\") == \"print('hello')\"\n            assert two.read_text(encoding=\"utf-8\") == 'print(\"hello\")\\n'\n            cache = black.read_cache(mode)\n            assert str(one) in cache\n            assert str(two) in cache\n            cache = black.Cache.read(mode)\n            assert not cache.is_changed(one)\n            assert not cache.is_changed(two)\n\n    @pytest.mark.parametrize(\"color\", [False, True], ids=[\"no-color\", \"with-color\"])\n    def test_no_cache_when_writeback_diff(self, color: bool) -> None:\n        mode = DEFAULT_MODE\n        with cache_dir() as workspace:\n            src = (workspace / \"test.py\").resolve()\n            src.write_text(\"print('hello')\", encoding=\"utf-8\")\n            with patch(\"black.read_cache\") as read_cache, patch(\n                \"black.write_cache\"\n            with patch.object(black.Cache, \"read\") as read_cache, patch.object(\n                black.Cache, \"write\"\n            ) as write_cache:\n                cmd = [str(src), \"--diff\"]\n                if color:\n                    cmd.append(\"--color\")\n                invokeBlack(cmd)\n                cache_file = get_cache_file(mode)\n                assert cache_file.exists() is False\n                read_cache.assert_called_once()\n```"}, {"role": "assistant", "content": "This is a change in behaviour from eagerly reading the cache. Doesn't seem like a big deal"}], "_meta": {"repo": "psf/black", "pr_number": 3821, "url": "https://github.com/psf/black/pull/3821#discussion_r1294274095"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_utils.py`:\n\n```python\n\"\"\"\n        self._require_grads_hook.remove()\n\n    def get_encoder(self, modality: Optional[str] = None):\n        \"\"\"\n        Best-effort lookup of the *encoder* module. If provided with `modality` argument,\n        it looks for a modality-specific encoder in multimodal models (e.g. \"image_encoder\")\n        By default the function returns model's text encoder if any, and otherwise returns `self`.\n\n        Possible `modality` values are \"image\", \"video\" and \"audio\".\n        \"\"\"\n        # NOTE: new models need to use existing names for layers if possible, so this list doesn't grow infinitely\n        if modality in [\"image\", \"video\"]:\n            possible_module_names = [\"vision_tower\", \"visual\", \"vision_model\", \"vision_encoder\", \"image_tower\"]\n        if modality == \"audio\":\n            possible_module_names = [\"audio_tower\", \"audio_encoder\"]\n        elif modality is None:\n            possible_module_names = [\"text_encoder\", \"encoder\"]\n        else:\n            raise ValueError(f'Unnrecognized modality, has to be \"image\", \"video\" or \"audio\" but found {modality}')\n\n        for name in possible_module_names:\n            if hasattr(self, name):\n                return getattr(self, name)\n\n        if self.base_model is not self and hasattr(self.base_model, \"get_encoder\"):\n            return self.base_model.get_encoder()\n\n        # If this is a base transformer model (no encoder/model attributes), return self\n        return self\n\n    def set_encoder(self, encoder, modality: Optional[str] = None):\n        \"\"\"\n        Symmetric setter. Mirrors the lookup logic used in `get_encoder`.\n        \"\"\"\n\n        # NOTE: new models need to use existing names for layers if possible, so this list doesn't grow infinitely\n```"}, {"role": "assistant", "content": "To note, this should be enforced in `make fixup` in code consistency part to save ourselves the hassle"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42156, "url": "https://github.com/huggingface/transformers/pull/42156#discussion_r2523626575"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `fmtskip10.py`:\n\n```python\ndef foo(): return \"mock\"  # fmt: skip\n```"}, {"role": "assistant", "content": "Thanks for the review.\r\n\r\nI added the suggested tests + some extra ones.\r\nI also mentioned #3682 on the PR description as suggested."}], "_meta": {"repo": "psf/black", "pr_number": 4552, "url": "https://github.com/psf/black/pull/4552#discussion_r1921118400"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_qwen3_omni_moe.py`:\n\n```python\n\"\"\"\n        pixel_values = pixel_values.type(self.visual.dtype)\n        image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n\n        if return_dict:\n            return BaseModelOutputWithPooling(\n                last_hidden_state=image_embeds,\n                # pooler_output=image_features,  # NOTE: @Tom no pooled embeddings here\n```"}, {"role": "assistant", "content": "same thing here, `image_embeds` are actually pooled embeddings and the last hidden state is not returned from `visual`"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42564, "url": "https://github.com/huggingface/transformers/pull/42564#discussion_r2584663455"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `deb822_repository.py`:\n\n```python\ndescription:\n        - Tells APT whether the source is enabled or not.\n        type: bool\n    exclude:\n```"}, {"role": "assistant", "content": "Please create a changelog fragment. See this fragment as an example. And test for this change."}], "_meta": {"repo": "ansible/ansible", "pr_number": 86171, "url": "https://github.com/ansible/ansible/pull/86171#discussion_r2515053194"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `exceptions.py`:\n\n```python\nclass Timeout(RequestException):\n    \"\"\"The request timed out.\"\"\"\n    \"\"\"The request timed out.\n\n    Catching this error will catch both :exc:`ConnectTimeout` and\n    :exc:`ReadTimeout` errors.\n    \"\"\"\n\n\nclass ConnectTimeout(Timeout):\n    \"\"\" The request timed out while trying to connect to the server.\n\n    Requests that produce this error are safe to retry\n    \"\"\"\n\n\nclass ReadTimeout(Timeout):\n    \"\"\"The server did not send any data in the allotted amount of time.\"\"\"\n```"}, {"role": "assistant", "content": "So I agree 100% that the fact this is a breaking change is important. Sub-classing `ConnectionError` is definitely the way to have backwards compatibility, but I'm not confident that all `Timeout` errors should inherit from it. `ConnectTimeout` makes perfect sense. A read timeout is different to me. There's no error in the connection itself. We were able to create it and maintain it. We are the ones terminating it arbitrarily because the user asked us to. There's no \"error\" there. Does that make sense? It's purely semantics, and I'm not entirely against having `Timeout` inherit from `ConnectionError`, I just don't know if it's the most correct way to do it."}], "_meta": {"repo": "psf/requests", "pr_number": 2176, "url": "https://github.com/psf/requests/pull/2176#discussion_r16636255"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_dataclasses.py`:\n\n```python\ntry:\n        inst.x = 'other'\n    except ValidationError as e:\n        assert 'Instance is frozen' in repr(e)\n        assert \"cannot assign to field 'x'\" in repr(e)\n```"}, {"role": "assistant", "content": "Previously the error was thrown from pydantic-core. I think it's better this way, as it is now consistent with the `frozen=True` dataclass argument above this one."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 12173, "url": "https://github.com/pydantic/pydantic/pull/12173#discussion_r2282016535"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_utils.py`:\n\n```python\n\"\"\"\n        self._require_grads_hook.remove()\n\n    def get_encoder(self, modality: Optional[str] = None):\n        \"\"\"\n        Best-effort lookup of the *encoder* module. If provided with `modality` argument,\n        it looks for a modality-specific encoder in multimodal models (e.g. \"image_encoder\")\n        By default the function returns model's text encoder if any, and otherwise returns `self`.\n\n        Possible `modality` values are \"image\", \"video\" and \"audio\".\n        \"\"\"\n        # NOTE: new models need to use existing names for layers if possible, so this list doesn't grow infinitely\n        if modality in [\"image\", \"video\"]:\n            possible_module_names = [\"vision_tower\", \"visual\", \"vision_model\", \"vision_encoder\", \"image_tower\"]\n        if modality == \"audio\":\n            possible_module_names = [\"audio_tower\", \"audio_encoder\"]\n        elif modality is None:\n            possible_module_names = [\"text_encoder\", \"encoder\"]\n        else:\n            raise ValueError(f'Unnrecognized modality, has to be \"image\", \"video\" or \"audio\" but found {modality}')\n\n        for name in possible_module_names:\n            if hasattr(self, name):\n                return getattr(self, name)\n\n        if self.base_model is not self and hasattr(self.base_model, \"get_encoder\"):\n            return self.base_model.get_encoder()\n\n        # If this is a base transformer model (no encoder/model attributes), return self\n        return self\n\n    def set_encoder(self, encoder, modality: Optional[str] = None):\n        \"\"\"\n        Symmetric setter. Mirrors the lookup logic used in `get_encoder`.\n        \"\"\"\n\n        # NOTE: new models need to use existing names for layers if possible, so this list doesn't grow infinitely\n```"}, {"role": "assistant", "content": "Hmm, lemme see where I can fit this in a non-disruptive way. Not sure if users actually read the warnings, we should be more strict in review process in any case imo \ud83d\ude06"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42156, "url": "https://github.com/huggingface/transformers/pull/42156#discussion_r2537406703"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `grid.py`:\n\n```python\n.distinct()\n            ),\n        )\n        .execution_options(yield_per=5)\n```"}, {"role": "assistant", "content": "This seems like a reasonable balance between not loading too much in memory, but also not doing too many round trips."}], "_meta": {"repo": "apache/airflow", "pr_number": 61273, "url": "https://github.com/apache/airflow/pull/61273#discussion_r2748491872"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `models.py`:\n\n```python\nif not self.encoding and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using chardet to make\n            # decoding fails, fall back to `self.text` (using charade to make\n```"}, {"role": "assistant", "content": "We're actually using chardet again since that project has come back to life over at https://github.com/chardet/chardet."}], "_meta": {"repo": "psf/requests", "pr_number": 1900, "url": "https://github.com/psf/requests/pull/1900#discussion_r9380974"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `files.py`:\n\n```python\n@mypyc_attr(patchable=True)\ndef parse_pyproject_toml(path_config: str) -> Dict[str, Any]:\n    \"\"\"Parse a pyproject toml file, pulling out relevant parts for Black\n    \"\"\"Parse a pyproject toml file, pulling out relevant parts for Black.\n\n    If parsing fails, will raise a tomllib.TOMLDecodeError\n    If parsing fails, will raise a tomllib.TOMLDecodeError.\n    \"\"\"\n    with open(path_config, \"rb\") as f:\n        pyproject_toml = tomllib.load(f)\n    config = pyproject_toml.get(\"tool\", {}).get(\"black\", {})\n    return {k.replace(\"--\", \"\").replace(\"-\", \"_\"): v for k, v in config.items()}\n    config: Dict[str, Any] = pyproject_toml.get(\"tool\", {}).get(\"black\", {})\n    config = {k.replace(\"--\", \"\").replace(\"-\", \"_\"): v for k, v in config.items()}\n\n    if \"target_version\" not in config:\n        inferred_target_version = infer_target_version(pyproject_toml)\n        if inferred_target_version is not None:\n            config[\"target_version\"] = [inferred_target_version.name.lower()]\n\n    return config\n\n\ndef infer_target_version(pyproject_toml: Dict[str, Any]) -> Optional[TargetVersion]:\n    \"\"\"Infer Black's target version from the project metadata in pyproject.toml.\n\n    If target version cannot be inferred, returns None.\n    \"\"\"\n    requires_python = get_req_python(pyproject_toml)\n    if requires_python is not None:\n        return parse_req_python(requires_python)\n    return None\n\n\ndef get_req_python(pyproject_toml: Dict[str, Any]) -> Optional[str]:\n    \"\"\"Get the required Python version from the project metadata.\n\n    Currently only supports the PyPA standard format:\n    https://packaging.python.org/en/latest/specifications/declaring-project-metadata/#requires-python\n\n    If the field is not present or cannot be parsed, returns None.\n    \"\"\"\n    project_metadata = pyproject_toml.get(\"project\", {})\n    requires_python: Optional[str] = project_metadata.get(\"requires-python\", None)\n    return requires_python\n\n\ndef parse_req_python(requires_python: str) -> Optional[TargetVersion]:\n    \"\"\"Parse the requires-python field to determine Black's target version.\n\n    If the field cannot be parsed, returns None.\n    \"\"\"\n    try:\n        return parse_req_python_version(requires_python)\n    except InvalidVersion:\n        pass\n    try:\n        return parse_req_python_specifier(requires_python)\n    except InvalidSpecifier:\n        pass\n    return None\n\n\ndef parse_req_python_version(requires_python: str) -> Optional[TargetVersion]:\n    \"\"\"Parse a version string (i.e. ``\"3.7\"``) to a TargetVersion.\n\n    If parsing fails, will raise a packaging.version.InvalidVersion error.\n    \"\"\"\n    version = Version(requires_python)\n```"}, {"role": "assistant", "content": "Oh I suppose it throws InvalidVersion in that case. That raises the question of when this raises `InvalidVersion` and when it returns None, if both of those represent an invalid version."}], "_meta": {"repo": "psf/black", "pr_number": 3219, "url": "https://github.com/psf/black/pull/3219#discussion_r945054466"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_ranges.py`:\n\n```python\n\"\"\"Test the ink_adjusted_lines module.\"\"\"\n```"}, {"role": "assistant", "content": "```suggestion\r\n\"\"\"Test the black.ranges module.\"\"\"\r\n```"}], "_meta": {"repo": "psf/black", "pr_number": 4020, "url": "https://github.com/psf/black/pull/4020#discussion_r1384230619"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `setup.py`:\n\n```python\nfrom setuptools import setup\nfrom setuptools.command.test import test as TestCommand\n\nhere = os.path.abspath(os.path.dirname(__file__))\n\nCURRENT_PYTHON = sys.version_info[:2]\nREQUIRED_PYTHON = (3, 7)\n\nif CURRENT_PYTHON < REQUIRED_PYTHON:\n    sys.stderr.write(\n        \"\"\"\n==========================\nUnsupported Python version\n==========================\nThis version of Requests requires Python {}.{}, but you're trying to\ninstall it on Python {}.{}.\nThis may be because you are using a version of pip that doesn't\nunderstand the python_requires classifier. Make sure you\n```"}, {"role": "assistant", "content": "I think `pip-with-requires-python` is likely the safest option. Alternatively, we'd vend the requires values from the package in our setup.py and compose the install message from those values as  suggested. That avoids any risk for Python 3.4 users who still comprise a significant number of downloads."}], "_meta": {"repo": "psf/requests", "pr_number": 6091, "url": "https://github.com/psf/requests/pull/6091#discussion_r834561046"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `subprocess.py`:\n\n```python\nsts = 0\n            return (pid, sts)\n\n        def _wait_pidfd(self, timeout):\n            \"\"\"Wait for PID to terminate using pidfd_open() + poll().\n            Linux >= 5.3 only.\n            \"\"\"\n            if not _CAN_USE_PIDFD_OPEN:\n                return False\n            try:\n                pidfd = os.pidfd_open(self.pid, 0)\n            except OSError:\n                # May be:\n                # - ESRCH: no such process\n                # - EMFILE, ENFILE: too many open files (usually 1024)\n                # - ENODEV: anonymous inode filesystem not supported\n                # - EPERM, EACCES, ENOSYS: undocumented; may happen if\n                #   blocked by security policy like SECCOMP\n                return False\n\n            try:\n                poller = select.poll()\n                poller.register(pidfd, select.POLLIN)\n                events = poller.poll(math.ceil(timeout * 1000))\n```"}, {"role": "assistant", "content": "In Python 3.15, select.poll.poll() supports timeout with nanosecond resolution. It does round the timeout away from zero (ROUND_UP). I don't think that an explicit ceil() is needed here:\n\n```suggestion\n                events = poller.poll(timeout * 1000)\n```"}], "_meta": {"repo": "python/cpython", "pr_number": 144047, "url": "https://github.com/python/cpython/pull/144047#discussion_r2712832025"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_types.py`:\n\n```python\nfrom ._config import Proxy, Timeout  # noqa: F401\n    from ._models import URL, Cookies, Headers, QueryParams, Request  # noqa: F401\n\nStrOrBytes = Union[str, bytes]\nStrOrBytes = TypeVar(\"StrOrBytes\", str, bytes)\n```"}, {"role": "assistant", "content": "Yeah, it's odd. I actually traced that back to a mypy limitation in `Mapping`/`MutableMapping` (the key type is invariant). Opened #993 with a tight fix inspired by yours here. :-)"}], "_meta": {"repo": "encode/httpx", "pr_number": 985, "url": "https://github.com/encode/httpx/pull/985#discussion_r429625738"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `conftest.py`:\n\n```python\ndef disable_error_urls():\n    # Don't add URLs during docs tests when printing\n    # Otherwise we'll get version numbers in the URLs that will update frequently\n    os.environ['PYDANTIC_ERRORS_OMIT_URL'] = 'true'\n    os.environ['PYDANTIC_ERRORS_INCLUDE_URL'] = 'false'\n```"}, {"role": "assistant", "content": "This could potentially be a breaking change (though I'm not sure who might have this enabled?)"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 9527, "url": "https://github.com/pydantic/pydantic/pull/9527#discussion_r1624634154"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_gcs.py`:\n\n```python\nfrom datetime import datetime\nfrom unittest.mock import Mock, call, patch\nimport sys\nfrom datetime import datetime, timedelta\nfrom unittest.mock import MagicMock, Mock, call, patch\n\nimport pytest\nfrom google.cloud.exceptions import NotFound\n\nfrom celery.backends.gcs import GCSBackend\nfrom celery.exceptions import ImproperlyConfigured\n\n# Workaround until python-firestore is fixed\nis_py313 = sys.version_info >= (3, 13)\nif not is_py313:\n    from celery.backends.gcs import GCSBackend\nelse:\n    GCSBackend = None\n\n\n@pytest.mark.skipif(\n    is_py313,\n    reason=\"https://github.com/googleapis/python-firestore/issues/973\",\n```"}, {"role": "assistant", "content": "I have opened a PR https://github.com/celery/celery/pull/9677 to fix this issue of testing python 3.13"}], "_meta": {"repo": "celery/celery", "pr_number": 9302, "url": "https://github.com/celery/celery/pull/9302#discussion_r2062570098"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_downloadermiddleware.py`:\n\n```python\nIn particular when some website returns a 30x response with header\n        'Content-Encoding: gzip' giving as result the error below:\n\n            exceptions.IOError: Not a gzipped file\n            BadGzipFile: Not a gzipped file (...)\n```"}, {"role": "assistant", "content": "`test_200_and_invalid_gzipped_body_must_fail` (which is the next test here) handles the second case, and while the code raises `BadGzipFile` the tests expects `IOError` so I think we have everything we need, except maybe better comments."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4716, "url": "https://github.com/scrapy/scrapy/pull/4716#discussion_r1237382142"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `trigger.py`:\n\n```python\nencrypted_kwargs: Mapped[str] = mapped_column(\"kwargs\", Text, nullable=False)\n    created_date: Mapped[datetime.datetime] = mapped_column(UtcDateTime, nullable=False)\n    triggerer_id: Mapped[int | None] = mapped_column(Integer, nullable=True)\n    queue: Mapped[str] = mapped_column(String(128), nullable=True)\n```"}, {"role": "assistant", "content": "in airflow-core/src/airflow/models/taskinstance.py:404 the queue field is defined to be 256, so should be the same in the triggerer\n```suggestion\n    queue: Mapped[str] = mapped_column(String(256), nullable=True)\n```"}], "_meta": {"repo": "apache/airflow", "pr_number": 59239, "url": "https://github.com/apache/airflow/pull/59239#discussion_r2649712490"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_peepholer.py`:\n\n```python\nwith self.assertRaisesRegex(TypeError,\n                    'not all arguments converted during string formatting'):\n            eval(\"'%s' % (x, y)\", {'x': 1, 'y': 2})\n        with self.assertRaisesRegex(ValueError, 'incomplete format'):\n        with self.assertRaisesRegex(ValueError, 'stray % at position 2'):\n```"}, {"role": "assistant", "content": "You might keep the word \"format\", like:\n\n```suggestion\n        with self.assertRaisesRegex(ValueError, 'incomplete format: stray % at position 2'):\n```"}], "_meta": {"repo": "python/cpython", "pr_number": 142081, "url": "https://github.com/python/cpython/pull/142081#discussion_r2681741343"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `celery_command.py`:\n\n```python\n@_providers_configuration_loaded\ndef worker(args):\n    \"\"\"Start Airflow Celery worker.\"\"\"\n    # This needs to be imported locally to not trigger Providers Manager initialization\n    from airflow.providers.celery.executors.celery_executor import app as celery_app\n    team_config = None\n    if hasattr(args, \"team\") and args.team:\n        # Multi-team is enabled, create team-specific Celery app and use team based config\n        # This requires Airflow 3.2+, and core.multi_team config to be true to be enabled.\n        if not AIRFLOW_V_3_2_PLUS:\n            raise AirflowConfigException(\n                \"Multi-team Celery workers require Airflow version 3.2 or higher. \"\n                \"Please upgrade your Airflow installation or remove the --team argument.\"\n            )\n        if not conf.getboolean(\"core\", \"multi_team\", fallback=False):\n            raise AirflowConfigException(\n```"}, {"role": "assistant", "content": "Same here\n```suggestion\n            raise SystemExit(\n```"}], "_meta": {"repo": "apache/airflow", "pr_number": 60675, "url": "https://github.com/apache/airflow/pull/60675#discussion_r2744597046"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_engine.py`:\n\n```python\nassert b\"Traceback\" not in stderr, stderr\n\n\nclass TestEngineDownloadSync(TestEngineBase):\n    \"\"\"Test cases for the synchronous download method of ExecutionEngine.\"\"\"\n\n    def setUp(self):\n        super().setUp()\n        self.crawler = get_crawler(MySpider)\n        self.engine = ExecutionEngine(self.crawler, lambda _: None)\n        self.engine.downloader = Mock()\n        self.engine._slot = Mock()\n        self.engine._slot.inprogress = set()\n\n        # Stop the downloader's slot GC loop to prevent reactor cleanup issues\n        # self.engine.downloader._slot_gc_loop = Mock()\n        # self.engine.downloader._slot_gc_loop.running = False\n\n    def tearDown(self):\n        from twisted.internet import reactor\n\n        # Clean up any pending calls\n        for c in reactor.getDelayedCalls():\n            c.cancel()\n```"}, {"role": "assistant", "content": "Are there problems with these tests if this is not done? We have some rare failures related to something like this, but they are a more general problem and it looks like these new tests at least don't fail every time."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6842, "url": "https://github.com/scrapy/scrapy/pull/6842#discussion_r2126223180"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `linegen.py`:\n\n```python\nself.current_line = Line(mode=self.mode)\n\n        v = self.visit_stmt\n        \u00d8: set[str] = set()\n```"}, {"role": "assistant", "content": "Oops, I didn't know. Sounds good in that case, reverted"}], "_meta": {"repo": "psf/black", "pr_number": 4694, "url": "https://github.com/psf/black/pull/4694#discussion_r2145311476"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_modeling_gpt_oss.py`:\n\n```python\ndef test_generate_compile_model_forward_fullgraph(self):\n        return super().test_generate_compile_model_forward_fullgraph()\n\n    def test_reverse_loading_mapping(self, check_keys_were_modified=False):\n        super().test_reverse_loading_mapping(check_keys_were_modified)\n```"}, {"role": "assistant", "content": "I think it's still important to have the test running just to be sure, so I added a kwarg for cases where we do not modify keys"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43227, "url": "https://github.com/huggingface/transformers/pull/43227#discussion_r2698696123"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_validators.py`:\n\n```python\nraise TypeError(f\"Unable to apply constraint 'max_length' to supplied value {x}\")\n\n\n_CONSTRAINT_TO_VALIDATOR_LOOKUP: dict[str, Callable] = {\ndef _extract_decimal_digits_info(decimal: Decimal) -> tuple[int, int]:\n    \"\"\"Compute the total number of digits and decimal places for a given [`Decimal`][decimal.Decimal] instance.\n\n    This function handles both normalized and non-normalized Decimal instances.\n    Example: Decimal('1.230'), normalize=False -> 4 digits, 3 decimal places\n\n    Args:\n        decimal (Decimal): The decimal number to analyze.\n\n    Returns:\n        tuple[int, int]: A tuple containing the number of decimal places and total digits.\n\n    Modeled after the `pydanitc-core` implementation of this function.\n    See https://github.com/pydantic/pydantic-core/blob/f389728432949ecceddecb1f59bb503b0998e9aa/src/validators/decimal.rs#L85.\n\n    Though this could be divided into two separate functions, the logic is easier to follow if we couple the computation\n    of the number of decimals and digits together.\n    \"\"\"\n    _, digit_tuple, exponent = decimal.as_tuple()\n\n    exponent = int(exponent)\n    digits = len(digit_tuple)\n```"}, {"role": "assistant", "content": "```suggestion\r\n    decimal_tuple = decimal.as_tuple()\r\n    exponent = decimal_tuple.exponent\r\n    num_digits = len(decimal_tuple.digits)\r\n```\r\n\r\nIt's just nicer IMO to not use tuple unpacking when names are available. Also renamed `digits` to `num_digits` to clearly distinguish from `decimal_tuple.digits`.\r\n\r\nI removed the `int()` because it seemed redundant, isn't exponent always an int?"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10506, "url": "https://github.com/pydantic/pydantic/pull/10506#discussion_r1778937810"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `form.py`:\n\n```python\nself.headers.setdefault(b'Content-Type', b'application/x-www-form-urlencoded')\n                self._set_body(querystr)\n            else:\n                self._set_url(self.url + ('&' if '?' in self.url else '?') + querystr)\n                url_split = urlsplit(self.url)\n                formdata_key_list = []\n                for k in querystr.split('&'):\n                    formdata_key_list.append(k.split('=')[0])\n                items = []\n                items += [(k, v) for k, v in parse_qsl(url_split.query) if k not in formdata_key_list]\n                query_str = _urlencode(items, self.encoding)\n                self._set_url(urljoin(self.url,'?'+ (query_str + '&' if query_str else '') + querystr + ('#'+ url_split.fragment if url_split.fragment else '')))\n```"}, {"role": "assistant", "content": "I thought of using urlunsplit, but both urljoin and urlunsplit yield the same thing."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 3579, "url": "https://github.com/scrapy/scrapy/pull/3579#discussion_r262397769"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `models.py`:\n\n```python\n*,\n        http_version: str = None,\n        headers: HeaderTypes = None,\n        content: ResponseContent = None,\n        on_close: typing.Callable = None,\n        content: typing.Union[bytes, Stream] = None,\n```"}, {"role": "assistant", "content": "So, right now we're only ever passing `content=<bytes>` in the test cases.\r\n\r\nWe may want to have seperate `content: bytes = None` and `stream: Stream = None` arguments. Instantiating `Response` instances isn't something that users will typically do, but we probably do want it to be doable, and we probably *don't* want to force users to pass a stream instance for the simple case."}], "_meta": {"repo": "encode/httpx", "pr_number": 648, "url": "https://github.com/encode/httpx/pull/648#discussion_r359818452"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_downloadermiddleware_httpproxy.py`:\n\n```python\ndef test_no_proxy(self):\n        os.environ['http_proxy'] = 'https://proxy.for.http:3128'\n        os.environ['no_proxy'] = '/var/run/docker.sock'\n        mw = HttpProxyMiddleware()\n        # '/var/run/docker.sock' may be used by the user for\n        # no_proxy value but is not parseable and should be skipped\n        assert 'no' not in mw.proxies\n```"}, {"role": "assistant", "content": "Yeah that makes sense.\r\nAlso should I do anything about the multi-valued no_proxy env variable like `no_proxy=127.0.0.1,example.com,another.example.com`? `_get_proxy`  does not deal with the multiple values well."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4778, "url": "https://github.com/scrapy/scrapy/pull/4778#discussion_r485140663"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_utils.py`:\n\n```python\nreturn links\n\n\ndef parse_content_type_charset(content_type: str) -> typing.Optional[str]:\n    # We used to use `cgi.parse_header()` here, but `cgi` became a dead battery.\n    # See: https://peps.python.org/pep-0594/#cgi\n    msg = email.message.Message()\n    msg[\"content-type\"] = content_type\n    charset = msg.get_param(\"charset\")\n    if charset is None:\n        return None\n    return str(charset).strip(\"'\\\"\")\n```"}, {"role": "assistant", "content": "```suggestion\r\n    return msg.get_content_charset(failobj=None)\r\n```"}], "_meta": {"repo": "encode/httpx", "pr_number": 2309, "url": "https://github.com/encode/httpx/pull/2309#discussion_r954775637"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `core_model_loading.py`:\n\n```python\nlogger = logging.get_logger(__name__)\n\n\ndef reverse_target_pattern(pattern: str) -> tuple[str, str | None]:\n    \"\"\"\n    Process a target pattern for reverse mapping (when targets become sources).\n\n    This handles several edge cases in checkpoint conversion mappings:\n    - Removes `^` prefix and `$` suffix (start/end of string anchors)\n    - Removes negative lookahead/lookbehind assertions\n    - Detects capturing groups and replaces them with `\\\\1` backreference\n\n    Args:\n        pattern: The target pattern to process for reverse mapping.\n\n    Returns:\n        A tuple of (processed_pattern, captured_group) where captured_group is\n        the original capturing group found (e.g., \"(encoder|decoder)\") or None.\n    \"\"\"\n    # Some mapping contains `^` to notify start of string when matching -> remove it during reverse mapping\n    pattern = pattern.removeprefix(\"^\")\n    # Some mapping contains `$` to notify end of string when matching -> remove it during reverse mapping\n    pattern = pattern.removesuffix(\"$\")\n    # Remove negative lookahead/behind if any. This is ugly but needed for reverse mapping of\n    # Qwen2.5, Sam3, Ernie4.5 VL MoE!\n    pattern = re.sub(r\"\\(\\?.+\\)\", \"\", pattern)\n    # Allow capturing groups in patterns, i.e. to add/remove a prefix to all keys (e.g. timm_wrapper, sam3)\n    capturing_group_match = re.search(r\"\\([^)]+\\)\", pattern)\n```"}, {"role": "assistant", "content": "A slightly simpler version such as this one should work the same way no?\r\n```suggestion\r\n    # Allow capturing groups in patterns, i.e. to add/remove a prefix to all keys (e.g. timm_wrapper, sam3)\r\n    capturing_group_match = re.search(r\"\\(.+?\\)\", pattern)\r\n```"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43205, "url": "https://github.com/huggingface/transformers/pull/43205#discussion_r2689785847"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_triggerer_job.py`:\n\n```python\n# Re-load the triggers\n        trigger_runner_supervisor.load_triggers()\n\n        # Wait for up to 3 seconds for it to vanish from the TriggerRunner's storage\n        for _ in range(30):\n        # Wait for up to 100 seconds for it to vanish from the TriggerRunner's storage\n```"}, {"role": "assistant", "content": "In your description, you said 10s. Do we need 100s here instead?"}], "_meta": {"repo": "apache/airflow", "pr_number": 60754, "url": "https://github.com/apache/airflow/pull/60754#discussion_r2708335341"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `main.py`:\n\n```python\nstrict: Whether to enforce types strictly.\n            from_attributes: Whether to extract data from object attributes.\n            context: Additional context to pass to the validator.\n            by_alias: Whether to use the field's alias to match the input data to an attribute.\n            by_name: Whether to use the field's name to match the input data to an attribute.\n```"}, {"role": "assistant", "content": "```suggestion\r\n            by_alias: Whether to use the field's alias when validating against the provided input data.            by_name: Whether to use the field's name when validating against the provided input data.\r\n```\r\n\r\n(and on the other docstrings)"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 11468, "url": "https://github.com/pydantic/pydantic/pull/11468#discussion_r1971485546"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `shared_memory.py`:\n\n```python\nIn order to ensure proper cleanup of resources, unlink should be\n        called once (and only once) across all processes which have access\n        to the shared memory block.\"\"\"\n        if not self._track:\n            raise TypeError(\"unlink() must be called by a tracking instance\")\n```"}, {"role": "assistant", "content": "I will undo this part and add a check to the resource tracker to not complain if the user did clean up, after all."}], "_meta": {"repo": "python/cpython", "pr_number": 110778, "url": "https://github.com/python/cpython/pull/110778#discussion_r1359939886"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `__init__.py`:\n\n```python\nreturn value\n\n\ndef spellcheck_pyproject_toml_keys(\n    ctx: click.Context, config_keys: List[str], config_file_path: str\n) -> None:\n    available_config_options = {param.name for param in ctx.command.params}\n    for key in config_keys:\n        if key not in available_config_options:\n            raise InvalidConfigKey(f\"Invalid key {key} in {config_file_path}\")\n```"}, {"role": "assistant", "content": "If there are multiple invalid keys, could you make the error message list them all at once? That makes for a better debugging experience, since otherwise if you have N invalid keys you have to run Black N times to see them all."}], "_meta": {"repo": "psf/black", "pr_number": 4165, "url": "https://github.com/psf/black/pull/4165#discussion_r1465451778"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_triggerer_job.py`:\n\n```python\n# Re-load the triggers\n        trigger_runner_supervisor.load_triggers()\n\n        # Wait for up to 3 seconds for it to vanish from the TriggerRunner's storage\n        for _ in range(30):\n        # Wait for up to 100 seconds for it to vanish from the TriggerRunner's storage\n```"}, {"role": "assistant", "content": "> In your description, you said 10s. Do we need 100s here instead?\n\nOops, I'm bad at math. Yes, 100 * 0.1 is 10 seconds. I think we should only need 10s"}], "_meta": {"repo": "apache/airflow", "pr_number": 60754, "url": "https://github.com/apache/airflow/pull/60754#discussion_r2708410951"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_musicgen_melody.py`:\n\n```python\n- [`~generation.GenerateBeamEncoderDecoderOutput`]\n        \"\"\"\n        # 1. Handle `generation_config` and kwargs that might update it, and validate the resulting objects\n        if generation_config is None:\n            generation_config = self.generation_config\n\n        generation_config = copy.deepcopy(generation_config)\n        model_kwargs = generation_config.update(**kwargs)  # All unused kwargs must be model kwargs\n        generation_config.validate()\n        generation_config = self.generation_config if generation_config is None else generation_config\n```"}, {"role": "assistant", "content": "Do we want to prio #43181 first then, I changed the logic to be within update directly (via a new kwarg), lmk what you think otherwise fine to fix this first"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43194, "url": "https://github.com/huggingface/transformers/pull/43194#discussion_r2676615964"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modular_youtu.py`:\n\n```python\nimport torch\nfrom torch import nn\n\nfrom ... import initialization as init\nfrom ...utils import logging\nfrom ..deepseek_v3.modeling_deepseek_v3 import DeepseekV3Attention\nfrom ..llama.modeling_llama import (\n    LlamaDecoderLayer,\n    LlamaForCausalLM,\n    LlamaMLP,\n    LlamaModel,\n    LlamaPreTrainedModel,\n    LlamaRMSNorm,\n    LlamaRotaryEmbedding,\n)\nfrom .configuration_youtu import YoutuConfig\n\n\nlogger = logging.get_logger(__name__)\n\n\nclass YoutuRMSNorm(LlamaRMSNorm):\n    pass\n\n\nclass YoutuRotaryEmbedding(LlamaRotaryEmbedding):\n    pass\n\n\nclass YoutuMLP(LlamaMLP):\n    def __init__(self, config):\n        super().__init__(config)\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n\n\nclass YoutuMLAttention(DeepseekV3Attention):\n    pass\n\n\nclass YoutuDecoderLayer(LlamaDecoderLayer):\n    def __init__(self, config: YoutuConfig, layer_idx: int):\n        nn.Module.__init__(self)\n        self.hidden_size = config.hidden_size\n\n        self.self_attn = YoutuMLAttention(config=config, layer_idx=layer_idx)\n\n        self.mlp = YoutuMLP(config)\n\n        self.input_layernorm = YoutuRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = YoutuRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n```"}, {"role": "assistant", "content": "I guess it clashes because of `YoutuMLAttention`, I think we can simply name it to `YoutuAttention` -> then we have the same prefix everywhere.\r\n\r\nOtherwise, even if we have a difference, we can still use `super().__init__(config, layer_idx)`. You can remove attributes via `del attr` or overwrite those directly `self.attr = SpecialClass(...)`."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43166, "url": "https://github.com/huggingface/transformers/pull/43166#discussion_r2717680558"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_generate_schema.py`:\n\n```python\ndef _types_namespace(self) -> dict[str, Any] | None:\n        return self._types_namespace_stack.tail\n\n    @property\n    def _current_generate_schema(self) -> GenerateSchema:\n        cls = self._config_wrapper.schema_generator or GenerateSchema\n        return cls.__from_parent(\n            self._config_wrapper_stack,\n            self._types_namespace_stack,\n            self.model_type_stack,\n            self._typevars_map,\n            self.defs,\n        )\n```"}, {"role": "assistant", "content": "In theory this can lead to inconsistent state, where in the process of generating a core schema for a field (and thus having one field name in the stack), we end up generating something else. However, the field names are only used when generating schema for `*Validator` classes, that can only be used when defining a field (meaning a new field name will be added to the stack, so it doesn't matter if the stack was reset or not)."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10303, "url": "https://github.com/pydantic/pydantic/pull/10303#discussion_r1746032390"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_typing_extra.py`:\n\n```python\nvalue, globalns, localns\n            )\n    except TypeError as e:\n        if not (isinstance(value, typing.ForwardRef) and is_backport_fixable_error(e)):\n            raise\n        try:\n            from eval_type_backport import eval_type_backport\n        except ImportError:\n            raise TypeError(\n                f'You have a type annotation {value.__forward_arg__!r} '\n                f'which makes use of newer typing features than are supported in your version of Python. '\n                f'To handle this error, you should either remove the use of new syntax '\n                f'or install the `eval_type_backport` package.'\n            ) from e\n\n        return eval_type_backport(value, globalns, localns, try_default=False)\n\n\ndef is_backport_fixable_error(e: TypeError) -> bool:\n    msg = str(e)\n    return msg.startswith('unsupported operand type(s) for |: ') or \"' object is not subscriptable\" in msg\n        if isinstance(value, typing.ForwardRef):\n            try:\n                from eval_type_backport import eval_type_backport\n            except ImportError:\n                raise TypeError(\n                    f'Unable to evaluate type annotation {value.__forward_arg__!r}. '\n                    'If you are making use of the new typing syntax (unions using `|` or builtins subscripting), '\n                    'you should either replace the use of new syntax with the existing `typing` constructs '\n                    'or install the `eval_type_backport` package. Otherwise, it might be '\n                    \"that the type being used isn't subscriptable.\"\n```"}, {"role": "assistant", "content": "Let's incorporate some of this old wording about Python versions:\r\n\r\n```\r\nraise TypeError(\r\n                f'which makes use of newer typing features than are supported in your version of Python. '\r\n            )\r\n```\r\n\r\nOtherwise, this looks good to me. Let's also fix the failing tests bc of the error wording mismatch \ud83d\udc4d"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10030, "url": "https://github.com/pydantic/pydantic/pull/10030#discussion_r1701928409"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_awq.py`:\n\n```python\nSetup quantized model\n        \"\"\"\n        cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_name)\n        cls.quantized_model = AutoModelForCausalLM.from_pretrained(cls.model_name, device_map=cls.device_map)\n        # Use GEMM so that test_save_pretrained() writes out the quantized weights.\n        quantization_config = AwqConfig(backend=AwqBackend.GEMM)\n        cls.quantized_model = AutoModelForCausalLM.from_pretrained(\n            cls.model_name, device_map=cls.device_map, quantization_config=quantization_config\n        )\n```"}, {"role": "assistant", "content": "Makes sense. Feel free to re-write / add some of the tests if you feel like they are way to outdated / can be simplified. For saving for example, I would expect that that we raise an error with one tries to save with exllama or marlin but it passes with `gemm`."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41567, "url": "https://github.com/huggingface/transformers/pull/41567#discussion_r2589046153"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `typing.py`:\n\n```python\nelse:\n                value = ForwardRef(value, is_argument=False)\n        try:\n            value = _eval_type(value, base_globals, None)\n            if sys.version_info > (3, 13):\n```"}, {"role": "assistant", "content": "I don't think I think we use the pattern I've suggested for almost all of our version info checks..."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 9751, "url": "https://github.com/pydantic/pydantic/pull/9751#discussion_r1652840503"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `default_settings.py`:\n\n```python\nRETRY_ENABLED = True\nRETRY_EXCEPTIONS = [\n    \"scrapy.exceptions.CannotResolveHostError\",\n    \"scrapy.exceptions.DownloadConnectionRefusedError\",\n    \"scrapy.exceptions.DownloadFailedError\",\n    \"scrapy.exceptions.DownloadTimeoutError\",\n    \"scrapy.exceptions.ResponseDataLoss\",\n    \"twisted.internet.defer.TimeoutError\",\n```"}, {"role": "assistant", "content": "A big question here is whether we want to remove some Twisted ones if we expected them to never be raised directly, but always be wrapped."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 7208, "url": "https://github.com/scrapy/scrapy/pull/7208#discussion_r2656462997"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `crawler.py`:\n\n```python\nif self.crawling:\n            raise RuntimeError(\"Crawling already taking place\")\n        if self._started:\n            warnings.warn(\n                \"Running Crawler.crawl() more than once is deprecated.\",\n                ScrapyDeprecationWarning,\n                stacklevel=2,\n            raise RuntimeError(\n                \"Running Crawler.crawl() more than once on the same instance is forbidden.\"\n```"}, {"role": "assistant", "content": "\u201cForbidden\u201d in this context sounds a bit weird to me. What about:\r\n\r\n```suggestion\r\n                \"Cannot run Crawler.crawl() more than once on the same instance.\"\r\n```"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6500, "url": "https://github.com/scrapy/scrapy/pull/6500#discussion_r1822137434"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modular_glm4v.py`:\n\n```python\nvalue_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n\n        cos, sin = position_embeddings\n        query_states, key_states = apply_multimodal_rotary_pos_emb(  # diff with Llama\n            query_states, key_states, cos, sin, self.rope_parameters[\"mrope_section\"]\n        )\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n```"}, {"role": "assistant", "content": "This is the same as Qwen2-VL, but Qwen2-VL is the old implementation that you wanted to be improved, so I'm now using the Qwen3-VL style."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43100, "url": "https://github.com/huggingface/transformers/pull/43100#discussion_r2678403787"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `linegen.py`:\n\n```python\nelif not (isinstance(child, Leaf) and is_multiline_string(child)):\n                wrap_in_parentheses(node, child, visible=False)\n\n        check_lpar = isinstance(child, Leaf) and child.value in parens_after\n        comma_check = child.type == token.COMMA if preview else False\n\n        check_lpar = isinstance(child, Leaf) and (\n            child.value in parens_after or comma_check\n        )\n\n\ndef remove_with_parens(node: Node, parent: Node) -> None:\n    \"\"\"Recursively hide optional parens in `with` statements.\"\"\"\n    if node.type == syms.atom:\n        if maybe_make_parens_invisible_in_atom(\n            node,\n            parent=parent,\n            remove_brackets_around_comma=True,\n        ):\n            wrap_in_parentheses(parent, node, visible=False)\n        if isinstance(node.children[1], Node):\n            remove_with_parens(node.children[1], node)\n    elif node.type == syms.testlist_gexp:\n        for child in node.children:\n            if isinstance(child, Node):\n                remove_with_parens(child, node)\n    elif node.type == syms.asexpr_test and not any(\n        leaf.type == token.COLONEQUAL for leaf in node.leaves()\n    ):\n        if maybe_make_parens_invisible_in_atom(\n            node.children[0],\n            parent=parent,\n            remove_brackets_around_comma=True,\n        ):\n            wrap_in_parentheses(parent, node.children[0], visible=False)\n```"}, {"role": "assistant", "content": "`with` is slightly more complicated to achieve in one pass so I've split this into it's own function.\r\n\r\nBasically the different variations of bracketed with statements give pretty different parse trees so we need to recursively solve this.\r\n\r\n`with (open(\"test.txt\")) as f: ...`: this is an `asexpr_test`\r\n`with (open(\"test.txt\") as f): ...`: this is an `atom` containing an `asexpr_test`\r\n`with (open(\"test.txt\")) as f, (open(\"test.txt\")) as f: ...`: this is `asexpr_test`, `COMMA`, `asexpr_test`\r\n`with (open(\"test.txt\") as f, open(\"test.txt\")) as f: ...`: this is an `atom` containing a `testlist_gexp` which then contains multiple `asexpr_test`s"}], "_meta": {"repo": "psf/black", "pr_number": 2926, "url": "https://github.com/psf/black/pull/2926#discussion_r835099427"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `dataclasses.py`:\n\n```python\n**kwargs,\n        )\n\n        if config_wrapper.validate_assignment:\n\n            @functools.wraps(cls.__setattr__)\n            def validated_setattr(instance: Any, field: str, value: str, /) -> None:\n                type(instance).__pydantic_validator__.validate_assignment(instance, field, value)\n\n            cls.__setattr__ = validated_setattr.__get__(None, cls)  # type: ignore\n\n            if slots and not hasattr(cls, '__setstate__'):\n                # If slots is set, `pickle` (relied on by `copy.copy()`) will use\n                # `__setattr__()` to reconstruct the dataclass. However, the custom\n                # `__setattr__()` set above relies on `validate_assignment()`, which\n                # in turn excepts all the field values to be already present on the\n                # instance, resulting in attribute errors.\n                # As such, we make use of `object.__setattr__()` instead.\n                # Note that we do so only if `__setstate__()` isn't already set (this is the\n                # case if on top of `slots`, `frozen` is used).\n\n                # Taken from `dataclasses._dataclass_get/setstate()`:\n                def _dataclass_getstate(self: Any) -> list[Any]:\n                    return [getattr(self, f.name) for f in dataclasses.fields(self)]\n```"}, {"role": "assistant", "content": "I'm not entirely sure I follow, but I think it should be fine: the provided `__get/setstate__()` implementations are used to pickle the dataclass _instances_. If the class definition changes between pickling/unpickling, then it is expected to fail anyway"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 11769, "url": "https://github.com/pydantic/pydantic/pull/11769#discussion_r2064341168"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `optimization.py`:\n\n```python\ncurrent_step: int,\n    *,\n    num_warmup_steps: int,\n    num_training_steps: int,\n    num_stable_steps: int,\n```"}, {"role": "assistant", "content": "If we use `max_steps` in `Trainer`, we have to manually calculate the `stable` ratio in order to pass `num_stable_steps`, and this calculation is more complicated if we use `num_train_epochs`..."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 35453, "url": "https://github.com/huggingface/transformers/pull/35453#discussion_r1909572080"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `types.py`:\n\n```python\nThat is, instead of a [`ValidationError`][pydantic_core.ValidationError] being propagated up and the entire iterable being discarded\nany invalid items are discarded and the valid ones are returned.\n\"\"\"\n\n\n@_dataclasses.dataclass(**_internal_dataclass.slots_true, frozen=True)\nclass FailFast:\n    \"\"\"A `FailFast` can be used to specify that validation should stop at the first error.\n\n    This can be useful when you want to validate a large amount of data and you only need to know if it's valid or not.\n\n    ```py\n    from typing import List\n    from typing_extensions import Annotated\n    from pydantic import BaseModel, FailFast\n\n    class Model(BaseModel):\n        x: Annotated[List[int], FailFast()]\n\n    # This will raise a single error for the first invalid value and stop validation\n    obj = Model(x=[1, 2, 'a', 4, 5, 'b', 7, 8, 9, 'c'])\n```"}, {"role": "assistant", "content": "I guess that's because this version of `pydantic-core` doesn't support `fail_fast` and raises `SchemaError`"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 9708, "url": "https://github.com/pydantic/pydantic/pull/9708#discussion_r1649334709"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `kubernetes.py`:\n\n```python\nself.config_dict = config_dict\n        self._extras: dict | None = connection_extras\n        self._event_polling_fallback = False\n        self._config_loaded = False\n\n    async def _load_config(self):\n        \"\"\"Return Kubernetes API session for use with requests.\"\"\"\n        \"\"\"Load Kubernetes configuration once per hook instance.\"\"\"\n        if self._config_loaded:\n            return\n```"}, {"role": "assistant", "content": "Ah, that is a good reason. Did not consider the `async`dependency. Yes and before adding the complexity of `aiocache` I assume another attribute is reasonable."}], "_meta": {"repo": "apache/airflow", "pr_number": 60532, "url": "https://github.com/apache/airflow/pull/60532#discussion_r2724162642"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_unsupported_python_version.py`:\n\n```python\nimport sys\nfrom io import StringIO\nfrom unittest import TestCase\n\nfrom pytest import mark\n\n\nclass UnsupportedPythonVersionTestCase(TestCase):\n\n    @mark.skipif(sys.version_info >= (3, 6), reason='only meant for unsupported Python versions')\n    def test_unsupported_python_version(self):\n```"}, {"role": "assistant", "content": "That's a very comprehensive approach, I'm impressed -  I've never seen a project testing against Python versions it doesn't support :) \r\n\r\nMy main worries were about the CI time and maintenance time. \r\n\r\nCI time doesn't seem to be large, but that's still a minute per job. \r\n\r\nRegarding maintenance: installing of old Pythons may stop working, pip may stop working in older Pythons, having 2 more envs increases chances for random failure of a build (e.g. because of a network issue in one of the jobs). Also, we now use https://packaging.python.org/guides/distributing-packages-using-setuptools/#python-requires in setup.py, so that the error message which we could break may become less and less relevant over time. \r\n\r\nBecause of that, +0.5 from me for **not** testing against Python 2.7 and 3.5."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4742, "url": "https://github.com/scrapy/scrapy/pull/4742#discussion_r477576939"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `celery_command.py`:\n\n```python\nautoscale = args.autoscale\n    skip_serve_logs = args.skip_serve_logs\n\n    if autoscale is None and conf.has_option(\"celery\", \"worker_autoscale\"):\n        autoscale = conf.get(\"celery\", \"worker_autoscale\")\n    if autoscale is None and config.has_option(\"celery\", \"worker_autoscale\"):\n```"}, {"role": "assistant", "content": "this is what I'm using for these backward compatibility tests:\r\n\r\nBuild the local airflow wheel:\r\n`breeze release-management prepare-provider-distributions --skip-tag-check celery`\r\n\r\nStart specific airflow version:\r\n`breeze start-airflow -d -e --user-airflow-version 3.1.3 --executor CeleryExecutor --airflow-extras celery`\r\n\r\nthen within the container, shutdown the airflow celery worker. Then `pip install ./dist/<celery>.whl`\r\n\r\nthen launch the worker with `airflow celery worker --team my_team`"}], "_meta": {"repo": "apache/airflow", "pr_number": 60675, "url": "https://github.com/apache/airflow/pull/60675#discussion_r2729408062"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_typing_extra.py`:\n\n```python\nwith ns_resolver.push(base):\n            globalns, localns = ns_resolver.types_namespace\n            for name, value in ann.items():\n                hints[name] = eval_type(value, globalns, localns, lenient=lenient)\n                if lenient:\n                    hints[name] = try_eval_type(value, globalns, localns)\n                else:\n                    hints[name] = eval_type(value, globalns, localns)\n    return hints\n\n\ndef eval_type(\ndef try_eval_type(\n    value: Any,\n    globalns: GlobalsNamespace | None = None,\n    localns: MappingNamespace | None = None,\n    *,\n    lenient: bool = False,\n) -> Any:\n    \"\"\"Evaluate the annotation using the provided namespaces.\n) -> tuple[Any, bool]:\n    \"\"\"Try evaluating the annotation using the provided namespaces.\n\n    Args:\n        value: The value to evaluate. If `None`, it will be replaced by `type[None]`. If an instance\n            of `str`, it will be converted to a `ForwardRef`.\n        localns: The global namespace to use during annotation evaluation.\n        globalns: The local namespace to use during annotation evaluation.\n        lenient: Whether to keep unresolvable annotations as is or re-raise the `NameError` exception. Default: re-raise.\n\n    Returns:\n        A two-tuple containing the possibly evaluated type and a boolean indicating\n            whether the evaluation succeeded or not.\n    \"\"\"\n    if value is None:\n        value = NoneType\n    elif isinstance(value, str):\n        value = _make_forward_ref(value, is_argument=False, is_class=True)\n    value = _type_convert(value)\n\n    try:\n        return eval_type_backport(value, globalns, localns)\n        return eval_type_backport(value, globalns, localns), True\n    except NameError:\n        if not lenient:\n            raise\n        # the point of this function is to be tolerant to this case\n        return value\n        return value, False\n\n\ndef eval_type(\n    value: Any,\n    globalns: GlobalsNamespace | None = None,\n    localns: MappingNamespace | None = None,\n) -> Any:\n    \"\"\"Evaluate the annotation using the provided namespaces.\n\n    Args:\n        value: The value to evaluate. If `None`, it will be replaced by `type[None]`. If an instance\n            of `str`, it will be converted to a `ForwardRef`.\n        localns: The global namespace to use during annotation evaluation.\n        globalns: The local namespace to use during annotation evaluation.\n    \"\"\"\n    value = _type_convert(value)\n    return eval_type_backport(value, globalns, localns)\n\n\n@deprecated(\n    '`eval_type_lenient` is deprecated, use `eval_type` with `lenient=True` instead.',\n    '`eval_type_lenient` is deprecated, use `try_eval_type` instead.',\n```"}, {"role": "assistant", "content": "A few more follow ups:\r\n1. This is a very internal function - can we just deprecate this?\r\n2. If we were to bring back a second function, to reduce churn, can we call it `eval_type_lenient`?"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10769, "url": "https://github.com/pydantic/pydantic/pull/10769#discussion_r1831898737"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `conftest.py`:\n\n```python\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nfrom __future__ import annotations\n\nimport os\n\nos.environ[\"_AIRFLOW__AS_LIBRARY\"] = \"true\"\n```"}, {"role": "assistant", "content": "Not a fan with this global side effect on import, especially since this leaks out of the tests (if you run multiple directories together)"}], "_meta": {"repo": "apache/airflow", "pr_number": 59883, "url": "https://github.com/apache/airflow/pull/59883#discussion_r2671243273"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_tutorial002.py`:\n\n```python\n}\n                ]\n            }\n        )\n        | IsDict(\n            # TODO: remove when deprecating Pydantic v1\n```"}, {"role": "assistant", "content": "Can we add it to the function? \r\n\r\n\"Remove v1 argument when deprecating Pydantic v1\"\r\n\r\nMaybe we can use codecrumbs in the future to clean things up. \r\n\r\nThere are a lot of refactorings missing which would be needed for this. But it would be a reason to continue this project :-)"}], "_meta": {"repo": "fastapi/fastapi", "pr_number": 12534, "url": "https://github.com/fastapi/fastapi/pull/12534#discussion_r2019206861"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_squeues_request.py`:\n\n```python\n)\nfrom scrapy.utils.test import get_crawler\n\nif TYPE_CHECKING:\n    from scrapy.crawler import Crawler\n\n\nHAVE_PEEK = hasattr(queuelib.queue.FifoMemoryQueue, \"peek\")\n\nclass TestBaseQueue:\n    def setup_method(self):\n        self.crawler = get_crawler(Spider)\n\n@pytest.fixture\ndef crawler() -> Crawler:\n    return get_crawler(Spider)\n\nclass RequestQueueTestMixin:\n    def queue(self, base_path: Path):\n\nclass TestRequestQueueBase(ABC):\n    @property\n    @abstractmethod\n    def is_fifo(self) -> bool:\n        raise NotImplementedError\n\n    def test_one_element_with_peek(self, tmp_path):\n        if not hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n    @pytest.mark.parametrize(\"test_peek\", [True, False])\n    def test_one_element(self, q: queuelib.queue.BaseQueue, test_peek: bool):\n        if test_peek and not HAVE_PEEK:\n            pytest.skip(\"The queuelib queues do not define peek\")\n        q = self.queue(tmp_path)\n        assert len(q) == 0\n        assert q.peek() is None\n        assert q.pop() is None\n        req = Request(\"http://www.example.com\")\n        q.push(req)\n        assert len(q) == 1\n        assert q.peek().url == req.url\n        assert q.pop().url == req.url\n        assert len(q) == 0\n        assert q.peek() is None\n        assert q.pop() is None\n        q.close()\n\n    def test_one_element_without_peek(self, tmp_path):\n        if hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n        if not test_peek and HAVE_PEEK:\n            pytest.skip(\"The queuelib queues define peek\")\n        q = self.queue(tmp_path)\n        assert len(q) == 0\n        assert q.pop() is None\n        assert q.peek() is None\n        if test_peek:\n            assert q.pop() is None\n        req = Request(\"http://www.example.com\")\n        q.push(req)\n        assert len(q) == 1\n        with pytest.raises(\n            NotImplementedError,\n            match=\"The underlying queue class does not implement 'peek'\",\n        ):\n            q.peek()\n        assert q.pop().url == req.url\n        if test_peek:\n            result = q.peek()\n            assert result is not None\n            assert result.url == req.url\n        result = q.pop()\n        assert result is not None\n        assert result.url == req.url\n        assert len(q) == 0\n        if test_peek:\n            assert q.peek() is None\n        assert q.pop() is None\n        q.close()\n\n\nclass FifoQueueMixin(RequestQueueTestMixin):\n    def test_fifo_with_peek(self, tmp_path):\n        if not hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n    @pytest.mark.parametrize(\"test_peek\", [True, False])\n    def test_order(self, q: queuelib.queue.BaseQueue, test_peek: bool):\n```"}, {"role": "assistant", "content": "This merges 4 methods (test/don't test peek(), fifo/lifo) into 1."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6928, "url": "https://github.com/scrapy/scrapy/pull/6928#discussion_r2173752653"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_modeling_lasr.py`:\n\n```python\ntorch.testing.assert_close(predicted_ids.cpu(), EXPECTED_TOKENS)\n        predicted_transcripts = self.processor.batch_decode(predicted_ids, skip_special_tokens=True)\n        self.assertListEqual(predicted_transcripts, EXPECTED_TRANSCRIPTIONS)\n\n    @slow\n    @unittest.skip(reason=\"TODO when checkpoint\")\n    def test_model_integration_pipe_with_chunk(self):\n        EXPECTED_TRANSCRIPTIONS = [\n            {\"text\": \"Mr. Kuer is thele of the middle classes and we are glad to welcome his gosal.</s>\"}\n        ]\n\n        samples = self._load_datasamples(1)\n        pipe = pipeline(\n            task=\"automatic-speech-recognition\", model=self.checkpoint_name, dtype=self.dtype, device_map=torch_device\n        )\n        self.assertEqual(pipe.type, \"ctc\")\n        predicted_transcripts = pipe(samples, chunk_length_s=3, stride_length_s=1)\n        self.assertListEqual(predicted_transcripts, EXPECTED_TRANSCRIPTIONS)\n```"}, {"role": "assistant", "content": "Moved to https://github.com/huggingface/transformers/pull/42823."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42720, "url": "https://github.com/huggingface/transformers/pull/42720#discussion_r2611995892"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_feedexport.py`:\n\n```python\ndata = yield self.run_and_export(TestSpider, settings)\n        return data\n\n    @defer.inlineCallbacks\n    def assertExported(self, items, header, rows, settings=None, ordered=True):\n        yield self.assertExportedCsv(items, header, rows, settings, ordered)\n        yield self.assertExportedJsonLines(items, rows, settings)\n        yield self.assertExportedXml(items, rows, settings)\n        yield self.assertExportedPickle(items, rows, settings)\n        yield self.assertExportedMarshal(items, rows, settings)\n        yield self.assertExportedMultiple(items, rows, settings)\n\n    @abstractmethod\n    def run_and_export(self, spider_cls, settings):\n        pass\n\n    def _load_until_eof(self, data, load_func):\n        result = []\n        with tempfile.TemporaryFile() as temp:\n            temp.write(data)\n            temp.seek(0)\n            while True:\n                try:\n                    result.append(load_func(temp))\n                except EOFError:\n                    break\n        return result\n\n\nclass FeedExportTest(FeedExportTestBase):\n    __test__ = True\n\n    @defer.inlineCallbacks\n    def run_and_export(self, spider_cls, settings):\n        \"\"\" Run spider with specified settings; return exported data. \"\"\"\n\n        FEEDS = settings.get('FEEDS') or {}\n        settings['FEEDS'] = {\n            urljoin('file:', pathname2url(str(file_path))): feed\n            for file_path, feed in FEEDS.items()\n        }\n        content = {}\n        try:\n            with MockServer() as s:\n```"}, {"role": "assistant", "content": "Okay, finally got it.\r\nMockServer was in FeedExportTest before, so believe there was any reason to do this such way. I will also remember that get_crawler is an alternative. =))"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4434, "url": "https://github.com/scrapy/scrapy/pull/4434#discussion_r418761707"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `crawler.py`:\n\n```python\n\"The crawler_or_spidercls argument cannot be a spider object, \"\n                \"it must be a spider class (or a Crawler object)\"\n            )\n        if not is_asyncio_reactor_installed():\n        if self.settings.getbool(\"TWISTED_ENABLED\"):\n            if not is_asyncio_reactor_installed():\n                raise RuntimeError(\n                    f\"{type(self).__name__} requires AsyncioSelectorReactor when using a reactor.\"\n                )\n        elif is_reactor_installed():\n            raise RuntimeError(\n                f\"{type(self).__name__} requires AsyncioSelectorReactor.\"\n                \"TWISTED_ENABLED is False but a Twisted reactor is installed.\"\n```"}, {"role": "assistant", "content": "In a future PR, e.g. the one adding docs, it may be a good idea to try to provide some next steps here to help users figure out how to debug the issue, maybe a link to some new docs section that we can expand with time."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 7199, "url": "https://github.com/scrapy/scrapy/pull/7199#discussion_r2705806909"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `torchao.py`:\n\n```python\ndef convert(\n        self,\n        input_dict: dict[str, torch.Tensor],\n        source_patterns: list[str] | None = None,\n        model: Optional[torch.nn.Module] = None,\n        full_layer_name: str | None = None,\n        missing_keys=None,\n        **kwargs,\n    ) -> dict[str, torch.Tensor]:\n        if isinstance(self.hf_quantizer.quantization_config.quant_type, str):\n            is_int_4 = \"int4\" in self.hf_quantizer.quantization_config.quant_type\n        else:\n            config_name = self.hf_quantizer.quantization_config.quant_type.__class__.__name__\n            is_int_4 = fuzzy_match_size(config_name) == \"4\"\n\n        # Simple case if we gather layermsnorm weights, we can just return the value since they are not quantized\n        if \"weight:_data\" in input_dict.keys():\n            value = (\n                input_dict[\"weight:_data\"][0]\n                if isinstance(input_dict[\"weight:_data\"], list)\n                else input_dict[\"weight:_data\"]\n            )\n            return {full_layer_name: value}\n\n        is_unsafe_serialization = \":\" not in list(input_dict.keys())[0]\n        \"\"\"\n        Consolidates tensor subclass components before reconstructing the object\n\n        For example:\n            input_dict: {\n                \"_weight_qdata\": torch.Tensor,\n                \"_weight_scale\": torch.Tensor,\n            }\n            full_layer_name: \"model.layers.0.self_attn.k_proj\"\n```"}, {"role": "assistant", "content": "```suggestion\n            full_layer_name: \"model.layers.0.self_attn.k_proj.weight\"\n\n```"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42529, "url": "https://github.com/huggingface/transformers/pull/42529#discussion_r2587920558"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `request.py`:\n\n```python\ntimeout, self.name, self.id)\n        else:\n            task_ready(self)\n            error('Hard time limit (%ss) exceeded for %s[%s]',\n                  timeout, self.name, self.id)\n            exc = TimeLimitExceeded(timeout)\n\n            self.task.backend.mark_as_failure(\n                self.id, exc, request=self._context,\n                store_result=self.store_errors,\n            )\n            # This is a special case where the task timeout handing is done during\n```"}, {"role": "assistant", "content": "```suggestion\r\n            # This is a special case where the task timeout handling is done during\r\n```"}], "_meta": {"repo": "celery/celery", "pr_number": 9678, "url": "https://github.com/celery/celery/pull/9678#discussion_r2065466099"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `scheduler_job_runner.py`:\n\n```python\nreturn executor\n\n    def _exceeds_max_active_runs(\n    def _check_exceeds_max_active_runs(\n```"}, {"role": "assistant", "content": "But i guess set makes it clear we're writing, so, that's good too. i'll just change it to set."}], "_meta": {"repo": "apache/airflow", "pr_number": 60006, "url": "https://github.com/apache/airflow/pull/60006#discussion_r2669221985"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `__init__.py`:\n\n```python\njson: bool\n    \"\"\"Whether to export results to JSON.\"\"\"\n\n    def run(self, *, cwd: Path | None = None) -> None:\n    def run(self, *, cwd: Path | None = None, env: Mapping[str, str] = dict()) -> None:\n```"}, {"role": "assistant", "content": "mutable default arguments are evil -- even if this happens to work fine here, I think it would be better to do:\r\n\r\n```suggestion\r\n    def run(self, *, cwd: Path | None = None, env: Mapping[str, str] | None = None) -> None:\r\n        env = env or {}\r\n```"}], "_meta": {"repo": "astral-sh/ruff", "pr_number": 21536, "url": "https://github.com/astral-sh/ruff/pull/21536#discussion_r2547072646"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `reactor.py`:\n\n```python\nasyncio.set_event_loop(event_loop)\n    else:\n        try:\n            asyncio.get_running_loop()\n            event_loop = policy.get_event_loop()\n        except RuntimeError:\n            # `get_event_loop` is expected to fail when called from a new thread\n            # Raised by policy.get_running_loop() when called from a new thread\n            # with no asyncio event loop yet installed. Such is the case when\n            # called from `scrapy shell`\n            # called from `scrapy shell`.\n            # In Python 3.10, but not in 3.11, policy.get_event_loop() seems to\n            # log a warning when called from the main thread, so we use\n            # asyncio.get_running_loop() to get an earlier RuntimeError as a\n            # workaround.\n```"}, {"role": "assistant", "content": "I think this is happening, it feels like a Python 3.10 bug, but I have found no information about it.\r\n\r\nIf you comment out `asyncio.get_running_loop()` above, `test_install_asyncio_reactor` fails in Python 3.10 but _not_ in Python 3.11. You can try for yourself with:\r\n\r\n```\r\ntox -e py311,py310 -- tests/test_utils_asyncio.py::AsyncioTest tests/test_command_shell.py::ShellTest::test_shell_fetch_async\r\n```"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5782, "url": "https://github.com/scrapy/scrapy/pull/5782#discussion_r1060141846"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_dataclasses.py`:\n\n```python\nx: Union[X, None]\n\n    assert Y(x={'y': None}).x.y is None\n\n@pytest.mark.parametrize(\"field_fn_unset\", [Field, dataclasses.field])\n@pytest.mark.parametrize(\"field_fn2\", [Field, dataclasses.field])\ndef test_annotated_with_field_default_factory( field_fn_unset, field_fn2) -> None:\n```"}, {"role": "assistant", "content": "I did run the `make format` command, but weirdly I only got the following output:\r\n```\r\nPDM, version 2.17.1\r\npdm run ruff check --fix pydantic tests docs/plugins\r\nAll checks passed!\r\npdm run ruff format pydantic tests docs/plugins\r\n87 files left unchanged\r\n```\r\nIt does not format the unformatted test file, so I have to explicitly format it: \r\n```\r\n$ pdm run ruff format .\\tests\\test_dataclasses.pytaclasses.py  test_datetime.py\r\n1 file reformatted                              \r\n```\r\nIt does work for files under `pydantic/` though."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 9971, "url": "https://github.com/pydantic/pydantic/pull/9971#discussion_r1691217255"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `canvas.py`:\n\n```python\n\"\"\"\n        headers = headers.copy()\n        if visitor is not None:\n            headers.update(visitor.on_signature(self, **headers))\n            visitor_headers = visitor.on_signature(self, **headers)\n            if \"stamped_headers\" not in visitor_headers:\n```"}, {"role": "assistant", "content": "Feel free to merge if it passes your code review"}], "_meta": {"repo": "celery/celery", "pr_number": 7856, "url": "https://github.com/celery/celery/pull/7856#discussion_r1008843342"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_instructblip.py`:\n\n```python\nquery_output = query_outputs[0][:, : query_tokens.size(1), :]\n\n        # step 3: use the language model, conditioned on the query outputs and the prompt\n        language_model_inputs = self.language_projection(query_output)\n        image_features = self.language_projection(query_output)\n\n        # NOTE: @Tom backwards incompatibility\n        if return_dict:\n            return language_model_inputs, vision_outputs, query_outputs\n        return language_model_inputs\n            return BaseModelOutputWithVisionQformerOutputs(\n                last_hidden_state=vision_outputs.last_hidden_state,\n                pooler_output=image_features,\n                vision_outputs=vision_outputs,\n                qformer_outputs=query_outputs,\n            )\n```"}, {"role": "assistant", "content": "wow, this is super weird, I always disliked InstructBLIP-style outputs! We would need to break it in the future, taking a note to myself"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42564, "url": "https://github.com/huggingface/transformers/pull/42564#discussion_r2619036930"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_compat.py`:\n\n```python\nreturn self.field_info.annotation\n\n        def __post_init__(self) -> None:\n            self._type_adapter: TypeAdapter[Any] = TypeAdapter(\n                Annotated[self.field_info.annotation, self.field_info]\n            )\n            with warnings.catch_warnings():\n                # Pydantic >= 2.12.0 warns about field specific metadata that is unused\n                # (e.g. `TypeAdapter(Annotated[int, Field(alias='b')])`). In some cases, we\n                # end up building the type adapter from a model field annotation so we\n                # need to ignore the warning:\n                if PYDANTIC_VERSION_MINOR_TUPLE >= (2, 12):  # pragma: no cover\n                    from pydantic.warnings import (  # type: ignore[attr-defined]\n                        UnsupportedFieldAttributeWarning,\n                    )\n\n                    warnings.simplefilter(\n```"}, {"role": "assistant", "content": "This is still incorrect, as per my first point here."}], "_meta": {"repo": "fastapi/fastapi", "pr_number": 14036, "url": "https://github.com/fastapi/fastapi/pull/14036#discussion_r2410098702"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_generate_schema.py`:\n\n```python\ndef _annotated_schema(self, annotated_type: Any) -> core_schema.CoreSchema:\n        \"\"\"Generate schema for an Annotated type, e.g. `Annotated[int, Field(...)]` or `Annotated[int, Gt(0)]`.\"\"\"\n        FieldInfo = import_cached_field_info()\n        # Ideally, we should delegate all this to `_typing_extra.unpack_annotated`, e.g.:\n        # `typ, annotations = _typing_extra.unpack_annotated(annotated_type); schema = self.apply_annotations(...)`\n        # if it was able to use a `NsResolver`. But because `unpack_annotated` is also used\n        # when constructing `FieldInfo` instances (where we don't have access to a `NsResolver`),\n        # the implementation of the function does *not* resolve forward annotations. This could\n        # be solved by calling `unpack_annotated` directly inside `collect_model_fields`.\n        # For now, we at least resolve the annotated type if it is a forward ref, but note that\n        # unexpected results will happen if you have something like `Annotated[Alias, ...]` and\n        # `Alias` is a PEP 695 type alias containing forward references.\n```"}, {"role": "assistant", "content": "This is no longer an issue because we no longer eagerly unpack type aliases here. So all the concerns written here don't stand anymore, as if you have something like:\r\n\r\n```python\r\ntype Alias = list['ForwardRef']\r\n\r\nGenerateSchema().generate_schema(Annotated[Alias, ...])\r\n```\r\n\r\nWe previously tried to call `unpack_annotated` with `Alias` (and we couldn't resolve forward annotations inside `unpack_annotated`).\r\n\r\nNow, We just leave `Alias` as is, generate a schema for it (so it will go through `GenerateSchema._type_alias_type_schema()`, and we can use the type namespace here if we have forward references), and apply the annotations after."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 11475, "url": "https://github.com/pydantic/pydantic/pull/11475#discussion_r1967609712"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_modeling_common.py`:\n\n```python\nelse:\n                    config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n                inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n                set_config_for_less_flaky_test(config)\n                model = model_class(config).eval().to(torch_device)\n                set_model_for_less_flaky_test(model)\n```"}, {"role": "assistant", "content": "i noticed some flakiness in models like pe_audio internvl and others, and it disappeared once i set these, it shouldn't be an issue setting them here because all we care about is graph capture, numerical stability is pytorch's responsibility \ud83d\ude06"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43436, "url": "https://github.com/huggingface/transformers/pull/43436#discussion_r2747119211"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `sql.py`:\n\n```python\ntherefore it is expected the table has them defined.\n    :param uppercase_names: if True use schema and table names uppercase\n    \"\"\"\n    if and_ is None or or_ is None:\n        raise AirflowOptionalProviderFeatureException(\n            \"sqlalchemy is required for SQL filter clause generation. \"\n            \"Install it with: pip install 'apache-airflow-providers-openlineage[sqlalchemy]'\"\n        )\n```"}, {"role": "assistant", "content": "```suggestion\ntry:\n    from sqlalchemy import and_, or_\nexcept ImportError:\n    raise AirflowOptionalProviderFeatureException(\n        \"sqlalchemy is required for SQL filter clause generation. \"\n        \"Install it with: pip install 'apache-airflow-providers-openlineage[sqlalchemy]'\"\n    )\n```\nSame as above"}], "_meta": {"repo": "apache/airflow", "pr_number": 59921, "url": "https://github.com/apache/airflow/pull/59921#discussion_r2652929590"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_security.py`:\n\n```python\nrole_name=role_name,\n        ) as user:\n            dag1 = DagModel(dag_id=dag_id, bundle_name=bundle_name)\n            dag1.timetable_type = \"\"\n            dag2 = DagModel(dag_id=dag_id_2, bundle_name=bundle_name)\n            dag2.timetable_type = \"\"\n```"}, {"role": "assistant", "content": "...or as this seems to be a tideus job making this for all hundrets of tests.. if no `schedule` attribute is given in DagModel constructor how about setting the timetable_type in the constructor directly such that not all tests need to be adjusted?"}], "_meta": {"repo": "apache/airflow", "pr_number": 58852, "url": "https://github.com/apache/airflow/pull/58852#discussion_r2664331789"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_return_with_argument_inside_generator.py`:\n\n```python\nimport unittest\n\nfrom scrapy.utils.misc import is_generator_with_return_value\n\n\nclass UtilsMiscPy3TestCase(unittest.TestCase):\n\n    def test_generators_with_return_statements(self):\n        \"\"\"\n        'return with argument inside generator' is a SyntaxError before Python 3.3\n        \"\"\"\n        def f():\n            yield 1\n            return 2\n\n        def g():\n            yield 1\n            return 'asdf'\n\n        def h():\n            yield 1\n            return None\n\n        def i():\n            yield 1\n            return\n\n        def j():\n            yield 1\n\n        assert is_generator_with_return_value(f)\n        assert is_generator_with_return_value(g)\n        assert not is_generator_with_return_value(h)\n        assert not is_generator_with_return_value(i)\n        assert not is_generator_with_return_value(j)\n```"}, {"role": "assistant", "content": "I can think of a test case (expected failure?) that we might want to cover here: A test that verifies that, if something like `yield from` is used and this issue affects the nested generator, we will still not detect it because we do not inspect callbacks recursively (which I assume would make things much more complicated and/or have a potentially-significant negative impact on performance)."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 3869, "url": "https://github.com/scrapy/scrapy/pull/3869#discussion_r303400904"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `main.py`:\n\n```python\n\"\"\"\n        if not force and cls.__pydantic_complete__:\n            return None\n\n        if '__pydantic_core_schema__' in cls.__dict__:\n            delattr(cls, '__pydantic_core_schema__')  # delete cached value to ensure full rebuild happens\n\n        if _types_namespace is not None:\n            rebuild_ns = _types_namespace\n        elif _parent_namespace_depth > 0:\n            rebuild_ns = _typing_extra.parent_frame_namespace(parent_depth=_parent_namespace_depth, force=True) or {}\n        else:\n            if '__pydantic_core_schema__' in cls.__dict__:\n                delattr(cls, '__pydantic_core_schema__')  # delete cached value to ensure full rebuild happens\n            if _types_namespace is not None:\n                types_namespace: dict[str, Any] | None = _types_namespace.copy()\n            else:\n                if _parent_namespace_depth > 0:\n                    frame_parent_ns = (\n                        _typing_extra.parent_frame_namespace(parent_depth=_parent_namespace_depth, force=True) or {}\n                    )\n                    cls_parent_ns = (\n                        _model_construction.unpack_lenient_weakvaluedict(cls.__pydantic_parent_namespace__) or {}\n                    )\n                    types_namespace = {**cls_parent_ns, **frame_parent_ns}\n                    cls.__pydantic_parent_namespace__ = _model_construction.build_lenient_weakvaluedict(types_namespace)\n                else:\n                    types_namespace = _model_construction.unpack_lenient_weakvaluedict(\n                        cls.__pydantic_parent_namespace__\n                    )\n\n                types_namespace = _typing_extra.merge_cls_and_parent_ns(cls, types_namespace)\n\n            # manually override defer_build so complete_model_class doesn't skip building the model again\n            config = {**cls.model_config, 'defer_build': False}\n            return _model_construction.complete_model_class(\n                cls,\n                cls.__name__,\n                _config.ConfigWrapper(config, check=False),\n                raise_errors=raise_errors,\n                types_namespace=types_namespace,\n            )\n            rebuild_ns = {}\n\n        parent_ns = _model_construction.unpack_lenient_weakvaluedict(cls.__pydantic_parent_namespace__) or {}\n\n        ns_resolver = _namespace_utils.NsResolver(\n            parent_namespace={**rebuild_ns, **parent_ns},\n```"}, {"role": "assistant", "content": "Another place collections.ChainMap could maybe be used, maybe not necessary"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10530, "url": "https://github.com/pydantic/pydantic/pull/10530#discussion_r1795451788"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `networks.py`:\n\n```python\n\"\"\"\n\n    _constraints = UrlConstraints(\n        allowed_schemes=['clickhouse+native', 'clickhouse+asynch'],\n        allowed_schemes=['clickhouse+native', 'clickhouse+asynch', 'clickhouse', 'clickhouses', 'clickhousedb'],\n```"}, {"role": "assistant", "content": "```suggestion\r\n        allowed_schemes=['clickhouse+native', 'clickhouse+asynch', 'clickhouse+http', 'clickhouse', 'clickhouses', 'clickhousedb'],\r\n```"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 11319, "url": "https://github.com/pydantic/pydantic/pull/11319#discussion_r1928494196"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `files.py`:\n\n```python\nfor directory in (common_base, *common_base.parents):\n        if (directory / \".git\").exists():\n            return directory\n            return directory, \".git directory\"\n\n        if (directory / \".hg\").is_dir():\n            return directory\n            return directory, \".hg directory\"\n\n        if (directory / \"pyproject.toml\").is_file():\n            return directory\n            return directory, \"pyproject.toml\"\n\n    return directory\n    return directory, None\n```"}, {"role": "assistant", "content": "```suggestion\r\n    return directory, \"file system root\"\r\n```\r\nIf this is the meaning and method, we should be clear about it."}], "_meta": {"repo": "psf/black", "pr_number": 2526, "url": "https://github.com/psf/black/pull/2526#discussion_r775400892"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `bigtable.py`:\n\n```python\n\"\"\"\n        instance = self.get_instance(instance_id=instance_id, project_id=project_id)\n        if instance is None:\n            raise RuntimeError(f\"Instance {instance_id} did not exist; unable to delete table {table_id}\")\n        table = instance.table(table_id=table_id)\n        table.delete()\n            self.log.warning(\"Instance '%s' does not exist in project '%s'. Exiting\", instance_id, project_id)\n```"}, {"role": "assistant", "content": "Here it actually makes sense to raise a `RuntimeError`, because the instance needs to exists so we could actually delete the table. After we ensure that it exists, then we could actually use `try..except` to supress the table if it doesn't exist - which apparently we already do in the operator."}], "_meta": {"repo": "apache/airflow", "pr_number": 60712, "url": "https://github.com/apache/airflow/pull/60712#discussion_r2702728664"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `kubernetes_helper_functions.py`:\n\n```python\n# Compat: Look up the run_id from the TI table!\n    from airflow.models.dagrun import DagRun\n    from airflow.models.taskinstance import TaskInstance, TaskInstanceKey\n    from airflow.models.taskinstance import TaskInstance\n    from airflow.models.taskinstancekey import TaskInstanceKey\n```"}, {"role": "assistant", "content": "I wanted to change this to common.compat too but unfortunately it is eventually going to go through the KE. Since baseexecutor is typed with `from airflow.models.taskinstancekey import TaskInstanceKey`, we won't be able to use `airflow.sdk.types.TaskInstanceKey ` for typing for _any_ executor related functions which somewhat makes sense too because executor is not supposed to consume from sdk i think"}], "_meta": {"repo": "apache/airflow", "pr_number": 60776, "url": "https://github.com/apache/airflow/pull/60776#discussion_r2719617821"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `http2.py`:\n\n```python\nclass HTTP2Stream:\n    def __init__(\n        self,\n        stream_id: int,\n        connection: HTTP2Connection,\n        state: h2.connection.H2Connection,\n    ) -> None:\n    def __init__(self, stream_id: int, connection: HTTP2Connection,) -> None:\n```"}, {"role": "assistant", "content": "Formatting nit (I've already had these, not sure why Black doesn't reformat them):\r\n\r\n```suggestion\r\n    def __init__(self, stream_id: int, connection: HTTP2Connection) -> None:\r\n```"}], "_meta": {"repo": "encode/httpx", "pr_number": 628, "url": "https://github.com/encode/httpx/pull/628#discussion_r357013303"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `conftest.py`:\n\n```python\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nfrom __future__ import annotations\n\nimport os\n\nos.environ[\"_AIRFLOW__AS_LIBRARY\"] = \"true\"\n```"}, {"role": "assistant", "content": "The thing I don\u2019t like is if you run the shared lib and core tests in the same process (say locally in breeze) it would mysteriously fail. I wonder if this would work in a module-scoped fixture instead."}], "_meta": {"repo": "apache/airflow", "pr_number": 59883, "url": "https://github.com/apache/airflow/pull/59883#discussion_r2671339443"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_llava_next.py`:\n\n```python\nraise ValueError(\n                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n            )\n\n        legacy_processing = False\n        if inputs_embeds is None:\n            inputs_embeds = self.get_input_embeddings()(input_ids)\n            input_ids_mask = input_ids.clone()\n            input_ids_mask[input_ids == self.config.image_token_index] = 0\n            inputs_embeds = self.get_input_embeddings()(input_ids_mask)\n```"}, {"role": "assistant", "content": "Hmm, makes sense indeed, but I am not sure if this would break compile compatibility for LLaVA models due to `clone()` op. Because if we accept this change, it has to be propagated to all LLaVA models. \r\n\r\nCurrently they are not all compatible with `torch.compile(fullgraph=True)` as we have to remove legacy path, but nice to make sure it will work. I am more inclined to asking the authors on the hub to resize their embedding layer than adding a workaround in code though"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 34728, "url": "https://github.com/huggingface/transformers/pull/34728#discussion_r1846360848"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_compat.py`:\n\n```python\nreturn FakeMetadata()\n\n    def post_init_field_info(field_info: FieldInfo) -> None:\n        return None\n        if IS_PYDANTIC_V2:\n```"}, {"role": "assistant", "content": "I just checked it with Python 3.11 and Pydantic 1.10.22 - tests passed.\r\nCould you please check it one more time?\r\n```py\r\n    def post_init_field_info(field_info: FieldInfo) -> None:\r\n        if field_info.alias and not field_info.validation_alias:\r\n            field_info.validation_alias = field_info.alias\r\n        if field_info.alias and not field_info.serialization_alias:\r\n            field_info.serialization_alias = field_info.alias\r\n```"}], "_meta": {"repo": "tiangolo/sqlmodel", "pr_number": 1577, "url": "https://github.com/fastapi/sqlmodel/pull/1577#discussion_r2405320135"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `pattern_matching_extras.py`:\n\n```python\nmatch match:\n    case case:\n        pass\n\n\nmatch a, *b(), c:\n```"}, {"role": "assistant", "content": "Well then, I need to play with pattern matching too."}], "_meta": {"repo": "psf/black", "pr_number": 2667, "url": "https://github.com/psf/black/pull/2667#discussion_r761999611"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `misc.py`:\n\n```python\ndel os.environ[k]\n            else:\n                os.environ[k] = v\n\n\n_trailing_whitespaces_re = re.compile(r'^(\\s*)')\n\n\ndef scan_generator_for_return_statement(callable):\n    \"\"\"\n    Returns True if a callable is a generator function and includes a\n    'return' statement with a value different than None, False otherwise\n    \"\"\"\n\n    def reduce_indentation_level(callable_lines):\n```"}, {"role": "assistant", "content": "Maybe it\u2019s better to use dedent, but other than that\u2026"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 3869, "url": "https://github.com/scrapy/scrapy/pull/3869#discussion_r303397557"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `setup.py`:\n\n```python\n'zope.interface>=4.1.3',\n    'protego>=0.1.15',\n    'itemadapter>=0.1.0',\n    'h2>=3.2.0',\n    'typing-extensions>=3.7.4',\n```"}, {"role": "assistant", "content": "I have removed the `typing-extensions` dependency in 5e36f539e28631625862b84d8ca1c7c0134584b0."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4610, "url": "https://github.com/scrapy/scrapy/pull/4610#discussion_r478293897"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_responses.py`:\n\n```python\nassert response.encoding == \"iso-8859-1\"\n\n\ndef test_response_force_encoding_after_text_accessed():\n    response = httpx.Response(\n        200,\n        content=b\"Hello, world!\",\n    )\n    assert response.status_code == 200\n    assert response.reason_phrase == \"OK\"\n    assert response.text == \"Hello, world!\"\n    assert response.encoding == \"utf-8\"\n\n    response.encoding = \"UTF8\"\n```"}, {"role": "assistant", "content": "I'd expect the `ValueError` to be raised here. I don't think we need the conditional \"don't raise if the encoding is being set but the resulting codec won't change\".\n\nSimplicity over complexity where possible."}], "_meta": {"repo": "encode/httpx", "pr_number": 2852, "url": "https://github.com/encode/httpx/pull/2852#discussion_r1328064919"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_align.py`:\n\n```python\n>>> with torch.inference_mode():\n        ...     text_features = model.get_text_features(**inputs)\n        ```\"\"\"\n        text_outputs = self.text_model(\n        text_outputs: BaseModelOutputWithPooling = self.text_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            inputs_embeds=inputs_embeds,\n            **kwargs,\n        )\n        last_hidden_state = text_outputs[0][:, 0, :]\n        text_features = self.text_projection(last_hidden_state)\n        text_outputs.pooler_output = self.text_projection(last_hidden_state)\n```"}, {"role": "assistant", "content": "It's indeed sometimes possible that the original `pooler_output` is preferred. The exact format of what's stored in this `pooler_output` also differs. For example, for `get_text_features` it's often `[batch_size, hidden_size]`, i.e. the sequence length dimension is \"pooled\" away, while with `get_image_features` it can be `[batch_size, hidden_size]`, but also e.g. a list of `[num_patches, hidden_size]` for each image.\r\n\r\nThere's still a handful of different formats used in `last_hidden_state`/`pooler_output`/`attentions`/`hidden_states`, depending on the architecture. That makes downstream use still somewhat complicated, but a lot more viable than currently. For `pooler_output` I've continuously chosen for what was already being returned on `main`, and `last_hidden_state`/`attentions`/`hidden_states` are preserved from the relevant modality encoder.\r\n\r\nFor context, my goal is to expose the `last_hidden_state` (and to a lesser extent also the pooled, per-layer hidden states, or attentions) for individual modalities."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42564, "url": "https://github.com/huggingface/transformers/pull/42564#discussion_r2695098911"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `item.py`:\n\n```python\nfrom scrapy.item import BaseItem\n\n\ndef is_item_like(obj):\n    \"\"\"\n    Returns True if *obj* is considered a Scrapy *item*, False otherwise.\n\n    An object is considered an *item* if it is:\n    - a scrapy.item.BaseItem or dict instance (or any subclass)\n    - a dataclass object\n    \"\"\"\n    return isinstance(obj, (BaseItem, dict)) or _is_dataclass_instance(obj)\n\n\ndef _is_dataclass_instance(obj):\n    \"\"\"\n    Returns True if *obj* is a dataclass object, False otherwise.\n    Taken from https://docs.python.org/3/library/dataclasses.html#dataclasses.is_dataclass.\n    \"\"\"\n    try:\n        from dataclasses import is_dataclass\n    except ImportError:\n        return False\n    else:\n        return is_dataclass(obj) and not isinstance(obj, type)\n\n\nclass ItemAdapter:\n```"}, {"role": "assistant", "content": "As for delitem, I think that's fine to raise NonImplementedError - but it seems you already implemented it :)"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 3881, "url": "https://github.com/scrapy/scrapy/pull/3881#discussion_r409523222"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `setup.py`:\n\n```python\nimport sys\n\nfrom setuptools import setup\n```"}, {"role": "assistant", "content": "I guess technically this import isn't required since it can never be used."}], "_meta": {"repo": "encode/httpx", "pr_number": 2351, "url": "https://github.com/encode/httpx/pull/2351#discussion_r958385038"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `__init__.py`:\n\n```python\ndef copy(self) -> \"Request\":\n        return self.replace()\n\n    def replace(self, *args, **kwargs) -> \"Request\":\n    def replace(self, *args: Any, **kwargs: Any) -> \"Request\":\n```"}, {"role": "assistant", "content": "> Without the cls edge case, Self typing looks correct\r\n\r\nYup.\r\n\r\nWe could add an overload to all these methods that covers the case when `cls is not None`, though that requires adding a `cls` named arg explicitly, not sure if that will break anything."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6127, "url": "https://github.com/scrapy/scrapy/pull/6127#discussion_r1388406846"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `configuration_mixtral.py`:\n\n```python\n\"layers.*.self_attn.k_proj\": \"colwise\",\n        \"layers.*.self_attn.v_proj\": \"colwise\",\n        \"layers.*.self_attn.o_proj\": \"rowwise\",\n        \"layers.*.mlp.gate\": \"ep_router\",  # we need to replicate here to correctly route experts\n```"}, {"role": "assistant", "content": "In TP, all ranks have all experts but just sharded weights on each GPU. If  we use`RouterParallel`, we will find ourselves in a case where we mask an expert that is needed (because EP assume we have a full expert in a GPU, not every experts but sharded in all GPUs ), thus missing its output, thus having partial output, thus incorrect `all_reduce` that happens at `local_rowise` in `down_proj`"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42809, "url": "https://github.com/huggingface/transformers/pull/42809#discussion_r2635155696"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `core_model_loading.py`:\n\n```python\nlogger = logging.get_logger(__name__)\n\n\ndef reverse_target_pattern(pattern: str) -> tuple[str, str | None]:\n    \"\"\"\n    Process a target pattern for reverse mapping (when targets become sources).\n\n    This handles several edge cases in checkpoint conversion mappings:\n    - Removes `^` prefix and `$` suffix (start/end of string anchors)\n    - Removes negative lookahead/lookbehind assertions\n    - Detects capturing groups and replaces them with `\\\\1` backreference\n\n    Args:\n        pattern: The target pattern to process for reverse mapping.\n\n    Returns:\n        A tuple of (processed_pattern, captured_group) where captured_group is\n        the original capturing group found (e.g., \"(encoder|decoder)\") or None.\n    \"\"\"\n    # Some mapping contains `^` to notify start of string when matching -> remove it during reverse mapping\n    pattern = pattern.removeprefix(\"^\")\n    # Some mapping contains `$` to notify end of string when matching -> remove it during reverse mapping\n    pattern = pattern.removesuffix(\"$\")\n    # Remove negative lookahead/behind if any. This is ugly but needed for reverse mapping of\n    # Qwen2.5, Sam3, Ernie4.5 VL MoE!\n    pattern = re.sub(r\"\\(\\?.+\\)\", \"\", pattern)\n    # Allow capturing groups in patterns, i.e. to add/remove a prefix to all keys (e.g. timm_wrapper, sam3)\n    capturing_group_match = re.search(r\"\\([^)]+\\)\", pattern)\n    captured_group = None\n    if capturing_group_match:\n        captured_group = capturing_group_match.group(0)\n        pattern = pattern.replace(captured_group, r\"\\1\", 1)\n    return pattern, captured_group\n```"}, {"role": "assistant", "content": "For this refactor, it seems unrelated, or is it doing something I'm missing? Can break it off to another PR if independent of DETR standards for commit history (in the target patterns indeed seems to be bugfixing, but we can do in another PR and merge quickly)"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41549, "url": "https://github.com/huggingface/transformers/pull/41549#discussion_r2685643466"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_su.py`:\n\n```python\nmocker.patch.object(become, 'get_option', return_value=['(invalid regex'])\n\n    assert become.check_password_prompt('(invalid regex:') is True\n    assert become.check_password_prompt('(invalid regex:'.encode()) is True\n```"}, {"role": "assistant", "content": "```suggestion\r\n    assert become.check_password_prompt(b'(invalid regex:') is True\r\n```"}], "_meta": {"repo": "ansible/ansible", "pr_number": 86473, "url": "https://github.com/ansible/ansible/pull/86473#discussion_r2743368038"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `token.py`:\n\n```python\naccess_token: str\n\n\nclass TokenBody(StrictBaseModel):\n    \"\"\"Token serializer for post bodies.\"\"\"\nclass TokenPasswordBody(StrictBaseModel):\n    \"\"\"Password Grant Token serializer for post bodies.\"\"\"\n\n    grant_type: Literal[\"password\"] = \"password\"\n    username: str = Field()\n    password: str = Field()\n\n    def create_token(self, expiration_time_in_seconds: int) -> str:\n        \"\"\"Create token using password grant.\"\"\"\n        return create_token_for(\n            self.username, self.password, expiration_time_in_seconds=expiration_time_in_seconds\n        )\n\n\nclass TokenClientCredentialsBody(StrictBaseModel):\n    \"\"\"Client Credentials Grant Token serializer for post bodies.\"\"\"\n```"}, {"role": "assistant", "content": "```suggestion\r\n    \"\"\"Client credentials grant token serializer for post bodies.\"\"\"\r\n```"}], "_meta": {"repo": "apache/airflow", "pr_number": 59411, "url": "https://github.com/apache/airflow/pull/59411#discussion_r2662285361"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `defaults.py`:\n\n```python\ninherit_parent_priority=Option(False, type='bool'),\n        default_delivery_mode=Option(2, type='string'),\n        default_queue=Option('celery'),\n        default_queue_type=Option('classic', type='string'),\n```"}, {"role": "assistant", "content": "there are many versions of rabbitmq where qourum is not default"}], "_meta": {"repo": "celery/celery", "pr_number": 9121, "url": "https://github.com/celery/celery/pull/9121#discussion_r2516671028"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `dataclasses.py`:\n\n```python\ntypes_namespace = {}\n\n            types_namespace = _typing_extra.merge_cls_and_parent_ns(cls, types_namespace)\n\n        return _pydantic_dataclasses.complete_dataclass(\n            cls,\n            _config.ConfigWrapper(cls.__pydantic_config__, check=False),\n            raise_errors=raise_errors,\n            types_namespace=types_namespace,\n            # We could provide a different config instead (with `'defer_build'` set to `True`)\n            # but this doesn't play well in some cases as unlike Pydantic models, Pydantic dataclasses\n            # can have the config coming from the `config` argument or from the `__pydantic_config__`\n            # property. So we use an explicit argument:\n            _force_build=True,\n```"}, {"role": "assistant", "content": "Hmm, I'd prefer we either disallow configs in both locations, or merge them with priority towards one.\r\n\r\nThat way, we could have one unified config for the model that we can patch with the `defer_build` when necessary."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10313, "url": "https://github.com/pydantic/pydantic/pull/10313#discussion_r1746196047"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `settings.py`:\n\n```python\nsys.path.append(PLUGINS_FOLDER)\n\n\ndef __getattr__(name: str):\n    \"\"\"Handle deprecated module attributes.\"\"\"\n    if name == \"MASK_SECRETS_IN_LOGS\":\n        import warnings\n\n        warnings.warn(\n            \"settings.MASK_SECRETS_IN_LOGS has been removed. \"\n            \"Use SecretsMasker.enable_log_masking(), disable_log_masking(), or is_log_masking_enabled() instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return\n```"}, {"role": "assistant", "content": "Oh my bad, wanted to commit with `return False` instead."}], "_meta": {"repo": "apache/airflow", "pr_number": 55712, "url": "https://github.com/apache/airflow/pull/55712#discussion_r2351586422"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `functional_validators.py`:\n\n```python\n)\n    _V2WrapValidatorType = TypeVar('_V2WrapValidatorType', bound=Union[_V2WrapValidator, _PartialClsOrStaticMethod])\n\nFieldValidatorModes: TypeAlias = Literal['before', 'after', 'wrap', 'plain']\n\n\n@overload\ndef field_validator(\n    field: str,\n    /,\n    *fields: str,\n    mode: Literal['before', 'after', 'plain'] = ...,\n    mode: Literal['wrap'],\n    check_fields: bool | None = ...,\n) -> Callable[[_V2BeforeAfterOrPlainValidatorType], _V2BeforeAfterOrPlainValidatorType]: ...\n    input_type: Any = ...,\n) -> Callable[[_V2WrapValidatorType], _V2WrapValidatorType]: ...\n\n\n@overload\ndef field_validator(\n    field: str,\n    /,\n    *fields: str,\n    mode: Literal['wrap'],\n    mode: Literal['before', 'plain'],\n    check_fields: bool | None = ...,\n) -> Callable[[_V2WrapValidatorType], _V2WrapValidatorType]: ...\n    input_type: Any = ...,\n) -> Callable[[_V2BeforeAfterOrPlainValidatorType], _V2BeforeAfterOrPlainValidatorType]: ...\n\n\nFieldValidatorModes: TypeAlias = Literal['before', 'after', 'wrap', 'plain']\n@overload\ndef field_validator(\n    field: str,\n    /,\n    *fields: str,\n    mode: Literal['after'] = ...,\n    check_fields: bool | None = ...,\n) -> Callable[[_V2BeforeAfterOrPlainValidatorType], _V2BeforeAfterOrPlainValidatorType]: ...\n\n\ndef field_validator(\n```"}, {"role": "assistant", "content": "Changes to this function look great, nice work with the consistent changes to the overloads."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10094, "url": "https://github.com/pydantic/pydantic/pull/10094#discussion_r1719903504"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_param.py`:\n\n```python\ndef test_repr(self):\n        pd = ParamsDict({\"key\": Param(\"value\", type=\"string\")})\n        assert repr(pd) == \"{'key': 'value'}\"\n\n    @pytest.mark.parametrize(\"source\", (\"dag\", \"task\"))\n    def test_fill_missing_param_source(self, source: Literal[\"dag\", \"task\"]):\n        pd = ParamsDict(\n            {\n                \"key\": Param(\"value\", type=\"string\"),\n                \"key2\": \"value2\",\n            }\n        )\n        pd._fill_missing_param_source(source)\n        for param in pd.values():\n            assert param.source == source\n\n    def test_fill_missing_param_source_not_overwrite_existing(self):\n        pd = ParamsDict(\n            {\n                \"key\": Param(\"value\", type=\"string\", source=\"dag\"),\n                \"key2\": \"value2\",\n                \"key3\": \"value3\",\n            }\n        )\n        pd._fill_missing_param_source(\"task\")\n        for key, expected_source in (\n            (\"key\", \"Dag\"),\n            (\"key2\", \"task\"),\n            (\"key3\", \"task\"),\n        ):\n            assert pd.get_param(key).source == expected_source\n\n    def test_filter_params_by_source(self):\n        pd = ParamsDict(\n            {\n                \"key\": Param(\"value\", type=\"string\", source=\"dag\"),\n                \"key2\": Param(\"value\", source=\"task\"),\n            }\n        )\n        assert ParamsDict.filter_params_by_source(pd, \"Dag\") == ParamsDict(\n            {\"key\": Param(\"value\", type=\"string\", source=\"Dag\")},\n```"}, {"role": "assistant", "content": "```suggestion\n        assert ParamsDict.filter_params_by_source(pd, \"dag\") == ParamsDict(\n            {\"key\": Param(\"value\", type=\"string\", source=\"dag\")},\n```"}], "_meta": {"repo": "apache/airflow", "pr_number": 58615, "url": "https://github.com/apache/airflow/pull/58615#discussion_r2567505699"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `pipeline.py`:\n\n```python\ns = s.copy()\n            s['to_upper'] = True\n            return s\n\n    # TODO: need to expand this logic to cover more cases, like datetimes, etc\n```"}, {"role": "assistant", "content": "Yeah we can... I was trying to improve JSON schema for this case  mentioned:\r\n\r\n```\r\nta = TypeAdapter(Annotated[int, validate_as(str).transform(int)])\r\n```"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 9459, "url": "https://github.com/pydantic/pydantic/pull/9459#discussion_r1628436832"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `functools.py`:\n\n```python\nreturn decorating_function\n\ndef _lru_cache_wrapper(user_function, maxsize, typed, _CacheInfo):\n    if not callable(user_function):\n```"}, {"role": "assistant", "content": "I would definitely be a bigger change. if we move it to the caller, we would also have to touch the C code in `_funtoolsmodule.c`:\r\n\r\n```c\r\n    if (!PyCallable_Check(func)) {\r\n        PyErr_SetString(PyExc_TypeError,\r\n                        \"the first argument must be callable\");\r\n        return NULL;\r\n    }\r\n```\r\n\r\nmy goal in this PR is to get the behaviour of the C-version of `_lru_cache_wrapper` and the Python version of the same function into alignment (which they should have been from the start)."}], "_meta": {"repo": "python/cpython", "pr_number": 144031, "url": "https://github.com/python/cpython/pull/144031#discussion_r2712158444"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `types.py`:\n\n```python\n@_dataclasses.dataclass\nclass PathType:\n    path_type: Literal['file', 'dir', 'new']\n    path_type: Literal['file', 'dir', 'new', 'socket']\n```"}, {"role": "assistant", "content": "```suggestion\r\n    if sys.platform != \"win32\":\r\n        path_type: Literal['file', 'dir', 'new', 'socket']\r\n    else:\r\n        path_type: Literal['file', 'dir', 'new']\r\n```"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10378, "url": "https://github.com/pydantic/pydantic/pull/10378#discussion_r1756676644"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `defer.py`:\n\n```python\nbreak\n        except Exception:\n            errback(failure.Failure(), *a, **kw)\n\n\ndef isfuture(o):\n```"}, {"role": "assistant", "content": "I hope this isn't too much bikeshedding, but I wonder if this function shouldn't be underscored, if it's only needed for py35"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4010, "url": "https://github.com/scrapy/scrapy/pull/4010#discussion_r354541682"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_modeling_modernbert.py`:\n\n```python\nf\"Model architecture does not support {attn_implementation}, or setting its attention dynamically\"\n            )\n\n    # Override tests that use from_pretrained to ensure SDPA attention is used instead of FlashAttention\n    def test_save_load(self):\n```"}, {"role": "assistant", "content": "Ah that's a shame, can we clarify a bit more with the comment then? I.e. compatibility with FA (dtype) and SDPA as the default attn implementation goes for FA"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43030, "url": "https://github.com/huggingface/transformers/pull/43030#discussion_r2709122817"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `pqueues.py`:\n\n```python\ndef push(self, request: Request) -> None:\n        slot = self._downloader_interface.get_slot_key(request)\n        # Handle None slot by converting to string\n        slot = str(slot)\n```"}, {"role": "assistant", "content": "Yeah I was not sure how to handle this. Any ideas?"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6940, "url": "https://github.com/scrapy/scrapy/pull/6940#discussion_r2231955176"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `linegen.py`:\n\n```python\n# We could enforce triple quotes at this point.\n            quote = quote_char * quote_len\n            leaf.value = prefix + quote + docstring + quote\n\n            if Preview.long_docstring_quotes_on_newline in self.mode:\n                # Put closing quotes on new line if max line length exceeded\n                last_line_length = len(docstring.splitlines()[-1]) if docstring else 0\n\n                # Make the docstring apart from the closing quotes, which happen below\n                docstring = prefix + quote + docstring\n                if (\n                    len(indent) + len(prefix) + 2 * len(quote) + last_line_length\n                    <= self.mode.line_length\n                ):\n```"}, {"role": "assistant", "content": "Nice \ud83d\udc4d looking at the new code: are you sure that the prefix doesn't affect the calculation similarly? Should we instead only calculate the line length once? Since we only really care about the last one, perhaps something like:\r\n\r\n```python\r\nlines = docstring.splitlines()\r\nlast_line_len = len(lines[-1])\r\nif len(lines) == 1:\r\n    last_line_len += len(indent) + len(prefix) + quote_len\r\n\r\nif last_line_len + quote_len > max_len:\r\n    # add newline\r\n...\r\n```\r\n\r\nI'm not sure how the indentation matters if we strip it only to add it back. Maybe that could simplify it a bit. But I might be disregarding some details."}], "_meta": {"repo": "psf/black", "pr_number": 3044, "url": "https://github.com/psf/black/pull/3044#discussion_r865418930"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `http11.py`:\n\n```python\npool=self._pool,\n                )\n            else:\n                proxyScheme = b'http' if not proxyScheme else proxyScheme\n                proxyHost = to_bytes(proxyHost, encoding='ascii')\n                proxyPort = to_bytes(str(proxyPort), encoding='ascii')\n                proxyURI = urlunparse((proxyScheme, proxyNetloc, proxyParams, '', '', ''))\n                return self._ProxyAgent(\n                    reactor=reactor,\n                    proxyURI=to_bytes(proxy, encoding='ascii'),\n```"}, {"role": "assistant", "content": "Had to construct `proxyURI` and pass it to `_ProxyAgent` since `proxy` is the schemaless url with the incorrect format (not starting with `//`)"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4649, "url": "https://github.com/scrapy/scrapy/pull/4649#discussion_r448190183"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `applications.py`:\n\n```python\n\"\"\"\n            ),\n        ] = True,\n        openapi_external_docs: Annotated[\n            Optional[Dict[str, Any]],\n            Doc(\n                \"\"\"\n                This field allows you to provide additional external documentation links\n                for an API operation or component. It\u2019s an optional field and can be added\n                to an operation or a tag. If provided, it must be a dictionary containing:\n\n                * `description`: A brief description of the external documentation, which may use\n                [CommonMark syntax](https://commonmark.org/) for rich text formatting.\n                * `url`: The URL pointing to the external documentation. The value **MUST**\n                be a valid URL format.\n\n                **Example**:\n\n                ```python\n                from fastapi import FastAPI\n                from typing import Dict, Any\n                from typing import Optional\n                from fastapi.openapi.models import Doc\n                from typing_extensions import Annotated\n\n                external_docs: Annotated[Optional[Dict[str, Any]], Doc()] = {\n                    \"description\": \"Detailed API Reference\",\n                    \"url\": \"https://example.com/api-docs\",\n                }\n\n                app = FastAPI(openapi_external_docs=external_docs)\n\n                ```\n\n                ### Explanation:\n                - `description`: Provides a short description explaining what this external documentation covers.\n                - `url`: Points to the full external documentation.\n\n                **Notes**:\n                - This field is **optional**; if not included, no external documentation will be linked.\n                - You can use this feature to link to full API documentation, tutorials, or any other\n```"}, {"role": "assistant", "content": "```suggestion\r\n```\r\nI think we don't need this. The above explanation should be enough"}], "_meta": {"repo": "fastapi/fastapi", "pr_number": 13713, "url": "https://github.com/fastapi/fastapi/pull/13713#discussion_r2210711862"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_video_processing_qwen3_vl.py`:\n\n```python\nself.video_processor_tester.min_resolution = prev_min_resolution\n            if prev_max_resolution is not None:\n                self.video_processor_tester.max_resolution = prev_max_resolution\n\n    def test_image_input(self):\n        for video_processing_class in self.video_processor_list:\n            video_processor_dict = self.video_processor_dict.copy()\n            video_processor_dict[\"size\"] = {\"longest_edge\": 40960, \"shortest_edge\": 4096}\n            video_processor_dict[\"do_sample_frames\"] = False\n            video_processor_dict[\"temporal_patch_size\"] = 3\n            video_processing = video_processing_class(**video_processor_dict)\n\n            n, w, h = 1, 64, 64\n            video_inputs = [(np.random.randint(0, 256, (h, w, 3), dtype=np.uint8)) for _ in range(n)]\n\n            video_processed = video_processing(video_inputs, return_tensors=\"pt\")\n            encoded_videos = video_processed[self.input_name]\n            self.assertEqual(list(encoded_videos.shape), [16, 2304])\n\n            video_grid_thw = video_processed[\"video_grid_thw\"]\n            self.assertEqual(video_grid_thw.tolist(), [[1, 4, 4]])\n```"}, {"role": "assistant", "content": "might be missing smth, why do we need a test for single frame? The issue solved by this PR is only about temporal padding \r\n\r\nnever mind, just saw another comment above"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42083, "url": "https://github.com/huggingface/transformers/pull/42083#discussion_r2509244745"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `middleware.py`:\n\n```python\nself.methods['close_spider'].appendleft(mw.close_spider)\n\n    def _process_parallel(self, methodname: str, obj, *args) -> Deferred:\n        return process_parallel(self.methods[methodname], obj, *args)\n        methods = cast(Iterable[Callable], self.methods[methodname])\n        return process_parallel(methods, obj, *args)\n\n    def _process_chain(self, methodname: str, obj, *args) -> Deferred:\n        return process_chain(self.methods[methodname], obj, *args)\n        methods = cast(Iterable[Callable], self.methods[methodname])\n        return process_chain(methods, obj, *args)\n\n    def _process_chain_both(self, cb_methodname: str, eb_methodname: str, obj, *args) -> Deferred:\n```"}, {"role": "assistant", "content": "This method is not covered because it's never used. I think we can drop it?"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5334, "url": "https://github.com/scrapy/scrapy/pull/5334#discussion_r758274435"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `param.py`:\n\n```python\nreturn ParamsDict(data)\n\n    def _fill_missing_param_source(\n        self,\n        source: Literal[\"dag\", \"task\"] | None = None,\n    ) -> None:\n        for key in self.__dict:\n            if self.__dict[key].source is None:\n                self.__dict[key].source = source\n\n    @staticmethod\n    def filter_params_by_source(params: ParamsDict, source: Literal[\"Dag\", \"task\"]) -> ParamsDict:\n```"}, {"role": "assistant", "content": "```suggestion\n    def filter_params_by_source(params: ParamsDict, source: Literal[\"dag\", \"task\"]) -> ParamsDict:\n```"}], "_meta": {"repo": "apache/airflow", "pr_number": 58615, "url": "https://github.com/apache/airflow/pull/58615#discussion_r2567503953"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_http_response.py`:\n\n```python\nwith mock.patch(\"json.loads\") as mock_json:\n                for _ in range(2):\n                    json_response.json()\n                mock_json.assert_called_once_with(json_body.decode())\n```"}, {"role": "assistant", "content": "Now,` json.loads()` receives bytes, so there's no need to use `decode()`"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6016, "url": "https://github.com/scrapy/scrapy/pull/6016#discussion_r1297487177"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_gemma3n.py`:\n\n```python\nself.ffw_layer_1 = nn.Linear(self.config.hidden_size, self.config.hidden_size * 4, bias=False)\n        self.ffw_layer_2 = nn.Linear(self.config.hidden_size * 4, self.config.hidden_size, bias=False)\n        self.post_layer_norm = Gemma3nRMSNorm(self.config.hidden_size)\n        self.post_layer_scale = torch.tensor(self.config.conf_residual_weight)\n        self.register_buffer(\"post_layer_scale\", torch.tensor(self.config.conf_residual_weight), persistent=False)\n```"}, {"role": "assistant", "content": "Ha indeed, there is still an explicit tensor casting in the __init__. You can simply remove it and use as a scalar"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43178, "url": "https://github.com/huggingface/transformers/pull/43178#discussion_r2691336080"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `feedexport.py`:\n\n```python\ndeferred_list.append(d)\n        return defer.DeferredList(deferred_list) if deferred_list else None\n\n    def _start_new_batch(self, previous_batch_slot, uri, feed, spider):\n        \"\"\"\n        Redirect the output data stream to a new file.\n        Execute multiple times if 'FEED_STORAGE_BATCH' setting is greater than zero.\n        \"\"\"\n        if previous_batch_slot is not None:\n            previous_batch_slot.exporter.finish_exporting()\n            previous_batch_slot.storage.store(previous_batch_slot.file)\n        storage = self._get_storage(uri)\n        file = storage.open(spider)\n        exporter = self._get_exporter(\n            file=file,\n            format=feed['format'],\n            fields_to_export=feed['fields'],\n            encoding=feed['encoding'],\n            indent=feed['indent']\n        )\n        slot = _FeedSlot(file, exporter, storage, uri, feed['format'], feed['store_empty'])\n        if slot.store_empty:\n            slot.start_exporting()\n        return slot\n\n    def _get_uri_of_partial(self, template_uri, feed, spider):\n```"}, {"role": "assistant", "content": "I guess you shouldn't force the timestamp in the file name.\r\nJust log a warning if `%(time)s` is not in the URI as it will cause errors.\r\nMaybe, we could add a new parameter, like `%(batch_id)s` also"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4434, "url": "https://github.com/scrapy/scrapy/pull/4434#discussion_r394392349"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `peft.py`:\n\n```python\ntoken = adapter_kwargs.pop(\"token\")\n\n        if peft_config is None:\n            new_args = adapter_kwargs.copy()\n            if 'token' in new_args:\n                new_args.pop('token')\n```"}, {"role": "assistant", "content": "oh sorry this code is not meant to be in this merge request (this was just a hack to fix model loading failing on master branch)"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41850, "url": "https://github.com/huggingface/transformers/pull/41850#discussion_r2470425704"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `__init__.py`:\n\n```python\n)\n@click.version_option(\n    version=__version__,\n    message=f\"%(prog)s, %(version)s (compiled: {'yes' if COMPILED else 'no'})\",\n    message=(\n        f\"%(prog)s, %(version)s (compiled: {'yes' if COMPILED else 'no'})\\n\"\n        f\"python, {platform.python_version()} ({platform.python_implementation()})\"\n```"}, {"role": "assistant", "content": "It might look a bit better if we stick the implementation directly to Python.\r\n\r\n```suggestion\r\n        f\"Python ({platform.python_implementation()}) {platform.python_version()}\"\r\n```"}], "_meta": {"repo": "psf/black", "pr_number": 2997, "url": "https://github.com/psf/black/pull/2997#discussion_r843905624"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `sdpa_attention.py`:\n\n```python\n#   - npu is not supported gqa currently\n    if _is_torch_xpu_available:\n        return _is_torch_greater_or_equal_than_2_8 and not isinstance(key, torch.fx.Proxy)\n    if _is_torch_npu_available:\n        return False\n```"}, {"role": "assistant", "content": "Sorry for missing updating comments, have made it correct :)"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41601, "url": "https://github.com/huggingface/transformers/pull/41601#discussion_r2432206810"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_adapters.py`:\n\n```python\nimport requests.adapters\n\n\ndef test_request_url_trims_leading_path_separators():\n    \"\"\"See also https://github.com/psf/requests/issues/6643.\"\"\"\n    a = requests.adapters.HTTPAdapter()\n    assert \"/v:h\" == a.request_url(\"http://127.0.0.1:10000//v:h\", {})\n```"}, {"role": "assistant", "content": "```suggestion\n    p = requests.Request(method=\"GET\", url=\"http://127.0.0.1:10000//v:h\").prepare()\n    assert \"/v:h\" == a.request_url(p, {})\n```"}], "_meta": {"repo": "psf/requests", "pr_number": 6644, "url": "https://github.com/psf/requests/pull/6644#discussion_r1498539954"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `benchmark_config.py`:\n\n```python\ntry:\n        from kernels import get_kernel\n\n        get_kernel(\"kernels-community/flash-attn\")\n        get_kernel(\"kernels-community/flash-attn2\")\n```"}, {"role": "assistant", "content": "No, I mean the `version` kwargs! You are right for the repos URL."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43428, "url": "https://github.com/huggingface/transformers/pull/43428#discussion_r2728178194"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `core_model_loading.py`:\n\n```python\nref = getattr(module_obj, param_name)\n    if ref is None:\n        unexpected_keys.add(target_name)\n        return\n\n    # case where we use local_rowise/colwise\n    is_local_tensor = not getattr(distributed_operation, \"use_dtensor\", True)\n    if distributed_operation is not None:\n        param_value = DTensor.from_local(\n            param_value,\n            distributed_operation.device_mesh,\n            getattr(distributed_operation, \"shard\", Replicate()),\n            run_check=False,\n            shape=ref.size(),\n            stride=ref.stride(),\n        )\n\n    # Remove from missing keys (it's either mismatched, or all good)\n    missing_keys.discard(target_name)\n    if ref is not None and ref.shape != param_value.shape and hf_quantizer is None:\n        mismatch_keys.add((target_name, param_value.shape, ref.shape))\n        return\n\n    # local_rowise/colwise case\n    if is_local_tensor and isinstance(param_value, DTensor):\n        param_value = param_value.to_local()\n\n    if param_name not in module_obj._buffers:\n        param_value = torch.nn.Parameter(param_value, requires_grad=param_value.is_floating_point())\n\n    param_value._is_hf_initialized = True\n\n    setattr(module_obj, param_name, param_value)\n    else:\n        if not isinstance(param_value, torch.nn.Parameter):\n            if distributed_operation is not None:\n                device_mesh = distributed_operation.device_mesh\n                target_device = torch.device(f\"{device_mesh.device_type}:{device_mesh.get_local_rank()}\")\n                param_value = param_value.to(target_device)\n            if param_name not in module_obj._buffers:\n                param_value = torch.nn.Parameter(param_value, requires_grad=param_value.is_floating_point())\n\n        # Remove from missing keys (it's either mismatched, or all good)\n        missing_keys.discard(target_name)\n        # Skip shape check when tensor parallel sharding is applied (shape is intentionally different)\n```"}, {"role": "assistant", "content": "this is not really optimal, we should build / copy the utils or just create a dummy Dtensor with meta device just to let it handle the shape! But we cannot ship without shape checking IMO!"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43157, "url": "https://github.com/huggingface/transformers/pull/43157#discussion_r2681761870"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `asyncio.py`:\n\n```python\nself, hostname: str, ssl_context: ssl.SSLContext, timeout: TimeoutConfig\n    ) -> \"SocketStream\":\n        loop = asyncio.get_event_loop()\n        if not hasattr(loop, \"start_tls\"):  # pragma: no cover\n            raise NotImplementedError(\n                \"asyncio.AbstractEventLoop.start_tls() is only available in Python 3.7+\"\n            )\n\n        stream_reader = asyncio.StreamReader()\n        protocol = asyncio.StreamReaderProtocol(stream_reader)\n        transport = self.stream_writer.transport\n\n        loop_start_tls = loop.start_tls  # type: ignore\n        if hasattr(loop, \"start_tls\"):\n            loop_start_tls = loop.start_tls  # type: ignore\n```"}, {"role": "assistant", "content": "Ah I didn't know about uvloop \ud83d\ude1e\r\n\r\n> That's interesting that mypy knows about features though!\r\n\r\nIt knows about if/else with sys.version_info comparisons but that's all. It's very limited.\r\n\r\n> Would an assert work here?\r\n\r\nI don't think we can assert much about the function at runtime, unfortunately. Could maybe do something with `typing.cast` but probably not worth the effort."}], "_meta": {"repo": "encode/httpx", "pr_number": 521, "url": "https://github.com/encode/httpx/pull/521#discussion_r345404688"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `log.py`:\n\n```python\nfrom airflow.sdk.types import Logger, RuntimeTaskInstanceProtocol as RuntimeTI\n\n\n__all__ = [\"configure_logging\", \"reset_logging\", \"mask_secret\"]\ndef redact(\n    value: Any, name: str | None = None, max_depth: int | None = None, replacement: str = \"***\"\n) -> Any:\n    \"\"\"\n    Redact any secrets found in ``value`` with the given replacement.\n\n    This is a re-export from airflow.sdk._shared.secrets_masker for provider compatibility.\n    \"\"\"\n    from airflow.sdk._shared.secrets_masker import redact as _redact\n\n    return _redact(value, name, max_depth, replacement)\n```"}, {"role": "assistant", "content": "Can we not just re-export? Why do we need the wrapper?\n\n```\nfrom airflow.sdk._shared.secrets_masker import redact as _redact\n```\n\nShould be enough"}], "_meta": {"repo": "apache/airflow", "pr_number": 59688, "url": "https://github.com/apache/airflow/pull/59688#discussion_r2642122108"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `networks.py`:\n\n```python\n@property\n    def defined_constraints(self) -> dict[str, Any]:\n        \"\"\"Fetch a key / value mapping of constraints to values that are not None. Used for core schema updates.\"\"\"\n        return {field.name: getattr(self, field.name) for field in fields(self)}\n        return {field.name: value for field in fields(self) if (value := getattr(self, field.name)) is not None}\n\n    def __get_pydantic_core_schema__(self, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        schema = handler(source)\n\n        if schema['type'] == 'function-wrap':\n            # for function-wrap schemas, url constraints is applied to the inner schema\n            inner = schema['schema']\n\n            if annotated_type := inner['type'] not in ('url', 'multi-host-url'):\n                raise PydanticUserError(\n                    f\"'UrlConstraints' cannot annotate '{annotated_type}'.\", code='invalid-annotated-type'\n                )\n            for constraint_key, constraint_value in self.defined_constraints.items():\n                inner[constraint_key] = constraint_value\n            schema['schema'] = inner\n        else:\n            if annotated_type := schema['type'] not in ('url', 'multi-host-url'):\n                raise PydanticUserError(\n```"}, {"role": "assistant", "content": "Is there a `url_constraints_schema` in core? I don't think so?\r\n\r\nPerhaps this can just go along with https://github.com/pydantic/pydantic/issues/10783..."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10922, "url": "https://github.com/pydantic/pydantic/pull/10922#discussion_r1852414122"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `pool_slots_available_dep.py`:\n\n```python\n)\n            return\n\n        # Check team compatibility\n```"}, {"role": "assistant", "content": "Scheduler/runtime. I also wanted to do it at parse time, but because the Dag file processor does not have access to the DB, it makes it more complicated than it seems. This is also what is happening today if a user specifies a non existing pool in their Dag. Parsing goes fine, it only fails at runtime."}], "_meta": {"repo": "apache/airflow", "pr_number": 61227, "url": "https://github.com/apache/airflow/pull/61227#discussion_r2743373508"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_utils.py`:\n\n```python\nopening the file, but avoids maintaining yet another property flag.\n        \"\"\"\n        class_file = sys.modules[cls.__module__].__file__\n        with open(class_file, \"r\") as f:\n        with open(class_file, \"r\", encoding=\"utf8\") as f:\n```"}, {"role": "assistant", "content": "PR in #43433, would be very nice to include in v5."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42564, "url": "https://github.com/huggingface/transformers/pull/42564#discussion_r2720433705"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_docs.py`:\n\n```python\n@pytest.mark.filterwarnings('ignore:(parse_obj_as|schema_json_of|schema_of) is deprecated.*:DeprecationWarning')\n@pytest.mark.filterwarnings('ignore:This functionality is experimental:UserWarning')\n```"}, {"role": "assistant", "content": "I think this falls into the general \"experimental\" category. How about we make a `PydanticExperimental` warning and document that  ?"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 9459, "url": "https://github.com/pydantic/pydantic/pull/9459#discussion_r1620698457"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `dataproc.py`:\n\n```python\nimport warnings\nfrom collections.abc import MutableSequence, Sequence\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom datetime import datetime, timedelta, timezone\n```"}, {"role": "assistant", "content": "<img width=\"1160\" height=\"418\" alt=\"image\" src=\"https://github.com/user-attachments/assets/521e9c5f-4418-47ee-95cc-f1a398d77f22\" />\r\noriginal traceback"}], "_meta": {"repo": "apache/airflow", "pr_number": 60083, "url": "https://github.com/apache/airflow/pull/60083#discussion_r2661441966"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_utils.py`:\n\n```python\ndef make_inputs_require_grads(module, input, output):\n            output.requires_grad_(True)\n\n        self._require_grads_hook = self.get_input_embeddings().register_forward_hook(make_inputs_require_grads)\n        if self.get_input_embeddings() is not None:\n```"}, {"role": "assistant", "content": "FAILED tests/models/speecht5/test_modeling_speecht5.py::SpeechT5ModelTest::test_peft_gradient_checkpointing_enable_disable - AttributeError: 'NoneType' object has no attribute 'register_forward_hook'\r\nFAILED tests/models/speecht5/test_modeling_speecht5.py::SpeechT5ForSpeechToSpeechTest::test_peft_gradient_checkpointing_enable_disable - AttributeError: 'NoneType' object has no attribute 'register_forward_hook'"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 33194, "url": "https://github.com/huggingface/transformers/pull/33194#discussion_r1756928515"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_decoders.py`:\n\n```python\n\"\"\"\n\n    def __init__(self) -> None:\n        if brotlicffi is None:  # pragma: nocover\n        if brotli is None:  # pragma: nocover\n            raise ImportError(\n                \"Using 'BrotliDecoder', but the 'brotlicffi' library \"\n                \"is not installed.\"\n                \"Using 'BrotliDecoder', but neither of the 'brotlicffi' or 'brotli' \"\n                \"packages have been installed. \"\n                \"Make sure to install httpx using `pip install httpx[brotli]`.\"\n            ) from None\n\n        self.decompressor = brotlicffi.Decompressor()\n        self.decompressor = brotli.Decompressor()\n        self.seen_data = False\n        if hasattr(self.decompressor, \"decompress\"):\n            self._decompress = self.decompressor.decompress\n            # The 'brotlicffi' package.\n            self._decompress = self.decompressor.decompress  # pragma: nocover\n        else:\n            # The 'brotli' package.\n            self._decompress = self.decompressor.process  # pragma: nocover\n```"}, {"role": "assistant", "content": "I think`brotlicffi`  also has a `process` alias for `decompress`:\r\n\r\nhttps://github.com/python-hyper/brotlicffi/blob/68c0431673e6c59549079e6588b043a707262c80/src/brotlicffi/_api.py#L423\r\n\r\nHow about just use `self.decompressor.process`?"}], "_meta": {"repo": "encode/httpx", "pr_number": 1618, "url": "https://github.com/encode/httpx/pull/1618#discussion_r623738358"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_processor_auto.py`:\n\n```python\nself.assertIsInstance(processor, Wav2Vec2Processor)\n\n    def test_processor_from_local_subfolder_from_repo(self):\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n            processor.save_pretrained(f\"{tmpdirname}/processor_subfolder\")\n\n            processor = Wav2Vec2Processor.from_pretrained(tmpdirname, subfolder=\"processor_subfolder\")\n\n        self.assertIsInstance(processor, Wav2Vec2Processor)\n```"}, {"role": "assistant", "content": "just wanted to not depend on diffusers test repos, in case it is changed in the future to suit diffusers' needs :)"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42031, "url": "https://github.com/huggingface/transformers/pull/42031#discussion_r2494363662"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_generate_schema.py`:\n\n```python\n)\n\n    def _unsubstituted_typevar_schema(self, typevar: typing.TypeVar) -> core_schema.CoreSchema:\n        assert isinstance(typevar, typing.TypeVar)\n\n        bound = typevar.__bound__\n        constraints = typevar.__constraints__\n\n        try:\n            typevar_has_default = typevar.has_default()  # type: ignore\n            has_default = typevar.has_default()\n        except AttributeError:\n            # could still have a default if it's an old version of typing_extensions.TypeVar\n            typevar_has_default = getattr(typevar, '__default__', None) is not None\n            # Happens if using `typing.TypeVar` on Python < 3.13\n            pass\n        else:\n            if has_default:\n                return self.generate_schema(getattr(typevar, '__default__', None))\n```"}, {"role": "assistant", "content": "```suggestion\r\n                return self.generate_schema(typevar.__default__)\r\n```"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10789, "url": "https://github.com/pydantic/pydantic/pull/10789#discussion_r1844271589"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `type_adapter.py`:\n\n```python\nraise AttributeError(attribute)\n\n\ndef _annotated_type(type_: Any) -> Any | None:\n    return get_args(type_)[0] if _typing_extra.is_annotated(type_) else None\n```"}, {"role": "assistant", "content": "Generally a cleaner approach than what we had though, so feel free to just resolve this as is."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 8939, "url": "https://github.com/pydantic/pydantic/pull/8939#discussion_r1548440462"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `default_settings.py`:\n\n```python\nUSER_AGENT = f\"Scrapy/{import_module('scrapy').__version__} (+https://scrapy.org)\"\n\nWARN_ON_GENERATOR_RETURN_VALUE = True\n\n\ndef __getattr__(name: str):\n    if name == \"CONCURRENT_REQUESTS_PER_IP\":\n```"}, {"role": "assistant", "content": "While making the messages as specific as possible is usually a good thing, I am not sure if it is necessary here. If the message focuses on stating that setting A is deprecated, and optionally providing guidelines for replacement, provided the traceback points to the right line (and I think that is the case in both cases, and both use stacklevel=2 for that), I think the message does not require further specificity."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6921, "url": "https://github.com/scrapy/scrapy/pull/6921#discussion_r2174849564"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `retry.py`:\n\n```python\nFailed pages are collected on the scraping process and rescheduled at the end,\nonce the spider has finished crawling all regular (non failed) pages.\n\"\"\"\nimport warnings\nfrom logging import Logger, getLogger\nfrom typing import Optional, Union\n\nfrom twisted.internet import defer\nfrom twisted.internet.error import (\n    ConnectError,\n    ConnectionDone,\n    ConnectionLost,\n    ConnectionRefusedError,\n    DNSLookupError,\n    TCPTimedOutError,\n    TimeoutError,\n)\nfrom twisted.web.client import ResponseFailed\n\nfrom scrapy.core.downloader.handlers.http11 import TunnelError\nfrom scrapy.exceptions import NotConfigured\nimport six\n```"}, {"role": "assistant", "content": "We shouldn't use six as our code doesn't support Python 2."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5929, "url": "https://github.com/scrapy/scrapy/pull/5929#discussion_r1188874819"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `configuration_albert.py`:\n\n```python\nclass AlbertConfig(PreTrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`AlbertModel`] or a [`TFAlbertModel`]. It is used\n    This is the configuration class to store the configuration of a [`AlbertModel`] . It is used\n```"}, {"role": "assistant", "content": "```suggestion\r\n    This is the configuration class to store the configuration of a [`AlbertModel`]. It is used\r\n```\r\nSuper big nit, seeing this elsewhere too"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42443, "url": "https://github.com/huggingface/transformers/pull/42443#discussion_r2567905042"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `utils.py`:\n\n```python\nimport pytest\nfrom fastapi._compat import PYDANTIC_V2\nfrom inline_snapshot import Snapshot\n\nneeds_py39 = pytest.mark.skipif(sys.version_info < (3, 9), reason=\"requires python3.9+\")\nneeds_py310 = pytest.mark.skipif(\n    sys.version_info < (3, 10), reason=\"requires python3.10+\"\n)\nneeds_pydanticv2 = pytest.mark.skipif(not PYDANTIC_V2, reason=\"requires Pydantic v2\")\nneeds_pydanticv1 = pytest.mark.skipif(PYDANTIC_V2, reason=\"requires Pydantic v1\")\n\n\ndef pydantic_snapshot(\n    *,\n    v2: Snapshot,\n    v1: Snapshot,\n):\n    \"\"\"\n    this function should be used like:\n```"}, {"role": "assistant", "content": "```suggestion\r\n    This function should be used like this:\r\n```"}], "_meta": {"repo": "fastapi/fastapi", "pr_number": 12534, "url": "https://github.com/fastapi/fastapi/pull/12534#discussion_r2018991952"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_generate_schema.py`:\n\n```python\nbound = typevar.__bound__\n        constraints = typevar.__constraints__\n        typevar_default_value = getattr(typevar, '__default__', None)\n```"}, {"role": "assistant", "content": "Yes, that's why `typing.NoDefault` was introduced. However, this is not an issue anymore with the implementation I suggested below"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10789, "url": "https://github.com/pydantic/pydantic/pull/10789#discussion_r1842549276"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `dynamodb.py`:\n\n```python\nsecret_access_key=aws_secret_access_key\n            )\n\n    @staticmethod\n    def _is_valid_ip(ip):\n```"}, {"role": "assistant", "content": "I wonder if that's the best way to check it.. or if it's worth delaying for \ud83e\udd14\n\nAnyway, should not be merged before v5.4 release. We're in RC2 at the moment."}], "_meta": {"repo": "celery/celery", "pr_number": 8965, "url": "https://github.com/celery/celery/pull/8965#discussion_r1564814298"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `triggerer_job_runner.py`:\n\n```python\nframe = _RequestFrame(id=next(self.id_counter), body=msg.model_dump())\n        bytes = frame.as_bytes()\n\n        async with self._lock:\n        async with self._async_lock:\n```"}, {"role": "assistant", "content": "It exists in the parent class CommsDecoder,  correctly suggested that I should align those, otherwise we would have duplicate lock definitions."}], "_meta": {"repo": "apache/airflow", "pr_number": 60268, "url": "https://github.com/apache/airflow/pull/60268#discussion_r2675051994"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modular_bamba.py`:\n\n```python\n):\n        # Overwritten -- has a unique cache type, `HybridMambaAttentionDynamicCache`\n\n        empty_past_kv = past_key_values is None\n\n        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n        # Exception 1: when passing input_embeds, input_ids may be missing entries\n        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n        # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n        #              (we can't check exception 3 while compiling)\n        if not empty_past_kv:\n            if (\n                inputs_embeds is not None  # Exception 1\n                or cache_position[-1] >= input_ids.shape[1]  # Exception 3\n            ):\n                input_ids = input_ids[:, -cache_position.shape[0] :]\n            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n                input_ids = input_ids[:, cache_position]\n        else:\n        if past_key_values is None:\n            past_key_values = HybridMambaAttentionDynamicCache(\n                self.config, input_ids.shape[0], self.dtype, device=self.device\n            )\n```"}, {"role": "assistant", "content": "special cache classes can't yet be prepared in `generate`, let's keep it as custom code for now. If needed, we can allow these models to have a `self.prepare_model_cache` method in the future and call it from within `generate`"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43280, "url": "https://github.com/huggingface/transformers/pull/43280#discussion_r2689785464"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_lfm2_moe.py`:\n\n```python\n_, token_idx = torch.where(expert_mask[expert_idx])\n            current_state = hidden_states[token_idx]\n            gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n            current_hidden_states = self.act_fn(gate) * up\n            current_hidden_states = gate * up\n```"}, {"role": "assistant", "content": "Yeah went a bit too fast there was a hardcoded silu haha"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42468, "url": "https://github.com/huggingface/transformers/pull/42468#discussion_r2572448049"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `core_model_loading.py`:\n\n```python\ncollected_tensors: dict[str, list[Future]] = field(default_factory=lambda: defaultdict(list), init=False)\n    layer_targets: dict[str, set[str]] = field(default_factory=lambda: defaultdict(set), init=False)\n\n    def __setattr__(self, name: str, value: Any):\n        if name == \"source_patterns\":\n            if isinstance(value, str):\n                value = [value]\n            normalized = []\n            for pattern in value:\n                if r\"\\1\" in pattern:\n                    pattern = pattern.replace(r\"\\1\", r\"(.+)\")\n                normalized.append(pattern)\n            object.__setattr__(self, name, normalized)\n            self._rebuild_compiled_sources()\n            return\n        if name == \"target_patterns\":\n            if isinstance(value, str):\n                value = [value]\n            normalized = []\n            for pattern in value:\n                # Some mapping contains `^` to notify start of string when matching -> remove it during reverse mapping\n                pattern = pattern.removeprefix(\"^\")\n                # Some mapping contains `$` to notify end of string when matching -> remove it during reverse mapping\n                pattern = pattern.removesuffix(\"$\")\n                # Remove negative lookahead/behind if any. This is ugly but needed for reverse mapping of\n                # Qwen2.5, Sam3, Ernie4.5 VL MoE!\n                pattern = re.sub(r\"\\(\\?.+\\)\", \"\", pattern)\n                # Allow capturing groups in patterns, i.e. to add/remove a prefix to all keys (e.g. timm_wrapper, sam3)\n                if r\"(.+)\" in pattern:\n                    pattern = pattern.replace(r\"(.+)\", r\"\\1\")\n                normalized.append(pattern)\n            object.__setattr__(self, name, normalized)\n            return\n    def __setattr__(self, name, value):\n        # We do not allow to re-set the patterns, as they are linked between each other and changing one\n        # without the other can mess-up with the capturing groups/compiled sources\n        if name in (\"source_patterns\", \"target_patterns\") and hasattr(self, name):\n            raise ValueError(f\"Cannot assign to field {name}\")\n```"}, {"role": "assistant", "content": "```suggestion\r\n            raise ValueError(f\"Cannot assign to field {name}, you should create a new instance\")\r\n```"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43585, "url": "https://github.com/huggingface/transformers/pull/43585#discussion_r2741437510"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `reactor.py`:\n\n```python\nreactor_class = load_object(reactor_path)\n    if reactor_class is asyncioreactor.AsyncioSelectorReactor:\n        with suppress(error.ReactorAlreadyInstalledError):\n            policy = asyncio.get_event_loop_policy()\n            if sys.version_info >= (3, 8) and sys.platform == \"win32\":\n```"}, {"role": "assistant", "content": "This condition could now be refactored as\r\n```python\r\nif (\r\n    sys.version_info >= (3, 8)\r\n    and sys.platform == \"win32\"\r\n    and not isinstance(policy, asyncio.WindowsSelectorEventLoopPolicy)\r\n):\r\n```"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5689, "url": "https://github.com/scrapy/scrapy/pull/5689#discussion_r1003554041"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `base_secrets.py`:\n\n```python\n# Re export for compat\nfrom airflow._shared.secrets_backend.base import BaseSecretsBackend as BaseSecretsBackend\n\n# Server side default secrets backend search path used by server components (scheduler, API server)\nDEFAULT_SECRETS_SEARCH_PATH = [\n    \"airflow.secrets.environment_variables.EnvironmentVariablesBackend\",\n    \"airflow.secrets.metastore.MetastoreBackend\",\n]\n```"}, {"role": "assistant", "content": "> Or better a prek hook to check/sync. Comments of that sort are pretty useless - people tend to develop blind spots for comments.\r\n\r\nAgree with this, having prek for checking whether the variables are same or not is pretty robust."}], "_meta": {"repo": "apache/airflow", "pr_number": 59597, "url": "https://github.com/apache/airflow/pull/59597#discussion_r2675296058"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_dataclasses.py`:\n\n```python\nx: Union[X, None]\n\n    assert Y(x={'y': None}).x.y is None\n\n@pytest.mark.parametrize(\"field_fn_unset\", [Field, dataclasses.field])\n@pytest.mark.parametrize(\"field_fn2\", [Field, dataclasses.field])\ndef test_annotated_with_field_default_factory( field_fn_unset, field_fn2) -> None:\n```"}, {"role": "assistant", "content": "Interesting \ud83e\udd14 might be worth a check IMO as `pdm run ruff format pydantic tests docs/plugins` does consider `tests`."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 9971, "url": "https://github.com/pydantic/pydantic/pull/9971#discussion_r1691376289"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `configuration_utils.py`:\n\n```python\ndef __init__(self, **kwargs):\n        # Parameters that control the length of the output\n        self.max_length = kwargs.pop(\"max_length\", 20)\n        self.max_length = kwargs.pop(\"max_length\", None)\n        self.max_new_tokens = kwargs.pop(\"max_new_tokens\", None)\n        self.min_length = kwargs.pop(\"min_length\", 0)\n        self.min_length = kwargs.pop(\"min_length\", None)\n        self.min_new_tokens = kwargs.pop(\"min_new_tokens\", None)\n        self.early_stopping = kwargs.pop(\"early_stopping\", False)\n        self.early_stopping = kwargs.pop(\"early_stopping\", None)\n        self.max_time = kwargs.pop(\"max_time\", None)\n        self.stop_strings = kwargs.pop(\"stop_strings\", None)\n\n        # Parameters that control the generation strategy used\n        self.do_sample = kwargs.pop(\"do_sample\", False)\n```"}, {"role": "assistant", "content": "If it's optional bool, we can still have `None` - would be equal to default `False` then. But I see the reason semantically, it should not be optional and we prefer explicit boolean values. \r\n\r\nShould we update the typing then? I think we mark it as optional for these"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42702, "url": "https://github.com/huggingface/transformers/pull/42702#discussion_r2607219880"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `main.py`:\n\n```python\nmode: Union[Literal[\"json\", \"python\"], str] = \"python\",\n        include: Union[IncEx, None] = None,\n        exclude: Union[IncEx, None] = None,\n        context: Union[Dict[str, Any], None] = None,\n        by_alias: bool = False,\n        context: Union[Any, None] = None,\n        by_alias: Union[bool, None] = None,\n        exclude_unset: bool = False,\n        exclude_defaults: bool = False,\n        exclude_none: bool = False,\n        round_trip: bool = False,\n        warnings: Union[bool, Literal[\"none\", \"warn\", \"error\"]] = True,\n        fallback: Union[Callable[[Any], Any], None] = None,\n        serialize_as_any: bool = False,\n    ) -> Dict[str, Any]:\n        if PYDANTIC_MINOR_VERSION < (2, 11):\n            by_alias = by_alias or False\n```"}, {"role": "assistant", "content": "With this we can handle older Pydantic versions but keep the most recent signature."}], "_meta": {"repo": "tiangolo/sqlmodel", "pr_number": 1340, "url": "https://github.com/fastapi/sqlmodel/pull/1340#discussion_r2061546410"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `asgi.py`:\n\n```python\nimport typing\nfrom typing import Callable, Dict, List, Optional, Tuple\n\nimport httpcore\nimport sniffio\n\nfrom .._content_streams import ByteStream\n\nif typing.TYPE_CHECKING:  # pragma: no cover\n    import asyncio\n    import trio\n```"}, {"role": "assistant", "content": "I tried what you're suggesting but unfortunately run into an error like:\r\n```\r\nerror: Cannot assign multiple types to name \"Event\" without an explicit \"Type[...]\" annotation\r\n```\r\n\r\nSeems like type aliases need to be statically resolvable: https://mypy.readthedocs.io/en/stable/common_issues.html#variables-vs-type-aliases"}], "_meta": {"repo": "encode/httpx", "pr_number": 919, "url": "https://github.com/encode/httpx/pull/919#discussion_r419425612"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `text_generation.py`:\n\n```python\npreprocess_params[\"prefix\"] = prefix\n        if prefix:\n            prefix_inputs = self.tokenizer(\n                prefix, padding=False, add_special_tokens=add_special_tokens, return_tensors=self.framework\n                prefix, padding=False, add_special_tokens=add_special_tokens, return_tensors=\"pt\"\n```"}, {"role": "assistant", "content": "i understand why you wanted it to return pt by default/ We could also have a \"bool\" return_tensors=True to return pt tensors"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 40760, "url": "https://github.com/huggingface/transformers/pull/40760#discussion_r2358251270"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_pipeline_images.py`:\n\n```python\nreturn 'thumbsup/%s/%s.jpg' % (thumb_id, thumb_guid)\n\n\nclass ImagesPipelineTestCaseFields(unittest.TestCase):\nclass ImagesPipelineTestCaseFieldsMixin:\n\n    def test_item_fields_default(self):\n        class TestItem(Item):\n            name = Field()\n            image_urls = Field()\n            images = Field()\n\n        for cls in TestItem, dict:\n            url = 'http://www.example.com/images/1.jpg'\n            item = cls({'name': 'item1', 'image_urls': [url]})\n            pipeline = ImagesPipeline.from_settings(Settings({'IMAGES_STORE': 's3://example/images/'}))\n            requests = list(pipeline.get_media_requests(item, None))\n            self.assertEqual(requests[0].url, url)\n            results = [(True, {'url': url})]\n            pipeline.item_completed(results, item, None)\n            self.assertEqual(item['images'], [results[0][1]])\n        url = 'http://www.example.com/images/1.jpg'\n        item = self.item_class(name='item1', image_urls=[url])\n```"}, {"role": "assistant", "content": "Could you please add a dataclass example to media-pipeline.rst as well (see \"Usage example\" - it shows how to add a field for Item)?"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 3881, "url": "https://github.com/scrapy/scrapy/pull/3881#discussion_r370316072"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `__init__.py`:\n\n```python\nTo translate a cURL command into a Scrapy request,\n        you may use `curl2scrapy <https://michael-shub.github.io/curl2scrapy/>`_.\n\n       \"\"\"\n        \"\"\"\n        request_kwargs = curl_to_request_kwargs(curl_command, ignore_unknown_options)\n        request_kwargs.update(kwargs)\n        return cls(**request_kwargs)\n\n    def to_dict(self, *, spider: Optional[\"scrapy.Spider\"] = None) -> dict:\n        \"\"\"Return a dictionary containing the Request's data.\n\n        Use :func:`~scrapy.utils.request.request_from_dict` to convert back into a :class:`~scrapy.Request` object.\n\n        If a spider is given, this method will try to find out the name of the spider method used\n        as callback and include it in the output dict, raising an exception if it cannot be found.\n        \"\"\"\n        d = {\n            \"url\": to_unicode(self.url),  # urls are safe (safe_string_url)\n```"}, {"role": "assistant", "content": "btw, it seems to_unicode is redundant now, becase self._set_url raises an error if url is bytes."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5130, "url": "https://github.com/scrapy/scrapy/pull/5130#discussion_r628912231"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_utils.py`:\n\n```python\ndef is_ipv4_hostname(hostname: str) -> bool:\n    try:\n        ipaddress.IPv4Address(hostname.split(\"/\")[0])\n    except:\n    except ValueError:\n```"}, {"role": "assistant", "content": "Yeah, and the only reason we allowed this was because of the ruff issue. Flake8 would have caught this too."}], "_meta": {"repo": "encode/httpx", "pr_number": 2680, "url": "https://github.com/encode/httpx/pull/2680#discussion_r1176767295"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `fuzz.py`:\n\n```python\nSeed = NewType(\"Seed\", int)\nExitCode = NewType(\"ExitCode\", int)\n\nTY_TARGET_PLATFORM: Final = \"linux\"\n\nwith Path(__file__).parent.parent.parent.joinpath(\"pyproject.toml\").open(\"rb\") as f:\n```"}, {"role": "assistant", "content": "> Fair enough, although it is somewhat confusing because ty doesn't officially support Python 3.7. I think we only support 3.8 or newer\r\n\r\nHrm, it's listed as a supported version in our `--help` message?\r\n\r\n<img width=\"3802\" height=\"448\" alt=\"image\" src=\"https://github.com/user-attachments/assets/6bb3cf30-e38b-4c94-8eb8-2825a855f20f\" />"}], "_meta": {"repo": "astral-sh/ruff", "pr_number": 21844, "url": "https://github.com/astral-sh/ruff/pull/21844#discussion_r2598784431"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_rwkv.py`:\n\n```python\ndef set_output_embeddings(self, new_embeddings):\n        self.head = new_embeddings\n\n    def prepare_inputs_for_generation(self, input_ids, state=None, inputs_embeds=None, use_cache=None, **kwargs):\n        # Overwritten -- this model uses `state`, but doesn't have a cache (`past_key_values`)\n\n        # only last token for inputs_ids if the state is passed along.\n        if state is not None:\n            input_ids = input_ids[:, -1].unsqueeze(-1)\n\n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and state is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            model_inputs = {\"input_ids\": input_ids}\n\n        model_inputs[\"state\"] = state\n```"}, {"role": "assistant", "content": "any arg which is non-common will be simply fast-forwarded in `model_kwargs` as is, no need to overwrite anything"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43280, "url": "https://github.com/huggingface/transformers/pull/43280#discussion_r2689788367"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_modeling_common.py`:\n\n```python\nmodel = model_class(copy.deepcopy(config))\n            self.assertFalse(model.is_gradient_checkpointing)\n\n            # Gradient checkpointing is implemented via GradientCheckpointingLayer, if none is present this is likely\n            # an implementation issue. Note we exclude xlstm and zamba* for now since they are still not using\n            # GradientCheckpointingLayer.\n            if config.model_type not in [\n                \"xlstm\",\n                \"zamba\",\n                \"zamba2\",\n                \"swiftformer\",\n                \"janus_vqgan\",\n                \"clvp_encoder\",\n                \"clvp_decoder\",\n            ]:\n                self.assertTrue([m for m in model.modules() if isinstance(m, GradientCheckpointingLayer)])\n```"}, {"role": "assistant", "content": "Like discussed, let's fix these within this PR directly. Seems like these were more like unintentional regressions due to other big PRs"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41818, "url": "https://github.com/huggingface/transformers/pull/41818#discussion_r2477467594"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_spider.py`:\n\n```python\nassert \"Error while reading start items and requests\" in str(log)\n        assert \"did you miss an 's'?\" in str(log)\n\n    def test_parse_response_use(self):\n        class _CrawlSpider(CrawlSpider):\n            name = \"test\"\n            start_urls = \"https://www.example.com\"\n            _follow_links = False\n\n        with warnings.catch_warnings(record=True) as w:\n            spider = _CrawlSpider()\n            assert len(w) == 0\n            spider._parse_response(\n                TextResponse(spider.start_urls, body=b\"\"), None, None\n            )\n            print(len(w))\n```"}, {"role": "assistant", "content": "I somehow missed those after I figured out what was causing the tests to fail on my machine. I'll add a commit that removes those in a bit"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6804, "url": "https://github.com/scrapy/scrapy/pull/6804#discussion_r2100898765"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `__init__.py`:\n\n```python\n# Scrapy and Twisted versions\n__version__ = (pkgutil.get_data(__package__, \"VERSION\") or b\"\").decode(\"ascii\").strip()\nversion_info = tuple(int(v) if v.isdigit() else v for v in __version__.split(\".\"))\ntwisted_version = (_txv.major, _txv.minor, _txv.micro)\n\n\n# Ignore noisy twisted deprecation warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"twisted\")\n```"}, {"role": "assistant", "content": "I get this with or without this line:\r\n```\r\n>>> from scrapy import twisted_version\r\n[...]/scrapy/scrapy/__init__.py:43: ScrapyDeprecationWarning: The scrapy.twisted_version attribute is deprecated, use twisted.version instead\r\n  warnings.warn(\r\n>>> print(twisted_version)\r\n(24, 7, 0)\r\n```"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6512, "url": "https://github.com/scrapy/scrapy/pull/6512#discussion_r1820972881"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `__init__.py`:\n\n```python\nclass.\n    \"\"\"\n\n    name = None\n    custom_settings = None\n    name: Optional[str] = None\n```"}, {"role": "assistant", "content": ":facepalm:\r\n\r\nServes me right for reviewing draft pull requests."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4691, "url": "https://github.com/scrapy/scrapy/pull/4691#discussion_r461793350"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `configuration_granitemoehybrid.py`:\n\n```python\nallow the model to output the auxiliary loss.\n        router_aux_loss_coef (`float`, *optional*, defaults to 0.001): router auxiliary loss coefficient\n        shared_intermediate_size (`int`, *optional*, defaults to 1024): intermediate size for shared experts.\n        position_embedding_type (`str`, *optional*): Positional embedding\n            type to be used; defaults to None. Allowed options: `[None, \"rope\"]`\n```"}, {"role": "assistant", "content": "super nit: can we put the whole arg description in second line"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42872, "url": "https://github.com/huggingface/transformers/pull/42872#discussion_r2619560439"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_discriminated_union.py`:\n\n```python\nclass LeafState(BaseModel):\n        state_type: Literal['leaf']\n\n    AnyState = Annotated[Union[NestedState, LoopState, LeafState], Field(..., discriminator='state_type')]\n    AnyState = Annotated[Union[NestedState, LoopState, LeafState], Field(discriminator='state_type')]\n    adapter = TypeAdapter(AnyState)\n\n    assert adapter.core_schema['schema']['type'] == 'tagged-union'\n    for definition in adapter.core_schema['definitions']:\n        if definition['schema']['model_name'] in ['NestedState', 'LoopState']:\n            assert definition['schema']['fields']['substate']['schema']['schema']['type'] == 'tagged-union'\n            assert definition['schema']['fields']['substate']['schema']['type'] == 'tagged-union'\n```"}, {"role": "assistant", "content": "As a consequence of the fixed bug: a slight change in schemas because `AnyState` is no longer considered to have a default value of `...`."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10661, "url": "https://github.com/pydantic/pydantic/pull/10661#discussion_r1806858824"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `project.py`:\n\n```python\ndef inside_project():\n    scrapy_module = os.environ.get(\"SCRAPY_SETTINGS_MODULE\")\n    if scrapy_module is not None:\n    scrapy_module = os.environ.get(ENVVAR)\n    if scrapy_module:\n```"}, {"role": "assistant", "content": "This is a change in behavior when the `SCRAPY_SETTINGS_MODULE` is set to an empty string. That said, an empty string cannot work here, so I guess it is fine and slightly improves readability, so +1 from me."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5965, "url": "https://github.com/scrapy/scrapy/pull/5965#discussion_r1252730312"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_config.py`:\n\n```python\nstacklevel=2,\n            )\n\n        if (populate_by_name := config.get('populate_by_name')) is not None:\n            # We include this patch for backwards compatibility purposes, but this config setting will be officially removed in v3.0.\n```"}, {"role": "assistant", "content": "```suggestion\r\n            # We include this patch for backwards compatibility purposes, but this config setting will be deprecated in v3.0, and likely removed in v4.0.\r\n```"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 11468, "url": "https://github.com/pydantic/pydantic/pull/11468#discussion_r1971333294"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_commands.py`:\n\n```python\n@contextmanager\n    def _create_file(self, content, name=None):\n        tmpdir = self.mktemp()\n        tmpdir = self.mkstemp()\n        os.mkdir(tmpdir)\n```"}, {"role": "assistant", "content": "Is https://github.com/scrapy/scrapy/pull/5285/commits/d82fcb0dc4f988a3a1a6e7cabc9e9c7390ea746b what you're thinking?"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5285, "url": "https://github.com/scrapy/scrapy/pull/5285#discussion_r733058999"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `addons.py`:\n\n```python\n},\n            extra={\"crawler\": self.crawler},\n        )\n\n    @classmethod\n    def load_early_settings(cls, settings: BaseSettings):\n        \"\"\"Update early settings that do not require a crawler instance, such as SPIDER_MODULES.\n\n        Similar to the load_settings method, this loads each add-on configured in the\n        ``ADDONS`` setting and calls their 'update_early_settings' class method if present.\n        This method doesn't have access to the crawler instance or the addons list.\n\n        :param settings: The :class:`~scrapy.settings.BaseSettings` object from \\\n            which to read the early add-on configuration\n```"}, {"role": "assistant", "content": "```suggestion\r\n            which to read the early add-on configuration\r\n        :type settings: :class:`~scrapy.settings.Settings`\r\n```"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6568, "url": "https://github.com/scrapy/scrapy/pull/6568#discussion_r1920492890"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `config.py`:\n\n```python\nif mode not in (\"DEFAULT\", \"CONNECT_ONLY\", \"TUNNEL_ONLY\"):\n            raise ValueError(f\"Unknown proxy mode {mode!r}\")\n\n        if url.username or url.password:\n            headers.setdefault(\n                \"Proxy-Authorization\",\n                self.build_auth_header(url.username, url.password),\n            )\n            # Remove userinfo from the URL authority, e.g.:\n            # 'username:password@proxy_host:proxy_port' -> 'proxy_host:proxy_port'\n            credentials, _, authority = url.authority.rpartition(\"@\")\n            url = url.copy_with(authority=authority)\n```"}, {"role": "assistant", "content": "Perhaps replace these two lines with `ul = url.copy_with(username=None, password=None)`?"}], "_meta": {"repo": "encode/httpx", "pr_number": 780, "url": "https://github.com/encode/httpx/pull/780#discussion_r368442692"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `bench.py`:\n\n```python\nbaseurl = \"http://localhost:8998\"\n    link_extractor = LinkExtractor()\n\n    def start_requests(self) -> Iterable[Request]:\n    async def start(self) -> AsyncIterator[Any]:\n```"}, {"role": "assistant", "content": "I looked into the differences between `AsyncIterable` and `AsyncIterator`, and ended up going with the later. Asynchronous generators are both, but in the end we need an iterator, and an iterable is simply an object that can provide an iterator, so asking for the iterator directly seems simpler."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6729, "url": "https://github.com/scrapy/scrapy/pull/6729#discussion_r2016375104"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_manager.py`:\n\n```python\nassert dag1_path not in manager._callback_to_execute\n            assert dag2_path not in manager._callback_to_execute\n\n    @mock.patch(\"airflow.dag_processing.manager.DagBundlesManager\")\n    def test_add_callback_initializes_versioned_bundle(self, mock_bundle_manager):\n```"}, {"role": "assistant", "content": "This test verifies that `bundle.initialize()` is called for versioned bundles, but it doesn't cover the exception handling path where initialization fails. The implementation logs the exception and skips the callback. Add a test case where `bundle.initialize()` raises an exception to verify that the error is logged and the callback is not queued."}], "_meta": {"repo": "apache/airflow", "pr_number": 60734, "url": "https://github.com/apache/airflow/pull/60734#discussion_r2739600737"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_known_annotated_metadata.py`:\n\n```python\n#  else, apply a function after validator to the schema to enforce the corresponding constraint\n        if constraint in chain_schema_constraints:\n            chain_schema_steps.append(cs.str_schema(**{constraint: value}))\n\n            def _apply_constraint_with_incompatibility_info(\n                value: Any, handler: cs.ValidatorFunctionWrapHandler\n            ) -> Any:\n                try:\n                    x = handler(value)\n                except ValidationError as ve:\n                    if 'type' in ve.errors()[0]['type']:\n                        raise TypeError(\n                            f\"Unable to apply constraint '{constraint}' to supplied value {value} for schema of type '{schema_type}'\"  # noqa: B023\n                        )\n                    raise ve\n                return x\n```"}, {"role": "assistant", "content": "Maybe add an example here in a comment? It's not clear to me what the goal is just looking at hte code"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10158, "url": "https://github.com/pydantic/pydantic/pull/10158#discussion_r1720006143"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `cache.py`:\n\n```python\nallocated_blocks = []\n        for b in range(len(prompt_ids) // self.block_size):\n            tokens = prompt_ids[b * self.block_size : (b + 1) * self.block_size]\n            current_hash = self._block_manager.compute_hash(current_hash, tokens)\n            # Prefix sharing is only supported when there is only one full attention layer group, so group_id=0.\n```"}, {"role": "assistant", "content": "Yes, because prefix sharing is still only activated if there is only one group -- cf comment above"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42877, "url": "https://github.com/huggingface/transformers/pull/42877#discussion_r2627236048"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_models.py`:\n\n```python\nfor header in headers:\n            self[header] = headers[header]\n\n    def copy(self) -> \"Headers\":\n        return Headers(self.items(), encoding=self.encoding)\n```"}, {"role": "assistant", "content": "I'd prefer to keep it altogether, I feel a lot of our users to header manipulation and rely on this method."}], "_meta": {"repo": "encode/httpx", "pr_number": 1003, "url": "https://github.com/encode/httpx/pull/1003#discussion_r431708729"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_test_multiprocessing.py`:\n\n```python\n\"resource_tracker: There appear to be 1 leaked \"\n                    \"shared_memory objects to clean up at shutdown\", err)\n\n    @unittest.skipIf(os.name != \"posix\", \"resource_tracker is posix only\")\n    def test_shared_memory_untracking(self):\n        # gh-82300: When a separate Python process accesses shared memory\n        # with track=False, it must not cause the memory to be deleted\n        # when terminating. When accessing with track=True, it must be\n        # deleted.\n        cmd = '''if 1:\n            import sys\n            from multiprocessing.shared_memory import SharedMemory\n            mem = SharedMemory(create=False, name=sys.argv[1], track=False)\n            mem.close()\n        '''\n        mem = shared_memory.SharedMemory(create=True, size=10)\n        # The resource tracker shares pipes with the subprocess, and so\n        # err existing means that the tracker process has terminated now.\n        try:\n            rc, out, err = script_helper.assert_python_ok(\"-c\", cmd, mem.name)\n            self.assertEqual(rc, 0)\n            mem2 = shared_memory.SharedMemory(create=False, name=mem.name)\n```"}, {"role": "assistant", "content": "it might seem a bit redundant but adding a `self.assertNotIn(b\"resource_tracker\", err)` here also makes sense from the point of view of identifying the specific way something could have gone wrong if the implementation regressed."}], "_meta": {"repo": "python/cpython", "pr_number": 110778, "url": "https://github.com/python/cpython/pull/110778#discussion_r1411716249"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `stream.py`:\n\n```python\nerrors.insert(0, InactiveStreamClosed(self._request))\n            self._deferred_response.errback(ResponseFailed(errors))\n\n        elif reason is StreamCloseReason.INVALID_HOSTNAME:\n        else:\n            assert reason is StreamCloseReason.INVALID_HOSTNAME\n```"}, {"role": "assistant", "content": "Fair enough, thanks for the explanation :slightly_smiling_face:"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4769, "url": "https://github.com/scrapy/scrapy/pull/4769#discussion_r582158624"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `json_schema.py`:\n\n```python\nif root_json_schema_extra:\n                json_schema_extra = root_json_schema_extra\n\n        json_schema = self._update_class_schema(json_schema, title, config.get('extra', None), cls, json_schema_extra)\n\n        return json_schema\n\n    def _update_class_schema(\n        self,\n        json_schema: JsonSchemaValue,\n        title: str | None,\n        extra: Literal['allow', 'ignore', 'forbid'] | None,\n        cls: type[Any],\n        json_schema_extra: JsonDict | JsonSchemaExtraCallable | None,\n    ) -> JsonSchemaValue:\n        if '$ref' in json_schema:\n            schema_to_update = self.get_schema_from_definitions(JsonRef(json_schema['$ref'])) or json_schema\n        else:\n            schema_to_update = json_schema\n\n        if title is not None:\n            # referenced_schema['title'] = title\n            schema_to_update.setdefault('title', title)\n\n        if 'additionalProperties' not in schema_to_update:\n            if extra == 'allow':\n                schema_to_update['additionalProperties'] = True\n            elif extra == 'forbid':\n                schema_to_update['additionalProperties'] = False\n\n        if isinstance(json_schema_extra, (staticmethod, classmethod)):\n            # In older versions of python, this is necessary to ensure staticmethod/classmethods are callable\n            json_schema_extra = json_schema_extra.__get__(cls)\n```"}, {"role": "assistant", "content": "We've discussed - just going to keep the type ignores here."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10675, "url": "https://github.com/pydantic/pydantic/pull/10675#discussion_r1819206113"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_downloader_handlers.py`:\n\n```python\nimport tempfile\nfrom typing import Optional, Type\nfrom unittest import mock\nfrom tempfile import mkdtemp, mkstemp\n```"}, {"role": "assistant", "content": ":lipstick: A-Z\r\n\r\n```suggestion\r\nimport tempfile\r\nfrom tempfile import mkdtemp, mkstemp\r\nfrom typing import Optional, Type\r\nfrom unittest import mock\r\n```\r\n\r\nYou could also take the chance to see where the `tempfile` module is used directly, maybe avoid that usage by extending `from tempfile import` as needed."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5285, "url": "https://github.com/scrapy/scrapy/pull/5285#discussion_r733069705"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `networks.py`:\n\n```python\n@property\n    def defined_constraints(self) -> dict[str, Any]:\n        \"\"\"Fetch a key / value mapping of constraints to values that are not None. Used for core schema updates.\"\"\"\n        return {field.name: getattr(self, field.name) for field in fields(self)}\n        return {field.name: value for field in fields(self) if (value := getattr(self, field.name)) is not None}\n\n    def __get_pydantic_core_schema__(self, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        schema = handler(source)\n\n        if schema['type'] == 'function-wrap':\n            # for function-wrap schemas, url constraints is applied to the inner schema\n            inner = schema['schema']\n\n            if annotated_type := inner['type'] not in ('url', 'multi-host-url'):\n                raise PydanticUserError(\n```"}, {"role": "assistant", "content": "Why not? It seems to me that 112-113 is repeated with 120-121 with the only difference being `inner` vs `schema`."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10922, "url": "https://github.com/pydantic/pydantic/pull/10922#discussion_r1852399907"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_utils.py`:\n\n```python\n# Reset the logger so we don't have verbose output in all unit tests\n    logging.getLogger(\"httpx\").handlers = []\n\n\ndef test_get_ssl_cert_file():\n```"}, {"role": "assistant", "content": "I'd say let\u2019s add tests at the top here for the cases when `SSL_CERT_FILE` and `SSL_CERT_DIR` are not set. Ideally we should be able to catch the potential bugs I mention in my other comment. :)"}], "_meta": {"repo": "encode/httpx", "pr_number": 307, "url": "https://github.com/encode/httpx/pull/307#discussion_r323460444"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `parsing.py`:\n\n```python\nreturn code\n\n\ndef parse_ast(src: str) -> Union[ast.AST, ast3.AST, ast27.AST]:\ndef parse_single_version(\n    src: str, version: Tuple[int, int]\n) -> Union[ast.AST, ast3.AST, ast27.AST]:\n    filename = \"<unknown>\"\n    if (3, 8) <= version < (4, 0):\n        return ast.parse(src, filename, feature_version=version)\n    elif version >= (3, 6):\n```"}, {"role": "assistant", "content": "Also, target versions include Pythons all the way from 3.3. Should they be here (and in the version list below) too?"}], "_meta": {"repo": "psf/black", "pr_number": 2304, "url": "https://github.com/psf/black/pull/2304#discussion_r645104182"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_model_construction.py`:\n\n```python\n_make_forward_ref,\n    eval_type_backport,\n    is_annotated,\n    is_classvar,\n    is_classvar_annotation,\n```"}, {"role": "assistant", "content": "We would have to change all the `from ._x import ...` in this file to be consistent, so I left it as is"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10725, "url": "https://github.com/pydantic/pydantic/pull/10725#discussion_r1832977137"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `bitsandbytes.py`:\n\n```python\nlogger = logging.get_logger(__name__)\n\nfrom ..core_model_loading import ConversionOps\n\nclass Bnb4bitQuantize(ConversionOps):\n    def convert(self, input_dict: torch.Tensor, model: Optional[torch.nn.Module] = None, **kwargs) -> dict[str, torch.Tensor]:\n```"}, {"role": "assistant", "content": "yes but it will be better to ask users to do model.dequantize()"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42043, "url": "https://github.com/huggingface/transformers/pull/42043#discussion_r2500052010"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_json_schema.py`:\n\n```python\n'ser_json_timedelta,properties',\n    [\n        ('float', {'duration': {'default': 300.0, 'title': 'Duration', 'type': 'number'}}),\n        ('seconds_float', {'duration': {'default': 300.0, 'title': 'Duration', 'type': 'number'}}),\n        ('milliseconds_float', {'duration': {'default': 300000.0, 'title': 'Duration', 'type': 'number'}}),\n        ('iso8601', {'duration': {'default': 'PT5M', 'format': 'duration', 'title': 'Duration', 'type': 'string'}}),\n    ],\n)\ndef test_model_default_timedelta(ser_json_timedelta: Literal['float', 'iso8601'], properties: typing.Dict[str, Any]):\n    class Model(BaseModel):\n        model_config = ConfigDict(ser_json_timedelta=ser_json_timedelta)\ndef test_model_default_timedelta(\n```"}, {"role": "assistant", "content": "Yeah, I think having their own tests would be good so that we can avoid the conditional warning checks."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10293, "url": "https://github.com/pydantic/pydantic/pull/10293#discussion_r1765154229"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `compat.py`:\n\n```python\ncompatibility until the next major version.\n\"\"\"\n\ntry:\n    import chardet\nexcept ImportError:\n    import charset_normalizer as chardet\n\nimport importlib\nimport sys\n\n# -------------------\n# Character Detection\n# -------------------\n\n\ndef _resolve_char_detection():\n    \"\"\"Find supported character detection libraries.\"\"\"\n    chardet = None\n    for lib in (\"chardet\", \"charset_normalizer\"):\n```"}, {"role": "assistant", "content": "While this keeps the logic of the earlier code, I think that the library which is required on should be tried first. https://github.com/psf/requests/pull/6714 addresses this."}], "_meta": {"repo": "psf/requests", "pr_number": 6702, "url": "https://github.com/psf/requests/pull/6702#discussion_r1609492798"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `processing_instructblip.py`:\n\n```python\nNumber of tokens used by the Qformer as queries, should be same as in model's config.\n    \"\"\"\n\n    attributes = [\"image_processor\", \"tokenizer\", \"qformer_tokenizer\"]\n    image_processor_class = (\"BlipImageProcessor\", \"BlipImageProcessorFast\")\n    tokenizer_class = \"AutoTokenizer\"\n    qformer_tokenizer_class = \"AutoTokenizer\"\n```"}, {"role": "assistant", "content": "do we need `qformer_tokenizer_class`? In the InstrutcBlipVideo I can see it is deleted"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41633, "url": "https://github.com/huggingface/transformers/pull/41633#discussion_r2435184034"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `pattern_matching_extras.py`:\n\n```python\nmatch match:\n    case case:\n        pass\n\n\nmatch a, *b(), c:\n```"}, {"role": "assistant", "content": "That is not a valid syntax.\r\n```py\r\n>>> match *a:\r\n  File \"<stdin>\", line 1\r\n    match *a:\r\n            ^\r\nSyntaxError: invalid syntax\r\n```\r\nFor one regular starred and one normal mixed, see the same file. We have an example for that."}], "_meta": {"repo": "psf/black", "pr_number": 2667, "url": "https://github.com/psf/black/pull/2667#discussion_r761985486"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `crypto.py`:\n\n```python\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nfrom __future__ import annotations\n\nimport logging\nfrom functools import cache\nfrom typing import Protocol\n\nlog = logging.getLogger(__name__)\n\n\nclass FernetProtocol(Protocol):\n    \"\"\"This class is only used for TypeChecking (for IDEs, mypy, etc).\"\"\"\n\n    is_encrypted: bool\n\n    def decrypt(self, msg: bytes | str, ttl: int | None = None) -> bytes:\n        \"\"\"Decrypt with Fernet.\"\"\"\n        ...\n\n    def encrypt(self, msg: bytes) -> bytes:\n        \"\"\"Encrypt with Fernet.\"\"\"\n        ...\n\n\nclass _NullFernet:\n    \"\"\"\n    A \"Null\" encryptor class that doesn't encrypt or decrypt but that presents a similar interface to Fernet.\n\n    The purpose of this is to make the rest of the code not have to know the\n    difference, and to only display the message once, not 20 times when\n    `airflow db migrate` is run.\n    \"\"\"\n\n    is_encrypted = False\n\n    def decrypt(self, msg: bytes | str, ttl: int | None = None) -> bytes:\n        \"\"\"Decrypt with Fernet.\"\"\"\n        if isinstance(msg, bytes):\n            return msg\n        if isinstance(msg, str):\n            return msg.encode(\"utf-8\")\n        raise ValueError(f\"Expected bytes or str, got {type(msg)}\")\n\n    def encrypt(self, msg: bytes) -> bytes:\n        \"\"\"Encrypt with Fernet.\"\"\"\n        return msg\n\n\nclass _RealFernet:\n    \"\"\"\n    A wrapper around the real Fernet to set is_encrypted to True.\n\n    This class is only used internally to avoid changing the interface of\n    the get_fernet function.\n    \"\"\"\n```"}, {"role": "assistant", "content": "ACK, removing `Fernet` but `MultiFernet` is used for typing so moving it up in a type checking block"}], "_meta": {"repo": "apache/airflow", "pr_number": 60771, "url": "https://github.com/apache/airflow/pull/60771#discussion_r2706932516"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `trainer.py`:\n\n```python\nreturn model\n            jit_inputs = []\n            example_batch = next(iter(dataloader))\n            if \"labels\" in example_batch:\n                if self.label_smoother is not None:\n                    example_batch.pop(\"labels\")\n                elif version.parse(version.parse(torch.__version__).base_version) < version.parse(\"1.14.0\"):\n                    logger.warning(\n                        \"fail to use PyTorch jit mode due to trace could not handle `labels` input. you could use\"\n                        \" --label_smoothing_factor 0.1 for WA\"\n                    )\n                    return model\n            for key in example_batch:\n                example_tensor = torch.ones_like(example_batch[key])\n                jit_inputs.append(example_tensor)\n            jit_inputs = tuple(jit_inputs)\n\n            try:\n                jit_model = model.eval()\n                with ContextManagers([self.autocast_smart_context_manager(), torch.no_grad()]):\n                with ContextManagers(\n                    [\n                        torch.cpu.amp.autocast(cache_enabled=False, dtype=self.amp_dtype)\n                        if self.use_cpu_amp\n                        else contextlib.nullcontext(),\n                        torch.no_grad(),\n```"}, {"role": "assistant", "content": "thanks for the reminding, add it back. and verified in GPU"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 19891, "url": "https://github.com/huggingface/transformers/pull/19891#discussion_r1006372701"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `configuration_layoutlmv2.py`:\n\n```python\nArgs:\n        vocab_size (`int`, *optional*, defaults to 30522):\n            Vocabulary size of the LayoutLMv2 model. Defines the number of different tokens that can be represented by\n            the `inputs_ids` passed when calling [`LayoutLMv2Model`] or [`TFLayoutLMv2Model`].\n            Vocabulary size of the LayoutXLM model. Defines the number of different tokens that can be represented by\n```"}, {"role": "assistant", "content": "These changes arent' needed, no? The config is called `LayoutLMv2Config`"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41773, "url": "https://github.com/huggingface/transformers/pull/41773#discussion_r2564465082"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `trackref.py`:\n\n```python\nalias to object in that case).\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom collections import defaultdict\nfrom operator import itemgetter\nfrom time import time\nfrom typing import TYPE_CHECKING, Any, DefaultDict, Iterable\nfrom typing import TYPE_CHECKING, Any\nfrom weakref import WeakKeyDictionary\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n    from collections.abc import Iterable\n\n\nNoneType = type(None)\nlive_refs: DefaultDict[type, WeakKeyDictionary] = defaultdict(WeakKeyDictionary)\nlive_refs: defaultdict[type, WeakKeyDictionary] = defaultdict(WeakKeyDictionary)\n\n\nclass object_ref:\n    \"\"\"Inherit from this class to a keep a record of live instances\"\"\"\n\n    __slots__ = ()\n\n    def __new__(cls, *args: Any, **kwargs: Any) -> \"Self\":\n    def __new__(cls, *args: Any, **kwargs: Any) -> object_ref:\n```"}, {"role": "assistant", "content": "I don't think this is correct, `object.__new__(cls)` returns a `cls` instance."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6472, "url": "https://github.com/scrapy/scrapy/pull/6472#discussion_r1741941645"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_known_annotated_metadata.py`:\n\n```python\nif constraint == 'union_mode' and schema_type == 'union':\n                schema['mode'] = value  # type: ignore  # schema is UnionSchema\n            else:\n                schema[constraint] = value\n                if schema_type == 'decimal' and constraint in {'multiple_of', 'le', 'ge', 'lt', 'gt'}:\n                    schema[constraint] = Decimal(value)\n                else:\n                    schema[constraint] = value\n```"}, {"role": "assistant", "content": "I thought about this, only concern is that as is, the core schema will not comply to the corresponding typed dict definition (e.g. with `f: Decimal = Field(gt=1)`, the core_schema is `{'type': 'decimal', 'gt': 1}`).\r\n\r\nUnless we change the constraints type on the typed dict to be `Decimal | float`, and then do the coercion during the validator build in `pydantic-core`?"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 11350, "url": "https://github.com/pydantic/pydantic/pull/11350#discussion_r1930748853"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `typing.py`:\n\n```python\n# TypeError: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'\n        return cast(Any, type_)._evaluate(globalns, localns, recursive_guard=set())\n\nelse:\n\n    def evaluate_forwardref(type_: ForwardRef, globalns: Any, localns: Any) -> Any:\n        # Pydantic 1.x will not support PEP 695 syntax, but provide `type_params` to avoid\n        # warnings:\n```"}, {"role": "assistant", "content": "```suggestion\r\n        # Pydantic 1.x will not support PEP 695 syntax, but provide `type_params` to avoid warnings\r\n```\r\n\r\nThanks for the comment, looks good"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10847, "url": "https://github.com/pydantic/pydantic/pull/10847#discussion_r1879290153"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_tokenization_plbart.py`:\n\n```python\nself.assertEqual(self.tokenizer.fairseq_tokens_to_ids[\"__python__\"], 50002)\n        self.assertEqual(self.tokenizer.fairseq_tokens_to_ids[\"__en_XX__\"], 50003)\n\n    @is_flaky\n```"}, {"role": "assistant", "content": "I have to agree with  here as well, they will just be forgotten eternally..."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43530, "url": "https://github.com/huggingface/transformers/pull/43530#discussion_r2732836430"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `base.py`:\n\n```python\noptions, route_name or name, args, kwargs, task_type)\n\n        driver_type = self.producer_pool.connections.connection.transport.driver_type\n        is_native_delayed_delivery = detect_quorum_queues(self, driver_type)[0]\n        if is_native_delayed_delivery and options['queue'].exchange.type != 'direct':\n            if eta:\n                if isinstance(eta, str):\n                    eta = isoparse(eta)\n                countdown = (maybe_make_aware(eta) - self.now()).total_seconds()\n\n            if countdown:\n                if countdown > 0:\n                    routing_key = calculate_routing_key(int(countdown), options[\"queue\"].routing_key)\n                    exchange = Exchange(\n                        'celery_delayed_27',\n                        type='topic',\n                    )\n                    del options['queue']\n                    options['routing_key'] = routing_key\n                    options['exchange'] = exchange\n        elif is_native_delayed_delivery and options['queue'].exchange.type == 'direct':\n            logger.warning(\n                'Direct exchanges are not supported with native delayed delivery.\\n'\n                f'{options[\"queue\"].exchange.name} is a direct exchange but should be a topic exchange or '\n                'a fanout exchange in order for native delayed delivery to work properly.\\n'\n                'If quorum queues are used, this task may block the worker process until the ETA arrives.'\n            )\n\n        if (eta or countdown) and detect_quorum_queues(self, driver_type)[0]:\n```"}, {"role": "assistant", "content": "To improve clarity and efficiency, consider calling detect_quorum_queues once by assigning its result to a variable instead of calling it multiple times.\n```suggestion\n        quorum_queues_detected = detect_quorum_queues(self, driver_type)\n\n        if (eta or countdown) and quorum_queues_detected[0]:\n```"}], "_meta": {"repo": "celery/celery", "pr_number": 9686, "url": "https://github.com/celery/celery/pull/9686#discussion_r2072845748"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `warnings.py`:\n\n```python\n__all__ = (\n    'PydanticDeprecatedSince20',\n    'PydanticDeprecationWarning',\n```"}, {"role": "assistant", "content": "We should probably still export this, might be used in user code, and we export in `__init__.py`?"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 11168, "url": "https://github.com/pydantic/pydantic/pull/11168#discussion_r1898612995"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_kernels.py`:\n\n```python\n# Copyright 2025 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Run the test: CUDA_VISIBLE_DEVICES=0 RUN_SLOW=1 pytest -sv tests/kernels/test_kernels.py\n\n\nimport copy\nimport gc\nimport types\nfrom unittest.mock import patch\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, KernelConfig\nfrom transformers.integrations.hub_kernels import (\n    _HUB_KERNEL_MAPPING,\n    _KERNEL_MODULE_MAPPING,\n    is_kernel,\n    lazy_load_kernel,\n    load_and_register_attn_kernel,\n)\nfrom transformers.masking_utils import ALL_MASK_ATTENTION_FUNCTIONS\nfrom transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS\nfrom transformers.testing_utils import (\n    TestCasePlus,\n    backend_empty_cache,\n    require_kernels,\n    require_torch_accelerator,\n    torch_device,\n)\nfrom transformers.utils import is_kernels_available\n\n\nif is_kernels_available():\n    import kernels as kernels_pkg\n    from kernels import Device, Mode, kernelize\n\n\n@require_kernels\nclass TestHubKernels(TestCasePlus):\n    def setUp(self):\n        self.model_id = \"unsloth/Llama-3.2-1B-Instruct\"\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n        self.model_kernelized = AutoModelForCausalLM.from_pretrained(\n            self.model_id, use_kernels=True, device_map=torch_device\n        )\n        self.model_not_kernelized = AutoModelForCausalLM.from_pretrained(\n            self.model_id, use_kernels=False, device_map=torch_device\n```"}, {"role": "assistant", "content": "is this loading fast enough? We can probably do it once in `setupClass`"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41765, "url": "https://github.com/huggingface/transformers/pull/41765#discussion_r2448567369"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `prefork.py`:\n\n```python\n\"\"\"Gracefully stop the pool.\"\"\"\n        if self._pool is not None and self._pool._state in (RUN, CLOSE):\n            self._pool.close()\n            self._pool.join()\n\n            # Keep firing timers (for heartbeats on async transports) while\n            # the pool drains. If not using an async transport, no hub exists\n            # and the timer thread is not created.\n            hub = get_event_loop()\n            if hub is not None:\n                shutdown_event = threading.Event()\n\n                def fire_timers_loop():\n                    while not shutdown_event.is_set():\n                        try:\n                            hub.fire_timers()\n                        except Exception:\n                            logger.warning(\n                                \"Exception in timer thread during prefork on_stop()\",\n                                exc_info=True,\n                            )\n                        # 0.5 seconds was chosen as a balance between joining quickly\n                        # after the pool join is complete and sleeping long enough to\n                        # avoid excessive CPU usage.\n                        time.sleep(0.5)\n\n                timer_thread = threading.Thread(target=fire_timers_loop, daemon=True)\n```"}, {"role": "assistant", "content": "[nitpick] Consider adding a descriptive name to the timer thread to aid debugging and observability. Thread names appear in stack traces, debuggers, and monitoring tools, making issues easier to diagnose. For example:\n\n```python\ntimer_thread = threading.Thread(\n    target=fire_timers_loop,\n    daemon=True,\n    name=\"prefork-timer-shutdown\"\n)\n```\n\nThis naming pattern is used elsewhere in the codebase (e.g., `celery/utils/threads.py:44-52`) where threads are given meaningful names.\n```suggestion\n                timer_thread = threading.Thread(\n                    target=fire_timers_loop,\n                    daemon=True,\n                    name=\"prefork-timer-shutdown\"\n                )\n```"}], "_meta": {"repo": "celery/celery", "pr_number": 9986, "url": "https://github.com/celery/celery/pull/9986#discussion_r2554637869"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `cmdline.py`:\n\n```python\nprint('Use \"scrapy <command> -h\" to see more info about a command')\n\n\ndef _print_unknown_command_msg(\n    settings: BaseSettings, cmdname: str, inproject: bool\n) -> None:\n    proj_only_cmds = _get_project_only_cmds(settings)\n    if cmdname in proj_only_cmds and not inproject:\n        cmd_list = \", \".join(sorted(proj_only_cmds))\n        print(\n            f\"{cmdname} command is not available from this location.\\n\"\n            f\"These commands can only be triggered from within a project: {cmd_list}\\n\"\n```"}, {"role": "assistant", "content": "```suggestion\r\n            f\"The {cmdname} command is not available from this location.\\n\"\r\n            f\"These commands are only available within a project: {cmd_list}\\n\"\r\n```"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 3426, "url": "https://github.com/scrapy/scrapy/pull/3426#discussion_r1907448567"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_generate_schema.py`:\n\n```python\nAt present the following types can be named/recursive:\n\n        - BaseModel\n        - Dataclasses\n        - TypedDict\n        - TypeAliasType\n        - Pydantic model\n        - Pydantic and stdlib dataclasses\n        - Typed dictionaries\n        - Named tuples\n        - `TypeAliasType` instances\n        - Enums\n        \"\"\"\n        ref = get_type_ref(tp)\n        # return the reference if we're either (1) in a cycle or (2) it was already defined\n        if ref in self.seen or ref in self.definitions:\n        # return the reference if we're either (1) in a cycle or (2) it the reference was already encountered:\n        if ref in self._recursively_seen or ref in self._definitions:\n            yield (ref, core_schema.definition_reference_schema(ref))\n        else:\n            self.seen.add(ref)\n            self._recursively_seen.add(ref)\n            try:\n                yield (ref, None)\n            finally:\n                self.seen.discard(ref)\n                self._recursively_seen.discard(ref)\n\n    def get_from_ref(self, ref: str) -> CoreSchema | None:\n```"}, {"role": "assistant", "content": "This is probably better, given that we have the above `get_schema_or_ref` function"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 11208, "url": "https://github.com/pydantic/pydantic/pull/11208#discussion_r1905600470"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_async_client.py`:\n\n```python\nresponse = await client.get(\"custom://www.example.com\")\n        assert response.status_code == 200\n        assert response.json() == {\"app\": \"mounted\"}\n\n\n@pytest.mark.usefixtures(\"async_environment\")\nasync def test_response_aclose_map_exceptions():\n    class BrokenStream:\n        async def __aiter__(self):\n            # so we're an AsyncIterator\n            pass  # pragma: nocover\n\n        async def aclose(self):\n            raise httpcore.CloseError(OSError(104, \"Connection reset by peer\"))\n\n    def handle(request: httpx.Request) -> httpx.Response:\n        return httpx.Response(200, stream=BrokenStream())\n\n    async with httpx.AsyncClient(transport=httpx.MockTransport(handle)) as client:\n        async with client.stream(\"GET\", \"http://example.com\") as response:\n            with pytest.raises(httpx.CloseError):\n                await response.aclose()\n```"}, {"role": "assistant", "content": "You are correct. And the `await client.request()` test is a worthy addition.\r\n\r\nYou're the maintainer, so you are welcome to change to nix the `aclose()` in the test ... but first, I'll explain my motivation.\r\n\r\nDuring debugging, I was surprised by #1464 -- a broken stream raises the \"wrong\" error on shutdown.\r\n\r\nI imagine a reasonable `__aexit__()` might _handle_ an errno=104 error -- by doing nothing and raising nothing. So I aimed for an explicit test that avoids `__aexit__()`.\r\n\r\nThe change would be _correct_. But as a user, I find `__axit__()` behavior confusing; so I wrote a more-explicit test."}], "_meta": {"repo": "encode/httpx", "pr_number": 1465, "url": "https://github.com/encode/httpx/pull/1465#discussion_r575204954"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modular_modernbert.py`:\n\n```python\nSee `forward` method for additional details.\n    \"\"\"\n\n    def __init__(self, config: ModernBertConfig, layer_id: int | None = None):\n    def __init__(self, config: ModernBertConfig, layer_idx: int | None = None):\n        super().__init__()\n        self.config = config\n        self.layer_id = layer_id\n        self.layer_idx = layer_idx\n\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention heads ({config.num_attention_heads})\"\n            )\n\n        self.attention_dropout = config.attention_dropout\n        self.deterministic_flash_attn = config.deterministic_flash_attn\n        self.num_heads = config.num_attention_heads\n        self.head_dim = config.hidden_size // config.num_attention_heads\n        self.all_head_size = self.head_dim * self.num_heads\n        self.Wqkv = nn.Linear(config.hidden_size, 3 * self.all_head_size, bias=config.attention_bias)\n        layer_type = config.layer_types[layer_id]\n        self.Wqkv = nn.Linear(config.hidden_size, 3 * config.hidden_size, bias=config.attention_bias)\n\n        if layer_id % config.global_attn_every_n_layers != 0:\n            self.local_attention = (config.local_attention // 2, config.local_attention // 2)\n            max_position_embeddings = config.local_attention\n        if layer_idx % config.global_attn_every_n_layers != 0:\n            self.sliding_window = config.sliding_window + 1\n        else:\n            self.local_attention = (-1, -1)\n            max_position_embeddings = config.max_position_embeddings\n            self.sliding_window = None\n\n        if config._attn_implementation == \"flash_attention_2\":\n            rope_parameters_dict = (\n                self.config.rope_parameters[layer_type] if layer_type is not None else self.config.rope_parameters\n            )\n            rope_theta = rope_parameters_dict[\"rope_theta\"]\n            self.rotary_emb = ModernBertUnpaddedRotaryEmbedding(\n                dim=self.head_dim, max_seqlen=max_position_embeddings, base=rope_theta\n            )\n        else:\n            self.rotary_emb = None\n        self.is_causal = False\n\n        self.Wo = nn.Linear(config.hidden_size, config.hidden_size, bias=config.attention_bias)\n        self.out_drop = nn.Dropout(config.attention_dropout) if config.attention_dropout > 0.0 else nn.Identity()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        position_embeddings: torch.Tensor | None = None,\n        output_attentions: bool | None = False,\n        attention_mask: torch.Tensor | None = None,\n        position_embeddings: tuple[torch.Tensor, torch.Tensor] | None = None,\n```"}, {"role": "assistant", "content": "It seems that the original implementation didn't have a consistent order to begin with.\r\nBut keeping it as it is has always been the safe choice\ud83d\udc4d."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43030, "url": "https://github.com/huggingface/transformers/pull/43030#discussion_r2682152770"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_content.py`:\n\n```python\ndef encode_multipart_data(\n    data: dict, files: RequestFiles, boundary: Optional[bytes] = None\n    data: dict,\n    files: RequestFiles,\n    boundary: Optional[bytes] = None,\n    content_type: Optional[str] = None,\n) -> Tuple[Dict[str, str], MultipartStream]:\n    if content_type:\n        boundary = get_multipart_boundary_from_content_type(content_type)\n```"}, {"role": "assistant", "content": "I think _should_ would be a bit strong of a word, we certainly _could_ do that. I think the main reason for having it here is that `_models.py` doesn't know about multipart request or other encoding stuff, so it would be a bit of a violation of the layering that is currently set up to put something like `if content_type.startswith(\"multipart/form-data\")`: <do multipart specific stuff>`."}], "_meta": {"repo": "encode/httpx", "pr_number": 2278, "url": "https://github.com/encode/httpx/pull/2278#discussion_r945185173"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_decorators.py`:\n\n```python\ndef count_positional_params(sig: Signature) -> int:\n    return sum(1 for param in sig.parameters.values() if can_be_positional(param))\n    \"\"\"Get the number of positional (required) arguments of a signature.\n\n    This function should only be used to inspect signatures of validation and serialization functions.\n```"}, {"role": "assistant", "content": "I'm not too happy with how this function grew, hence this \"warning\" in the docstring. At some point, maybe this inspection of validators/serializers signature could be refactored, e.g.:\r\n\r\n```python\r\n\r\nclass FunctionInfo:\r\n    func: Callable[..., Any]\r\n\r\n    \r\n    def requires_info_arg(self) -> bool: ...\r\n\r\n    ...\r\n```"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 9478, "url": "https://github.com/pydantic/pydantic/pull/9478#discussion_r1608916010"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_xcodec.py`:\n\n```python\nif hasattr(m, \"parametrizations\") and \"weight\" in m.parametrizations:\n                    torch.nn.utils.parametrize.remove_parametrizations(m, \"weight\", leave_parametrized=True)\n\n    @lru_cache\n    def _get_conv1d_layers(self, module):\n        \"\"\"\n        Recursively iterate to fetch all Conv1d layers.\n        \"\"\"\n\n        def get_conv1d_layers_recursive(module: nn.Module):\n            params_list = []\n\n            if isinstance(module, nn.Conv1d):\n                params_list.append(module)\n\n            # Recursively check all child modules\n            for child in module.children():\n                params_list.extend(get_conv1d_layers_recursive(child))\n\n            return params_list\n\n        return tuple(get_conv1d_layers_recursive(module))\n\n    def _get_conv1d_output_lengths(self, input_length, module=None):\n        \"\"\"\n        For a given module, compute the output length that would be obtained after all Conv1d layers.\n        \"\"\"\n        if module is None:\n            module = self\n\n        conv1d_layers = self._get_conv1d_layers(module)\n\n        for layer in conv1d_layers:\n            input_length = conv1d_output_length(layer, input_length)\n\n        return input_length\n```"}, {"role": "assistant", "content": "When we'll standardize this computation (see #41203), we could be able to do it from the config, provided we ensure that every convolution layer\u2019s parameters are stored there, and in the correct order relative to their appearance in the forward pass. However, I don\u2019t think expanding the config with such parameters is a good idea, especially since we decided to hardcode them to avoid exposing them to the user.\r\n\r\nI was thinking more along the lines of handling this directly during model initialization, when the modules are already being iterated over. For now, is it okay to merge this?"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42095, "url": "https://github.com/huggingface/transformers/pull/42095#discussion_r2543025739"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `item.py`:\n\n```python\nfrom collections.abc import MutableMapping\n\nfrom scrapy.item import BaseItem\n\n\ndef _is_dataclass_instance(obj):\n    \"\"\"\n    Return True if *obj* is a dataclass object, False otherwise.\n    Taken from https://docs.python.org/3/library/dataclasses.html#dataclasses.is_dataclass.\n    \"\"\"\n    try:\n        from dataclasses import is_dataclass\n    except ImportError:\n        return False\n    else:\n        return is_dataclass(obj) and not isinstance(obj, type)\n\n\ndef is_item_like(obj):\n    \"\"\"\n    Return True if *obj* is considered a Scrapy *item*, False otherwise.\n\n    An object is considered an *item* if it is:\n    - a scrapy.item.BaseItem or dict instance (or any subclass)\n    - a dataclass object\n    \"\"\"\n    return isinstance(obj, (BaseItem, dict)) or _is_dataclass_instance(obj)\n\n\nclass ItemAdapter(MutableMapping):\n```"}, {"role": "assistant", "content": "I agree with ; I think scrapy.Item should be handled not in the same way as MutableMapping in itemadapter package, and it is better to do a conditional import of scrapy.Item in itemadapter.\r\n\r\nSo in a way, it is a circular dependency (not necessarily circular import). itemadapter can work without Scrapy, but supports Scrapy Items, just like it supports dataclasses or attr.s objects.\r\n\r\nFor me it looks like a separate wrapper is not required here; we can use it directly."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 3881, "url": "https://github.com/scrapy/scrapy/pull/3881#discussion_r415838245"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `defer.py`:\n\n```python\nbreak\n        except Exception:\n            errback(failure.Failure(), *a, **kw)\n\n\ndef _isfuture(o):\n    # workaround for Python before 3.5.3 not having asyncio.isfuture\n    if hasattr(asyncio, 'isfuture'):\n        return asyncio.isfuture(o)\n    return isinstance(o, asyncio.Future)\n```"}, {"role": "assistant", "content": "Yup.\r\nMy only concern is that the current Python3 version in Ubuntu 16.04 LTS is 3.5.2. Though I couldn't find how to make travis actually use the system Python3."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4259, "url": "https://github.com/scrapy/scrapy/pull/4259#discussion_r362192958"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `http11.py`:\n\n```python\ndef download_request(self, request: Request, spider: Spider) -> Deferred[Response]:\n        \"\"\"Return a deferred for the HTTP download\"\"\"\n        if hasattr(spider, \"download_maxsize\"):\n            warn_on_deprecated_spider_attribute(\"download_maxsize\", \"DOWNLOAD_MAXSIZE\")\n        if hasattr(spider, \"download_warnsize\"):\n            warn_on_deprecated_spider_attribute(\n                \"download_warnsize\", \"DOWNLOAD_WARNSIZE\"\n            )\n```"}, {"role": "assistant", "content": "I wonder if we should try to find a way to trigger this only once per spider, but no strong opinion. Maybe louder is better :slightly_smiling_face:"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 7039, "url": "https://github.com/scrapy/scrapy/pull/7039#discussion_r2405773111"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `pattern_matching_with_if_stmt.py`:\n\n```python\n# flags: --preview --minimum-version=3.10\nmatch \"test\":\n```"}, {"role": "assistant", "content": "Can you add some test cases where there are comments inside existing parentheses? That often causes us grief. Something like this:\r\n\r\n```\r\nmatch match:\r\n    case case if (True # some comment\r\n    ):\r\n        pass\r\n```"}], "_meta": {"repo": "psf/black", "pr_number": 4269, "url": "https://github.com/psf/black/pull/4269#discussion_r1522171947"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `django.py`:\n\n```python\n# Support Django < 4.1\n            connections = self._db.connections.all()\n\n        is_prefork = self.app.conf.get('worker_pool', 'prefork') == \"prefork\"\n        is_prefork = \"prefork\" in self.worker.pool_cls.__module__\n```"}, {"role": "assistant", "content": "The prefork detection logic `\"prefork\" in self.worker.pool_cls.__module__` assumes `self.worker` is not `None`. However, there are code paths where `worker` could be `None` (e.g., when `worker_fixup` is accessed before `on_worker_init`). Should we add a safety check here, such as `is_prefork = self.worker and \"prefork\" in self.worker.pool_cls.__module__` or ensure `worker` is always set before this code is reached?\n```suggestion\n        is_prefork = self.worker is not None and \"prefork\" in self.worker.pool_cls.__module__\n```"}], "_meta": {"repo": "celery/celery", "pr_number": 10023, "url": "https://github.com/celery/celery/pull/10023#discussion_r2616204641"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `genspider.py`:\n\n```python\ntry:\n            spidercls = self.crawler_process.spider_loader.load(name)\n        except KeyError:\n            pass\n            if not self.settings.get('NEWSPIDER_MODULE'):\n                if exists(name + \".py\"):\n                    print(\"File %r already exists in the current directory\" % (name + \".py\"))\n                    return\n            else:\n```"}, {"role": "assistant", "content": "Sorry, should have phrased it better. From what I understand of the code base, when the `scrapy` command is run in the command line, the `settings` object is set in line 112 of the cmdline module, digging further into `get_project_settings()`, I found that the value of `NEWSPIDER_MODULE` **is set here to either an empty string or the module where the spiders are located** in case of a project. This `settings` object is then passed on to cmd in line 137.\r\n\r\nAt this point `cmd` **is already set to** `genspider` and the line 137 `cmd.settings = settings` assigns the previously mentioned `settings` object to `genspider.settings`. This settings dictionary is what I'm accessing through `self.settings`. So essentially the `NEWSPIDER_MODULE` property will always be defined (either to `''` or the spiders module)"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4623, "url": "https://github.com/scrapy/scrapy/pull/4623#discussion_r440105195"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_utils.py`:\n\n```python\n_is_quantized = False\n_is_ds_init_called = False\n\n# Mapping from flash attention implementations to their kernel fallback repositories\n_FLASH_ATTN_KERNEL_FALLBACK = {\n    \"flash_attention_2\": \"kernels-community/flash-attn2\",\n    \"flash_attention_3\": \"kernels-community/vllm-flash-attn3\",\n}\n```"}, {"role": "assistant", "content": "```suggestion\r\nFLASH_ATTN_KERNEL_FALLBACK = {\r\n    \"flash_attention_2\": \"kernels-community/flash-attn2\",\r\n    \"flash_attention_3\": \"kernels-community/vllm-flash-attn3\",\r\n}\r\n```\r\nI think it's fair to make it non-private"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42803, "url": "https://github.com/huggingface/transformers/pull/42803#discussion_r2614121435"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `benchmark_runner.py`:\n\n```python\n# Load model\n        self.logger.debug(f\"Loading model {model_id} on device {config.device}...\")\n        dtype = getattr(torch, config.dtype.removeprefix(\"torch.\"))\n        use_kernels = config.kernelize and kernelize is not None and Mode is not None\n```"}, {"role": "assistant", "content": "So is it the difference in mode that is making a change? If so, it's still better to target inference IMO"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43428, "url": "https://github.com/huggingface/transformers/pull/43428#discussion_r2737780448"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_modeling_qwen2_vl.py`:\n\n```python\ndef test_multi_gpu_data_parallel_forward(self):\n        pass\n\n    def test_enable_input_require_grads_with_gradient_checkpointing(self):\n        if not self.model_tester.is_training:\n            self.skipTest(reason=\"ModelTester not in training mode\")\n\n        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n        config.use_cache = False\n        config.return_dict = True\n\n        for model_class in self.all_model_classes:\n            if not model_class.supports_gradient_checkpointing:\n                continue\n\n            model = model_class(config)\n            model.to(torch_device)\n            model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": True})\n            model.enable_input_require_grads()\n            model.train()\n\n            for parameter in model.parameters():\n                parameter.requires_grad = False\n\n            vision_module = None\n            if hasattr(model, \"visual\"):\n                vision_module = model.visual\n            elif hasattr(model, \"model\") and hasattr(model.model, \"visual\"):\n                vision_module = model.model.visual\n\n            if vision_module is not None:\n                for parameter in vision_module.parameters():\n                    parameter.requires_grad = True\n```"}, {"role": "assistant", "content": "I see, hmm- followed the implem of idefics2/smolvlm as I remembered they faced this issue at the time. You're right that this isn't necessary, we register twice. The lowest module trick should work though, and I'm not sure targeting an attention layer works either. Currently  's script outputs grad norms properly with gradient checkpointing enabled and PEFT disabled on this branch, so it seems to do the trick?\r\n\r\nno GC\r\n```bash\r\n{'loss': 9.4971, 'grad_norm': 23.421083450317383, 'learning_rate': 2e-05, 'epoch': 0.33}                                                                     \r\n{'loss': 7.9526, 'grad_norm': 675.1868896484375, 'learning_rate': 1.866666666666667e-05, 'epoch': 0.67}                                                      \r\n```\r\n\r\nwith GC\r\n```bash\r\n{'loss': 9.4971, 'grad_norm': 23.421083450317383, 'learning_rate': 2e-05, 'epoch': 0.33}\r\n {'loss': 7.9526, 'grad_norm': 675.1868896484375, 'learning_rate': 1.866666666666667e-05, 'epoch': 0.67} \r\n```\r\n\r\n\r\nin either case, agree double registering is useless, will remove!"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41993, "url": "https://github.com/huggingface/transformers/pull/41993#discussion_r2491002109"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `__init__.py`:\n\n```python\n\"\"\"The uncompromising code formatter.\"\"\"\n    ctx.ensure_object(dict)\n\n    assert sys.version_info >= (3, 9), \"Black requires Python 3.9+\"\n```"}, {"role": "assistant", "content": "It's harmless enough, Black is definitely broken on 3.8, I have dev environments lying around with random Python versions. Is it causing you some issue?"}], "_meta": {"repo": "psf/black", "pr_number": 4452, "url": "https://github.com/psf/black/pull/4452#discussion_r1792686970"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `type_params.py`:\n\n```python\ndef magic[Trailing, Comma,](): pass\n\nclass Foo:\n```"}, {"role": "assistant", "content": "This file is meant for PEP 695 syntax, which your new test doesn't use. Therefore, it's also run in 3.12+ mode. I couldn't find a better file to put it, so I'd suggest adding a new test file, maybe `annotation.py`."}], "_meta": {"repo": "psf/black", "pr_number": 4530, "url": "https://github.com/psf/black/pull/4530#discussion_r1876843542"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_connections.py`:\n\n```python\n},\n                {\"secret_key\": \"***\", \"extra_fields\": \"test-extra_fields\", \"password\": \"***\"},\n            ),\n            (\n                {\n                    \"auth_type\": {\n                        \"value\": \"oauth2\",\n                        \"schema\": {\n                            \"type\": [\"string\", \"null\"],\n                            \"title\": \"Auth Type\",\n                        },\n                        \"description\": \"Authentication type: 'basic' (default) | 'oauth2'\",\n                        \"source\": None,\n                    },\n                    \"access_token\": {  # sensitive field\n                        \"value\": \"oauth2-bearer-token\",\n                        \"schema\": {\n                            \"type\": [\"string\", \"null\"],  # Optional field\n                            \"title\": \"Access Token\",\n                        },\n                        \"description\": \"OAuth 2 bearer (one-hour).\",\n                        \"source\": None,\n                    },\n                },\n                {\n                    \"auth_type\": {\n                        \"value\": \"oauth2\",\n                        \"schema\": {\n                            \"type\": [\"string\", \"null\"],\n                            \"title\": \"Auth Type\",\n                        },\n                        \"description\": \"Authentication type: 'basic' (default) | 'oauth2'\",\n                        \"source\": None,\n                    },\n                    \"access_token\": {  # sensitive field - masked but schema.type preserved\n                        \"value\": \"***\",\n                        \"schema\": {\n                            \"type\": [\"string\", \"null\"],  # Should be preserved\n                            \"title\": \"***\",\n                        },\n                        \"description\": \"***\",\n```"}, {"role": "assistant", "content": "Thanks for the review!\r\n\r\nI initially misunderstood the previous implementation - I thought it was intentionally masking everything (including schema.title and description) and accidentally masked schema.type as well. However, only the `value` field should be masked for sensitive fields.\r\n\r\nI'll update the code to mask only the `value` field.\r\n\r\nThanks for catching this!"}], "_meta": {"repo": "apache/airflow", "pr_number": 60392, "url": "https://github.com/apache/airflow/pull/60392#discussion_r2682533899"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `main.py`:\n\n```python\nreturn cls.__name__.lower()\n\n    @classmethod\n    def model_validate(\n    def model_validate(  # type: ignore[override]\n```"}, {"role": "assistant", "content": "I changed this to not use `kwargs` and instead ignore the type here, that way people will keep having autocompletion for the params, and kwargs won't swallow extra / invald params.\r\n\r\nSeeing this, I realize it was probably not the best idea to include the extra `update` here. :thinking: \r\n\r\nAlso, it's probably not needed for most use cases, maybe it would make sense to remove those docs, explain how to extend data to validate an object using just dicts, then deprecate using it, and finally at some point dropping the `update`... \r\n\r\nBut anyway, for now, it probably works to just `# type: ignore` and handle it on our side so that final users get the best developer experience / editor support."}], "_meta": {"repo": "tiangolo/sqlmodel", "pr_number": 1340, "url": "https://github.com/fastapi/sqlmodel/pull/1340#discussion_r2061540780"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_qwen2_audio.py`:\n\n```python\n# Overwritten -- we should not pass input_features when we are in cached decoding stage\n\n        input_features = kwargs.pop(\"input_features\", None)\n        cache_position = kwargs.get(\"cache_position\")\n        is_first_iteration = kwargs.get(\"is_first_iteration\", False)\n\n        model_inputs = super().prepare_inputs_for_generation(*args, **kwargs)\n\n        if cache_position is not None and cache_position[0] == 0:\n        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n```"}, {"role": "assistant", "content": "Is this `or` intentional? Not sure if this happened elsewhere"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42088, "url": "https://github.com/huggingface/transformers/pull/42088#discussion_r2592857118"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `docs.py`:\n\n```python\n{!../../docs/missing-translation.md!}\n\"\"\"\n\nnon_translated_sections = [\nnon_translated_sections = (\n    \"reference/\",\n```"}, {"role": "assistant", "content": "To be OS agnostic:\r\n```suggestion\r\n    \"reference\",\r\n```\r\nassuming we have no \"normal\" pages starting with \"reference\" (I don't think that's a real issue)"}], "_meta": {"repo": "fastapi/fastapi", "pr_number": 14033, "url": "https://github.com/fastapi/fastapi/pull/14033#discussion_r2371531886"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `cache.py`:\n\n```python\nprompt_ids=(state.initial_tokens + state.generated_tokens),\n                )\n\n    def copy_cache(self, source_blocks: list[int], forked_blocks: list[int]) -> None:\n        \"\"\"Copy the cache from the source blocks to the forked blocks.\"\"\"\n        source_blocks = torch.tensor(source_blocks, device=self.device, dtype=torch.int32)\n        forked_blocks = torch.tensor(forked_blocks, device=self.device, dtype=torch.int32)\n        for key_cache, value_cache in zip(self.key_cache, self.value_cache):\n            key_cache = key_cache.view(-1, self.block_size, self.num_key_value_heads, self.head_dim)\n            value_cache = value_cache.view(-1, self.block_size, self.num_key_value_heads, self.head_dim)\n            key_cache[forked_blocks] = key_cache[source_blocks]\n            value_cache[forked_blocks] = value_cache[source_blocks]\n            # FIXME: should be one copy for al CMs with only the changing blocks\n            # FIXME: even once per fork batch\n\n    def fork_request(self, state: RequestState, new_request_id: str) -> RequestState:\n        \"\"\"Fork a request into a new request. The new request is created by copying the state and updating the\n        request_id.\"\"\"\n        new_state = state.fork(new_request_id)\n        for cm in self.group_cache_managers:\n            source_blocks, forked_blocks = cm.fork_blocks(state.request_id, new_state.request_id, self._block_manager)\n            self.copy_cache(source_blocks, forked_blocks)\n```"}, {"role": "assistant", "content": "same here we should \"schedule\" the copy. Remember we are in Python and the gil is killing us"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42921, "url": "https://github.com/huggingface/transformers/pull/42921#discussion_r2627092509"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `compat.py`:\n\n```python\n#: Python 3.x?\nis_py3 = (_ver[0] == 3)\n\ntry:\n    import simplejson as json\nexcept ImportError:\n    import json\n```"}, {"role": "assistant", "content": "This is backwards incompatible and needs to be reverted"}], "_meta": {"repo": "psf/requests", "pr_number": 5856, "url": "https://github.com/psf/requests/pull/5856#discussion_r664037480"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_generate_schema.py`:\n\n```python\nAt present the following types can be named/recursive:\n\n        - BaseModel\n        - Dataclasses\n        - TypedDict\n        - TypeAliasType\n        - Pydantic model\n        - Pydantic and stdlib dataclasses\n        - Typed dictionaries\n        - Named tuples\n        - `TypeAliasType` instances\n        - Enums\n        \"\"\"\n        ref = get_type_ref(tp)\n        # return the reference if we're either (1) in a cycle or (2) it was already defined\n        if ref in self.seen or ref in self.definitions:\n        # return the reference if we're either (1) in a cycle or (2) it the reference was already encountered:\n        if ref in self._recursively_seen or ref in self._definitions:\n            yield (ref, core_schema.definition_reference_schema(ref))\n        else:\n            self.seen.add(ref)\n            self._recursively_seen.add(ref)\n            try:\n                yield (ref, None)\n            finally:\n                self.seen.discard(ref)\n                self._recursively_seen.discard(ref)\n\n    def get_from_ref(self, ref: str) -> CoreSchema | None:\n        \"\"\"Resolve the schema from the given reference.\"\"\"\n        return self._definitions.get(ref)\n\n    def create_reference_to_schema(self, schema: CoreSchema) -> core_schema.DefinitionReferenceSchema:\n```"}, {"role": "assistant", "content": "Or even `create_definition_reference_schema` to be more explicit"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 11208, "url": "https://github.com/pydantic/pydantic/pull/11208#discussion_r1905606673"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_dagbag.py`:\n\n```python\nDagBag(dag_folder=os.fspath(tmp_path), include_examples=False)\n            assert \"SIGSEGV signal handler registration failed. Not in the main thread\" in caplog.text\n\n    def test_bundle_path_is_in_syspath_during_parse(self, tmp_path):\n        \"\"\"\n        Test that bundle_path is correctly added to and removed from sys.path during DAG import.\n        \"\"\"\n        util_file = tmp_path / \"bundle_util.py\"\n        util_file.write_text('def get_message(): return \"Hello from bundle!\"')\n\n        dag_file = tmp_path / \"test_dag.py\"\n        dag_file.write_text(\n            textwrap.dedent(\n                \"\"\"\\\n                from airflow import DAG\n                from airflow.decorators import task\n                import sys\n                import bundle_util\n\n                with DAG('test_import', description=f\"DAG with sys.path: {sys.path}\"):\n                    @task\n                    def mytask():\n                        print(util.get_message())\n                    mytask()\n                \"\"\"\n            )\n        )\n        syspath_before = deepcopy(sys.path)\n        assert str(tmp_path) not in sys.path  # bundle path is not in sys.path before bundle parse\n        dagbag = DagBag(dag_folder=str(dag_file), bundle_path=tmp_path, include_examples=False)\n\n        # Check import was successful\n        assert len(dagbag.dags) == 1\n        assert not dagbag.import_errors\n\n        dag = dagbag.get_dag(\"test_import\")\n        assert dag is not None\n        assert str(tmp_path) in dag.description  # sys.path is enhanced during parse\n\n        assert str(tmp_path) not in sys.path  # bundle path is not preserved in sys.path\n        assert sys.path == syspath_before  # sys.path doesn't change\n\n    def test_bundle_path_syspath_is_cleanup_on_failure(self, tmp_path):\n        \"\"\"\n        Test that bundle_path is correctly added to and removed from sys.path during DAG import.\n        \"\"\"\n        util_file = tmp_path / \"bundle_util.py\"\n        util_file.write_text('def get_message(): return \"Hello from bundle!\"')\n\n        dag_file = tmp_path / \"test_dag.py\"\n        dag_file.write_text(\n            textwrap.dedent(\n                \"\"\"\\\n                from airflow import DAG\n                from airflow.decorators import task\n```"}, {"role": "assistant", "content": "```suggestion\n                from airflow.sdk import DAG\n```"}], "_meta": {"repo": "apache/airflow", "pr_number": 55894, "url": "https://github.com/apache/airflow/pull/55894#discussion_r2365620029"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `auto_docstring.py`:\n\n```python\nfunc (`function`): The function the parameter belongs to\n    \"\"\"\n    optional = False\n    if param.annotation != inspect.Parameter.empty:\n        param_type = param.annotation\n        if \"typing\" in str(param_type):\n            param_type = \"\".join(str(param_type).split(\"typing.\")).replace(\"transformers.\", \"~\")\n        elif hasattr(param_type, \"__module__\"):\n            param_type = f\"{param_type.__module__.replace('transformers.', '~').replace('builtins', '')}.{param.annotation.__name__}\"\n            if param_type[0] == \".\":\n                param_type = param_type[1:]\n        else:\n            if False:\n                print(\n                    f\"[ERROR] {param_type} for {param_name} of {func.__qualname__} in file {func.__code__.co_filename} has an invalid type\"\n                )\n        if \"ForwardRef\" in param_type:\n            param_type = re.sub(r\"ForwardRef\\('([\\w.]+)'\\)\", r\"\\1\", param_type)\n        if \"Optional\" in param_type:\n            param_type = re.sub(r\"Optional\\[(.*?)\\]\", r\"\\1\", param_type)\n            optional = True\n    if param.annotation == inspect.Parameter.empty:\n        return \"\", False\n    elif get_origin(param.annotation) is Union:\n        subtypes = get_args(param.annotation)\n    else:\n        param_type = \"\"\n        subtypes = [param.annotation]  # Just pretend it's a single-element union so we don't need two code paths\n    out_str = []\n    for subtype in subtypes:\n        if subtype is type(None):\n            optional = True\n            out_str.append(\"None\")\n            continue\n        subtype = str(subtype)\n        if \"typing\" in subtype:\n            subtype = \"\".join(subtype.split(\"typing.\")).replace(\"transformers.\", \"~\")\n        elif hasattr(subtype, \"__module__\") and hasattr(subtype, \"__name__\"):\n            subtype = f\"{subtype.__module__.replace('transformers.', '~').replace('builtins', '')}.{subtype.__name__}\".removeprefix(\n```"}, {"role": "assistant", "content": "I just read the docs of removeprefix. It sounds good and yeah we could add it in."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42431, "url": "https://github.com/huggingface/transformers/pull/42431#discussion_r2611320430"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `subprocess.py`:\n\n```python\nsts = 0\n            return (pid, sts)\n\n        def _wait_pidfd(self, timeout):\n            \"\"\"Wait for PID to terminate using pidfd_open() + poll().\n            Linux >= 5.3 only.\n            \"\"\"\n            if not _CAN_USE_PIDFD_OPEN:\n                return False\n            try:\n                pidfd = os.pidfd_open(self.pid, 0)\n            except OSError:\n                # May be:\n                # - ESRCH: no such process\n                # - EMFILE, ENFILE: too many open files (usually 1024)\n                # - ENODEV: anonymous inode filesystem not supported\n                # - EPERM, EACCES, ENOSYS: undocumented; may happen if\n                #   blocked by security policy like SECCOMP\n                return False\n\n            try:\n                poller = select.poll()\n                poller.register(pidfd, select.POLLIN)\n                events = poller.poll(math.ceil(timeout * 1000))\n                if not events:\n                    raise TimeoutExpired(self.args, timeout)\n                return True\n            finally:\n                os.close(pidfd)\n\n        def _wait_kqueue(self, timeout):\n            \"\"\"Wait for PID to terminate using kqueue(). macOS and BSD only.\"\"\"\n            if not _CAN_USE_KQUEUE:\n                return False\n            try:\n                kq = select.kqueue()\n            except OSError:\n                # likely EMFILE / ENFILE (too many open files)\n                return False\n\n            try:\n                kev = select.kevent(\n                    self.pid,\n                    filter=select.KQ_FILTER_PROC,\n                    flags=select.KQ_EV_ADD | select.KQ_EV_ONESHOT,\n                    fflags=select.KQ_NOTE_EXIT,\n                )\n                try:\n                    events = kq.control([kev], 1, timeout)  # wait\n                except OSError as err:  # should never happen\n```"}, {"role": "assistant", "content": "```suggestion\n                except OSError:  # should never happen\n```"}], "_meta": {"repo": "python/cpython", "pr_number": 144047, "url": "https://github.com/python/cpython/pull/144047#discussion_r2712835659"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_responses.py`:\n\n```python\n200,\n        content=b\"Hello,\\nworld!\",\n    )\n\n    content = []\n    for line in response.iter_lines():\n        content.append(line)\n    content = [line for line in response.iter_lines()]\n```"}, {"role": "assistant", "content": "We could also use `content = list(response.iter_lines())` here."}], "_meta": {"repo": "encode/httpx", "pr_number": 1759, "url": "https://github.com/encode/httpx/pull/1759#discussion_r673974528"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_apertus.py`:\n\n```python\ndef __init__(self, config: ApertusConfig, device=None):\n        super().__init__()\n        # BC: \"rope_type\" was originally \"type\"\n        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n        else:\n            self.rope_type = \"default\"\n        self.max_seq_len_cached = config.max_position_embeddings\n        self.original_max_seq_len = config.max_position_embeddings\n\n        standardize_rope_params(config)\n        self.config = config\n        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n\n        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n        rope_init_fn: Callable = self.compute_default_rope_parameters\n        if self.rope_type != \"default\":\n            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n        self.original_inv_freq = self.inv_freq\n        self.original_inv_freq = inv_freq\n\n    @staticmethod\n    def compute_default_rope_parameters(\n        config: Optional[ApertusConfig] = None,\n        device: Optional[\"torch.device\"] = None,\n        seq_len: Optional[int] = None,\n        layer_type: Optional[str] = None,\n    ) -> tuple[\"torch.Tensor\", float]:\n        \"\"\"\n        Computes the inverse frequencies according to the original RoPE implementation\n        Args:\n            config ([`~transformers.PreTrainedConfig`]):\n                The model configuration.\n            device (`torch.device`):\n                The device to use for initialization of the inverse frequencies.\n            seq_len (`int`, *optional*):\n                The current sequence length. Unused for this type of RoPE.\n            layer_type (`str`, *optional*):\n                The current layer type if the model has different RoPE parameters per type.\n                Should not be used unless `config.layer_types is not None`\n        Returns:\n            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n        \"\"\"\n        # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n        standardize_rope_params(config)\n\n        base = (\n            config.rope_parameters[layer_type][\"rope_theta\"]\n            if layer_type is not None\n            else config.rope_parameters[\"rope_theta\"]\n        )\n        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n```"}, {"role": "assistant", "content": "It's kind of the contrary \ud83d\ude05 models that don't support MHA should not have kv_groups. The reason they do is most probably for convenience / we were not careful IMO. \r\n\r\nI might be wrong, all the comments I did for apertus is assuming that apertus is kinda llama, has a single inv freq with no partial factor"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 39847, "url": "https://github.com/huggingface/transformers/pull/39847#discussion_r2426244860"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `image_processing_conditional_detr.py`:\n\n```python\nif \"pad_and_return_pixel_mask\" in kwargs:\n            do_pad = kwargs.pop(\"pad_and_return_pixel_mask\")\n\n        if \"max_size\" in kwargs:\n            logger.warning_once(\n                \"The `max_size` parameter is deprecated and will be removed in v4.26. \"\n                \"Please specify in `size['longest_edge'] instead`.\",\n            )\n            max_size = kwargs.pop(\"max_size\")\n        else:\n            max_size = None if size is None else 1333\n```"}, {"role": "assistant", "content": "I think in this case it's better to convert warning to a `ValueError`, so users know what is the reason if it crashes and how to fix it\r\n\r\nHub-related deprecations are so annoying sometimes \ud83d\ude43"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41681, "url": "https://github.com/huggingface/transformers/pull/41681#discussion_r2445131158"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `feature_extraction_utils.py`:\n\n```python\n# We cast only floating point tensors to avoid issues with tokenizers casting `LongTensor` to `FloatTensor`\n        def maybe_to(v):\n            # check if v is a floating point\n            # check if v is a floating point tensor\n            if isinstance(v, torch.Tensor) and torch.is_floating_point(v):\n                # cast and send to device\n                return v.to(*args, **kwargs)\n            elif isinstance(v, torch.Tensor) and device is not None:\n                return v.to(device=device, non_blocking=non_blocking)\n            # recursively handle lists and tuples\n            elif isinstance(v, (list, tuple)):\n                return type(v)(maybe_to(item) for item in v)\n```"}, {"role": "assistant", "content": "A good example is in the issue I mentioned https://github.com/huggingface/transformers/issues/42205\r\n\r\nBasically for ragged lists of tensors"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42884, "url": "https://github.com/huggingface/transformers/pull/42884#discussion_r2624185599"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `exceptions.py`:\n\n```python\n\"\"\"\nfrom urllib3.exceptions import HTTPError as BaseHTTPError\n\ntry:\n```"}, {"role": "assistant", "content": "We have logic for which simplejson/json exceptions we're using spread across a few places now. Is there a compelling reason not to simply import JSONDecodeError from compat since it's already been resolved and supplying it as a single parent class?"}], "_meta": {"repo": "psf/requests", "pr_number": 5856, "url": "https://github.com/psf/requests/pull/5856#discussion_r671444604"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_types.py`:\n\n```python\n]\n\nHeaderTypes = Union[\n    \"Headers\", Dict[StrOrBytes, StrOrBytes], Sequence[Tuple[StrOrBytes, StrOrBytes]],\n    \"Headers\",\n    MutableMapping[StrOrBytes, StrOrBytes],\n```"}, {"role": "assistant", "content": "FYI I'm in the process of opening a PR for this that also resolves the odd mypy error you mentioned here https://github.com/encode/httpx/pull/985#discussion_r429624707 :-)\r\n\r\nEdit: https://github.com/encode/httpx/pull/993"}], "_meta": {"repo": "encode/httpx", "pr_number": 985, "url": "https://github.com/encode/httpx/pull/985#discussion_r429624891"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `string_prefixes.py`:\n\n```python\n#!/usr/bin/env python3.6\n#!/usr/bin/env python3\n\nname = R\"\u0141ukasz\"\n```"}, {"role": "assistant", "content": "Just for reducing a redundant test, since all the prefixes are tested below! I thought about removing it altogether, but seems fine to test f-string substitution there at least. Would you like to keep it?"}], "_meta": {"repo": "psf/black", "pr_number": 2297, "url": "https://github.com/psf/black/pull/2297#discussion_r646625180"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `base.py`:\n\n```python\nself.loader_cls = loader or self._get_default_loader()\n        self.log_cls = log or self.log_cls\n        self.control_cls = control or self.control_cls\n        self._custom_task_cls_used = bool(task_cls)\n        self._custom_task_cls_used = (\n```"}, {"role": "assistant", "content": "Yes it _should_ be fine, and I'm probably being extra careful. \r\n\r\nI'm trying to make a change that has the minimum amount of side effects, as any side effects could be considered a breaking change. With a project the size of Celery, it's hard to know what folks are using and how... As your own use case proves (I didn't forsee it coming)."}], "_meta": {"repo": "celery/celery", "pr_number": 9038, "url": "https://github.com/celery/celery/pull/9038#discussion_r1610245644"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `check_notice_files.py`:\n\n```python\n#!/usr/bin/env python\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n# /// script\n# requires-python = \">=3.10\"\n# dependencies = []\n# ///\n\"\"\"\nCheck that NOTICE files contain the current year and Apache Software Foundation reference.\n\nThis script validates NOTICE files to ensure they:\n- Include the current year in copyright statements\n- Reference the Apache Software Foundation\n\nUsage: check_notice_files.py <notice_file_paths...>\n\"\"\"\n\nfrom __future__ import annotations\n\nimport sys\nfrom datetime import datetime\nfrom pathlib import Path\n\nCURRENT_YEAR = str(datetime.now().year)\n\nerrors = 0\n\nfor notice_file in sys.argv[1:]:\n    content = Path(notice_file).read_text()\n\n    if \"Copyright\" in content and CURRENT_YEAR not in content:\n        print(f\"\u274c {notice_file}: Missing current year ({CURRENT_YEAR}) in copyright\")\n        errors += 1\n\n    if \"Apache Software Foundation\" not in content:\n        print(f\"\u274c {notice_file}: Missing 'Apache Software Foundation' reference\")\n        errors += 1\n```"}, {"role": "assistant", "content": "I just found that the format is `Copyright 2016-{CURRENT_YEAR} The Apache Software Foundation`, not sure whether it would be bette to use f-string approach.\r\n\r\n```suggestion\r\n    expected = f\"Copyright 2016-{CURRENT_YEAR} The Apache Software Foundation\"\r\n    if \"Copyright\" in content and expected not in content:\r\n        print(f\"\u274c {notice_file}: Missing expected string: {expected!r}\")\r\n        errors += 1\r\n```"}], "_meta": {"repo": "apache/airflow", "pr_number": 60699, "url": "https://github.com/apache/airflow/pull/60699#discussion_r2701046292"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `json_schema.py`:\n\n```python\ncontinue  # this means that the \"alias\" does not represent a field\n                alias_is_present_on_all_choices = True\n                for choice in one_of_choices:\n                    while '$ref' in choice:\n                        assert isinstance(choice['$ref'], str)\n                        choice = self.get_schema_from_definitions(JsonRef(choice['$ref'])) or {}\n                    choice = self.resolve_schema_to_update(choice)\n```"}, {"role": "assistant", "content": "This function handles the above removed logic more gracefully, with an intuitive error."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10440, "url": "https://github.com/pydantic/pydantic/pull/10440#discussion_r1786775570"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_pickle.py`:\n\n```python\nmodel_type = pickle.loads(pickle.dumps(model_type))\n\n    assert model_type.model_config['title'] == 'MyTitle'\n\n\n@pytest.mark.skipif(\n    sys.version_info[:2] != (3, 14),\n    reason='Pickle data generated on 3.14',\n```"}, {"role": "assistant", "content": "I managed to find a way to make it work with `subprocess`. I've pushed a commit doing so, I also confirmed that the issue reproduces with that approach on the commit before #12689 merged."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 12712, "url": "https://github.com/pydantic/pydantic/pull/12712#discussion_r2708626832"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `remove_except_parens.py`:\n\n```python\n# These brackets are redundant, therefore remove.\ntry:\n    a.something\nexcept (AttributeError) as err:\n    raise err\n\n# This is tuple of exceptions.\n# Although this could be replaced with just the exception,\n# we do not remove brackets to preserve AST.\ntry:\n    a.something\nexcept (AttributeError,) as err:\n    raise err\n\n# This is a tuple of exceptions. Do not remove brackets.\ntry:\n    a.something\nexcept (AttributeError, ValueError) as err:\n```"}, {"role": "assistant", "content": "Have added one for these as well! \ud83d\ude04 \r\n\r\nNote that for the tuple cases this PR has no impact on existing behaviour. This line detects a comma with the assumption that it indicates a tuple (hence why some of my other PRs add an extra condition here to remove brackets in `with`/`for`) and stops the brackets from being made invisible \ud83d\udc4d"}], "_meta": {"repo": "psf/black", "pr_number": 2939, "url": "https://github.com/psf/black/pull/2939#discussion_r833066623"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `configuration_colqwen2.py`:\n\n```python\nvlm_config=None,\n        embedding_dim: int = 128,\n        initializer_range: float = 0.02,\n        use_qwen2_5=False,\n        **kwargs,\n    ):\n        if vlm_config is None:\n            vlm_config = CONFIG_MAPPING[\"qwen2_vl\"]()\n            model_name = \"qwen2_5_vl\" if use_qwen2_5 else \"qwen2_vl\"\n            vlm_config = CONFIG_MAPPING[model_name]()\n            config_name = \"Qwen2_5VLConfig\" if use_qwen2_5 else \"Qwen2VLConfig\"\n```"}, {"role": "assistant", "content": "This is just used as default instantiation when the vlm_config is not provided, we can keep qwen2_vl only here, and so we won't need to have a `use_qwen2_5` config param.\r\nTo instantiate colqwen2.5 from scratch, users will just need to instantiate a `Qwen2_5VLConfig` and pass it to `vlm_config`."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 40600, "url": "https://github.com/huggingface/transformers/pull/40600#discussion_r2322489172"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `lxmlhtml.py`:\n\n```python\nurl = url.encode(response_encoding)\n            # to fix relative links after process_value\n            url = urljoin(response_url, url)\n            rel = el.get('rel')\n            link = Link(url, _collect_string_content(el) or u'',\n                nofollow=True if el.get('rel') == 'nofollow' else False)\n                nofollow=True if rel is not None and 'nofollow' in rel.split() else False)\n```"}, {"role": "assistant", "content": "What do you think about creating a small utility function or method for it? Something like `has_nofollow(rel_value)`."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 1214, "url": "https://github.com/scrapy/scrapy/pull/1214#discussion_r29853667"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `version.py`:\n\n```python\nreturn '\\n'.join('{:>30} {}'.format(k + ':', str(v).replace('\\n', ' ')) for k, v in info.items())\n\n\ndef check_pydantic_core_version() -> bool:\n    \"\"\"Check that the installed `pydantic-core` dependency is compatible.\"\"\"\n    return version('pydantic_core') == '2.27.2'\n```"}, {"role": "assistant", "content": "Does pydantic_core offer a `__version__` attribute? If so we should use that."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 11324, "url": "https://github.com/pydantic/pydantic/pull/11324#discussion_r1928225249"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `config.py`:\n\n```python\ndefer_build: bool\n    \"\"\"\n    Whether to defer model validator and serializer construction until the first model validation.\n    Whether to defer model validator and serializer construction until the first model validation. Defaults to False.\n\n    This can be useful to avoid the overhead of building models which are only\n    used nested within other models, or when you want to manually define type namespace via\n    [`Model.model_rebuild(_types_namespace=...)`][pydantic.BaseModel.model_rebuild]. Defaults to False.\n    [`Model.model_rebuild(_types_namespace=...)`][pydantic.BaseModel.model_rebuild].\n\n    See also [`defer_build_mode`][pydantic.config.ConfigDict.defer_build_mode].\n\n    !!! note\n        `defer_build` does not work by default with FastAPI Pydantic models. Meaning the validator and serializer\n        is constructed immediately when the model is used in FastAPI routes instead of construction being deferred.\n        You also need to use [`defer_build_mode='always'`][pydantic.config.ConfigDict.defer_build_mode] with\n        FastAPI models.\n    \"\"\"\n\n    defer_build_mode: Literal['only_model', 'always']\n```"}, {"role": "assistant", "content": "The caveat with this is that user could remove the `model` from tuple \ud83d\ude05 So either we somewhere validate that `model` must be present or support skipping defer building of model in that case \ud83d\ude05 Another option is to instead have `_defer_build_additional: tuple['type_adapter'] | None`"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 8939, "url": "https://github.com/pydantic/pydantic/pull/8939#discussion_r1531521564"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_bart.py`:\n\n```python\n)\n\n        if embed_tokens is not None:\n            self.embed_tokens.weight = embed_tokens.weight\n            self.embed_tokens = embed_tokens\n```"}, {"role": "assistant", "content": "It might need to be directly set, not after creation. E.g. https://github.com/huggingface/transformers/blob/81b4f9882c8a46c8274084503d297874bb372260/src/transformers/models/t5/modeling_t5.py#L653"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41821, "url": "https://github.com/huggingface/transformers/pull/41821#discussion_r2456555837"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_xpickle.py`:\n\n```python\ncls.enterClassContext(support.swap_attr(pickle, 'HIGHEST_PROTOCOL',\n                                                highest_protocol))\n\n    @staticmethod\n    def send_to_worker(python, data):\n    @classmethod\n    def start_worker(cls):\n        target = os.path.join(os.path.dirname(__file__), 'xpickle_worker.py')\n        worker = subprocess.Popen([*python, target],\n                                  stdin=subprocess.PIPE,\n                                  stdout=subprocess.PIPE,\n                                  stderr=subprocess.PIPE,\n                                  # For windows bpo-17023.\n                                  shell=is_windows)\n        cls.worker = worker\n\n    @classmethod\n    def close_worker(cls):\n```"}, {"role": "assistant", "content": "\"stop_worker()\" name would be better to have start/stop_worker methods."}], "_meta": {"repo": "python/cpython", "pr_number": 144393, "url": "https://github.com/python/cpython/pull/144393#discussion_r2754196212"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_test_multiprocessing.py`:\n\n```python\n\"resource_tracker: There appear to be 1 leaked \"\n                    \"shared_memory objects to clean up at shutdown\", err)\n\n    @unittest.skipIf(os.name != \"posix\", \"resource_tracker is posix only\")\n    def test_shared_memory_untracking(self):\n        # gh-82300: When a separate Python process accesses shared memory\n        # with track=False, it must not register with the resource tracker.\n```"}, {"role": "assistant", "content": "I think technically the previous test guarantees that but I have now added both the `track=False` and `track=True` cases explicitly. The second case needs something from stderr and so it would fail if the resource tracker did not use the same pipe."}], "_meta": {"repo": "python/cpython", "pr_number": 110778, "url": "https://github.com/python/cpython/pull/110778#discussion_r1372138986"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `strings.py`:\n\n```python\nreturn s  # Prefer double quotes\n\n    return f\"{prefix}{new_quote}{new_body}{new_quote}\"\n\n\ndef char_width(char: str) -> int:\n    \"\"\"Return the width of a single character as it would be displayed in a\n    terminal or editor (which respects Unicode East Asian Width).\n\n    Full width characters are counted as 2, while half width characters are\n    counted as 1.\n    \"\"\"\n    return 2 if unicodedata.east_asian_width(char) in (\"F\", \"W\") else 1\n\n\ndef str_width(line_str: str) -> int:\n    \"\"\"Return the width of `line_str` as it would be displayed in a terminal\n    or editor (which respects Unicode East Asian Width).\n\n    You could utilize this function to determine, for example, if a string\n    is too wide to display in a terminal or editor.\n    \"\"\"\n    if line_str.isalpha():\n```"}, {"role": "assistant", "content": "```suggestion\r\n    if line_str.isascii():\r\n```\r\n`isascii` is O(1) in CPython because string objects store internally whether they're ASCII-only. `isalpha` has to actually loop over the string. It's also not obvious that `isalpha` is correct; I'd have to look up whether any East Asian characters are counted as \"alpha\" for Unicode purposes."}], "_meta": {"repo": "psf/black", "pr_number": 3445, "url": "https://github.com/psf/black/pull/3445#discussion_r1059159577"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `trainer.py`:\n\n```python\n\"You should subclass `Trainer` and override the `create_optimizer_and_scheduler` method.\"\n            )\n        default_callbacks = DEFAULT_CALLBACKS + get_reporting_integration_callbacks(self.args.report_to)\n\n        # Add JIT checkpoint callback if enabled\n        if hasattr(self.args, \"enable_jit_checkpoint\") and self.args.enable_jit_checkpoint:\n            from .trainer_jit_checkpoint import JITCheckpointCallback\n```"}, {"role": "assistant", "content": "```suggestion\r\n        # Add JIT checkpoint callback if enabled\r\n        if self.args.enable_jit_checkpoint:\r\n            from .trainer_jit_checkpoint import JITCheckpointCallback\r\n```"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41723, "url": "https://github.com/huggingface/transformers/pull/41723#discussion_r2542069494"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_generate_schema.py`:\n\n```python\n) -> _CommonField:\n        # Update FieldInfo annotation if appropriate:\n        FieldInfo = import_cached_field_info()\n        if has_instance_in_type(field_info.annotation, (ForwardRef, str)):\n            # TODO Can we use field_info.apply_typevars_map here? Shouldn't we use lenient=False?\n            evaluated = _typing_extra.eval_type(field_info.annotation, *self._types_namespace, lenient=True)\n        if not field_info.evaluated:\n            # TODO Can we use field_info.apply_typevars_map here?\n            try:\n                evaluated = _typing_extra.eval_type(field_info.annotation, *self._types_namespace)\n```"}, {"role": "assistant", "content": "I'm guessing `evaluated` is an evaluated type here, which is a bit confusing, given that we use `evaluated` elsewhere as a bool to indicate successful / unsuccessful evaluation.\r\n\r\nPerhaps we could decouple these by naming one differently?"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10769, "url": "https://github.com/pydantic/pydantic/pull/10769#discussion_r1831899638"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `__init__.py`:\n\n```python\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\"\"\"Shared provider discovery utilities and data structures.\"\"\"\n\nfrom __future__ import annotations\n\nimport contextlib\nimport json\nimport logging\nimport pathlib\nfrom collections.abc import Callable, MutableMapping\nfrom dataclasses import dataclass\nfrom functools import wraps\nfrom importlib.resources import files as resource_files\nfrom time import perf_counter\nfrom typing import Any, NamedTuple, ParamSpec\n\nfrom packaging.utils import canonicalize_name\n\nfrom airflow._shared.module_loading import entry_points_with_dist, import_string\nfrom airflow.exceptions import AirflowOptionalProviderFeatureException\n```"}, {"role": "assistant", "content": "Moving it to task sdk: https://github.com/apache/airflow/pull/60323"}], "_meta": {"repo": "apache/airflow", "pr_number": 60218, "url": "https://github.com/apache/airflow/pull/60218#discussion_r2676118842"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_format.py`:\n\n```python\nif verbose:\n            print('Testing exceptions')\n        test_exc_common('%', (), ValueError, \"incomplete format\")\n        test_exc_common('% %s', 1, ValueError,\n                        \"unsupported format character '%' (0x25) at index 2\")\n        test_exc_common('abc %', (), ValueError, \"stray % at position 4\")\n        test_exc_common('abc % %s', 1, ValueError,\n                        \"stray % at position 4 or unexpected format character '%' at position 6\")\n        test_exc_common('abc %z', 1, ValueError,\n                        \"unsupported format %z at position 4\")\n        test_exc_common(\"abc %Id\", 1, ValueError,\n                        \"unsupported format %I at position 4\")\n        test_exc_common(\"abc %'d\", 1, ValueError,\n                        \"stray % at position 4 or unexpected format character ''' at position 5\")\n```"}, {"role": "assistant", "content": "`'''` is not very readable. Would it be possible to format it as U+HHHH or 0xHH?"}], "_meta": {"repo": "python/cpython", "pr_number": 142081, "url": "https://github.com/python/cpython/pull/142081#discussion_r2577210575"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `models.py`:\n\n```python\n\"\"\"\n        Read and return the request content.\n        \"\"\"\n        return await self.content.aread()\n\n    async def stream(self) -> typing.AsyncIterator[bytes]:\n        async for part in self.content:\n            yield part\n        return await self.stream.aread()\n```"}, {"role": "assistant", "content": "It should yes. It's also not *actually* clear if we should expose this method at all."}], "_meta": {"repo": "encode/httpx", "pr_number": 648, "url": "https://github.com/encode/httpx/pull/648#discussion_r359822154"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `text_to_audio.py`:\n\n```python\n\"add_special_tokens\": False,\n                \"return_attention_mask\": True,\n                \"return_token_type_ids\": False,\n                \"padding\": \"max_length\",\n```"}, {"role": "assistant", "content": "Something Joao had removed in https://github.com/huggingface/transformers/pull/39796. I think to get tests passing?"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42326, "url": "https://github.com/huggingface/transformers/pull/42326#discussion_r2549980822"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_configmap.py`:\n\n```python\n\"execution_api_server_url should not be set for Airflow 2.x versions\"\n            )\n\n    @pytest.mark.parametrize(\n        (\"git_sync_enabled\", \"ssh_key_secret\", \"ssh_key\", \"expected_volume\"),\n        [\n            (True, \"my-secret\", None, True),\n            (True, None, \"my-key\", True),\n            (True, \"my-secret\", \"my-key\", True),\n            (True, None, None, False),\n            (False, \"my-secret\", None, False),\n            (False, None, \"my-key\", False),\n        ],\n    )\n    def test_pod_template_git_sync_ssh_key_volume(\n        self, git_sync_enabled, ssh_key_secret, ssh_key, expected_volume\n    ):\n        dag_values = {\"gitSync\": {\"enabled\": git_sync_enabled}}\n        if ssh_key_secret:\n            dag_values[\"gitSync\"][\"sshKeySecret\"] = ssh_key_secret\n        if ssh_key:\n            dag_values[\"gitSync\"][\"sshKey\"] = ssh_key\n\n        docs = render_chart(\n            values={\n                \"executor\": \"KubernetesExecutor\",\n                \"dags\": dag_values,\n            },\n            show_only=[\"templates/configmaps/configmap.yaml\"],\n        )\n\n        pod_template_file = jmespath.search('data.\"pod_template_file.yaml\"', docs[0])\n        if expected_volume:\n            assert \"git-sync-ssh-key\" in pod_template_file\n        else:\n            assert \"git-sync-ssh-key\" not in pod_template_file\n```"}, {"role": "assistant", "content": "```suggestion\r\n        assert (\"git-sync-ssh-key\" in pod_template_file) == expected_volume\r\n```"}], "_meta": {"repo": "apache/airflow", "pr_number": 59418, "url": "https://github.com/apache/airflow/pull/59418#discussion_r2624637434"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `utils.py`:\n\n```python\nreturn netrc_info.authenticators(host)  # type: ignore\n\n\ndef get_ssl_cert_file() -> typing.Optional[str]:\n    for env_name in (\"REQUESTS_CA_BUNDLE\", \"SSL_CERT_FILE\", \"CURL_CA_BUNDLE\"):\n```"}, {"role": "assistant", "content": "> We should check the three environment variables in the order above\r\n> `(SSL_CERT_FILE, REQUESTS_CA_BUNDLE, CURL_CA_BUNDLE)`,\r\n> as SSL_CERT_FILE is a PEP and a standard whereas the other are products of other projects.\r\n-  \thttps://github.com/encode/httpx/issues/306#issue-488260040\r\n\r\nThis also affects the docs"}], "_meta": {"repo": "encode/httpx", "pr_number": 307, "url": "https://github.com/encode/httpx/pull/307#discussion_r320042951"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `dnf5.py`:\n\n```python\nrc=1,\n                    )\n                elif result != libdnf5.base.Transaction.TransactionRunResult_SUCCESS:\n                    failures = []\n                    if result == libdnf5.base.Transaction.TransactionRunResult_ERROR_RPM_RUN:\n                        try:\n                            failures = transaction.get_rpm_messages()\n                        except AttributeError:\n                            # get_rpm_messages is not available in dnf5 < 5.2.7.0\n                            pass\n                    if not failures:\n```"}, {"role": "assistant", "content": "It just occurred to me that we could probably extend `failures` with what we do below in every case (even with `get_rpm_messages` being called)? I suppose it wouldn't hurt to pass more information to the user."}], "_meta": {"repo": "ansible/ansible", "pr_number": 86123, "url": "https://github.com/ansible/ansible/pull/86123#discussion_r2584379344"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_docs_extraction.py`:\n\n```python\n\"\"\"Utilities related to attribute docstring extraction.\"\"\"\nfrom __future__ import annotations\n\nimport ast\nimport inspect\nimport re\nimport textwrap\nfrom typing import Any\n\n\nclass DocstringVisitor(ast.NodeVisitor):\n    def __init__(self) -> None:\n        super().__init__()\n\n        self.target: str | None = None\n        self.attrs: dict[str, str] = {}\n        self.previous_node_type: type[ast.AST] | None = None\n\n    def visit(self, node: ast.AST) -> Any:\n        node_result = super().visit(node)\n        self.previous_node_type = type(node)\n        return node_result\n\n    def visit_AnnAssign(self, node: ast.AnnAssign) -> Any:\n        if isinstance(node.target, ast.Name):\n            self.target = node.target.id\n\n    def visit_Expr(self, node: ast.Expr) -> Any:\n        if isinstance(node.value, ast.Str) and self.previous_node_type is ast.AnnAssign:\n            docstring = inspect.cleandoc(node.value.s)\n            if self.target:\n                self.attrs[self.target] = docstring\n            self.target = None\n\n\ndef _extract_source_from_frame(lines: list[str], cls_name: str) -> list[str] | None:\n    source: list[str] = []\n    frame = inspect.currentframe()\n\n    if frame is None:\n        return None\n\n    while frame and frame.f_back:\n        lnum = frame.f_back.f_lineno\n        if isinstance(lnum, int) and len(lines) >= lnum and re.match(fr'class\\s+{cls_name}', lines[lnum - 1].strip()):\n            source = inspect.getblock(lines[lnum - 1 :])  # type: ignore\n            break\n        frame = frame.f_back\n\n    if not source:\n        return None\n\n    return source\n\n\ndef extract_docstrings_from_cls(cls: type[Any]) -> dict[str, str]:\n    \"\"\"Map model attributes and their corresponding docstring.\n\n    Args:\n        cls: The class of the Pydantic model to inspect.\n\n    Returns:\n        A mapping containing attribute names and their corresponding docstring.\n    \"\"\"\n    try:\n        lines, _ = inspect.findsource(cls)\n    except OSError:\n        # Source can't be parsed (maybe because running in an interactive terminal),\n        # we don't want to error here.\n        return {}\n\n    # We first try to fetch the source lines by walking back the frames:\n    source = _extract_source_from_frame(lines, cls.__name__)\n    if not source:\n        # Fallback to how inspect fetch the source lines, might not work as expected\n        # if two classes have the same name in the same source file.\n        source, _ = inspect.getsourcelines(cls)\n\n    # Required for nested class definitions, e.g. in a function block\n    dedent_source = textwrap.dedent(''.join(source))\n    if not dedent_source.startswith('class'):\n        # We are in the case where there's a dedented (usually multiline) string\n        # at a lower indentation level than the class itself. We wrap our class\n        # in a function as a workaround.\n        dedent_source = f'def _():\\n{dedent_source}'\n\n    visitor = DocstringVisitor()\n    visitor.visit(ast.parse(dedent_source))\n```"}, {"role": "assistant", "content": "```suggestion\r\n    # Prevent syntax errors if the class is indented (e.g. inside a function block)\r\n    source_str = textwrap.indent(''.join(source), '    ')\r\n    source_str = f'def _():\\n{source_str}'\r\n\r\n    visitor = DocstringVisitor()\r\n    visitor.visit(ast.parse(source_str))\r\n```\r\n\r\nSuggestion to avoid an `if`"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 6563, "url": "https://github.com/pydantic/pydantic/pull/6563#discussion_r1369924622"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modular_longcat_flash.py`:\n\n```python\ncache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        if self.config._attn_implementation == \"flash_attention_2\" and self.qk_head_dim != self.v_head_dim:\n        uses_flash_attention_2 = (\n            \"flash\" in self.config._attn_implementation and self.config._attn_implementation.endswith(\"2\")\n        )\n        if uses_flash_attention_2 and self.qk_head_dim != self.v_head_dim:\n```"}, {"role": "assistant", "content": "```suggestion\r\n        if \"flash\" in self.config._attn_implementation and self.qk_head_dim != self.v_head_dim:\r\n```\r\nI think we should generalize this here to check for all flavors. FA3 etc would face the same issue"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42549, "url": "https://github.com/huggingface/transformers/pull/42549#discussion_r2585110675"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `utils.py`:\n\n```python\nreturn can_compile\n\n    def _check_and_get_compiled_call(self, generation_config: GenerationConfig, model_kwargs: dict) -> Callable:\n        \"\"\"\n        Checks the auto-compilation criteria and returns either the compiled or the original forward call.\n        \"\"\"\n        model_forward = self.__call__\n        compile_forward = self._valid_auto_compile_criteria(model_kwargs, generation_config)\n        if compile_forward:\n```"}, {"role": "assistant", "content": "suggestion: \r\n- this `if` could be moved to `_valid_auto_compile_criteria` (which imo makes more sense, it's a validation-related warning)\r\n- then we wouldn't need this funtion. In each decoding method, we can simply have `if self._valid_auto_compile_criteria(...): compile`\r\n\r\n\ud83d\udc49 fewer helper functions \ud83d\ude4f"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 40652, "url": "https://github.com/huggingface/transformers/pull/40652#discussion_r2362277036"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_models.py`:\n\n```python\nCookies(cookies).set_cookie_header(self)\n\n        if stream is None:\n            headers, stream = encode_request(content, data, files, json)\n            headers, stream = encode_request(\n                content=content,\n                data=data,\n                files=files,\n                json=json,\n                content_type=self.headers.get(\"content-type\"),\n            )\n```"}, {"role": "assistant", "content": "Yes, that kind of was my thought (if `encode_request` did not have `boundary` I would not have suggested that). Thank you for accepting my suggestion.\r\n\r\nI just wonder if you would also go also look at potentially reviewing the remaining changes in `httpx/_content.py`? I mean, these changes seem mostly vestigial to me (e.g. the `content_type` in `encode_request`, and the changes in `encode_multipart_data` are now purely of variable and formatting (and `boundary` is now a required argument)"}], "_meta": {"repo": "encode/httpx", "pr_number": 2278, "url": "https://github.com/encode/httpx/pull/2278#discussion_r945186211"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_models.py`:\n\n```python\nreturn value.encode(encoding or \"ascii\")\n\n\ndef _parse_content_type_charset(content_type: str) -> str | None:\n    # We used to use `cgi.parse_header()` here, but `cgi` became a dead battery.\n    # See: https://peps.python.org/pep-0594/#cgi\n    msg = email.message.Message()\n    msg[\"content-type\"] = content_type\n    return msg.get_content_charset(failobj=None)\n\n\ndef _parse_header_links(value: str) -> list[dict[str, str]]:\n    \"\"\"\n    Returns a list of parsed link headers, for more info see:\n    https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Link\n    The generic syntax of those is:\n    Link: < uri-reference >; param1=value1; param2=\"value2\"\n    So for instance:\n    Link; '<http:/.../front.jpeg>; type=\"image/jpeg\",<http://.../back.jpeg>;'\n    would return\n        [\n            {\"url\": \"http:/.../front.jpeg\", \"type\": \"image/jpeg\"},\n            {\"url\": \"http://.../back.jpeg\"},\n        ]\n    :param value: HTTP Link entity-header field\n    :return: list of parsed link headers\n    \"\"\"\n    links: list[dict[str, str]] = []\n    replace_chars = \" '\\\"\"\n    value = value.strip(replace_chars)\n    if not value:\n        return links\n    for val in re.split(\", *<\", value):\n        try:\n            url, params = val.split(\";\", 1)\n        except ValueError:\n            url, params = val, \"\"\n        link = {\"url\": url.strip(\"<> '\\\"\")}\n        for param in params.split(\";\"):\n            try:\n                key, value = param.split(\"=\")\n            except ValueError:\n                break\n            link[key.strip(replace_chars)] = value.strip(replace_chars)\n        links.append(link)\n    return links\n\n\ndef _obfuscate_sensitive_headers(\n```"}, {"role": "assistant", "content": "Is now a good time to review this? Eg does flask provide similar behaviour onto its headers data structures? Does Django?\n\n(Possibly too off topic??)"}], "_meta": {"repo": "encode/httpx", "pr_number": 3387, "url": "https://github.com/encode/httpx/pull/3387#discussion_r1826205536"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_auth.py`:\n\n```python\n}\n\n\ndef test_auth_hidden_url():\ndef test_auth_hidden_url() -> None:\n    url = \"http://example-username:example-password@example.org/\"\n    expected = \"URL('http://example-username:[secure]@example.org/')\"\n    assert url == URL(url)\n    assert expected == repr(URL(url))\n\n\n@pytest.mark.asyncio\nasync def test_auth_hidden_header():\nasync def test_auth_hidden_header() -> None:\n    url = \"https://example.org/\"\n    auth = (\"example-username\", \"example-password\")\n\n    client = Client(dispatch=MockDispatch())\n    response = await client.get(url, auth=auth)\n\n    assert \"'authorization': '[secure]'\" in str(response.request.headers)\n    request = typing.cast(Request, response.request)\n```"}, {"role": "assistant", "content": "This might prove to be bad UX for our users: `response.request` is *almost always* set, *except* for some tests where *we* create a `Response` without a request. May look into those situations and then turn on `request` as non-optional on `Response`."}], "_meta": {"repo": "encode/httpx", "pr_number": 665, "url": "https://github.com/encode/httpx/pull/665#discussion_r360601429"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_typing_extra.py`:\n\n```python\nother cases. See https://discuss.python.org/t/is-there-a-way-to-access-parent-nested-namespaces/20659.\n    \"\"\"\n    frame = sys._getframe(parent_depth)\n    # if f_back is None, it's the global module namespace and we don't need to include it here\n    if frame.f_back is None:\n    # if the class is defined at the top module level, we don't need to add namespace information\n    if frame.f_back is None or getframeinfo(frame).function == '<module>':\n```"}, {"role": "assistant", "content": "```suggestion\r\n    if frame.f_back is None or frame.f_code.co_name == '<module>':\r\n```\r\nif that works?"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10113, "url": "https://github.com/pydantic/pydantic/pull/10113#discussion_r1715648194"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_test_multiprocessing.py`:\n\n```python\n\"resource_tracker: There appear to be 1 leaked \"\n                    \"shared_memory objects to clean up at shutdown\", err)\n\n    @unittest.skipIf(os.name != \"posix\", \"resource_tracker is posix only\")\n    def test_shared_memory_untracking(self):\n        # gh-82300: When a separate Python process accesses shared memory\n        # with track=False, it must not register with the resource tracker.\n        cmd = '''if 1:\n            import sys\n            from unittest.mock import Mock\n            from multiprocessing import resource_tracker\n            from multiprocessing.shared_memory import SharedMemory\n            resource_tracker.register = Mock(side_effect=AssertionError)\n            mem = SharedMemory(create=False, name=sys.argv[1], track=False)\n            mem.close()\n        '''\n        mem = shared_memory.SharedMemory(create=True, size=10)\n        try:\n            *_, err = test.support.script_helper.assert_python_ok(\"-c\", cmd,\n                                                                  mem.name)\n            self.assertEqual(err, b'')\n        finally:\n            mem.close()\n            try:\n                mem.unlink()\n            except OSError:\n```"}, {"role": "assistant", "content": "technically you don't know why the subprocess died. it could've started the resource tracker and died for any reason (before or after registering the memory with the tracker)... This is a test suite run in CI on prerelease code... these things happen \ud83d\ude05.\r\n\r\nSo I think moving the `assert_python_ok` within the `try:` block and accepting that even `resource_tracker.unregister` may fail as well as `mem.unlink` is good defensive state cleanup coding here."}], "_meta": {"repo": "python/cpython", "pr_number": 110778, "url": "https://github.com/python/cpython/pull/110778#discussion_r1410364169"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `preview_hug_parens_with_braces_and_square_brackets.py`:\n\n```python\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], {x}, \"a string\", [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n)\n\nfoo(*[\"long long long long long line\", \"long long long long long line\", \"long long long long long line\"])\n\nfoo(*[str(i) for i in range(100000000000000000000000000000000000000000000000000000000000)])\n\nfoo(\n```"}, {"role": "assistant", "content": "Also test a dict comprehension for completeness (like the one in cache.py)."}], "_meta": {"repo": "psf/black", "pr_number": 3992, "url": "https://github.com/psf/black/pull/3992#discussion_r1375328402"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_clip.py`:\n\n```python\nTransformersKwargs,\n    auto_docstring,\n    can_return_tuple,\n    filter_out_non_signature_kwargs,\n```"}, {"role": "assistant", "content": "Seeing this so often, is it even used anymore :eyes:"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42564, "url": "https://github.com/huggingface/transformers/pull/42564#discussion_r2691850125"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `django.py`:\n\n```python\nself._close_database()\n        self._db_recycles += 1\n\n    def _is_prefork(self) -> bool:\n        if self.worker is None:\n            return False\n        pool = self.worker.pool_cls if isinstance(self.worker.pool_cls, str) else self.worker.pool_cls.__module__\n        return \"prefork\" in pool\n```"}, {"role": "assistant", "content": "The new `_is_prefork()` method lacks test coverage. Since this method introduces new logic to handle cases where `worker` is None or `pool_cls` is a string, should we add unit tests to verify:\n1. Behavior when `self.worker` is None (returns False)\n2. Behavior when `pool_cls` is a string containing \"prefork\"\n3. Behavior when `pool_cls` is a class with __module__ containing \"prefork\"\n4. Behavior when neither condition contains \"prefork\"\n\nThis is important because the method replaces inline logic that was previously tested indirectly through test_close_database_conn_pool_thread_pool."}], "_meta": {"repo": "celery/celery", "pr_number": 10045, "url": "https://github.com/celery/celery/pull/10045#discussion_r2656759499"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_utils.py`:\n\n```python\n}\n\n        # Model's definition arriving here is final (TP hooks added, quantized layers replaces)\n        expected_keys = list(model.state_dict().keys())\n        # We need parameters + buffers here, as state_dict does not count non-persistent buffers which are taking space\n        expected_keys = [name for name, _ in model.named_parameters()] + [name for name, _ in model.named_buffers()]\n```"}, {"role": "assistant", "content": "Humm, the fact here is that non-persistent buffers are NOT loaded with the other params (because they are non-persistent of course), so it is not needed to account for them when allocating memory before loading\r\nThus I believe we should revert this change"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42818, "url": "https://github.com/huggingface/transformers/pull/42818#discussion_r2620003189"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_model_construction.py`:\n\n```python\nPydanticDeprecatedSince20,\n            stacklevel=2,\n        )\n        return self.model_fields\n\n    @property\n    def model_fields(self) -> dict[str, FieldInfo]:\n        \"\"\"Get metadata about the fields defined on the model.\n\n        Returns:\n            A mapping of field names to [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n        \"\"\"\n        return getattr(self, '__pydantic_fields__', {})\n```"}, {"role": "assistant", "content": "I think it won't be possible because we want `__fields__` to have the following deprecation message:\r\n\r\n> The `__fields__` attribute is deprecated, use `model_fields` instead\r\n\r\nAnd using the decorator will \"override\" this message with\r\n\r\n> Accessing this attribute on the instance is deprecated, and will be removed in Pydantic V3."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 11169, "url": "https://github.com/pydantic/pydantic/pull/11169#discussion_r1898631648"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `config.py`:\n\n```python\ndefer_build: bool\n    \"\"\"\n    Whether to defer model validator and serializer construction until the first model validation.\n    Whether to defer model validator and serializer construction until the first model validation. Defaults to False.\n\n    This can be useful to avoid the overhead of building models which are only\n    used nested within other models, or when you want to manually define type namespace via\n    [`Model.model_rebuild(_types_namespace=...)`][pydantic.BaseModel.model_rebuild]. Defaults to False.\n    [`Model.model_rebuild(_types_namespace=...)`][pydantic.BaseModel.model_rebuild].\n\n    See also [`_defer_build_mode`][pydantic.config.ConfigDict._defer_build_mode].\n\n    !!! note\n        `defer_build` does not work by default with FastAPI Pydantic models. Meaning the validator and serializer\n        is constructed immediately when the model is used in FastAPI routes. You also need to define\n        [`_defer_build_mode=('model', 'type_adapter')`][pydantic.config.ConfigDict._defer_build_mode] with FastAPI\n        models. This parameter is required for the deferred building due to FastAPI relying on `TypeAdapter`s.\n    \"\"\"\n\n    _defer_build_mode: tuple[Literal['model', 'type_adapter'], ...]\n    \"\"\"\n    Controls when [`defer_build`][pydantic.config.ConfigDict.defer_build] is applicable. Defaults to `('model',)`.\n\n    Due to backwards compatibility reasons [`TypeAdapter`][pydantic.type_adapter.TypeAdapter] does not by default\n    respect `defer_build`. Meaning when `defer_build` is `True` and `_defer_build_mode` is the default `('model',)`\n    then `TypeAdapter` immediately constructs validator and serializer instead of postponing it until the first model\n    validation. Set this to `('model', 'type_adapter')` to make `TypeAdapter` respect the `defer_build` so it postpones\n    validator and serializer construction until the first validation.\n```"}, {"role": "assistant", "content": "What about `first serialization`? Ex, if you used `defer_build=True` and `_defer_build_mode=('model', 'type_adapter')`, but then used the `TypeAdapter` for the first time to `dump_json` or something like that.\r\n\r\nWe should probably write a test to confirm that this works, and then also update the docs accordingly!"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 8939, "url": "https://github.com/pydantic/pydantic/pull/8939#discussion_r1533138331"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `engine.py`:\n\n```python\ndef download(self, request: Request, spider: Optional[Spider] = None) -> Deferred:\n        \"\"\"Return a Deferred which fires with a Response as result, only downloader middlewares are applied\"\"\"\n        if spider is None:\n            spider = self.spider\n        else:\n        if spider is not None:\n            warnings.warn(\n                \"Passing a 'spider' argument to ExecutionEngine.download is deprecated\",\n                category=ScrapyDeprecationWarning,\n                stacklevel=2,\n            )\n            if spider is not self.spider:\n                logger.warning(\"The spider '%s' does not match the open spider\", spider.name)\n        if spider is None:\n        if self.spider is None:\n            raise RuntimeError(f\"No open spider to crawl: {request}\")\n        return self._download(request, spider).addBoth(self._downloaded, request, spider)\n        return self._download(request, spider).addBoth(self._downloaded, request, spider)  # type: ignore\n```"}, {"role": "assistant", "content": "I see\u2026\r\n\r\nIt looks like we should change the typing of `Spider` in those 2 methods to `Optional[Spider]`, instead of ignoring the typing issue.\r\n\r\nIt also seems like `_download` should get the `if spider is None: spider = self.spider` logic."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5589, "url": "https://github.com/scrapy/scrapy/pull/5589#discussion_r936925849"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `feedexport.py`:\n\n```python\nparams['time'] = utc_now.replace(microsecond=0).isoformat().replace(':', '-')\n        params['batch_time'] = utc_now.isoformat().replace(':', '-')\n        params['batch_id'] = slot.batch_id + 1 if slot is not None else 1\n        uripar_function = load_object(uri_params) if uri_params else lambda x, y: None\n        uripar_function(params, spider)\n        return params\n        uripar_function = load_object(uri_params) if uri_params else lambda x, y: x\n        new_params = uripar_function(params, spider)\n```"}, {"role": "assistant", "content": "What if we checked this? Making a shallow copy of the original `params` dict and making sure it didn't change after calling the `uripar_function`. Something like:\r\n\r\n```python\r\noriginal_params = params.copy()\r\nnew_params = uripar_function(params, spider)\r\nif new_params is None or original_params != params:\r\n   warnings.warn(...)\r\n```"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4966, "url": "https://github.com/scrapy/scrapy/pull/4966#discussion_r669587622"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `middleware.py`:\n\n```python\nimport logging\nimport pprint\nfrom collections import defaultdict, deque\nfrom typing import Callable\nfrom typing import Callable, Dict, Deque\n\nfrom twisted.internet import defer\n```"}, {"role": "assistant", "content": "What about `from twisted.internet.defer import Deferred` as you did in the first file? I would rather use `Deferred` than `defer.Deferred`. I would even add such an import to `scrapy.utils.defer`, which was already importing `defer`."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5077, "url": "https://github.com/scrapy/scrapy/pull/5077#discussion_r612576229"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `dataclasses.py`:\n\n```python\n__getattr__ = getattr_migration(__name__)\n\nif (3, 8) <= sys.version_info < (3, 11):\nif (3, 9) <= sys.version_info < (3, 11):\n```"}, {"role": "assistant", "content": "```suggestion\r\nif sys.version_info < (3, 11):\r\n```"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 11258, "url": "https://github.com/pydantic/pydantic/pull/11258#discussion_r1914932978"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `base.py`:\n\n```python\ndef __init__(self, *args, **kwargs):\n        if hasattr(self.key_t, '__func__'):  # pragma: no cover\n            self.key_t = self.key_t.__func__  # remove binding\n        self._encode_prefixes()\n        super().__init__(*args, **kwargs)\n        self._add_global_keyprefix()\n        self._encode_prefixes()\n        if self.implements_incr:\n            self.apply_chord = self._apply_chord_incr\n\n    def _add_global_keyprefix(self):\n```"}, {"role": "assistant", "content": "from my understanding, this feature needs to be documented properly"}], "_meta": {"repo": "celery/celery", "pr_number": 7620, "url": "https://github.com/celery/celery/pull/7620#discussion_r1012654253"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `compute-changes.py`:\n\n```python\nelse:\n        print(\"Branch too old for CIFuzz tests; or no C files were changed\")\n\n    if outputs.run_ci_fuzz_stdlib:\n        print(\"Run CIFuzz tests for libraries\")\n    else:\n        print(\"Branch too old for CIFuzz tests; or no library files were changed\")\n```"}, {"role": "assistant", "content": "```suggestion\r\n        print(\"Run CIFuzz tests for stdlib\")\r\n    else:\r\n        print(\"Branch too old for CIFuzz tests; or no stdlib files were changed\")\r\n```"}], "_meta": {"repo": "python/cpython", "pr_number": 143749, "url": "https://github.com/python/cpython/pull/143749#discussion_r2695927211"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `text_to_audio.py`:\n\n```python\n# generate_kwargs get priority over forward_params\n            forward_params.update(generate_kwargs)\n\n            # ensure dict output to facilitate postprocessing\n            if self.model.config.model_type not in [\"bark\", \"musicgen\"]:\n                forward_params.update({\"return_dict_in_generate\": True})\n```"}, {"role": "assistant", "content": "added this as noted in the comment: having the dictionary makes it easier to detect whether calling `self.processor.decode(audio)` is needed. \r\n\r\nSo fewer exceptions below, and only adding exceptions here for backwards compatibility \ud83d\ude42"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42326, "url": "https://github.com/huggingface/transformers/pull/42326#discussion_r2564620796"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_modeling_jais2.py`:\n\n```python\n\"\"\"Testing suite for the PyTorch Jais2 model.\"\"\"\n```"}, {"role": "assistant", "content": "thanks for suggesting, added template similar to llama"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42684, "url": "https://github.com/huggingface/transformers/pull/42684#discussion_r2610521417"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_modeling_utils.py`:\n\n```python\n_prepare_4d_attention_mask,\n        _prepare_4d_causal_attention_mask,\n    )\n    from transformers.modeling_utils import _find_disjoint, _find_identical\n    from transformers.modeling_utils import (\n        _FLASH_ATTN_KERNEL_FALLBACK,\n        _find_disjoint,\n        _find_identical,\n```"}, {"role": "assistant", "content": "You're right, I didnt notice it was just reformatting the imports here :facepalm:"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42803, "url": "https://github.com/huggingface/transformers/pull/42803#discussion_r2618598543"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `moe.py`:\n\n```python\nfinal_hidden_states = torch.zeros_like(hidden_states)\n\n    # Flatten top_k_index to get expert_ids per selected sample\n    expert_ids = top_k_index.reshape(-1)\n    expert_ids = top_k_index.view(-1)\n    token_idx = torch.arange(num_tokens, device=device).unsqueeze(1).expand(-1, num_top_k).reshape(-1)\n\n    # Resolve routing weights per selected sample, allowing top_k_weights to be either:\n    # - (num_tokens, num_top_k) Qwen2MoE style\n    # - (num_tokens, num_experts) DeepseekV2 style\n    if top_k_weights.shape == (num_tokens, num_top_k):\n        sample_weights = top_k_weights.reshape(-1)  # (S,)\n        sample_weights = top_k_weights.view(-1, 1)  # (S, 1)\n    elif top_k_weights.shape == (num_tokens, num_experts):\n        sample_weights = top_k_weights[token_idx, expert_ids]  # (S,)\n        # TODO: routers that output full expert distribution\n        # should probably be corrected to output only top_k weights\n        sample_weights = top_k_weights[token_idx, expert_ids].view(-1, 1)  # (S, 1)\n```"}, {"role": "assistant", "content": "This is from e.g. deepseek v2? Or at least before it was fixed"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43227, "url": "https://github.com/huggingface/transformers/pull/43227#discussion_r2698717901"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `tokenization_auto.py`:\n\n```python\n(\"ovis2\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n        (\"owlv2\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None),\n        (\"owlvit\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None),\n        (\"paddleocr_vl\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n```"}, {"role": "assistant", "content": "```suggestion\r\n        (\"paddleocr_vl\", \"TokenizersBackend\" if is_tokenizers_available() else None),\r\n```\r\nfrom looking at the `tokenizer.json` on the hub, its not a llama!\r\n```python\r\n\"normalizer\": {\r\n    \"type\": \"Sequence\",\r\n    \"normalizers\": [\r\n      {\r\n        \"type\": \"Replace\",\r\n        \"pattern\": {\r\n          \"String\": \" \"\r\n        },\r\n        \"content\": \"\u2581\"\r\n      }\r\n    ]\r\n  },\r\n  \"pre_tokenizer\": null,\r\n```\r\n\r\nwhile llama would initialize: \r\n```python \r\n        self._tokenizer.normalizer = None\r\n        self._tokenizer.pre_tokenizer = pre_tokenizers.Metaspace(\r\n            replacement=\"\u2581\", prepend_scheme=_get_prepend_scheme(self.add_prefix_space, self), split=False\r\n        )\r\n```"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42178, "url": "https://github.com/huggingface/transformers/pull/42178#discussion_r2610336911"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_feedexport.py`:\n\n```python\ndata = yield self.exported_data(items, settings)\n            self.assertEqual(row['expected'], data[feed_options['format']])\n\n\nclass FeedPostProcessedExportsTest(FeedExportTestBase):\n    __test__ = True\n    items = [\n        {'foo': 'bar1', 'baz': ''},\n        {'egg': 'bar2', 'baz': 'quux'},\n    ]\n```"}, {"role": "assistant", "content": "I don\u2019t _think_ multiple items and multiple fields can affect post-processing. So, to keep things minimal, maybe it would make sense to keep a single item with a single field."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5190, "url": "https://github.com/scrapy/scrapy/pull/5190#discussion_r663676587"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_type_adapter.py`:\n\n```python\nimport sys\nfrom dataclasses import dataclass\nfrom datetime import date, datetime\nfrom typing import Any, Dict, ForwardRef, Generic, List, NamedTuple, Tuple, TypeVar, Union\nfrom typing import Any, Dict, ForwardRef, Generic, List, NamedTuple, Optional, Tuple, TypeVar, Union\n\nimport pytest\nfrom pydantic_core import ValidationError\nfrom typing_extensions import Annotated, TypeAlias, TypedDict\nfrom typing_extensions import Annotated, Literal, TypeAlias, TypedDict\n\nfrom pydantic import BaseModel, TypeAdapter, ValidationInfo, field_validator\nimport pydantic\n```"}, {"role": "assistant", "content": "Let's just import pydantic's `dataclass` directly instead of doing this catchall import"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 8939, "url": "https://github.com/pydantic/pydantic/pull/8939#discussion_r1530464765"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `json_schema.py`:\n\n```python\nelse:\n                json_schema['patternProperties'] = {keys_pattern: values_schema}\n\n        if ks is not None:\n            keys_type = ks.get('type', None)\n            if keys_type == 'enum':\n                keys_members = ks.get('members', [])\n                json_schema['propertyNames'] = {'enum': keys_members}\n```"}, {"role": "assistant", "content": "> While doing it, this will also add a propertyNames schema to an existing test case. This should be expected behaviour, as python's Path in json schema of pydantic has the ' format' 'path' schema.\r\n\r\nAgreed, this change looks correct to me. I second ' request for that additional test case with constrained dict keys.\r\n\r\n> Would it then be best to add the ref to propertyNames schema, or the resolved one? I'll go with the resolved for now but am open for your inputs :)\r\n\r\nRe refs - I think the current behavior makes sense. I haven't seen strings as references in JSON schemas yet. Maybe  has?"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10478, "url": "https://github.com/pydantic/pydantic/pull/10478#discussion_r1777258849"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_multipart.py`:\n\n```python\n)\n\n\ndef get_multipart_boundary_from_content_type(\n    content_type: typing.Optional[str],\n) -> typing.Optional[bytes]:\n    if not content_type or not content_type.startswith(\"multipart/form-data\"):\n        return None\n    if \";\" in content_type:\n        for section in content_type.split(\";\"):\n            if section.strip().startswith(\"boundary=\"):\n                return section.strip().split(\"boundary=\")[-1].encode(\"latin-1\")\n```"}, {"role": "assistant", "content": "RFC 2046 has examples where boundary is contained in quotes. Do we want to support that?"}], "_meta": {"repo": "encode/httpx", "pr_number": 2278, "url": "https://github.com/encode/httpx/pull/2278#discussion_r945186942"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_utils.py`:\n\n```python\nhooks = []\n        seen_modules = set()\n        found_embeddings = False\n\n        for module in self.modules():\n            if not (isinstance(module, PreTrainedModel) and hasattr(module, \"get_input_embeddings\")):\n                continue\n\n            input_embeddings = module.get_input_embeddings()\n            try:\n                input_embeddings = module.get_input_embeddings()\n            except NotImplementedError:\n                continue\n```"}, {"role": "assistant", "content": "oke, I think with the warning below, it is more explicit"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42558, "url": "https://github.com/huggingface/transformers/pull/42558#discussion_r2592509710"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `strings.py`:\n\n```python\nimport re\nimport sys\nimport unicodedata\nfrom functools import lru_cache\nfrom typing import List, Pattern\nfrom typing import List, Pattern, Text\n```"}, {"role": "assistant", "content": "Instead of importing `typing.Text`, I replaced `Text` with `str`!"}], "_meta": {"repo": "psf/black", "pr_number": 3445, "url": "https://github.com/psf/black/pull/3445#discussion_r1051428821"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modular_nanochat.py`:\n\n```python\n# coding=utf-8\n# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\nfrom collections.abc import Callable\nfrom typing import Optional, Union\n\nimport torch\nimport torch.nn as nn\n\nfrom ...activations import ACT2FN\nfrom ...cache_utils import Cache, DynamicCache\nfrom ...generation import GenerationMixin\nfrom ...masking_utils import create_causal_mask\nfrom ...modeling_layers import GradientCheckpointingLayer\nfrom ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\nfrom ...modeling_utils import ALL_ATTENTION_FUNCTIONS\nfrom ...processing_utils import Unpack\nfrom ...utils import TransformersKwargs, auto_docstring, can_return_tuple\nfrom ...utils.generic import check_model_inputs\nfrom ..llama.modeling_llama import LlamaPreTrainedModel, LlamaRotaryEmbedding, apply_rotary_pos_emb, eager_attention_forward\nfrom ..llama4.modeling_llama4 import Llama4TextL2Norm\nfrom .configuration_nanochat import NanoChatConfig\n\n\nclass NanoChatRMSNorm(Llama4TextL2Norm):\n    \"\"\"\n    NanoChatRMSNorm inherits from Llama4TextL2Norm (weight-less RMS normalization).\n    Overrides __init__ to match NanoChat's API with hidden_size parameter.\n    \"\"\"\n```"}, {"role": "assistant", "content": "Let's not make comments like, it inherits from xxx or modular specific comments (override). We could either drop this completely or just say that it is a weightless RMSNorm."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41634, "url": "https://github.com/huggingface/transformers/pull/41634#discussion_r2444827621"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `trans.py`:\n\n```python\nset_of_prefixes.add(prefix)\n\n            if \"f\" in prefix and leaf.value[-1] != QUOTE:\n```"}, {"role": "assistant", "content": "This seems a bit too strong; perhaps we could do it only if there are any nested quotes in the string."}], "_meta": {"repo": "psf/black", "pr_number": 4498, "url": "https://github.com/psf/black/pull/4498#discussion_r1812863068"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_models.py`:\n\n```python\nHeaderTypes = typing.Union[\n    \"Headers\",\n    typing.Dict[typing.AnyStr, typing.AnyStr],\n    typing.List[typing.Tuple[typing.AnyStr, typing.AnyStr]],\n    typing.Dict[StrOrBytes, StrOrBytes],\n    typing.Sequence[typing.Tuple[StrOrBytes, StrOrBytes]],\n```"}, {"role": "assistant", "content": "Note: this change from `List` to `Sequence` was required, or mypy would raise errors here about `List` being invariant (so not working well with unions of types):\r\n\r\nhttps://github.com/encode/httpx/blob/21f533774f87292c2381376eb40f3216a48eee06/httpx/_models.py#L492"}], "_meta": {"repo": "encode/httpx", "pr_number": 882, "url": "https://github.com/encode/httpx/pull/882#discussion_r399642653"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_nanochat.py`:\n\n```python\n@torch.no_grad()\n    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n    def forward(self, x, position_ids):\n        \"\"\"\n        Returns cos and sin tensors for NanoChat's RoPE.\n\n        Unlike LlamaRotaryEmbedding which duplicates freqs to full head_dim,\n        NanoChat keeps only head_dim//2 for memory efficiency.\n        \"\"\"\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n        position_ids_expanded = position_ids[:, None, :].float()\n\n        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n            emb = torch.cat((freqs, freqs), dim=-1)\n            cos = emb.cos() * self.attention_scaling\n            sin = emb.sin() * self.attention_scaling\n\n        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n\n            # NanoChat-specific: Don't duplicate freqs - keep as head_dim//2\n            cos = freqs.cos() * self.attention_scaling\n            sin = freqs.sin() * self.attention_scaling\n\nclass NanoChatRMSNorm(torch.nn.Module):\n    \"\"\"\n    NanoChatRMSNorm inherits from Llama4TextL2Norm (weight-less RMS normalization).\n    Overrides __init__ to match NanoChat's API with hidden_size parameter.\n    \"\"\"\n        # Add extra dimension for NanoChat's broadcasting: [batch, seq_len] -> [batch, seq_len, 1, head_dim//2]\n        return cos.unsqueeze(2).to(dtype=x.dtype), sin.unsqueeze(2).to(dtype=x.dtype)\n```"}, {"role": "assistant", "content": "We later squeeze and unsqueeze again. Imo, the changes to the RoPE seems unnecessary to me :thinking:"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41634, "url": "https://github.com/huggingface/transformers/pull/41634#discussion_r2442562661"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `scraper.py`:\n\n```python\nif isinstance(output, Failure):\n            ex = output.value\n            if isinstance(ex, DropItem):\n                if ex.log_level is None:\n                    ex.log_level = spider.crawler.settings.get(\n                        \"DEFAULT_DROPITEM_LOG_LEVEL\", \"WARNING\"\n                    )\n                else:\n                    ex.log_level = ex.log_level.upper()\n```"}, {"role": "assistant", "content": "Yes, that is correct. I think I should also write a list of valid logging levels in the code and check if the log level provided by the component is valid instead of just some random string."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6608, "url": "https://github.com/scrapy/scrapy/pull/6608#discussion_r1909238895"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `base.py`:\n\n```python\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Any\n\nfrom scrapy import Request, Spider\n\nif TYPE_CHECKING:\n    from collections.abc import AsyncIterable, Iterable\n\n    from scrapy.http import Response\n\n\nclass BaseSpiderMiddleware:\n    \"\"\"Optional base class for spider middlewares.\n\n    This class provides helper methods for asynchronous ``process_spider_output``\n    methods. Middlewares that don't have a ``process_spider_output`` method don't need\n    to use it.\n    \"\"\"\n\n    def process_spider_output(\n        self, response: Response, result: Iterable[Any], spider: Spider\n    ) -> Iterable[Any]:\n        for o in result:\n            if isinstance(o, Request):\n                o = self._process_request(o, response, spider)\n                if o is not None:\n                    yield o\n```"}, {"role": "assistant", "content": "I wonder if we should use `yield from self.process_request(o, response)`, to allow yielding multiple items/requests based on a given request (or item below), and to keep `spider` off since it feels like an obsolete parameter not needed nowadays with the 1 crawler = 1 spider constraint."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6693, "url": "https://github.com/scrapy/scrapy/pull/6693#discussion_r1970545514"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `celery_executor_utils.py`:\n\n```python\nreturn Celery(celery_app_name, config_source=get_celery_configuration())\n\n\ndef create_celery_app(team_conf) -> Celery:\n```"}, {"role": "assistant", "content": "Do we know the type? If yes, it would be better to have type annotation"}], "_meta": {"repo": "apache/airflow", "pr_number": 60675, "url": "https://github.com/apache/airflow/pull/60675#discussion_r2713195448"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `scheduler_job_runner.py`:\n\n```python\nwith DebugTrace.start_span(span_name=\"emit_pool_metrics\", component=\"SchedulerJobRunner\") as span:\n            pools = Pool.slots_stats(session=session)\n            for pool_name, slot_stats in pools.items():\n                Stats.gauge(f\"pool.open_slots.{pool_name}\", slot_stats[\"open\"])\n                Stats.gauge(f\"pool.queued_slots.{pool_name}\", slot_stats[\"queued\"])\n                Stats.gauge(f\"pool.running_slots.{pool_name}\", slot_stats[\"running\"])\n                Stats.gauge(f\"pool.deferred_slots.{pool_name}\", slot_stats[\"deferred\"])\n                Stats.gauge(f\"pool.scheduled_slots.{pool_name}\", slot_stats[\"scheduled\"])\n\n                # Same metrics with tagging\n                Stats.gauge(\"pool.open_slots\", slot_stats[\"open\"], tags={\"pool_name\": pool_name})\n                Stats.gauge(\"pool.queued_slots\", slot_stats[\"queued\"], tags={\"pool_name\": pool_name})\n                Stats.gauge(\"pool.running_slots\", slot_stats[\"running\"], tags={\"pool_name\": pool_name})\n                Stats.gauge(\"pool.deferred_slots\", slot_stats[\"deferred\"], tags={\"pool_name\": pool_name})\n                Stats.gauge(\"pool.scheduled_slots\", slot_stats[\"scheduled\"], tags={\"pool_name\": pool_name})\n                # If enabled on the config, publish metrics twice,\n                # once with backward compatible name, and then with tags.\n                DualStatsManager.gauge(\n                    f\"pool.open_slots.{pool_name}\",\n                    \"pool.open_slots\",\n                    slot_stats[\"open\"],\n                    tags={},\n                    extra_tags={\"pool_name\": pool_name},\n                )\n```"}, {"role": "assistant", "content": "Don't worry about it. I'm also not done with the CI errors. Thanks!"}], "_meta": {"repo": "apache/airflow", "pr_number": 53722, "url": "https://github.com/apache/airflow/pull/53722#discussion_r2495746142"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `cookies.py`:\n\n```python\ndef __init__(self, debug=False):\n        self.jars = defaultdict(CookieJar)\n        self.debug = debug\n        self.re_cookie_header = re.compile(b';\\s*')\n```"}, {"role": "assistant", "content": "```suggestion\r\n        self.re_cookie_header = re.compile(br';\\s*')\r\n```\r\n\r\nOtherwise, I think it will result in a warning due to `\\s`.\r\n\r\nI also wonder if it would make sense to add `\\s*` before the semicolon as well."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 2400, "url": "https://github.com/scrapy/scrapy/pull/2400#discussion_r268095558"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `transform_experimental.py`:\n\n```python\n\"\"\"Experimental functionality, be prepared for this to break.\"\"\"\n\nfrom __future__ import annotations\n\nimport datetime\nimport re\nfrom collections import deque\nfrom dataclasses import dataclass\nfrom decimal import Decimal\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Annotated, Any, Callable, Generic, Protocol, TypeVar, Union, overload\n\nimport annotated_types\n\nif TYPE_CHECKING:\n    from pydantic_core import core_schema as cs\n\n    from pydantic import GetCoreSchemaHandler\n\nfrom pydantic._internal._internal_dataclass import slots_true\n\n\n@dataclass(**slots_true)\nclass _Parse:\n    tp: type[Any]\n    strict: bool = False\n\n\n@dataclass\nclass _ParseDefer:\n    func: Callable[[], type[Any]]\n\n    @cached_property\n    def tp(self) -> type[Any]:\n        return self.func()\n\n\n@dataclass(**slots_true)\nclass _Transform:\n    func: Callable[[Any], Any]\n\n\n@dataclass(**slots_true)\nclass _ValidateOr:\n    left: Validate[Any, Any]\n    right: Validate[Any, Any]\n\n\n@dataclass(**slots_true)\nclass _ValidateAnd:\n    left: Validate[Any, Any]\n    right: Validate[Any, Any]\n\n\n_ConstraintAnnotation = Union[\n    annotated_types.Le,\n    annotated_types.Ge,\n    annotated_types.Lt,\n    annotated_types.Gt,\n    annotated_types.Len,\n    annotated_types.MultipleOf,\n    annotated_types.Timezone,\n    annotated_types.Interval,\n    annotated_types.Predicate,\n    re.Pattern[str],\n]\n\n\n@dataclass(**slots_true)\nclass _Constraint:\n    constraint: _ConstraintAnnotation\n\n\n_Step = Union[_Parse, _ParseDefer, _Transform, _ValidateOr, _ValidateAnd, _Constraint]\n\n_InT = TypeVar('_InT')\n_OutT = TypeVar('_OutT')\n```"}, {"role": "assistant", "content": "Yes this is the output type, which gets passed on as the input to the next transformation."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 9459, "url": "https://github.com/pydantic/pydantic/pull/9459#discussion_r1620471326"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_python.py`:\n\n```python\nfrom airflow.models.taskinstance import TaskInstance\nfrom airflow.models.taskmap import TaskMap\nfrom airflow.providers.common.compat.sdk import AirflowException, XComNotFound\nfrom airflow.providers.common.compat.sdk import AirflowException, XComNotFound, task\n```"}, {"role": "assistant", "content": "It's the same, except task_decorator  is not used as the  decorator itself on a method, but it's referencing the decorator on which tests are being peformed.\r\n\r\nI added  from the the common compat shared module to allow decorating a python function as Airflow task for testing purposes."}], "_meta": {"repo": "apache/airflow", "pr_number": 59087, "url": "https://github.com/apache/airflow/pull/59087#discussion_r2634571187"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_compat.py`:\n\n```python\ndef set_minimum_tls_version_1_2(context: ssl.SSLContext) -> None:\n    if sys.version_info >= (3, 10):\n    if sys.version_info >= (3, 10) or (\n        sys.version_info >= (3, 7) and ssl.OPENSSL_VERSION_INFO >= (1, 1, 0, 7)\n    ):\n        # The OP_NO_SSL* and OP_NO_TLS* become deprecated in favor of\n        # 'SSLContext.minimum_version' from Python 3.7 onwards, however\n        # this attribute is not available unless the ssl module is compiled\n        # with OpenSSL 1.1.0g or newer.\n        # https://docs.python.org/3.10/library/ssl.html#ssl.SSLContext.minimum_version\n        # https://docs.python.org/3.7/library/ssl.html#ssl.SSLContext.minimum_version\n        context.minimum_version = ssl.TLSVersion.TLSv1_2\n    else:\n        # These become deprecated in favor of 'context.minimum_version'\n        # from Python 3.10 onwards.\n        context.options |= ssl.OP_NO_SSLv2\n```"}, {"role": "assistant", "content": "```suggestion\r\n        # If 'minimum_version' isn't available, we configure these options with\r\n        # the older deprecated variants.\r\n        context.options |= ssl.OP_NO_SSLv2\r\n```"}], "_meta": {"repo": "encode/httpx", "pr_number": 1714, "url": "https://github.com/encode/httpx/pull/1714#discussion_r658669795"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_aimv2.py`:\n\n```python\ndef get_input_embeddings(self) -> nn.Module:\n        return self.embeddings.patch_embed\n\n    @deprecate_kwarg(\"attention_mask\", version=\"v4.58.0\")\n    @check_model_inputs(tie_last_hidden_states=False)\n    @auto_docstring\n    def forward(\n        self,\n        pixel_values,\n```"}, {"role": "assistant", "content": "Weird that it was marked as deprecation then but don't have anything against it if it has a proper use-case :+1:"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41681, "url": "https://github.com/huggingface/transformers/pull/41681#discussion_r2439934517"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `request.py`:\n\n```python\nif referrer is None:\n        return referrer\n    return to_unicode(referrer, errors='replace')\n\n\ndef request_from_dict(d: dict, spider: Optional[Spider] = None) -> Request:\n    \"\"\"Create Request object from a dict.\n\n    If a spider is given, it will try to resolve the callbacks looking at the\n    spider for methods with the same name.\n    \"\"\"\n    request_cls = load_object(d[\"_class\"]) if \"_class\" in d else Request\n    kwargs = {key: value for key, value in d.items() if key in request_cls._attributes}\n```"}, {"role": "assistant", "content": "Since we expect Request subclasses to extend that variable as needed, maybe it should not be private."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5130, "url": "https://github.com/scrapy/scrapy/pull/5130#discussion_r625870698"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `configuration_llama4.py`:\n\n```python\nThe size (resolution) of each patch.\n        norm_eps (`float`, *optional*, defaults to 1e-05):\n            The epsilon used by the layer normalization layers.\n        vision_feature_select_strategy (`int`, *optional*, defaults to `\"default\"`): TODO\n        vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n            The feature selection strategy used to select the vision features from the vision model.\n            Should be same as in model's config. Can be one of `\"default\"` or `\"full\"`. If `\"default\"`, the CLS token is\n            removed from the vision features.\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        pixel_shuffle_ratio (`int`, *optional*, defaults to 0.5): TODO\n        projector_input_dim (`int`, *optional*, defaults to 4096): TODO\n        projector_output_dim (`int`, *optional*, defaults to 4096): TODO\n        multi_modal_projector_bias (`int`, *optional*, defaults to `False`): TODO\n        projector_dropout (`int`, *optional*, defaults to 0.0): TODO\n        attention_dropout (`int`, *optional*, defaults to 0.0): TODO\n        pixel_shuffle_ratio (`float`, *optional*, defaults to 0.5):\n            The ratio used for the pixel shuffle operation in the multi-modal projector. This controls the downsampling\n            factor applied to image patches. The actual downsampling ratio is calculated as `1 / (pixel_shuffle_ratio^2)`.\n            A value < 1.0 downsamples the visual features, reducing the number of tokens passed to the LLM.\n        projector_input_dim (`int`, *optional*, defaults to 4096):\n            Dimensionality of the input to the multi-modal projector.\n        projector_output_dim (`int`, *optional*, defaults to 4096):\n            Dimensionality of the output of the multi-modal projector.\n        multi_modal_projector_bias (`bool`, *optional*, defaults to `False`):\n            Whether to use bias in the multi-modal projector layers.\n        projector_dropout (`float`, *optional*, defaults to 0.0):\n            The dropout probability for the multi-modal projector. Increasing this value during training acts as\n            regularization to prevent overfitting.\n        attention_dropout (`float`, *optional*, defaults to 0.0):\n            The dropout ratio for the attention probabilities. Similar to projector_dropout, increasing this value during\n            training acts as regularization.\n```"}, {"role": "assistant", "content": "```suggestion\n        pixel_shuffle_ratio (`float`, *optional*, defaults to 0.5):\n            Pixel-shuffle ratio for downsampling patch tokens. Smaller values produce fewer tokens (more downsampling).\n        projector_input_dim (`int`, *optional*, defaults to 4096):\n            Width of the vision adapter MLP before pixel shuffle. Larger value increases capacity and compute.\n        projector_output_dim (`int`, *optional*, defaults to 4096):\n            Output width of the vision adapter. Larger value yields higher-dimensional image features.\n        multi_modal_projector_bias (`bool`, *optional*, defaults to `False`):\n            Whether to use bias in the multi-modal projector layers.\n        projector_dropout (`float`, *optional*, defaults to 0.0):\n            Dropout rate inside the vision adapter MLP. Higher value adds more regularization.\n        attention_dropout (`float`, *optional*, defaults to 0.0):\n            Dropout rate on vision attention probabilities. Higher value adds more regularization.\n```"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43460, "url": "https://github.com/huggingface/transformers/pull/43460#discussion_r2728377765"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_client.py`:\n\n```python\nproxy_map = self.get_proxy_map(proxies, trust_env)\n\n        self.dispatch = self.init_dispatch(\n        if dispatch is not None:\n            warnings.warn(\n```"}, {"role": "assistant", "content": "I'm tempted to follow up with a PR that adds a small `warn_deprecated()` util that we can use in cases like this, to not forget the `DeprecationWarning` :)"}], "_meta": {"repo": "encode/httpx", "pr_number": 963, "url": "https://github.com/encode/httpx/pull/963#discussion_r428502778"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_utils.py`:\n\n```python\nif ignore_unexpected_regex is not None:\n            unexpected_keys = {key for key in unexpected_keys if ignore_unexpected_regex.search(key) is None}\n\n        # Note: only the unexpected keys should remove the added prefix here, to correctly display the original name\n        # in the warnings. For missing keys, we should show the prefix in the warning as it's part of the final model\n        if loading_task_model_from_base_state_dict:\n            _prefix = f\"{self.base_model_prefix}.\"\n            unexpected_keys = {k.removeprefix(_prefix) for k in unexpected_keys}\n        # Set the flag (very important to avoid initializing them!!)\n        for tied_param in self._tied_weights_keys.keys():\n```"}, {"role": "assistant", "content": "only if `tie_words_embeddings` or `tie_encoder_decoder`"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42191, "url": "https://github.com/huggingface/transformers/pull/42191#discussion_r2527425694"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `main.py`:\n\n```python\nThis replaces `Model.__fields__` from Pydantic V1.\n    \"\"\"\n\n    __pydantic_setattr_handlers__: ClassVar[Dict[str, Callable[[BaseModel, Any], None]]]  # noqa: UP006\n    \"\"\"__setattr__ handlers. Used to speed up __setattr__.\"\"\"\n```"}, {"role": "assistant", "content": "I think it's worth a bit more strength/detail in this comment, e.g.:\r\n```suggestion\r\n    \"\"\"__setattr__ handlers. Memoizing the setattr handlers leads to a dramatic performance improvement in `__setattr__`\"\"\"\r\n```\r\n(that suggestion might need reformatting.)"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10868, "url": "https://github.com/pydantic/pydantic/pull/10868#discussion_r1847168578"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `chat.py`:\n\n```python\ndef check_health(url):\n        health_url = urljoin(url + \"/\", \"health\")\n        try:\n            output = httpx.get(health_url)\n            output = session.get(health_url)\n```"}, {"role": "assistant", "content": "I'd rather use `httpx` directly here since we are fetching a local endpoint (`get_session` is meant mostly for calling the Hub or at least \"the internet\")."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42845, "url": "https://github.com/huggingface/transformers/pull/42845#discussion_r2664612277"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_funcattrs.py`:\n\n```python\n# XXX Not sure where this should really go since I can't find a\n    # test module specifically for builtin_function_or_method.\n\n    def test_builtin__module__(self):\n        import decimal\n        import math\n\n        # builtin function:\n        self.assertEqual(len.__module__, 'builtins')\n        self.assertEqual(math.sin.__module__, 'math')\n\n        # instance method:\n        self.assertRaises(AttributeError, getattr, int.to_bytes, '__module__')\n        self.assertRaises(AttributeError, getattr, decimal.Decimal.exp, '__module__')\n        self.assertEqual(int.to_bytes.__objclass__.__module__, 'builtins')\n        self.assertEqual(decimal.Decimal.exp.__objclass__.__module__, 'decimal')\n\n        # builtin classmethod:\n        self.assertEqual(int.from_bytes.__module__, None)\n        self.assertEqual(int.from_bytes.__self__.__module__, 'builtins')\n```"}, {"role": "assistant", "content": "Note that for classmethods the ``__module__`` attribute is also empty.  We could extend proposed solution to cover this case as well.  But this looks as a less severe issue for me: the module value is available in this case via the ``__self__`` attribute."}], "_meta": {"repo": "python/cpython", "pr_number": 115232, "url": "https://github.com/python/cpython/pull/115232#discussion_r1484942697"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `main.py`:\n\n```python\n__tracebackhide__ = True\n        return cls.__pydantic_validator__.validate_strings(obj, strict=strict, context=context)\n\n    @classmethod\n    def __get_pydantic_core_schema__(cls, source: type[BaseModel], handler: GetCoreSchemaHandler, /) -> CoreSchema:\n```"}, {"role": "assistant", "content": "Ah interesting. Will give this another review then. Didn't realize users could still implement support. Nice!"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10863, "url": "https://github.com/pydantic/pydantic/pull/10863#discussion_r1848312831"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `kubernetes_helper_functions.py`:\n\n```python\n# Compat: Look up the run_id from the TI table!\n    from airflow.models.dagrun import DagRun\n    from airflow.models.taskinstance import TaskInstance, TaskInstanceKey\n    from airflow.models.taskinstance import TaskInstance\n    from airflow.models.taskinstancekey import TaskInstanceKey\n```"}, {"role": "assistant", "content": "So I just went ahead and consumed from the canonical path: `airflow.models.taskinstancekey` instead of some re-export"}], "_meta": {"repo": "apache/airflow", "pr_number": 60776, "url": "https://github.com/apache/airflow/pull/60776#discussion_r2719618903"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `fields.py`:\n\n```python\ntry:\n                json_schema_extra = attributes_set.pop('json_schema_extra')\n                existing_json_schema_extra = merged_field_info_kwargs.get('json_schema_extra', {})\n                existing_json_schema_extra = merged_field_info_kwargs.get('json_schema_extra')\n\n                if isinstance(existing_json_schema_extra, dict) and isinstance(json_schema_extra, dict):\n                    merged_field_info_kwargs['json_schema_extra'] = {**existing_json_schema_extra, **json_schema_extra}\n                else:\n                if existing_json_schema_extra is None:\n                    merged_field_info_kwargs['json_schema_extra'] = json_schema_extra\n                if isinstance(existing_json_schema_extra, dict):\n                    if isinstance(json_schema_extra, dict):\n                        merged_field_info_kwargs['json_schema_extra'] = {\n                            **existing_json_schema_extra,\n                            **json_schema_extra,\n                        }\n                    if isinstance(json_schema_extra, Callable):\n```"}, {"role": "assistant", "content": "```suggestion\r\n                    if callable(json_schema_extra):\r\n```"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10675, "url": "https://github.com/pydantic/pydantic/pull/10675#discussion_r1815170471"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_client.py`:\n\n```python\n\"\"\"\n        location = response.headers[\"Location\"]\n\n        url = URL(location)\n        try:\n            url = URL(location)\n        except InvalidURL as exc:\n            raise RemoteProtocolError(\n                f\"Invalid URL in location header: {exc}.\", request=request\n            )\n```"}, {"role": "assistant", "content": "Let's use `from None` here - we don't need to expose the extra traceback info."}], "_meta": {"repo": "encode/httpx", "pr_number": 1163, "url": "https://github.com/encode/httpx/pull/1163#discussion_r468368877"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_generate_schema.py`:\n\n```python\nref = get_ref(schema)\n\n        if ref:\n            self.defs.definitions[ref] = schema\n            return core_schema.definition_reference_schema(ref)\n            return self.defs.create_reference_to_schema(schema)\n\n        # TODO\n        # if schema['type'] == 'definition-ref':\n        #     return core_schema.definition_reference_schema(schema_ref=schema['schema_ref'])\n```"}, {"role": "assistant", "content": "I'm removing it from this PR, as if it needs to be included, this will be in the schema walking refactor PR."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 11208, "url": "https://github.com/pydantic/pydantic/pull/11208#discussion_r1905805335"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_pipeline_images.py`:\n\n```python\nskip_pillow = None\n\n\ndef _mocked_download_func(request, info):\n    response = request.meta.get('response')\n    return response() if callable(response) else response\n\n\nclass ImagesPipelineTestCase(unittest.TestCase):\n\n    skip = skip_pillow\n\n    def setUp(self):\n        self.tempdir = mkdtemp()\n        self.pipeline = ImagesPipeline(self.tempdir, download_func=_mocked_download_func)\n```"}, {"role": "assistant", "content": "It is unused. I tried raising an exception from the download function, and the tests passed nonetheless, indicating that it is not actually used."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5798, "url": "https://github.com/scrapy/scrapy/pull/5798#discussion_r1081686136"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `feedexport.py`:\n\n```python\nself.storages = self._load_components('FEED_STORAGES')\n        self.exporters = self._load_components('FEED_EXPORTERS')\n        self.storage_batch_item_count = self.settings.getint('FEED_STORAGE_BATCH_ITEM_COUNT')\n```"}, {"role": "assistant", "content": "Instead of doing this here, please update `scrapy.utils.conf.feed_complete_default_values_from_settings`. That would simplify the `if` at line 339."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4434, "url": "https://github.com/scrapy/scrapy/pull/4434#discussion_r446725481"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `main.py`:\n\n```python\nraise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')\n\n        def __setattr__(self, name: str, value: Any) -> None:\n            if name in self.__class_vars__:\n            if (fast_memo_handler := self.__pydantic_setattr_handlers__.get(name)) is not None:\n                fast_memo_handler(self, value)\n```"}, {"role": "assistant", "content": "I think the word `fast` in the name of this variable is carrying some weight in terms of documenting what is going on here \u2014\u00a0it doesn't seem to be indicated explicitly that the reason for memoizing the attribute handler is purely for performance and isn't otherwise logically required. I mean, I know we have the docstring of `__pydantic_setattr_handlers__`, but that is far away from the implementing code and still doesn't make it clear that the current logic _only_ makes use of the memoization for performance reasons (and isn't otherwise dependent on the memoization), which may be useful context if we refactor this in the future.\r\n\r\nI don't really mind the word fast in the name, but I think either way we should add a comment here reiterating that the memoization is purely for performance reasons. And if we do, the word fast probably becomes less useful."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10868, "url": "https://github.com/pydantic/pydantic/pull/10868#discussion_r1847174753"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `encoders.py`:\n\n```python\nif isinstance(obj, BaseModel):\n        encoder = getattr(obj.__config__, \"json_encoders\", {})\n        if custom_encoder:\n            encoder.update(custom_encoder)\n            encoder = {**encoder, **custom_encoder}\n```"}, {"role": "assistant", "content": "The following snippet from slackner illustrates the side effect in 2 cases:\r\n\r\n- With `print(c)` : side effect from `b = jsonable_encoder(creds, custom_encoder=ENCODERS)` when calling `jsonable_encoder` later on the same instance but with a different custom_encoder\r\n- With `print(d)` : side effect from `b = jsonable_encoder(creds, custom_encoder=ENCODERS)` when calling `jsonable_encoder` later on another instance\r\n\r\n```python\r\nfrom pydantic import BaseModel, SecretStr\r\nfrom fastapi.encoders import jsonable_encoder\r\n\r\n\r\nclass Credentials(BaseModel):\r\n    password: SecretStr\r\n\r\n\r\nENCODERS = {SecretStr: lambda v: v.get_secret_value() if v is not None else None}\r\n\r\ncreds = Credentials(password=\"helloworld\")\r\n\r\na = jsonable_encoder(creds)\r\nprint(a)  # {'password': '**********'}, as expected\r\nb = jsonable_encoder(creds, custom_encoder=ENCODERS)\r\nprint(b)  # {'password': 'helloworld'}, as expected\r\nc = jsonable_encoder(creds)\r\nprint(c)  # gives {'password': 'helloworld'}, but should be {'password': '**********'}?\r\n\r\n\r\ncreds = Credentials(password=\"123456789\")\r\n\r\nd = jsonable_encoder(creds)\r\nprint(d)  # gives {'password': '123456789'}, but should be {'password': '**********'}?\r\n```\r\n\r\nBefore this PR, calling jsonable_encoder with a custom_encoder updates the global \"json_encoders\" config of the model.\r\nAfter this PR, calling jsonable_encoder with a custom_encoer argument only affects the current call"}], "_meta": {"repo": "fastapi/fastapi", "pr_number": 4972, "url": "https://github.com/fastapi/fastapi/pull/4972#discussion_r892603230"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_multipart.py`:\n\n```python\ndef __init__(self, name: str, value: typing.Union[str, bytes]) -> None:\n        if not isinstance(name, str):\n            raise TypeError(\"Invalid type for name. Expected str.\")\n            raise TypeError(f\"Invalid type for name. Expected str, \"\n                            f\"got {type(name).__name__}: {name!r}\")\n        if not isinstance(value, (str, bytes)):\n            raise TypeError(\"Invalid type for value. Expected str or bytes.\")\n            raise TypeError(f\"Invalid type for value. Expected str or bytes, \"\n                            f\"got {type(value).__name__}: {value!r}\")\n```"}, {"role": "assistant", "content": "```suggestion\r\n            raise TypeError(\r\n                f\"Invalid type for value. Expected str or bytes, got {type(value)}: {value!r}\"\r\n            )\r\n```"}], "_meta": {"repo": "encode/httpx", "pr_number": 1368, "url": "https://github.com/encode/httpx/pull/1368#discussion_r511991985"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `referer.py`:\n\n```python\ntry:\n        return cast(Type[ReferrerPolicy], load_object(policy))\n    except ValueError:\n        try:\n            return _policy_classes[policy.lower()]\n        except KeyError:\n            msg = f\"Could not load referrer policy {policy!r}\"\n            if not warning_only:\n                raise RuntimeError(msg)\n            else:\n                warnings.warn(msg, RuntimeWarning)\n                return None\n        tokens = [token.strip() for token in policy.lower().split(\",\")]\n        for token in tokens[::-1]:\n```"}, {"role": "assistant", "content": "For future reference, it was not obvious to me why we are picking the last known policy instead of the first one.\r\n\r\n```suggestion\r\n        # https://www.w3.org/TR/referrer-policy/#parse-referrer-policy-from-header\r\n        for token in tokens[::-1]:\r\n```"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6381, "url": "https://github.com/scrapy/scrapy/pull/6381#discussion_r1618557742"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_client.py`:\n\n```python\nURLs.\n    * **timeout** - *(optional)* The timeout configuration to use when sending\n    requests.\n    * **pool_limits** - *(optional)* The connection pool configuration to use\n    when determining the maximum number of concurrently open HTTP connections.\n    * **limits** - *(optional)* The limits configuration to use.\n```"}, {"role": "assistant", "content": "I guess we could say something like... \"Allows for configuration options, such as the maximum number of open connections to allow.\" Just to give a bit of an indication what we mean by \"limits\" here(?)"}], "_meta": {"repo": "encode/httpx", "pr_number": 1113, "url": "https://github.com/encode/httpx/pull/1113#discussion_r463990298"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_downloader_handler_twisted_ftp.py`:\n\n```python\nself, server_url: str, dh: FTPDownloadHandler\n    ) -> None:\n        request = Request(url=server_url + \"nonexistent.txt\", meta=self.req_meta)\n        r = await self.download_request(dh, request)\n        r = await dh.download_request(request)\n        assert r.status == 404\n        assert r.body == b\"['550 nonexistent.txt: No such file or directory.']\"\n```"}, {"role": "assistant", "content": "The assertion for response body on line 104 was added but there is no corresponding test to verify this assertion fails when the body is incorrect. Consider adding a test case that verifies different error message formats."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 7164, "url": "https://github.com/scrapy/scrapy/pull/7164#discussion_r2623560964"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `__init__.py`:\n\n```python\nreturn methods\n\n    def extract_contracts(self, method):\n        contracts = []\n    def extract_contracts(self, method: Callable) -> List[Contract]:\n        contracts: List[Contract] = []\n        assert method.__doc__ is not None\n        for line in method.__doc__.split(\"\\n\"):\n            line = line.strip()\n\n            if line.startswith(\"@\"):\n                name, args = re.match(r\"@(\\w+)\\s*(.*)\", line).groups()\n                m = re.match(r\"@(\\w+)\\s*(.*)\", line)\n                assert m is not None\n```"}, {"role": "assistant", "content": "As I said in #6383, \"I think we should just skip lines that don't match.\""}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6371, "url": "https://github.com/scrapy/scrapy/pull/6371#discussion_r1624297078"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_model_validator.py`:\n\n```python\nModel.model_validate({'inner': {'inner': {'inner': None}}})\n    assert calls == ['before'] * 3 + ['after'] * 3\n    calls.clear()\n\n\ndef test_after_validator_wrong_signature() -> None:\n    with pytest.raises(PydanticUserError):\n\n        class Model(BaseModel):\n            @model_validator(mode='after')\n            # This used to be converted into a classmethod, resulting\n            # in this inconsistent signature still accepted:\n            def validator(cls, model, info): ...\n```"}, {"role": "assistant", "content": "This is wrapped in a `pytest.raises()` context manager, and was meant to test that this particular invalid signature is now rejected."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 11957, "url": "https://github.com/pydantic/pydantic/pull/11957#discussion_r2417108247"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modular_sam2.py`:\n\n```python\nlogger = logging.get_logger(__name__)\n\n\n@dataclass\n@auto_docstring\nclass BaseModelOutputWithFeatureMaps(BaseModelOutputWithPooling):\n    \"\"\"\n    feature_maps (`list[torch.Tensor]`):\n        List of feature maps from different layers of the model.\n    feature_maps_position_embeddings (`list[torch.Tensor]`):\n        List of position embeddings corresponding to the feature maps.\n    \"\"\"\n\n    feature_maps: list[torch.Tensor] | None = None\n```"}, {"role": "assistant", "content": "Well spotted! Removed in e480bc0e8fde90b578c3d1c2ff0a87698caf1cc8"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42564, "url": "https://github.com/huggingface/transformers/pull/42564#discussion_r2682927355"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `functional_validators.py`:\n\n```python\n@dataclasses.dataclass(frozen=True, **_internal_dataclass.slots_true)\nclass AfterValidator:\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.10/concepts/validators/#annotated-validators\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.10/concepts/validators/#field-validators\n```"}, {"role": "assistant", "content": "We have a plugin that enforces linking between these sections, and it needs the anchor to be a heading."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 11060, "url": "https://github.com/pydantic/pydantic/pull/11060#discussion_r1881899565"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_client.py`:\n\n```python\nKEEPALIVE_EXPIRY = 5.0\n\n\ndef check_not_closed(method: typing.Callable) -> typing.Callable:\n```"}, {"role": "assistant", "content": "I've removed the decorator. Simple is better than complex."}], "_meta": {"repo": "encode/httpx", "pr_number": 1197, "url": "https://github.com/encode/httpx/pull/1197#discussion_r472963455"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `connection.py`:\n\n```python\nhttp_version = stream.get_http_version()\n        logger.trace(f\"connected http_version={http_version!r}\")\n\n        if http_version == \"HTTP/2\":\n            self.h2_connection = HTTP2Connection(\n                stream, backend=self.backend, on_release=on_release\n            )\n        else:\n            assert http_version == \"HTTP/1.1\"\n            self.h11_connection = HTTP11Connection(stream, on_release=on_release)\n        self.set_open_connection(http_version, socket=stream, on_release=on_release)\n```"}, {"role": "assistant", "content": "Note that we've got an outstanding `stream` vs `socket` instance here (following up #601). Wondering if should push the renaming further with `.open_tcp_socket()` and `.open_uds_socket()` on the `ConcurrencyBackend` interface?"}], "_meta": {"repo": "encode/httpx", "pr_number": 616, "url": "https://github.com/encode/httpx/pull/616#discussion_r355125412"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `griffe_doclinks.py`:\n\n```python\n)\n\n\ndef update_docstrings_recursively(obj: GriffeObject) -> None:\n    if obj.docstring:\n        obj.docstring.value = update_docstring(obj)\n    for member in obj.members.values():\n        if not member.is_alias:\n            update_docstrings_recursively(member)\n\n\nclass Extension(VisitorExtension):\n    def visit_module(self, node: ast.AST) -> None:\n        module = self.visitor.current.module\n        update_docstrings_recursively(module)\nclass UpdateDocstringsExtension(Extension):\n    def on_instance(self, *, node: ast.AST | ObjectNode, obj: GriffeObject) -> None:\n```"}, {"role": "assistant", "content": "Sorry, upcoming v1 will pass an additional `agent` argument, and parent signatures also declare `**kwargs` so that downstream subclasses declare it too. It means you can remove unused params since they'll be swallowed by kwargs :)\n\nI'll let you check the changelog. I can also send PRs to update everything for upcoming v1! Just tell me ^^"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10084, "url": "https://github.com/pydantic/pydantic/pull/10084#discussion_r1718894422"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `finegrained_fp8.py`:\n\n```python\nassert len(block_size) == 2\n    block_n, block_k = block_size[0], block_size[1]\n\n    # if we have per-tensor quantization, we use 128x128 block size for tiled matmul multiplication\n    if block_n == B.shape[-2] and block_k == B.shape[-1]:\n        block_n = 128\n        block_k = 128\n```"}, {"role": "assistant", "content": "it doesn't make sense before to set blocks to something else than None when doing per tensor in the FP8Linear. Can we change that so that we fix it here also ?"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42560, "url": "https://github.com/huggingface/transformers/pull/42560#discussion_r2581709212"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `configuration_audioflamingo3.py`:\n\n```python\n\"\"\"AudioFlamingo3 model configuration\"\"\"\n\nfrom ...configuration_utils import PretrainedConfig\nfrom ...utils import logging\nfrom ..auto import CONFIG_MAPPING, AutoConfig\n\n\nlogger = logging.get_logger(__name__)\n\n\n# Model Constants\nIGNORE_INDEX = -100\n\nMEDIA_TOKENS = {\"sound\": \"<sound>\"}\n\nNUM_EXTRA_TOKENS = 10\n\n\nclass LlavaConfig(PretrainedConfig):\n    model_type = \"llava_llama\"\n\n    def __init__(\n        self,\n        llm_cfg=None,\n        sound_tower_cfg=None,\n        sound_mm_projector_cfg=None,\n        architectures=None,\n        resume_path=None,\n        hidden_size=None,\n        sound_hidden_size=None,\n        sound_encoder: str = '{\"_target_\": \"llava.encoders.BasicSoundEncoder\"}',\n```"}, {"role": "assistant", "content": "this seems to be an audio tokenizer? If so, let's remove and this will be handled within the processor"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 40290, "url": "https://github.com/huggingface/transformers/pull/40290#discussion_r2294200934"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `init_wsgi_middlewares.py`:\n\n```python\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nfrom airflow.configuration import conf\nfrom airflow.exceptions import AirflowConfigException\nfrom werkzeug.middleware.proxy_fix import ProxyFix\n\nif TYPE_CHECKING:\n    from flask import Flask\n\n\ndef init_wsgi_middleware(flask_app: Flask) -> None:\n    \"\"\"Handle X-Forwarded-* headers and base_url support.\"\"\"\n\n    # Apply ProxyFix middleware\n    if conf.getboolean(\"webserver\", \"ENABLE_PROXY_FIX\"):\n```"}, {"role": "assistant", "content": "We removed this config because it was no longer used. Please update `airflow-core/src/airflow/cli/commands/config_command.py` as well to no longer mark this config as removed."}], "_meta": {"repo": "apache/airflow", "pr_number": 49942, "url": "https://github.com/apache/airflow/pull/49942#discussion_r2065920671"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `sdpa_attention.py`:\n\n```python\n#   - npu is not supported gqa currently\n    if _is_torch_xpu_available:\n        return _is_torch_greater_or_equal_than_2_8 and not isinstance(key, torch.fx.Proxy)\n    if _is_torch_npu_available:\n        return False\n```"}, {"role": "assistant", "content": "Just to be 100% sure so we have the same conditions as in base cuda torch? Can update the comments above accordingly then"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41601, "url": "https://github.com/huggingface/transformers/pull/41601#discussion_r2432076727"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `auto_docstring.py`:\n\n```python\n}\n\n\nclass ProcessorArgs:\n    # __init__ arguments\n    image_processor = {\n        \"description\": \"\"\"\n    The image processor is a required input.\n    \"\"\",\n        \"type\": \"{image_processor_class}\",\n    }\n\n    tokenizer = {\n        \"description\": \"\"\"\n    The tokenizer is a required input.\n    \"\"\",\n        \"type\": \"{tokenizer_class}\",\n    }\n\n    video_processor = {\n        \"description\": \"\"\"\n    The video processor is a required input.\n    \"\"\",\n        \"type\": \"{video_processor_class}\",\n    }\n\n    audio_processor = {\n        \"description\": \"\"\"\n    The audio processor is a required input.\n    \"\"\",\n        \"type\": \"{audio_processor_class}\",\n    }\n\n    feature_extractor = {\n        \"description\": \"\"\"\n    The feature extractor is a required input.\n    \"\"\",\n        \"type\": \"{feature_extractor_class}\",\n    }\n\n    chat_template = {\n        \"description\": \"\"\"\n    A Jinja template which will be used to convert lists of messages in a chat into a tokenizable string.\n    \"\"\",\n        \"type\": \"str\",\n    }\n\n    # __call__ arguments\n    text = {\n        \"description\": \"\"\"\n    The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n    (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n    `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n    \"\"\",\n    }\n\n    audio = {\n```"}, {"role": "assistant", "content": "I think audios is deprecated but still present in some places"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42101, "url": "https://github.com/huggingface/transformers/pull/42101#discussion_r2669994786"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `module_common.py`:\n\n```python\ndef _apply_action_arg_defaults(action: str, task: Task, action_args: dict[str, t.Any], templar: Templar) -> dict[str, t.Any]:\n    args = _get_action_arg_defaults(action, task, templar._engine)\n    args = templar.template(_get_action_arg_defaults(action, task, templar._engine))\n```"}, {"role": "assistant", "content": "```suggestion\r\n    # Perform templating for action plugins.\r\n    args = templar.template(_get_action_arg_defaults(action, task, templar._engine))\r\n```"}], "_meta": {"repo": "ansible/ansible", "pr_number": 85857, "url": "https://github.com/ansible/ansible/pull/85857#discussion_r2665395885"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `engine.py`:\n\n```python\nreturn dfd\n\n    def _close_all_spiders(self):\n        dfds = [self.close_spider(s, reason='shutdown') for s in self.open_spiders]\n        dlist = defer.DeferredList(dfds)\n        return dlist\n    @property\n    def open_spiders(self) -> list:\n        warnings.warn(\n            \"ExecutionEngine.open_spiders is deprecated, please use ExecutionEngine.spider instead\",\n            category=ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        return [self.spider] if self.spider is not None else []\n\n    @defer.inlineCallbacks\n    def _finish_stopping_engine(self):\n        yield self.signals.send_catch_log_deferred(signal=signals.engine_stopped)\n        self._closewait.callback(None)\n    def has_capacity(self) -> bool:\n        warnings.warn(\"ExecutionEngine.has_capacity is deprecated\", ScrapyDeprecationWarning, stacklevel=2)\n        return not bool(self.slot)\n\n    def schedule(self, request: Request, spider: Spider) -> None:\n        warnings.warn(\n            \"ExecutionEngine.schedule is deprecated, please use \"\n            \"ExecutionEngine.crawl or ExecutionEngine.download instead\",\n            category=ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        if self.slot is None:\n            raise RuntimeError(\"Engine slot not assigned\")\n        self._schedule_request(request)\n```"}, {"role": "assistant", "content": "A potential issue I see here and in public methods where the `spider` argument is deprecated is that there is a slight behavior change that, while unlikely to affect many, if any, still happens without a deprecation period: we are just assuming that the `spider` argument, if passed, is the same as `self.spider`. And while that is what _should_ be happening, code-wise we cannot be sure of that.\r\n\r\nSo I\u2019m thinking that, to deprecate things properly, we may need to add a check specifically for that, and have things work as before in those cases (i.e. pass the `spider` argument forward to subsequent calls, instead of letting `self.spider` be used) but log a deprecation warning about it."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5090, "url": "https://github.com/scrapy/scrapy/pull/5090#discussion_r611596888"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `awq.py`:\n\n```python\nfor module_name, module in model.named_modules():\n        if not should_convert_module(module_name, modules_to_not_convert):\n            continue\n        with init_empty_weights():\n        with torch.device(\"meta\"):\n```"}, {"role": "assistant", "content": "cc  for this change, do you know if this switch on all the quantizers's `replace_with_xxx` is fine? Basically, the only difference is if the quantized layer registers some buffers, they would now be on meta as well. I checked for bnb and there it seems to be alright at least (no buffers)"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42941, "url": "https://github.com/huggingface/transformers/pull/42941#discussion_r2634836232"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `convert_gpt_oss_weights_to_hf.py`:\n\n```python\nconverter = GptOssConverter(\n        vocab_file=tokenizer_path,\n        model_max_length=None,\n        chat_template=chat_template if instruct else None,\n        chat_template=chat_template,\n    )\n    tokenizer = converter.tokenizer\n    tokenizer.save_pretrained(save_dir)\n\n    if instruct:\n        print(\"Saving chat template...\")\n        chat_template_path = os.path.join(save_dir, \"chat_template.json\")\n        with open(chat_template_path, \"w\") as f:\n            json.dump({\"chat_template\": chat_template}, f, indent=2)\n    print(\"Saving chat template...\")\n    chat_template_path = os.path.join(save_dir, \"chat_template.jinja\")\n    with open(chat_template_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(chat_template)\n```"}, {"role": "assistant", "content": "I think this block still runs for non-instruct models, right? Is that intended? Those models won't support chat in the standard chat template format because they're still just pure causal LMs without a post-training phase"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42901, "url": "https://github.com/huggingface/transformers/pull/42901#discussion_r2728023795"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `setup.py`:\n\n```python\n'Twisted>=17.9.0',\n        'cryptography>=2.0',\n        'cssselect>=0.9.1',\n        'itemloaders>=1.0.0',\n```"}, {"role": "assistant", "content": "If itemloaders require e.g. parsel 1.6, and scrapy requires 1.5, and now scrapy requires itemloaders, than we're implicitly bumping minimal supported parsel version in scrapy. \r\n\r\nThese versions should either agree, or itemloaders version should be lower."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4516, "url": "https://github.com/scrapy/scrapy/pull/4516#discussion_r427831096"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_urlparse.py`:\n\n```python\n)\n\n\ndef quote(string: str, safe: str = \"/\") -> str:\n    \"\"\"\n    Use percent-encoding to quote a string, omitting existing '%xx' escape sequences.\n    \"\"\"\n    parts = []\n    current_position = 0\n    for match in re.finditer(PERCENT_ENCODED_REGEX, string):\n        start_position, end_position = match.start(), match.end()\n        matched_text = match.group(0)\n        # Add any text up to the '%xx' escape sequence.\n        if start_position != current_position:\n            leading_text = string[current_position:start_position]\n            parts.append(percent_encoded(leading_text, safe=safe))\n\n        # Add the '%xx' escape sequence.\n        parts.append(matched_text)\n        current_position = end_position\n\n    # Add any text after the final '%xx' escape sequence.\n    if current_position != len(string):\n        trailing_text = string[current_position:]\n        parts.append(percent_encoded(trailing_text, safe=safe))\n\n    return \"\".join(parts)\n```"}, {"role": "assistant", "content": "`parts` could be string. `parts +=` instead of `parts.append`"}], "_meta": {"repo": "encode/httpx", "pr_number": 2990, "url": "https://github.com/encode/httpx/pull/2990#discussion_r1421694231"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `conf.py`:\n\n```python\n\"\"\"Fail if a value in the components dict is not a real number or None.\"\"\"\n        for name, value in compdict.items():\n            if value is not None and not isinstance(value, numbers.Real):\n                raise ValueError('Invalid value {} for component {}, please provide '\n                                 'a real number or None instead'.format(value, name))\n                raise ValueError(f'Invalid value {value} for component {name}, '\n                                 'please provide a real number or None instead'\n                                 )\n```"}, {"role": "assistant", "content": ":lipstick: \r\n\r\n```suggestion\r\n                raise ValueError(f'Invalid value {value} for component {name}, '\r\n                                 'please provide a real number or None instead')\r\n```"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4324, "url": "https://github.com/scrapy/scrapy/pull/4324#discussion_r477165383"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `canvas.py`:\n\n```python\ndef _apply_tasks(self, tasks, producer=None, app=None, p=None,\n                     add_to_parent=None, chord=None,\n                     args=None, kwargs=None, **options):\n                     args=None, kwargs=None, group_index=None, **options):\n```"}, {"role": "assistant", "content": "Adding the `group_index` parameter appears to be a signature change to the `_apply_tasks` method. Is this intended to be a breaking change, or should backward compatibility be maintained? If this is a private method (indicated by the underscore prefix), the change may be acceptable, but it would be helpful to understand if any internal callers need updates.\n```suggestion\n                     args=None, kwargs=None, **options, group_index=None):\n```"}], "_meta": {"repo": "celery/celery", "pr_number": 9910, "url": "https://github.com/celery/celery/pull/9910#discussion_r2390871908"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `serialization.py`:\n\n```python\ndef _pack(self, body, content_type, content_encoding, signer, signature,\n              sep=str_to_bytes('\\x00\\x01')):\n        fields = sep.join(\n            ensure_bytes(s) for s in [signer, signature, content_type,\n                                      content_encoding, body]\n            ensure_bytes(s) for s in [b64encode(signer), b64encode(signature),\n                                      content_type, content_encoding, body]\n        )\n        return b64encode(fields)\n\n    def _unpack(self, payload, sep=str_to_bytes('\\x00\\x01')):\n        raw_payload = b64decode(ensure_bytes(payload))\n        first_sep = raw_payload.find(sep)\n```"}, {"role": "assistant", "content": "Sure  . This is the logic that we have today for unpacking the serialized data:\r\n1. extract signer and `signature_start_index` using first appearance of `sep`\r\n2. get signature bytes according to `signature_start_index` and signature length. Note, that we assume. here that the signature is decoded as-is in the raw_payload, so we can use the signature length to calculate `signature_end_index`\r\n3. extract all other fields from remaining bytes using split by `sep`\r\n\r\nThe proposed solution simplifies this process, by assuming that the `sep` bytes can be present in the payload only as separators (and not randomly be as part of signature bytes, for example). Since we guarantee that when we encode the binary data using `base64` (which will not contain `\\x00\\x01`).\r\nWith that assumption, we can just split all fields according to `sep` and get all fields at once.\r\nThe only exception here is the `body` itself, which may contain the `sep` bytes inside. Since the body is encoded to the end of the payload, we can limit the number of splits according to `sep`, to prevent more splits in that case.\r\n(We can also encode the body as base64 and then we can assure `sep` will not be in `body` as well, but I like this better since it is more performant).\r\n\r\nMoreover, keeping the current `unpack` process is broken, since the signature is encoded as base64, and it results with a different encoded length then the original signature length"}], "_meta": {"repo": "celery/celery", "pr_number": 9010, "url": "https://github.com/celery/celery/pull/9010#discussion_r1595339016"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `processing_smolvlm.py`:\n\n```python\ndef expand_text_with_image_tokens(self, text, image_rows, image_cols):\n        prompt_strings = []\n        image_rows = image_rows if image_rows is not None else [[0] * len(text)]\n        image_cols = image_cols if image_cols is not None else [[0] * len(text)]\n        image_rows = (\n            image_rows if image_rows is not None else [[0] * sample.count(self.image_token) for sample in text]\n        )\n        image_cols = (\n            image_cols if image_cols is not None else [[0] * sample.count(self.image_token) for sample in text]\n        )\n```"}, {"role": "assistant", "content": "Same here, you can define image_rows and image_cols before the call to expand_text_with_image_tokens using n_images_in_text"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41871, "url": "https://github.com/huggingface/transformers/pull/41871#discussion_r2488149427"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_worker.py`:\n\n```python\nassert initContainers[1][\"name\"] == \"kerberos-init\"\n            assert initContainers[1][\"args\"] == [\"kerberos\", \"-o\"]\n\n    @pytest.mark.parametrize(\n        \"workers_values\",\n        [\n            {\"kerberosInitContainer\": {\"enabled\": True}},\n            {\"celery\": {\"kerberosInitContainer\": {\"enabled\": True}}},\n            {\n                \"kerberosInitContainer\": {\"enabled\": False},\n                \"celery\": {\"kerberosInitContainer\": {\"enabled\": True}},\n            },\n        ],\n    )\n    def test_airflow_kerberos_init_container_celery_values(self, workers_values):\n        \"\"\"Test that workers.celery.kerberosInitContainer configuration works and takes precedence.\"\"\"\n        docs = render_chart(\n            values={\n                \"airflowVersion\": \"2.8.0\",\n                \"workers\": {\n                    **workers_values,\n                    \"celery\": {\n                        **workers_values.get(\"celery\", {}),\n                        \"persistence\": {\"fixPermissions\": True},\n                    },\n                },\n            },\n            show_only=[\"templates/workers/worker-deployment.yaml\"],\n        )\n\n        initContainers = jmespath.search(\"spec.template.spec.initContainers\", docs[0])\n        # Should have 3 init containers: wait-for-migrations, kerberos-init, volume-permissions\n        assert len(initContainers) == 3\n        assert initContainers[1][\"name\"] == \"kerberos-init\"\n        assert initContainers[1][\"args\"] == [\"kerberos\", \"-o\"]\n```"}, {"role": "assistant", "content": "Personally, I would split tests to test particular arguments and this one left only for testing `enabled` flag"}], "_meta": {"repo": "apache/airflow", "pr_number": 60427, "url": "https://github.com/apache/airflow/pull/60427#discussion_r2696186117"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_multipart.py`:\n\n```python\nfileobj: FileContent\n\n        headers: typing.Dict[str, str] = {}\n        content_type: typing.Optional[str] = None\n\n        if isinstance(value, tuple):\n            try:\n                filename, fileobj, content_type = value  # type: ignore\n                filename, fileobj, content_type, headers = value  # type: ignore\n            except ValueError:\n                filename, fileobj = value  # type: ignore\n                content_type = guess_content_type(filename)\n                try:\n                    filename, fileobj, content_type = value  # type: ignore\n                except ValueError:\n                    filename, fileobj = value  # type: ignore\n            else:\n                headers = {k.title(): v for k, v in headers.items()}\n                if \"Content-Type\" in headers:\n                    raise ValueError(\n                        \"Content-Type cannot be included in multipart headers\"\n                    )\n```"}, {"role": "assistant", "content": "Makes sense. I thought it'd be a good idea to check what requests does here. It looks like it silently ignores the header in the `header`. That is:\r\n\r\n```python\r\nrequests.post(\"http://example.com\", files=[(\"test\", (\"test_filename\", b\"data\", \"text/plain\", {\"Content-Type\": \"text/csv\"}))])\r\n```\r\n\r\nGets sent as `text/plain`.\r\n\r\nDigging into _why_ this is the case, it seems like it's just an implementation detail in urllib3. It happens here.\r\n\r\nI'm not sure what the right thing to do here is, but if you feel like it's best to go with no error and making `header` values take precedence, I'm happy to implement that.\r\n\r\nAnother alternative would be to have the 3rd parameter be either a string representing the content type _or_ a headers dict. We can't really make the 3rd parameter always be a headers dict because that would be a breaking change for httpx.\r\nThis would eliminate the edge case, but deviates from requests' API. It seems pretty reasonable that if I'm specifying headers I'm doing advanced stuff and so specifying the content type in the headers directly would not be an issue."}], "_meta": {"repo": "encode/httpx", "pr_number": 1936, "url": "https://github.com/encode/httpx/pull/1936#discussion_r780401103"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_modeling_utils.py`:\n\n```python\n_prepare_4d_attention_mask,\n        _prepare_4d_causal_attention_mask,\n    )\n    from transformers.modeling_utils import _find_disjoint, _find_identical\n    from transformers.modeling_utils import (\n        _FLASH_ATTN_KERNEL_FALLBACK,\n        _find_disjoint,\n        _find_identical,\n```"}, {"role": "assistant", "content": "Unnecessary imports? (`_find_disjoint`, `_find_identical`)"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42803, "url": "https://github.com/huggingface/transformers/pull/42803#discussion_r2614134649"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `log.py`:\n\n```python\nand adapts it into a tuple of positional arguments for logger.log calls,\n    handling backward compatibility as well.\n    \"\"\"\n    if not {\"level\", \"msg\", \"args\"} <= set(logkws):\n        warnings.warn(\"Missing keys in LogFormatter method\", ScrapyDeprecationWarning)\n\n    if \"format\" in logkws:\n        warnings.warn(\n            \"`format` key in LogFormatter methods has been \"\n            \"deprecated, use `msg` instead\",\n            ScrapyDeprecationWarning,\n        )\n\n    level = logkws.get(\"level\", logging.INFO)\n    message = logkws.get(\"format\", logkws.get(\"msg\"))\n    message = logkws.get(\"msg\", \"\")\n    assert message is not None\n```"}, {"role": "assistant", "content": "Hi  \r\nI finally edited to ```message = logkws.get(\"msg\") or \"\"``` as your suggestion.\r\nCould you please review and merge my pull request?\r\n\r\nThank you!"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6116, "url": "https://github.com/scrapy/scrapy/pull/6116#discussion_r1368288895"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `symtable.py`:\n\n```python\nself.__frees = self.__idents_matching(is_free)\n        return self.__frees\n\n    def get_cells(self):\n        \"\"\"Return a tuple of cells in the function.\"\"\"\n        if self.__cells is None:\n            is_cell = lambda x: _get_scope(x) == CELL\n            self.__cells = self.__idents_matching(is_cell)\n        return self.__cells\n```"}, {"role": "assistant", "content": "According to the PEP8, there should be only two blank lines.\n\n```suggestion\n```"}], "_meta": {"repo": "python/cpython", "pr_number": 143549, "url": "https://github.com/python/cpython/pull/143549#discussion_r2671695258"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_gpt2.py`:\n\n```python\nvalue_states,\n                attention_mask,\n                dropout=self.attn_dropout.p if self.training else 0.0,\n                is_causal=is_causal,\n                is_causal=self.is_causal,\n```"}, {"role": "assistant", "content": "We should be able to remove this line as well, the integrations grep it via the passed self (module)"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41966, "url": "https://github.com/huggingface/transformers/pull/41966#discussion_r2510725761"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `main.py`:\n\n```python\nclass SQLModelMetaclass(ModelMetaclass, DeclarativeMeta):\n    __sqlmodel_relationships__: Dict[str, RelationshipInfo]\n    model_config: SQLModelConfig\n    model_fields: Dict[str, FieldInfo]  # type: ignore[assignment]\n    model_fields: ClassVar[Dict[str, FieldInfo]]\n```"}, {"role": "assistant", "content": "As the idea is that this should now be only in the class and not instances, I think it's good to type it right away. It won't affect at runtime if people are using it, but will start warning them in their linters, so they can start preparing for this change in Pydantic v3."}], "_meta": {"repo": "tiangolo/sqlmodel", "pr_number": 1340, "url": "https://github.com/fastapi/sqlmodel/pull/1340#discussion_r2061539665"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `exceptions.py`:\n\n```python\nsuper(RequestException, self).__init__(*args, **kwargs)\n\n\nclass JSONDecodeError(StandardJSONDecodeError, SimpleJSONDecodeError):\n    \"\"\"Couldn't decode the text into json\"\"\"\n\n\nclass InvalidJSONError(RequestException):\n```"}, {"role": "assistant", "content": "I'm talking about the `InvalidJSONError` exception here (the line I commented on)"}], "_meta": {"repo": "psf/requests", "pr_number": 5856, "url": "https://github.com/psf/requests/pull/5856#discussion_r664949709"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_clip.py`:\n\n```python\nself,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        causal_attention_mask: Optional[torch.Tensor] = None,\n        output_attentions: Optional[bool] = False,\n        **kwargs,\n```"}, {"role": "assistant", "content": "so here shouldn't the kwargs be typed? It's my only comment :D"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41750, "url": "https://github.com/huggingface/transformers/pull/41750#discussion_r2445482240"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `dags.py`:\n\n```python\nreadable_dags_filter: ReadableDagsFilterDep,\n    session: SessionDep,\n    is_favorite: QueryFavoriteFilter,\n    timetable_type: Annotated[\n        FilterParam[list[str] | None],\n        Depends(filter_param_factory(DagModel.timetable_type, list[str], FilterOptionEnum.IN)),\n    ],\n```"}, {"role": "assistant", "content": "How about calling this simply `timetable`? (No strong preference)"}], "_meta": {"repo": "apache/airflow", "pr_number": 58852, "url": "https://github.com/apache/airflow/pull/58852#discussion_r2609900187"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `processing_gemma3n.py`:\n\n```python\nif images is not None:\n            images = self.image_processor.fetch_images(images)\n            print(\"images\", images)\n            batched_images = make_nested_list_of_images(images)\n            print(\"batched_images\", batched_images)\n```"}, {"role": "assistant", "content": "ig this was supposed to be deleted after debugging  :)"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41773, "url": "https://github.com/huggingface/transformers/pull/41773#discussion_r2509449922"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_feedexport.py`:\n\n```python\ndef test_lzma_plugin_filters(self):\n        import sys\n        if \"PyPy\" in sys.version:\n            raise unittest.SkipTest(\"lzma filters doesn't work in PyPy\")\n            pypy_version = sys.version.split('\\n')[1].split()[1]\n            if pypy_version <= '7.3.1':\n```"}, {"role": "assistant", "content": "This is not a reliable way to test versions:\r\n\r\n```python\r\n>>> '10.0.0' <= '7.3.1'\r\nTrue\r\n```\r\n\r\nSee `sys.version_info` usage in the code base, something like that should work."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5190, "url": "https://github.com/scrapy/scrapy/pull/5190#discussion_r676518692"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_typing_extra.py`:\n\n```python\nvalue, globalns, localns\n            )\n    except TypeError as e:\n        if not (isinstance(value, typing.ForwardRef) and is_backport_fixable_error(e)):\n            raise\n        try:\n            from eval_type_backport import eval_type_backport\n        except ImportError:\n        if isinstance(value, typing.ForwardRef):\n            if is_backport_fixable_error(e):\n                try:\n                    from eval_type_backport import eval_type_backport\n                except ImportError:\n                    raise TypeError(\n                        f'Unable to evaluate type annotation {value.__forward_arg__!r}. If you are making use '\n                        'of the new typing syntax (unions using `|` since Python 3.10 or builtins subscripting '\n                        'since Python 3.9), you should either replace the use of new syntax with the existing '\n                        '`typing` constructs or install the `eval_type_backport` package. Otherwise, it might be '\n                        \"that the type being used isn't subscriptable.\"\n                    ) from e\n\n                return eval_type_backport(value, globalns, localns, try_default=False)\n\n            raise TypeError(\n                f'You have a type annotation {value.__forward_arg__!r} '\n                f'which makes use of newer typing features than are supported in your version of Python. '\n                f'To handle this error, you should either remove the use of new syntax '\n                f'or install the `eval_type_backport` package.'\n                f'Unable to evaluate type annotation {value.__forward_arg__!r}. '\n                \"It might be that the type being used isn't subscriptable.\"\n            ) from e\n\n        return eval_type_backport(value, globalns, localns, try_default=False)\n        raise e\n\n\ndef is_backport_fixable_error(e: TypeError) -> bool:\n    msg = str(e)\n    return msg.startswith('unsupported operand type(s) for |: ') or \"' object is not subscriptable\" in msg\n\n    return (\n        sys.version_info <= (3, 8)\n        and msg.startswith('unsupported operand type(s) for |: ')\n        or sys.version_info <= (3, 9)\n        and \"' object is not subscriptable\" in msg\n```"}, {"role": "assistant", "content": "Indeed comparing with `<=` is confusing.\r\n\r\nAs a side note, I've raised https://github.com/microsoft/pyright/discussions/8648."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10030, "url": "https://github.com/pydantic/pydantic/pull/10030#discussion_r1703159257"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `functional_serializers.py`:\n\n```python\n\"\"\"\n\n    def dec(\n        f: Callable[..., Any] | staticmethod[Any, Any] | classmethod[Any, Any, Any],\n```"}, {"role": "assistant", "content": "When type checking the body of `field_serializer`, type checkers will use the signature of that function (they won't use the overloaded signatures).\r\n\r\nPrevious to my changes, the return type was too broad (`Callable[[Any], Any]`). the argument type `Any` would match the type hint of `f` here, and the return type `Any` would also match `PydanticDescriptorProxy[Any]`.\r\n\r\nNothing we can really do here as we lie about the return type because we don't want to expose the `PydanticDescriptorProxy` class."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10092, "url": "https://github.com/pydantic/pydantic/pull/10092#discussion_r1712948091"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `beat.py`:\n\n```python\nif not isinstance(self.loglevel, numbers.Integral):\n            self.loglevel = LOG_LEVELS[self.loglevel.upper()]\n\n    def run(self):\n    @property\n    def app(self) -> Celery:\n        if self._app:\n            return self._app\n        raise ValueError(\"Beat must provided a Celery app.\")\n```"}, {"role": "assistant", "content": "Fixed a type hint (`scheduler`/`scheduler_cls`), and added a test that covers both."}], "_meta": {"repo": "celery/celery", "pr_number": 8108, "url": "https://github.com/celery/celery/pull/8108#discussion_r1125743861"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `benchmark_runner.py`:\n\n```python\n\"config\": config,\n            }\n\n    def time_generate_batch(\n        self,\n        max_new_tokens: int,\n        gpu_monitor: GPUMonitor | None = None,\n    ) -> tuple[float, list[float], str, GPURawMetrics | None]:\n        if gpu_monitor is not None:\n            gpu_monitor.start()\n        config = GenerationConfig(\n            max_new_tokens=max_new_tokens,\n            use_cuda_graphs=False,\n            eos_token_id=self.tokenizer.eos_token_id,\n            pad_token_id=self.tokenizer.pad_token_id,\n            do_sample=False,\n```"}, {"role": "assistant", "content": "It seems like the real-world case scenario is `do_sample=True`"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41916, "url": "https://github.com/huggingface/transformers/pull/41916#discussion_r2486044233"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_canvas.py`:\n\n```python\nbackend = fail.app.backend\n        j_key = backend.get_key_for_group(original_group_id, '.j')\n        redis_connection = get_redis_connection()\n        # The redis key is either a list or zset depending on configuration\n        # The redis key is either a list or a set depending on configuration\n```"}, {"role": "assistant", "content": "from search: The Redis zset function is used to store the key type and value. In a redis, zset keys are unique, and values are limited by the number of floating point operations. The unique property in it is used to access the members. We can also access the items by using sorted orders and order score values."}], "_meta": {"repo": "celery/celery", "pr_number": 8498, "url": "https://github.com/celery/celery/pull/8498#discussion_r1323999175"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `networks.py`:\n\n```python\n@property\n    def defined_constraints(self) -> dict[str, Any]:\n        \"\"\"Fetch a key / value mapping of constraints to values that are not None. Used for core schema updates.\"\"\"\n        return {field.name: getattr(self, field.name) for field in fields(self)}\n        return {field.name: value for field in fields(self) if (value := getattr(self, field.name)) is not None}\n\n    def __get_pydantic_core_schema__(self, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        schema = handler(source)\n\n        if schema['type'] == 'function-wrap':\n            # for function-wrap schemas, url constraints is applied to the inner schema\n            inner = schema['schema']\n\n            if annotated_type := inner['type'] not in ('url', 'multi-host-url'):\n                raise PydanticUserError(\n                    f\"'UrlConstraints' cannot annotate '{annotated_type}'.\", code='invalid-annotated-type'\n                )\n            for constraint_key, constraint_value in self.defined_constraints.items():\n                inner[constraint_key] = constraint_value\n            schema['schema'] = inner\n        else:\n            if annotated_type := schema['type'] not in ('url', 'multi-host-url'):\n                raise PydanticUserError(\n                    f\"'UrlConstraints' cannot annotate '{annotated_type}'.\", code='invalid-annotated-type'\n                )\n            for constraint_key, constraint_value in self.defined_constraints.items():\n                schema[constraint_key] = constraint_value\n\n        return schema\n```"}, {"role": "assistant", "content": "```suggestion\r\n        # for function-wrap schemas, url constraints is applied to the inner schema\r\n        schema_to_mutate = schema['schema'] if schema['type'] == 'function-wrap' else schema\r\n\r\n        if annotated_type := schema_to_mutate['type'] not in ('url', 'multi-host-url'):\r\n            raise PydanticUserError(\r\n                f\"'UrlConstraints' cannot annotate '{annotated_type}'.\", code='invalid-annotated-type'\r\n            )\r\n        for constraint_key, constraint_value in self.defined_constraints.items():\r\n            schema_to_mutate[constraint_key] = constraint_value\r\n\r\n        return schema\r\n```\r\n\r\nmaybe?"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10922, "url": "https://github.com/pydantic/pydantic/pull/10922#discussion_r1852576621"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `linegen.py`:\n\n```python\n# We could enforce triple quotes at this point.\n            quote = quote_char * quote_len\n            leaf.value = prefix + quote + docstring + quote\n\n            if Preview.long_docstring_quotes_on_newline in self.mode:\n                leaf.value = prefix + quote + docstring + quote\n            else:\n```"}, {"role": "assistant", "content": "This should be the other way around, no? The idea is that if preview is enabled, we do the new thing."}], "_meta": {"repo": "psf/black", "pr_number": 3044, "url": "https://github.com/psf/black/pull/3044#discussion_r865291195"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_aimv2.py`:\n\n```python\ndef get_input_embeddings(self) -> nn.Module:\n        return self.embeddings.patch_embed\n\n    @deprecate_kwarg(\"attention_mask\", version=\"v4.58.0\")\n    @check_model_inputs(tie_last_hidden_states=False)\n    @auto_docstring\n    def forward(\n        self,\n        pixel_values,\n```"}, {"role": "assistant", "content": "A few tests will fail if we start propagating the mask, so I will remove it as it's not actually used currently. It can be re-added in subsequent PRs with adjusting tests"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41681, "url": "https://github.com/huggingface/transformers/pull/41681#discussion_r2445230147"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `consumer.py`:\n\n```python\ndef _error_handler(exc, interval, next_step=CONNECTION_RETRY_STEP):\n            if getattr(conn, 'alt', None) and interval == 0:\n                next_step = CONNECTION_FAILOVER\n            self.broker_connection_retry_attempt += 1\n            next_step = next_step.format(\n                when=humanize_seconds(interval, 'in', ' '),\n                retries=int(interval / 2),\n                retries=self.broker_connection_retry_attempt,\n                max_retries=self.app.conf.broker_connection_max_retries)\n```"}, {"role": "assistant", "content": "The retry counter is incremented on every error, but should this happen before the failover check? If `interval == 0` indicates an immediate failover rather than a retry attempt, incrementing the counter might be misleading for users monitoring retry counts."}], "_meta": {"repo": "celery/celery", "pr_number": 9911, "url": "https://github.com/celery/celery/pull/9911#discussion_r2378864348"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_model_construction.py`:\n\n```python\nsuper(cls, cls).__pydantic_init_subclass__(**kwargs)  # type: ignore[misc]\n            return cls\n        else:\n            # this is the BaseModel class itself being created, no logic required\n            # These are instance variables, but have been assigned value to trick type checker.\n```"}, {"role": "assistant", "content": "```suggestion\r\n            # These are instance variables, but have been assigned to `NoInitField` to trick the type checker.\r\n```"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10110, "url": "https://github.com/pydantic/pydantic/pull/10110#discussion_r1714809873"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `abstractoperator.py`:\n\n```python\nDEFAULT_TASK_EXECUTION_TIMEOUT: datetime.timedelta | None = conf.gettimedelta(\n    \"core\", \"default_task_execution_timeout\"\n)\n\nDEFAULT_EMAIL_ON_FAILURE: bool = conf.getboolean(\"email\", \"default_email_on_failure\", fallback=True)\nDEFAULT_EMAIL_ON_RETRY: bool = conf.getboolean(\"email\", \"default_email_on_retry\", fallback=True)\n```"}, {"role": "assistant", "content": "We should have some test coverage for this. The test: `test_default_email_on_actions` only checks for true. Maybe use `conf_vars` to explicitly set to false and assert it."}], "_meta": {"repo": "apache/airflow", "pr_number": 59912, "url": "https://github.com/apache/airflow/pull/59912#discussion_r2652335798"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `base.py`:\n\n```python\n\"\"\"\n        return type(self).__name__\n\n    @property\n    def type_name(self) -> str:\n        \"\"\"\n        This is primarily intended for filtering dags based on timetable type.\n\n        For built-in timetables (defined in airflow.timetables or\n        airflow.sdk.definitions.timetables), this returns the class name only.\n        For custom timetables (user-defined via plugins), this returns the full\n        import path to avoid confusion between multiple implementations with the\n        same class name.\n\n        For example, built-in timetables return:\n        ``\"NullTimetable\"`` or ``\"CronDataIntervalTimetable\"``\n        while custom timetables return the full path:\n        ``\"my_company.timetables.CustomTimetable\"``\n        \"\"\"\n        module = self.__class__.__module__\n        class_name = self.__class__.__name__\n\n        # Built-in timetables from Core or SDK use class name only\n        if module.startswith(\"airflow.timetables.\") or module.startswith(\n            \"airflow.sdk.definitions.timetables.\"\n        ):\n            return class_name\n```"}, {"role": "assistant", "content": "This would not work for types in `airflow.sdk.definitions.timetables` since they don\u2019t inherit from this class. I also feel there\u2019s probably a better way to do this than string manipulation (which can break too easily if we move the classes around)."}], "_meta": {"repo": "apache/airflow", "pr_number": 58852, "url": "https://github.com/apache/airflow/pull/58852#discussion_r2609906840"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `scheduler.py`:\n\n```python\nself.logunser: bool = logunser\n        self.stats: StatsCollector | None = stats\n        self.crawler: Crawler | None = crawler\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        \"\"\"\n        Factory method, initializes the scheduler with arguments taken from the crawl settings\n        \"\"\"\n        dupefilter_cls = load_object(crawler.settings[\"DUPEFILTER_CLASS\"])\n        return cls(\n            dupefilter=build_from_crawler(dupefilter_cls, crawler),\n            jobdir=job_dir(crawler.settings),\n            dqclass=load_object(crawler.settings[\"SCHEDULER_DISK_QUEUE\"]),\n            mqclass=load_object(crawler.settings[\"SCHEDULER_MEMORY_QUEUE\"]),\n            logunser=crawler.settings.getbool(\"SCHEDULER_DEBUG\"),\n            stats=crawler.stats,\n            pqclass=load_object(crawler.settings[\"SCHEDULER_PRIORITY_QUEUE\"]),\n            crawler=crawler,\n        self._sdqclass: type[BaseQueue] | None = self._get_start_queue_cls(\n            crawler, \"DISK\"\n        )\n        self._smqclass: type[BaseQueue] | None = self._get_start_queue_cls(\n            crawler, \"MEMORY\"\n        )\n```"}, {"role": "assistant", "content": "I'm fine with either approach. My main concern is about consistency (some things are passed to `__init__`, others are created later), but it's not a big deal, if it helps to make a backwards compatible change with less code to maintain."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6752, "url": "https://github.com/scrapy/scrapy/pull/6752#discussion_r2073057407"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `functools.py`:\n\n```python\nreturn decorating_function\n\ndef _lru_cache_wrapper(user_function, maxsize, typed, _CacheInfo):\n    if not callable(user_function):\n```"}, {"role": "assistant", "content": "There is no big refactoring actually. It is just about moving your check in another function (a few lines above). Considering the LRU wrapper cache function is already an implementation detail, I expect the caller to check for callability, not the wrapper itself."}], "_meta": {"repo": "python/cpython", "pr_number": 144031, "url": "https://github.com/python/cpython/pull/144031#discussion_r2709474183"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `wsgi.py`:\n\n```python\nfrom .base import SyncDispatcher\n\n\ndef _get_non_empty_chunk(body: typing.Iterable) -> typing.Iterable:\n    \"\"\"Get a non-empty chunk from body.\"\"\"\n```"}, {"role": "assistant", "content": "I notice you reverted the name, but I still think the current \"get chunk\" naming is not conveying correctly what this function does\u2026 A \"get chunk\" function should return a `chunk`, but this returns a modified `body` iterable that skips any leading empty chunks.\r\n\r\nSo, maybe\u2026?\r\n\r\n```suggestion\r\ndef _skip_leading_empty_chunks(body: typing.Iterable) -> typing.Iterable:\r\n```"}], "_meta": {"repo": "encode/httpx", "pr_number": 887, "url": "https://github.com/encode/httpx/pull/887#discussion_r399778004"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `typing.py`:\n\n```python\n# TypeError: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'\n        return cast(Any, type_)._evaluate(globalns, localns, recursive_guard=set())\n\nelse:\n\n    def evaluate_forwardref(type_: ForwardRef, globalns: Any, localns: Any) -> Any:\n        # Pydantic 1.x will not support PEP 695 syntax, but provide `type_params` to avoid\n        # warnings:\n        return cast(Any, type_)._evaluate(globalns, localns, type_params=(), recursive_guard=set())\n```"}, {"role": "assistant", "content": "Seems this block has introduced a regression for some python3.12 versions https://github.com/pydantic/pydantic/issues/11230"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10847, "url": "https://github.com/pydantic/pydantic/pull/10847#discussion_r1906086954"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_models.py`:\n\n```python\nmessage = message.format(self, error_type=\"Server Error\")\n            raise HTTPError(message, response=self)\n\n    def json(self, **kwargs: typing.Any) -> typing.Union[dict, list]:\n    def json(self, **kwargs: typing.Any) -> typing.Any:\n```"}, {"role": "assistant", "content": "I get a different (and even more confusing) error:\r\n\r\n```python\r\ndata: dict = httpx.get(\"http://example.com\").json()\r\n```\r\n\r\n```console\r\nerror: Incompatible types in assignment (expression has type \"Union[Dict[Any, Any], List[Any]]\", variable has type \"Dict[Any, Any]\")\r\n```\r\n\r\nI assume it's because mypy doesn't allow just down-sizing a union like that - you'd need to make a proper `assert`, or `cast()`."}], "_meta": {"repo": "encode/httpx", "pr_number": 923, "url": "https://github.com/encode/httpx/pull/923#discussion_r418951855"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modular_siglip2.py`:\n\n```python\n)\n\nfrom ...modeling_attn_mask_utils import _prepare_4d_attention_mask\nfrom ...tokenization_utils_tokenizers import TokenizersBackend\nfrom ...utils import auto_docstring, filter_out_non_signature_kwargs\nfrom ...utils.generic import check_model_inputs, is_flash_attention_requested\n\n\nVOCAB_FILES_NAMES = {\"tokenizer_file\": \"tokenizer.json\"}\n\n\ndef _ensure_lowercase_normalizer(tok):\n    backend = getattr(tok, \"_tokenizer\", None)\n    if backend is None:\n        return\n\n    current = backend.normalizer\n    if current is None:\n        backend.normalizer = normalizers.Lowercase()\n        return\n\n    if isinstance(current, normalizers.Lowercase):\n        return\n\n    if isinstance(current, normalizers.Sequence):\n        items = list(current.normalizers)\n        if not any(isinstance(n, normalizers.Lowercase) for n in items):\n            backend.normalizer = normalizers.Sequence([normalizers.Lowercase(), *items])\n        return\n\n    backend.normalizer = normalizers.Sequence([normalizers.Lowercase(), current])\n\n\nclass Siglip2Tokenizer(TokenizersBackend):\n```"}, {"role": "assistant", "content": "Sure,  I started with TokenizersBackend to keep SigLIP2 decoupled from Gemma, but since the tokenizer behavior is Gemma-compatible, inheriting from GemmaTokenizer makes more sense. Updating the PR."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43101, "url": "https://github.com/huggingface/transformers/pull/43101#discussion_r2698768817"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `comments2.py`:\n\n```python\nsyms.simple_stmt,\n        [\n            Node(statement, result),\n            Leaf(token.NEWLINE, '\\n'),  # FIXME: \\r\\n?\n            Leaf(token.NEWLINE, '\\n')  # FIXME: \\r\\n?\n```"}, {"role": "assistant", "content": "Thanks! Sorry for that, it's not always easy to tell from changes in the test suite what is actually changing."}], "_meta": {"repo": "psf/black", "pr_number": 826, "url": "https://github.com/psf/black/pull/826#discussion_r282331786"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_black.py`:\n\n```python\nself.assertEqual(black.get_features_used(node), {Feature.NUMERIC_UNDERSCORES})\n        node = black.lib2to3_parse(\"123456\\n\")\n        self.assertEqual(black.get_features_used(node), set())\n        source, expected = read_data(\"simple_cases/function.py\")\n        source, expected = read_data(\"simple_cases\", \"function.py\")\n```"}, {"role": "assistant", "content": "```suggestion\r\n        source, expected = read_data(\"simple_cases\", \"function\")\r\n```"}], "_meta": {"repo": "psf/black", "pr_number": 3062, "url": "https://github.com/psf/black/pull/3062#discussion_r868102927"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `msgraph.py`:\n\n```python\nfrom kiota_abstractions.serialization import ParsableFactory\n\n    from airflow.providers.common.compat.sdk import Connection\n    from airflow.sdk._shared.secrets_masker import redact\nelse:\n    try:\n        from airflow.sdk._shared.secrets_masker import redact\n```"}, {"role": "assistant", "content": "I wasn't aware of `airflow.providers.common.compat.sdk`. I can use this."}], "_meta": {"repo": "apache/airflow", "pr_number": 59688, "url": "https://github.com/apache/airflow/pull/59688#discussion_r2638866083"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_configuration_utils.py`:\n\n```python\n\"output_hidden_states\": True,\n    \"output_attentions\": True,\n    \"dtype\": \"float16\",\n    # \"tie_word_embeddings\": True, # attribute is hardcoded in many models, hard to test\n    \"is_decoder\": True,\n    \"cross_attention_hidden_size\": 128,\n    \"add_cross_attention\": True,\n    \"chunk_size_feed_forward\": 5,\n    \"architectures\": [\"BertModel\"],\n    \"finetuning_task\": \"translation\",\n    \"id2label\": {0: \"label\"},\n    \"label2id\": {\"label\": \"0\"},\n    \"tokenizer_class\": \"BertTokenizerFast\",\n    \"prefix\": \"prefix\",\n    \"bos_token_id\": 6,\n    \"pad_token_id\": 7,\n    \"eos_token_id\": 8,\n    \"sep_token_id\": 9,\n    \"decoder_start_token_id\": 10,\n    \"task_specific_params\": {\"translation\": \"some_params\"},\n```"}, {"role": "assistant", "content": "they aren't used for model tester right now. They were needed only for config tests to check defaults and yeah, the tests are passing now"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41541, "url": "https://github.com/huggingface/transformers/pull/41541#discussion_r2672184375"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `decorator.py`:\n\n```python\nreturn f\"{core}__{max(_find_id_suffixes(dag)) + 1}\"\n\n\ndef unwrap_partial(fn):\n    while isinstance(fn, partial):\n        fn = fn.func\n    return fn\n\n\ndef unwrap_callable(func):\n    from airflow.sdk.bases.decorator import _TaskDecorator\n```"}, {"role": "assistant", "content": "This class is in the same file ! :) We don't need to import it"}], "_meta": {"repo": "apache/airflow", "pr_number": 60268, "url": "https://github.com/apache/airflow/pull/60268#discussion_r2677523914"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_generate_schema.py`:\n\n```python\nif isinstance(obj, PydanticRecursiveRef):\n            return core_schema.definition_reference_schema(schema_ref=obj.type_ref)\n\n        try:\n            if obj in {bool, int, float, str, bytes, list, set, frozenset, dict}:\n                # Note: obj may fail to be hashable if it has an unhashable annotation\n                return {'type': obj.__name__}\n            elif obj is tuple:\n                return {'type': 'tuple-variable'}\n        except TypeError:  # obj not hashable; can happen due to unhashable annotations\n            pass\n\n        if obj is Any or obj is object:\n            return core_schema.any_schema()\n        return self.match_type(obj)\n\n    def match_type(self, obj: Any) -> core_schema.CoreSchema:  # noqa: C901\n        \"\"\"Main mapping of types to schemas.\n\n        The general structure is a series of if statements starting with the simple cases\n        (non-generic primitive types) and then handling generics and other more complex cases.\n\n        Each case either generates a schema directly, calls into a public user-overridable method\n        (like `GenerateSchema.tuple_variable_schema`) or calls into a private method that handles some\n        boilerplate before calling into the user-facing method (e.g. `GenerateSchema._tuple_schema`).\n\n        The idea is that we'll evolve this into adding more and more user facing methods over time\n        as they get requested and we figure out what the right API for them is.\n        \"\"\"\n        if obj is str:\n            return self.str_schema()\n        elif obj is bytes:\n            return self.bytes_schema()\n        elif obj is int:\n            return self.int_schema()\n        elif obj is float:\n            return self.float_schema()\n        elif obj is bool:\n            return self.bool_schema()\n        elif obj is Any or obj is object:\n            return self.any_schema()\n        elif obj is None or obj is _typing_extra.NoneType:\n            return core_schema.none_schema()\n            return self.none_schema()\n        elif obj in TUPLE_TYPES:\n            return self._tuple_schema(obj)\n        elif obj in LIST_TYPES:\n            return self.list_schema(self._get_first_arg_or_any(obj))\n        elif obj in SET_TYPES:\n            return self.set_schema(self._get_first_arg_or_any(obj))\n        elif obj in FROZEN_SET_TYPES:\n            return self.frozenset_schema(self._get_first_arg_or_any(obj))\n        elif obj in DICT_TYPES:\n            return self.dict_schema(*(self._get_args_resolving_forward_refs(obj) or (Any, Any)))\n        elif isinstance(obj, TypeAliasType):\n            return self._type_alias_type_schema(obj)\n```"}, {"role": "assistant", "content": "I'm somewhat arbitrarily deciding what to make public or not. I picked the ones that I felt were important, simple or proved that something works (e.g. lists). No reason to make everything public now, let's do it a bit at a time as we understand the use cases."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 6737, "url": "https://github.com/pydantic/pydantic/pull/6737#discussion_r1271292374"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `config.py`:\n\n```python\n# whether instances of models and dataclasses (including subclass instances) should re-validate, default 'never'\n    revalidate_instances: Literal['always', 'never', 'subclass-instances']\n    \"\"\"\n    When and how to revalidate models and dataclasses during validation. Accepts the string\n    values of `'never'`, `'always'` and `'subclass-instances'`. Defaults to `'never'`.\n    When and how to revalidate models and dataclasses during validation. Can be one of:\n\n    - `'never'` will not revalidate models and dataclasses during validation\n    - `'always'` will revalidate models and dataclasses during validation\n    - `'subclass-instances'` will revalidate models and dataclasses during validation if the instance is a\n    - `'never'`: will *not* revalidate models and dataclasses during validation\n    - `'always'`: will revalidate models and dataclasses during validation\n    - `'subclass-instances'`: will revalidate models and dataclasses during validation if the instance is a\n        subclass of the model or dataclass\n\n    By default, model and dataclass instances are not revalidated during validation.\n    The default is `'never'` (no revalidation).\n\n    This configuration only affects *the current model* it is applied on, and does *not* populate to the models\n    referenced in fields.\n\n    ```python\n    from pydantic import BaseModel\n\n    class User(BaseModel, revalidate_instances='never'):  # (1)!\n        hobbies: list[str]\n\n    class SubUser(User):\n        sins: list[str]\n        name: str\n\n    class Transaction(BaseModel):\n        user: User\n\n    my_user = User(hobbies=['reading'])\n    my_user = User(name='John')\n    t = Transaction(user=my_user)\n    print(t)\n    #> user=User(hobbies=['reading'])\n\n    my_user.hobbies = [1]  # (2)!\n    my_user.name = 1  # (2)!\n    t = Transaction(user=my_user)  # (3)!\n    print(t)\n    #> user=User(hobbies=[1])\n\n    my_sub_user = SubUser(hobbies=['scuba diving'], sins=['lying'])\n    t = Transaction(user=my_sub_user)\n    print(t)\n    #> user=SubUser(hobbies=['scuba diving'], sins=['lying'])\n    #> user=User(name=1)\n    ```\n\n    1. `revalidate_instances` is set to `'never'` by **default.\n    2. The assignment is not validated, unless you set `validate_assignment` to `True` in the model's config.\n    3. Since `revalidate_instances` is set to `never`, this is not revalidated.\n    1. This is the default behavior.\n    2. The assignment is *not* validated, unless you set [`validate_assignment`][pydantic.ConfigDict.validate_assignment] in the configuration.\n    3. Since `revalidate_instances` is set to `'never'`, the user instance is not revalidated.\n\n    If you want to revalidate instances during validation, you can set `revalidate_instances` to `'always'`\n```"}, {"role": "assistant", "content": "Don't think this example hurt but it's also fine to delete"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 12511, "url": "https://github.com/pydantic/pydantic/pull/12511#discussion_r2512032415"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `quantizer_torchao.py`:\n\n```python\nmetadata.update(metadata_)\n            # Save it\n            self.metadata = metadata\n\n    def get_quantize_ops(self):\n        from ..integrations.torchao import TorchAoQuantize\n\n        return TorchAoQuantize(self)\n\n    def get_weight_conversions(self):\n        from ..integrations.torchao import TorchAoDeserialize, TorchAoQuantize\n\n        if self.pre_quantized:\n            return [\n                WeightConverter(\n                    source_keys=[\"weight:qdata\", \"weight:scale\", \"weight:zero_point\"],\n                    target_keys=\"weight\",\n                    operations=[TorchAoDeserialize(self)],\n                ),\n                WeightConverter(\n                    source_keys=[\"weight:_data\"],\n                    target_keys=\"weight\",\n                    operations=[TorchAoDeserialize(self)],\n```"}, {"role": "assistant", "content": "FYI we just changed `weight:_data` to `weight_qdata` so these things can be attached to module directly incase we need it in the future. https://github.com/pytorch/ao/commit/ba3ac9f2f6117ba35ff28fbb8811f61ad992dfcf"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42289, "url": "https://github.com/huggingface/transformers/pull/42289#discussion_r2551589359"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `video_processing_ernie4_5_vl.py`:\n\n```python\n# coding=utf-8\n# Copyright 2025 Baidu and HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom functools import partial\nfrom pathlib import Path\nfrom typing import Any, Optional, Union\n\nimport numpy as np\nimport torch\nfrom huggingface_hub.dataclasses import validate_typed_dict\nfrom PIL import ImageDraw, ImageFont\nfrom torchvision.transforms.functional import pil_to_tensor, to_pil_image\n\nfrom ...image_processing_utils import BatchFeature\nfrom ...image_utils import (\n    OPENAI_CLIP_MEAN,\n    OPENAI_CLIP_STD,\n    ChannelDimension,\n    PILImageResampling,\n    SizeDict,\n    get_image_size,\n    validate_kwargs,\n)\nfrom ...processing_utils import Unpack, VideosKwargs\nfrom ...utils import (\n    TensorType,\n    add_start_docstrings,\n    logging,\n)\nfrom ...utils.import_utils import is_tracing, requires\nfrom ...video_processing_utils import BASE_VIDEO_PROCESSOR_DOCSTRING, BaseVideoProcessor\nfrom ...video_utils import (\n    VideoInput,\n    VideoMetadata,\n    group_videos_by_shape,\n    infer_channel_dimension_format,\n    reorder_videos,\n)\nfrom .image_processing_ernie4_5_vl import smart_resize\n\n\nlogger = logging.get_logger(__name__)\n\n\nclass Ernie4_5_VLVideoProcessorInitKwargs(VideosKwargs, total=False):\n    patch_size: int\n```"}, {"role": "assistant", "content": "It is tho or am I missing something? Just a few lines below every arg here is in there"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 39585, "url": "https://github.com/huggingface/transformers/pull/39585#discussion_r2599935811"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `stats.py`:\n\n```python\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.utils.python import global_object_name, to_bytes\nfrom scrapy.utils.request import request_httprepr\nfrom scrapy.utils.response import response_httprepr\nfrom scrapy.utils.python import global_object_name\n\nfrom twisted.web import http\n\n\ndef get_header_size(headers):\n    size = 0\n    for key, value in headers.items():\n        if isinstance(value, (list, tuple)):\n            for v in value:\n                size += len(b\": \") + len(key) + len(v)\n    return size + len(b'\\r\\n') * (len(headers.keys()) - 1)\n\n\ndef get_status_size(response_status):\n    return sum((\n        9,  # b\"HTTP/1.1 \"\n        len(to_bytes(str(response_status))),\n        1,  # \" \"\n```"}, {"role": "assistant", "content": "```suggestion\r\n        13,  # b\"HTTP/1.1 <100-599> \"\r\n```"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4972, "url": "https://github.com/scrapy/scrapy/pull/4972#discussion_r716022532"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_list.py`:\n\n```python\nfrom typing import List\n\nimport pytest\nfrom dirty_equals import IsDict, IsOneOf, IsPartialDict\nfrom fastapi import FastAPI, File, Form, UploadFile\nfrom fastapi._compat import PYDANTIC_V2\nfrom fastapi.testclient import TestClient\nfrom pydantic import BaseModel\nfrom typing_extensions import Annotated\n\nfrom tests.utils import needs_pydanticv2\n\nfrom .utils import get_body_model_name\n\napp = FastAPI()\n\n# =====================================================================================\n# Without aliases\n\n\n@app.post(\"/list-bytes\", operation_id=\"list_bytes\")\nasync def read_list_bytes(p: Annotated[List[bytes], File()]):\n    return {\"file_size\": [len(file) for file in p]}\n\n\n@app.post(\"/list-uploadfile\", operation_id=\"list_uploadfile\")\nasync def read_list_uploadfile(p: Annotated[List[UploadFile], File()]):\n    return {\"file_size\": [file.size for file in p]}\n\n\nclass FormModelListBytes(BaseModel):\n    p: List[bytes] = File()\n\n\n@app.post(\"/model-list-bytes\", operation_id=\"model_list_bytes\")\nasync def read_model_list_bytes(\n    p: Annotated[\n        FormModelListBytes,\n        Form(\n            media_type=\"multipart/form-data\"  # Remove media_type when https://github.com/fastapi/fastapi/pull/14343 is fixed\n        ),\n    ],\n):\n    return {\"file_size\": [len(file) for file in p.p]}\n\n\nclass FormModelListUploadFile(BaseModel):\n    p: List[UploadFile] = File()\n\n\n@app.post(\"/model-list-uploadfile\", operation_id=\"model_list_uploadfile\")\nasync def read_model_list_uploadfile(\n    p: Annotated[\n        FormModelListUploadFile,\n        Form(\n            media_type=\"multipart/form-data\"  # Remove media_type when https://github.com/fastapi/fastapi/pull/14343 is fixed\n        ),\n    ],\n):\n    return {\"file_size\": [file.size for file in p.p]}\n```"}, {"role": "assistant", "content": "Let's remove these. Pydantic models with fields that should be read from bytes are not supported in FastAPI, at least not yet, and I'm not sure if it would make sense supporting it. Currently it's not supported to use a different function to declare a field in a Pydantic model other than `Field()` (at least not intentionally), I would prefer to avoid that for now."}], "_meta": {"repo": "fastapi/fastapi", "pr_number": 14358, "url": "https://github.com/fastapi/fastapi/pull/14358#discussion_r2607250726"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_vault.py`:\n\n```python\n)\n        assert test_client.get_variable(\"hello\") is None\n\n    @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac\")\n    @mock.patch(\"airflow.utils.log.logging_mixin.structlog.get_logger\")\n    def test_get_variable_does_not_contain_value_key(self, mock_hvac, mock_get_logger):\n        \"\"\"\n        Test that if the 'value' key is not present in Vault, _VaultClient.get_variable\n        should log a warning and return None\n        \"\"\"\n        mock_client = mock.MagicMock()\n        mock_hvac.Client.return_value = mock_client\n        mock_logger = mock.MagicMock()\n        mock_get_logger.return_value = mock_logger\n\n        kwargs = {\n            \"variables_path\": \"variables\",\n            \"mount_point\": \"airflow\",\n            \"auth_type\": \"token\",\n            \"url\": \"http://127.0.0.1:8200\",\n            \"token\": \"s.7AU0I51yv1Q1lxOIg1F3ZRAS\",\n        }\n        test_client = VaultBackend(**kwargs)\n        test_client._log = mock_logger\n        response = {\"test_key\": \"data\"}\n        test_client.vault_client.get_secret = mock.MagicMock(return_value=response)\n```"}, {"role": "assistant", "content": "some variable are not re-used and thus might not be needed"}], "_meta": {"repo": "apache/airflow", "pr_number": 59224, "url": "https://github.com/apache/airflow/pull/59224#discussion_r2660836786"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `feature_extraction_gemma3n.py`:\n\n```python\nraw_speech = np.asarray(raw_speech)\n\n        if not is_batched:  # always return a batch\n            raw_speech = [np.asarray([raw_speech])]\n            raw_speech = [np.asarray([raw_speech]).T]\n```"}, {"role": "assistant", "content": "Good point! I just realized that we also don't need the `np.asarray()` call in the if statement. PTAL and thanks for reviewing!"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42076, "url": "https://github.com/huggingface/transformers/pull/42076#discussion_r2504591341"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `serialized_dag.py`:\n\n```python\nlog.debug(\"Writing Serialized DAG: %s to the DB\", dag.dag_id)\n        new_serialized_dag.dag_version = dagv\n        session.add(new_serialized_dag)\n        session.flush()\n```"}, {"role": "assistant", "content": "I think I have an answer.  I'm going to test it out and make sure it works, but it will remove the flush here and in SerializedDagModel::_process_deadline_alerts  I'll let you know tomorrow."}], "_meta": {"repo": "apache/airflow", "pr_number": 58248, "url": "https://github.com/apache/airflow/pull/58248#discussion_r2566762425"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `generate_schema.py`:\n\n```python\nresult[name][\"description\"] = param.help\n\n        if param.default is not None and not param.multiple:\n            if Version(imp_version(\"click\")) >= Version(\"8.3.0\"):\n```"}, {"role": "assistant", "content": "Could also do something like `hasattr(click, \"_utils\") and hasattr(click._utils, \"UNSET\")`, though I guess it will still need a mypy type ignore. The current code is fine with me too.\r\n\r\nNormally I'd be concerned about breaking future users if click removes or moves the private object later, but this is just an internal script; if it breaks later we can just fix it and users won't be affected."}], "_meta": {"repo": "psf/black", "pr_number": 4750, "url": "https://github.com/psf/black/pull/4750#discussion_r2360971482"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `databricks_workflow.py`:\n\n```python\nif not session:\n            raise AirflowException(\"Session not provided.\")\n\n        return session.query(DagRun).filter(DagRun.dag_id == dag.dag_id, DagRun.run_id == run_id).first()\n        dagrun = session.query(DagRun).filter(DagRun.dag_id == dag.dag_id, DagRun.run_id == run_id).first()\n```"}, {"role": "assistant", "content": "How about just use `.one()` instead and let SQLAlchemy raise the error?"}], "_meta": {"repo": "apache/airflow", "pr_number": 57768, "url": "https://github.com/apache/airflow/pull/57768#discussion_r2488747122"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `configuration_colqwen2.py`:\n\n```python\nvlm_config=None,\n        embedding_dim: int = 128,\n        initializer_range: float = 0.02,\n        use_qwen2_5=False,\n        **kwargs,\n    ):\n        if vlm_config is None:\n            vlm_config = CONFIG_MAPPING[\"qwen2_vl\"]()\n            vlm_config = CONFIG_MAPPING[\"qwen2_5_vl\"]() if use_qwen2_5 else CONFIG_MAPPING[\"qwen2_vl\"]()\n```"}, {"role": "assistant", "content": "I would suggest to abstract the model name to a separate variable as follows\n```\nmodel_name = \"qwen2_5_vl\" if use_qwen2_5 else \"qwen2_vl\"\nvlm_config = CONFIG_MAPPING[model_name]\n```"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 40600, "url": "https://github.com/huggingface/transformers/pull/40600#discussion_r2314469148"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `setup.py`:\n\n```python\nextras_require={\n        'security': [],\n        'socks': ['PySocks>=1.5.6, !=1.5.7'],\n        'socks:sys_platform == \"win32\" and python_version == \"2.7\"': ['win_inet_pton'],\n```"}, {"role": "assistant", "content": "Finally free... :smiling_face_with_tear:"}], "_meta": {"repo": "psf/requests", "pr_number": 6091, "url": "https://github.com/psf/requests/pull/6091#discussion_r833413598"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `autoretry.py`:\n\n```python\nif retry_backoff:\n                    retry_kwargs['countdown'] = \\\n                        get_exponential_backoff_interval(\n                            factor=retry_backoff,\n                            factor=int(max(1.0, retry_backoff)),\n```"}, {"role": "assistant", "content": "are we going to turn everything into 1 only or multiple of 1.0?"}], "_meta": {"repo": "celery/celery", "pr_number": 8295, "url": "https://github.com/celery/celery/pull/8295#discussion_r1233044571"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `configuration_utils.py`:\n\n```python\ndef _get_generation_parameters(self) -> dict[str, Any]:\n        \"\"\"\n        Gets the non-default generation parameters on the PreTrainedConfig instance\n        Checks if there are generation parameters in `PreTrainedConfig` instance. Note that\n        we should not save generation params in PreTrainedConfig, and we will raise error\n        if there are any.\n        \"\"\"\n        generation_params = {}\n        default_config = self.__class__().to_dict() if not self.has_no_defaults_at_init else {}\n        for key in GenerationConfig._get_default_generation_params().keys():\n            if key == \"use_cache\":\n                continue  # common key for most models\n```"}, {"role": "assistant", "content": "btw, could we remove `use_cache` as a key from config and instead keep it only in `generation_config`? Imo it isn't really a model attribute"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43000, "url": "https://github.com/huggingface/transformers/pull/43000#discussion_r2642366668"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `db.py`:\n\n```python\nimport_all_models()\n\n    connection = settings.engine.connect()\n    if get_dialect_name(session) == \"mysql\":\n```"}, {"role": "assistant", "content": "There is quite a bit of logic here specific for MySQL, maybe it is a good idea to separate it to another method"}], "_meta": {"repo": "apache/airflow", "pr_number": 59218, "url": "https://github.com/apache/airflow/pull/59218#discussion_r2645203744"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_compat.py`:\n\n```python\nexcept ImportError:\n        brotli = None\n\n# Zstandard support is optional\ntry:\n    import zstandard as zstd\nexcept (AttributeError, ImportError, ValueError):  # Defensive:\n    zstd = False\nelse:\n    # The package 'zstandard' added the 'eof' property starting\n    # in v0.18.0 which we require to ensure a complete and\n    # valid zstd stream was fed into the ZstdDecoder.\n    # See: https://github.com/urllib3/urllib3/pull/2624\n    _zstd_version = tuple(\n        map(int, re.search(r\"^([0-9]+)\\.([0-9]+)\", zstd.__version__).groups())  # type: ignore[union-attr]\n    )\n    if _zstd_version < (0, 18):  # Defensive:\n        zstd = False\n```"}, {"role": "assistant", "content": "zstandard is an optional install. So if your `requirements.txt` just has `httpx` and not `httpx[zstd]`, and you have `zstandard==[old version]` somehow, this would prevent breakage."}], "_meta": {"repo": "encode/httpx", "pr_number": 3139, "url": "https://github.com/encode/httpx/pull/3139#discussion_r1518815892"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_downloadermiddleware.py`:\n\n```python\nfrom pytest import mark\nfrom twisted.internet import defer\nfrom twisted.internet.defer import Deferred\nfrom twisted.internet.defer import Deferred, inlineCallbacks\nfrom twisted.python.failure import Failure\nfrom twisted.trial.unittest import TestCase\n\nfrom scrapy.core.downloader.middleware import DownloaderMiddlewareManager\nfrom scrapy.exceptions import _InvalidOutput\nfrom scrapy.http import Request, Response\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.test import get_crawler, get_from_asyncio_queue\nfrom tests.spiders import NoRequestsSpider\n\n\nclass ManagerTestCase(TestCase):\n    settings_dict = None\n\n    @inlineCallbacks\n    def setUp(self):\n        self.crawler = get_crawler(Spider, self.settings_dict)\n        self.spider = self.crawler._create_spider(\"foo\")\n        self.crawler = get_crawler(NoRequestsSpider, self.settings_dict)\n        yield self.crawler.crawl()\n        self.spider = self.crawler.spider\n        self.mwman = DownloaderMiddlewareManager.from_crawler(self.crawler)\n        # some mw depends on stats collector\n        self.crawler.stats.open_spider(self.spider)\n        return self.mwman.open_spider(self.spider)\n        yield self.mwman.open_spider(self.spider)\n\n    def tearDown(self):\n        self.crawler.stats.close_spider(self.spider, \"\")\n        return self.mwman.close_spider(self.spider)\n```"}, {"role": "assistant", "content": "> The crawler should be \"closed\" when yield crawl() returns.\r\n\r\nHm, why would it be guaranteed? It seems crawler.crawl waits until `self.engine.start` is successful, but it doesn't wait for the crawling to be stopped."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6038, "url": "https://github.com/scrapy/scrapy/pull/6038#discussion_r1321483215"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_dag_serialization.py`:\n\n```python\nassert deserialized_task.resources == task.resources\n        assert isinstance(deserialized_task.resources, Resources)\n\n    def test_template_field_via_callable_serialization(self):\n        \"\"\"\n        Test operator template fields serialization when provided as a callable.\n        \"\"\"\n\n        def fn_template_field_callable(context, jinja_env):\n            pass\n\n        def fn_returns_callable():\n            def get_arg(context, jinja_env):\n                pass\n\n            return get_arg\n\n        task = MockOperator(task_id=\"task1\", arg1=fn_template_field_callable, arg2=fn_returns_callable())\n        serialized_task = OperatorSerialization.serialize_operator(task)\n        assert (\n            serialized_task.get(\"arg1\")\n            == \"<function unit.serialization.test_dag_serialization.TestStringifiedDAGs.test_template_field_via_callable_serialization.<locals>.fn_template_field_callable>\"\n```"}, {"role": "assistant", "content": "```suggestion\n            == \"<callable unit.serialization.test_dag_serialization.TestStringifiedDAGs.test_template_field_via_callable_serialization.<locals>.fn_template_field_callable>\"\n```"}], "_meta": {"repo": "apache/airflow", "pr_number": 60065, "url": "https://github.com/apache/airflow/pull/60065#discussion_r2667318209"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `httpcache.py`:\n\n```python\nresponse = respcls(url=url, headers=headers, status=status, body=body)\n        return response\n\n    def store_response(self, spider, request, response):\n    def store_response(self, spider: Spider, request: Request, response):\n        \"\"\"Store the given response in the cache.\"\"\"\n        rpath = self._get_request_path(spider, request)\n        if not os.path.exists(rpath):\n            os.makedirs(rpath)\n        if not rpath.exists():\n            rpath.mkdir(parents=True)\n        metadata = {\n            'url': request.url,\n            'method': request.method,\n            'status': response.status,\n            'response_url': response.url,\n            'timestamp': time(),\n        }\n        with self._open(os.path.join(rpath, 'meta'), 'wb') as f:\n        with self._open(rpath / 'meta', 'wb') as f:\n            f.write(to_bytes(repr(metadata)))\n        with self._open(os.path.join(rpath, 'pickled_meta'), 'wb') as f:\n        with self._open(rpath / 'pickled_meta', 'wb') as f:\n            pickle.dump(metadata, f, protocol=4)\n        with self._open(os.path.join(rpath, 'response_headers'), 'wb') as f:\n        with self._open(rpath / 'response_headers', 'wb') as f:\n            f.write(headers_dict_to_raw(response.headers))\n        with self._open(os.path.join(rpath, 'response_body'), 'wb') as f:\n        with self._open(rpath / 'response_body', 'wb') as f:\n            f.write(response.body)\n        with self._open(os.path.join(rpath, 'request_headers'), 'wb') as f:\n        with self._open(rpath / 'request_headers', 'wb') as f:\n            f.write(headers_dict_to_raw(request.headers))\n        with self._open(os.path.join(rpath, 'request_body'), 'wb') as f:\n        with self._open(rpath / 'request_body', 'wb') as f:\n            f.write(request.body)\n\n    def _get_request_path(self, spider, request):\n    def _get_request_path(self, spider: Spider, request: Request) -> Path:\n        key = self._fingerprinter.fingerprint(request).hex()\n        return os.path.join(self.cachedir, spider.name, key[0:2], key)\n        return Path(self.cachedir, spider.name, key[0:2], key)\n```"}, {"role": "assistant", "content": "That's a risky change; even if a method is declared as private, it's still overridden in the wild, e.g. https://github.com/scrapy-plugins/scrapy-splash/blob/f5273b3a0ef15a9f6809d305a1dbe8f3efd12c5f/scrapy_splash/cache.py#L17"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5682, "url": "https://github.com/scrapy/scrapy/pull/5682#discussion_r1000653920"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_black.py`:\n\n```python\ndef test_required_version_matches_version(self) -> None:\n        self.invokeBlack(\n            [\"--required-version\", black.__version__], exit_code=0, ignore_config=True\n            [\"--required-version\", black.__version__, \"-c\", \"0\"],\n            exit_code=0,\n            ignore_config=True,\n        )\n\n    def test_required_version_does_not_match_version(self) -> None:\n        self.invokeBlack(\n            [\"--required-version\", \"20.99b\"], exit_code=1, ignore_config=True\n            [\"--required-version\", \"20.99b\", \"-c\", \"0\"], exit_code=1, ignore_config=True\n```"}, {"role": "assistant", "content": "Technically still would succeed without the dummy code, but the exit code can't tell the source. Should we assert on error message?"}], "_meta": {"repo": "psf/black", "pr_number": 2804, "url": "https://github.com/psf/black/pull/2804#discussion_r790599681"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `check_config_attributes.py`:\n\n```python\n\"GPTNeoXConfig\": [\"rotary_emb_base\"],\n    \"ShieldGemma2Config\": [\"mm_tokens_per_image\", \"vision_config\"],\n    \"Llama4VisionConfig\": [\"multi_modal_projector_bias\", \"norm_eps\"],\n    \"ModernBertConfig\": [\"local_attention\"],\n```"}, {"role": "assistant", "content": "I think this changed on main? Might need to double-check after merge"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43030, "url": "https://github.com/huggingface/transformers/pull/43030#discussion_r2699268914"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `urllength.py`:\n\n```python\nelse:\n                return True\n\n        return (r for r in result or () if _filter(r))\n        @_process_iterable_universal\n        async def process(result):\n            async for r in result or ():\n                if _filter(r):\n                    yield r\n\n        return process(result)\n```"}, {"role": "assistant", "content": "It's a tricky question and I'm going to add docs for this question and maybe do some more research when the current situation is documented. Old sync middlewares indeed cannot work with async callbacks at least with the code in this PR."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4978, "url": "https://github.com/scrapy/scrapy/pull/4978#discussion_r603171373"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_buffer.py`:\n\n```python\nwith self.assertRaises(SystemError):\n            obj.__buffer__(inspect.BufferFlags.WRITE)\n\n    @support.cpython_only\n    @unittest.skipIf(_testcapi is None, \"requires _testcapi\")\n    @unittest.skipIf(struct is None, \"requires struct\")\n```"}, {"role": "assistant", "content": "I think the `struct` / `_struct` module is always compiled in with `CPython` nowadays so this skipIf can be removed as it's covered by `cpython_only`"}], "_meta": {"repo": "python/cpython", "pr_number": 140559, "url": "https://github.com/python/cpython/pull/140559#discussion_r2666997247"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_generate_schema.py`:\n\n```python\n):\n            schema = existing_schema\n        elif (validators := getattr(obj, '__get_validators__', None)) is not None:\n            warn(\n                '`__get_validators__` is deprecated and will be removed, use `__get_pydantic_core_schema__` instead.',\n                PydanticDeprecatedSince20,\n            )\n            from pydantic.v1 import BaseModel as BaseModelV1\n\n            if issubclass(obj, BaseModelV1):\n```"}, {"role": "assistant", "content": "```suggestion\r\n            if inspect.isclass(obj) and issubclass(obj, BaseModelV1):\r\n```\r\nBecause `issubclass` raises a `TypeError` if `obj` is not a class. But I'm wondering if it's possible for `obj` to have `__get_validators__` and not being a class in this context, so probably not necessary to add the `isclass` check."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10431, "url": "https://github.com/pydantic/pydantic/pull/10431#discussion_r1763678637"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `connections.py`:\n\n```python\ntry:\n        data = test_body.model_dump(by_alias=True)\n        data[\"conn_id\"] = transient_conn_id\n\n        if use_existing_credentials:\n            try:\n                existing_conn = Connection.get_connection_from_secrets(test_body.connection_id)\n```"}, {"role": "assistant", "content": "I was suggesting that in my previous comment, but I realized that `get_connection_from_secrets` already returns an orm instance, so it will not be a big effort to support private backend connection too. We can keep the `get_connection_from_secrets` to allow testing all connections, just get rid of `use_existing_credentials`, use the utility method to merge the object, test the connection, and discard the merged orm object before returning."}], "_meta": {"repo": "apache/airflow", "pr_number": 59643, "url": "https://github.com/apache/airflow/pull/59643#discussion_r2686753255"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `initialization.py`:\n\n```python\nreturn tensor\n\n\n# Here, we need to check several modules imported, and hot patch all of them, as sometimes torch does\n# something like `from torch.nn.init import xavier_uniform_` in their internals (e.g in torch.nn.modules.activations,\n# where MultiHeadAttention lives), so the function name is binded at import time and just doing\n# `setattr(torch.nn.init, name, gloabls()[name])` is thus not enough\n```"}, {"role": "assistant", "content": "Typo in comment: 'gloabls' should be 'globals'.\n```suggestion\n# `setattr(torch.nn.init, name, globals()[name])` is thus not enough\n```"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42260, "url": "https://github.com/huggingface/transformers/pull/42260#discussion_r2539083450"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `sessions.py`:\n\n```python\nreturn r\n\n    def merge_environment_settings(self, url, proxies, stream, verify, cert):\n        \"\"\"Checks the environment and merges it with some settings.\"\"\"\n        # Gather clues from the surrounding environment.\n        if self.trust_env:\n            # Set environment's proxies.\n            env_proxies = get_environ_proxies(url) or {}\n            for (k, v) in env_proxies.items():\n                proxies.setdefault(k, v)\n\n            # Look for configuration.\n            if verify is True or verify is None:\n                verify = os.environ.get('REQUESTS_CA_BUNDLE')\n\n            # Curl compatibility.\n            if verify is True or verify is None:\n                verify = os.environ.get('CURL_CA_BUNDLE')\n```"}, {"role": "assistant", "content": "I agree. I never liked the duplicated check, but didn't want to add noise to the PR =P"}], "_meta": {"repo": "psf/requests", "pr_number": 2178, "url": "https://github.com/psf/requests/pull/2178#discussion_r16636190"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `mongodb.py`:\n\n```python\n\"\"\"Get task meta-data for a task by id.\"\"\"\n        obj = self.collection.find_one({'_id': task_id})\n        if obj:\n            if self.app.conf.find_value_for_key('extended', 'result'):\n                return self.meta_from_decoded({\n```"}, {"role": "assistant", "content": "we will need full unit test coverage for this change"}], "_meta": {"repo": "celery/celery", "pr_number": 8391, "url": "https://github.com/celery/celery/pull/8391#discussion_r1273497383"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `peft.py`:\n\n```python\nis_trainable (`bool`, *optional*, defaults to `False`):\n                Whether the adapter should be trainable or not. If `False`, the adapter will be frozen and can only be\n                used for inference.\n            hotswap : (`\"auto\"` or `bool`, *optional*, defaults to `\"auto\"`)\n                Whether to substitute an existing (LoRA) adapter with the newly loaded adapter in-place. This means\n                that, instead of loading an additional adapter, this will take the existing adapter weights and replace\n                them with the weights of the new adapter. This can be faster and more memory efficient. However, the\n                main advantage of hotswapping is that when the model is compiled with torch.compile, loading the new\n                adapter does not require recompilation of the model. When using hotswapping, the passed `adapter_name`\n                should be the name of an already loaded adapter.\n\n                If the new adapter and the old adapter have different ranks and/or LoRA alphas (i.e. scaling), you need\n                to call an additional method before loading the adapter:\n\n                ```py\n                model = AutoModel.from_pretrained(...)\n                max_rank = ...  # the highest rank among all LoRAs that you want to load\n                # call *before* compiling and loading the LoRA adapter\n                model.enable_peft_hotswap(target_rank=max_rank)\n                model.load_adapter(file_name_1, adapter_name=\"default\")\n                # optionally compile the model now\n                model = torch.compile(model, ...)\n                output_1 = model(...)\n                # now you can hotswap the 2nd adapter, use the same name as for the 1st\n                # hotswap is activated by default since enable_peft_hotswap was called\n                model.load_adapter(file_name_2, adapter_name=\"default\")\n                output_2 = model(...)\n                ```\n\n                By default, hotswap is disabled and requires passing `hotswap=True`. If you called\n                `enable_peft_hotswap` first, it is enabled. You can still manually disable it in that case by passing\n                `hotswap=False`.\n\n                Note that hotswapping comes with a couple of limitations documented here:\n                https://huggingface.co/docs/peft/main/en/package_reference/hotswap\n            adapter_kwargs (`dict[str, Any]`, *optional*):\n                Additional keyword arguments passed along to the `from_pretrained` method of the adapter config and\n                `find_adapter_config_file` method.\n        \"\"\"\n        from peft import PeftType\n```"}, {"role": "assistant", "content": "Does that import exist even for the `MIN_VERSION` below? If not, let's import after the check, so that we have gracious crash"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41297, "url": "https://github.com/huggingface/transformers/pull/41297#discussion_r2436584896"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_aliases.py`:\n\n```python\nfrom typing import Type, Union\n\nimport pytest\nfrom pydantic import VERSION, BaseModel, ValidationError\nfrom pydantic import Field as PField\nfrom sqlmodel import Field, SQLModel\n\n# -----------------------------------------------------------------------------------\n# Models\n```"}, {"role": "assistant", "content": "We will need to remove unnecessary comments in final version"}], "_meta": {"repo": "tiangolo/sqlmodel", "pr_number": 1577, "url": "https://github.com/fastapi/sqlmodel/pull/1577#discussion_r2392446448"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `exporters.py`:\n\n```python\nimport csv\nimport io\nimport pprint\nimport marshal\nimport warnings\nimport pickle\nimport pprint\nimport warnings\nfrom xml.sax.saxutils import XMLGenerator\n\nfrom scrapy.utils.serialize import ScrapyJSONEncoder\nfrom scrapy.utils.python import to_bytes, to_unicode, is_listlike\nfrom scrapy.item import _BaseItem\nfrom itemadapter import ItemAdapter, is_item\n```"}, {"role": "assistant", "content": ":lipstick: \r\n\r\n```suggestion\r\nfrom itemadapter import is_item, ItemAdapter\r\n```"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 3881, "url": "https://github.com/scrapy/scrapy/pull/3881#discussion_r426709638"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_opt.py`:\n\n```python\nself.assertIsNotNone(ex)\n        uops = get_opnames(ex)\n        self.assertIn(\"_BINARY_OP_EXTEND\", uops)\n        self.assertIn(\"_POP_TOP_NOP\", uops)\n```"}, {"role": "assistant", "content": "Sorry, this was removed by accident during the merge, resolved again."}], "_meta": {"repo": "python/cpython", "pr_number": 144011, "url": "https://github.com/python/cpython/pull/144011#discussion_r2723472089"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `spider.py`:\n\n```python\nd = deferred_from_coro(collect_asyncgen(result))\n        d.addCallback(iterate_spider_output)\n        return d\n    return arg_to_iter(deferred_from_coro(result))\n    elif inspect.iscoroutine(result):\n        d = deferred_from_coro(result)\n        d.addCallback(iterate_spider_output)\n        return d\n    return arg_to_iter(result)\n```"}, {"role": "assistant", "content": "It's safe to remove the `deferred_from_coro` call here, because at this point this condition is `False` so the passed object is returned unmodified."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4609, "url": "https://github.com/scrapy/scrapy/pull/4609#discussion_r433466521"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `context.py`:\n\n```python\n)\n\n\nclass Context(TypedDict, total=False):\nclass Context(TypedDict):\n    \"\"\"Jinja2 template context for task rendering.\"\"\"\n\n    conn: Any\n    dag: DAG\n    dag_run: DagRunProtocol\n    data_interval_end: DateTime | None\n    data_interval_start: DateTime | None\n    data_interval_end: NotRequired[DateTime | None]\n```"}, {"role": "assistant", "content": "Nice. However this was added in Python 3.11, so we'll have to pull this from `typing_extension` if it's not found in the `typing` module"}], "_meta": {"repo": "apache/airflow", "pr_number": 54421, "url": "https://github.com/apache/airflow/pull/54421#discussion_r2304081116"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_multi_body_errors.py`:\n\n```python\n\"title\": \"Age\",\n                                \"anyOf\": [\n                                    {\"exclusiveMinimum\": 0.0, \"type\": \"number\"},\n                                    {\"type\": \"string\"},\n                                    IsOneOf(\n                                        # pydantic < 2.12.0a1\n                                        {\"type\": \"string\"},\n                                        # pydantic >= 2.12.0a1\n```"}, {"role": "assistant", "content": "```suggestion\r\n                                        # pydantic >= 2.12.0\r\n```"}], "_meta": {"repo": "fastapi/fastapi", "pr_number": 14036, "url": "https://github.com/fastapi/fastapi/pull/14036#discussion_r2369203300"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_modeling_glm4v_moe.py`:\n\n```python\n@require_torch\n@slow\nclass Glm4vMoeIntegrationTest(unittest.TestCase):\n    model = None\n    @classmethod\n    def setUpClass(cls):\n        cls.model = None\n\n    @classmethod\n    def get_model(cls):\n        if cls.model is None:\n        if not hasattr(cls, \"model\") or cls.model is None:\n```"}, {"role": "assistant", "content": "we don't need `not hasattr(cls, \"model\")`. Any reason you add this?"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42488, "url": "https://github.com/huggingface/transformers/pull/42488#discussion_r2580470989"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `adapters.py`:\n\n```python\nproxy_scheme = urlparse(proxy).scheme.lower()\n            using_socks_proxy = proxy_scheme.startswith(\"socks\")\n\n        url = request.path_url\n        url = re.sub(\"^/+\", \"/\", request.path_url)\n```"}, {"role": "assistant", "content": "Yeah, I'm also happy to shove this into a branch too like \n\n```python\nif path.startswith('//'):\n```\n\nTo make it clearer that we only care about the separator being repeated. What I want is clarity in the reader as to why we're doing this. My old school brain things the regexp is clearer and the f-string looks sus but  that's just my opinion and I'm not holding it closely"}], "_meta": {"repo": "psf/requests", "pr_number": 6644, "url": "https://github.com/psf/requests/pull/6644#discussion_r1499711431"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `bitnet.py`:\n\n```python\nreturn output\n\n\ndef _replace_with_bitnet_linear(\n    model,\n    modules_to_not_convert=None,\n    current_key_name=None,\n    quantization_config=None,\n    has_been_replaced=False,\n    pre_quantized=False,\n):\ndef replace_with_bitnet_linear(model, modules_to_not_convert: list[str] | None = None, quantization_config=None):\n    \"\"\"\n    Private method that wraps the recursion for module replacement.\n    Public method that recursively replaces the kinear layers of the given model with bitnet quantized layers.\n```"}, {"role": "assistant", "content": "```suggestion\n    Public method that replaces the linear layers of the given model with bitnet quantized layers.\n```"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42711, "url": "https://github.com/huggingface/transformers/pull/42711#discussion_r2602584343"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modular_glm_image.py`:\n\n```python\nself,\n        pixel_values: torch.FloatTensor,\n        image_grid_thw: torch.LongTensor | None = None,\n        return_dict: bool | None = None,\n```"}, {"role": "assistant", "content": "I tested Diffusers Batch Support and PEFT finetune, with `uv pip install -e ~/transformers`, so it's working well with return_dict=False. If u think get rid of the `return_dict` in `GlmImageForConditionalGeneration.get_image_feature` is necessary, I will go change both side"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43342, "url": "https://github.com/huggingface/transformers/pull/43342#discussion_r2737033966"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_generate_schema.py`:\n\n```python\nsource_type, annotations = res\n            return self._apply_annotations(source_type, annotations)\n\n        origin = get_origin(obj)\n        origin = get_origin(obj) or getattr(obj, '__origin__', None)\n```"}, {"role": "assistant", "content": "I remember needing it for some case but no tests fail and I forgot what it was so removing it."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 6737, "url": "https://github.com/pydantic/pydantic/pull/6737#discussion_r1271291968"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modular_florence2.py`:\n\n```python\n_supports_flex_attn = True\n\n    _can_compile_fullgraph = True\n    _can_record_outputs = {\"hidden_states\": Florence2VisionBlock}\n```"}, {"role": "assistant", "content": "Looks like there are also attentions that have been ignored before? https://github.com/huggingface/transformers/blob/a41491fce14b476d44bdc76acaf5b8bd1dadba4a/src/transformers/models/florence2/modular_florence2.py#L1097"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42564, "url": "https://github.com/huggingface/transformers/pull/42564#discussion_r2691955387"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_downloader_handlers.py`:\n\n```python\nclass FileTestCase(unittest.TestCase):\n\n    def setUp(self):\n        self.tmpname = self.mktemp()\n        with open(self.tmpname + '^', 'w') as f:\n            f.write('0123456789')\n        self.tmpname = Path(self.mktemp() + '^')\n```"}, {"role": "assistant", "content": "Previous time I've noticed it: https://github.com/scrapy/scrapy/pull/5285/files#r844017508"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5682, "url": "https://github.com/scrapy/scrapy/pull/5682#discussion_r1030092597"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `canvas.py`:\n\n```python\napplied to the body.\n        \"\"\"\n        if self.app.conf.task_allow_error_cb_on_chord_header:\n            # self.tasks can be a list of the chord header workflow.\n            if isinstance(self.tasks, (list, tuple)):\n                for task in self.tasks:\n                    task.link_error(errback)\n            else:\n                self.tasks.link_error(errback)\n            for task in self.tasks:\n                task.link_error(errback)\n```"}, {"role": "assistant", "content": "There's no problem with that, `self.tasks` is accessed correctly with both examples.\r\nTo prove it, I've added the PR a test case with your both examples: `test_link_error_on_chord_header()`\r\nCheck it out to confirm and resolve conversation if you're satisfied"}], "_meta": {"repo": "celery/celery", "pr_number": 7942, "url": "https://github.com/celery/celery/pull/7942#discussion_r1043443321"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `main.py`:\n\n```python\nsa_relationship_args: Optional[Sequence[Any]] = None,\n    sa_relationship_kwargs: Optional[Mapping[str, Any]] = None,\n) -> Any:\n    relationship_info = RelationshipInfo(\n    return RelationshipInfo(\n        back_populates=back_populates,\n        link_model=link_model,\n        sa_relationship=sa_relationship,\n        sa_relationship_args=sa_relationship_args,\n        sa_relationship_kwargs=sa_relationship_kwargs,\n    )\n    return relationship_info\n```"}, {"role": "assistant", "content": "I actually like to keep these variables returned separated when they become useful for debugging, for running the code through the debugger, so I would prefer to keep this one."}], "_meta": {"repo": "tiangolo/sqlmodel", "pr_number": 53, "url": "https://github.com/fastapi/sqlmodel/pull/53#discussion_r956620111"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `base.py`:\n\n```python\nstate, traceback, request, format_date=True,\n                         encode=False):\n        if state in self.READY_STATES:\n            date_done = datetime.utcnow()\n            date_done = self.app.now()\n```"}, {"role": "assistant", "content": "+1 -- https://docs.python.org/3/library/datetime.html#datetime.datetime.utcnow is deprecated in Python 3.12"}], "_meta": {"repo": "celery/celery", "pr_number": 8385, "url": "https://github.com/celery/celery/pull/8385#discussion_r1426410379"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `processing_utils.py`:\n\n```python\n- `'np'`: Return NumPy `np.ndarray` objects.\n        disable_grouping (`bool`, *optional*):\n            Whether to group images by shapes when processing or not, only relevant for fast image processing.\n        image_seq_length (`int`, *optional*):\n            The number of image tokens to be used for each image in the input.\n            Added for backward compatibility but this should be set as a processor attribute in future models.\n```"}, {"role": "assistant", "content": "sorry for nitty-picking after PR is merged, For my own understanding, do we need it in here, because not all image processors are VLM-specific and `image_seq_length` isn't always needed for them"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41997, "url": "https://github.com/huggingface/transformers/pull/41997#discussion_r2494805853"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `datafusion.py`:\n\n```python\n@staticmethod\n    def _check_response_status_and_data(response, message: str) -> None:\n        if not response:\n            raise AirflowException(\n                \"Invalid / Empty response received. Please, check for possible root \"\n                \"causes of this behavior either in DAG code or on Cloud DataFusion side\"\n            )\n```"}, {"role": "assistant", "content": "I'd like to ask to refactor all usages of `AirflowException` in the code, as according to a recent policy we want to avoid further usage of this exception at all (more about that in the dev list thread).\nYou may use a simple `ValueError` exception instead."}], "_meta": {"repo": "apache/airflow", "pr_number": 60688, "url": "https://github.com/apache/airflow/pull/60688#discussion_r2724290110"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `asyncio_deferred_signal.py`:\n\n```python\nimport asyncio\nimport sys\n\nfrom scrapy import Spider\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.defer import deferred_from_coro\nfrom twisted.internet.defer import Deferred\n\n\nclass UppercasePipeline:\n    async def _open_spider(self, spider):\n        spider.logger.info(\"async pipeline opened!\")\n        await asyncio.sleep(0.1)\n\n    def open_spider(self, spider):\n        loop = asyncio.get_event_loop()\n```"}, {"role": "assistant", "content": "Good eye, it is not needed. I wonder why the flake8 check didn't fail \ud83e\udd14"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4872, "url": "https://github.com/scrapy/scrapy/pull/4872#discussion_r519968939"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `functional_validators.py`:\n\n```python\n\"\"\"\n\n    func: core_schema.NoInfoValidatorFunction | core_schema.WithInfoValidatorFunction\n    input_type: Any = PydanticUndefined\n\n    def __get_pydantic_core_schema__(self, source_type: Any, handler: _GetCoreSchemaHandler) -> core_schema.CoreSchema:\n    def __get_pydantic_core_schema__(self, source_type: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        schema = handler(source_type)\n        input_schema = None if self.input_type is PydanticUndefined else handler.generate_schema(self.input_type)\n        metadata = {'input_schema': input_schema}\n```"}, {"role": "assistant", "content": "I'm using the `metadata` field to hold this information, instead of creating a new attribute in the relevant core schemas. That is because `input_schema` is only relevant for JSON Schema generation (unlike serializers where `return_schema` is actually used by `pydantic-core`)."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10094, "url": "https://github.com/pydantic/pydantic/pull/10094#discussion_r1712164434"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `__init__.py`:\n\n```python\nEntry point for running commands\n        \"\"\"\n        raise NotImplementedError\n\n\n''''\nThe BaseRunSpiderCommands class inherits the ScrapyCommand class and it Used for\nperforming common functionality between crawl.py and runspider.py\n'''\n```"}, {"role": "assistant", "content": "Move this right under the `class \u2026` line below, indented."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4552, "url": "https://github.com/scrapy/scrapy/pull/4552#discussion_r422501868"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_commands.py`:\n\n```python\nspider_filename = 'myspider.pyw'\n\n    def setUp(self):\n        if platform != 'win32':\n            raise unittest.SkipTest(\".pyw file extension only supported in Windows\")\n        super(WindowsRunSpiderCommandTest, self).setUp()\n\n    if platform != 'win32':\n        def test_run_good_spider(self):\n            raise unittest.SkipTest(\"Windows required\")\n\n        def test_runspider(self):\n            raise unittest.SkipTest(\"Windows required\")\n\n        def test_runspider_dnscache_disabled(self):\n            raise unittest.SkipTest(\"Windows required\")\n\n        def test_runspider_log_level(self):\n            raise unittest.SkipTest(\"Windows required\")\n\n        def test_runspider_log_short_names(self):\n            raise unittest.SkipTest(\"Windows required\")\n\n        def test_runspider_no_spider_found(self):\n            raise unittest.SkipTest(\"Windows required\")\n```"}, {"role": "assistant", "content": "> In any case, do all of those test fail on Linux?\r\n\r\nI don't think there is any case for it."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4646, "url": "https://github.com/scrapy/scrapy/pull/4646#discussion_r449263696"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_extra_routes.py`:\n\n```python\n@app.head(\"/items/{item_id}\")\ndef head_item(item_id: str):\n    return JSONResponse(headers={\"x-fastapi-item-id\": item_id})\n    return JSONResponse(None, headers={\"x-fastapi-item-id\": item_id})\n```"}, {"role": "assistant", "content": "If there is no body, it shouldn't be a JSONResponse:\r\n\r\n```suggestion\r\n    return Response(headers={\"x-fastapi-item-id\": item_id})\r\n```"}], "_meta": {"repo": "fastapi/fastapi", "pr_number": 4488, "url": "https://github.com/fastapi/fastapi/pull/4488#discussion_r818190708"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `connection.py`:\n\n```python\nif self.extra:\n            try:\n                query: str | None = urlencode(self.extra_dejson)\n            except TypeError:\n                query = None\n            if query and self.extra_dejson == dict(parse_qsl(query, keep_blank_values=True)):\n                uri += (\"?\" if self.schema else \"/?\") + query\n            else:\n                extra_dict = self.extra_dejson\n                can_flatten = True\n                for _, value in extra_dict.items():\n                    if not isinstance(value, str):\n                        can_flatten = False\n                        break\n\n                if can_flatten:\n                    query = urlencode(extra_dict)\n                    if extra_dict == dict(parse_qsl(query, keep_blank_values=True)):\n                        uri += (\"?\" if self.schema else \"/?\") + query\n                    else:\n                        uri += (\"?\" if self.schema else \"/?\") + urlencode({self.EXTRA_KEY: self.extra})\n                else:\n                    uri += (\"?\" if self.schema else \"/?\") + urlencode({self.EXTRA_KEY: self.extra})\n            except (TypeError, AttributeError):\n```"}, {"role": "assistant", "content": "It seems the whole flow could be simplify as:\n\n```python\ncan_flatten = True\n# the for loop to check whether could we flatten \n\ntry:\n    query: str | None = urlencode(self.extra_dejson)\nexcept TypeError:\n    query = None\n\n# we just need to add can_flatten condition check\nif can_flatten and query and self.extra_dejson == dict(parse_qsl(query, keep_blank_values=True)):\n    uri += (\"?\" if self.schema else \"/?\") + query\nelse:\n# fallback to nested EXTRA_KEY\n    uri += (\"?\" if self.schema else \"/?\") + urlencode({self.EXTRA_KEY: self.extra})\n```\n\nwhich also minimized the change."}], "_meta": {"repo": "apache/airflow", "pr_number": 59339, "url": "https://github.com/apache/airflow/pull/59339#discussion_r2625584065"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_test_multiprocessing.py`:\n\n```python\n\"resource_tracker: There appear to be 1 leaked \"\n                    \"shared_memory objects to clean up at shutdown\", err)\n\n    @unittest.skipIf(os.name != \"posix\", \"resource_tracker is posix only\")\n    def test_shared_memory_untracking(self):\n        # gh-82300: When a separate Python process accesses shared memory\n        # with track=False, it must not register with the resource tracker.\n```"}, {"role": "assistant", "content": "The sole effect of `track=False` is that `register` is not called. Thus it would be faulty behavior if `register` is called when `track=False`. The function checks the error message from the subprocess and will fail when that happens."}], "_meta": {"repo": "python/cpython", "pr_number": 110778, "url": "https://github.com/python/cpython/pull/110778#discussion_r1365319245"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `ftp.py`:\n\n```python\ncreator = ClientCreator(\n            reactor, FTPClient, user, password, passive=passive_mode\n        )\n        dfd: Deferred[FTPClient] = creator.connectTCP(\n            parsed_url.hostname, parsed_url.port or 21\n        client: FTPClient = await maybe_deferred_to_future(\n            creator.connectTCP(parsed_url.hostname, parsed_url.port or 21)\n        )\n        return dfd.addCallback(self.gotClient, request, unquote(parsed_url.path))\n\n    def gotClient(\n        self, client: FTPClient, request: Request, filepath: str\n    ) -> Deferred[Response]:\n        self.client = client\n```"}, {"role": "assistant", "content": "Never used in Scrapy (and changing it on every request makes it not useful)."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 7164, "url": "https://github.com/scrapy/scrapy/pull/7164#discussion_r2572893669"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `content.py`:\n\n```python\nimport typing\nfrom json import dumps as json_dumps\nfrom urllib.parse import urlencode\n\nfrom .multipart import multipart_encode\n\nRequestData = typing.Union[dict, str, bytes, typing.AsyncIterator[bytes]]\n\nRequestFiles = typing.Dict[\n    str,\n    typing.Union[\n        # file (or str)\n        typing.Union[typing.IO[typing.AnyStr], typing.AnyStr],\n        # (filename, file (or str))\n        typing.Tuple[\n            typing.Optional[str], typing.Union[typing.IO[typing.AnyStr], typing.AnyStr],\n        ],\n        # (filename, file (or str), content_type)\n        typing.Tuple[\n            typing.Optional[str],\n            typing.Union[typing.IO[typing.AnyStr], typing.AnyStr],\n            typing.Optional[str],\n        ],\n    ],\n]\n\n\nclass RequestContent:\n    \"\"\"\n    Base class for request content.\n    Defaults to a \"no request body\" implementation.\n    \"\"\"\n\n    def get_headers(self) -> typing.Dict[str, str]:\n        return {}\n\n    def can_rewind(self) -> bool:\n        return True\n```"}, {"role": "assistant", "content": "What is this property supposed to enable in practice? Does that mean we can `async for chunk in content` again once we've done so at least once? I can't think of better names (`.is_replayable()`? `.can_replay()`? `.can_read_multiple_times`? `.is_isomorphic()`? \ud83e\udd23), but maybe a docstring on this base implementation could help."}], "_meta": {"repo": "encode/httpx", "pr_number": 636, "url": "https://github.com/encode/httpx/pull/636#discussion_r357912684"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `video_processing_qwen3_vl.py`:\n\n```python\nresized_videos_grouped = {}\n\n        for shape, stacked_videos in grouped_videos.items():\n            B, T, C, H, W = stacked_videos.shape\n            num_frames, height, width = T, H, W\n            if do_resize:\n                T = stacked_videos.shape[1]\n                if pad := -T % temporal_patch_size:\n                    repeats = stacked_videos[:, -1:].expand(-1, pad, -1, -1, -1)\n                    stacked_videos = torch.cat((stacked_videos, repeats), dim=1)\n                B, T, C, H, W = stacked_videos.shape\n                num_frames, height, width = T, H, W\n```"}, {"role": "assistant", "content": "Ah interesting. I remember we fixes similar issue with qwen2-vl series in https://github.com/huggingface/transformers/pull/38706\r\n\r\nImo we should not be restricting the inputs to be certain size before resizing, so we can adjust the `t_bar` so it is never zero"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42083, "url": "https://github.com/huggingface/transformers/pull/42083#discussion_r2509274897"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `text_to_audio.py`:\n\n```python\nreturn preprocess_params, params, postprocess_params\n\n    def postprocess(self, audio):\n        output_dict = {}\n\n        if self.model.config.model_type == \"csm\":\n            waveform_key = \"audio\"\n        else:\n            waveform_key = \"waveform\"\n\n        # We directly get the waveform\n        if self.no_processor:\n            if isinstance(audio, dict):\n                waveform = audio[waveform_key]\n            elif isinstance(audio, tuple):\n                waveform = audio[0]\n        needs_decoding = False\n        if isinstance(audio, dict):\n            if \"audio\" in audio:\n                audio = audio[\"audio\"]\n            else:\n                waveform = audio\n        # Or we need to postprocess to get the waveform\n        else:\n            waveform = self.processor.decode(audio)\n\n        if isinstance(audio, list):\n            output_dict[\"audio\"] = [el.to(device=\"cpu\", dtype=torch.float).numpy() for el in waveform]\n                needs_decoding = True\n                audio = audio[\"sequences\"]\n        elif isinstance(audio, tuple):\n            audio = audio[0]\n\n        if needs_decoding:\n            audio = self.processor.decode(audio)\n\n        if isinstance(audio, list) and len(audio) > 1:\n            audio = [el.to(device=\"cpu\", dtype=torch.float).numpy() for el in audio]\n        elif isinstance(audio, list):\n            audio = audio[0].to(device=\"cpu\", dtype=torch.float).numpy()\n```"}, {"role": "assistant", "content": "The reasoning is to standardize the output. Because CSM was returning a list even for a single audio, and to be able to do extract the audio with `outputs[\"audio\"]` as in the other models, rather than having to index it like such `outputs[\"audio\"][0]`\r\n\r\n\r\nBut I can change to something like this so it's more concise:\r\n```python\r\nif isinstance(audio, list):\r\n    audio = [el.to(device=\"cpu\", dtype=torch.float).numpy() for el in audio]\r\n    audio = audio if len(audio) > 1 else audio[0]\r\n```"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42326, "url": "https://github.com/huggingface/transformers/pull/42326#discussion_r2565562101"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `decompression.py`:\n\n```python\ndef _is_tar(self, response):\n        archive = BytesIO(response.body)\n        try:\n            tar_file = tarfile.open(name=mktemp(), fileobj=archive)\n        except tarfile.ReadError:\n            return\n        with NamedTemporaryFile() as temporary_file:\n            try:\n                tar_file = tarfile.open(name=temporary_file.name, fileobj=archive)\n```"}, {"role": "assistant", "content": "According to the documentation, the `name` parameter is optional. We added it 13 years ago because \u201c`None` is failing in some cases\u201d. But I have no idea what those cases would be. It may be that the Python version back then had a bug or something.\r\n\r\nI am thinking that we could remove the parameter altogether. If anyone can provide a scenario where it fails, we can try to address that scenario in a point release of Scrapy, this time documenting the issue and adding a test for it, which would be best in the long run."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5285, "url": "https://github.com/scrapy/scrapy/pull/5285#discussion_r850444842"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_modeling_qwen2.py`:\n\n```python\nwith torch.no_grad():\n            out = model(input_ids).logits.float().cpu()\n        # Expected mean on dim = -1\n        EXPECTED_MEAN = torch.tensor([[-1.9537, -1.6193, -1.4123, -1.4673, -1.8511, -1.9309, -1.9826, -2.1776]])\n        EXPECTED_MEAN = torch.tensor([[-2.2121, -1.6335, -1.4816, -1.5035, -1.9110, -1.8979, -1.9682, -2.1980]])\n```"}, {"role": "assistant", "content": "I will run the slow test to see if this does not break things on CUDA, otherwise you should add Expectations"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42879, "url": "https://github.com/huggingface/transformers/pull/42879#discussion_r2626694795"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_utils.py`:\n\n```python\n# If no attention layer, assume `True`. Most probably a multimodal model or inherits from existing models\n            return True\n\n    @classmethod\n    def _can_set_experts_implementation(cls) -> bool:\n        \"\"\"Detect whether the class supports setting its experts implementation dynamically. It is an ugly check based on\n        opening the file, but avoids maintaining yet another property flag.\n        \"\"\"\n        class_file = sys.modules[cls.__module__].__file__\n        with open(class_file, \"r\") as f:\n            code = f.read()\n```"}, {"role": "assistant", "content": "actually sorry, IDK how fast this is having the explicit arg might be better!"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42697, "url": "https://github.com/huggingface/transformers/pull/42697#discussion_r2635722903"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `functional_validators.py`:\n\n```python\n@dataclasses.dataclass(frozen=True, **_internal_dataclass.slots_true)\nclass AfterValidator:\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.10/concepts/validators/#annotated-validators\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.10/concepts/validators/#field-validators\n```"}, {"role": "assistant", "content": "```suggestion\r\n    \"\"\"Usage docs: https://docs.pydantic.dev/2.10/concepts/validators/#field-after-validator\r\n```"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 11060, "url": "https://github.com/pydantic/pydantic/pull/11060#discussion_r1881227442"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `httpcompression.py`:\n\n```python\nreturn response\n\n    def _handle_encoding(self, body, content_encoding):\n        content_to_decode = body\n\n        separator = b\",\"\n        for encodings in reversed(content_encoding):\n            if separator in encodings:\n```"}, {"role": "assistant", "content": "What about handling the comma issue earlier? i.e. (untested)\r\n\r\n```python\r\nencodings = reversed(\r\n    item.strip()\r\n    for value in content_encoding\r\n    for item in value.split(b\",\")\r\n)\r\n```\r\n\r\nThat should simplify the later code."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6063, "url": "https://github.com/scrapy/scrapy/pull/6063#discussion_r1335597711"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `text_to_audio.py`:\n\n```python\nelse vocoder\n            )\n\n        if self.model.config.model_type in [\"musicgen\"]:\n        if self.model.config.model_type in [\"musicgen\", \"speecht5\"]:\n            # MusicGen expect to use the tokenizer\n```"}, {"role": "assistant", "content": "```suggestion\r\n        if self.model.config.model_type in [\"musicgen\", \"speecht5\"]:\r\n            # MusicGen and SpeechT5 expect to use their tokenizer instead\r\n```"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42830, "url": "https://github.com/huggingface/transformers/pull/42830#discussion_r2614151934"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_commands.py`:\n\n```python\nspider_filename = 'myspider.pyw'\n\n    def setUp(self):\n        if platform != 'win32':\n            raise unittest.SkipTest(\".pyw file extension only supported in Windows\")\n        super(WindowsRunSpiderCommandTest, self).setUp()\n\n    if platform != 'win32':\n        def test_run_good_spider(self):\n            raise unittest.SkipTest(\"Windows required\")\n\n        def test_runspider(self):\n            raise unittest.SkipTest(\"Windows required\")\n\n        def test_runspider_dnscache_disabled(self):\n            raise unittest.SkipTest(\"Windows required\")\n\n        def test_runspider_log_level(self):\n            raise unittest.SkipTest(\"Windows required\")\n\n        def test_runspider_log_short_names(self):\n            raise unittest.SkipTest(\"Windows required\")\n\n        def test_runspider_no_spider_found(self):\n            raise unittest.SkipTest(\"Windows required\")\n```"}, {"role": "assistant", "content": "I think you should use a decorator, `if` statements for method definition seem rather unidiomatic.\r\n\r\nI wonder if you can use a skipping decorator or similar at the class level, though."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4646, "url": "https://github.com/scrapy/scrapy/pull/4646#discussion_r461483654"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `mode.py`:\n\n```python\nclass Preview(Enum):\n    \"\"\"Individual preview style features.\"\"\"\n\n    add_trailing_comma_consistently = auto()\n    blank_line_after_nested_stub_class = auto()\n    blank_line_between_nested_and_def_stub_file = auto()\n    hex_codes_in_unicode_sequences = auto()\n```"}, {"role": "assistant", "content": "I believe there's a mistake in the changelog or the flag here is a leftover. The changelog mentions that the hex normalisation shipped, but the preview flag still exists in the code."}], "_meta": {"repo": "psf/black", "pr_number": 4106, "url": "https://github.com/psf/black/pull/4106#discussion_r1485962615"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_requests.py`:\n\n```python\ndef test_json_decode_compatibility(self, httpbin):\n        r = requests.get(httpbin('bytes/20'))\n        with pytest.raises(requests.exceptions.JSONDecodeError):\n        data = r.text\n        with pytest.raises(requests.exceptions.JSONDecodeError) as excinfo:\n            r.json()\n        assert isinstance(excinfo.value, RequestException)\n```"}, {"role": "assistant", "content": "Do we need to use `.value` in these assertions? Just `excinfo` should be sufficient and that's how the typical user will probably interact with them."}], "_meta": {"repo": "psf/requests", "pr_number": 6036, "url": "https://github.com/psf/requests/pull/6036#discussion_r781592203"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `feedexport.py`:\n\n```python\nreturn defer.DeferredList(deferred_list) if deferred_list else None\n\n    def _close_slot(self, slot, spider):\n\n        def get_file(slot_):\n            if isinstance(slot_.file, PostProcessingManager):\n                return slot_.file.file\n```"}, {"role": "assistant", "content": "After a quick glance at the failures, I am thinking we may need to close PostProcessingManager without closing the underlying file. Calling `slot_.close()` here before `return` may be enough."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5581, "url": "https://github.com/scrapy/scrapy/pull/5581#discussion_r932952058"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_pipeline.py`:\n\n```python\n[\n        (\n            str,\n            validate_as(str).len(min_len=2, max_len=5),\n            validate_as(str).len(min_len=2, max_len=5),  # type: ignore\n```"}, {"role": "assistant", "content": "This one was because some of the types like `list` where missing parametrization below (needs to be `list[int]`)"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 9674, "url": "https://github.com/pydantic/pydantic/pull/9674#discussion_r1643525075"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_typing_extra.py`:\n\n```python\nother cases. See https://discuss.python.org/t/is-there-a-way-to-access-parent-nested-namespaces/20659.\n    \"\"\"\n    frame = sys._getframe(parent_depth)\n    # if f_back is None, it's the global module namespace and we don't need to include it here\n    if frame.f_back is None:\n    # if the class is defined at the top module level, we don't need to add namespace information\n    if frame.f_back is None or frame.f_code.co_name == '<module>':\n```"}, {"role": "assistant", "content": "ye, I thought the exact same thing as .\r\n\r\nTypeshed suggests that both `.f_code` and `.co_name` should always exist."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10113, "url": "https://github.com/pydantic/pydantic/pull/10113#discussion_r1715743997"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_test_multiprocessing.py`:\n\n```python\n\"resource_tracker: There appear to be 1 leaked \"\n                    \"shared_memory objects to clean up at shutdown\", err)\n\n    @unittest.skipIf(os.name != \"posix\", \"resource_tracker is posix only\")\n    def test_shared_memory_untracking(self):\n        # gh-82300: When a separate Python process accesses shared memory\n        # with track=False, it must not register with the resource tracker.\n```"}, {"role": "assistant", "content": "The main concern I had with that approach is the indeterminate amount of time between closing the process and the resource tracker kicking in. Right now such a time is not needed because the test waits until stderr is fully available and that includes the resource tracker itself. But that's only due to the way the resource tracker is created. If we do rely on that internal behavior then the current test would be in the same vein. Otherwise, what would a good delay be? A second?"}], "_meta": {"repo": "python/cpython", "pr_number": 110778, "url": "https://github.com/python/cpython/pull/110778#discussion_r1370780307"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `main.py`:\n\n```python\n_object_setattr(self, name, value)\n                    return None  # Can not return memoized handler with possibly freeform attr names\n\n            cls._check_frozen(name, value)\n\n            attr = getattr(cls, name, None)\n            # NOTE: We currently special case properties and `cached_property`, but we might need\n            # to generalize this to all data/non-data descriptors at some point. For non-data descriptors\n            # (such as `cached_property`), it isn't obvious though. `cached_property` caches the value\n            # to the instance's `__dict__`, but other non-data descriptors might do things differently.\n            if isinstance(attr, cached_property):\n                return _SIMPLE_SETATTR_HANDLERS['cached_property']\n\n            model_frozen = cls.model_config.get('frozen')\n```"}, {"role": "assistant", "content": "I inlined the logic as I couldn't find a good way to keep it in a single method as I need to raise it differently in `__delattr__`. A bit unfortunate, but at least this removes the `_check_frozen` method on the `BaseModel` class, so it avoids polluting the namespace."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 11432, "url": "https://github.com/pydantic/pydantic/pull/11432#discussion_r1952489773"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `task.py`:\n\n```python\nProvide a nicer API to trigger tasks at the end of the DB transaction.\n    \"\"\"\n\n    def delay_on_commit(self, *args, **kwargs):\n    def delay_on_commit(self, *args, **kwargs) -> None:\n        \"\"\"Call :meth:`~celery.app.task.Task.delay` with Django's ``on_commit()``.\"\"\"\n        return transaction.on_commit(functools.partial(self.delay, *args, **kwargs))\n```"}, {"role": "assistant", "content": "As we can see in Django codebase, `transaction.on_commit()` does not return anything. This `return` statement might mislead the reader in thinking that it will return something. Clearer to not have it in my opinion."}], "_meta": {"repo": "celery/celery", "pr_number": 8984, "url": "https://github.com/celery/celery/pull/8984#discussion_r1581817937"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_namespace_utils.py`:\n\n```python\nfrom __future__ import annotations\n\nimport sys\nfrom collections.abc import Generator, Mapping\nfrom contextlib import contextmanager\nfrom functools import cached_property\nfrom typing import Any, Iterator, NamedTuple, TypeAlias, TypeVar\n\nGlobalsNamespace: TypeAlias = dict[str, Any]\n```"}, {"role": "assistant", "content": "Happy with these as is for now - they're internal structures, so we can always bikeshed later. Resolving :)."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10530, "url": "https://github.com/pydantic/pydantic/pull/10530#discussion_r1792170071"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `trainer.py`:\n\n```python\ngetattr(self.accelerator, \"parallelism_config\", None) is not None\n            and self.accelerator.parallelism_config.cp_enabled\n        ):\n            if hasattr(model, \"config\"):\n                if model.config._attn_implementation != \"sdpa\":\n                    raise ValueError(\n                        f\"Context parallelism is supported only with SDPA attention, you are using {model.config._attn_implementation}.\"\n                    )\n            if self.accelerator.parallelism_config.cp_backend == \"torch\":\n                if hasattr(model, \"config\"):\n                    if model.config._attn_implementation != \"sdpa\":\n                        raise ValueError(\n                            f\"Context parallelism is supported only with SDPA attention, you are using {model.config._attn_implementation}.\"\n                        )\n\n                if \"shift_labels\" not in inputs:\n                    logger.warning_once(\"Shift labels not found in the inputs, shifting manually\")\n                    if \"labels\" in inputs:\n                        _ignore_index = -100\n                        labels = nn.functional.pad(inputs[\"labels\"], (0, 1), value=_ignore_index)\n                        inputs[\"shift_labels\"] = labels[:, 1:].contiguous()\n\n            # carve out space to make it clear there are other backends with different requirements, even though no code needs to be run at the moment\n            elif self.accelerator.parallelism_config.sp_backend == \"deepspeed\":\n                # - accelerator.parallelism_config performs the `model.config._attn_implementation` checks already and it supports more than `dspa`\n                # - UlyssesSPDataLoaderAdapter called from Accelerate performs the `shift_label` creation - must not interfere\n                # - position_ids generation should be done by HF Trainer if it wasn't done by the user\n                pass\n```"}, {"role": "assistant", "content": "remove the code but to leave the comments in place? Breadcrumbs help the maintainers to know why some parts are skipped. But it's up to you"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41832, "url": "https://github.com/huggingface/transformers/pull/41832#discussion_r2539082322"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `core_model_loading.py`:\n\n```python\nempty_param = meta_model_state_dict.get(t)\n            # If it does not exist, it's unexpected\n            if empty_param is None:\n                unexpected_keys.add(t)\n                continue\n\n            if quantizer is not None and quantizer.param_needs_quantization(model, t):\n                if quantizer.__class__.__name__ == \"FineGrainedFP8HfQuantizer\":\n                    from .integrations.finegrained_fp8 import Fp8Quantize\n\n                    converter.quantization_operation = Fp8Quantize()  # TODO support other methods\n                if hf_quantizer is not None and hf_quantizer.is_valid_unexpected_keys(t):\n                    pass\n                else:\n                    raise ValueError(\"This quantization method is gonna be supported SOOOON\")\n                    unexpected_keys.add(t)\n                    continue\n    \n            if hf_quantizer is not None and hf_quantizer.param_needs_quantization(model, t):\n                converter.quantization_operation = hf_quantizer.get_quantize_ops()\n                # TODO: to clean later. We need to use the empty_param from the checkpoint to decide if we upcast the param to a specific dtype\n                k_dtype = tensor.get_dtype()\n                dtype = str_to_torch_dtype[k_dtype]\n                empty_param_checkpoint = torch.empty(size=tensor.get_shape(), dtype=dtype, device=\"meta\")\n                _, _dtype = _infer_parameter_dtype(model, t, empty_param_checkpoint, hf_quantizer)\n```"}, {"role": "assistant", "content": "but if `quantize` we never change the dtype of the param, which is the source of truth"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42043, "url": "https://github.com/huggingface/transformers/pull/42043#discussion_r2504404607"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_docs_extraction.py`:\n\n```python\nfrom typing_extensions import Annotated, TypedDict\n\nfrom pydantic import BaseModel, ConfigDict, Field, TypeAdapter\nfrom pydantic.dataclasses import dataclass as pydantic_dataclass\n\n\ndef test_model_no_docs_extraction():\n    class Model(BaseModel):\n        a: int = 1\n        \"\"\"A docs\"\"\"\n\n        b: str = '1'\n\n        \"\"\"B docs\"\"\"\n\n    assert Model.model_fields['a'].description is None\n    assert Model.model_fields['b'].description is None\n\n\ndef test_model_docs_extraction():\n    class Model(BaseModel):\n```"}, {"role": "assistant", "content": "Ah yes didn't think about that \ud83d\ude05 That was what I was afraid of by using `inspect`, and I don't see any other path except shipping our own source parsing logic, which probably isn't a good idea"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 6563, "url": "https://github.com/pydantic/pydantic/pull/6563#discussion_r1262932132"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_models.py`:\n\n```python\nreturn True\n        return False\n\n    def __repr__(self) -> str:\n        template = \"<Cookie {name}={value} for {domain} />\"\n\n        cookies_repr = \", \".join(\n            [\n                template.format(\n                    name=cookie.name, value=cookie.value, domain=cookie.domain\n                )\n                for cookie in self.jar\n            ]\n        )\n```"}, {"role": "assistant", "content": "(Nit) Possible simplification using f-strings:\r\n\r\n```suggestion\r\n        cookies_repr = \", \".join(\r\n            f\"<Cookie {cookie.name}={cookie.value} for {cookie.domain} />\"\r\n            for cookie in self.jar\r\n        )\r\n```"}], "_meta": {"repo": "encode/httpx", "pr_number": 1411, "url": "https://github.com/encode/httpx/pull/1411#discussion_r534447007"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `configuration_utils.py`:\n\n```python\nreturn_dict: bool = True,\n        dtype: Optional[Union[str, \"torch.dtype\"]] = None,\n        # Common arguments\n        tie_word_embeddings: bool = True,\n        chunk_size_feed_forward: int = 0,\n        is_encoder_decoder: bool = False,\n        is_decoder: bool = False,\n        cross_attention_hidden_size: Optional[int] = None,\n        add_cross_attention: bool = False,\n        tie_encoder_decoder: bool = False,\n        # Fine-tuning task arguments\n        architectures: Optional[list[str]] = None,\n        finetuning_task: Optional[str] = None,\n        id2label: Optional[dict[int, str]] = None,\n        label2id: Optional[dict[str, int]] = None,\n        num_labels: Optional[int] = None,\n```"}, {"role": "assistant", "content": "these I was not sure 100% because they are kind of general. Any model with a Sequence Classification head on top would need these and i think all models potentially can be classifiers"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41541, "url": "https://github.com/huggingface/transformers/pull/41541#discussion_r2514979317"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `utils.py`:\n\n```python\n**repo_loading_kwargs,\n                )\n            except OSError:\n                # `self` already has a generation config created from model config so we can simply raise a warning\n                logger.info(\n                    \"Generation config file not found, using a generation config created from the model config.\"\n                )\n```"}, {"role": "assistant", "content": "Super nit: But it's an info, not a warning (or do we want to increase this to a warning)"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42922, "url": "https://github.com/huggingface/transformers/pull/42922#discussion_r2627855212"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `__init__.py`:\n\n```python\n# Scrapy and Twisted versions\n__version__ = (pkgutil.get_data(__package__, \"VERSION\") or b\"\").decode(\"ascii\").strip()\nversion_info = tuple(int(v) if v.isdigit() else v for v in __version__.split(\".\"))\ntwisted_version = (_txv.major, _txv.minor, _txv.micro)\n\n\n# Ignore noisy twisted deprecation warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"twisted\")\ndef __getattr__(name: str):\n    if name == \"twisted_version\":\n        import warnings\n\n        from twisted import version as _txv\n\n        from scrapy.exceptions import ScrapyDeprecationWarning\n\n        warnings.warn(\n            \"The scrapy.twisted_version attribute is deprecated, use twisted.version instead\",\n            ScrapyDeprecationWarning,\n        )\n        return _txv.major, _txv.minor, _txv.micro\n\n    raise AttributeError\n```"}, {"role": "assistant", "content": "Wondering since this is a module if we should raise `ImportError` instead."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6512, "url": "https://github.com/scrapy/scrapy/pull/6512#discussion_r1817945668"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `serialized_dag.py`:\n\n```python\nserialized_dag_hash = session.scalars(\n            select(cls.dag_hash).where(cls.dag_id == dag.dag_id).order_by(cls.created_at.desc())\n        ).first()\n        dag_version = DagVersion.get_latest_version(dag.dag_id, session=session, load_serialized_dag=True)\n        dag_version = session.scalars(\n            select(DagVersion)\n            .where(\n                DagVersion.dag_id == dag.dag_id,\n                DagVersion.bundle_version == bundle_version,\n                DagVersion.bundle_name == bundle_name,\n            )\n            .options(joinedload(DagVersion.task_instances))\n            .options(joinedload(DagVersion.serialized_dag))\n            .order_by(DagVersion.created_at.desc())\n        ).first()\n```"}, {"role": "assistant", "content": "Thanks for the suggestion! I\u2019ll rebase my branch onto origin/main and resolve the conflicts, then update the query as you recommended."}], "_meta": {"repo": "apache/airflow", "pr_number": 56422, "url": "https://github.com/apache/airflow/pull/56422#discussion_r2436148185"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `awq.py`:\n\n```python\nbackend = quantization_config.backend\n\n    if not is_auto_awq_available():\n    if not is_gptqmodel_available():\n        raise ValueError(\n            \"AWQ (either `autoawq` or `llmawq`) is not available. Please install it with `pip install autoawq` or check out the installation guide in https://github.com/mit-han-lab/llm-awq\"\n            \"AWQ (either `llmawq`) is not available. Please install it with `pip install gptqmodel` or check out the installation guide in https://github.com/mit-han-lab/llm-awq\"\n```"}, {"role": "assistant", "content": "Just a note but if this doesn't make into v5, we will have to slowly deprecate autoawq"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41567, "url": "https://github.com/huggingface/transformers/pull/41567#discussion_r2550396322"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `defer.py`:\n\n```python\nreturn DeferredList([coop.coiterate(work) for _ in range(count)])\n\n\nclass _AsyncCooperatorAdapter(Iterator[Deferred], Generic[_T]):\nclass _AsyncCooperatorAdapter(Iterator[Deferred], Generic[_T, _SelfResultT]):\n```"}, {"role": "assistant", "content": "See full error stack trace here: https://github.com/vsevolodbreus/scrapy/actions/runs/10700184494/job/29663370539"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6472, "url": "https://github.com/scrapy/scrapy/pull/6472#discussion_r1743543183"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_rabbitmq_default_queue_type_fallback.py`:\n\n```python\nimport socket\nimport time\n\nimport pytest\nfrom kombu import Connection\n\nfrom celery import Celery\n\n\ndef wait_for_port(host, port, timeout=30.0):\n    \"\"\"Wait for a port to become available.\"\"\"\n    start = time.time()\n    while time.time() - start < timeout:\n        try:\n            with socket.create_connection((host, port), timeout=2):\n                return\n        except OSError:\n            time.sleep(1)\n    raise TimeoutError(f\"Timed out waiting for {host}:{port}\")\n\n\n@pytest.fixture()\ndef app(rabbitmq):\n    wait_for_port(rabbitmq.hostname, rabbitmq.ports[5672])\n\n    return Celery(\n        \"test_app\",\n        broker=f\"pyamqp://guest:guest@{rabbitmq.hostname}:{rabbitmq.ports[5672]}/\",\n        backend=\"redis://redis:6379/0\",  # You may also want to make redis dynamic if needed\n```"}, {"role": "assistant", "content": "[nitpick] Hard-coding the Redis backend URI can make tests brittle. Consider parameterizing the Redis host/port via a fixture or environment variable, similar to the `rabbitmq` fixture, for consistency and flexibility.\n```suggestion\ndef redis():\n    \"\"\"Fixture to provide Redis hostname and port.\"\"\"\n    return {\"hostname\": \"redis\", \"port\": 6379}\n\n.fixture()\ndef app(rabbitmq, redis):\n    wait_for_port(rabbitmq.hostname, rabbitmq.ports[5672])\n    wait_for_port(redis[\"hostname\"], redis[\"port\"])\n\n    return Celery(\n        \"test_app\",\n        broker=f\"pyamqp://guest:guest@{rabbitmq.hostname}:{rabbitmq.ports[5672]}/\",\n        backend=f\"redis://{redis['hostname']}:{redis['port']}/0\",\n```"}], "_meta": {"repo": "celery/celery", "pr_number": 9765, "url": "https://github.com/celery/celery/pull/9765#discussion_r2156064001"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `RUF052_1.py`:\n\n```python\nreturn ___\n    return x\n\n# Correct usage in loop and comprehension\ndef process_data():\n    return 42\n```"}, {"role": "assistant", "content": "Do we need his function here? I usually try to strip down test cases as much as possible. I think the example usage below would be fine as\n\n```py\n[1 for _ in my_list]\n```\n\nfor example."}], "_meta": {"repo": "astral-sh/ruff", "pr_number": 19799, "url": "https://github.com/astral-sh/ruff/pull/19799#discussion_r2274174454"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `image_processing_glpn_fast.py`:\n\n```python\n# coding=utf-8\n# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Fast Image processor class for GLPN.\"\"\"\n\nfrom typing import Optional, Union\n\nimport torch\nfrom torchvision.transforms.v2 import functional as F\n\nfrom ...image_processing_utils import BatchFeature\nfrom ...image_processing_utils_fast import BaseImageProcessorFast, group_images_by_shape, reorder_images\nfrom ...image_utils import (\n    IMAGENET_STANDARD_MEAN,\n    IMAGENET_STANDARD_STD,\n    PILImageResampling,\n)\nfrom ...utils import (\n    TensorType,\n    auto_docstring,\n    requires_backends,\n)\n\n\n@auto_docstring\nclass GLPNImageProcessorFast(BaseImageProcessorFast):\n    \"\"\"\n    Fast image processor for GLPN using the Torch/TorchVision backend.\n\n    Performs:\n    - Crop H,W down to the nearest multiple of `size_divisor` (default 32)\n    - Rescale [0,255] \u2192 [0,1]\n    - (No normalization by default)\n    \"\"\"\n\n    # Persist ONLY the same keys as the slow processor\n    do_resize = True\n    do_rescale = True\n    do_normalize = False\n    resample = PILImageResampling.BILINEAR\n    size_divisor = 32\n    # Don't persist an explicit `size` for GLPN (slow doesn't)\n    image_mean = IMAGENET_STANDARD_MEAN\n    image_std = IMAGENET_STANDARD_STD\n    size = {\"height\": 480, \"width\": 640}  # only for validation; we still crop, not resize\n    interpolation = F.InterpolationMode.BILINEAR\n    # valid_kwargs = GLPNImageProcessorKwargs\n```"}, {"role": "assistant", "content": "I defined the kwargs in the slow processor and imported it."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41725, "url": "https://github.com/huggingface/transformers/pull/41725#discussion_r2449867777"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `scheduler.py`:\n\n```python\nself._smqclass: type[BaseQueue] | None = self._get_start_queue_cls(\n            crawler, \"MEMORY\"\n        )\n        if (\n            self.crawler\n            and self.crawler.settings.getint(\"CONCURRENT_REQUESTS_PER_IP\") != 0\n        ):\n            warn(\n                \"CONCURRENT_REQUESTS_PER_IP setting is deprecated, use CONCURRENT_REQUESTS_PER_DOMAIN instead.\",\n```"}, {"role": "assistant", "content": "```suggestion\r\n                \"The CONCURRENT_REQUESTS_PER_IP setting is deprecated, use CONCURRENT_REQUESTS_PER_DOMAIN instead.\",\r\n```"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6921, "url": "https://github.com/scrapy/scrapy/pull/6921#discussion_r2173143291"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `trace.py`:\n\n```python\n'name': get_task_name(req, task.name),\n                'exc': str(reason),\n            })\n            # MEMORY LEAK FIX: Clear traceback frames to prevent memory retention (Issue #8882)\n            traceback_clear(einfo.exception)\n            return einfo\n        finally:\n            del tb\n            # MEMORY LEAK FIX: Clean up direct traceback reference to prevent\n            # retention of frame objects and their local variables (Issue #8882)\n            if tb is not None:\n                del tb\n```"}, {"role": "assistant", "content": "[nitpick] There are multiple scattered calls to 'traceback_clear' and 'del tb' sequences across this module; consider extracting a reusable cleanup helper or decorator to reduce duplication and simplify maintenance.\n```suggestion\n            # MEMORY LEAK FIX: Perform traceback cleanup using helper function\n            cleanup_traceback(einfo.exception, tb)\n            return einfo\n        finally:\n            # MEMORY LEAK FIX: Ensure cleanup is performed even in case of exceptions\n            cleanup_traceback(None, tb)\n```"}], "_meta": {"repo": "celery/celery", "pr_number": 9799, "url": "https://github.com/celery/celery/pull/9799#discussion_r2187374134"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `exceptions.py`:\n\n```python\n\"\"\"\n\n\nclass RequestBodyUnavailable(RedirectBodyUnavailable):\n    \"\"\"\n    Got a request body, but the request body is no longer available.\n    \"\"\"\n```"}, {"role": "assistant", "content": "Yeah it looks  better. once the PR gets merged I will make the changes here."}], "_meta": {"repo": "encode/httpx", "pr_number": 685, "url": "https://github.com/encode/httpx/pull/685#discussion_r361775918"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `deb822_repository.py`:\n\n```python\ndescription:\n        - Tells APT whether the source is enabled or not.\n        type: bool\n    exclude:\n```"}, {"role": "assistant", "content": "> And test for this change.\r\n\r\nWhat do you mean by that?"}], "_meta": {"repo": "ansible/ansible", "pr_number": 86171, "url": "https://github.com/ansible/ansible/pull/86171#discussion_r2517801633"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_select_gen.py`:\n\n```python\n@needs_py39\ndef test_select_gen() -> None:\n    result = subprocess.run(\n        [sys.executable, \"scripts/generate_select.py\"],\n        env={\"CHECK_JINJA\": \"1\"},\n        [sys.executable, Path(\"scripts\") / \"generate_select.py\"],\n        env={**os.environ, \"CHECK_JINJA\": \"1\"},\n```"}, {"role": "assistant", "content": "The `env` stuff is basically the same fix as what was done in https://github.com/fastapi/sqlmodel/pull/969, but shortened.\r\n\r\n(we can probably also just keep the current way it's done on `main` - as long as the things in `os.environ` are passed to `env` as well)"}], "_meta": {"repo": "tiangolo/sqlmodel", "pr_number": 1307, "url": "https://github.com/fastapi/sqlmodel/pull/1307#discussion_r2354865365"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `type_adapter.py`:\n\n```python\n@cached_property\n    @_frame_depth(2)  # +2 for @cached_property + validator(self)\n    def validator(self) -> SchemaValidator:\n    def validator(self) -> SchemaValidator | PluggableSchemaValidator:\n        \"\"\"The pydantic-core SchemaValidator used to validate instances of the model.\"\"\"\n        if not isinstance(self._validator, SchemaValidator):\n        if not isinstance(self._validator, (SchemaValidator, PluggableSchemaValidator)):\n            self._init_core_attrs(rebuild_mocks=True)  # Do not expose MockValSer from public function\n        assert isinstance(self._validator, SchemaValidator)\n        assert isinstance(self._validator, (SchemaValidator, PluggableSchemaValidator))\n```"}, {"role": "assistant", "content": "This is where the problem is \u2014\u00a0because we were asserting that it was a SchemaValidator, but it could be a PluggableSchemaValidator. While we could just drop this assert,  wanted to just stop lying in the `create_schema_validator` return type annotation."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 9838, "url": "https://github.com/pydantic/pydantic/pull/9838#discussion_r1664868444"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `core_model_loading.py`:\n\n```python\nmapping.distributed_operation,\n                            hf_quantizer,\n                        )\n\n                # Cleanup the tensors\n                mapping.reset()\n            except SkipLayer:\n                continue\n\n    # Keep computed mapping as attribute for later saving\n    model._weight_loading_mapping = param_name_to_load\n    thread_pool.shutdown(wait=False)\n    return missing_keys, unexpected_keys, mismatch_keys, disk_offload_index, misc\n\n\n# TODO this is not done yet!\ndef revert_weight_conversion(model, state_dict):\n    mapping = getattr(model, \"_checkpoint_conversion_mapping\", {})  # IDK why but setting this will fail all llava.\n    reverse_key_mapping = [(v, k) for k, v in mapping.items()]\n    original_state_dict = {}\n    for key, value in state_dict.items():\n        for pattern, inverse_converter in reverse_key_mapping:\n            # TODO FIXME you name it\n            replacement = inverse_converter.lstrip(\"^\")  # strip off un-needed chars and patterns\n            replacement = re.sub(r\"\\(.*\\)\", \"\", replacement)\n            key, n_replace = re.subn(pattern, replacement, key)\n            # Early exit of the loop\n            if n_replace > 0:\n                break\n        original_state_dict[key] = value\n    state_dict = original_state_dict\n    return state_dict\n    original_mapping = getattr(model, \"_weight_loading_mapping\", None)\n    if original_mapping is None:\n```"}, {"role": "assistant", "content": "well, if we init + train a mistral model then the `_weight_loading_mapping` will be inexistant. \nFine for now, can come in another PR probaly?"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42396, "url": "https://github.com/huggingface/transformers/pull/42396#discussion_r2560860773"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `memusage.py`:\n\n```python\nself.crawler.engine.close_spider(self.crawler.engine.spider, 'memusage_exceeded')\n            else:\n                self.crawler.stop()\n        else:\n            logger.info(\"Current memory usage is %(virtualsize)dM\", {'virtualsize': self.get_virtual_size()})\n```"}, {"role": "assistant", "content": "Pushed changes to address the use of a variable and correctly converting bytes -> megabytes. I did not convert before the mem check to maintain as much precision as possible. \r\n\r\nNot so sure about the switch to mebibytes and how that scopes to this particular issue. Granted, isn't this already in mebibytes since we're doing `size / 1024 / 1024` instead of `size / 1000 / 1000` ? If so, this would just require the label be changed in the logging."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5722, "url": "https://github.com/scrapy/scrapy/pull/5722#discussion_r1029018840"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_project_structure.py`:\n\n```python\n\"providers/google/tests/unit/google/cloud/links/test_cloud_functions.py\",\n            \"providers/google/tests/unit/google/cloud/links/test_cloud_memorystore.py\",\n            \"providers/google/tests/unit/google/cloud/links/test_cloud_sql.py\",\n            \"providers/google/tests/unit/google/cloud/links/test_cloud_storage_transfer.py\",\n```"}, {"role": "assistant", "content": "I added a test file for cloud_storage_transfer, this line should be deleted."}], "_meta": {"repo": "apache/airflow", "pr_number": 58476, "url": "https://github.com/apache/airflow/pull/58476#discussion_r2540671302"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_otel.py`:\n\n```python\nlog = logging.getLogger(\"integration.otel.test_otel\")\n\n\ndef wait_for_otel_collector(host: str, port: int, timeout: int = 120) -> None:\n    \"\"\"\n    Wait for the OTel collector to be reachable before running tests.\n\n    This prevents flaky test failures caused by transient DNS resolution issues\n    (e.g., 'Temporary failure in name resolution' for breeze-otel-collector).\n\n    Note: If the collector is not reachable after timeout, logs a warning but\n    does not fail - allows tests to run and fail naturally if needed.\n    \"\"\"\n    deadline = time.monotonic() + timeout\n    last_error = None\n    while time.monotonic() < deadline:\n        try:\n            # Test DNS resolution and TCP connectivity\n            sock = socket.create_connection((host, port), timeout=5)\n            sock.close()\n```"}, {"role": "assistant", "content": "Good catch, Henry! Using a context manager is much cleaner. I\u2019ll update the implementation to use with `socket.create_connection` to ensure the socket is always properly closed."}], "_meta": {"repo": "apache/airflow", "pr_number": 61242, "url": "https://github.com/apache/airflow/pull/61242#discussion_r2745688602"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `candidate_generator.py`:\n\n```python\n# Prepare the assistant and the starting number of candidate tokens\n        self.assistant_model = assistant_model\n        self.num_assistant_tokens = assistant_model.generation_config.num_assistant_tokens\n        self.assistant_confidence_threshold = assistant_model.generation_config.assistant_confidence_threshold\n\n        # Prepare the generation config by updating with default values if not already set by users\n        self.assistant_generation_config = copy.deepcopy(assistant_model.generation_config)\n        global_defaults = self.assistant_generation_config._get_default_generation_params()\n        _ = self.assistant_generation_config.update(**global_defaults, defaults_only=True)\n```"}, {"role": "assistant", "content": "```suggestion\r\n        self.assistant_generation_config.update(**global_defaults, defaults_only=True)\r\n```\r\nNit, just a personal preference"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42702, "url": "https://github.com/huggingface/transformers/pull/42702#discussion_r2607198273"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `core_model_loading.py`:\n\n```python\n- target_keys: str | list[str] | None\n    - distributed_operation / operations / quantization_operations are ALWAYS lists.\n\n    TODO: for BNB we need to collect model.weight.quant_state_keys\n    \"\"\"\n\n    source_keys: Union[str, list[str]]\n    target_keys: Optional[Union[str, list[str]]] = None\n    source_keys: Union[str, list[str]] = field(init=True)\n    target_keys: Union[str, list[str]] = field(init=True)\n    operations: list[ConversionOps] = field(default_factory=list, repr=False)\n\n    distributed_operation: Optional[TensorParallelLayer] = None\n    quantization_operation: Optional[ConversionOps] = None\n\n    collected_tensors: dict[str, list[Future]] = field(default_factory=dict, init=False)\n    layer_targets: dict[str, set[str]] = field(default_factory=dict, init=False)\n\n    def __post_init__(self):\n        if not isinstance(self.source_keys, list):\n        if isinstance(self.source_keys, str):\n            self.source_keys = [self.source_keys]\n        targets_were_none = False\n        if not isinstance(self.target_keys, list):\n            if self.target_keys is None:\n                self.target_keys = list(self.source_keys)\n                targets_were_none = True\n            else:\n                self.target_keys = [self.target_keys]\n        if isinstance(self.target_keys, str):\n            self.target_keys = [self.target_keys]\n\n        if not targets_were_none and bool(len(self.source_keys) - 1) + bool(len(self.target_keys) - 1) >= 2:\n        if bool(len(self.source_keys) - 1) + bool(len(self.target_keys) - 1) >= 2:\n```"}, {"role": "assistant", "content": "nit: I'd rather have this without the `bool` - makes it harder to understand instead of simpler patterns IMO"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42254, "url": "https://github.com/huggingface/transformers/pull/42254#discussion_r2541763818"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `feedexport.py`:\n\n```python\n# properly closed.\n            return defer.maybeDeferred(slot.storage.store, slot.file)\n        slot.finish_exporting()\n        logfmt = \"%s %%(format)s feed (%%(itemcount)d items) in: %%(uri)s\"\n        log_args = {'format': slot.format,\n                    'itemcount': slot.itemcount,\n                    'uri': slot.uri}\n        logfmt = f\"{slot.format} feed ({str(slot.itemcount)} items) in: {slot.uri}\"\n```"}, {"role": "assistant", "content": ", fixed https://github.com/scrapy/scrapy/pull/5246/commits/91b776e47287ddcbce2f675041ef5b6366d748a8"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5246, "url": "https://github.com/scrapy/scrapy/pull/5246#discussion_r715482010"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_multipart.py`:\n\n```python\nfileobj: FileContent\n\n        headers: typing.Dict[str, str] = {}\n        content_type: typing.Optional[str] = None\n\n        # This large tuple based API largely mirror's requests' API\n        # It would be good to think of better APIs for this that we could include in httpx 2.0\n        # since variable length tuples (especially of 4 elements) are quite unwieldly\n        if isinstance(value, tuple):\n            try:\n                filename, fileobj, content_type = value  # type: ignore\n                filename, fileobj, content_type, headers = value  # type: ignore\n            except ValueError:\n                filename, fileobj = value  # type: ignore\n                content_type = guess_content_type(filename)\n                # 4th parameter (headers) not included\n                try:\n                    filename, fileobj, content_type = value  # type: ignore\n                except ValueError:\n                    # neither the 3rd parameter (content_type) nor the 4th (headers) was included\n                    filename, fileobj = value  # type: ignore\n            else:\n                # corresponds to (filename, fileobj, content_type, headers)\n                headers = {k.title(): v for k, v in headers.items()}\n        else:\n            filename = Path(str(getattr(value, \"name\", \"upload\"))).name\n            fileobj = value\n\n        if content_type is None:\n            content_type = guess_content_type(filename)\n\n        if content_type is not None and \"Content-Type\" not in headers:\n            # note that unlike requests, we ignore the content_type\n            # provided in the 3rd tuple element if it is also included in the headers\n            # requests does the opposite\n```"}, {"role": "assistant", "content": "requests does the opposite: it ignores the header in the 4th tuple element. so we'll still need the case-based header checking if we want to do exactly what requests does. either way, we need to know if the content type header exists in the 4th element tuple so we can either ignore the 3rd element or overwrite it with the 3rd element."}], "_meta": {"repo": "encode/httpx", "pr_number": 1936, "url": "https://github.com/encode/httpx/pull/1936#discussion_r782406811"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modular_hunyuan_v1_moe.py`:\n\n```python\nrouting_weights = F.softmax(hidden_states, dim=1, dtype=torch.float)\n        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n        routing_weights = torch.zeros_like(hidden_states, dtype=torch.float32).scatter_(\n            1, selected_experts, routing_weights\n        )\n        return selected_experts, routing_weights.to(hidden_states.dtype)\n\n        return selected_experts, routing_weights.to(hidden_states.dtype)\n```"}, {"role": "assistant", "content": "it's interesting that this dead code is not caught by styling"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43445, "url": "https://github.com/huggingface/transformers/pull/43445#discussion_r2721622215"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `item.py`:\n\n```python\nfrom collections.abc import MutableMapping\n\nfrom scrapy.item import BaseItem\n\n\ndef _is_dataclass_instance(obj):\n    \"\"\"\n    Return True if *obj* is a dataclass object, False otherwise.\n    Taken from https://docs.python.org/3/library/dataclasses.html#dataclasses.is_dataclass.\n    \"\"\"\n    try:\n        from dataclasses import is_dataclass\n    except ImportError:\n        return False\n    else:\n        return is_dataclass(obj) and not isinstance(obj, type)\n\n\ndef is_item_like(obj):\n    \"\"\"\n    Return True if *obj* is considered a Scrapy *item*, False otherwise.\n\n    An object is considered an *item* if it is:\n    - a scrapy.item.BaseItem or dict instance (or any subclass)\n    - a dataclass object\n    \"\"\"\n    return isinstance(obj, (BaseItem, dict)) or _is_dataclass_instance(obj)\n\n\nclass ItemAdapter(MutableMapping):\n```"}, {"role": "assistant", "content": "That makes sense to me :+1:  My idea was to start working on `attrs` support once we got the `ItemAdapter` class merged.\r\nI can create a repo under my account and start working on it, we can transfer it later. Does that sound good?"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 3881, "url": "https://github.com/scrapy/scrapy/pull/3881#discussion_r410463830"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `parse.py`:\n\n```python\nreturn hostname, port\n\n\n_DefragResultBase = namedtuple('_DefragResultBase', 'url fragment')\n_SplitResultBase = namedtuple(\n    '_SplitResultBase', 'scheme netloc path query fragment')\n_ParseResultBase = namedtuple(\n    '_ParseResultBase', 'scheme netloc path params query fragment')\n_UNSPECIFIED = ['not specified']\n_MISSING_AS_NONE_DEFAULT = False\n\nclass _ResultBase:\n    __slots__ = ()\n\n    def __replace__(self, /, **kwargs):\n        result = super().__replace__(**kwargs)\n        try:\n            result._keep_empty = self._keep_empty\n        except AttributeError:\n            pass\n        return result\n\n    def _replace(self, /, **kwargs):\n        result = super()._replace(**kwargs)\n        try:\n            result._keep_empty = self._keep_empty\n        except AttributeError:\n            pass\n        return result\n\n    def __copy__(self):\n        return self\n\n    def __deepcopy__(self, memo):\n        return self\n\n    def __getstate__(self):\n        state = super().__getstate__()\n        try:\n            if state[1]['_keep_empty'] == _MISSING_AS_NONE_DEFAULT:\n                del state[1]['_keep_empty']\n                if state == (None, {}):\n                    state = None\n        except LookupError:\n            pass\n        return state\n\n\nclass _DefragResultBase(_ResultBase, namedtuple('_DefragResultBase', 'url fragment')):\n    __slots__ = ('_keep_empty',)\n\n    def geturl(self):\n        if self.fragment or (self.fragment is not None and\n                             getattr(self, '_keep_empty', _MISSING_AS_NONE_DEFAULT)):\n            return self.url + self._HASH + self.fragment\n        else:\n            return self.url\n\nclass _SplitResultBase(_ResultBase, namedtuple(\n    '_SplitResultBase', 'scheme netloc path query fragment')):\n    __slots__ = ('_keep_empty',)\n\n    def geturl(self):\n        return urlunsplit(self)\n```"}, {"role": "assistant", "content": "`urlunsplit()` has the `keep_empty` parameter, `geturl()` does not.\n\nThis code was not changed."}], "_meta": {"repo": "python/cpython", "pr_number": 123305, "url": "https://github.com/python/cpython/pull/123305#discussion_r2665591979"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `pqueues.py`:\n\n```python\nself.key + \"/\" + str(key),\n        )\n\n    def _sqfactory(self, key: int) -> QueueProtocol:\n        assert self._start_queue_cls is not None\n        return build_from_crawler(\n            self._start_queue_cls,\n            self.crawler,\n            f\"{self.key}/{key}s\",\n        )\n\n    def priority(self, request: Request) -> int:\n        return -request.priority\n\n    def push(self, request: Request) -> None:\n        priority = self.priority(request)\n        if priority not in self.queues:\n            self.queues[priority] = self.qfactory(priority)\n        q = self.queues[priority]\n        is_start_request = request.meta.get(\"is_start_request\", False)\n        if is_start_request and self._start_queue_cls:\n            if priority not in self._start_queues:\n                self._start_queues[priority] = self._sqfactory(priority)\n            q = self._start_queues[priority]\n        else:\n            if priority not in self.queues:\n                self.queues[priority] = self.qfactory(priority)\n            q = self.queues[priority]\n        q.push(request)  # this may fail (eg. serialization error)\n        if self.curprio is None or priority < self.curprio:\n            self.curprio = priority\n\n    def pop(self) -> Request | None:\n        if self.curprio is None:\n            return None\n        q = self.queues[self.curprio]\n        m = q.pop()\n        if not q:\n            del self.queues[self.curprio]\n            q.close()\n            prios = [p for p, q in self.queues.items() if q]\n            self.curprio = min(prios) if prios else None\n        return m\n        while self.curprio is not None:\n            if self._start_queues:\n                try:\n                    q = self._start_queues[self.curprio]\n                except KeyError:\n                    pass\n```"}, {"role": "assistant", "content": "The logic is to call `self._update_curprio()` when *both* queues raise a `KeyError`. Here it is not called because self.curprio may still exist in `self.queues` below, and `self._update_curprio()` only needs to be changed once you know that neither queue has the current `curprio`."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6752, "url": "https://github.com/scrapy/scrapy/pull/6752#discussion_r2072942577"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `dag_run.py`:\n\n```python\nend=timezone.coerce_datetime(self.data_interval_end),\n                )\n            else:\n                data_interval = dag.timetable.infer_manual_data_interval(\n                    run_after=coerced_logical_date or timezone.coerce_datetime(run_after)\n                )\n                run_after = data_interval.end\n                data_interval = dag.timetable.infer_manual_data_interval(run_after=coerced_logical_date)\n```"}, {"role": "assistant", "content": "Can you explain why you removed the `data_interval` being inferred from the `run_after` date if the `coerced_logical_date` is None?\r\n\r\nThis piece doesn't seem directly related to the fix. (just removing the `run_after = data_interval.end` should be enough)?"}], "_meta": {"repo": "apache/airflow", "pr_number": 59477, "url": "https://github.com/apache/airflow/pull/59477#discussion_r2631654573"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_connections.py`:\n\n```python\n{\"connection_id\": TEST_CONN_ID, \"conn_type\": \"ftp\"},\n        ],\n    )\n    def test_should_respond_403_by_default(self, test_client, body):\n    @mock.patch(\"airflow.api_fastapi.core_api.routes.public.connections.conf.get\")\n    def test_should_respond_403_by_default(self, mock_conf_get, test_client, body):\n        mock_conf_get.return_value = \"Disabled\"\n```"}, {"role": "assistant", "content": "I removed the unnecessary mock since the default value is already \"Disabled\" :)"}], "_meta": {"repo": "apache/airflow", "pr_number": 59643, "url": "https://github.com/apache/airflow/pull/59643#discussion_r2702174365"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_utils.py`:\n\n```python\n{chr(c): \"%{:02X}\".format(c) for c in range(0x1F + 1) if c != 0x1B}\n)\n_HTML5_FORM_ENCODING_RE = re.compile(\n    r\"|\".join([re.escape(c) for c in _HTML5_FORM_ENCODING_REPLACEMENTS.keys()])\n    r\"|\".join([re.escape(c) for c in _HTML5_FORM_ENCODING_REPLACEMENTS])\n```"}, {"role": "assistant", "content": "I could go either way on this one. Possibly it's more obvious with `.keys()` what the effect is. Possibly not. But, okay, sure."}], "_meta": {"repo": "encode/httpx", "pr_number": 1943, "url": "https://github.com/encode/httpx/pull/1943#discussion_r754259285"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_generate_schema.py`:\n\n```python\nreturn schema\n\n\ndef modify_model_json_schema(\n    schema_or_field: CoreSchemaOrField,\n    handler: GetJsonSchemaHandler,\n    *,\n    cls: type[Any],\n    title: str | None = None,\n) -> JsonSchemaValue:\n    \"\"\"Add title and description for model-like classes' JSON schema.\n\n    Args:\n        schema_or_field: The schema data to generate a JSON schema from.\n        handler: The `GetCoreSchemaHandler` instance.\n        cls: The model-like class.\n        title: The title to set for the model's schema, defaults to the model's name\n\n    Returns:\n        JsonSchemaValue: The updated JSON schema.\n    \"\"\"\n    from ..root_model import RootModel\n\n    BaseModel = import_cached_base_model()\n\n    json_schema = handler(schema_or_field)\n    original_schema = handler.resolve_ref_schema(json_schema)\n    if title is not None:\n        original_schema['title'] = title\n    elif 'title' not in original_schema:\n        original_schema['title'] = cls.__name__\n    # BaseModel and dataclasses; don't use cls.__doc__ as it will contain the verbose class signature by default\n    docstring = None if cls is BaseModel or dataclasses.is_dataclass(cls) else cls.__doc__\n    if docstring and 'description' not in original_schema:\n        original_schema['description'] = inspect.cleandoc(docstring)\n    elif issubclass(cls, RootModel) and cls.__pydantic_fields__['root'].description:\n        original_schema['description'] = cls.__pydantic_fields__['root'].description\n    return json_schema\n```"}, {"role": "assistant", "content": "See ya! Don't like having this sort of thing here - belongs in `json_schema.py`."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10675, "url": "https://github.com/pydantic/pydantic/pull/10675#discussion_r1813077675"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_utils.py`:\n\n```python\nloss_type = None\n        self.loss_type = loss_type\n\n        self.name_or_path = config.name_or_path\n        self.warnings_issued = {}\n        # Overwrite the class attribute to make it an instance attribute, so models like\n        # `InstructBlipForConditionalGeneration` can dynamically update it without modifying the class attribute\n        # when a different component (e.g. language_model) is used. Same for `_tied_weights_keys` which pops/adds\n        # new keys dynamically depending on config values\n        self._keep_in_fp32_modules = copy.copy(self.__class__._keep_in_fp32_modules)\n        self._keep_in_fp32_modules_strict = copy.copy(self.__class__._keep_in_fp32_modules_strict)\n        self._tied_weights_keys = copy.copy(self.__class__._tied_weights_keys)\n\n        self._no_split_modules = self._no_split_modules or []\n        _CAN_RECORD_REGISTRY[str(self.__class__)] = self._can_record_outputs  # added for executorch support only\n\n    def post_init(self):\n        \"\"\"\n        A method executed at the end of each Transformer model initialization, to execute code that needs the model's\n        modules properly initialized (such as weight initialization).\n        It is also used to obtain all correct static properties (parallelism plans, tied_weights_keys, _keep_in_fp32_modules, etc)\n        correctly in the case of composite models (that is, the top level model should know about those properties from its children).\n        \"\"\"\n        # Attach the different parallel plans and tied weight keys to the top-most model, so that everything is\n        # easily available\n        self._tp_plan, self._ep_plan, self._pp_plan = {}, {}, {}\n        # Current submodel should register its tied weights\n        self.all_tied_weights_keys = self.get_expanded_tied_weights_keys(all_submodels=False)\n        # If current model is a base model, attach `base_model_tp_plan` and `base_model_pp_plan` from config\n        if self.base_model is self:\n            self._pp_plan = self.config.base_model_pp_plan.copy() if self.config.base_model_pp_plan is not None else {}\n            self._tp_plan = self.config.base_model_tp_plan.copy() if self.config.base_model_tp_plan is not None else {}\n            self._ep_plan = self.config.base_model_ep_plan.copy() if self.config.base_model_ep_plan is not None else {}\n        # Current submodel should register its tied weights\n        self.all_tied_weights_keys = self.get_expanded_tied_weights_keys(all_submodels=False)\n        # Current submodel should register its `_keep_in_fp32_modules`\n        self._keep_in_fp32_modules = set(self._keep_in_fp32_modules or [])\n        self._keep_in_fp32_modules_strict = set(self._keep_in_fp32_modules_strict or [])\n        # Current submodel must register its `_no_split_modules` as well\n```"}, {"role": "assistant", "content": "Unfortunately the `dtype_plan` needs to be built later on when we have access to `dtype` in `from_pretrained`, as `_keep_in_fp32_modules` is only activated when using fp16, not bf16"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43691, "url": "https://github.com/huggingface/transformers/pull/43691#discussion_r2758521590"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `auto_docstring.py`:\n\n```python\nfunc (`function`): The function the parameter belongs to\n    \"\"\"\n    optional = False\n    if param.annotation != inspect.Parameter.empty:\n        param_type = param.annotation\n        if \"typing\" in str(param_type):\n            param_type = \"\".join(str(param_type).split(\"typing.\")).replace(\"transformers.\", \"~\")\n        elif hasattr(param_type, \"__module__\"):\n            param_type = f\"{param_type.__module__.replace('transformers.', '~').replace('builtins', '')}.{param.annotation.__name__}\"\n            if param_type[0] == \".\":\n                param_type = param_type[1:]\n        else:\n            if False:\n                print(\n                    f\"[ERROR] {param_type} for {param_name} of {func.__qualname__} in file {func.__code__.co_filename} has an invalid type\"\n                )\n        if \"ForwardRef\" in param_type:\n            param_type = re.sub(r\"ForwardRef\\('([\\w.]+)'\\)\", r\"\\1\", param_type)\n        if \"Optional\" in param_type:\n            param_type = re.sub(r\"Optional\\[(.*?)\\]\", r\"\\1\", param_type)\n    if param.annotation == inspect.Parameter.empty:\n        return \"\", False\n    # This is, astonishingly, the right way to do it: https://docs.python.org/3/library/typing.html#typing.Union\n```"}, {"role": "assistant", "content": "You are right. This is best approach and i ignored it lol"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42431, "url": "https://github.com/huggingface/transformers/pull/42431#discussion_r2620390401"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `configuration_auto.py`:\n\n```python\n(\"glm_image_text\", \"GlmImageTextConfig\"),\n        (\"glm_image_vision\", \"GlmImageVisionConfig\"),\n        (\"glm_image_vqmodel\", \"GlmImageVQVAEConfig\"),\n        (\"glm_ocr\", \"GlmOcrConfig\"),\n        (\"glm_ocr_text\", \"GlmOcrTextConfig\"),\n        (\"glm_ocr_vision\", \"GlmOcrVisionConfig\"),\n```"}, {"role": "assistant", "content": "note to myself: this is getting too reprtitive with more MM models added. Will try to come up with a cleaner way!"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43391, "url": "https://github.com/huggingface/transformers/pull/43391#discussion_r2716449870"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `files.py`:\n\n```python\nreturn root_relative_path\n\n\ndef path_is_ignored(\n    path: Path, gitignore_dict: Dict[Path, PathSpec], report: Report\ndef _path_is_ignored(\n    root_relative_path: str,\n    root: Path,\n    gitignore_dict: Dict[Path, PathSpec],\n    report: Report,\n) -> bool:\n    path = root / root_relative_path\n    # Note that this logic is sensitive to the ordering of gitignore_dict. Callers must\n```"}, {"role": "assistant", "content": "OK I stared at the code some more and this does make sense now. (I thought it mattered only for broken symlinks, but that's not the case.)\r\n\r\nWe pass a dict here that gets built separately for every directory, since we look for `.gitignore` in every directory, so given a directory structure `root/a/b/c.py`, the dict will contain `root/.gitignore`, `root/a/.gitignore`, `root/a/b/.gitignore` in that order. The `break` happens for the first path where the symlink points outside the directory, so it makes sense to ignore all subsequent gitignores as they definitely won't match either.\r\n\r\nAgree that it probably makes more sense to apply gitignore *before* resolving symlinks, but that's something do in a separate PR."}], "_meta": {"repo": "psf/black", "pr_number": 3751, "url": "https://github.com/psf/black/pull/3751#discussion_r1257552324"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_generate_schema.py`:\n\n```python\nif maybe_schema is not None:\n                return maybe_schema\n\n            schema = cls.__dict__.get('__pydantic_core_schema__')\n            if (\n                schema is not None\n                and not isinstance(schema, MockCoreSchema)\n                # Due to the way generic classes are built, it's possible that an invalid schema may be temporarily\n                # set on generic classes. Probably we could resolve this to ensure that we get proper schema caching\n                # for generics, but for simplicity for now, we just always rebuild if the class has a generic origin:\n                and not cls.__pydantic_generic_metadata__['origin']\n            ):\n```"}, {"role": "assistant", "content": "I also need to check that `cls.__pydantic_generic_metadata__['origin']` is `None` for Pydantic models, so maybe it's best to keep the (small) duplication of code here."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10863, "url": "https://github.com/pydantic/pydantic/pull/10863#discussion_r1848522677"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `__init__.py`:\n\n```python\nsources.add(p)\n        elif p.is_dir():\n            if exclude is None:\n                exclude = re_compile_maybe_verbose(DEFAULT_EXCLUDES)\n                gitignore = get_gitignore(root)\n                if _exclude is None:\n                    _exclude = re_compile_maybe_verbose(DEFAULT_EXCLUDES)\n                if gitignore is None:\n                    gitignore = get_gitignore(root)\n                p_gitignore = get_gitignore(p)\n                # No need to use p's gitignore if it is identical to root's gitignore\n                # (i.e. root and p point to the same directory).\n                if gitignore != p_gitignore:\n                    gitignore += p_gitignore\n            else:\n                gitignore = None\n                if gitignore == p_gitignore:\n                    _gitignore = gitignore\n                else:\n                    _gitignore = gitignore + p_gitignore\n            sources.update(\n                gen_python_files(\n                    p.iterdir(),\n                    ctx.obj[\"root\"],\n                    include,\n                    exclude,\n                    cast(Pattern[str], _exclude),\n```"}, {"role": "assistant", "content": "I'm not sure how to assert that. The value will never be `None`, but is a limitation (?) of mypy https://github.com/psf/black/pull/3336/files#r997119695\r\n\r\nBut, this issue should go away if we just initialize `_exclude` as `re_compile_maybe_verbose(DEFAULT_EXCLUDES)`, as you suggested. Then, casting nor asserting should be necessary (mypy will correctly identify `_exclude` as `Pattern[str]`)"}], "_meta": {"repo": "psf/black", "pr_number": 3336, "url": "https://github.com/psf/black/pull/3336#discussion_r1006131541"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_exceptions.py`:\n\n```python\n\"\"\"\n\n\nclass UnsupportedProtocol(TransportError):\n    \"\"\"\n    Attempted to make a request to a no-supported protocol.\n```"}, {"role": "assistant", "content": "```suggestion\r\n    Attempted to make a request to an unsupported protocol.\r\n```"}], "_meta": {"repo": "encode/httpx", "pr_number": 1126, "url": "https://github.com/encode/httpx/pull/1126#discussion_r464487607"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_config.py`:\n\n```python\n\"\"\"\n        config = self.config_dict\n\n        if config.get('ser_json_timedelta') == 'float':\n```"}, {"role": "assistant", "content": "LGTM for now. Reminds me - I should probably add back `schema_generator` to the config and add a deprecation warning here before v2.10."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10293, "url": "https://github.com/pydantic/pydantic/pull/10293#discussion_r1757086427"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `migrate-black.py`:\n\n```python\ngit(\"checkout\", base_branch, f\"-b{current_branch}-black\")\n\n    for last_commit, commit in zip(commits, commits[1:]):\n    for last_commit, commit in zip(commits, commits[1:], strict=True):\n```"}, {"role": "assistant", "content": "We can't remove it because Ruff's B905 demands something: https://docs.astral.sh/ruff/rules/zip-without-explicit-strict/\r\n\r\nThen it needs to be `strict=False`:\r\n\r\n```suggestion\r\n    for last_commit, commit in zip(commits, commits[1:], strict=False):\r\n```"}], "_meta": {"repo": "psf/black", "pr_number": 4842, "url": "https://github.com/psf/black/pull/4842#discussion_r2524655758"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `task.py`:\n\n```python\nstamped_headers = self.request.stamped_headers.copy()\n            stamps = self.request.stamps.copy()\n            stamped_headers.extend(sig.options.get('stamped_headers', []))\n            stamped_headers = list(set(stamped_headers))\n            stamps.update({\n                stamp: value\n                for stamp, value in sig.options.items() if stamp in sig.options.get('stamped_headers', [])\n            })\n            sig.options['stamped_headers'] = stamped_headers\n            sig.options.update(stamps)\n\n            links = sig.options['link'] if 'link' in sig.options else []\n            links.extend(sig.options['link_error'] if 'link_error' in sig.options else [])\n\n            if hasattr(sig, \"tasks\"):\n                tasks = sig.tasks\n                if isinstance(tasks, group):\n```"}, {"role": "assistant", "content": "Yes, the new test cases are passing with this code which confirms your original question is resolved.\r\n> What happens if tasks is a list or a tuple?\r\n\r\nIt works.\r\n\r\n> What happens if it is a regen?\r\n\r\nIt works.\r\n\r\n> We shouldn't ignore such cases.\r\n\r\nWe do not :)"}], "_meta": {"repo": "celery/celery", "pr_number": 8034, "url": "https://github.com/celery/celery/pull/8034#discussion_r1091812624"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `mode.py`:\n\n```python\nstr(int(self.is_ipynb)),\n            str(int(self.magic_trailing_comma)),\n            str(int(self.experimental_string_processing)),\n            \",\".join(self.python_cell_magics),\n```"}, {"role": "assistant", "content": "```suggestion\r\n            \",\".join(sorted(self.python_cell_magics)),\r\n```\r\nSet iteration order is randomized, so this would make the cache key unstable"}], "_meta": {"repo": "psf/black", "pr_number": 2744, "url": "https://github.com/psf/black/pull/2744#discussion_r784458194"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `video_processing_utils.py`:\n\n```python\nlogger.info(\n                f\"loading configuration file {video_processor_file} from cache at {resolved_video_processor_file}\"\n            )\n\n        # Specific models need the original path for modification in `from_dict`, e.g. see `Ernie 4.5 VL` with fonts\n        kwargs[\"resolved_file_path\"] = final_resolved_file_path\n```"}, {"role": "assistant", "content": "Hmm, the issue is that at the point that the local path is resolved, i.e. we loaded the base video dict, all necessary kwargs are popped to refind the proper file --> offline, cache, from the hub etc. As you said, this could also be subfolders etc. Without this, I would need to pass these kwargs myself afterwards so I can re-resolve the path which is a not so nice looking function :D Meaning this function here https://github.com/huggingface/transformers/blob/6564633bc87b1f808de5773a74db2ec01f681520/src/transformers/video_processing_utils.py#L597\r\n\r\nAnd no, I cannot override this because the auto video processor will be used, always calling the one from the base video processor. So I have no chance to override it myself (without breaking quite a bit of logic that was recently introduced, i.e. use auto classes to load everything). In my original version I overrode this function directly but yea no longer viable (as of now)."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 39585, "url": "https://github.com/huggingface/transformers/pull/39585#discussion_r2599988717"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `stream.py`:\n\n```python\ndef receive_headers(self, headers):\n        for name, value in headers:\n            self._response_headers[name] = value\n\n    def reset(self):\n        \"\"\"Received a RST_STREAM -- forcefully reset\"\"\"\n        # TODO:\n        #  Q1. Do we need to send the request again?\n        #  Q2. What response should we send now?\n        self.stream_closed_server = True\n        self._cb_close(self.stream_id)\n            self._response['headers'][name] = value\n\n    def lost_connection(self):\n        # TODO: Same as self.reset\n        self.stream_closed_server = True\n        self._cb_close(self.stream_id)\n    def close(self, event=None):\n        \"\"\"Based on the event sent we will handle each case.\n\n    def end_stream(self):\n        \"\"\"Stream is ended by the server hence no further\n        event: StreamEnded\n        Stream is ended by the server hence no further\n        data or headers should be expected on this stream.\n\n        We will call the response deferred callback passing\n        the response object\n\n        event: StreamReset\n        Stream reset via RST_FRAME by the upstream hence forcefully close\n        this stream and send TODO: ?\n\n        event: None\n        No event is launched -- Hence we will simply close this stream\n        \"\"\"\n        # TODO: In case of abruptly stream close\n        #  Q1. Do we need to send the request again?\n```"}, {"role": "assistant", "content": "Sending requests again is something we want to be able to control from Scrapy, so that should definitely not be done here."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4610, "url": "https://github.com/scrapy/scrapy/pull/4610#discussion_r443721909"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_models.py`:\n\n```python\nself._content = b\"\".join(self.iter_bytes())\n        return self._content\n\n    def iter_bytes(self) -> typing.Iterator[bytes]:\n    def iter_bytes(self, chunk_size: int = None) -> typing.Iterator[bytes]:\n```"}, {"role": "assistant", "content": "In this PR, when `chunk_size=None` we just return the input `content` unchanged, as one single big chunk.\r\n\r\nYes, this would deviate from what Requests seems to do, but:\r\n\r\n- Setting a non-`None` default would break backward compatibility on our side.\r\n- Defaulting to \"transparently pass the chunk sent by the server\" is probably the most reasonable approach _anyway_.\r\n\r\nThat said, we'd need to add this deviation from Requests to the compatibility guide. :+1:"}], "_meta": {"repo": "encode/httpx", "pr_number": 1277, "url": "https://github.com/encode/httpx/pull/1277#discussion_r502757453"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `http_resources.py`:\n\n```python\nif request.method != b\"CONNECT\":\n            return request.uri\n        return b\"\"\n\n\nclass ResponseHeadersResource(resource.Resource):\n```"}, {"role": "assistant", "content": "I wasn't sure if another mock resource that sets http headers on the response already exists somewhere."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 7059, "url": "https://github.com/scrapy/scrapy/pull/7059#discussion_r2384478717"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `httpcache.py`:\n\n```python\n# process_response hook\n        request.meta['cached_response'] = cachedresponse\n\n    def process_response(self, request, response, spider):\n    def process_response(self, request: Type[Request], response: Type[Response], spider: Type[Spider]) -> Type[Response]:\n```"}, {"role": "assistant", "content": "Right, I find the right way is to use `TypeVar` for subclass typing hint."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4243, "url": "https://github.com/scrapy/scrapy/pull/4243#discussion_r359606652"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `files.py`:\n\n```python\nclass FSFilesStore:\n    def __init__(self, basedir):\n    def __init__(self, basedir: str):\n        if '://' in basedir:\n            basedir = basedir.split('://', 1)[1]\n        self.basedir = basedir\n        self._mkdir(self.basedir)\n        self.created_directories = defaultdict(set)\n        self._mkdir(Path(self.basedir))\n        self.created_directories: defaultdict[str, set[str]] = defaultdict(set)\n```"}, {"role": "assistant", "content": "This is not the first time I hear \"this typing syntax shouldn't be supported on the oldest version but the tests pass for it\". Not sure why is it so."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5682, "url": "https://github.com/scrapy/scrapy/pull/5682#discussion_r1001441042"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `check_core_imports_in_sdk.py`:\n\n```python\n#!/usr/bin/env python\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n# /// script\n# requires-python = \">=3.10,<3.11\"\n# dependencies = [\n#   \"rich>=13.6.0\",\n# ]\n# ///\nfrom __future__ import annotations\n\nimport argparse\nimport ast\nimport sys\nfrom pathlib import Path\n\nsys.path.insert(0, str(Path(__file__).parent.resolve()))\nfrom common_prek_utils import console\n\n\ndef check_file_for_core_imports(file_path: Path) -> list[tuple[int, str]]:\n    \"\"\"Check file for airflow-core imports (anything except airflow.sdk). Returns list of (line_num, import_statement).\"\"\"\n    try:\n        with open(file_path, encoding=\"utf-8\") as f:\n            source = f.read()\n            tree = ast.parse(source, filename=str(file_path))\n    except (OSError, UnicodeDecodeError, SyntaxError):\n        return []\n\n    mismatches = []\n\n    for node in ast.walk(tree):\n        # for `from airflow.x import y` statements\n        if isinstance(node, ast.ImportFrom):\n            if (\n                node.module\n                and node.module.startswith(\"airflow.\")\n                and not node.module.startswith(\"airflow.sdk\")\n            ):\n                import_names = \", \".join(alias.name for alias in node.names)\n                statement = f\"from {node.module} import {import_names}\"\n                mismatches.append((node.lineno, statement))\n        # for `import airflow.x` statements\n        elif isinstance(node, ast.Import):\n            for alias in node.names:\n                if alias.name.startswith(\"airflow.\") and not alias.name.startswith(\"airflow.sdk\"):\n                    statement = f\"import {alias.name}\"\n                    if alias.asname:\n                        statement += f\" as {alias.asname}\"\n                    mismatches.append((node.lineno, statement))\n\n    return mismatches\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Check for core imports in task-sdk files\")\n    parser.add_argument(\"files\", nargs=\"*\", help=\"Files to check\")\n    args = parser.parse_args()\n\n    if not args.files:\n        return\n\n    files_to_check = [Path(f) for f in args.files if f.endswith(\".py\")]\n```"}, {"role": "assistant", "content": "Wow! I didn\u2019t know about this \u2014 I tried running `uv run --script task-sdk/src/airflow/sdk/log.py`, and it's building (it has also some --python options). \r\n\r\nI felt that UV installs dependencies much faster than Breeze image building. Could I also use only `UV` tools instead of image building ? (like rebuilding the image and other things )"}], "_meta": {"repo": "apache/airflow", "pr_number": 60676, "url": "https://github.com/apache/airflow/pull/60676#discussion_r2703403507"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_detr.py`:\n\n```python\nself.row_embeddings = nn.Embedding(50, embedding_dim)\n        self.column_embeddings = nn.Embedding(50, embedding_dim)\n\n    def forward(self, pixel_values, pixel_mask=None):\n        height, width = pixel_values.shape[-2:]\n        width_values = torch.arange(width, device=pixel_values.device)\n        height_values = torch.arange(height, device=pixel_values.device)\n    @compile_compatible_method_lru_cache(maxsize=1)\n    def forward(\n        self,\n        shape: torch.Size,\n        device: Union[torch.device, str],\n        dtype: torch.dtype,\n        mask: Optional[torch.Tensor] = None,\n    ):\n        height, width = shape[-2:]\n        width_values = torch.arange(width, device=device)\n        height_values = torch.arange(height, device=device)\n        x_emb = self.column_embeddings(width_values)\n        y_emb = self.row_embeddings(height_values)\n        pos = torch.cat([x_emb.unsqueeze(0).repeat(height, 1, 1), y_emb.unsqueeze(1).repeat(1, width, 1)], dim=-1)\n        pos = pos.permute(2, 0, 1)\n        pos = pos.unsqueeze(0)\n        pos = pos.repeat(pixel_values.shape[0], 1, 1, 1)\n        pos = pos.repeat(shape[0], 1, 1, 1)\n        return pos\n\n\ndef build_position_encoding(config):\n    n_steps = config.d_model // 2\n    if config.position_embedding_type == \"sine\":\n        # TODO find a better way of exposing other arguments\n        position_embedding = DetrSinePositionEmbedding(n_steps, normalize=True)\n    elif config.position_embedding_type == \"learned\":\n        position_embedding = DetrLearnedPositionEmbedding(n_steps)\n    else:\n        raise ValueError(f\"Not supported {config.position_embedding_type}\")\ndef eager_attention_forward(\n```"}, {"role": "assistant", "content": "It's only for models that do not use modular yet. The issue is that if we don't do this and change anything upstream on Bert, it won't be reflected here (and the dependent models) - otherwise, we need to manually keep track of these deps :sweat_smile: Don't mind this if we refactor to modular here!"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41549, "url": "https://github.com/huggingface/transformers/pull/41549#discussion_r2440438239"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `continuous_api.py`:\n\n```python\nnum_kv_padding_intervals: (optional) Number of intervals used to pad the keys/values dimension\n            allow_block_sharing: (optional) Whether to allow block sharing if the model has some full attention layers\n        \"\"\"\n        # Reloade paged version if necessary\n        # Reload paged version of the attention implementation if necessary\n        if \"paged|\" not in model.config._attn_implementation:\n            model.set_attn_implementation(f\"paged|{model.config._attn_implementation}\")\n\n        # Internal arguments\n        self.model = model.eval()\n        generation_config = model.generation_config if generation_config is None else generation_config\n        self.generation_config = generation_config\n        self.manual_eviction = manual_eviction\n        self._allow_block_sharing = allow_block_sharing\n        self._use_prefix_sharing = allow_block_sharing  # approximation until the cache is created\n\n        self.input_queue = queue.Queue(maxsize=max_queue_size)\n        self.output_queue = queue.Queue()\n        self.stop_event = threading.Event()\n        self.log_prob_generation = getattr(generation_config, \"log_prob_generation\", False)\n        self.batch_processor: ContinuousBatchProcessor | None = None\n        self._generation_thread = None\n        self._request_counter = 0\n        self._request_lock = threading.Lock()\n        self.model.generation_config.top_p = None\n\n        # Generation config related arguments\n        generation_config = model.generation_config if generation_config is None else generation_config\n        self.generation_config = generation_config\n        self.log_prob_generation = getattr(generation_config, \"log_prob_generation\", False)\n```"}, {"role": "assistant", "content": "For the log probs? Sure, I will add it to the todo list"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42921, "url": "https://github.com/huggingface/transformers/pull/42921#discussion_r2631952694"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `chat.py`:\n\n```python\n},\n                    )\n\n                    model_output = await interface.stream_output(stream)\n                    model_output, finish_reason = await interface.stream_output(stream)\n\n                    chat.append({\"role\": \"assistant\", \"content\": model_output})\n\n                    if finish_reason == \"length\":\n```"}, {"role": "assistant", "content": "~don't we want to write something for other reasons? (at least write the reason in terminal). Or is length the only finish reason implemented so far?~\n\nnevermind, saw below that it's only \"length\" and \"stop\" so probably fine not to write it"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42157, "url": "https://github.com/huggingface/transformers/pull/42157#discussion_r2568956828"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `sessions.py`:\n\n```python\nnew_proxies = proxies.copy()\n        no_proxy = proxies.get('no_proxy')\n\n        bypass_proxy = should_bypass_proxies(url, no_proxy=no_proxy)\n        bypass_proxy = False\n        if self.trust_env:\n```"}, {"role": "assistant", "content": "/  /    - this change fixes the performance regression of #5891 but does not fix #5888. IMHO fixing #5888 is something that you might do by checking if a proxy has been selected as if one has not been selected you likely would not want to leak proxy credentials. Of course it might be easier to suggest that we do not explicitly support setting `Proxy-Authorization` as a header and that users must specify proxy credentials in the proxy url."}], "_meta": {"repo": "psf/requests", "pr_number": 5894, "url": "https://github.com/psf/requests/pull/5894#discussion_r679537744"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_requests.py`:\n\n```python\ndef test_no_content():\n    request = httpx.Request(\"GET\", \"http://example.org\")\n    assert \"Content-Length\" not in request.headers\n    assert request.headers[\"Content-Length\"] == \"0\"\n```"}, {"role": "assistant", "content": "We shouldn't send Content-Length with GET/HEAD requests\r\nReferences:\r\n - https://github.com/encode/httpx/pull/866#issuecomment-605899948\r\n - https://tools.ietf.org/html/rfc7230#section-3.3.2 \"A user agent SHOULD NOT ...\"\r\n - `requests` implementation for non-stream requests https://github.com/psf/requests/blob/fd13816d015c4c90ee65297fa996caea6a094ed1/requests/models.py#L522-L533\r\n - `requests` implementation for stream requests https://github.com/psf/requests/blob/fd13816d015c4c90ee65297fa996caea6a094ed1/requests/models.py#L476-L479"}], "_meta": {"repo": "encode/httpx", "pr_number": 866, "url": "https://github.com/encode/httpx/pull/866#discussion_r400079171"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `base.py`:\n\n```python\ndef __init__(self, *args, **kwargs):\n        if hasattr(self.key_t, '__func__'):  # pragma: no cover\n            self.key_t = self.key_t.__func__  # remove binding\n        self._encode_prefixes()\n        super().__init__(*args, **kwargs)\n        self._add_global_keyprefix()\n        self._encode_prefixes()\n        if self.implements_incr:\n            self.apply_chord = self._apply_chord_incr\n\n    def _add_global_keyprefix(self):\n```"}, {"role": "assistant", "content": "If you could elaborate on what you exactly mean here, that would help a lot on our side to adjust the PR as per your needs.\r\n\r\nAnyways, the method already has docstrings and the feature is already documented based on my understanding: https://docs.celeryq.dev/en/master/search.html?q=keyprefix."}], "_meta": {"repo": "celery/celery", "pr_number": 7620, "url": "https://github.com/celery/celery/pull/7620#discussion_r1015162243"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_fp_quant.py`:\n\n```python\noutput = model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n            self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n\n    @require_torch_multi_gpu\n    def test_quantized_model_multi_gpu(self):\n    @require_torch_multi_accelerator\n    def test_quantized_model_multi_accelerator(self):\n        \"\"\"\n        Simple test that checks if the quantized model is working properly with multiple GPUs\n        set CUDA_VISIBLE_DEVICES=0,1 if you have more than 2 GPUs\n        Simple test that checks if the quantized model is working properly with multiple accelerators\n        set CUDA_VISIBLE_DEVICES=0,1 if you have more than 2 CUDA GPUs, or set ZE_AFFINITY_MASK=0,1\n        if you have more than 2 Intel XPUs\n```"}, {"role": "assistant", "content": "```suggestion\r\n        Simple test that checks if the quantized model is working properly with multiple accelerators.\r\n        Set CUDA_VISIBLE_DEVICES=0,1 if you have more than 2 CUDA GPUs. Or set ZE_AFFINITY_MASK=0,1\r\n        if you have more than 2 Intel XPUs.\r\n```"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41833, "url": "https://github.com/huggingface/transformers/pull/41833#discussion_r2469898074"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `models.py`:\n\n```python\nfp = StringIO(fp)\n            fields.update({k: (fn, fp.read())})\n\n        for field in fields:\n            if isinstance(fields[field], float):\n                fields[field] = str(fields[field])\n            if isinstance(fields[field], list):\n                newvalue = ', '.join(fields[field])\n```"}, {"role": "assistant", "content": "This makes serialisation of lists is inconsistent between `application/x-www-form-urlencoded` and `multipart/form-data` requests. \n\nSee https://github.com/kennethreitz/requests/issues/737#issuecomment-7305046 for more details."}], "_meta": {"repo": "psf/requests", "pr_number": 729, "url": "https://github.com/psf/requests/pull/729#discussion_r1252408"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `misc.py`:\n\n```python\ndel os.environ[k]\n            else:\n                os.environ[k] = v\n\n\nif not six.PY2:\n    from textwrap import dedent\n\n    def is_generator_with_return_value(callable):\n        \"\"\"\n        Returns True if a callable is a generator function which includes a\n        'return' statement with a value different than None, False otherwise\n        \"\"\"\n\n        def returns_none(return_node):\n            value = return_node.value\n            return value is None or isinstance(value, ast.NameConstant) and value.value is None\n\n        if inspect.isgeneratorfunction(callable):\n            tree = ast.parse(dedent(inspect.getsource(callable)))\n            for node in ast.walk(tree):\n                if isinstance(node, ast.Return) and not returns_none(node):\n                    return True\n        return False\n\n    def warn_on_generator_with_return_value(spider, callable, logger):\n        \"\"\"\n        Logs a warning if a callable is a generator function and includes\n        a 'return' statement with a value different than None\n        \"\"\"\n        if is_generator_with_return_value(callable):\n            logger.warning(\n```"}, {"role": "assistant", "content": "What do you think about using warnings.warn instead? Otherwise this message will be logged at each request. By using warnings you'll also avoid passing logger argument."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 3869, "url": "https://github.com/scrapy/scrapy/pull/3869#discussion_r304315127"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_downloadermiddleware.py`:\n\n```python\nfrom pytest import mark\nfrom twisted.internet import defer\nfrom twisted.internet.defer import Deferred\nfrom twisted.internet.defer import Deferred, inlineCallbacks\nfrom twisted.python.failure import Failure\nfrom twisted.trial.unittest import TestCase\n\nfrom scrapy.core.downloader.middleware import DownloaderMiddlewareManager\nfrom scrapy.exceptions import _InvalidOutput\nfrom scrapy.http import Request, Response\nfrom scrapy.spiders import Spider\nfrom scrapy.utils.python import to_bytes\nfrom scrapy.utils.test import get_crawler, get_from_asyncio_queue\nfrom tests.spiders import NoRequestsSpider\n\n\nclass ManagerTestCase(TestCase):\n    settings_dict = None\n\n    @inlineCallbacks\n    def setUp(self):\n        self.crawler = get_crawler(Spider, self.settings_dict)\n        self.spider = self.crawler._create_spider(\"foo\")\n        self.crawler = get_crawler(NoRequestsSpider, self.settings_dict)\n        yield self.crawler.crawl()\n        self.spider = self.crawler.spider\n        self.mwman = DownloaderMiddlewareManager.from_crawler(self.crawler)\n        # some mw depends on stats collector\n        self.crawler.stats.open_spider(self.spider)\n        return self.mwman.open_spider(self.spider)\n        yield self.mwman.open_spider(self.spider)\n\n    def tearDown(self):\n        self.crawler.stats.close_spider(self.spider, \"\")\n        return self.mwman.close_spider(self.spider)\n```"}, {"role": "assistant", "content": "`ExecutionEngine.start()` returns a deferred that fires when the crawling finishes."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6038, "url": "https://github.com/scrapy/scrapy/pull/6038#discussion_r1321486553"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `main.py`:\n\n```python\nraise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')\n\n        def __setattr__(self, name: str, value: Any) -> None:\n            if name in self.__class_vars__:\n            if (setattr_handler := self.__pydantic_setattr_handlers__.get(name)) is not None:\n                setattr_handler(self, value)\n                return\n\n            if (setattr_handler := self._setattr_handler(name, value)) is not None:\n```"}, {"role": "assistant", "content": "```suggestion\r\n            elif (setattr_handler := self._setattr_handler(name, value)) is not None:\r\n```"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10868, "url": "https://github.com/pydantic/pydantic/pull/10868#discussion_r1847216034"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `crawl.py`:\n\n```python\nrule = self._rules[failure.request.meta['rule']]\n        return self._handle_failure(failure, rule.errback)\n\n    def _parse_response(self, response, callback, cb_kwargs, follow=True):\n    async def _parse_response(self, response, callback, cb_kwargs, follow=True):\n        if callback:\n            cb_res = callback(response, **cb_kwargs) or ()\n            if isinstance(cb_res, AsyncIterable):\n                cb_res = await collect_asyncgen(cb_res)\n```"}, {"role": "assistant", "content": "Makes sense, thanks for pointing it out. Please note that this is coherent with `CrawlSpider.process_results`, which is documented to receive a list (although said docs only mention it in the context of the XMLFeedSpider)"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5657, "url": "https://github.com/scrapy/scrapy/pull/5657#discussion_r986110758"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `stream.py`:\n\n```python\n# TODO: Check if scheme can be \"http\" for HTTP/2 ?\n            (\":scheme\", \"https\"),\n            (\":path\", url.path)\n            (\":path\", url.path),\n\n            # TODO: Make sure 'Content-Type' and 'Content-Encoding' headers\n            #  are sent for request having body\n        ]\n\n        self._conn.send_headers(self.stream_id, http2_request_headers)\n        self._conn.send_headers(self.stream_id, http2_request_headers, end_stream=False)\n        self._write_to_transport()\n\n        self.send_data()\n\n    def send_data(self):\n        \"\"\"Called immediately after the headers are sent. Here we send all the\n         data as part of the request.\n\n         If the content length is 0 initially then we end the stream immediately and\n         wait for response data.\n         \"\"\"\n\n        # TODO:\n        #  1. Add test for sending very large data\n        #  2. Add test for small data\n        #  3. Both (1) and (2) should be tested for\n        #    3.1 Large number of request\n        #    3.2 Small number of requests\n\n        # Firstly, check what the flow control window is for current stream.\n        window_size = self._conn.local_flow_control_window(stream_id=self.stream_id)\n\n        # Next, check what the maximum frame size is.\n        max_frame_size = self._conn.max_outbound_frame_size\n\n        # We will send no more than the window size or the remaining file size\n        # of data in this call, whichever is smaller.\n        bytes_to_send = min(window_size, self.remaining_content_length)\n\n        # We now need to send a number of data frames.\n        while bytes_to_send > 0:\n            chunk_size = min(bytes_to_send, max_frame_size)\n\n            data_chunk_start = self.content_length - self.remaining_content_length\n            data_chunk = self._request_body[data_chunk_start:data_chunk_start + chunk_size]\n\n    def receive_data(self, data: bytes):\n            self._conn.send_data(self.stream_id, data_chunk, end_stream=False)\n            self._write_to_transport()\n\n            bytes_to_send = max(0, bytes_to_send - chunk_size)\n            self.remaining_content_length = max(0, self.remaining_content_length - chunk_size)\n```"}, {"role": "assistant", "content": "Given the prior lines:\r\n\r\n```\r\nbytes_to_send = min(window_size, self.remaining_content_length)\r\n# \u2026\r\nchunk_size = min(bytes_to_send, max_frame_size)\r\n```\r\n\r\nI _think_ it would be safe to assume that the subtraction value will be \u22650."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4610, "url": "https://github.com/scrapy/scrapy/pull/4610#discussion_r439860809"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `hitl.py`:\n\n```python\nhitl_details: Iterable[HITLDetail]\n    total_entries: int\n\n\nclass HITLDetailHisotry(BaseHITLDetail):\n```"}, {"role": "assistant", "content": "Needs to be deleted, this doesn't exist anymore, it's `HITLDetailHistory`"}], "_meta": {"repo": "apache/airflow", "pr_number": 56760, "url": "https://github.com/apache/airflow/pull/56760#discussion_r2577683278"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `display.py`:\n\n```python\n\"\"\"\n\nimport sys\nfrom packaging.version import parse\nfrom platform import version\nfrom pprint import pformat as pformat_\n\n\ndef _colorize(text, colorize=True):\n    if not colorize or not sys.stdout.isatty():\n        return text\n    try:\n        import pygments.util\n        from pygments.formatters import get_formatter_by_name\n        from pygments import highlight\n        from pygments.formatters import TerminalFormatter\n        from pygments.lexers import PythonLexer\n        return highlight(text, PythonLexer(), TerminalFormatter())\n    except ImportError:\n        return text\n\n    # All Windows versions >= \"10.0.14393\" interpret ANSI escape sequences\n    # using terminal processing.\n    #\n    # Enable enivornment variable `ENABLE_VIRTUAL_TERMINAL_PROCESSING`\n    # to activate terminal processing.\n    if sys.platform == \"win32\" and parse(version()) >= parse(\"10.0.14393\"):\n        import ctypes\n        kernel32 = ctypes.windll.kernel32\n        kernel32.SetConsoleMode(kernel32.GetStdHandle(-11), 7)\n```"}, {"role": "assistant", "content": "I have added some error handling there. Please check if its correct."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4403, "url": "https://github.com/scrapy/scrapy/pull/4403#discussion_r400333503"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `main.py`:\n\n```python\nreturn self.__fields__.keys()  # | self.__sqlmodel_relationships__.keys()\n\n        keys: AbstractSet[str]\n        if exclude_unset:\n            keys = self.__fields_set__.copy()\n        else:\n            # Original in Pydantic:\n            # keys = self.__dict__.keys()\n            # Updated to not return SQLAlchemy attributes\n            # Do not include relationships as that would easily lead to infinite\n            # recursion, or traversing the whole database\n            keys = self.__fields__.keys()  # | self.__sqlmodel_relationships__.keys()\n\n        keys = self.__fields_set__.copy() if exclude_unset else self.__fields__.keys()\n```"}, {"role": "assistant", "content": "I prefer to keep the previous version with the comments explaining what's happening."}], "_meta": {"repo": "tiangolo/sqlmodel", "pr_number": 53, "url": "https://github.com/fastapi/sqlmodel/pull/53#discussion_r956620315"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `scheduler.py`:\n\n```python\n@classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        dupefilter_cls = load_object(settings['DUPEFILTER_CLASS'])\n        dupefilter = create_instance(dupefilter_cls, settings, crawler)\n        pqclass = load_object(settings['SCHEDULER_PRIORITY_QUEUE'])\n        dqclass = load_object(settings['SCHEDULER_DISK_QUEUE'])\n        mqclass = load_object(settings['SCHEDULER_MEMORY_QUEUE'])\n        logunser = settings.getbool('SCHEDULER_DEBUG')\n        return cls(dupefilter, jobdir=job_dir(settings), logunser=logunser,\n                   stats=crawler.stats, pqclass=pqclass, dqclass=dqclass,\n                   mqclass=mqclass, crawler=crawler)\n\n    def has_pending_requests(self):\n        \"\"\"\n        Factory method, initializes the scheduler with arguments taken from the crawl settings\n        \"\"\"\n        dupefilter_cls = load_object(crawler.settings['DUPEFILTER_CLASS'])\n        return cls(\n            dupefilter=create_instance(dupefilter_cls, crawler.settings, crawler),\n            jobdir=job_dir(crawler.settings),\n            dqclass=load_object(crawler.settings['SCHEDULER_DISK_QUEUE']),\n            mqclass=load_object(crawler.settings['SCHEDULER_MEMORY_QUEUE']),\n            logunser=crawler.settings.getbool('SCHEDULER_DEBUG'),\n            stats=crawler.stats,\n            pqclass=load_object(crawler.settings['SCHEDULER_PRIORITY_QUEUE']),\n            crawler=crawler,\n        )\n\n    def has_pending_requests(self) -> bool:\n        return len(self) > 0\n\n    def open(self, spider):\n    def open(self, spider: Spider) -> Optional[Deferred]:\n        \"\"\"\n        (1) initialize the memory queue\n        (2) initialize the disk queue if the ``jobdir`` attribute is a valid directory\n        (3) return the result of the dupefilter's ``open`` method\n        \"\"\"\n        self.spider = spider\n        self.mqs = self._mq()\n        self.dqs = self._dq() if self.dqdir else None\n        return self.df.open()\n\n    def close(self, reason):\n        if self.dqs:\n    def close(self, reason: str) -> Optional[Deferred]:\n        \"\"\"\n        (1) dump pending requests to disk if there is a disk queue\n        (2) return the result of the dupefilter's ``close`` method\n        \"\"\"\n        if self.dqs is not None:\n            state = self.dqs.close()\n            assert isinstance(self.dqdir, str)\n```"}, {"role": "assistant", "content": "This assertion is only to avoid a typing error in the next line (`_write_dqs_state` expects a `str` but it gets `Optional[str]`). At this point, if `self.dqs is not None` it's only because `self.dqdir` is a `str`."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 3559, "url": "https://github.com/scrapy/scrapy/pull/3559#discussion_r601734280"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_auth.py`:\n\n```python\nassert len(response.history) == 1\n\n\n@pytest.mark.asyncio\nasync def test_digest_auth_reuses_challenge() -> None:\n    url = \"https://example.org/\"\n    auth = DigestAuth(username=\"tomchristie\", password=\"password123\")\n    app = DigestApp()\n\n    async with httpx.AsyncClient(transport=httpx.MockTransport(app)) as client:\n        response_1 = await client.get(url, auth=auth)\n        response_2 = await client.get(url, auth=auth)\n\n        assert response_1.status_code == 200\n        assert response_2.status_code == 200\n\n        assert len(response_1.history) == 1\n        assert len(response_2.history) == 0\n\n\n@pytest.mark.asyncio\nasync def test_digest_auth_resets_nonce_count_after_401() -> None:\n    url = \"https://example.org/\"\n    auth = DigestAuth(username=\"tomchristie\", password=\"password123\")\n    app = DigestApp()\n\n    async with httpx.AsyncClient(transport=httpx.MockTransport(app)) as client:\n        response_1 = await client.get(url, auth=auth)\n        assert response_1.status_code == 200\n        assert len(response_1.history) == 1\n        first_nonce = response_1.json()[\"auth\"].split(\"nonce=\")[1].split(\",\")[0]\n        first_nc = response_1.json()[\"auth\"].split(\"nc=\")[1].split(\",\")[0]\n\n        app.send_response_after_attempt = 2\n```"}, {"role": "assistant", "content": "This is a bit obscure to me, I think this line deserves a comment."}], "_meta": {"repo": "encode/httpx", "pr_number": 2463, "url": "https://github.com/encode/httpx/pull/2463#discussion_r1032528869"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_fuyu.py`:\n\n```python\npixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n                The tensors corresponding to the input images.\n        \"\"\"\n        patch_embeddings = [\n            self.vision_embed_tokens(patch.to(self.vision_embed_tokens.weight.dtype)).squeeze(0)\n            for patch in pixel_values\n        ]\n        return patch_embeddings\n        return self.vision_embed_tokens(pixel_values)\n```"}, {"role": "assistant", "content": "cc  maybe (looks like this was added in https://github.com/huggingface/transformers/pull/27007)"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42317, "url": "https://github.com/huggingface/transformers/pull/42317#discussion_r2569524622"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_utils.py`:\n\n```python\nsub_dtype = getattr(torch, sub_dtype) if isinstance(sub_dtype, str) else sub_dtype\n            else:\n                sub_dtype = main_dtype\n            if main_dtype != sub_dtype:\n                logger.warning_once(\n                    f\"The dtype for {sub_config_key}={sub_dtype} is different from the main dtype={main_dtype}. \"\n                    \"Setting different dtypes per backbone model might cause device errors downstream, setting the \"\n                    f\"dtype={main_dtype} for all modules. Note, using different dtypes per module is deprecated and will \"\n                    \"be removed in future versions.\"\n                )\n```"}, {"role": "assistant", "content": "oh, we also could check `Noneness` to reduce the warnings. It's not common for models to have different dtypes saved per backbone, the above ones for ex have the same dtype duplicated many times"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42990, "url": "https://github.com/huggingface/transformers/pull/42990#discussion_r2661313964"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `setup.py`:\n\n```python\nfrom setuptools import setup\nfrom setuptools.command.test import test as TestCommand\n\nhere = os.path.abspath(os.path.dirname(__file__))\n\nCURRENT_PYTHON = sys.version_info[:2]\nREQUIRED_PYTHON = (3, 7)\n\nif CURRENT_PYTHON < REQUIRED_PYTHON:\n    sys.stderr.write(\n        \"\"\"\n==========================\nUnsupported Python version\n==========================\nThis version of Requests requires Python {}.{}, but you're trying to\ninstall it on Python {}.{}.\nThis may be because you are using a version of pip that doesn't\nunderstand the python_requires classifier. Make sure you\n```"}, {"role": "assistant", "content": "Noted that new Python versions come with a \"good\" pip version out of the box. Maybe our strategy should be to instruct the user to either pin to <2.28 or upgrade to at least Python 3.7 (since they'll need that version to install Requests anyways) and then we don't need to mention pip/setuptools at all."}], "_meta": {"repo": "psf/requests", "pr_number": 6091, "url": "https://github.com/psf/requests/pull/6091#discussion_r834612265"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `utils.py`:\n\n```python\nif generation_config.cache_implementation == \"hybrid\":\n            generation_config.cache_implementation = None\n\n        # It doesn't make sense to allow kwargs and `generation_config`, that should be mutually exclusive\n        if generation_config_provided is not None and set(kwargs.keys()) - set(model_kwargs.keys()):\n            generation_kwargs = set(kwargs.keys()) - set(model_kwargs.keys())\n            logger.warning_once(\n                f\"Passing `generation_config` together with generation-related \"\n                f\"arguments=({generation_kwargs}) is deprecated and will be removed in future versions. \"\n                \"Please pass either a `generation_config` object OR all generation \"\n                \"parameters explicitly, but not both.\",\n            )\n```"}, {"role": "assistant", "content": "yeah, we don't want to break it yet. This is to nudge users for better practices and give them time to get used to it. Maybe we can raise errors after 10-15 releases"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43194, "url": "https://github.com/huggingface/transformers/pull/43194#discussion_r2676701030"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `linegen.py`:\n\n```python\nelif not (isinstance(child, Leaf) and is_multiline_string(child)):\n                wrap_in_parentheses(node, child, visible=False)\n\n        check_lpar = isinstance(child, Leaf) and child.value in parens_after\n        comma_check = child.type == token.COMMA if preview else False\n\n        check_lpar = (\n            isinstance(child, Leaf) and child.value in parens_after or comma_check\n```"}, {"role": "assistant", "content": "I don't think this change is right. A comma is a `Leaf` as well so the comma wouldn't get checked here. The comma check doesn't need the `Leaf` check since it uses `.type` so I can do either of these if it's more clear?\r\n```python\r\nisinstance(child, Leaf) and (child.value in parens_after or comma_check)\r\n```\r\nor\r\n```python\r\ncomma_check or (isinstance(child, Leaf) and (child.value in parens_after or comma_check)\r\n```"}], "_meta": {"repo": "psf/black", "pr_number": 2926, "url": "https://github.com/psf/black/pull/2926#discussion_r833082659"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `scraper.py`:\n\n```python\ndef call_spider(self, result, request, spider):\n        result.request = request\n        dfd = defer_result(result)\n        dfd.addCallbacks(callback=request.callback or spider.parse,\n        callback = request.callback or spider.parse\n        if not six.PY2:\n            warn_on_generator_with_return_value(spider, callback, logger)\n            warn_on_generator_with_return_value(spider, request.errback, logger)\n```"}, {"role": "assistant", "content": "Mind https://stackoverflow.com/questions/11815873/memoization-library-for-python-2-7"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 3869, "url": "https://github.com/scrapy/scrapy/pull/3869#discussion_r304975164"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `quanto.py`:\n\n```python\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom ..utils import is_optimum_quanto_available, is_torch_available, logging\nfrom ..core_model_loading import ConversionOps\nfrom ..quantizers.quantizers_utils import get_module_from_name, should_convert_module\nfrom ..utils import is_torch_available, logging\n\n\nif is_torch_available():\n    import torch\n    import torch.nn as nn\n\nlogger = logging.get_logger(__name__)\n\n\nclass QuantoQuantize(ConversionOps):\n    def __init__(self, hf_quantizer):\n        self.hf_quantizer = hf_quantizer\n\n    def convert(\n        self,\n        input_dict: dict[str, list[torch.Tensor]],\n        model: torch.nn.Module | None = None,\n        full_layer_name: str | None = None,\n        missing_keys: list[str] | None = None,\n        **kwargs,\n    ) -> dict[str, torch.Tensor]:\n        _, value = tuple(input_dict.items())[0]\n        value = value[0]\n\n        from ..modeling_utils import _load_parameter_into_model\n\n        _load_parameter_into_model(model, full_layer_name, value)\n        module, _ = get_module_from_name(model, full_layer_name)\n        module.freeze()\n        module.weight.requires_grad = False\n\n        # need to discard some missing keys we already updated the module in freeze. For the weight, we still pass it\n        module_name = full_layer_name.rsplit(\".\", 1)[0]\n        missing_keys.discard(f\"{module_name}.input_scale\")\n        missing_keys.discard(f\"{module_name}.output_scale\")\n        return {full_layer_name: module.weight}\n```"}, {"role": "assistant", "content": "lets return empty dict to show its already loaded"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42557, "url": "https://github.com/huggingface/transformers/pull/42557#discussion_r2581489526"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `configuration_llama4.py`:\n\n```python\nThe id of the end of sentence token.\n        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n            Whether to tie weight embeddings\n        attention_dropout (`int`, *optional*, defaults to 0.0): TODO\n        num_experts_per_tok (`int`, *optional*, defaults to 1): TODO\n        num_local_experts (`int`, *optional*, defaults to 16): TODO\n        moe_layers (`int`, *optional*): TODO\n        interleave_moe_layer_step (`int`, *optional*, defaults to 1): TODO\n        use_qk_norm (`int`, *optional*, defaults to `True`): TODO\n        output_router_logits (`int`, *optional*, defaults to `False`): TODO\n        router_aux_loss_coef (`int`, *optional*, defaults to 0.001): TODO\n        router_jitter_noise (`int`, *optional*, defaults to 0.0): TODO\n        attention_dropout (`int`, *optional*, defaults to 0.0):\n            The dropout ratio for the attention probabilities.\n        num_experts_per_tok (`int`, *optional*, defaults to 1):\n            Number of experts to route each token to. This is the top-k value for the token-choice routing.\n        num_local_experts (`int`, *optional*, defaults to 16):\n            Number of experts for each Softmax router.\n        moe_layers (`int`, *optional*):\n            Indices of the layers that are MoE layers. If not specified, will be calculated using `interleave_moe_layer_step`.\n        interleave_moe_layer_step (`int`, *optional*, defaults to 1):\n            The frequency of MoE layers in the model. For example, setting it to 2 means every 2nd layer is an MoE layer.\n        use_qk_norm (`int`, *optional*, defaults to `True`):\n            Whether to normalize the Query and Key matrices in the attention layer.\n        output_router_logits (`int`, *optional*, defaults to `False`):\n```"}, {"role": "assistant", "content": "The parameter type should be `bool` instead of `int`. Looking at the function signature (line 289), `output_router_logits` is defined as `bool` with a default value of `False`, not `int`.\n```suggestion\n        output_router_logits (`bool`, *optional*, defaults to `False`):\n```"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43460, "url": "https://github.com/huggingface/transformers/pull/43460#discussion_r2723836595"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `cache.py`:\n\n```python\nprompt_ids=(state.initial_tokens + state.generated_tokens),\n                )\n\n    def copy_cache(self, source_blocks: list[int], forked_blocks: list[int]) -> None:\n        \"\"\"Copy the cache from the source blocks to the forked blocks.\"\"\"\n        source_blocks = torch.tensor(source_blocks, device=self.device, dtype=torch.int32)\n        forked_blocks = torch.tensor(forked_blocks, device=self.device, dtype=torch.int32)\n        for key_cache, value_cache in zip(self.key_cache, self.value_cache):\n            key_cache = key_cache.view(-1, self.block_size, self.num_key_value_heads, self.head_dim)\n            value_cache = value_cache.view(-1, self.block_size, self.num_key_value_heads, self.head_dim)\n            key_cache[forked_blocks] = key_cache[source_blocks]\n            value_cache[forked_blocks] = value_cache[source_blocks]\n            # FIXME: should be one copy for al CMs with only the changing blocks\n            # FIXME: even once per fork batch\n\n    def fork_request(self, state: RequestState, new_request_id: str) -> RequestState:\n        \"\"\"Fork a request into a new request. The new request is created by copying the state and updating the\n        request_id.\"\"\"\n        new_state = state.fork(new_request_id)\n        for cm in self.group_cache_managers:\n            source_blocks, forked_blocks = cm.fork_blocks(state.request_id, new_state.request_id, self._block_manager)\n            self.copy_cache(source_blocks, forked_blocks)\n```"}, {"role": "assistant", "content": "Since the copy is device to device, I think the best we can do for now is one copy as is the case right now. Plus, I think pytorch works in async in that case, ie. CPU operations continue after the copy is launched"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42921, "url": "https://github.com/huggingface/transformers/pull/42921#discussion_r2660697352"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_model_construction.py`:\n\n```python\nprivate_attributes = self.__dict__.get('__private_attributes__')\n            if private_attributes and item in private_attributes:\n                return private_attributes[item]\n            if item == '__pydantic_core_schema__':\n                # This means the class didn't get a schema generated for it, likely because there was an undefined reference\n                maybe_mock_validator = getattr(self, '__pydantic_validator__', None)\n                if isinstance(maybe_mock_validator, MockValSer):\n                    rebuilt_validator = maybe_mock_validator.rebuild()\n                    if rebuilt_validator is not None:\n                        # In this case, a validator was built, and so `__pydantic_core_schema__` should now be set\n                        return getattr(self, '__pydantic_core_schema__')\n```"}, {"role": "assistant", "content": "> Can this really be removed entirely\r\n\r\nYes this was now dead code as the `__pydantic_core_schema__` is handled similarly as the other `MockValSer`s. So avoiding also the issues with the duplicated schema building."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 8939, "url": "https://github.com/pydantic/pydantic/pull/8939#discussion_r1549798625"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_ssl.py`:\n\n```python\nwith self.assertRaises(OSError) as cm:\n            ctx.load_cert_chain(NONEXISTINGCERT)\n        self.assertEqual(cm.exception.errno, errno.ENOENT)\n        self.assertEqual(cm.exception.filename, NONEXISTINGCERT)\n        with self.assertRaises(OSError) as cm:\n```"}, {"role": "assistant", "content": "That was what I had in mind, but maybe not that important actually. You can forget about this one actually (otherwise for consistency we would need to add it everywhere else)"}], "_meta": {"repo": "python/cpython", "pr_number": 144250, "url": "https://github.com/python/cpython/pull/144250#discussion_r2732453543"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `common.py`:\n\n```python\nimport pathlib\n\nTESTS_DIR = pathlib.Path(__file__).parent\nTESTS_DIR = TESTS_DIR.relative_to(TESTS_DIR.parent)  # Ensure relative to project root.\n```"}, {"role": "assistant", "content": "Good point, I missed this part (and learned about `relative_to()` in the way!)."}], "_meta": {"repo": "encode/httpx", "pr_number": 1052, "url": "https://github.com/encode/httpx/pull/1052#discussion_r449894246"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `setup.py`:\n\n```python\n\"Programming Language :: Python :: 3.7\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n```"}, {"role": "assistant", "content": "We already declare Python 3.9 support without formatting all such syntax (e.g. parenthesised context managers), so I'd support adding this classifier - `black` does run on 3.10 after all!"}], "_meta": {"repo": "psf/black", "pr_number": 2562, "url": "https://github.com/psf/black/pull/2562#discussion_r736164887"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_model_construction.py`:\n\n```python\n_make_forward_ref,\n    eval_type_backport,\n    is_annotated,\n    is_classvar,\n    is_classvar_annotation,\n```"}, {"role": "assistant", "content": "Should we follow the same pattern here and just do `import _typing_extra` then call associated methods like `_typing_extra.XXX`?"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10725, "url": "https://github.com/pydantic/pydantic/pull/10725#discussion_r1825745805"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `continuous_api.py`:\n\n```python\n# some NaNs in the output logits even for non-padded tokens.\n        if use_padding:\n            self.max_seqlen_q = max(self.max_seqlen_q, q_len - self.total_seqlen_q)\n            kwargs[\"max_seqlen_q\"] = self.max_seqlen_q  # Update kwargs with the new value\n```"}, {"role": "assistant", "content": "```suggestion\n            kwargs.max_seqlen_q = self.max_seqlen_q  # Update kwargs with the new value\n```"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42869, "url": "https://github.com/huggingface/transformers/pull/42869#discussion_r2742060781"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `concurrency.py`:\n\n```python\nif sys.platform == \"win32\":\n        # Work around https://bugs.python.org/issue26903\n        workers = min(workers, 60)\n    if getattr(sys, \"frozen\", False):\n        # In frozen (PyInstaller) builds, avoid multiprocessing to prevent shutdown\n        # errors when worker processes try to import modules after cleanup begins.\n```"}, {"role": "assistant", "content": "The inline comment says \"avoid multiprocessing\", but this change specifically avoids using `ProcessPoolExecutor` (and `schedule_formatting()` can still create a `multiprocessing.Manager()` for `--diff/--color-diff`). Consider rewording to something more precise like avoiding spawning worker processes / avoiding `ProcessPoolExecutor`, and (optionally) broaden \"PyInstaller\" to \"frozen builds (e.g. PyInstaller)\" since `sys.frozen` isn\u2019t PyInstaller-specific.\n```suggestion\n        # In frozen builds (e.g. PyInstaller), avoid spawning worker processes (i.e.\n        # avoid using ProcessPoolExecutor) to prevent shutdown errors when workers\n        # try to import modules after cleanup begins.\n```"}], "_meta": {"repo": "psf/black", "pr_number": 4930, "url": "https://github.com/psf/black/pull/4930#discussion_r2719370230"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_exceptions.py`:\n\n```python\n# Other transport exceptions...\n\n\nclass ProxyError(HTTPError):\nclass ProxyError(TransportError):\n    \"\"\"\n    An error occurred while proxying a request.\n    \"\"\"\n\n\nclass ProtocolError(HTTPError):\nclass ProtocolError(TransportError):\n    \"\"\"\n    A protocol was violated by the server.\n    \"\"\"\n\n\n# HTTP exceptions...\n# Other request exceptions...\n\n\nclass DecodingError(HTTPError):\nclass DecodingError(RequestError):\n    \"\"\"\n    Decoding of the response failed.\n    \"\"\"\n\n\nclass HTTPStatusError(HTTPError):\nclass TooManyRedirects(RequestError):\n    \"\"\"\n    Response sent an error HTTP status.\n    Too many redirects.\n    \"\"\"\n\n    def __init__(self, *args: typing.Any, response: \"Response\") -> None:\n        super().__init__(*args)\n        self._request = response.request\n        self.response = response\n\n\n# Redirect exceptions...\nclass RequestBodyUnavailable(RequestError):\n    \"\"\"\n    Had to send the request again, but the request body was streaming, and is\n    no longer available.\n    \"\"\"\n\n\nclass RedirectError(HTTPError):\nclass InvalidURL(RequestError):\n    \"\"\"\n    Base class for HTTP redirect errors.\n    URL was missing a hostname, or was not one of HTTP/HTTPS.\n    \"\"\"\n\n\nclass TooManyRedirects(RedirectError):\n# Client errors\n\n\nclass HTTPStatusError(Exception):\n    \"\"\"\n    Too many redirects.\n    Response sent an error HTTP status.\n\n    May be raised when calling `response.raise_for_status()`\n    \"\"\"\n\n    def __init__(\n        self, message: str, *, request: \"Request\", response: \"Response\"\n    ) -> None:\n        super().__init__(message)\n        self.request = request\n        self.response = response\n\nclass NotRedirectResponse(RedirectError):\n\nclass NotRedirectResponse(Exception):\n    \"\"\"\n    Response was not a redirect response.\n\n    May be raised if `response.next()` is called without first\n    properly checking `response.is_redirect`.\n    \"\"\"\n\n    def __init__(self, message: str) -> None:\n        super().__init__(message)\n\n\nclass CookieConflict(Exception):\n    \"\"\"\n    Attempted to lookup a cookie by name, but multiple cookies existed.\n\n    Can occur when calling `response.cookies.get(...)`\n    \"\"\"\n\n    def __init__(self, message: str) -> None:\n        super().__init__(message)\n\n\n# Stream exceptions...\n\n# These may occur as the result of a programming error, by accessing\n# the request/respose stream in an invalid manner.\n```"}, {"role": "assistant", "content": "```suggestion\r\n# the request/response stream in an invalid manner.\r\n```"}], "_meta": {"repo": "encode/httpx", "pr_number": 1095, "url": "https://github.com/encode/httpx/pull/1095#discussion_r463131050"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `spiders.py`:\n\n```python\nself.logger.info(\"Got response %d\" % response.status)\n\n\nclass AsyncDefSpider(SimpleSpider):\n\n    name = 'asyncdef'\n\n    async def parse(self, response):\n        await defer.succeed(42)\n        self.logger.info(\"Got response %d\" % response.status)\n\n\nclass AsyncDefAsyncioSpider(SimpleSpider):\n\n    name = 'asyncdef_asyncio'\n\n    async def parse(self, response):\n        await asyncio.sleep(0.2)\n        status = await get_from_asyncio_queue(response.status)\n        self.logger.info(\"Got response %d\" % status)\n\n\nclass AsyncDefAsyncioReturnSpider(SimpleSpider):\n\n    name = 'asyncdef_asyncio_return'\n\n    async def parse(self, response):\n        await asyncio.sleep(0.2)\n        status = await get_from_asyncio_queue(response.status)\n        self.logger.info(\"Got response %d\" % status)\n        return [{'id': 1}, {'id': 2}]\n```"}, {"role": "assistant", "content": "Nice, so lists of items are supported. What about requests?"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4269, "url": "https://github.com/scrapy/scrapy/pull/4269#discussion_r373117947"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `networks.py`:\n\n```python\nimport email_validator\n    except ImportError as e:\n        raise ImportError('email-validator is not installed, run `pip install pydantic[email]`') from e\n    if getattr(email_validator, '__version__', '').partition('.')[0] == '2':\n        return\n    else:\n        input_email = 'testaddr@example.tld'\n        valid_email = email_validator.validate_email(input_email, check_deliverability=False)\n        if not hasattr(valid_email, 'normalized'):\n            raise ImportError('email-validator version >= 2.0 required')\n```"}, {"role": "assistant", "content": "Good call. Let's go with:\r\n\r\n```suggestion\r\n    if not version(\"email-validator\").partition('.')[0] == '2':\r\n        raise ImportError('email-validator version >= 2.0 required, run pip install -U email-validator')\r\n```"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 6033, "url": "https://github.com/pydantic/pydantic/pull/6033#discussion_r1406610267"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_forward_ref.py`:\n\n```python\nfunc_val(a=1)\n\n\n@pytest.mark.xfail(\n    reason='In `GenerateSchema`, only the current class module is taken into account. '\n    'This is similar to `test_uses_the_correct_globals_to_resolve_model_forward_refs`.'\n)\ndef test_uses_the_correct_globals_to_resolve_forward_refs_on_serializers(create_module):\n    # Note: unlike `test_uses_the_correct_globals_to_resolve_model_forward_refs`,\n    # we use the globals of the underlying func to resolve the return type.\n    @create_module\n    def module_1():\n        from pydantic import BaseModel, field_serializer  # or model_serializer\n        from pydantic import BaseModel, field_serializer  # or model_serializer, computed_field\n```"}, {"role": "assistant", "content": "The goal was to mentioned that this also applied to `computed_field`"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10929, "url": "https://github.com/pydantic/pydantic/pull/10929#discussion_r1852556025"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `feedexport.py`:\n\n```python\ndeferred_list.append(d)\n        return defer.DeferredList(deferred_list) if deferred_list else None\n\n    def _start_new_batch(self, previous_batch_slot, uri, feed, spider, batch_id, template_uri):\n        \"\"\"\n        Redirect the output data stream to a new file.\n        Execute multiple times if 'FEED_STORAGE_BATCH' setting is greater than zero.\n        :param previous_batch_slot: slot of previous batch. We need to call slot.storage.store\n        to get the file properly closed.\n        :param uri: uri of the new batch to start\n        :param feed: dict with parameters of feed\n        :param spider: user spider\n        :param batch_id: sequential batch id starting at 1\n        :param template_uri: template uri which contains %(time)s or %(batch_id)s to create new uri\n        \"\"\"\n        if previous_batch_slot is not None:\n            previous_batch_slot.exporter.finish_exporting()\n            previous_batch_slot.storage.store(previous_batch_slot.file)\n        storage = self._get_storage(uri)\n        file = storage.open(spider)\n        exporter = self._get_exporter(\n            file=file,\n            format=feed['format'],\n            fields_to_export=feed['fields'],\n            encoding=feed['encoding'],\n            indent=feed['indent']\n        )\n        slot = _FeedSlot(file, exporter, storage, uri, feed['format'], feed['store_empty'], batch_id, template_uri)\n        if slot.store_empty:\n            slot.start_exporting()\n        return slot\n\n    def item_scraped(self, item, spider):\n        for slot in self.slots:\n        slots = []\n        for idx, slot in enumerate(self.slots):\n            slot.start_exporting()\n            slot.exporter.export_item(item)\n            slot.itemcount += 1\n            if self.storage_batch_size and slot.itemcount % self.storage_batch_size == 0:\n                batch_id = slot.batch_id + 1\n                uri_params = self._get_uri_params(spider, self.feeds[slot.template_uri]['uri_params'])\n                uri_params['batch_id'] = batch_id\n                self.slots.append(self._start_new_batch(\n                    previous_batch_slot=slot,\n                    uri=slot.template_uri % uri_params,\n                    feed=self.feeds[slot.template_uri],\n                    spider=spider,\n                    batch_id=batch_id,\n                    template_uri=slot.template_uri\n                ))\n                self.slots[idx] = None\n        self.slots = [slot for slot in self.slots if slot is not None]\n        self.slots.extend(slots)\n```"}, {"role": "assistant", "content": "Thank you, `slots.append(self._start_new_batch` should be instead of `self.slots.append(self._start_new_batch`"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4434, "url": "https://github.com/scrapy/scrapy/pull/4434#discussion_r395968505"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_config.py`:\n\n```python\nUNSET = UnsetType()\n\n\ndef create_ssl_context(\n    cert: CertTypes | None = None,\n    verify: VerifyTypes = True,\n    trust_env: bool = True,\n    http2: bool = False,\n) -> ssl.SSLContext:\n    return SSLConfig(\n        cert=cert, verify=verify, trust_env=trust_env, http2=http2\n    ).ssl_context\n\n\nclass SSLConfig:\n    \"\"\"\n    SSL Configuration.\n    \"\"\"\n\nclass SSLContext(ssl.SSLContext):\n    DEFAULT_CA_BUNDLE_PATH = Path(certifi.where())\n\n    def __init__(\n        self,\n        *,\n        cert: CertTypes | None = None,\n        verify: VerifyTypes = True,\n        trust_env: bool = True,\n        http2: bool = False,\n        cert: CertTypes | None = None,\n    ) -> None:\n        self.cert = cert\n        self.verify = verify\n        self.trust_env = trust_env\n        self.http2 = http2\n        self.ssl_context = self.load_ssl_context()\n        set_minimum_tls_version_1_2(self)\n        self.options |= ssl.OP_NO_COMPRESSION\n        self.set_ciphers(DEFAULT_CIPHERS)\n\n    def load_ssl_context(self) -> ssl.SSLContext:\n        logger.debug(\n            \"load_ssl_context verify=%r cert=%r trust_env=%r http2=%r\",\n            self.verify,\n            self.cert,\n            self.trust_env,\n            self.http2,\n            \"load_ssl_context verify=%r cert=%r\",\n            verify,\n            cert,\n        )\n\n        if self.verify:\n            return self.load_ssl_context_verify()\n        return self.load_ssl_context_no_verify()\n        if verify:\n            self.load_ssl_context_verify(cert, verify)\n        else:\n            self.load_ssl_context_no_verify(cert)\n```"}, {"role": "assistant", "content": "since `verify` attribute is set to instance, I think we could mix these in one method:\r\n```py\r\ndef load_ssl_context(self, cert: typing.Optional[CertTypes]) -> None:\r\n    if not self.verify:\r\n        self.check_hostname = False\r\n        self.verify_mode = ssl.CERT_NONE\r\n        self._load_client_certs(cert)\r\n        return\r\n    if isinstance(self.verify, bool):\r\n        ca_bundle_path = self.DEFAULT_CA_BUNDLE_PATH\r\n    ...\r\n    ...\r\n\r\n```"}], "_meta": {"repo": "encode/httpx", "pr_number": 3022, "url": "https://github.com/encode/httpx/pull/3022#discussion_r1771142229"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `scraper.py`:\n\n```python\nif isinstance(output, Failure):\n            ex = output.value\n            if isinstance(ex, DropItem):\n                if ex.log_level is None:\n                    ex.log_level = spider.crawler.settings.get(\n                        \"DEFAULT_DROPITEM_LOG_LEVEL\", \"WARNING\"\n                    )\n                else:\n                    ex.log_level = ex.log_level.upper()\n```"}, {"role": "assistant", "content": "> I have tried both logging.info and logging.INFO in the python interpreter, Python will not raise an error in either case\r\n\r\n```python\r\n>>> logging.log(logging.INFO, \"foo\")\r\n>>> logging.log(logging.info, \"foo\")\r\nTraceback (most recent call last):\r\n  File \"<python-input-8>\", line 1, in <module>\r\n    logging.log(logging.info, \"foo\")\r\n    ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib/python3.13/logging/__init__.py\", line 2215, in log\r\n    root.log(level, msg, *args, **kwargs)\r\n    ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib/python3.13/logging/__init__.py\", line 1585, in log\r\n    raise TypeError(\"level must be an integer\")\r\nTypeError: level must be an integer\r\n```"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6608, "url": "https://github.com/scrapy/scrapy/pull/6608#discussion_r1913340478"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `configuration_aimv2.py`:\n\n```python\nqkv_bias: bool = False,\n        mlp_bias: bool = False,\n        hidden_act: str = \"silu\",\n        pad_token_id: Optional[int] = None,\n        bos_token_id: Optional[int] = None,\n```"}, {"role": "assistant", "content": "modular file shows that these were supposed to be deleted with `del bos_token_id`, but the conversion script could not handle it correctly"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41541, "url": "https://github.com/huggingface/transformers/pull/41541#discussion_r2440382773"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_test_multiprocessing.py`:\n\n```python\n\"resource_tracker: There appear to be 1 leaked \"\n                    \"shared_memory objects to clean up at shutdown\", err)\n\n    @unittest.skipIf(os.name != \"posix\", \"resource_tracker is posix only\")\n    def test_shared_memory_untracking(self):\n        # gh-82300: When a separate Python process accesses shared memory\n        # with track=False, it must not register with the resource tracker.\n        cmd = '''if 1:\n            import sys\n            from unittest.mock import Mock\n            from multiprocessing import resource_tracker\n            from multiprocessing.shared_memory import SharedMemory\n            resource_tracker.register = Mock(side_effect=AssertionError)\n            mem = SharedMemory(create=False, name=sys.argv[1], track=False)\n            mem.close()\n        '''\n        mem = shared_memory.SharedMemory(create=True, size=10)\n        try:\n            *_, err = test.support.script_helper.assert_python_ok(\"-c\", cmd,\n                                                                  mem.name)\n            self.assertEqual(err, b'')\n        finally:\n            mem.close()\n            try:\n                mem.unlink()\n            except OSError:\n```"}, {"role": "assistant", "content": "Good points. The code should be a bit more defensive on cleanup now. \r\n\r\nUnregistering of the resource tracker should not be allowed to fail. This is the resource tracker process that belongs to the main process."}], "_meta": {"repo": "python/cpython", "pr_number": 110778, "url": "https://github.com/python/cpython/pull/110778#discussion_r1410989588"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `gcs_to_bigquery.py`:\n\n```python\nsrc_fmt_configs: dict,\n        valid_configs: list[str],\n        backward_compatibility_configs: dict | None = None,\n        src_fmt_param: str | None = None,\n        valid_nested_configs: list[str] | None = None,\n    ) -> dict:\n        \"\"\"\n        Validate the given src_fmt_configs against a valid configuration for the source format.\n        Validate and format the given src_fmt_configs against a valid configuration for the source format.\n\n        Adds the backward compatibility config to the src_fmt_configs.\n\n        Adds nested source format configurations if valid_nested_configs is provided.\n\n        :param source_format: File format to export.\n        :param src_fmt_configs: Configure optional fields specific to the source format.\n        :param valid_configs: Valid configuration specific to the source format\n        :param backward_compatibility_configs: The top-level params for backward-compatibility\n        :param src_fmt_param: The source format parameter for nested configurations.\n        Required when valid_nested_configs is provided.\n        :param valid_nested_configs: Valid nested configuration specific to the source format.\n        \"\"\"\n        valid_src_fmt_configs = {}\n\n        if backward_compatibility_configs is None:\n            backward_compatibility_configs = {}\n\n        for k, v in backward_compatibility_configs.items():\n            if k not in src_fmt_configs and k in valid_configs:\n                src_fmt_configs[k] = v\n                valid_src_fmt_configs[k] = v\n\n        if valid_nested_configs is None:\n            valid_nested_configs = []\n\n        if valid_nested_configs:\n            if src_fmt_param is None:\n                raise ValueError(\"src_fmt_param is required when valid_nested_configs is provided.\")\n\n        for k in src_fmt_configs:\n            if k not in valid_configs:\n            valid_src_fmt_configs[src_fmt_param] = {}\n\n        for k, v in src_fmt_configs.items():\n            if k in valid_configs:\n                valid_src_fmt_configs[k] = v\n            elif k in valid_nested_configs:\n                valid_src_fmt_configs[src_fmt_param][k] = v\n            else:\n```"}, {"role": "assistant", "content": "I'm not wild about adding complexity here, I think its pretty gross. \n\nThe issue is that both the external table and non external table code paths allow `src_fmt_configs` and use this method, but the non external table code path previously only allowed arguments that were available at the top level of the job config rather than nested `<srcFmt>Options` options.\n\nI'm totally open to suggestions if you think this warrants a larger refactor."}], "_meta": {"repo": "apache/airflow", "pr_number": 60876, "url": "https://github.com/apache/airflow/pull/60876#discussion_r2713134066"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `default_settings.py`:\n\n```python\nFEED_TEMPDIR = None\nFEEDS = {}\nFEED_URI_PARAMS = None  # a function to extend uri arguments\nFEED_STORE_EMPTY = False\nFEED_STORE_EMPTY = True\n```"}, {"role": "assistant", "content": "We need to update the docs accordingly."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5847, "url": "https://github.com/scrapy/scrapy/pull/5847#discussion_r1137406774"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `reactor.py`:\n\n```python\nreturn self._func(*self._a, **self._kw)\n\n\ndef get_asyncio_event_loop_policy():\ndef set_asyncio_event_loop_policy():\n```"}, {"role": "assistant", "content": "For backward compatibility reasons, we should keep the old function around, and log a deprecation warning when called."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5832, "url": "https://github.com/scrapy/scrapy/pull/5832#discussion_r1127575117"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `__init__.py`:\n\n```python\nif value_head in {'f\"', 'F\"', \"f'\", \"F'\", \"rf\", \"fr\", \"RF\", \"FR\"}:\n                features.add(Feature.F_STRINGS)\n\n        elif n.type == token.NUMBER:\n            assert isinstance(n, Leaf)\n```"}, {"role": "assistant", "content": "I'm not familiar with mypyc, so handling over to  for this one"}], "_meta": {"repo": "psf/black", "pr_number": 3069, "url": "https://github.com/psf/black/pull/3069#discussion_r894989525"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_processor_glm_image.py`:\n\n```python\ninputs = processor(**inputs_dict, return_tensors=\"pt\", do_sample_frames=False)\n\n        self.assertSetEqual(set(inputs.keys()), set(processor.model_input_names))\n\n    @unittest.skip(\n        reason=\"GLM-Image processor adds image placeholder tokens which makes sequence length depend on image size\"\n    )\n    def test_kwargs_overrides_default_tokenizer_kwargs(self):\n        pass\n\n    @unittest.skip(\n```"}, {"role": "assistant", "content": "this should be handled already by tests, since we have many models that expand text with placeholders. \n\nWe need to make sure the test has `self.image_token` and override the `sizes` in image processor to tiny values. It will produce small amount of placeholder tokens in that case\n\nLike this:\n\nhttps://github.com/huggingface/transformers/blob/be0115e42fbf5d7ffe915e01b0cf9a68b29958c6/tests/models/glm4v/test_processor_glm4v.py#L39-L51"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43342, "url": "https://github.com/huggingface/transformers/pull/43342#discussion_r2735675213"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `telnet.py`:\n\n```python\nif not crawler.settings.getbool(\"TELNETCONSOLE_ENABLED\"):\n            raise NotConfigured\n\n        if not crawler.settings.getbool(\"TWISTED_ENABLED\"):\n            raise NotConfigured(\n                \"The TelnetConsole extension requires a Twisted reactor.\"\n                \" You can set the TELNETCONSOLE_ENABLED setting to False to remove this warning.\"\n            )\n```"}, {"role": "assistant", "content": "> How would we detect this? Via priorities?\r\n\r\nYes.\r\n\r\nI was hoping to do it at the `default_settings.py` level when I wrote that, but now that I think about it that would not really work. But we may be able to find some place in the code after add-ons have been evaluated, and set the telnet setting to the value of the twisted setting with the basic priority (so it is effectively a no-op if any higher priority has set it)."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 7199, "url": "https://github.com/scrapy/scrapy/pull/7199#discussion_r2709027709"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_mock_val_ser.py`:\n\n```python\nreturn None\n\n\ndef set_type_adapter_mocks(adapter: TypeAdapter, type_repr: str) -> None:\n    \"\"\"Set `core_schema`, `validator` and `serializer` to mock core types on a type adapter instance.\n\n    Args:\n        adapter: The type adapter instance to set the mocks on\n        type_repr: Name of the type used in the adapter, used in error messages\n    \"\"\"\n    undefined_type_error_message = (\n        f'`TypeAdapter[{type_repr}]` is not fully defined; you should define `{type_repr}` and all referenced types,'\n```"}, {"role": "assistant", "content": "Suggested the same in other comment about making the error message forming lazy. So it only happens on errors"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10537, "url": "https://github.com/pydantic/pydantic/pull/10537#discussion_r1841057809"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_modeling_common.py`:\n\n```python\nmodel = model_class.from_pretrained(tmpdirname, dtype=torch.float16, attn_implementation=\"sdpa\")\n                model.to(torch_device)\n\n                if model.config._experts_implementation == \"grouped_mm\" and model.dtype != torch.bfloat16:\n                    # torch._grouped_mm still only supports bfloat16 when used with torch.compile\n                    model.set_experts_implementation(\"batched_mm\")\n```"}, {"role": "assistant", "content": "fixed in https://github.com/huggingface/transformers/pull/43129/files\n```suggestion\n```"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43126, "url": "https://github.com/huggingface/transformers/pull/43126#discussion_r2665033097"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `serialized_dag.py`:\n\n```python\nlog.debug(\"Writing Serialized DAG: %s to the DB\", dag.dag_id)\n        new_serialized_dag.dag_version = dagv\n        session.add(new_serialized_dag)\n        session.flush()\n```"}, {"role": "assistant", "content": "I think I may need a hand on this one.  If we don't flush then we don't have the serdag id.  I initially had all of this tied to dag_id instead of serdag_id, and we do have dag_id at this point so we didn't need the flush, but serdag does feel like the more accurate \"connection point\".\r\n\r\nI'll have a look and see if I can shift the logic to somewhere outside of write_dag maybe"}], "_meta": {"repo": "apache/airflow", "pr_number": 58248, "url": "https://github.com/apache/airflow/pull/58248#discussion_r2566580844"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `trigger.py`:\n\n```python\n@classmethod\n    @provide_session\n    def ids_for_triggerer(cls, triggerer_id, session: Session = NEW_SESSION) -> list[int]:\n    def ids_for_triggerer(\n        cls, triggerer_id, consume_trigger_queues: set[str] | None = None, session: Session = NEW_SESSION\n    ) -> list[int]:\n        \"\"\"Retrieve a list of trigger ids.\"\"\"\n        return list(session.scalars(select(cls.id).where(cls.triggerer_id == triggerer_id)).all())\n        query = select(cls.id).where(cls.triggerer_id == triggerer_id)\n        # By default, there is no trigger queue assignment. Only filter by queue when explicitly set in the triggerer CLI.\n        if consume_trigger_queues:\n            query = query.filter(cls.trigger_queue.in_(consume_trigger_queues))\n```"}, {"role": "assistant", "content": "Good catch -- you are right this is not in agreement with the docs. I will update this code since this was a change I missed during an early refactor. More info on my other related comment here."}], "_meta": {"repo": "apache/airflow", "pr_number": 59239, "url": "https://github.com/apache/airflow/pull/59239#discussion_r2645745144"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `type_adapter.py`:\n\n```python\nfrom typing import Annotated\n\nfrom typing_extensions import assert_type\n\nfrom pydantic import TypeAdapter\n\nta1 = TypeAdapter(int)\nassert_type(ta1, TypeAdapter[int])\n\nassert_type(ta1.validate_python('1'), int)\nta1.dump_python(1)\nta1.dump_python('1')  # pyright: ignore[reportArgumentType]\nta1.dump_json(1)\nta1.dump_json('1')  # pyright: ignore[reportArgumentType]\n\n# The following use cases require PEP 747: TypeExpr:\n```"}, {"role": "assistant", "content": "Nice. Will be great to have this as a reference to update when that PEP is formally adopted \ud83d\udc4d"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10092, "url": "https://github.com/pydantic/pydantic/pull/10092#discussion_r1712486846"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `utils.py`:\n\n```python\nbatch_size, cur_len = input_ids.shape[:2]\n        this_peer_finished = False\n        unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n        model_kwargs = self._get_initial_cache_position(cur_len, input_ids.device, model_kwargs)\n\n        model_forward = self.__call__\n        compile_forward = self._valid_auto_compile_criteria(model_kwargs, generation_config)\n        if compile_forward:\n            os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n            # If we use FA2 and a static cache, we cannot compile with fullgraph\n            if self.config._attn_implementation == \"flash_attention_2\":\n                # only raise warning if the user passed an explicit compile-config\n                if generation_config.compile_config is not None and generation_config.compile_config.fullgraph:\n                    logger.warning_once(\n                        \"When using Flash Attention 2 and a static cache, you cannot use the option `CompileConfig(fullgraph=True)` as \"\n                        \"FA2 introduces graph breaks. We overrode the option with `fullgraph=False`.\"\n                    )\n                    generation_config.compile_config.fullgraph = False\n            model_forward = self.get_compiled_call(generation_config.compile_config)\n\n        if generation_config.prefill_chunk_size is not None:\n            model_kwargs = self._prefill_chunking(input_ids, generation_config, **model_kwargs)\n            is_prefill = False\n        else:\n            is_prefill = True\n        model_forward = self.maybe_get_compiled_call(model_kwargs, generation_config)\n\n        while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n            # prepare model inputs\n            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\n            # prepare variable output controls (note: some models won't accept all output controls)\n            model_inputs.update({\"output_attentions\": output_attentions} if output_attentions else {})\n            model_inputs.update({\"output_hidden_states\": output_hidden_states} if output_hidden_states else {})\n\n            if is_prefill:\n                outputs = self(**model_inputs, return_dict=True)\n                is_prefill = False\n            if prefill_outputs is not None:\n```"}, {"role": "assistant", "content": "It is not an easy task \ud83d\ude22:\r\n1. We cant move the prepare and forward calls to the bottom of the loop, since it is critical that `_has_unfinished_sequences` is checked just after updating `this_peer_finished`\r\n2. Another reason for prepare, forward and updateKwargs calls to be on top is that DeepSpeed's `synced_gpus` mode needs to run only that over and over if this peer finished but the others didnt\r\n3. With these constraints, we either need to unroll the whole first iteration of the loop or have some sort of `if` in the beginning of the loop that accounts for the prefill. I think it is less complex to have the if.\r\n\r\nSo I have kept the existing approach with a boolean variable for readability, WDYT?"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 40652, "url": "https://github.com/huggingface/transformers/pull/40652#discussion_r2348061804"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `core_model_loading.py`:\n\n```python\nclass ConversionOps:\n    \"\"\"Base class for weight conversion operations.\"\"\"\n\n    # The inverse operation class, will be used when saving the checkpoint\n    reverse_op: type[ConversionOps]\n    def __repr__(self):\n        return f\"{self.__class__.__name__}(dim={self.dim})\"\n```"}, {"role": "assistant", "content": "```suggestion\n        return f\"{self.__class__.__name__}(dim={self.dim}, {self.reverse_op}\"\n```"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42396, "url": "https://github.com/huggingface/transformers/pull/42396#discussion_r2567839197"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `canvas.py`:\n\n```python\ndef _apply_tasks(self, tasks, producer=None, app=None, p=None,\n                     add_to_parent=None, chord=None,\n                     args=None, kwargs=None, **options):\n                     args=None, kwargs=None, group_index=None, **options):\n```"}, {"role": "assistant", "content": "It won't be used, I added the parameter to prevent it from being included in the `options`"}], "_meta": {"repo": "celery/celery", "pr_number": 9910, "url": "https://github.com/celery/celery/pull/9910#discussion_r2377870192"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_fields.py`:\n\n```python\nargs = getattr(ann_type, '__args__', None)\n    if args:\n        for anno_arg in args:\n            if _typing_extra.is_annotated(anno_arg):\n            if typing_objects.is_annotated(get_origin(anno_arg)):\n```"}, {"role": "assistant", "content": "Yes, definitely good to minimize `get_origin` calls."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 11479, "url": "https://github.com/pydantic/pydantic/pull/11479#discussion_r1973831192"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `function.py`:\n\n```python\n}\n\n\ndef f(a, **kwargs) -> A:\ndef f(a, **kwargs,) -> A:\n```"}, {"role": "assistant", "content": "I don't see it as avoidable: we're now preserving trailing commas, and it's pretty hard to remove them only in function signatures as far as I can tell. I'm very open to suggestions, but as you say I don't think human devs would write a function like this so it's moot."}], "_meta": {"repo": "psf/black", "pr_number": 826, "url": "https://github.com/psf/black/pull/826#discussion_r292959211"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_update.py`:\n\n```python\nfrom sqlmodel import Field, SQLModel, create_engine\n\n\ndef test_sqlmodel_update():\n    class Organization(SQLModel, table=True):\n        id: int = Field(default=None, primary_key=True)\n        name: str\n        headquarters: str\n\n    class OrganizationUpdate(SQLModel):\n        name: str\n\n    engine = create_engine(\"sqlite:///\", echo=True)\n    SQLModel.metadata.create_all(engine)\n```"}, {"role": "assistant", "content": "```suggestion\r\n```\r\n\r\nWe don't need this for this test"}], "_meta": {"repo": "tiangolo/sqlmodel", "pr_number": 997, "url": "https://github.com/fastapi/sqlmodel/pull/997#discussion_r2292050955"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `__init__.py`:\n\n```python\n.. versionadded:: 2.0\n           The *flags* parameter.\n        \"\"\"\n        if encoding is None:\n```"}, {"role": "assistant", "content": "Allowing None in the signature because `TextResponse` needs it allowed in its method implementations and those call the base class methods (`TextResponse` methods don't call `Response.follow()` but I'm not sure if it can be detected statically)."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6127, "url": "https://github.com/scrapy/scrapy/pull/6127#discussion_r1378205864"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_clip.py`:\n\n```python\nTransformersKwargs,\n    auto_docstring,\n    can_return_tuple,\n    filter_out_non_signature_kwargs,\n```"}, {"role": "assistant", "content": "Nowadays almost exclusively in `image_processing_....py` files. I reckon we'll eventually be able to get rid of it if we wanted. Not now yet, though."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42564, "url": "https://github.com/huggingface/transformers/pull/42564#discussion_r2703794518"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `python2.py`:\n\n```python\nprint >> sys.stderr , \"Warning:\" ,\nprint >> sys.stderr , \"this is a blast from the past.\"\nprint >> sys.stderr , \"Look, a repr:\", `sys`\nprint >> sys.stderr , U\"Even these prefixes are normalized!\"\n\n\ndef function((_globals, _locals)):\n    exec ur\"print 'hi from exec!'\" in _globals, _locals\n    exec \"print 'hi from exec!'\" in _globals, _locals\n```"}, {"role": "assistant", "content": "Oh wow. \"ru\" is invalid but \"ur\" isn't? \ud83d\ude05 Good catch. Let me go through the others once more."}], "_meta": {"repo": "psf/black", "pr_number": 2297, "url": "https://github.com/psf/black/pull/2297#discussion_r643252907"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `processing_video_llava.py`:\n\n```python\nclass VideoLlavaProcessor(ProcessorMixin):\n    r\"\"\"\n    Constructs a VideoLlava processor which wraps a VideoLlava image processor and a Llava tokenizer into a single processor.\n    Constructs a VideoLlava processor which wraps a AutoImageProcessor and a Llava tokenizer into a single processor.\n```"}, {"role": "assistant", "content": "Yes you're right, not very useful to have AutoImageProcessor as in the docstring. I'll change these back. I'm also working on getting auto_docstring to work on processors which should do all that automatically (check which subprocessors are in auto for this model) ;)"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41388, "url": "https://github.com/huggingface/transformers/pull/41388#discussion_r2411222218"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `config.py`:\n\n```python\ndefer_build: bool\n    \"\"\"\n    Whether to defer model validator and serializer construction until the first model validation.\n    Whether to defer model validator and serializer construction until the first model validation. Defaults to False.\n\n    This can be useful to avoid the overhead of building models which are only\n    used nested within other models, or when you want to manually define type namespace via\n    [`Model.model_rebuild(_types_namespace=...)`][pydantic.BaseModel.model_rebuild]. Defaults to False.\n    [`Model.model_rebuild(_types_namespace=...)`][pydantic.BaseModel.model_rebuild].\n\n    See also [`defer_build_mode`][pydantic.config.ConfigDict.defer_build_mode].\n\n    !!! note\n        `defer_build` does not work by default with FastAPI Pydantic models. Meaning the validator and serializer\n        is constructed immediately when the model is used in FastAPI routes instead of construction being deferred.\n        You also need to use [`defer_build_mode='always'`][pydantic.config.ConfigDict.defer_build_mode] with\n        FastAPI models.\n```"}, {"role": "assistant", "content": "I think the mention of FastAPI is helpful here, but we should probably explain that this is because of FastAPI's use of `TypeAdapter`s"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 8939, "url": "https://github.com/pydantic/pydantic/pull/8939#discussion_r1530461295"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `requests.py`:\n\n```python\nerror=self.error,\n            timestamps=self.timestamps,\n        )\n\n    def fork(self, new_request_id: str) -> \"RequestState\":\n        \"\"\"Fork the request into a new request with the same state expect for request_id, created_time and lifespan.\"\"\"\n        return RequestState(\n```"}, {"role": "assistant", "content": "I will macro benchmark the two and it the deepcopy is on par / better we will move to that, agreed it's better looking"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42921, "url": "https://github.com/huggingface/transformers/pull/42921#discussion_r2632022179"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `functional_validators.py`:\n\n```python\nA metadata class that indicates that a validation should be applied **instead** of the inner validation logic.\n\n    !!! note:\n        Before v2.9, `PlainValidator` wasn't always compatible with JSON Schema generation for `mode='validation'`.\n        You can now use the `json_schema_input_type` argument to specify the input type of the function\n        to be used in the JSON schema when `mode='validation'` (the default). See the example below for more details.\n\n        If you'd like the JSON Schema type for an annotated field to be populated automatically,\n        you might consider using `WrapValidator` instead.\n```"}, {"role": "assistant", "content": "Maybe we shouldn't recommend a specific validator type. before and wrap validators both populate the JSON Schema from the annotated type, but the usage of one of the user will depend on the user"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10597, "url": "https://github.com/pydantic/pydantic/pull/10597#discussion_r1795911493"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `type_adapter.py`:\n\n```python\ncode='type-adapter-config-unused',\n            )\n\n        config_wrapper = _config.ConfigWrapper(config)\n\n        core_schema: CoreSchema\n        self._type = type\n        self._config = config\n        self._parent_depth = _parent_depth\n        if module is None:\n            f = sys._getframe(1)\n            self._module_name = cast(str, f.f_globals.get('__name__', ''))\n        else:\n            self._module_name = module\n\n        self._core_schema: CoreSchema | None = None\n        self._validator: SchemaValidator | None = None\n        self._serializer: SchemaSerializer | None = None\n\n        if not self._defer_build():\n            # Immediately initialize the core schema, validator and serializer\n            with self._with_frame_depth(1):  # +1 frame depth for this __init__\n                # Model itself may be using deferred building. For backward compatibility we don't rebuild model mocks\n                # here as part of __init__ even though TypeAdapter itself is not using deferred building.\n                self._init_core_attrs(rebuild_mocks=False)\n```"}, {"role": "assistant", "content": "I believe this comment can be resolved, given that we dropped the `_defer_build_mode` setting :)"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 8939, "url": "https://github.com/pydantic/pydantic/pull/8939#discussion_r1781612198"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_florence2.py`:\n\n```python\n# Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(self, hidden_states: torch.Tensor, **kwargs):\n    @check_model_inputs(tie_last_hidden_states=False)\n```"}, {"role": "assistant", "content": "yeah, the above would work. We can customize `_can_record_outputs` by setting explicitly the index and layer name as in Mllama. Also we can use any naming for `key` which will be simply copied in the output dict\r\n\r\nI agree it's not straightforward in florence, and returning spatial/temporal attention is not super necessary"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42564, "url": "https://github.com/huggingface/transformers/pull/42564#discussion_r2661077780"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_time.py`:\n\n```python\nassert localized_utc_offset == local_zone.utcoffset(local_time)\n\n\n@pytest.mark.parametrize('s,expected', [\n@ pytest.mark.parametrize('s,expected', [\n```"}, {"role": "assistant", "content": "There are a few spaces that sneaked into this file :)\r\nCheck below as well"}], "_meta": {"repo": "celery/celery", "pr_number": 8159, "url": "https://github.com/celery/celery/pull/8159#discussion_r1149720188"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `comms.py`:\n\n```python\nerr_decoder: TypeAdapter[ErrorResponse] = attrs.field(factory=lambda: TypeAdapter(ToTask), repr=False)\n\n    # Threading lock for sync operations\n    _thread_lock: threading.Lock = attrs.field(factory=threading.Lock, repr=False)\n    # Async lock for async operations\n    _async_lock: asyncio.Lock = attrs.field(factory=asyncio.Lock, repr=False)\n\n    def send(self, msg: SendMsgType) -> ReceiveMsgType | None:\n        \"\"\"Send a request to the parent and block until the response is received.\"\"\"\n        frame = _RequestFrame(id=next(self.id_counter), body=msg.model_dump())\n        frame_bytes = frame.as_bytes()\n\n        self.socket.sendall(frame_bytes)\n        if isinstance(msg, ResendLoggingFD):\n            if recv_fds is None:\n                return None\n            # We need special handling here! The server can't send us the fd number, as the number on the\n            # supervisor will be different to in this process, so we have to mutate the message ourselves here.\n            frame, fds = self._read_frame(maxfds=1)\n            resp = self._from_frame(frame)\n            if TYPE_CHECKING:\n                assert isinstance(resp, SentFDs)\n            resp.fds = fds\n            # Since we know this is an expliclt SendFDs, and since this class is generic SendFDs might not\n            # always be in the return type union\n            return resp  # type: ignore[return-value]\n        # We must make sure sockets aren't intermixed between sync and async calls,\n        # thus we need a dual locking mechanism to ensure that.\n        with self._thread_lock:\n            self.socket.sendall(frame_bytes)\n            if isinstance(msg, ResendLoggingFD):\n                if recv_fds is None:\n                    return None\n                # We need special handling here! The server can't send us the fd number, as the number on the\n                # supervisor will be different to in this process, so we have to mutate the message ourselves here.\n                frame, fds = self._read_frame(maxfds=1)\n                resp = self._from_frame(frame)\n                if TYPE_CHECKING:\n                    assert isinstance(resp, SentFDs)\n                resp.fds = fds\n                # Since we know this is an expliclt SendFDs, and since this class is generic SendFDs might not\n                # always be in the return type union\n                return resp  # type: ignore[return-value]\n\n        return self._get_response()\n```"}, {"role": "assistant", "content": "Normally it should behave correctly, unless I missed something, as the async version checks both async and sync lock, the sync version only checks the sync lock.  We are now running the async PythonOperator since right after the Summit and until today haven't experienced any issues with it, I had issues in the beginning and this is how I came up with this solution as indeed it could lead to mix-ups very quickly without those locks."}], "_meta": {"repo": "apache/airflow", "pr_number": 59087, "url": "https://github.com/apache/airflow/pull/59087#discussion_r2672685995"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_models.py`:\n\n```python\nclass URL:\n    \"\"\"\n    url = httpx.URL(\"HTTPS://jo%40email.com:a%20secret@example.com:1234/pa%20th?search=ab#anchorlink\")\n    url = httpx.URL(\"HTTPS://jo%40email.com:a%20secret@m\u00fcller.de:1234/pa%20th?search=ab#anchorlink\")\n\n    assert url.scheme == \"https\"\n    assert url.username == \"jo@email.com\"\n    assert url.password == \"a secret\"\n    assert url.userinfo == b\"jo%40email.com:a%20secret\"\n    assert url.host == \"example.com\"\n    assert url.host == \"m\u00fcller.de\"\n    assert url.raw_host == b\"xn--mller-kva.dem\"\n```"}, {"role": "assistant", "content": "I think this is a typo, based on the test below\r\n```suggestion\r\n    assert url.raw_host == b\"xn--mller-kva.de\"\r\n```"}], "_meta": {"repo": "encode/httpx", "pr_number": 1590, "url": "https://github.com/encode/httpx/pull/1590#discussion_r618620007"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `adapters.py`:\n\n```python\nparsed_request_url = urlparse(request.url)\n    scheme = parsed_request_url.scheme.lower()\n    port = parsed_request_url.port\n\n    # Determine if we have and should use our default SSLContext\n    # to optimize performance on standard requests.\n    poolmanager_kwargs = getattr(poolmanager, \"connection_pool_kw\", {})\n    has_poolmanager_ssl_context = poolmanager_kwargs.get(\"ssl_context\")\n    should_use_default_ssl_context = (\n        _preloaded_ssl_context is not None and not has_poolmanager_ssl_context\n    )\n\n    cert_reqs = \"CERT_REQUIRED\"\n    if verify is False:\n        cert_reqs = \"CERT_NONE\"\n    elif verify is True and not has_poolmanager_ssl_context:\n    elif verify is True and should_use_default_ssl_context:\n        pool_kwargs[\"ssl_context\"] = _preloaded_ssl_context\n    elif isinstance(verify, str):\n        if not os.path.isdir(verify):\n```"}, {"role": "assistant", "content": "Looking at this again with something of a performance mindset, do we want to cache somehow lookups to `isdir`? I'm not sure this hurts us at all, but just thinking about things that could slow us down in certain cases now. (Not for this pull request, just putting it out there)"}], "_meta": {"repo": "psf/requests", "pr_number": 6724, "url": "https://github.com/psf/requests/pull/6724#discussion_r1619003075"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `configuration_llama4.py`:\n\n```python\nThe id of the end of sentence token.\n        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n            Whether to tie weight embeddings\n        attention_dropout (`int`, *optional*, defaults to 0.0): TODO\n        num_experts_per_tok (`int`, *optional*, defaults to 1): TODO\n        num_local_experts (`int`, *optional*, defaults to 16): TODO\n        moe_layers (`int`, *optional*): TODO\n        interleave_moe_layer_step (`int`, *optional*, defaults to 1): TODO\n        use_qk_norm (`int`, *optional*, defaults to `True`): TODO\n        output_router_logits (`int`, *optional*, defaults to `False`): TODO\n        router_aux_loss_coef (`int`, *optional*, defaults to 0.001): TODO\n        router_jitter_noise (`int`, *optional*, defaults to 0.0): TODO\n        attention_dropout (`int`, *optional*, defaults to 0.0):\n            The dropout ratio for the attention probabilities.\n        num_experts_per_tok (`int`, *optional*, defaults to 1):\n            Number of experts to route each token to. This is the top-k value for the token-choice routing.\n        num_local_experts (`int`, *optional*, defaults to 16):\n            Number of experts for each Softmax router.\n        moe_layers (`int`, *optional*):\n```"}, {"role": "assistant", "content": "The parameter type should be `list` or `list[int]` instead of `int`. Looking at the function signature (line 286), `moe_layers` is defined as a list parameter that can be None. According to the description, it contains indices of layers that are MoE layers.\n```suggestion\n        moe_layers (`list[int]`, *optional*):\n```"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43460, "url": "https://github.com/huggingface/transformers/pull/43460#discussion_r2723836591"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_audioflamingo3.py`:\n\n```python\ndef set_input_embeddings(self, value: nn.Module):\n        self.conv1 = value\n\n    @can_return_tuple\n    @check_model_inputs(tie_last_hidden_states=False)\n```"}, {"role": "assistant", "content": "personally I don't like the untied last hidden states, which we had to support because of some vision models. If the model never returned `hidden_states`, imo we can return them tied, wdyt  ?"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42564, "url": "https://github.com/huggingface/transformers/pull/42564#discussion_r2693483692"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_modeling_common.py`:\n\n```python\nself.skipTest(f\"bfloat16 not supported on {torch_device} (on the specific device currently used)\")\n\n        dtype = torch.bfloat16\n\n        def _expected_attn_impls(attn_impl: str) -> set[str]:\n            # Allow kernels fallbacks for flash attention tests.\n            requested = attn_impl\n```"}, {"role": "assistant", "content": "Let's be explicit and go for `attention_implementation`"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42803, "url": "https://github.com/huggingface/transformers/pull/42803#discussion_r2610802953"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `genspider.py`:\n\n```python\nif filename.endswith('.tmpl'):\n                print(\"  %s\" % splitext(filename)[0])\n\n    def _spider_exists(self, name):\n        if not self.settings.get('NEWSPIDER_MODULE'):\n            # if run as a standalone command and file with same filename already exists\n            if exists(name + \".py\"):\n                print(\"%s already exists\" % (abspath(name + \".py\")))\n                return True\n        else:\n            try:\n                spidercls = self.crawler_process.spider_loader.load(name)\n            except KeyError:\n                pass\n            else:\n                # if spider with same name exists\n                print(\"Spider %r already exists in module:\" % name)\n                print(\"  %s\" % spidercls.__module__)\n                return False\n```"}, {"role": "assistant", "content": "I think we could check the modification time of the file before and after the command is run to ensure the file wasn't rewritten on top of asserting the output to make the test stronger. What do you think? The `os.path` module in python has a method `os.path.getmtime` to get the modification time, so shouldn't be too overbearing."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4623, "url": "https://github.com/scrapy/scrapy/pull/4623#discussion_r442200022"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `exceptions.py`:\n\n```python\nclass JSONDecodeError(InvalidJSONError, CompatJSONDecodeError):\n    \"\"\"Couldn't decode the text into json\"\"\"\n    def __init__(self, *args, **kwargs):\n        CompatJSONDecodeError.__init__(self, *args)\n        InvalidJSONError.__init__(self, *self.args, **kwargs)\n```"}, {"role": "assistant", "content": "Can we add comments here explaining why:\r\n\r\n1. We're using super old Python 2.5 methods of calling parent `__init__` methods\r\n2. We're calling the `__init__`s in this particular order - presuming the order matters\r\n3. This fixes the issue of `.doc` ending up in the message/default `__repr__`"}], "_meta": {"repo": "psf/requests", "pr_number": 6036, "url": "https://github.com/psf/requests/pull/6036#discussion_r780674790"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `linegen.py`:\n\n```python\nif is_lpar_token(first) and is_rpar_token(last):\n        middle = node.children[1]\n        # make parentheses invisible\n        first.value = \"\"\n        last.value = \"\"\n        if (\n            not node.children[1]\n            .prefix.strip()\n            .replace(\" \", \"\")\n            .startswith(\"#type:ignore\")\n```"}, {"role": "assistant", "content": "I have updated the code. I also removed the optional string parameter `suffix` in `is_type_coment()`, This parameter was only used to pass in `\"ignore\"`, and since the handling of ignore comments is now done by the new function `is_type_ignore_comment()`, there is no need to keep this parameter. Also, considering that `is_type_comment()` is responsible for handling only general types comments and there are plans to deprecate its support in the future, I added a note to its docstring stating that it might be deprecated in the future."}], "_meta": {"repo": "psf/black", "pr_number": 3740, "url": "https://github.com/psf/black/pull/3740#discussion_r1240906410"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `defaults.py`:\n\n```python\npool_restarts=Option(False, type='bool'),\n        proc_alive_timeout=Option(4.0, type='float'),\n        prefetch_multiplier=Option(4, type='int'),\n        enable_prefetch_count=Option(True, type='bool'),\n```"}, {"role": "assistant", "content": "`enable_prefetch_count_reduction_on_connection_restart` was too long. Went for:\r\n`enable_prefetch_count_reduction`"}], "_meta": {"repo": "celery/celery", "pr_number": 8581, "url": "https://github.com/celery/celery/pull/8581#discussion_r1367961236"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `encoders.py`:\n\n```python\ncustom_encoder=encoders,\n            sqlalchemy_safe=sqlalchemy_safe,\n        )\n    if dataclasses.is_dataclass(obj):\n    if dataclasses.is_dataclass(type(obj)):\n```"}, {"role": "assistant", "content": "I have separated this change into a second commit so that you can see the failure in CI. Please have a look."}], "_meta": {"repo": "fastapi/fastapi", "pr_number": 12970, "url": "https://github.com/fastapi/fastapi/pull/12970#discussion_r1864265876"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_ast.py`:\n\n```python\nself.assertIs(node.ctx, context)\n        self.assertRaises(AttributeError, getattr, node, 'unknown')\n\n    def test_replace_non_str_kwarg(self):\n        node = ast.Name(id=\"x\")\n        with self.assertRaisesRegex(TypeError, \"got an unexpected keyword argument <object object\"):\n```"}, {"role": "assistant", "content": "This line is longer than 80 columns, I suggest to write it like that:\n\n```suggestion\n        errmsg = \"got an unexpected keyword argument <object object\"\n        with self.assertRaisesRegex(TypeError, errmsg):\n```"}], "_meta": {"repo": "python/cpython", "pr_number": 144178, "url": "https://github.com/python/cpython/pull/144178#discussion_r2721584768"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `brackets.py`:\n\n```python\nif leaf.type == token.COMMENT:\n            return\n\n        if (\n            self.depth == 0\n            and leaf.type in CLOSING_BRACKETS\n            and (self.depth, leaf.type) not in self.bracket_match\n        ):\n            return\n\n        self.maybe_decrement_after_for_loop_variable(leaf)\n        self.maybe_decrement_after_lambda_arguments(leaf)\n        if leaf.type in CLOSING_BRACKETS:\n            self.depth -= 1\n            try:\n                opening_bracket = self.bracket_match.pop((self.depth, leaf.type))\n            except KeyError as e:\n                raise BracketMatchError(\n                    \"Unable to match a closing bracket to the following opening\"\n                    f\" bracket: {leaf}\"\n                ) from e\n            opening_bracket = self.bracket_match.pop((self.depth, leaf.type))\n```"}, {"role": "assistant", "content": "Why not keep the BracketMatchError? Seems like it makes for a better error message."}], "_meta": {"repo": "psf/black", "pr_number": 3462, "url": "https://github.com/psf/black/pull/3462#discussion_r1052942197"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `configuration_llama4.py`:\n\n```python\nIf `no_rope_layers` is `None`, it will be created using a NoPE layer every\n            `no_rope_layer_interval` layers.\n        attention_chunk_size (`int`, *optional*, defaults to 8192):\n            <TODO>\n            Chunk size for the attention computation.\n        layer_types (`list`, *optional*):\n            Attention pattern for each layer.\n        attn_temperature_tuning (`bool`, *optional*, defaults to `True`):\n            Whether to dynamically scale the attention temperature for each query token based on sequence length.\n            Recommended for long sequences (e.g., >32k tokens) to maintain stable output results.\n        floor_scale (`int`, *optional*, defaults to 8192): TODO\n        attn_scale (`int`, *optional*, defaults to 0.1): TODO\n        floor_scale (`int`, *optional*, defaults to 8192):\n            Scaling factor for the floor operation in the attention mechanism.\n        attn_scale (`int`, *optional*, defaults to 0.1):\n```"}, {"role": "assistant", "content": "The parameter type should be `float` instead of `int`. Looking at the function signature (line 299), `attn_scale` is defined as `float` with a default value of `0.1`, not `int`. Scaling factors for attention scores are typically floating-point values.\n```suggestion\n        attn_scale (`float`, *optional*, defaults to 0.1):\n```"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 43460, "url": "https://github.com/huggingface/transformers/pull/43460#discussion_r2723836598"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `__init__.py`:\n\n```python\nself.start_urls: List[str] = []\n\n    @property\n    def logger(self) -> logging.LoggerAdapter:\n    def logger(self) -> SpiderLoggerAdapter:\n        logger = logging.getLogger(self.name)\n        return logging.LoggerAdapter(logger, {\"spider\": self})\n        return SpiderLoggerAdapter(logger, {\"spider\": self})\n```"}, {"role": "assistant", "content": "```suggestion\r\n        return SpiderLoggerAdapter(logger, {\"spider\": self.name})\r\n```\r\nMaybe also change `self` to `self.name`? So we get actual spider name in logs instead of `<Spider 'spider_name' at 0x1b2fc089580>`.\r\n\r\nProbably not the best idea since it's gonna change current behavior"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6324, "url": "https://github.com/scrapy/scrapy/pull/6324#discussion_r1592787002"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `cache_manager.py`:\n\n```python\n# limitations under the License.\nfrom abc import ABC, abstractmethod\nfrom collections import deque\nfrom collections.abc import Iterator\nfrom math import ceil\nfrom typing import Optional\nfrom typing import Optional, TypeVar\n\nfrom .requests import logger\n\n\nT = TypeVar('T')\ndef reverse_enumerate(xs: list[T]) -> Iterator[tuple[int, T]]:\n    index = len(xs) - 1\n    for x in xs[::-1]:\n        yield index, x\n        index -= 1\n```"}, {"role": "assistant", "content": "```suggestion\n    for idx, x in enumerate(xs[::-1]):\n        yield len(xs) - idx - 1, x\n```\n\ud83e\udd0f\ud83c\udffb"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42094, "url": "https://github.com/huggingface/transformers/pull/42094#discussion_r2518298339"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_asgi.py`:\n\n```python\nawait send({\"type\": \"http.response.body\", \"body\": output})\n\n\nasync def echo_raw_path(scope, receive, send):\n    status = 200\n    output = json.dumps({\"raw_path\": repr(scope[\"raw_path\"])}).encode(\"utf-8\")\n```"}, {"role": "assistant", "content": "I'm using `repr(scope[\"raw_path\"])` here as an easy way to confirm that the value is indeed a Python bytestring - it ends up being returned as `\"b'/user%40example.org'\"` in the JSON."}], "_meta": {"repo": "encode/httpx", "pr_number": 1357, "url": "https://github.com/encode/httpx/pull/1357#discussion_r502169081"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modular_granitemoehybrid.py`:\n\n```python\n[GraniteMoeHybridDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n        )\n        self.embedding_multiplier = config.embedding_multiplier\n        if config.position_embedding_type != \"rope\":\n            self.rotary_emb = None\n```"}, {"role": "assistant", "content": "The inheritance continues to `MixtralModel` - which initializes RoPE. \r\n```\r\nclass GraniteMoeHybridModel(GraniteMoeSharedModel):\r\nclass GraniteMoeSharedModel(GraniteMoeModel):\r\nclass GraniteMoeModel(MixtralModel):\r\n```\r\nI also didn't like my solution (see here) but it was local, which is probably safer. What do you think?"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42872, "url": "https://github.com/huggingface/transformers/pull/42872#discussion_r2619787453"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `optimization.py`:\n\n```python\nif name == SchedulerType.INVERSE_SQRT:\n        return schedule_func(optimizer, num_warmup_steps=num_warmup_steps)\n\n    if name == SchedulerType.WARMUP_STABLE_DECAY:\n        return schedule_func(optimizer, num_warmup_steps=num_warmup_steps, **scheduler_specific_kwargs)\n\n    # All other schedulers require `num_training_steps`\n    if num_training_steps is None:\n        raise ValueError(f\"{name} requires `num_training_steps`, please provide that argument.\")\n```"}, {"role": "assistant", "content": "because of this, we will always trigger an error asking the user to pass num_training_steps. This is a breaking change,"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 35453, "url": "https://github.com/huggingface/transformers/pull/35453#discussion_r1905803091"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `display.py`:\n\n```python\n\"\"\"\n\nimport sys\nfrom platform import version\nfrom packaging.version import parse\nfrom pprint import pformat as pformat_\n\n\ndef _colorize(text, colorize=True):\n    if not colorize or not sys.stdout.isatty():\n        return text\n    try:\n        import pygments.util\n        from pygments.formatters import get_formatter_by_name\n        from pygments import highlight\n        from pygments.formatters import TerminalFormatter\n        from pygments.lexers import PythonLexer\n        return highlight(text, PythonLexer(), TerminalFormatter())\n    except ImportError:\n        return text\n\n    \"\"\"\n    All Windows versions >= \"10.0.14393\" interpret ANSI escape sequences\n    using terminal processing.\n\n    Enable enivornment variable `ENABLE_VIRTUAL_TERMINAL_PROCESSING`\n    to activate terminal processing.\n    \"\"\"\n```"}, {"role": "assistant", "content": ":lipstick: Docstrings are not meant to be used in the middle of a function implementation.\r\n\r\n```suggestion\r\n    # All Windows versions >= \"10.0.14393\" interpret ANSI escape sequences\r\n    # using terminal processing.\r\n    #\r\n    # Enable enivornment variable `ENABLE_VIRTUAL_TERMINAL_PROCESSING`\r\n    # to activate terminal processing.\r\n```"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4403, "url": "https://github.com/scrapy/scrapy/pull/4403#discussion_r399684091"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `__init__.py`:\n\n```python\nelse:\n            return get_settings_priority('default')\n\n    def getinstance(self, name, *, crawler=None, singleton=False):\n```"}, {"role": "assistant", "content": "Aha, I see the problem now: you can't just create RequestFingerprinter in a dupefilter, because other components also may need to access it.\r\n\r\nI'm not sure though that settings is a best place to handle this singleton. \r\n\r\n1) All other settings return simple values at the moment - str, list, dict.\r\n2) You need to pass crawler here, but only once - on a first call. On all other calls crawler is not necessary. But we need to ensure that the first call was a call which passed a crawler.\r\n\r\nI haven't checked it in detail, but what do you think about making fingerprinter a more \"fundamental\" thing, maybe a Crawler attribute, similar to logformatter or stats? In this case logic around singletons won't be needed."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4524, "url": "https://github.com/scrapy/scrapy/pull/4524#discussion_r416927742"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `datatypes.py`:\n\n```python\ndef __contains__(self, item):\n        return item not in self.seq\n\n\ndef get_item_field(item, field_name, default=None):\n    \"\"\"\n    Access item fields on both \"regular\" and dataclass-based items\n    \"\"\"\n    if is_dataclass_instance(item):\n        return getattr(item, field_name, default)\n    else:\n        return item.get(field_name, default)\n\n\ndef set_item_field(item, field_name, value):\n    \"\"\"\n    Set item fields on both \"regular\" and dataclass-based items\n    \"\"\"\n    if is_dataclass_instance(item):\n        if not hasattr(item, \"_field_names\"):\n            from dataclasses import fields\n            item._field_names = [f.name for f in fields(item)]\n```"}, {"role": "assistant", "content": "Is it a performance optimization, to cache available field names? I'm not sure that's good to add attributes to user's objects. It can be fine to just use fields."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 3881, "url": "https://github.com/scrapy/scrapy/pull/3881#discussion_r370307961"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_compat.py`:\n\n```python\nexcept ImportError:\n        brotli = None\n\n# Zstandard support is optional\ntry:\n    import zstandard as zstd\nexcept (AttributeError, ImportError, ValueError):  # Defensive:\n    zstd = False\nelse:\n    # The package 'zstandard' added the 'eof' property starting\n    # in v0.18.0 which we require to ensure a complete and\n    # valid zstd stream was fed into the ZstdDecoder.\n    # See: https://github.com/urllib3/urllib3/pull/2624\n    _zstd_version = tuple(\n        map(int, re.search(r\"^([0-9]+)\\.([0-9]+)\", zstd.__version__).groups())  # type: ignore[union-attr]\n    )\n    if _zstd_version < (0, 18):  # Defensive:\n        zstd = False\n```"}, {"role": "assistant", "content": "This is just out of my curiosity, feel free to not answer it.\r\nWhy do we need this code if we already specify zstandard>=0.18.0, wouldn't package manager guarantee it already?"}], "_meta": {"repo": "encode/httpx", "pr_number": 3139, "url": "https://github.com/encode/httpx/pull/3139#discussion_r1518813076"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `default_settings.py`:\n\n```python\nCONCURRENT_REQUESTS = 16\nCONCURRENT_REQUESTS_PER_DOMAIN = 8\nCONCURRENT_REQUESTS_PER_IP = 0\n```"}, {"role": "assistant", "content": "I think we should keep it here during the deprecation period.\r\n\r\nWe could use a `__getattr__` module-level function to log a deprecation warning upon import. I wonder if that means we also need to define `__all__` with the whole list of settings defined here, but to be honest, it may be better to break any `from scrapy.settings.default_settings import *`, seems like a bad practice and I cannot picture a valid reason to use that."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6921, "url": "https://github.com/scrapy/scrapy/pull/6921#discussion_r2173142926"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `functional_validators.py`:\n\n```python\n)\n\n        __hash__ = object.__hash__\n\n\n_FromTypeT = TypeVar('_FromTypeT')\n\n\nclass ValidateFrom:\n    \"\"\"A helper class to validate a custom type from a type natively supported by Pydantic.\n\n    Args:\n        from_type: The type natively supported by Pydantic to use to perform validation.\n        instantiation_hook: A callable taking the validated type as an argument, and returning\n            the populated custom type.\n\n    Example:\n        ```python {lint=\"skip\"}\n        from typing import Annotated\n\n        from pydantic import BaseModel, TypeAdapter, ValidateFrom\n\n        class MyCls:\n            def __init__(self, a: int) -> None:\n                self.a = a\n\n            def __repr__(self) -> str:\n                return f\"MyCls(a={self.a})\"\n\n        class Model(BaseModel):\n            a: int\n\n\n        ta = TypeAdapter(\n            Annotated[MyCls, ValidateFrom(Model, instantiation_hook=lambda v: MyCls(a=v.a))]\n        )\n\n        print(ta.validate_python({'a': 1}))\n        #> MyCls(a=1)\n        ```\n    \"\"\"\n\n    # TODO: make use of PEP 747\n    def __init__(self, from_type: type[_FromTypeT], /, *, instantiation_hook: Callable[[_FromTypeT], Any]) -> None:\n```"}, {"role": "assistant", "content": "I think `Annotated[MyType, ValidateAs(SomeModel, lambda v: ...)]` implies pretty well that the input will be validated as a `SomeModel`, and then you're supposed to turn a `SomeModel` into a `MyType`. `ValidateAs` implies that a bit more clearly to me than `ValidateFrom`"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 11942, "url": "https://github.com/pydantic/pydantic/pull/11942#discussion_r2279282215"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `misc.py`:\n\n```python\nreturn instance\n\n\ndef build_from_crawler(objcls, crawler, *args, **kwargs):\n```"}, {"role": "assistant", "content": "```suggestion\r\ndef build_from_crawler(objcls, crawler, /, *args, **kwargs):\r\n```"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6169, "url": "https://github.com/scrapy/scrapy/pull/6169#discussion_r1421729460"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_utils.py`:\n\n```python\nelif device_type == \"cpu\":\n                        cpu_backend = \"ccl\" if int(os.environ.get(\"CCL_WORKER_COUNT\", 0)) else \"gloo\"\n                        torch.distributed.init_process_group(cpu_backend, rank=rank, world_size=world_size)\n                    elif device_type == \"xpu\":\n                        torch.distributed.init_process_group(\"ccl\", rank=rank, world_size=world_size)\n                        torch.xpu.set_device(int(os.environ[\"LOCAL_RANK\"]))\n```"}, {"role": "assistant", "content": "Yes. This block doesn't seem to be `tp_plan` specific: it's about `torch distributed` initialization, which is necessary for TP, but also for CP (context parallel that we will support soon) etc. \r\n\r\nHaving this block in a separate method make `from_pretrained` clear to understand, but also allow different parallelism (TP, CP, and anything in the future) could re-use the same code.\r\n\r\nBut it's OK to do that in a separate PR and by us internally."}], "_meta": {"repo": "huggingface/transformers", "pr_number": 37382, "url": "https://github.com/huggingface/transformers/pull/37382#discussion_r2036588923"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `common.py`:\n\n```python\nimport pathlib\n\nTESTS_DIR = pathlib.Path(__file__).parent\nTESTS_DIR = TESTS_DIR.relative_to(TESTS_DIR.parent)  # Ensure relative to project root.\n```"}, {"role": "assistant", "content": "Is there any benefit of using a relative path? I think all the usages admit a full one."}], "_meta": {"repo": "encode/httpx", "pr_number": 1052, "url": "https://github.com/encode/httpx/pull/1052#discussion_r449892897"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_models.py`:\n\n```python\nitems = parse_qsl(value)\n        elif isinstance(value, QueryParams):\n            items = value.multi_items()\n        elif isinstance(value, list):\n        elif type(value) in [list, tuple]:\n```"}, {"role": "assistant", "content": "In theory any `Sequence` of 2-tuples would do. We _could_ consider switching to `isinstance(value, typing.Sequence)`. But hmm, I don't think there's much of a case for anything else than lists and tuples, so this sounds good enough."}], "_meta": {"repo": "encode/httpx", "pr_number": 1426, "url": "https://github.com/encode/httpx/pull/1426#discussion_r541150439"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `json_schema.py`:\n\n```python\nelse:\n                json_schema['patternProperties'] = {keys_pattern: values_schema}\n\n        if ks is not None:\n            keys_type = ks.get('type', None)\n            if keys_type == 'enum':\n                keys_members = ks.get('members', [])\n                json_schema['propertyNames'] = {'enum': keys_members}\n```"}, {"role": "assistant", "content": "Thanks for the review and the suggestion :) Just for better understanding from my side:\r\nkeys_schema could be a ref only, which means that I would need to resolve that before I can do any checks on that, right?\r\nWould it then be best to add the ref to propertyNames schema, or the resolved one? I'll go with the resolved for now but am open for your inputs :)"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10478, "url": "https://github.com/pydantic/pydantic/pull/10478#discussion_r1775735466"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_model_construction.py`:\n\n```python\nand `raise_errors=True`.\n    \"\"\"\n    typevars_map = get_model_typevars_map(cls)\n\n    if not cls.__pydantic_fields_complete__:\n```"}, {"role": "assistant", "content": "> We now do the same on `ModelMetaclass.__new__`, to make sure that subclasses of incomplete superclasses don't incorrectly get marked as complete.\r\n\r\nHum, did you have any example where this happened? When we create the model fields for a subclass, we copy the parent `FieldInfo` instances, keeping the `_complete` property."}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 11759, "url": "https://github.com/pydantic/pydantic/pull/11759#discussion_r2047469054"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_generate_schema.py`:\n\n```python\nif maybe_schema is not None:\n                return maybe_schema\n\n            schema = cls.__dict__.get('__pydantic_core_schema__')\n            if (\n                schema is not None\n                and not isinstance(schema, MockCoreSchema)\n                # Due to the way generic classes are built, it's possible that an invalid schema may be temporarily\n                # set on generic classes. Probably we could resolve this to ensure that we get proper schema caching\n                # for generics, but for simplicity for now, we just always rebuild if the class has a generic origin:\n                and not cls.__pydantic_generic_metadata__['origin']\n            ):\n```"}, {"role": "assistant", "content": "did a good job at intuitively extracting some of this logic as follows:\r\n\r\n```py\r\ndef get_existing_core_schema(obj: Any) -> core_schema.CoreSchema | None:\r\n    # Only use the cached value from this _exact_ class; we don't want one from a parent class\r\n    # This is why we check `cls.__dict__` and don't use `cls.__pydantic_core_schema__` or similar.\r\n    if (\r\n        hasattr(obj, '__dict__')\r\n        and (existing_schema := obj.__dict__.get('__pydantic_core_schema__')) is not None\r\n        and not isinstance(existing_schema, MockCoreSchema)\r\n    ):\r\n        return existing_schema\r\n    return None\r\n```\r\n\r\nMaybe we could eagerly pull changes like that into this PR, given that https://github.com/pydantic/pydantic/pull/10655 isn't quite ready to merge yet?"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10863, "url": "https://github.com/pydantic/pydantic/pull/10863#discussion_r1848393876"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `image_processing_utils_fast.py`:\n\n```python\ndef to_dict(self):\n        encoder_dict = super().to_dict()\n        encoder_dict.pop(\"_valid_processor_keys\", None)\n        encoder_dict.pop(\"_valid_kwargs_names\", None)\n        return encoder_dict\n\n        # Filter out None values that are class defaults, but preserve explicitly set None values\n        filtered_dict = {}\n        for key, value in encoder_dict.items():\n            if value is None:\n                class_default = getattr(type(self), key, \"NOT_FOUND\")\n                # Keep None if user explicitly set it (class default is non-None)\n                if class_default != \"NOT_FOUND\" and class_default is not None:\n                    filtered_dict[key] = value\n            else:\n```"}, {"role": "assistant", "content": "Yes I was thinking about this, but since image processors configs are relatively small, I feel that it's nice to explicitely see all the parameters in the config files"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 42780, "url": "https://github.com/huggingface/transformers/pull/42780#discussion_r2611001940"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_utils_python.py`:\n\n```python\nself.assertEqual(get_func_args(str.split, stripself=True), [\"sep\", \"maxsplit\"])\n        self.assertEqual(get_func_args(\" \".join, stripself=True), [\"iterable\"])\n\n        if platform.python_implementation() == \"CPython\":\n            # doesn't work on CPython: https://bugs.python.org/issue42785\n            self.assertEqual(get_func_args(operator.itemgetter(2)), [])\n        elif platform.python_implementation() == \"PyPy\":\n            self.assertEqual(\n                get_func_args(operator.itemgetter(2), stripself=True), [\"obj\"]\n            )\n        # Didn't used to work on CPython: https://bugs.python.org/issue42785\n        self.assertIn(\n            get_func_args(operator.itemgetter(2), stripself=True),\n            [[], [\"obj\"], [\"args\", \"kwargs\"]],\n```"}, {"role": "assistant", "content": "Is `[\"args\", \"kwargs\"]` what CPython returns now? If so, it might be best to keep the old if/else for PyPy, and assert CPython for `[[], [\"args\", \"kwargs\"]]` instead."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 6316, "url": "https://github.com/scrapy/scrapy/pull/6316#discussion_r1568349603"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_mypy.py`:\n\n```python\nassert str(e.value) == 'Configuration value must be a boolean for key: init_forbid_extra'\n\n\ndef get_expected_return_code(source_code: str) -> int:\n    if re.findall(r'^\\s*# MYPY:', source_code, flags=re.MULTILINE):\n        return 1\n    return 0\n\n\n@pytest.mark.parametrize('module', ['dataclass_no_any', 'plugin_success', 'plugin_success_baseConfig'])\n@pytest.mark.filterwarnings('ignore:.*is deprecated.*:DeprecationWarning')\n@pytest.mark.filterwarnings('ignore:.*are deprecated.*:DeprecationWarning')\ndef test_success_cases_run(module: str) -> None:\n    \"\"\"\n    Ensure the \"success\" files can actually be executed\n    \"\"\"\n    importlib.import_module(f'tests.mypy.modules.{module}')\n\n\ndef test_explicit_reexports():\n    from pydantic import __all__ as root_all\n    from pydantic.deprecated.tools import __all__ as tools\n    from pydantic.main import __all__ as main\n    from pydantic.networks import __all__ as networks\n    from pydantic.types import __all__ as types\n\n    for name, export_all in [('main', main), ('network', networks), ('tools', tools), ('types', types)]:\n        for export in export_all:\n            assert export in root_all, f'{export} is in {name}.__all__ but missing from re-export in __init__.py'\n\n\ndef test_explicit_reexports_exist():\n    import pydantic\n\n    for name in pydantic.__all__:\n        assert hasattr(pydantic, name), f'{name} is in pydantic.__all__ but missing from pydantic'\n    module_name = f'tests.mypy.modules.{module}'\n    try:\n        importlib.import_module(module_name)\n    except Exception:\n        pytest.fail(reason=f'Unable to execute module {module_name}')\n\n\n@pytest.mark.parametrize(\n    'v_str,v_tuple',\n    ['v_str', 'v_tuple'],\n    [\n        ('0', (0,)),\n        ('0.930', (0, 930)),\n        ('0.940+dev.04cac4b5d911c4f9529e6ce86a27b44f28846f5d.dirty', (0, 940)),\n        ('1.11.0', (1, 11, 0)),\n        ('1.11.0+dev.d6d9d8cd4f27c52edac1f537e236ec48a01e54cb.dirty', (1, 11, 0)),\n    ],\n)\ndef test_parse_mypy_version(v_str, v_tuple):\ndef test_parse_mypy_version(v_str: str, v_tuple: tuple[int, ...]) -> None:\n```"}, {"role": "assistant", "content": "```suggestion\r\ndef test_parse_mypy_version(v_str: str, v_tuple: tuple[int, int, int]) -> None:\r\n```"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10612, "url": "https://github.com/pydantic/pydantic/pull/10612#discussion_r1798338832"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `optimization.py`:\n\n```python\ncurrent_step: int,\n    *,\n    num_warmup_steps: int,\n    num_training_steps: int,\n    num_stable_steps: int,\n```"}, {"role": "assistant", "content": "I still feel like we shouldn't need to pass both values. \r\nCan we just keep num_stable_steps and in `get_wsd_schedule`, we set \r\n`num_stable_steps = num_training_steps - num_decay_steps - num_warmup_steps` if `num_training_steps is not None` ?"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 35453, "url": "https://github.com/huggingface/transformers/pull/35453#discussion_r1908577262"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_scheduler_job.py`:\n\n```python\nreturn _create_dagrun\n\n\ndef task_maker(dag_maker, session, dag_id: str, task_num: int, max_active_tasks: int):\n    dag_tasks = {}\n\n    with dag_maker(dag_id=dag_id):\n        for i in range(task_num):\n            # Assign priority weight to certain tasks.\n            if (i % 10) == 0:  # 10, 20, 30, 40, 50, ...\n                weight = int(i / 2)\n                dag_tasks[f\"op{i}\"] = EmptyOperator(task_id=f\"dummy{i}\", priority_weight=weight)\n            else:\n                # No executor specified, runs on default executor\n                dag_tasks[f\"op{i}\"] = EmptyOperator(task_id=f\"dummy{i}\")\n\n    # 'logical_date' is used to create the 'run_id'. Set it to 'now', in order to get distinct run ids.\n    dag_run = dag_maker.create_dagrun(run_type=DagRunType.SCHEDULED, logical_date=timezone.utcnow())\n\n    task_tis = {}\n\n    tis_list = []\n    for i in range(task_num):\n        task_tis[f\"ti{i}\"] = dag_run.get_task_instance(dag_tasks[f\"op{i}\"].task_id, session)\n        # Add to the list.\n        tis_list.append(task_tis[f\"ti{i}\"])\n\n    for ti in tis_list:\n        ti.state = State.SCHEDULED\n        ti.dag_model.max_active_tasks = max_active_tasks\n\n    session.flush()\n\n    return tis_list\n```"}, {"role": "assistant", "content": "Yeah that makes sense. I first read it as a test that wasn't testing much and was confused."}], "_meta": {"repo": "apache/airflow", "pr_number": 54103, "url": "https://github.com/apache/airflow/pull/54103#discussion_r2285132956"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `asynpool.py`:\n\n```python\ntry:\n                setblocking(reader, 0)\n            except OSError:\n                return remove(fd)\n                remove(fd)\n```"}, {"role": "assistant", "content": "This change removes the `return` inside the `finally` block, which is good for Python 3.14 compatibility. However, other error paths in this function (lines 387, 393, 401) use `return remove(fd)`. Should we verify whether the return value of `remove(fd)` is significant? If `remove(fd)` returns a value that matters to the caller, this change would alter the function's behavior in the error case. If the return value doesn't matter, consider documenting why the finally-block case differs from the other error paths."}], "_meta": {"repo": "celery/celery", "pr_number": 10000, "url": "https://github.com/celery/celery/pull/10000#discussion_r2541912308"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `misc.py`:\n\n```python\nLogs a warning if a callable is a generator function and includes\n    a 'return' statement with a value different than None\n    \"\"\"\n    if is_generator_with_return_value(callable):\n    try:\n        should_warn = is_generator_with_return_value(callable)\n    except SyntaxError:  # includes IndentationError\n```"}, {"role": "assistant", "content": "I guess this cannot be tested without unclean mocking, so :heavy_check_mark:"}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 4935, "url": "https://github.com/scrapy/scrapy/pull/4935#discussion_r552126146"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_validators.py`:\n\n```python\nraise TypeError(f\"Unable to apply constraint 'max_length' to supplied value {x}\")\n\n\n_CONSTRAINT_TO_VALIDATOR_LOOKUP: dict[str, Callable] = {\ndef _extract_decimal_digits_info(decimal: Decimal, normalized: bool) -> tuple[int, int]:\n    \"\"\"Modeled after the `pydanitc-core` implementation of this function.\n\n    See https://github.com/pydantic/pydantic-core/blob/f389728432949ecceddecb1f59bb503b0998e9aa/src/validators/decimal.rs#L85.\n    Though this could be divided into two separate functions, the logic is easier to follow if we couple the computation\n    of the number of decimals and digits together.\n    \"\"\"\n    normalized_decimal = decimal.normalize() if normalized else decimal\n    _, digit_tuple, exponent = normalized_decimal.as_tuple()\n\n    exponent = int(exponent)\n    digits = len(digit_tuple)\n\n    if exponent >= 0:\n        # // A positive exponent adds that many trailing zeros.\n        digits += exponent\n        decimals = 0\n    else:\n        #  If the absolute value of the negative exponent is larger than the\n        #  number of digits, then it's the same as the number of digits,\n        #  because it'll consume all the digits in digit_tuple and then\n        #  add abs(exponent) - len(digit_tuple) leading zeros after the\n        #  decimal point.\n        decimals = abs(exponent)\n        digits = max(digits, decimals)\n\n    return decimals, digits\n```"}, {"role": "assistant", "content": "Also intimidated by this pattern - see the `pydantic-core` ref above that does basically the exact same thing. Thanks for the review :)"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 10506, "url": "https://github.com/pydantic/pydantic/pull/10506#discussion_r1778852682"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `strategy.py`:\n\n```python\nbucket = get_bucket(task.name)\n\n        if eta and bucket:\n            # Check if we've reached the ETA task limit\n            if eta_tracker.check_limit(req):\n```"}, {"role": "assistant", "content": "I think if we cap the prefetch count from kombu directly, we wouldn't need this change. It will be automatically handled by `consumer.qos.increment_eventually()`, which won't increment the count when it reaches the limit. Just sharing my investigation results when I was debugging that issue."}], "_meta": {"repo": "celery/celery", "pr_number": 9853, "url": "https://github.com/celery/celery/pull/9853#discussion_r2264634746"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_docs_extraction.py`:\n\n```python\n\"\"\"Utilities related to attribute docstring extraction.\"\"\"\nfrom __future__ import annotations\n\nimport ast\nimport inspect\nimport re\nimport textwrap\nfrom typing import Any\n\n\nclass DocstringVisitor(ast.NodeVisitor):\n    def __init__(self) -> None:\n        super().__init__()\n\n        self.target: str | None = None\n        self.attrs: dict[str, str] = {}\n        self.previous_node_type: type[ast.AST] | None = None\n\n    def visit(self, node: ast.AST) -> Any:\n        node_result = super().visit(node)\n        self.previous_node_type = type(node)\n        return node_result\n\n    def visit_AnnAssign(self, node: ast.AnnAssign) -> Any:\n        if isinstance(node.target, ast.Name):\n            self.target = node.target.id\n\n    def visit_Expr(self, node: ast.Expr) -> Any:\n        if isinstance(node.value, ast.Str) and self.previous_node_type is ast.AnnAssign:\n            docstring = inspect.cleandoc(node.value.s)\n            if self.target:\n                self.attrs[self.target] = docstring\n            self.target = None\n\n\ndef _extract_source_from_frame(lines: list[str], cls_name: str) -> list[str] | None:\n    source: list[str] = []\n    frame = inspect.currentframe()\n\n    if frame is None:\n        return None\n\n    while frame and frame.f_back:\n        lnum = frame.f_back.f_lineno\n        if isinstance(lnum, int) and len(lines) >= lnum and re.match(fr\"class\\s+{cls_name}\", lines[lnum - 1].strip()):\n            source = inspect.getblock(lines[lnum - 1 :])  # type: ignore\n```"}, {"role": "assistant", "content": "I might be able to remove the `# type: ignore` with https://github.com/python/typeshed/pull/10920"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 6563, "url": "https://github.com/pydantic/pydantic/pull/6563#discussion_r1369177308"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_proxies.py`:\n\n```python\nassert transport == client._transport\n    else:\n        assert transport.proxy_origin == url_to_origin(expected)\n\n\n@pytest.mark.parametrize(\n    [\"proxies\", \"expected_scheme\"],\n    [\n        ({\"http\": \"http://127.0.0.1\"}, [\"http://\"]),\n        ({\"https\": \"http://127.0.0.1\"}, [\"https://\"]),\n        ({\"all\": \"http://127.0.0.1\"}, [\"all://\"]),\n    ],\n)\ndef test_for_deprecated_proxy_params(proxies, expected_scheme):\n    with pytest.deprecated_call() as block:\n        httpx.AsyncClient(proxies=proxies)\n\n    warning_message = str(block.pop(DeprecationWarning))\n\n    for scheme in expected_scheme:\n        assert scheme in warning_message\n```"}, {"role": "assistant", "content": "No, it was correct (I expect CI to fail now! ;-)), but `expected_scheme` was unecessarily a 1-item list. Sent a patch\u2026"}], "_meta": {"repo": "encode/httpx", "pr_number": 1127, "url": "https://github.com/encode/httpx/pull/1127#discussion_r465891189"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_content_streams.py`:\n\n```python\nelse:\n            return ByteStream(body=b\"\")\n    elif isinstance(data, dict):\n```"}, {"role": "assistant", "content": "Seems like we should also check for empty data here, so as to not send an empty `URLEncodedStream`, but instead an empty `ByteStream`:\r\n\r\n```suggestion\r\n    elif isinstance(data, dict) and data:\r\n```\r\n\r\nThis would put us in line with what requests does:\r\n\r\n```python\r\n>>> import requests\r\n>>> url = 'https://httpbin.org/post'\r\n>>> r = requests.post(url, data={}, files={})\r\n>>> r.request.headers\r\n{'User-Agent': 'python-requests/2.23.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive', 'Content-Length': '0'}\r\n```"}], "_meta": {"repo": "encode/httpx", "pr_number": 861, "url": "https://github.com/encode/httpx/pull/861#discussion_r392665739"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `modeling_rt_detr.py`:\n\n```python\nattention_weights = self.attention_weights(hidden_states).view(\n            batch_size, num_queries, self.n_heads, self.n_levels * self.n_points\n        )\n        attention_weights = F.softmax(attention_weights, -1).view(\n        attention_weights = softmax(attention_weights, -1).view(\n```"}, {"role": "assistant", "content": "This is a bit weird, would like to not have a direct import"}], "_meta": {"repo": "huggingface/transformers", "pr_number": 41549, "url": "https://github.com/huggingface/transformers/pull/41549#discussion_r2712276255"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_request.py`:\n\n```python\njob.on_failure(exc_info)\n        assert not job.eventer.send.called\n\n    def test_on_failure_should_terminate(self):\n        from celery.worker import state\n        state.should_terminate = True\n        job = self.xRequest()\n        job.send_event = Mock(name='send_event')\n        job.task.backend = Mock(name='backend')\n\n        try:\n            try:\n                raise KeyError('foo')\n            except KeyError:\n                exc_info = ExceptionInfo()\n                job.on_failure(exc_info)\n\n            job.send_event.assert_not_called()\n            job.task.backend.mark_as_failure.assert_not_called()\n        finally:\n            state.should_terminate = None\n    def test_from_message_invalid_kwargs(self):\n```"}, {"role": "assistant", "content": "```suggestion\r\n\r\n    def test_from_message_invalid_kwargs(self):\r\n```"}], "_meta": {"repo": "celery/celery", "pr_number": 9678, "url": "https://github.com/celery/celery/pull/9678#discussion_r2065477519"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `test_types.py`:\n\n```python\nassert import_things.model_dump_json() == '{\"obj\":\"sys.stdout\"}'\n\n\ndef test_import_string_thing_with_name() -> None:\n```"}, {"role": "assistant", "content": "```suggestion\r\ndef test_import_string_thing_with_name() -> None:\r\n    \"\"\"https://github.com/pydantic/pydantic/issues/12218\"\"\"\r\n```"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 12219, "url": "https://github.com/pydantic/pydantic/pull/12219#discussion_r2346394555"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `_generate_schema.py`:\n\n```python\nif metadata_schema:\n                self._add_js_function(metadata_schema, metadata_js_function)\n\n        _add_custom_serialization_from_json_encoders(self._json_encoders, obj, schema)\n```"}, {"role": "assistant", "content": "Does it matter? We're editing the CoreSchema here directly while the js func won't get executed until later. Or maybe I'm misunderstanding. Is there a test case we can add?"}], "_meta": {"repo": "pydantic/pydantic", "pr_number": 6737, "url": "https://github.com/pydantic/pydantic/pull/6737#discussion_r1271291783"}}
{"messages": [{"role": "system", "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."}, {"role": "user", "content": "Review this Python code from `httpcache.py`:\n\n```python\nresponse = respcls(url=url, headers=headers, status=status, body=body)\n        return response\n\n    def store_response(self, spider, request, response):\n    def store_response(self, spider: Spider, request: Request, response):\n        \"\"\"Store the given response in the cache.\"\"\"\n        rpath = self._get_request_path(spider, request)\n        if not os.path.exists(rpath):\n            os.makedirs(rpath)\n        if not rpath.exists():\n            rpath.mkdir(parents=True)\n        metadata = {\n            'url': request.url,\n            'method': request.method,\n            'status': response.status,\n            'response_url': response.url,\n            'timestamp': time(),\n        }\n        with self._open(os.path.join(rpath, 'meta'), 'wb') as f:\n        with self._open(rpath / 'meta', 'wb') as f:\n            f.write(to_bytes(repr(metadata)))\n        with self._open(os.path.join(rpath, 'pickled_meta'), 'wb') as f:\n        with self._open(rpath / 'pickled_meta', 'wb') as f:\n            pickle.dump(metadata, f, protocol=4)\n        with self._open(os.path.join(rpath, 'response_headers'), 'wb') as f:\n        with self._open(rpath / 'response_headers', 'wb') as f:\n            f.write(headers_dict_to_raw(response.headers))\n        with self._open(os.path.join(rpath, 'response_body'), 'wb') as f:\n        with self._open(rpath / 'response_body', 'wb') as f:\n            f.write(response.body)\n        with self._open(os.path.join(rpath, 'request_headers'), 'wb') as f:\n        with self._open(rpath / 'request_headers', 'wb') as f:\n            f.write(headers_dict_to_raw(request.headers))\n        with self._open(os.path.join(rpath, 'request_body'), 'wb') as f:\n        with self._open(rpath / 'request_body', 'wb') as f:\n            f.write(request.body)\n\n    def _get_request_path(self, spider, request):\n    def _get_request_path(self, spider: Spider, request: Request) -> Path:\n        key = self._fingerprinter.fingerprint(request).hex()\n        return os.path.join(self.cachedir, spider.name, key[0:2], key)\n        return Path(self.cachedir, spider.name, key[0:2], key)\n```"}, {"role": "assistant", "content": "I guess the question is, is it private as in it's not supposed to be called by other types or is it private as in it's part of Scrapy and if you touch it you're accepting the risk of it unexpectedly changing? I can put a `str()` around it."}], "_meta": {"repo": "scrapy/scrapy", "pr_number": 5682, "url": "https://github.com/scrapy/scrapy/pull/5682#discussion_r1001228259"}}
