{
  "format": "chatml",
  "stats": {
    "total": 10809,
    "kept": 8118,
    "filtered": {
      "code_too_long": 1902,
      "mostly_questions": 695,
      "mostly_code_blocks": 20,
      "comment_too_short": 9,
      "comment_too_long": 34,
      "code_too_short": 31
    }
  },
  "train_count": 7306,
  "eval_count": 812,
  "examples": [
    {
      "messages": [
        {
          "role": "system",
          "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."
        },
        {
          "role": "user",
          "content": "Review this Python code from `utils.py`:\n\n```python\ngeneration_config = copy.deepcopy(generation_config)\n\n        # First set values from the loaded `self.generation_config`, then set default values (BC)\n        # Do not update any values that aren't `None`, i.e. if set by users explicitly and passed\n        # to `generate()`. Thus the `defaults_only=True` is used\n        #\n        # Only update values that are `None`, i.e. these values were not explicitly set by by users to `generate()`,\n        # or values that are not present in the current config, i.e. custom entries that were set via `**kwargs`.\n        # Thus we use the specific kwargs `defaults_only=True` (`None` values only) and `allow_custom_entries=True`\n        # (custom entries are carried over).\n        global_defaults = self.generation_config._get_default_generation_params()\n        generation_config.update(**self.generation_config.to_dict(), defaults_only=True)\n        generation_config.update(**self.generation_config.to_dict(), defaults_only=True, allow_custom_entries=True)\n```"
        },
        {
          "role": "assistant",
          "content": "Nope, not the case, e.g.\r\n```python\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\n    \"meta-llama/Llama-3.2-3B-Instruct\",\r\n    device_map=\"auto\",\r\n    torch_dtype=\"auto\",\r\n    attn_implementation=\"flash_attention_2\",\r\n).eval()\r\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\r\ntokenizer.pad_token = tokenizer.eos_token\r\ninputs = tokenizer(\r\n    [\"Hello, how are you?\", \"is this life?\"],\r\n    padding=True,\r\n    padding_side=\"left\",\r\n    return_tensors=\"pt\",\r\n).to(model.device)\r\n\r\noutputs = model.generate(**inputs, max_new_tokens=50, generation_config=GenerationConfig(do_sample=False))\r\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\r\n```\r\nWe will have different generation configs so it's not guaranteed."
        }
      ],
      "_meta": {
        "repo": "huggingface/transformers",
        "pr_number": 43181,
        "url": "https://github.com/huggingface/transformers/pull/43181#discussion_r2676708169"
      }
    },
    {
      "messages": [
        {
          "role": "system",
          "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."
        },
        {
          "role": "user",
          "content": "Review this Python code from `pep_572_remove_parens.py`:\n\n```python\ndef this_is_so_dumb() -> (please := no):\n    pass\n\n\nasync def await_the_walrus():\n    with (x := y):\n        pass\n\n    with (x := y) as z, (a := b) as c:\n        pass\n\n    with (x := await y):\n        pass\n\n    with (x := await a, y := await b):\n        pass\n\n    # Ideally we should remove one set of parentheses\n    with ((x := await a, y := await b)):\n        pass\n```"
        },
        {
          "role": "assistant",
          "content": "That seems fine, since the main purpose of this test is to make sure we don't crash on this case. We can always change the exact test output when that part of the preview style becomes stable."
        }
      ],
      "_meta": {
        "repo": "psf/black",
        "pr_number": 3473,
        "url": "https://github.com/psf/black/pull/3473#discussion_r1057396263"
      }
    },
    {
      "messages": [
        {
          "role": "system",
          "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."
        },
        {
          "role": "user",
          "content": "Review this Python code from `test_requests.py`:\n\n```python\nassert isinstance(s, builtin_str)\n        assert s == \"Basic dGVzdDp0ZXN0\"\n\n    def test_requests_history_is_saved(self):\n        r = requests.get('https://httpbin.org/redirect/5')\n        count = 0\n        for item in r.history:\n            assert len(item.history) == count\n            count = count + 1\n```"
        },
        {
          "role": "assistant",
          "content": "Can we make this test a bit more rigorous? We should be able to assert that each item has the correct slice of the history as well."
        }
      ],
      "_meta": {
        "repo": "psf/requests",
        "pr_number": 2168,
        "url": "https://github.com/psf/requests/pull/2168#discussion_r16338170"
      }
    },
    {
      "messages": [
        {
          "role": "system",
          "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."
        },
        {
          "role": "user",
          "content": "Review this Python code from `test_feedexport.py`:\n\n```python\nmaxDiff = None\n\n    def test_init(self):\n        settings_dict = {\n            'FEED_URI': 'file:///tmp/foobar',\n            'FEED_STORAGES': {\n                'file': FileFeedStorageWithoutFeedOptions\n            },\n        }\n        crawler = get_crawler(settings_dict=settings_dict)\n        feed_exporter = FeedExporter.from_crawler(crawler)\n        with tempfile.NamedTemporaryFile() as temp:\n            settings_dict = {\n                'FEED_URI': f'file:///{temp.name}',\n```"
        },
        {
          "role": "assistant",
          "content": "What is the value of `temp.name` here? Aren\u2019t there 1 too many / in the URI scheme here? And if so, how are tests working nonetheless?\r\n\r\nAlso, according to https://docs.python.org/3/library/tempfile.html#tempfile.NamedTemporaryFile:\r\n\r\n> Whether the name can be used to open the file a second time, while the named temporary file is still open, varies across platforms (it can be so used on Unix; it cannot on Windows NT or later).\r\n\r\nWhich makes me wonder how tests are passing in Windows."
        }
      ],
      "_meta": {
        "repo": "scrapy/scrapy",
        "pr_number": 5198,
        "url": "https://github.com/scrapy/scrapy/pull/5198#discussion_r676646957"
      }
    },
    {
      "messages": [
        {
          "role": "system",
          "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."
        },
        {
          "role": "user",
          "content": "Review this Python code from `_client.py`:\n\n```python\n) -> None:\n        await self.aclose()\n\n    def __del__(self) -> None:\n        if not self.is_closed:\n            warnings.warn(\n                f\"Unclosed {self!r}. \"\n                \"Please call\\n\"\n                \"\\t>>> await async_client.aclose()\\n\"\n                \"or use it as a context manager\\n\"\n                \"\\t>>> async with httpx.AsyncClient() as client:\\n\"\n                \"\\t>>>     ...\",\n                ResourceWarning,\n            )\n```"
        },
        {
          "role": "assistant",
          "content": "Right, I'm going to suggest that we use `UserWarning` for this case, for better visibility.\r\n\r\nWe'll eventually end up tracking individual connections with proper a `ResourceWarning` against each of those, but this is a good case for a higher-level warning I think.\r\n\r\nAlso, let's make sure to use a more concise warning format.\r\n\r\nPerhaps...\r\n\r\n```python\r\nwarnings.warn(\"Unclosed {self!r}. See https://www.python-httpx.org/async/#opening-and-closing-clients for details.\")\r\n```\r\n\r\nThen we can add a little more context in the documentation.\r\n\r\nWhat do we think?"
        }
      ],
      "_meta": {
        "repo": "encode/httpx",
        "pr_number": 1197,
        "url": "https://github.com/encode/httpx/pull/1197#discussion_r474644263"
      }
    },
    {
      "messages": [
        {
          "role": "system",
          "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."
        },
        {
          "role": "user",
          "content": "Review this Python code from `_urlparse.py`:\n\n```python\n)\n\n\ndef quote(string: str, safe: str = \"/\") -> str:\n    \"\"\"\n    Use percent-encoding to quote a string, omitting existing '%xx' escape sequences.\n    \"\"\"\n    parts = []\n    current_position = 0\n    for match in re.finditer(PERCENT_ENCODED_REGEX, string):\n        start_position, end_position = match.start(), match.end()\n        matched_text = match.group(0)\n        # Add any text up to the '%xx' escape sequence.\n        if start_position != current_position:\n            leading_text = string[current_position:start_position]\n            parts.append(percent_encoded(leading_text, safe=safe))\n\n        # Add the '%xx' escape sequence.\n        parts.append(matched_text)\n        current_position = end_position\n\n    # Add any text after the final '%xx' escape sequence.\n    if current_position != len(string):\n        trailing_text = string[current_position:]\n        parts.append(percent_encoded(trailing_text, safe=safe))\n\n    return \"\".join(parts)\n```"
        },
        {
          "role": "assistant",
          "content": "It could, yes. I've gone with this pattern because in the past we've seen performance issues with the `+=` pattern, that a `.append() ... \"\".join()` pattern resolves. Might not be the case in this context, since we've got strict limits on the allowable string lengths in URLs. But... let's handle any performance related considerations separately to the behavioural fixes."
        }
      ],
      "_meta": {
        "repo": "encode/httpx",
        "pr_number": 2990,
        "url": "https://github.com/encode/httpx/pull/2990#discussion_r1422164211"
      }
    },
    {
      "messages": [
        {
          "role": "system",
          "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."
        },
        {
          "role": "user",
          "content": "Review this Python code from `_api.py`:\n\n```python\nauth: AuthTypes | None = None,\n    proxy: ProxyTypes | None = None,\n    follow_redirects: bool = False,\n    cert: CertTypes | None = None,\n    verify: VerifyTypes = True,\n    ssl_context: typing.Optional[ssl.SSLContext] = None,\n```"
        },
        {
          "role": "assistant",
          "content": "```suggestion\r\n    ssl_context: ssl.SSLContext | None = None,\r\n```"
        }
      ],
      "_meta": {
        "repo": "encode/httpx",
        "pr_number": 3022,
        "url": "https://github.com/encode/httpx/pull/3022#discussion_r1790227449"
      }
    },
    {
      "messages": [
        {
          "role": "system",
          "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."
        },
        {
          "role": "user",
          "content": "Review this Python code from `test_modeling_bart.py`:\n\n```python\nwith torch.no_grad():\n                model(**inputs)[0]\n\n    def test_input_embeddings_support_forward_hook(self):\n```"
        },
        {
          "role": "assistant",
          "content": "Can you add a comment here:\r\n- Why we have this test\r\n- Reference this PR"
        }
      ],
      "_meta": {
        "repo": "huggingface/transformers",
        "pr_number": 41821,
        "url": "https://github.com/huggingface/transformers/pull/41821#discussion_r2456466214"
      }
    },
    {
      "messages": [
        {
          "role": "system",
          "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."
        },
        {
          "role": "user",
          "content": "Review this Python code from `test_supervisor.py`:\n\n```python\nbackend = ExampleBackend()\n    monkeypatch.setattr(supervisor, \"ensure_secrets_backend_loaded\", lambda: [backend])\n    monkeypatch.setattr(sdk_log, \"load_remote_log_handler\", lambda: object())\n    monkeypatch.setattr(sdk_log, \"load_remote_conn_id\", lambda: \"test_conn\")\n    monkeypatch.delenv(\"AIRFLOW_CONN_TEST_CONN\", raising=False)\n\n    def noop_request(request: httpx.Request) -> httpx.Response:\n        return httpx.Response(200)\n\n    clients = []\n    for _ in range(3):\n        client = make_client(transport=httpx.MockTransport(noop_request))\n        clients.append(weakref.ref(client))\n        with _remote_logging_conn(client):\n            pass\n        client.close()\n        del client\n\n    gc.collect()\n    assert backend.calls == 1, \"Connection should be cached, not fetched multiple times\"\n    assert all(ref() is None for ref in clients), \"Client instances should be garbage collected\"\n    with conf_vars(\n        {\n            (\"logging\", \"remote_logging\"): \"True\",\n            (\"logging\", \"remote_base_log_folder\"): \"s3://bucket/logs\",\n            (\"logging\", \"remote_log_conn_id\"): \"test_conn\",\n        }\n    ):\n```"
        },
        {
          "role": "assistant",
          "content": "Instead of patching `load_remote_log_handler` and `load_remote_conn_id`, we can just populate the right env's and rest will be handled by the code called when `_remote_logging_conn` is called."
        }
      ],
      "_meta": {
        "repo": "apache/airflow",
        "pr_number": 60826,
        "url": "https://github.com/apache/airflow/pull/60826#discussion_r2719525953"
      }
    },
    {
      "messages": [
        {
          "role": "system",
          "content": "You are an expert code reviewer. Analyze the provided Python code and give constructive, specific feedback. Focus on bugs, potential issues, code quality, and improvements. Be direct and actionable."
        },
        {
          "role": "user",
          "content": "Review this Python code from `test_modeling_glm_image.py`:\n\n```python\ndef test_model_parallelism(self):\n        pass\n\n    @unittest.skip(reason=\"GLM-Image has special image token IDs that get clamped when vocab is resized smaller\")\n    def test_resize_embeddings_untied(self):\n        pass\n\n    @unittest.skip(reason=\"GLM-Image has special image token IDs that get clamped when vocab is resized smaller\")\n    def test_resize_tokens_embeddings(self):\n        pass\n```"
        },
        {
          "role": "assistant",
          "content": "we need to assign `image_token_id` to a different token id in that case. It can be done in `GlmImageModelTester`"
        }
      ],
      "_meta": {
        "repo": "huggingface/transformers",
        "pr_number": 43342,
        "url": "https://github.com/huggingface/transformers/pull/43342#discussion_r2732471374"
      }
    }
  ]
}