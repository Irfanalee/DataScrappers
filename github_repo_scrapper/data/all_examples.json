{
  "scraped_at": "2026-02-03T11:00:21.325289",
  "total_examples": 1546,
  "stats": {
    "total_comments": 4158,
    "kept": 1546,
    "by_repo": {
      "fastapi/fastapi": 82,
      "pydantic/pydantic": 182,
      "psf/requests": 142,
      "encode/httpx": 227,
      "astral-sh/ruff": 40,
      "tiangolo/sqlmodel": 31,
      "pallets/flask": 18,
      "django/django": 824,
      "pytorch/pytorch": 0
    }
  },
  "examples": [
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14619,
      "file_path": "scripts/translate.py",
      "line": 28,
      "side": "RIGHT",
      "diff_hunk": "@@ -25,650 +25,8 @@\n     \"contributing.md\",\n )\n \n-\n-general_prompt = \"\"\"\n-### About literal text in this prompt\n-\n-1) In the following instructions (after I say: `The above rules are in effect now`) the two characters `\u00ab` and `\u00bb` will be used to surround LITERAL TEXT, which is text or characters you shall interpret literally. The `\u00ab` and the `\u00bb` are not part of the literal text, they are the meta characters denoting it.\n-\n-2) Furthermore, text surrounded by `\u00ab\u00ab\u00ab` and `\u00bb\u00bb\u00bb` is a BLOCK OF LITERAL TEXT which spans multiple lines. To get its content, dedent all lines of the block until the `\u00ab\u00ab\u00ab` and `\u00bb\u00bb\u00bb` are at column zero, then remove the newline (`\\n`) after the `\u00ab\u00ab\u00ab` and the newline before the `\u00bb\u00bb\u00bb`. The `\u00ab\u00ab\u00ab` and the `\u00bb\u00bb\u00bb` are not part of the literal text block, they are the meta characters denoting it.\n-\n-3) If you see backticks or any other quotes inside literal text \u2013 inside `\u00ab` and `\u00bb` \u2013  or inside blocks of literal text \u2013 inside `\u00ab\u00ab\u00ab` and `\u00bb\u00bb\u00bb` \u2013 then interpret them as literal characters, do NOT interpret them as meta characters.\n-\n-The above rules are in effect now.\n-\n-\n-### Definitions of terms used in this prompt\n-\n-\"backtick\"\n-\n-    The character \u00ab`\u00bb\n-    Unicode U+0060 (GRAVE ACCENT)\n-\n-\"single backtick\"\n-\n-    A single backtick \u2013 \u00ab`\u00bb\n-\n-\"triple backticks\"\n-\n-    Three backticks in a row \u2013 \u00ab```\u00bb\n-\n-\"neutral double quote\"\n-\n-    The character \u00ab\"\u00bb\n-    Unicode U+0022 (QUOTATION MARK)\n-\n-\"neutral single quote\"\n-\n-    The character \u00ab'\u00bb\n-    Unicode U+0027 (APOSTROPHE)\n-\n-\"English double typographic quotes\"\n-\n-    The characters \u00ab\u201c\u00bb and \u00ab\u201d\u00bb\n-    Unicode U+201C (LEFT DOUBLE QUOTATION MARK) and Unicode U+201D (RIGHT DOUBLE QUOTATION MARK)\n-\n-\"English single typographic quotes\"\n-\n-    The characters \u00ab\u2018\u00bb and \u00ab\u2019\u00bb\n-    Unicode U+2018 (LEFT SINGLE QUOTATION MARK) and Unicode U+2019 (RIGHT SINGLE QUOTATION MARK)\n-\n-\"code snippet\"\n-\n-    Also called \"inline code\". Text in a Markdown document which is surrounded by single backticks. A paragraph in a Markdown document can have a more than one code snippet.\n-\n-    Example:\n-\n-        \u00ab\u00ab\u00ab\n-        `i am a code snippet`\n-        \u00bb\u00bb\u00bb\n-\n-    Example:\n-\n-        \u00ab\u00ab\u00ab\n-        `first code snippet` `second code snippet` `third code snippet`\n-        \u00bb\u00bb\u00bb\n-\n-\"code block\"\n-\n-    Text in a Markdown document which is surrounded by triple backticks. Spreads multiple lines.\n-\n-    Example:\n-\n-        \u00ab\u00ab\u00ab\n-        ```\n-        Hello\n-        World\n-        ```\n-        \u00bb\u00bb\u00bb\n-\n-    Example:\n-\n-        \u00ab\u00ab\u00ab\n-        ```python\n-        print(\"hello World\")\n-        ```\n-        \u00bb\u00bb\u00bb\n-\n-\"HTML element\"\n-\n-    a HTML opening tag \u2013 e.g. \u00ab<div>\u00bb \u2013 and a HTML closing tag \u2013 e.g. \u00ab</div>\u00bb \u2013 surrounding text or other HTML elements.\n-\n-\n-### Your task\n-\n-Translate an English text \u2013 the original content \u2013 to a target language.\n-\n-The original content is written in Markdown, write the translation in Markdown as well.\n-\n-The original content will be surrounded by triple percentage signs (\u00ab%%%\u00bb). Do not include the triple percentage signs in the translation.\n-\n-\n-### Technical terms in English\n-\n-For technical terms in English that don't have a common translation term, use the original term in English.\n-\n-\n-### Content of code snippets\n-\n-Do not translate the content of code snippets, keep the original in English. For example, \u00ab`list`\u00bb, \u00ab`dict`\u00bb, keep them as is.\n-\n-\n-### Content of code blocks\n-\n-Do not translate the content of code blocks, except for comments in the language which the code block uses.\n-\n-Examples:\n-\n-    Source (English) \u2013 The code block is a bash code example with one comment:\n-\n-        \u00ab\u00ab\u00ab\n-        ```bash\n-        # Print greeting\n-        echo \"Hello, World!\"\n-        ```\n-        \u00bb\u00bb\u00bb\n-\n-    Result (German):\n-\n-        \u00ab\u00ab\u00ab\n-        ```bash\n-        # Gru\u00df ausgeben\n-        echo \"Hello, World!\"\n-        ```\n-        \u00bb\u00bb\u00bb\n-\n-    Source (English) \u2013 The code block is a console example containing HTML tags. No comments, so nothing to change here:\n-\n-        \u00ab\u00ab\u00ab\n-        ```console\n-        $ <font color=\"#4E9A06\">fastapi</font> run <u style=\"text-decoration-style:solid\">main.py</u>\n-        <span style=\"background-color:#009485\"><font color=\"#D3D7CF\"> FastAPI </font></span>  Starting server\n-                Searching for package file structure\n-        ```\n-        \u00bb\u00bb\u00bb\n-\n-    Result (German):\n-\n-        \u00ab\u00ab\u00ab\n-        ```console\n-        $ <font color=\"#4E9A06\">fastapi</font> run <u style=\"text-decoration-style:solid\">main.py</u>\n-        <span style=\"background-color:#009485\"><font color=\"#D3D7CF\"> FastAPI </font></span>  Starting server\n-                Searching for package file structure\n-        ```\n-        \u00bb\u00bb\u00bb\n-\n-    Source (English) \u2013 The code block is a console example containing 5 comments:\n-\n-        \u00ab\u00ab\u00ab\n-        ```console\n-        // Go to the home directory\n-        $ cd\n-        // Create a directory for all your code projects\n-        $ mkdir code\n-        // Enter into that code directory\n-        $ cd code\n-        // Create a directory for this project\n-        $ mkdir awesome-project\n-        // Enter into that project directory\n-        $ cd awesome-project\n-        ```\n-        \u00bb\u00bb\u00bb\n-\n-    Result (German):\n-\n-        \u00ab\u00ab\u00ab\n-        ```console\n-        // Gehe zum Home-Verzeichnis\n-        $ cd\n-        // Erstelle ein Verzeichnis f\u00fcr alle Ihre Code-Projekte\n-        $ mkdir code\n-        // Gehe in dieses Code-Verzeichnis\n-        $ cd code\n-        // Erstelle ein Verzeichnis f\u00fcr dieses Projekt\n-        $ mkdir awesome-project\n-        // Gehe in dieses Projektverzeichnis\n-        $ cd awesome-project\n-        ```\n-        \u00bb\u00bb\u00bb\n-\n-If there is an existing translation and its Mermaid diagram is in sync with the Mermaid diagram in the English source, except a few translated words, then use the Mermaid diagram of the existing translation. The human editor of the translation translated these words in the Mermaid diagram. Keep these translations, do not revert them back to the English source.\n-\n-Example:\n-\n-    Source (English):\n-\n-        \u00ab\u00ab\u00ab\n-        ```mermaid\n-        flowchart LR\n-            subgraph global[global env]\n-                harry-1[harry v1]\n-            end\n-            subgraph stone-project[philosophers-stone project]\n-                stone(philosophers-stone) -->|requires| harry-1\n-            end\n-        ```\n-        \u00bb\u00bb\u00bb\n-\n-    Existing translation (German) \u2013 has three translations:\n-\n-        \u00ab\u00ab\u00ab\n-        ```mermaid\n-        flowchart LR\n-            subgraph global[globale Umgebung]\n-                harry-1[harry v1]\n-            end\n-            subgraph stone-project[philosophers-stone-Projekt]\n-                stone(philosophers-stone) -->|ben\u00f6tigt| harry-1\n-            end\n-        ```\n-        \u00bb\u00bb\u00bb\n-\n-    Result (German) \u2013 you change nothing:\n-\n-        \u00ab\u00ab\u00ab\n-        ```mermaid\n-        flowchart LR\n-            subgraph global[globale Umgebung]\n-                harry-1[harry v1]\n-            end\n-            subgraph stone-project[philosophers-stone-Projekt]\n-                stone(philosophers-stone) -->|ben\u00f6tigt| harry-1\n-            end\n-        ```\n-        \u00bb\u00bb\u00bb\n-\n-\n-### Special blocks\n-\n-There are special blocks of notes, tips and others that look like:\n-\n-    \u00ab\u00ab\u00ab\n-    /// note\n-    \u00bb\u00bb\u00bb\n-\n-To translate it, keep the same line and add the translation after a vertical bar.\n-\n-For example, if you were translating to Spanish, you would write:\n-\n-    \u00ab\u00ab\u00ab\n-    /// note | Nota\n-    \u00bb\u00bb\u00bb\n-\n-Some examples in Spanish:\n-\n-    Source:\n-\n-        \u00ab\u00ab\u00ab\n-        /// tip\n-        \u00bb\u00bb\u00bb\n-\n-    Result:\n-\n-        \u00ab\u00ab\u00ab\n-        /// tip | Consejo\n-        \u00bb\u00bb\u00bb\n-\n-    Source:\n-\n-        \u00ab\u00ab\u00ab\n-        /// details | Preview\n-        \u00bb\u00bb\u00bb\n-\n-    Result:\n-\n-        \u00ab\u00ab\u00ab\n-        /// details | Vista previa\n-        \u00bb\u00bb\u00bb\n-\n-\n-### Tab blocks\n-\n-There are special blocks surrounded by four slashes (\u00ab////\u00bb). They mark text, which will be rendered as part of a tab in the final document. The scheme is:\n-\n-    //// tab | {tab title}\n-    {tab content, may span many lines}\n-    ////\n-\n-Keep everything before the vertical bar (\u00ab|\u00bb) as is, including the vertical bar. Translate the tab title. Translate the tab content, applying the rules you know. Keep the four block closing slashes as is.\n-\n-Examples:\n-\n-    Source (English):\n-\n-        \u00ab\u00ab\u00ab\n-        //// tab | Python 3.8+ non-Annotated\n-        Hello\n-        ////\n-        \u00bb\u00bb\u00bb\n-\n-    Result (German):\n-\n-        \u00ab\u00ab\u00ab\n-        //// tab | Python 3.8+ nicht annotiert\n-        Hallo\n-        ////\n-        \u00bb\u00bb\u00bb\n-\n-    Source (English) \u2013 Here there is nothing to translate in the tab title:\n-\n-        \u00ab\u00ab\u00ab\n-        //// tab | Linux, macOS, Windows Bash\n-        Hello again\n-        ////\n-        \u00bb\u00bb\u00bb\n-\n-    Result (German):\n-\n-        \u00ab\u00ab\u00ab\n-        //// tab | Linux, macOS, Windows Bash\n-        Hallo wieder\n-        ////\n-        \u00bb\u00bb\u00bb\n-\n-\n-### Headings\n-\n-Every Markdown heading in the English text (all levels) ends with a part inside curly brackets. This part denotes the hash of this heading, which is used in links to this heading. In translations, translate the heading, but do not translate this hash part, so that links do not break.\n-\n-Examples of how to translate a heading:\n-\n-    Source (English):\n-\n-        \u00ab\u00ab\u00ab\n-        ## Alternative API docs { #alternative-api-docs }\n-        \u00bb\u00bb\u00bb\n-\n-    Result (Spanish):\n-\n-        \u00ab\u00ab\u00ab\n-        ## Documentaci\u00f3n de la API alternativa { #alternative-api-docs }\n-        \u00bb\u00bb\u00bb\n-\n-    Source (English):\n-\n-        \u00ab\u00ab\u00ab\n-        ### Example { #example }\n-        \u00bb\u00bb\u00bb\n-\n-    Result (German):\n-\n-        \u00ab\u00ab\u00ab\n-        ### Beispiel { #example }\n-        \u00bb\u00bb\u00bb\n-\n-\n-### Links\n-\n-Use the following rules for links (apply both to Markdown-style links ([text](url)) and to HTML-style <a> tags):\n-\n-1) For relative URLs, only translate link text. Do not translate the URL or its parts\n-\n-Example:\n-\n-    Source (English):\n-\n-        \u00ab\u00ab\u00ab\n-        [One of the fastest Python frameworks available](#performance)\n-        \u00bb\u00bb\u00bb\n-\n-    Result (German):\n-\n-        \u00ab\u00ab\u00ab\n-        [Eines der schnellsten verf\u00fcgbaren Python-Frameworks](#performance)\n-        \u00bb\u00bb\u00bb\n-\n-2) For absolute URLs which DO NOT start EXACTLY with \u00abhttps://fastapi.tiangolo.com\u00bb, only translate link text and leave the URL unchanged.\n-\n-Example:\n-\n-    Source (English):\n-\n-        \u00ab\u00ab\u00ab\n-        <a href=\"https://sqlmodel.tiangolo.com/\" class=\"external-link\" target=\"_blank\">SQLModel docs</a>\n-        \u00bb\u00bb\u00bb\n-\n-    Result (German):\n-\n-        \u00ab\u00ab\u00ab\n-        <a href=\"https://sqlmodel.tiangolo.com/\" class=\"external-link\" target=\"_blank\">SQLModel-Dokumentation</a>\n-        \u00bb\u00bb\u00bb\n-\n-3) For absolute URLs which DO start EXACTLY with \u00abhttps://fastapi.tiangolo.com\u00bb, only translate link text and change the URL by adding language code (\u00abhttps://fastapi.tiangolo.com/{language_code}[rest part of the url]\u00bb).\n-\n-Example:\n-\n-    Source (English):\n-\n-        \u00ab\u00ab\u00ab\n-        <a href=\"https://fastapi.tiangolo.com/tutorial/path-params/#documentation\" class=\"external-link\" target=\"_blank\">Documentation</a>\n-        \u00bb\u00bb\u00bb\n-\n-    Result (Spanish):\n-\n-        \u00ab\u00ab\u00ab\n-        <a href=\"https://fastapi.tiangolo.com/es/tutorial/path-params/#documentation\" class=\"external-link\" target=\"_blank\">Documentaci\u00f3n</a>\n-        \u00bb\u00bb\u00bb\n-\n-3.1) Do not add language codes for URLs that point to static assets (e.g., images, CSS, JavaScript).\n-\n-Example:\n-\n-    Source (English):\n-\n-        \u00ab\u00ab\u00ab\n-        <a href=\"https://fastapi.tiangolo.com/img/something.jpg\" class=\"external-link\" target=\"_blank\">Something</a>\n-        \u00bb\u00bb\u00bb\n-\n-    Result (Spanish):\n-\n-        \u00ab\u00ab\u00ab\n-        <a href=\"https://fastapi.tiangolo.com/img/something.jpg\" class=\"external-link\" target=\"_blank\">Algo</a>\n-        \u00bb\u00bb\u00bb\n-\n-4) For internal links, only translate link text.\n-\n-Example:\n-\n-    Source (English):\n-\n-        \u00ab\u00ab\u00ab\n-        [Create Pull Requests](help-fastapi.md#create-a-pull-request){.internal-link target=_blank}\n-        \u00bb\u00bb\u00bb\n-\n-    Result (German):\n-\n-        \u00ab\u00ab\u00ab\n-        [Pull Requests erzeugen](help-fastapi.md#create-a-pull-request){.internal-link target=_blank}\n-        \u00bb\u00bb\u00bb\n-\n-5) Do not translate anchor fragments in links (the part after \u00ab#\u00bb), as they must remain the same to work correctly.\n-\n-5.1) If an existing translation has a link with an anchor fragment different to the anchor fragment in the English source, then this is an error. Fix this by using the anchor fragment of the English source.\n-\n-Example:\n-\n-    Source (English):\n-\n-        \u00ab\u00ab\u00ab\n-        [Body - Multiple Parameters: Singular values in body](body-multiple-params.md#singular-values-in-body){.internal-link target=_blank}\n-        \u00bb\u00bb\u00bb\n-\n-    Existing wrong translation (German) \u2013 notice the wrongly translated anchor fragment:\n-\n-        \u00ab\u00ab\u00ab\n-        [Body \u2013 Mehrere Parameter: Einfache Werte im Body](body-multiple-params.md#einzelne-werte-im-body){.internal-link target=_blank}.\n-        \u00bb\u00bb\u00bb\n-\n-    Result (German) \u2013 you fix the anchor fragment:\n-\n-        \u00ab\u00ab\u00ab\n-        [Body \u2013 Mehrere Parameter: Einfache Werte im Body](body-multiple-params.md#singular-values-in-body){.internal-link target=_blank}.\n-        \u00bb\u00bb\u00bb\n-\n-5.2) Do not add anchor fragments at will, even if this makes sense. If the English source has no anchor, don't add one.\n-\n-Example:\n-\n-    Source (English):\n-\n-        \u00ab\u00ab\u00ab\n-        Create a [virtual environment](../virtual-environments.md){.internal-link target=_blank}\n-        \u00bb\u00bb\u00bb\n-\n-    Wrong translation (German) \u2013 Anchor added to the URL.\n-\n-        \u00ab\u00ab\u00ab\n-        Erstelle eine [virtuelle Umgebung](../virtual-environments.md#create-a-virtual-environment){.internal-link target=_blank}\n-        \u00bb\u00bb\u00bb\n-\n-    Good translation (German) \u2013 URL stays like in the English source.\n-\n-        \u00ab\u00ab\u00ab\n-        Erstelle eine [Virtuelle Umgebung](../virtual-environments.md){.internal-link target=_blank}\n-        \u00bb\u00bb\u00bb\n-\n-\n-### HTML abbr elements\n-\n-Translate HTML abbr elements (\u00ab<abbr title=\"description\">text</abbr>\u00bb) as follows:\n-\n-1) If the text surrounded by the abbr element is an abbreviation (the text may be surrounded by further HTML or Markdown markup or quotes, for example \u00ab<code>text</code>\u00bb or \u00ab`text`\u00bb or \u00ab\"text\"\u00bb, ignore that further markup when deciding if the text is an abbreviation), and if the description (the text inside the title attribute) contains the full phrase for this abbreviation, then append a dash (\u00ab\u2013\u00bb) to the full phrase, followed by the translation of the full phrase.\n-\n-Conversion scheme:\n-\n-    Source (English):\n-\n-        <abbr title=\"{full phrase}\">{abbreviation}</abbr>\n-\n-    Result:\n-\n-        <abbr title=\"{full phrase} \u2013 {translation of full phrase}\">{abbreviation}</abbr>\n-\n-Examples:\n-\n-    Source (English):\n-\n-        \u00ab\u00ab\u00ab\n-        <abbr title=\"Internet of Things\">IoT</abbr>\n-        <abbr title=\"Central Processing Unit\">CPU</abbr>\n-        <abbr title=\"too long; didn't read\"><strong>TL;DR:</strong></abbr>\n-        \u00bb\u00bb\u00bb\n-\n-    Result (German):\n-\n-        \u00ab\u00ab\u00ab\n-        <abbr title=\"Internet of Things \u2013 Internet der Dinge\">IoT</abbr>\n-        <abbr title=\"Central Processing Unit \u2013 Zentrale Verarbeitungseinheit\">CPU</abbr>\n-        <abbr title=\"too long; didn't read \u2013 zu lang; hab's nicht gelesen\"><strong>TL;DR:</strong></abbr>\n-        \u00bb\u00bb\u00bb\n-\n-1.1) If the language to which you translate mostly uses the letters of the ASCII char set (for example Spanish, French, German, but not Russian, Chinese) and if the translation of the full phrase is identical to, or starts with the same letters as the original full phrase, then only give the translation of the full phrase.\n-\n-Conversion scheme:\n-\n-    Source (English):\n-\n-        <abbr title=\"{full phrase}\">{abbreviation}</abbr>\n-\n-    Result:\n-\n-        <abbr title=\"{translation of full phrase}\">{abbreviation}</abbr>\n-\n-Examples:\n-\n-    Source (English):\n-\n-        \u00ab\u00ab\u00ab\n-        <abbr title=\"JSON Web Tokens\">JWT</abbr>\n-        <abbr title=\"Enumeration\">Enum</abbr>\n-        <abbr title=\"Asynchronous Server Gateway Interface\">ASGI</abbr>\n-        \u00bb\u00bb\u00bb\n-\n-    Result (German):\n-\n-        \u00ab\u00ab\u00ab\n-        <abbr title=\"JSON Web Tokens\">JWT</abbr>\n-        <abbr title=\"Enumeration\">Enum</abbr>\n-        <abbr title=\"Asynchrones Server-Gateway-Interface\">ASGI</abbr>\n-        \u00bb\u00bb\u00bb\n-\n-2) If the description is not a full phrase for an abbreviation which the abbr element surrounds, but some other information, then just translate the description.\n-\n-Conversion scheme:\n-\n-    Source (English):\n-\n-        <abbr title=\"{description}\">{text}</abbr>\n-\n-    Result:\n-\n-        <abbr title=\"{translation of description}\">{translation of text}</abbr>\n-\n-Examples:\n-\n-    Source (English):\n-\n-        \u00ab\u00ab\u00ab\n-        <abbr title=\"also known as: endpoints, routes\">path</abbr>\n-        <abbr title=\"a program that checks for code errors\">linter</abbr>\n-        <abbr title=\"converting the string that comes from an HTTP request into Python data\">parsing</abbr>\n-        <abbr title=\"before 2023-03\">0.95.0</abbr>\n-        <abbr title=\"2023-08-26\">at the time of writing this</abbr>\n-        \u00bb\u00bb\u00bb\n-\n-    Result (German):\n-\n-        \u00ab\u00ab\u00ab\n-        <abbr title=\"auch bekannt als: Endpunkte, Routen\">Pfad</abbr>\n-        <abbr title=\"Programm das auf Fehler im Code pr\u00fcft\">Linter</abbr>\n-        <abbr title=\"Konvertieren des Strings eines HTTP-Requests in Python-Daten\">Parsen</abbr>\n-        <abbr title=\"vor 2023-03\">0.95.0</abbr>\n-        <abbr title=\"2023-08-26\">zum Zeitpunkt als das hier geschrieben wurde</abbr>\n-        \u00bb\u00bb\u00bb\n-\n-\n-3) If the text surrounded by the abbr element is an abbreviation and the description contains both the full phrase for that abbreviation, and other information, separated by a colon (\u00ab:\u00bb), then append a dash (\u00ab\u2013\u00bb) and the translation of the full phrase to the original full phrase and translate the other information.\n-\n-Conversion scheme:\n-\n-    Source (English):\n-\n-        <abbr title=\"{full phrase}: {other information}\">{abbreviation}</abbr>\n-\n-    Result:\n-\n-        <abbr title=\"{full phrase} \u2013 {translation of full phrase}: {translation of other information}\">{abbreviation}</abbr>\n-\n-Examples:\n-\n-    Source (English):\n-\n-        \u00ab\u00ab\u00ab\n-        <abbr title=\"Input/Output: disk reading or writing, network communication.\">I/O</abbr>\n-        <abbr title=\"Content Delivery Network: service, that provides static files.\">CDN</abbr>\n-        <abbr title=\"Integrated Development Environment: similar to a code editor\">IDE</abbr>\n-        \u00bb\u00bb\u00bb\n-\n-    Result (German):\n-\n-        \u00ab\u00ab\u00ab\n-        <abbr title=\"Input/Output \u2013 Eingabe/Ausgabe: Lesen oder Schreiben auf der Festplatte, Netzwerkkommunikation.\">I/O</abbr>\n-        <abbr title=\"Content Delivery Network \u2013 Inhalte auslieferndes Netzwerk: Dienst, der statische Dateien bereitstellt.\">CDN</abbr>\n-        <abbr title=\"Integrated Development Environment \u2013 Integrierte Entwicklungsumgebung: \u00c4hnlich einem Code-Editor\">IDE</abbr>\n-        \u00bb\u00bb\u00bb\n-\n-3.1) Like in rule 2.1, you can leave the original full phrase away, if the translated full phrase is identical or starts with the same letters as the original full phrase.\n-\n-Conversion scheme:\n-\n-    Source (English):\n-\n-        <abbr title=\"{full phrase}: {information}\">{abbreviation}</abbr>\n-\n-    Result:\n-\n-        <abbr title=\"{translation of full phrase}: {translation of information}\">{abbreviation}</abbr>\n-\n-Example:\n-\n-    Source (English):\n-\n-        \u00ab\u00ab\u00ab\n-        <abbr title=\"Object Relational Mapper: a fancy term for a library where some classes represent SQL tables and instances represent rows in those tables\">ORM</abbr>\n-        \u00bb\u00bb\u00bb\n-\n-    Result (German):\n-\n-        \u00ab\u00ab\u00ab\n-        <abbr title=\"Objektrelationaler Mapper: Ein Fachbegriff f\u00fcr eine Bibliothek, in der einige Klassen SQL-Tabellen und Instanzen Zeilen in diesen Tabellen darstellen\">ORM</abbr>\n-        \u00bb\u00bb\u00bb\n-\n-4) If there is an existing translation, and it has ADDITIONAL abbr elements in a sentence, and these additional abbr elements do not exist in the related sentence in the English text, then KEEP those additional abbr elements in the translation. Do not remove them. Except when you remove the whole sentence from the translation, because the whole sentence was removed from the English text, then also remove the abbr element. The reasoning for this rule is, that such additional abbr elements are manually added by the human editor of the translation, in order to translate or explain an English word to the human readers of the translation. These additional abbr elements would not make sense in the English text, but they do make sense in the translation. So keep them in the translation, even though they are not part of the English text. This rule only applies to abbr elements.\n-\n-5) Apply above rules also when there is an existing translation! Make sure that all title attributes in abbr elements get properly translated or updated, using the schemes given above. However, leave the ADDITIONAL abbr's from rule 4 alone. Do not change their formatting or content.\n-\n-\"\"\"\n+general_prompt_path = Path(__file__).absolute().parent / \"llm-general-prompt.md\"",
      "comment": "There appears to be a typo. The filename should be `general-llm-prompt.md`.",
      "comment_id": 2676767606,
      "user": "DoctorJohn",
      "created_at": "2026-01-09T16:10:39Z",
      "url": "https://github.com/fastapi/fastapi/pull/14619#discussion_r2676767606"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14371,
      "file_path": "fastapi/params.py",
      "line": 118,
      "side": "RIGHT",
      "diff_hunk": "@@ -115,6 +115,10 @@ def __init__(\n         else:\n             kwargs[\"deprecated\"] = deprecated\n         if PYDANTIC_V2:\n+            if serialization_alias in (_Unset, None) and isinstance(alias, str):",
      "comment": "This is how it's done in Pydantic's Field: https://github.com/pydantic/pydantic/blob/407934af10962473d8db0b8c693393cad3975eda/pydantic/fields.py#L1346-L1350",
      "comment_id": 2541006268,
      "user": "YuriiMotov",
      "created_at": "2025-11-19T08:24:57Z",
      "url": "https://github.com/fastapi/fastapi/pull/14371#discussion_r2541006268"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14371,
      "file_path": "fastapi/dependencies/utils.py",
      "line": 1024,
      "side": "RIGHT",
      "diff_hunk": "@@ -1021,3 +1019,8 @@ def get_body_field(\n         field_info=BodyFieldInfo(**BodyFieldInfo_kwargs),\n     )\n     return final_field\n+\n+\n+def get_validation_alias(field: ModelField) -> str:",
      "comment": "It's fine like this for now, we'll soon remove support for Pydantic v1 and will be able to refactor this.",
      "comment_id": 2614498979,
      "user": "tiangolo",
      "created_at": "2025-12-12T14:56:32Z",
      "url": "https://github.com/fastapi/fastapi/pull/14371#discussion_r2614498979"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 797,
      "file_path": "fastapi/security/oauth2.py",
      "line": 191,
      "side": "RIGHT",
      "diff_hunk": "@@ -163,6 +163,45 @@ def __init__(\n         return param\n \n \n+class OAuth2AuthorizationCodeBearer(OAuth2):\n+    def __init__(\n+        self,\n+        authorizationUrl: str,\n+        tokenUrl: str,\n+        refreshUrl: str = None,\n+        scopes: dict = None,\n+        scheme_name: str = None,\n+        auto_error: bool = True,\n+    ):\n+        if not scopes:\n+            scopes = {}\n+        flows = OAuthFlowsModel(\n+            authorizationCode={\n+                \"authorizationUrl\": authorizationUrl,\n+                \"tokenUrl\": tokenUrl,\n+                \"refreshUrl\": refreshUrl,\n+                \"scopes\": scopes,\n+            })\n+        super().__init__(\n+            flows=flows, scheme_name=scheme_name, auto_error=auto_error\n+        )\n+\n+    async def __call__(self, request: Request) -> Optional[str]:\n+        authorization: str = request.headers.get(\"Authorization\")\n+        print(authorization)",
      "comment": "Presumably you didn't mean to keep this print statement there?",
      "comment_id": 359139146,
      "user": "dmontagu",
      "created_at": "2019-12-18T03:39:53Z",
      "url": "https://github.com/fastapi/fastapi/pull/797#discussion_r359139146"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14575,
      "file_path": "fastapi/routing.py",
      "line": 155,
      "side": "RIGHT",
      "diff_hunk": "@@ -153,16 +152,16 @@ def _prepare_response_content(\n     exclude_defaults: bool = False,\n     exclude_none: bool = False,\n ) -> Any:\n-    if isinstance(res, BaseModel):\n-        read_with_orm_mode = getattr(_get_model_config(res), \"read_with_orm_mode\", None)\n+    if isinstance(res, may_v1.BaseModel):",
      "comment": "This isn't quite the same. It causes v2 models to never be dumped to dicts, causing round-trips to error when using FastAPI with some other magical libraries that are quite popular.\r\n\r\nIt caused the regression described in https://github.com/fastapi/fastapi/pull/14575#issuecomment-3678630709.\r\n\r\nIn this particular case:\r\n- Cadwyn deeply copies the model class `ConnectionResponse` internally\r\n- The response from the endpoint is no longer a dict, but an instance of the original class `ConnectionResponse`\r\n- The validation fails because pydantic doesn't validate an instance of the copied class, but an instance of the original `ConnectionResponse` (but Cadwyn replaced type adapter's type with the copied class!)\r\n\r\n@tiangolo, was this intended to start dumping v1 models *only*, without v2 models, in `_prepare_response_content` since this change?",
      "comment_id": 2637871247,
      "user": "johnslavik",
      "created_at": "2025-12-21T14:49:04Z",
      "url": "https://github.com/fastapi/fastapi/pull/14575#discussion_r2637871247"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 13505,
      "file_path": "tests/test_tutorial/test_settings/test_tutorial001.py",
      "line": 21,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,14 +1,30 @@\n+import importlib\n+\n+import pytest\n from fastapi.testclient import TestClient\n from pytest import MonkeyPatch\n \n-from ...utils import needs_pydanticv2\n+from ...utils import needs_pydanticv1, needs_pydanticv2\n+\n+\n+@pytest.fixture(\n+    params=[\n+        pytest.param(\"tutorial001\", marks=needs_pydanticv2),\n+        pytest.param(\"tutorial001_pv1\", marks=needs_pydanticv1),\n+    ],\n+)\n+def get_app(request: pytest.FixtureRequest):\n+    def app_wrapper():\n+        mod = importlib.import_module(f\"docs_src.settings.{request.param}\")\n+        return mod.app\n+\n+    return app_wrapper",
      "comment": "I think that it is better to monkeypatch `get_app()`, instead of test_settings:\r\n\r\n```python\r\n\r\n@pytest.fixture(\r\n    name=\"app\",\r\n    params=[\r\n        pytest.param(\"tutorial001\", marks=needs_pydanticv2),\r\n        pytest.param(\"tutorial001_pv1\", marks=needs_pydanticv1),\r\n    ],\r\n)\r\ndef get_app(request: pytest.FixtureRequest, monkeypatch: MonkeyPatch):\r\n    monkeypatch.setenv(\"ADMIN_EMAIL\", \"admin@example.com\")\r\n    mod = importlib.import_module(f\"docs_src.settings.{request.param}\")\r\n    return mod.app\r\n\r\n\r\ndef test_settings(app):\r\n    client = TestClient(app)\r\n    response = client.get(\"/info\")\r\n    assert response.status_code == 200, response.text\r\n    assert response.json() == {\r\n        \"app_name\": \"Awesome API\",\r\n        \"admin_email\": \"admin@example.com\",\r\n        \"items_per_user\": 50,\r\n    }\r\n\r\n```",
      "comment_id": 2005481524,
      "user": "alv2017",
      "created_at": "2025-03-20T12:05:13Z",
      "url": "https://github.com/fastapi/fastapi/pull/13505#discussion_r2005481524"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14358,
      "file_path": "tests/test_request_params/test_file/test_list.py",
      "line": 60,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,845 @@\n+from typing import List\n+\n+import pytest\n+from dirty_equals import IsDict, IsOneOf, IsPartialDict\n+from fastapi import FastAPI, File, Form, UploadFile\n+from fastapi._compat import PYDANTIC_V2\n+from fastapi.testclient import TestClient\n+from pydantic import BaseModel\n+from typing_extensions import Annotated\n+\n+from tests.utils import needs_pydanticv2\n+\n+from .utils import get_body_model_name\n+\n+app = FastAPI()\n+\n+# =====================================================================================\n+# Without aliases\n+\n+\n+@app.post(\"/list-bytes\", operation_id=\"list_bytes\")\n+async def read_list_bytes(p: Annotated[List[bytes], File()]):\n+    return {\"file_size\": [len(file) for file in p]}\n+\n+\n+@app.post(\"/list-uploadfile\", operation_id=\"list_uploadfile\")\n+async def read_list_uploadfile(p: Annotated[List[UploadFile], File()]):\n+    return {\"file_size\": [file.size for file in p]}\n+\n+\n+class FormModelListBytes(BaseModel):\n+    p: List[bytes] = File()\n+\n+\n+@app.post(\"/model-list-bytes\", operation_id=\"model_list_bytes\")\n+async def read_model_list_bytes(\n+    p: Annotated[\n+        FormModelListBytes,\n+        Form(\n+            media_type=\"multipart/form-data\"  # Remove media_type when https://github.com/fastapi/fastapi/pull/14343 is fixed\n+        ),\n+    ],\n+):\n+    return {\"file_size\": [len(file) for file in p.p]}\n+\n+\n+class FormModelListUploadFile(BaseModel):\n+    p: List[UploadFile] = File()\n+\n+\n+@app.post(\"/model-list-uploadfile\", operation_id=\"model_list_uploadfile\")\n+async def read_model_list_uploadfile(\n+    p: Annotated[\n+        FormModelListUploadFile,\n+        Form(\n+            media_type=\"multipart/form-data\"  # Remove media_type when https://github.com/fastapi/fastapi/pull/14343 is fixed\n+        ),\n+    ],\n+):\n+    return {\"file_size\": [file.size for file in p.p]}",
      "comment": "Let's remove these. Pydantic models with fields that should be read from bytes are not supported in FastAPI, at least not yet, and I'm not sure if it would make sense supporting it. Currently it's not supported to use a different function to declare a field in a Pydantic model other than `Field()` (at least not intentionally), I would prefer to avoid that for now.",
      "comment_id": 2607250726,
      "user": "tiangolo",
      "created_at": "2025-12-10T15:59:09Z",
      "url": "https://github.com/fastapi/fastapi/pull/14358#discussion_r2607250726"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 11355,
      "file_path": "fastapi/dependencies/utils.py",
      "line": 198,
      "side": "RIGHT",
      "diff_hunk": "@@ -191,7 +192,10 @@ def get_flat_params(dependant: Dependant) -> List[ModelField]:\n \n \n def get_typed_signature(call: Callable[..., Any]) -> inspect.Signature:\n-    signature = inspect.signature(call)\n+    if sys.version_info >= (3, 10):\n+        signature = inspect.signature(call, eval_str=True)\n+    else:\n+        signature = inspect.signature(call)",
      "comment": "This change makes [jupyverse](https://github.com/jupyter-server/jupyverse) crash with:\n```\nNameError: name 'NoAuthStrategy' is not defined\n```\nWhich doesn't make sense to me, because `NoAuthStrategy` is defined [here](https://github.com/jupyter-server/jupyverse/blob/785564bee44fc340fb51ff078a8ca22783adcf3d/plugins/auth/src/fps_auth/backends.py#L67) and used in the same file.\nThere is also [fastapi-users](https://github.com/fastapi-users/fastapi-users) involved, so I'm pinging @frankie567 here but I don't think there's anything wrong there.",
      "comment_id": 2592995490,
      "user": "davidbrochart",
      "created_at": "2025-12-05T15:02:40Z",
      "url": "https://github.com/fastapi/fastapi/pull/11355#discussion_r2592995490"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 11355,
      "file_path": "fastapi/dependencies/utils.py",
      "line": 198,
      "side": "RIGHT",
      "diff_hunk": "@@ -191,7 +192,10 @@ def get_flat_params(dependant: Dependant) -> List[ModelField]:\n \n \n def get_typed_signature(call: Callable[..., Any]) -> inspect.Signature:\n-    signature = inspect.signature(call)\n+    if sys.version_info >= (3, 10):\n+        signature = inspect.signature(call, eval_str=True)\n+    else:\n+        signature = inspect.signature(call)",
      "comment": "Can you please share the full stacktrace and a reproducer script and how to run it?",
      "comment_id": 2593013316,
      "user": "tirkarthi",
      "created_at": "2025-12-05T15:08:21Z",
      "url": "https://github.com/fastapi/fastapi/pull/11355#discussion_r2593013316"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 11355,
      "file_path": "fastapi/dependencies/utils.py",
      "line": 198,
      "side": "RIGHT",
      "diff_hunk": "@@ -191,7 +192,10 @@ def get_flat_params(dependant: Dependant) -> List[ModelField]:\n \n \n def get_typed_signature(call: Callable[..., Any]) -> inspect.Signature:\n-    signature = inspect.signature(call)\n+    if sys.version_info >= (3, 10):\n+        signature = inspect.signature(call, eval_str=True)\n+    else:\n+        signature = inspect.signature(call)",
      "comment": "Removing `from __future__ import annotations` in this file seems to solve the issue.",
      "comment_id": 2593060054,
      "user": "davidbrochart",
      "created_at": "2025-12-05T15:22:03Z",
      "url": "https://github.com/fastapi/fastapi/pull/11355#discussion_r2593060054"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 11355,
      "file_path": "fastapi/dependencies/utils.py",
      "line": 198,
      "side": "RIGHT",
      "diff_hunk": "@@ -191,7 +192,10 @@ def get_flat_params(dependant: Dependant) -> List[ModelField]:\n \n \n def get_typed_signature(call: Callable[..., Any]) -> inspect.Signature:\n-    signature = inspect.signature(call)\n+    if sys.version_info >= (3, 10):\n+        signature = inspect.signature(call, eval_str=True)\n+    else:\n+        signature = inspect.signature(call)",
      "comment": "https://github.com/fastapi/fastapi/discussions/14464",
      "comment_id": 2593734061,
      "user": "edgarrmondragon",
      "created_at": "2025-12-05T19:22:26Z",
      "url": "https://github.com/fastapi/fastapi/pull/11355#discussion_r2593734061"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14434,
      "file_path": "fastapi/routing.py",
      "line": 83,
      "side": "RIGHT",
      "diff_hunk": "@@ -80,9 +80,9 @@\n from typing_extensions import Annotated, deprecated\n \n if sys.version_info >= (3, 13):  # pragma: no cover\n-    from inspect import iscoroutinefunction\n+    pass",
      "comment": "Good catch! This was pre-commit and I didn't notice it. Thanks!",
      "comment_id": 2582016616,
      "user": "tiangolo",
      "created_at": "2025-12-02T16:44:04Z",
      "url": "https://github.com/fastapi/fastapi/pull/14434#discussion_r2582016616"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 9753,
      "file_path": "fastapi/dependencies/utils.py",
      "line": 535,
      "side": "RIGHT",
      "diff_hunk": "@@ -528,24 +529,28 @@ def add_param_to_fields(*, field: ModelField, dependant: Dependant) -> None:\n \n \n def is_coroutine_callable(call: Callable[..., Any]) -> bool:\n-    if inspect.isroutine(call):\n-        return inspect.iscoroutinefunction(call)\n-    if inspect.isclass(call):\n-        return False\n+    if inspect.iscoroutinefunction(call):\n+        return True\n+    if isinstance(call, partial):\n+        return is_coroutine_callable(call.func)",
      "comment": "```suggestion\r\n    if inspect.isclass(call):\r\n        return False\r\n    if isinstance(call, partial):\r\n        return is_coroutine_callable(call.func)\r\n```\r\nWhy do we need to remove this condition (`isclass`)?\r\nI just tried running tests with these lines and it still works.\r\nI think if it's not necessary for making partial work, we shouldn't remove this lines in this PR",
      "comment_id": 2192922090,
      "user": "YuriiMotov",
      "created_at": "2025-07-08T16:11:16Z",
      "url": "https://github.com/fastapi/fastapi/pull/9753#discussion_r2192922090"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 9753,
      "file_path": "fastapi/dependencies/models.py",
      "line": 82,
      "side": "RIGHT",
      "diff_hunk": "@@ -53,25 +53,34 @@ def cache_key(self) -> DependencyCacheKey:\n \n     @cached_property\n     def is_gen_callable(self) -> bool:\n-        if inspect.isgeneratorfunction(self.call):\n+        use_call: Any = self.call\n+        if isinstance(self.call, partial):\n+            use_call = self.call.func\n+        if inspect.isgeneratorfunction(use_call):\n             return True\n-        dunder_call = getattr(self.call, \"__call__\", None)  # noqa: B004\n+        dunder_call = getattr(use_call, \"__call__\", None)  # noqa: B004\n         return inspect.isgeneratorfunction(dunder_call)\n \n     @cached_property\n     def is_async_gen_callable(self) -> bool:\n-        if inspect.isasyncgenfunction(self.call):\n+        use_call: Any = self.call\n+        if isinstance(self.call, partial):\n+            use_call = self.call.func\n+        if inspect.isasyncgenfunction(use_call):\n             return True\n-        dunder_call = getattr(self.call, \"__call__\", None)  # noqa: B004\n+        dunder_call = getattr(use_call, \"__call__\", None)  # noqa: B004\n         return inspect.isasyncgenfunction(dunder_call)\n \n     @cached_property\n     def is_coroutine_callable(self) -> bool:\n-        if inspect.isroutine(self.call):\n-            return iscoroutinefunction(self.call)\n-        if inspect.isclass(self.call):\n+        use_call: Any = self.call\n+        if isinstance(self.call, partial):\n+            use_call = self.call.func\n+        if inspect.isroutine(use_call):\n+            return iscoroutinefunction(use_call)\n+        if inspect.isclass(use_call):\n             return False",
      "comment": "In the initial version of this PR it was suggested to change these lines to something like:\r\n```diff\r\n-        if inspect.isroutine(use_call):\r\n-            return inspect.iscoroutinefunction(use_call)\r\n+        if inspect.iscoroutinefunction(use_call):\r\n+            return True\r\n-        if inspect.isclass(use_call):\r\n-            return False\r\n```\r\n\r\nSee original commit: https://github.com/fastapi/fastapi/pull/9753/commits/f16dd38b478c0a88ae4ae79fd73094fdb3ffa425\r\n\r\nI updated it to be closer to current implementation (don't simplify first condition and don't remove `if inspect.isclass(use_call):`)",
      "comment_id": 2493781785,
      "user": "YuriiMotov",
      "created_at": "2025-11-05T10:00:22Z",
      "url": "https://github.com/fastapi/fastapi/pull/9753#discussion_r2493781785"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 5077,
      "file_path": "fastapi/dependencies/utils.py",
      "line": 253,
      "side": "RIGHT",
      "diff_hunk": "@@ -245,7 +245,13 @@ def is_scalar_sequence_field(field: ModelField) -> bool:\n \n def get_typed_signature(call: Callable[..., Any]) -> inspect.Signature:\n     signature = inspect.signature(call)\n-    globalns = getattr(call, \"__globals__\", {})\n+    nsobj = call\n+    while hasattr(nsobj, \"__wrapped__\"):\n+        # The __wrapped__ attribute is set by decorators, e.g. functools.wraps.\n+        # This while loop allows rereferencing forward references on decorated\n+        # methods.\n+        nsobj = nsobj.__wrapped__  # type: ignore",
      "comment": "It can be simplified using `inspect.unwrap`:\r\n```suggestion\r\n    nsobj = inspect.unwrap(call)\r\n```",
      "comment_id": 906798942,
      "user": "uriyyo",
      "created_at": "2022-06-26T11:25:15Z",
      "url": "https://github.com/fastapi/fastapi/pull/5077#discussion_r906798942"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 5077,
      "file_path": "fastapi/dependencies/utils.py",
      "line": 253,
      "side": "RIGHT",
      "diff_hunk": "@@ -245,7 +245,13 @@ def is_scalar_sequence_field(field: ModelField) -> bool:\n \n def get_typed_signature(call: Callable[..., Any]) -> inspect.Signature:\n     signature = inspect.signature(call)\n-    globalns = getattr(call, \"__globals__\", {})\n+    nsobj = call\n+    while hasattr(nsobj, \"__wrapped__\"):\n+        # The __wrapped__ attribute is set by decorators, e.g. functools.wraps.\n+        # This while loop allows rereferencing forward references on decorated\n+        # methods.\n+        nsobj = nsobj.__wrapped__  # type: ignore",
      "comment": "Looks like that was added in python 3.4 and the earliest supported version of python for FastAPI is 3.6, so sure. It also checks for cycles (which `typing.get_type_hints` does not).",
      "comment_id": 906853334,
      "user": "lucaswiman",
      "created_at": "2022-06-26T18:06:47Z",
      "url": "https://github.com/fastapi/fastapi/pull/5077#discussion_r906853334"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 13207,
      "file_path": "fastapi/_compat.py",
      "line": 267,
      "side": "RIGHT",
      "diff_hunk": "@@ -238,6 +235,59 @@ def get_definitions(\n                 item_def[\"description\"] = item_description\n         return field_mapping, definitions  # type: ignore[return-value]\n \n+    def get_definitions(\n+        *,\n+        fields: List[ModelField],\n+        schema_generator: GenerateJsonSchema,\n+        model_name_map: ModelNameMap,\n+        separate_input_output_schemas: bool = True,\n+    ) -> Tuple[\n+        Dict[\n+            Tuple[ModelField, Literal[\"validation\", \"serialization\"]], JsonSchemaValue\n+        ],\n+        Dict[str, Dict[str, Any]],\n+    ]:\n+        if separate_input_output_schemas:\n+            # No override mode\n+            return _get_definitions_with_override_mode(\n+                fields=fields,\n+                schema_generator=schema_generator,\n+                override_mode=None,\n+            )\n+\n+        # Set override to 'validation' as baseline\n+        field_mapping, base_definitions = _get_definitions_with_override_mode(\n+            fields=fields,\n+            schema_generator=schema_generator,\n+            override_mode=\"validation\",\n+        )\n+\n+        # Set override to 'serialization' to be able to add computed fields to the output\n+        # Create a new schema generator as it can't be reused\n+        schema_generator_2 = GenerateJsonSchema(ref_template=REF_TEMPLATE)",
      "comment": "Not too happy with this solution, perhaps we should copy the given `schema_generator` instead?",
      "comment_id": 2288512317,
      "user": "svlandeg",
      "created_at": "2025-08-20T15:17:41Z",
      "url": "https://github.com/fastapi/fastapi/pull/13207#discussion_r2288512317"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 13537,
      "file_path": "tests/test_form_default.py",
      "line": 19,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,47 @@\n+from typing import Optional\n+\n+from fastapi import FastAPI, File, Form\n+from starlette.testclient import TestClient\n+from typing_extensions import Annotated\n+\n+app = FastAPI()\n+client = TestClient(app)\n+\n+\n+@app.post(\"/urlencoded\")\n+async def post_url_encoded(age: Annotated[Optional[int], Form()] = None):\n+    return age\n+\n+\n+def test_form_default_url_encoded():\n+    response = client.post(\"/urlencoded\", data={\"age\": \"\"})\n+    assert response.status_code == 200\n+    assert response.text == \"null\"",
      "comment": "This looks odd to me.\r\nIf you pass a string to an integer field, I would expect a `4xx` error code.",
      "comment_id": 2076095083,
      "user": "PidgeyBE",
      "created_at": "2025-05-06T19:11:16Z",
      "url": "https://github.com/fastapi/fastapi/pull/13537#discussion_r2076095083"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 13537,
      "file_path": "tests/test_form_default.py",
      "line": 19,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,47 @@\n+from typing import Optional\n+\n+from fastapi import FastAPI, File, Form\n+from starlette.testclient import TestClient\n+from typing_extensions import Annotated\n+\n+app = FastAPI()\n+client = TestClient(app)\n+\n+\n+@app.post(\"/urlencoded\")\n+async def post_url_encoded(age: Annotated[Optional[int], Form()] = None):\n+    return age\n+\n+\n+def test_form_default_url_encoded():\n+    response = client.post(\"/urlencoded\", data={\"age\": \"\"})\n+    assert response.status_code == 200\n+    assert response.text == \"null\"",
      "comment": "This is a form POST and empty numeric inputs would be submitted as empty strings. IMO it's reasonable that they are accepted and treated like *no value* (`None` / `null`) when the field is an optional integer.",
      "comment_id": 2079233102,
      "user": "schneebuzz",
      "created_at": "2025-05-08T09:02:21Z",
      "url": "https://github.com/fastapi/fastapi/pull/13537#discussion_r2079233102"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 12935,
      "file_path": "fastapi/encoders.py",
      "line": 53,
      "side": "RIGHT",
      "diff_hunk": "@@ -48,8 +48,12 @@ def decimal_encoder(dec_value: Decimal) -> Union[int, float]:\n \n     >>> decimal_encoder(Decimal(\"1\"))\n     1\n+    \n+    >>> decimal_encoder(Decimal(\"NaN\"))\n+    nan",
      "comment": "The docstring example should show the actual float NaN output format. In Python, `float('nan')` displays as `nan` but the docstring should clarify this is a float value, not a string.\n```suggestion\n    nan  # float\n```",
      "comment_id": 2342890747,
      "user": "Copilot",
      "created_at": "2025-09-12T04:05:41Z",
      "url": "https://github.com/fastapi/fastapi/pull/12935#discussion_r2342890747"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 5624,
      "file_path": "fastapi/dependencies/utils.py",
      "line": 134,
      "side": "RIGHT",
      "diff_hunk": "@@ -131,7 +131,7 @@ def get_parameterless_sub_dependant(*, depends: params.Depends, path: str) -> De\n     )\n     use_security_scopes: List[str] = []\n     if isinstance(depends, params.Security) and depends.scopes:\n-        use_security_scopes.extend(depends.scopes)\n+        use_security_scopes = use_security_scopes + list(depends.scopes)",
      "comment": "revert \r\n```suggestion\r\n        use_security_scopes.extend(depends.scopes)\r\n```",
      "comment_id": 2481833287,
      "user": "svlandeg",
      "created_at": "2025-10-31T15:35:23Z",
      "url": "https://github.com/fastapi/fastapi/pull/5624#discussion_r2481833287"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14033,
      "file_path": "scripts/docs.py",
      "line": 33,
      "side": "RIGHT",
      "diff_hunk": "@@ -27,7 +29,7 @@\n {!../../docs/missing-translation.md!}\n \"\"\"\n \n-non_translated_sections = [\n+non_translated_sections = (\n     \"reference/\",",
      "comment": "To be OS agnostic:\r\n```suggestion\r\n    \"reference\",\r\n```\r\nassuming we have no \"normal\" pages starting with \"reference\" (I don't think that's a real issue)",
      "comment_id": 2371531886,
      "user": "svlandeg",
      "created_at": "2025-09-23T08:14:49Z",
      "url": "https://github.com/fastapi/fastapi/pull/14033#discussion_r2371531886"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14033,
      "file_path": "scripts/docs.py",
      "line": 33,
      "side": "RIGHT",
      "diff_hunk": "@@ -27,7 +29,7 @@\n {!../../docs/missing-translation.md!}\n \"\"\"\n \n-non_translated_sections = [\n+non_translated_sections = (\n     \"reference/\",",
      "comment": "> Why is that we exclude the reference directory?\r\n\r\nI guess that's because it's generated from sources? If we decided to translate it, we would need to update all translations every time code changes..\r\n\r\nAs for slash in the end - good catch!\r\nBut instead of removing it, I would go with `f\"reference{os.sep}\",`",
      "comment_id": 2371616410,
      "user": "YuriiMotov",
      "created_at": "2025-09-23T08:50:57Z",
      "url": "https://github.com/fastapi/fastapi/pull/14033#discussion_r2371616410"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14033,
      "file_path": "scripts/docs.py",
      "line": 33,
      "side": "RIGHT",
      "diff_hunk": "@@ -27,7 +29,7 @@\n {!../../docs/missing-translation.md!}\n \"\"\"\n \n-non_translated_sections = [\n+non_translated_sections = (\n     \"reference/\",",
      "comment": "Looked at code and realized that there are a lot of other occurrences of hardcoded `/`.\r\nSo, we need to decide whether we want to refactor it in this PR on in separate PR?",
      "comment_id": 2371631610,
      "user": "YuriiMotov",
      "created_at": "2025-09-23T08:57:02Z",
      "url": "https://github.com/fastapi/fastapi/pull/14033#discussion_r2371631610"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14033,
      "file_path": "scripts/docs.py",
      "line": 33,
      "side": "RIGHT",
      "diff_hunk": "@@ -27,7 +29,7 @@\n {!../../docs/missing-translation.md!}\n \"\"\"\n \n-non_translated_sections = [\n+non_translated_sections = (\n     \"reference/\",",
      "comment": "I suggest we (only) update the hardcoded slashes of the new code that is contributed with this PR. Because when I run this new script on my Windows machine, it starts generating permalinks for the `reference` docs and we don't want to do deal with any PRs of users that get the same thing.\r\n\r\nWith respect to hardcoded slashes already in the current code base: let's edit those in a separate PR.",
      "comment_id": 2371748044,
      "user": "svlandeg",
      "created_at": "2025-09-23T09:45:15Z",
      "url": "https://github.com/fastapi/fastapi/pull/14033#discussion_r2371748044"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14033,
      "file_path": "scripts/docs.py",
      "line": 33,
      "side": "RIGHT",
      "diff_hunk": "@@ -27,7 +29,7 @@\n {!../../docs/missing-translation.md!}\n \"\"\"\n \n-non_translated_sections = [\n+non_translated_sections = (\n     \"reference/\",",
      "comment": "> But instead of removing it, I would go with f\"reference{os.sep}\",\r\n\r\nThat's fine too by me!",
      "comment_id": 2371749205,
      "user": "svlandeg",
      "created_at": "2025-09-23T09:45:45Z",
      "url": "https://github.com/fastapi/fastapi/pull/14033#discussion_r2371749205"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14186,
      "file_path": "fastapi/_compat/may_v1.py",
      "line": 6,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,123 @@\n+import sys\n+from typing import Any, Dict, List, Literal, Sequence, Tuple, Type, Union\n+\n+from fastapi.types import ModelNameMap\n+\n+if sys.version_info >= (3, 14):",
      "comment": "Probably better to check directly for Pydantic 2.12.0+ ?\r\n\r\nThen again, Pydantic themselves say \r\n\r\n> The core functionality of Pydantic V1 is not compatible with Python 3.14 or greater.",
      "comment_id": 2432655612,
      "user": "svlandeg",
      "created_at": "2025-10-15T13:52:08Z",
      "url": "https://github.com/fastapi/fastapi/pull/14186#discussion_r2432655612"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14036,
      "file_path": "fastapi/_compat.py",
      "line": 117,
      "side": "RIGHT",
      "diff_hunk": "@@ -109,9 +110,24 @@ def type_(self) -> Any:\n             return self.field_info.annotation\n \n         def __post_init__(self) -> None:\n-            self._type_adapter: TypeAdapter[Any] = TypeAdapter(\n-                Annotated[self.field_info.annotation, self.field_info]\n-            )\n+            with warnings.catch_warnings():\n+                # Pydantic >= 2.12.0a1 warns about this when building\n+                # TypeAdapters from field information that uses aliases.\n+                # The Pydantic team recommends ignoring this in this case:\n+                # https://github.com/fastapi/fastapi/pull/14036#issuecomment-3316045587",
      "comment": "```suggestion\r\n                # Pydantic >= 2.12.0 warns about field specific metadata that is unused\r\n                # (e.g. `TypeAdapter(Annotated[int, Field(alias='b')])`). In some cases, we\r\n                # end up building the type adapter from a model field annotation so we\r\n                # need to ignore the warning:",
      "comment_id": 2369192894,
      "user": "Viicos",
      "created_at": "2025-09-22T16:05:41Z",
      "url": "https://github.com/fastapi/fastapi/pull/14036#discussion_r2369192894"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14036,
      "file_path": "fastapi/_compat.py",
      "line": 130,
      "side": "RIGHT",
      "diff_hunk": "@@ -109,9 +110,24 @@ def type_(self) -> Any:\n             return self.field_info.annotation\n \n         def __post_init__(self) -> None:\n-            self._type_adapter: TypeAdapter[Any] = TypeAdapter(\n-                Annotated[self.field_info.annotation, self.field_info]\n-            )\n+            with warnings.catch_warnings():\n+                # Pydantic >= 2.12.0a1 warns about this when building\n+                # TypeAdapters from field information that uses aliases.\n+                # The Pydantic team recommends ignoring this in this case:\n+                # https://github.com/fastapi/fastapi/pull/14036#issuecomment-3316045587\n+                try:\n+                    from pydantic.warnings import (  # type: ignore[attr-defined]\n+                        UnsupportedFieldAttributeWarning,\n+                    )\n+                except ImportError:  # pragma: no cover\n+                    pass\n+                else:  # pragma: no cover\n+                    warnings.simplefilter(\n+                        \"ignore\", category=UnsupportedFieldAttributeWarning\n+                    )\n+                self._type_adapter: TypeAdapter[Any] = TypeAdapter(\n+                    Annotated[self.field_info.annotation, self.field_info]\n+                )",
      "comment": "- I believe you need to use the `catch_warnings` context manager, because as it stands the warning will always be ignored from now on.\r\n- I would suggest using an explicit version check on the `pydantic` version instead of a `try..except` statement, so that it can easily be unconditionally imported once FastAPI drops support for Pydantic < 2.12.",
      "comment_id": 2369201471,
      "user": "Viicos",
      "created_at": "2025-09-22T16:07:36Z",
      "url": "https://github.com/fastapi/fastapi/pull/14036#discussion_r2369201471"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14036,
      "file_path": "tests/test_multi_body_errors.py",
      "line": 189,
      "side": "RIGHT",
      "diff_hunk": "@@ -185,7 +185,15 @@ def test_openapi_schema():\n                                 \"title\": \"Age\",\n                                 \"anyOf\": [\n                                     {\"exclusiveMinimum\": 0.0, \"type\": \"number\"},\n-                                    {\"type\": \"string\"},\n+                                    IsOneOf(\n+                                        # pydantic < 2.12.0a1",
      "comment": "```suggestion\r\n                                        # pydantic < 2.12.0\r\n```",
      "comment_id": 2369202655,
      "user": "Viicos",
      "created_at": "2025-09-22T16:07:49Z",
      "url": "https://github.com/fastapi/fastapi/pull/14036#discussion_r2369202655"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14036,
      "file_path": "tests/test_multi_body_errors.py",
      "line": 191,
      "side": "RIGHT",
      "diff_hunk": "@@ -185,7 +185,15 @@ def test_openapi_schema():\n                                 \"title\": \"Age\",\n                                 \"anyOf\": [\n                                     {\"exclusiveMinimum\": 0.0, \"type\": \"number\"},\n-                                    {\"type\": \"string\"},\n+                                    IsOneOf(\n+                                        # pydantic < 2.12.0a1\n+                                        {\"type\": \"string\"},\n+                                        # pydantic >= 2.12.0a1",
      "comment": "```suggestion\r\n                                        # pydantic >= 2.12.0\r\n```",
      "comment_id": 2369203300,
      "user": "Viicos",
      "created_at": "2025-09-22T16:07:58Z",
      "url": "https://github.com/fastapi/fastapi/pull/14036#discussion_r2369203300"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14036,
      "file_path": "fastapi/_compat.py",
      "line": 130,
      "side": "RIGHT",
      "diff_hunk": "@@ -109,9 +110,24 @@ def type_(self) -> Any:\n             return self.field_info.annotation\n \n         def __post_init__(self) -> None:\n-            self._type_adapter: TypeAdapter[Any] = TypeAdapter(\n-                Annotated[self.field_info.annotation, self.field_info]\n-            )\n+            with warnings.catch_warnings():\n+                # Pydantic >= 2.12.0a1 warns about this when building\n+                # TypeAdapters from field information that uses aliases.\n+                # The Pydantic team recommends ignoring this in this case:\n+                # https://github.com/fastapi/fastapi/pull/14036#issuecomment-3316045587\n+                try:\n+                    from pydantic.warnings import (  # type: ignore[attr-defined]\n+                        UnsupportedFieldAttributeWarning,\n+                    )\n+                except ImportError:  # pragma: no cover\n+                    pass\n+                else:  # pragma: no cover\n+                    warnings.simplefilter(\n+                        \"ignore\", category=UnsupportedFieldAttributeWarning\n+                    )\n+                self._type_adapter: TypeAdapter[Any] = TypeAdapter(\n+                    Annotated[self.field_info.annotation, self.field_info]\n+                )",
      "comment": "I'm already using the `catch_warnings` context manager just above, so I'm confused about your first point.\r\n\r\nI've made your suggested second change.",
      "comment_id": 2378822890,
      "user": "cjwatson",
      "created_at": "2025-09-25T11:49:02Z",
      "url": "https://github.com/fastapi/fastapi/pull/14036#discussion_r2378822890"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14036,
      "file_path": "fastapi/_compat.py",
      "line": 118,
      "side": "RIGHT",
      "diff_hunk": "@@ -109,9 +110,22 @@ def type_(self) -> Any:\n             return self.field_info.annotation\n \n         def __post_init__(self) -> None:\n-            self._type_adapter: TypeAdapter[Any] = TypeAdapter(\n-                Annotated[self.field_info.annotation, self.field_info]\n-            )\n+            with warnings.catch_warnings():\n+                # Pydantic >= 2.12.0 warns about field specific metadata that is unused\n+                # (e.g. `TypeAdapter(Annotated[int, Field(alias='b')])`). In some cases, we\n+                # end up building the type adapter from a model field annotation so we\n+                # need to ignore the warning:\n+                if PYDANTIC_VERSION_MINOR_TUPLE >= (2, 12):  # pragma: no cover",
      "comment": "Once Pydantic 2.12 is released, we'll be able to remove the `no cover` statement here.",
      "comment_id": 2410097108,
      "user": "svlandeg",
      "created_at": "2025-10-07T10:04:18Z",
      "url": "https://github.com/fastapi/fastapi/pull/14036#discussion_r2410097108"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14036,
      "file_path": "fastapi/_compat.py",
      "line": 123,
      "side": "RIGHT",
      "diff_hunk": "@@ -109,9 +110,22 @@ def type_(self) -> Any:\n             return self.field_info.annotation\n \n         def __post_init__(self) -> None:\n-            self._type_adapter: TypeAdapter[Any] = TypeAdapter(\n-                Annotated[self.field_info.annotation, self.field_info]\n-            )\n+            with warnings.catch_warnings():\n+                # Pydantic >= 2.12.0 warns about field specific metadata that is unused\n+                # (e.g. `TypeAdapter(Annotated[int, Field(alias='b')])`). In some cases, we\n+                # end up building the type adapter from a model field annotation so we\n+                # need to ignore the warning:\n+                if PYDANTIC_VERSION_MINOR_TUPLE >= (2, 12):  # pragma: no cover\n+                    from pydantic.warnings import (  # type: ignore[attr-defined]\n+                        UnsupportedFieldAttributeWarning,\n+                    )\n+\n+                    warnings.simplefilter(",
      "comment": "This is still incorrect, as per my first point [here](https://github.com/fastapi/fastapi/pull/14036#discussion_r2369201471).",
      "comment_id": 2410098702,
      "user": "Viicos",
      "created_at": "2025-10-07T10:04:56Z",
      "url": "https://github.com/fastapi/fastapi/pull/14036#discussion_r2410098702"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14036,
      "file_path": "fastapi/_compat.py",
      "line": 118,
      "side": "RIGHT",
      "diff_hunk": "@@ -109,9 +110,22 @@ def type_(self) -> Any:\n             return self.field_info.annotation\n \n         def __post_init__(self) -> None:\n-            self._type_adapter: TypeAdapter[Any] = TypeAdapter(\n-                Annotated[self.field_info.annotation, self.field_info]\n-            )\n+            with warnings.catch_warnings():\n+                # Pydantic >= 2.12.0 warns about field specific metadata that is unused\n+                # (e.g. `TypeAdapter(Annotated[int, Field(alias='b')])`). In some cases, we\n+                # end up building the type adapter from a model field annotation so we\n+                # need to ignore the warning:\n+                if PYDANTIC_VERSION_MINOR_TUPLE >= (2, 12):  # pragma: no cover",
      "comment": "(I know, but we have a bit of a chicken-or-egg situation going on here \ud83d\ude09)",
      "comment_id": 2410125638,
      "user": "svlandeg",
      "created_at": "2025-10-07T10:14:55Z",
      "url": "https://github.com/fastapi/fastapi/pull/14036#discussion_r2410125638"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14036,
      "file_path": "fastapi/_compat.py",
      "line": 118,
      "side": "RIGHT",
      "diff_hunk": "@@ -109,9 +110,22 @@ def type_(self) -> Any:\n             return self.field_info.annotation\n \n         def __post_init__(self) -> None:\n-            self._type_adapter: TypeAdapter[Any] = TypeAdapter(\n-                Annotated[self.field_info.annotation, self.field_info]\n-            )\n+            with warnings.catch_warnings():\n+                # Pydantic >= 2.12.0 warns about field specific metadata that is unused\n+                # (e.g. `TypeAdapter(Annotated[int, Field(alias='b')])`). In some cases, we\n+                # end up building the type adapter from a model field annotation so we\n+                # need to ignore the warning:\n+                if PYDANTIC_VERSION_MINOR_TUPLE >= (2, 12):  # pragma: no cover",
      "comment": "Yeah no worries, just wanted to give you a release estimate!",
      "comment_id": 2410138271,
      "user": "Viicos",
      "created_at": "2025-10-07T10:19:31Z",
      "url": "https://github.com/fastapi/fastapi/pull/14036#discussion_r2410138271"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14036,
      "file_path": "fastapi/_compat.py",
      "line": 123,
      "side": "RIGHT",
      "diff_hunk": "@@ -109,9 +110,22 @@ def type_(self) -> Any:\n             return self.field_info.annotation\n \n         def __post_init__(self) -> None:\n-            self._type_adapter: TypeAdapter[Any] = TypeAdapter(\n-                Annotated[self.field_info.annotation, self.field_info]\n-            )\n+            with warnings.catch_warnings():\n+                # Pydantic >= 2.12.0 warns about field specific metadata that is unused\n+                # (e.g. `TypeAdapter(Annotated[int, Field(alias='b')])`). In some cases, we\n+                # end up building the type adapter from a model field annotation so we\n+                # need to ignore the warning:\n+                if PYDANTIC_VERSION_MINOR_TUPLE >= (2, 12):  # pragma: no cover\n+                    from pydantic.warnings import (  # type: ignore[attr-defined]\n+                        UnsupportedFieldAttributeWarning,\n+                    )\n+\n+                    warnings.simplefilter(",
      "comment": "Hi @Viicos, thanks for the additional review! \ud83d\ude4f \r\n\r\nCan you elaborate on your point?\r\n\r\nThe current code uses `warnings.simplefilter` within a `with warnings.catch_warnings()` context manager. It is my understanding that, once the context manager exits, after L128, the warnings filter is restored to its original state when the context was entered (L113) ?",
      "comment_id": 2410162259,
      "user": "svlandeg",
      "created_at": "2025-10-07T10:28:15Z",
      "url": "https://github.com/fastapi/fastapi/pull/14036#discussion_r2410162259"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14036,
      "file_path": "fastapi/_compat.py",
      "line": 123,
      "side": "RIGHT",
      "diff_hunk": "@@ -109,9 +110,22 @@ def type_(self) -> Any:\n             return self.field_info.annotation\n \n         def __post_init__(self) -> None:\n-            self._type_adapter: TypeAdapter[Any] = TypeAdapter(\n-                Annotated[self.field_info.annotation, self.field_info]\n-            )\n+            with warnings.catch_warnings():\n+                # Pydantic >= 2.12.0 warns about field specific metadata that is unused\n+                # (e.g. `TypeAdapter(Annotated[int, Field(alias='b')])`). In some cases, we\n+                # end up building the type adapter from a model field annotation so we\n+                # need to ignore the warning:\n+                if PYDANTIC_VERSION_MINOR_TUPLE >= (2, 12):  # pragma: no cover\n+                    from pydantic.warnings import (  # type: ignore[attr-defined]\n+                        UnsupportedFieldAttributeWarning,\n+                    )\n+\n+                    warnings.simplefilter(",
      "comment": "My bad, I always get confused by this `warnings` API, as the context manager isn't opened right on top of the filter. All good!",
      "comment_id": 2410177447,
      "user": "Viicos",
      "created_at": "2025-10-07T10:34:50Z",
      "url": "https://github.com/fastapi/fastapi/pull/14036#discussion_r2410177447"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 11194,
      "file_path": "tests/test_request_form_multiple_files.py",
      "line": 1,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,175 @@\n+from typing import Annotated",
      "comment": "```suggestion\r\nfrom typing_extensions import Annotated\r\n```",
      "comment_id": 1501844755,
      "user": "YuriiMotov",
      "created_at": "2024-02-25T15:35:49Z",
      "url": "https://github.com/fastapi/fastapi/pull/11194#discussion_r1501844755"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 11194,
      "file_path": "tests/test_request_form_file.py",
      "line": 1,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,139 @@\n+from typing import Annotated",
      "comment": "```suggestion\r\nfrom typing_extensions import Annotated\r\n```",
      "comment_id": 1501844886,
      "user": "YuriiMotov",
      "created_at": "2024-02-25T15:36:50Z",
      "url": "https://github.com/fastapi/fastapi/pull/11194#discussion_r1501844886"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 11194,
      "file_path": "tests/test_request_form_file.py",
      "line": 100,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,139 @@\n+from typing import Annotated\n+\n+from fastapi import FastAPI, File, Form\n+from fastapi.testclient import TestClient\n+\n+app = FastAPI()\n+\n+\n+@app.post(\"/files/\")\n+async def create_file(token: Annotated[str, Form()], file: Annotated[bytes, File()]):\n+    return {\n+        \"file_size\": len(file),\n+        \"token\": token,\n+    }\n+\n+\n+client = TestClient(app)\n+\n+openapi_schema = {\n+    \"openapi\": \"3.1.0\",\n+    \"info\": {\"title\": \"FastAPI\", \"version\": \"0.1.0\"},\n+    \"paths\": {\n+        \"/files/\": {\n+            \"post\": {\n+                \"summary\": \"Create File\",\n+                \"operationId\": \"create_file_files__post\",\n+                \"requestBody\": {\n+                    \"content\": {\n+                        \"multipart/form-data\": {\n+                            \"schema\": {\n+                                \"$ref\": \"#/components/schemas/Body_create_file_files__post\"\n+                            }\n+                        }\n+                    },\n+                    \"required\": True,\n+                },\n+                \"responses\": {\n+                    \"200\": {\n+                        \"description\": \"Successful Response\",\n+                        \"content\": {\"application/json\": {\"schema\": {}}},\n+                    },\n+                    \"422\": {\n+                        \"description\": \"Validation Error\",\n+                        \"content\": {\n+                            \"application/json\": {\n+                                \"schema\": {\n+                                    \"$ref\": \"#/components/schemas/HTTPValidationError\"\n+                                }\n+                            }\n+                        },\n+                    },\n+                },\n+            }\n+        }\n+    },\n+    \"components\": {\n+        \"schemas\": {\n+            \"Body_create_file_files__post\": {\n+                \"title\": \"Body_create_file_files__post\",\n+                \"required\": [\"token\", \"file\"],\n+                \"type\": \"object\",\n+                \"properties\": {\n+                    \"token\": {\"title\": \"Token\", \"type\": \"string\"},\n+                    \"file\": {\"title\": \"File\", \"type\": \"string\", \"format\": \"binary\"},\n+                },\n+            },\n+            \"HTTPValidationError\": {\n+                \"title\": \"HTTPValidationError\",\n+                \"type\": \"object\",\n+                \"properties\": {\n+                    \"detail\": {\n+                        \"title\": \"Detail\",\n+                        \"type\": \"array\",\n+                        \"items\": {\"$ref\": \"#/components/schemas/ValidationError\"},\n+                    }\n+                },\n+            },\n+            \"ValidationError\": {\n+                \"title\": \"ValidationError\",\n+                \"required\": [\"loc\", \"msg\", \"type\"],\n+                \"type\": \"object\",\n+                \"properties\": {\n+                    \"loc\": {\n+                        \"title\": \"Location\",\n+                        \"type\": \"array\",\n+                        \"items\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"integer\"}]},\n+                    },\n+                    \"msg\": {\"title\": \"Message\", \"type\": \"string\"},\n+                    \"type\": {\"title\": \"Error Type\", \"type\": \"string\"},\n+                },\n+            },\n+        }\n+    },\n+}\n+\n+\n+def test_openapi_schema():\n+    response = client.get(\"/openapi.json\")\n+    assert response.status_code == 200, response.text\n+    assert response.json() == openapi_schema",
      "comment": "```suggestion\r\nbody_create_file_files__post = {\r\n    \"title\": \"Body_create_file_files__post\",\r\n    \"required\": [\"token\", \"file\"],\r\n    \"type\": \"object\",\r\n    \"properties\": {\r\n        \"token\": {\"title\": \"Token\", \"type\": \"string\"},\r\n        \"file\": {\"title\": \"File\", \"type\": \"string\", \"format\": \"binary\"},\r\n    },\r\n}\r\n\r\n\r\ndef test_openapi_schema():\r\n    response = client.get(\"/openapi.json\")\r\n    assert response.status_code == 200, response.text\r\n    json_response = response.json()\r\n    assert (\r\n        json_response[\"components\"][\"schemas\"][\"Body_create_file_files__post\"]\r\n        == body_create_file_files__post\r\n    )\r\n```\r\nWe can make it more readable",
      "comment_id": 1501846593,
      "user": "YuriiMotov",
      "created_at": "2024-02-25T15:46:53Z",
      "url": "https://github.com/fastapi/fastapi/pull/11194#discussion_r1501846593"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14015,
      "file_path": "scripts/translate.py",
      "line": 512,
      "side": "RIGHT",
      "diff_hunk": "@@ -22,55 +24,642 @@\n     \"contributing.md\",\n )\n \n-\n general_prompt = \"\"\"\n-For technical terms in English that don't have a common translation term use the original term in English.\n+### About literal text in this prompt\n+\n+1) In the following instructions (after I say: `The above rules are in effect now`) the two characters `\u00ab` and `\u00bb` will be used to surround LITERAL TEXT, which is text or characters you shall interpret literally. The `\u00ab` and the `\u00bb` are not part of the literal text, they are the meta characters denoting it.\n+\n+2) Furthermore, text surrounded by `\u00ab\u00ab\u00ab` and `\u00bb\u00bb\u00bb` is a BLOCK OF LITERAL TEXT which spans multiple lines. To get its content, dedent all lines of the block until the `\u00ab\u00ab\u00ab` and `\u00bb\u00bb\u00bb` are at column zero, then remove the newline (`\\n`) after the `\u00ab\u00ab\u00ab` and the newline before the `\u00bb\u00bb\u00bb`. The `\u00ab\u00ab\u00ab` and the `\u00bb\u00bb\u00bb` are not part of the literal text block, they are the meta characters denoting it.\n+\n+3) If you see backticks inside literal text \u2013 inside `\u00ab` and `\u00bb` \u2013  or inside blocks of literal text \u2013 inside `\u00ab\u00ab\u00ab` and `\u00bb\u00bb\u00bb` \u2013 then interpret them as literal characters, do NOT interpret them as meta characters.\n+\n+The above rules are in effect now.\n+\n+\n+### Definitions of terms used in this prompt\n+\n+\"backtick\"\n+\n+    The character \u00ab`\u00bb\n+    Unicode U+0060 (GRAVE ACCENT)\n+\n+\"single backtick\"\n+\n+    A single backtick \u2013 \u00ab`\u00bb\n+\n+\"triple backticks\"\n+\n+    Three backticks in a row \u2013 \u00ab```\u00bb\n+\n+\"neutral double quote\"\n+\n+    The character \u00ab\"\u00bb\n+    Unicode U+0022 (QUOTATION MARK)\n+\n+\"neutral single quote\"\n+\n+    The character \u00ab'\u00bb\n+    Unicode U+0027 (APOSTROPHE)\n+\n+\"English double typographic quotes\"\n+\n+    The characters \u00ab\u201c\u00bb and \u00ab\u201d\u00bb\n+    Unicode U+201C (LEFT DOUBLE QUOTATION MARK) and Unicode U+201D (RIGHT DOUBLE QUOTATION MARK)\n+\n+\"English single typographic quotes\"\n+\n+    The characters \u00ab\u2018\u00bb and \u00ab\u2019\u00bb\n+    Unicode U+2018 (LEFT SINGLE QUOTATION MARK) and Unicode U+2019 (RIGHT SINGLE QUOTATION MARK)\n+\n+\"code snippet\"\n+\n+    Also called \"inline code\". Text in a Markdown document which is surrounded by single backticks. A paragraph in a markdown document can have a more than one code snippets.\n+\n+    Example:\n+\n+        \u00ab\u00ab\u00ab\n+        `i am a code snippet`\n+        \u00bb\u00bb\u00bb\n+\n+    Example:\n+\n+        \u00ab\u00ab\u00ab\n+        `first code snippet` `second code snippet` `third code snippet`\n+        \u00bb\u00bb\u00bb\n+\n+\"code block\"\n+\n+    Text in a Markdown document which is surrounded by triple backticks. Spreads multiple lines.\n+\n+    Example:\n+\n+        \u00ab\u00ab\u00ab\n+        ```\n+        Hello\n+        World\n+        ```\n+        \u00bb\u00bb\u00bb\n+\n+    Example:\n+\n+        \u00ab\u00ab\u00ab\n+        ```python\n+        print(\"hello World\")\n+        ```\n+        \u00bb\u00bb\u00bb\n+\n+\n+### Your task\n+\n+Translate an English text \u2013 the original content \u2013 to a target language.\n+\n+The original content is written in Markdown, write the translation in Markdown as well.\n+\n+The original content will be surrounded by triple percentage signs (\u00ab%%%\u00bb). Do not include the triple percentage signs in the translation.\n+\n+\n+### Technical terms in English\n+\n+For technical terms in English that don't have a common translation term, use the original term in English.\n+\n+\n+### Content of code snippets\n+\n+Do not translate the content of code snippets, keep the original in English. For example, \u00ab`list`\u00bb, \u00ab`dict`\u00bb, keep them as is.\n+\n+\n+### Content of code blocks\n+\n+Do not translate the content of code blocks, except for comments in the language which the code block uses.\n+\n+Examples:\n+\n+    Source (English) \u2013 The code block is a bash code example with one comment:\n+\n+        \u00ab\u00ab\u00ab\n+        ```bash\n+        # Print greeting\n+        echo \"Hello, World!\"\n+        ```\n+        \u00bb\u00bb\u00bb\n \n-If you have instructions to translate specific terms or phrases in a specific way, please follow those instructions instead of keeping the old and outdated content.\n+    Result (German):\n \n-For code snippets or fragments, surrounded by backticks (`), don't translate the content, keep the original in English. For example, `list`, `dict`, keep them as is.\n+        \u00ab\u00ab\u00ab\n+        ```bash\n+        # Gru\u00df ausgeben\n+        echo \"Hello, World!\"\n+        ```\n+        \u00bb\u00bb\u00bb\n \n-The content is written in markdown, write the translation in markdown as well. Don't add triple backticks (`) around the generated translation content.\n+    Source (English) \u2013 The code block is a console example containing HTML tags. No comments, so nothing to change here:\n \n-When there's an example of code, the console or a terminal, normally surrounded by triple backticks and a keyword like \"console\" or \"bash\" (e.g. ```console), do not translate the content, keep the original in English.\n+        \u00ab\u00ab\u00ab\n+        ```console\n+        $ <font color=\"#4E9A06\">fastapi</font> run <u style=\"text-decoration-style:solid\">main.py</u>\n+        <span style=\"background-color:#009485\"><font color=\"#D3D7CF\"> FastAPI </font></span>  Starting server\n+                Searching for package file structure\n+        ```\n+        \u00bb\u00bb\u00bb\n \n-The original content will be surrounded by triple percentage signs (%) and you should translate it to the target language. Do not include the triple percentage signs in the translation.\n+    Result (German):\n+\n+        \u00ab\u00ab\u00ab\n+        ```console\n+        $ <font color=\"#4E9A06\">fastapi</font> run <u style=\"text-decoration-style:solid\">main.py</u>\n+        <span style=\"background-color:#009485\"><font color=\"#D3D7CF\"> FastAPI </font></span>  Starting server\n+                Searching for package file structure\n+        ```\n+        \u00bb\u00bb\u00bb\n+\n+    Source (English) \u2013 The code block is a console example containing 5 comments:\n+\n+        \u00ab\u00ab\u00ab\n+        ```console\n+        // Go to the home directory\n+        $ cd\n+        // Create a directory for all your code projects\n+        $ mkdir code\n+        // Enter into that code directory\n+        $ cd code\n+        // Create a directory for this project\n+        $ mkdir awesome-project\n+        // Enter into that project directory\n+        $ cd awesome-project\n+        ```\n+        \u00bb\u00bb\u00bb\n+\n+    Result (German):\n+\n+        \u00ab\u00ab\u00ab\n+        ```console\n+        // Gehe zum Home-Verzeichnis\n+        $ cd\n+        // Erstelle ein Verzeichnis f\u00fcr alle Ihre Code-Projekte\n+        $ mkdir code\n+        // Gehe in dieses Code-Verzeichnis\n+        $ cd code\n+        // Erstelle ein Verzeichnis f\u00fcr dieses Projekt\n+        $ mkdir awesome-project\n+        // Gehe in dieses Projektverzeichnis\n+        $ cd awesome-project\n+        ```\n+        \u00bb\u00bb\u00bb\n+\n+If there is an existing translation and its Mermaid diagram is in sync with the Mermaid diagram in the English source, except a few translated words, then use the Mermaid diagram of the existing translation. The human editor of the translation translated these words in the Mermaid diagram. Keep these translations, do not revert them back to the English source.\n+\n+Example:\n+\n+    Source (English):\n+\n+        \u00ab\u00ab\u00ab\n+        ```mermaid\n+        flowchart LR\n+            subgraph global[global env]\n+                harry-1[harry v1]\n+            end\n+            subgraph stone-project[philosophers-stone project]\n+                stone(philosophers-stone) -->|requires| harry-1\n+            end\n+        ```\n+        \u00bb\u00bb\u00bb\n+\n+    Existing translation (German) \u2013 has three translations:\n+\n+        \u00ab\u00ab\u00ab\n+        ```mermaid\n+        flowchart LR\n+            subgraph global[globale Umgebung]\n+                harry-1[harry v1]\n+            end\n+            subgraph stone-project[philosophers-stone-Projekt]\n+                stone(philosophers-stone) -->|ben\u00f6tigt| harry-1\n+            end\n+        ```\n+        \u00bb\u00bb\u00bb\n+\n+    Result (German) \u2013 you change nothing:\n+\n+        \u00ab\u00ab\u00ab\n+        ```mermaid\n+        flowchart LR\n+            subgraph global[globale Umgebung]\n+                harry-1[harry v1]\n+            end\n+            subgraph stone-project[philosophers-stone-Projekt]\n+                stone(philosophers-stone) -->|ben\u00f6tigt| harry-1\n+            end\n+        ```\n+        \u00bb\u00bb\u00bb\n+\n+\n+### Special blocks\n \n There are special blocks of notes, tips and others that look like:\n \n-/// note\n+    \u00ab\u00ab\u00ab\n+    /// note\n+    \u00bb\u00bb\u00bb\n \n To translate it, keep the same line and add the translation after a vertical bar.\n \n For example, if you were translating to Spanish, you would write:\n \n-/// note | Nota\n+    \u00ab\u00ab\u00ab\n+    /// note | Nota\n+    \u00bb\u00bb\u00bb\n \n Some examples in Spanish:\n \n-Source:\n+    Source:\n+\n+        \u00ab\u00ab\u00ab\n+        /// tip\n+        \u00bb\u00bb\u00bb\n+\n+    Result:\n+\n+        \u00ab\u00ab\u00ab\n+        /// tip | Consejo\n+        \u00bb\u00bb\u00bb\n+\n+    Source:\n+\n+        \u00ab\u00ab\u00ab\n+        /// details | Preview\n+        \u00bb\u00bb\u00bb\n+\n+    Result:\n+\n+        \u00ab\u00ab\u00ab\n+        /// details | Vista previa\n+        \u00bb\u00bb\u00bb\n+\n+\n+### Tab blocks\n+\n+There are special blocks surrounded by four slashes (\u00ab////\u00bb). They mark text, which will be rendered as part of a tab in the final document. The scheme is:\n+\n+    \u00ab\u00ab\u00ab\n+    //// tab | {tab title}\n+    {tab content, may span many lines}\n+    ////\n+    \u00bb\u00bb\u00bb\n+\n+Keep everything before the vertical bar (\u00ab|\u00bb) as is, including the vertical bar. Translate the tab title. Translate the tab content, applying the rules you know. Keep the four block closing slashes as is.\n+\n+Examples:\n+\n+    Source (English):\n+\n+        \u00ab\u00ab\u00ab\n+        //// tab | Python 3.8+ non-Annotated\n+        Hello\n+        ////\n+        \u00bb\u00bb\u00bb\n+\n+    Result (German):\n+\n+        \u00ab\u00ab\u00ab\n+        //// tab | Python 3.8+ nicht annotiert\n+        Hallo\n+        ////\n+        \u00bb\u00bb\u00bb\n+\n+    Source (English) \u2013 Here there is nothing to translate in the tab title:\n+\n+        \u00ab\u00ab\u00ab\n+        //// tab | Linux, macOS, Windows Bash\n+        Hello again\n+        ////\n+        \u00bb\u00bb\u00bb\n+\n+    Result (German):\n+\n+        \u00ab\u00ab\u00ab\n+        //// tab | Linux, macOS, Windows Bash\n+        Hallo wieder\n+        ////\n+        \u00bb\u00bb\u00bb\n+\n+\n+### Headings\n+\n+Every Markdown heading in the English text (all levels) ends with a part inside curly brackets. This part denotes the hash of this heading, which is used in links to this heading. In translations, translate the heading, but do not translate this hash part, so that links do not break.\n+\n+Examples of how to translate a heading:\n+\n+    Source (English):\n+\n+        \u00ab\u00ab\u00ab\n+        ## Alternative API docs { #alternative-api-docs }\n+        \u00bb\u00bb\u00bb\n+\n+    Result (Spanish):\n+\n+        \u00ab\u00ab\u00ab\n+        ## Documentaci\u00f3n de la API alternativa { #alternative-api-docs }\n+        \u00bb\u00bb\u00bb\n+\n+    Source (English):\n+\n+        \u00ab\u00ab\u00ab\n+        ### Example { #example }\n+        \u00bb\u00bb\u00bb\n+\n+    Result (German):\n+\n+        \u00ab\u00ab\u00ab\n+        ### Beispiel { #example }\n+        \u00bb\u00bb\u00bb\n+\n+\n+### Links\n+\n+Use the following rules for links (apply both to Markdown-style links (\u00ab[text](url)\u00bb) and to HTML-style \u00ab<a>\u00bb tags):\n+\n+1) For relative URLs, only translate link text. Do not translate the URL or its parts\n+\n+Example:\n+\n+    Source (English):\n+\n+        \u00ab\u00ab\u00ab\n+        [One of the fastest Python frameworks available](#performance)\n+        \u00bb\u00bb\u00bb\n+\n+    Result (German):\n \n-/// tip\n+        \u00ab\u00ab\u00ab\n+        [Eines der schnellsten verf\u00fcgbaren Python-Frameworks](#performance)\n+        \u00bb\u00bb\u00bb\n \n-Result:\n+2) For absolute URLs which DO NOT start EXACTLY with \u00abhttps://fastapi.tiangolo.com\u00bb, only translate link text and leave the URL unchanged.\n \n-/// tip | Consejo\n+Example:\n \n-Source:\n+    Source (English):\n \n-/// details | Preview\n+        \u00ab\u00ab\u00ab\n+        <a href=\"https://sqlmodel.tiangolo.com/\" class=\"external-link\" target=\"_blank\">SQLModel docs</a>\n+        \u00bb\u00bb\u00bb\n+\n+    Result (German):\n+\n+        \u00ab\u00ab\u00ab\n+        <a href=\"https://sqlmodel.tiangolo.com/\" class=\"external-link\" target=\"_blank\">SQLModel-Dokumentation</a>\n+        \u00bb\u00bb\u00bb\n+\n+3) For absolute URLs which DO start EXACTLY with \u00abhttps://fastapi.tiangolo.com\u00bb, only translate link text and change the URL by adding language code (\u00abhttps://fastapi.tiangolo.com/{language_code}[rest part of the url]\u00bb).\n+\n+Example:\n+\n+    Source (English):\n+\n+        \u00ab\u00ab\u00ab\n+        <a href=\"https://fastapi.tiangolo.com/tutorial/path-params/#documentation\" class=\"external-link\" target=\"_blank\">Documentation</a>\n+        \u00bb\u00bb\u00bb\n+\n+    Result (Spanish):\n+\n+        \u00ab\u00ab\u00ab\n+        <a href=\"https://fastapi.tiangolo.com/es/tutorial/path-params/#documentation\" class=\"external-link\" target=\"_blank\">Documentaci\u00f3n</a>\n+        \u00bb\u00bb\u00bb\n+\n+3.1) Do not add language codes for URLs that point to static assets (e.g., images, CSS, JavaScript).\n+\n+Example:\n+\n+    Source (English):\n+\n+        \u00ab\u00ab\u00ab\n+        <a href=\"https://fastapi.tiangolo.com/img/something.jpg\" class=\"external-link\" target=\"_blank\">Something</a>\n+        \u00bb\u00bb\u00bb\n+\n+    Result (Spanish):\n+\n+        \u00ab\u00ab\u00ab\n+        <a href=\"https://fastapi.tiangolo.com/img/something.jpg\" class=\"external-link\" target=\"_blank\">Algo</a>\n+        \u00bb\u00bb\u00bb\n+\n+4) For internal links, only translate link text.\n+\n+Example:\n+\n+    Source (English):\n+\n+        \u00ab\u00ab\u00ab\n+        [Create Pull Requests](help-fastapi.md#create-a-pull-request){.internal-link target=_blank}\n+        \u00bb\u00bb\u00bb\n+\n+    Result (German):\n+\n+        \u00ab\u00ab\u00ab\n+        [Pull Requests erzeugen](help-fastapi.md#create-a-pull-request){.internal-link target=_blank}\n+        \u00bb\u00bb\u00bb\n+\n+5) Do not translate anchor fragments in links (the part after \u00ab#\u00bb), as they must remain the same to work correctly.\n+\n+5.1) If an existing translation has a link with an anchor fragment different to the anchor fragment in the English source, then this is an error. Fix this by using the anchor fragment of the English source.\n+\n+Example:\n+\n+    Source (English):\n+\n+        \u00ab\u00ab\u00ab\n+        [Body - Multiple Parameters: Singular values in body](body-multiple-params.md#singular-values-in-body){.internal-link target=_blank}\n+        \u00bb\u00bb\u00bb\n+\n+    Existing wrong translation (German) \u2013 notice the wrongly translated anchor fragment:\n+\n+        \u00ab\u00ab\u00ab\n+        [Body \u2013 Mehrere Parameter: Einfache Werte im Body](body-multiple-params.md#einzelne-werte-im-body){.internal-link target=_blank}.\n+        \u00bb\u00bb\u00bb\n+\n+    Result (German) \u2013 you fix the anchor fragment:\n+\n+        \u00ab\u00ab\u00ab\n+        [Body \u2013 Mehrere Parameter: Einfache Werte im Body](body-multiple-params.md#singular-values-in-body){.internal-link target=_blank}.\n+        \u00bb\u00bb\u00bb\n+\n+5.2) Do not add anchor fragments at will, even if this makes sense. If the English source has no anchor, don't add one.\n+\n+Example:\n+\n+    Source (English):\n+\n+        \u00ab\u00ab\u00ab\n+        Create a [virtual environment](../virtual-environments.md){.internal-link target=_blank}\n+        \u00bb\u00bb\u00bb\n+\n+    Wrong translation (German) \u2013 Anchor added to the URL.\n+\n+        \u00ab\u00ab\u00ab\n+        Erstelle eine [virtuelle Umgebung](../virtual-environments.md#create-a-virtual-environment){.internal-link target=_blank}\n+        \u00bb\u00bb\u00bb\n+\n+    Good translation (German) \u2013 URL stays like in the English source.\n+\n+        \u00ab\u00ab\u00ab\n+        Erstelle eine [Virtuelle Umgebung](../virtual-environments.md){.internal-link target=_blank}\n+        \u00bb\u00bb\u00bb\n+\n+\n+### HTML abbr elements\n+\n+Translate HTML abbr elements as follows:\n+\n+1) If the title attribute gives the full phrase for an abbreviation, then keep the phrase, append a dash (\u00ab\u2013\u00bb), followed by the translation of the phrase.",
      "comment": "In Russian it fails to follow this rule sometimes.\r\n<img width=\"1367\" height=\"47\" alt=\"image\" src=\"https://github.com/user-attachments/assets/038f593e-0e25-4443-b015-01afec6996ff\" />\r\n",
      "comment_id": 2315421269,
      "user": "YuriiMotov",
      "created_at": "2025-09-02T09:04:17Z",
      "url": "https://github.com/fastapi/fastapi/pull/14015#discussion_r2315421269"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14015,
      "file_path": "scripts/translate.py",
      "line": 606,
      "side": "RIGHT",
      "diff_hunk": "@@ -22,55 +24,642 @@\n     \"contributing.md\",\n )\n \n-\n general_prompt = \"\"\"\n-For technical terms in English that don't have a common translation term use the original term in English.\n+### About literal text in this prompt\n+\n+1) In the following instructions (after I say: `The above rules are in effect now`) the two characters `\u00ab` and `\u00bb` will be used to surround LITERAL TEXT, which is text or characters you shall interpret literally. The `\u00ab` and the `\u00bb` are not part of the literal text, they are the meta characters denoting it.\n+\n+2) Furthermore, text surrounded by `\u00ab\u00ab\u00ab` and `\u00bb\u00bb\u00bb` is a BLOCK OF LITERAL TEXT which spans multiple lines. To get its content, dedent all lines of the block until the `\u00ab\u00ab\u00ab` and `\u00bb\u00bb\u00bb` are at column zero, then remove the newline (`\\n`) after the `\u00ab\u00ab\u00ab` and the newline before the `\u00bb\u00bb\u00bb`. The `\u00ab\u00ab\u00ab` and the `\u00bb\u00bb\u00bb` are not part of the literal text block, they are the meta characters denoting it.\n+\n+3) If you see backticks inside literal text \u2013 inside `\u00ab` and `\u00bb` \u2013  or inside blocks of literal text \u2013 inside `\u00ab\u00ab\u00ab` and `\u00bb\u00bb\u00bb` \u2013 then interpret them as literal characters, do NOT interpret them as meta characters.\n+\n+The above rules are in effect now.\n+\n+\n+### Definitions of terms used in this prompt\n+\n+\"backtick\"\n+\n+    The character \u00ab`\u00bb\n+    Unicode U+0060 (GRAVE ACCENT)\n+\n+\"single backtick\"\n+\n+    A single backtick \u2013 \u00ab`\u00bb\n+\n+\"triple backticks\"\n+\n+    Three backticks in a row \u2013 \u00ab```\u00bb\n+\n+\"neutral double quote\"\n+\n+    The character \u00ab\"\u00bb\n+    Unicode U+0022 (QUOTATION MARK)\n+\n+\"neutral single quote\"\n+\n+    The character \u00ab'\u00bb\n+    Unicode U+0027 (APOSTROPHE)\n+\n+\"English double typographic quotes\"\n+\n+    The characters \u00ab\u201c\u00bb and \u00ab\u201d\u00bb\n+    Unicode U+201C (LEFT DOUBLE QUOTATION MARK) and Unicode U+201D (RIGHT DOUBLE QUOTATION MARK)\n+\n+\"English single typographic quotes\"\n+\n+    The characters \u00ab\u2018\u00bb and \u00ab\u2019\u00bb\n+    Unicode U+2018 (LEFT SINGLE QUOTATION MARK) and Unicode U+2019 (RIGHT SINGLE QUOTATION MARK)\n+\n+\"code snippet\"\n+\n+    Also called \"inline code\". Text in a Markdown document which is surrounded by single backticks. A paragraph in a markdown document can have a more than one code snippets.\n+\n+    Example:\n+\n+        \u00ab\u00ab\u00ab\n+        `i am a code snippet`\n+        \u00bb\u00bb\u00bb\n+\n+    Example:\n+\n+        \u00ab\u00ab\u00ab\n+        `first code snippet` `second code snippet` `third code snippet`\n+        \u00bb\u00bb\u00bb\n+\n+\"code block\"\n+\n+    Text in a Markdown document which is surrounded by triple backticks. Spreads multiple lines.\n+\n+    Example:\n+\n+        \u00ab\u00ab\u00ab\n+        ```\n+        Hello\n+        World\n+        ```\n+        \u00bb\u00bb\u00bb\n+\n+    Example:\n+\n+        \u00ab\u00ab\u00ab\n+        ```python\n+        print(\"hello World\")\n+        ```\n+        \u00bb\u00bb\u00bb\n+\n+\n+### Your task\n+\n+Translate an English text \u2013 the original content \u2013 to a target language.\n+\n+The original content is written in Markdown, write the translation in Markdown as well.\n+\n+The original content will be surrounded by triple percentage signs (\u00ab%%%\u00bb). Do not include the triple percentage signs in the translation.\n+\n+\n+### Technical terms in English\n+\n+For technical terms in English that don't have a common translation term, use the original term in English.\n+\n+\n+### Content of code snippets\n+\n+Do not translate the content of code snippets, keep the original in English. For example, \u00ab`list`\u00bb, \u00ab`dict`\u00bb, keep them as is.\n+\n+\n+### Content of code blocks\n+\n+Do not translate the content of code blocks, except for comments in the language which the code block uses.\n+\n+Examples:\n+\n+    Source (English) \u2013 The code block is a bash code example with one comment:\n+\n+        \u00ab\u00ab\u00ab\n+        ```bash\n+        # Print greeting\n+        echo \"Hello, World!\"\n+        ```\n+        \u00bb\u00bb\u00bb\n \n-If you have instructions to translate specific terms or phrases in a specific way, please follow those instructions instead of keeping the old and outdated content.\n+    Result (German):\n \n-For code snippets or fragments, surrounded by backticks (`), don't translate the content, keep the original in English. For example, `list`, `dict`, keep them as is.\n+        \u00ab\u00ab\u00ab\n+        ```bash\n+        # Gru\u00df ausgeben\n+        echo \"Hello, World!\"\n+        ```\n+        \u00bb\u00bb\u00bb\n \n-The content is written in markdown, write the translation in markdown as well. Don't add triple backticks (`) around the generated translation content.\n+    Source (English) \u2013 The code block is a console example containing HTML tags. No comments, so nothing to change here:\n \n-When there's an example of code, the console or a terminal, normally surrounded by triple backticks and a keyword like \"console\" or \"bash\" (e.g. ```console), do not translate the content, keep the original in English.\n+        \u00ab\u00ab\u00ab\n+        ```console\n+        $ <font color=\"#4E9A06\">fastapi</font> run <u style=\"text-decoration-style:solid\">main.py</u>\n+        <span style=\"background-color:#009485\"><font color=\"#D3D7CF\"> FastAPI </font></span>  Starting server\n+                Searching for package file structure\n+        ```\n+        \u00bb\u00bb\u00bb\n \n-The original content will be surrounded by triple percentage signs (%) and you should translate it to the target language. Do not include the triple percentage signs in the translation.\n+    Result (German):\n+\n+        \u00ab\u00ab\u00ab\n+        ```console\n+        $ <font color=\"#4E9A06\">fastapi</font> run <u style=\"text-decoration-style:solid\">main.py</u>\n+        <span style=\"background-color:#009485\"><font color=\"#D3D7CF\"> FastAPI </font></span>  Starting server\n+                Searching for package file structure\n+        ```\n+        \u00bb\u00bb\u00bb\n+\n+    Source (English) \u2013 The code block is a console example containing 5 comments:\n+\n+        \u00ab\u00ab\u00ab\n+        ```console\n+        // Go to the home directory\n+        $ cd\n+        // Create a directory for all your code projects\n+        $ mkdir code\n+        // Enter into that code directory\n+        $ cd code\n+        // Create a directory for this project\n+        $ mkdir awesome-project\n+        // Enter into that project directory\n+        $ cd awesome-project\n+        ```\n+        \u00bb\u00bb\u00bb\n+\n+    Result (German):\n+\n+        \u00ab\u00ab\u00ab\n+        ```console\n+        // Gehe zum Home-Verzeichnis\n+        $ cd\n+        // Erstelle ein Verzeichnis f\u00fcr alle Ihre Code-Projekte\n+        $ mkdir code\n+        // Gehe in dieses Code-Verzeichnis\n+        $ cd code\n+        // Erstelle ein Verzeichnis f\u00fcr dieses Projekt\n+        $ mkdir awesome-project\n+        // Gehe in dieses Projektverzeichnis\n+        $ cd awesome-project\n+        ```\n+        \u00bb\u00bb\u00bb\n+\n+If there is an existing translation and its Mermaid diagram is in sync with the Mermaid diagram in the English source, except a few translated words, then use the Mermaid diagram of the existing translation. The human editor of the translation translated these words in the Mermaid diagram. Keep these translations, do not revert them back to the English source.\n+\n+Example:\n+\n+    Source (English):\n+\n+        \u00ab\u00ab\u00ab\n+        ```mermaid\n+        flowchart LR\n+            subgraph global[global env]\n+                harry-1[harry v1]\n+            end\n+            subgraph stone-project[philosophers-stone project]\n+                stone(philosophers-stone) -->|requires| harry-1\n+            end\n+        ```\n+        \u00bb\u00bb\u00bb\n+\n+    Existing translation (German) \u2013 has three translations:\n+\n+        \u00ab\u00ab\u00ab\n+        ```mermaid\n+        flowchart LR\n+            subgraph global[globale Umgebung]\n+                harry-1[harry v1]\n+            end\n+            subgraph stone-project[philosophers-stone-Projekt]\n+                stone(philosophers-stone) -->|ben\u00f6tigt| harry-1\n+            end\n+        ```\n+        \u00bb\u00bb\u00bb\n+\n+    Result (German) \u2013 you change nothing:\n+\n+        \u00ab\u00ab\u00ab\n+        ```mermaid\n+        flowchart LR\n+            subgraph global[globale Umgebung]\n+                harry-1[harry v1]\n+            end\n+            subgraph stone-project[philosophers-stone-Projekt]\n+                stone(philosophers-stone) -->|ben\u00f6tigt| harry-1\n+            end\n+        ```\n+        \u00bb\u00bb\u00bb\n+\n+\n+### Special blocks\n \n There are special blocks of notes, tips and others that look like:\n \n-/// note\n+    \u00ab\u00ab\u00ab\n+    /// note\n+    \u00bb\u00bb\u00bb\n \n To translate it, keep the same line and add the translation after a vertical bar.\n \n For example, if you were translating to Spanish, you would write:\n \n-/// note | Nota\n+    \u00ab\u00ab\u00ab\n+    /// note | Nota\n+    \u00bb\u00bb\u00bb\n \n Some examples in Spanish:\n \n-Source:\n+    Source:\n+\n+        \u00ab\u00ab\u00ab\n+        /// tip\n+        \u00bb\u00bb\u00bb\n+\n+    Result:\n+\n+        \u00ab\u00ab\u00ab\n+        /// tip | Consejo\n+        \u00bb\u00bb\u00bb\n+\n+    Source:\n+\n+        \u00ab\u00ab\u00ab\n+        /// details | Preview\n+        \u00bb\u00bb\u00bb\n+\n+    Result:\n+\n+        \u00ab\u00ab\u00ab\n+        /// details | Vista previa\n+        \u00bb\u00bb\u00bb\n+\n+\n+### Tab blocks\n+\n+There are special blocks surrounded by four slashes (\u00ab////\u00bb). They mark text, which will be rendered as part of a tab in the final document. The scheme is:\n+\n+    \u00ab\u00ab\u00ab\n+    //// tab | {tab title}\n+    {tab content, may span many lines}\n+    ////\n+    \u00bb\u00bb\u00bb\n+\n+Keep everything before the vertical bar (\u00ab|\u00bb) as is, including the vertical bar. Translate the tab title. Translate the tab content, applying the rules you know. Keep the four block closing slashes as is.\n+\n+Examples:\n+\n+    Source (English):\n+\n+        \u00ab\u00ab\u00ab\n+        //// tab | Python 3.8+ non-Annotated\n+        Hello\n+        ////\n+        \u00bb\u00bb\u00bb\n+\n+    Result (German):\n+\n+        \u00ab\u00ab\u00ab\n+        //// tab | Python 3.8+ nicht annotiert\n+        Hallo\n+        ////\n+        \u00bb\u00bb\u00bb\n+\n+    Source (English) \u2013 Here there is nothing to translate in the tab title:\n+\n+        \u00ab\u00ab\u00ab\n+        //// tab | Linux, macOS, Windows Bash\n+        Hello again\n+        ////\n+        \u00bb\u00bb\u00bb\n+\n+    Result (German):\n+\n+        \u00ab\u00ab\u00ab\n+        //// tab | Linux, macOS, Windows Bash\n+        Hallo wieder\n+        ////\n+        \u00bb\u00bb\u00bb\n+\n+\n+### Headings\n+\n+Every Markdown heading in the English text (all levels) ends with a part inside curly brackets. This part denotes the hash of this heading, which is used in links to this heading. In translations, translate the heading, but do not translate this hash part, so that links do not break.\n+\n+Examples of how to translate a heading:\n+\n+    Source (English):\n+\n+        \u00ab\u00ab\u00ab\n+        ## Alternative API docs { #alternative-api-docs }\n+        \u00bb\u00bb\u00bb\n+\n+    Result (Spanish):\n+\n+        \u00ab\u00ab\u00ab\n+        ## Documentaci\u00f3n de la API alternativa { #alternative-api-docs }\n+        \u00bb\u00bb\u00bb\n+\n+    Source (English):\n+\n+        \u00ab\u00ab\u00ab\n+        ### Example { #example }\n+        \u00bb\u00bb\u00bb\n+\n+    Result (German):\n+\n+        \u00ab\u00ab\u00ab\n+        ### Beispiel { #example }\n+        \u00bb\u00bb\u00bb\n+\n+\n+### Links\n+\n+Use the following rules for links (apply both to Markdown-style links (\u00ab[text](url)\u00bb) and to HTML-style \u00ab<a>\u00bb tags):\n+\n+1) For relative URLs, only translate link text. Do not translate the URL or its parts\n+\n+Example:\n+\n+    Source (English):\n+\n+        \u00ab\u00ab\u00ab\n+        [One of the fastest Python frameworks available](#performance)\n+        \u00bb\u00bb\u00bb\n+\n+    Result (German):\n \n-/// tip\n+        \u00ab\u00ab\u00ab\n+        [Eines der schnellsten verf\u00fcgbaren Python-Frameworks](#performance)\n+        \u00bb\u00bb\u00bb\n \n-Result:\n+2) For absolute URLs which DO NOT start EXACTLY with \u00abhttps://fastapi.tiangolo.com\u00bb, only translate link text and leave the URL unchanged.\n \n-/// tip | Consejo\n+Example:\n \n-Source:\n+    Source (English):\n \n-/// details | Preview\n+        \u00ab\u00ab\u00ab\n+        <a href=\"https://sqlmodel.tiangolo.com/\" class=\"external-link\" target=\"_blank\">SQLModel docs</a>\n+        \u00bb\u00bb\u00bb\n+\n+    Result (German):\n+\n+        \u00ab\u00ab\u00ab\n+        <a href=\"https://sqlmodel.tiangolo.com/\" class=\"external-link\" target=\"_blank\">SQLModel-Dokumentation</a>\n+        \u00bb\u00bb\u00bb\n+\n+3) For absolute URLs which DO start EXACTLY with \u00abhttps://fastapi.tiangolo.com\u00bb, only translate link text and change the URL by adding language code (\u00abhttps://fastapi.tiangolo.com/{language_code}[rest part of the url]\u00bb).\n+\n+Example:\n+\n+    Source (English):\n+\n+        \u00ab\u00ab\u00ab\n+        <a href=\"https://fastapi.tiangolo.com/tutorial/path-params/#documentation\" class=\"external-link\" target=\"_blank\">Documentation</a>\n+        \u00bb\u00bb\u00bb\n+\n+    Result (Spanish):\n+\n+        \u00ab\u00ab\u00ab\n+        <a href=\"https://fastapi.tiangolo.com/es/tutorial/path-params/#documentation\" class=\"external-link\" target=\"_blank\">Documentaci\u00f3n</a>\n+        \u00bb\u00bb\u00bb\n+\n+3.1) Do not add language codes for URLs that point to static assets (e.g., images, CSS, JavaScript).\n+\n+Example:\n+\n+    Source (English):\n+\n+        \u00ab\u00ab\u00ab\n+        <a href=\"https://fastapi.tiangolo.com/img/something.jpg\" class=\"external-link\" target=\"_blank\">Something</a>\n+        \u00bb\u00bb\u00bb\n+\n+    Result (Spanish):\n+\n+        \u00ab\u00ab\u00ab\n+        <a href=\"https://fastapi.tiangolo.com/img/something.jpg\" class=\"external-link\" target=\"_blank\">Algo</a>\n+        \u00bb\u00bb\u00bb\n+\n+4) For internal links, only translate link text.\n+\n+Example:\n+\n+    Source (English):\n+\n+        \u00ab\u00ab\u00ab\n+        [Create Pull Requests](help-fastapi.md#create-a-pull-request){.internal-link target=_blank}\n+        \u00bb\u00bb\u00bb\n+\n+    Result (German):\n+\n+        \u00ab\u00ab\u00ab\n+        [Pull Requests erzeugen](help-fastapi.md#create-a-pull-request){.internal-link target=_blank}\n+        \u00bb\u00bb\u00bb\n+\n+5) Do not translate anchor fragments in links (the part after \u00ab#\u00bb), as they must remain the same to work correctly.\n+\n+5.1) If an existing translation has a link with an anchor fragment different to the anchor fragment in the English source, then this is an error. Fix this by using the anchor fragment of the English source.\n+\n+Example:\n+\n+    Source (English):\n+\n+        \u00ab\u00ab\u00ab\n+        [Body - Multiple Parameters: Singular values in body](body-multiple-params.md#singular-values-in-body){.internal-link target=_blank}\n+        \u00bb\u00bb\u00bb\n+\n+    Existing wrong translation (German) \u2013 notice the wrongly translated anchor fragment:\n+\n+        \u00ab\u00ab\u00ab\n+        [Body \u2013 Mehrere Parameter: Einfache Werte im Body](body-multiple-params.md#einzelne-werte-im-body){.internal-link target=_blank}.\n+        \u00bb\u00bb\u00bb\n+\n+    Result (German) \u2013 you fix the anchor fragment:\n+\n+        \u00ab\u00ab\u00ab\n+        [Body \u2013 Mehrere Parameter: Einfache Werte im Body](body-multiple-params.md#singular-values-in-body){.internal-link target=_blank}.\n+        \u00bb\u00bb\u00bb\n+\n+5.2) Do not add anchor fragments at will, even if this makes sense. If the English source has no anchor, don't add one.\n+\n+Example:\n+\n+    Source (English):\n+\n+        \u00ab\u00ab\u00ab\n+        Create a [virtual environment](../virtual-environments.md){.internal-link target=_blank}\n+        \u00bb\u00bb\u00bb\n+\n+    Wrong translation (German) \u2013 Anchor added to the URL.\n+\n+        \u00ab\u00ab\u00ab\n+        Erstelle eine [virtuelle Umgebung](../virtual-environments.md#create-a-virtual-environment){.internal-link target=_blank}\n+        \u00bb\u00bb\u00bb\n+\n+    Good translation (German) \u2013 URL stays like in the English source.\n+\n+        \u00ab\u00ab\u00ab\n+        Erstelle eine [Virtuelle Umgebung](../virtual-environments.md){.internal-link target=_blank}\n+        \u00bb\u00bb\u00bb\n+\n+\n+### HTML abbr elements\n+\n+Translate HTML abbr elements as follows:\n+\n+1) If the title attribute gives the full phrase for an abbreviation, then keep the phrase, append a dash (\u00ab\u2013\u00bb), followed by the translation of the phrase.\n+\n+Examples:\n+\n+    Source (English):\n+\n+        \u00ab\u00ab\u00ab\n+        <abbr title=\"Internet of Things\">IoT</abbr>\n+        <abbr title=\"Central Processing Unit\">CPU</abbr>\n+        <abbr title=\"too long; didn't read\"><strong>TL;DR:</strong></abbr>\n+        \u00bb\u00bb\u00bb\n+\n+    Result (German):\n+\n+        \u00ab\u00ab\u00ab\n+        <abbr title=\"Internet of Things \u2013 Internet der Dinge\">IoT</abbr>\n+        <abbr title=\"Central Processing Unit \u2013 Zentrale Verarbeitungseinheit\">CPU</abbr>\n+        <abbr title=\"too long; didn't read \u2013 zu lang; hab's nicht gelesen\"><strong>TL;DR:</strong></abbr>\n+        \u00bb\u00bb\u00bb\n+\n+Conversion scheme title attribute:\n+\n+    Source (English):\n+\n+        {full phrase}\n+\n+    Result (German):\n+\n+        {full phrase} \u2013 {translation of full phrase}\n+\n+1.1) If the translation of the phrase starts with the same letters, then just use the translation.\n+\n+Examples:\n+\n+    Source (English):\n+\n+        \u00ab\u00ab\u00ab\n+        <abbr title=\"JSON Web Tokens\">JWT</abbr>\n+        <abbr title=\"Enumeration\">`Enum`</abbr>\n+        <abbr title=\"Asynchronous Server Gateway Interface\">ASGI</abbr>\n+        \u00bb\u00bb\u00bb\n+\n+    Result (German):\n+\n+        \u00ab\u00ab\u00ab\n+        <abbr title=\"JSON Web Tokens\">JWT</abbr>\n+        <abbr title=\"Enumeration\">`Enum`</abbr>\n+        <abbr title=\"Asynchrones Server-Gateway-Interface\">ASGI</abbr>\n+        \u00bb\u00bb\u00bb\n+\n+Conversion scheme title attribute:\n+\n+    Source (English):\n+\n+        {full phrase}\n+\n+    Result (German):\n+\n+        {translation of full phrase}\n+\n+2) If the title attribute explains something in its own words, then translate it, if possible.\n+\n+Examples:\n+\n+    Source (English):\n+\n+        \u00ab\u00ab\u00ab\n+        <abbr title=\"also known as: endpoints, routes\">path</abbr>\n+        <abbr title=\"A program that checks for code errors\">linter</abbr>\n+        <abbr title=\"converting the string that comes from an HTTP request into Python data\">\"parsing\"</abbr>\n+        <abbr title=\"before 2023-03\">0.95.0</abbr>\n+        <abbr title=\"2023-08-26\">at the time of writing this</abbr>\n+        \u00bb\u00bb\u00bb\n+\n+    Result (German):\n+\n+        \u00ab\u00ab\u00ab\n+        <abbr title=\"auch bekannt als: Endpunkte, Routen\">Pfad</abbr>\n+        <abbr title=\"Programm das auf Fehler im Code pr\u00fcft\">Linter</abbr>\n+        <abbr title=\"Konvertieren des Strings eines HTTP-Requests in Python-Daten\">\u201eParsen\u201c</abbr>\n+        <abbr title=\"vor 2023-03\">0.95.0</abbr>\n+        <abbr title=\"2023-08-26\">zum Zeitpunkt als das hier geschrieben wurde</abbr>\n+        \u00bb\u00bb\u00bb\n+\n+Conversion scheme title attribute:\n+\n+    Source (English):\n+\n+        {explanation}\n+\n+    Result (German):\n+\n+        {translation of explanation}\n+\n+3) If the title attribute gives the full phrase for an abbreviation, followed by a colon (\u00ab:\u00bb) or a comma (\u00ab,\u00bb), followed by an explanation, then keep the phrase, append a dash (\u00ab\u2013\u00bb), followed by the translation of the phrase, followed by a colon (\u00ab:\u00bb), followed by the translation of the explanation.",
      "comment": "Sometimes fails to follow this rule in Russian\r\n\r\n<img width=\"1367\" height=\"47\" alt=\"image\" src=\"https://github.com/user-attachments/assets/71f1ed21-bd66-4976-b21a-b8572297869e\" />\r\n",
      "comment_id": 2315463043,
      "user": "YuriiMotov",
      "created_at": "2025-09-02T09:19:17Z",
      "url": "https://github.com/fastapi/fastapi/pull/14015#discussion_r2315463043"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14015,
      "file_path": "scripts/translate.py",
      "line": 806,
      "side": "RIGHT",
      "diff_hunk": "@@ -182,39 +783,72 @@ def iter_all_en_paths() -> Iterable[Path]:\n \n \n def iter_en_paths_to_translate() -> Iterable[Path]:\n+    en_docs_root = Path(\"docs/en/docs/\")\n     for path in iter_all_en_paths():\n-        if str(path).replace(\"docs/en/docs/\", \"\").startswith(non_translated_sections):\n-            continue\n-        yield path\n+        relpath = path.relative_to(en_docs_root)\n+        if not str(relpath).startswith(non_translated_sections):\n+            yield path\n \n \n @app.command()\n-def translate_lang(language: Annotated[str, typer.Option(envvar=\"LANGUAGE\")]) -> None:\n-    paths_to_process = list(iter_en_paths_to_translate())\n-    print(\"Original paths:\")\n-    for p in paths_to_process:\n-        print(f\"  - {p}\")\n-    print(f\"Total original paths: {len(paths_to_process)}\")\n+def translate_lang(\n+    language: Annotated[\n+        str,\n+        typer.Option(envvar=\"LANGUAGE\", help=\"Target language, e.g. `es`, `fr`, `de`\"),\n+    ],\n+    mode: Annotated[\n+        str,\n+        typer.Option(\n+            help=\"Which files of the target language to translate, one of: `missing`, `existing`, `all`\"\n+        ),\n+    ] = \"missing\",\n+    verbose: Annotated[bool, typer.Option(help=\"Print all paths\")] = False,\n+    preview: Annotated[",
      "comment": "Not sure we need this `preview` parameter. We can always commit previous changes, then run script and discard changes if we don't like them.",
      "comment_id": 2315521909,
      "user": "YuriiMotov",
      "created_at": "2025-09-02T09:37:41Z",
      "url": "https://github.com/fastapi/fastapi/pull/14015#discussion_r2315521909"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14015,
      "file_path": "scripts/translate.py",
      "line": 804,
      "side": "RIGHT",
      "diff_hunk": "@@ -182,39 +783,72 @@ def iter_all_en_paths() -> Iterable[Path]:\n \n \n def iter_en_paths_to_translate() -> Iterable[Path]:\n+    en_docs_root = Path(\"docs/en/docs/\")\n     for path in iter_all_en_paths():\n-        if str(path).replace(\"docs/en/docs/\", \"\").startswith(non_translated_sections):\n-            continue\n-        yield path\n+        relpath = path.relative_to(en_docs_root)\n+        if not str(relpath).startswith(non_translated_sections):\n+            yield path\n \n \n @app.command()\n-def translate_lang(language: Annotated[str, typer.Option(envvar=\"LANGUAGE\")]) -> None:\n-    paths_to_process = list(iter_en_paths_to_translate())\n-    print(\"Original paths:\")\n-    for p in paths_to_process:\n-        print(f\"  - {p}\")\n-    print(f\"Total original paths: {len(paths_to_process)}\")\n+def translate_lang(\n+    language: Annotated[\n+        str,\n+        typer.Option(envvar=\"LANGUAGE\", help=\"Target language, e.g. `es`, `fr`, `de`\"),\n+    ],\n+    mode: Annotated[\n+        str,\n+        typer.Option(\n+            help=\"Which files of the target language to translate, one of: `missing`, `existing`, `all`\"\n+        ),\n+    ] = \"missing\",",
      "comment": "I think mode should be: `missing`, `updated`, `missing-and-updated`, `all`.\r\nAnd it should be `missing-and-updated` by default.\r\n`all` should include existing even if they were not updated.",
      "comment_id": 2315559758,
      "user": "YuriiMotov",
      "created_at": "2025-09-02T09:50:41Z",
      "url": "https://github.com/fastapi/fastapi/pull/14015#discussion_r2315559758"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14015,
      "file_path": "scripts/translate.py",
      "line": 698,
      "side": "RIGHT",
      "diff_hunk": "@@ -96,30 +685,42 @@ def generate_en_path(*, lang: str, path: Path) -> Path:\n @app.command()\n def translate_page(\n     *,\n-    language: Annotated[str, typer.Option(envvar=\"LANGUAGE\")],\n-    en_path: Annotated[Path, typer.Option(envvar=\"EN_PATH\")],\n+    language: Annotated[\n+        str,\n+        typer.Option(envvar=\"LANGUAGE\", help=\"Target language, e.g. `es`, `fr`, `de`\"),\n+    ],\n+    en_path: Annotated[\n+        Path,\n+        typer.Option(\n+            envvar=\"EN_PATH\",\n+            help=\"Path to the English source, relative to the FastAPI root directory. If not given, `docs/en/docs/_llm-test.md` is used.\",\n+        ),\n+    ] = Path(\"docs/en/docs/_llm-test.md\"),",
      "comment": "Yeah, this may result in running a translation unintended. Will remove.",
      "comment_id": 2316035102,
      "user": "nilslindemann",
      "created_at": "2025-09-02T13:05:33Z",
      "url": "https://github.com/fastapi/fastapi/pull/14015#discussion_r2316035102"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14015,
      "file_path": "scripts/translate.py",
      "line": 806,
      "side": "RIGHT",
      "diff_hunk": "@@ -182,39 +783,72 @@ def iter_all_en_paths() -> Iterable[Path]:\n \n \n def iter_en_paths_to_translate() -> Iterable[Path]:\n+    en_docs_root = Path(\"docs/en/docs/\")\n     for path in iter_all_en_paths():\n-        if str(path).replace(\"docs/en/docs/\", \"\").startswith(non_translated_sections):\n-            continue\n-        yield path\n+        relpath = path.relative_to(en_docs_root)\n+        if not str(relpath).startswith(non_translated_sections):\n+            yield path\n \n \n @app.command()\n-def translate_lang(language: Annotated[str, typer.Option(envvar=\"LANGUAGE\")]) -> None:\n-    paths_to_process = list(iter_en_paths_to_translate())\n-    print(\"Original paths:\")\n-    for p in paths_to_process:\n-        print(f\"  - {p}\")\n-    print(f\"Total original paths: {len(paths_to_process)}\")\n+def translate_lang(\n+    language: Annotated[\n+        str,\n+        typer.Option(envvar=\"LANGUAGE\", help=\"Target language, e.g. `es`, `fr`, `de`\"),\n+    ],\n+    mode: Annotated[\n+        str,\n+        typer.Option(\n+            help=\"Which files of the target language to translate, one of: `missing`, `existing`, `all`\"\n+        ),\n+    ] = \"missing\",\n+    verbose: Annotated[bool, typer.Option(help=\"Print all paths\")] = False,\n+    preview: Annotated[",
      "comment": "It is practical to be able to preview which files will be modified, before running the actual translations, which costs time and money. For example, to figure out if the `mode` parameter was set correctly. I am in favor of keeping it.",
      "comment_id": 2316051064,
      "user": "nilslindemann",
      "created_at": "2025-09-02T13:11:22Z",
      "url": "https://github.com/fastapi/fastapi/pull/14015#discussion_r2316051064"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14015,
      "file_path": "scripts/translate.py",
      "line": 806,
      "side": "RIGHT",
      "diff_hunk": "@@ -182,39 +783,72 @@ def iter_all_en_paths() -> Iterable[Path]:\n \n \n def iter_en_paths_to_translate() -> Iterable[Path]:\n+    en_docs_root = Path(\"docs/en/docs/\")\n     for path in iter_all_en_paths():\n-        if str(path).replace(\"docs/en/docs/\", \"\").startswith(non_translated_sections):\n-            continue\n-        yield path\n+        relpath = path.relative_to(en_docs_root)\n+        if not str(relpath).startswith(non_translated_sections):\n+            yield path\n \n \n @app.command()\n-def translate_lang(language: Annotated[str, typer.Option(envvar=\"LANGUAGE\")]) -> None:\n-    paths_to_process = list(iter_en_paths_to_translate())\n-    print(\"Original paths:\")\n-    for p in paths_to_process:\n-        print(f\"  - {p}\")\n-    print(f\"Total original paths: {len(paths_to_process)}\")\n+def translate_lang(\n+    language: Annotated[\n+        str,\n+        typer.Option(envvar=\"LANGUAGE\", help=\"Target language, e.g. `es`, `fr`, `de`\"),\n+    ],\n+    mode: Annotated[\n+        str,\n+        typer.Option(\n+            help=\"Which files of the target language to translate, one of: `missing`, `existing`, `all`\"\n+        ),\n+    ] = \"missing\",\n+    verbose: Annotated[bool, typer.Option(help=\"Print all paths\")] = False,\n+    preview: Annotated[",
      "comment": "Just realized that we also have `update_outdated`, `add_missing` and `update_and_add` commands. And also `list_outdated` and `list_missing`.\r\nI suppose we will mostly use those commands, not `translate_lang`.\r\nI would suggest we ask Sebastian whether we even need this command as it's duplication of functionality.",
      "comment_id": 2316246036,
      "user": "YuriiMotov",
      "created_at": "2025-09-02T14:19:06Z",
      "url": "https://github.com/fastapi/fastapi/pull/14015#discussion_r2316246036"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14015,
      "file_path": "scripts/translate.py",
      "line": 806,
      "side": "RIGHT",
      "diff_hunk": "@@ -182,39 +783,72 @@ def iter_all_en_paths() -> Iterable[Path]:\n \n \n def iter_en_paths_to_translate() -> Iterable[Path]:\n+    en_docs_root = Path(\"docs/en/docs/\")\n     for path in iter_all_en_paths():\n-        if str(path).replace(\"docs/en/docs/\", \"\").startswith(non_translated_sections):\n-            continue\n-        yield path\n+        relpath = path.relative_to(en_docs_root)\n+        if not str(relpath).startswith(non_translated_sections):\n+            yield path\n \n \n @app.command()\n-def translate_lang(language: Annotated[str, typer.Option(envvar=\"LANGUAGE\")]) -> None:\n-    paths_to_process = list(iter_en_paths_to_translate())\n-    print(\"Original paths:\")\n-    for p in paths_to_process:\n-        print(f\"  - {p}\")\n-    print(f\"Total original paths: {len(paths_to_process)}\")\n+def translate_lang(\n+    language: Annotated[\n+        str,\n+        typer.Option(envvar=\"LANGUAGE\", help=\"Target language, e.g. `es`, `fr`, `de`\"),\n+    ],\n+    mode: Annotated[\n+        str,\n+        typer.Option(\n+            help=\"Which files of the target language to translate, one of: `missing`, `existing`, `all`\"\n+        ),\n+    ] = \"missing\",\n+    verbose: Annotated[bool, typer.Option(help=\"Print all paths\")] = False,\n+    preview: Annotated[",
      "comment": "Okay, let's do that. Edit: to make it clear, I am also against duplication of functionality and in favor of deleting it.",
      "comment_id": 2316393300,
      "user": "nilslindemann",
      "created_at": "2025-09-02T15:10:47Z",
      "url": "https://github.com/fastapi/fastapi/pull/14015#discussion_r2316393300"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14015,
      "file_path": "scripts/translate.py",
      "line": 804,
      "side": "RIGHT",
      "diff_hunk": "@@ -182,39 +783,72 @@ def iter_all_en_paths() -> Iterable[Path]:\n \n \n def iter_en_paths_to_translate() -> Iterable[Path]:\n+    en_docs_root = Path(\"docs/en/docs/\")\n     for path in iter_all_en_paths():\n-        if str(path).replace(\"docs/en/docs/\", \"\").startswith(non_translated_sections):\n-            continue\n-        yield path\n+        relpath = path.relative_to(en_docs_root)\n+        if not str(relpath).startswith(non_translated_sections):\n+            yield path\n \n \n @app.command()\n-def translate_lang(language: Annotated[str, typer.Option(envvar=\"LANGUAGE\")]) -> None:\n-    paths_to_process = list(iter_en_paths_to_translate())\n-    print(\"Original paths:\")\n-    for p in paths_to_process:\n-        print(f\"  - {p}\")\n-    print(f\"Total original paths: {len(paths_to_process)}\")\n+def translate_lang(\n+    language: Annotated[\n+        str,\n+        typer.Option(envvar=\"LANGUAGE\", help=\"Target language, e.g. `es`, `fr`, `de`\"),\n+    ],\n+    mode: Annotated[\n+        str,\n+        typer.Option(\n+            help=\"Which files of the target language to translate, one of: `missing`, `existing`, `all`\"\n+        ),\n+    ] = \"missing\",",
      "comment": "(waiting for tiangolo's verdict regarding removal of this function)",
      "comment_id": 2316407757,
      "user": "nilslindemann",
      "created_at": "2025-09-02T15:15:58Z",
      "url": "https://github.com/fastapi/fastapi/pull/14015#discussion_r2316407757"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14015,
      "file_path": "scripts/translate.py",
      "line": 750,
      "side": "RIGHT",
      "diff_hunk": "@@ -128,16 +743,19 @@ def translate_page(\n     if old_translation:\n         prompt_segments.extend(\n             [\n-                \"There's an existing previous translation for this content that is probably outdated with old content or old instructions.\",\n+                \"There is an existing previous translation for the original English content, that may be outdated.\",\n                 \"Update the translation only where necessary:\",\n-                \"- If the original English content has changed, reflect that in the translation.\",\n+                \"- If the original English content has added parts, also add these parts to the translation.\",\n+                \"- If the original English content has removed parts, also remove them from the translation, unless you were instructed earlier to not do that in specific cases.\",\n+                \"- If parts of the original English content have changed, also change those parts in the translation.\",",
      "comment": "I think this is a tricky part.\r\nWe don't provide old version of En document.\r\nSo, LLM can't distinguish between two cases:\r\n* part of the sentence was removed from the original document\r\n* sentence in the original document remains unchanged, but translation contains additional parts added by human editor (and we instruct LLM to preserve such human edits)\r\n\r\nI'm afraid these instructions will not be followed. So, maybe leave the shorter version?\r\n",
      "comment_id": 2321482022,
      "user": "YuriiMotov",
      "created_at": "2025-09-04T09:52:43Z",
      "url": "https://github.com/fastapi/fastapi/pull/14015#discussion_r2321482022"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14015,
      "file_path": "scripts/translate.py",
      "line": 750,
      "side": "RIGHT",
      "diff_hunk": "@@ -128,16 +743,19 @@ def translate_page(\n     if old_translation:\n         prompt_segments.extend(\n             [\n-                \"There's an existing previous translation for this content that is probably outdated with old content or old instructions.\",\n+                \"There is an existing previous translation for the original English content, that may be outdated.\",\n                 \"Update the translation only where necessary:\",\n-                \"- If the original English content has changed, reflect that in the translation.\",\n+                \"- If the original English content has added parts, also add these parts to the translation.\",\n+                \"- If the original English content has removed parts, also remove them from the translation, unless you were instructed earlier to not do that in specific cases.\",\n+                \"- If parts of the original English content have changed, also change those parts in the translation.\",",
      "comment": "We can probably use some kind of markers to mark such human editor's additions.\r\n\r\nOriginal:\r\n```\r\nThis is original text of the sentence\r\n```\r\n\r\nTranslation:\r\n```\r\nThis is original text <!-- PRESERVE --> (modified a bit) <!-- END PRESERVE --> of the sentence\r\n```\r\nOr:\r\n```\r\nThis is original text <span class=\"human-edit\"> (modified a bit) </span> of the sentence\r\n```\r\n",
      "comment_id": 2321498403,
      "user": "YuriiMotov",
      "created_at": "2025-09-04T09:59:30Z",
      "url": "https://github.com/fastapi/fastapi/pull/14015#discussion_r2321498403"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14015,
      "file_path": "scripts/translate.py",
      "line": 750,
      "side": "RIGHT",
      "diff_hunk": "@@ -128,16 +743,19 @@ def translate_page(\n     if old_translation:\n         prompt_segments.extend(\n             [\n-                \"There's an existing previous translation for this content that is probably outdated with old content or old instructions.\",\n+                \"There is an existing previous translation for the original English content, that may be outdated.\",\n                 \"Update the translation only where necessary:\",\n-                \"- If the original English content has changed, reflect that in the translation.\",\n+                \"- If the original English content has added parts, also add these parts to the translation.\",\n+                \"- If the original English content has removed parts, also remove them from the translation, unless you were instructed earlier to not do that in specific cases.\",\n+                \"- If parts of the original English content have changed, also change those parts in the translation.\",",
      "comment": "But still, if the original text was modified, this addition may also become invalid",
      "comment_id": 2321500718,
      "user": "YuriiMotov",
      "created_at": "2025-09-04T10:00:31Z",
      "url": "https://github.com/fastapi/fastapi/pull/14015#discussion_r2321500718"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14015,
      "file_path": "scripts/translate.py",
      "line": 750,
      "side": "RIGHT",
      "diff_hunk": "@@ -128,16 +743,19 @@ def translate_page(\n     if old_translation:\n         prompt_segments.extend(\n             [\n-                \"There's an existing previous translation for this content that is probably outdated with old content or old instructions.\",\n+                \"There is an existing previous translation for the original English content, that may be outdated.\",\n                 \"Update the translation only where necessary:\",\n-                \"- If the original English content has changed, reflect that in the translation.\",\n+                \"- If the original English content has added parts, also add these parts to the translation.\",\n+                \"- If the original English content has removed parts, also remove them from the translation, unless you were instructed earlier to not do that in specific cases.\",\n+                \"- If parts of the original English content have changed, also change those parts in the translation.\",",
      "comment": "As I understand it ...\r\n\r\n```\r\nIf the original English content has added parts (...)\r\nIf the original English content has removed parts (...)\r\nIf parts of the original English content have changed (...)\r\n```\r\n\r\n... means _\"compared to the existing translation\"_ and not _\"compared to the previous version of the English document\"_. Do you agree? If so, then I do not see an issue, do you?\r\n\r\nNotice, the shorter version did not add a part to a sentence, in a situation when retranslating the test file ([this sentence](https://github.com/fastapi/fastapi/pull/14015/files#diff-a8f0312a235b9188421c47b1ff84a2692f65601a22b4da0c660f9b6968270ce2R9)). It was stubborn, even when I retranslated again. After I changed the prompt, it detected the addition.\r\n\r\nRegarding comment markers, I also thought about this one, and the problem you mentioned (when the addition may become invalid), and I found no good solution, which is why stuck to just [allow added `abbr`s](https://github.com/fastapi/fastapi/pull/14015/files#diff-83e241ff55d1dc272582b5e61fd0582a36e2c2bea760d6b028ffd63f8e0a50e6R666), which are removed when the sentence gets removed.\r\n\r\nBTW, I will make another complete retranslation before merging, to check if it works with the updated prompts.",
      "comment_id": 2323501046,
      "user": "nilslindemann",
      "created_at": "2025-09-04T20:38:21Z",
      "url": "https://github.com/fastapi/fastapi/pull/14015#discussion_r2323501046"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14015,
      "file_path": "scripts/translate.py",
      "line": 750,
      "side": "RIGHT",
      "diff_hunk": "@@ -128,16 +743,19 @@ def translate_page(\n     if old_translation:\n         prompt_segments.extend(\n             [\n-                \"There's an existing previous translation for this content that is probably outdated with old content or old instructions.\",\n+                \"There is an existing previous translation for the original English content, that may be outdated.\",\n                 \"Update the translation only where necessary:\",\n-                \"- If the original English content has changed, reflect that in the translation.\",\n+                \"- If the original English content has added parts, also add these parts to the translation.\",\n+                \"- If the original English content has removed parts, also remove them from the translation, unless you were instructed earlier to not do that in specific cases.\",\n+                \"- If parts of the original English content have changed, also change those parts in the translation.\",",
      "comment": "To clarify: The moment it detected the addition was when I added the [last sentence](https://github.com/fastapi/fastapi/pull/14015/files#diff-83e241ff55d1dc272582b5e61fd0582a36e2c2bea760d6b028ffd63f8e0a50e6R758). So it is possible that it also works well when just keeping that change and reverting the others. But I find that the rule is formulated more precise this way, as it mentions when not to remove things, So I am in favor of keeping it like this. If you insist, let's revert it, but keep the last sentence.",
      "comment_id": 2323523354,
      "user": "nilslindemann",
      "created_at": "2025-09-04T20:51:07Z",
      "url": "https://github.com/fastapi/fastapi/pull/14015#discussion_r2323523354"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14015,
      "file_path": "scripts/translate.py",
      "line": 750,
      "side": "RIGHT",
      "diff_hunk": "@@ -128,16 +743,19 @@ def translate_page(\n     if old_translation:\n         prompt_segments.extend(\n             [\n-                \"There's an existing previous translation for this content that is probably outdated with old content or old instructions.\",\n+                \"There is an existing previous translation for the original English content, that may be outdated.\",\n                 \"Update the translation only where necessary:\",\n-                \"- If the original English content has changed, reflect that in the translation.\",\n+                \"- If the original English content has added parts, also add these parts to the translation.\",\n+                \"- If the original English content has removed parts, also remove them from the translation, unless you were instructed earlier to not do that in specific cases.\",\n+                \"- If parts of the original English content have changed, also change those parts in the translation.\",",
      "comment": "BTW, I am sorry for not adding you as co-author in some commits, I just learned about this feature (the button in GitHub Desktop).",
      "comment_id": 2323838905,
      "user": "nilslindemann",
      "created_at": "2025-09-05T01:06:10Z",
      "url": "https://github.com/fastapi/fastapi/pull/14015#discussion_r2323838905"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14015,
      "file_path": "scripts/translate.py",
      "line": 750,
      "side": "RIGHT",
      "diff_hunk": "@@ -128,16 +743,19 @@ def translate_page(\n     if old_translation:\n         prompt_segments.extend(\n             [\n-                \"There's an existing previous translation for this content that is probably outdated with old content or old instructions.\",\n+                \"There is an existing previous translation for the original English content, that may be outdated.\",\n                 \"Update the translation only where necessary:\",\n-                \"- If the original English content has changed, reflect that in the translation.\",\n+                \"- If the original English content has added parts, also add these parts to the translation.\",\n+                \"- If the original English content has removed parts, also remove them from the translation, unless you were instructed earlier to not do that in specific cases.\",\n+                \"- If parts of the original English content have changed, also change those parts in the translation.\",",
      "comment": "When you refer to the last sentence, do you mean this?\r\n> UNLESS you were instructed earlier to behave different, there MUST NOT be whole sentences or partial sentences in the updated translation, which are not in the original English content, and there MUST NOT be whole sentences or partial sentences in the original English content, which are not in the updated translation. Remember: the updated translation shall be IN SYNC with the original English content.\r\n\r\nThe links you provided point to a big diff and I can't understand which sentences you are talking about..",
      "comment_id": 2324106397,
      "user": "YuriiMotov",
      "created_at": "2025-09-05T05:04:17Z",
      "url": "https://github.com/fastapi/fastapi/pull/14015#discussion_r2324106397"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14015,
      "file_path": "scripts/translate.py",
      "line": 750,
      "side": "RIGHT",
      "diff_hunk": "@@ -128,16 +743,19 @@ def translate_page(\n     if old_translation:\n         prompt_segments.extend(\n             [\n-                \"There's an existing previous translation for this content that is probably outdated with old content or old instructions.\",\n+                \"There is an existing previous translation for the original English content, that may be outdated.\",\n                 \"Update the translation only where necessary:\",\n-                \"- If the original English content has changed, reflect that in the translation.\",\n+                \"- If the original English content has added parts, also add these parts to the translation.\",\n+                \"- If the original English content has removed parts, also remove them from the translation, unless you were instructed earlier to not do that in specific cases.\",\n+                \"- If parts of the original English content have changed, also change those parts in the translation.\",",
      "comment": "But, it seems that I got it)\r\nWe only preserve `abbr` elements added by human editor.\r\nThen yes, it should work",
      "comment_id": 2324114367,
      "user": "YuriiMotov",
      "created_at": "2025-09-05T05:09:32Z",
      "url": "https://github.com/fastapi/fastapi/pull/14015#discussion_r2324114367"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14015,
      "file_path": "scripts/translate.py",
      "line": 750,
      "side": "RIGHT",
      "diff_hunk": "@@ -128,16 +743,19 @@ def translate_page(\n     if old_translation:\n         prompt_segments.extend(\n             [\n-                \"There's an existing previous translation for this content that is probably outdated with old content or old instructions.\",\n+                \"There is an existing previous translation for the original English content, that may be outdated.\",\n                 \"Update the translation only where necessary:\",\n-                \"- If the original English content has changed, reflect that in the translation.\",\n+                \"- If the original English content has added parts, also add these parts to the translation.\",\n+                \"- If the original English content has removed parts, also remove them from the translation, unless you were instructed earlier to not do that in specific cases.\",\n+                \"- If parts of the original English content have changed, also change those parts in the translation.\",",
      "comment": "Yes, I meant this sentence. It takes a while to load for such a link when the diff is big in the GitHub GUI.",
      "comment_id": 2324803331,
      "user": "nilslindemann",
      "created_at": "2025-09-05T11:12:56Z",
      "url": "https://github.com/fastapi/fastapi/pull/14015#discussion_r2324803331"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14015,
      "file_path": "scripts/translate.py",
      "line": 806,
      "side": "RIGHT",
      "diff_hunk": "@@ -182,39 +783,72 @@ def iter_all_en_paths() -> Iterable[Path]:\n \n \n def iter_en_paths_to_translate() -> Iterable[Path]:\n+    en_docs_root = Path(\"docs/en/docs/\")\n     for path in iter_all_en_paths():\n-        if str(path).replace(\"docs/en/docs/\", \"\").startswith(non_translated_sections):\n-            continue\n-        yield path\n+        relpath = path.relative_to(en_docs_root)\n+        if not str(relpath).startswith(non_translated_sections):\n+            yield path\n \n \n @app.command()\n-def translate_lang(language: Annotated[str, typer.Option(envvar=\"LANGUAGE\")]) -> None:\n-    paths_to_process = list(iter_en_paths_to_translate())\n-    print(\"Original paths:\")\n-    for p in paths_to_process:\n-        print(f\"  - {p}\")\n-    print(f\"Total original paths: {len(paths_to_process)}\")\n+def translate_lang(\n+    language: Annotated[\n+        str,\n+        typer.Option(envvar=\"LANGUAGE\", help=\"Target language, e.g. `es`, `fr`, `de`\"),\n+    ],\n+    mode: Annotated[\n+        str,\n+        typer.Option(\n+            help=\"Which files of the target language to translate, one of: `missing`, `existing`, `all`\"\n+        ),\n+    ] = \"missing\",\n+    verbose: Annotated[bool, typer.Option(help=\"Print all paths\")] = False,\n+    preview: Annotated[",
      "comment": "@YuriiMotov, is it okay for you if I do this and the below issue in a separate PR after this one and the PRs above are resolved? After all, this change does not remove any functionality, so nothing breaks for the moment.",
      "comment_id": 2324830540,
      "user": "nilslindemann",
      "created_at": "2025-09-05T11:25:53Z",
      "url": "https://github.com/fastapi/fastapi/pull/14015#discussion_r2324830540"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 12970,
      "file_path": "fastapi/encoders.py",
      "line": 243,
      "side": "RIGHT",
      "diff_hunk": "@@ -240,7 +240,7 @@ def jsonable_encoder(\n             custom_encoder=encoders,\n             sqlalchemy_safe=sqlalchemy_safe,\n         )\n-    if dataclasses.is_dataclass(obj):\n+    if dataclasses.is_dataclass(type(obj)):",
      "comment": "https://github.com/python/typeshed/blob/e1c74f08f12b8bec6afe266951f0756dc1b43ebe/stdlib/dataclasses.pyi#L217-L223",
      "comment_id": 1854356339,
      "user": "tamird",
      "created_at": "2024-11-22T17:30:12Z",
      "url": "https://github.com/fastapi/fastapi/pull/12970#discussion_r1854356339"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 12970,
      "file_path": "fastapi/encoders.py",
      "line": 243,
      "side": "RIGHT",
      "diff_hunk": "@@ -240,7 +240,7 @@ def jsonable_encoder(\n             custom_encoder=encoders,\n             sqlalchemy_safe=sqlalchemy_safe,\n         )\n-    if dataclasses.is_dataclass(obj):\n+    if dataclasses.is_dataclass(type(obj)):",
      "comment": "I have separated this change into a second commit so that you can see the failure in CI. Please have a look.",
      "comment_id": 1864265876,
      "user": "tamird",
      "created_at": "2024-11-30T14:11:12Z",
      "url": "https://github.com/fastapi/fastapi/pull/12970#discussion_r1864265876"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 9704,
      "file_path": "tests/test_response_model_default_factory.py",
      "line": 15,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,52 @@\n+import functools\n+import random\n+\n+from fastapi import FastAPI\n+from fastapi.testclient import TestClient\n+from pydantic import BaseModel, Field\n+\n+app = FastAPI()\n+\n+messages = [\"\u64cd\u4f5c\u6210\u529f.\", \"Successful operation.\"]\n+\n+\n+class ResponseModel(BaseModel):\n+    code: int = 200\n+    message: str = Field(default_factory=functools.partial(random.choice, messages))",
      "comment": "```suggestion\r\n    message: str = Field(default_factory=lambda: \"Successful operation.\")\r\n```",
      "comment_id": 2231038682,
      "user": "YuriiMotov",
      "created_at": "2025-07-25T13:05:43Z",
      "url": "https://github.com/fastapi/fastapi/pull/9704#discussion_r2231038682"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 9704,
      "file_path": "tests/test_response_model_default_factory.py",
      "line": 43,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,52 @@\n+import functools\n+import random\n+\n+from fastapi import FastAPI\n+from fastapi.testclient import TestClient\n+from pydantic import BaseModel, Field\n+\n+app = FastAPI()\n+\n+messages = [\"\u64cd\u4f5c\u6210\u529f.\", \"Successful operation.\"]\n+\n+\n+class ResponseModel(BaseModel):\n+    code: int = 200\n+    message: str = Field(default_factory=functools.partial(random.choice, messages))\n+\n+\n+@app.get(\n+    \"/response_model_has_default_factory_return_dict\",\n+    response_model=ResponseModel,\n+)\n+async def response_model_has_default_factory_return_dict():\n+    return {\"code\": 200}\n+\n+\n+@app.get(\n+    \"/response_model_has_default_factory_return_model\",\n+    response_model=ResponseModel,\n+)\n+async def response_model_has_default_factory_return_model():\n+    return ResponseModel()\n+\n+\n+client = TestClient(app)\n+\n+\n+def test_response_model_has_default_factory_return_dict():\n+    response = client.get(\"/response_model_has_default_factory_return_dict\")\n+\n+    assert response.status_code == 200, response.text\n+\n+    assert response.json()[\"code\"] == 200\n+    assert response.json()[\"message\"] in messages",
      "comment": "```suggestion\r\n    assert response.json()[\"message\"] == \"Successful operation.\"\r\n```",
      "comment_id": 2231039999,
      "user": "YuriiMotov",
      "created_at": "2025-07-25T13:06:25Z",
      "url": "https://github.com/fastapi/fastapi/pull/9704#discussion_r2231039999"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 9704,
      "file_path": "tests/test_response_model_default_factory.py",
      "line": 52,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,52 @@\n+import functools\n+import random\n+\n+from fastapi import FastAPI\n+from fastapi.testclient import TestClient\n+from pydantic import BaseModel, Field\n+\n+app = FastAPI()\n+\n+messages = [\"\u64cd\u4f5c\u6210\u529f.\", \"Successful operation.\"]\n+\n+\n+class ResponseModel(BaseModel):\n+    code: int = 200\n+    message: str = Field(default_factory=functools.partial(random.choice, messages))\n+\n+\n+@app.get(\n+    \"/response_model_has_default_factory_return_dict\",\n+    response_model=ResponseModel,\n+)\n+async def response_model_has_default_factory_return_dict():\n+    return {\"code\": 200}\n+\n+\n+@app.get(\n+    \"/response_model_has_default_factory_return_model\",\n+    response_model=ResponseModel,\n+)\n+async def response_model_has_default_factory_return_model():\n+    return ResponseModel()\n+\n+\n+client = TestClient(app)\n+\n+\n+def test_response_model_has_default_factory_return_dict():\n+    response = client.get(\"/response_model_has_default_factory_return_dict\")\n+\n+    assert response.status_code == 200, response.text\n+\n+    assert response.json()[\"code\"] == 200\n+    assert response.json()[\"message\"] in messages\n+\n+\n+def test_response_model_has_default_factory_return_model():\n+    response = client.get(\"/response_model_has_default_factory_return_model\")\n+\n+    assert response.status_code == 200, response.text\n+\n+    assert response.json()[\"code\"] == 200\n+    assert response.json()[\"message\"] in messages",
      "comment": "```suggestion\r\n    assert response.json()[\"message\"] == \"Successful operation.\"\r\n```",
      "comment_id": 2231040652,
      "user": "YuriiMotov",
      "created_at": "2025-07-25T13:06:44Z",
      "url": "https://github.com/fastapi/fastapi/pull/9704#discussion_r2231040652"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 9425,
      "file_path": "tests/test_return_none_future_annotations.py",
      "line": 26,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,26 @@\n+from __future__ import annotations\n+\n+import http\n+import logging\n+\n+from fastapi import APIRouter, FastAPI\n+from fastapi.testclient import TestClient\n+\n+router = APIRouter()\n+\n+app = FastAPI()\n+\n+\n+@router.get(\"/no-content\", status_code=http.HTTPStatus.NO_CONTENT)\n+def return_no_content() -> None:\n+    logging.info(\"endpoint called\")\n+\n+\n+app.include_router(router)\n+\n+client = TestClient(app)\n+\n+\n+def test_no_content():\n+    response = client.get(\"/no-content\")\n+    assert response.status_code == http.HTTPStatus.NO_CONTENT, response.text",
      "comment": "```suggestion\r\n    assert response.status_code == http.HTTPStatus.NO_CONTENT, response.text\r\n    assert not response.content\r\n```\r\nHow about adding this assertion?",
      "comment_id": 2162519671,
      "user": "YuriiMotov",
      "created_at": "2025-06-23T20:56:47Z",
      "url": "https://github.com/fastapi/fastapi/pull/9425#discussion_r2162519671"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 4972,
      "file_path": "fastapi/encoders.py",
      "line": 55,
      "side": "RIGHT",
      "diff_hunk": "@@ -52,7 +52,7 @@ def jsonable_encoder(\n     if isinstance(obj, BaseModel):\n         encoder = getattr(obj.__config__, \"json_encoders\", {})\n         if custom_encoder:\n-            encoder.update(custom_encoder)\n+            encoder = {**encoder, **custom_encoder}",
      "comment": "Can you explain, how does this fix the `jsonable_encoder` side effect?",
      "comment_id": 892478985,
      "user": "odiseo0",
      "created_at": "2022-06-08T14:45:23Z",
      "url": "https://github.com/fastapi/fastapi/pull/4972#discussion_r892478985"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 4972,
      "file_path": "fastapi/encoders.py",
      "line": 55,
      "side": "RIGHT",
      "diff_hunk": "@@ -52,7 +52,7 @@ def jsonable_encoder(\n     if isinstance(obj, BaseModel):\n         encoder = getattr(obj.__config__, \"json_encoders\", {})\n         if custom_encoder:\n-            encoder.update(custom_encoder)\n+            encoder = {**encoder, **custom_encoder}",
      "comment": "The following snippet from [slackner](https://github.com/slackner) illustrates the side effect in 2 cases:\r\n\r\n- With `print(c)` : side effect from `b = jsonable_encoder(creds, custom_encoder=ENCODERS)` when calling `jsonable_encoder` later on the same instance but with a different custom_encoder\r\n- With `print(d)` : side effect from `b = jsonable_encoder(creds, custom_encoder=ENCODERS)` when calling `jsonable_encoder` later on another instance\r\n\r\n```python\r\nfrom pydantic import BaseModel, SecretStr\r\nfrom fastapi.encoders import jsonable_encoder\r\n\r\n\r\nclass Credentials(BaseModel):\r\n    password: SecretStr\r\n\r\n\r\nENCODERS = {SecretStr: lambda v: v.get_secret_value() if v is not None else None}\r\n\r\ncreds = Credentials(password=\"helloworld\")\r\n\r\na = jsonable_encoder(creds)\r\nprint(a)  # {'password': '**********'}, as expected\r\nb = jsonable_encoder(creds, custom_encoder=ENCODERS)\r\nprint(b)  # {'password': 'helloworld'}, as expected\r\nc = jsonable_encoder(creds)\r\nprint(c)  # gives {'password': 'helloworld'}, but should be {'password': '**********'}?\r\n\r\n\r\ncreds = Credentials(password=\"123456789\")\r\n\r\nd = jsonable_encoder(creds)\r\nprint(d)  # gives {'password': '123456789'}, but should be {'password': '**********'}?\r\n```\r\n\r\nBefore this PR, calling jsonable_encoder with a custom_encoder updates the global \"json_encoders\" config of the model.\r\nAfter this PR, calling jsonable_encoder with a custom_encoer argument only affects the current call",
      "comment_id": 892603230,
      "user": "aboubacs",
      "created_at": "2022-06-08T16:34:14Z",
      "url": "https://github.com/fastapi/fastapi/pull/4972#discussion_r892603230"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 13639,
      "file_path": "tests/test_openapi_schema_type.py",
      "line": 38,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,50 @@\n+import itertools\n+from typing import List\n+\n+import pytest\n+from fastapi.openapi.constants import TypeValue\n+from fastapi.openapi.models import Schema\n+\n+# Define all possible type values\n+TYPE_VALUES: List[TypeValue] = [\n+    \"array\",\n+    \"boolean\",\n+    \"integer\",\n+    \"null\",\n+    \"number\",\n+    \"object\",\n+    \"string\",\n+]\n+\n+# Generate all combinations of 2 or more types\n+TYPE_COMBINATIONS = [\n+    list(combo)\n+    for size in range(2, len(TYPE_VALUES) + 1)\n+    for combo in itertools.combinations(TYPE_VALUES, size)\n+]\n+\n+\n+@pytest.mark.parametrize(\"type_val\", TYPE_VALUES)\n+def test_schema_type_single_type_value(type_val: TypeValue) -> None:\n+    \"\"\"Test that Schema accepts single TypeValue for type field.\"\"\"\n+    schema = Schema(type=type_val)\n+    assert schema.type == type_val\n+\n+\n+@pytest.mark.parametrize(\"type_list\", TYPE_COMBINATIONS)\n+def test_schema_type_multiple_type_value(type_list: List[TypeValue]) -> None:\n+    \"\"\"Test all possible combinations of TypeValue for Schema type field.\"\"\"\n+    schema = Schema(type=type_list)\n+    assert schema.type == type_list",
      "comment": "```suggestion\r\ndef test_schema_type_multiple_type_value() -> None:\r\n    \"\"\"Test all possible values of TypeValue for Schema type field.\"\"\"\r\n    schema = Schema(type=TYPE_VALUES)\r\n    assert schema.type == TYPE_VALUES\r\n```\r\n\r\nI think now it's too much tests for such small feature. \ud83e\udd23\r\nLet's just test one combination of all possible values?",
      "comment_id": 2228756723,
      "user": "YuriiMotov",
      "created_at": "2025-07-24T14:51:40Z",
      "url": "https://github.com/fastapi/fastapi/pull/13639#discussion_r2228756723"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 13639,
      "file_path": "tests/test_openapi_schema_type.py",
      "line": 24,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,27 @@\n+from typing import List, Optional, Union\n+\n+import pytest\n+from fastapi.openapi.constants import TypeValue\n+from fastapi.openapi.models import Schema\n+\n+\n+@pytest.mark.parametrize(\n+    \"type_value\",\n+    [\n+        \"array\",\n+        [\"string\", \"null\"],\n+        None,\n+    ],\n+)\n+def test_allowed_schema_type(\n+    type_value: Optional[Union[TypeValue, List[TypeValue]]],\n+) -> None:\n+    \"\"\"Test that Schema accepts TypeValue, List[TypeValue] and None for type field.\"\"\"\n+    schema = Schema(type=type_value)\n+    assert schema.type == type_value\n+\n+\n+def test_invlid_type_value() -> None:",
      "comment": "```suggestion\r\ndef test_invalid_type_value() -> None:\r\n```",
      "comment_id": 2229423740,
      "user": "YuriiMotov",
      "created_at": "2025-07-24T19:39:59Z",
      "url": "https://github.com/fastapi/fastapi/pull/13639#discussion_r2229423740"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 13713,
      "file_path": "fastapi/applications.py",
      "line": 819,
      "side": "RIGHT",
      "diff_hunk": "@@ -810,6 +810,47 @@ class Item(BaseModel):\n                 \"\"\"\n             ),\n         ] = True,\n+        openapi_external_docs: Annotated[\n+            Optional[Dict[str, Any]],\n+            Doc(\n+                \"\"\"\n+                This field allows you to provide additional external documentation links\n+                for an API operation or component. It\u2019s an optional field and can be added\n+                to an operation or a tag. If provided, it must be a dictionary containing:",
      "comment": "```suggestion\r\n                This field allows you to provide additional external documentation links.\r\n                If provided, it must be a dictionary containing:\r\n```",
      "comment_id": 2210694116,
      "user": "YuriiMotov",
      "created_at": "2025-07-16T15:04:15Z",
      "url": "https://github.com/fastapi/fastapi/pull/13713#discussion_r2210694116"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 13713,
      "file_path": "fastapi/applications.py",
      "line": 822,
      "side": "RIGHT",
      "diff_hunk": "@@ -810,6 +810,47 @@ class Item(BaseModel):\n                 \"\"\"\n             ),\n         ] = True,\n+        openapi_external_docs: Annotated[\n+            Optional[Dict[str, Any]],\n+            Doc(\n+                \"\"\"\n+                This field allows you to provide additional external documentation links\n+                for an API operation or component. It\u2019s an optional field and can be added\n+                to an operation or a tag. If provided, it must be a dictionary containing:\n+\n+                * `description`: A brief description of the external documentation, which may use\n+                [CommonMark syntax](https://commonmark.org/) for rich text formatting.",
      "comment": "I think we need to make sure it works (CommonMark syntax for rich text formatting)",
      "comment_id": 2210700949,
      "user": "YuriiMotov",
      "created_at": "2025-07-16T15:07:13Z",
      "url": "https://github.com/fastapi/fastapi/pull/13713#discussion_r2210700949"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 13713,
      "file_path": "fastapi/applications.py",
      "line": 841,
      "side": "RIGHT",
      "diff_hunk": "@@ -810,6 +810,47 @@ class Item(BaseModel):\n                 \"\"\"\n             ),\n         ] = True,\n+        openapi_external_docs: Annotated[\n+            Optional[Dict[str, Any]],\n+            Doc(\n+                \"\"\"\n+                This field allows you to provide additional external documentation links\n+                for an API operation or component. It\u2019s an optional field and can be added\n+                to an operation or a tag. If provided, it must be a dictionary containing:\n+\n+                * `description`: A brief description of the external documentation, which may use\n+                [CommonMark syntax](https://commonmark.org/) for rich text formatting.\n+                * `url`: The URL pointing to the external documentation. The value **MUST**\n+                be a valid URL format.\n+\n+                **Example**:\n+\n+                ```python\n+                from fastapi import FastAPI\n+                from typing import Dict, Any\n+                from typing import Optional\n+                from fastapi.openapi.models import Doc\n+                from typing_extensions import Annotated\n+\n+                external_docs: Annotated[Optional[Dict[str, Any]], Doc()] = {\n+                    \"description\": \"Detailed API Reference\",\n+                    \"url\": \"https://example.com/api-docs\",\n+                }\n+\n+                app = FastAPI(openapi_external_docs=external_docs)\n+",
      "comment": "```suggestion\r\n                from fastapi import FastAPI\r\n\r\n                external_docs = {\r\n                    \"description\": \"Detailed API Reference\",\r\n                    \"url\": \"https://example.com/api-docs\",\r\n                }\r\n\r\n                app = FastAPI(openapi_external_docs=external_docs)\r\n```\r\nNo need to specify type explicitly here",
      "comment_id": 2210706514,
      "user": "YuriiMotov",
      "created_at": "2025-07-16T15:09:35Z",
      "url": "https://github.com/fastapi/fastapi/pull/13713#discussion_r2210706514"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 13713,
      "file_path": "fastapi/applications.py",
      "line": 850,
      "side": "RIGHT",
      "diff_hunk": "@@ -810,6 +810,47 @@ class Item(BaseModel):\n                 \"\"\"\n             ),\n         ] = True,\n+        openapi_external_docs: Annotated[\n+            Optional[Dict[str, Any]],\n+            Doc(\n+                \"\"\"\n+                This field allows you to provide additional external documentation links\n+                for an API operation or component. It\u2019s an optional field and can be added\n+                to an operation or a tag. If provided, it must be a dictionary containing:\n+\n+                * `description`: A brief description of the external documentation, which may use\n+                [CommonMark syntax](https://commonmark.org/) for rich text formatting.\n+                * `url`: The URL pointing to the external documentation. The value **MUST**\n+                be a valid URL format.\n+\n+                **Example**:\n+\n+                ```python\n+                from fastapi import FastAPI\n+                from typing import Dict, Any\n+                from typing import Optional\n+                from fastapi.openapi.models import Doc\n+                from typing_extensions import Annotated\n+\n+                external_docs: Annotated[Optional[Dict[str, Any]], Doc()] = {\n+                    \"description\": \"Detailed API Reference\",\n+                    \"url\": \"https://example.com/api-docs\",\n+                }\n+\n+                app = FastAPI(openapi_external_docs=external_docs)\n+\n+                ```\n+\n+                ### Explanation:\n+                - `description`: Provides a short description explaining what this external documentation covers.\n+                - `url`: Points to the full external documentation.\n+\n+                **Notes**:\n+                - This field is **optional**; if not included, no external documentation will be linked.\n+                - You can use this feature to link to full API documentation, tutorials, or any other",
      "comment": "```suggestion\r\n```\r\nI think we don't need this. The above explanation should be enough",
      "comment_id": 2210711862,
      "user": "YuriiMotov",
      "created_at": "2025-07-16T15:11:10Z",
      "url": "https://github.com/fastapi/fastapi/pull/13713#discussion_r2210711862"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 13713,
      "file_path": "tests/test_openapi_examples.py",
      "line": 13,
      "side": "RIGHT",
      "diff_hunk": "@@ -5,7 +5,12 @@\n from fastapi.testclient import TestClient\n from pydantic import BaseModel\n \n-app = FastAPI()\n+external_docs = {\n+    \"description\": \"External API documentation.\",\n+    \"url\": \"https://docs.example.com/api-general\",\n+}\n+\n+app = FastAPI(openapi_external_docs=external_docs)",
      "comment": "Not sure this is right place for this. Maybe add this to `tests/main.py` and test in `tests/test_application.py`?",
      "comment_id": 2210726845,
      "user": "YuriiMotov",
      "created_at": "2025-07-16T15:14:24Z",
      "url": "https://github.com/fastapi/fastapi/pull/13713#discussion_r2210726845"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 13713,
      "file_path": "tests/test_openapi_examples.py",
      "line": 13,
      "side": "RIGHT",
      "diff_hunk": "@@ -5,7 +5,12 @@\n from fastapi.testclient import TestClient\n from pydantic import BaseModel\n \n-app = FastAPI()\n+external_docs = {\n+    \"description\": \"External API documentation.\",\n+    \"url\": \"https://docs.example.com/api-general\",\n+}\n+\n+app = FastAPI(openapi_external_docs=external_docs)",
      "comment": "Hi Yurii, thanks a lot for your comments\u2014I really appreciate you taking the time to review this!\r\n\r\nCould you help me understand a bit more why you think this might not be the right place for the example? It seemed like a good fit to include it here, since the file is already dedicated to OpenAPI scenarios and includes examples that demonstrate the various OpenAPI-related options and attributes.",
      "comment_id": 2214533550,
      "user": "cmtoro",
      "created_at": "2025-07-18T00:02:35Z",
      "url": "https://github.com/fastapi/fastapi/pull/13713#discussion_r2214533550"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 13713,
      "file_path": "tests/test_openapi_examples.py",
      "line": 13,
      "side": "RIGHT",
      "diff_hunk": "@@ -5,7 +5,12 @@\n from fastapi.testclient import TestClient\n from pydantic import BaseModel\n \n-app = FastAPI()\n+external_docs = {\n+    \"description\": \"External API documentation.\",\n+    \"url\": \"https://docs.example.com/api-general\",\n+}\n+\n+app = FastAPI(openapi_external_docs=external_docs)",
      "comment": "Hi @cmtoro! You are welcome!\r\n\r\n`test_openapi_examples.py` is intended to test `examples` and `openapi_examples` parameters. Look at tests that are already there and you will see that all of them are about these parameters.\r\n\r\n`tests/main.py` includes a lot of different use-cases and `tests/test_application.py` tests openapi schema of app from `tests/main.py`.",
      "comment_id": 2215173315,
      "user": "YuriiMotov",
      "created_at": "2025-07-18T06:58:45Z",
      "url": "https://github.com/fastapi/fastapi/pull/13713#discussion_r2215173315"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14077,
      "file_path": "docs_src/handling_errors/tutorial005.py",
      "line": 15,
      "side": "RIGHT",
      "diff_hunk": "@@ -10,7 +10,9 @@\n @app.exception_handler(RequestValidationError)\n async def validation_exception_handler(request: Request, exc: RequestValidationError):\n     return JSONResponse(\n-        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n+        # 422 = fastapi.status.HTTP_422_UNPROCESSABLE_CONTENT (RFC 9110,\n+        # Starlette >=0.48), previously HTTP_422_UNPROCESSABLE_ENTITY\n+        status_code=422,",
      "comment": "We have to be carefull here, this script is referenced in the docs [here](https://fastapi.tiangolo.com/tutorial/handling-errors/#use-the-requestvalidationerror-body), and by adding more lines, the wrong line will be highlighted in the docs (used to be L14 which is now L16).\r\n\r\nAnyway, I don't think we need this comment here, or elsewhere where we're switching to 422. Let's just edit to `status_code=422` without the comments.",
      "comment_id": 2349101580,
      "user": "svlandeg",
      "created_at": "2025-09-15T13:58:01Z",
      "url": "https://github.com/fastapi/fastapi/pull/14077#discussion_r2349101580"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 14077,
      "file_path": "docs_src/handling_errors/tutorial005.py",
      "line": 15,
      "side": "RIGHT",
      "diff_hunk": "@@ -10,7 +10,9 @@\n @app.exception_handler(RequestValidationError)\n async def validation_exception_handler(request: Request, exc: RequestValidationError):\n     return JSONResponse(\n-        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n+        # 422 = fastapi.status.HTTP_422_UNPROCESSABLE_CONTENT (RFC 9110,\n+        # Starlette >=0.48), previously HTTP_422_UNPROCESSABLE_ENTITY\n+        status_code=422,",
      "comment": "> We have to be carefull here, this script is referenced in the docs [here](https://fastapi.tiangolo.com/tutorial/handling-errors/#use-the-requestvalidationerror-body), and by adding more lines, the wrong line will be highlighted in the docs (used to be L14 which is now L16).\r\n> \r\n> Anyway, I don't think we need this comment here, or elsewhere where we're switching to 422. Let's just edit to `status_code=422` without the comments.\r\n\r\nThanks. Your comment-removal commit looks good.",
      "comment_id": 2351646324,
      "user": "musicinmybrain",
      "created_at": "2025-09-16T09:21:29Z",
      "url": "https://github.com/fastapi/fastapi/pull/14077#discussion_r2351646324"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 13259,
      "file_path": "docs_src/app_testing/tutorial003.py",
      "line": 24,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,15 +1,27 @@\n+from contextlib import asynccontextmanager\n+\n from fastapi import FastAPI\n from fastapi.testclient import TestClient\n \n-app = FastAPI()\n-\n items = {}\n \n \n-@app.on_event(\"startup\")\n-async def startup_event():\n+@asynccontextmanager\n+async def lifespan(app: FastAPI):\n     items[\"foo\"] = {\"name\": \"Fighters\"}\n     items[\"bar\"] = {\"name\": \"Tenders\"}\n+    yield\n+    # clean up items or other work\n+    ...\n+\n+\n+app = FastAPI(lifespan=lifespan)\n+\n+# startup event and shutdown event are deprecated, you should use lifespan instead\n+# @app.on_event(\"startup\")\n+# async def startup_event():\n+#     items[\"foo\"] = {\"name\": \"Fighters\"}\n+#     items[\"bar\"] = {\"name\": \"Tenders\"}",
      "comment": "To ensure the code with startup\\shutdown is still tested, I would split this code example into two separate: first is for `lifespan`, second is for `startup` and `shutdown`.\r\nWe can add the warning that it's deprecated outside the code block.\r\nWhat do you think?",
      "comment_id": 2206838994,
      "user": "YuriiMotov",
      "created_at": "2025-07-15T08:34:15Z",
      "url": "https://github.com/fastapi/fastapi/pull/13259#discussion_r2206838994"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 13259,
      "file_path": "docs_src/app_testing/tutorial003.py",
      "line": 24,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,15 +1,27 @@\n+from contextlib import asynccontextmanager\n+\n from fastapi import FastAPI\n from fastapi.testclient import TestClient\n \n-app = FastAPI()\n-\n items = {}\n \n \n-@app.on_event(\"startup\")\n-async def startup_event():\n+@asynccontextmanager\n+async def lifespan(app: FastAPI):\n     items[\"foo\"] = {\"name\": \"Fighters\"}\n     items[\"bar\"] = {\"name\": \"Tenders\"}\n+    yield\n+    # clean up items or other work\n+    ...\n+\n+\n+app = FastAPI(lifespan=lifespan)\n+\n+# startup event and shutdown event are deprecated, you should use lifespan instead\n+# @app.on_event(\"startup\")\n+# async def startup_event():\n+#     items[\"foo\"] = {\"name\": \"Fighters\"}\n+#     items[\"bar\"] = {\"name\": \"Tenders\"}",
      "comment": "Look at the original code of `tests/test_tutorial/test_testing/test_tutorial003.py`:\r\n```python\r\ndef test_main():\r\n    with pytest.warns(DeprecationWarning):\r\n        from docs_src.app_testing.tutorial003 import test_read_items\r\n    test_read_items()\r\n```",
      "comment_id": 2207159264,
      "user": "YuriiMotov",
      "created_at": "2025-07-15T11:01:01Z",
      "url": "https://github.com/fastapi/fastapi/pull/13259#discussion_r2207159264"
    },
    {
      "repo": "fastapi/fastapi",
      "pr_number": 13259,
      "file_path": "docs_src/app_testing/tutorial003.py",
      "line": 24,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,15 +1,27 @@\n+from contextlib import asynccontextmanager\n+\n from fastapi import FastAPI\n from fastapi.testclient import TestClient\n \n-app = FastAPI()\n-\n items = {}\n \n \n-@app.on_event(\"startup\")\n-async def startup_event():\n+@asynccontextmanager\n+async def lifespan(app: FastAPI):\n     items[\"foo\"] = {\"name\": \"Fighters\"}\n     items[\"bar\"] = {\"name\": \"Tenders\"}\n+    yield\n+    # clean up items or other work\n+    ...\n+\n+\n+app = FastAPI(lifespan=lifespan)\n+\n+# startup event and shutdown event are deprecated, you should use lifespan instead\n+# @app.on_event(\"startup\")\n+# async def startup_event():\n+#     items[\"foo\"] = {\"name\": \"Fighters\"}\n+#     items[\"bar\"] = {\"name\": \"Tenders\"}",
      "comment": "oh! I missed this.\r\n\r\nSorry, I basically forgot the content of this PR...hh",
      "comment_id": 2207165455,
      "user": "z0z0r4",
      "created_at": "2025-07-15T11:04:39Z",
      "url": "https://github.com/fastapi/fastapi/pull/13259#discussion_r2207165455"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12494,
      "file_path": "tests/test_json_schema.py",
      "line": 7198,
      "side": "RIGHT",
      "diff_hunk": "@@ -7193,3 +7193,25 @@ def test_union_format_primitive_type_array_deduplicated() -> None:\n             ]\n         )\n     ) == {'anyOf': [{'type': 'integer'}, {'type': 'string'}, {'type': 'string', 'maxLength': 1}]}\n+\n+\n+def test_nested_model_deduplication():",
      "comment": "```suggestion\r\ndef test_nested_model_deduplication() -> None:\r\n    \"\"\"https://github.com/pydantic/pydantic/issues/12492\"\"\"\r\n\r\n```",
      "comment_id": 2502696314,
      "user": "Viicos",
      "created_at": "2025-11-07T10:36:26Z",
      "url": "https://github.com/pydantic/pydantic/pull/12494#discussion_r2502696314"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12741,
      "file_path": "tests/test_dataclasses.py",
      "line": 3308,
      "side": "RIGHT",
      "diff_hunk": "@@ -3282,3 +3282,27 @@ class Foo:\n     assert ta.dump_json(Foo(foo='bar', bar=1)).decode('utf-8') == '{\"bar\":1}'\n     assert ta.dump_json(Foo(foo='bar', bar=1), exclude={'bar'}).decode('utf-8') == '{}'\n     assert ta.dump_json(Foo(foo='bar', bar=2)).decode('utf-8') == '{}'\n+\n+\n+@pytest.mark.skipif(sys.version_info < (3, 10), reason='kw_only is not available in python < 3.10')\n+@pytest.mark.parametrize('class_kw_only', [True, False], ids=['class_kw=True', 'class_kw=False'])\n+@pytest.mark.parametrize('field_kw_only', [True, False], ids=['field_kw=True', 'field_kw=False'])\n+def test_dataclass_field_override_kw_only(class_kw_only, field_kw_only) -> None:\n+    \"\"\"\n+    Verifies that pydantic.Field(kw_only=...) is able to correctly\n+        override class-level kw_only\n+    \"\"\"\n+\n+    @pydantic.dataclasses.dataclass(kw_only=class_kw_only)\n+    class Foo:\n+        a: int = Field(kw_only=field_kw_only)\n+\n+    sig = str(inspect.signature(Foo))\n+    if (field_kw_only) or (class_kw_only and field_kw_only is None):\n+        with pytest.raises(ValidationError):\n+            Foo(1)\n+\n+        assert re.match(r'\\(\\*,.+', sig)\n+\n+    else:\n+        assert re.match(r'\\([^\\*]+', sig)",
      "comment": "```suggestion\r\n@pytest.mark.skipif(sys.version_info < (3, 10), reason='kw_only is not available in python >= 3.10')\r\ndef test_dataclass_field_override_kw_only() -> None:\r\n    \"\"\"https://github.com/pydantic/pydantic/issues/12736\"\"\"\r\n\r\n    @pydantic.dataclasses.dataclass(kw_only=True)\r\n    class Foo:\r\n        a: int = Field(kw_only=False)\r\n\r\n    a_param = inspect.signature(Foo).parameters['a']\r\n\r\n    assert a_param.kind is inspect.Parameter.POSITIONAL_OR_KEYWORD\r\n    assert a_param.default is inspect.Parameter.empty\r\n```",
      "comment_id": 2726927326,
      "user": "Viicos",
      "created_at": "2026-01-26T09:41:26Z",
      "url": "https://github.com/pydantic/pydantic/pull/12741#discussion_r2726927326"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12712,
      "file_path": "tests/test_pickle.py",
      "line": 328,
      "side": "RIGHT",
      "diff_hunk": "@@ -318,3 +321,51 @@ def test_pickle_model_with_config(model_type: type, use_cloudpickle: bool):\n         model_type = pickle.loads(pickle.dumps(model_type))\n \n     assert model_type.model_config['title'] == 'MyTitle'\n+\n+\n+@pytest.mark.skipif(\n+    sys.version_info[:2] != (3, 14),\n+    reason='Pickle data generated on 3.14',",
      "comment": "Rather than hard-coding, could potentially use `multiprocessing` to produce a pickle bytes, to unpickle in the current process?",
      "comment_id": 2701057019,
      "user": "davidhewitt",
      "created_at": "2026-01-17T12:38:30Z",
      "url": "https://github.com/pydantic/pydantic/pull/12712#discussion_r2701057019"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12712,
      "file_path": "tests/test_pickle.py",
      "line": 328,
      "side": "RIGHT",
      "diff_hunk": "@@ -318,3 +321,51 @@ def test_pickle_model_with_config(model_type: type, use_cloudpickle: bool):\n         model_type = pickle.loads(pickle.dumps(model_type))\n \n     assert model_type.model_config['title'] == 'MyTitle'\n+\n+\n+@pytest.mark.skipif(\n+    sys.version_info[:2] != (3, 14),\n+    reason='Pickle data generated on 3.14',",
      "comment": "It doesn't pickle it properly, that's why I went with this option.\r\n\r\n```python\r\nfrom multiprocessing import Process, Manager\r\n\r\nimport cloudpickle\r\n\r\nfrom pydantic import BaseModel\r\n\r\n\r\nclass Foo(BaseModel):\r\n    foo: int\r\n\r\n\r\nclass Bar(BaseModel):\r\n    bar1: Foo\r\n    bar2: Foo\r\n\r\n\r\ndef bar_repr() -> str:\r\n    json = '{\"bar1\": {\"foo\": 1}, \"bar2\": {\"foo\": 2}}'\r\n    bar = Bar.model_validate_json(json)\r\n    return repr(bar)\r\n\r\n\r\ndef create_pickled(return_dict):\r\n    pickled = cloudpickle.dumps(bar_repr)\r\n    return_dict['return'] = pickled\r\n\r\ndef test_tmp():\r\n    manager = Manager()\r\n    return_dict = manager.dict()\r\n\r\n    p = Process(target=create_pickled, args=(return_dict,))\r\n    p.start()\r\n    p.join()\r\n\r\n    print(return_dict)\r\n    #> {'return': b'\\x80\\x05\\x95\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x11tests.test_pickle\\x94\\x8c\\x08bar_repr\\x94\\x93\\x94.'}\r\n```",
      "comment_id": 2701209494,
      "user": "Viicos",
      "created_at": "2026-01-17T15:05:24Z",
      "url": "https://github.com/pydantic/pydantic/pull/12712#discussion_r2701209494"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12712,
      "file_path": "tests/test_pickle.py",
      "line": 328,
      "side": "RIGHT",
      "diff_hunk": "@@ -318,3 +321,51 @@ def test_pickle_model_with_config(model_type: type, use_cloudpickle: bool):\n         model_type = pickle.loads(pickle.dumps(model_type))\n \n     assert model_type.model_config['title'] == 'MyTitle'\n+\n+\n+@pytest.mark.skipif(\n+    sys.version_info[:2] != (3, 14),\n+    reason='Pickle data generated on 3.14',",
      "comment": "I managed to find a way to make it work with `subprocess`. I've pushed a commit doing so, I also confirmed that the issue reproduces with that approach on the commit before #12689 merged.",
      "comment_id": 2708626832,
      "user": "davidhewitt",
      "created_at": "2026-01-20T14:34:37Z",
      "url": "https://github.com/pydantic/pydantic/pull/12712#discussion_r2708626832"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12712,
      "file_path": "tests/test_pickle.py",
      "line": 328,
      "side": "RIGHT",
      "diff_hunk": "@@ -318,3 +321,51 @@ def test_pickle_model_with_config(model_type: type, use_cloudpickle: bool):\n         model_type = pickle.loads(pickle.dumps(model_type))\n \n     assert model_type.model_config['title'] == 'MyTitle'\n+\n+\n+@pytest.mark.skipif(\n+    sys.version_info[:2] != (3, 14),\n+    reason='Pickle data generated on 3.14',",
      "comment": "Did not want to mess with `subprocess` initially but this looks simple enough",
      "comment_id": 2709332051,
      "user": "Viicos",
      "created_at": "2026-01-20T17:20:34Z",
      "url": "https://github.com/pydantic/pydantic/pull/12712#discussion_r2709332051"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12681,
      "file_path": "pydantic/_internal/_fields.py",
      "line": 322,
      "side": "RIGHT",
      "diff_hunk": "@@ -312,27 +311,27 @@ def collect_model_fields(  # noqa: C901\n                 f\"Unexpected field with name {ann_name!r}; only 'root' is allowed as a field of a `RootModel`\"\n             )\n \n-        # when building a generic model with `MyModel[int]`, the generic_origin check makes sure we don't get\n-        # \"... shadows an attribute\" warnings\n-        generic_origin = getattr(cls, '__pydantic_generic_metadata__', {}).get('origin')\n         for base in bases:\n-            dataclass_fields = {\n-                field.name for field in (dataclasses.fields(base) if dataclasses.is_dataclass(base) else ())\n-            }\n             if hasattr(base, ann_name):\n+                if ann_name not in cls_annotations:\n+                    # Don't warn when a field exists in a parent class but has not been defined in the current class\n+                    continue\n+\n+                # when building a generic model with `MyModel[int]`, the generic_origin check makes sure we don't get\n+                # \"... shadows an attribute\" warnings\n+                generic_origin = getattr(cls, '__pydantic_generic_metadata__', {}).get('origin')",
      "comment": "I'm surprised this was worth moving to a lower loop level, it's presumably not expensive to evaluate. Should we instead be moving this up to the top level above the loop on line 273?",
      "comment_id": 2697569904,
      "user": "davidhewitt",
      "created_at": "2026-01-16T09:01:57Z",
      "url": "https://github.com/pydantic/pydantic/pull/12681#discussion_r2697569904"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12681,
      "file_path": "pydantic/_internal/_fields.py",
      "line": 329,
      "side": "RIGHT",
      "diff_hunk": "@@ -312,27 +311,27 @@ def collect_model_fields(  # noqa: C901\n                 f\"Unexpected field with name {ann_name!r}; only 'root' is allowed as a field of a `RootModel`\"\n             )\n \n-        # when building a generic model with `MyModel[int]`, the generic_origin check makes sure we don't get\n-        # \"... shadows an attribute\" warnings\n-        generic_origin = getattr(cls, '__pydantic_generic_metadata__', {}).get('origin')\n         for base in bases:\n-            dataclass_fields = {\n-                field.name for field in (dataclasses.fields(base) if dataclasses.is_dataclass(base) else ())\n-            }\n             if hasattr(base, ann_name):\n+                if ann_name not in cls_annotations:\n+                    # Don't warn when a field exists in a parent class but has not been defined in the current class\n+                    continue\n+\n+                # when building a generic model with `MyModel[int]`, the generic_origin check makes sure we don't get\n+                # \"... shadows an attribute\" warnings\n+                generic_origin = getattr(cls, '__pydantic_generic_metadata__', {}).get('origin')\n                 if base is generic_origin:\n                     # Don't warn when \"shadowing\" of attributes in parametrized generics\n                     continue\n \n+                dataclass_fields = {\n+                    field.name for field in (dataclasses.fields(base) if dataclasses.is_dataclass(base) else ())\n+                }",
      "comment": "Is this worth caching in a function-local dict? We might evaluate this repeatedly for each `(ann_name, base)` pair when really it looks like we only need to ever evaluate this once per base?",
      "comment_id": 2697573727,
      "user": "davidhewitt",
      "created_at": "2026-01-16T09:03:02Z",
      "url": "https://github.com/pydantic/pydantic/pull/12681#discussion_r2697573727"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12681,
      "file_path": "pydantic/_internal/_fields.py",
      "line": 322,
      "side": "RIGHT",
      "diff_hunk": "@@ -312,27 +311,27 @@ def collect_model_fields(  # noqa: C901\n                 f\"Unexpected field with name {ann_name!r}; only 'root' is allowed as a field of a `RootModel`\"\n             )\n \n-        # when building a generic model with `MyModel[int]`, the generic_origin check makes sure we don't get\n-        # \"... shadows an attribute\" warnings\n-        generic_origin = getattr(cls, '__pydantic_generic_metadata__', {}).get('origin')\n         for base in bases:\n-            dataclass_fields = {\n-                field.name for field in (dataclasses.fields(base) if dataclasses.is_dataclass(base) else ())\n-            }\n             if hasattr(base, ann_name):\n+                if ann_name not in cls_annotations:\n+                    # Don't warn when a field exists in a parent class but has not been defined in the current class\n+                    continue\n+\n+                # when building a generic model with `MyModel[int]`, the generic_origin check makes sure we don't get\n+                # \"... shadows an attribute\" warnings\n+                generic_origin = getattr(cls, '__pydantic_generic_metadata__', {}).get('origin')",
      "comment": "My assumption was that hitting this code path is really uncommon (in fact I'm even wondering if the origin check is possible; it is currently uncovered and I'm not sure there's a case for this).\r\n\r\nWe could move the `generic_origin` fetch up, although I'm pretty sure both would be equivalent.",
      "comment_id": 2703881914,
      "user": "Viicos",
      "created_at": "2026-01-19T09:12:33Z",
      "url": "https://github.com/pydantic/pydantic/pull/12681#discussion_r2703881914"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12681,
      "file_path": "pydantic/_internal/_fields.py",
      "line": 329,
      "side": "RIGHT",
      "diff_hunk": "@@ -312,27 +311,27 @@ def collect_model_fields(  # noqa: C901\n                 f\"Unexpected field with name {ann_name!r}; only 'root' is allowed as a field of a `RootModel`\"\n             )\n \n-        # when building a generic model with `MyModel[int]`, the generic_origin check makes sure we don't get\n-        # \"... shadows an attribute\" warnings\n-        generic_origin = getattr(cls, '__pydantic_generic_metadata__', {}).get('origin')\n         for base in bases:\n-            dataclass_fields = {\n-                field.name for field in (dataclasses.fields(base) if dataclasses.is_dataclass(base) else ())\n-            }\n             if hasattr(base, ann_name):\n+                if ann_name not in cls_annotations:\n+                    # Don't warn when a field exists in a parent class but has not been defined in the current class\n+                    continue\n+\n+                # when building a generic model with `MyModel[int]`, the generic_origin check makes sure we don't get\n+                # \"... shadows an attribute\" warnings\n+                generic_origin = getattr(cls, '__pydantic_generic_metadata__', {}).get('origin')\n                 if base is generic_origin:\n                     # Don't warn when \"shadowing\" of attributes in parametrized generics\n                     continue\n \n+                dataclass_fields = {\n+                    field.name for field in (dataclasses.fields(base) if dataclasses.is_dataclass(base) else ())\n+                }",
      "comment": "We could have a local cache of the `dataclass_fields` for each base, so that we can reuse it when we process the other `ann_names`, but hitting this path is really uncommon (see added test), so I don't think we should worry too much.",
      "comment_id": 2703888774,
      "user": "Viicos",
      "created_at": "2026-01-19T09:14:39Z",
      "url": "https://github.com/pydantic/pydantic/pull/12681#discussion_r2703888774"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12689,
      "file_path": "pydantic/_internal/_model_construction.py",
      "line": 602,
      "side": "RIGHT",
      "diff_hunk": "@@ -599,6 +599,7 @@ def complete_model_class(\n     raise_errors: bool = True,\n     call_on_complete_hook: bool = True,\n     create_model_module: str | None = None,\n+    rebuild: bool = False,",
      "comment": "```suggestion\r\n    is_force_rebuild: bool = False,\r\n```",
      "comment_id": 2689585331,
      "user": "Viicos",
      "created_at": "2026-01-14T09:08:20Z",
      "url": "https://github.com/pydantic/pydantic/pull/12689#discussion_r2689585331"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12689,
      "file_path": "pydantic/_internal/_model_construction.py",
      "line": 602,
      "side": "RIGHT",
      "diff_hunk": "@@ -599,6 +599,7 @@ def complete_model_class(\n     raise_errors: bool = True,\n     call_on_complete_hook: bool = True,\n     create_model_module: str | None = None,\n+    rebuild: bool = False,",
      "comment": "Just amended this here (not also in SchemaValidator/PluggableSchemaValidator) since they were not commented on [but let me know if that's the wrong assumption]",
      "comment_id": 2690124736,
      "user": "lmmx",
      "created_at": "2026-01-14T11:47:48Z",
      "url": "https://github.com/pydantic/pydantic/pull/12689#discussion_r2690124736"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12568,
      "file_path": "pydantic-core/tests/serializers/test_functions.py",
      "line": 671,
      "side": "RIGHT",
      "diff_hunk": "@@ -667,7 +668,10 @@ def f(value, handler, _info):\n \n \n @pytest.mark.skipif(\n-    platform.python_implementation() in ('PyPy', 'GraalVM') or sys.platform in {'emscripten', 'win32'},\n+    # Also skip in local, as segfaults may happen on some platforms for Python < 3.14:",
      "comment": "```suggestion\n    # Also skip in local, as segfaults may happen on some platforms for Python < 3.14:\n    # TODO: re-enable locally after https://github.com/pydantic/pydantic/issues/12592\n```",
      "comment_id": 2697658712,
      "user": "davidhewitt",
      "created_at": "2026-01-16T09:25:16Z",
      "url": "https://github.com/pydantic/pydantic/pull/12568#discussion_r2697658712"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12522,
      "file_path": "tests/test_missing_sentinel.py",
      "line": 74,
      "side": "RIGHT",
      "diff_hunk": "@@ -69,3 +69,14 @@ class Model(BaseModel):\n     assert Model.model_json_schema()['properties'] == {\n         'f': {'title': 'F', 'type': 'integer'},\n     }\n+\n+\n+def test_model_construct_with_missing_default_does_not_crash():",
      "comment": "```suggestion\r\ndef test_model_construct_with_missing_default_does_not_crash() -> None:\r\n```",
      "comment_id": 2516006789,
      "user": "Viicos",
      "created_at": "2025-11-11T22:26:45Z",
      "url": "https://github.com/pydantic/pydantic/pull/12522#discussion_r2516006789"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12604,
      "file_path": "tests/types/test_union.py",
      "line": 75,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,75 @@\n+from typing import ClassVar, Literal, TypedDict\n+\n+import pydantic\n+\n+\n+def test_field_serializer_in_nested_union_called_only_twice():\n+    class MyModel(pydantic.BaseModel):\n+        a: int\n+        b: int\n+\n+        field_a_serializer_calls: ClassVar[int] = 0\n+\n+        @pydantic.field_serializer('a')\n+        def serialize_my_field(self, value: int) -> str:\n+            self.__class__.field_a_serializer_calls += 1\n+            return str(value)\n+\n+    class Container(TypedDict):\n+        u: MyModel | int\n+\n+    class Container2(TypedDict):\n+        u: Container | int\n+\n+    value = MyModel(a=1, b=False)\n+    assert value.b is False\n+\n+    ta = pydantic.TypeAdapter(Container2 | int)\n+    ta.dump_json(Container2(u=Container(u=value)), warnings=False)\n+\n+    # Historical implementations of pydantic would call the field serializer many times\n+    # as nested unions were individually attempted with each of strict and lax checking.\n+    #\n+    # 2 comes from:\n+    # - one attempt in strict mode, which fails because of `b=False` as a subclass\n+    # - one attempt in lax mode, which succeeds\n+    assert MyModel.field_a_serializer_calls == 2\n+\n+\n+def test_field_serializer_in_nested_tagged_union_called_only_twice():\n+    class MyModel(pydantic.BaseModel):\n+        type_: Literal['a'] = 'a'\n+\n+        a: int\n+        b: int\n+\n+        field_a_serializer_calls: ClassVar[int] = 0\n+\n+        @pydantic.field_serializer('a')\n+        def serialize_my_field(self, value: int) -> str:\n+            self.__class__.field_a_serializer_calls += 1\n+            return str(value)\n+\n+    class ModelB(pydantic.BaseModel):\n+        type_: Literal['b'] = 'b'\n+\n+    class Container(pydantic.BaseModel):\n+        type_: Literal['a'] = 'a'\n+        u: MyModel | ModelB = pydantic.Field(..., discriminator='type_')\n+\n+    class Container2(pydantic.BaseModel):\n+        u: Container | ModelB = pydantic.Field(..., discriminator='type_')\n+\n+    ta = pydantic.TypeAdapter(Container2 | int)\n+    ta.dump_json(Container2(u=Container(u=MyModel.model_construct(a=1, b=False))), warnings=False)\n+\n+    # Historical implementations of pydantic would call the field serializer many MANY times\n+    # as nested unions were individually attempted with each of strict and lax checking.\n+    #\n+    # 5 comes from:\n+    # - tagged discriminator in outer union at strict mode\n+    # - fall back to left to right in outer union at strict mode\n+    # - tagged discriminator in inner union at strict mode\n+    # - fall back to left to right in inner union still at strict mode\n+    # - tagged discriminator in outer union at lax mode, which calls tagged discriminator in inner union at lax mode, which finally succeeds\n+    assert MyModel.field_a_serializer_calls == 5",
      "comment": "I think we could do better here; when making tagged union serialization fall back to left-to-right we could potentially try all variants except the one which we already attempted (and failed) in the tagged union serialization.\n\ni.e. this should be 2 like above, I think.",
      "comment_id": 2584568651,
      "user": "davidhewitt",
      "created_at": "2025-12-03T10:39:37Z",
      "url": "https://github.com/pydantic/pydantic/pull/12604#discussion_r2584568651"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12627,
      "file_path": "tests/test_type_adapter.py",
      "line": 690,
      "side": "RIGHT",
      "diff_hunk": "@@ -681,3 +681,30 @@ def module_1() -> None:\n \n     with pytest.raises(ValidationError):\n         module_1.ta.validate_python('a')\n+\n+\n+def test_validate_python_with_incorrect_configuration():\n+    ta = TypeAdapter(int)\n+\n+    with pytest.raises(PydanticUserError) as exc_info:\n+        ta.validate_python({'foo': [1, '2']}, by_alias=False, by_name=False)",
      "comment": "```suggestion\r\n        ta.validate_python(1, by_alias=False, by_name=False)\r\n```",
      "comment_id": 2618579209,
      "user": "Viicos",
      "created_at": "2025-12-15T09:10:37Z",
      "url": "https://github.com/pydantic/pydantic/pull/12627#discussion_r2618579209"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12627,
      "file_path": "tests/test_type_adapter.py",
      "line": 699,
      "side": "RIGHT",
      "diff_hunk": "@@ -681,3 +681,30 @@ def module_1() -> None:\n \n     with pytest.raises(ValidationError):\n         module_1.ta.validate_python('a')\n+\n+\n+def test_validate_python_with_incorrect_configuration():\n+    ta = TypeAdapter(int)\n+\n+    with pytest.raises(PydanticUserError) as exc_info:\n+        ta.validate_python({'foo': [1, '2']}, by_alias=False, by_name=False)\n+\n+    assert 'At least one of `by_alias` or `by_name` must be set to True.' in str(exc_info.value)\n+\n+\n+def test_validate_json_with_incorrect_configuration():\n+    ta = TypeAdapter(int)\n+\n+    with pytest.raises(PydanticUserError) as exc_info:\n+        ta.validate_json(json.dumps({'x': '1'}), by_alias=False, by_name=False)",
      "comment": "```suggestion\r\n        ta.validate_json(1, by_alias=False, by_name=False)\r\n```",
      "comment_id": 2618580194,
      "user": "Viicos",
      "created_at": "2025-12-15T09:10:55Z",
      "url": "https://github.com/pydantic/pydantic/pull/12627#discussion_r2618580194"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12627,
      "file_path": "tests/test_type_adapter.py",
      "line": 708,
      "side": "RIGHT",
      "diff_hunk": "@@ -681,3 +681,30 @@ def module_1() -> None:\n \n     with pytest.raises(ValidationError):\n         module_1.ta.validate_python('a')\n+\n+\n+def test_validate_python_with_incorrect_configuration():\n+    ta = TypeAdapter(int)\n+\n+    with pytest.raises(PydanticUserError) as exc_info:\n+        ta.validate_python({'foo': [1, '2']}, by_alias=False, by_name=False)\n+\n+    assert 'At least one of `by_alias` or `by_name` must be set to True.' in str(exc_info.value)\n+\n+\n+def test_validate_json_with_incorrect_configuration():\n+    ta = TypeAdapter(int)\n+\n+    with pytest.raises(PydanticUserError) as exc_info:\n+        ta.validate_json(json.dumps({'x': '1'}), by_alias=False, by_name=False)\n+\n+    assert 'At least one of `by_alias` or `by_name` must be set to True.' in str(exc_info.value)\n+\n+\n+def test_validate_strings_with_incorrect_configuration():\n+    ta = TypeAdapter(int)\n+\n+    with pytest.raises(PydanticUserError) as exc_info:\n+        ta.validate_strings({'x': 'true', 'y': 'true'}, by_alias=False, by_name=False)",
      "comment": "```suggestion\r\n        ta.validate_strings(1, by_alias=False, by_name=False)\r\n```",
      "comment_id": 2618580573,
      "user": "Viicos",
      "created_at": "2025-12-15T09:11:03Z",
      "url": "https://github.com/pydantic/pydantic/pull/12627#discussion_r2618580573"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12627,
      "file_path": "tests/test_internal.py",
      "line": 196,
      "side": "RIGHT",
      "diff_hunk": "@@ -188,3 +191,128 @@ def test_decimal_digits_calculation(decimal: Decimal, decimal_places: int, digit\n def test_decimal_digits_calculation_type_error(value) -> None:\n     with pytest.raises(TypeError, match=f'Unable to extract decimal digits info from supplied value {value}'):\n         _extract_decimal_digits_info(value)\n+\n+\n+class TestCoreMetadata:",
      "comment": "We don't use test classes, plain functions are fine.\r\n\r\nLet's keep only `test_update_js_extra_as_callable_when_existing_js_extra_is_dict_type()`, `test_update_js_extra_as_callable_when_existing_js_extra_is_callable_type()`, and a simple test with `pydantic_js_functions` only being provided.",
      "comment_id": 2618639511,
      "user": "Viicos",
      "created_at": "2025-12-15T09:28:20Z",
      "url": "https://github.com/pydantic/pydantic/pull/12627#discussion_r2618639511"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12627,
      "file_path": "tests/test_plugin_loader.py",
      "line": 110,
      "side": "RIGHT",
      "diff_hunk": "@@ -81,3 +93,49 @@ def test_disable_multiple(reset_plugins):\n     assert len(list(res)) == 1\n     assert 'test_plugin:plugin1' not in list(res)\n     assert 'test_plugin:plugin2' not in list(res)\n+\n+\n+def test_caching_of_loaded_plugins(reset_plugins):\n+    os.environ['PYDANTIC_DISABLE_PLUGINS'] = 'test_plugin1,test_plugin2'\n+    res = loader.get_plugins()\n+    assert list(res) == ['test_plugin:plugin3']\n+    res = loader.get_plugins()\n+    assert list(res) == ['test_plugin:plugin3']\n+\n+\n+def test_load_same_plugin_multiple_times(reset_plugins):\n+    mock_entry_1 = EntryPoint(name='test_plugin1', value='test_plugin:plugin1', group='pydantic')\n+    mock_dist = Dist([mock_entry_1, mock_entry_1])\n+\n+    os.environ.pop('PYDANTIC_DISABLE_PLUGINS', None)",
      "comment": "This should be done using the [`monkeypatch` fixture](https://docs.pytest.org/en/stable/reference/reference.html#pytest.MonkeyPatch.setenv).",
      "comment_id": 2618654388,
      "user": "Viicos",
      "created_at": "2025-12-15T09:32:51Z",
      "url": "https://github.com/pydantic/pydantic/pull/12627#discussion_r2618654388"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12627,
      "file_path": "tests/test_internal.py",
      "line": 196,
      "side": "RIGHT",
      "diff_hunk": "@@ -188,3 +191,128 @@ def test_decimal_digits_calculation(decimal: Decimal, decimal_places: int, digit\n def test_decimal_digits_calculation_type_error(value) -> None:\n     with pytest.raises(TypeError, match=f'Unable to extract decimal digits info from supplied value {value}'):\n         _extract_decimal_digits_info(value)\n+\n+\n+class TestCoreMetadata:",
      "comment": "I understand. Removed the class. Retained only the tests sufficient to cover the missed lines.",
      "comment_id": 2620319121,
      "user": "sesmi123",
      "created_at": "2025-12-15T17:50:47Z",
      "url": "https://github.com/pydantic/pydantic/pull/12627#discussion_r2620319121"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12627,
      "file_path": "tests/test_plugin_loader.py",
      "line": 110,
      "side": "RIGHT",
      "diff_hunk": "@@ -81,3 +93,49 @@ def test_disable_multiple(reset_plugins):\n     assert len(list(res)) == 1\n     assert 'test_plugin:plugin1' not in list(res)\n     assert 'test_plugin:plugin2' not in list(res)\n+\n+\n+def test_caching_of_loaded_plugins(reset_plugins):\n+    os.environ['PYDANTIC_DISABLE_PLUGINS'] = 'test_plugin1,test_plugin2'\n+    res = loader.get_plugins()\n+    assert list(res) == ['test_plugin:plugin3']\n+    res = loader.get_plugins()\n+    assert list(res) == ['test_plugin:plugin3']\n+\n+\n+def test_load_same_plugin_multiple_times(reset_plugins):\n+    mock_entry_1 = EntryPoint(name='test_plugin1', value='test_plugin:plugin1', group='pydantic')\n+    mock_dist = Dist([mock_entry_1, mock_entry_1])\n+\n+    os.environ.pop('PYDANTIC_DISABLE_PLUGINS', None)",
      "comment": "Thanks for the review! I learnt to use monkeypatch for handling global state for tests :)",
      "comment_id": 2620324239,
      "user": "sesmi123",
      "created_at": "2025-12-15T17:52:36Z",
      "url": "https://github.com/pydantic/pydantic/pull/12627#discussion_r2620324239"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12627,
      "file_path": "tests/test_internal.py",
      "line": 238,
      "side": "RIGHT",
      "diff_hunk": "@@ -188,3 +191,128 @@ def test_decimal_digits_calculation(decimal: Decimal, decimal_places: int, digit\n def test_decimal_digits_calculation_type_error(value) -> None:\n     with pytest.raises(TypeError, match=f'Unable to extract decimal digits info from supplied value {value}'):\n         _extract_decimal_digits_info(value)\n+\n+\n+class TestCoreMetadata:\n+    def test_update_adds_key_to_existing_metadata(self):\n+        metadata = {'pydantic_js_prefer_positional_arguments': True}\n+        update_core_metadata(metadata, pydantic_js_updates={'title': 'Test'})\n+        assert metadata['pydantic_js_prefer_positional_arguments']\n+        assert metadata['pydantic_js_updates'] == {'title': 'Test'}\n+\n+    def test_multiple_updates_merge_js_updates_dict(self):\n+        metadata: dict[str, Any] = {}\n+        update_core_metadata(metadata, pydantic_js_updates={'title': 'Test'})\n+        update_core_metadata(metadata, pydantic_js_updates={'description': 'A test field'})\n+\n+        assert metadata['pydantic_js_updates']['title'] == 'Test'\n+        assert metadata['pydantic_js_updates']['description'] == 'A test field'\n+\n+    def test_update_override_earlier_values_for_existing_keys(self):\n+        metadata: dict[str, Any] = {}\n+        update_core_metadata(metadata, pydantic_js_updates={'title': 'First'})\n+        update_core_metadata(metadata, pydantic_js_updates={'title': 'Second'})\n+        assert metadata['pydantic_js_updates']['title'] == 'Second'\n+\n+    def test_update_js_extra_as_callable_when_existing_js_extra_is_none(self):\n+        metadata: dict[str, Any] = {}\n+\n+        def extra_func(schema: JsonDict) -> None:\n+            schema['testKey'] = 'testValue'\n+\n+        update_core_metadata(metadata, pydantic_js_extra=extra_func)\n+        assert metadata['pydantic_js_extra'] is extra_func\n+\n+    def test_update_js_extra_as_callable_when_existing_js_extra_is_dict_type(self):\n+        \"\"\"\n+        It should ignore the callable with a warning.\n+        \"\"\"\n+        metadata: dict[str, Any] = {}\n+\n+        extra_dict: JsonDict = {'testKey': 'testValue'}\n+\n+        def extra_func(schema: JsonDict) -> None:\n+            schema['testKey'] = 'testValue'\n+\n+        update_core_metadata(metadata, pydantic_js_extra=extra_dict)\n+        from pydantic.json_schema import PydanticJsonSchemaWarning",
      "comment": "I've moved the import to the top of the file. Resolving.",
      "comment_id": 2620327577,
      "user": "sesmi123",
      "created_at": "2025-12-15T17:53:51Z",
      "url": "https://github.com/pydantic/pydantic/pull/12627#discussion_r2620327577"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12579,
      "file_path": "pydantic/errors.py",
      "line": 101,
      "side": "RIGHT",
      "diff_hunk": "@@ -98,7 +98,7 @@ def __str__(self) -> str:\n             return f'{self.message}\\n\\nFor further information visit {DEV_ERROR_DOCS_URL}{self.code}'\n \n \n-class PydanticUserError(PydanticErrorMixin, TypeError):\n+class PydanticUserError(PydanticErrorMixin, Exception):",
      "comment": "```suggestion\r\nclass PydanticUserError(PydanticErrorMixin, RuntimeError):\r\n```",
      "comment_id": 2577110115,
      "user": "Viicos",
      "created_at": "2025-12-01T13:37:29Z",
      "url": "https://github.com/pydantic/pydantic/pull/12579#discussion_r2577110115"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11898,
      "file_path": "pydantic/_internal/_fields.py",
      "line": 331,
      "side": "RIGHT",
      "diff_hunk": "@@ -326,20 +326,17 @@ def collect_model_fields(  # noqa: C901\n                 # Note that we only do this for method descriptors for now, we might want to\n                 # extend this to any descriptor in the future (by simply checking for\n                 # `hasattr(assigned_value.default, '__get__')`).\n-                assigned_value.default = assigned_value.default.__get__(None, cls)\n-\n-            # The `from_annotated_attribute()` call below mutates the assigned `Field()`, so make a copy:\n-            original_assignment = (\n-                copy(assigned_value) if not evaluated and isinstance(assigned_value, FieldInfo_) else assigned_value\n-            )\n+                default = assigned_value.default.__get__(None, cls)\n+                assigned_value.default = default\n+                assigned_value._attributes_set['default'] = default",
      "comment": "Next step would be to get rid of the `_attributes_set` logic when merging instances, which is error prone as you need to update it whenever you do a manual assignment on a `FieldInfo` instance.",
      "comment_id": 2101124381,
      "user": "Viicos",
      "created_at": "2025-05-21T20:37:27Z",
      "url": "https://github.com/pydantic/pydantic/pull/11898#discussion_r2101124381"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11898,
      "file_path": "pydantic/fields.py",
      "line": 217,
      "side": "RIGHT",
      "diff_hunk": "@@ -213,7 +213,7 @@ def __init__(self, **kwargs: Unpack[_FieldInfoInputs]) -> None:\n \n         See the signature of `pydantic.fields.Field` for more details about the expected arguments.\n         \"\"\"\n-        self._attributes_set = {k: v for k, v in kwargs.items() if v is not _Unset}\n+        self._attributes_set = {k: v for k, v in kwargs.items() if v is not _Unset and k not in self.metadata_lookup}",
      "comment": "This is kind of important, although it also works without the change:\r\n\r\nWhen we merge field infos, we don't want to reinclude kwargs that are transformed to metadata elements (`gt` -> `annotated_types.Gt`, etc). This will result in unnecessary metadata classes to be created (at the end of the `FieldInfo.__init__()` logic). ",
      "comment_id": 2101126466,
      "user": "Viicos",
      "created_at": "2025-05-21T20:39:10Z",
      "url": "https://github.com/pydantic/pydantic/pull/11898#discussion_r2101126466"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11898,
      "file_path": "pydantic/fields.py",
      "line": 762,
      "side": "RIGHT",
      "diff_hunk": "@@ -700,6 +738,19 @@ def apply_typevars_map(\n             self._complete = False\n             self._original_annotation = self.annotation\n \n+    def _copy(self) -> Self:\n+        \"\"\"Return a copy of the `FieldInfo` instance.\"\"\"\n+        # Note: we can't define a custom `__copy__()`, as `FieldInfo` is being subclassed\n+        # by some third-party libraries with extra attributes defined (and as `FieldInfo`\n+        # is slotted, we can't make a copy of the `__dict__`).",
      "comment": "Alternatively, we can drop slots on `FieldInfo`, and do the following:\r\n\r\n\r\n```python\r\n    def __copy__(self) -> Self:\r\n        copied = type(self)()\r\n        copied.__dict__ = self.__dict__\r\n        for attr_name in ('metadata', '_attributes_set', '_qualifiers'):\r\n             # Apply \"deep-copy\" behavior on collections attributes:\r\n             value = getattr(self, attr_name).copy()\r\n             setattr(copied, attr_name, value)\r\n\r\n        return copied\r\n```",
      "comment_id": 2104889496,
      "user": "Viicos",
      "created_at": "2025-05-23T15:54:47Z",
      "url": "https://github.com/pydantic/pydantic/pull/11898#discussion_r2104889496"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11898,
      "file_path": "tests/test_annotated.py",
      "line": 508,
      "side": "LEFT",
      "diff_hunk": "@@ -505,28 +505,6 @@ class AnnotatedFieldModel(BaseModel):\n         }\n     ]\n \n-    # Ensure that the inner annotation does not override the outer, even for metadata:",
      "comment": "Due to one of the bugs being fixed in this PR (related to the usage of forward references), the constraint in `Annotated` was dropped. Now the test is failing because the unexpected order (described in https://github.com/pydantic/pydantic/issues/10507) applies.",
      "comment_id": 2104893756,
      "user": "Viicos",
      "created_at": "2025-05-23T15:56:20Z",
      "url": "https://github.com/pydantic/pydantic/pull/11898#discussion_r2104893756"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11898,
      "file_path": "pydantic/fields.py",
      "line": 418,
      "side": "RIGHT",
      "diff_hunk": "@@ -417,57 +399,124 @@ class MyModel(pydantic.BaseModel):\n         final = 'final' in inspected_ann.qualifiers\n         metadata = inspected_ann.metadata\n \n-        if isinstance(default, FieldInfo):\n-            # e.g. `field: int = Field(...)`\n-            default.annotation = type_expr\n-            default.metadata += metadata\n-            merged_default = FieldInfo.merge_field_infos(\n-                *[x for x in metadata if isinstance(x, FieldInfo)],\n-                default,\n-                annotation=default.annotation,\n-            )\n-            merged_default.frozen = final or merged_default.frozen\n-            merged_default._qualifiers = inspected_ann.qualifiers\n-            return merged_default\n-\n-        if isinstance(default, dataclasses.Field):\n-            # `collect_dataclass_fields()` passes the dataclass Field as a default.\n-            pydantic_field = FieldInfo._from_dataclass_field(default)\n-            pydantic_field.annotation = type_expr\n-            pydantic_field.metadata += metadata\n-            pydantic_field = FieldInfo.merge_field_infos(\n-                *[x for x in metadata if isinstance(x, FieldInfo)],\n-                pydantic_field,\n-                annotation=pydantic_field.annotation,\n-            )\n-            pydantic_field.frozen = final or pydantic_field.frozen\n-            pydantic_field.init_var = 'init_var' in inspected_ann.qualifiers\n-            pydantic_field.init = getattr(default, 'init', None)\n-            pydantic_field.kw_only = getattr(default, 'kw_only', None)\n-            pydantic_field._qualifiers = inspected_ann.qualifiers\n-            return pydantic_field\n-\n-        if not metadata:\n-            # No metadata, e.g. `field: int = ...`, or `field: Final[str] = ...`:\n-            field_info = FieldInfo(annotation=type_expr, default=default, frozen=final or None)\n-            field_info._qualifiers = inspected_ann.qualifiers\n+        # HACK 1: the order in which the metadata is merged is inconsistent; we need to prepend\n+        # metadata from the assignment at the beginning of the metadata. Changing this is only\n+        # possible in v3 (at least). See https://github.com/pydantic/pydantic/issues/10507\n+        prepend_metadata: list[Any] | None = None\n+        attr_overrides = {'annotation': type_expr}\n+        if final:\n+            attr_overrides['frozen'] = True\n+\n+        # HACK 2: FastAPI is subclassing `FieldInfo` and historically expected the actual\n+        # instance's type to be preserved when constructing new models with its subclasses as assignments.\n+        # This code is never reached by Pydantic itself, and in an ideal world this shouldn't be necessary.\n+        if not metadata and isinstance(default, FieldInfo) and type(default) is not FieldInfo:\n+            field_info = default._copy()\n+            field_info._attributes_set.update(attr_overrides)\n+            for k, v in attr_overrides.items():\n+                setattr(field_info, k, v)",
      "comment": "Unfortunate stuff introduced since https://github.com/pydantic/pydantic/pull/6862..",
      "comment_id": 2104943652,
      "user": "Viicos",
      "created_at": "2025-05-23T16:12:40Z",
      "url": "https://github.com/pydantic/pydantic/pull/11898#discussion_r2104943652"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11898,
      "file_path": "pydantic/fields.py",
      "line": 425,
      "side": "RIGHT",
      "diff_hunk": "@@ -417,58 +400,130 @@ class MyModel(pydantic.BaseModel):\n         final = 'final' in inspected_ann.qualifiers\n         metadata = inspected_ann.metadata\n \n-        if isinstance(default, FieldInfo):\n-            # e.g. `field: int = Field(...)`\n-            default.annotation = type_expr\n-            default.metadata += metadata\n-            merged_default = FieldInfo.merge_field_infos(\n-                *[x for x in metadata if isinstance(x, FieldInfo)],\n-                default,\n-                annotation=default.annotation,\n-            )\n-            merged_default.frozen = final or merged_default.frozen\n-            merged_default._qualifiers = inspected_ann.qualifiers\n-            return merged_default\n-\n-        if isinstance(default, dataclasses.Field):\n-            # `collect_dataclass_fields()` passes the dataclass Field as a default.\n-            pydantic_field = FieldInfo._from_dataclass_field(default)\n-            pydantic_field.annotation = type_expr\n-            pydantic_field.metadata += metadata\n-            pydantic_field = FieldInfo.merge_field_infos(\n-                *[x for x in metadata if isinstance(x, FieldInfo)],\n-                pydantic_field,\n-                annotation=pydantic_field.annotation,\n-            )\n-            pydantic_field.frozen = final or pydantic_field.frozen\n-            pydantic_field.init_var = 'init_var' in inspected_ann.qualifiers\n-            pydantic_field.init = getattr(default, 'init', None)\n-            pydantic_field.kw_only = getattr(default, 'kw_only', None)\n-            pydantic_field._qualifiers = inspected_ann.qualifiers\n-            return pydantic_field\n-\n-        if not metadata:\n-            # No metadata, e.g. `field: int = ...`, or `field: Final[str] = ...`:\n-            field_info = FieldInfo(annotation=type_expr, default=default, frozen=final or None)\n-            field_info._qualifiers = inspected_ann.qualifiers\n+        # HACK 1: the order in which the metadata is merged is inconsistent; we need to prepend\n+        # metadata from the assignment at the beginning of the metadata. Changing this is only\n+        # possible in v3 (at least). See https://github.com/pydantic/pydantic/issues/10507\n+        prepend_metadata: list[Any] | None = None\n+        attr_overrides = {'annotation': type_expr}\n+        if final:\n+            attr_overrides['frozen'] = True\n+\n+        # HACK 2: FastAPI is subclassing `FieldInfo` and historically expected the actual\n+        # instance's type to be preserved when constructing new models with its subclasses as assignments.\n+        # This code is never reached by Pydantic itself, and in an ideal world this shouldn't be necessary.\n+        if not metadata and isinstance(default, FieldInfo) and type(default) is not FieldInfo:\n+            field_info = default._copy()\n+            field_info._attributes_set.update(attr_overrides)\n+            for k, v in attr_overrides.items():\n+                setattr(field_info, k, v)\n             return field_info\n \n-        # With metadata, e.g. `field: Annotated[int, Field(...), Gt(1)] = ...`:\n-        field_infos = [a for a in metadata if isinstance(a, FieldInfo)]\n-        field_info = FieldInfo.merge_field_infos(*field_infos, annotation=type_expr, default=default)\n-        field_metadata: list[Any] = []\n-        for a in metadata:\n-            if typing_objects.is_deprecated(a):\n-                field_info.deprecated = a.message\n-            elif not isinstance(a, FieldInfo):\n-                field_metadata.append(a)\n-            else:\n-                field_metadata.extend(a.metadata)\n-        field_info.metadata = field_metadata\n+        if isinstance(default, FieldInfo):\n+            default_copy = default._copy()  # Copy unnecessary when we remove HACK 1.\n+            prepend_metadata = default_copy.metadata\n+            default_copy.metadata = []\n+            metadata = metadata + [default_copy]",
      "comment": "Why couldn't this be the following?\r\n\r\n```suggestion\r\n            metadata = [default] + metadata\r\n```\r\n\r\nI guess because we want other properties on `default` to still be applied at the end?",
      "comment_id": 2143393776,
      "user": "DouweM",
      "created_at": "2025-06-12T18:39:02Z",
      "url": "https://github.com/pydantic/pydantic/pull/11898#discussion_r2143393776"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11898,
      "file_path": "pydantic/fields.py",
      "line": 444,
      "side": "RIGHT",
      "diff_hunk": "@@ -417,58 +400,130 @@ class MyModel(pydantic.BaseModel):\n         final = 'final' in inspected_ann.qualifiers\n         metadata = inspected_ann.metadata\n \n-        if isinstance(default, FieldInfo):\n-            # e.g. `field: int = Field(...)`\n-            default.annotation = type_expr\n-            default.metadata += metadata\n-            merged_default = FieldInfo.merge_field_infos(\n-                *[x for x in metadata if isinstance(x, FieldInfo)],\n-                default,\n-                annotation=default.annotation,\n-            )\n-            merged_default.frozen = final or merged_default.frozen\n-            merged_default._qualifiers = inspected_ann.qualifiers\n-            return merged_default\n-\n-        if isinstance(default, dataclasses.Field):\n-            # `collect_dataclass_fields()` passes the dataclass Field as a default.\n-            pydantic_field = FieldInfo._from_dataclass_field(default)\n-            pydantic_field.annotation = type_expr\n-            pydantic_field.metadata += metadata\n-            pydantic_field = FieldInfo.merge_field_infos(\n-                *[x for x in metadata if isinstance(x, FieldInfo)],\n-                pydantic_field,\n-                annotation=pydantic_field.annotation,\n-            )\n-            pydantic_field.frozen = final or pydantic_field.frozen\n-            pydantic_field.init_var = 'init_var' in inspected_ann.qualifiers\n-            pydantic_field.init = getattr(default, 'init', None)\n-            pydantic_field.kw_only = getattr(default, 'kw_only', None)\n-            pydantic_field._qualifiers = inspected_ann.qualifiers\n-            return pydantic_field\n-\n-        if not metadata:\n-            # No metadata, e.g. `field: int = ...`, or `field: Final[str] = ...`:\n-            field_info = FieldInfo(annotation=type_expr, default=default, frozen=final or None)\n-            field_info._qualifiers = inspected_ann.qualifiers\n+        # HACK 1: the order in which the metadata is merged is inconsistent; we need to prepend\n+        # metadata from the assignment at the beginning of the metadata. Changing this is only\n+        # possible in v3 (at least). See https://github.com/pydantic/pydantic/issues/10507\n+        prepend_metadata: list[Any] | None = None\n+        attr_overrides = {'annotation': type_expr}\n+        if final:\n+            attr_overrides['frozen'] = True\n+\n+        # HACK 2: FastAPI is subclassing `FieldInfo` and historically expected the actual\n+        # instance's type to be preserved when constructing new models with its subclasses as assignments.\n+        # This code is never reached by Pydantic itself, and in an ideal world this shouldn't be necessary.\n+        if not metadata and isinstance(default, FieldInfo) and type(default) is not FieldInfo:\n+            field_info = default._copy()\n+            field_info._attributes_set.update(attr_overrides)\n+            for k, v in attr_overrides.items():\n+                setattr(field_info, k, v)\n             return field_info\n \n-        # With metadata, e.g. `field: Annotated[int, Field(...), Gt(1)] = ...`:\n-        field_infos = [a for a in metadata if isinstance(a, FieldInfo)]\n-        field_info = FieldInfo.merge_field_infos(*field_infos, annotation=type_expr, default=default)\n-        field_metadata: list[Any] = []\n-        for a in metadata:\n-            if typing_objects.is_deprecated(a):\n-                field_info.deprecated = a.message\n-            elif not isinstance(a, FieldInfo):\n-                field_metadata.append(a)\n-            else:\n-                field_metadata.extend(a.metadata)\n-        field_info.metadata = field_metadata\n+        if isinstance(default, FieldInfo):\n+            default_copy = default._copy()  # Copy unnecessary when we remove HACK 1.\n+            prepend_metadata = default_copy.metadata\n+            default_copy.metadata = []\n+            metadata = metadata + [default_copy]\n+        elif isinstance(default, dataclasses.Field):\n+            from_field = FieldInfo._from_dataclass_field(default)\n+            prepend_metadata = from_field.metadata  # Unnecessary when we remove HACK 1.\n+            from_field.metadata = []\n+            metadata = metadata + [from_field]\n+            if 'init_var' in inspected_ann.qualifiers:\n+                attr_overrides['init_var'] = True\n+            if (init := getattr(default, 'init', None)) is not None:\n+                attr_overrides['init'] = init\n+            if (kw_only := getattr(default, 'kw_only', None)) is not None:\n+                attr_overrides['kw_only'] = kw_only\n+        else:\n+            # `default` is the actual default value\n+            attr_overrides['default'] = default\n+\n+        field_info = FieldInfo._construct(metadata, **attr_overrides)\n         field_info._qualifiers = inspected_ann.qualifiers\n+        if prepend_metadata is not None:\n+            field_info.metadata = prepend_metadata + field_info.metadata",
      "comment": "Why are we assigning this after `_construct` instead of passing it `prepend_metadata + metadata`?",
      "comment_id": 2143396716,
      "user": "DouweM",
      "created_at": "2025-06-12T18:41:38Z",
      "url": "https://github.com/pydantic/pydantic/pull/11898#discussion_r2143396716"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11898,
      "file_path": "pydantic/fields.py",
      "line": 425,
      "side": "RIGHT",
      "diff_hunk": "@@ -417,58 +400,130 @@ class MyModel(pydantic.BaseModel):\n         final = 'final' in inspected_ann.qualifiers\n         metadata = inspected_ann.metadata\n \n-        if isinstance(default, FieldInfo):\n-            # e.g. `field: int = Field(...)`\n-            default.annotation = type_expr\n-            default.metadata += metadata\n-            merged_default = FieldInfo.merge_field_infos(\n-                *[x for x in metadata if isinstance(x, FieldInfo)],\n-                default,\n-                annotation=default.annotation,\n-            )\n-            merged_default.frozen = final or merged_default.frozen\n-            merged_default._qualifiers = inspected_ann.qualifiers\n-            return merged_default\n-\n-        if isinstance(default, dataclasses.Field):\n-            # `collect_dataclass_fields()` passes the dataclass Field as a default.\n-            pydantic_field = FieldInfo._from_dataclass_field(default)\n-            pydantic_field.annotation = type_expr\n-            pydantic_field.metadata += metadata\n-            pydantic_field = FieldInfo.merge_field_infos(\n-                *[x for x in metadata if isinstance(x, FieldInfo)],\n-                pydantic_field,\n-                annotation=pydantic_field.annotation,\n-            )\n-            pydantic_field.frozen = final or pydantic_field.frozen\n-            pydantic_field.init_var = 'init_var' in inspected_ann.qualifiers\n-            pydantic_field.init = getattr(default, 'init', None)\n-            pydantic_field.kw_only = getattr(default, 'kw_only', None)\n-            pydantic_field._qualifiers = inspected_ann.qualifiers\n-            return pydantic_field\n-\n-        if not metadata:\n-            # No metadata, e.g. `field: int = ...`, or `field: Final[str] = ...`:\n-            field_info = FieldInfo(annotation=type_expr, default=default, frozen=final or None)\n-            field_info._qualifiers = inspected_ann.qualifiers\n+        # HACK 1: the order in which the metadata is merged is inconsistent; we need to prepend\n+        # metadata from the assignment at the beginning of the metadata. Changing this is only\n+        # possible in v3 (at least). See https://github.com/pydantic/pydantic/issues/10507\n+        prepend_metadata: list[Any] | None = None\n+        attr_overrides = {'annotation': type_expr}\n+        if final:\n+            attr_overrides['frozen'] = True\n+\n+        # HACK 2: FastAPI is subclassing `FieldInfo` and historically expected the actual\n+        # instance's type to be preserved when constructing new models with its subclasses as assignments.\n+        # This code is never reached by Pydantic itself, and in an ideal world this shouldn't be necessary.\n+        if not metadata and isinstance(default, FieldInfo) and type(default) is not FieldInfo:\n+            field_info = default._copy()\n+            field_info._attributes_set.update(attr_overrides)\n+            for k, v in attr_overrides.items():\n+                setattr(field_info, k, v)\n             return field_info\n \n-        # With metadata, e.g. `field: Annotated[int, Field(...), Gt(1)] = ...`:\n-        field_infos = [a for a in metadata if isinstance(a, FieldInfo)]\n-        field_info = FieldInfo.merge_field_infos(*field_infos, annotation=type_expr, default=default)\n-        field_metadata: list[Any] = []\n-        for a in metadata:\n-            if typing_objects.is_deprecated(a):\n-                field_info.deprecated = a.message\n-            elif not isinstance(a, FieldInfo):\n-                field_metadata.append(a)\n-            else:\n-                field_metadata.extend(a.metadata)\n-        field_info.metadata = field_metadata\n+        if isinstance(default, FieldInfo):\n+            default_copy = default._copy()  # Copy unnecessary when we remove HACK 1.\n+            prepend_metadata = default_copy.metadata\n+            default_copy.metadata = []\n+            metadata = metadata + [default_copy]",
      "comment": "> I guess because we want other properties on `default` to still be applied at the end?\r\n\r\nExactly, as per https://github.com/pydantic/pydantic/issues/10507, any `FieldInfo` attribute that isn't converted into the `metadata` array (e.g. `description`) is applied correctly.",
      "comment_id": 2144537661,
      "user": "Viicos",
      "created_at": "2025-06-13T08:57:19Z",
      "url": "https://github.com/pydantic/pydantic/pull/11898#discussion_r2144537661"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11898,
      "file_path": "pydantic/fields.py",
      "line": 444,
      "side": "RIGHT",
      "diff_hunk": "@@ -417,58 +400,130 @@ class MyModel(pydantic.BaseModel):\n         final = 'final' in inspected_ann.qualifiers\n         metadata = inspected_ann.metadata\n \n-        if isinstance(default, FieldInfo):\n-            # e.g. `field: int = Field(...)`\n-            default.annotation = type_expr\n-            default.metadata += metadata\n-            merged_default = FieldInfo.merge_field_infos(\n-                *[x for x in metadata if isinstance(x, FieldInfo)],\n-                default,\n-                annotation=default.annotation,\n-            )\n-            merged_default.frozen = final or merged_default.frozen\n-            merged_default._qualifiers = inspected_ann.qualifiers\n-            return merged_default\n-\n-        if isinstance(default, dataclasses.Field):\n-            # `collect_dataclass_fields()` passes the dataclass Field as a default.\n-            pydantic_field = FieldInfo._from_dataclass_field(default)\n-            pydantic_field.annotation = type_expr\n-            pydantic_field.metadata += metadata\n-            pydantic_field = FieldInfo.merge_field_infos(\n-                *[x for x in metadata if isinstance(x, FieldInfo)],\n-                pydantic_field,\n-                annotation=pydantic_field.annotation,\n-            )\n-            pydantic_field.frozen = final or pydantic_field.frozen\n-            pydantic_field.init_var = 'init_var' in inspected_ann.qualifiers\n-            pydantic_field.init = getattr(default, 'init', None)\n-            pydantic_field.kw_only = getattr(default, 'kw_only', None)\n-            pydantic_field._qualifiers = inspected_ann.qualifiers\n-            return pydantic_field\n-\n-        if not metadata:\n-            # No metadata, e.g. `field: int = ...`, or `field: Final[str] = ...`:\n-            field_info = FieldInfo(annotation=type_expr, default=default, frozen=final or None)\n-            field_info._qualifiers = inspected_ann.qualifiers\n+        # HACK 1: the order in which the metadata is merged is inconsistent; we need to prepend\n+        # metadata from the assignment at the beginning of the metadata. Changing this is only\n+        # possible in v3 (at least). See https://github.com/pydantic/pydantic/issues/10507\n+        prepend_metadata: list[Any] | None = None\n+        attr_overrides = {'annotation': type_expr}\n+        if final:\n+            attr_overrides['frozen'] = True\n+\n+        # HACK 2: FastAPI is subclassing `FieldInfo` and historically expected the actual\n+        # instance's type to be preserved when constructing new models with its subclasses as assignments.\n+        # This code is never reached by Pydantic itself, and in an ideal world this shouldn't be necessary.\n+        if not metadata and isinstance(default, FieldInfo) and type(default) is not FieldInfo:\n+            field_info = default._copy()\n+            field_info._attributes_set.update(attr_overrides)\n+            for k, v in attr_overrides.items():\n+                setattr(field_info, k, v)\n             return field_info\n \n-        # With metadata, e.g. `field: Annotated[int, Field(...), Gt(1)] = ...`:\n-        field_infos = [a for a in metadata if isinstance(a, FieldInfo)]\n-        field_info = FieldInfo.merge_field_infos(*field_infos, annotation=type_expr, default=default)\n-        field_metadata: list[Any] = []\n-        for a in metadata:\n-            if typing_objects.is_deprecated(a):\n-                field_info.deprecated = a.message\n-            elif not isinstance(a, FieldInfo):\n-                field_metadata.append(a)\n-            else:\n-                field_metadata.extend(a.metadata)\n-        field_info.metadata = field_metadata\n+        if isinstance(default, FieldInfo):\n+            default_copy = default._copy()  # Copy unnecessary when we remove HACK 1.\n+            prepend_metadata = default_copy.metadata\n+            default_copy.metadata = []\n+            metadata = metadata + [default_copy]\n+        elif isinstance(default, dataclasses.Field):\n+            from_field = FieldInfo._from_dataclass_field(default)\n+            prepend_metadata = from_field.metadata  # Unnecessary when we remove HACK 1.\n+            from_field.metadata = []\n+            metadata = metadata + [from_field]\n+            if 'init_var' in inspected_ann.qualifiers:\n+                attr_overrides['init_var'] = True\n+            if (init := getattr(default, 'init', None)) is not None:\n+                attr_overrides['init'] = init\n+            if (kw_only := getattr(default, 'kw_only', None)) is not None:\n+                attr_overrides['kw_only'] = kw_only\n+        else:\n+            # `default` is the actual default value\n+            attr_overrides['default'] = default\n+\n+        field_info = FieldInfo._construct(metadata, **attr_overrides)\n         field_info._qualifiers = inspected_ann.qualifiers\n+        if prepend_metadata is not None:\n+            field_info.metadata = prepend_metadata + field_info.metadata",
      "comment": "Both are equivalent, but indeed passing all the metadata directly to `_construct()` is cleaner. Applied.",
      "comment_id": 2144563306,
      "user": "Viicos",
      "created_at": "2025-06-13T09:12:19Z",
      "url": "https://github.com/pydantic/pydantic/pull/11898#discussion_r2144563306"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11898,
      "file_path": "pydantic/_internal/_fields.py",
      "line": 331,
      "side": "RIGHT",
      "diff_hunk": "@@ -326,20 +326,17 @@ def collect_model_fields(  # noqa: C901\n                 # Note that we only do this for method descriptors for now, we might want to\n                 # extend this to any descriptor in the future (by simply checking for\n                 # `hasattr(assigned_value.default, '__get__')`).\n-                assigned_value.default = assigned_value.default.__get__(None, cls)\n-\n-            # The `from_annotated_attribute()` call below mutates the assigned `Field()`, so make a copy:\n-            original_assignment = (\n-                copy(assigned_value) if not evaluated and isinstance(assigned_value, FieldInfo_) else assigned_value\n-            )\n+                default = assigned_value.default.__get__(None, cls)\n+                assigned_value.default = default\n+                assigned_value._attributes_set['default'] = default",
      "comment": "Where can I subscribe to see when this happens?\n\nApologies if this seems weird to respond to now, please let me know if I should turn this into a proper issue or some such for visibility.\n\nI just spent a few hours adjusting to the v2.12 release, which broke our filter-system. In essence, we're creating lots of filters dynamically using a MetaClass and these filters have pydantic `Field`s. We usually just pass some extra information by setting\n\n```python\n    field.json_schema_extra = {\"sqla_column\": name}\n```\n\nThis used to work with previous versions, but now we need to add\n\n```python\n    field._attributes_set[\"json_schema_extra\"] = {\"sqla_column\": name}\n```\n\nto make the final `pydantic.BaseModel.__new__()` call recognize the `json_schema_extra` attribute.\n\nSo thanks for providing the fix in this comment :)",
      "comment_id": 2413630831,
      "user": "glatterf42",
      "created_at": "2025-10-08T12:04:09Z",
      "url": "https://github.com/pydantic/pydantic/pull/11898#discussion_r2413630831"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11898,
      "file_path": "pydantic/_internal/_fields.py",
      "line": 331,
      "side": "RIGHT",
      "diff_hunk": "@@ -326,20 +326,17 @@ def collect_model_fields(  # noqa: C901\n                 # Note that we only do this for method descriptors for now, we might want to\n                 # extend this to any descriptor in the future (by simply checking for\n                 # `hasattr(assigned_value.default, '__get__')`).\n-                assigned_value.default = assigned_value.default.__get__(None, cls)\n-\n-            # The `from_annotated_attribute()` call below mutates the assigned `Field()`, so make a copy:\n-            original_assignment = (\n-                copy(assigned_value) if not evaluated and isinstance(assigned_value, FieldInfo_) else assigned_value\n-            )\n+                default = assigned_value.default.__get__(None, cls)\n+                assigned_value.default = default\n+                assigned_value._attributes_set['default'] = default",
      "comment": "Not really a specific issue for `_attributes_set`, but I gathered everything into https://github.com/pydantic/pydantic/issues/12374. To be help to give some feedback on your approach (and if extending our metaclass is the only possible option=, I would just require a small example of what your library achieves if possible!",
      "comment_id": 2418264128,
      "user": "Viicos",
      "created_at": "2025-10-10T00:52:17Z",
      "url": "https://github.com/pydantic/pydantic/pull/11898#discussion_r2418264128"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11898,
      "file_path": "pydantic/_internal/_fields.py",
      "line": 331,
      "side": "RIGHT",
      "diff_hunk": "@@ -326,20 +326,17 @@ def collect_model_fields(  # noqa: C901\n                 # Note that we only do this for method descriptors for now, we might want to\n                 # extend this to any descriptor in the future (by simply checking for\n                 # `hasattr(assigned_value.default, '__get__')`).\n-                assigned_value.default = assigned_value.default.__get__(None, cls)\n-\n-            # The `from_annotated_attribute()` call below mutates the assigned `Field()`, so make a copy:\n-            original_assignment = (\n-                copy(assigned_value) if not evaluated and isinstance(assigned_value, FieldInfo_) else assigned_value\n-            )\n+                default = assigned_value.default.__get__(None, cls)\n+                assigned_value.default = default\n+                assigned_value._attributes_set['default'] = default",
      "comment": "Thanks for your offer and sorry for the late reply! In the meantime, our library was refactored quite substantially, which eliminated our usage of `_attributes_set` :)",
      "comment_id": 2572118575,
      "user": "glatterf42",
      "created_at": "2025-11-28T16:33:51Z",
      "url": "https://github.com/pydantic/pydantic/pull/11898#discussion_r2572118575"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12563,
      "file_path": "tests/test_forward_ref.py",
      "line": 1094,
      "side": "RIGHT",
      "diff_hunk": "@@ -1090,10 +1090,12 @@ def test_pydantic_extra_forward_ref_separate_module(create_module: Any) -> None:\n     def module_1():\n         from pydantic import BaseModel, ConfigDict\n \n+        MyDict = dict\n+",
      "comment": "This was wrongly updated when dropping support for Python 3.9.",
      "comment_id": 2553174445,
      "user": "Viicos",
      "created_at": "2025-11-22T14:55:04Z",
      "url": "https://github.com/pydantic/pydantic/pull/12563#discussion_r2553174445"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12563,
      "file_path": "tests/test_forward_ref.py",
      "line": 1118,
      "side": "RIGHT",
      "diff_hunk": "@@ -1113,6 +1115,42 @@ class Foo(BaseModel):\n     assert extras_schema == {'type': 'int'}\n \n \n+def test_pydantic_extra_forward_ref_separate_module_subclass(create_module: Any) -> None:",
      "comment": "This was a use case that wasn't working until now.",
      "comment_id": 2553174547,
      "user": "Viicos",
      "created_at": "2025-11-22T14:55:20Z",
      "url": "https://github.com/pydantic/pydantic/pull/12563#discussion_r2553174547"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12563,
      "file_path": "pydantic/_internal/_fields.py",
      "line": 48,
      "side": "RIGHT",
      "diff_hunk": "@@ -41,6 +42,13 @@ class PydanticMetadata(Representation):\n     __slots__ = ()\n \n \n+@dataclasses.dataclass(**slots_true)  # TODO: make kw_only when we drop support for 3.9.\n+class PydanticExtraInfo:\n+    # TODO: make use of PEP 747:\n+    annotation: Any",
      "comment": "`typing_extensions.TypeForm`? Or is the point that we'll need to update places across the code for this?",
      "comment_id": 2555593501,
      "user": "davidhewitt",
      "created_at": "2025-11-24T10:14:36Z",
      "url": "https://github.com/pydantic/pydantic/pull/12563#discussion_r2555593501"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12563,
      "file_path": "pydantic/_internal/_generate_schema.py",
      "line": 970,
      "side": "RIGHT",
      "diff_hunk": "@@ -978,6 +965,9 @@ def _get_args_resolving_forward_refs(self, obj: Any, required: bool = False) ->\n         if args:\n             if isinstance(obj, GenericAlias):\n                 # PEP 585 generic aliases don't convert args to ForwardRefs, unlike `typing.List/Dict` etc.\n+                # This was fixed in https://github.com/python/cpython/pull/30900 (Python 3.11).\n+                # TODO: this shouldn't be necessary (probably even this `_get_args_resolving_forward_refs()` function)\n+                # once we drop support for Python 3.10 *or* if we implement our own `typing._eval_type()` implementation.",
      "comment": "Should we update the `isinstance` to have a condition on `sys.version_info < (3, 11)` (or even just make the whole function just return early)?",
      "comment_id": 2555611087,
      "user": "davidhewitt",
      "created_at": "2025-11-24T10:18:19Z",
      "url": "https://github.com/pydantic/pydantic/pull/12563#discussion_r2555611087"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12563,
      "file_path": "pydantic/_internal/_fields.py",
      "line": 48,
      "side": "RIGHT",
      "diff_hunk": "@@ -41,6 +42,13 @@ class PydanticMetadata(Representation):\n     __slots__ = ()\n \n \n+@dataclasses.dataclass(**slots_true)  # TODO: make kw_only when we drop support for 3.9.\n+class PydanticExtraInfo:\n+    # TODO: make use of PEP 747:\n+    annotation: Any",
      "comment": "Yes, once the PEP is accepted I'll grep for this type of comment and replace.",
      "comment_id": 2555793503,
      "user": "Viicos",
      "created_at": "2025-11-24T11:00:59Z",
      "url": "https://github.com/pydantic/pydantic/pull/12563#discussion_r2555793503"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12563,
      "file_path": "pydantic/_internal/_generate_schema.py",
      "line": 970,
      "side": "RIGHT",
      "diff_hunk": "@@ -978,6 +965,9 @@ def _get_args_resolving_forward_refs(self, obj: Any, required: bool = False) ->\n         if args:\n             if isinstance(obj, GenericAlias):\n                 # PEP 585 generic aliases don't convert args to ForwardRefs, unlike `typing.List/Dict` etc.\n+                # This was fixed in https://github.com/python/cpython/pull/30900 (Python 3.11).\n+                # TODO: this shouldn't be necessary (probably even this `_get_args_resolving_forward_refs()` function)\n+                # once we drop support for Python 3.10 *or* if we implement our own `typing._eval_type()` implementation.",
      "comment": "In theory we could, but I'm a bit worried this would introduce subtle regressions for other annotations that were not actually evaluated (and not due to the <3.11 bug). The end goal is to have the guarantee that everything passed to `GenerateSchema` is evaluated, but I'm not entirely sure this is the case yet, so probably I'll try to tackle this properly separately.",
      "comment_id": 2555802965,
      "user": "Viicos",
      "created_at": "2025-11-24T11:03:00Z",
      "url": "https://github.com/pydantic/pydantic/pull/12563#discussion_r2555802965"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12560,
      "file_path": "pydantic/config.py",
      "line": 599,
      "side": "RIGHT",
      "diff_hunk": "@@ -588,10 +595,10 @@ class Transaction(BaseModel):\n \n     Defaults to `'iso8601'`.\n \n-    !!! note\n-        This setting was introduced in v2.12. It overlaps with the [`ser_json_timedelta`][pydantic.config.ConfigDict.ser_json_timedelta]\n-        setting which will be deprecated in v3. It also adds more configurability for\n-        the other temporal types.\n+    /// version-added | v2.12\n+    This setting is meant to be a replacement for [`ser_json_timedelta`][pydantic.config.ConfigDict.ser_json_timedelta],",
      "comment": "```suggestion\n    This setting replaces [`ser_json_timedelta`][pydantic.config.ConfigDict.ser_json_timedelta],\n```",
      "comment_id": 2549669485,
      "user": "davidhewitt",
      "created_at": "2025-11-21T12:51:42Z",
      "url": "https://github.com/pydantic/pydantic/pull/12560#discussion_r2549669485"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12560,
      "file_path": "pydantic/config.py",
      "line": 1148,
      "side": "RIGHT",
      "diff_hunk": "@@ -1120,6 +1141,12 @@ class Model(BaseModel):\n         This would make it impossible to populate an attribute.\n \n         See [usage errors](../errors/usage_errors.md#validate-by-alias-and-name-false) for an example.\n+\n+    /// version-added | v2.11\n+    This setting was introduced in conjunction with [`validate_by_alias`][pydantic.ConfigDict.validate_by_alias]\n+    to empower users with more fine grained validation control. It is an alternative to [`populate_by_name`][pydantic.ConfigDict.populate_by_name],\n+    thas enames validation by name **and** by alias.",
      "comment": "```suggestion\n    that enables validation by name **and** by alias.\n```",
      "comment_id": 2549674368,
      "user": "davidhewitt",
      "created_at": "2025-11-21T12:53:40Z",
      "url": "https://github.com/pydantic/pydantic/pull/12560#discussion_r2549674368"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12536,
      "file_path": "tests/test_decorators.py",
      "line": 146,
      "side": "RIGHT",
      "diff_hunk": "@@ -115,3 +115,32 @@ def serializer():\n         inspect_annotated_serializer(serializer, mode=mode)\n \n     assert e.value.code == 'field-serializer-signature'\n+\n+\n+def test_plain_class_not_mutated() -> None:\n+    class A:\n+        @computed_field\n+        def func(self) -> int:\n+            return 1\n+\n+    class B(A, BaseModel):\n+        pass\n+\n+    assert B.__pydantic_decorators__.computed_fields['func'].cls_var_name == 'func'\n+\n+    assert '__pydantic_decorators__' not in A.__dict__\n+\n+\n+def test_decorator_info_not_mutated() -> None:\n+    class A(BaseModel):\n+        @computed_field\n+        def func(self) -> int:\n+            return 1\n+\n+    assert A.__pydantic_decorators__.computed_fields['func'].info.title is None\n+\n+    class B(A, field_title_generator=lambda _, __: 'test'):\n+        pass\n+\n+    assert A.__pydantic_decorators__.computed_fields['func'].info.title is None\n+    assert B.__pydantic_decorators__.computed_fields['func'].info.title == 'test'",
      "comment": "This seems strange that `B` is able to change semantics of fields on A?",
      "comment_id": 2534468162,
      "user": "davidhewitt",
      "created_at": "2025-11-17T15:10:21Z",
      "url": "https://github.com/pydantic/pydantic/pull/12536#discussion_r2534468162"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12536,
      "file_path": "pydantic/fields.py",
      "line": 1522,
      "side": "RIGHT",
      "diff_hunk": "@@ -1519,7 +1519,7 @@ def PrivateAttr(\n     )\n \n \n-@dataclasses.dataclass(**_internal_dataclass.slots_true)\n+@dataclasses.dataclass",
      "comment": "I guess removing slots made the copy implementation _easier_, but if it's better performance to keep it, should we just do the more complex `copy` implementation which does all fields by hand?.",
      "comment_id": 2534473000,
      "user": "davidhewitt",
      "created_at": "2025-11-17T15:11:46Z",
      "url": "https://github.com/pydantic/pydantic/pull/12536#discussion_r2534473000"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12536,
      "file_path": "tests/test_decorators.py",
      "line": 146,
      "side": "RIGHT",
      "diff_hunk": "@@ -115,3 +115,32 @@ def serializer():\n         inspect_annotated_serializer(serializer, mode=mode)\n \n     assert e.value.code == 'field-serializer-signature'\n+\n+\n+def test_plain_class_not_mutated() -> None:\n+    class A:\n+        @computed_field\n+        def func(self) -> int:\n+            return 1\n+\n+    class B(A, BaseModel):\n+        pass\n+\n+    assert B.__pydantic_decorators__.computed_fields['func'].cls_var_name == 'func'\n+\n+    assert '__pydantic_decorators__' not in A.__dict__\n+\n+\n+def test_decorator_info_not_mutated() -> None:\n+    class A(BaseModel):\n+        @computed_field\n+        def func(self) -> int:\n+            return 1\n+\n+    assert A.__pydantic_decorators__.computed_fields['func'].info.title is None\n+\n+    class B(A, field_title_generator=lambda _, __: 'test'):\n+        pass\n+\n+    assert A.__pydantic_decorators__.computed_fields['func'].info.title is None\n+    assert B.__pydantic_decorators__.computed_fields['func'].info.title == 'test'",
      "comment": "It also applies to normal fields. In some way, the inheritance can be seen as a way to reuse existing fields, similar to what it means to make subclasses of `TypedDict`s. Anyway, this is only relevant for `field_title_generator` and `alias_generator` _and_ when subclassing and specifying these config, so quite unusual.",
      "comment_id": 2534611673,
      "user": "Viicos",
      "created_at": "2025-11-17T15:49:16Z",
      "url": "https://github.com/pydantic/pydantic/pull/12536#discussion_r2534611673"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12500,
      "file_path": "pydantic-core/tests/validators/test_decimal.py",
      "line": 120,
      "side": "RIGHT",
      "diff_hunk": "@@ -92,6 +92,41 @@ def test_decimal(py_and_json: PyAndJson, input_value, expected):\n         assert isinstance(output, Decimal)\n \n \n+@pytest.mark.parametrize(\n+    'input_value,expected',\n+    [\n+        # Three-tuple constructor: (sign, (digits...), exponent)\n+        # sign: 0 for positive, 1 for negative\n+        # digits: tuple of digits\n+        # exponent: integer exponent\n+        ((0, (1, 4, 1, 4), -3), Decimal('1.414')),\n+        ((0, (1, 2, 3), 0), Decimal('123')),\n+        ((0, (1, 2, 3), 2), Decimal('12300')),\n+        ((0, (1, 2, 3), -2), Decimal('1.23')),\n+        ((1, (1, 4, 1, 4), -3), Decimal('-1.414')),\n+        ((1, (1, 2, 3), 0), Decimal('-123')),\n+        ((1, (1, 2, 3), 2), Decimal('-12300')),\n+        ((1, (1, 2, 3), -2), Decimal('-1.23')),\n+        ((0, (0,), 0), Decimal('0')),\n+        ((0, (5,), -1), Decimal('0.5')),\n+        ((1, (5,), -1), Decimal('-0.5')),\n+        ((0, (1, 0, 0), -2), Decimal('1.00')),\n+        ((0, (9, 9, 9), 3), Decimal('999000')),\n+    ],\n+    ids=repr,\n+)\n+def test_decimal_three_tuple_constructor(py_and_json: PyAndJson, input_value, expected):\n+    \"\"\"Test that Decimal can be constructed from a three-tuple (sign, digits, exponent).\"\"\"\n+    v = py_and_json({'type': 'decimal'})",
      "comment": "```suggestion\r\n    v = py_and_json(cs.decimal_schema())\r\n```",
      "comment_id": 2517465961,
      "user": "Viicos",
      "created_at": "2025-11-12T09:00:41Z",
      "url": "https://github.com/pydantic/pydantic/pull/12500#discussion_r2517465961"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12500,
      "file_path": "pydantic-core/tests/validators/test_decimal.py",
      "line": 124,
      "side": "RIGHT",
      "diff_hunk": "@@ -92,6 +92,41 @@ def test_decimal(py_and_json: PyAndJson, input_value, expected):\n         assert isinstance(output, Decimal)\n \n \n+@pytest.mark.parametrize(\n+    'input_value,expected',\n+    [\n+        # Three-tuple constructor: (sign, (digits...), exponent)\n+        # sign: 0 for positive, 1 for negative\n+        # digits: tuple of digits\n+        # exponent: integer exponent\n+        ((0, (1, 4, 1, 4), -3), Decimal('1.414')),\n+        ((0, (1, 2, 3), 0), Decimal('123')),\n+        ((0, (1, 2, 3), 2), Decimal('12300')),\n+        ((0, (1, 2, 3), -2), Decimal('1.23')),\n+        ((1, (1, 4, 1, 4), -3), Decimal('-1.414')),\n+        ((1, (1, 2, 3), 0), Decimal('-123')),\n+        ((1, (1, 2, 3), 2), Decimal('-12300')),\n+        ((1, (1, 2, 3), -2), Decimal('-1.23')),\n+        ((0, (0,), 0), Decimal('0')),\n+        ((0, (5,), -1), Decimal('0.5')),\n+        ((1, (5,), -1), Decimal('-0.5')),\n+        ((0, (1, 0, 0), -2), Decimal('1.00')),\n+        ((0, (9, 9, 9), 3), Decimal('999000')),\n+    ],\n+    ids=repr,\n+)\n+def test_decimal_three_tuple_constructor(py_and_json: PyAndJson, input_value, expected):\n+    \"\"\"Test that Decimal can be constructed from a three-tuple (sign, digits, exponent).\"\"\"\n+    v = py_and_json({'type': 'decimal'})\n+    # Three-tuple constructor is only valid for Python input, not JSON\n+    if v.validator_type == 'json':\n+        # For JSON, we skip this test as tuples aren't JSON serializable\n+        pytest.skip('Three-tuple constructor is only valid for Python input')",
      "comment": "We actually want to validate in JSON as well (with array values).",
      "comment_id": 2517469953,
      "user": "Viicos",
      "created_at": "2025-11-12T09:01:44Z",
      "url": "https://github.com/pydantic/pydantic/pull/12500#discussion_r2517469953"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12373,
      "file_path": "tests/test_fields.py",
      "line": 371,
      "side": "RIGHT",
      "diff_hunk": "@@ -353,3 +353,19 @@ def test_default_factory_validated_data_argument_unsupported() -> None:\n         ),\n     ):\n         TypeAdapter(Annotated[int, Field(default_factory=lambda v: v['key'])])\n+\n+\n+def test_default_factory_without_validated_data_unsupported() -> None:\n+    with pytest.raises(ValueError):\n+\n+        class FooBar(BaseModel):\n+            a: int = Field(default_factory=lambda x: x)\n+\n+        [field.get_default(call_default_factory=True) for field in FooBar.model_fields.values()]\n+\n+\n+def test_default_factory_without_flag() -> None:\n+    class FooBar(BaseModel):\n+        a: int = Field(default_factory=lambda x: x)\n+\n+    assert [field.get_default() for field in FooBar.model_fields.values()] == [None]",
      "comment": "```suggestion\r\ndef test_default_factory_without_validated_data_unsupported() -> None:\r\n    class FooBar(BaseModel):\r\n        a: int = Field(default_factory=lambda x: x)\r\n \r\n    assert FooBar.model_fields['a'].get_default() is None\r\n          \r\n    with pytest.raises(ValueError):\r\n        FooBar.model_fields['a'].get_default(call_default_factory=True)\r\n```",
      "comment_id": 2418103389,
      "user": "Viicos",
      "created_at": "2025-10-09T22:39:14Z",
      "url": "https://github.com/pydantic/pydantic/pull/12373#discussion_r2418103389"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12511,
      "file_path": "pydantic/config.py",
      "line": 536,
      "side": "LEFT",
      "diff_hunk": "@@ -491,123 +491,70 @@ class Model(BaseModel):\n     # whether instances of models and dataclasses (including subclass instances) should re-validate, default 'never'\n     revalidate_instances: Literal['always', 'never', 'subclass-instances']\n     \"\"\"\n-    When and how to revalidate models and dataclasses during validation. Accepts the string\n-    values of `'never'`, `'always'` and `'subclass-instances'`. Defaults to `'never'`.\n+    When and how to revalidate models and dataclasses during validation. Can be one of:\n \n-    - `'never'` will not revalidate models and dataclasses during validation\n-    - `'always'` will revalidate models and dataclasses during validation\n-    - `'subclass-instances'` will revalidate models and dataclasses during validation if the instance is a\n+    - `'never'`: will *not* revalidate models and dataclasses during validation\n+    - `'always'`: will revalidate models and dataclasses during validation\n+    - `'subclass-instances'`: will revalidate models and dataclasses during validation if the instance is a\n         subclass of the model or dataclass\n \n-    By default, model and dataclass instances are not revalidated during validation.\n+    The default is `'never'` (no revalidation).\n+\n+    This configuration only affects *the current model* it is applied on, and does *not* populate to the models\n+    referenced in fields.\n \n     ```python\n     from pydantic import BaseModel\n \n     class User(BaseModel, revalidate_instances='never'):  # (1)!\n-        hobbies: list[str]\n-\n-    class SubUser(User):\n-        sins: list[str]\n+        name: str\n \n     class Transaction(BaseModel):\n         user: User\n \n-    my_user = User(hobbies=['reading'])\n+    my_user = User(name='John')\n     t = Transaction(user=my_user)\n-    print(t)\n-    #> user=User(hobbies=['reading'])\n \n-    my_user.hobbies = [1]  # (2)!\n+    my_user.name = 1  # (2)!\n     t = Transaction(user=my_user)  # (3)!\n     print(t)\n-    #> user=User(hobbies=[1])\n-\n-    my_sub_user = SubUser(hobbies=['scuba diving'], sins=['lying'])\n-    t = Transaction(user=my_sub_user)\n-    print(t)\n-    #> user=SubUser(hobbies=['scuba diving'], sins=['lying'])\n+    #> user=User(name=1)\n     ```\n \n-    1. `revalidate_instances` is set to `'never'` by **default.\n-    2. The assignment is not validated, unless you set `validate_assignment` to `True` in the model's config.\n-    3. Since `revalidate_instances` is set to `never`, this is not revalidated.\n+    1. This is the default behavior.\n+    2. The assignment is *not* validated, unless you set [`validate_assignment`][pydantic.ConfigDict.validate_assignment] in the configuration.\n+    3. Since `revalidate_instances` is set to `'never'`, the user instance is not revalidated.\n \n-    If you want to revalidate instances during validation, you can set `revalidate_instances` to `'always'`",
      "comment": "Don't think this example hurt but it's also fine to delete",
      "comment_id": 2512032415,
      "user": "davidhewitt",
      "created_at": "2025-11-10T21:32:03Z",
      "url": "https://github.com/pydantic/pydantic/pull/12511#discussion_r2512032415"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12511,
      "file_path": "pydantic/config.py",
      "line": 536,
      "side": "LEFT",
      "diff_hunk": "@@ -491,123 +491,70 @@ class Model(BaseModel):\n     # whether instances of models and dataclasses (including subclass instances) should re-validate, default 'never'\n     revalidate_instances: Literal['always', 'never', 'subclass-instances']\n     \"\"\"\n-    When and how to revalidate models and dataclasses during validation. Accepts the string\n-    values of `'never'`, `'always'` and `'subclass-instances'`. Defaults to `'never'`.\n+    When and how to revalidate models and dataclasses during validation. Can be one of:\n \n-    - `'never'` will not revalidate models and dataclasses during validation\n-    - `'always'` will revalidate models and dataclasses during validation\n-    - `'subclass-instances'` will revalidate models and dataclasses during validation if the instance is a\n+    - `'never'`: will *not* revalidate models and dataclasses during validation\n+    - `'always'`: will revalidate models and dataclasses during validation\n+    - `'subclass-instances'`: will revalidate models and dataclasses during validation if the instance is a\n         subclass of the model or dataclass\n \n-    By default, model and dataclass instances are not revalidated during validation.\n+    The default is `'never'` (no revalidation).\n+\n+    This configuration only affects *the current model* it is applied on, and does *not* populate to the models\n+    referenced in fields.\n \n     ```python\n     from pydantic import BaseModel\n \n     class User(BaseModel, revalidate_instances='never'):  # (1)!\n-        hobbies: list[str]\n-\n-    class SubUser(User):\n-        sins: list[str]\n+        name: str\n \n     class Transaction(BaseModel):\n         user: User\n \n-    my_user = User(hobbies=['reading'])\n+    my_user = User(name='John')\n     t = Transaction(user=my_user)\n-    print(t)\n-    #> user=User(hobbies=['reading'])\n \n-    my_user.hobbies = [1]  # (2)!\n+    my_user.name = 1  # (2)!\n     t = Transaction(user=my_user)  # (3)!\n     print(t)\n-    #> user=User(hobbies=[1])\n-\n-    my_sub_user = SubUser(hobbies=['scuba diving'], sins=['lying'])\n-    t = Transaction(user=my_sub_user)\n-    print(t)\n-    #> user=SubUser(hobbies=['scuba diving'], sins=['lying'])\n+    #> user=User(name=1)\n     ```\n \n-    1. `revalidate_instances` is set to `'never'` by **default.\n-    2. The assignment is not validated, unless you set `validate_assignment` to `True` in the model's config.\n-    3. Since `revalidate_instances` is set to `never`, this is not revalidated.\n+    1. This is the default behavior.\n+    2. The assignment is *not* validated, unless you set [`validate_assignment`][pydantic.ConfigDict.validate_assignment] in the configuration.\n+    3. Since `revalidate_instances` is set to `'never'`, the user instance is not revalidated.\n \n-    If you want to revalidate instances during validation, you can set `revalidate_instances` to `'always'`",
      "comment": "I kept only the `'subclass-instances'` example as it somehow also shows what is the behavior with a \"direct\" instance, and the fact that it is _not_ revalidated with `'subclass-instances'` (and then users can easily assume it would have been with `'always'`). I generally try to reduce the size of the examples as it adds cognitive overhead to understand them, and when I see too long examples by brain usually just gives up.",
      "comment_id": 2513568128,
      "user": "Viicos",
      "created_at": "2025-11-11T09:55:20Z",
      "url": "https://github.com/pydantic/pydantic/pull/12511#discussion_r2513568128"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12495,
      "file_path": "tests/test_dataclasses.py",
      "line": 706,
      "side": "RIGHT",
      "diff_hunk": "@@ -686,6 +686,35 @@ class TestInitVar:\n         TestInitVar(1, 2, 0)\n \n \n+def test_initvar_pydantic_field() -> None:\n+    @pydantic.dataclasses.dataclass\n+    class TestInitVar:\n+        x: InitVar[int] = Field(title='X')\n+\n+        def __post_init__(self, x: int):\n+            assert x == 1\n+\n+    assert TestInitVar.__pydantic_fields__['x'].init_var\n+\n+    t = TestInitVar(x=1)\n+\n+    with pytest.raises(AttributeError):\n+        t.x\n+\n+\n+@pytest.mark.xfail(reason='Ideally we should raise an attribute error, like stdlib dataclasses')\n+def test_initvar_pydantic_field() -> None:",
      "comment": "```suggestion\r\ndef test_initvar_pydantic_field_attribute_access() -> None:\r\n```",
      "comment_id": 2503936803,
      "user": "Viicos",
      "created_at": "2025-11-07T14:45:20Z",
      "url": "https://github.com/pydantic/pydantic/pull/12495#discussion_r2503936803"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12028,
      "file_path": "pydantic/_internal/_fields.py",
      "line": 429,
      "side": "LEFT",
      "diff_hunk": "@@ -413,22 +411,6 @@ def collect_model_fields(  # noqa: C901\n     return fields, class_vars\n \n \n-def _warn_on_nested_alias_in_annotation(ann_type: type[Any], ann_name: str) -> None:\n-    FieldInfo = import_cached_field_info()\n-\n-    args = getattr(ann_type, '__args__', None)\n-    if args:\n-        for anno_arg in args:\n-            if typing_objects.is_annotated(get_origin(anno_arg)):\n-                for anno_type_arg in _typing_extra.get_args(anno_arg):\n-                    if isinstance(anno_type_arg, FieldInfo) and anno_type_arg.alias is not None:\n-                        warnings.warn(\n-                            f'`alias` specification on field \"{ann_name}\" must be set on outermost annotation to take effect.',\n-                            UserWarning,\n-                        )\n-                        return",
      "comment": "This was fragile logic implemented a long time ago, that is now fully covered by the new warning.",
      "comment_id": 2175579732,
      "user": "Viicos",
      "created_at": "2025-06-30T17:36:38Z",
      "url": "https://github.com/pydantic/pydantic/pull/12028#discussion_r2175579732"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12028,
      "file_path": "pydantic/_internal/_generate_schema.py",
      "line": 587,
      "side": "RIGHT",
      "diff_hunk": "@@ -562,7 +579,15 @@ def _mapping_schema(self, tp: Any, keys_type: Any, values_type: Any) -> CoreSche\n \n         mapped_origin = MAPPING_ORIGIN_MAP[tp]\n         keys_schema = self.generate_schema(keys_type)\n-        values_schema = self.generate_schema(values_type)\n+        with warnings.catch_warnings():\n+            # We kind of abused `Field()` default factories to be able to specify\n+            # the `defaultdict`'s `default_factory`. As a consequence, we get warnings\n+            # as normally `FieldInfo.default_factory` is unsupported in the context where\n+            # `Field()` is used and our only solution is to ignore them (note that this might\n+            # wrongfully ignore valid warnings, e.g. if the `value_type` to a PEP 695 type alias",
      "comment": "This is referring to the following use case:\n\n```python\nclass Model(BaseModel):\n    a: defaultdict[int, Annotated[list[int], Field(default_factory=lambda: MyList())]]\n```",
      "comment_id": 2175581350,
      "user": "Viicos",
      "created_at": "2025-06-30T17:37:55Z",
      "url": "https://github.com/pydantic/pydantic/pull/12028#discussion_r2175581350"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12028,
      "file_path": "pydantic/_internal/_generate_schema.py",
      "line": 1559,
      "side": "RIGHT",
      "diff_hunk": "@@ -1525,7 +1550,14 @@ def _generate_parameter_schema(\n         update_field_from_config(self._config_wrapper, name, field)\n \n         with self.field_name_stack.push(name):\n-            schema = self._apply_annotations(field.annotation, [field])\n+            schema = self._apply_annotations(\n+                field.annotation,\n+                [field],\n+                # Because we pass `field` as metadata above (required for attributes relevant for\n+                # JSON Scheme generation), we need to ignore the potential warnings about `FieldInfo`\n+                # attributes that will not be used:\n+                check_unsupported_field_info_attributes=False,",
      "comment": "This is for validated function calls (see `test_unsupported_field_attribute_nested_with_function()`). We don't want to raise a warning for:\n\n```python\n@validate_call\ndef func(a: Annotated[int, Field(alias='b')]): ...\n```",
      "comment_id": 2175584535,
      "user": "Viicos",
      "created_at": "2025-06-30T17:39:21Z",
      "url": "https://github.com/pydantic/pydantic/pull/12028#discussion_r2175584535"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12028,
      "file_path": "pydantic/_internal/_generate_schema.py",
      "line": 2220,
      "side": "RIGHT",
      "diff_hunk": "@@ -2159,10 +2202,27 @@ def inner_handler(obj: Any) -> CoreSchema:\n             update_core_metadata(core_metadata, pydantic_js_annotation_functions=pydantic_js_annotation_functions)\n         return _add_custom_serialization_from_json_encoders(self._config_wrapper.json_encoders, source_type, schema)\n \n-    def _apply_single_annotation(self, schema: core_schema.CoreSchema, metadata: Any) -> core_schema.CoreSchema:\n+    def _apply_single_annotation(\n+        self,\n+        schema: core_schema.CoreSchema,\n+        metadata: Any,\n+        check_unsupported_field_info_attributes: bool = True,\n+    ) -> core_schema.CoreSchema:\n         FieldInfo = import_cached_field_info()\n \n         if isinstance(metadata, FieldInfo):\n+            if check_unsupported_field_info_attributes and (\n+                unsupported_attributes := self._get_unsupported_field_info_attributes(metadata)\n+            ):\n+                for unsupported_attr in unsupported_attributes:\n+                    warnings.warn(\n+                        f'The {unsupported_attr[0]!r} attribute with value {unsupported_attr[1]!r} was provided '\n+                        'to the `Field()` function, which is unsupported in the context it was used. '",
      "comment": "```suggestion\n                        'to the `Field()` function, which has no effect in the context it was used. '\n```\n\nmaybe?",
      "comment_id": 2175585140,
      "user": "Viicos",
      "created_at": "2025-06-30T17:39:40Z",
      "url": "https://github.com/pydantic/pydantic/pull/12028#discussion_r2175585140"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12028,
      "file_path": "pydantic/_internal/_generate_schema.py",
      "line": 2217,
      "side": "RIGHT",
      "diff_hunk": "@@ -2158,10 +2202,27 @@ def inner_handler(obj: Any) -> CoreSchema:\n             update_core_metadata(core_metadata, pydantic_js_annotation_functions=pydantic_js_annotation_functions)\n         return _add_custom_serialization_from_json_encoders(self._config_wrapper.json_encoders, source_type, schema)\n \n-    def _apply_single_annotation(self, schema: core_schema.CoreSchema, metadata: Any) -> core_schema.CoreSchema:\n+    def _apply_single_annotation(\n+        self,\n+        schema: core_schema.CoreSchema,\n+        metadata: Any,\n+        check_unsupported_field_info_attributes: bool = True,\n+    ) -> core_schema.CoreSchema:\n         FieldInfo = import_cached_field_info()\n \n         if isinstance(metadata, FieldInfo):\n+            if check_unsupported_field_info_attributes and (\n+                unsupported_attributes := self._get_unsupported_field_info_attributes(metadata)\n+            ):\n+                for unsupported_attr in unsupported_attributes:",
      "comment": "Can we do `for (attr, value) in unsupported_attributes` so we don't need `[0]` and `[1]`?",
      "comment_id": 2177807207,
      "user": "DouweM",
      "created_at": "2025-07-01T14:46:54Z",
      "url": "https://github.com/pydantic/pydantic/pull/12028#discussion_r2177807207"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12028,
      "file_path": "pydantic/_internal/_generate_schema.py",
      "line": 2285,
      "side": "RIGHT",
      "diff_hunk": "@@ -2216,11 +2277,34 @@ def _apply_single_annotation_json_schema(\n             )\n         return schema\n \n+    def _get_unsupported_field_info_attributes(self, field_info: FieldInfo) -> list[tuple[str, Any]]:\n+        \"\"\"Get the list of unsupported `FieldInfo` attributes when not directly used in `Annotated` for field annotations.\"\"\"\n+        unused_metadata: list[tuple[str, Any]] = []\n+        for unused_metadata_name in UNSUPPORTED_STANDALONE_FIELDINFO_ATTRIBUTES:\n+            if (\n+                unused_metadata_name in field_info._attributes_set",
      "comment": "If we make `UNSUPPORTED_STANDALONE_FIELDINFO_ATTRIBUTES` a set as well, we could potentially clean this up a bit by using `unsupported_attributes = UNSUPPORTED_STANDALONE_FIELDINFO_ATTRIBUTES & field_info._attributes_set` and then iterating over those\r\n\r\nEdit: Likely not relevant because we added the default values to `UNSUPPORTED_STANDALONE_FIELDINFO_ATTRIBUTES` in a later commit",
      "comment_id": 2177816765,
      "user": "DouweM",
      "created_at": "2025-07-01T14:49:09Z",
      "url": "https://github.com/pydantic/pydantic/pull/12028#discussion_r2177816765"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12420,
      "file_path": "tests/test_validators.py",
      "line": 3105,
      "side": "RIGHT",
      "diff_hunk": "@@ -3101,3 +3101,23 @@ def wrapped_field_serializer(cls, field_value, validator):\n \n     my_model = MyParentModel.model_validate({'nested': {'inner_value': 'foo'}})\n     assert my_model.nested.inner_value == 'after_prefix:wrap_prefix:foo'\n+\n+@pytest.mark.xfail(reason=\"Bug: Nested 'after' model_validator is re-executed. See issue #8452.\", raises=AssertionError)",
      "comment": "```suggestion\n@pytest.mark.xfail(reason=\"Bug: Nested 'after' model_validator is re-executed. See issue #8452.\", raises=ValidationError)\n```",
      "comment_id": 2454969535,
      "user": "davidhewitt",
      "created_at": "2025-10-23T12:31:46Z",
      "url": "https://github.com/pydantic/pydantic/pull/12420#discussion_r2454969535"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12420,
      "file_path": "tests/test_validators.py",
      "line": 3123,
      "side": "RIGHT",
      "diff_hunk": "@@ -3101,3 +3101,23 @@ def wrapped_field_serializer(cls, field_value, validator):\n \n     my_model = MyParentModel.model_validate({'nested': {'inner_value': 'foo'}})\n     assert my_model.nested.inner_value == 'after_prefix:wrap_prefix:foo'\n+\n+@pytest.mark.xfail(reason=\"Bug: Nested 'after' model_validator is re-executed. See issue #8452.\", raises=AssertionError)\n+def test_nested_model_validator_not_reexecuted():\n+    \"\"\"See https://github.com/pydantic/pydantic/issues/8452 for context.\n+\n+    Reproduces the bug in issue #8452 where a nested model's `model_validator` with `mode='after'` is unexpectedly re-executed.\n+    \"\"\"\n+    class Sub(BaseModel):\n+        @model_validator(mode='after')\n+        def _validate(self):\n+            # This line should not be reached when `Sub` is nested inside `Base`\n+            assert False, 'Sub model_validator was re-executed'\n+\n+    class Base(BaseModel):\n+        sub: Sub #<-- This throws assertionerror\n+\n+    sub: Sub = Sub.model_construct() # Create a Sub instance without triggering validation (e.g., using model_construct)\n+    \n+    # Attempt to create Base with the Sub instance. This line should succeed if the bug is fixed, but currently raises ValidationError.\n+    base: Base = Base(sub=sub) # <-- This throws AssertionError because Sub's 'after' validator runs again.",
      "comment": "```suggestion\n    Base(sub=sub) # <-- This throws AssertionError because Sub's 'after' validator runs again.\n```",
      "comment_id": 2454970072,
      "user": "davidhewitt",
      "created_at": "2025-10-23T12:31:56Z",
      "url": "https://github.com/pydantic/pydantic/pull/12420#discussion_r2454970072"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12430,
      "file_path": "pydantic/json_schema.py",
      "line": 1760,
      "side": "LEFT",
      "diff_hunk": "@@ -1756,13 +1756,19 @@ def field_is_required(\n         Returns:\n             `True` if the field should be marked as required in the generated JSON schema, `False` otherwise.\n         \"\"\"\n-        if self.mode == 'serialization' and self._config.json_schema_serialization_defaults_required:\n-            return not field.get('serialization_exclude')",
      "comment": "Note that there's a change in behavior here, but should be considered as a bug fix: we only take into account `serialization_exclude` if `json_schema_serialization_defaults_required=True`, which doesn't really make sense.",
      "comment_id": 2445335651,
      "user": "Viicos",
      "created_at": "2025-10-20T15:20:08Z",
      "url": "https://github.com/pydantic/pydantic/pull/12430#discussion_r2445335651"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12411,
      "file_path": "pydantic/fields.py",
      "line": 241,
      "side": "RIGHT",
      "diff_hunk": "@@ -215,10 +231,14 @@ def __init__(self, **kwargs: Unpack[_FieldInfoInputs]) -> None:\n \n         See the signature of `pydantic.fields.Field` for more details about the expected arguments.\n         \"\"\"\n+        # Tracking the explicitly set attributes is necessary to correctly merge `Field()` functions\n+        # (e.g. with `Annotated[int, Field(alias='a'), Field(alias=None)]`, even though `None` is the default value,\n+        # we need to track that `alias=None` was explicitly set):\n         self._attributes_set = {k: v for k, v in kwargs.items() if v is not _Unset and k not in self.metadata_lookup}\n         kwargs = {k: _DefaultValues.get(k) if v is _Unset else v for k, v in kwargs.items()}  # type: ignore\n         self.annotation = kwargs.get('annotation')\n \n+        # Note: in theory, the second `pop()` arguments are not required below, as defaults are already set from `_DefaultsValues`.",
      "comment": "As you can see in this PR, I added the missing default values in `_DefaultsValues`, including the one for `deprecated` (but it is only bound to happen again with this complex logic..)  ",
      "comment_id": 2439409032,
      "user": "Viicos",
      "created_at": "2025-10-17T11:14:09Z",
      "url": "https://github.com/pydantic/pydantic/pull/12411#discussion_r2439409032"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11957,
      "file_path": "pydantic/functional_validators.py",
      "line": 716,
      "side": "RIGHT",
      "diff_hunk": "@@ -713,7 +713,8 @@ def verify_square(self) -> Self:\n \n     def dec(f: Any) -> _decorators.PydanticDescriptorProxy[Any]:\n         # auto apply the @classmethod decorator\n-        f = _decorators.ensure_classmethod_based_on_signature(f)\n+        if mode != 'after':",
      "comment": "I think it's worth putting a comment in to explain why we want this in all modes except for `after`.",
      "comment_id": 2143334773,
      "user": "DouweM",
      "created_at": "2025-06-12T17:55:58Z",
      "url": "https://github.com/pydantic/pydantic/pull/11957#discussion_r2143334773"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11957,
      "file_path": "tests/test_model_validator.py",
      "line": 148,
      "side": "RIGHT",
      "diff_hunk": "@@ -136,3 +136,13 @@ def validate_model_after(self) -> Model:\n     Model.model_validate({'inner': {'inner': {'inner': None}}})\n     assert calls == ['before'] * 3 + ['after'] * 3\n     calls.clear()\n+\n+\n+def test_after_validator_wrong_signature() -> None:\n+    with pytest.raises(PydanticUserError):\n+\n+        class Model(BaseModel):\n+            @model_validator(mode='after')\n+            # This used to be converted into a classmethod, resulting\n+            # in this inconsistent signature still accepted:\n+            def validator(cls, model, info): ...",
      "comment": "Shouldn't this be `self` instead of `cls` in this case?",
      "comment_id": 2417020325,
      "user": "antazoey",
      "created_at": "2025-10-09T14:32:44Z",
      "url": "https://github.com/pydantic/pydantic/pull/11957#discussion_r2417020325"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11957,
      "file_path": "tests/test_model_validator.py",
      "line": 148,
      "side": "RIGHT",
      "diff_hunk": "@@ -136,3 +136,13 @@ def validate_model_after(self) -> Model:\n     Model.model_validate({'inner': {'inner': {'inner': None}}})\n     assert calls == ['before'] * 3 + ['after'] * 3\n     calls.clear()\n+\n+\n+def test_after_validator_wrong_signature() -> None:\n+    with pytest.raises(PydanticUserError):\n+\n+        class Model(BaseModel):\n+            @model_validator(mode='after')\n+            # This used to be converted into a classmethod, resulting\n+            # in this inconsistent signature still accepted:\n+            def validator(cls, model, info): ...",
      "comment": "This is wrapped in a `pytest.raises()` context manager, and was meant to test that this particular invalid signature is now rejected.",
      "comment_id": 2417108247,
      "user": "Viicos",
      "created_at": "2025-10-09T15:17:37Z",
      "url": "https://github.com/pydantic/pydantic/pull/11957#discussion_r2417108247"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12366,
      "file_path": "pydantic/_internal/_decorators.py",
      "line": 560,
      "side": "RIGHT",
      "diff_hunk": "@@ -554,7 +557,7 @@ def inspect_validator(validator: Callable[..., Any], mode: FieldValidatorModes)\n             return False\n \n     raise PydanticUserError(\n-        f'Unrecognized field_validator function signature for {validator} with `mode={mode}`:{sig}',\n+        f'Unrecognized {type} function signature for {validator} with `mode={mode}`: {sig}',",
      "comment": "```suggestion\n        f'Unrecognized {type}_validator function signature for {validator} with `mode={mode}`: {sig}',\n```",
      "comment_id": 2419159200,
      "user": "davidhewitt",
      "created_at": "2025-10-10T10:00:22Z",
      "url": "https://github.com/pydantic/pydantic/pull/12366#discussion_r2419159200"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12366,
      "file_path": "pydantic/_internal/_decorators.py",
      "line": 560,
      "side": "RIGHT",
      "diff_hunk": "@@ -554,7 +557,7 @@ def inspect_validator(validator: Callable[..., Any], mode: FieldValidatorModes)\n             return False\n \n     raise PydanticUserError(\n-        f'Unrecognized field_validator function signature for {validator} with `mode={mode}`:{sig}',\n+        f'Unrecognized {type} function signature for {validator} with `mode={mode}`: {sig}',",
      "comment": "I removed the underscore in particular for field validators, that aren't necessarily defined using the `@field_validator` decorator (i.e. with the `*Validator` metadata classes).",
      "comment_id": 2422854022,
      "user": "Viicos",
      "created_at": "2025-10-11T13:41:53Z",
      "url": "https://github.com/pydantic/pydantic/pull/12366#discussion_r2422854022"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12370,
      "file_path": "pydantic/_internal/_typing_extra.py",
      "line": 522,
      "side": "RIGHT",
      "diff_hunk": "@@ -518,7 +518,19 @@ def _eval_type(\n     localns: MappingNamespace | None = None,\n     type_params: tuple[Any, ...] | None = None,\n ) -> Any:\n-    if sys.version_info >= (3, 13):\n+    if sys.version_info >= (3, 14):\n+        # Starting in 3.14, `_eval_type()` does *not* apply `_type_convert()`",
      "comment": "Did `_type_convert()` do anything else, do those cases also matter?",
      "comment_id": 2419153873,
      "user": "davidhewitt",
      "created_at": "2025-10-10T09:57:55Z",
      "url": "https://github.com/pydantic/pydantic/pull/12370#discussion_r2419153873"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12370,
      "file_path": "pydantic/_internal/_typing_extra.py",
      "line": 522,
      "side": "RIGHT",
      "diff_hunk": "@@ -518,7 +518,19 @@ def _eval_type(\n     localns: MappingNamespace | None = None,\n     type_params: tuple[Any, ...] | None = None,\n ) -> Any:\n-    if sys.version_info >= (3, 13):\n+    if sys.version_info >= (3, 14):\n+        # Starting in 3.14, `_eval_type()` does *not* apply `_type_convert()`",
      "comment": "It converts str values to forward refs, but this is already handled by `_eval_type()` so after calling it we are guaranteed to not have to deal with forward refs anymore.",
      "comment_id": 2422853593,
      "user": "Viicos",
      "created_at": "2025-10-11T13:40:48Z",
      "url": "https://github.com/pydantic/pydantic/pull/12370#discussion_r2422853593"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12355,
      "file_path": "tests/test_deferred_annotations.py",
      "line": 93,
      "side": "RIGHT",
      "diff_hunk": "@@ -80,3 +88,31 @@ class A:\n     Int = int\n \n     assert A(a='1').a == 1\n+\n+\n+def test_deferred_annotations_return_values() -> None:",
      "comment": "The decorated function inspection doesn't special case modes, so I think this should be fine",
      "comment_id": 2413886829,
      "user": "Viicos",
      "created_at": "2025-10-08T13:34:51Z",
      "url": "https://github.com/pydantic/pydantic/pull/12355#discussion_r2413886829"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12001,
      "file_path": "pydantic/main.py",
      "line": 1751,
      "side": "RIGHT",
      "diff_hunk": "@@ -1744,6 +1748,7 @@ def create_model(  # noqa: C901\n     namespace: dict[str, Any] = {'__annotations__': annotations, '__module__': __module__}\n     if __doc__:\n         namespace.update({'__doc__': __doc__})\n+    namespace.update({'__qualname__': __qualname__ or model_name})",
      "comment": "```suggestion\r\n    if __qualname__ is not None:\r\n        namespace.update({'__qualname__': __qualname__})\r\n```",
      "comment_id": 2410781701,
      "user": "Viicos",
      "created_at": "2025-10-07T14:10:30Z",
      "url": "https://github.com/pydantic/pydantic/pull/12001#discussion_r2410781701"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12001,
      "file_path": "pydantic/main.py",
      "line": 1752,
      "side": "RIGHT",
      "diff_hunk": "@@ -1744,6 +1748,8 @@ def create_model(  # noqa: C901\n     namespace: dict[str, Any] = {'__annotations__': annotations, '__module__': __module__}\n     if __doc__:\n         namespace.update({'__doc__': __doc__})\n+    if __qualname__ is not None:\n+        namespace.update({'__qualname__': __qualname__})",
      "comment": "```suggestion\r\n    if __doc__:\r\n        namespace['__doc__'] = __doc__\r\n    if __qualname__ is not None:\r\n        namespace['__qualname__'] = __qualname__\r\n```",
      "comment_id": 2410797171,
      "user": "Viicos",
      "created_at": "2025-10-07T14:15:13Z",
      "url": "https://github.com/pydantic/pydantic/pull/12001#discussion_r2410797171"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12147,
      "file_path": "pydantic/main.py",
      "line": 544,
      "side": "RIGHT",
      "diff_hunk": "@@ -541,6 +541,7 @@ def model_json_schema(\n         cls,\n         by_alias: bool = True,\n         ref_template: str = DEFAULT_REF_TEMPLATE,\n+        union_format: Literal['any_of', 'primitive_type_array'] = 'any_of',",
      "comment": "Adding extra parameters like this is fine, but can be annoying for users subclassing `BaseModel` and overriding the `model_json_schema()` method, as they will need to update the definition.\r\n\r\nOne alternative is to have a single parameter (e.g. `TypedDict`) holding all the configuration, but it adds complexity on the caller side:\r\n\r\n```python\r\nModel.model_json_schema(settings={'union_format': ...})\r\n```\r\n\r\nAnd overriding the method isn't really a robust solution anyway.\r\n\r\nHowever, we should definitely make these keyword only. This will be a breaking change, so either we do it know but make sure to properly announce it, or wait for V3 (and use deprecated overloads until then).",
      "comment_id": 2261377497,
      "user": "Viicos",
      "created_at": "2025-08-07T20:54:58Z",
      "url": "https://github.com/pydantic/pydantic/pull/12147#discussion_r2261377497"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12147,
      "file_path": "pydantic/main.py",
      "line": 544,
      "side": "RIGHT",
      "diff_hunk": "@@ -541,6 +541,7 @@ def model_json_schema(\n         cls,\n         by_alias: bool = True,\n         ref_template: str = DEFAULT_REF_TEMPLATE,\n+        union_format: Literal['any_of', 'primitive_type_array'] = 'any_of',",
      "comment": "Can you make `union_format` keyword-only now, and leave the rest for later? I guess that would also then not be breaking?",
      "comment_id": 2401562807,
      "user": "davidhewitt",
      "created_at": "2025-10-03T11:15:56Z",
      "url": "https://github.com/pydantic/pydantic/pull/12147#discussion_r2401562807"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12147,
      "file_path": "tests/test_json_schema.py",
      "line": 7131,
      "side": "RIGHT",
      "diff_hunk": "@@ -7072,3 +7073,58 @@ def test_decimal_pattern_reject_invalid_not_numerical_values_with_decimal_places\n ) -> None:\n     pattern = get_decimal_pattern()\n     assert re.fullmatch(pattern, invalid_decimal) is None\n+\n+\n+def test_union_format_primitive_type_array() -> None:\n+    class Sub(BaseModel):\n+        pass\n+\n+    class Model(BaseModel):\n+        a: Optional[int]\n+        b: Union[int, str, bool]\n+        c: Union[Annotated[str, Field(max_length=3)], Annotated[str, Field(min_length=5)]]\n+        d: Union[int, str, Annotated[bool, Field(description='test')]]\n+        e: Union[int, list[int]]\n+        f: Union[int, Sub]\n+\n+    assert Model.model_json_schema(union_format='primitive_type_array') == {\n+        '$defs': {'Sub': {'properties': {}, 'title': 'Sub', 'type': 'object'}},\n+        'properties': {\n+            'a': {'title': 'A', 'type': ['integer', 'null']},\n+            'b': {'title': 'B', 'type': ['integer', 'string', 'boolean']},\n+            'c': {\n+                'anyOf': [\n+                    {'maxLength': 3, 'type': 'string'},\n+                    {'minLength': 5, 'type': 'string'},\n+                ],\n+                'title': 'C',\n+            },\n+            'd': {\n+                'anyOf': [\n+                    {'type': 'integer'},\n+                    {'type': 'string'},\n+                    {'description': 'test', 'type': 'boolean'},\n+                ],\n+                'title': 'D',\n+            },\n+            'e': {\n+                'anyOf': [\n+                    {'type': 'integer'},\n+                    {'items': {'type': 'integer'}, 'type': 'array'},\n+                ],\n+                'title': 'E',\n+            },\n+            'f': {'anyOf': [{'type': 'integer'}, {'$ref': '#/$defs/Sub'}], 'title': 'F'},\n+        },\n+        'required': ['a', 'b', 'c', 'd', 'e', 'f'],\n+        'title': 'Model',\n+        'type': 'object',\n+    }\n+\n+\n+def test_union_format_primitive_type_array_deduplicated() -> None:\n+    gen_js = GenerateJsonSchema(union_format='primitive_type_array')\n+\n+    assert gen_js.union_schema(\n+        core_schema.union_schema([core_schema.int_schema(), core_schema.str_schema(), core_schema.int_schema()])\n+    ) == {'type': ['integer', 'string']}",
      "comment": "Maybe add a case where there's a str schema with a constraint too, and check the result still deduplicates the ints even if it switches back to `anyOf`?",
      "comment_id": 2401567816,
      "user": "davidhewitt",
      "created_at": "2025-10-03T11:18:52Z",
      "url": "https://github.com/pydantic/pydantic/pull/12147#discussion_r2401567816"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12147,
      "file_path": "pydantic/main.py",
      "line": 544,
      "side": "RIGHT",
      "diff_hunk": "@@ -541,6 +541,7 @@ def model_json_schema(\n         cls,\n         by_alias: bool = True,\n         ref_template: str = DEFAULT_REF_TEMPLATE,\n+        union_format: Literal['any_of', 'primitive_type_array'] = 'any_of',",
      "comment": ":+1: , created https://github.com/pydantic/pydantic/issues/12332 as well.",
      "comment_id": 2406276369,
      "user": "Viicos",
      "created_at": "2025-10-06T13:22:12Z",
      "url": "https://github.com/pydantic/pydantic/pull/12147#discussion_r2406276369"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11987,
      "file_path": "tests/test_json_schema.py",
      "line": 6951,
      "side": "RIGHT",
      "diff_hunk": "@@ -6811,3 +6946,147 @@ def test_json_schema_arguments_v3_aliases() -> None:\n         },\n         'required': ['b'],\n     }\n+\n+\n+class TestDecimalPattern:",
      "comment": "Let's flatten this to use functions instead of methods.",
      "comment_id": 2180865070,
      "user": "Viicos",
      "created_at": "2025-07-02T19:56:07Z",
      "url": "https://github.com/pydantic/pydantic/pull/11987#discussion_r2180865070"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11987,
      "file_path": "pydantic/json_schema.py",
      "line": 690,
      "side": "RIGHT",
      "diff_hunk": "@@ -674,7 +674,31 @@ def decimal_schema(self, schema: core_schema.DecimalSchema) -> JsonSchemaValue:\n         Returns:\n             The generated JSON schema.\n         \"\"\"\n-        json_schema = self.str_schema(core_schema.str_schema())\n+\n+        def get_decimal_pattern(schema: core_schema.DecimalSchema) -> str:\n+            max_digits = schema.get('max_digits', '')\n+            decimal_places = schema.get('decimal_places', '')\n+            integer_places = max_digits\n+\n+            if isinstance(max_digits, int) and isinstance(decimal_places, int):\n+                if (diff := max_digits - decimal_places) > 0:\n+                    integer_places = diff\n+                else:\n+                    integer_places = 0\n+\n+            pattern = (\n+                r'^(?!^[+-\\.]*$)'  # check string is not empty and not single or sequence of \".+-\" characters.",
      "comment": "I think you don't need to escape the `.` when inside brackets.",
      "comment_id": 2180866129,
      "user": "Viicos",
      "created_at": "2025-07-02T19:56:48Z",
      "url": "https://github.com/pydantic/pydantic/pull/11987#discussion_r2180866129"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11987,
      "file_path": "tests/test_json_schema.py",
      "line": 6951,
      "side": "RIGHT",
      "diff_hunk": "@@ -6811,3 +6946,147 @@ def test_json_schema_arguments_v3_aliases() -> None:\n         },\n         'required': ['b'],\n     }\n+\n+\n+class TestDecimalPattern:",
      "comment": "Resolved with [this commit](https://github.com/pydantic/pydantic/pull/11987/commits/73afd017f4cb1683540cdc08f3601dc90886cd61)\r\n",
      "comment_id": 2184144466,
      "user": "Dima-Bulavenko",
      "created_at": "2025-07-04T02:16:08Z",
      "url": "https://github.com/pydantic/pydantic/pull/11987#discussion_r2184144466"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11987,
      "file_path": "pydantic/json_schema.py",
      "line": 690,
      "side": "RIGHT",
      "diff_hunk": "@@ -674,7 +674,31 @@ def decimal_schema(self, schema: core_schema.DecimalSchema) -> JsonSchemaValue:\n         Returns:\n             The generated JSON schema.\n         \"\"\"\n-        json_schema = self.str_schema(core_schema.str_schema())\n+\n+        def get_decimal_pattern(schema: core_schema.DecimalSchema) -> str:\n+            max_digits = schema.get('max_digits', '')\n+            decimal_places = schema.get('decimal_places', '')\n+            integer_places = max_digits\n+\n+            if isinstance(max_digits, int) and isinstance(decimal_places, int):\n+                if (diff := max_digits - decimal_places) > 0:\n+                    integer_places = diff\n+                else:\n+                    integer_places = 0\n+\n+            pattern = (\n+                r'^(?!^[+-\\.]*$)'  # check string is not empty and not single or sequence of \".+-\" characters.",
      "comment": "Thanks for pointing that out! I\u2019ve resolved it in commit https://github.com/pydantic/pydantic/commit/5874292e75f955fe80334c7c083be0832edcf0df.",
      "comment_id": 2184340350,
      "user": "Dima-Bulavenko",
      "created_at": "2025-07-04T04:38:24Z",
      "url": "https://github.com/pydantic/pydantic/pull/11987#discussion_r2184340350"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11987,
      "file_path": "pydantic/json_schema.py",
      "line": 715,
      "side": "RIGHT",
      "diff_hunk": "@@ -674,7 +674,49 @@ def decimal_schema(self, schema: core_schema.DecimalSchema) -> JsonSchemaValue:\n         Returns:\n             The generated JSON schema.\n         \"\"\"\n-        json_schema = self.str_schema(core_schema.str_schema())\n+\n+        def get_decimal_pattern(schema: core_schema.DecimalSchema) -> str:\n+            max_digits = schema.get('max_digits')\n+            decimal_places = schema.get('decimal_places')\n+\n+            pattern = (\n+                r'^(?!^[-+.]*$)[+-]?0*'  # check it is not empty string and not one or sequence of \".+-\" characters.\n+            )\n+\n+            # Case 1: Both max_digits and decimal_places are set\n+            if max_digits is not None and decimal_places is not None:\n+                integer_places = max(0, max_digits - decimal_places)\n+                pattern += (\n+                    rf'(?:'\n+                    rf'\\d{{0,{integer_places}}}'\n+                    rf'|'\n+                    rf'(?=[\\d.]{{1,{max_digits + 1}}}0*$)'\n+                    rf'\\d{{0,{integer_places}}}\\.\\d{{0,{decimal_places}}}0*$'\n+                    rf')'\n+                )\n+\n+            # Case 2: Only max_digits is set\n+            elif max_digits is not None and decimal_places is None:\n+                pattern += (\n+                    rf'(?:'\n+                    rf'\\d{{0,{max_digits}}}'\n+                    rf'|'\n+                    rf'(?=[\\d.]{{1,{max_digits + 1}}}0*$)'\n+                    rf'\\d*\\.\\d*0*$'\n+                    rf')'\n+                )\n+\n+            # Case 3: Only decimal_places is set\n+            elif max_digits is None and decimal_places is not None:\n+                pattern += rf'\\d*\\.?\\d{{0,{decimal_places}}}0*$'\n+\n+            # Case 4: Both are None (no restrictions)\n+            else:\n+                pattern += r'\\d*\\.?\\d*$'  # look for arbitrary integer or decimal",
      "comment": "This allows `'.'` as an input, which can't be validated.",
      "comment_id": 2187005604,
      "user": "Viicos",
      "created_at": "2025-07-05T09:17:10Z",
      "url": "https://github.com/pydantic/pydantic/pull/11987#discussion_r2187005604"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11987,
      "file_path": "pydantic/json_schema.py",
      "line": 699,
      "side": "RIGHT",
      "diff_hunk": "@@ -674,7 +674,49 @@ def decimal_schema(self, schema: core_schema.DecimalSchema) -> JsonSchemaValue:\n         Returns:\n             The generated JSON schema.\n         \"\"\"\n-        json_schema = self.str_schema(core_schema.str_schema())\n+\n+        def get_decimal_pattern(schema: core_schema.DecimalSchema) -> str:\n+            max_digits = schema.get('max_digits')\n+            decimal_places = schema.get('decimal_places')\n+\n+            pattern = (\n+                r'^(?!^[-+.]*$)[+-]?0*'  # check it is not empty string and not one or sequence of \".+-\" characters.\n+            )\n+\n+            # Case 1: Both max_digits and decimal_places are set\n+            if max_digits is not None and decimal_places is not None:\n+                integer_places = max(0, max_digits - decimal_places)\n+                pattern += (\n+                    rf'(?:'\n+                    rf'\\d{{0,{integer_places}}}'\n+                    rf'|'\n+                    rf'(?=[\\d.]{{1,{max_digits + 1}}}0*$)'\n+                    rf'\\d{{0,{integer_places}}}\\.\\d{{0,{decimal_places}}}0*$'\n+                    rf')'\n+                )\n+\n+            # Case 2: Only max_digits is set\n+            elif max_digits is not None and decimal_places is None:",
      "comment": "With `max_digits` set to e.g. 5, it wrongfully matches `'1000.1111111'`, etc.",
      "comment_id": 2187013721,
      "user": "Viicos",
      "created_at": "2025-07-05T09:26:46Z",
      "url": "https://github.com/pydantic/pydantic/pull/11987#discussion_r2187013721"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11987,
      "file_path": "pydantic/json_schema.py",
      "line": 715,
      "side": "RIGHT",
      "diff_hunk": "@@ -674,7 +674,49 @@ def decimal_schema(self, schema: core_schema.DecimalSchema) -> JsonSchemaValue:\n         Returns:\n             The generated JSON schema.\n         \"\"\"\n-        json_schema = self.str_schema(core_schema.str_schema())\n+\n+        def get_decimal_pattern(schema: core_schema.DecimalSchema) -> str:\n+            max_digits = schema.get('max_digits')\n+            decimal_places = schema.get('decimal_places')\n+\n+            pattern = (\n+                r'^(?!^[-+.]*$)[+-]?0*'  # check it is not empty string and not one or sequence of \".+-\" characters.\n+            )\n+\n+            # Case 1: Both max_digits and decimal_places are set\n+            if max_digits is not None and decimal_places is not None:\n+                integer_places = max(0, max_digits - decimal_places)\n+                pattern += (\n+                    rf'(?:'\n+                    rf'\\d{{0,{integer_places}}}'\n+                    rf'|'\n+                    rf'(?=[\\d.]{{1,{max_digits + 1}}}0*$)'\n+                    rf'\\d{{0,{integer_places}}}\\.\\d{{0,{decimal_places}}}0*$'\n+                    rf')'\n+                )\n+\n+            # Case 2: Only max_digits is set\n+            elif max_digits is not None and decimal_places is None:\n+                pattern += (\n+                    rf'(?:'\n+                    rf'\\d{{0,{max_digits}}}'\n+                    rf'|'\n+                    rf'(?=[\\d.]{{1,{max_digits + 1}}}0*$)'\n+                    rf'\\d*\\.\\d*0*$'\n+                    rf')'\n+                )\n+\n+            # Case 3: Only decimal_places is set\n+            elif max_digits is None and decimal_places is not None:\n+                pattern += rf'\\d*\\.?\\d{{0,{decimal_places}}}0*$'\n+\n+            # Case 4: Both are None (no restrictions)\n+            else:\n+                pattern += r'\\d*\\.?\\d*$'  # look for arbitrary integer or decimal",
      "comment": "Thank you for pointing this out.\r\nHowever, `r'\\d*\\.?\\d*$'` is not the full pattern used for validation.\r\n\r\nThe complete pattern includes an additional regex component that prevents `'.'` from being a valid input:\r\nhttps://github.com/pydantic/pydantic/blob/88adb2d4806a37fd21ae06f88d4670190759974f/pydantic/json_schema.py#L682-L684\r\n\r\nAdditionally, the edge case for `'.'` is covered by the following test:\r\nhttps://github.com/pydantic/pydantic/blob/88adb2d4806a37fd21ae06f88d4670190759974f/tests/test_json_schema.py#L7005-L7009",
      "comment_id": 2187273655,
      "user": "Dima-Bulavenko",
      "created_at": "2025-07-05T13:35:22Z",
      "url": "https://github.com/pydantic/pydantic/pull/11987#discussion_r2187273655"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11987,
      "file_path": "pydantic/json_schema.py",
      "line": 699,
      "side": "RIGHT",
      "diff_hunk": "@@ -674,7 +674,49 @@ def decimal_schema(self, schema: core_schema.DecimalSchema) -> JsonSchemaValue:\n         Returns:\n             The generated JSON schema.\n         \"\"\"\n-        json_schema = self.str_schema(core_schema.str_schema())\n+\n+        def get_decimal_pattern(schema: core_schema.DecimalSchema) -> str:\n+            max_digits = schema.get('max_digits')\n+            decimal_places = schema.get('decimal_places')\n+\n+            pattern = (\n+                r'^(?!^[-+.]*$)[+-]?0*'  # check it is not empty string and not one or sequence of \".+-\" characters.\n+            )\n+\n+            # Case 1: Both max_digits and decimal_places are set\n+            if max_digits is not None and decimal_places is not None:\n+                integer_places = max(0, max_digits - decimal_places)\n+                pattern += (\n+                    rf'(?:'\n+                    rf'\\d{{0,{integer_places}}}'\n+                    rf'|'\n+                    rf'(?=[\\d.]{{1,{max_digits + 1}}}0*$)'\n+                    rf'\\d{{0,{integer_places}}}\\.\\d{{0,{decimal_places}}}0*$'\n+                    rf')'\n+                )\n+\n+            # Case 2: Only max_digits is set\n+            elif max_digits is not None and decimal_places is None:",
      "comment": "Thank you for your feedback.\r\n\r\nFrom my testing, the pattern appears to work correctly when\u00a0`max_digits=5`\u00a0and the invalid value\u00a0`'1000.1111111'`\u00a0is rejected as expected.\r\n\r\nHere is a test demonstrating this behavior:\r\n```python\r\n@pytest.fixture\r\ndef get_decimal_pattern():\r\n    def pattern(max_digits=None, decimal_places=None) -> str:\r\n        field = TypeAdapter(Annotated[Decimal, Field(max_digits=max_digits, decimal_places=decimal_places)])\r\n        return field.json_schema()['anyOf'][1]['pattern']\r\n    return pattern\r\n\r\n@pytest.mark.parametrize('invalid_decimal', ['1000.1111111'])\r\ndef test_only_max_digits_set(invalid_decimal, get_decimal_pattern):\r\n    pattern = get_decimal_pattern(max_digits=5, decimal_places=None)\r\n    assert re.fullmatch(pattern, invalid_decimal) is None\r\n```\r\n\r\nLet me know if there's a specific case I may have missed!",
      "comment_id": 2187293022,
      "user": "Dima-Bulavenko",
      "created_at": "2025-07-05T13:47:39Z",
      "url": "https://github.com/pydantic/pydantic/pull/11987#discussion_r2187293022"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12324,
      "file_path": "pydantic/type_adapter.py",
      "line": 225,
      "side": "RIGHT",
      "diff_hunk": "@@ -215,19 +216,36 @@ def __init__(\n         self.pydantic_complete = False\n \n         parent_frame = self._fetch_parent_frame()\n-        if parent_frame is not None:\n-            globalns = parent_frame.f_globals\n-            # Do not provide a local ns if the type adapter happens to be instantiated at the module level:\n-            localns = parent_frame.f_locals if parent_frame.f_locals is not globalns else {}\n+        if isinstance(type, types.FunctionType):\n+            # Special case functions, which are *not* pushed to the `NsResolver` stack and without this special case\n+            # would only have access to the parent namespace where the `TypeAdapter` was instantiated (if the function is defined\n+            # in another module, we need to look at that module's globals).\n+            if parent_frame is not None:\n+                parent_ns = (\n+                    parent_frame.f_globals if parent_frame.f_globals is parent_frame.f_locals else parent_frame.f_locals",
      "comment": "`globals if globals is locals else locals` .. isn't this always going to be equivalent to `locals`?\r\n\r\n```suggestion\r\n                    parent_frame.f_locals\r\n```",
      "comment_id": 2404459009,
      "user": "davidhewitt",
      "created_at": "2025-10-05T12:22:09Z",
      "url": "https://github.com/pydantic/pydantic/pull/12324#discussion_r2404459009"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12324,
      "file_path": "pydantic/type_adapter.py",
      "line": 225,
      "side": "RIGHT",
      "diff_hunk": "@@ -215,19 +216,36 @@ def __init__(\n         self.pydantic_complete = False\n \n         parent_frame = self._fetch_parent_frame()\n-        if parent_frame is not None:\n-            globalns = parent_frame.f_globals\n-            # Do not provide a local ns if the type adapter happens to be instantiated at the module level:\n-            localns = parent_frame.f_locals if parent_frame.f_locals is not globalns else {}\n+        if isinstance(type, types.FunctionType):\n+            # Special case functions, which are *not* pushed to the `NsResolver` stack and without this special case\n+            # would only have access to the parent namespace where the `TypeAdapter` was instantiated (if the function is defined\n+            # in another module, we need to look at that module's globals).\n+            if parent_frame is not None:\n+                parent_ns = (\n+                    parent_frame.f_globals if parent_frame.f_globals is parent_frame.f_locals else parent_frame.f_locals",
      "comment": "In other words, this means _use the globals if the TypeAdapter was created at the module level, else use the locals (e.g. in a function)_. In the former, globals and locals are the same (module's `__dict__`).",
      "comment_id": 2404659532,
      "user": "Viicos",
      "created_at": "2025-10-05T20:43:16Z",
      "url": "https://github.com/pydantic/pydantic/pull/12324#discussion_r2404659532"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12324,
      "file_path": "pydantic/type_adapter.py",
      "line": 225,
      "side": "RIGHT",
      "diff_hunk": "@@ -215,19 +216,36 @@ def __init__(\n         self.pydantic_complete = False\n \n         parent_frame = self._fetch_parent_frame()\n-        if parent_frame is not None:\n-            globalns = parent_frame.f_globals\n-            # Do not provide a local ns if the type adapter happens to be instantiated at the module level:\n-            localns = parent_frame.f_locals if parent_frame.f_locals is not globalns else {}\n+        if isinstance(type, types.FunctionType):\n+            # Special case functions, which are *not* pushed to the `NsResolver` stack and without this special case\n+            # would only have access to the parent namespace where the `TypeAdapter` was instantiated (if the function is defined\n+            # in another module, we need to look at that module's globals).\n+            if parent_frame is not None:\n+                parent_ns = (\n+                    parent_frame.f_globals if parent_frame.f_globals is parent_frame.f_locals else parent_frame.f_locals",
      "comment": "Right, but even if the intent is to do that, the expression written here completely simplifies away? Maybe a bug?",
      "comment_id": 2404675968,
      "user": "davidhewitt",
      "created_at": "2025-10-05T21:27:58Z",
      "url": "https://github.com/pydantic/pydantic/pull/12324#discussion_r2404675968"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12324,
      "file_path": "pydantic/type_adapter.py",
      "line": 225,
      "side": "RIGHT",
      "diff_hunk": "@@ -215,19 +216,36 @@ def __init__(\n         self.pydantic_complete = False\n \n         parent_frame = self._fetch_parent_frame()\n-        if parent_frame is not None:\n-            globalns = parent_frame.f_globals\n-            # Do not provide a local ns if the type adapter happens to be instantiated at the module level:\n-            localns = parent_frame.f_locals if parent_frame.f_locals is not globalns else {}\n+        if isinstance(type, types.FunctionType):\n+            # Special case functions, which are *not* pushed to the `NsResolver` stack and without this special case\n+            # would only have access to the parent namespace where the `TypeAdapter` was instantiated (if the function is defined\n+            # in another module, we need to look at that module's globals).\n+            if parent_frame is not None:\n+                parent_ns = (\n+                    parent_frame.f_globals if parent_frame.f_globals is parent_frame.f_locals else parent_frame.f_locals",
      "comment": "Hum I'm not sure? It's not `locals if globals is locals else locals`, but `globals if globals is locals else locals`",
      "comment_id": 2405256165,
      "user": "Viicos",
      "created_at": "2025-10-06T08:01:39Z",
      "url": "https://github.com/pydantic/pydantic/pull/12324#discussion_r2405256165"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12324,
      "file_path": "pydantic/type_adapter.py",
      "line": 225,
      "side": "RIGHT",
      "diff_hunk": "@@ -215,19 +216,36 @@ def __init__(\n         self.pydantic_complete = False\n \n         parent_frame = self._fetch_parent_frame()\n-        if parent_frame is not None:\n-            globalns = parent_frame.f_globals\n-            # Do not provide a local ns if the type adapter happens to be instantiated at the module level:\n-            localns = parent_frame.f_locals if parent_frame.f_locals is not globalns else {}\n+        if isinstance(type, types.FunctionType):\n+            # Special case functions, which are *not* pushed to the `NsResolver` stack and without this special case\n+            # would only have access to the parent namespace where the `TypeAdapter` was instantiated (if the function is defined\n+            # in another module, we need to look at that module's globals).\n+            if parent_frame is not None:\n+                parent_ns = (\n+                    parent_frame.f_globals if parent_frame.f_globals is parent_frame.f_locals else parent_frame.f_locals",
      "comment": "But I think that `if globals is locals` implies that `locals` and `globals` are the same object, so\r\n\r\n`locals if globals is locals else locals`\r\nand\r\n`globals if globals is locals else locals`\r\n\r\nare equivalent?\r\n",
      "comment_id": 2405461212,
      "user": "davidhewitt",
      "created_at": "2025-10-06T09:21:13Z",
      "url": "https://github.com/pydantic/pydantic/pull/12324#discussion_r2405461212"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12324,
      "file_path": "pydantic/type_adapter.py",
      "line": 225,
      "side": "RIGHT",
      "diff_hunk": "@@ -215,19 +216,36 @@ def __init__(\n         self.pydantic_complete = False\n \n         parent_frame = self._fetch_parent_frame()\n-        if parent_frame is not None:\n-            globalns = parent_frame.f_globals\n-            # Do not provide a local ns if the type adapter happens to be instantiated at the module level:\n-            localns = parent_frame.f_locals if parent_frame.f_locals is not globalns else {}\n+        if isinstance(type, types.FunctionType):\n+            # Special case functions, which are *not* pushed to the `NsResolver` stack and without this special case\n+            # would only have access to the parent namespace where the `TypeAdapter` was instantiated (if the function is defined\n+            # in another module, we need to look at that module's globals).\n+            if parent_frame is not None:\n+                parent_ns = (\n+                    parent_frame.f_globals if parent_frame.f_globals is parent_frame.f_locals else parent_frame.f_locals",
      "comment": "Ah yeah right, I'll use locals but add a comment to explain what it is referring to ~~(also update the same code later in this branch)~~.",
      "comment_id": 2405470178,
      "user": "Viicos",
      "created_at": "2025-10-06T09:24:40Z",
      "url": "https://github.com/pydantic/pydantic/pull/12324#discussion_r2405470178"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12254,
      "file_path": "pydantic/mypy.py",
      "line": 213,
      "side": "RIGHT",
      "diff_hunk": "@@ -209,14 +209,16 @@ def _pydantic_field_callback(self, ctx: FunctionContext) -> 'Type':\n                     default_factory_type = default_factory_type.items()[0]  # type: ignore[operator]\n \n             if isinstance(default_factory_type, CallableType):\n-                ret_type = default_factory_type.ret_type\n+                ret_type = get_proper_type(default_factory_type.ret_type)\n                 # mypy doesn't think `ret_type` has `args`, you'd think mypy should know,",
      "comment": "This comment is now obsolete (and has never been correct, `ret_type` isn't guaranteed to be an Instance)",
      "comment_id": 2353155864,
      "user": "sterliakov",
      "created_at": "2025-09-16T17:15:10Z",
      "url": "https://github.com/pydantic/pydantic/pull/12254#discussion_r2353155864"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12289,
      "file_path": "pydantic/_internal/_known_annotated_metadata.py",
      "line": 302,
      "side": "RIGHT",
      "diff_hunk": "@@ -296,25 +297,28 @@ def _apply_constraint_with_incompatibility_info(\n             )\n             continue\n         elif isinstance(annotation, (at.Predicate, at.Not)):\n-            predicate_name = f'{annotation.func.__qualname__}' if hasattr(annotation.func, '__qualname__') else ''\n+            predicate_name = f'{annotation.func.__qualname__!r} ' if hasattr(annotation.func, '__qualname__') else ''\n \n             def val_func(v: Any) -> Any:",
      "comment": "For performance we could consider formatting the error message before the `def val_func`, so it's reused on each error. Or could have it be cached in some way.\n\nSimilarly might want to have the `at.Predicate` / `at.Not` branches define two separate `val_func`?\n\n",
      "comment_id": 2401609542,
      "user": "davidhewitt",
      "created_at": "2025-10-03T11:40:58Z",
      "url": "https://github.com/pydantic/pydantic/pull/12289#discussion_r2401609542"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12077,
      "file_path": "tests/test_dataclasses.py",
      "line": 1844,
      "side": "RIGHT",
      "diff_hunk": "@@ -1841,6 +1841,20 @@ class Child(Parent):\n     assert child.y == 1\n \n \n+def test_kw_only_inheritance_on_field() -> None:",
      "comment": "This would previously fail with an unhandled exception with 3.9. Although it doesn't make sense to use it in 3.9 (and we could add a warning -- not worth the effort as we'll drop support for it soon), still better to not hard error.",
      "comment_id": 2219806416,
      "user": "Viicos",
      "created_at": "2025-07-21T17:21:16Z",
      "url": "https://github.com/pydantic/pydantic/pull/12077#discussion_r2219806416"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12179,
      "file_path": "pydantic/_internal/_generate_schema.py",
      "line": 1493,
      "side": "RIGHT",
      "diff_hunk": "@@ -1474,13 +1479,44 @@ def _typed_dict_schema(self, typed_dict_cls: Any, origin: Any) -> core_schema.Co\n                         UserWarning,\n                     )\n \n+                extra_behavior: core_schema.ExtraBehavior = 'ignore'\n+                extras_schema: CoreSchema | None = None  # For 'allow', equivalent to `Any` - no validation performed.\n+\n+                # `__closed__` is `None` when not specified (equivalent to `False`):\n+                is_closed = bool(getattr(typed_dict_cls, '__closed__', False))\n+                extra_items = getattr(typed_dict_cls, '__extra_items__', typing_extensions.NoExtraItems)\n+                if is_closed:\n+                    extra_behavior = 'forbid'\n+                    extras_schema = None\n+                elif not typing_objects.is_noextraitems(extra_items):\n+                    extra_behavior = 'allow'\n+                    extras_schema = self.generate_schema(replace_types(extra_items, typevars_map))",
      "comment": "Under what conditions does `extra_behavior` remain `'ignore'`?",
      "comment_id": 2394077681,
      "user": "davidhewitt",
      "created_at": "2025-10-01T10:12:38Z",
      "url": "https://github.com/pydantic/pydantic/pull/12179#discussion_r2394077681"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12179,
      "file_path": "pydantic/_internal/_generate_schema.py",
      "line": 1493,
      "side": "RIGHT",
      "diff_hunk": "@@ -1474,13 +1479,44 @@ def _typed_dict_schema(self, typed_dict_cls: Any, origin: Any) -> core_schema.Co\n                         UserWarning,\n                     )\n \n+                extra_behavior: core_schema.ExtraBehavior = 'ignore'\n+                extras_schema: CoreSchema | None = None  # For 'allow', equivalent to `Any` - no validation performed.\n+\n+                # `__closed__` is `None` when not specified (equivalent to `False`):\n+                is_closed = bool(getattr(typed_dict_cls, '__closed__', False))\n+                extra_items = getattr(typed_dict_cls, '__extra_items__', typing_extensions.NoExtraItems)\n+                if is_closed:\n+                    extra_behavior = 'forbid'\n+                    extras_schema = None\n+                elif not typing_objects.is_noextraitems(extra_items):\n+                    extra_behavior = 'allow'\n+                    extras_schema = self.generate_schema(replace_types(extra_items, typevars_map))",
      "comment": "It is first assigned the value of `'ignore'`, so if:\r\n- the TD is not closed\r\n- the TD has no extra items\r\n- the extra from the config is not `'allow'` or `'forbid'`",
      "comment_id": 2394130478,
      "user": "Viicos",
      "created_at": "2025-10-01T10:37:02Z",
      "url": "https://github.com/pydantic/pydantic/pull/12179#discussion_r2394130478"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12179,
      "file_path": "tests/test_types_typeddict.py",
      "line": 971,
      "side": "RIGHT",
      "diff_hunk": "@@ -957,3 +958,111 @@ class Foo(TypedDict):\n     assert ta.dump_json(Foo(foo='bar', bar=1)).decode('utf-8') == '{\"bar\":1}'\n     assert ta.dump_json(Foo(foo='bar', bar=1), exclude={'bar'}).decode('utf-8') == '{}'\n     assert ta.dump_json(Foo(foo='bar', bar=2)).decode('utf-8') == '{}'\n+\n+\n+def test_typeddict_extra_allow_serialization() -> None:\n+    \"\"\"https://github.com/pydantic/pydantic/issues/11136.\n+\n+    Seems like specifying `extra_behavior` in the core schema (which was done when implementing PEP 728)\n+    was necessary to make this work.\n+    \"\"\"\n+\n+    @with_config(extra='allow')\n+    class TD(TypedDict, closed=False):",
      "comment": "Answering in https://github.com/pydantic/pydantic/issues/10785.",
      "comment_id": 2394135784,
      "user": "Viicos",
      "created_at": "2025-10-01T10:39:33Z",
      "url": "https://github.com/pydantic/pydantic/pull/12179#discussion_r2394135784"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12290,
      "file_path": "tests/mypy/modules/frozen_field.py",
      "line": 17,
      "side": "RIGHT",
      "diff_hunk": "@@ -8,3 +8,16 @@ class Foo(BaseModel):\n foo = Foo()\n \n foo.a = 2\n+\n+\n+class Parent(BaseModel):\n+    parent_attr: str = Field(exclude=True)\n+\n+\n+# We don't wan't to froze `parent_attr` in the plugin:",
      "comment": "See the fixed issue, it results in unexpected errors.",
      "comment_id": 2387753994,
      "user": "Viicos",
      "created_at": "2025-09-29T12:24:40Z",
      "url": "https://github.com/pydantic/pydantic/pull/12290#discussion_r2387753994"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12290,
      "file_path": "tests/mypy/modules/frozen_field.py",
      "line": 17,
      "side": "RIGHT",
      "diff_hunk": "@@ -8,3 +8,16 @@ class Foo(BaseModel):\n foo = Foo()\n \n foo.a = 2\n+\n+\n+class Parent(BaseModel):\n+    parent_attr: str = Field(exclude=True)\n+\n+\n+# We don't wan't to froze `parent_attr` in the plugin:",
      "comment": "I see, maybe this is clearer:\n\n```suggestion\n# `parent_attr` is writable, mypy should error when overriding with a read-only property\n```",
      "comment_id": 2390980473,
      "user": "davidhewitt",
      "created_at": "2025-09-30T11:17:30Z",
      "url": "https://github.com/pydantic/pydantic/pull/12290#discussion_r2390980473"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12290,
      "file_path": "tests/mypy/modules/frozen_field.py",
      "line": 17,
      "side": "RIGHT",
      "diff_hunk": "@@ -8,3 +8,16 @@ class Foo(BaseModel):\n foo = Foo()\n \n foo.a = 2\n+\n+\n+class Parent(BaseModel):\n+    parent_attr: str = Field(exclude=True)\n+\n+\n+# We don't want to freeze `parent_attr` in the plugin:",
      "comment": "```suggestion\r\n# `parent_attr` is writable, mypy should error when overriding with a read-only property\r\n```",
      "comment_id": 2391286202,
      "user": "Viicos",
      "created_at": "2025-09-30T12:42:50Z",
      "url": "https://github.com/pydantic/pydantic/pull/12290#discussion_r2391286202"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12290,
      "file_path": "tests/mypy/outputs/mypy-plugin_ini/frozen_field.py",
      "line": 18,
      "side": "RIGHT",
      "diff_hunk": "@@ -9,3 +9,18 @@ class Foo(BaseModel):\n \n foo.a = 2\n # MYPY: error: Property \"a\" defined in \"Foo\" is read-only  [misc]\n+\n+\n+class Parent(BaseModel):\n+    parent_attr: str = Field(exclude=True)\n+\n+\n+# We don't want to freeze `parent_attr` in the plugin:",
      "comment": "```suggestion\r\n# `parent_attr` is writable, mypy should error when overriding with a read-only property\r\n```",
      "comment_id": 2391286422,
      "user": "Viicos",
      "created_at": "2025-09-30T12:42:57Z",
      "url": "https://github.com/pydantic/pydantic/pull/12290#discussion_r2391286422"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 7196,
      "file_path": "tests/test_abc.py",
      "line": 14,
      "side": "RIGHT",
      "diff_hunk": "@@ -10,6 +11,7 @@ class Model(BaseModel, abc.ABC):\n         some_field: str\n \n \n+@pytest.mark.skipif(sys.version_info < (3, 12), reason='error value different on older versions')",
      "comment": "I'd rather don't throw out tests for everything except an unreleased Python version.",
      "comment_id": 1331429375,
      "user": "lig",
      "created_at": "2023-09-20T10:40:05Z",
      "url": "https://github.com/pydantic/pydantic/pull/7196#discussion_r1331429375"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 9953,
      "file_path": "pydantic/_internal/_config.py",
      "line": 173,
      "side": "RIGHT",
      "diff_hunk": "@@ -166,37 +166,38 @@ def core_config(self, obj: Any) -> core_schema.CoreConfig:\n         Returns:\n             A `CoreConfig` object created from config.\n         \"\"\"\n-\n-        def dict_not_none(**kwargs: Any) -> Any:\n-            return {k: v for k, v in kwargs.items() if v is not None}\n-\n-        core_config = core_schema.CoreConfig(\n-            **dict_not_none(\n-                title=self.config_dict.get('title') or (obj and obj.__name__),\n-                extra_fields_behavior=self.config_dict.get('extra'),\n-                allow_inf_nan=self.config_dict.get('allow_inf_nan'),\n-                populate_by_name=self.config_dict.get('populate_by_name'),\n-                str_strip_whitespace=self.config_dict.get('str_strip_whitespace'),\n-                str_to_lower=self.config_dict.get('str_to_lower'),\n-                str_to_upper=self.config_dict.get('str_to_upper'),\n-                strict=self.config_dict.get('strict'),\n-                ser_json_timedelta=self.config_dict.get('ser_json_timedelta'),\n-                ser_json_bytes=self.config_dict.get('ser_json_bytes'),\n-                ser_json_inf_nan=self.config_dict.get('ser_json_inf_nan'),\n-                from_attributes=self.config_dict.get('from_attributes'),\n-                loc_by_alias=self.config_dict.get('loc_by_alias'),\n-                revalidate_instances=self.config_dict.get('revalidate_instances'),\n-                validate_default=self.config_dict.get('validate_default'),\n-                str_max_length=self.config_dict.get('str_max_length'),\n-                str_min_length=self.config_dict.get('str_min_length'),\n-                hide_input_in_errors=self.config_dict.get('hide_input_in_errors'),\n-                coerce_numbers_to_str=self.config_dict.get('coerce_numbers_to_str'),\n-                regex_engine=self.config_dict.get('regex_engine'),\n-                validation_error_cause=self.config_dict.get('validation_error_cause'),\n-                cache_strings=self.config_dict.get('cache_strings'),\n-            )\n-        )\n-        return core_config\n+        config = self.config_dict\n+        title = config.get('title') or (obj and obj.__name__)\n+\n+        core_config_values = {\n+            'title': title,",
      "comment": "```suggestion\r\n\r\n        core_config_values = {\r\n            'title': config.get('title') or (obj and obj.__name__),\r\n```",
      "comment_id": 1689989522,
      "user": "sydney-runkle",
      "created_at": "2024-07-24T15:07:08Z",
      "url": "https://github.com/pydantic/pydantic/pull/9953#discussion_r1689989522"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 9953,
      "file_path": "pydantic/_internal/_config.py",
      "line": 200,
      "side": "RIGHT",
      "diff_hunk": "@@ -166,37 +166,38 @@ def core_config(self, obj: Any) -> core_schema.CoreConfig:\n         Returns:\n             A `CoreConfig` object created from config.\n         \"\"\"\n-\n-        def dict_not_none(**kwargs: Any) -> Any:\n-            return {k: v for k, v in kwargs.items() if v is not None}\n-\n-        core_config = core_schema.CoreConfig(\n-            **dict_not_none(\n-                title=self.config_dict.get('title') or (obj and obj.__name__),\n-                extra_fields_behavior=self.config_dict.get('extra'),\n-                allow_inf_nan=self.config_dict.get('allow_inf_nan'),\n-                populate_by_name=self.config_dict.get('populate_by_name'),\n-                str_strip_whitespace=self.config_dict.get('str_strip_whitespace'),\n-                str_to_lower=self.config_dict.get('str_to_lower'),\n-                str_to_upper=self.config_dict.get('str_to_upper'),\n-                strict=self.config_dict.get('strict'),\n-                ser_json_timedelta=self.config_dict.get('ser_json_timedelta'),\n-                ser_json_bytes=self.config_dict.get('ser_json_bytes'),\n-                ser_json_inf_nan=self.config_dict.get('ser_json_inf_nan'),\n-                from_attributes=self.config_dict.get('from_attributes'),\n-                loc_by_alias=self.config_dict.get('loc_by_alias'),\n-                revalidate_instances=self.config_dict.get('revalidate_instances'),\n-                validate_default=self.config_dict.get('validate_default'),\n-                str_max_length=self.config_dict.get('str_max_length'),\n-                str_min_length=self.config_dict.get('str_min_length'),\n-                hide_input_in_errors=self.config_dict.get('hide_input_in_errors'),\n-                coerce_numbers_to_str=self.config_dict.get('coerce_numbers_to_str'),\n-                regex_engine=self.config_dict.get('regex_engine'),\n-                validation_error_cause=self.config_dict.get('validation_error_cause'),\n-                cache_strings=self.config_dict.get('cache_strings'),\n-            )\n-        )\n-        return core_config\n+        config = self.config_dict\n+        title = config.get('title') or (obj and obj.__name__)\n+\n+        core_config_values = {\n+            'title': title,\n+            'extra_fields_behavior': config.get('extra'),\n+            'allow_inf_nan': config.get('allow_inf_nan'),\n+            'populate_by_name': config.get('populate_by_name'),\n+            'str_strip_whitespace': config.get('str_strip_whitespace'),\n+            'str_to_lower': config.get('str_to_lower'),\n+            'str_to_upper': config.get('str_to_upper'),\n+            'strict': config.get('strict'),\n+            'ser_json_timedelta': config.get('ser_json_timedelta'),\n+            'ser_json_bytes': config.get('ser_json_bytes'),\n+            'ser_json_inf_nan': config.get('ser_json_inf_nan'),\n+            'from_attributes': config.get('from_attributes'),\n+            'loc_by_alias': config.get('loc_by_alias'),\n+            'revalidate_instances': config.get('revalidate_instances'),\n+            'validate_default': config.get('validate_default'),\n+            'str_max_length': config.get('str_max_length'),\n+            'str_min_length': config.get('str_min_length'),\n+            'hide_input_in_errors': config.get('hide_input_in_errors'),\n+            'coerce_numbers_to_str': config.get('coerce_numbers_to_str'),\n+            'regex_engine': config.get('regex_engine'),\n+            'validation_error_cause': config.get('validation_error_cause'),\n+            'cache_strings': config.get('cache_strings'),\n+        }\n+\n+        # Create dictionary excluding None values\n+        non_none_config = {k: v for k, v in core_config_values.items() if v is not None}\n+\n+        return core_schema.CoreConfig(**non_none_config)",
      "comment": "```suggestion\r\n        return core_schema.CoreConfig(**{k: v for k, v in core_config_values.items() if v is not None})\r\n```",
      "comment_id": 1689990512,
      "user": "sydney-runkle",
      "created_at": "2024-07-24T15:07:46Z",
      "url": "https://github.com/pydantic/pydantic/pull/9953#discussion_r1689990512"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12173,
      "file_path": "tests/test_dataclasses.py",
      "line": 3189,
      "side": "RIGHT",
      "diff_hunk": "@@ -3152,7 +3186,7 @@ class MyDataclass2:\n     try:\n         inst.x = 'other'\n     except ValidationError as e:\n-        assert 'Instance is frozen' in repr(e)\n+        assert \"cannot assign to field 'x'\" in repr(e)",
      "comment": "Previously the error was thrown from pydantic-core. I think it's better this way, as it is now consistent with the `frozen=True` dataclass argument above this one.",
      "comment_id": 2282016535,
      "user": "Viicos",
      "created_at": "2025-08-18T10:38:12Z",
      "url": "https://github.com/pydantic/pydantic/pull/12173#discussion_r2282016535"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12219,
      "file_path": "tests/test_types.py",
      "line": 1053,
      "side": "RIGHT",
      "diff_hunk": "@@ -1050,6 +1050,18 @@ class ImportThings(BaseModel):\n     assert import_things.model_dump_json() == '{\"obj\":\"sys.stdout\"}'\n \n \n+def test_import_string_thing_with_name() -> None:",
      "comment": "```suggestion\r\ndef test_import_string_thing_with_name() -> None:\r\n    \"\"\"https://github.com/pydantic/pydantic/issues/12218\"\"\"\r\n```",
      "comment_id": 2346394555,
      "user": "Viicos",
      "created_at": "2025-09-13T11:13:26Z",
      "url": "https://github.com/pydantic/pydantic/pull/12219#discussion_r2346394555"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12219,
      "file_path": "tests/test_types.py",
      "line": 1054,
      "side": "RIGHT",
      "diff_hunk": "@@ -1050,6 +1050,19 @@ class ImportThings(BaseModel):\n     assert import_things.model_dump_json() == '{\"obj\":\"sys.stdout\"}'\n \n \n+def test_import_string_thing_with_name() -> None:\n+    \"\"\"https://github.com/pydantic/pydantic/issues/12218\"\"\"",
      "comment": "```suggestion\r\n    \"\"\"https://github.com/pydantic/pydantic/issues/12218\"\"\"\r\n\r\n```",
      "comment_id": 2346646743,
      "user": "Viicos",
      "created_at": "2025-09-13T13:38:20Z",
      "url": "https://github.com/pydantic/pydantic/pull/12219#discussion_r2346646743"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12068,
      "file_path": "pydantic/config.py",
      "line": 601,
      "side": "RIGHT",
      "diff_hunk": "@@ -594,10 +594,43 @@ class Transaction(BaseModel):\n     The format of JSON serialized timedeltas. Accepts the string values of `'iso8601'` and\n     `'float'`. Defaults to `'iso8601'`.\n \n-    - `'iso8601'` will serialize timedeltas to ISO 8601 durations.\n+    !!! warning\n+        In v2.11+ it is recommended to use the [`ser_json_temporal`][pydantic.config.ConfigDict.ser_json_temporal]\n+        setting instead of `ser_json_timedelta`. This setting will be deprecated in v3.\n+    - `'iso8601'` will serialize timedeltas to [ISO 8601 text format](https://en.wikipedia.org/wiki/ISO_8601#Durations).\n     - `'float'` will serialize timedeltas to the total number of seconds.",
      "comment": "```suggestion\r\n    - `'iso8601'` will serialize timedeltas to [ISO 8601 text format](https://en.wikipedia.org/wiki/ISO_8601#Durations).\r\n    - `'float'` will serialize timedeltas to the total number of seconds.\r\n    \r\n    !!! warning\r\n        Starting in v2.11, it is recommended to use the [`ser_json_temporal`][pydantic.config.ConfigDict.ser_json_temporal]\r\n        setting instead of `ser_json_timedelta`. This setting will be deprecated in v3.\r\n```",
      "comment_id": 2281395813,
      "user": "Viicos",
      "created_at": "2025-08-18T06:18:32Z",
      "url": "https://github.com/pydantic/pydantic/pull/12068#discussion_r2281395813"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12068,
      "file_path": "pydantic/config.py",
      "line": 631,
      "side": "RIGHT",
      "diff_hunk": "@@ -594,10 +594,43 @@ class Transaction(BaseModel):\n     The format of JSON serialized timedeltas. Accepts the string values of `'iso8601'` and\n     `'float'`. Defaults to `'iso8601'`.\n \n-    - `'iso8601'` will serialize timedeltas to ISO 8601 durations.\n+    !!! warning\n+        In v2.11+ it is recommended to use the [`ser_json_temporal`][pydantic.config.ConfigDict.ser_json_temporal]\n+        setting instead of `ser_json_timedelta`. This setting will be deprecated in v3.\n+    - `'iso8601'` will serialize timedeltas to [ISO 8601 text format](https://en.wikipedia.org/wiki/ISO_8601#Durations).\n     - `'float'` will serialize timedeltas to the total number of seconds.\n     \"\"\"\n \n+    ser_json_temporal: Literal['iso8601', 'seconds', 'milliseconds']\n+    \"\"\"\n+    The format of JSON serialized temporal types from the `datetime` library. This includes:\n+    - [`datetime.datetime`][]\n+    - [`datetime.date`][]\n+    - [`datetime.time`][]\n+    - [`datetime.timedelta`][]\n+    !!! note\n+        This setting was introduced in v2.11. It overlaps with the `ser_json_timedelta`\n+        setting which will likely be deprecated in v3. It also adds more configurability for\n+        the other temporal types.\n+    Accepts the string values of `'iso8601'`, `'milliseconds'`, and `'seconds'`. Defaults to `'iso8601'`.\n+    - `'iso8601'` will serialize date-like types to [ISO 8601 text format](https://en.wikipedia.org/wiki/ISO_8601#Durations).\n+    - `'milliseconds'` will serialize date-like types to a floating point number of milliseconds since the epoch.\n+    - `'seconds'` will serialize date-like types to a floating point number of seconds since the epoch.\n+    \"\"\"\n+\n+    val_temporal_unit: Literal['seconds', 'milliseconds', 'infer']\n+    \"\"\"\n+    The unit to assume for validating numeric input for datetime-like types. This includes:\n+    - [`datetime.datetime`][]\n+    - [`datetime.date`][]\n+    Defaults to `'infer'`.\n+    The \"epoch\" references below refer to the Unix epoch, which is 1970-01-01 00:00:00 UTC.\n+    - `'seconds'` will validate date or time numeric inputs as seconds since the epoch.\n+    - `'milliseconds'` will validate date or time numeric inputs as milliseconds since the epoch.\n+    - `'infer'` will infer the unit from the string numeric input on unix time:\n+        i.e. seconds (if >= -2^10 and <= 2^10) or milliseconds (if < -2^10or > 2^10) since the epoch.",
      "comment": "Let's add the necessary [configuration for MathJax](https://squidfunk.github.io/mkdocs-material/reference/math/#mathjax) and use it:\r\n\r\n```suggestion\r\n    The unit to assume for validating numeric input for datetime-like types ([`datetime.datetime`][] and [`datetime.date`][]). Can be one of:\r\n\r\n    - `'seconds'` will validate date or time numeric inputs as seconds since the [epoch].\r\n    - `'milliseconds'` will validate date or time numeric inputs as milliseconds since the [epoch].\r\n    - `'infer'` will infer the unit from the string numeric input on unix time as:\r\n\r\n        * seconds since the [epoch] if $-2^{10} <= v <= 2^{10}$\r\n        * milliseconds since the [epoch] (if $v < -2^{10}$ or $v > 2^{10}$).\r\n\r\n    Defaults to `'infer'`.\r\n\r\n    [epoch]: https://en.wikipedia.org/wiki/Unix_time\r\n```",
      "comment_id": 2281435837,
      "user": "Viicos",
      "created_at": "2025-08-18T06:42:23Z",
      "url": "https://github.com/pydantic/pydantic/pull/12068#discussion_r2281435837"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12068,
      "file_path": "pydantic/config.py",
      "line": 618,
      "side": "RIGHT",
      "diff_hunk": "@@ -594,10 +594,43 @@ class Transaction(BaseModel):\n     The format of JSON serialized timedeltas. Accepts the string values of `'iso8601'` and\n     `'float'`. Defaults to `'iso8601'`.\n \n-    - `'iso8601'` will serialize timedeltas to ISO 8601 durations.\n+    !!! warning\n+        In v2.11+ it is recommended to use the [`ser_json_temporal`][pydantic.config.ConfigDict.ser_json_temporal]\n+        setting instead of `ser_json_timedelta`. This setting will be deprecated in v3.\n+    - `'iso8601'` will serialize timedeltas to [ISO 8601 text format](https://en.wikipedia.org/wiki/ISO_8601#Durations).\n     - `'float'` will serialize timedeltas to the total number of seconds.\n     \"\"\"\n \n+    ser_json_temporal: Literal['iso8601', 'seconds', 'milliseconds']\n+    \"\"\"\n+    The format of JSON serialized temporal types from the `datetime` library. This includes:\n+    - [`datetime.datetime`][]\n+    - [`datetime.date`][]\n+    - [`datetime.time`][]\n+    - [`datetime.timedelta`][]\n+    !!! note\n+        This setting was introduced in v2.11. It overlaps with the `ser_json_timedelta`\n+        setting which will likely be deprecated in v3. It also adds more configurability for\n+        the other temporal types.\n+    Accepts the string values of `'iso8601'`, `'milliseconds'`, and `'seconds'`. Defaults to `'iso8601'`.\n+    - `'iso8601'` will serialize date-like types to [ISO 8601 text format](https://en.wikipedia.org/wiki/ISO_8601#Durations).\n+    - `'milliseconds'` will serialize date-like types to a floating point number of milliseconds since the epoch.\n+    - `'seconds'` will serialize date-like types to a floating point number of seconds since the epoch.",
      "comment": "```suggestion\r\n    The format of JSON serialized temporal types from the [`datetime`][] module. This includes:\r\n\r\n    - [`datetime.datetime`][]\r\n    - [`datetime.date`][]\r\n    - [`datetime.time`][]\r\n    - [`datetime.timedelta`][]\r\n\r\n    Can be one of:\r\n\r\n    - `'iso8601'` will serialize date-like types to [ISO 8601 text format](https://en.wikipedia.org/wiki/ISO_8601#Durations).\r\n    - `'milliseconds'` will serialize date-like types to a floating point number of milliseconds since the epoch.\r\n    - `'seconds'` will serialize date-like types to a floating point number of seconds since the epoch.\r\n\r\n    Defaults to `'iso8601'`.\r\n\r\n    !!! note\r\n        This setting was introduced in v2.11. It overlaps with the [`ser_json_timedelta`][pydantic.config.ConfigDict.ser_json_timedelta]\r\n        setting which will be deprecated in v3. It also adds more configurability for\r\n        the other temporal types.\r\n```",
      "comment_id": 2281436895,
      "user": "Viicos",
      "created_at": "2025-08-18T06:43:05Z",
      "url": "https://github.com/pydantic/pydantic/pull/12068#discussion_r2281436895"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 7188,
      "file_path": "pydantic/_internal/_generate_schema.py",
      "line": 1599,
      "side": "LEFT",
      "diff_hunk": "@@ -1497,7 +1498,6 @@ def _get_prepare_pydantic_annotations_for_known_type(\n     ) -> tuple[Any, list[Any]] | None:\n         from ._std_types_schema import PREPARE_METHODS\n \n-        # This check for hashability is only necessary for python 3.7",
      "comment": "Seem we need this check for other versions as well. by removing this check some tests fail. like https://github.com/pydantic/pydantic/blob/8989f96415b0c693585fa9363eaf520d5b9a30a2/tests/test_json_schema.py#L4891",
      "comment_id": 1300047061,
      "user": "hramezani",
      "created_at": "2023-08-21T12:31:22Z",
      "url": "https://github.com/pydantic/pydantic/pull/7188#discussion_r1300047061"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12196,
      "file_path": "pydantic/version.py",
      "line": 87,
      "side": "RIGHT",
      "diff_hunk": "@@ -72,8 +77,26 @@ def version_info() -> str:\n \n def check_pydantic_core_version() -> bool:\n     \"\"\"Check that the installed `pydantic-core` dependency is compatible.\"\"\"\n-    # Keep this in sync with the version constraint in the `pyproject.toml` dependencies:\n-    return __pydantic_core_version__ == '2.37.2'\n+    return __pydantic_core_version__ == _COMPATIBLE_PYDANTIC_CORE_VERSION\n+\n+\n+def _ensure_pydantic_core_version() -> None:  # pragma: no cover\n+    if not check_pydantic_core_version():\n+        raise_error = True\n+        # Do not raise the error if pydantic is installed in editable mode (i.e. in development):\n+        if sys.version_info >= (3, 13):  # origin property added in 3.13",
      "comment": "Do we potentially want an env var to silence this for development on older Python versions?",
      "comment_id": 2303898997,
      "user": "davidhewitt",
      "created_at": "2025-08-27T13:14:39Z",
      "url": "https://github.com/pydantic/pydantic/pull/12196#discussion_r2303898997"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12196,
      "file_path": "pydantic/version.py",
      "line": 87,
      "side": "RIGHT",
      "diff_hunk": "@@ -72,8 +77,26 @@ def version_info() -> str:\n \n def check_pydantic_core_version() -> bool:\n     \"\"\"Check that the installed `pydantic-core` dependency is compatible.\"\"\"\n-    # Keep this in sync with the version constraint in the `pyproject.toml` dependencies:\n-    return __pydantic_core_version__ == '2.37.2'\n+    return __pydantic_core_version__ == _COMPATIBLE_PYDANTIC_CORE_VERSION\n+\n+\n+def _ensure_pydantic_core_version() -> None:  # pragma: no cover\n+    if not check_pydantic_core_version():\n+        raise_error = True\n+        # Do not raise the error if pydantic is installed in editable mode (i.e. in development):\n+        if sys.version_info >= (3, 13):  # origin property added in 3.13",
      "comment": "I thought about it, but I think most one-time contributors will have the version right on `uv sync`, and for us well I can only recommend using 3.13 or soon 3.14 :smile: ",
      "comment_id": 2304000700,
      "user": "Viicos",
      "created_at": "2025-08-27T13:49:21Z",
      "url": "https://github.com/pydantic/pydantic/pull/12196#discussion_r2304000700"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11942,
      "file_path": "pydantic/functional_validators.py",
      "line": 881,
      "side": "RIGHT",
      "diff_hunk": "@@ -831,3 +831,53 @@ def __get_pydantic_core_schema__(cls, source: Any, handler: GetCoreSchemaHandler\n             )\n \n         __hash__ = object.__hash__\n+\n+\n+_FromTypeT = TypeVar('_FromTypeT')\n+\n+\n+class ValidateFrom:\n+    \"\"\"A helper class to validate a custom type from a type natively supported by Pydantic.\n+\n+    Args:\n+        from_type: The type natively supported by Pydantic to use to perform validation.\n+        instantiation_hook: A callable taking the validated type as an argument, and returning\n+            the populated custom type.\n+\n+    Example:\n+        ```python {lint=\"skip\"}\n+        from typing import Annotated\n+\n+        from pydantic import BaseModel, TypeAdapter, ValidateFrom\n+\n+        class MyCls:\n+            def __init__(self, a: int) -> None:\n+                self.a = a\n+\n+            def __repr__(self) -> str:\n+                return f\"MyCls(a={self.a})\"\n+\n+        class Model(BaseModel):\n+            a: int\n+\n+\n+        ta = TypeAdapter(\n+            Annotated[MyCls, ValidateFrom(Model, instantiation_hook=lambda v: MyCls(a=v.a))]\n+        )\n+\n+        print(ta.validate_python({'a': 1}))\n+        #> MyCls(a=1)\n+        ```\n+    \"\"\"\n+\n+    # TODO: make use of PEP 747\n+    def __init__(self, from_type: type[_FromTypeT], /, *, instantiation_hook: Callable[[_FromTypeT], Any]) -> None:\n+        self.from_type = from_type\n+        self.instantiation_hook = instantiation_hook\n+\n+    def __get_pydantic_core_schema__(self, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n+        schema = handler(self.from_type)\n+        return core_schema.no_info_after_validator_function(\n+            lambda value: self.instantiation_hook(value),",
      "comment": "Would this work?\r\n\r\n```suggestion\r\n            self.instantiation_hook,\r\n```",
      "comment_id": 2278964398,
      "user": "DouweM",
      "created_at": "2025-08-15T13:10:49Z",
      "url": "https://github.com/pydantic/pydantic/pull/11942#discussion_r2278964398"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11942,
      "file_path": "pydantic/functional_validators.py",
      "line": 881,
      "side": "RIGHT",
      "diff_hunk": "@@ -831,3 +831,53 @@ def __get_pydantic_core_schema__(cls, source: Any, handler: GetCoreSchemaHandler\n             )\n \n         __hash__ = object.__hash__\n+\n+\n+_FromTypeT = TypeVar('_FromTypeT')\n+\n+\n+class ValidateFrom:\n+    \"\"\"A helper class to validate a custom type from a type natively supported by Pydantic.\n+\n+    Args:\n+        from_type: The type natively supported by Pydantic to use to perform validation.\n+        instantiation_hook: A callable taking the validated type as an argument, and returning\n+            the populated custom type.\n+\n+    Example:\n+        ```python {lint=\"skip\"}\n+        from typing import Annotated\n+\n+        from pydantic import BaseModel, TypeAdapter, ValidateFrom\n+\n+        class MyCls:\n+            def __init__(self, a: int) -> None:\n+                self.a = a\n+\n+            def __repr__(self) -> str:\n+                return f\"MyCls(a={self.a})\"\n+\n+        class Model(BaseModel):\n+            a: int\n+\n+\n+        ta = TypeAdapter(\n+            Annotated[MyCls, ValidateFrom(Model, instantiation_hook=lambda v: MyCls(a=v.a))]\n+        )\n+\n+        print(ta.validate_python({'a': 1}))\n+        #> MyCls(a=1)\n+        ```\n+    \"\"\"\n+\n+    # TODO: make use of PEP 747\n+    def __init__(self, from_type: type[_FromTypeT], /, *, instantiation_hook: Callable[[_FromTypeT], Any]) -> None:\n+        self.from_type = from_type\n+        self.instantiation_hook = instantiation_hook\n+\n+    def __get_pydantic_core_schema__(self, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n+        schema = handler(self.from_type)\n+        return core_schema.no_info_after_validator_function(\n+            lambda value: self.instantiation_hook(value),",
      "comment": "> > It can currently be anything (supported by Pydantic: models, dataclasses, typeddicts, etc), but we will have to constrain it to some specific types (perhaps only supporting Pydantic models?) as we will most likely have to assume `__dict__` is present to fetch the instance attributes and pass them to `__new__()`.\r\n\r\nCould `TypeAdapter(from_type).dump_python(value)` work instead of `__dict__`?",
      "comment_id": 2278971975,
      "user": "DouweM",
      "created_at": "2025-08-15T13:16:08Z",
      "url": "https://github.com/pydantic/pydantic/pull/11942#discussion_r2278971975"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11942,
      "file_path": "pydantic/functional_validators.py",
      "line": 874,
      "side": "RIGHT",
      "diff_hunk": "@@ -831,3 +831,53 @@ def __get_pydantic_core_schema__(cls, source: Any, handler: GetCoreSchemaHandler\n             )\n \n         __hash__ = object.__hash__\n+\n+\n+_FromTypeT = TypeVar('_FromTypeT')\n+\n+\n+class ValidateFrom:\n+    \"\"\"A helper class to validate a custom type from a type natively supported by Pydantic.\n+\n+    Args:\n+        from_type: The type natively supported by Pydantic to use to perform validation.\n+        instantiation_hook: A callable taking the validated type as an argument, and returning\n+            the populated custom type.\n+\n+    Example:\n+        ```python {lint=\"skip\"}\n+        from typing import Annotated\n+\n+        from pydantic import BaseModel, TypeAdapter, ValidateFrom\n+\n+        class MyCls:\n+            def __init__(self, a: int) -> None:\n+                self.a = a\n+\n+            def __repr__(self) -> str:\n+                return f\"MyCls(a={self.a})\"\n+\n+        class Model(BaseModel):\n+            a: int\n+\n+\n+        ta = TypeAdapter(\n+            Annotated[MyCls, ValidateFrom(Model, instantiation_hook=lambda v: MyCls(a=v.a))]\n+        )\n+\n+        print(ta.validate_python({'a': 1}))\n+        #> MyCls(a=1)\n+        ```\n+    \"\"\"\n+\n+    # TODO: make use of PEP 747\n+    def __init__(self, from_type: type[_FromTypeT], /, *, instantiation_hook: Callable[[_FromTypeT], Any]) -> None:",
      "comment": "`instantiation_hook` is pretty long, does it have to be a kwarg? `ValidateFrom(Model, lambda v: MyCls(a=v.a))` is pretty clear ",
      "comment_id": 2278973814,
      "user": "DouweM",
      "created_at": "2025-08-15T13:17:29Z",
      "url": "https://github.com/pydantic/pydantic/pull/11942#discussion_r2278973814"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11942,
      "file_path": "pydantic/functional_validators.py",
      "line": 881,
      "side": "RIGHT",
      "diff_hunk": "@@ -831,3 +831,53 @@ def __get_pydantic_core_schema__(cls, source: Any, handler: GetCoreSchemaHandler\n             )\n \n         __hash__ = object.__hash__\n+\n+\n+_FromTypeT = TypeVar('_FromTypeT')\n+\n+\n+class ValidateFrom:\n+    \"\"\"A helper class to validate a custom type from a type natively supported by Pydantic.\n+\n+    Args:\n+        from_type: The type natively supported by Pydantic to use to perform validation.\n+        instantiation_hook: A callable taking the validated type as an argument, and returning\n+            the populated custom type.\n+\n+    Example:\n+        ```python {lint=\"skip\"}\n+        from typing import Annotated\n+\n+        from pydantic import BaseModel, TypeAdapter, ValidateFrom\n+\n+        class MyCls:\n+            def __init__(self, a: int) -> None:\n+                self.a = a\n+\n+            def __repr__(self) -> str:\n+                return f\"MyCls(a={self.a})\"\n+\n+        class Model(BaseModel):\n+            a: int\n+\n+\n+        ta = TypeAdapter(\n+            Annotated[MyCls, ValidateFrom(Model, instantiation_hook=lambda v: MyCls(a=v.a))]\n+        )\n+\n+        print(ta.validate_python({'a': 1}))\n+        #> MyCls(a=1)\n+        ```\n+    \"\"\"\n+\n+    # TODO: make use of PEP 747\n+    def __init__(self, from_type: type[_FromTypeT], /, *, instantiation_hook: Callable[[_FromTypeT], Any]) -> None:\n+        self.from_type = from_type\n+        self.instantiation_hook = instantiation_hook\n+\n+    def __get_pydantic_core_schema__(self, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n+        schema = handler(self.from_type)\n+        return core_schema.no_info_after_validator_function(\n+            lambda value: self.instantiation_hook(value),",
      "comment": "> Would this work?\r\n\r\nNice catch, updated.\r\n\r\n> Could `TypeAdapter(from_type).dump_python(value)` work instead of `__dict__`?\r\n\r\nWell even this way, if `from_type` happens to be e.g. a list we can't make assumptions on the dumped value, and how to use it to populate the final custom type.",
      "comment_id": 2279216054,
      "user": "Viicos",
      "created_at": "2025-08-15T15:19:23Z",
      "url": "https://github.com/pydantic/pydantic/pull/11942#discussion_r2279216054"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11942,
      "file_path": "pydantic/functional_validators.py",
      "line": 874,
      "side": "RIGHT",
      "diff_hunk": "@@ -831,3 +831,53 @@ def __get_pydantic_core_schema__(cls, source: Any, handler: GetCoreSchemaHandler\n             )\n \n         __hash__ = object.__hash__\n+\n+\n+_FromTypeT = TypeVar('_FromTypeT')\n+\n+\n+class ValidateFrom:\n+    \"\"\"A helper class to validate a custom type from a type natively supported by Pydantic.\n+\n+    Args:\n+        from_type: The type natively supported by Pydantic to use to perform validation.\n+        instantiation_hook: A callable taking the validated type as an argument, and returning\n+            the populated custom type.\n+\n+    Example:\n+        ```python {lint=\"skip\"}\n+        from typing import Annotated\n+\n+        from pydantic import BaseModel, TypeAdapter, ValidateFrom\n+\n+        class MyCls:\n+            def __init__(self, a: int) -> None:\n+                self.a = a\n+\n+            def __repr__(self) -> str:\n+                return f\"MyCls(a={self.a})\"\n+\n+        class Model(BaseModel):\n+            a: int\n+\n+\n+        ta = TypeAdapter(\n+            Annotated[MyCls, ValidateFrom(Model, instantiation_hook=lambda v: MyCls(a=v.a))]\n+        )\n+\n+        print(ta.validate_python({'a': 1}))\n+        #> MyCls(a=1)\n+        ```\n+    \"\"\"\n+\n+    # TODO: make use of PEP 747\n+    def __init__(self, from_type: type[_FromTypeT], /, *, instantiation_hook: Callable[[_FromTypeT], Any]) -> None:",
      "comment": "I'm a bit worried that if users end up using the positional form, it won't be obvious to know what `ValidateFrom` is doing:\r\n\r\n```python\r\nclass Model(BaseModel):\r\n    a: Annotated[MyType, ValidateFrom(SomeModel, lambda v: ...)]\r\n```\r\n\r\nbut if I don't manage to find a shorter name, I'll make it positional or kw.",
      "comment_id": 2279226566,
      "user": "Viicos",
      "created_at": "2025-08-15T15:21:04Z",
      "url": "https://github.com/pydantic/pydantic/pull/11942#discussion_r2279226566"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11942,
      "file_path": "pydantic/functional_validators.py",
      "line": 881,
      "side": "RIGHT",
      "diff_hunk": "@@ -831,3 +831,53 @@ def __get_pydantic_core_schema__(cls, source: Any, handler: GetCoreSchemaHandler\n             )\n \n         __hash__ = object.__hash__\n+\n+\n+_FromTypeT = TypeVar('_FromTypeT')\n+\n+\n+class ValidateFrom:\n+    \"\"\"A helper class to validate a custom type from a type natively supported by Pydantic.\n+\n+    Args:\n+        from_type: The type natively supported by Pydantic to use to perform validation.\n+        instantiation_hook: A callable taking the validated type as an argument, and returning\n+            the populated custom type.\n+\n+    Example:\n+        ```python {lint=\"skip\"}\n+        from typing import Annotated\n+\n+        from pydantic import BaseModel, TypeAdapter, ValidateFrom\n+\n+        class MyCls:\n+            def __init__(self, a: int) -> None:\n+                self.a = a\n+\n+            def __repr__(self) -> str:\n+                return f\"MyCls(a={self.a})\"\n+\n+        class Model(BaseModel):\n+            a: int\n+\n+\n+        ta = TypeAdapter(\n+            Annotated[MyCls, ValidateFrom(Model, instantiation_hook=lambda v: MyCls(a=v.a))]\n+        )\n+\n+        print(ta.validate_python({'a': 1}))\n+        #> MyCls(a=1)\n+        ```\n+    \"\"\"\n+\n+    # TODO: make use of PEP 747\n+    def __init__(self, from_type: type[_FromTypeT], /, *, instantiation_hook: Callable[[_FromTypeT], Any]) -> None:\n+        self.from_type = from_type\n+        self.instantiation_hook = instantiation_hook\n+\n+    def __get_pydantic_core_schema__(self, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n+        schema = handler(self.from_type)\n+        return core_schema.no_info_after_validator_function(\n+            lambda value: self.instantiation_hook(value),",
      "comment": "@Viicos Right, then keeping it like this at least to start makes sense",
      "comment_id": 2279274658,
      "user": "DouweM",
      "created_at": "2025-08-15T15:31:01Z",
      "url": "https://github.com/pydantic/pydantic/pull/11942#discussion_r2279274658"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11942,
      "file_path": "pydantic/functional_validators.py",
      "line": 874,
      "side": "RIGHT",
      "diff_hunk": "@@ -831,3 +831,53 @@ def __get_pydantic_core_schema__(cls, source: Any, handler: GetCoreSchemaHandler\n             )\n \n         __hash__ = object.__hash__\n+\n+\n+_FromTypeT = TypeVar('_FromTypeT')\n+\n+\n+class ValidateFrom:\n+    \"\"\"A helper class to validate a custom type from a type natively supported by Pydantic.\n+\n+    Args:\n+        from_type: The type natively supported by Pydantic to use to perform validation.\n+        instantiation_hook: A callable taking the validated type as an argument, and returning\n+            the populated custom type.\n+\n+    Example:\n+        ```python {lint=\"skip\"}\n+        from typing import Annotated\n+\n+        from pydantic import BaseModel, TypeAdapter, ValidateFrom\n+\n+        class MyCls:\n+            def __init__(self, a: int) -> None:\n+                self.a = a\n+\n+            def __repr__(self) -> str:\n+                return f\"MyCls(a={self.a})\"\n+\n+        class Model(BaseModel):\n+            a: int\n+\n+\n+        ta = TypeAdapter(\n+            Annotated[MyCls, ValidateFrom(Model, instantiation_hook=lambda v: MyCls(a=v.a))]\n+        )\n+\n+        print(ta.validate_python({'a': 1}))\n+        #> MyCls(a=1)\n+        ```\n+    \"\"\"\n+\n+    # TODO: make use of PEP 747\n+    def __init__(self, from_type: type[_FromTypeT], /, *, instantiation_hook: Callable[[_FromTypeT], Any]) -> None:",
      "comment": "I think `Annotated[MyType, ValidateAs(SomeModel, lambda v: ...)]` implies pretty well that the input will be validated as a `SomeModel`, and then you're supposed to turn a `SomeModel` into a `MyType`. `ValidateAs` implies that a bit more clearly to me than `ValidateFrom`",
      "comment_id": 2279282215,
      "user": "DouweM",
      "created_at": "2025-08-15T15:32:37Z",
      "url": "https://github.com/pydantic/pydantic/pull/11942#discussion_r2279282215"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12002,
      "file_path": "pydantic/_internal/_generate_schema.py",
      "line": 622,
      "side": "RIGHT",
      "diff_hunk": "@@ -619,7 +619,7 @@ def _arbitrary_type_schema(self, tp: Any) -> CoreSchema:\n                 ' Pydantic will allow any object with no validation since we cannot even'\n                 ' enforce that the input is an instance of the given type.'\n                 ' To get rid of this error wrap the type with `pydantic.SkipValidation`.',\n-                UserWarning,\n+                PydanticSkipValidationWarning,  # Updated warning type,",
      "comment": "```suggestion\r\n                PydanticArbitraryTypeWarning,\r\n```",
      "comment_id": 2161109248,
      "user": "Viicos",
      "created_at": "2025-06-23T09:10:17Z",
      "url": "https://github.com/pydantic/pydantic/pull/12002#discussion_r2161109248"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12002,
      "file_path": "pydantic/functional_validators.py",
      "line": 831,
      "side": "RIGHT",
      "diff_hunk": "@@ -817,13 +819,15 @@ def __class_getitem__(cls, item: Any) -> Any:\n \n         @classmethod\n         def __get_pydantic_core_schema__(cls, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n-            original_schema = handler(source)\n-            metadata = {'pydantic_js_annotation_functions': [lambda _c, h: h(original_schema)]}\n-            return core_schema.any_schema(\n-                metadata=metadata,\n-                serialization=core_schema.wrap_serializer_function_ser_schema(\n-                    function=lambda v, h: h(v), schema=original_schema\n-                ),\n-            )\n+            with warnings.catch_warnings():\n+                warnings.simplefilter('ignore', PydanticSkipValidationWarning)\n+                original_schema = handler(source)\n+                metadata = {'pydantic_js_annotation_functions': [lambda _c, h: h(original_schema)]}\n+                return core_schema.any_schema(\n+                    metadata=metadata,\n+                    serialization=core_schema.wrap_serializer_function_ser_schema(\n+                        function=lambda v, h: h(v), schema=original_schema\n+                    ),\n+                )",
      "comment": "```suggestion\r\n            metadata = {'pydantic_js_annotation_functions': [lambda _c, h: h(original_schema)]}\r\n            return core_schema.any_schema(\r\n                metadata=metadata,\r\n                serialization=core_schema.wrap_serializer_function_ser_schema(\r\n                    function=lambda v, h: h(v), schema=original_schema\r\n                ),\r\n            )\r\n```",
      "comment_id": 2161110729,
      "user": "Viicos",
      "created_at": "2025-06-23T09:11:02Z",
      "url": "https://github.com/pydantic/pydantic/pull/12002#discussion_r2161110729"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12002,
      "file_path": "pydantic/_internal/_generate_schema.py",
      "line": 65,
      "side": "RIGHT",
      "diff_hunk": "@@ -62,7 +62,7 @@\n from ..functional_validators import AfterValidator, BeforeValidator, FieldValidatorModes, PlainValidator, WrapValidator\n from ..json_schema import JsonSchemaValue\n from ..version import version_short\n-from ..warnings import PydanticDeprecatedSince20\n+from ..warnings import PydanticDeprecatedSince20, PydanticSkipValidationWarning",
      "comment": "```suggestion\r\nfrom ..warnings import PydanticArbitraryTypeWarning, PydanticDeprecatedSince20\r\n```",
      "comment_id": 2161114862,
      "user": "Viicos",
      "created_at": "2025-06-23T09:13:11Z",
      "url": "https://github.com/pydantic/pydantic/pull/12002#discussion_r2161114862"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12002,
      "file_path": "pydantic/functional_validators.py",
      "line": 18,
      "side": "RIGHT",
      "diff_hunk": "@@ -14,6 +15,7 @@\n from ._internal import _decorators, _generics, _internal_dataclass\n from .annotated_handlers import GetCoreSchemaHandler\n from .errors import PydanticUserError\n+from .warnings import PydanticSkipValidationWarning",
      "comment": "```suggestion\r\nfrom .warnings import PydanticArbitraryTypeWarning\r\n```",
      "comment_id": 2161115592,
      "user": "Viicos",
      "created_at": "2025-06-23T09:13:24Z",
      "url": "https://github.com/pydantic/pydantic/pull/12002#discussion_r2161115592"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12002,
      "file_path": "pydantic/functional_validators.py",
      "line": 823,
      "side": "RIGHT",
      "diff_hunk": "@@ -817,13 +819,15 @@ def __class_getitem__(cls, item: Any) -> Any:\n \n         @classmethod\n         def __get_pydantic_core_schema__(cls, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n-            original_schema = handler(source)\n-            metadata = {'pydantic_js_annotation_functions': [lambda _c, h: h(original_schema)]}\n-            return core_schema.any_schema(\n-                metadata=metadata,\n-                serialization=core_schema.wrap_serializer_function_ser_schema(\n-                    function=lambda v, h: h(v), schema=original_schema\n-                ),\n-            )\n+            with warnings.catch_warnings():\n+                warnings.simplefilter('ignore', PydanticSkipValidationWarning)",
      "comment": "```suggestion\r\n                warnings.simplefilter('ignore', PydanticArbitraryTypeWarning)\r\n```",
      "comment_id": 2161115936,
      "user": "Viicos",
      "created_at": "2025-06-23T09:13:32Z",
      "url": "https://github.com/pydantic/pydantic/pull/12002#discussion_r2161115936"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12002,
      "file_path": "pydantic/warnings.py",
      "line": 100,
      "side": "RIGHT",
      "diff_hunk": "@@ -94,3 +94,7 @@ class PydanticExperimentalWarning(Warning):\n     This warning is raised when using experimental functionality in Pydantic.\n     It is raised to warn users that the functionality may change or be removed in future versions of Pydantic.\n     \"\"\"\n+\n+\n+class PydanticSkipValidationWarning(UserWarning):\n+    \"\"\"Warning raised when SkipValidation is used for unsupported types.\"\"\"",
      "comment": "```suggestion\r\nclass PydanticArbitraryTypeWarning(UserWarning):\r\n    \"\"\"Warning raised when Pydantic fails to generate a core schema for an arbitrary type.\"\"\"\r\n```",
      "comment_id": 2161117398,
      "user": "Viicos",
      "created_at": "2025-06-23T09:14:09Z",
      "url": "https://github.com/pydantic/pydantic/pull/12002#discussion_r2161117398"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11858,
      "file_path": "pydantic/functional_validators.py",
      "line": 98,
      "side": "RIGHT",
      "diff_hunk": "@@ -94,7 +94,7 @@ class BeforeValidator:\n \n     Attributes:\n         func: The validator function.\n-        json_schema_input_type: The input type of the function. This is only used to generate the appropriate\n+        json_schema_input_type: This is only used to generate the appropriate\n             JSON Schema (in validation mode).",
      "comment": "```suggestion\r\n        json_schema_input_type: The input type used to generate the appropriate\r\n            JSON Schema (in validation mode). The actual input type is `Any`.\r\n```",
      "comment_id": 2084439283,
      "user": "DouweM",
      "created_at": "2025-05-12T11:06:59Z",
      "url": "https://github.com/pydantic/pydantic/pull/11858#discussion_r2084439283"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 1881,
      "file_path": "pydantic/main.py",
      "line": 316,
      "side": "RIGHT",
      "diff_hunk": "@@ -313,6 +313,7 @@ def __new__(mcs, name, bases, namespace, **kwargs):  # noqa C901\n             '__schema_cache__': {},\n             '__json_encoder__': staticmethod(json_encoder),\n             '__custom_root_type__': _custom_root_type,\n+            '__hash__': (lambda self: hash(tuple(self.__dict__.values()))) if not config.allow_mutation else None,",
      "comment": "Instead of `.values` we should probably have `.items` here.  The keys ought to be considered a part of the identity of the object.\r\n\r\nExample:\r\n```\r\nx = {\"a\": 1, \"b\": True}\r\ny = {\"d\": 1, \"c\": True}\r\nhash(tuple(x.values())) == hash(tuple(y.values()))  # True\r\nhash(tuple(x.items())) == hash(tuple(y.items()))  # False\r\n```",
      "comment_id": 490409076,
      "user": "wozniakty",
      "created_at": "2020-09-17T16:45:59Z",
      "url": "https://github.com/pydantic/pydantic/pull/1881#discussion_r490409076"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 1881,
      "file_path": "tests/test_main.py",
      "line": 421,
      "side": "RIGHT",
      "diff_hunk": "@@ -373,6 +386,32 @@ class Config:\n     assert '\"TestModel\" object has no field \"b\"' in exc_info.value.args[0]\n \n \n+def test_immutable_with_hashable_fields_are_hashable():\n+    class TestModel(BaseModel):\n+        a: int = 10\n+\n+        class Config:\n+            allow_mutation = False\n+\n+    m = TestModel()\n+    assert m.__hash__ is not None\n+    assert isinstance(hash(m), int)\n+\n+\n+def test_immutable_with_unhashable_fields_are_not_hashable():\n+    class TestModel(BaseModel):\n+        a: int = 10\n+        y: List[int] = [1, 2, 3]\n+\n+        class Config:\n+            allow_mutation = False\n+\n+    m = TestModel()\n+    with pytest.raises(TypeError) as exc_info:\n+        hash(m)\n+    assert \"unhashable type: 'list'\" in exc_info.value.args[0]",
      "comment": "IMO would be nice for the exception message to be more like \"unhashable type: 'TestModel' contains unhashable type: 'list'  but I wouldn't say it's a strong opinion.  Food for thought.",
      "comment_id": 490416671,
      "user": "wozniakty",
      "created_at": "2020-09-17T16:57:54Z",
      "url": "https://github.com/pydantic/pydantic/pull/1881#discussion_r490416671"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 1881,
      "file_path": "pydantic/main.py",
      "line": 316,
      "side": "RIGHT",
      "diff_hunk": "@@ -313,6 +313,7 @@ def __new__(mcs, name, bases, namespace, **kwargs):  # noqa C901\n             '__schema_cache__': {},\n             '__json_encoder__': staticmethod(json_encoder),\n             '__custom_root_type__': _custom_root_type,\n+            '__hash__': (lambda self: hash(tuple(self.__dict__.values()))) if not config.allow_mutation else None,",
      "comment": "In that case, we diverge from behaviour of the built-in dataclass. But I do not know if it is a problem.\r\nIf it not a problem, then I agree with you ! :)",
      "comment_id": 498257798,
      "user": "rhuille",
      "created_at": "2020-10-01T13:48:07Z",
      "url": "https://github.com/pydantic/pydantic/pull/1881#discussion_r498257798"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 1881,
      "file_path": "pydantic/main.py",
      "line": 316,
      "side": "RIGHT",
      "diff_hunk": "@@ -313,6 +313,7 @@ def __new__(mcs, name, bases, namespace, **kwargs):  # noqa C901\n             '__schema_cache__': {},\n             '__json_encoder__': staticmethod(json_encoder),\n             '__custom_root_type__': _custom_root_type,\n+            '__hash__': (lambda self: hash(tuple(self.__dict__.values()))) if not config.allow_mutation else None,",
      "comment": "I don't think this needs to be a lambda, it can be proper method on `BaseModel` I imagine. It could raise an error if `allow_mutation is True` .\r\n",
      "comment_id": 502362557,
      "user": "samuelcolvin",
      "created_at": "2020-10-09T11:25:49Z",
      "url": "https://github.com/pydantic/pydantic/pull/1881#discussion_r502362557"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 1881,
      "file_path": "pydantic/main.py",
      "line": 316,
      "side": "RIGHT",
      "diff_hunk": "@@ -313,6 +313,7 @@ def __new__(mcs, name, bases, namespace, **kwargs):  # noqa C901\n             '__schema_cache__': {},\n             '__json_encoder__': staticmethod(json_encoder),\n             '__custom_root_type__': _custom_root_type,\n+            '__hash__': (lambda self: hash(tuple(self.__dict__.values()))) if not config.allow_mutation else None,",
      "comment": "okay, so it can't just be a method on `BaseModel`, but let's make it a proper function, not a lambda.",
      "comment_id": 502392221,
      "user": "samuelcolvin",
      "created_at": "2020-10-09T12:28:13Z",
      "url": "https://github.com/pydantic/pydantic/pull/1881#discussion_r502392221"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 1881,
      "file_path": "pydantic/main.py",
      "line": 316,
      "side": "RIGHT",
      "diff_hunk": "@@ -313,6 +313,7 @@ def __new__(mcs, name, bases, namespace, **kwargs):  # noqa C901\n             '__schema_cache__': {},\n             '__json_encoder__': staticmethod(json_encoder),\n             '__custom_root_type__': _custom_root_type,\n+            '__hash__': (lambda self: hash(tuple(self.__dict__.values()))) if not config.allow_mutation else None,",
      "comment": "> Instead of `.values` we should probably have `.items` here. The keys ought to be considered a part of the identity of the object.\r\n\r\nI think this would be unnecessary if we include a reference to the model class.",
      "comment_id": 502398896,
      "user": "layday",
      "created_at": "2020-10-09T12:40:45Z",
      "url": "https://github.com/pydantic/pydantic/pull/1881#discussion_r502398896"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 1881,
      "file_path": "pydantic/main.py",
      "line": 332,
      "side": "RIGHT",
      "diff_hunk": "@@ -321,6 +329,7 @@ def __new__(mcs, name, bases, namespace, **kwargs):  # noqa C901\n             '__custom_root_type__': _custom_root_type,\n             '__private_attributes__': private_attributes,\n             '__slots__': slots | private_attributes.keys(),\n+            '__hash__': generate_hash_function(config.frozen, super()),",
      "comment": "`super()` is the metaclass here. I don't understand why you need to do that \ud83e\udd14 ",
      "comment_id": 560801908,
      "user": "PrettyWood",
      "created_at": "2021-01-20T09:16:48Z",
      "url": "https://github.com/pydantic/pydantic/pull/1881#discussion_r560801908"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 1881,
      "file_path": "pydantic/main.py",
      "line": 204,
      "side": "RIGHT",
      "diff_hunk": "@@ -198,6 +199,13 @@ def validate_custom_root_type(fields: Dict[str, ModelField]) -> None:\n         raise ValueError('__root__ cannot be mixed with other fields')\n \n \n+def generate_hash_function(frozen: bool, super_class: Any) -> Optional[Callable[[Any], int]]:\n+    def hash_function(self_: Any) -> int:\n+        return hash(super_class) + hash(tuple(self_.__dict__.values()))",
      "comment": "IMO you could just do `hash(self_.__class__)` instead of using `super_class`",
      "comment_id": 560802090,
      "user": "PrettyWood",
      "created_at": "2021-01-20T09:17:07Z",
      "url": "https://github.com/pydantic/pydantic/pull/1881#discussion_r560802090"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 1881,
      "file_path": "pydantic/main.py",
      "line": 204,
      "side": "RIGHT",
      "diff_hunk": "@@ -198,6 +199,13 @@ def validate_custom_root_type(fields: Dict[str, ModelField]) -> None:\n         raise ValueError('__root__ cannot be mixed with other fields')\n \n \n+def generate_hash_function(frozen: bool, super_class: Any) -> Optional[Callable[[Any], int]]:\n+    def hash_function(self_: Any) -> int:\n+        return hash(super_class) + hash(tuple(self_.__dict__.values()))",
      "comment": "Yes your are right ! https://github.com/samuelcolvin/pydantic/pull/1881/commits/12cff48de640035718d1308c04eb007fe5e1ef05 (I was overthinked this...)",
      "comment_id": 565464587,
      "user": "rhuille",
      "created_at": "2021-01-27T16:45:40Z",
      "url": "https://github.com/pydantic/pydantic/pull/1881#discussion_r565464587"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 1881,
      "file_path": "tests/mypy/modules/plugin_fail.py",
      "line": 224,
      "side": "RIGHT",
      "diff_hunk": "@@ -202,3 +202,37 @@ class AddProject:\n \n \n p = AddProject(name='x', slug='y', description='z')\n+\n+\n+# Same as Model, but with frozen = True\n+class FrozenModel(BaseModel):\n+    x: int\n+    y: str\n+\n+    def method(self) -> None:\n+        pass\n+\n+    class Config:\n+        alias_generator = None\n+        frozen = True\n+        extra = Extra.forbid\n+\n+        def config_method(self) -> None:\n+            ...\n+\n+\n+frozenmodel = FrozenModel(x=1, y='y', z='z')",
      "comment": "this isn't testing frozen behaviour I think, again can be removed.",
      "comment_id": 575662771,
      "user": "samuelcolvin",
      "created_at": "2021-02-13T12:40:41Z",
      "url": "https://github.com/pydantic/pydantic/pull/1881#discussion_r575662771"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 1881,
      "file_path": "tests/mypy/modules/plugin_success.py",
      "line": 152,
      "side": "RIGHT",
      "diff_hunk": "@@ -139,3 +139,22 @@ class Model(BaseModel):\n \n dynamic_model = DynamicModel(x=1, y='y')\n dynamic_model.x = 2\n+\n+\n+class FrozenModel(BaseModel):\n+    x: int\n+\n+    class Config:\n+        frozen = True\n+\n+\n+class NotFrozenModel(FrozenModel):\n+    a = 1",
      "comment": "add a type here to avoid extra errors in `plugin-success-strict.txt `",
      "comment_id": 575662903,
      "user": "samuelcolvin",
      "created_at": "2021-02-13T12:42:16Z",
      "url": "https://github.com/pydantic/pydantic/pull/1881#discussion_r575662903"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 1881,
      "file_path": "tests/test_main.py",
      "line": 368,
      "side": "RIGHT",
      "diff_hunk": "@@ -351,39 +351,96 @@ class Model(BaseModel):\n     assert exc_info.value.errors() == [{'loc': ('a',), 'msg': 'field required', 'type': 'value_error.missing'}]\n \n \n-def test_not_immutability():\n+@pytest.mark.parametrize('allow_mutation_, frozen_', [(False, False), (True, False), (False, True), (True, True)])\n+def test_immutability(allow_mutation_, frozen_):\n     class TestModel(BaseModel):\n         a: int = 10\n \n         class Config:\n-            allow_mutation = True\n+            allow_mutation = allow_mutation_\n             extra = Extra.forbid\n+            frozen = frozen_\n \n     m = TestModel()\n-    assert m.a == 10\n-    m.a = 11\n-    assert m.a == 11\n-    with pytest.raises(ValueError) as exc_info:\n-        m.b = 11\n-    assert '\"TestModel\" object has no field \"b\"' in exc_info.value.args[0]\n+\n+    immutable = allow_mutation_ is False or frozen_ is True\n+\n+    if immutable:",
      "comment": "rather than this if clause, better to setup two separate tests.",
      "comment_id": 575662957,
      "user": "samuelcolvin",
      "created_at": "2021-02-13T12:43:25Z",
      "url": "https://github.com/pydantic/pydantic/pull/1881#discussion_r575662957"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12051,
      "file_path": "tests/test_dataclasses.py",
      "line": 2146,
      "side": "RIGHT",
      "diff_hunk": "@@ -2115,6 +2115,34 @@ class Child(Parent):\n     assert Child().a == 1\n \n \n+def test_dataclasses_inheritance_bare_class_not_used() -> None:\n+    \"\"\"https://github.com/pydantic/pydantic/issues/12045\"\"\"\n+\n+    class BareClass:\n+        a: int = Field(kw_only=True)\n+\n+    @pydantic.dataclasses.dataclass\n+    class DC(BareClass):\n+        pass\n+\n+    assert len(DC.__dataclass_fields__) == 0\n+    assert len(DC.__pydantic_fields__) == 0\n+\n+\n+def test_dataclasses_type_override_pydantic_field() -> None:\n+    \"\"\"https://github.com/pydantic/pydantic/issues/12045\"\"\"\n+\n+    @dataclasses.dataclass\n+    class A:\n+        a: int = Field()\n+\n+    @pydantic.dataclasses.dataclass\n+    class B(A):\n+        a: str = dataclasses.field()\n+\n+    assert B(a='test').a == 'test'",
      "comment": "These two tests are technically breaking changes, but this was just undefined behavior and imo actual bugs. I think we should update our [stability policy](https://docs.pydantic.dev/latest/version-policy/#pydantic-v2) to state that bug fixes resulting in theoretical breaking change can be occur in minor releases. ",
      "comment_id": 2197144618,
      "user": "Viicos",
      "created_at": "2025-07-10T09:38:27Z",
      "url": "https://github.com/pydantic/pydantic/pull/12051#discussion_r2197144618"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12051,
      "file_path": "tests/test_dataclasses.py",
      "line": 2146,
      "side": "RIGHT",
      "diff_hunk": "@@ -2115,6 +2115,34 @@ class Child(Parent):\n     assert Child().a == 1\n \n \n+def test_dataclasses_inheritance_bare_class_not_used() -> None:\n+    \"\"\"https://github.com/pydantic/pydantic/issues/12045\"\"\"\n+\n+    class BareClass:\n+        a: int = Field(kw_only=True)\n+\n+    @pydantic.dataclasses.dataclass\n+    class DC(BareClass):\n+        pass\n+\n+    assert len(DC.__dataclass_fields__) == 0\n+    assert len(DC.__pydantic_fields__) == 0\n+\n+\n+def test_dataclasses_type_override_pydantic_field() -> None:\n+    \"\"\"https://github.com/pydantic/pydantic/issues/12045\"\"\"\n+\n+    @dataclasses.dataclass\n+    class A:\n+        a: int = Field()\n+\n+    @pydantic.dataclasses.dataclass\n+    class B(A):\n+        a: str = dataclasses.field()\n+\n+    assert B(a='test').a == 'test'",
      "comment": "I think yes, as long as we also qualify that by saying that we'll make judgement call based on how much we expect the theoretical breaking change to actually be a real-world problem.",
      "comment_id": 2197348987,
      "user": "davidhewitt",
      "created_at": "2025-07-10T10:45:32Z",
      "url": "https://github.com/pydantic/pydantic/pull/12051#discussion_r2197348987"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12051,
      "file_path": "tests/test_dataclasses.py",
      "line": 2146,
      "side": "RIGHT",
      "diff_hunk": "@@ -2115,6 +2115,34 @@ class Child(Parent):\n     assert Child().a == 1\n \n \n+def test_dataclasses_inheritance_bare_class_not_used() -> None:\n+    \"\"\"https://github.com/pydantic/pydantic/issues/12045\"\"\"\n+\n+    class BareClass:\n+        a: int = Field(kw_only=True)\n+\n+    @pydantic.dataclasses.dataclass\n+    class DC(BareClass):\n+        pass\n+\n+    assert len(DC.__dataclass_fields__) == 0\n+    assert len(DC.__pydantic_fields__) == 0\n+\n+\n+def test_dataclasses_type_override_pydantic_field() -> None:\n+    \"\"\"https://github.com/pydantic/pydantic/issues/12045\"\"\"\n+\n+    @dataclasses.dataclass\n+    class A:\n+        a: int = Field()\n+\n+    @pydantic.dataclasses.dataclass\n+    class B(A):\n+        a: str = dataclasses.field()\n+\n+    assert B(a='test').a == 'test'",
      "comment": "Yep, in this case you'd have to make use of a stdlib (that is not a Pydantic) dataclass or a bare class _with_ a Pydantic field used on it, which is pretty uncommon already.",
      "comment_id": 2197559701,
      "user": "Viicos",
      "created_at": "2025-07-10T12:15:11Z",
      "url": "https://github.com/pydantic/pydantic/pull/12051#discussion_r2197559701"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12051,
      "file_path": "pydantic/_internal/_dataclasses.py",
      "line": 278,
      "side": "RIGHT",
      "diff_hunk": "@@ -203,3 +208,105 @@ def is_stdlib_dataclass(cls: type[Any], /) -> TypeIs[type[StandardDataclass]]:\n         `True` if the class is a stdlib dataclass, `False` otherwise.\n     \"\"\"\n     return '__dataclass_fields__' in cls.__dict__ and not hasattr(cls, '__pydantic_validator__')\n+\n+\n+def as_dataclass_field(pydantic_field: FieldInfo) -> dataclasses.Field[Any]:\n+    field_args: dict[str, Any] = {'default': pydantic_field}\n+\n+    if sys.version_info >= (3, 10) and pydantic_field.kw_only:\n+        field_args['kw_only'] = True\n+\n+    if pydantic_field.repr is not True:\n+        field_args['repr'] = pydantic_field.repr\n+\n+    return dataclasses.field(**field_args)\n+\n+\n+DcFields: TypeAlias = dict[str, dataclasses.Field[Any]]\n+\n+\n+@contextmanager\n+def patch_base_fields(cls: type[Any]) -> Generator[None]:\n+    \"\"\"Temporarily patch the stdlib dataclasses bases of `cls` if the Pydantic `Field()` function is used.\n+\n+    When creating a Pydantic dataclass, it is possible to inherit from stdlib dataclasses, where\n+    the Pydantic `Field()` function is used. To create this Pydantic dataclass, we first apply\n+    the stdlib `@dataclass` decorator on it. During the construction of the stdlib dataclass,\n+    the `kw_only` and `repr` field arguments need to be understood by the stdlib *during* the\n+    dataclass construction. To do so, we temporarily patch the fields dictionary of the affected\n+    bases.\n+\n+    For instance, with the following example:\n+\n+    ```python {test=\"skip\" lint=\"skip\"}\n+    import dataclasses as stdlib_dc\n+\n+    import pydantic\n+    import pydantic.dataclasses as pydantic_dc\n+\n+    @stdlib_dc.dataclass\n+    class A:\n+        a: int = pydantic.Field(repr=False)\n+\n+    # Notice that the `repr` attribute of the dataclass field is `True`:\n+    A.__dataclass_fields__['a']\n+    #> dataclass.Field(default=FieldInfo(repr=False), repr=True, ...)\n+\n+    @pydantic_dc.dataclass\n+    class B(A):\n+        b: int = pydantic.Field(repr=False)\n+    ```\n+\n+    When passing `B` to the stdlib `@dataclass` decorator, it will look for fields in the parent classes\n+    and reuse them directly. When this contextmanager is active, `A` will be temporarily patched to be\n+    equivalent to:\n+\n+    ```python {test=\"skip\" lint=\"skip\"}\n+    @stdlib_dc.dataclass\n+    class A:\n+        a: int = stdlib_dc.field(default=Field(repr=False), repr=False)\n+    ```\n+\n+    !!! note\n+        This is only applied to the bases of `cls`, and not `cls` itself. The reason is that the Pydantic\n+        dataclass decorator \"owns\" `cls` (in the previous example, `B`). As such, we instead modify the fields\n+        directly (in the previous example, we simply do `setattr(B, 'b', as_dataclass_field(pydantic_field))`).\n+\n+    !!! note\n+        This approach is far from ideal, and can probably be the source of unwanted side effects/race conditions.\n+        The previous implemented approach was mutating the `__annotations__` dict of `cls`, which is no longer a\n+        safe operation in Python 3.14+, and resulted in unexpected behavior with field ordering anyway.",
      "comment": "Agreed, scary. If this became a problem, I think we could do stuff like set the field to be a wrapper object which checks what thread it's on and only exhibits the patched behaviour on this thread, but even that is observable so why bother \ud83d\ude22 ",
      "comment_id": 2210853813,
      "user": "davidhewitt",
      "created_at": "2025-07-16T16:02:36Z",
      "url": "https://github.com/pydantic/pydantic/pull/12051#discussion_r2210853813"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12051,
      "file_path": "pydantic/_internal/_dataclasses.py",
      "line": 305,
      "side": "RIGHT",
      "diff_hunk": "@@ -203,3 +208,105 @@ def is_stdlib_dataclass(cls: type[Any], /) -> TypeIs[type[StandardDataclass]]:\n         `True` if the class is a stdlib dataclass, `False` otherwise.\n     \"\"\"\n     return '__dataclass_fields__' in cls.__dict__ and not hasattr(cls, '__pydantic_validator__')\n+\n+\n+def as_dataclass_field(pydantic_field: FieldInfo) -> dataclasses.Field[Any]:\n+    field_args: dict[str, Any] = {'default': pydantic_field}\n+\n+    if sys.version_info >= (3, 10) and pydantic_field.kw_only:\n+        field_args['kw_only'] = True\n+\n+    if pydantic_field.repr is not True:\n+        field_args['repr'] = pydantic_field.repr\n+\n+    return dataclasses.field(**field_args)\n+\n+\n+DcFields: TypeAlias = dict[str, dataclasses.Field[Any]]\n+\n+\n+@contextmanager\n+def patch_base_fields(cls: type[Any]) -> Generator[None]:\n+    \"\"\"Temporarily patch the stdlib dataclasses bases of `cls` if the Pydantic `Field()` function is used.\n+\n+    When creating a Pydantic dataclass, it is possible to inherit from stdlib dataclasses, where\n+    the Pydantic `Field()` function is used. To create this Pydantic dataclass, we first apply\n+    the stdlib `@dataclass` decorator on it. During the construction of the stdlib dataclass,\n+    the `kw_only` and `repr` field arguments need to be understood by the stdlib *during* the\n+    dataclass construction. To do so, we temporarily patch the fields dictionary of the affected\n+    bases.\n+\n+    For instance, with the following example:\n+\n+    ```python {test=\"skip\" lint=\"skip\"}\n+    import dataclasses as stdlib_dc\n+\n+    import pydantic\n+    import pydantic.dataclasses as pydantic_dc\n+\n+    @stdlib_dc.dataclass\n+    class A:\n+        a: int = pydantic.Field(repr=False)\n+\n+    # Notice that the `repr` attribute of the dataclass field is `True`:\n+    A.__dataclass_fields__['a']\n+    #> dataclass.Field(default=FieldInfo(repr=False), repr=True, ...)\n+\n+    @pydantic_dc.dataclass\n+    class B(A):\n+        b: int = pydantic.Field(repr=False)\n+    ```\n+\n+    When passing `B` to the stdlib `@dataclass` decorator, it will look for fields in the parent classes\n+    and reuse them directly. When this contextmanager is active, `A` will be temporarily patched to be\n+    equivalent to:\n+\n+    ```python {test=\"skip\" lint=\"skip\"}\n+    @stdlib_dc.dataclass\n+    class A:\n+        a: int = stdlib_dc.field(default=Field(repr=False), repr=False)\n+    ```\n+\n+    !!! note\n+        This is only applied to the bases of `cls`, and not `cls` itself. The reason is that the Pydantic\n+        dataclass decorator \"owns\" `cls` (in the previous example, `B`). As such, we instead modify the fields\n+        directly (in the previous example, we simply do `setattr(B, 'b', as_dataclass_field(pydantic_field))`).\n+\n+    !!! note\n+        This approach is far from ideal, and can probably be the source of unwanted side effects/race conditions.\n+        The previous implemented approach was mutating the `__annotations__` dict of `cls`, which is no longer a\n+        safe operation in Python 3.14+, and resulted in unexpected behavior with field ordering anyway.\n+    \"\"\"\n+    # A list of two-tuples, the first element being a reference to the\n+    # dataclass fields dictionary, the second element being a mapping between\n+    # the field names that were modified, and their original `Field`:\n+    original_fields_list: list[tuple[DcFields, DcFields]] = []\n+\n+    for base in cls.__mro__[1:]:\n+        dc_fields: dict[str, dataclasses.Field[Any]] = base.__dict__.get('__dataclass_fields__', {})\n+        dc_fields_with_pydantic_field_defaults = {\n+            field_name: field\n+            for field_name, field in dc_fields.items()\n+            if isinstance(field.default, FieldInfo)\n+            # Only do the patching if one of the affected attributes is set:\n+            and (field.default.kw_only or field.default.repr is not True)\n+        }\n+        if dc_fields_with_pydantic_field_defaults:\n+            original_fields_list.append((dc_fields, dc_fields_with_pydantic_field_defaults))\n+            for field_name, field in dc_fields_with_pydantic_field_defaults.items():\n+                default = cast(FieldInfo, field.default)\n+                # `dataclasses.Field` isn't documented as working with `copy.copy()`.\n+                # It is a class with `__slots__`, so should work (and we hope for the best):\n+                new_dc_field = copy.copy(field)\n+                if default.kw_only:\n+                    new_dc_field.kw_only = True\n+                if default.repr is not True:\n+                    new_dc_field.repr = default.repr\n+                dc_fields[field_name] = new_dc_field",
      "comment": "I wonder, rather than mutating the fields of the base objects, is there a world where we instead patch the class attributes of the type under construction? That might be significantly less far-reaching in potential conflict?",
      "comment_id": 2210866912,
      "user": "davidhewitt",
      "created_at": "2025-07-16T16:08:41Z",
      "url": "https://github.com/pydantic/pydantic/pull/12051#discussion_r2210866912"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12051,
      "file_path": "pydantic/_internal/_dataclasses.py",
      "line": 305,
      "side": "RIGHT",
      "diff_hunk": "@@ -203,3 +208,105 @@ def is_stdlib_dataclass(cls: type[Any], /) -> TypeIs[type[StandardDataclass]]:\n         `True` if the class is a stdlib dataclass, `False` otherwise.\n     \"\"\"\n     return '__dataclass_fields__' in cls.__dict__ and not hasattr(cls, '__pydantic_validator__')\n+\n+\n+def as_dataclass_field(pydantic_field: FieldInfo) -> dataclasses.Field[Any]:\n+    field_args: dict[str, Any] = {'default': pydantic_field}\n+\n+    if sys.version_info >= (3, 10) and pydantic_field.kw_only:\n+        field_args['kw_only'] = True\n+\n+    if pydantic_field.repr is not True:\n+        field_args['repr'] = pydantic_field.repr\n+\n+    return dataclasses.field(**field_args)\n+\n+\n+DcFields: TypeAlias = dict[str, dataclasses.Field[Any]]\n+\n+\n+@contextmanager\n+def patch_base_fields(cls: type[Any]) -> Generator[None]:\n+    \"\"\"Temporarily patch the stdlib dataclasses bases of `cls` if the Pydantic `Field()` function is used.\n+\n+    When creating a Pydantic dataclass, it is possible to inherit from stdlib dataclasses, where\n+    the Pydantic `Field()` function is used. To create this Pydantic dataclass, we first apply\n+    the stdlib `@dataclass` decorator on it. During the construction of the stdlib dataclass,\n+    the `kw_only` and `repr` field arguments need to be understood by the stdlib *during* the\n+    dataclass construction. To do so, we temporarily patch the fields dictionary of the affected\n+    bases.\n+\n+    For instance, with the following example:\n+\n+    ```python {test=\"skip\" lint=\"skip\"}\n+    import dataclasses as stdlib_dc\n+\n+    import pydantic\n+    import pydantic.dataclasses as pydantic_dc\n+\n+    @stdlib_dc.dataclass\n+    class A:\n+        a: int = pydantic.Field(repr=False)\n+\n+    # Notice that the `repr` attribute of the dataclass field is `True`:\n+    A.__dataclass_fields__['a']\n+    #> dataclass.Field(default=FieldInfo(repr=False), repr=True, ...)\n+\n+    @pydantic_dc.dataclass\n+    class B(A):\n+        b: int = pydantic.Field(repr=False)\n+    ```\n+\n+    When passing `B` to the stdlib `@dataclass` decorator, it will look for fields in the parent classes\n+    and reuse them directly. When this contextmanager is active, `A` will be temporarily patched to be\n+    equivalent to:\n+\n+    ```python {test=\"skip\" lint=\"skip\"}\n+    @stdlib_dc.dataclass\n+    class A:\n+        a: int = stdlib_dc.field(default=Field(repr=False), repr=False)\n+    ```\n+\n+    !!! note\n+        This is only applied to the bases of `cls`, and not `cls` itself. The reason is that the Pydantic\n+        dataclass decorator \"owns\" `cls` (in the previous example, `B`). As such, we instead modify the fields\n+        directly (in the previous example, we simply do `setattr(B, 'b', as_dataclass_field(pydantic_field))`).\n+\n+    !!! note\n+        This approach is far from ideal, and can probably be the source of unwanted side effects/race conditions.\n+        The previous implemented approach was mutating the `__annotations__` dict of `cls`, which is no longer a\n+        safe operation in Python 3.14+, and resulted in unexpected behavior with field ordering anyway.\n+    \"\"\"\n+    # A list of two-tuples, the first element being a reference to the\n+    # dataclass fields dictionary, the second element being a mapping between\n+    # the field names that were modified, and their original `Field`:\n+    original_fields_list: list[tuple[DcFields, DcFields]] = []\n+\n+    for base in cls.__mro__[1:]:\n+        dc_fields: dict[str, dataclasses.Field[Any]] = base.__dict__.get('__dataclass_fields__', {})\n+        dc_fields_with_pydantic_field_defaults = {\n+            field_name: field\n+            for field_name, field in dc_fields.items()\n+            if isinstance(field.default, FieldInfo)\n+            # Only do the patching if one of the affected attributes is set:\n+            and (field.default.kw_only or field.default.repr is not True)\n+        }\n+        if dc_fields_with_pydantic_field_defaults:\n+            original_fields_list.append((dc_fields, dc_fields_with_pydantic_field_defaults))\n+            for field_name, field in dc_fields_with_pydantic_field_defaults.items():\n+                default = cast(FieldInfo, field.default)\n+                # `dataclasses.Field` isn't documented as working with `copy.copy()`.\n+                # It is a class with `__slots__`, so should work (and we hope for the best):\n+                new_dc_field = copy.copy(field)\n+                if default.kw_only:\n+                    new_dc_field.kw_only = True\n+                if default.repr is not True:\n+                    new_dc_field.repr = default.repr\n+                dc_fields[field_name] = new_dc_field",
      "comment": "The issue is that the dataclasses module is doing roughly:\r\n\r\n```python\r\nfields: dict[str, Field] = {}\r\n\r\nfor b in cls.__mro__[-1:0:-1]:\r\n    base_fields = getattr(b, '__dataclass_fields__', None)\r\n    if base_fields is not None:\r\n        for f_name, f in base_fields.items():\r\n            fields[f_name] = f\r\n\r\n# Then fetch the annotations of the class under construction and build fields for it\r\n```\r\n\r\nI thought about patching the whole `__dataclass_fields__` attribute directly, without mutating the original dict, but I don't know which one is best. I think we can revisit if we ever get issues.",
      "comment_id": 2212331202,
      "user": "Viicos",
      "created_at": "2025-07-17T05:46:55Z",
      "url": "https://github.com/pydantic/pydantic/pull/12051#discussion_r2212331202"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12051,
      "file_path": "pydantic/_internal/_dataclasses.py",
      "line": 278,
      "side": "RIGHT",
      "diff_hunk": "@@ -203,3 +208,105 @@ def is_stdlib_dataclass(cls: type[Any], /) -> TypeIs[type[StandardDataclass]]:\n         `True` if the class is a stdlib dataclass, `False` otherwise.\n     \"\"\"\n     return '__dataclass_fields__' in cls.__dict__ and not hasattr(cls, '__pydantic_validator__')\n+\n+\n+def as_dataclass_field(pydantic_field: FieldInfo) -> dataclasses.Field[Any]:\n+    field_args: dict[str, Any] = {'default': pydantic_field}\n+\n+    if sys.version_info >= (3, 10) and pydantic_field.kw_only:\n+        field_args['kw_only'] = True\n+\n+    if pydantic_field.repr is not True:\n+        field_args['repr'] = pydantic_field.repr\n+\n+    return dataclasses.field(**field_args)\n+\n+\n+DcFields: TypeAlias = dict[str, dataclasses.Field[Any]]\n+\n+\n+@contextmanager\n+def patch_base_fields(cls: type[Any]) -> Generator[None]:\n+    \"\"\"Temporarily patch the stdlib dataclasses bases of `cls` if the Pydantic `Field()` function is used.\n+\n+    When creating a Pydantic dataclass, it is possible to inherit from stdlib dataclasses, where\n+    the Pydantic `Field()` function is used. To create this Pydantic dataclass, we first apply\n+    the stdlib `@dataclass` decorator on it. During the construction of the stdlib dataclass,\n+    the `kw_only` and `repr` field arguments need to be understood by the stdlib *during* the\n+    dataclass construction. To do so, we temporarily patch the fields dictionary of the affected\n+    bases.\n+\n+    For instance, with the following example:\n+\n+    ```python {test=\"skip\" lint=\"skip\"}\n+    import dataclasses as stdlib_dc\n+\n+    import pydantic\n+    import pydantic.dataclasses as pydantic_dc\n+\n+    @stdlib_dc.dataclass\n+    class A:\n+        a: int = pydantic.Field(repr=False)\n+\n+    # Notice that the `repr` attribute of the dataclass field is `True`:\n+    A.__dataclass_fields__['a']\n+    #> dataclass.Field(default=FieldInfo(repr=False), repr=True, ...)\n+\n+    @pydantic_dc.dataclass\n+    class B(A):\n+        b: int = pydantic.Field(repr=False)\n+    ```\n+\n+    When passing `B` to the stdlib `@dataclass` decorator, it will look for fields in the parent classes\n+    and reuse them directly. When this contextmanager is active, `A` will be temporarily patched to be\n+    equivalent to:\n+\n+    ```python {test=\"skip\" lint=\"skip\"}\n+    @stdlib_dc.dataclass\n+    class A:\n+        a: int = stdlib_dc.field(default=Field(repr=False), repr=False)\n+    ```\n+\n+    !!! note\n+        This is only applied to the bases of `cls`, and not `cls` itself. The reason is that the Pydantic\n+        dataclass decorator \"owns\" `cls` (in the previous example, `B`). As such, we instead modify the fields\n+        directly (in the previous example, we simply do `setattr(B, 'b', as_dataclass_field(pydantic_field))`).\n+\n+    !!! note\n+        This approach is far from ideal, and can probably be the source of unwanted side effects/race conditions.\n+        The previous implemented approach was mutating the `__annotations__` dict of `cls`, which is no longer a\n+        safe operation in Python 3.14+, and resulted in unexpected behavior with field ordering anyway.",
      "comment": "Yeah I think if we ever get issues about this, we could wrap the mutation in a threading lock (similar to what I've initially tried to do in https://github.com/pydantic/pydantic/pull/11851). For this to be an issue, a user would have to dynamically create Pydantic dataclasses in a multi-threaded environment, and have them subclassing a stdlib dataclass with at least one field using the Pydantic's `Field()` function which is quite unlikely to happen.",
      "comment_id": 2212339644,
      "user": "Viicos",
      "created_at": "2025-07-17T05:53:26Z",
      "url": "https://github.com/pydantic/pydantic/pull/12051#discussion_r2212339644"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11759,
      "file_path": "pydantic/_internal/_model_construction.py",
      "line": 588,
      "side": "RIGHT",
      "diff_hunk": "@@ -587,6 +586,27 @@ def complete_model_class(\n             and `raise_errors=True`.\n     \"\"\"\n     typevars_map = get_model_typevars_map(cls)\n+\n+    if not cls.__pydantic_fields_complete__:",
      "comment": "Was the change required to implement the hook?\r\n\r\nIn both situations, we have inconsistent behavior but this can only be easily fixed if/when we remove support for accessing incomplete `model_fields` (well we can also make it so that `collect_model_fields()` \u2014 currently lenient to unresolvable type hints \u2014 also returns the `NameError`s, but this is quite ugly).\r\n\r\nWhen a model with unresolvable forward references is defined:\r\n\r\n- On `main`, we call `collect_model_fields()`, and then go through the `GenerateSchema` process, which will end up calling `rebuild_model_fields()`:\r\n\r\n    https://github.com/pydantic/pydantic/blob/625dd4284b266c17f0b70d282f237dd3ee7ec9f2/pydantic/_internal/_generate_schema.py#L741-L749\r\n\r\n- On this branch, we call `collect_model_fields()` and then call `rebuild_model_fields()` here.\r\n\r\n(In both cases, fields collection happens twice).",
      "comment_id": 2046466779,
      "user": "Viicos",
      "created_at": "2025-04-16T09:08:54Z",
      "url": "https://github.com/pydantic/pydantic/pull/11759#discussion_r2046466779"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11759,
      "file_path": "pydantic/_internal/_model_construction.py",
      "line": 588,
      "side": "RIGHT",
      "diff_hunk": "@@ -587,6 +586,27 @@ def complete_model_class(\n             and `raise_errors=True`.\n     \"\"\"\n     typevars_map = get_model_typevars_map(cls)\n+\n+    if not cls.__pydantic_fields_complete__:",
      "comment": "@Viicos Not strictly necessary to implement the hook, but I want to guarantee that when the hook is called (and the complete boolean is set), the fields are complete as well (see first two paragraphs of the PR description).\r\n\r\nThe __pydantic_init_subclass__ description states that model_fields will be present (although not necessarily complete), so I think we need to keep separataly collecting the model fields quietly and rebuilding them loudly. We could possibly also have collect_model_fields behave quietly but return an extra list of names that raised NameErrors.\r\n\r\nLet me know if you'd like to see a change here or if this is acceptable.",
      "comment_id": 2047206101,
      "user": "DouweM",
      "created_at": "2025-04-16T15:36:01Z",
      "url": "https://github.com/pydantic/pydantic/pull/11759#discussion_r2047206101"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11759,
      "file_path": "pydantic/_internal/_model_construction.py",
      "line": 588,
      "side": "RIGHT",
      "diff_hunk": "@@ -587,6 +586,27 @@ def complete_model_class(\n             and `raise_errors=True`.\n     \"\"\"\n     typevars_map = get_model_typevars_map(cls)\n+\n+    if not cls.__pydantic_fields_complete__:",
      "comment": "> We now do the same on `ModelMetaclass.__new__`, to make sure that subclasses of incomplete superclasses don't incorrectly get marked as complete.\r\n\r\nHum, did you have any example where this happened? When we create the model fields for a subclass, we copy the parent `FieldInfo` instances, keeping the `_complete` property.",
      "comment_id": 2047469054,
      "user": "Viicos",
      "created_at": "2025-04-16T18:12:24Z",
      "url": "https://github.com/pydantic/pydantic/pull/11759#discussion_r2047469054"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11759,
      "file_path": "pydantic/_internal/_model_construction.py",
      "line": 588,
      "side": "RIGHT",
      "diff_hunk": "@@ -587,6 +586,27 @@ def complete_model_class(\n             and `raise_errors=True`.\n     \"\"\"\n     typevars_map = get_model_typevars_map(cls)\n+\n+    if not cls.__pydantic_fields_complete__:",
      "comment": "@Viicos This is what I was describing in https://github.com/pydantic/pydantic/issues/11453#issuecomment-2803435274, where I added `assert cls.__pydantic_fields_complete__` at the end of `complete_model_class` (after `__pydantic_complete__ = True` and it failed in one test with a super and submodel. Rebuilding the fields solved the issue, because `SubModel` can resolve the `'SubModel'` ref to itself. You mentioned there that that was expected but not ideal, so I thought I'd fix it.",
      "comment_id": 2047478795,
      "user": "DouweM",
      "created_at": "2025-04-16T18:19:06Z",
      "url": "https://github.com/pydantic/pydantic/pull/11759#discussion_r2047478795"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11759,
      "file_path": "pydantic/_internal/_model_construction.py",
      "line": 562,
      "side": "RIGHT",
      "diff_hunk": "@@ -562,9 +559,9 @@ def set_model_fields(\n def complete_model_class(\n     cls: type[BaseModel],\n     config_wrapper: ConfigWrapper,\n+    ns_resolver: NsResolver,",
      "comment": "Let's update the order in the docstring and also where `complete_model_class()` is called.",
      "comment_id": 2048493997,
      "user": "Viicos",
      "created_at": "2025-04-17T08:30:29Z",
      "url": "https://github.com/pydantic/pydantic/pull/11759#discussion_r2048493997"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11759,
      "file_path": "pydantic/_internal/_model_construction.py",
      "line": 589,
      "side": "RIGHT",
      "diff_hunk": "@@ -587,6 +584,27 @@ def complete_model_class(\n             and `raise_errors=True`.\n     \"\"\"\n     typevars_map = get_model_typevars_map(cls)\n+\n+    if not cls.__pydantic_fields_complete__:\n+        # Rebuild the model fields so we can get the NameError for the specific undefined annotation",
      "comment": "```suggestion\r\n        # Note: when coming from `ModelMetaclass.__new__()`, this results in fields being built twice.\r\n        # We do so a second time here so that we can get the `NameError` for the specific undefined annotation.\r\n        # Alternatively, we could let `GenerateSchema()` raise the error, but there are cases where incomplete\r\n        # fields are inherited in `collect_model_fields()` and can actually have their annotation resolved in the\r\n        # generate schema process. As we want to avoid having `__pydantic_fields_complete__` set to `False`\r\n        # when `__pydantic_complete__` is `True`, we rebuild here:\r\n```",
      "comment_id": 2048511273,
      "user": "Viicos",
      "created_at": "2025-04-17T08:41:25Z",
      "url": "https://github.com/pydantic/pydantic/pull/11759#discussion_r2048511273"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11991,
      "file_path": "pydantic/_internal/_fields.py",
      "line": 263,
      "side": "RIGHT",
      "diff_hunk": "@@ -259,7 +260,13 @@ def collect_model_fields(  # noqa: C901\n \n     # https://docs.python.org/3/howto/annotations.html#accessing-the-annotations-dict-of-an-object-in-python-3-9-and-older\n     # annotations is only used for finding fields in parent classes\n-    annotations = cls.__dict__.get('__annotations__', {})\n+    if sys.version_info >= (3, 14):",
      "comment": "I'll try to move this logic into a reusable function",
      "comment_id": 2190208996,
      "user": "Viicos",
      "created_at": "2025-07-07T14:01:01Z",
      "url": "https://github.com/pydantic/pydantic/pull/11991#discussion_r2190208996"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11991,
      "file_path": "pydantic/_internal/_fields.py",
      "line": 268,
      "side": "RIGHT",
      "diff_hunk": "@@ -259,7 +260,13 @@ def collect_model_fields(  # noqa: C901\n \n     # https://docs.python.org/3/howto/annotations.html#accessing-the-annotations-dict-of-an-object-in-python-3-9-and-older\n     # annotations is only used for finding fields in parent classes\n-    annotations = cls.__dict__.get('__annotations__', {})\n+    if sys.version_info >= (3, 14):\n+        from annotationlib import Format, get_annotations\n+\n+        annotations = get_annotations(cls, format=Format.FORWARDREF)\n+    else:\n+        annotations = cls.__dict__.get('__annotations__', {})",
      "comment": "Think there's a call to `_typing_extra.safe_get_annotations(cls)` still to be added here?\r\n\r\n```suggestion\r\n    annotations = _typing_extra.safe_get_annotations(cls)\r\n```",
      "comment_id": 2197049949,
      "user": "davidhewitt",
      "created_at": "2025-07-10T08:57:18Z",
      "url": "https://github.com/pydantic/pydantic/pull/11991#discussion_r2197049949"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 12008,
      "file_path": "pydantic/types.py",
      "line": 1532,
      "side": "RIGHT",
      "diff_hunk": "@@ -1530,7 +1530,7 @@ def __eq__(self, other: Any) -> bool:\n \n # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SECRET TYPES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n ",
      "comment": "```suggestion\r\n\r\n# The `Secret` class being conceptually immutable, make the type variable covariant:\r\n```",
      "comment_id": 2177759350,
      "user": "Viicos",
      "created_at": "2025-07-01T14:28:42Z",
      "url": "https://github.com/pydantic/pydantic/pull/12008#discussion_r2177759350"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11914,
      "file_path": "pydantic/_internal/_fields.py",
      "line": 285,
      "side": "RIGHT",
      "diff_hunk": "@@ -265,6 +282,20 @@ def collect_model_fields(  # noqa: C901\n             continue\n \n         assigned_value = getattr(cls, ann_name, PydanticUndefined)\n+        if (",
      "comment": "I think we can make this a bit more performant by skipping the checks if `assigned_values is PydanticUndefined`",
      "comment_id": 2143344719,
      "user": "DouweM",
      "created_at": "2025-06-12T18:00:55Z",
      "url": "https://github.com/pydantic/pydantic/pull/11914#discussion_r2143344719"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 10863,
      "file_path": "pydantic/main.py",
      "line": 688,
      "side": "LEFT",
      "diff_hunk": "@@ -671,30 +671,6 @@ def model_validate_strings(\n         __tracebackhide__ = True\n         return cls.__pydantic_validator__.validate_strings(obj, strict=strict, context=context)\n \n-    @classmethod\n-    def __get_pydantic_core_schema__(cls, source: type[BaseModel], handler: GetCoreSchemaHandler, /) -> CoreSchema:",
      "comment": "I dont think this is a good idea. This is definitely being relied on. Its not internal and also documented for customizing the schema.",
      "comment_id": 1847732698,
      "user": "MarkusSintonen",
      "created_at": "2024-11-19T06:50:48Z",
      "url": "https://github.com/pydantic/pydantic/pull/10863#discussion_r1847732698"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 10863,
      "file_path": "pydantic/main.py",
      "line": 688,
      "side": "LEFT",
      "diff_hunk": "@@ -671,30 +671,6 @@ def model_validate_strings(\n         __tracebackhide__ = True\n         return cls.__pydantic_validator__.validate_strings(obj, strict=strict, context=context)\n \n-    @classmethod\n-    def __get_pydantic_core_schema__(cls, source: type[BaseModel], handler: GetCoreSchemaHandler, /) -> CoreSchema:",
      "comment": "But the perf improvement could be maybe done so that the `__get_pydantic_core_schema__` can not be overriden but instead coming from model_config (with similar args). Users then have option to migrate into that. Then if its defined in model config the perf improvements are not applied. (Didnt fully follow why its causing issues here but maybe possible)",
      "comment_id": 1847824205,
      "user": "MarkusSintonen",
      "created_at": "2024-11-19T07:52:34Z",
      "url": "https://github.com/pydantic/pydantic/pull/10863#discussion_r1847824205"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 10863,
      "file_path": "pydantic/_internal/_core_utils.py",
      "line": 70,
      "side": "RIGHT",
      "diff_hunk": "@@ -66,6 +67,7 @@ def is_list_like_schema_with_items_schema(\n     return schema['type'] in _LIST_LIKE_SCHEMA_WITH_ITEMS_TYPES\n \n \n+@lru_cache",
      "comment": "Should it be some weak cache like in generics side to avoid (dynamic cough cough) models being leaked.",
      "comment_id": 1847840249,
      "user": "MarkusSintonen",
      "created_at": "2024-11-19T07:56:38Z",
      "url": "https://github.com/pydantic/pydantic/pull/10863#discussion_r1847840249"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 10863,
      "file_path": "pydantic/_internal/_core_utils.py",
      "line": 70,
      "side": "RIGHT",
      "diff_hunk": "@@ -66,6 +67,7 @@ def is_list_like_schema_with_items_schema(\n     return schema['type'] in _LIST_LIKE_SCHEMA_WITH_ITEMS_TYPES\n \n \n+@lru_cache",
      "comment": "Yeah, I currently went with the default maxsize here (128) to avoid memory leaks, but this can still hold references to types/classes that should be gc'd, and with a small application where you have less than 128 referenceable types, this will never reach the limit and thus keep a reference to all of them.\r\n\r\nIt's a bit annoying that the `functools` cache utilities are limited in that aspect. It would be great if we could define our own cache key (currently hash) and customize the internal cache implementation (currently a dict). The `cachetools` 3rd party lib states [it supports](https://cachetools.readthedocs.io/en/latest/#cachetools.cached) using a `WeakValueDictionary`.\r\n\r\nOn the K8S file, with the cache impl, `get_type_ref` takes 30ms/~7s, without it takes 100ms/~7s, so I think we can live without it. Alternatively, we could a class level cached property on Pydantic models so that we only compute the reference once, I can take a look. ",
      "comment_id": 1848279501,
      "user": "Viicos",
      "created_at": "2024-11-19T12:33:59Z",
      "url": "https://github.com/pydantic/pydantic/pull/10863#discussion_r1848279501"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 10863,
      "file_path": "pydantic/main.py",
      "line": 688,
      "side": "LEFT",
      "diff_hunk": "@@ -671,30 +671,6 @@ def model_validate_strings(\n         __tracebackhide__ = True\n         return cls.__pydantic_validator__.validate_strings(obj, strict=strict, context=context)\n \n-    @classmethod\n-    def __get_pydantic_core_schema__(cls, source: type[BaseModel], handler: GetCoreSchemaHandler, /) -> CoreSchema:",
      "comment": "To be clear, this doesn't remove support for `__get_pydantic_core_schema__`. This is still handled by the added `_generate_schema_from_get_schema_method` method.\r\n\r\nUsers are still free to implement `__get_pydantic_core_schema__` on Pydantic models, however we provide no guarantees (and never did, see the _breaking changes concerns_ section in my original post) it will work flawlessly. ",
      "comment_id": 1848282518,
      "user": "Viicos",
      "created_at": "2024-11-19T12:36:05Z",
      "url": "https://github.com/pydantic/pydantic/pull/10863#discussion_r1848282518"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 10863,
      "file_path": "pydantic/main.py",
      "line": 688,
      "side": "LEFT",
      "diff_hunk": "@@ -671,30 +671,6 @@ def model_validate_strings(\n         __tracebackhide__ = True\n         return cls.__pydantic_validator__.validate_strings(obj, strict=strict, context=context)\n \n-    @classmethod\n-    def __get_pydantic_core_schema__(cls, source: type[BaseModel], handler: GetCoreSchemaHandler, /) -> CoreSchema:",
      "comment": "Ah interesting. Will give this another review then. Didn't realize users could still implement support. Nice!",
      "comment_id": 1848312831,
      "user": "sydney-runkle",
      "created_at": "2024-11-19T12:56:44Z",
      "url": "https://github.com/pydantic/pydantic/pull/10863#discussion_r1848312831"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 10863,
      "file_path": "pydantic/_internal/_generate_schema.py",
      "line": 839,
      "side": "RIGHT",
      "diff_hunk": "@@ -861,6 +836,9 @@ def _get_first_two_args_or_any(self, obj: Any) -> tuple[Any, Any]:\n         return args[0], args[1]\n \n     def _generate_schema_inner(self, obj: Any) -> core_schema.CoreSchema:\n+        if _typing_extra.is_self(obj):\n+            obj = self._resolve_self_type(obj)\n+",
      "comment": "Just curious, why do we need to add this here now?",
      "comment_id": 1848385488,
      "user": "sydney-runkle",
      "created_at": "2024-11-19T13:39:27Z",
      "url": "https://github.com/pydantic/pydantic/pull/10863#discussion_r1848385488"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 10863,
      "file_path": "pydantic/_internal/_generate_schema.py",
      "line": 1724,
      "side": "RIGHT",
      "diff_hunk": "@@ -1734,6 +1712,16 @@ def _dataclass_schema(\n             if maybe_schema is not None:\n                 return maybe_schema\n \n+            schema = dataclass.__dict__.get('__pydantic_core_schema__')\n+            if schema is not None and not isinstance(schema, MockCoreSchema):\n+                schema = self._unpack_refs_defs(schema)\n+                ref = get_ref(schema)\n+                if ref:\n+                    self.defs.definitions[ref] = schema\n+                    return core_schema.definition_reference_schema(ref)\n+                else:\n+                    return schema",
      "comment": "Let's consolidate this shared pattern to avoid potential inconsistencies across models, dataclasses, etc",
      "comment_id": 1848387102,
      "user": "sydney-runkle",
      "created_at": "2024-11-19T13:40:25Z",
      "url": "https://github.com/pydantic/pydantic/pull/10863#discussion_r1848387102"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 10863,
      "file_path": "pydantic/_internal/_generate_schema.py",
      "line": 2037,
      "side": "RIGHT",
      "diff_hunk": "@@ -2032,11 +2020,12 @@ def _apply_annotations(\n         pydantic_js_annotation_functions: list[GetJsonSchemaFunction] = []\n \n         def inner_handler(obj: Any) -> CoreSchema:\n-            from_property = self._generate_schema_from_property(obj, source_type)\n-            if from_property is None:\n+            # TODO can obj be != source? if not, then just call self.generate_schema\n+            schema = self._generate_schema_from_get_schema_method(obj, source_type)\n+\n+            if schema is None:\n                 schema = self._generate_schema_inner(obj)",
      "comment": "Have you tried with the `self.generate_schema` approach? Seems like it would make more sense here...",
      "comment_id": 1848389752,
      "user": "sydney-runkle",
      "created_at": "2024-11-19T13:41:59Z",
      "url": "https://github.com/pydantic/pydantic/pull/10863#discussion_r1848389752"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 10863,
      "file_path": "pydantic/_internal/_generate_schema.py",
      "line": 627,
      "side": "RIGHT",
      "diff_hunk": "@@ -627,6 +618,23 @@ def _model_schema(self, cls: type[BaseModel]) -> core_schema.CoreSchema:\n             if maybe_schema is not None:\n                 return maybe_schema\n \n+            schema = cls.__dict__.get('__pydantic_core_schema__')\n+            if (\n+                schema is not None\n+                and not isinstance(schema, MockCoreSchema)\n+                # Due to the way generic classes are built, it's possible that an invalid schema may be temporarily\n+                # set on generic classes. Probably we could resolve this to ensure that we get proper schema caching\n+                # for generics, but for simplicity for now, we just always rebuild if the class has a generic origin:\n+                and not cls.__pydantic_generic_metadata__['origin']\n+            ):",
      "comment": "@MarkusSintonen did a good job at intuitively extracting some of this logic as follows:\r\n\r\n```py\r\ndef get_existing_core_schema(obj: Any) -> core_schema.CoreSchema | None:\r\n    # Only use the cached value from this _exact_ class; we don't want one from a parent class\r\n    # This is why we check `cls.__dict__` and don't use `cls.__pydantic_core_schema__` or similar.\r\n    if (\r\n        hasattr(obj, '__dict__')\r\n        and (existing_schema := obj.__dict__.get('__pydantic_core_schema__')) is not None\r\n        and not isinstance(existing_schema, MockCoreSchema)\r\n    ):\r\n        return existing_schema\r\n    return None\r\n```\r\n\r\nMaybe we could eagerly pull changes like that into this PR, given that https://github.com/pydantic/pydantic/pull/10655 isn't quite ready to merge yet?",
      "comment_id": 1848393876,
      "user": "sydney-runkle",
      "created_at": "2024-11-19T13:44:44Z",
      "url": "https://github.com/pydantic/pydantic/pull/10863#discussion_r1848393876"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 10863,
      "file_path": "pydantic/_internal/_generate_schema.py",
      "line": 873,
      "side": "LEFT",
      "diff_hunk": "@@ -783,25 +776,7 @@ def _generate_schema_from_property(self, obj: Any, source: Any) -> core_schema.C\n                     '`__get_validators__` is deprecated and will be removed, use `__get_pydantic_core_schema__` instead.',\n                     PydanticDeprecatedSince20,\n                 )\n-            schema = core_schema.chain_schema([core_schema.with_info_plain_validator_function(v) for v in validators()])\n-        else:\n-            # we have no existing schema information on the property, exit early so that we can go generate a schema\n-            return None\n-\n-        schema = self._unpack_refs_defs(schema)\n-\n-        if is_function_with_inner_schema(schema):\n-            ref = schema['schema'].pop('ref', None)  # pyright: ignore[reportCallIssue, reportArgumentType]\n-            if ref:\n-                schema['ref'] = ref\n-        else:\n-            ref = get_ref(schema)",
      "comment": "Seems like we lost this logic - is this needed anywhere?",
      "comment_id": 1848394960,
      "user": "sydney-runkle",
      "created_at": "2024-11-19T13:45:29Z",
      "url": "https://github.com/pydantic/pydantic/pull/10863#discussion_r1848394960"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 10863,
      "file_path": "pydantic/_internal/_generate_schema.py",
      "line": 839,
      "side": "RIGHT",
      "diff_hunk": "@@ -861,6 +836,9 @@ def _get_first_two_args_or_any(self, obj: Any) -> tuple[Any, Any]:\n         return args[0], args[1]\n \n     def _generate_schema_inner(self, obj: Any) -> core_schema.CoreSchema:\n+        if _typing_extra.is_self(obj):\n+            obj = self._resolve_self_type(obj)\n+",
      "comment": "This was present inside `_generate_schema_from_property` before, but actually I think it should come first. Whenever you call `generate_schema`, if we pass in `typing(_extensions).Self`, we need to resolve the type before trying to build the schema.\r\n\r\nI moved it at the top of `GenerateSchema.generate_schema`",
      "comment_id": 1848502818,
      "user": "Viicos",
      "created_at": "2024-11-19T14:47:35Z",
      "url": "https://github.com/pydantic/pydantic/pull/10863#discussion_r1848502818"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 10863,
      "file_path": "pydantic/_internal/_generate_schema.py",
      "line": 627,
      "side": "RIGHT",
      "diff_hunk": "@@ -627,6 +618,23 @@ def _model_schema(self, cls: type[BaseModel]) -> core_schema.CoreSchema:\n             if maybe_schema is not None:\n                 return maybe_schema\n \n+            schema = cls.__dict__.get('__pydantic_core_schema__')\n+            if (\n+                schema is not None\n+                and not isinstance(schema, MockCoreSchema)\n+                # Due to the way generic classes are built, it's possible that an invalid schema may be temporarily\n+                # set on generic classes. Probably we could resolve this to ensure that we get proper schema caching\n+                # for generics, but for simplicity for now, we just always rebuild if the class has a generic origin:\n+                and not cls.__pydantic_generic_metadata__['origin']\n+            ):",
      "comment": "I also need to check that `cls.__pydantic_generic_metadata__['origin']` is `None` for Pydantic models, so maybe it's best to keep the (small) duplication of code here.",
      "comment_id": 1848522677,
      "user": "Viicos",
      "created_at": "2024-11-19T14:58:26Z",
      "url": "https://github.com/pydantic/pydantic/pull/10863#discussion_r1848522677"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 10863,
      "file_path": "pydantic/_internal/_generate_schema.py",
      "line": 1724,
      "side": "RIGHT",
      "diff_hunk": "@@ -1734,6 +1712,16 @@ def _dataclass_schema(\n             if maybe_schema is not None:\n                 return maybe_schema\n \n+            schema = dataclass.__dict__.get('__pydantic_core_schema__')\n+            if schema is not None and not isinstance(schema, MockCoreSchema):\n+                schema = self._unpack_refs_defs(schema)\n+                ref = get_ref(schema)\n+                if ref:\n+                    self.defs.definitions[ref] = schema\n+                    return core_schema.definition_reference_schema(ref)\n+                else:\n+                    return schema",
      "comment": "This certainly should be in a shared pattern, but seems like https://github.com/pydantic/pydantic/pull/10655 does something similar already. Maybe it's best to have it implemented there when rebasing?",
      "comment_id": 1848525378,
      "user": "Viicos",
      "created_at": "2024-11-19T14:59:58Z",
      "url": "https://github.com/pydantic/pydantic/pull/10863#discussion_r1848525378"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 10863,
      "file_path": "pydantic/_internal/_generate_schema.py",
      "line": 1724,
      "side": "RIGHT",
      "diff_hunk": "@@ -1734,6 +1712,16 @@ def _dataclass_schema(\n             if maybe_schema is not None:\n                 return maybe_schema\n \n+            schema = dataclass.__dict__.get('__pydantic_core_schema__')\n+            if schema is not None and not isinstance(schema, MockCoreSchema):\n+                schema = self._unpack_refs_defs(schema)\n+                ref = get_ref(schema)\n+                if ref:\n+                    self.defs.definitions[ref] = schema\n+                    return core_schema.definition_reference_schema(ref)\n+                else:\n+                    return schema",
      "comment": "Hmm I'd rather have it shared now, then that refactor should look more simple in the long run? That PR is already big enough \ud83d\ude05 ",
      "comment_id": 1848547791,
      "user": "sydney-runkle",
      "created_at": "2024-11-19T15:12:54Z",
      "url": "https://github.com/pydantic/pydantic/pull/10863#discussion_r1848547791"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 10863,
      "file_path": "pydantic/_internal/_generate_schema.py",
      "line": 2037,
      "side": "RIGHT",
      "diff_hunk": "@@ -2032,11 +2020,12 @@ def _apply_annotations(\n         pydantic_js_annotation_functions: list[GetJsonSchemaFunction] = []\n \n         def inner_handler(obj: Any) -> CoreSchema:\n-            from_property = self._generate_schema_from_property(obj, source_type)\n-            if from_property is None:\n+            # TODO can obj be != source? if not, then just call self.generate_schema\n+            schema = self._generate_schema_from_get_schema_method(obj, source_type)\n+\n+            if schema is None:\n                 schema = self._generate_schema_inner(obj)",
      "comment": "Turns out there's some cases where `obj != source_type` :/",
      "comment_id": 1848712466,
      "user": "Viicos",
      "created_at": "2024-11-19T16:47:26Z",
      "url": "https://github.com/pydantic/pydantic/pull/10863#discussion_r1848712466"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 10863,
      "file_path": "pydantic/_internal/_core_utils.py",
      "line": 70,
      "side": "RIGHT",
      "diff_hunk": "@@ -66,6 +67,7 @@ def is_list_like_schema_with_items_schema(\n     return schema['type'] in _LIST_LIKE_SCHEMA_WITH_ITEMS_TYPES\n \n \n+@lru_cache",
      "comment": "It could be maybe simple as:\r\n\r\n```python\r\nclass _TypeRefCache(WeakKeyDictionary):\r\n    def __setitem__(self, key: type[Any], value: Any) -> None:\r\n        size = len(self)\r\n        if size >= 1000:\r\n            for remove_key in [*self.keys()][: size // 10]:  # Remove 10% of the cache (FIFO)\r\n                del self[remove_key]\r\n        super().__setitem__(key, value)\r\n\r\n\r\n_TYPE_REF_CACHE = _TypeRefCache()\r\n\r\n\r\ndef get_type_ref(type_: type[Any], args_override: tuple[type[Any], ...] | None = None) -> str:\r\n    \"\"\"....\"\"\"\r\n    if isinstance(type_, type) and not args_override:\r\n        if (cached := _TYPE_REF_CACHE.get(type_)) is not None:\r\n            return cached\r\n        type_ref = _get_type_ref(type_, args_override)\r\n        _TYPE_REF_CACHE[type_] = type_ref\r\n        return type_ref\r\n    else:\r\n        return _get_type_ref(type_, args_override)\r\n\r\n\r\ndef _get_type_ref(type_: type[Any], args_override: tuple[type[Any], ...] | None = None) -> str:\r\n    ... rest of stuff\r\n```",
      "comment_id": 1848837011,
      "user": "MarkusSintonen",
      "created_at": "2024-11-19T18:15:02Z",
      "url": "https://github.com/pydantic/pydantic/pull/10863#discussion_r1848837011"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 10863,
      "file_path": "pydantic/_internal/_core_utils.py",
      "line": 70,
      "side": "RIGHT",
      "diff_hunk": "@@ -66,6 +67,7 @@ def is_list_like_schema_with_items_schema(\n     return schema['type'] in _LIST_LIKE_SCHEMA_WITH_ITEMS_TYPES\n \n \n+@lru_cache",
      "comment": "A better API would be for each type to compute the ref, as currently `get_type_ref` is doing useless work as it needs to account for type alias types, dataclasses, etc. Instead, we should be able to decouple everything from this function. Postponing for now.",
      "comment_id": 1848965033,
      "user": "Viicos",
      "created_at": "2024-11-19T19:42:13Z",
      "url": "https://github.com/pydantic/pydantic/pull/10863#discussion_r1848965033"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 10863,
      "file_path": "pydantic/_internal/_generate_schema.py",
      "line": 839,
      "side": "RIGHT",
      "diff_hunk": "@@ -861,6 +836,9 @@ def _get_first_two_args_or_any(self, obj: Any) -> tuple[Any, Any]:\n         return args[0], args[1]\n \n     def _generate_schema_inner(self, obj: Any) -> core_schema.CoreSchema:\n+        if _typing_extra.is_self(obj):\n+            obj = self._resolve_self_type(obj)\n+",
      "comment": "Oops, seems like moving it breaks things, it needs to be right after the `__get_pydantic_core_schema__` check, so I'll leave it here",
      "comment_id": 1850672364,
      "user": "Viicos",
      "created_at": "2024-11-20T16:51:04Z",
      "url": "https://github.com/pydantic/pydantic/pull/10863#discussion_r1850672364"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 10863,
      "file_path": "pydantic/main.py",
      "line": 697,
      "side": "RIGHT",
      "diff_hunk": "@@ -687,23 +687,17 @@ def model_validate_strings(\n \n     @classmethod\n     def __get_pydantic_core_schema__(cls, source: type[BaseModel], handler: GetCoreSchemaHandler, /) -> CoreSchema:\n-        \"\"\"Hook into generating the model's CoreSchema.\n-\n-        Args:\n-            source: The class we are generating a schema for.\n-                This will generally be the same as the `cls` argument if this is a classmethod.\n-            handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n-\n-        Returns:\n-            A `pydantic-core` `CoreSchema`.\n-        \"\"\"\n-        # Only use the cached value from this _exact_ class; we don't want one from a parent class\n-        # This is why we check `cls.__dict__` and don't use `cls.__pydantic_core_schema__` or similar.\n+        warnings.warn(\n+            'The `__get_pydantic_core_schema__` method of the `BaseModel` class is deprecated. If you are calling '\n+            '`super().__get_pydantic_core_schema__` when overriding the method on a Pydantic model, consider using '\n+            '`handler(source)` instead. However, note that overriding this method on models can lead to unexpected '\n+            'side effects.',\n+            PydanticDeprecatedSince211,\n+            stacklevel=2,\n+        )",
      "comment": "As discussed in person, let's add a comment that this warning is only omitted when calling super. I think we need to make it more clear what the consequences of this deprecation are.",
      "comment_id": 1914654183,
      "user": "sydney-runkle",
      "created_at": "2025-01-14T11:20:16Z",
      "url": "https://github.com/pydantic/pydantic/pull/10863#discussion_r1914654183"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 10863,
      "file_path": "pydantic/_internal/_generate_schema.py",
      "line": 873,
      "side": "LEFT",
      "diff_hunk": "@@ -783,25 +776,7 @@ def _generate_schema_from_property(self, obj: Any, source: Any) -> core_schema.C\n                     '`__get_validators__` is deprecated and will be removed, use `__get_pydantic_core_schema__` instead.',\n                     PydanticDeprecatedSince20,\n                 )\n-            schema = core_schema.chain_schema([core_schema.with_info_plain_validator_function(v) for v in validators()])\n-        else:\n-            # we have no existing schema information on the property, exit early so that we can go generate a schema\n-            return None\n-\n-        schema = self._unpack_refs_defs(schema)\n-\n-        if is_function_with_inner_schema(schema):\n-            ref = schema['schema'].pop('ref', None)  # pyright: ignore[reportCallIssue, reportArgumentType]\n-            if ref:\n-                schema['ref'] = ref\n-        else:\n-            ref = get_ref(schema)",
      "comment": "Would it make sense to break this change out into a different PR?",
      "comment_id": 1914860077,
      "user": "sydney-runkle",
      "created_at": "2025-01-14T13:58:28Z",
      "url": "https://github.com/pydantic/pydantic/pull/10863#discussion_r1914860077"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 10863,
      "file_path": "pydantic/_internal/_generate_schema.py",
      "line": 1830,
      "side": "RIGHT",
      "diff_hunk": "@@ -1817,6 +1819,16 @@ def _dataclass_schema(\n             if maybe_schema is not None:\n                 return maybe_schema\n \n+            schema = dataclass.__dict__.get('__pydantic_core_schema__')\n+            if schema is not None and not isinstance(schema, MockCoreSchema):\n+                if schema['type'] == 'definitions':\n+                    schema = self.defs.unpack_definitions(schema)\n+                ref = get_ref(schema)\n+                if ref:\n+                    return self.defs.create_definition_reference_schema(schema)\n+                else:\n+                    return schema",
      "comment": "Does it make sense to do this in a different PR as well?",
      "comment_id": 1914864884,
      "user": "sydney-runkle",
      "created_at": "2025-01-14T14:01:40Z",
      "url": "https://github.com/pydantic/pydantic/pull/10863#discussion_r1914864884"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 10863,
      "file_path": "pydantic/_internal/_generate_schema.py",
      "line": 873,
      "side": "LEFT",
      "diff_hunk": "@@ -783,25 +776,7 @@ def _generate_schema_from_property(self, obj: Any, source: Any) -> core_schema.C\n                     '`__get_validators__` is deprecated and will be removed, use `__get_pydantic_core_schema__` instead.',\n                     PydanticDeprecatedSince20,\n                 )\n-            schema = core_schema.chain_schema([core_schema.with_info_plain_validator_function(v) for v in validators()])\n-        else:\n-            # we have no existing schema information on the property, exit early so that we can go generate a schema\n-            return None\n-\n-        schema = self._unpack_refs_defs(schema)\n-\n-        if is_function_with_inner_schema(schema):\n-            ref = schema['schema'].pop('ref', None)  # pyright: ignore[reportCallIssue, reportArgumentType]\n-            if ref:\n-                schema['ref'] = ref\n-        else:\n-            ref = get_ref(schema)",
      "comment": "iirc (but I'm not sure), I was able to remove it only thanks to the other changes. This won't clutter the git diff though, because it's just a removal. Probably by having a proper commit description when merging, I can add a note about this?",
      "comment_id": 1916202889,
      "user": "Viicos",
      "created_at": "2025-01-15T09:13:01Z",
      "url": "https://github.com/pydantic/pydantic/pull/10863#discussion_r1916202889"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 10863,
      "file_path": "pydantic/_internal/_generate_schema.py",
      "line": 1830,
      "side": "RIGHT",
      "diff_hunk": "@@ -1817,6 +1819,16 @@ def _dataclass_schema(\n             if maybe_schema is not None:\n                 return maybe_schema\n \n+            schema = dataclass.__dict__.get('__pydantic_core_schema__')\n+            if schema is not None and not isinstance(schema, MockCoreSchema):\n+                if schema['type'] == 'definitions':\n+                    schema = self.defs.unpack_definitions(schema)\n+                ref = get_ref(schema)\n+                if ref:\n+                    return self.defs.create_definition_reference_schema(schema)\n+                else:\n+                    return schema",
      "comment": "It has to be done together with the rest of the changes iirc, because in `pydantic/_internal/_dataclasses.py` we also have the `from_dunder_get_core_schema` logic which was removed",
      "comment_id": 1916214847,
      "user": "Viicos",
      "created_at": "2025-01-15T09:20:15Z",
      "url": "https://github.com/pydantic/pydantic/pull/10863#discussion_r1916214847"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11941,
      "file_path": "pydantic/types.py",
      "line": 3108,
      "side": "RIGHT",
      "diff_hunk": "@@ -3093,10 +3095,23 @@ def _convert_schema(self, original_schema: core_schema.CoreSchema) -> core_schem\n             if metadata is not None:\n                 tag = metadata.get('pydantic_internal_union_tag_key') or tag\n             if tag is None:\n-                raise PydanticUserError(\n-                    f'`Tag` not provided for choice {choice} used with `Discriminator`',\n-                    code='callable-discriminator-no-tag',\n-                )\n+                # `handler` is None when this method is called from `apply_discriminator()` (deferred discriminators)\n+                if handler is not None and choice['type'] == 'definition-ref':\n+                    # If choice was built from a PEP 695 type alias, try to resolve the def:\n+                    try:\n+                        choice = handler.resolve_ref_schema(choice)\n+                    except LookupError:\n+                        pass\n+                    else:\n+                        metadata = cast('CoreMetadata | None', choice.get('metadata'))\n+                        if metadata is not None:\n+                            tag = metadata.get('pydantic_internal_union_tag_key') or tag",
      "comment": "`tag` is known to be `None` here so we don't need the `or tag`",
      "comment_id": 2143351247,
      "user": "DouweM",
      "created_at": "2025-06-12T18:05:37Z",
      "url": "https://github.com/pydantic/pydantic/pull/11941#discussion_r2143351247"
    },
    {
      "repo": "pydantic/pydantic",
      "pr_number": 11902,
      "file_path": "pydantic/fields.py",
      "line": 598,
      "side": "RIGHT",
      "diff_hunk": "@@ -588,6 +588,15 @@ def _collect_metadata(kwargs: dict[str, Any]) -> list[Any]:\n             metadata.append(_fields.pydantic_general_metadata(**general_metadata))\n         return metadata\n \n+    def _copy(self) -> Self:\n+        copied = copy(self)\n+        for attr_name in ('metadata', '_attributes_set', '_qualifiers'):\n+            # Apply \"deep-copy\" behavior on collections attributes:\n+            value = getattr(copied, attr_name).copy()\n+            setattr(copied, attr_name, value)\n+\n+        return copied",
      "comment": "We can't define a custom `__copy__()`, because I couldn't find a way to delegate to `copy.copy()` and then apply the special case for `metadata`, `_attributes_set` and `_qualifiers`.\r\n\r\nThe following could still be done:\r\n\r\n```python\r\n    def __copy__(self) -> Self:\r\n        cls = type(self)\r\n        copied = cls()\r\n        for attr_name in cls.__slots__:\r\n            value = getattr(self, attr_name)\r\n            if attr_name in ('metadata', '_attributes_set', '_qualifiers'):\r\n                # Apply \"deep-copy\" behavior on collections attributes:\r\n                value = value.copy()\r\n            setattr(copied, attr_name, value)\r\n\r\n        return copied\r\n```\r\n\r\nBut this blows up on libraries (FastAPI/SQLModel) subclassing `FieldInfo` (not the first time this is causing issues..) as we don't know which extra attributes are defined on these classes.",
      "comment_id": 2103354317,
      "user": "Viicos",
      "created_at": "2025-05-22T20:31:22Z",
      "url": "https://github.com/pydantic/pydantic/pull/11902#discussion_r2103354317"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6724,
      "file_path": "src/requests/adapters.py",
      "line": 113,
      "side": "RIGHT",
      "diff_hunk": "@@ -90,13 +98,19 @@ def _urllib3_request_context(\n     parsed_request_url = urlparse(request.url)\n     scheme = parsed_request_url.scheme.lower()\n     port = parsed_request_url.port\n+\n+    # Determine if we have and should use our default SSLContext\n+    # to optimize performance on standard requests.\n     poolmanager_kwargs = getattr(poolmanager, \"connection_pool_kw\", {})\n     has_poolmanager_ssl_context = poolmanager_kwargs.get(\"ssl_context\")\n+    should_use_default_ssl_context = (\n+        _preloaded_ssl_context is not None and not has_poolmanager_ssl_context\n+    )\n \n     cert_reqs = \"CERT_REQUIRED\"\n     if verify is False:\n         cert_reqs = \"CERT_NONE\"\n-    elif verify is True and not has_poolmanager_ssl_context:\n+    elif verify is True and should_use_default_ssl_context:",
      "comment": "Missed this earlier but do we need any logic for verify=True without a condition here? ",
      "comment_id": 1617854876,
      "user": "sigmavirus24",
      "created_at": "2024-05-28T20:27:08Z",
      "url": "https://github.com/psf/requests/pull/6724#discussion_r1617854876"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6724,
      "file_path": "src/requests/adapters.py",
      "line": 113,
      "side": "RIGHT",
      "diff_hunk": "@@ -90,13 +98,19 @@ def _urllib3_request_context(\n     parsed_request_url = urlparse(request.url)\n     scheme = parsed_request_url.scheme.lower()\n     port = parsed_request_url.port\n+\n+    # Determine if we have and should use our default SSLContext\n+    # to optimize performance on standard requests.\n     poolmanager_kwargs = getattr(poolmanager, \"connection_pool_kw\", {})\n     has_poolmanager_ssl_context = poolmanager_kwargs.get(\"ssl_context\")\n+    should_use_default_ssl_context = (\n+        _preloaded_ssl_context is not None and not has_poolmanager_ssl_context\n+    )\n \n     cert_reqs = \"CERT_REQUIRED\"\n     if verify is False:\n         cert_reqs = \"CERT_NONE\"\n-    elif verify is True and not has_poolmanager_ssl_context:\n+    elif verify is True and should_use_default_ssl_context:",
      "comment": "Are you asking about the behavior if `verify is True` but `should_use_default_ssl_context` is False? This was a no-op prior to #6655 which fellback to letting the SSLContext be created when fetching the connection from the pool. [`_ssl_wrap_socket_and_match_hostname`](https://github.com/urllib3/urllib3/blob/b07a669bd970d69847801148286b726f0570b625/src/urllib3/connection.py#L755-L765) handles this for us, so I don't know if there's any additional concern from our previous behavior unless I'm missing part of the question.",
      "comment_id": 1617955993,
      "user": "nateprewitt",
      "created_at": "2024-05-28T22:28:51Z",
      "url": "https://github.com/psf/requests/pull/6724#discussion_r1617955993"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6724,
      "file_path": "src/requests/adapters.py",
      "line": 113,
      "side": "RIGHT",
      "diff_hunk": "@@ -90,13 +98,19 @@ def _urllib3_request_context(\n     parsed_request_url = urlparse(request.url)\n     scheme = parsed_request_url.scheme.lower()\n     port = parsed_request_url.port\n+\n+    # Determine if we have and should use our default SSLContext\n+    # to optimize performance on standard requests.\n     poolmanager_kwargs = getattr(poolmanager, \"connection_pool_kw\", {})\n     has_poolmanager_ssl_context = poolmanager_kwargs.get(\"ssl_context\")\n+    should_use_default_ssl_context = (\n+        _preloaded_ssl_context is not None and not has_poolmanager_ssl_context\n+    )\n \n     cert_reqs = \"CERT_REQUIRED\"\n     if verify is False:\n         cert_reqs = \"CERT_NONE\"\n-    elif verify is True and not has_poolmanager_ssl_context:\n+    elif verify is True and should_use_default_ssl_context:",
      "comment": "Makes sense. I've a terrible headache so just looking at the branches and concerned about missing something is all. ",
      "comment_id": 1618008653,
      "user": "sigmavirus24",
      "created_at": "2024-05-28T23:58:29Z",
      "url": "https://github.com/psf/requests/pull/6724#discussion_r1618008653"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6724,
      "file_path": "src/requests/adapters.py",
      "line": 113,
      "side": "RIGHT",
      "diff_hunk": "@@ -90,13 +98,19 @@ def _urllib3_request_context(\n     parsed_request_url = urlparse(request.url)\n     scheme = parsed_request_url.scheme.lower()\n     port = parsed_request_url.port\n+\n+    # Determine if we have and should use our default SSLContext\n+    # to optimize performance on standard requests.\n     poolmanager_kwargs = getattr(poolmanager, \"connection_pool_kw\", {})\n     has_poolmanager_ssl_context = poolmanager_kwargs.get(\"ssl_context\")\n+    should_use_default_ssl_context = (\n+        _preloaded_ssl_context is not None and not has_poolmanager_ssl_context\n+    )\n \n     cert_reqs = \"CERT_REQUIRED\"\n     if verify is False:\n         cert_reqs = \"CERT_NONE\"\n-    elif verify is True and not has_poolmanager_ssl_context:\n+    elif verify is True and should_use_default_ssl_context:",
      "comment": "Nope, that's a good call out. I'll give it one more look before merging but I think we're ok for that case.",
      "comment_id": 1618009506,
      "user": "nateprewitt",
      "created_at": "2024-05-29T00:00:07Z",
      "url": "https://github.com/psf/requests/pull/6724#discussion_r1618009506"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6724,
      "file_path": "src/requests/adapters.py",
      "line": 113,
      "side": "RIGHT",
      "diff_hunk": "@@ -90,13 +98,19 @@ def _urllib3_request_context(\n     parsed_request_url = urlparse(request.url)\n     scheme = parsed_request_url.scheme.lower()\n     port = parsed_request_url.port\n+\n+    # Determine if we have and should use our default SSLContext\n+    # to optimize performance on standard requests.\n     poolmanager_kwargs = getattr(poolmanager, \"connection_pool_kw\", {})\n     has_poolmanager_ssl_context = poolmanager_kwargs.get(\"ssl_context\")\n+    should_use_default_ssl_context = (\n+        _preloaded_ssl_context is not None and not has_poolmanager_ssl_context\n+    )\n \n     cert_reqs = \"CERT_REQUIRED\"\n     if verify is False:\n         cert_reqs = \"CERT_NONE\"\n-    elif verify is True and not has_poolmanager_ssl_context:\n+    elif verify is True and should_use_default_ssl_context:",
      "comment": "Oh, looking again, not on a phone, and without a migraine, I see the `cert_reqs = \"CERT_REQUIRED\"` line on L110 that I clearly wrote, and which is what I was thinking we may want to be concerned about. So, I'm not nearly as worried.",
      "comment_id": 1619000165,
      "user": "sigmavirus24",
      "created_at": "2024-05-29T14:31:10Z",
      "url": "https://github.com/psf/requests/pull/6724#discussion_r1619000165"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6724,
      "file_path": "src/requests/adapters.py",
      "line": 116,
      "side": "RIGHT",
      "diff_hunk": "@@ -90,13 +98,19 @@ def _urllib3_request_context(\n     parsed_request_url = urlparse(request.url)\n     scheme = parsed_request_url.scheme.lower()\n     port = parsed_request_url.port\n+\n+    # Determine if we have and should use our default SSLContext\n+    # to optimize performance on standard requests.\n     poolmanager_kwargs = getattr(poolmanager, \"connection_pool_kw\", {})\n     has_poolmanager_ssl_context = poolmanager_kwargs.get(\"ssl_context\")\n+    should_use_default_ssl_context = (\n+        _preloaded_ssl_context is not None and not has_poolmanager_ssl_context\n+    )\n \n     cert_reqs = \"CERT_REQUIRED\"\n     if verify is False:\n         cert_reqs = \"CERT_NONE\"\n-    elif verify is True and not has_poolmanager_ssl_context:\n+    elif verify is True and should_use_default_ssl_context:\n         pool_kwargs[\"ssl_context\"] = _preloaded_ssl_context\n     elif isinstance(verify, str):\n         if not os.path.isdir(verify):",
      "comment": "Looking at this again with something of a performance mindset, do we want to cache somehow lookups to `isdir`? I'm not sure this hurts us at all, but just thinking about things that could slow us down in certain cases now. (Not for this pull request, just putting it out there)",
      "comment_id": 1619003075,
      "user": "sigmavirus24",
      "created_at": "2024-05-29T14:32:37Z",
      "url": "https://github.com/psf/requests/pull/6724#discussion_r1619003075"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6724,
      "file_path": "src/requests/adapters.py",
      "line": 116,
      "side": "RIGHT",
      "diff_hunk": "@@ -90,13 +98,19 @@ def _urllib3_request_context(\n     parsed_request_url = urlparse(request.url)\n     scheme = parsed_request_url.scheme.lower()\n     port = parsed_request_url.port\n+\n+    # Determine if we have and should use our default SSLContext\n+    # to optimize performance on standard requests.\n     poolmanager_kwargs = getattr(poolmanager, \"connection_pool_kw\", {})\n     has_poolmanager_ssl_context = poolmanager_kwargs.get(\"ssl_context\")\n+    should_use_default_ssl_context = (\n+        _preloaded_ssl_context is not None and not has_poolmanager_ssl_context\n+    )\n \n     cert_reqs = \"CERT_REQUIRED\"\n     if verify is False:\n         cert_reqs = \"CERT_NONE\"\n-    elif verify is True and not has_poolmanager_ssl_context:\n+    elif verify is True and should_use_default_ssl_context:\n         pool_kwargs[\"ssl_context\"] = _preloaded_ssl_context\n     elif isinstance(verify, str):\n         if not os.path.isdir(verify):",
      "comment": "That seems like a reasonable optimization, I guess we'd need to check how much time we're actually spending on the dir check. I assume we'll get feedback if we have cases where this is a bottleneck.",
      "comment_id": 1619084935,
      "user": "nateprewitt",
      "created_at": "2024-05-29T15:23:20Z",
      "url": "https://github.com/psf/requests/pull/6724#discussion_r1619084935"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6667,
      "file_path": "src/requests/adapters.py",
      "line": 294,
      "side": "LEFT",
      "diff_hunk": "@@ -284,27 +284,26 @@ def cert_verify(self, conn, url, verify, cert):\n         :param cert: The SSL certificate to verify.\n         \"\"\"\n         if url.lower().startswith(\"https\") and verify:\n-            cert_loc = None\n+            conn.cert_reqs = \"CERT_REQUIRED\"\n \n-            # Allow self-specified cert location.\n+            # Only load the CA certificates if 'verify' is a string indicating the CA bundle to use.\n+            # Otherwise, if verify is a boolean, we don't load anything since\n+            # the connection will be using a context with the default certificates already loaded,\n+            # and this avoids a call to the slow load_verify_locations()\n             if verify is not True:\n+                # `verify` must be a str with a path then\n                 cert_loc = verify\n \n-            if not cert_loc:\n-                cert_loc = extract_zipped_paths(DEFAULT_CA_BUNDLE_PATH)",
      "comment": "This is actually critical behavior you're removing ",
      "comment_id": 1533138744,
      "user": "sigmavirus24",
      "created_at": "2024-03-21T01:45:42Z",
      "url": "https://github.com/psf/requests/pull/6667#discussion_r1533138744"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6667,
      "file_path": "src/requests/adapters.py",
      "line": 294,
      "side": "LEFT",
      "diff_hunk": "@@ -284,27 +284,26 @@ def cert_verify(self, conn, url, verify, cert):\n         :param cert: The SSL certificate to verify.\n         \"\"\"\n         if url.lower().startswith(\"https\") and verify:\n-            cert_loc = None\n+            conn.cert_reqs = \"CERT_REQUIRED\"\n \n-            # Allow self-specified cert location.\n+            # Only load the CA certificates if 'verify' is a string indicating the CA bundle to use.\n+            # Otherwise, if verify is a boolean, we don't load anything since\n+            # the connection will be using a context with the default certificates already loaded,\n+            # and this avoids a call to the slow load_verify_locations()\n             if verify is not True:\n+                # `verify` must be a str with a path then\n                 cert_loc = verify\n \n-            if not cert_loc:\n-                cert_loc = extract_zipped_paths(DEFAULT_CA_BUNDLE_PATH)",
      "comment": "Thanks for the catch, I had to adapt the patch quite a bit from what we're using right now and that inconsistency slipped by.\r\n\r\nI pushed a change to explicitly use `DEFAULT_CA_BUNDLE_PATH` when `verify=True`. This is done by creating a module-level `SSLContext` with that bundle already loaded, and instructing the connection pool to use that context when no custom bundle is specified. Since the server's cert is verified using the CA certificates loaded in the `SSLContext` used in the request, this should work.\r\n\r\nAgain, the goal is to avoid setting `ca_certs` or `ca_cert_dir` in the most common use case as it triggers another (in this case redundant) call to `load_verify_locations()` by urllib3:\r\n\r\n```py\r\nif ca_certs or ca_cert_dir or ca_cert_data:\r\n        try:\r\n            context.load_verify_locations(ca_certs, ca_cert_dir, ca_cert_data)\r\n```\r\n\r\nSince `DEFAULT_CA_BUNDLE_PATH` is sugar for `certifi.where()`, which in turn always returns the path to a single bundle file, I decided to skip checking for `os.path.isdir()` because it should always be False. If you're not comfortable with this please let me know and I'll change it.\r\n\r\nI also changed `_urllib3_request_context()` slightly to handle the case where `verify` is a path to a dir instead to a single file, as we should set `ca_cert_dir` instead of `ca_certs` in that case. I believe this is now fully redundant with the corresponding logic in `cert_verify()`.\r\n\r\nI tried to write corresponding tests to verify that the `SSLContext`s used in different scenarios have the correct certificates loaded, but I couldn't find a way to access such low-level information about a request in the exposed classes. If it's possible, please give me a few pointers and I'll be glad to expand the test suite.",
      "comment_id": 1533587993,
      "user": "agubelu",
      "created_at": "2024-03-21T10:14:37Z",
      "url": "https://github.com/psf/requests/pull/6667#discussion_r1533587993"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6655,
      "file_path": "src/requests/adapters.py",
      "line": 66,
      "side": "RIGHT",
      "diff_hunk": "@@ -61,12 +62,38 @@ def SOCKSProxyManager(*args, **kwargs):\n         raise InvalidSchema(\"Missing dependencies for SOCKS support.\")\n \n \n+if typing.TYPE_CHECKING:\n+    from .models import PreparedRequest",
      "comment": "Not against this, but I think this is the first time we're introducing typing into Requests. I'm curious if we want to start that or push it into typeshed since this will be precedent for future inline typing?",
      "comment_id": 1515206804,
      "user": "nateprewitt",
      "created_at": "2024-03-06T21:56:23Z",
      "url": "https://github.com/psf/requests/pull/6655#discussion_r1515206804"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6655,
      "file_path": "tests/test_requests.py",
      "line": 2836,
      "side": "RIGHT",
      "diff_hunk": "@@ -2828,6 +2828,13 @@ def test_status_code_425(self):\n         assert r5 == 425\n         assert r6 == 425\n \n+    def test_different_connection_pool_for_tls_settings(self):\n+        s = requests.Session()\n+        r1 = s.get(\"https://invalid.badssl.com\", verify=False)\n+        assert r1.status_code == 421\n+        with pytest.raises(requests.exceptions.SSLError):\n+            s.get(\"https://invalid.badssl.com\")",
      "comment": "There may not be a better way to test this but I don't know if we have other tests that require contacting a live site with TLS disabled. That may have some durability issues and means we're going to take the first response we get back. Probably minor, but figured I'd call it out.",
      "comment_id": 1518892861,
      "user": "nateprewitt",
      "created_at": "2024-03-10T16:33:36Z",
      "url": "https://github.com/psf/requests/pull/6655#discussion_r1518892861"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6655,
      "file_path": "src/requests/adapters.py",
      "line": 66,
      "side": "RIGHT",
      "diff_hunk": "@@ -61,12 +62,38 @@ def SOCKSProxyManager(*args, **kwargs):\n         raise InvalidSchema(\"Missing dependencies for SOCKS support.\")\n \n \n+if typing.TYPE_CHECKING:\n+    from .models import PreparedRequest",
      "comment": "This is for a private method (that I fully anticipate people abusing) but we're not advertising things are typed and so it's not something I'm concerned with. ",
      "comment_id": 1518900181,
      "user": "sigmavirus24",
      "created_at": "2024-03-10T17:11:39Z",
      "url": "https://github.com/psf/requests/pull/6655#discussion_r1518900181"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6655,
      "file_path": "tests/test_requests.py",
      "line": 2836,
      "side": "RIGHT",
      "diff_hunk": "@@ -2828,6 +2828,13 @@ def test_status_code_425(self):\n         assert r5 == 425\n         assert r6 == 425\n \n+    def test_different_connection_pool_for_tls_settings(self):\n+        s = requests.Session()\n+        r1 = s.get(\"https://invalid.badssl.com\", verify=False)\n+        assert r1.status_code == 421\n+        with pytest.raises(requests.exceptions.SSLError):\n+            s.get(\"https://invalid.badssl.com\")",
      "comment": "There are many alternatives here, but those are all significantly more effort and this shows the behaviour is fixed before and after handily. I'm sure Linux folks will get pissed but I'm not as bothered about finding time later to do this a different way after we have fixed this",
      "comment_id": 1518900465,
      "user": "sigmavirus24",
      "created_at": "2024-03-10T17:12:49Z",
      "url": "https://github.com/psf/requests/pull/6655#discussion_r1518900465"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6655,
      "file_path": "tests/test_requests.py",
      "line": 2836,
      "side": "RIGHT",
      "diff_hunk": "@@ -2828,6 +2828,13 @@ def test_status_code_425(self):\n         assert r5 == 425\n         assert r6 == 425\n \n+    def test_different_connection_pool_for_tls_settings(self):\n+        s = requests.Session()\n+        r1 = s.get(\"https://invalid.badssl.com\", verify=False)\n+        assert r1.status_code == 421\n+        with pytest.raises(requests.exceptions.SSLError):\n+            s.get(\"https://invalid.badssl.com\")",
      "comment": "I'll try to prioritize better (offline) tests soon",
      "comment_id": 1519559679,
      "user": "sigmavirus24",
      "created_at": "2024-03-11T11:21:54Z",
      "url": "https://github.com/psf/requests/pull/6655#discussion_r1519559679"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6767,
      "file_path": "src/requests/adapters.py",
      "line": 92,
      "side": "RIGHT",
      "diff_hunk": "@@ -99,19 +85,9 @@ def _urllib3_request_context(\n     scheme = parsed_request_url.scheme.lower()\n     port = parsed_request_url.port\n \n-    # Determine if we have and should use our default SSLContext\n-    # to optimize performance on standard requests.\n-    poolmanager_kwargs = getattr(poolmanager, \"connection_pool_kw\", {})\n-    has_poolmanager_ssl_context = poolmanager_kwargs.get(\"ssl_context\")\n-    should_use_default_ssl_context = (\n-        _preloaded_ssl_context is not None and not has_poolmanager_ssl_context\n-    )\n-\n     cert_reqs = \"CERT_REQUIRED\"\n     if verify is False:\n         cert_reqs = \"CERT_NONE\"\n-    elif verify is True and should_use_default_ssl_context:\n-        pool_kwargs[\"ssl_context\"] = _preloaded_ssl_context\n     elif isinstance(verify, str):\n         if not os.path.isdir(verify):",
      "comment": "This is one piece I left in place from #6667. We were unilaterally considering any `verify` string to be `ca_certs` instead of detecting if it was a directory. That seems like a miss from #6655 unless I'm missing something?",
      "comment_id": 1683276287,
      "user": "nateprewitt",
      "created_at": "2024-07-18T18:02:54Z",
      "url": "https://github.com/psf/requests/pull/6767#discussion_r1683276287"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6757,
      "file_path": "src/requests/compat.py",
      "line": 19,
      "side": "RIGHT",
      "diff_hunk": "@@ -10,6 +10,14 @@\n import importlib\n import sys\n \n+# -------\n+# urllib3\n+# -------\n+from urllib3 import __version__ as urllib3_version\n+\n+# Detect which major version of urllib3 is being used.\n+is_urllib3_2 = urllib3_version.split('.')[0] == 2",
      "comment": "I believe this is a string, not an integer. Also, should do a >= check instead of equals.",
      "comment_id": 1683193453,
      "user": "sethmlarson",
      "created_at": "2024-07-18T16:49:44Z",
      "url": "https://github.com/psf/requests/pull/6757#discussion_r1683193453"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6757,
      "file_path": "src/requests/compat.py",
      "line": 19,
      "side": "RIGHT",
      "diff_hunk": "@@ -10,6 +10,14 @@\n import importlib\n import sys\n \n+# -------\n+# urllib3\n+# -------\n+from urllib3 import __version__ as urllib3_version\n+\n+# Detect which major version of urllib3 is being used.\n+is_urllib3_2 = urllib3_version.split('.')[0] == 2",
      "comment": "Good catch on the `int`. For the equivalence, I don't know if `>=` is what we want if this is specifically scoping the 2.x major version. In the same way I wouldn't want an `is_py3` to include Python 4.x. We could do something like `is_gt_urllib3_1` but that seems like it may be premature forwards-compatibility?\r\n\r\nRight now our dependencies are scoped at `urllib3<3` and if we add this check to any other behaviors, I'd rather they stay scoped to the major version. That will let tests fail if we major version again and we can make an informed decision at that point when adding support. Otherwise, we may unintentionally carry forward behaviors that are subtly wrong.\r\n\r\nI can see similar risks with both sides, so not a hill I'm going to die on, but that was my initial thought process.",
      "comment_id": 1683204168,
      "user": "nateprewitt",
      "created_at": "2024-07-18T16:58:58Z",
      "url": "https://github.com/psf/requests/pull/6757#discussion_r1683204168"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6757,
      "file_path": "src/requests/compat.py",
      "line": 19,
      "side": "RIGHT",
      "diff_hunk": "@@ -10,6 +10,14 @@\n import importlib\n import sys\n \n+# -------\n+# urllib3\n+# -------\n+from urllib3 import __version__ as urllib3_version\n+\n+# Detect which major version of urllib3 is being used.\n+is_urllib3_2 = urllib3_version.split('.')[0] == 2",
      "comment": "I've changed the check from checking for urllib3 2.x to check for 1.x. That leaves us open to forward compatibility without the confusing behavior with `is_urllib3_2` including newer major versions.",
      "comment_id": 1695374605,
      "user": "nateprewitt",
      "created_at": "2024-07-29T14:49:41Z",
      "url": "https://github.com/psf/requests/pull/6757#discussion_r1695374605"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6963,
      "file_path": "src/requests/utils.py",
      "line": 240,
      "side": "RIGHT",
      "diff_hunk": "@@ -236,16 +236,8 @@ def get_netrc_auth(url, raise_errors=False):\n             return\n \n         ri = urlparse(url)\n-\n-        # Strip port numbers from netloc. This weird `if...encode`` dance is\n-        # used for Python 3.2, which doesn't support unicode literals.\n-        splitstr = b\":\"\n-        if isinstance(url, str):\n-            splitstr = splitstr.decode(\"ascii\")\n-        host = ri.netloc.split(splitstr)[0]\n-\n         try:\n-            _netrc = netrc(netrc_path).authenticators(host)\n+            _netrc = netrc(netrc_path).authenticators(ri.hostname)",
      "comment": "This is fixed now in main: https://github.com/psf/requests/commit/96ba401c1296ab1dda74a2365ef36d88f7d144ef",
      "comment_id": 2128118428,
      "user": "danigm",
      "created_at": "2025-06-05T07:12:17Z",
      "url": "https://github.com/psf/requests/pull/6963#discussion_r2128118428"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6951,
      "file_path": "docs/conf.py",
      "line": 61,
      "side": "RIGHT",
      "diff_hunk": "@@ -58,7 +58,7 @@\n \n # General information about the project.\n project = u\"Requests\"\n-copyright = u'MMXVIX. A <a href=\"https://kenreitz.org/projects\">Kenneth Reitz</a> Project'\n+copyright = u'MMXVIX. A <a href=\"https://kennethreitz.org/software/\">Kenneth Reitz</a> Project'",
      "comment": "```suggestion\r\ncopyright = u'MMXVIX. A Kenneth Reitz Project'\r\n```",
      "comment_id": 2101041830,
      "user": "nateprewitt",
      "created_at": "2025-05-21T19:45:04Z",
      "url": "https://github.com/psf/requests/pull/6951#discussion_r2101041830"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6710,
      "file_path": "src/requests/adapters.py",
      "line": 417,
      "side": "RIGHT",
      "diff_hunk": "@@ -404,7 +414,10 @@ def _get_connection(self, request, verify, proxies=None, cert=None):\n         return conn\n \n     def get_connection(self, url, proxies=None):\n-        \"\"\"Returns a urllib3 connection for the given URL. This should not be\n+        \"\"\"DEPRECATED: Users should move to `get_connection_with_tls_context`",
      "comment": "If this method is deprecated shall we start emitting `DeprecationWarning`?",
      "comment_id": 1608603801,
      "user": "sethmlarson",
      "created_at": "2024-05-21T16:12:52Z",
      "url": "https://github.com/psf/requests/pull/6710#discussion_r1608603801"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6710,
      "file_path": "src/requests/adapters.py",
      "line": 417,
      "side": "RIGHT",
      "diff_hunk": "@@ -404,7 +414,10 @@ def _get_connection(self, request, verify, proxies=None, cert=None):\n         return conn\n \n     def get_connection(self, url, proxies=None):\n-        \"\"\"Returns a urllib3 connection for the given URL. This should not be\n+        \"\"\"DEPRECATED: Users should move to `get_connection_with_tls_context`",
      "comment": "I thought about that but it's not reachable in any of our code anymore and if someone is using a custom implementation it won't have the warning.\n\nHappy to be wrong if I'm missing something but it seems like it will just be dead code on arrival.",
      "comment_id": 1608609666,
      "user": "nateprewitt",
      "created_at": "2024-05-21T16:17:36Z",
      "url": "https://github.com/psf/requests/pull/6710#discussion_r1608609666"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6710,
      "file_path": "src/requests/adapters.py",
      "line": 417,
      "side": "RIGHT",
      "diff_hunk": "@@ -404,7 +414,10 @@ def _get_connection(self, request, verify, proxies=None, cert=None):\n         return conn\n \n     def get_connection(self, url, proxies=None):\n-        \"\"\"Returns a urllib3 connection for the given URL. This should not be\n+        \"\"\"DEPRECATED: Users should move to `get_connection_with_tls_context`",
      "comment": "I guess it's better to be safe, this should be addressed in 92075b330a30b9883f466a43d3f7566ab849f91b.",
      "comment_id": 1608624860,
      "user": "nateprewitt",
      "created_at": "2024-05-21T16:29:48Z",
      "url": "https://github.com/psf/requests/pull/6710#discussion_r1608624860"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6702,
      "file_path": "src/requests/compat.py",
      "line": 21,
      "side": "RIGHT",
      "diff_hunk": "@@ -7,13 +7,28 @@\n compatibility until the next major version.\n \"\"\"\n \n-try:\n-    import chardet\n-except ImportError:\n-    import charset_normalizer as chardet\n-\n+import importlib\n import sys\n \n+# -------------------\n+# Character Detection\n+# -------------------\n+\n+\n+def _resolve_char_detection():\n+    \"\"\"Find supported character detection libraries.\"\"\"\n+    chardet = None\n+    for lib in (\"chardet\", \"charset_normalizer\"):",
      "comment": "While this keeps the logic of the earlier code, I think that the library which is required on should be tried first. https://github.com/psf/requests/pull/6714 addresses this.",
      "comment_id": 1609492798,
      "user": "nijel",
      "created_at": "2024-05-22T08:08:55Z",
      "url": "https://github.com/psf/requests/pull/6702#discussion_r1609492798"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6644,
      "file_path": "tests/test_adapters.py",
      "line": 6,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,7 @@\n+import requests.adapters\n+\n+\n+def test_request_url_trims_leading_path_separators():\n+    \"\"\"See also https://github.com/psf/requests/issues/6643.\"\"\"\n+    a = request.adapters.HTTPAdapter()",
      "comment": "```suggestion\n    a = requests.adapters.HTTPAdapter()\n```",
      "comment_id": 1498532708,
      "user": "sigmavirus24",
      "created_at": "2024-02-22T02:17:57Z",
      "url": "https://github.com/psf/requests/pull/6644#discussion_r1498532708"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6644,
      "file_path": "tests/test_adapters.py",
      "line": 7,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,7 @@\n+import requests.adapters\n+\n+\n+def test_request_url_trims_leading_path_separators():\n+    \"\"\"See also https://github.com/psf/requests/issues/6643.\"\"\"\n+    a = requests.adapters.HTTPAdapter()\n+    assert \"/v:h\" == a.request_url(\"http://127.0.0.1:10000//v:h\")",
      "comment": "```suggestion\n    assert \"/v:h\" == a.request_url(\"http://127.0.0.1:10000//v:h\", {})\n```",
      "comment_id": 1498534695,
      "user": "sigmavirus24",
      "created_at": "2024-02-22T02:21:39Z",
      "url": "https://github.com/psf/requests/pull/6644#discussion_r1498534695"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6644,
      "file_path": "tests/test_adapters.py",
      "line": 7,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,7 @@\n+import requests.adapters\n+\n+\n+def test_request_url_trims_leading_path_separators():\n+    \"\"\"See also https://github.com/psf/requests/issues/6643.\"\"\"\n+    a = requests.adapters.HTTPAdapter()\n+    assert \"/v:h\" == a.request_url(\"http://127.0.0.1:10000//v:h\", {})",
      "comment": "```suggestion\n    p = requests.Request(method=\"GET\", url=\"http://127.0.0.1:10000//v:h\").prepare()\n    assert \"/v:h\" == a.request_url(p, {})\n```",
      "comment_id": 1498539954,
      "user": "sigmavirus24",
      "created_at": "2024-02-22T02:31:51Z",
      "url": "https://github.com/psf/requests/pull/6644#discussion_r1498539954"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6644,
      "file_path": "src/requests/adapters.py",
      "line": 393,
      "side": "RIGHT",
      "diff_hunk": "@@ -389,7 +390,7 @@ def request_url(self, request, proxies):\n             proxy_scheme = urlparse(proxy).scheme.lower()\n             using_socks_proxy = proxy_scheme.startswith(\"socks\")\n \n-        url = request.path_url\n+        url = re.sub(\"^/+\", \"/\", request.path_url)",
      "comment": "As mentioned on https://github.com/urllib3/urllib3/issues/3352 this could also be\r\n\r\n```python\r\nurl = f\"/{request.path_url.lstrip('/')}\"\r\n```\r\n\r\nI could benchmark these but I don't particularly care what the implementation is. I just threw this together to show that it can be fixed",
      "comment_id": 1499181040,
      "user": "sigmavirus24",
      "created_at": "2024-02-22T12:39:14Z",
      "url": "https://github.com/psf/requests/pull/6644#discussion_r1499181040"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6644,
      "file_path": "src/requests/adapters.py",
      "line": 393,
      "side": "RIGHT",
      "diff_hunk": "@@ -389,7 +390,7 @@ def request_url(self, request, proxies):\n             proxy_scheme = urlparse(proxy).scheme.lower()\n             using_socks_proxy = proxy_scheme.startswith(\"socks\")\n \n-        url = request.path_url\n+        url = re.sub(\"^/+\", \"/\", request.path_url)",
      "comment": "It looks like the f-string (Python 3.9-3.12 tested) is ~4x faster but we're talking on the scale of nanoseconds so it's basically moot. I'd vote the f-string for readability, but don't have a strong opinion.",
      "comment_id": 1499501563,
      "user": "nateprewitt",
      "created_at": "2024-02-22T16:10:48Z",
      "url": "https://github.com/psf/requests/pull/6644#discussion_r1499501563"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6644,
      "file_path": "src/requests/adapters.py",
      "line": 393,
      "side": "RIGHT",
      "diff_hunk": "@@ -389,7 +390,7 @@ def request_url(self, request, proxies):\n             proxy_scheme = urlparse(proxy).scheme.lower()\n             using_socks_proxy = proxy_scheme.startswith(\"socks\")\n \n-        url = request.path_url\n+        url = re.sub(\"^/+\", \"/\", request.path_url)",
      "comment": "Yeah, I'm also happy to shove this into a branch too like \n\n```python\nif path.startswith('//'):\n```\n\nTo make it clearer that we only care about the separator being repeated. What I want is clarity in the reader as to why we're doing this. My old school brain things the regexp is clearer and the f-string looks sus but  that's just my opinion and I'm not holding it closely ",
      "comment_id": 1499711431,
      "user": "sigmavirus24",
      "created_at": "2024-02-22T18:22:38Z",
      "url": "https://github.com/psf/requests/pull/6644#discussion_r1499711431"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6589,
      "file_path": "src/requests/utils.py",
      "line": 137,
      "side": "RIGHT",
      "diff_hunk": "@@ -134,7 +134,10 @@ def super_len(o):\n     total_length = None\n     current_position = 0\n \n-    if hasattr(o, \"__len__\"):\n+    if isinstance(o, str):",
      "comment": "More simply this can be an if without changing the following conditions and just set `o = o.encode(...)` in the block then let the rest of the logic work. As it will hit another condition which will get it's length",
      "comment_id": 1409296382,
      "user": "sigmavirus24",
      "created_at": "2023-11-29T13:38:39Z",
      "url": "https://github.com/psf/requests/pull/6589#discussion_r1409296382"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6589,
      "file_path": "src/requests/utils.py",
      "line": 137,
      "side": "RIGHT",
      "diff_hunk": "@@ -134,7 +134,10 @@ def super_len(o):\n     total_length = None\n     current_position = 0\n \n-    if hasattr(o, \"__len__\"):\n+    if isinstance(o, str):",
      "comment": "Sounds fine to me. My usual workflow would be to rewrite my commit and do a force push to my branch (in my fork). Is this ok or would you prefer a separate commit for the change?",
      "comment_id": 1409321632,
      "user": "bruceadams",
      "created_at": "2023-11-29T13:57:59Z",
      "url": "https://github.com/psf/requests/pull/6589#discussion_r1409321632"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6562,
      "file_path": "src/requests/packages.py",
      "line": 26,
      "side": "LEFT",
      "diff_hunk": "@@ -23,6 +23,5 @@\n target = chardet.__name__\n for mod in list(sys.modules):\n     if mod == target or mod.startswith(f\"{target}.\"):\n-        target = target.replace(target, \"chardet\")",
      "comment": "This is unfortunately necessary. This is necessary to support charset-normalizer. The idea here is to import charset if it's installed instead of chardet from this location for users.\r\n\r\nThis is why we look at `chardet.__name__` (to determine which is most important: https://github.com/psf/requests/blob/839a8edec37c81a18ac8332cfbd44f44e1ae6206/src/requests/packages.py#L8 )\r\n\r\nSo what we need to do is determine how we want to handle this appropriately.\r\n\r\nThe idea is that we have two values:\r\n\r\n```py\r\ntarget = \"chardet\"\r\ntarget = \"charset_normalizer\"\r\n```\r\n\r\nIn the former case if `mod.startswith(\"chardet.\")` then we will be replacing `chardet` with `chardet`. In the latter case, we'd want to replace `charset_normalizer` with `chardet`.",
      "comment_id": 1377461842,
      "user": "sigmavirus24",
      "created_at": "2023-10-31T11:48:59Z",
      "url": "https://github.com/psf/requests/pull/6562#discussion_r1377461842"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6562,
      "file_path": "src/requests/packages.py",
      "line": 26,
      "side": "LEFT",
      "diff_hunk": "@@ -23,6 +23,5 @@\n target = chardet.__name__\n for mod in list(sys.modules):\n     if mod == target or mod.startswith(f\"{target}.\"):\n-        target = target.replace(target, \"chardet\")",
      "comment": "Got it, but still the for loop seems necessary. It'll just be mapping the last package of chardet modules to chardet package of requests. \r\n\r\nIf that is the expected behaviour then we can just assign the last package without running the for loop. ",
      "comment_id": 1377593896,
      "user": "amkarn258",
      "created_at": "2023-10-31T13:33:00Z",
      "url": "https://github.com/psf/requests/pull/6562#discussion_r1377593896"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6562,
      "file_path": "src/requests/packages.py",
      "line": 26,
      "side": "LEFT",
      "diff_hunk": "@@ -23,6 +23,5 @@\n target = chardet.__name__\n for mod in list(sys.modules):\n     if mod == target or mod.startswith(f\"{target}.\"):\n-        target = target.replace(target, \"chardet\")",
      "comment": "So I'm talking about the intent behind the line, there's obviously a bug in it. And I was trying to leave that as something for you to see for yourself.\n\nThe reality is that what we want is \n\n```py\nsys.modules[f\"requests.packages.{mod}\"] = sys.modules[mod]\nmod = mod.replace(target, \"chardet\")\nsys.modules[f\"requests.packages.{mod}\"] = sys.modules[mod]\n```\n\nBecause we may as well add charset normalizer in there but also, we want backwards compatibility with chardet\n\nSo it's a tiny bit obvious bug and we'd like the intended behavior fixed",
      "comment_id": 1377658476,
      "user": "sigmavirus24",
      "created_at": "2023-10-31T14:13:28Z",
      "url": "https://github.com/psf/requests/pull/6562#discussion_r1377658476"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6562,
      "file_path": "src/requests/packages.py",
      "line": 26,
      "side": "LEFT",
      "diff_hunk": "@@ -23,6 +23,5 @@\n target = chardet.__name__\n for mod in list(sys.modules):\n     if mod == target or mod.startswith(f\"{target}.\"):\n-        target = target.replace(target, \"chardet\")",
      "comment": "```\r\nsys.modules[f\"requests.packages.{mod}\"]` = sys.modules[mod]\r\nmod = mod.replace(target, \"chardet\")\r\nsys.modules[f\"requests.packages.{mod}\"] = sys.modules[mod]\r\n```\r\n\r\nDoesn't make sense, since then the mod will itself change to chardet and will throw a keyerror. \r\n\r\nHave added the support for both chardet and charset_normalizer in the latest commit, with this change - \r\n\r\n```\r\nsys.modules[f\"requests.packages.{mod}\"] = sys.modules[mod]\r\ntarget = target.replace(target, \"chardet\")\r\nsys.modules[f\"requests.packages.{target}\"] = sys.modules[mod]\r\n```\r\n",
      "comment_id": 1377751368,
      "user": "amkarn258",
      "created_at": "2023-10-31T15:10:40Z",
      "url": "https://github.com/psf/requests/pull/6562#discussion_r1377751368"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6562,
      "file_path": "src/requests/packages.py",
      "line": 26,
      "side": "LEFT",
      "diff_hunk": "@@ -23,6 +23,5 @@\n target = chardet.__name__\n for mod in list(sys.modules):\n     if mod == target or mod.startswith(f\"{target}.\"):\n-        target = target.replace(target, \"chardet\")",
      "comment": "Correct, it won't find the module, but it's still very wrong to do `target.replace(target, \"chardet\")`",
      "comment_id": 1377897436,
      "user": "sigmavirus24",
      "created_at": "2023-10-31T16:51:45Z",
      "url": "https://github.com/psf/requests/pull/6562#discussion_r1377897436"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6562,
      "file_path": "src/requests/packages.py",
      "line": 28,
      "side": "RIGHT",
      "diff_hunk": "@@ -23,6 +23,7 @@\n target = chardet.__name__\n for mod in list(sys.modules):\n     if mod == target or mod.startswith(f\"{target}.\"):\n+        sys.modules[f\"requests.packages.{mod}\"] = sys.modules[mod]\n         target = target.replace(target, \"chardet\")\n         sys.modules[f\"requests.packages.{target}\"] = sys.modules[mod]",
      "comment": "```suggestion\r\n        imported_mod = sys.modules[mod]\r\n        sys.modules[f\"requests.packages.{mod}\"] = imported_mod\r\n        mod = mod.replace(target, \"chardet\")\r\n        sys.modules[f\"requests.packages.{mod}\"] = imported_mod\r\n```\r\n",
      "comment_id": 1377900725,
      "user": "sigmavirus24",
      "created_at": "2023-10-31T16:54:08Z",
      "url": "https://github.com/psf/requests/pull/6562#discussion_r1377900725"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6529,
      "file_path": "src/requests/models.py",
      "line": 948,
      "side": "RIGHT",
      "diff_hunk": "@@ -945,7 +945,7 @@ def text(self):\n         return content\n \n     def json(self, **kwargs):\n-        r\"\"\"Returns the json-encoded content of a response, if any.\n+        r\"\"\"Returns the json-decoded dict of a response, if any.",
      "comment": "```suggestion\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n\n           This may return a dictionary, list, etc. depending on what is in the response.\n```\n",
      "comment_id": 1950711358,
      "user": "sigmavirus24",
      "created_at": "2025-02-11T11:55:52Z",
      "url": "https://github.com/psf/requests/pull/6529#discussion_r1950711358"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6529,
      "file_path": "src/requests/models.py",
      "line": 948,
      "side": "RIGHT",
      "diff_hunk": "@@ -945,7 +945,8 @@ def text(self):\n         return content\n \n     def json(self, **kwargs):\n-        r\"\"\"Returns the json-encoded content of a response, if any.\n+        r\"\"\"Decodes the JSON response body (if any) as a Python object.",
      "comment": "```suggestion\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n\n```\n",
      "comment_id": 1952510987,
      "user": "sigmavirus24",
      "created_at": "2025-02-12T11:57:20Z",
      "url": "https://github.com/psf/requests/pull/6529#discussion_r1952510987"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6448,
      "file_path": "setup.py",
      "line": 68,
      "side": "RIGHT",
      "diff_hunk": "@@ -65,7 +65,7 @@ def run_tests(self):\n     \"certifi>=2017.4.17\",\n ]\n test_requirements = [\n-    \"pytest-httpbin==0.0.7\",\n+    \"pytest-httpbin==2.0.0rc1\",",
      "comment": "```suggestion\r\n    \"pytest-httpbin==2.0.0rc2\",\r\n```",
      "comment_id": 1187828270,
      "user": "graingert",
      "created_at": "2023-05-08T19:47:43Z",
      "url": "https://github.com/psf/requests/pull/6448#discussion_r1187828270"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6036,
      "file_path": "requests/exceptions.py",
      "line": 45,
      "side": "RIGHT",
      "diff_hunk": "@@ -33,6 +33,9 @@ class InvalidJSONError(RequestException):\n \n class JSONDecodeError(InvalidJSONError, CompatJSONDecodeError):\n     \"\"\"Couldn't decode the text into json\"\"\"\n+    def __init__(self, *args, **kwargs):\n+        CompatJSONDecodeError.__init__(self, *args)\n+        InvalidJSONError.__init__(self, *self.args, **kwargs)",
      "comment": "Can we add comments here explaining why:\r\n\r\n1. We're using super old Python 2.5 methods of calling parent `__init__` methods\r\n2. We're calling the `__init__`s in this particular order - presuming the order matters\r\n3. This fixes the issue of `.doc` ending up in the message/default `__repr__`",
      "comment_id": 780674790,
      "user": "sigmavirus24",
      "created_at": "2022-01-08T14:42:14Z",
      "url": "https://github.com/psf/requests/pull/6036#discussion_r780674790"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6036,
      "file_path": "requests/exceptions.py",
      "line": 37,
      "side": "RIGHT",
      "diff_hunk": "@@ -33,6 +33,9 @@ class InvalidJSONError(RequestException):\n \n class JSONDecodeError(InvalidJSONError, CompatJSONDecodeError):\n     \"\"\"Couldn't decode the text into json\"\"\"\n+    def __init__(self, *args, **kwargs):",
      "comment": "Can we add an empty line prior to the `__init__` here? That's already the style of this sub-module if not the entirety of `requests`",
      "comment_id": 780674982,
      "user": "sigmavirus24",
      "created_at": "2022-01-08T14:44:32Z",
      "url": "https://github.com/psf/requests/pull/6036#discussion_r780674982"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6036,
      "file_path": "requests/exceptions.py",
      "line": 42,
      "side": "RIGHT",
      "diff_hunk": "@@ -34,6 +34,16 @@ class InvalidJSONError(RequestException):\n class JSONDecodeError(InvalidJSONError, CompatJSONDecodeError):\n     \"\"\"Couldn't decode the text into json\"\"\"\n \n+    def __init__(self, *args, **kwargs):\n+        \"\"\"\n+        Construct the JSONDecodeError instance first with all\n+        args. Then use it's args to construct the IOError so that\n+        the json specific args aren't used as IOError specific args\n+        and the error message that JSONDecodeError builds is preserved",
      "comment": "nit; This might be phrased \"[...] the error message from JSONDecodeError is preserved.\"\r\n\r\nAlso note the missing period in the current sentence.",
      "comment_id": 781581198,
      "user": "nateprewitt",
      "created_at": "2022-01-10T21:47:23Z",
      "url": "https://github.com/psf/requests/pull/6036#discussion_r781581198"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6036,
      "file_path": "tests/test_requests.py",
      "line": 2590,
      "side": "RIGHT",
      "diff_hunk": "@@ -2585,5 +2585,11 @@ def test_post_json_nan(self, httpbin):\n \n     def test_json_decode_compatibility(self, httpbin):\n         r = requests.get(httpbin('bytes/20'))\n-        with pytest.raises(requests.exceptions.JSONDecodeError):\n+        data = r.text\n+        with pytest.raises(requests.exceptions.JSONDecodeError) as excinfo:\n             r.json()\n+        assert isinstance(excinfo.value, RequestException)",
      "comment": "Do we need to use `.value` in these assertions? Just `excinfo` should be sufficient and that's how the typical user will probably interact with them.",
      "comment_id": 781592203,
      "user": "nateprewitt",
      "created_at": "2022-01-10T21:57:30Z",
      "url": "https://github.com/psf/requests/pull/6036#discussion_r781592203"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6036,
      "file_path": "tests/test_requests.py",
      "line": 2594,
      "side": "RIGHT",
      "diff_hunk": "@@ -2585,5 +2585,11 @@ def test_post_json_nan(self, httpbin):\n \n     def test_json_decode_compatibility(self, httpbin):\n         r = requests.get(httpbin('bytes/20'))\n-        with pytest.raises(requests.exceptions.JSONDecodeError):\n+        data = r.text\n+        with pytest.raises(requests.exceptions.JSONDecodeError) as excinfo:\n             r.json()\n+        assert isinstance(excinfo.value, RequestException)\n+        assert isinstance(excinfo.value, JSONDecodeError)\n+        assert data not in str(excinfo.value)\n+        if is_py3:",
      "comment": "Also somewhat of a nit. It may be better to just make this its own test with a `@unittest.skipIf(not is_py3)` at the top.\r\n\r\ne.g.\r\n\r\n```python\r\n@unittest.skipIf(not is_py3)\r\ndef test_json_decode_persists_doc_attr(self, httpbin):\r\n    r = requests.get(httpbin('bytes/20'))\r\n    with pytest.raises(requests.exceptions.JSONDecodeError) as e:\r\n        r.json()\r\n    assert e.doc == r.text\r\n```\r\n        \r\n",
      "comment_id": 781595242,
      "user": "nateprewitt",
      "created_at": "2022-01-10T22:00:15Z",
      "url": "https://github.com/psf/requests/pull/6036#discussion_r781595242"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6036,
      "file_path": "tests/test_requests.py",
      "line": 2594,
      "side": "RIGHT",
      "diff_hunk": "@@ -2585,5 +2585,11 @@ def test_post_json_nan(self, httpbin):\n \n     def test_json_decode_compatibility(self, httpbin):\n         r = requests.get(httpbin('bytes/20'))\n-        with pytest.raises(requests.exceptions.JSONDecodeError):\n+        data = r.text\n+        with pytest.raises(requests.exceptions.JSONDecodeError) as excinfo:\n             r.json()\n+        assert isinstance(excinfo.value, RequestException)\n+        assert isinstance(excinfo.value, JSONDecodeError)\n+        assert data not in str(excinfo.value)\n+        if is_py3:",
      "comment": "don't we want to test that the py2 object is also what we expect? Or if we don't can i can make it py3 only?",
      "comment_id": 781656546,
      "user": "chyzzqo2",
      "created_at": "2022-01-10T23:41:24Z",
      "url": "https://github.com/psf/requests/pull/6036#discussion_r781656546"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6036,
      "file_path": "tests/test_requests.py",
      "line": 2590,
      "side": "RIGHT",
      "diff_hunk": "@@ -2585,5 +2585,11 @@ def test_post_json_nan(self, httpbin):\n \n     def test_json_decode_compatibility(self, httpbin):\n         r = requests.get(httpbin('bytes/20'))\n-        with pytest.raises(requests.exceptions.JSONDecodeError):\n+        data = r.text\n+        with pytest.raises(requests.exceptions.JSONDecodeError) as excinfo:\n             r.json()\n+        assert isinstance(excinfo.value, RequestException)",
      "comment": "excinfo is a pytest thing that wraps the actual exception, i think we have to look at value to get the original object",
      "comment_id": 781658238,
      "user": "chyzzqo2",
      "created_at": "2022-01-10T23:45:42Z",
      "url": "https://github.com/psf/requests/pull/6036#discussion_r781658238"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6036,
      "file_path": "tests/test_requests.py",
      "line": 2594,
      "side": "RIGHT",
      "diff_hunk": "@@ -2585,5 +2585,11 @@ def test_post_json_nan(self, httpbin):\n \n     def test_json_decode_compatibility(self, httpbin):\n         r = requests.get(httpbin('bytes/20'))\n-        with pytest.raises(requests.exceptions.JSONDecodeError):\n+        data = r.text\n+        with pytest.raises(requests.exceptions.JSONDecodeError) as excinfo:\n             r.json()\n+        assert isinstance(excinfo.value, RequestException)\n+        assert isinstance(excinfo.value, JSONDecodeError)\n+        assert data not in str(excinfo.value)\n+        if is_py3:",
      "comment": "The original test doesn't assert anything about Python 2 either. So we don't lose anything with this change. I suppose we could assert doc _isn't_ on the Python 2 exception but I'm not sure how meaningful that is.\n\nEdit: to be clear I'm proposing two tests. The one you have and moving the Python 3 specifics to its own test.",
      "comment_id": 781659079,
      "user": "nateprewitt",
      "created_at": "2022-01-10T23:48:06Z",
      "url": "https://github.com/psf/requests/pull/6036#discussion_r781659079"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6036,
      "file_path": "tests/test_requests.py",
      "line": 2594,
      "side": "RIGHT",
      "diff_hunk": "@@ -2585,5 +2585,11 @@ def test_post_json_nan(self, httpbin):\n \n     def test_json_decode_compatibility(self, httpbin):\n         r = requests.get(httpbin('bytes/20'))\n-        with pytest.raises(requests.exceptions.JSONDecodeError):\n+        data = r.text\n+        with pytest.raises(requests.exceptions.JSONDecodeError) as excinfo:\n             r.json()\n+        assert isinstance(excinfo.value, RequestException)\n+        assert isinstance(excinfo.value, JSONDecodeError)\n+        assert data not in str(excinfo.value)\n+        if is_py3:",
      "comment": "ok, i think i updated it with what you had in mind.",
      "comment_id": 781684776,
      "user": "chyzzqo2",
      "created_at": "2022-01-11T00:58:10Z",
      "url": "https://github.com/psf/requests/pull/6036#discussion_r781684776"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6036,
      "file_path": "requests/exceptions.py",
      "line": 42,
      "side": "RIGHT",
      "diff_hunk": "@@ -34,6 +34,16 @@ class InvalidJSONError(RequestException):\n class JSONDecodeError(InvalidJSONError, CompatJSONDecodeError):\n     \"\"\"Couldn't decode the text into json\"\"\"\n \n+    def __init__(self, *args, **kwargs):\n+        \"\"\"\n+        Construct the JSONDecodeError instance first with all\n+        args. Then use it's args to construct the IOError so that\n+        the json specific args aren't used as IOError specific args\n+        and the error message from JSONDecodeError is preserved",
      "comment": "I think the last sentence still needs a period at the end.",
      "comment_id": 782303428,
      "user": "nateprewitt",
      "created_at": "2022-01-11T16:11:57Z",
      "url": "https://github.com/psf/requests/pull/6036#discussion_r782303428"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6036,
      "file_path": "tests/test_requests.py",
      "line": 2601,
      "side": "RIGHT",
      "diff_hunk": "@@ -2585,5 +2585,18 @@ def test_post_json_nan(self, httpbin):\n \n     def test_json_decode_compatibility(self, httpbin):\n         r = requests.get(httpbin('bytes/20'))\n-        with pytest.raises(requests.exceptions.JSONDecodeError):\n+        data = r.text\n+        with pytest.raises(requests.exceptions.JSONDecodeError) as excinfo:\n             r.json()\n+        assert isinstance(excinfo.value, RequestException)\n+        assert isinstance(excinfo.value, JSONDecodeError)\n+        assert data not in str(excinfo.value)\n+\n+    @pytest.mark.skipif(not is_py3, reason=\"doc attribute is only present on py3\")\n+    def test_json_decode_persists_doc_attr(self, httpbin):\n+        r = requests.get(httpbin('bytes/20'))\n+        data = r.text\n+        with pytest.raises(requests.exceptions.JSONDecodeError) as excinfo:\n+            r.json()\n+        assert data not in str(excinfo.value)",
      "comment": "We probably don't need to reassert this since it's already handled in the test above.",
      "comment_id": 782304038,
      "user": "nateprewitt",
      "created_at": "2022-01-11T16:12:37Z",
      "url": "https://github.com/psf/requests/pull/6036#discussion_r782304038"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6036,
      "file_path": "tests/test_requests.py",
      "line": 2598,
      "side": "RIGHT",
      "diff_hunk": "@@ -2585,5 +2585,18 @@ def test_post_json_nan(self, httpbin):\n \n     def test_json_decode_compatibility(self, httpbin):\n         r = requests.get(httpbin('bytes/20'))\n-        with pytest.raises(requests.exceptions.JSONDecodeError):\n+        data = r.text\n+        with pytest.raises(requests.exceptions.JSONDecodeError) as excinfo:\n             r.json()\n+        assert isinstance(excinfo.value, RequestException)\n+        assert isinstance(excinfo.value, JSONDecodeError)\n+        assert data not in str(excinfo.value)\n+\n+    @pytest.mark.skipif(not is_py3, reason=\"doc attribute is only present on py3\")\n+    def test_json_decode_persists_doc_attr(self, httpbin):\n+        r = requests.get(httpbin('bytes/20'))\n+        data = r.text",
      "comment": "nit; since we're only using this once we can probably just use r.text in the final assertion as shown in the original proposal.",
      "comment_id": 782312495,
      "user": "nateprewitt",
      "created_at": "2022-01-11T16:21:42Z",
      "url": "https://github.com/psf/requests/pull/6036#discussion_r782312495"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6036,
      "file_path": "tests/test_requests.py",
      "line": 2601,
      "side": "RIGHT",
      "diff_hunk": "@@ -2585,5 +2585,18 @@ def test_post_json_nan(self, httpbin):\n \n     def test_json_decode_compatibility(self, httpbin):\n         r = requests.get(httpbin('bytes/20'))\n-        with pytest.raises(requests.exceptions.JSONDecodeError):\n+        data = r.text\n+        with pytest.raises(requests.exceptions.JSONDecodeError) as excinfo:\n             r.json()\n+        assert isinstance(excinfo.value, RequestException)\n+        assert isinstance(excinfo.value, JSONDecodeError)\n+        assert data not in str(excinfo.value)\n+\n+    @pytest.mark.skipif(not is_py3, reason=\"doc attribute is only present on py3\")\n+    def test_json_decode_persists_doc_attr(self, httpbin):\n+        r = requests.get(httpbin('bytes/20'))\n+        data = r.text\n+        with pytest.raises(requests.exceptions.JSONDecodeError) as excinfo:\n+            r.json()\n+        assert data not in str(excinfo.value)",
      "comment": "I liked having both in the same test, assures us that we are testing with the right string. but i'll take it out.",
      "comment_id": 782337397,
      "user": "chyzzqo2",
      "created_at": "2022-01-11T16:43:54Z",
      "url": "https://github.com/psf/requests/pull/6036#discussion_r782337397"
    },
    {
      "repo": "psf/requests",
      "pr_number": 4766,
      "file_path": "tests/test_requests.py",
      "line": 883,
      "side": "RIGHT",
      "diff_hunk": "@@ -875,11 +875,12 @@ def test_form_encoded_post_query_multivalued_element(self, httpbin):\n         assert prep.body == 'test=foo&test=baz'\n \n     def test_different_encodings_dont_break_post(self, httpbin):\n-        r = requests.post(httpbin('post'),\n-            data={'stuff': json.dumps({'a': 123})},\n-            params={'blah': 'asdf1234'},\n-            files={'file': ('test_requests.py', open(__file__, 'rb'))})\n-        assert r.status_code == 200\n+        with open(__file__, 'rb') as f:\n+            r = requests.post(httpbin('post'),\n+                data={'stuff': json.dumps({'a': 123})},\n+                params={'blah': 'asdf1234'},\n+                files={'file': ('test_requests.py', f)})\n+            assert r.status_code == 200",
      "comment": "Let\u2019s pull the `assert` out to the same indentation level as `with`.",
      "comment_id": 209787773,
      "user": "nateprewitt",
      "created_at": "2018-08-13T23:14:55Z",
      "url": "https://github.com/psf/requests/pull/4766#discussion_r209787773"
    },
    {
      "repo": "psf/requests",
      "pr_number": 4766,
      "file_path": "tests/test_requests.py",
      "line": 912,
      "side": "RIGHT",
      "diff_hunk": "@@ -889,37 +890,41 @@ def test_different_encodings_dont_break_post(self, httpbin):\n             {'stuff': 'elixr'.encode('utf-8')},\n         ))\n     def test_unicode_multipart_post(self, httpbin, data):\n-        r = requests.post(httpbin('post'),\n-            data=data,\n-            files={'file': ('test_requests.py', open(__file__, 'rb'))})\n-        assert r.status_code == 200\n+        with open(__file__, 'rb') as f:\n+            r = requests.post(httpbin('post'),\n+                data=data,\n+                files={'file': ('test_requests.py', f)})\n+            assert r.status_code == 200\n \n     def test_unicode_multipart_post_fieldnames(self, httpbin):\n         filename = os.path.splitext(__file__)[0] + '.py'\n-        r = requests.Request(\n-            method='POST', url=httpbin('post'),\n-            data={'stuff'.encode('utf-8'): 'elixr'},\n-            files={'file': ('test_requests.py', open(filename, 'rb'))})\n-        prep = r.prepare()\n-        assert b'name=\"stuff\"' in prep.body\n-        assert b'name=\"b\\'stuff\\'\"' not in prep.body\n+        with open(filename, 'rb') as f:\n+            r = requests.Request(\n+                method='POST', url=httpbin('post'),\n+                data={'stuff'.encode('utf-8'): 'elixr'},\n+                files={'file': ('test_requests.py', f)})\n+            prep = r.prepare()\n+            assert b'name=\"stuff\"' in prep.body\n+            assert b'name=\"b\\'stuff\\'\"' not in prep.body\n \n     def test_unicode_method_name(self, httpbin):\n-        files = {'file': open(__file__, 'rb')}\n-        r = requests.request(\n-            method=u('POST'), url=httpbin('post'), files=files)\n-        assert r.status_code == 200\n+        with open(__file__, 'rb') as f:\n+            files = {'file': f}",
      "comment": "Let's just pass this directly to the `files` parameter.",
      "comment_id": 221472175,
      "user": "nateprewitt",
      "created_at": "2018-09-30T20:23:42Z",
      "url": "https://github.com/psf/requests/pull/4766#discussion_r221472175"
    },
    {
      "repo": "psf/requests",
      "pr_number": 4766,
      "file_path": "tests/test_requests.py",
      "line": 907,
      "side": "RIGHT",
      "diff_hunk": "@@ -889,37 +890,41 @@ def test_different_encodings_dont_break_post(self, httpbin):\n             {'stuff': 'elixr'.encode('utf-8')},\n         ))\n     def test_unicode_multipart_post(self, httpbin, data):\n-        r = requests.post(httpbin('post'),\n-            data=data,\n-            files={'file': ('test_requests.py', open(__file__, 'rb'))})\n-        assert r.status_code == 200\n+        with open(__file__, 'rb') as f:\n+            r = requests.post(httpbin('post'),\n+                data=data,\n+                files={'file': ('test_requests.py', f)})\n+            assert r.status_code == 200\n \n     def test_unicode_multipart_post_fieldnames(self, httpbin):\n         filename = os.path.splitext(__file__)[0] + '.py'\n-        r = requests.Request(\n-            method='POST', url=httpbin('post'),\n-            data={'stuff'.encode('utf-8'): 'elixr'},\n-            files={'file': ('test_requests.py', open(filename, 'rb'))})\n-        prep = r.prepare()\n-        assert b'name=\"stuff\"' in prep.body\n-        assert b'name=\"b\\'stuff\\'\"' not in prep.body\n+        with open(filename, 'rb') as f:\n+            r = requests.Request(\n+                method='POST', url=httpbin('post'),\n+                data={'stuff'.encode('utf-8'): 'elixr'},\n+                files={'file': ('test_requests.py', f)})\n+            prep = r.prepare()\n+            assert b'name=\"stuff\"' in prep.body",
      "comment": "These asserts can be done at the same indentation as the `with` block.",
      "comment_id": 221472224,
      "user": "nateprewitt",
      "created_at": "2018-09-30T20:25:23Z",
      "url": "https://github.com/psf/requests/pull/4766#discussion_r221472224"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6091,
      "file_path": "requests/sessions.py",
      "line": 41,
      "side": "RIGHT",
      "diff_hunk": "@@ -39,10 +39,7 @@\n \n # Preferred clock, based on which one is more accurate on a given system.\n if sys.platform == 'win32':",
      "comment": "Since we're now Python 3.7+ I wonder if we can start using `time.perf_counter` everywhere? Might need to do more research into why Windows was given only perf_counter/clock instead of `time.time()`",
      "comment_id": 833410951,
      "user": "sethmlarson",
      "created_at": "2022-03-23T15:32:31Z",
      "url": "https://github.com/psf/requests/pull/6091#discussion_r833410951"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6091,
      "file_path": "setup.py",
      "line": 111,
      "side": "LEFT",
      "diff_hunk": "@@ -108,7 +102,6 @@ def run_tests(self):\n     extras_require={\n         'security': [],\n         'socks': ['PySocks>=1.5.6, !=1.5.7'],\n-        'socks:sys_platform == \"win32\" and python_version == \"2.7\"': ['win_inet_pton'],",
      "comment": "[Finally free...](https://pypi.org/project/win-inet-pton) :smiling_face_with_tear:",
      "comment_id": 833413598,
      "user": "sethmlarson",
      "created_at": "2022-03-23T15:34:32Z",
      "url": "https://github.com/psf/requests/pull/6091#discussion_r833413598"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6091,
      "file_path": "tests/compat.py",
      "line": 15,
      "side": "RIGHT",
      "diff_hunk": "@@ -13,9 +10,5 @@\n except ImportError:\n     cStringIO = None\n \n-if is_py3:\n-    def u(s):\n-        return s\n-else:\n-    def u(s):\n-        return s.decode('unicode-escape')\n+def u(s):",
      "comment": "Should we go through and remove all uses of `u(...)` in the tests to simplify them?",
      "comment_id": 833414757,
      "user": "sethmlarson",
      "created_at": "2022-03-23T15:35:27Z",
      "url": "https://github.com/psf/requests/pull/6091#discussion_r833414757"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6091,
      "file_path": "tests/test_requests.py",
      "line": 846,
      "side": "RIGHT",
      "diff_hunk": "@@ -844,7 +843,7 @@ def test_conflicting_post_params(self, httpbin):\n             with pytest.raises(ValueError):\n                 requests.post(url, data='[{\"some\": \"data\"}]', files={'some': f})\n             with pytest.raises(ValueError):\n-                requests.post(url, data=u('[{\"some\": \"data\"}]'), files={'some': f})\n+                requests.post(url, data=u'[{\"some\": \"data\"}]', files={'some': f})",
      "comment": "No we don't, I have a separate linting PR that removes these lines entirely since they're redundant. I can move that into this one.",
      "comment_id": 833421896,
      "user": "nateprewitt",
      "created_at": "2022-03-23T15:41:49Z",
      "url": "https://github.com/psf/requests/pull/6091#discussion_r833421896"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6091,
      "file_path": "tests/compat.py",
      "line": 15,
      "side": "RIGHT",
      "diff_hunk": "@@ -13,9 +10,5 @@\n except ImportError:\n     cStringIO = None\n \n-if is_py3:\n-    def u(s):\n-        return s\n-else:\n-    def u(s):\n-        return s.decode('unicode-escape')\n+def u(s):",
      "comment": "I left the definition in case it's in use externally, but yes, it should be removed from all of our tests. I'll do a second pass if I missed some.",
      "comment_id": 833422816,
      "user": "nateprewitt",
      "created_at": "2022-03-23T15:42:39Z",
      "url": "https://github.com/psf/requests/pull/6091#discussion_r833422816"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6091,
      "file_path": "requests/sessions.py",
      "line": 41,
      "side": "RIGHT",
      "diff_hunk": "@@ -39,10 +39,7 @@\n \n # Preferred clock, based on which one is more accurate on a given system.\n if sys.platform == 'win32':",
      "comment": "It was a performance issue for Windows where the clock wasn't granular enough for CI to pass on Appveyor (#3988). I can take a look at standardizing everything onto perf_counter but that's probably a separate deep dive.",
      "comment_id": 833422938,
      "user": "nateprewitt",
      "created_at": "2022-03-23T15:42:44Z",
      "url": "https://github.com/psf/requests/pull/6091#discussion_r833422938"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6091,
      "file_path": "tests/test_requests.py",
      "line": 846,
      "side": "RIGHT",
      "diff_hunk": "@@ -844,7 +843,7 @@ def test_conflicting_post_params(self, httpbin):\n             with pytest.raises(ValueError):\n                 requests.post(url, data='[{\"some\": \"data\"}]', files={'some': f})\n             with pytest.raises(ValueError):\n-                requests.post(url, data=u('[{\"some\": \"data\"}]'), files={'some': f})\n+                requests.post(url, data=u'[{\"some\": \"data\"}]', files={'some': f})",
      "comment": "`pyupgrade **/*.py --py37-plus` will deal with a bunch of `u''`s and other stuff.",
      "comment_id": 833439166,
      "user": "hugovk",
      "created_at": "2022-03-23T15:56:05Z",
      "url": "https://github.com/psf/requests/pull/6091#discussion_r833439166"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6091,
      "file_path": "tests/test_requests.py",
      "line": 846,
      "side": "RIGHT",
      "diff_hunk": "@@ -844,7 +843,7 @@ def test_conflicting_post_params(self, httpbin):\n             with pytest.raises(ValueError):\n                 requests.post(url, data='[{\"some\": \"data\"}]', files={'some': f})\n             with pytest.raises(ValueError):\n-                requests.post(url, data=u('[{\"some\": \"data\"}]'), files={'some': f})\n+                requests.post(url, data=u'[{\"some\": \"data\"}]', files={'some': f})",
      "comment": "Yep, that's what's in the linting commit currently. Unfortunately, it just makes the two declarations identical so it will still require manual removal in one of the PRs.",
      "comment_id": 833449625,
      "user": "nateprewitt",
      "created_at": "2022-03-23T16:04:19Z",
      "url": "https://github.com/psf/requests/pull/6091#discussion_r833449625"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6091,
      "file_path": "tests/compat.py",
      "line": 15,
      "side": "RIGHT",
      "diff_hunk": "@@ -13,9 +10,5 @@\n except ImportError:\n     cStringIO = None\n \n-if is_py3:\n-    def u(s):\n-        return s\n-else:\n-    def u(s):\n-        return s.decode('unicode-escape')\n+def u(s):",
      "comment": "Perhaps add a deprecation warning for any external use?",
      "comment_id": 833454405,
      "user": "hugovk",
      "created_at": "2022-03-23T16:08:55Z",
      "url": "https://github.com/psf/requests/pull/6091#discussion_r833454405"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6091,
      "file_path": "setup.py",
      "line": 23,
      "side": "RIGHT",
      "diff_hunk": "@@ -8,7 +7,32 @@\n from setuptools import setup\n from setuptools.command.test import test as TestCommand\n \n-here = os.path.abspath(os.path.dirname(__file__))\n+\n+CURRENT_PYTHON = sys.version_info[:2]\n+REQUIRED_PYTHON = (3, 7)\n+\n+if CURRENT_PYTHON < REQUIRED_PYTHON:\n+    sys.stderr.write(\n+        \"\"\"\n+==========================\n+Unsupported Python version\n+==========================\n+This version of Requests requires Python {}.{}, but you're trying to\n+install it on Python {}.{}.\n+This may be because you are using a version of pip that doesn't\n+understand the python_requires classifier. Make sure you",
      "comment": "We discussed this in the urllib3 Discord with @nateprewitt and @graingert and figured out that this series of steps isn't always going to solve the problem because pip *also* dropped Python versions over time.\r\n\r\nThere were a few solutions proposed:\r\n- Keep the current message\r\n- Always recommend installing pip<21 (last version that supported Python 2 and 3.5)\r\n- Recommend installing [pip-with-requires-python](https://pypi.org/project/pip-with-requires-python)\r\n- Not give a recommendation on *how* to upgrade pip and setuptools?\r\n\r\nI also raised the point of recommending users upgrade pip/setuptools would likely break OS installations.",
      "comment_id": 834551612,
      "user": "sethmlarson",
      "created_at": "2022-03-24T17:20:47Z",
      "url": "https://github.com/psf/requests/pull/6091#discussion_r834551612"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6091,
      "file_path": "setup.py",
      "line": 23,
      "side": "RIGHT",
      "diff_hunk": "@@ -8,7 +7,32 @@\n from setuptools import setup\n from setuptools.command.test import test as TestCommand\n \n-here = os.path.abspath(os.path.dirname(__file__))\n+\n+CURRENT_PYTHON = sys.version_info[:2]\n+REQUIRED_PYTHON = (3, 7)\n+\n+if CURRENT_PYTHON < REQUIRED_PYTHON:\n+    sys.stderr.write(\n+        \"\"\"\n+==========================\n+Unsupported Python version\n+==========================\n+This version of Requests requires Python {}.{}, but you're trying to\n+install it on Python {}.{}.\n+This may be because you are using a version of pip that doesn't\n+understand the python_requires classifier. Make sure you",
      "comment": "I think `pip-with-requires-python` is likely the safest option. Alternatively, we'd vend the requires values from the package in our setup.py and compose the install message from those values as @graingert suggested. That avoids any risk for Python 3.4 users who still comprise a significant number of downloads.",
      "comment_id": 834561046,
      "user": "nateprewitt",
      "created_at": "2022-03-24T17:31:20Z",
      "url": "https://github.com/psf/requests/pull/6091#discussion_r834561046"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6091,
      "file_path": "setup.py",
      "line": 23,
      "side": "RIGHT",
      "diff_hunk": "@@ -8,7 +7,32 @@\n from setuptools import setup\n from setuptools.command.test import test as TestCommand\n \n-here = os.path.abspath(os.path.dirname(__file__))\n+\n+CURRENT_PYTHON = sys.version_info[:2]\n+REQUIRED_PYTHON = (3, 7)\n+\n+if CURRENT_PYTHON < REQUIRED_PYTHON:\n+    sys.stderr.write(\n+        \"\"\"\n+==========================\n+Unsupported Python version\n+==========================\n+This version of Requests requires Python {}.{}, but you're trying to\n+install it on Python {}.{}.\n+This may be because you are using a version of pip that doesn't\n+understand the python_requires classifier. Make sure you",
      "comment": "@pradyunsg Noted that new Python versions come with a \"good\" pip version out of the box. Maybe our strategy should be to instruct the user to either pin to <2.28 or upgrade to at least Python 3.7 (since they'll need that version to install Requests anyways) and then we don't need to mention pip/setuptools at all.",
      "comment_id": 834612265,
      "user": "sethmlarson",
      "created_at": "2022-03-24T18:32:48Z",
      "url": "https://github.com/psf/requests/pull/6091#discussion_r834612265"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6091,
      "file_path": "setup.py",
      "line": 23,
      "side": "RIGHT",
      "diff_hunk": "@@ -8,7 +7,32 @@\n from setuptools import setup\n from setuptools.command.test import test as TestCommand\n \n-here = os.path.abspath(os.path.dirname(__file__))\n+\n+CURRENT_PYTHON = sys.version_info[:2]\n+REQUIRED_PYTHON = (3, 7)\n+\n+if CURRENT_PYTHON < REQUIRED_PYTHON:\n+    sys.stderr.write(\n+        \"\"\"\n+==========================\n+Unsupported Python version\n+==========================\n+This version of Requests requires Python {}.{}, but you're trying to\n+install it on Python {}.{}.\n+This may be because you are using a version of pip that doesn't\n+understand the python_requires classifier. Make sure you",
      "comment": "I went ahead and [ripped out the whole of touching pip versions](https://github.com/psf/requests/pull/6091/commits/febcd5199c278d99232124e22b3ab2d912b7cc16) after further discussion. If users want to upgrade their pip, that's something they can manage, otherwise the simplest option is to upgrade their Python version which will provide this automatically.\r\n\r\nOther users are free to pin to older supported versions as discussed in the original deprecation announcement.",
      "comment_id": 834679845,
      "user": "nateprewitt",
      "created_at": "2022-03-24T20:03:49Z",
      "url": "https://github.com/psf/requests/pull/6091#discussion_r834679845"
    },
    {
      "repo": "psf/requests",
      "pr_number": 6091,
      "file_path": "setup.py",
      "line": 26,
      "side": "RIGHT",
      "diff_hunk": "@@ -17,17 +17,13 @@\n ==========================\n Unsupported Python version\n ==========================\n-This version of Requests requires Python {}.{}, but you're trying to\n-install it on Python {}.{}.\n-This may be because you are using a version of pip that doesn't\n-understand the python_requires classifier. Make sure you\n-have pip >= 9.0 and setuptools >= 24.2, then try again:\n-    $ python -m pip install --upgrade pip setuptools\n-    $ python -m pip install requests\n-This will install the latest version of Requests which works on your\n-version of Python. If you can't upgrade your pip (or Python), request\n-an older version of Requests:\n-    $ python -m pip install \"requests<2.28\"\n+This version of Requests requires at least Python {}.{}, but\n+you're trying to install it on Python {}.{}. To resolve this,\n+consider upgrading to a supported Python version.\n+\n+If you can't upgrade your Python version, you'll need to\n+pin to an older version of Requests:\n+    python -m pip install \"requests<2.28\"",
      "comment": "```suggestion\r\npin to an older version of Requests (<2.28).\r\n```\r\n\r\nLet's just go all the way here. :P",
      "comment_id": 834692016,
      "user": "pradyunsg",
      "created_at": "2022-03-24T20:18:55Z",
      "url": "https://github.com/psf/requests/pull/6091#discussion_r834692016"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5917,
      "file_path": "requests/utils.py",
      "line": 974,
      "side": "RIGHT",
      "diff_hunk": "@@ -932,15 +933,22 @@ def prepend_scheme_if_needed(url, new_scheme):\n \n     :rtype: str\n     \"\"\"\n-    scheme, netloc, path, params, query, fragment = urlparse(url, new_scheme)\n+    parsed = parse_url(url)\n+    scheme, auth, host, port, path, query, fragment = parsed\n \n-    # urlparse is a finicky beast, and sometimes decides that there isn't a\n-    # netloc present. Assume that it's being over-cautious, and switch netloc\n-    # and path if urlparse decided there was no netloc.\n+    # urlparse and parse_url determine that there isn't a netloc present in some\n+    # urls. We've chosen to assume parsing is being over-cautious, and switch\n+    # the netloc and path. This is maintained for backwards compatibility.\n+    netloc = parsed.netloc",
      "comment": "Uhhh, what? This doesn't make any sense to me, is this something that's only needed for `urlparse`?",
      "comment_id": 761620552,
      "user": "sethmlarson",
      "created_at": "2021-12-03T03:18:15Z",
      "url": "https://github.com/psf/requests/pull/5917#discussion_r761620552"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5917,
      "file_path": "requests/utils.py",
      "line": 974,
      "side": "RIGHT",
      "diff_hunk": "@@ -932,15 +933,22 @@ def prepend_scheme_if_needed(url, new_scheme):\n \n     :rtype: str\n     \"\"\"\n-    scheme, netloc, path, params, query, fragment = urlparse(url, new_scheme)\n+    parsed = parse_url(url)\n+    scheme, auth, host, port, path, query, fragment = parsed\n \n-    # urlparse is a finicky beast, and sometimes decides that there isn't a\n-    # netloc present. Assume that it's being over-cautious, and switch netloc\n-    # and path if urlparse decided there was no netloc.\n+    # urlparse and parse_url determine that there isn't a netloc present in some\n+    # urls. We've chosen to assume parsing is being over-cautious, and switch\n+    # the netloc and path. This is maintained for backwards compatibility.\n+    netloc = parsed.netloc",
      "comment": "Yeah, this is only a quirk for `urlparse`. I'd thought I repro'd with `parse_url` initially, but I'm unable to now.\r\n\r\nI'll reword the whole comment for clarity. I'm not confident the test suite covers all of the initial issue though. I think I'm still in favor of leaving the swap in place as a fallback for anything we're missing.",
      "comment_id": 763565145,
      "user": "nateprewitt",
      "created_at": "2021-12-07T01:46:39Z",
      "url": "https://github.com/psf/requests/pull/5917#discussion_r763565145"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5917,
      "file_path": "requests/utils.py",
      "line": 974,
      "side": "RIGHT",
      "diff_hunk": "@@ -932,15 +933,22 @@ def prepend_scheme_if_needed(url, new_scheme):\n \n     :rtype: str\n     \"\"\"\n-    scheme, netloc, path, params, query, fragment = urlparse(url, new_scheme)\n+    parsed = parse_url(url)\n+    scheme, auth, host, port, path, query, fragment = parsed\n \n-    # urlparse is a finicky beast, and sometimes decides that there isn't a\n-    # netloc present. Assume that it's being over-cautious, and switch netloc\n-    # and path if urlparse decided there was no netloc.\n+    # urlparse and parse_url determine that there isn't a netloc present in some\n+    # urls. We've chosen to assume parsing is being over-cautious, and switch\n+    # the netloc and path. This is maintained for backwards compatibility.\n+    netloc = parsed.netloc",
      "comment": "@sethmlarson I've reworded things to be a bit clearer.",
      "comment_id": 776146587,
      "user": "nateprewitt",
      "created_at": "2021-12-29T04:13:20Z",
      "url": "https://github.com/psf/requests/pull/5917#discussion_r776146587"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5894,
      "file_path": "requests/sessions.py",
      "line": 636,
      "side": "LEFT",
      "diff_hunk": "@@ -633,7 +633,8 @@ def send(self, request, **kwargs):\n         kwargs.setdefault('stream', self.stream)\n         kwargs.setdefault('verify', self.verify)\n         kwargs.setdefault('cert', self.cert)\n-        kwargs.setdefault('proxies', self.rebuild_proxies(request, self.proxies))",
      "comment": "@sigmavirus24 this change is needed because without it we are calling to re-build proxies from the environment even when proxies have been set/provided. \r\n\r\nIn one use test case I have setup - the time to \"retrieve\" a cached connection (using the cache control adapter) 10,000 times goes from 9.741 seconds and 25212240 function calls to 9902240 function calls & 6.169 seconds after applying this change. ",
      "comment_id": 678786441,
      "user": "dbaxa",
      "created_at": "2021-07-29T02:59:31Z",
      "url": "https://github.com/psf/requests/pull/5894#discussion_r678786441"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5894,
      "file_path": "requests/sessions.py",
      "line": 293,
      "side": "RIGHT",
      "diff_hunk": "@@ -289,7 +289,9 @@ def rebuild_proxies(self, prepared_request, proxies):\n         new_proxies = proxies.copy()\n         no_proxy = proxies.get('no_proxy')\n \n-        bypass_proxy = should_bypass_proxies(url, no_proxy=no_proxy)\n+        bypass_proxy = False\n+        if self.trust_env:",
      "comment": "@omermizr  / @sigmavirus24 / @nateprewitt   - this change fixes the performance regression of #5891 but does not fix #5888. IMHO fixing #5888 is something that you might do by checking if a proxy has been selected as if one has not been selected you likely would not want to leak proxy credentials. Of course it might be easier to suggest that we do not explicitly support setting `Proxy-Authorization` as a header and that users must specify proxy credentials in the proxy url.",
      "comment_id": 679537744,
      "user": "dbaxa",
      "created_at": "2021-07-29T22:59:55Z",
      "url": "https://github.com/psf/requests/pull/5894#discussion_r679537744"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5894,
      "file_path": "requests/sessions.py",
      "line": 295,
      "side": "RIGHT",
      "diff_hunk": "@@ -289,7 +289,9 @@ def rebuild_proxies(self, prepared_request, proxies):\n         new_proxies = proxies.copy()\n         no_proxy = proxies.get('no_proxy')\n \n-        bypass_proxy = should_bypass_proxies(url, no_proxy=no_proxy)\n+        bypass_proxy = False\n+        if self.trust_env:\n+            bypass_proxy = should_bypass_proxies(url, no_proxy=no_proxy)\n         if self.trust_env and not bypass_proxy:",
      "comment": "Looking at the new change, I don't have an issue with the general idea. This seems like a performance improvement for rebuild_proxies, regardless of the outcome for #5888. Stylistically, I'd like to simplify the logic here a bit more though. Something along the lines of this would be preferred:\r\n\r\n```suggestion\r\n        if self.trust_env and not should_bypass_proxies(url, no_proxy=no_proxy):\r\n```\r\n\r\n@omermizr I don't want to volunteer you but if you have a moment, would you mind confirming this PR resolves your issue in #5891?",
      "comment_id": 687377459,
      "user": "nateprewitt",
      "created_at": "2021-08-12T04:24:39Z",
      "url": "https://github.com/psf/requests/pull/5894#discussion_r687377459"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5894,
      "file_path": "requests/sessions.py",
      "line": 295,
      "side": "RIGHT",
      "diff_hunk": "@@ -289,7 +289,9 @@ def rebuild_proxies(self, prepared_request, proxies):\n         new_proxies = proxies.copy()\n         no_proxy = proxies.get('no_proxy')\n \n-        bypass_proxy = should_bypass_proxies(url, no_proxy=no_proxy)\n+        bypass_proxy = False\n+        if self.trust_env:\n+            bypass_proxy = should_bypass_proxies(url, no_proxy=no_proxy)\n         if self.trust_env and not bypass_proxy:",
      "comment": "@nateprewitt While this is probably a good change, unless I'm missing something it doesn't resolve the issue - when 'trust_env' is set to True (which is the default), this changes nothing. \nMore specifically to my use case - I have no control over how the third party package I'm using creates the session, and it uses the default value for 'trust_env' (like most usages). ",
      "comment_id": 689175728,
      "user": "omermizr",
      "created_at": "2021-08-16T01:02:39Z",
      "url": "https://github.com/psf/requests/pull/5894#discussion_r689175728"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5391,
      "file_path": "requests/adapters.py",
      "line": 463,
      "side": "RIGHT",
      "diff_hunk": "@@ -459,7 +459,8 @@ def send(self, request, stream=False, timeout=None, verify=True, cert=None, prox\n                 try:\n                     low_conn.putrequest(request.method,\n                                         url,\n-                                        skip_accept_encoding=True)\n+                                        skip_accept_encoding=True,\n+                                        skip_host='Host' in request.headers)",
      "comment": "That's good, this appears to be the same way that the same situation is handled in the urllib3 code itself: https://github.com/urllib3/urllib3/blob/74d6be1ab66cef44c0f479c24b0fc1756a8fe4e9/src/urllib3/connection.py#L243-L246",
      "comment_id": 523465475,
      "user": "AaronRobson",
      "created_at": "2020-11-14T21:30:07Z",
      "url": "https://github.com/psf/requests/pull/5391#discussion_r523465475"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5924,
      "file_path": "requests/sessions.py",
      "line": 623,
      "side": "RIGHT",
      "diff_hunk": "@@ -633,7 +620,10 @@ def send(self, request, **kwargs):\n         kwargs.setdefault('stream', self.stream)\n         kwargs.setdefault('verify', self.verify)\n         kwargs.setdefault('cert', self.cert)\n-        kwargs.setdefault('proxies', self.rebuild_proxies(request, self.proxies))\n+        if 'proxies' not in kwargs:",
      "comment": "This change should actually fix the performance regression in most cases (`Session.request` - which I assume is the most common flow - always sets `proxies` on `kwargs`).",
      "comment_id": 701262009,
      "user": "omermizr",
      "created_at": "2021-09-02T16:52:25Z",
      "url": "https://github.com/psf/requests/pull/5924#discussion_r701262009"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5924,
      "file_path": "requests/sessions.py",
      "line": 624,
      "side": "RIGHT",
      "diff_hunk": "@@ -633,7 +620,10 @@ def send(self, request, **kwargs):\n         kwargs.setdefault('stream', self.stream)\n         kwargs.setdefault('verify', self.verify)\n         kwargs.setdefault('cert', self.cert)\n-        kwargs.setdefault('proxies', self.rebuild_proxies(request, self.proxies))\n+        if 'proxies' not in kwargs:\n+            kwargs['proxies'] = resolve_proxies(",
      "comment": "So now we're:\r\n1. No longer stripping the proxy-auth header. Sounds right to me.\r\n2. No longer setting the proxy-auth header at all. Is this the desired behavior? What if someone uses a proxy and passes the username + password in the url? (though I'm not sure if that flow is supported or not)",
      "comment_id": 701265200,
      "user": "omermizr",
      "created_at": "2021-09-02T16:56:54Z",
      "url": "https://github.com/psf/requests/pull/5924#discussion_r701265200"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5924,
      "file_path": "requests/utils.py",
      "line": 850,
      "side": "RIGHT",
      "diff_hunk": "@@ -830,6 +830,34 @@ def select_proxy(url, proxies):\n     return proxy\n \n \n+def resolve_proxies(request, proxies, trust_env=True):\n+    \"\"\"This method takes proxy information from a request and configuration\n+    input to resolve a mapping of target proxies. This will consider settings\n+    such a NO_PROXY to strip proxy configurations.\n+\n+    :param request: Request or PreparedRequest\n+    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs\n+    :param trust_env: Boolean declaring whether to trust environment configs\n+\n+    :rtype: dict\n+    \"\"\"\n+    proxies = proxies if proxies is not None else {}\n+    url = request.url\n+    scheme = urlparse(url).scheme\n+    no_proxy = proxies.get('no_proxy')\n+    new_proxies = proxies.copy()\n+\n+    bypass_proxy = should_bypass_proxies(url, no_proxy=no_proxy)",
      "comment": "Didn't we want to unify 850 and 851 so we don't call `should_bypass_proxies` if `trust_env` is `False`?",
      "comment_id": 701266063,
      "user": "omermizr",
      "created_at": "2021-09-02T16:58:04Z",
      "url": "https://github.com/psf/requests/pull/5924#discussion_r701266063"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5924,
      "file_path": "requests/sessions.py",
      "line": 624,
      "side": "RIGHT",
      "diff_hunk": "@@ -633,7 +620,10 @@ def send(self, request, **kwargs):\n         kwargs.setdefault('stream', self.stream)\n         kwargs.setdefault('verify', self.verify)\n         kwargs.setdefault('cert', self.cert)\n-        kwargs.setdefault('proxies', self.rebuild_proxies(request, self.proxies))\n+        if 'proxies' not in kwargs:\n+            kwargs['proxies'] = resolve_proxies(",
      "comment": "For 2.) we were never doing that before this change and I don't think that was an intended byproduct. We should still support our normal proxy auth flows that were available prior to #5681.",
      "comment_id": 701276964,
      "user": "nateprewitt",
      "created_at": "2021-09-02T17:13:41Z",
      "url": "https://github.com/psf/requests/pull/5924#discussion_r701276964"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5924,
      "file_path": "requests/utils.py",
      "line": 850,
      "side": "RIGHT",
      "diff_hunk": "@@ -830,6 +830,34 @@ def select_proxy(url, proxies):\n     return proxy\n \n \n+def resolve_proxies(request, proxies, trust_env=True):\n+    \"\"\"This method takes proxy information from a request and configuration\n+    input to resolve a mapping of target proxies. This will consider settings\n+    such a NO_PROXY to strip proxy configurations.\n+\n+    :param request: Request or PreparedRequest\n+    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs\n+    :param trust_env: Boolean declaring whether to trust environment configs\n+\n+    :rtype: dict\n+    \"\"\"\n+    proxies = proxies if proxies is not None else {}\n+    url = request.url\n+    scheme = urlparse(url).scheme\n+    no_proxy = proxies.get('no_proxy')\n+    new_proxies = proxies.copy()\n+\n+    bypass_proxy = should_bypass_proxies(url, no_proxy=no_proxy)",
      "comment": "We do, I was going to have @dbaxa rebase their change onto whatever we merge so they can still have a commit in the repo.",
      "comment_id": 701277868,
      "user": "nateprewitt",
      "created_at": "2021-09-02T17:14:52Z",
      "url": "https://github.com/psf/requests/pull/5924#discussion_r701277868"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5924,
      "file_path": "requests/sessions.py",
      "line": 623,
      "side": "RIGHT",
      "diff_hunk": "@@ -633,7 +620,10 @@ def send(self, request, **kwargs):\n         kwargs.setdefault('stream', self.stream)\n         kwargs.setdefault('verify', self.verify)\n         kwargs.setdefault('cert', self.cert)\n-        kwargs.setdefault('proxies', self.rebuild_proxies(request, self.proxies))\n+        if 'proxies' not in kwargs:",
      "comment": "Yep, I think `proxies` will be the escape hatch when performance is a concern here :)",
      "comment_id": 701278739,
      "user": "nateprewitt",
      "created_at": "2021-09-02T17:16:02Z",
      "url": "https://github.com/psf/requests/pull/5924#discussion_r701278739"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5707,
      "file_path": "requests/utils.py",
      "line": 268,
      "side": "RIGHT",
      "diff_hunk": "@@ -256,13 +256,28 @@ def extract_zipped_paths(path):\n \n     # we have a valid zip archive and a valid member of that archive\n     tmp = tempfile.gettempdir()\n-    extracted_path = os.path.join(tmp, *member.split('/'))\n+    extracted_path = os.path.join(tmp, member.split('/')[-1])\n     if not os.path.exists(extracted_path):\n-        extracted_path = zip_file.extract(member, path=tmp)\n-\n+        # use read + write to avoid the creating nested folders, we only want the file, avoids mkdir racing condition\n+        with atomic_open(extracted_path) as file_handler:\n+            file_handler.write(zip_file.read(member))\n     return extracted_path\n \n \n+@contextlib.contextmanager\n+def atomic_open(filename):",
      "comment": "Just to handle a further race condition that can happen between opening and writing the file.",
      "comment_id": 548761782,
      "user": "gaborbernat",
      "created_at": "2020-12-24T23:18:20Z",
      "url": "https://github.com/psf/requests/pull/5707#discussion_r548761782"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5856,
      "file_path": "requests/compat.py",
      "line": 34,
      "side": "LEFT",
      "diff_hunk": "@@ -25,10 +26,6 @@\n #: Python 3.x?\n is_py3 = (_ver[0] == 3)\n \n-try:\n-    import simplejson as json\n-except ImportError:\n-    import json",
      "comment": "This is backwards incompatible and needs to be reverted",
      "comment_id": 664037480,
      "user": "sigmavirus24",
      "created_at": "2021-07-05T16:19:59Z",
      "url": "https://github.com/psf/requests/pull/5856#discussion_r664037480"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5856,
      "file_path": "requests/exceptions.py",
      "line": 42,
      "side": "RIGHT",
      "diff_hunk": "@@ -25,6 +35,10 @@ def __init__(self, *args, **kwargs):\n         super(RequestException, self).__init__(*args, **kwargs)\n \n \n+class JSONDecodeError(StandardJSONDecodeError, SimpleJSONDecodeError):\n+    \"\"\"Couldn't decode the text into json\"\"\"\n+\n+\n class InvalidJSONError(RequestException):",
      "comment": "I forgot we had this exception, where do we use it? Should we sub-class this in `JSONDecodeError` as well?",
      "comment_id": 664037858,
      "user": "sigmavirus24",
      "created_at": "2021-07-05T16:20:44Z",
      "url": "https://github.com/psf/requests/pull/5856#discussion_r664037858"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5856,
      "file_path": "requests/models.py",
      "line": 893,
      "side": "RIGHT",
      "diff_hunk": "@@ -882,12 +890,8 @@ def json(self, **kwargs):\n         r\"\"\"Returns the json-encoded content of a response, if any.\n \n         :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n-        :raises simplejson.JSONDecodeError: If the response body does not\n-            contain valid json and simplejson is installed.\n-        :raises json.JSONDecodeError: If the response body does not contain\n-            valid json and simplejson is not installed on Python 3.\n-        :raises ValueError: If the response body does not contain valid\n-            json and simplejson is not installed on Python 2.        \n+        :raises requests.JSONDecodeError: If the response body does not",
      "comment": "Please document this as `requests.exceptions.JSONDecodeError`. Eventually I desperately want to stop importing every exception class into `requests/__init__.py`",
      "comment_id": 664039064,
      "user": "sigmavirus24",
      "created_at": "2021-07-05T16:23:21Z",
      "url": "https://github.com/psf/requests/pull/5856#discussion_r664039064"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5856,
      "file_path": "requests/models.py",
      "line": 920,
      "side": "RIGHT",
      "diff_hunk": "@@ -907,7 +911,13 @@ def json(self, **kwargs):\n                     # and the server didn't bother to tell us what codec *was*\n                     # used.\n                     pass\n-        return complexjson.loads(self.text, **kwargs)\n+\n+        try:\n+            return json.loads(self.text, **kwargs)\n+        except json.JSONDecodeError as e:\n+            # Catch JSON-related errors and raise as requests.JSONDecodeError\n+            # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n+            raise JSONDecodeError(e.msg, e.doc, e.pos)",
      "comment": "We should probably preserve the original exception instance in this as well and proxy attributes down to it. That way folks can find the original exception. I'm not certain `simplejson` and `json` have the same exception attributes with the same meaning",
      "comment_id": 664039537,
      "user": "sigmavirus24",
      "created_at": "2021-07-05T16:24:32Z",
      "url": "https://github.com/psf/requests/pull/5856#discussion_r664039537"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5856,
      "file_path": "requests/exceptions.py",
      "line": 42,
      "side": "RIGHT",
      "diff_hunk": "@@ -25,6 +35,10 @@ def __init__(self, *args, **kwargs):\n         super(RequestException, self).__init__(*args, **kwargs)\n \n \n+class JSONDecodeError(StandardJSONDecodeError, SimpleJSONDecodeError):\n+    \"\"\"Couldn't decode the text into json\"\"\"\n+\n+\n class InvalidJSONError(RequestException):",
      "comment": "I added this exception to sub-class both `JSONDecodeError`s",
      "comment_id": 664043187,
      "user": "steveberdy",
      "created_at": "2021-07-05T16:32:45Z",
      "url": "https://github.com/psf/requests/pull/5856#discussion_r664043187"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5856,
      "file_path": "requests/exceptions.py",
      "line": 42,
      "side": "RIGHT",
      "diff_hunk": "@@ -25,6 +35,10 @@ def __init__(self, *args, **kwargs):\n         super(RequestException, self).__init__(*args, **kwargs)\n \n \n+class JSONDecodeError(StandardJSONDecodeError, SimpleJSONDecodeError):\n+    \"\"\"Couldn't decode the text into json\"\"\"\n+\n+\n class InvalidJSONError(RequestException):",
      "comment": "I'm talking about the `InvalidJSONError` exception here (the line I commented on)",
      "comment_id": 664949709,
      "user": "sigmavirus24",
      "created_at": "2021-07-07T00:02:39Z",
      "url": "https://github.com/psf/requests/pull/5856#discussion_r664949709"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5856,
      "file_path": "requests/compat.py",
      "line": 36,
      "side": "RIGHT",
      "diff_hunk": "@@ -33,6 +33,7 @@\n except ImportError:\n     import json\n \n+",
      "comment": "These spacing changes aren't really related to the current PR, can we remove them.",
      "comment_id": 667094313,
      "user": "nateprewitt",
      "created_at": "2021-07-09T17:05:30Z",
      "url": "https://github.com/psf/requests/pull/5856#discussion_r667094313"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5856,
      "file_path": "requests/exceptions.py",
      "line": 38,
      "side": "RIGHT",
      "diff_hunk": "@@ -25,6 +35,10 @@ def __init__(self, *args, **kwargs):\n         super(RequestException, self).__init__(*args, **kwargs)\n \n \n+class JSONDecodeError(StandardJSONDecodeError, SimpleJSONDecodeError):",
      "comment": "Our custom exceptions in Requests should ideally fall under a catch-all parent exception, `RequestException`. That allows users to catch everything thrown by Requests.",
      "comment_id": 667095839,
      "user": "nateprewitt",
      "created_at": "2021-07-09T17:08:32Z",
      "url": "https://github.com/psf/requests/pull/5856#discussion_r667095839"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5856,
      "file_path": "requests/exceptions.py",
      "line": 42,
      "side": "RIGHT",
      "diff_hunk": "@@ -25,6 +35,10 @@ def __init__(self, *args, **kwargs):\n         super(RequestException, self).__init__(*args, **kwargs)\n \n \n+class JSONDecodeError(StandardJSONDecodeError, SimpleJSONDecodeError):\n+    \"\"\"Couldn't decode the text into json\"\"\"\n+\n+\n class InvalidJSONError(RequestException):",
      "comment": "I think we're still waiting on a response here, @steveberdy.",
      "comment_id": 667096117,
      "user": "nateprewitt",
      "created_at": "2021-07-09T17:09:04Z",
      "url": "https://github.com/psf/requests/pull/5856#discussion_r667096117"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5856,
      "file_path": "requests/models.py",
      "line": 920,
      "side": "RIGHT",
      "diff_hunk": "@@ -907,7 +911,13 @@ def json(self, **kwargs):\n                     # and the server didn't bother to tell us what codec *was*\n                     # used.\n                     pass\n-        return complexjson.loads(self.text, **kwargs)\n+\n+        try:\n+            return json.loads(self.text, **kwargs)\n+        except json.JSONDecodeError as e:\n+            # Catch JSON-related errors and raise as requests.JSONDecodeError\n+            # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n+            raise JSONDecodeError(e.msg, e.doc, e.pos)",
      "comment": "I'm not sure I'm following how this will work in the Python 2 `json` case. Where is `ValueError` being handled?",
      "comment_id": 667104578,
      "user": "nateprewitt",
      "created_at": "2021-07-09T17:24:48Z",
      "url": "https://github.com/psf/requests/pull/5856#discussion_r667104578"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5856,
      "file_path": "requests/models.py",
      "line": 920,
      "side": "RIGHT",
      "diff_hunk": "@@ -907,7 +911,13 @@ def json(self, **kwargs):\n                     # and the server didn't bother to tell us what codec *was*\n                     # used.\n                     pass\n-        return complexjson.loads(self.text, **kwargs)\n+\n+        try:\n+            return json.loads(self.text, **kwargs)\n+        except json.JSONDecodeError as e:\n+            # Catch JSON-related errors and raise as requests.JSONDecodeError\n+            # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n+            raise JSONDecodeError(e.msg, e.doc, e.pos)",
      "comment": "https://github.com/psf/requests/pull/5856/checks?check_run_id=3031103164 Yeah, this is broken. We need to be able to handle this a different way.",
      "comment_id": 667105634,
      "user": "nateprewitt",
      "created_at": "2021-07-09T17:26:43Z",
      "url": "https://github.com/psf/requests/pull/5856#discussion_r667105634"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5856,
      "file_path": "requests/models.py",
      "line": 920,
      "side": "RIGHT",
      "diff_hunk": "@@ -907,7 +911,13 @@ def json(self, **kwargs):\n                     # and the server didn't bother to tell us what codec *was*\n                     # used.\n                     pass\n-        return complexjson.loads(self.text, **kwargs)\n+\n+        try:\n+            return json.loads(self.text, **kwargs)\n+        except json.JSONDecodeError as e:\n+            # Catch JSON-related errors and raise as requests.JSONDecodeError\n+            # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n+            raise JSONDecodeError(e.msg, e.doc, e.pos)",
      "comment": "I see your last comment was made outside of this thread, @steveberdy. To clarify, while `json.JSONDecodeError` does inherit from `ValueError`, it's not going to work in this case. The exception doesn't exist in Python 2, so we fall over with an attribute error when handling the exception. You'll need to take another approach to handle this.",
      "comment_id": 667121799,
      "user": "nateprewitt",
      "created_at": "2021-07-09T17:56:48Z",
      "url": "https://github.com/psf/requests/pull/5856#discussion_r667121799"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5856,
      "file_path": "requests/exceptions.py",
      "line": 42,
      "side": "RIGHT",
      "diff_hunk": "@@ -25,6 +35,10 @@ def __init__(self, *args, **kwargs):\n         super(RequestException, self).__init__(*args, **kwargs)\n \n \n+class JSONDecodeError(StandardJSONDecodeError, SimpleJSONDecodeError):\n+    \"\"\"Couldn't decode the text into json\"\"\"\n+\n+\n class InvalidJSONError(RequestException):",
      "comment": "Yes, I can subclass that in `JSONDecodeError` as well.",
      "comment_id": 668015646,
      "user": "steveberdy",
      "created_at": "2021-07-12T15:07:16Z",
      "url": "https://github.com/psf/requests/pull/5856#discussion_r668015646"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5856,
      "file_path": "requests/models.py",
      "line": 918,
      "side": "RIGHT",
      "diff_hunk": "@@ -907,7 +905,16 @@ def json(self, **kwargs):\n                     # and the server didn't bother to tell us what codec *was*\n                     # used.\n                     pass\n-        return complexjson.loads(self.text, **kwargs)\n+\n+        try:\n+            return complexjson.loads(self.text, **kwargs)\n+        except JSONDecodeError as e:\n+            # Catch JSON-related errors and raise as requests.JSONDecodeError\n+            # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n+            if is_py2: # e is a ValueError\n+                raise RequestsJSONDecodeError()\n+            else:\n+                raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n ",
      "comment": "Giving the user 0 useful information is unacceptable. Why can't we use `e.message` there?\r\n\r\n```py\r\nPython 2.7.18 (default, May 19 2021, 00:00:00)\r\n[GCC 11.1.1 20210428 (Red Hat 11.1.1-1)] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import json\r\n>>> try:\r\n...   json.loads(\"foo\")\r\n... except ValueError as ve:\r\n...   pass\r\n...\r\n>>> dir(ve)\r\n['__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__getitem__', '__getslice__', '__hash__', '__init__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', 'args', 'message']\r\n>>> ve.message\r\n'No JSON object could be decoded'\r\n>>> ve.args\r\n('No JSON object could be decoded',)\r\n>>>\r\n```",
      "comment_id": 669195075,
      "user": "sigmavirus24",
      "created_at": "2021-07-14T00:25:15Z",
      "url": "https://github.com/psf/requests/pull/5856#discussion_r669195075"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5856,
      "file_path": "requests/compat.py",
      "line": 31,
      "side": "RIGHT",
      "diff_hunk": "@@ -28,8 +28,10 @@\n #: Python 3.x?\n is_py3 = (_ver[0] == 3)\n \n+has_simplejson = False",
      "comment": "It doesn't look like we use this anywhere except to declare the exceptions. We're already setting either dependency to be imported under the `json` namespace. Can we not just do `from json import JSONDecodeError` under the py3 section and have it automatically resolve? Perhaps adding a short comment above the import saying it's using simplejson or json depending on what was resolved?",
      "comment_id": 669741494,
      "user": "nateprewitt",
      "created_at": "2021-07-14T15:49:47Z",
      "url": "https://github.com/psf/requests/pull/5856#discussion_r669741494"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5856,
      "file_path": "requests/exceptions.py",
      "line": 11,
      "side": "RIGHT",
      "diff_hunk": "@@ -8,6 +8,16 @@\n \"\"\"\n from urllib3.exceptions import HTTPError as BaseHTTPError\n \n+try:",
      "comment": "We have logic for which simplejson/json exceptions we're using spread across a few places now. Is there a compelling reason not to simply import JSONDecodeError from compat since it's already been resolved and supplying it as a single parent class?",
      "comment_id": 671444604,
      "user": "nateprewitt",
      "created_at": "2021-07-16T18:18:17Z",
      "url": "https://github.com/psf/requests/pull/5856#discussion_r671444604"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5856,
      "file_path": "requests/exceptions.py",
      "line": 11,
      "side": "RIGHT",
      "diff_hunk": "@@ -8,6 +8,16 @@\n \"\"\"\n from urllib3.exceptions import HTTPError as BaseHTTPError\n \n+try:",
      "comment": "@nateprewitt We want to know which properties the `JSONDecodeError` has, since it may have different properties depending on compatibility. That requires an extra check on import.",
      "comment_id": 673240914,
      "user": "steveberdy",
      "created_at": "2021-07-20T15:40:55Z",
      "url": "https://github.com/psf/requests/pull/5856#discussion_r673240914"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5856,
      "file_path": "requests/exceptions.py",
      "line": 11,
      "side": "RIGHT",
      "diff_hunk": "@@ -8,6 +8,16 @@\n \"\"\"\n from urllib3.exceptions import HTTPError as BaseHTTPError\n \n+try:",
      "comment": "@steveberdy I'm not sure I'm following, could you clarify which properties you're referring to? We have 3 parent classes that we'd ever use in our environment, `json.JSONDecodeError`, `simplejson.JSONDecodeError`, and `ValueError`. Only one of those can be relevant in a given execution of Requests.\r\n\r\nWhat I was asking about was if we did:\r\n\r\n```python\r\nfrom .compat import JSONDecodeError as CompatJSONDecodeError\r\n\r\n[...]\r\n\r\nclass JSONDecodeError(InvalidJSONError, CompatJSONDecodeError):\r\n```\r\n\r\nWe've already resolved these values once in compat, so I wanted to know why we're doing it again here.",
      "comment_id": 673356731,
      "user": "nateprewitt",
      "created_at": "2021-07-20T17:53:17Z",
      "url": "https://github.com/psf/requests/pull/5856#discussion_r673356731"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5856,
      "file_path": "requests/exceptions.py",
      "line": 11,
      "side": "RIGHT",
      "diff_hunk": "@@ -8,6 +8,16 @@\n \"\"\"\n from urllib3.exceptions import HTTPError as BaseHTTPError\n \n+try:",
      "comment": "@nateprewitt For Python 3, in the `compat.py` file itself, reference to the `json` package directly, whether aliased as `json` or actually the `json` package itself, would always look for the `json` package rather than whatever was aliased as it. So, just in the `compat.py` file itself, I used the check. From all other scripts that have `from .compat import json`, however, it works as expected.",
      "comment_id": 673381345,
      "user": "steveberdy",
      "created_at": "2021-07-20T18:30:36Z",
      "url": "https://github.com/psf/requests/pull/5856#discussion_r673381345"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5856,
      "file_path": "requests/exceptions.py",
      "line": 11,
      "side": "RIGHT",
      "diff_hunk": "@@ -8,6 +8,16 @@\n \"\"\"\n from urllib3.exceptions import HTTPError as BaseHTTPError\n \n+try:",
      "comment": "Hey @steveberdy, I think there may still be some confusion. If you open the _Files changed_ tab, you should be able to see the full context of this thread. I've written out the [full diff](https://github.com/nateprewitt/requests/commit/665cf30861fb356d84103808e240d6940e8ff3d6) from the suggestion above to minimize ambiguity. You should find this passes the test suite without issue, so please let me know if there were other concerns you had.\r\n\r\nWhile we're here it would also be good to add a test exercising this change. Something to the effect of the example below should be sufficient:\r\n\r\n```\r\ndef test_json_decode_compatibility(self, httpbin):\r\n    r = requests.get(httpbin('bytes/20'))\r\n    with pytest.raises(requests.exceptions.JSONDecodeError):\r\n        r.json()\r\n```\r\n\r\nThis won't cover simplejson in our general test suite, but I've done a one off run to ensure that's working.",
      "comment_id": 675272659,
      "user": "nateprewitt",
      "created_at": "2021-07-23T02:30:46Z",
      "url": "https://github.com/psf/requests/pull/5856#discussion_r675272659"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5851,
      "file_path": "requests/utils.py",
      "line": 248,
      "side": "RIGHT",
      "diff_hunk": "@@ -245,6 +245,9 @@ def extract_zipped_paths(path):\n     archive, member = os.path.split(path)\n     while archive and not os.path.exists(archive):\n         archive, prefix = os.path.split(archive)\n+        if not prefix:",
      "comment": "This explains why, but the code isn't clear as to what's happening or why this fixes the infinite loop. For example, a far superior comment might be:\r\n\r\n```\r\n# If we don't check for an empty prefix after the split (in other words, archive = / before and after the split), we _can_ end up in an infinite loop on a rare corner case affecting a small number of users\r\n```\r\n\r\nThis comment explains the conditions, why we care (it affects users), and the risk to making a change (if we break this again, we may not find out about it quickly)",
      "comment_id": 664947915,
      "user": "sigmavirus24",
      "created_at": "2021-07-06T23:56:55Z",
      "url": "https://github.com/psf/requests/pull/5851#discussion_r664947915"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5851,
      "file_path": "requests/utils.py",
      "line": 248,
      "side": "RIGHT",
      "diff_hunk": "@@ -245,6 +245,9 @@ def extract_zipped_paths(path):\n     archive, member = os.path.split(path)\n     while archive and not os.path.exists(archive):\n         archive, prefix = os.path.split(archive)\n+        if not prefix:",
      "comment": "Thanks for the feedback, I've updated the comment. ",
      "comment_id": 681158168,
      "user": "tl-hbk",
      "created_at": "2021-08-02T17:45:39Z",
      "url": "https://github.com/psf/requests/pull/5851#discussion_r681158168"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5797,
      "file_path": "setup.py",
      "line": 44,
      "side": "RIGHT",
      "diff_hunk": "@@ -41,7 +41,8 @@ def run_tests(self):\n packages = ['requests']\n \n requires = [\n-    'chardet>=3.0.2,<5',\n+    'charset_normalizer>=1.3.9,<2; python_version >= \"3\"',",
      "comment": "```suggestion\r\n    'charset_normalizer~=1.4.0,<2; python_version >= \"3\"',\r\n```",
      "comment_id": 637236978,
      "user": "potiuk",
      "created_at": "2021-05-21T21:16:35Z",
      "url": "https://github.com/psf/requests/pull/5797#discussion_r637236978"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5797,
      "file_path": "setup.py",
      "line": 44,
      "side": "RIGHT",
      "diff_hunk": "@@ -41,7 +41,8 @@ def run_tests(self):\n packages = ['requests']\n \n requires = [\n-    'chardet>=3.0.2,<5',\n+    'charset_normalizer~=1.4.0,<2; python_version >= \"3\"',",
      "comment": "```suggestion\r\n    'charset_normalizer~=1.4.0; python_version >= \"3\"',\r\n```",
      "comment_id": 637237986,
      "user": "potiuk",
      "created_at": "2021-05-21T21:17:59Z",
      "url": "https://github.com/psf/requests/pull/5797#discussion_r637237986"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5797,
      "file_path": "setup.py",
      "line": 107,
      "side": "RIGHT",
      "diff_hunk": "@@ -103,6 +104,7 @@ def run_tests(self):\n         'security': ['pyOpenSSL >= 0.14', 'cryptography>=1.3.4'],\n         'socks': ['PySocks>=1.5.6, !=1.5.7'],\n         'socks:sys_platform == \"win32\" and python_version == \"2.7\"': ['win_inet_pton'],\n+        'lgpl': ['chardet>=3.0.2,<5']",
      "comment": "This seems somewhat disingenuous. For one thing, it's not the _only_ way to get `lgpl` dependencies. For another, it really exists to just force `chardet` to be installed on py3 so should be named something more like `use_chardet_on_py3`",
      "comment_id": 641068555,
      "user": "sigmavirus24",
      "created_at": "2021-05-28T00:15:43Z",
      "url": "https://github.com/psf/requests/pull/5797#discussion_r641068555"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5797,
      "file_path": "requests/__init__.py",
      "line": 83,
      "side": "RIGHT",
      "diff_hunk": "@@ -62,12 +70,19 @@ def check_compatibility(urllib3_version, chardet_version):\n     assert minor >= 21\n     assert minor <= 26\n \n-    # Check chardet for compatibility.\n-    major, minor, patch = chardet_version.split('.')[:3]\n-    major, minor, patch = int(major), int(minor), int(patch)\n-    # chardet >= 3.0.2, < 5.0.0\n-    assert (3, 0, 2) <= (major, minor, patch) < (5, 0, 0)\n-\n+    # Check charset_normalizer for compatibility.\n+    if chardet_version:\n+        major, minor, patch = chardet_version.split('.')[:3]\n+        major, minor, patch = int(major), int(minor), int(patch)\n+        # chardet_version >= 3.0.2, < 5.0.0\n+        assert (3, 0, 2) <= (major, minor, patch) < (5, 0, 0)\n+    elif charset_normalizer_version:\n+        major, minor, patch = charset_normalizer_version.split('.')[:3]\n+        major, minor, patch = int(major), int(minor), int(patch)\n+        # charset_normalizer >= 1.3.9, < 2.0.0\n+        assert (1, 3, 9) <= (major, minor, patch) < (2, 0, 0)",
      "comment": "Changed to 1.4.1 to account for logging fix just added by @ousret ",
      "comment_id": 641297465,
      "user": "potiuk",
      "created_at": "2021-05-28T06:22:26Z",
      "url": "https://github.com/psf/requests/pull/5797#discussion_r641297465"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5797,
      "file_path": "requests/__init__.py",
      "line": 82,
      "side": "RIGHT",
      "diff_hunk": "@@ -62,12 +70,19 @@ def check_compatibility(urllib3_version, chardet_version):\n     assert minor >= 21\n     assert minor <= 26\n \n-    # Check chardet for compatibility.\n-    major, minor, patch = chardet_version.split('.')[:3]\n-    major, minor, patch = int(major), int(minor), int(patch)\n-    # chardet >= 3.0.2, < 5.0.0\n-    assert (3, 0, 2) <= (major, minor, patch) < (5, 0, 0)\n-\n+    # Check charset_normalizer for compatibility.\n+    if chardet_version:\n+        major, minor, patch = chardet_version.split('.')[:3]\n+        major, minor, patch = int(major), int(minor), int(patch)\n+        # chardet_version >= 3.0.2, < 5.0.0\n+        assert (3, 0, 2) <= (major, minor, patch) < (5, 0, 0)\n+    elif charset_normalizer_version:\n+        major, minor, patch = charset_normalizer_version.split('.')[:3]\n+        major, minor, patch = int(major), int(minor), int(patch)\n+        # charset_normalizer >= 1.3.9, < 2.0.0",
      "comment": "```suggestion\r\n        # charset_normalizer >= 1.4.1, < 2.0.0\r\n```",
      "comment_id": 641438297,
      "user": "mik-laj",
      "created_at": "2021-05-28T10:16:17Z",
      "url": "https://github.com/psf/requests/pull/5797#discussion_r641438297"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5797,
      "file_path": "requests/__init__.py",
      "line": 83,
      "side": "RIGHT",
      "diff_hunk": "@@ -62,12 +70,19 @@ def check_compatibility(urllib3_version, chardet_version):\n     assert minor >= 21\n     assert minor <= 26\n \n-    # Check chardet for compatibility.\n-    major, minor, patch = chardet_version.split('.')[:3]\n-    major, minor, patch = int(major), int(minor), int(patch)\n-    # chardet >= 3.0.2, < 5.0.0\n-    assert (3, 0, 2) <= (major, minor, patch) < (5, 0, 0)\n-\n+    # Check charset_normalizer for compatibility.\n+    if chardet_version:\n+        major, minor, patch = chardet_version.split('.')[:3]\n+        major, minor, patch = int(major), int(minor), int(patch)\n+        # chardet_version >= 3.0.2, < 5.0.0\n+        assert (3, 0, 2) <= (major, minor, patch) < (5, 0, 0)\n+    elif charset_normalizer_version:\n+        major, minor, patch = charset_normalizer_version.split('.')[:3]\n+        major, minor, patch = int(major), int(minor), int(patch)\n+        # charset_normalizer >= 1.4.1, < 2.0.0\n+        assert (1, 4, 1) <= (major, minor, patch) < (2, 0, 0)",
      "comment": "If charset_normalizer v2.0.0 is out do we need to update this to be >2?",
      "comment_id": 664014262,
      "user": "sethmlarson",
      "created_at": "2021-07-05T15:34:07Z",
      "url": "https://github.com/psf/requests/pull/5797#discussion_r664014262"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5797,
      "file_path": "setup.py",
      "line": 44,
      "side": "RIGHT",
      "diff_hunk": "@@ -41,7 +41,8 @@ def run_tests(self):\n packages = ['requests']\n \n requires = [\n-    'chardet>=3.0.2,<5',\n+    'charset_normalizer~=1.4.1; python_version >= \"3\"',",
      "comment": "We haven't tested v2. I'd rather get something imperfect out than something perfect. Today's literally the only time I've had to review anything",
      "comment_id": 664015764,
      "user": "sigmavirus24",
      "created_at": "2021-07-05T15:36:53Z",
      "url": "https://github.com/psf/requests/pull/5797#discussion_r664015764"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5797,
      "file_path": "setup.py",
      "line": 44,
      "side": "RIGHT",
      "diff_hunk": "@@ -41,7 +41,8 @@ def run_tests(self):\n packages = ['requests']\n \n requires = [\n-    'chardet>=3.0.2,<5',\n+    'charset_normalizer~=1.4.1; python_version >= \"3\"',",
      "comment": "Makes sense, I/we can handle the upgrade in a follow-up PR.",
      "comment_id": 664016609,
      "user": "sethmlarson",
      "created_at": "2021-07-05T15:38:30Z",
      "url": "https://github.com/psf/requests/pull/5797#discussion_r664016609"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5783,
      "file_path": "requests/utils.py",
      "line": 824,
      "side": "RIGHT",
      "diff_hunk": "@@ -820,7 +821,7 @@ def default_headers():\n     \"\"\"\n     return CaseInsensitiveDict({\n         'User-Agent': default_user_agent(),\n-        'Accept-Encoding': ', '.join(('gzip', 'deflate')),\n+        'Accept-Encoding': make_headers(accept_encoding=True)[\"accept-encoding\"],",
      "comment": "Should we generate this header once and store the result so it doesn't need to be called every time? The value is determined by imports so shouldn't be changing often.",
      "comment_id": 636462152,
      "user": "sethmlarson",
      "created_at": "2021-05-20T20:52:06Z",
      "url": "https://github.com/psf/requests/pull/5783#discussion_r636462152"
    },
    {
      "repo": "psf/requests",
      "pr_number": 5783,
      "file_path": "requests/utils.py",
      "line": 824,
      "side": "RIGHT",
      "diff_hunk": "@@ -820,7 +821,7 @@ def default_headers():\n     \"\"\"\n     return CaseInsensitiveDict({\n         'User-Agent': default_user_agent(),\n-        'Accept-Encoding': ', '.join(('gzip', 'deflate')),\n+        'Accept-Encoding': make_headers(accept_encoding=True)[\"accept-encoding\"],",
      "comment": "This optimization makes sense.  I kindly ask you to implement it in the way, you think fits best.",
      "comment_id": 636746931,
      "user": "dilyanpalauzov",
      "created_at": "2021-05-21T08:45:36Z",
      "url": "https://github.com/psf/requests/pull/5783#discussion_r636746931"
    },
    {
      "repo": "psf/requests",
      "pr_number": 1660,
      "file_path": "requests/cookies.py",
      "line": 59,
      "side": "RIGHT",
      "diff_hunk": "@@ -45,7 +45,14 @@ def get_origin_req_host(self):\n         return self.get_host()\n \n     def get_full_url(self):\n-        return self._r.url\n+        if not self._r.headers.get('Host'):\n+            return self._r.url\n+        host = self._r.headers['Host']\n+        parsed = urlparse(self._r.url)\n+        return urlunparse([\n+            parsed.scheme, host, parsed.path, parsed.params, parsed.query,\n+            parsed.fragment\n+        ])",
      "comment": "Might be a good idea to comment this so we can remember why we did this weird thing. =)\n",
      "comment_id": 6863250,
      "user": "Lukasa",
      "created_at": "2013-10-09T17:34:25Z",
      "url": "https://github.com/psf/requests/pull/1660#discussion_r6863250"
    },
    {
      "repo": "psf/requests",
      "pr_number": 1660,
      "file_path": "requests/cookies.py",
      "line": 59,
      "side": "RIGHT",
      "diff_hunk": "@@ -45,7 +45,14 @@ def get_origin_req_host(self):\n         return self.get_host()\n \n     def get_full_url(self):\n-        return self._r.url\n+        if not self._r.headers.get('Host'):\n+            return self._r.url\n+        host = self._r.headers['Host']\n+        parsed = urlparse(self._r.url)\n+        return urlunparse([\n+            parsed.scheme, host, parsed.path, parsed.params, parsed.query,\n+            parsed.fragment\n+        ])",
      "comment": "LOL If you don't write Ruby and your code isn't self-documenting. ;)\n",
      "comment_id": 6865822,
      "user": "sigmavirus24",
      "created_at": "2013-10-09T18:44:50Z",
      "url": "https://github.com/psf/requests/pull/1660#discussion_r6865822"
    },
    {
      "repo": "psf/requests",
      "pr_number": 1660,
      "file_path": "requests/cookies.py",
      "line": null,
      "side": "RIGHT",
      "diff_hunk": "@@ -45,7 +45,18 @@ def get_origin_req_host(self):\n         return self.get_host()\n \n     def get_full_url(self):\n-        return self._r.url\n+        # Only return the response's URL if the user hadn't set the Host\n+        # header\n+        if not self._r.headers.get('Host'):\n+            return self._r.url\n+        # If they did set it, retrieve it and reconstruct the expected doain",
      "comment": "Psh. Who cares about proper spelling? :wink: \n\nSeriously though, thanks! :cake:\n",
      "comment_id": 6930211,
      "user": "sigmavirus24",
      "created_at": "2013-10-12T01:41:34Z",
      "url": "https://github.com/psf/requests/pull/1660#discussion_r6930211"
    },
    {
      "repo": "psf/requests",
      "pr_number": 1515,
      "file_path": "requests/packages/urllib3/poolmanager.py",
      "line": 253,
      "side": "RIGHT",
      "diff_hunk": "@@ -187,16 +238,22 @@ def _set_proxy_headers(self, url, headers=None):\n \n         if headers:\n             headers_.update(headers)\n-\n         return headers_\n \n-    def urlopen(self, method, url, **kw):\n+    def urlopen(self, method, url, redirect=True, **kw):\n         \"Same as HTTP(S)ConnectionPool.urlopen, ``url`` must be absolute.\"\n-        kw['assert_same_host'] = False\n-        kw['headers'] = self._set_proxy_headers(url, headers=kw.get('headers'))\n-        return self.proxy_pool.urlopen(method, url, **kw)\n+        u = parse_url(url)\n+\n+        if u.scheme == \"http\":\n+            # It's too late to set proxy headers on per-request basis for\n+            # tunnelled HTTPS connections, should use\n+            # constructor's proxy_headers instead.\n+            kw['headers'] = self._set_proxy_headers(url, kw.get('headers',\n+                                                                self.headers))\n+            kw['headers'].update(self.proxy_headers)",
      "comment": "We need the set the Proxy-Authorization header regardless of HTTP or HTTPS. Is there a reason urllib3 can't do it? Because if there is, we'll need to fix it up in Requests.\n",
      "comment_id": 5728742,
      "user": "Lukasa",
      "created_at": "2013-08-13T05:52:06Z",
      "url": "https://github.com/psf/requests/pull/1515#discussion_r5728742"
    },
    {
      "repo": "psf/requests",
      "pr_number": 1515,
      "file_path": "requests/packages/urllib3/poolmanager.py",
      "line": 253,
      "side": "RIGHT",
      "diff_hunk": "@@ -187,16 +238,22 @@ def _set_proxy_headers(self, url, headers=None):\n \n         if headers:\n             headers_.update(headers)\n-\n         return headers_\n \n-    def urlopen(self, method, url, **kw):\n+    def urlopen(self, method, url, redirect=True, **kw):\n         \"Same as HTTP(S)ConnectionPool.urlopen, ``url`` must be absolute.\"\n-        kw['assert_same_host'] = False\n-        kw['headers'] = self._set_proxy_headers(url, headers=kw.get('headers'))\n-        return self.proxy_pool.urlopen(method, url, **kw)\n+        u = parse_url(url)\n+\n+        if u.scheme == \"http\":\n+            # It's too late to set proxy headers on per-request basis for\n+            # tunnelled HTTPS connections, should use\n+            # constructor's proxy_headers instead.\n+            kw['headers'] = self._set_proxy_headers(url, kw.get('headers',\n+                                                                self.headers))\n+            kw['headers'].update(self.proxy_headers)",
      "comment": "Appropriate `Proxy-Authorization` header could be set in `proxy_headers` keyword argument for `proxy_from_url()`. I don't really think that proxy authentication belongs to urllib3, so I suggest to wrap `proxy_from_url` in requests with something like:\n\n``` python\nfrom urlparse import urlparse\nimport base64\nfrom .packages.urllib3.poolmanager import  proxy_from_url\n\ndef authenticated_proxy_from_url(url, **kw):\n    parsed_url = urlparse(url)\n    if '@' in parsed_url.netloc:\n        credentials = parsed_url.netloc.split('@')[0]\n        base64string = base64.encodestring(credentials)[:-1]\n        if 'proxy_headers' not in kw:\n            kw['proxy_headers'] = {}\n        kw['proxy_headers']['Proxy-Authorization'] = 'Basic %s' % base64string\n    return proxy_from_url(url, **kw)\n```\n\nOf course it's the most crude example, there's room for further sophistication, such as authentication methods other than Basic.\n",
      "comment_id": 5757098,
      "user": "stanvit",
      "created_at": "2013-08-14T07:39:43Z",
      "url": "https://github.com/psf/requests/pull/1515#discussion_r5757098"
    },
    {
      "repo": "psf/requests",
      "pr_number": 1515,
      "file_path": "requests/packages/urllib3/poolmanager.py",
      "line": 253,
      "side": "RIGHT",
      "diff_hunk": "@@ -187,16 +238,22 @@ def _set_proxy_headers(self, url, headers=None):\n \n         if headers:\n             headers_.update(headers)\n-\n         return headers_\n \n-    def urlopen(self, method, url, **kw):\n+    def urlopen(self, method, url, redirect=True, **kw):\n         \"Same as HTTP(S)ConnectionPool.urlopen, ``url`` must be absolute.\"\n-        kw['assert_same_host'] = False\n-        kw['headers'] = self._set_proxy_headers(url, headers=kw.get('headers'))\n-        return self.proxy_pool.urlopen(method, url, **kw)\n+        u = parse_url(url)\n+\n+        if u.scheme == \"http\":\n+            # It's too late to set proxy headers on per-request basis for\n+            # tunnelled HTTPS connections, should use\n+            # constructor's proxy_headers instead.\n+            kw['headers'] = self._set_proxy_headers(url, kw.get('headers',\n+                                                                self.headers))\n+            kw['headers'].update(self.proxy_headers)",
      "comment": "Alright, I think we should add this header as part of this PR. We have a `get_auth_from_url` function in `utils.py` which should be used for this. @schlamar you can do this yourself, or if you prefer I can raise a PR against your branch that includes the change.\n",
      "comment_id": 5757723,
      "user": "Lukasa",
      "created_at": "2013-08-14T08:26:22Z",
      "url": "https://github.com/psf/requests/pull/1515#discussion_r5757723"
    },
    {
      "repo": "psf/requests",
      "pr_number": 1515,
      "file_path": "requests/packages/urllib3/poolmanager.py",
      "line": 253,
      "side": "RIGHT",
      "diff_hunk": "@@ -187,16 +238,22 @@ def _set_proxy_headers(self, url, headers=None):\n \n         if headers:\n             headers_.update(headers)\n-\n         return headers_\n \n-    def urlopen(self, method, url, **kw):\n+    def urlopen(self, method, url, redirect=True, **kw):\n         \"Same as HTTP(S)ConnectionPool.urlopen, ``url`` must be absolute.\"\n-        kw['assert_same_host'] = False\n-        kw['headers'] = self._set_proxy_headers(url, headers=kw.get('headers'))\n-        return self.proxy_pool.urlopen(method, url, **kw)\n+        u = parse_url(url)\n+\n+        if u.scheme == \"http\":\n+            # It's too late to set proxy headers on per-request basis for\n+            # tunnelled HTTPS connections, should use\n+            # constructor's proxy_headers instead.\n+            kw['headers'] = self._set_proxy_headers(url, kw.get('headers',\n+                                                                self.headers))\n+            kw['headers'].update(self.proxy_headers)",
      "comment": "To my utter disgust, GitHub seems unwilling to let me open a PR against your fork, @schlamar. Are you happy to accept an emailed patch? =D\n",
      "comment_id": 5758219,
      "user": "Lukasa",
      "created_at": "2013-08-14T08:59:24Z",
      "url": "https://github.com/psf/requests/pull/1515#discussion_r5758219"
    },
    {
      "repo": "psf/requests",
      "pr_number": 1515,
      "file_path": "requests/packages/urllib3/poolmanager.py",
      "line": 253,
      "side": "RIGHT",
      "diff_hunk": "@@ -187,16 +238,22 @@ def _set_proxy_headers(self, url, headers=None):\n \n         if headers:\n             headers_.update(headers)\n-\n         return headers_\n \n-    def urlopen(self, method, url, **kw):\n+    def urlopen(self, method, url, redirect=True, **kw):\n         \"Same as HTTP(S)ConnectionPool.urlopen, ``url`` must be absolute.\"\n-        kw['assert_same_host'] = False\n-        kw['headers'] = self._set_proxy_headers(url, headers=kw.get('headers'))\n-        return self.proxy_pool.urlopen(method, url, **kw)\n+        u = parse_url(url)\n+\n+        if u.scheme == \"http\":\n+            # It's too late to set proxy headers on per-request basis for\n+            # tunnelled HTTPS connections, should use\n+            # constructor's proxy_headers instead.\n+            kw['headers'] = self._set_proxy_headers(url, kw.get('headers',\n+                                                                self.headers))\n+            kw['headers'].update(self.proxy_headers)",
      "comment": "> To my utter disgust, GitHub seems unwilling to let me open a PR against your fork,\n\nAny error message or why doesn't that work?\n\n> Are you happy to accept an emailed patch? =D\n\nI can just pull from your branch :) I guess https://github.com/Lukasa/requests/commits/https-proxy-2.0, right?\n",
      "comment_id": 5760175,
      "user": "schlamar",
      "created_at": "2013-08-14T10:49:16Z",
      "url": "https://github.com/psf/requests/pull/1515#discussion_r5760175"
    },
    {
      "repo": "psf/requests",
      "pr_number": 1515,
      "file_path": "requests/packages/urllib3/poolmanager.py",
      "line": 253,
      "side": "RIGHT",
      "diff_hunk": "@@ -187,16 +238,22 @@ def _set_proxy_headers(self, url, headers=None):\n \n         if headers:\n             headers_.update(headers)\n-\n         return headers_\n \n-    def urlopen(self, method, url, **kw):\n+    def urlopen(self, method, url, redirect=True, **kw):\n         \"Same as HTTP(S)ConnectionPool.urlopen, ``url`` must be absolute.\"\n-        kw['assert_same_host'] = False\n-        kw['headers'] = self._set_proxy_headers(url, headers=kw.get('headers'))\n-        return self.proxy_pool.urlopen(method, url, **kw)\n+        u = parse_url(url)\n+\n+        if u.scheme == \"http\":\n+            # It's too late to set proxy headers on per-request basis for\n+            # tunnelled HTTPS connections, should use\n+            # constructor's proxy_headers instead.\n+            kw['headers'] = self._set_proxy_headers(url, kw.get('headers',\n+                                                                self.headers))\n+            kw['headers'].update(self.proxy_headers)",
      "comment": "> To my utter disgust, GitHub seems unwilling to let me open a PR against your fork\n\nMhh, I see the issue. Our branches our not comparable for whatever reason... I send an issue report to GitHub.\n",
      "comment_id": 5760232,
      "user": "schlamar",
      "created_at": "2013-08-14T10:52:40Z",
      "url": "https://github.com/psf/requests/pull/1515#discussion_r5760232"
    },
    {
      "repo": "psf/requests",
      "pr_number": 1515,
      "file_path": "requests/packages/urllib3/poolmanager.py",
      "line": 253,
      "side": "RIGHT",
      "diff_hunk": "@@ -187,16 +238,22 @@ def _set_proxy_headers(self, url, headers=None):\n \n         if headers:\n             headers_.update(headers)\n-\n         return headers_\n \n-    def urlopen(self, method, url, **kw):\n+    def urlopen(self, method, url, redirect=True, **kw):\n         \"Same as HTTP(S)ConnectionPool.urlopen, ``url`` must be absolute.\"\n-        kw['assert_same_host'] = False\n-        kw['headers'] = self._set_proxy_headers(url, headers=kw.get('headers'))\n-        return self.proxy_pool.urlopen(method, url, **kw)\n+        u = parse_url(url)\n+\n+        if u.scheme == \"http\":\n+            # It's too late to set proxy headers on per-request basis for\n+            # tunnelled HTTPS connections, should use\n+            # constructor's proxy_headers instead.\n+            kw['headers'] = self._set_proxy_headers(url, kw.get('headers',\n+                                                                self.headers))\n+            kw['headers'].update(self.proxy_headers)",
      "comment": "Yeah, I don't really know how GitHub decides that. There are loads of forks I _can_ open PRs against, just not yours. Very strange. Anyway, I can just email the patch if it's faster.\n",
      "comment_id": 5760248,
      "user": "Lukasa",
      "created_at": "2013-08-14T10:53:27Z",
      "url": "https://github.com/psf/requests/pull/1515#discussion_r5760248"
    },
    {
      "repo": "psf/requests",
      "pr_number": 1515,
      "file_path": "requests/packages/urllib3/poolmanager.py",
      "line": 253,
      "side": "RIGHT",
      "diff_hunk": "@@ -187,16 +238,22 @@ def _set_proxy_headers(self, url, headers=None):\n \n         if headers:\n             headers_.update(headers)\n-\n         return headers_\n \n-    def urlopen(self, method, url, **kw):\n+    def urlopen(self, method, url, redirect=True, **kw):\n         \"Same as HTTP(S)ConnectionPool.urlopen, ``url`` must be absolute.\"\n-        kw['assert_same_host'] = False\n-        kw['headers'] = self._set_proxy_headers(url, headers=kw.get('headers'))\n-        return self.proxy_pool.urlopen(method, url, **kw)\n+        u = parse_url(url)\n+\n+        if u.scheme == \"http\":\n+            # It's too late to set proxy headers on per-request basis for\n+            # tunnelled HTTPS connections, should use\n+            # constructor's proxy_headers instead.\n+            kw['headers'] = self._set_proxy_headers(url, kw.get('headers',\n+                                                                self.headers))\n+            kw['headers'].update(self.proxy_headers)",
      "comment": "I guess \n\n```\n$ git remote add lukasa https://github.com/Lukasa/requests.git\n$ git fetch lukasa\n$ git merge lukasa/https-proxy-2.0\n```\n\nis faster than processing a patch per mail :)\n",
      "comment_id": 5760951,
      "user": "schlamar",
      "created_at": "2013-08-14T11:36:56Z",
      "url": "https://github.com/psf/requests/pull/1515#discussion_r5760951"
    },
    {
      "repo": "psf/requests",
      "pr_number": 1515,
      "file_path": "requests/packages/urllib3/poolmanager.py",
      "line": 253,
      "side": "RIGHT",
      "diff_hunk": "@@ -187,16 +238,22 @@ def _set_proxy_headers(self, url, headers=None):\n \n         if headers:\n             headers_.update(headers)\n-\n         return headers_\n \n-    def urlopen(self, method, url, **kw):\n+    def urlopen(self, method, url, redirect=True, **kw):\n         \"Same as HTTP(S)ConnectionPool.urlopen, ``url`` must be absolute.\"\n-        kw['assert_same_host'] = False\n-        kw['headers'] = self._set_proxy_headers(url, headers=kw.get('headers'))\n-        return self.proxy_pool.urlopen(method, url, **kw)\n+        u = parse_url(url)\n+\n+        if u.scheme == \"http\":\n+            # It's too late to set proxy headers on per-request basis for\n+            # tunnelled HTTPS connections, should use\n+            # constructor's proxy_headers instead.\n+            kw['headers'] = self._set_proxy_headers(url, kw.get('headers',\n+                                                                self.headers))\n+            kw['headers'].update(self.proxy_headers)",
      "comment": "Almost certainly true. I blame GitHub for our pain today. =D\n",
      "comment_id": 5760971,
      "user": "Lukasa",
      "created_at": "2013-08-14T11:37:55Z",
      "url": "https://github.com/psf/requests/pull/1515#discussion_r5760971"
    },
    {
      "repo": "psf/requests",
      "pr_number": 1515,
      "file_path": "requests/packages/urllib3/poolmanager.py",
      "line": 253,
      "side": "RIGHT",
      "diff_hunk": "@@ -187,16 +238,22 @@ def _set_proxy_headers(self, url, headers=None):\n \n         if headers:\n             headers_.update(headers)\n-\n         return headers_\n \n-    def urlopen(self, method, url, **kw):\n+    def urlopen(self, method, url, redirect=True, **kw):\n         \"Same as HTTP(S)ConnectionPool.urlopen, ``url`` must be absolute.\"\n-        kw['assert_same_host'] = False\n-        kw['headers'] = self._set_proxy_headers(url, headers=kw.get('headers'))\n-        return self.proxy_pool.urlopen(method, url, **kw)\n+        u = parse_url(url)\n+\n+        if u.scheme == \"http\":\n+            # It's too late to set proxy headers on per-request basis for\n+            # tunnelled HTTPS connections, should use\n+            # constructor's proxy_headers instead.\n+            kw['headers'] = self._set_proxy_headers(url, kw.get('headers',\n+                                                                self.headers))\n+            kw['headers'].update(self.proxy_headers)",
      "comment": "> That network has a very large number of forks. For performance reasons, we don't show all of the available forked repositories in the Compare drop-down. We're looking into ways of improving this, but in the meantime you can generate a comparison by manually entering the URL. Try this one, for example:\n> \n> https://github.com/Lukasa/requests/compare/schlamar:master...master\n",
      "comment_id": 5772110,
      "user": "schlamar",
      "created_at": "2013-08-14T18:43:14Z",
      "url": "https://github.com/psf/requests/pull/1515#discussion_r5772110"
    },
    {
      "repo": "psf/requests",
      "pr_number": 1515,
      "file_path": "requests/packages/urllib3/poolmanager.py",
      "line": 253,
      "side": "RIGHT",
      "diff_hunk": "@@ -187,16 +238,22 @@ def _set_proxy_headers(self, url, headers=None):\n \n         if headers:\n             headers_.update(headers)\n-\n         return headers_\n \n-    def urlopen(self, method, url, **kw):\n+    def urlopen(self, method, url, redirect=True, **kw):\n         \"Same as HTTP(S)ConnectionPool.urlopen, ``url`` must be absolute.\"\n-        kw['assert_same_host'] = False\n-        kw['headers'] = self._set_proxy_headers(url, headers=kw.get('headers'))\n-        return self.proxy_pool.urlopen(method, url, **kw)\n+        u = parse_url(url)\n+\n+        if u.scheme == \"http\":\n+            # It's too late to set proxy headers on per-request basis for\n+            # tunnelled HTTPS connections, should use\n+            # constructor's proxy_headers instead.\n+            kw['headers'] = self._set_proxy_headers(url, kw.get('headers',\n+                                                                self.headers))\n+            kw['headers'].update(self.proxy_headers)",
      "comment": "Ah, that makes perfect sense. I could have sworn I tried that, but clearly I didn't. Awesome!\n",
      "comment_id": 5772318,
      "user": "Lukasa",
      "created_at": "2013-08-14T18:49:55Z",
      "url": "https://github.com/psf/requests/pull/1515#discussion_r5772318"
    },
    {
      "repo": "psf/requests",
      "pr_number": 1972,
      "file_path": "requests/structures.py",
      "line": null,
      "side": "RIGHT",
      "diff_hunk": "@@ -108,6 +108,9 @@ def copy(self):\n     def __repr__(self):\n         return '%s(%r)' % (self.__class__.__name__, dict(self.items()))\n \n+    def __str__(self):\n+        return '%s' % (dict(self.items()))",
      "comment": "This could be more simply:\n\n``` python\ndef __str__(self):\n    return str(dict(self.items()))\n```\n\nThis isn't a merge blocker though.\n",
      "comment_id": 10865063,
      "user": "sigmavirus24",
      "created_at": "2014-03-23T00:49:11Z",
      "url": "https://github.com/psf/requests/pull/1972#discussion_r10865063"
    },
    {
      "repo": "psf/requests",
      "pr_number": 1507,
      "file_path": "requests/sessions.py",
      "line": 445,
      "side": "RIGHT",
      "diff_hunk": "@@ -422,7 +441,7 @@ def send(self, request, **kwargs):\n \n         # It's possible that users might accidentally send a Request object.\n         # Guard against that specific failure case.\n-        if getattr(request, 'prepare', None):\n+        if not isinstance(request, PreparedRequest):\n             raise ValueError('You can only send PreparedRequests.')",
      "comment": "Out of curiousity, why doesn't this just call `request.prepare()` (or `self.prepare_request(request)`) instead of failing?\n",
      "comment_id": 5517892,
      "user": "rwe",
      "created_at": "2013-07-31T22:28:45Z",
      "url": "https://github.com/psf/requests/pull/1507#discussion_r5517892"
    },
    {
      "repo": "psf/requests",
      "pr_number": 1507,
      "file_path": "requests/sessions.py",
      "line": 445,
      "side": "RIGHT",
      "diff_hunk": "@@ -422,7 +441,7 @@ def send(self, request, **kwargs):\n \n         # It's possible that users might accidentally send a Request object.\n         # Guard against that specific failure case.\n-        if getattr(request, 'prepare', None):\n+        if not isinstance(request, PreparedRequest):\n             raise ValueError('You can only send PreparedRequests.')",
      "comment": "Why would we raise such a horrible stacktrace when we can raise a much cleaner one and provide the user with an exact message as to why their request cannot be sent?\n",
      "comment_id": 5520356,
      "user": "sigmavirus24",
      "created_at": "2013-08-01T01:06:07Z",
      "url": "https://github.com/psf/requests/pull/1507#discussion_r5520356"
    },
    {
      "repo": "psf/requests",
      "pr_number": 1507,
      "file_path": "requests/sessions.py",
      "line": 445,
      "side": "RIGHT",
      "diff_hunk": "@@ -422,7 +441,7 @@ def send(self, request, **kwargs):\n \n         # It's possible that users might accidentally send a Request object.\n         # Guard against that specific failure case.\n-        if getattr(request, 'prepare', None):\n+        if not isinstance(request, PreparedRequest):\n             raise ValueError('You can only send PreparedRequests.')",
      "comment": "I meant this:\n\n``` python\nif isinstance(request, Request):\n    request = request.prepare()\nelif not isinstance(request, PreparedRequest):\n    raise ValueError('You can only send Requests or PreparedRequests')\n```\n\nThis is outside this PR though; not important.\n",
      "comment_id": 5520474,
      "user": "rwe",
      "created_at": "2013-08-01T01:16:20Z",
      "url": "https://github.com/psf/requests/pull/1507#discussion_r5520474"
    },
    {
      "repo": "psf/requests",
      "pr_number": 1507,
      "file_path": "requests/sessions.py",
      "line": 445,
      "side": "RIGHT",
      "diff_hunk": "@@ -422,7 +441,7 @@ def send(self, request, **kwargs):\n \n         # It's possible that users might accidentally send a Request object.\n         # Guard against that specific failure case.\n-        if getattr(request, 'prepare', None):\n+        if not isinstance(request, PreparedRequest):\n             raise ValueError('You can only send PreparedRequests.')",
      "comment": "Why would we contradict ourselves in the documentation then? We documented the API before we made this change. This was to ensure the API (and its documentation) was correct.\n",
      "comment_id": 5520681,
      "user": "sigmavirus24",
      "created_at": "2013-08-01T01:32:24Z",
      "url": "https://github.com/psf/requests/pull/1507#discussion_r5520681"
    },
    {
      "repo": "psf/requests",
      "pr_number": 1507,
      "file_path": "requests/sessions.py",
      "line": 445,
      "side": "RIGHT",
      "diff_hunk": "@@ -422,7 +441,7 @@ def send(self, request, **kwargs):\n \n         # It's possible that users might accidentally send a Request object.\n         # Guard against that specific failure case.\n-        if getattr(request, 'prepare', None):\n+        if not isinstance(request, PreparedRequest):\n             raise ValueError('You can only send PreparedRequests.')",
      "comment": "I can't seem to find where that restriction is documented. That said, my question was more along the lines of why that restriction was designed in without supporting `Request` objects in the first place. I suspect that most users using `Request` objects only create `PreparedRequest` with directly before calling `Session.send()`, without modifying it, and so that extra hoop just gets in the way of the common case. It seemed like an artificial obstacle.\n\nAvoiding the burden lf supporting multiple argument types was the kind of rationale I expected; because it was documented before it was designed isn't.\n\nHowever: this doesn't affect my use case and I was just trying to understand the design decision better. Thanks for your answer.\n",
      "comment_id": 5520977,
      "user": "rwe",
      "created_at": "2013-08-01T02:01:16Z",
      "url": "https://github.com/psf/requests/pull/1507#discussion_r5520977"
    },
    {
      "repo": "psf/requests",
      "pr_number": 1507,
      "file_path": "requests/sessions.py",
      "line": 445,
      "side": "RIGHT",
      "diff_hunk": "@@ -422,7 +441,7 @@ def send(self, request, **kwargs):\n \n         # It's possible that users might accidentally send a Request object.\n         # Guard against that specific failure case.\n-        if getattr(request, 'prepare', None):\n+        if not isinstance(request, PreparedRequest):\n             raise ValueError('You can only send PreparedRequests.')",
      "comment": "The reasoning, essentially, is that Requests assumes that if you're building `Request` and `PreparedRequest` objects yourself, you are doing _everything_ about preparing them yourself. The `Request` -> `PreparedRequest` flow is basically an advanced use case, when you need to work around or change something Requests does internally.\n\nWhen considered in that light, passing a `Request` to `Session.send()` is likely to be a _mistake_: you have a flow through your code where the `Request` doesn't get prepared prior to sending. That was the purpose of this exception: rather than quietly succeed but potentially do something wrong, we assume that advanced users will just fix the exception.\n",
      "comment_id": 5522899,
      "user": "Lukasa",
      "created_at": "2013-08-01T05:58:43Z",
      "url": "https://github.com/psf/requests/pull/1507#discussion_r5522899"
    },
    {
      "repo": "psf/requests",
      "pr_number": 2088,
      "file_path": "test_requests.py",
      "line": null,
      "side": "RIGHT",
      "diff_hunk": "@@ -456,31 +460,36 @@ def test_urlencoded_get_query_multivalued_param(self):\n         assert r.url == httpbin('get?test=foo&test=baz')\n \n     def test_different_encodings_dont_break_post(self):\n-        r = requests.post(httpbin('post'),\n-                          data={'stuff': json.dumps({'a': 123})},\n-                          params={'blah': 'asdf1234'},\n-                          files={'file': ('test_requests.py', open(__file__, 'rb'))})\n+        r = requests.post(\n+            httpbin('post'),",
      "comment": "I'd consider these if the first parameter was included on the first line. \n",
      "comment_id": 13596504,
      "user": "kennethreitz",
      "created_at": "2014-06-10T14:32:31Z",
      "url": "https://github.com/psf/requests/pull/2088#discussion_r13596504"
    },
    {
      "repo": "psf/requests",
      "pr_number": 1959,
      "file_path": "requests/models.py",
      "line": null,
      "side": "RIGHT",
      "diff_hunk": "@@ -410,6 +410,7 @@ def prepare_body(self, data, files):\n             hasattr(data, '__iter__'),\n             not isinstance(data, basestring),\n             not isinstance(data, list),\n+            not isinstance(data, tuple),",
      "comment": "I don't pretend to know who wrote this originally, but all of these should be shortened to:\n\n``` python\nnot isinstance(data, (basestring, list, tuple, dict))\n```\n\n`isinstance` will take a tuple for the second argument and make sure that the class is not one of the classes inside the tuple of classes. Do you mind fixing this up for us @Feng23 ?\n",
      "comment_id": 10606203,
      "user": "sigmavirus24",
      "created_at": "2014-03-14T13:09:05Z",
      "url": "https://github.com/psf/requests/pull/1959#discussion_r10606203"
    },
    {
      "repo": "psf/requests",
      "pr_number": 1959,
      "file_path": "test_requests.py",
      "line": null,
      "side": "RIGHT",
      "diff_hunk": "@@ -1187,5 +1189,28 @@ def test_stream_timeout(self):\n             assert 'Read timed out' in e.args[0].args[0]\n \n \n+class TestModels:",
      "comment": "This can be rewritten like so:\n\n``` python\n@pytest.fixture\ndef list_of_tuples():\n    return [\n        (('a', 'b'), ('c', 'd')),\n        (('c', 'd'), ('e', 'f')),\n        (('a', 'b'), ('c', 'd'), ('e', 'f')),\n    ]\n\ndef test_data_argument_accepts_tuples(list_of_tuples):\n    \"\"\"\n    Ensure that the data argument will accept tuples of strings\n    and properly encode them.\n    \"\"\"\n    for data_tuple in list_of_tuples:\n        p = PreparedRequest()\n        p.prepare(\n            method='GET',\n            url='http://example.com',\n            data=tuple_data,\n            hooks=default_hooks()\n        )\n        assert p.body == urlencode(tuple_data)\n```\n\nIt doesn't need to be in its own class and you're not really generating random data.\n",
      "comment_id": 10606404,
      "user": "sigmavirus24",
      "created_at": "2014-03-14T13:15:04Z",
      "url": "https://github.com/psf/requests/pull/1959#discussion_r10606404"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "httpx/utils.py",
      "line": 111,
      "side": "RIGHT",
      "diff_hunk": "@@ -107,6 +107,14 @@ def get_netrc_login(host: str) -> typing.Optional[typing.Tuple[str, str, str]]:\n     return netrc_info.authenticators(host)  # type: ignore\n \n \n+def get_ssl_cert_file() -> typing.Optional[str]:\n+    for env_name in (\"REQUESTS_CA_BUNDLE\", \"SSL_CERT_FILE\", \"CURL_CA_BUNDLE\"):",
      "comment": "> We should check the three environment variables in the order above\r\n> `(SSL_CERT_FILE, REQUESTS_CA_BUNDLE, CURL_CA_BUNDLE)`,\r\n> as SSL_CERT_FILE is a PEP and a standard whereas the other are products of other projects.\r\n- @sethmlarson \thttps://github.com/encode/httpx/issues/306#issue-488260040\r\n\r\nThis also affects [the](https://github.com/encode/httpx/pull/307/files#diff-c73723c2d8a7c6dfecc80f229ebcef55R83) [docs](https://github.com/encode/httpx/pull/307/files#diff-c73723c2d8a7c6dfecc80f229ebcef55R88)",
      "comment_id": 320042951,
      "user": "StephenBrown2",
      "created_at": "2019-09-02T20:32:01Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r320042951"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "tests/test_utils.py",
      "line": 121,
      "side": "RIGHT",
      "diff_hunk": "@@ -111,3 +116,19 @@ def test_parse_header_links(value, expected):\n \n     # Reset the logger so we don't have verbose output in all unit tests\n     logging.getLogger(\"httpx\").handlers = []\n+\n+\n+def test_get_ssl_cert_file():",
      "comment": "This should also test if all three are set and if none are valid files, and verify the order tested.",
      "comment_id": 320043132,
      "user": "StephenBrown2",
      "created_at": "2019-09-02T20:33:59Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r320043132"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "httpx/config.py",
      "line": 67,
      "side": "RIGHT",
      "diff_hunk": "@@ -61,6 +62,10 @@ def __init__(\n             verify = True\n             self._load_client_certs(ssl_context)\n \n+        if trust_env:\n+            if verify is True or verify is None:\n+                verify = get_ssl_cert_file()  # type: ignore",
      "comment": "This doesn't look quite right to me, but perhaps I've got myself in a muddle.\r\n\r\nIf verify is True, but `SSL_CERT_FILE` is not set, then `get_ssl_cert_file` will return `None`, which looks like a behavioral change from what we've got right now.",
      "comment_id": 320250534,
      "user": "lovelydinosaur",
      "created_at": "2019-09-03T12:43:53Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r320250534"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "httpx/config.py",
      "line": 67,
      "side": "RIGHT",
      "diff_hunk": "@@ -61,6 +62,10 @@ def __init__(\n             verify = True\n             self._load_client_certs(ssl_context)\n \n+        if trust_env:\n+            if verify is True or verify is None:\n+                verify = get_ssl_cert_file()  # type: ignore",
      "comment": "I also think we should *probably* inspect the environment at the point of calling `load_ssl_context`, rather than on `__init__`.",
      "comment_id": 320250946,
      "user": "lovelydinosaur",
      "created_at": "2019-09-03T12:44:49Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r320250946"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "httpx/utils.py",
      "line": 113,
      "side": "RIGHT",
      "diff_hunk": "@@ -107,6 +107,14 @@ def get_netrc_login(host: str) -> typing.Optional[typing.Tuple[str, str, str]]:\n     return netrc_info.authenticators(host)  # type: ignore\n \n \n+def get_ssl_cert_file() -> typing.Optional[str]:\n+    for env_name in (\"SSL_CERT_FILE\", \"SSL_CERT_DIR\"):\n+        ssl_file = Path(os.getenv(env_name, \"\"))\n+        if ssl_file and ssl_file.is_file():",
      "comment": "This won't work for `SSL_CERT_DIR`, for hopefully obvious reasons.",
      "comment_id": 320416523,
      "user": "StephenBrown2",
      "created_at": "2019-09-03T18:30:00Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r320416523"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "httpx/utils.py",
      "line": 110,
      "side": "RIGHT",
      "diff_hunk": "@@ -107,6 +107,16 @@ def get_netrc_login(host: str) -> typing.Optional[typing.Tuple[str, str, str]]:\n     return netrc_info.authenticators(host)  # type: ignore\n \n \n+def get_ssl_cert_file() -> typing.Optional[str]:",
      "comment": "Let's change the title of this since \"cert\" is what we use for client cert. Really this should be `get_ca_bundle_from_env()` or something like it?",
      "comment_id": 321404735,
      "user": "sethmlarson",
      "created_at": "2019-09-05T18:02:10Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r321404735"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "tests/test_config.py",
      "line": 32,
      "side": "RIGHT",
      "diff_hunk": "@@ -26,6 +27,19 @@ def test_load_ssl_config_verify_existing_file():\n     assert context.check_hostname is True\n \n \n+@pytest.mark.parametrize(\"config\", (\"SSL_CERT_FILE\", \"SSL_CERT_DIR\"))\n+def test_load_ssl_config_verify_env_file(config):\n+    default_path = httpx.config.DEFAULT_CA_BUNDLE_PATH",
      "comment": "We should use a non-default and verify that it's loaded. I think there's a way to see how many certificates are loaded in an SSLContext?",
      "comment_id": 321405215,
      "user": "sethmlarson",
      "created_at": "2019-09-05T18:03:16Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r321405215"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "tests/test_config.py",
      "line": 32,
      "side": "RIGHT",
      "diff_hunk": "@@ -26,6 +27,19 @@ def test_load_ssl_config_verify_existing_file():\n     assert context.check_hostname is True\n \n \n+@pytest.mark.parametrize(\"config\", (\"SSL_CERT_FILE\", \"SSL_CERT_DIR\"))\n+def test_load_ssl_config_verify_env_file(config):\n+    default_path = httpx.config.DEFAULT_CA_BUNDLE_PATH",
      "comment": "I couldn't understand this part. Could you give an example roughly?",
      "comment_id": 321487564,
      "user": "cansarigol",
      "created_at": "2019-09-05T21:18:25Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r321487564"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "tests/test_config.py",
      "line": 32,
      "side": "RIGHT",
      "diff_hunk": "@@ -26,6 +27,19 @@ def test_load_ssl_config_verify_existing_file():\n     assert context.check_hostname is True\n \n \n+@pytest.mark.parametrize(\"config\", (\"SSL_CERT_FILE\", \"SSL_CERT_DIR\"))\n+def test_load_ssl_config_verify_env_file(config):\n+    default_path = httpx.config.DEFAULT_CA_BUNDLE_PATH",
      "comment": "Basically if we don't set any `SSL_CERT_...` environment variable we choose `httpx.config.DEFAULT_CA_BUNDLE_PATH` so we can't test the behavior of `SSL_CERT_...` using that path as our expected value because there's no way to tell if the environment variable worked or we just picked the default anyways.\r\n\r\nI suggest you use one of the auto-generated cert fixtures to test this functionality. :)",
      "comment_id": 321546711,
      "user": "sethmlarson",
      "created_at": "2019-09-06T01:34:35Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r321546711"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "tests/test_config.py",
      "line": 32,
      "side": "RIGHT",
      "diff_hunk": "@@ -26,6 +27,19 @@ def test_load_ssl_config_verify_existing_file():\n     assert context.check_hostname is True\n \n \n+@pytest.mark.parametrize(\"config\", (\"SSL_CERT_FILE\", \"SSL_CERT_DIR\"))\n+def test_load_ssl_config_verify_env_file(config):\n+    default_path = httpx.config.DEFAULT_CA_BUNDLE_PATH",
      "comment": "thanks. in addition, added ssl_config.verify validation to the end of the test to make sure.",
      "comment_id": 321593551,
      "user": "cansarigol",
      "created_at": "2019-09-06T06:36:45Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r321593551"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "httpx/config.py",
      "line": 94,
      "side": "RIGHT",
      "diff_hunk": "@@ -90,6 +91,12 @@ def load_ssl_context(\n     ) -> ssl.SSLContext:\n         http_versions = HTTPVersionConfig() if http_versions is None else http_versions\n \n+        if self.trust_env:",
      "comment": "Can we move this section into the `load_ssl_context_verify()`? Since we're only doing the loading if we've got verify active anyways.\r\n\r\nAlso is there any harm in having `SSLConfig.verify` default to `True` in the constructor? It defaults to `True` on the client anyways.",
      "comment_id": 321714538,
      "user": "sethmlarson",
      "created_at": "2019-09-06T12:39:56Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r321714538"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "tests/test_config.py",
      "line": 40,
      "side": "RIGHT",
      "diff_hunk": "@@ -26,6 +28,18 @@ def test_load_ssl_config_verify_existing_file():\n     assert context.check_hostname is True\n \n \n+@pytest.mark.parametrize(\"config\", (\"SSL_CERT_FILE\", \"SSL_CERT_DIR\"))\n+def test_load_ssl_config_verify_env_file(cert_pem_file, config):\n+    os.environ[config] = (\n+        cert_pem_file if config.endswith(\"_FILE\") else str(Path(cert_pem_file).parent)\n+    )\n+    ssl_config = httpx.SSLConfig(trust_env=True)\n+    context = ssl_config.load_ssl_context()\n+    assert context.verify_mode == ssl.VerifyMode.CERT_REQUIRED\n+    assert context.check_hostname is True\n+    assert ssl_config.verify == os.environ[config]",
      "comment": "Verify that certs are loaded via `assert len(context.get_ca_certs) > 0`",
      "comment_id": 321715100,
      "user": "sethmlarson",
      "created_at": "2019-09-06T12:41:32Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r321715100"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "tests/test_config.py",
      "line": 40,
      "side": "RIGHT",
      "diff_hunk": "@@ -26,6 +28,18 @@ def test_load_ssl_config_verify_existing_file():\n     assert context.check_hostname is True\n \n \n+@pytest.mark.parametrize(\"config\", (\"SSL_CERT_FILE\", \"SSL_CERT_DIR\"))\n+def test_load_ssl_config_verify_env_file(cert_pem_file, config):\n+    os.environ[config] = (\n+        cert_pem_file if config.endswith(\"_FILE\") else str(Path(cert_pem_file).parent)\n+    )\n+    ssl_config = httpx.SSLConfig(trust_env=True)\n+    context = ssl_config.load_ssl_context()\n+    assert context.verify_mode == ssl.VerifyMode.CERT_REQUIRED\n+    assert context.check_hostname is True\n+    assert ssl_config.verify == os.environ[config]",
      "comment": "`get_ca_certs` returns empty if use `context.load_verify_locations(capath=str(ca_bundle_path))`. \r\nTo get a result, we should create an `SSLSocket` and call [getpeercert](https://docs.python.org/3/library/ssl.html#ssl.SSLSocket.getpeercert) like below\r\n\r\n```\r\nif config == \"SSL_CERT_DIR\":\r\n        HOST = \"example.org\"\r\n        PORT = 443\r\n        conn: ssl.SSLSocket = ssl_config.ssl_context.wrap_socket(\r\n            socket.socket(socket.AF_INET, socket.SOCK_STREAM), server_hostname=HOST\r\n        )\r\n        conn.connect((HOST, PORT))\r\n        conn.getpeercert()\r\n```\r\n",
      "comment_id": 321744962,
      "user": "cansarigol",
      "created_at": "2019-09-06T13:52:03Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r321744962"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "httpx/config.py",
      "line": 94,
      "side": "RIGHT",
      "diff_hunk": "@@ -90,6 +91,12 @@ def load_ssl_context(\n     ) -> ssl.SSLContext:\n         http_versions = HTTPVersionConfig() if http_versions is None else http_versions\n \n+        if self.trust_env:",
      "comment": "Did you mean `load_ssl_context_verify`? when `verify` active, `load_ssl_context_verify` is called.",
      "comment_id": 321748486,
      "user": "cansarigol",
      "created_at": "2019-09-06T13:59:49Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r321748486"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "tests/test_config.py",
      "line": 40,
      "side": "RIGHT",
      "diff_hunk": "@@ -26,6 +28,18 @@ def test_load_ssl_config_verify_existing_file():\n     assert context.check_hostname is True\n \n \n+@pytest.mark.parametrize(\"config\", (\"SSL_CERT_FILE\", \"SSL_CERT_DIR\"))\n+def test_load_ssl_config_verify_env_file(cert_pem_file, config):\n+    os.environ[config] = (\n+        cert_pem_file if config.endswith(\"_FILE\") else str(Path(cert_pem_file).parent)\n+    )\n+    ssl_config = httpx.SSLConfig(trust_env=True)\n+    context = ssl_config.load_ssl_context()\n+    assert context.verify_mode == ssl.VerifyMode.CERT_REQUIRED\n+    assert context.check_hostname is True\n+    assert ssl_config.verify == os.environ[config]",
      "comment": "Maybe we can use https_server.url.host and https_server.url.port to accomplish that. Then our certificate will verify as well :)",
      "comment_id": 321753686,
      "user": "sethmlarson",
      "created_at": "2019-09-06T14:11:41Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r321753686"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "tests/test_config.py",
      "line": 40,
      "side": "RIGHT",
      "diff_hunk": "@@ -26,6 +28,18 @@ def test_load_ssl_config_verify_existing_file():\n     assert context.check_hostname is True\n \n \n+@pytest.mark.parametrize(\"config\", (\"SSL_CERT_FILE\", \"SSL_CERT_DIR\"))\n+def test_load_ssl_config_verify_env_file(cert_pem_file, config):\n+    os.environ[config] = (\n+        cert_pem_file if config.endswith(\"_FILE\") else str(Path(cert_pem_file).parent)\n+    )\n+    ssl_config = httpx.SSLConfig(trust_env=True)\n+    context = ssl_config.load_ssl_context()\n+    assert context.verify_mode == ssl.VerifyMode.CERT_REQUIRED\n+    assert context.check_hostname is True\n+    assert ssl_config.verify == os.environ[config]",
      "comment": "I couldn't verify with `cert_pem_file` for `https_server.url`. Please help :)",
      "comment_id": 321870931,
      "user": "cansarigol",
      "created_at": "2019-09-06T19:04:19Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r321870931"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "tests/test_utils.py",
      "line": 121,
      "side": "RIGHT",
      "diff_hunk": "@@ -111,3 +116,19 @@ def test_parse_header_links(value, expected):\n \n     # Reset the logger so we don't have verbose output in all unit tests\n     logging.getLogger(\"httpx\").handlers = []\n+\n+\n+def test_get_ssl_cert_file():",
      "comment": "I'd probably tend to be okay either way on this sort of thing.\r\nWe can't ever test every possible permutation, and test code does have it's own maintenance cost, too.",
      "comment_id": 322308585,
      "user": "lovelydinosaur",
      "created_at": "2019-09-09T15:30:03Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r322308585"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "httpx/utils.py",
      "line": 115,
      "side": "RIGHT",
      "diff_hunk": "@@ -107,6 +107,16 @@ def get_netrc_login(host: str) -> typing.Optional[typing.Tuple[str, str, str]]:\n     return netrc_info.authenticators(host)  # type: ignore\n \n \n+def get_ca_bundle_from_env() -> typing.Optional[str]:\n+    ssl_path = Path(os.getenv(\"SSL_CERT_FILE\", \"\"))\n+    if ssl_path and ssl_path.is_file():\n+        return str(ssl_path)\n+    ssl_path = Path(os.getenv(\"SSL_CERT_DIR\", \"\"))\n+    if ssl_path and ssl_path.is_dir():",
      "comment": "I don't think this is doing what we want: `bool(ssl_path)` is always going to be `True` because it refers to a `Path` object.\r\n\r\nAs a result, we'll probably end up using the current directory as the `SSL_CERT_DIR` in all cases. \ud83d\ude15 \r\n\r\nWe should probably instead write something like\u2026\r\n\r\n```python\r\nif \"SSL_CERT_DIR\" in os.environ:\r\n    ssl_path = Path(os.environ[\"SSL_CERT_DIR\"])\r\n    if ssl_path.is_dir():\r\n        return str(ssl_path)\r\n```\r\n\r\nand do the same kind of procesing for `SSL_CERT_FILE` above (although the bug wouldn't happen because the current directory is never a file).\r\n",
      "comment_id": 323458263,
      "user": "florimondmanca",
      "created_at": "2019-09-11T21:03:18Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r323458263"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "httpx/utils.py",
      "line": 115,
      "side": "RIGHT",
      "diff_hunk": "@@ -107,6 +107,16 @@ def get_netrc_login(host: str) -> typing.Optional[typing.Tuple[str, str, str]]:\n     return netrc_info.authenticators(host)  # type: ignore\n \n \n+def get_ca_bundle_from_env() -> typing.Optional[str]:\n+    ssl_path = Path(os.getenv(\"SSL_CERT_FILE\", \"\"))\n+    if ssl_path and ssl_path.is_file():\n+        return str(ssl_path)\n+    ssl_path = Path(os.getenv(\"SSL_CERT_DIR\", \"\"))\n+    if ssl_path and ssl_path.is_dir():",
      "comment": "(Trivia: I think this is typically a case where the Python 3.8 walrus operator would shine \ud83d\ude04)\r\n\r\n```python\r\nif (\r\n    ssl_cert_dir := os.getenv(\"SSL_CERT_DIR\") is not None\r\n    and (ssl_path := Path(ssl_cert_dir)).is_dir()\r\n):\r\n    return str(ssl_path)\r\n```",
      "comment_id": 323459451,
      "user": "florimondmanca",
      "created_at": "2019-09-11T21:06:27Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r323459451"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "tests/test_utils.py",
      "line": 121,
      "side": "RIGHT",
      "diff_hunk": "@@ -111,3 +116,15 @@ def test_parse_header_links(value, expected):\n \n     # Reset the logger so we don't have verbose output in all unit tests\n     logging.getLogger(\"httpx\").handlers = []\n+\n+\n+def test_get_ssl_cert_file():",
      "comment": "I'd say let\u2019s add tests at the top here for the cases when `SSL_CERT_FILE` and `SSL_CERT_DIR` are not set. Ideally we should be able to catch the potential bugs I mention in my other comment. :)",
      "comment_id": 323460444,
      "user": "florimondmanca",
      "created_at": "2019-09-11T21:09:01Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r323460444"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "httpx/utils.py",
      "line": 115,
      "side": "RIGHT",
      "diff_hunk": "@@ -107,6 +107,16 @@ def get_netrc_login(host: str) -> typing.Optional[typing.Tuple[str, str, str]]:\n     return netrc_info.authenticators(host)  # type: ignore\n \n \n+def get_ca_bundle_from_env() -> typing.Optional[str]:\n+    ssl_path = Path(os.getenv(\"SSL_CERT_FILE\", \"\"))\n+    if ssl_path and ssl_path.is_file():\n+        return str(ssl_path)\n+    ssl_path = Path(os.getenv(\"SSL_CERT_DIR\", \"\"))\n+    if ssl_path and ssl_path.is_dir():",
      "comment": "I got it. `Path` always returns a `PosixPath` and never be `None`. I'm fixing thanks.",
      "comment_id": 323605716,
      "user": "cansarigol",
      "created_at": "2019-09-12T08:02:24Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r323605716"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "httpx/utils.py",
      "line": 115,
      "side": "RIGHT",
      "diff_hunk": "@@ -107,6 +107,16 @@ def get_netrc_login(host: str) -> typing.Optional[typing.Tuple[str, str, str]]:\n     return netrc_info.authenticators(host)  # type: ignore\n \n \n+def get_ca_bundle_from_env() -> typing.Optional[str]:\n+    ssl_path = Path(os.getenv(\"SSL_CERT_FILE\", \"\"))\n+    if ssl_path and ssl_path.is_file():\n+        return str(ssl_path)\n+    ssl_path = Path(os.getenv(\"SSL_CERT_DIR\", \"\"))\n+    if ssl_path and ssl_path.is_dir():",
      "comment": "> (Trivia: I think this is typically a case where the Python 3.8 walrus operator would shine \ud83d\ude04)\r\n> \r\n> ```python\r\n> if (\r\n>     ssl_cert_dir := os.getenv(\"SSL_CERT_DIR\") is not None\r\n>     and (ssl_path := Path(ssl_cert_dir)).is_dir()\r\n> ):\r\n>     return str(ssl_path)\r\n> ```\r\n\r\ndefinitely, this syntax is great \ud83d\ude80 ",
      "comment_id": 323620515,
      "user": "cansarigol",
      "created_at": "2019-09-12T08:37:50Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r323620515"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "tests/test_config.py",
      "line": 40,
      "side": "RIGHT",
      "diff_hunk": "@@ -26,6 +28,18 @@ def test_load_ssl_config_verify_existing_file():\n     assert context.check_hostname is True\n \n \n+@pytest.mark.parametrize(\"config\", (\"SSL_CERT_FILE\", \"SSL_CERT_DIR\"))\n+def test_load_ssl_config_verify_env_file(cert_pem_file, config):\n+    os.environ[config] = (\n+        cert_pem_file if config.endswith(\"_FILE\") else str(Path(cert_pem_file).parent)\n+    )\n+    ssl_config = httpx.SSLConfig(trust_env=True)\n+    context = ssl_config.load_ssl_context()\n+    assert context.verify_mode == ssl.VerifyMode.CERT_REQUIRED\n+    assert context.check_hostname is True\n+    assert ssl_config.verify == os.environ[config]",
      "comment": "Not yet, would be good to resolve this and maybe add a case for the default of certifi getting loaded. :)",
      "comment_id": 324024303,
      "user": "sethmlarson",
      "created_at": "2019-09-13T03:25:18Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r324024303"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "tests/test_config.py",
      "line": 40,
      "side": "RIGHT",
      "diff_hunk": "@@ -26,6 +28,18 @@ def test_load_ssl_config_verify_existing_file():\n     assert context.check_hostname is True\n \n \n+@pytest.mark.parametrize(\"config\", (\"SSL_CERT_FILE\", \"SSL_CERT_DIR\"))\n+def test_load_ssl_config_verify_env_file(cert_pem_file, config):\n+    os.environ[config] = (\n+        cert_pem_file if config.endswith(\"_FILE\") else str(Path(cert_pem_file).parent)\n+    )\n+    ssl_config = httpx.SSLConfig(trust_env=True)\n+    context = ssl_config.load_ssl_context()\n+    assert context.verify_mode == ssl.VerifyMode.CERT_REQUIRED\n+    assert context.check_hostname is True\n+    assert ssl_config.verify == os.environ[config]",
      "comment": "I'm trying to fix this like [cpython test_ssl](https://github.com/python/cpython/blob/7cad53e6b084435a220e6604010f1fa5778bd0b1/Lib/test/test_ssl.py#L1931). However, I haven't been able to verify the certificate yet.",
      "comment_id": 324054753,
      "user": "cansarigol",
      "created_at": "2019-09-13T06:47:35Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r324054753"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "tests/test_config.py",
      "line": 40,
      "side": "RIGHT",
      "diff_hunk": "@@ -26,6 +28,18 @@ def test_load_ssl_config_verify_existing_file():\n     assert context.check_hostname is True\n \n \n+@pytest.mark.parametrize(\"config\", (\"SSL_CERT_FILE\", \"SSL_CERT_DIR\"))\n+def test_load_ssl_config_verify_env_file(cert_pem_file, config):\n+    os.environ[config] = (\n+        cert_pem_file if config.endswith(\"_FILE\") else str(Path(cert_pem_file).parent)\n+    )\n+    ssl_config = httpx.SSLConfig(trust_env=True)\n+    context = ssl_config.load_ssl_context()\n+    assert context.verify_mode == ssl.VerifyMode.CERT_REQUIRED\n+    assert context.check_hostname is True\n+    assert ssl_config.verify == os.environ[config]",
      "comment": "@cansarigol Is there anything we can do to help unblock this?",
      "comment_id": 324460274,
      "user": "florimondmanca",
      "created_at": "2019-09-15T12:25:42Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r324460274"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "tests/test_config.py",
      "line": 40,
      "side": "RIGHT",
      "diff_hunk": "@@ -26,6 +28,18 @@ def test_load_ssl_config_verify_existing_file():\n     assert context.check_hostname is True\n \n \n+@pytest.mark.parametrize(\"config\", (\"SSL_CERT_FILE\", \"SSL_CERT_DIR\"))\n+def test_load_ssl_config_verify_env_file(cert_pem_file, config):\n+    os.environ[config] = (\n+        cert_pem_file if config.endswith(\"_FILE\") else str(Path(cert_pem_file).parent)\n+    )\n+    ssl_config = httpx.SSLConfig(trust_env=True)\n+    context = ssl_config.load_ssl_context()\n+    assert context.verify_mode == ssl.VerifyMode.CERT_REQUIRED\n+    assert context.check_hostname is True\n+    assert ssl_config.verify == os.environ[config]",
      "comment": "Would be great. Thanks.  I'm stuck about create a sslsocket because of ssl validation error. I don't know if it is related trustme scope",
      "comment_id": 324462839,
      "user": "cansarigol",
      "created_at": "2019-09-15T13:24:53Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r324462839"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "tests/test_config.py",
      "line": 40,
      "side": "RIGHT",
      "diff_hunk": "@@ -26,6 +28,18 @@ def test_load_ssl_config_verify_existing_file():\n     assert context.check_hostname is True\n \n \n+@pytest.mark.parametrize(\"config\", (\"SSL_CERT_FILE\", \"SSL_CERT_DIR\"))\n+def test_load_ssl_config_verify_env_file(cert_pem_file, config):\n+    os.environ[config] = (\n+        cert_pem_file if config.endswith(\"_FILE\") else str(Path(cert_pem_file).parent)\n+    )\n+    ssl_config = httpx.SSLConfig(trust_env=True)\n+    context = ssl_config.load_ssl_context()\n+    assert context.verify_mode == ssl.VerifyMode.CERT_REQUIRED\n+    assert context.check_hostname is True\n+    assert ssl_config.verify == os.environ[config]",
      "comment": "@cansarigol Can you share a traceback so we know what part is failing exactly? :) Thanks",
      "comment_id": 324465172,
      "user": "florimondmanca",
      "created_at": "2019-09-15T14:18:24Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r324465172"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "tests/test_config.py",
      "line": 40,
      "side": "RIGHT",
      "diff_hunk": "@@ -26,6 +28,18 @@ def test_load_ssl_config_verify_existing_file():\n     assert context.check_hostname is True\n \n \n+@pytest.mark.parametrize(\"config\", (\"SSL_CERT_FILE\", \"SSL_CERT_DIR\"))\n+def test_load_ssl_config_verify_env_file(cert_pem_file, config):\n+    os.environ[config] = (\n+        cert_pem_file if config.endswith(\"_FILE\") else str(Path(cert_pem_file).parent)\n+    )\n+    ssl_config = httpx.SSLConfig(trust_env=True)\n+    context = ssl_config.load_ssl_context()\n+    assert context.verify_mode == ssl.VerifyMode.CERT_REQUIRED\n+    assert context.check_hostname is True\n+    assert ssl_config.verify == os.environ[config]",
      "comment": "I'm very sorry that I didn't do anything but i will asap",
      "comment_id": 324579390,
      "user": "cansarigol",
      "created_at": "2019-09-16T09:28:17Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r324579390"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "tests/test_config.py",
      "line": 40,
      "side": "RIGHT",
      "diff_hunk": "@@ -26,6 +28,18 @@ def test_load_ssl_config_verify_existing_file():\n     assert context.check_hostname is True\n \n \n+@pytest.mark.parametrize(\"config\", (\"SSL_CERT_FILE\", \"SSL_CERT_DIR\"))\n+def test_load_ssl_config_verify_env_file(cert_pem_file, config):\n+    os.environ[config] = (\n+        cert_pem_file if config.endswith(\"_FILE\") else str(Path(cert_pem_file).parent)\n+    )\n+    ssl_config = httpx.SSLConfig(trust_env=True)\n+    context = ssl_config.load_ssl_context()\n+    assert context.verify_mode == ssl.VerifyMode.CERT_REQUIRED\n+    assert context.check_hostname is True\n+    assert ssl_config.verify == os.environ[config]",
      "comment": "This is blocked by #354, just gave it a spin myself. I think we should maybe merge this now?",
      "comment_id": 325815705,
      "user": "sethmlarson",
      "created_at": "2019-09-18T18:00:45Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r325815705"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "tests/test_config.py",
      "line": 40,
      "side": "RIGHT",
      "diff_hunk": "@@ -26,6 +28,18 @@ def test_load_ssl_config_verify_existing_file():\n     assert context.check_hostname is True\n \n \n+@pytest.mark.parametrize(\"config\", (\"SSL_CERT_FILE\", \"SSL_CERT_DIR\"))\n+def test_load_ssl_config_verify_env_file(cert_pem_file, config):\n+    os.environ[config] = (\n+        cert_pem_file if config.endswith(\"_FILE\") else str(Path(cert_pem_file).parent)\n+    )\n+    ssl_config = httpx.SSLConfig(trust_env=True)\n+    context = ssl_config.load_ssl_context()\n+    assert context.verify_mode == ssl.VerifyMode.CERT_REQUIRED\n+    assert context.check_hostname is True\n+    assert ssl_config.verify == os.environ[config]",
      "comment": "#354 is closed now, I tried changing the test to look like this right now:\r\n\r\n```python\r\n    os.environ[config] = (\r\n        ca_cert_pem_file\r\n        if config.endswith(\"_FILE\")\r\n        else str(Path(ca_cert_pem_file).parent)\r\n    )\r\n    ssl_config = httpx.SSLConfig(trust_env=True)\r\n    context = ssl_config.load_ssl_context()\r\n    assert context.verify_mode == ssl.VerifyMode.CERT_REQUIRED\r\n    assert context.check_hostname is True\r\n    assert ssl_config.verify == os.environ[config]\r\n\r\n    host = https_server.url.host\r\n    port = https_server.url.port\r\n    conn = socket.create_connection((host, port))\r\n    context.wrap_socket(conn, server_hostname=host)\r\n    assert len(context.get_ca_certs()) == 1\r\n```\r\n\r\nBut I'm still failing on the `SSL_CERT_DIR` test case and I don't know why, pretty annoying!",
      "comment_id": 325958024,
      "user": "sethmlarson",
      "created_at": "2019-09-19T01:35:30Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r325958024"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "tests/test_config.py",
      "line": 40,
      "side": "RIGHT",
      "diff_hunk": "@@ -26,6 +28,18 @@ def test_load_ssl_config_verify_existing_file():\n     assert context.check_hostname is True\n \n \n+@pytest.mark.parametrize(\"config\", (\"SSL_CERT_FILE\", \"SSL_CERT_DIR\"))\n+def test_load_ssl_config_verify_env_file(cert_pem_file, config):\n+    os.environ[config] = (\n+        cert_pem_file if config.endswith(\"_FILE\") else str(Path(cert_pem_file).parent)\n+    )\n+    ssl_config = httpx.SSLConfig(trust_env=True)\n+    context = ssl_config.load_ssl_context()\n+    assert context.verify_mode == ssl.VerifyMode.CERT_REQUIRED\n+    assert context.check_hostname is True\n+    assert ssl_config.verify == os.environ[config]",
      "comment": "I added a test to be able to compare with below traceback.\r\n\r\n```\r\ntests/test_config.py ...FSSL error in data received\r\nprotocol: <asyncio.sslproto.SSLProtocol object at 0x103c044a8>\r\ntransport: <_SelectorSocketTransport closing fd=19 read=idle write=<idle, bufsize=0>>\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/sslproto.py\", line 526, in data_received\r\n    ssldata, appdata = self._sslpipe.feed_ssldata(data)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/sslproto.py\", line 189, in feed_ssldata\r\n    self._sslobj.do_handshake()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ssl.py\", line 763, in do_handshake\r\n    self._sslobj.do_handshake()\r\nssl.SSLError: [SSL: TLSV1_ALERT_UNKNOWN_CA] tlsv1 alert unknown ca (_ssl.c:1045)\r\n```",
      "comment_id": 326024810,
      "user": "cansarigol",
      "created_at": "2019-09-19T07:26:53Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r326024810"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 307,
      "file_path": "tests/test_config.py",
      "line": 40,
      "side": "RIGHT",
      "diff_hunk": "@@ -26,6 +28,18 @@ def test_load_ssl_config_verify_existing_file():\n     assert context.check_hostname is True\n \n \n+@pytest.mark.parametrize(\"config\", (\"SSL_CERT_FILE\", \"SSL_CERT_DIR\"))\n+def test_load_ssl_config_verify_env_file(cert_pem_file, config):\n+    os.environ[config] = (\n+        cert_pem_file if config.endswith(\"_FILE\") else str(Path(cert_pem_file).parent)\n+    )\n+    ssl_config = httpx.SSLConfig(trust_env=True)\n+    context = ssl_config.load_ssl_context()\n+    assert context.verify_mode == ssl.VerifyMode.CERT_REQUIRED\n+    assert context.check_hostname is True\n+    assert ssl_config.verify == os.environ[config]",
      "comment": "Spoke with @florimondmanca and I think we're just going to skip the `SSL_CERT_DIR` case here and if we run into issues with it we'll fix them then. Until then this PR is good to go. Thank you so much @cansarigol for following this one all the way through. :)",
      "comment_id": 327178731,
      "user": "sethmlarson",
      "created_at": "2019-09-23T15:23:46Z",
      "url": "https://github.com/encode/httpx/pull/307#discussion_r327178731"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2252,
      "file_path": "tests/models/test_url.py",
      "line": 316,
      "side": "RIGHT",
      "diff_hunk": "@@ -312,49 +312,13 @@ def test_url_copywith_security():\n     \"\"\"\n     Prevent unexpected changes on URL after calling copy_with (CVE-2021-41945)\n     \"\"\"\n-    url = httpx.URL(\"https://u:p@[invalid!]//evilHost/path?t=w#tw\")\n-    original_scheme = url.scheme\n-    original_userinfo = url.userinfo\n-    original_netloc = url.netloc\n-    original_raw_path = url.raw_path\n-    original_query = url.query\n-    original_fragment = url.fragment\n-    url = url.copy_with()\n-    assert url.scheme == original_scheme\n-    assert url.userinfo == original_userinfo\n-    assert url.netloc == original_netloc\n-    assert url.raw_path == original_raw_path\n-    assert url.query == original_query\n-    assert url.fragment == original_fragment\n-\n-    url = httpx.URL(\"https://u:p@[invalid!]//evilHost/path?t=w#tw\")\n-    original_scheme = url.scheme\n-    original_netloc = url.netloc\n-    original_raw_path = url.raw_path\n-    original_query = url.query\n-    original_fragment = url.fragment\n-    url = url.copy_with(userinfo=b\"\")\n-    assert url.scheme == original_scheme\n-    assert url.userinfo == b\"\"\n-    assert url.netloc == original_netloc\n-    assert url.raw_path == original_raw_path\n-    assert url.query == original_query\n-    assert url.fragment == original_fragment\n+    with pytest.raises(httpx.InvalidURL):\n+        httpx.URL(\"https://u:p@[invalid!]//evilHost/path?t=w#tw\")",
      "comment": "Our test here is much simpler, since this URL no longer passes validation. \ud83d\udc4d",
      "comment_id": 885618832,
      "user": "lovelydinosaur",
      "created_at": "2022-05-31T13:11:00Z",
      "url": "https://github.com/encode/httpx/pull/2252#discussion_r885618832"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2252,
      "file_path": "tests/models/test_url.py",
      "line": 321,
      "side": "RIGHT",
      "diff_hunk": "@@ -312,49 +312,13 @@ def test_url_copywith_security():\n     \"\"\"\n     Prevent unexpected changes on URL after calling copy_with (CVE-2021-41945)\n     \"\"\"\n-    url = httpx.URL(\"https://u:p@[invalid!]//evilHost/path?t=w#tw\")\n-    original_scheme = url.scheme\n-    original_userinfo = url.userinfo\n-    original_netloc = url.netloc\n-    original_raw_path = url.raw_path\n-    original_query = url.query\n-    original_fragment = url.fragment\n-    url = url.copy_with()\n-    assert url.scheme == original_scheme\n-    assert url.userinfo == original_userinfo\n-    assert url.netloc == original_netloc\n-    assert url.raw_path == original_raw_path\n-    assert url.query == original_query\n-    assert url.fragment == original_fragment\n-\n-    url = httpx.URL(\"https://u:p@[invalid!]//evilHost/path?t=w#tw\")\n-    original_scheme = url.scheme\n-    original_netloc = url.netloc\n-    original_raw_path = url.raw_path\n-    original_query = url.query\n-    original_fragment = url.fragment\n-    url = url.copy_with(userinfo=b\"\")\n-    assert url.scheme == original_scheme\n-    assert url.userinfo == b\"\"\n-    assert url.netloc == original_netloc\n-    assert url.raw_path == original_raw_path\n-    assert url.query == original_query\n-    assert url.fragment == original_fragment\n+    with pytest.raises(httpx.InvalidURL):\n+        httpx.URL(\"https://u:p@[invalid!]//evilHost/path?t=w#tw\")\n \n     url = httpx.URL(\"https://example.com/path?t=w#tw\")\n-    original_userinfo = url.userinfo\n-    original_netloc = url.netloc\n-    original_raw_path = url.raw_path\n-    original_query = url.query\n-    original_fragment = url.fragment\n     bad = \"https://xxxx:xxxx@xxxxxxx/xxxxx/xxx?x=x#xxxxx\"\n-    url = url.copy_with(scheme=bad)\n-    assert url.scheme == bad\n-    assert url.userinfo == original_userinfo\n-    assert url.netloc == original_netloc\n-    assert url.raw_path == original_raw_path\n-    assert url.query == original_query\n-    assert url.fragment == original_fragment\n+    with pytest.raises(httpx.InvalidURL):\n+        url.copy_with(scheme=bad)",
      "comment": "Similarly, this scheme no longer validates, which is an improved behaviour.",
      "comment_id": 885619451,
      "user": "lovelydinosaur",
      "created_at": "2022-05-31T13:11:34Z",
      "url": "https://github.com/encode/httpx/pull/2252#discussion_r885619451"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2252,
      "file_path": "tests/test_asgi.py",
      "line": 119,
      "side": "RIGHT",
      "diff_hunk": "@@ -116,7 +116,7 @@ async def test_asgi_raw_path():\n         response = await client.get(url)\n \n     assert response.status_code == 200\n-    assert response.json() == {\"raw_path\": \"/user%40example.org\"}\n+    assert response.json() == {\"raw_path\": \"/user@example.org\"}",
      "comment": "This test changes, because of some improved behaviour. \"@\" should not be an auto-escaping character in the path.\r\n\r\nTry `https://www.example.com/some@path` in a browser, or see RFC sec 3.3...\r\n\r\nFrom https://datatracker.ietf.org/doc/html/rfc3986.html#section-3.3...\r\n\r\n> `pchar = unreserved / pct-encoded / sub-delims / \":\" / \"@\"`",
      "comment_id": 885623387,
      "user": "lovelydinosaur",
      "created_at": "2022-05-31T13:14:58Z",
      "url": "https://github.com/encode/httpx/pull/2252#discussion_r885623387"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2252,
      "file_path": "httpx/_urls.py",
      "line": 130,
      "side": "RIGHT",
      "diff_hunk": "@@ -73,76 +71,87 @@ class URL:\n     def __init__(\n         self, url: typing.Union[\"URL\", str] = \"\", **kwargs: typing.Any\n     ) -> None:\n+        if kwargs:\n+            allowed = {\n+                \"scheme\": str,\n+                \"username\": str,\n+                \"password\": str,\n+                \"userinfo\": bytes,\n+                \"host\": str,\n+                \"port\": int,\n+                \"netloc\": bytes,\n+                \"path\": str,\n+                \"query\": bytes,\n+                \"raw_path\": bytes,\n+                \"fragment\": str,\n+                \"params\": object,\n+            }\n+\n+            # Perform type checking for all supported keyword arguments.\n+            for key, value in kwargs.items():\n+                if key not in allowed:\n+                    message = f\"{key!r} is an invalid keyword argument for URL()\"\n+                    raise TypeError(message)\n+                if value is not None and not isinstance(value, allowed[key]):\n+                    expected = allowed[key].__name__\n+                    seen = type(value).__name__\n+                    message = f\"Argument {key!r} must be {expected} but got {seen}\"\n+                    raise TypeError(message)\n+                if isinstance(value, bytes):\n+                    kwargs[key] = value.decode(\"ascii\")\n+\n+            if \"raw_path\" in kwargs:\n+                kwargs[\"full_path\"] = kwargs.pop(\"raw_path\")\n+\n+            if \"params\" in kwargs:\n+                # Replace any \"params\" keyword with the raw \"query\" instead.\n+                #\n+                # Ensure that empty params use `kwargs[\"query\"] = None` rather\n+                # than `kwargs[\"query\"] = \"\"`, so that generated URLs do not\n+                # include an empty trailing \"?\".\n+                params = kwargs.pop(\"params\")\n+                kwargs[\"query\"] = None if not params else str(QueryParams(params))\n+\n         if isinstance(url, str):\n-            try:\n-                self._uri_reference = rfc3986.iri_reference(url).encode()\n-            except rfc3986.exceptions.InvalidAuthority as exc:\n-                raise InvalidURL(message=str(exc)) from None\n-\n-            if self.is_absolute_url:\n-                # We don't want to normalize relative URLs, since doing so\n-                # removes any leading `../` portion.\n-                self._uri_reference = self._uri_reference.normalize()\n+            self._uri_reference = urlparse(url, **kwargs)\n         elif isinstance(url, URL):\n-            self._uri_reference = url._uri_reference\n+            self._uri_reference = url._uri_reference.copy_with(**kwargs)\n         else:\n             raise TypeError(\n                 f\"Invalid type for url.  Expected str or httpx.URL, got {type(url)}: {url!r}\"\n             )\n \n-        # Perform port normalization, following the WHATWG spec for default ports.\n-        #\n-        # See:\n-        # * https://tools.ietf.org/html/rfc3986#section-3.2.3\n-        # * https://url.spec.whatwg.org/#url-miscellaneous\n-        # * https://url.spec.whatwg.org/#scheme-state\n-        default_port = {\n-            \"ftp\": \":21\",\n-            \"http\": \":80\",\n-            \"https\": \":443\",\n-            \"ws\": \":80\",\n-            \"wss\": \":443\",\n-        }.get(self._uri_reference.scheme, \"\")\n-        authority = self._uri_reference.authority or \"\"\n-        if default_port and authority.endswith(default_port):\n-            authority = authority[: -len(default_port)]\n-            self._uri_reference = self._uri_reference.copy_with(authority=authority)\n-\n-        if kwargs:\n-            self._uri_reference = self.copy_with(**kwargs)._uri_reference\n-\n     @property\n     def scheme(self) -> str:\n         \"\"\"\n         The URL scheme, such as \"http\", \"https\".\n         Always normalised to lowercase.\n         \"\"\"\n-        return self._uri_reference.scheme or \"\"\n+        return self._uri_reference.scheme",
      "comment": "Unlike with `rfc3986`, this value can no longer be `None`.\r\nIt's not needed, since an empty string is sufficient.\r\n\r\nSee also other cases below.",
      "comment_id": 886778198,
      "user": "lovelydinosaur",
      "created_at": "2022-06-01T13:01:22Z",
      "url": "https://github.com/encode/httpx/pull/2252#discussion_r886778198"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2252,
      "file_path": "httpx/_urls.py",
      "line": 336,
      "side": "RIGHT",
      "diff_hunk": "@@ -340,127 +333,7 @@ def copy_with(self, **kwargs: typing.Any) -> \"URL\":\n         url = httpx.URL(\"https://www.example.com\").copy_with(username=\"jo@gmail.com\", password=\"a secret\")\n         assert url == \"https://jo%40email.com:a%20secret@www.example.com\"\n         \"\"\"\n-        allowed = {\n-            \"scheme\": str,\n-            \"username\": str,\n-            \"password\": str,\n-            \"userinfo\": bytes,\n-            \"host\": str,\n-            \"port\": int,\n-            \"netloc\": bytes,\n-            \"path\": str,\n-            \"query\": bytes,\n-            \"raw_path\": bytes,\n-            \"fragment\": str,\n-            \"params\": object,\n-        }\n-\n-        # Step 1\n-        # ======\n-        #\n-        # Perform type checking for all supported keyword arguments.\n-        for key, value in kwargs.items():\n-            if key not in allowed:\n-                message = f\"{key!r} is an invalid keyword argument for copy_with()\"\n-                raise TypeError(message)\n-            if value is not None and not isinstance(value, allowed[key]):\n-                expected = allowed[key].__name__\n-                seen = type(value).__name__\n-                message = f\"Argument {key!r} must be {expected} but got {seen}\"\n-                raise TypeError(message)\n-\n-        # Step 2\n-        # ======\n-        #\n-        # Consolidate \"username\", \"password\", \"userinfo\", \"host\", \"port\" and \"netloc\"\n-        # into a single \"authority\" keyword, for `rfc3986`.\n-        if \"username\" in kwargs or \"password\" in kwargs:\n-            # Consolidate \"username\" and \"password\" into \"userinfo\".\n-            username = quote(kwargs.pop(\"username\", self.username) or \"\")\n-            password = quote(kwargs.pop(\"password\", self.password) or \"\")\n-            userinfo = f\"{username}:{password}\" if password else username\n-            kwargs[\"userinfo\"] = userinfo.encode(\"ascii\")\n-\n-        if \"host\" in kwargs or \"port\" in kwargs:\n-            # Consolidate \"host\" and \"port\" into \"netloc\".\n-            host = kwargs.pop(\"host\", self.host) or \"\"\n-            port = kwargs.pop(\"port\", self.port)\n-\n-            if host and \":\" in host and host[0] != \"[\":\n-                # IPv6 addresses need to be escaped within square brackets.\n-                host = f\"[{host}]\"\n-\n-            kwargs[\"netloc\"] = (\n-                f\"{host}:{port}\".encode(\"ascii\")\n-                if port is not None\n-                else host.encode(\"ascii\")\n-            )\n-\n-        if \"userinfo\" in kwargs or \"netloc\" in kwargs:\n-            # Consolidate \"userinfo\" and \"netloc\" into authority.\n-            userinfo = (kwargs.pop(\"userinfo\", self.userinfo) or b\"\").decode(\"ascii\")\n-            netloc = (kwargs.pop(\"netloc\", self.netloc) or b\"\").decode(\"ascii\")\n-            authority = f\"{userinfo}@{netloc}\" if userinfo else netloc\n-            kwargs[\"authority\"] = authority\n-\n-        # Step 3\n-        # ======\n-        #\n-        # Wrangle any \"path\", \"query\", \"raw_path\" and \"params\" keywords into\n-        # \"query\" and \"path\" keywords for `rfc3986`.\n-        if \"raw_path\" in kwargs:\n-            # If \"raw_path\" is included, then split it into \"path\" and \"query\" components.\n-            raw_path = kwargs.pop(\"raw_path\") or b\"\"\n-            path, has_query, query = raw_path.decode(\"ascii\").partition(\"?\")\n-            kwargs[\"path\"] = path\n-            kwargs[\"query\"] = query if has_query else None\n-\n-        else:\n-            if kwargs.get(\"path\") is not None:\n-                # Ensure `kwargs[\"path\"] = <url quoted str>` for `rfc3986`.\n-                kwargs[\"path\"] = quote(kwargs[\"path\"])\n-\n-            if kwargs.get(\"query\") is not None:\n-                # Ensure `kwargs[\"query\"] = <str>` for `rfc3986`.\n-                #\n-                # Note that `.copy_with(query=None)` and `.copy_with(query=b\"\")`\n-                # are subtly different. The `None` style will not include an empty\n-                # trailing \"?\" character.\n-                kwargs[\"query\"] = kwargs[\"query\"].decode(\"ascii\")\n-\n-            if \"params\" in kwargs:\n-                # Replace any \"params\" keyword with the raw \"query\" instead.\n-                #\n-                # Ensure that empty params use `kwargs[\"query\"] = None` rather\n-                # than `kwargs[\"query\"] = \"\"`, so that generated URLs do not\n-                # include an empty trailing \"?\".\n-                params = kwargs.pop(\"params\")\n-                kwargs[\"query\"] = None if not params else str(QueryParams(params))\n-\n-        # Step 4\n-        # ======\n-        #\n-        # Ensure any fragment component is quoted.\n-        if kwargs.get(\"fragment\") is not None:\n-            kwargs[\"fragment\"] = quote(kwargs[\"fragment\"])\n-\n-        # Step 5\n-        # ======\n-        #\n-        # At this point kwargs may include keys for \"scheme\", \"authority\", \"path\",\n-        # \"query\" and \"fragment\". Together these constitute the entire URL.\n-        #\n-        # See https://tools.ietf.org/html/rfc3986#section-3\n-        #\n-        #  foo://example.com:8042/over/there?name=ferret#nose\n-        #  \\_/   \\______________/\\_________/ \\_________/ \\__/\n-        #   |           |            |            |        |\n-        # scheme     authority       path        query   fragment\n-        new_url = URL(self)\n-        new_url._uri_reference = self._uri_reference.copy_with(**kwargs)\n-        if new_url.is_absolute_url:\n-            new_url._uri_reference = new_url._uri_reference.normalize()\n-        return URL(new_url)\n+        return URL(self, **kwargs)",
      "comment": "Note that our parameter checking moves into `__init__(...)` instead.",
      "comment_id": 886792776,
      "user": "lovelydinosaur",
      "created_at": "2022-06-01T13:15:17Z",
      "url": "https://github.com/encode/httpx/pull/2252#discussion_r886792776"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2252,
      "file_path": "httpx/_urls.py",
      "line": 362,
      "side": "RIGHT",
      "diff_hunk": "@@ -484,21 +357,9 @@ def join(self, url: URLTypes) -> \"URL\":\n         url = url.join(\"/new/path\")\n         assert url == \"https://www.example.com/new/path\"\n         \"\"\"\n-        if self.is_relative_url:\n-            # Workaround to handle relative URLs, which otherwise raise\n-            # rfc3986.exceptions.ResolutionError when used as an argument\n-            # in `.resolve_with`.\n-            return (\n-                self.copy_with(scheme=\"http\", host=\"example.com\")\n-                .join(url)\n-                .copy_with(scheme=None, host=None)\n-            )\n+        from urllib.parse import urljoin\n \n-        # We drop any fragment portion, because RFC 3986 strictly\n-        # treats URLs with a fragment portion as not being absolute URLs.\n-        base_uri = self._uri_reference.copy_with(fragment=None)\n-        relative_url = URL(url)\n-        return URL(relative_url._uri_reference.resolve_with(base_uri).unsplit())\n+        return URL(urljoin(str(self), str(URL(url))))",
      "comment": "We're just leaning on the stdlib's built-in implementation of `urljoin` now, but making sure to use our URL validation and normalisation first.",
      "comment_id": 886793608,
      "user": "lovelydinosaur",
      "created_at": "2022-06-01T13:16:04Z",
      "url": "https://github.com/encode/httpx/pull/2252#discussion_r886793608"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2252,
      "file_path": "httpx/_urls.py",
      "line": 101,
      "side": "RIGHT",
      "diff_hunk": "@@ -73,76 +71,87 @@ class URL:\n     def __init__(\n         self, url: typing.Union[\"URL\", str] = \"\", **kwargs: typing.Any\n     ) -> None:\n+        if kwargs:\n+            allowed = {\n+                \"scheme\": str,\n+                \"username\": str,\n+                \"password\": str,\n+                \"userinfo\": bytes,\n+                \"host\": str,\n+                \"port\": int,\n+                \"netloc\": bytes,\n+                \"path\": str,\n+                \"query\": bytes,\n+                \"raw_path\": bytes,\n+                \"fragment\": str,\n+                \"params\": object,\n+            }\n+\n+            # Perform type checking for all supported keyword arguments.\n+            for key, value in kwargs.items():\n+                if key not in allowed:\n+                    message = f\"{key!r} is an invalid keyword argument for URL()\"\n+                    raise TypeError(message)\n+                if value is not None and not isinstance(value, allowed[key]):\n+                    expected = allowed[key].__name__\n+                    seen = type(value).__name__\n+                    message = f\"Argument {key!r} must be {expected} but got {seen}\"\n+                    raise TypeError(message)\n+                if isinstance(value, bytes):\n+                    kwargs[key] = value.decode(\"ascii\")",
      "comment": "Our `urlparse` implementation uses strings everywhere. If `bytes` are provided, then coerce to an ascii string.\r\n\r\nThis is internal detail, but there are some interesting public API considerations that this work has prompted, tho going to leave those as follow-up.",
      "comment_id": 886796756,
      "user": "lovelydinosaur",
      "created_at": "2022-06-01T13:18:49Z",
      "url": "https://github.com/encode/httpx/pull/2252#discussion_r886796756"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2252,
      "file_path": "httpx/_urls.py",
      "line": 113,
      "side": "RIGHT",
      "diff_hunk": "@@ -73,76 +71,87 @@ class URL:\n     def __init__(\n         self, url: typing.Union[\"URL\", str] = \"\", **kwargs: typing.Any\n     ) -> None:\n+        if kwargs:\n+            allowed = {\n+                \"scheme\": str,\n+                \"username\": str,\n+                \"password\": str,\n+                \"userinfo\": bytes,\n+                \"host\": str,\n+                \"port\": int,\n+                \"netloc\": bytes,\n+                \"path\": str,\n+                \"query\": bytes,\n+                \"raw_path\": bytes,\n+                \"fragment\": str,\n+                \"params\": object,\n+            }\n+\n+            # Perform type checking for all supported keyword arguments.\n+            for key, value in kwargs.items():\n+                if key not in allowed:\n+                    message = f\"{key!r} is an invalid keyword argument for URL()\"\n+                    raise TypeError(message)\n+                if value is not None and not isinstance(value, allowed[key]):\n+                    expected = allowed[key].__name__\n+                    seen = type(value).__name__\n+                    message = f\"Argument {key!r} must be {expected} but got {seen}\"\n+                    raise TypeError(message)\n+                if isinstance(value, bytes):\n+                    kwargs[key] = value.decode(\"ascii\")\n+\n+            if \"raw_path\" in kwargs:\n+                kwargs[\"full_path\"] = kwargs.pop(\"raw_path\")\n+\n+            if \"params\" in kwargs:\n+                # Replace any \"params\" keyword with the raw \"query\" instead.\n+                #\n+                # Ensure that empty params use `kwargs[\"query\"] = None` rather\n+                # than `kwargs[\"query\"] = \"\"`, so that generated URLs do not\n+                # include an empty trailing \"?\".\n+                params = kwargs.pop(\"params\")\n+                kwargs[\"query\"] = None if not params else str(QueryParams(params))",
      "comment": "The \"params\" argument isn't used but the `urlparse` implementation, because the `QueryParams` model doesn't exist at that level of abstraction.",
      "comment_id": 886797472,
      "user": "lovelydinosaur",
      "created_at": "2022-06-01T13:19:32Z",
      "url": "https://github.com/encode/httpx/pull/2252#discussion_r886797472"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2252,
      "file_path": "httpx/_urlparse.py",
      "line": 57,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,435 @@\n+\"\"\"\n+An implementation of `urlparse` that provides URL validation and normalization\n+as described by RFC3986.\n+\n+We rely on this implementation rather than the one in Python's stdlib, because:\n+\n+* It provides more complete URL validation.\n+* It properly differentiates between an empty querystring and an absent querystring,\n+  to distinguish URLs with a trailing '?'.\n+* It handles scheme, hostname, port, and path normalization.\n+* It supports IDNA hostnames, normalizing them to their encoded form.\n+* The API supports passing individual components, as well as the complete URL string.\n+\n+Previously we relied on the excellent `rfc3986` package to handle URL parsing and\n+validation, but this module provides a simpler alternative, with less indirection\n+required.\n+\"\"\"\n+import ipaddress\n+import re\n+import typing\n+\n+import idna\n+\n+from ._exceptions import InvalidURL\n+\n+MAX_URL_LENGTH = 65536\n+\n+# https://datatracker.ietf.org/doc/html/rfc3986.html#section-2.3\n+UNRESERVED_CHARACTERS = (\n+    \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789-._~\"\n+)\n+SUB_DELIMS = \"!$&'()*+,;=\"\n+\n+PERCENT_ENCODED_REGEX = re.compile(\"%[A-Fa-f0-9]{2}\")\n+\n+\n+# {scheme}:      (optional)\n+# //{authority}  (optional)\n+# {path}\n+# ?{query}       (optional)\n+# #{fragment}    (optional)\n+URL_REGEX = re.compile(\n+    (\n+        r\"(?:(?P<scheme>{scheme}):)?\"\n+        r\"(?://(?P<authority>{authority}))?\"\n+        r\"(?P<path>{path})\"\n+        r\"(?:\\?(?P<query>{query}))?\"\n+        r\"(?:#(?P<fragment>{fragment}))?\"\n+    ).format(\n+        scheme=\"([a-zA-Z][a-zA-Z0-9+.-]*)?\",\n+        authority=\"[^/?#]*\",\n+        path=\"[^?#]*\",\n+        query=\"[^#]*\",\n+        fragment=\".*\",\n+    )\n+)\n+",
      "comment": "I just want to point out, I love love LOVE using format strings to improve readability and maintainability of regex.",
      "comment_id": 906862566,
      "user": "xkortex",
      "created_at": "2022-06-26T19:39:48Z",
      "url": "https://github.com/encode/httpx/pull/2252#discussion_r906862566"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2252,
      "file_path": "httpx/_urls.py",
      "line": 384,
      "side": "RIGHT",
      "diff_hunk": "@@ -507,21 +365,33 @@ def __eq__(self, other: typing.Any) -> bool:\n         return isinstance(other, (URL, str)) and str(self) == str(URL(other))\n \n     def __str__(self) -> str:\n-        return self._uri_reference.unsplit()\n+        return str(self._uri_reference)\n \n     def __repr__(self) -> str:\n-        class_name = self.__class__.__name__\n-        url_str = str(self)\n-        if self._uri_reference.userinfo:\n-            # Mask any password component in the URL representation, to lower the\n-            # risk of unintended leakage, such as in debug information and logging.\n-            username = quote(self.username)\n-            url_str = (\n-                rfc3986.urlparse(url_str)\n-                .copy_with(userinfo=f\"{username}:[secure]\")\n-                .unsplit()\n-            )\n-        return f\"{class_name}({url_str!r})\"\n+        scheme, userinfo, host, port, path, query, fragment = self._uri_reference\n+\n+        if \":\" in userinfo:\n+            # Mask any password component.\n+            userinfo = f'{userinfo.split(\":\")[0]}:[secure]'\n+\n+        authority = \"\".join(\n+            [\n+                f\"{userinfo}@\" if userinfo else \"\",\n+                f\"[{host}]\" if \":\" in host else host,\n+                f\":{port}\" if port is not None else \"\",\n+            ]\n+        )\n+        url = \"\".join(",
      "comment": "Question: could this just be str(self._url_reference)? Just curious what the implications are of the different rendering methods are. I think it'd be more parsimonious to have a single URL->str but I could see this being a backwards-compat thing.",
      "comment_id": 906863406,
      "user": "xkortex",
      "created_at": "2022-06-26T19:48:46Z",
      "url": "https://github.com/encode/httpx/pull/2252#discussion_r906863406"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2252,
      "file_path": "httpx/_urls.py",
      "line": 384,
      "side": "RIGHT",
      "diff_hunk": "@@ -507,21 +365,33 @@ def __eq__(self, other: typing.Any) -> bool:\n         return isinstance(other, (URL, str)) and str(self) == str(URL(other))\n \n     def __str__(self) -> str:\n-        return self._uri_reference.unsplit()\n+        return str(self._uri_reference)\n \n     def __repr__(self) -> str:\n-        class_name = self.__class__.__name__\n-        url_str = str(self)\n-        if self._uri_reference.userinfo:\n-            # Mask any password component in the URL representation, to lower the\n-            # risk of unintended leakage, such as in debug information and logging.\n-            username = quote(self.username)\n-            url_str = (\n-                rfc3986.urlparse(url_str)\n-                .copy_with(userinfo=f\"{username}:[secure]\")\n-                .unsplit()\n-            )\n-        return f\"{class_name}({url_str!r})\"\n+        scheme, userinfo, host, port, path, query, fragment = self._uri_reference\n+\n+        if \":\" in userinfo:\n+            # Mask any password component.\n+            userinfo = f'{userinfo.split(\":\")[0]}:[secure]'\n+\n+        authority = \"\".join(\n+            [\n+                f\"{userinfo}@\" if userinfo else \"\",\n+                f\"[{host}]\" if \":\" in host else host,\n+                f\":{port}\" if port is not None else \"\",\n+            ]\n+        )\n+        url = \"\".join(",
      "comment": "Right now we're doing this here because we need to mask the password for `__repr__`, but not for the `__str__`.\r\n\r\nWe could perhaps simplify a little by having a `to_string(mask_password: bool)` method on the `ParseResult` class.\r\n",
      "comment_id": 907152909,
      "user": "lovelydinosaur",
      "created_at": "2022-06-27T09:06:03Z",
      "url": "https://github.com/encode/httpx/pull/2252#discussion_r907152909"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2252,
      "file_path": "httpx/_urls.py",
      "line": 384,
      "side": "RIGHT",
      "diff_hunk": "@@ -507,21 +365,33 @@ def __eq__(self, other: typing.Any) -> bool:\n         return isinstance(other, (URL, str)) and str(self) == str(URL(other))\n \n     def __str__(self) -> str:\n-        return self._uri_reference.unsplit()\n+        return str(self._uri_reference)\n \n     def __repr__(self) -> str:\n-        class_name = self.__class__.__name__\n-        url_str = str(self)\n-        if self._uri_reference.userinfo:\n-            # Mask any password component in the URL representation, to lower the\n-            # risk of unintended leakage, such as in debug information and logging.\n-            username = quote(self.username)\n-            url_str = (\n-                rfc3986.urlparse(url_str)\n-                .copy_with(userinfo=f\"{username}:[secure]\")\n-                .unsplit()\n-            )\n-        return f\"{class_name}({url_str!r})\"\n+        scheme, userinfo, host, port, path, query, fragment = self._uri_reference\n+\n+        if \":\" in userinfo:\n+            # Mask any password component.\n+            userinfo = f'{userinfo.split(\":\")[0]}:[secure]'\n+\n+        authority = \"\".join(\n+            [\n+                f\"{userinfo}@\" if userinfo else \"\",\n+                f\"[{host}]\" if \":\" in host else host,\n+                f\":{port}\" if port is not None else \"\",\n+            ]\n+        )\n+        url = \"\".join(",
      "comment": "Ohhhh right, because of this, right? https://datatracker.ietf.org/doc/html/rfc3986#section-3.2.1\r\n\r\n> We could perhaps simplify a little by having a to_string(mask_password: bool) method on the ParseResult class.\r\n\r\nThat also sounds reasonable, if it were my codebase, I'd prefer that. Not a big deal either way (obviously not my call here :) ). \r\n\r\n>  we need to mask the password for __repr__, but not for the __str__\r\n\r\nThis is indeed the current behavior of httpx, but boy does that bug me, that the `repr` of a user info string doesn't convert back to even a correct URL, let alone the equivalent URL.  \r\n```\r\nu = httpx.URL('http://user:hunter2@example.com')\r\nu\r\nOut[10]: URL('http://user:[secure]@example.com')\r\nstr(u)\r\nOut[11]: 'http://user:hunter2@example.com'\r\nhttpx.URL('http://user:[secure]@example.com')\r\nOut[12]: URL('http:')\r\n```\r\n\r\nI think it would be better if it didn't use square braces to elide the password, since that screws up later parses, but that's beyond the purview of this PR, so... `\u00af\\_(\u30c4)_/\u00af`  ",
      "comment_id": 907469214,
      "user": "xkortex",
      "created_at": "2022-06-27T14:41:37Z",
      "url": "https://github.com/encode/httpx/pull/2252#discussion_r907469214"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2252,
      "file_path": "httpx/_urls.py",
      "line": 384,
      "side": "RIGHT",
      "diff_hunk": "@@ -507,21 +365,33 @@ def __eq__(self, other: typing.Any) -> bool:\n         return isinstance(other, (URL, str)) and str(self) == str(URL(other))\n \n     def __str__(self) -> str:\n-        return self._uri_reference.unsplit()\n+        return str(self._uri_reference)\n \n     def __repr__(self) -> str:\n-        class_name = self.__class__.__name__\n-        url_str = str(self)\n-        if self._uri_reference.userinfo:\n-            # Mask any password component in the URL representation, to lower the\n-            # risk of unintended leakage, such as in debug information and logging.\n-            username = quote(self.username)\n-            url_str = (\n-                rfc3986.urlparse(url_str)\n-                .copy_with(userinfo=f\"{username}:[secure]\")\n-                .unsplit()\n-            )\n-        return f\"{class_name}({url_str!r})\"\n+        scheme, userinfo, host, port, path, query, fragment = self._uri_reference\n+\n+        if \":\" in userinfo:\n+            # Mask any password component.\n+            userinfo = f'{userinfo.split(\":\")[0]}:[secure]'\n+\n+        authority = \"\".join(\n+            [\n+                f\"{userinfo}@\" if userinfo else \"\",\n+                f\"[{host}]\" if \":\" in host else host,\n+                f\":{port}\" if port is not None else \"\",\n+            ]\n+        )\n+        url = \"\".join(",
      "comment": "> I think it would be better if it didn't use square braces to elide the password, since that screws up later parses, but that's beyond the purview of this PR\r\n\r\nHrm, yes, we could certainly reconsider that. As you say tho, we'd do so independently of this pull request.",
      "comment_id": 933312779,
      "user": "lovelydinosaur",
      "created_at": "2022-07-29T14:09:27Z",
      "url": "https://github.com/encode/httpx/pull/2252#discussion_r933312779"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2278,
      "file_path": "tests/test_multipart.py",
      "line": 100,
      "side": "RIGHT",
      "diff_hunk": "@@ -42,6 +42,64 @@ def test_multipart(value, output):\n     assert multipart[\"file\"] == [b\"<file content>\"]\n \n \n+@pytest.mark.parametrize(\n+    \"header\",\n+    [\n+        \"multipart/form-data; boundary=+++; charset=utf-8\",\n+        \"multipart/form-data; charset=utf-8; boundary=+++\",\n+        \"multipart/form-data; boundary=+++\",\n+        \"multipart/form-data; boundary=+++ ;\",\n+    ],\n+)\n+def test_multipart_explicit_boundary(header: str) -> None:\n+    client = httpx.Client(transport=httpx.MockTransport(echo_request_content))\n+\n+    files = {\"file\": io.BytesIO(b\"<file content>\")}\n+    headers = {\"content-type\": header}\n+    response = client.post(\"http://127.0.0.1:8000/\", files=files, headers=headers)\n+    assert response.status_code == 200\n+\n+    # We're using the cgi module to verify the behavior here, which is a\n+    # bit grungy, but sufficient just for our testing purposes.\n+    assert response.request.headers[\"Content-Type\"] == header\n+    content_length = response.request.headers[\"Content-Length\"]\n+    pdict: dict = {\n+        \"boundary\": b\"+++\",\n+        \"CONTENT-LENGTH\": content_length,\n+    }\n+    multipart = cgi.parse_multipart(io.BytesIO(response.content), pdict)\n+\n+    assert multipart[\"file\"] == [b\"<file content>\"]\n+\n+\n+@pytest.mark.parametrize(\n+    \"header\",\n+    [\n+        \"multipart/form-data; charset=utf-8\",\n+        \"multipart/form-data; charset=utf-8; \",\n+    ],\n+)\n+def test_multipart_header_without_boundary(header: str) -> None:\n+    client = httpx.Client(transport=httpx.MockTransport(echo_request_content))\n+\n+    files = {\"file\": io.BytesIO(b\"<file content>\")}\n+    headers = {\"content-type\": header}\n+    response = client.post(\"http://127.0.0.1:8000/\", files=files, headers=headers)\n+    assert response.status_code == 200\n+\n+    # We're using the cgi module to verify the behavior here, which is a\n+    # bit grungy, but sufficient just for our testing purposes.\n+    boundary = response.request.headers[\"Content-Type\"].split(\"boundary=\")[-1]\n+    content_length = response.request.headers[\"Content-Length\"]\n+    pdict: dict = {\n+        \"boundary\": boundary.encode(\"ascii\"),\n+        \"CONTENT-LENGTH\": content_length,\n+    }\n+    multipart = cgi.parse_multipart(io.BytesIO(response.content), pdict)\n+\n+    assert multipart[\"file\"] == [b\"<file content>\"]",
      "comment": "This test is fails on main/master as well, but I think it would be important to fix in this PR. What needs to happen is we need to create the random boundary and then insert that into the existing header. The issue is that there is currently no machinery / precedent for `encode_request()` modifying headers, so it would take a bit more refactoring to get working. Alternatively, we can raise an error if a user gives us a `content-type: multipart/form-data` without an explicit header?",
      "comment_id": 905240480,
      "user": "adriangb",
      "created_at": "2022-06-23T16:33:10Z",
      "url": "https://github.com/encode/httpx/pull/2278#discussion_r905240480"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2278,
      "file_path": "httpx/_content.py",
      "line": 201,
      "side": "RIGHT",
      "diff_hunk": "@@ -187,6 +198,7 @@ def encode_request(\n     files: Optional[RequestFiles] = None,\n     json: Optional[Any] = None,\n     boundary: Optional[bytes] = None,\n+    headers: Optional[Mapping[str, str]] = None,",
      "comment": "How about we keep things a little tighter here by passing `content_type: str = None` instead? That restricts the information we're passing around to the one thing we're actually interested in.",
      "comment_id": 905940496,
      "user": "lovelydinosaur",
      "created_at": "2022-06-24T10:35:58Z",
      "url": "https://github.com/encode/httpx/pull/2278#discussion_r905940496"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2278,
      "file_path": "httpx/_multipart.py",
      "line": 30,
      "side": "RIGHT",
      "diff_hunk": "@@ -20,6 +20,16 @@\n )\n \n \n+def get_multipart_boundary_from_content_type(\n+    content_type: str,\n+) -> bytes:\n+    if \";\" in content_type:\n+        for section in content_type.split(\";\"):\n+            if section.strip().startswith(\"boundary=\"):\n+                return section.strip().split(\"boundary=\")[-1].encode(\"latin-1\")\n+    raise ValueError(\"Missing boundary in multipart/form-data content-type header\")",
      "comment": "Instead of raising `ValueError` we could just return `None` if the content type doesn't start with \"multipart/form-data\", or doesn't include a valid boundary.\r\n\r\n(Users can currently submit requests with `Content-Type` headers that don't properly match up to the data they include, so just being lax here seems okay to me.)",
      "comment_id": 905947972,
      "user": "lovelydinosaur",
      "created_at": "2022-06-24T10:47:18Z",
      "url": "https://github.com/encode/httpx/pull/2278#discussion_r905947972"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2278,
      "file_path": "httpx/_content.py",
      "line": 161,
      "side": "RIGHT",
      "diff_hunk": "@@ -150,11 +151,21 @@ def encode_urlencoded_data(\n \n \n def encode_multipart_data(\n-    data: dict, files: RequestFiles, boundary: Optional[bytes] = None\n+    data: dict,\n+    files: RequestFiles,\n+    boundary: Optional[bytes] = None,\n+    headers: Optional[Mapping[str, str]] = None,\n ) -> Tuple[Dict[str, str], MultipartStream]:\n+    if headers and boundary is None and \"content-type\" in headers:\n+        content_type = headers[\"content-type\"]\n+        if not content_type.startswith(\"multipart/form-data\"):",
      "comment": "I think we could drop this case, as with [this comment](https://github.com/encode/httpx/pull/2278/files#r905947972).",
      "comment_id": 905948476,
      "user": "lovelydinosaur",
      "created_at": "2022-06-24T10:48:07Z",
      "url": "https://github.com/encode/httpx/pull/2278#discussion_r905948476"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2278,
      "file_path": "httpx/_multipart.py",
      "line": 30,
      "side": "RIGHT",
      "diff_hunk": "@@ -20,6 +20,16 @@\n )\n \n \n+def get_multipart_boundary_from_content_type(\n+    content_type: str,\n+) -> bytes:\n+    if \";\" in content_type:\n+        for section in content_type.split(\";\"):\n+            if section.strip().startswith(\"boundary=\"):\n+                return section.strip().split(\"boundary=\")[-1].encode(\"latin-1\")\n+    raise ValueError(\"Missing boundary in multipart/form-data content-type header\")",
      "comment": "Note that since `content-type` gets set [via `setdefault`](https://github.com/encode/httpx/blob/aad60a4f123801e7ce0e02bc49138e7f0f9ca0a5/httpx/_models.py#L362). If it already exists, it won't be overwritten. So in the case where the user provided the `content-type` header but did not provide the multipart boundary we'd be sending out a request with no multipart boundary, which completely violates the spec and no server will be able to parse.",
      "comment_id": 906733887,
      "user": "adriangb",
      "created_at": "2022-06-25T23:17:15Z",
      "url": "https://github.com/encode/httpx/pull/2278#discussion_r906733887"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2278,
      "file_path": "httpx/_multipart.py",
      "line": 30,
      "side": "RIGHT",
      "diff_hunk": "@@ -20,6 +20,16 @@\n )\n \n \n+def get_multipart_boundary_from_content_type(\n+    content_type: str,\n+) -> bytes:\n+    if \";\" in content_type:\n+        for section in content_type.split(\";\"):\n+            if section.strip().startswith(\"boundary=\"):\n+                return section.strip().split(\"boundary=\")[-1].encode(\"latin-1\")\n+    raise ValueError(\"Missing boundary in multipart/form-data content-type header\")",
      "comment": "> So in the case where the user provided the content-type header but did not provide the multipart boundary we'd be sending out a request with no multipart boundary, which completely violates the spec and no server will be able to parse.\r\n\r\nTrue. We could either hard-error in that case, or just ignore it and allow users to do that.\r\n\r\nThere is also a whole class of cases here where a user can set a `Content-Type` that doesn't match the encoding type they're using with `data=`/`files=`/`json=`. (Eg. they can set `Content-Type: application/json` on either form or multipart requests.) But that's probably okay.\r\n\r\n\r\n\r\n",
      "comment_id": 907177499,
      "user": "lovelydinosaur",
      "created_at": "2022-06-27T09:30:20Z",
      "url": "https://github.com/encode/httpx/pull/2278#discussion_r907177499"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2278,
      "file_path": "httpx/_content.py",
      "line": 159,
      "side": "RIGHT",
      "diff_hunk": "@@ -150,11 +150,16 @@ def encode_urlencoded_data(\n \n \n def encode_multipart_data(\n-    data: dict, files: RequestFiles, boundary: Optional[bytes] = None\n+    data: dict,\n+    files: RequestFiles,\n+    boundary: Optional[bytes] = None,\n+    content_type: Optional[str] = None,\n ) -> Tuple[Dict[str, str], MultipartStream]:\n+    if content_type:\n+        boundary = get_multipart_boundary_from_content_type(content_type)",
      "comment": "Shouldn't we check that `boundary is None` before computing `boundary`? (In other words, what happens when `content_type` and `boundary` do not agree on the value to use for `boundary`?)",
      "comment_id": 945183299,
      "user": "jhominal",
      "created_at": "2022-08-13T19:44:33Z",
      "url": "https://github.com/encode/httpx/pull/2278#discussion_r945183299"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2278,
      "file_path": "httpx/_content.py",
      "line": 159,
      "side": "RIGHT",
      "diff_hunk": "@@ -150,11 +150,16 @@ def encode_urlencoded_data(\n \n \n def encode_multipart_data(\n-    data: dict, files: RequestFiles, boundary: Optional[bytes] = None\n+    data: dict,\n+    files: RequestFiles,\n+    boundary: Optional[bytes] = None,\n+    content_type: Optional[str] = None,\n ) -> Tuple[Dict[str, str], MultipartStream]:\n+    if content_type:\n+        boundary = get_multipart_boundary_from_content_type(content_type)",
      "comment": "We're the only ones ever making the call into this function and we never call it with a mismatch. If I add the error I would either have to (1) add a test just to check the error or (2) pragma: no cover it. I'll add a comment instead.",
      "comment_id": 945184522,
      "user": "adriangb",
      "created_at": "2022-08-13T20:00:32Z",
      "url": "https://github.com/encode/httpx/pull/2278#discussion_r945184522"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2278,
      "file_path": "httpx/_content.py",
      "line": 159,
      "side": "RIGHT",
      "diff_hunk": "@@ -150,11 +150,16 @@ def encode_urlencoded_data(\n \n \n def encode_multipart_data(\n-    data: dict, files: RequestFiles, boundary: Optional[bytes] = None\n+    data: dict,\n+    files: RequestFiles,\n+    boundary: Optional[bytes] = None,\n+    content_type: Optional[str] = None,\n ) -> Tuple[Dict[str, str], MultipartStream]:\n+    if content_type:\n+        boundary = get_multipart_boundary_from_content_type(content_type)",
      "comment": "Shouldn't the `get_multipart_boundary_from_content_type` call be made upstream? e.g. in httpx/_models.py?",
      "comment_id": 945184797,
      "user": "jhominal",
      "created_at": "2022-08-13T20:03:24Z",
      "url": "https://github.com/encode/httpx/pull/2278#discussion_r945184797"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2278,
      "file_path": "httpx/_models.py",
      "line": 341,
      "side": "RIGHT",
      "diff_hunk": "@@ -332,7 +332,13 @@ def __init__(\n             Cookies(cookies).set_cookie_header(self)\n \n         if stream is None:\n-            headers, stream = encode_request(content, data, files, json)\n+            headers, stream = encode_request(\n+                content=content,\n+                data=data,\n+                files=files,\n+                json=json,\n+                content_type=self.headers.get(\"content-type\"),\n+            )",
      "comment": "By the comment on the other thread, I mean, why not write:\r\n```suggestion\r\n            headers, stream = encode_request(\r\n                content=content,\r\n                data=data,\r\n                files=files,\r\n                json=json,\r\n                boundary= get_multipart_boundary_from_content_type(self.headers.get(\"content-type\")),\r\n            )\r\n```\r\n\r\nThat way, we do not have function signatures that have both a `boundary` and a `content_type` that could possibly have contradictory data?",
      "comment_id": 945184961,
      "user": "jhominal",
      "created_at": "2022-08-13T20:06:12Z",
      "url": "https://github.com/encode/httpx/pull/2278#discussion_r945184961"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2278,
      "file_path": "httpx/_models.py",
      "line": 341,
      "side": "RIGHT",
      "diff_hunk": "@@ -332,7 +332,13 @@ def __init__(\n             Cookies(cookies).set_cookie_header(self)\n \n         if stream is None:\n-            headers, stream = encode_request(content, data, files, json)\n+            headers, stream = encode_request(\n+                content=content,\n+                data=data,\n+                files=files,\n+                json=json,\n+                content_type=self.headers.get(\"content-type\"),\n+            )",
      "comment": "By which I mean, the `content_type` argument could be removed from `encode_request` and `encode_multipart_data`?",
      "comment_id": 945185034,
      "user": "jhominal",
      "created_at": "2022-08-13T20:07:03Z",
      "url": "https://github.com/encode/httpx/pull/2278#discussion_r945185034"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2278,
      "file_path": "httpx/_content.py",
      "line": 159,
      "side": "RIGHT",
      "diff_hunk": "@@ -150,11 +150,16 @@ def encode_urlencoded_data(\n \n \n def encode_multipart_data(\n-    data: dict, files: RequestFiles, boundary: Optional[bytes] = None\n+    data: dict,\n+    files: RequestFiles,\n+    boundary: Optional[bytes] = None,\n+    content_type: Optional[str] = None,\n ) -> Tuple[Dict[str, str], MultipartStream]:\n+    if content_type:\n+        boundary = get_multipart_boundary_from_content_type(content_type)",
      "comment": "I think _should_ would be a bit strong of a word, we certainly _could_ do that. I think the main reason for having it here is that `_models.py` doesn't know about multipart request or other encoding stuff, so it would be a bit of a violation of the layering that is currently set up to put something like `if content_type.startswith(\"multipart/form-data\")`: <do multipart specific stuff>`.",
      "comment_id": 945185173,
      "user": "adriangb",
      "created_at": "2022-08-13T20:08:37Z",
      "url": "https://github.com/encode/httpx/pull/2278#discussion_r945185173"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2278,
      "file_path": "httpx/_content.py",
      "line": 159,
      "side": "RIGHT",
      "diff_hunk": "@@ -150,11 +150,16 @@ def encode_urlencoded_data(\n \n \n def encode_multipart_data(\n-    data: dict, files: RequestFiles, boundary: Optional[bytes] = None\n+    data: dict,\n+    files: RequestFiles,\n+    boundary: Optional[bytes] = None,\n+    content_type: Optional[str] = None,\n ) -> Tuple[Dict[str, str], MultipartStream]:\n+    if content_type:\n+        boundary = get_multipart_boundary_from_content_type(content_type)",
      "comment": "No need to apologize for a good suggestion \ud83d\ude04 \r\nLike I said I'm open to it, but I do feel that it breaks a bit with the way abstractions are currently set up in the codebase",
      "comment_id": 945185588,
      "user": "adriangb",
      "created_at": "2022-08-13T20:13:18Z",
      "url": "https://github.com/encode/httpx/pull/2278#discussion_r945185588"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2278,
      "file_path": "httpx/_models.py",
      "line": 341,
      "side": "RIGHT",
      "diff_hunk": "@@ -332,7 +332,13 @@ def __init__(\n             Cookies(cookies).set_cookie_header(self)\n \n         if stream is None:\n-            headers, stream = encode_request(content, data, files, json)\n+            headers, stream = encode_request(\n+                content=content,\n+                data=data,\n+                files=files,\n+                json=json,\n+                content_type=self.headers.get(\"content-type\"),\n+            )",
      "comment": "Okay I liked it once I wrote it, I think given that `encode_request` already takes a `boundary` parameter (which only makes sense in the context of multipart requests) moving the call to where you are suggesting is not a big deal",
      "comment_id": 945185780,
      "user": "adriangb",
      "created_at": "2022-08-13T20:15:59Z",
      "url": "https://github.com/encode/httpx/pull/2278#discussion_r945185780"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2278,
      "file_path": "httpx/_models.py",
      "line": 341,
      "side": "RIGHT",
      "diff_hunk": "@@ -332,7 +332,13 @@ def __init__(\n             Cookies(cookies).set_cookie_header(self)\n \n         if stream is None:\n-            headers, stream = encode_request(content, data, files, json)\n+            headers, stream = encode_request(\n+                content=content,\n+                data=data,\n+                files=files,\n+                json=json,\n+                content_type=self.headers.get(\"content-type\"),\n+            )",
      "comment": "Yes, that kind of was my thought (if `encode_request` did not have `boundary` I would not have suggested that). Thank you for accepting my suggestion.\r\n\r\nI just wonder if you would also go also look at potentially reviewing the remaining changes in `httpx/_content.py`? I mean, these changes seem mostly vestigial to me (e.g. the `content_type` in `encode_request`, and the changes in `encode_multipart_data` are now purely of variable and formatting (and `boundary` is now a required argument)",
      "comment_id": 945186211,
      "user": "jhominal",
      "created_at": "2022-08-13T20:20:56Z",
      "url": "https://github.com/encode/httpx/pull/2278#discussion_r945186211"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2278,
      "file_path": "httpx/_multipart.py",
      "line": 31,
      "side": "RIGHT",
      "diff_hunk": "@@ -20,6 +20,18 @@\n )\n \n \n+def get_multipart_boundary_from_content_type(\n+    content_type: typing.Optional[str],\n+) -> typing.Optional[bytes]:\n+    if not content_type or not content_type.startswith(\"multipart/form-data\"):\n+        return None\n+    if \";\" in content_type:\n+        for section in content_type.split(\";\"):\n+            if section.strip().startswith(\"boundary=\"):\n+                return section.strip().split(\"boundary=\")[-1].encode(\"latin-1\")",
      "comment": "Is there any possibility that `boundary` could be quoted, with either simple or double quotes?",
      "comment_id": 945186645,
      "user": "jhominal",
      "created_at": "2022-08-13T20:26:29Z",
      "url": "https://github.com/encode/httpx/pull/2278#discussion_r945186645"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2278,
      "file_path": "httpx/_multipart.py",
      "line": 31,
      "side": "RIGHT",
      "diff_hunk": "@@ -20,6 +20,18 @@\n )\n \n \n+def get_multipart_boundary_from_content_type(\n+    content_type: typing.Optional[str],\n+) -> typing.Optional[bytes]:\n+    if not content_type or not content_type.startswith(\"multipart/form-data\"):\n+        return None\n+    if \";\" in content_type:\n+        for section in content_type.split(\";\"):\n+            if section.strip().startswith(\"boundary=\"):\n+                return section.strip().split(\"boundary=\")[-1].encode(\"latin-1\")",
      "comment": "[RFC 2046](https://www.rfc-editor.org/rfc/rfc2046#section-5.1.1) has examples where boundary is contained in quotes. Do we want to support that?",
      "comment_id": 945186942,
      "user": "jhominal",
      "created_at": "2022-08-13T20:30:19Z",
      "url": "https://github.com/encode/httpx/pull/2278#discussion_r945186942"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2278,
      "file_path": "httpx/_multipart.py",
      "line": 31,
      "side": "RIGHT",
      "diff_hunk": "@@ -20,6 +20,18 @@\n )\n \n \n+def get_multipart_boundary_from_content_type(\n+    content_type: typing.Optional[str],\n+) -> typing.Optional[bytes]:\n+    if not content_type or not content_type.startswith(\"multipart/form-data\"):\n+        return None\n+    if \";\" in content_type:\n+        for section in content_type.split(\";\"):\n+            if section.strip().startswith(\"boundary=\"):\n+                return section.strip().split(\"boundary=\")[-1].encode(\"latin-1\")",
      "comment": "Great catch! Not sure how you find these things but thank you",
      "comment_id": 945209881,
      "user": "adriangb",
      "created_at": "2022-08-13T23:56:04Z",
      "url": "https://github.com/encode/httpx/pull/2278#discussion_r945209881"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2278,
      "file_path": "httpx/_multipart.py",
      "line": 31,
      "side": "RIGHT",
      "diff_hunk": "@@ -20,6 +20,18 @@\n )\n \n \n+def get_multipart_boundary_from_content_type(\n+    content_type: typing.Optional[str],\n+) -> typing.Optional[bytes]:\n+    if not content_type or not content_type.startswith(\"multipart/form-data\"):\n+        return None\n+    if \";\" in content_type:\n+        for section in content_type.split(\";\"):\n+            if section.strip().startswith(\"boundary=\"):\n+                return section.strip().split(\"boundary=\")[-1].encode(\"latin-1\")",
      "comment": "After reviewing the grammar for boundary ([RFC 2046 Appendix A](https://www.rfc-editor.org/rfc/rfc2046#page-43), referencing [RFC 2045 \u00a75.1](https://www.rfc-editor.org/rfc/rfc2045#section-5.1), referencing [RFC 822 \u00a73.3](https://www.rfc-editor.org/rfc/rfc822#section-3.3) which has been replaced by [RFC 5322 \u00a73.2.4](https://www.rfc-editor.org/rfc/rfc5322#section-3.2.4)):\r\n\r\n1. The only quoting character that seems to be valid is the double quote `\"` (`'` is not valid for that usage);\r\n2. However, when the double quote is in use, any backslash `\\` will be part of a \"quoted pair\", meaning that it is semantically equivalent to the next character (`\\\\` => `\\`, `\\\"` => `\"`);\r\n3. However, there is a limitation on allowed characters in a boundary - none of the characters that would require a quoted pair escape are allowed in a boundary;\r\n\r\nIn other words:\r\n * We need to handle only `\"` for quoting;\r\n * We also need to decide what to do about quoted pairs:\r\n   * Either we write a simpler implementation that does not handle them, as all legal boundaries can be written without using quoted pairs;\r\n   * Or we go the extra mile of supporting quoted pairs;\r\n * What do we want to do about the fact that not all ascii characters are allowed as part of a boundary?",
      "comment_id": 945285489,
      "user": "jhominal",
      "created_at": "2022-08-14T12:50:56Z",
      "url": "https://github.com/encode/httpx/pull/2278#discussion_r945285489"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2278,
      "file_path": "httpx/_multipart.py",
      "line": 31,
      "side": "RIGHT",
      "diff_hunk": "@@ -20,6 +20,18 @@\n )\n \n \n+def get_multipart_boundary_from_content_type(\n+    content_type: typing.Optional[str],\n+) -> typing.Optional[bytes]:\n+    if not content_type or not content_type.startswith(\"multipart/form-data\"):\n+        return None\n+    if \";\" in content_type:\n+        for section in content_type.split(\";\"):\n+            if section.strip().startswith(\"boundary=\"):\n+                return section.strip().split(\"boundary=\")[-1].encode(\"latin-1\")",
      "comment": "I think we can go with the simpler implementation. I also don't think we need to handle non-valid characters, that'd be too much introspection ",
      "comment_id": 945291083,
      "user": "adriangb",
      "created_at": "2022-08-14T13:32:51Z",
      "url": "https://github.com/encode/httpx/pull/2278#discussion_r945291083"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2278,
      "file_path": "httpx/_multipart.py",
      "line": 31,
      "side": "RIGHT",
      "diff_hunk": "@@ -20,6 +20,18 @@\n )\n \n \n+def get_multipart_boundary_from_content_type(\n+    content_type: typing.Optional[str],\n+) -> typing.Optional[bytes]:\n+    if not content_type or not content_type.startswith(\"multipart/form-data\"):\n+        return None\n+    if \";\" in content_type:\n+        for section in content_type.split(\";\"):\n+            if section.strip().startswith(\"boundary=\"):\n+                return section.strip().split(\"boundary=\")[-1].encode(\"latin-1\")",
      "comment": "I just removed the `'` stripping, which I think now makes this able to parse any boundary except the ones with quoted pairs (valid but never necessary).",
      "comment_id": 945728832,
      "user": "adriangb",
      "created_at": "2022-08-15T13:22:20Z",
      "url": "https://github.com/encode/httpx/pull/2278#discussion_r945728832"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2278,
      "file_path": "httpx/_multipart.py",
      "line": 33,
      "side": "RIGHT",
      "diff_hunk": "@@ -20,6 +20,20 @@\n )\n \n \n+def get_multipart_boundary_from_content_type(\n+    content_type: typing.Optional[bytes],\n+) -> typing.Optional[bytes]:\n+    if not content_type or not content_type.startswith(b\"multipart/form-data\"):\n+        return None\n+    # parse boundary according to\n+    # https://www.rfc-editor.org/rfc/rfc2046#section-5.1.1\n+    if b\";\" in content_type:\n+        for section in content_type.split(b\";\"):\n+            if section.strip().startswith(b\"boundary=\"):\n+                return section.strip().split(b\"boundary=\")[-1].strip(b'\"')",
      "comment": "```suggestion\r\n            if section.strip().lower().startswith(b\"boundary=\"):\r\n                return section.strip()[len(b\"boundary=\"):].strip(b'\"')\r\n```\r\n\r\nI am making this suggestion to solve two things I see as issues (you can argue that these are niche edge cases, but I think that the changes I suggest are reasonable) with the current implementation:\r\n\r\n 1. As indicated in [RFC 2045](https://www.rfc-editor.org/rfc/rfc2045#section-5.1), matching of attributes (of which `boundary` is one) is always case insensitive (thus `BOUNDARY=\"abcd\"` would also work) - so I have added a call to `lower()` on line 32;\r\n 2. The implementation with `split` will fail on a boundary that contains the `boundary=` substring. I think the intent is clearer if we write code that simply removes the prefix, and also reduces the number of potential edge cases;",
      "comment_id": 945822783,
      "user": "jhominal",
      "created_at": "2022-08-15T14:44:39Z",
      "url": "https://github.com/encode/httpx/pull/2278#discussion_r945822783"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2278,
      "file_path": "tests/test_multipart.py",
      "line": 94,
      "side": "RIGHT",
      "diff_hunk": "@@ -42,6 +42,58 @@ def test_multipart(value, output):\n     assert multipart[\"file\"] == [b\"<file content>\"]\n \n \n+@pytest.mark.parametrize(\n+    \"header\",\n+    [\n+        \"multipart/form-data; boundary=+++; charset=utf-8\",\n+        \"multipart/form-data; charset=utf-8; boundary=+++\",\n+        \"multipart/form-data; boundary=+++\",\n+        \"multipart/form-data; boundary=+++ ;\",\n+        'multipart/form-data; boundary=\"+++\"; charset=utf-8',\n+        'multipart/form-data; charset=utf-8; boundary=\"+++\"',\n+        'multipart/form-data; boundary=\"+++\"',\n+        'multipart/form-data; boundary=\"+++\" ;',\n+    ],\n+)\n+def test_multipart_explicit_boundary(header: str) -> None:\n+    client = httpx.Client(transport=httpx.MockTransport(echo_request_content))\n+\n+    files = {\"file\": io.BytesIO(b\"<file content>\")}\n+    headers = {\"content-type\": header}\n+    response = client.post(\"http://127.0.0.1:8000/\", files=files, headers=headers)\n+    assert response.status_code == 200\n+\n+    # We're using the cgi module to verify the behavior here, which is a\n+    # bit grungy, but sufficient just for our testing purposes.\n+    assert response.request.headers[\"Content-Type\"] == header\n+    content_length = response.request.headers[\"Content-Length\"]\n+    pdict: dict = {\n+        \"boundary\": b\"+++\",\n+        \"CONTENT-LENGTH\": content_length,\n+    }\n+    multipart = cgi.parse_multipart(io.BytesIO(response.content), pdict)\n+\n+    assert multipart[\"file\"] == [b\"<file content>\"]\n+\n+\n+@pytest.mark.parametrize(\n+    \"header\",\n+    [\n+        \"multipart/form-data; charset=utf-8\",\n+        \"multipart/form-data; charset=utf-8; \",\n+    ],\n+)\n+def test_multipart_header_without_boundary(header: str) -> None:\n+    client = httpx.Client(transport=httpx.MockTransport(echo_request_content))\n+\n+    files = {\"file\": io.BytesIO(b\"<file content>\")}\n+    headers = {\"content-type\": header}\n+    response = client.post(\"http://127.0.0.1:8000/\", files=files, headers=headers)\n+\n+    assert response.status_code == 200\n+    assert response.request.headers[\"Content-Type\"] == header",
      "comment": "@adriangb Shouldn't it provide autogenerated boundary in the header ? Seems like it causes this [issue](https://github.com/encode/httpx/issues/3522) ?",
      "comment_id": 1977589495,
      "user": "Anton-Shutik",
      "created_at": "2025-03-03T14:15:18Z",
      "url": "https://github.com/encode/httpx/pull/2278#discussion_r1977589495"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3371,
      "file_path": "tests/models/test_url.py",
      "line": 620,
      "side": "RIGHT",
      "diff_hunk": "@@ -614,10 +614,10 @@ def test_url_copywith_userinfo_subcomponents():\n     }\n     url = httpx.URL(\"https://example.org\")\n     new = url.copy_with(**copy_with_kwargs)\n-    assert str(new) == \"https://tom%40example.org:abc123%40%20%25@example.org\"\n+    assert str(new) == \"https://tom%40example.org:abc123%40%20%@example.org\"\n     assert new.username == \"tom@example.org\"\n     assert new.password == \"abc123@ %\"\n-    assert new.userinfo == b\"tom%40example.org:abc123%40%20%25\"\n+    assert new.userinfo == b\"tom%40example.org:abc123%40%20%\"",
      "comment": "This looks weird. I could be wrong. But should the userinfo field contain partial escapes? The trialing % sign change here seem odd. \r\n\r\n",
      "comment_id": 1895670033,
      "user": "elupus",
      "created_at": "2024-12-23T11:57:19Z",
      "url": "https://github.com/encode/httpx/pull/3371#discussion_r1895670033"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2911,
      "file_path": "httpx/_models.py",
      "line": 766,
      "side": "LEFT",
      "diff_hunk": "@@ -759,11 +758,7 @@ def raise_for_status(self) -> \"Response\":\n         raise HTTPStatusError(message, request=request, response=self)\n \n     def json(self, **kwargs: typing.Any) -> typing.Any:\n-        if self.charset_encoding is None and self.content and len(self.content) > 3:\n-            encoding = guess_json_utf(self.content)\n-            if encoding is not None:\n-                return jsonlib.loads(self.content.decode(encoding), **kwargs)\n-        return jsonlib.loads(self.text, **kwargs)",
      "comment": "Just a random thing I noticed looking through the code, not a problem I've seen in the wild: isn't using `self.text` like this this better than the new version when there's a non-utf charset?",
      "comment_id": 1890475281,
      "user": "alexmojaki",
      "created_at": "2024-12-18T15:56:52Z",
      "url": "https://github.com/encode/httpx/pull/2911#discussion_r1890475281"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3367,
      "file_path": "tests/test_content.py",
      "line": 7,
      "side": "RIGHT",
      "diff_hunk": "@@ -4,6 +4,7 @@\n import pytest\n \n import httpx\n+from httpx._content import encode_json",
      "comment": "I've just noticed this accidentally snuck in a private import.\r\n\r\n@BERRADA-Omar would you be up for refactoring these tests?\r\n\r\nWe can test this against public API using `httpx.Request()`. Eg...\r\n\r\n```python\r\ndata = {...}\r\nreq = httpx.Request(\"POST\", \"https://www.example.com/\", json=data)\r\nassert req.content == ...\r\nassert req.headers = ...\r\n```",
      "comment_id": 1829240738,
      "user": "lovelydinosaur",
      "created_at": "2024-11-05T12:08:10Z",
      "url": "https://github.com/encode/httpx/pull/3367#discussion_r1829240738"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3418,
      "file_path": "httpx/_config.py",
      "line": 34,
      "side": "RIGHT",
      "diff_hunk": "@@ -31,6 +31,14 @@ def create_ssl_context(verify: ssl.SSLContext | bool = True) -> ssl.SSLContext:\n         ssl_context.check_hostname = False\n         ssl_context.verify_mode = ssl.CERT_NONE\n         return ssl_context\n+    elif isinstance(verify, str):  # pagma: nocover",
      "comment": "```suggestion\r\n    elif isinstance(verify, str):  # pragma: nocover\r\n```",
      "comment_id": 1862031777,
      "user": "lovelydinosaur",
      "created_at": "2024-11-28T11:42:48Z",
      "url": "https://github.com/encode/httpx/pull/3418#discussion_r1862031777"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3387,
      "file_path": "httpx/_models.py",
      "line": 130,
      "side": "RIGHT",
      "diff_hunk": "@@ -72,6 +82,60 @@ def _normalize_header_value(value: str | bytes, encoding: str | None = None) ->\n     return value.encode(encoding or \"ascii\")\n \n \n+def _parse_content_type_charset(content_type: str) -> str | None:\n+    # We used to use `cgi.parse_header()` here, but `cgi` became a dead battery.\n+    # See: https://peps.python.org/pep-0594/#cgi\n+    msg = email.message.Message()\n+    msg[\"content-type\"] = content_type\n+    return msg.get_content_charset(failobj=None)\n+\n+\n+def _parse_header_links(value: str) -> list[dict[str, str]]:\n+    \"\"\"\n+    Returns a list of parsed link headers, for more info see:\n+    https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Link\n+    The generic syntax of those is:\n+    Link: < uri-reference >; param1=value1; param2=\"value2\"\n+    So for instance:\n+    Link; '<http:/.../front.jpeg>; type=\"image/jpeg\",<http://.../back.jpeg>;'\n+    would return\n+        [\n+            {\"url\": \"http:/.../front.jpeg\", \"type\": \"image/jpeg\"},\n+            {\"url\": \"http://.../back.jpeg\"},\n+        ]\n+    :param value: HTTP Link entity-header field\n+    :return: list of parsed link headers\n+    \"\"\"\n+    links: list[dict[str, str]] = []\n+    replace_chars = \" '\\\"\"\n+    value = value.strip(replace_chars)\n+    if not value:\n+        return links\n+    for val in re.split(\", *<\", value):\n+        try:\n+            url, params = val.split(\";\", 1)\n+        except ValueError:\n+            url, params = val, \"\"\n+        link = {\"url\": url.strip(\"<> '\\\"\")}\n+        for param in params.split(\";\"):\n+            try:\n+                key, value = param.split(\"=\")\n+            except ValueError:\n+                break\n+            link[key.strip(replace_chars)] = value.strip(replace_chars)\n+        links.append(link)\n+    return links\n+\n+\n+def _obfuscate_sensitive_headers(",
      "comment": "Is now a good time to review this? Eg does flask provide similar behaviour onto its headers data structures? Does Django?\n\n(Possibly too off topic??)",
      "comment_id": 1826205536,
      "user": "lovelydinosaur",
      "created_at": "2024-11-01T19:15:43Z",
      "url": "https://github.com/encode/httpx/pull/3387#discussion_r1826205536"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3387,
      "file_path": "httpx/_models.py",
      "line": 130,
      "side": "RIGHT",
      "diff_hunk": "@@ -72,6 +82,60 @@ def _normalize_header_value(value: str | bytes, encoding: str | None = None) ->\n     return value.encode(encoding or \"ascii\")\n \n \n+def _parse_content_type_charset(content_type: str) -> str | None:\n+    # We used to use `cgi.parse_header()` here, but `cgi` became a dead battery.\n+    # See: https://peps.python.org/pep-0594/#cgi\n+    msg = email.message.Message()\n+    msg[\"content-type\"] = content_type\n+    return msg.get_content_charset(failobj=None)\n+\n+\n+def _parse_header_links(value: str) -> list[dict[str, str]]:\n+    \"\"\"\n+    Returns a list of parsed link headers, for more info see:\n+    https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Link\n+    The generic syntax of those is:\n+    Link: < uri-reference >; param1=value1; param2=\"value2\"\n+    So for instance:\n+    Link; '<http:/.../front.jpeg>; type=\"image/jpeg\",<http://.../back.jpeg>;'\n+    would return\n+        [\n+            {\"url\": \"http:/.../front.jpeg\", \"type\": \"image/jpeg\"},\n+            {\"url\": \"http://.../back.jpeg\"},\n+        ]\n+    :param value: HTTP Link entity-header field\n+    :return: list of parsed link headers\n+    \"\"\"\n+    links: list[dict[str, str]] = []\n+    replace_chars = \" '\\\"\"\n+    value = value.strip(replace_chars)\n+    if not value:\n+        return links\n+    for val in re.split(\", *<\", value):\n+        try:\n+            url, params = val.split(\";\", 1)\n+        except ValueError:\n+            url, params = val, \"\"\n+        link = {\"url\": url.strip(\"<> '\\\"\")}\n+        for param in params.split(\";\"):\n+            try:\n+                key, value = param.split(\"=\")\n+            except ValueError:\n+                break\n+            link[key.strip(replace_chars)] = value.strip(replace_chars)\n+        links.append(link)\n+    return links\n+\n+\n+def _obfuscate_sensitive_headers(",
      "comment": "It seems that flask does not obfuscate header values for `repr` (see [werkzeug](https://github.com/pallets/werkzeug/blob/357681fc26dd46ec5b7c3c067732873f57db0bc8/src/werkzeug/datastructures/headers.py#L20))\r\n\r\n```python\r\n>>> from werkzeug.datastructures import Headers\r\n>>>\r\n>>> head = Headers({\"authorization\": \"s3kr3t\"})\r\n>>> head\r\nHeaders([('authorization', 's3kr3t')])\r\n>>> repr(head)\r\n\"Headers([('authorization', 's3kr3t')])\"\r\n```\r\n\r\nSame for Django (see [HttpHeaders ](https://github.com/django/django/blob/611bf6c2e2a1b4ab93273980c45150c099ab146d/django/http/request.py#L468) or [ResponseHeaders](https://github.com/django/django/blob/611bf6c2e2a1b4ab93273980c45150c099ab146d/django/http/response.py#L33))\r\n\r\n```python\r\n>>> from django.http.response import ResponseHeaders\r\n>>> head = ResponseHeaders({\"Authorization\": \"s3kr3t\"})\r\n>>> repr(head)\r\n\"{'Authorization': 's3kr3t'}\"\r\n```",
      "comment_id": 1826536279,
      "user": "RafaelWO",
      "created_at": "2024-11-02T09:45:25Z",
      "url": "https://github.com/encode/httpx/pull/3387#discussion_r1826536279"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3370,
      "file_path": "httpx/_api.py",
      "line": 94,
      "side": "LEFT",
      "diff_hunk": "@@ -80,18 +79,11 @@ def request(\n     * **auth** - *(optional)* An authentication class to use when sending the\n     request.\n     * **proxy** - *(optional)* A proxy URL where all the traffic should be routed.\n-    * **proxies** - *(optional)* A dictionary mapping proxy keys to proxy URLs.\n     * **timeout** - *(optional)* The timeout configuration to use when sending\n     the request.\n     * **follow_redirects** - *(optional)* Enables or disables HTTP redirects.\n-    * **verify** - *(optional)* SSL certificates (a.k.a CA bundle) used to\n-    verify the identity of requested hosts. Either `True` (default CA bundle),\n-    a path to an SSL certificate file, an `ssl.SSLContext`, or `False`\n-    (which will disable verification).\n-    * **cert** - *(optional)* An SSL certificate used by the requested host\n-    to authenticate the client. Either a path to an SSL certificate file, or\n-    two-tuple of (certificate file, key file), or a three-tuple of (certificate\n-    file, key file, password).",
      "comment": "Perhaps it's better to keep these with `*(deprecated)*` tag?",
      "comment_id": 1819083352,
      "user": "T-256",
      "created_at": "2024-10-28T13:43:33Z",
      "url": "https://github.com/encode/httpx/pull/3370#discussion_r1819083352"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3370,
      "file_path": "httpx/_transports/default.py",
      "line": 146,
      "side": "RIGHT",
      "diff_hunk": "@@ -124,20 +125,25 @@ def close(self) -> None:\n class HTTPTransport(BaseTransport):\n     def __init__(\n         self,\n-        verify: VerifyTypes = True,\n-        cert: CertTypes | None = None,\n+        ssl_context: ssl.SSLContext | None = None,\n         http1: bool = True,\n         http2: bool = False,\n         limits: Limits = DEFAULT_LIMITS,\n-        trust_env: bool = True,\n         proxy: ProxyTypes | None = None,\n         uds: str | None = None,\n         local_address: str | None = None,\n         retries: int = 0,\n         socket_options: typing.Iterable[SOCKET_OPTION] | None = None,\n+        # Deprecated...\n+        verify: typing.Any = None,\n+        cert: typing.Any = None,\n     ) -> None:\n-        ssl_context = create_ssl_context(verify=verify, cert=cert, trust_env=trust_env)\n         proxy = Proxy(url=proxy) if isinstance(proxy, (str, URL)) else proxy\n+        if verify is not None or cert is not None:  # pragma: nocover\n+            # Deprecated...\n+            ssl_context = create_ssl_context(verify, cert)\n+        else:\n+            ssl_context = ssl_context or SSLContext()",
      "comment": "perhaps, raising error when used together?\r\n```python\r\nhttpx.get(\"https://example.com/\", verify=False, ssl_context=httpx..SSLContext())\r\n```",
      "comment_id": 1819097875,
      "user": "T-256",
      "created_at": "2024-10-28T13:52:11Z",
      "url": "https://github.com/encode/httpx/pull/3370#discussion_r1819097875"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3370,
      "file_path": "httpx/_transports/default.py",
      "line": 146,
      "side": "RIGHT",
      "diff_hunk": "@@ -124,20 +125,25 @@ def close(self) -> None:\n class HTTPTransport(BaseTransport):\n     def __init__(\n         self,\n-        verify: VerifyTypes = True,\n-        cert: CertTypes | None = None,\n+        ssl_context: ssl.SSLContext | None = None,\n         http1: bool = True,\n         http2: bool = False,\n         limits: Limits = DEFAULT_LIMITS,\n-        trust_env: bool = True,\n         proxy: ProxyTypes | None = None,\n         uds: str | None = None,\n         local_address: str | None = None,\n         retries: int = 0,\n         socket_options: typing.Iterable[SOCKET_OPTION] | None = None,\n+        # Deprecated...\n+        verify: typing.Any = None,\n+        cert: typing.Any = None,\n     ) -> None:\n-        ssl_context = create_ssl_context(verify=verify, cert=cert, trust_env=trust_env)\n         proxy = Proxy(url=proxy) if isinstance(proxy, (str, URL)) else proxy\n+        if verify is not None or cert is not None:  # pragma: nocover\n+            # Deprecated...\n+            ssl_context = create_ssl_context(verify, cert)\n+        else:\n+            ssl_context = ssl_context or SSLContext()",
      "comment": "Okay yep... I'm going to retitle our `version-1.0` branch and will resolve there.",
      "comment_id": 1819146801,
      "user": "lovelydinosaur",
      "created_at": "2024-10-28T14:13:07Z",
      "url": "https://github.com/encode/httpx/pull/3370#discussion_r1819146801"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_config.py",
      "line": 51,
      "side": "RIGHT",
      "diff_hunk": "@@ -42,150 +40,102 @@ class UnsetType:\n UNSET = UnsetType()\n \n \n-def create_ssl_context(\n-    cert: typing.Optional[CertTypes] = None,\n-    verify: VerifyTypes = True,\n-    trust_env: bool = True,\n-    http2: bool = False,\n-) -> ssl.SSLContext:\n-    return SSLConfig(\n-        cert=cert, verify=verify, trust_env=trust_env, http2=http2\n-    ).ssl_context\n-\n-\n-class SSLConfig:\n-    \"\"\"\n-    SSL Configuration.\n-    \"\"\"\n-\n+class SSLContext(ssl.SSLContext):\n     DEFAULT_CA_BUNDLE_PATH = Path(certifi.where())\n \n-    def __init__(\n-        self,\n-        *,\n-        cert: typing.Optional[CertTypes] = None,\n+    def __new__(\n+        cls,\n+        protocol: ssl._SSLMethod = ssl.PROTOCOL_TLS_CLIENT,\n         verify: VerifyTypes = True,\n-        trust_env: bool = True,\n-        http2: bool = False,\n-    ) -> None:\n-        self.cert = cert\n-        self.verify = verify\n-        self.trust_env = trust_env\n-        self.http2 = http2\n-        self.ssl_context = self.load_ssl_context()\n+        cert: typing.Optional[CertTypes] = None,\n+    ) -> \"SSLContext\":",
      "comment": "I'm curious why we're overriding `__new__` instead of `__init__` here?",
      "comment_id": 1437601093,
      "user": "lovelydinosaur",
      "created_at": "2023-12-28T12:16:24Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1437601093"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_config.py",
      "line": 95,
      "side": "RIGHT",
      "diff_hunk": "@@ -42,150 +40,102 @@ class UnsetType:\n UNSET = UnsetType()\n \n \n-def create_ssl_context(\n-    cert: typing.Optional[CertTypes] = None,\n-    verify: VerifyTypes = True,\n-    trust_env: bool = True,\n-    http2: bool = False,\n-) -> ssl.SSLContext:\n-    return SSLConfig(\n-        cert=cert, verify=verify, trust_env=trust_env, http2=http2\n-    ).ssl_context\n-\n-\n-class SSLConfig:\n-    \"\"\"\n-    SSL Configuration.\n-    \"\"\"\n-\n+class SSLContext(ssl.SSLContext):\n     DEFAULT_CA_BUNDLE_PATH = Path(certifi.where())\n \n-    def __init__(\n-        self,\n-        *,\n-        cert: typing.Optional[CertTypes] = None,\n+    def __new__(\n+        cls,\n+        protocol: ssl._SSLMethod = ssl.PROTOCOL_TLS_CLIENT,\n         verify: VerifyTypes = True,\n-        trust_env: bool = True,\n-        http2: bool = False,\n-    ) -> None:\n-        self.cert = cert\n-        self.verify = verify\n-        self.trust_env = trust_env\n-        self.http2 = http2\n-        self.ssl_context = self.load_ssl_context()\n+        cert: typing.Optional[CertTypes] = None,\n+    ) -> \"SSLContext\":\n+        self = super().__new__(cls, protocol)\n+\n+        set_minimum_tls_version_1_2(self)\n+        self.options |= ssl.OP_NO_COMPRESSION\n+        self.set_ciphers(DEFAULT_CIPHERS)\n \n-    def load_ssl_context(self) -> ssl.SSLContext:\n         logger.debug(\n-            \"load_ssl_context verify=%r cert=%r trust_env=%r http2=%r\",\n-            self.verify,\n-            self.cert,\n-            self.trust_env,\n-            self.http2,\n+            \"load_ssl_context verify=%r cert=%r\",\n+            verify,\n+            cert,\n         )\n \n-        if self.verify:\n-            return self.load_ssl_context_verify()\n-        return self.load_ssl_context_no_verify()\n+        if verify:\n+            self.load_ssl_context_verify(cert, verify)\n+        else:\n+            self.load_ssl_context_no_verify(cert)\n+        return self\n \n-    def load_ssl_context_no_verify(self) -> ssl.SSLContext:\n+    def load_ssl_context_no_verify(\n+        self, cert: typing.Optional[CertTypes]\n+    ) -> ssl.SSLContext:\n         \"\"\"\n         Return an SSL context for unverified connections.\n         \"\"\"\n-        context = self._create_default_ssl_context()\n-        context.check_hostname = False\n-        context.verify_mode = ssl.CERT_NONE\n-        self._load_client_certs(context)\n-        return context\n+        self.check_hostname = False\n+        self.verify_mode = ssl.CERT_NONE\n+        self._load_client_certs(cert)\n+        return self\n \n-    def load_ssl_context_verify(self) -> ssl.SSLContext:\n+    def load_ssl_context_verify(\n+        self, cert: typing.Optional[CertTypes], verify: VerifyTypes\n+    ) -> None:\n         \"\"\"\n         Return an SSL context for verified connections.\n         \"\"\"\n-        if self.trust_env and self.verify is True:\n-            ca_bundle = get_ca_bundle_from_env()\n-            if ca_bundle is not None:\n-                self.verify = ca_bundle\n-\n-        if isinstance(self.verify, ssl.SSLContext):\n-            # Allow passing in our own SSLContext object that's pre-configured.\n-            context = self.verify\n-            self._load_client_certs(context)\n-            return context\n-        elif isinstance(self.verify, bool):\n+        if isinstance(verify, bool):\n             ca_bundle_path = self.DEFAULT_CA_BUNDLE_PATH\n-        elif Path(self.verify).exists():\n-            ca_bundle_path = Path(self.verify)\n+        elif Path(verify).exists():\n+            ca_bundle_path = Path(verify)\n         else:\n             raise IOError(\n                 \"Could not find a suitable TLS CA certificate bundle, \"\n-                \"invalid path: {}\".format(self.verify)\n+                \"invalid path: {}\".format(verify)\n             )",
      "comment": "We confuse the flow here by having this section of code separated from where `ca_bundle_path` is actually used. It'd be more clear if we pushed this block down to just before `if ca_bundle_path.is_file():`. ",
      "comment_id": 1437606317,
      "user": "lovelydinosaur",
      "created_at": "2023-12-28T12:25:23Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1437606317"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_config.py",
      "line": 95,
      "side": "RIGHT",
      "diff_hunk": "@@ -42,150 +40,102 @@ class UnsetType:\n UNSET = UnsetType()\n \n \n-def create_ssl_context(\n-    cert: typing.Optional[CertTypes] = None,\n-    verify: VerifyTypes = True,\n-    trust_env: bool = True,\n-    http2: bool = False,\n-) -> ssl.SSLContext:\n-    return SSLConfig(\n-        cert=cert, verify=verify, trust_env=trust_env, http2=http2\n-    ).ssl_context\n-\n-\n-class SSLConfig:\n-    \"\"\"\n-    SSL Configuration.\n-    \"\"\"\n-\n+class SSLContext(ssl.SSLContext):\n     DEFAULT_CA_BUNDLE_PATH = Path(certifi.where())\n \n-    def __init__(\n-        self,\n-        *,\n-        cert: typing.Optional[CertTypes] = None,\n+    def __new__(\n+        cls,\n+        protocol: ssl._SSLMethod = ssl.PROTOCOL_TLS_CLIENT,\n         verify: VerifyTypes = True,\n-        trust_env: bool = True,\n-        http2: bool = False,\n-    ) -> None:\n-        self.cert = cert\n-        self.verify = verify\n-        self.trust_env = trust_env\n-        self.http2 = http2\n-        self.ssl_context = self.load_ssl_context()\n+        cert: typing.Optional[CertTypes] = None,\n+    ) -> \"SSLContext\":\n+        self = super().__new__(cls, protocol)\n+\n+        set_minimum_tls_version_1_2(self)\n+        self.options |= ssl.OP_NO_COMPRESSION\n+        self.set_ciphers(DEFAULT_CIPHERS)\n \n-    def load_ssl_context(self) -> ssl.SSLContext:\n         logger.debug(\n-            \"load_ssl_context verify=%r cert=%r trust_env=%r http2=%r\",\n-            self.verify,\n-            self.cert,\n-            self.trust_env,\n-            self.http2,\n+            \"load_ssl_context verify=%r cert=%r\",\n+            verify,\n+            cert,\n         )\n \n-        if self.verify:\n-            return self.load_ssl_context_verify()\n-        return self.load_ssl_context_no_verify()\n+        if verify:\n+            self.load_ssl_context_verify(cert, verify)\n+        else:\n+            self.load_ssl_context_no_verify(cert)\n+        return self\n \n-    def load_ssl_context_no_verify(self) -> ssl.SSLContext:\n+    def load_ssl_context_no_verify(\n+        self, cert: typing.Optional[CertTypes]\n+    ) -> ssl.SSLContext:\n         \"\"\"\n         Return an SSL context for unverified connections.\n         \"\"\"\n-        context = self._create_default_ssl_context()\n-        context.check_hostname = False\n-        context.verify_mode = ssl.CERT_NONE\n-        self._load_client_certs(context)\n-        return context\n+        self.check_hostname = False\n+        self.verify_mode = ssl.CERT_NONE\n+        self._load_client_certs(cert)\n+        return self\n \n-    def load_ssl_context_verify(self) -> ssl.SSLContext:\n+    def load_ssl_context_verify(\n+        self, cert: typing.Optional[CertTypes], verify: VerifyTypes\n+    ) -> None:\n         \"\"\"\n         Return an SSL context for verified connections.\n         \"\"\"\n-        if self.trust_env and self.verify is True:\n-            ca_bundle = get_ca_bundle_from_env()\n-            if ca_bundle is not None:\n-                self.verify = ca_bundle\n-\n-        if isinstance(self.verify, ssl.SSLContext):\n-            # Allow passing in our own SSLContext object that's pre-configured.\n-            context = self.verify\n-            self._load_client_certs(context)\n-            return context\n-        elif isinstance(self.verify, bool):\n+        if isinstance(verify, bool):\n             ca_bundle_path = self.DEFAULT_CA_BUNDLE_PATH\n-        elif Path(self.verify).exists():\n-            ca_bundle_path = Path(self.verify)\n+        elif Path(verify).exists():\n+            ca_bundle_path = Path(verify)\n         else:\n             raise IOError(\n                 \"Could not find a suitable TLS CA certificate bundle, \"\n-                \"invalid path: {}\".format(self.verify)\n+                \"invalid path: {}\".format(verify)\n             )",
      "comment": "(Tho perhaps we address clean-ups separately to the main API behavioral changes here?)",
      "comment_id": 1437607038,
      "user": "lovelydinosaur",
      "created_at": "2023-12-28T12:26:46Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1437607038"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_config.py",
      "line": 51,
      "side": "RIGHT",
      "diff_hunk": "@@ -42,150 +40,102 @@ class UnsetType:\n UNSET = UnsetType()\n \n \n-def create_ssl_context(\n-    cert: typing.Optional[CertTypes] = None,\n-    verify: VerifyTypes = True,\n-    trust_env: bool = True,\n-    http2: bool = False,\n-) -> ssl.SSLContext:\n-    return SSLConfig(\n-        cert=cert, verify=verify, trust_env=trust_env, http2=http2\n-    ).ssl_context\n-\n-\n-class SSLConfig:\n-    \"\"\"\n-    SSL Configuration.\n-    \"\"\"\n-\n+class SSLContext(ssl.SSLContext):\n     DEFAULT_CA_BUNDLE_PATH = Path(certifi.where())\n \n-    def __init__(\n-        self,\n-        *,\n-        cert: typing.Optional[CertTypes] = None,\n+    def __new__(\n+        cls,\n+        protocol: ssl._SSLMethod = ssl.PROTOCOL_TLS_CLIENT,\n         verify: VerifyTypes = True,\n-        trust_env: bool = True,\n-        http2: bool = False,\n-    ) -> None:\n-        self.cert = cert\n-        self.verify = verify\n-        self.trust_env = trust_env\n-        self.http2 = http2\n-        self.ssl_context = self.load_ssl_context()\n+        cert: typing.Optional[CertTypes] = None,\n+    ) -> \"SSLContext\":",
      "comment": "Yes, it should definitely be in __init__. I believe I encountered some issues and have temporarily moved it to __new__ to see how it looks in general.",
      "comment_id": 1437608213,
      "user": "karpetrosyan",
      "created_at": "2023-12-28T12:28:59Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1437608213"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_config.py",
      "line": 95,
      "side": "RIGHT",
      "diff_hunk": "@@ -42,150 +40,102 @@ class UnsetType:\n UNSET = UnsetType()\n \n \n-def create_ssl_context(\n-    cert: typing.Optional[CertTypes] = None,\n-    verify: VerifyTypes = True,\n-    trust_env: bool = True,\n-    http2: bool = False,\n-) -> ssl.SSLContext:\n-    return SSLConfig(\n-        cert=cert, verify=verify, trust_env=trust_env, http2=http2\n-    ).ssl_context\n-\n-\n-class SSLConfig:\n-    \"\"\"\n-    SSL Configuration.\n-    \"\"\"\n-\n+class SSLContext(ssl.SSLContext):\n     DEFAULT_CA_BUNDLE_PATH = Path(certifi.where())\n \n-    def __init__(\n-        self,\n-        *,\n-        cert: typing.Optional[CertTypes] = None,\n+    def __new__(\n+        cls,\n+        protocol: ssl._SSLMethod = ssl.PROTOCOL_TLS_CLIENT,\n         verify: VerifyTypes = True,\n-        trust_env: bool = True,\n-        http2: bool = False,\n-    ) -> None:\n-        self.cert = cert\n-        self.verify = verify\n-        self.trust_env = trust_env\n-        self.http2 = http2\n-        self.ssl_context = self.load_ssl_context()\n+        cert: typing.Optional[CertTypes] = None,\n+    ) -> \"SSLContext\":\n+        self = super().__new__(cls, protocol)\n+\n+        set_minimum_tls_version_1_2(self)\n+        self.options |= ssl.OP_NO_COMPRESSION\n+        self.set_ciphers(DEFAULT_CIPHERS)\n \n-    def load_ssl_context(self) -> ssl.SSLContext:\n         logger.debug(\n-            \"load_ssl_context verify=%r cert=%r trust_env=%r http2=%r\",\n-            self.verify,\n-            self.cert,\n-            self.trust_env,\n-            self.http2,\n+            \"load_ssl_context verify=%r cert=%r\",\n+            verify,\n+            cert,\n         )\n \n-        if self.verify:\n-            return self.load_ssl_context_verify()\n-        return self.load_ssl_context_no_verify()\n+        if verify:\n+            self.load_ssl_context_verify(cert, verify)\n+        else:\n+            self.load_ssl_context_no_verify(cert)\n+        return self\n \n-    def load_ssl_context_no_verify(self) -> ssl.SSLContext:\n+    def load_ssl_context_no_verify(\n+        self, cert: typing.Optional[CertTypes]\n+    ) -> ssl.SSLContext:\n         \"\"\"\n         Return an SSL context for unverified connections.\n         \"\"\"\n-        context = self._create_default_ssl_context()\n-        context.check_hostname = False\n-        context.verify_mode = ssl.CERT_NONE\n-        self._load_client_certs(context)\n-        return context\n+        self.check_hostname = False\n+        self.verify_mode = ssl.CERT_NONE\n+        self._load_client_certs(cert)\n+        return self\n \n-    def load_ssl_context_verify(self) -> ssl.SSLContext:\n+    def load_ssl_context_verify(\n+        self, cert: typing.Optional[CertTypes], verify: VerifyTypes\n+    ) -> None:\n         \"\"\"\n         Return an SSL context for verified connections.\n         \"\"\"\n-        if self.trust_env and self.verify is True:\n-            ca_bundle = get_ca_bundle_from_env()\n-            if ca_bundle is not None:\n-                self.verify = ca_bundle\n-\n-        if isinstance(self.verify, ssl.SSLContext):\n-            # Allow passing in our own SSLContext object that's pre-configured.\n-            context = self.verify\n-            self._load_client_certs(context)\n-            return context\n-        elif isinstance(self.verify, bool):\n+        if isinstance(verify, bool):\n             ca_bundle_path = self.DEFAULT_CA_BUNDLE_PATH\n-        elif Path(self.verify).exists():\n-            ca_bundle_path = Path(self.verify)\n+        elif Path(verify).exists():\n+            ca_bundle_path = Path(verify)\n         else:\n             raise IOError(\n                 \"Could not find a suitable TLS CA certificate bundle, \"\n-                \"invalid path: {}\".format(self.verify)\n+                \"invalid path: {}\".format(verify)\n             )",
      "comment": "Separating them will make it easier to review this PR, which contains an important API change.\r\n",
      "comment_id": 1437611108,
      "user": "karpetrosyan",
      "created_at": "2023-12-28T12:34:38Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1437611108"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_config.py",
      "line": 95,
      "side": "RIGHT",
      "diff_hunk": "@@ -42,150 +40,102 @@ class UnsetType:\n UNSET = UnsetType()\n \n \n-def create_ssl_context(\n-    cert: typing.Optional[CertTypes] = None,\n-    verify: VerifyTypes = True,\n-    trust_env: bool = True,\n-    http2: bool = False,\n-) -> ssl.SSLContext:\n-    return SSLConfig(\n-        cert=cert, verify=verify, trust_env=trust_env, http2=http2\n-    ).ssl_context\n-\n-\n-class SSLConfig:\n-    \"\"\"\n-    SSL Configuration.\n-    \"\"\"\n-\n+class SSLContext(ssl.SSLContext):\n     DEFAULT_CA_BUNDLE_PATH = Path(certifi.where())\n \n-    def __init__(\n-        self,\n-        *,\n-        cert: typing.Optional[CertTypes] = None,\n+    def __new__(\n+        cls,\n+        protocol: ssl._SSLMethod = ssl.PROTOCOL_TLS_CLIENT,\n         verify: VerifyTypes = True,\n-        trust_env: bool = True,\n-        http2: bool = False,\n-    ) -> None:\n-        self.cert = cert\n-        self.verify = verify\n-        self.trust_env = trust_env\n-        self.http2 = http2\n-        self.ssl_context = self.load_ssl_context()\n+        cert: typing.Optional[CertTypes] = None,\n+    ) -> \"SSLContext\":\n+        self = super().__new__(cls, protocol)\n+\n+        set_minimum_tls_version_1_2(self)\n+        self.options |= ssl.OP_NO_COMPRESSION\n+        self.set_ciphers(DEFAULT_CIPHERS)\n \n-    def load_ssl_context(self) -> ssl.SSLContext:\n         logger.debug(\n-            \"load_ssl_context verify=%r cert=%r trust_env=%r http2=%r\",\n-            self.verify,\n-            self.cert,\n-            self.trust_env,\n-            self.http2,\n+            \"load_ssl_context verify=%r cert=%r\",\n+            verify,\n+            cert,\n         )\n \n-        if self.verify:\n-            return self.load_ssl_context_verify()\n-        return self.load_ssl_context_no_verify()\n+        if verify:\n+            self.load_ssl_context_verify(cert, verify)\n+        else:\n+            self.load_ssl_context_no_verify(cert)\n+        return self\n \n-    def load_ssl_context_no_verify(self) -> ssl.SSLContext:\n+    def load_ssl_context_no_verify(\n+        self, cert: typing.Optional[CertTypes]\n+    ) -> ssl.SSLContext:\n         \"\"\"\n         Return an SSL context for unverified connections.\n         \"\"\"\n-        context = self._create_default_ssl_context()\n-        context.check_hostname = False\n-        context.verify_mode = ssl.CERT_NONE\n-        self._load_client_certs(context)\n-        return context\n+        self.check_hostname = False\n+        self.verify_mode = ssl.CERT_NONE\n+        self._load_client_certs(cert)\n+        return self\n \n-    def load_ssl_context_verify(self) -> ssl.SSLContext:\n+    def load_ssl_context_verify(\n+        self, cert: typing.Optional[CertTypes], verify: VerifyTypes\n+    ) -> None:\n         \"\"\"\n         Return an SSL context for verified connections.\n         \"\"\"\n-        if self.trust_env and self.verify is True:\n-            ca_bundle = get_ca_bundle_from_env()\n-            if ca_bundle is not None:\n-                self.verify = ca_bundle\n-\n-        if isinstance(self.verify, ssl.SSLContext):\n-            # Allow passing in our own SSLContext object that's pre-configured.\n-            context = self.verify\n-            self._load_client_certs(context)\n-            return context\n-        elif isinstance(self.verify, bool):\n+        if isinstance(verify, bool):\n             ca_bundle_path = self.DEFAULT_CA_BUNDLE_PATH\n-        elif Path(self.verify).exists():\n-            ca_bundle_path = Path(self.verify)\n+        elif Path(verify).exists():\n+            ca_bundle_path = Path(verify)\n         else:\n             raise IOError(\n                 \"Could not find a suitable TLS CA certificate bundle, \"\n-                \"invalid path: {}\".format(self.verify)\n+                \"invalid path: {}\".format(verify)\n             )",
      "comment": "Another important question is whether we need to deprecate previous arguments or simply delete them.",
      "comment_id": 1437621343,
      "user": "karpetrosyan",
      "created_at": "2023-12-28T12:54:22Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1437621343"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_config.py",
      "line": 95,
      "side": "RIGHT",
      "diff_hunk": "@@ -42,150 +40,102 @@ class UnsetType:\n UNSET = UnsetType()\n \n \n-def create_ssl_context(\n-    cert: typing.Optional[CertTypes] = None,\n-    verify: VerifyTypes = True,\n-    trust_env: bool = True,\n-    http2: bool = False,\n-) -> ssl.SSLContext:\n-    return SSLConfig(\n-        cert=cert, verify=verify, trust_env=trust_env, http2=http2\n-    ).ssl_context\n-\n-\n-class SSLConfig:\n-    \"\"\"\n-    SSL Configuration.\n-    \"\"\"\n-\n+class SSLContext(ssl.SSLContext):\n     DEFAULT_CA_BUNDLE_PATH = Path(certifi.where())\n \n-    def __init__(\n-        self,\n-        *,\n-        cert: typing.Optional[CertTypes] = None,\n+    def __new__(\n+        cls,\n+        protocol: ssl._SSLMethod = ssl.PROTOCOL_TLS_CLIENT,\n         verify: VerifyTypes = True,\n-        trust_env: bool = True,\n-        http2: bool = False,\n-    ) -> None:\n-        self.cert = cert\n-        self.verify = verify\n-        self.trust_env = trust_env\n-        self.http2 = http2\n-        self.ssl_context = self.load_ssl_context()\n+        cert: typing.Optional[CertTypes] = None,\n+    ) -> \"SSLContext\":\n+        self = super().__new__(cls, protocol)\n+\n+        set_minimum_tls_version_1_2(self)\n+        self.options |= ssl.OP_NO_COMPRESSION\n+        self.set_ciphers(DEFAULT_CIPHERS)\n \n-    def load_ssl_context(self) -> ssl.SSLContext:\n         logger.debug(\n-            \"load_ssl_context verify=%r cert=%r trust_env=%r http2=%r\",\n-            self.verify,\n-            self.cert,\n-            self.trust_env,\n-            self.http2,\n+            \"load_ssl_context verify=%r cert=%r\",\n+            verify,\n+            cert,\n         )\n \n-        if self.verify:\n-            return self.load_ssl_context_verify()\n-        return self.load_ssl_context_no_verify()\n+        if verify:\n+            self.load_ssl_context_verify(cert, verify)\n+        else:\n+            self.load_ssl_context_no_verify(cert)\n+        return self\n \n-    def load_ssl_context_no_verify(self) -> ssl.SSLContext:\n+    def load_ssl_context_no_verify(\n+        self, cert: typing.Optional[CertTypes]\n+    ) -> ssl.SSLContext:\n         \"\"\"\n         Return an SSL context for unverified connections.\n         \"\"\"\n-        context = self._create_default_ssl_context()\n-        context.check_hostname = False\n-        context.verify_mode = ssl.CERT_NONE\n-        self._load_client_certs(context)\n-        return context\n+        self.check_hostname = False\n+        self.verify_mode = ssl.CERT_NONE\n+        self._load_client_certs(cert)\n+        return self\n \n-    def load_ssl_context_verify(self) -> ssl.SSLContext:\n+    def load_ssl_context_verify(\n+        self, cert: typing.Optional[CertTypes], verify: VerifyTypes\n+    ) -> None:\n         \"\"\"\n         Return an SSL context for verified connections.\n         \"\"\"\n-        if self.trust_env and self.verify is True:\n-            ca_bundle = get_ca_bundle_from_env()\n-            if ca_bundle is not None:\n-                self.verify = ca_bundle\n-\n-        if isinstance(self.verify, ssl.SSLContext):\n-            # Allow passing in our own SSLContext object that's pre-configured.\n-            context = self.verify\n-            self._load_client_certs(context)\n-            return context\n-        elif isinstance(self.verify, bool):\n+        if isinstance(verify, bool):\n             ca_bundle_path = self.DEFAULT_CA_BUNDLE_PATH\n-        elif Path(self.verify).exists():\n-            ca_bundle_path = Path(self.verify)\n+        elif Path(verify).exists():\n+            ca_bundle_path = Path(verify)\n         else:\n             raise IOError(\n                 \"Could not find a suitable TLS CA certificate bundle, \"\n-                \"invalid path: {}\".format(self.verify)\n+                \"invalid path: {}\".format(verify)\n             )",
      "comment": "> (Tho perhaps we address clean-ups separately to the main API behavioral changes here?)\r\n\r\nAgree with tom, you could add `ssl_context=` and `httpx.SSLContext` in current PR and decide to deprecate/remove in separated PR.",
      "comment_id": 1437656569,
      "user": "T-256",
      "created_at": "2023-12-28T13:10:24Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1437656569"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_config.py",
      "line": 143,
      "side": "RIGHT",
      "diff_hunk": "@@ -138,6 +134,14 @@ def _load_client_certs(self, cert: typing.Optional[CertTypes] = None) -> None:\n                     password=cert[2],  # type: ignore\n                 )\n \n+    def __new__(\n+        cls,\n+        protocol: ssl._SSLMethod = ssl.PROTOCOL_TLS_CLIENT,\n+        *args: typing.Any,\n+        **kwargs: typing.Any,\n+    ) -> \"SSLContext\":\n+        return super().__new__(cls, protocol, *args, **kwargs)",
      "comment": "We need this to avoid \"DeprecationWarning: ssl.SSLContext() without protocol argument is deprecated.\"",
      "comment_id": 1438067431,
      "user": "karpetrosyan",
      "created_at": "2023-12-29T07:24:28Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1438067431"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_config.py",
      "line": 69,
      "side": "RIGHT",
      "diff_hunk": "@@ -46,151 +44,113 @@ class UnsetType:\n UNSET = UnsetType()\n \n \n-def create_ssl_context(\n-    cert: CertTypes | None = None,\n-    verify: VerifyTypes = True,\n-    trust_env: bool = True,\n-    http2: bool = False,\n-) -> ssl.SSLContext:\n-    return SSLConfig(\n-        cert=cert, verify=verify, trust_env=trust_env, http2=http2\n-    ).ssl_context\n-\n-\n-class SSLConfig:\n-    \"\"\"\n-    SSL Configuration.\n-    \"\"\"\n-\n+class SSLContext(ssl.SSLContext):\n     DEFAULT_CA_BUNDLE_PATH = Path(certifi.where())\n \n     def __init__(\n         self,\n-        *,\n-        cert: CertTypes | None = None,\n         verify: VerifyTypes = True,\n-        trust_env: bool = True,\n-        http2: bool = False,\n+        cert: CertTypes | None = None,\n     ) -> None:\n-        self.cert = cert\n         self.verify = verify\n-        self.trust_env = trust_env\n-        self.http2 = http2\n-        self.ssl_context = self.load_ssl_context()\n+        set_minimum_tls_version_1_2(self)\n+        self.options |= ssl.OP_NO_COMPRESSION\n+        self.set_ciphers(DEFAULT_CIPHERS)\n \n-    def load_ssl_context(self) -> ssl.SSLContext:\n         logger.debug(\n-            \"load_ssl_context verify=%r cert=%r trust_env=%r http2=%r\",\n-            self.verify,\n-            self.cert,\n-            self.trust_env,\n-            self.http2,\n+            \"load_ssl_context verify=%r cert=%r\",\n+            verify,\n+            cert,\n         )\n \n-        if self.verify:\n-            return self.load_ssl_context_verify()\n-        return self.load_ssl_context_no_verify()\n+        if verify:\n+            self.load_ssl_context_verify(cert, verify)\n+        else:\n+            self.load_ssl_context_no_verify(cert)",
      "comment": "since `verify` attribute is set to instance, I think we could mix these in one method:\r\n```py\r\ndef load_ssl_context(self, cert: typing.Optional[CertTypes]) -> None:\r\n    if not self.verify:\r\n        self.check_hostname = False\r\n        self.verify_mode = ssl.CERT_NONE\r\n        self._load_client_certs(cert)\r\n        return\r\n    if isinstance(self.verify, bool):\r\n        ca_bundle_path = self.DEFAULT_CA_BUNDLE_PATH\r\n    ...\r\n    ...\r\n\r\n```",
      "comment_id": 1771142229,
      "user": "T-256",
      "created_at": "2024-09-23T10:24:55Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1771142229"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_config.py",
      "line": 47,
      "side": "RIGHT",
      "diff_hunk": "@@ -46,151 +44,113 @@ class UnsetType:\n UNSET = UnsetType()\n \n \n-def create_ssl_context(\n-    cert: CertTypes | None = None,\n-    verify: VerifyTypes = True,\n-    trust_env: bool = True,\n-    http2: bool = False,\n-) -> ssl.SSLContext:\n-    return SSLConfig(\n-        cert=cert, verify=verify, trust_env=trust_env, http2=http2\n-    ).ssl_context\n-\n-\n-class SSLConfig:\n-    \"\"\"\n-    SSL Configuration.\n-    \"\"\"\n-\n+class SSLContext(ssl.SSLContext):",
      "comment": "If possible I'd like this not to subclass ssl.SSLContext, so we can use pyopenssl with http3",
      "comment_id": 1771217918,
      "user": "graingert",
      "created_at": "2024-09-23T11:16:07Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1771217918"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_config.py",
      "line": 47,
      "side": "RIGHT",
      "diff_hunk": "@@ -46,151 +44,113 @@ class UnsetType:\n UNSET = UnsetType()\n \n \n-def create_ssl_context(\n-    cert: CertTypes | None = None,\n-    verify: VerifyTypes = True,\n-    trust_env: bool = True,\n-    http2: bool = False,\n-) -> ssl.SSLContext:\n-    return SSLConfig(\n-        cert=cert, verify=verify, trust_env=trust_env, http2=http2\n-    ).ssl_context\n-\n-\n-class SSLConfig:\n-    \"\"\"\n-    SSL Configuration.\n-    \"\"\"\n-\n+class SSLContext(ssl.SSLContext):",
      "comment": "Ah, so...\r\n\r\nThe *type* required by the client methods should be `ssl.SSLContext`.\r\nFor example in correctly typecheck against this documented usage...\r\n\r\nhttps://github.com/encode/httpx/blob/998f445c83efb140b7e9ae05c993e8f36663cba2/docs/advanced/ssl.md?plain=1#L148-L149\r\n\r\nI think it's okay for us to have an `httpx.SSLContext()` subclass.",
      "comment_id": 1771332015,
      "user": "lovelydinosaur",
      "created_at": "2024-09-23T12:43:02Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1771332015"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_config.py",
      "line": 47,
      "side": "RIGHT",
      "diff_hunk": "@@ -46,151 +44,113 @@ class UnsetType:\n UNSET = UnsetType()\n \n \n-def create_ssl_context(\n-    cert: CertTypes | None = None,\n-    verify: VerifyTypes = True,\n-    trust_env: bool = True,\n-    http2: bool = False,\n-) -> ssl.SSLContext:\n-    return SSLConfig(\n-        cert=cert, verify=verify, trust_env=trust_env, http2=http2\n-    ).ssl_context\n-\n-\n-class SSLConfig:\n-    \"\"\"\n-    SSL Configuration.\n-    \"\"\"\n-\n+class SSLContext(ssl.SSLContext):",
      "comment": "I'd like to change the type required by the Client methods to a custom httpcore class that can wrap either an ssl.SSLContext or a pyopenssl SSLContext ",
      "comment_id": 1771393713,
      "user": "graingert",
      "created_at": "2024-09-23T13:12:01Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1771393713"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_config.py",
      "line": 47,
      "side": "RIGHT",
      "diff_hunk": "@@ -46,151 +44,113 @@ class UnsetType:\n UNSET = UnsetType()\n \n \n-def create_ssl_context(\n-    cert: CertTypes | None = None,\n-    verify: VerifyTypes = True,\n-    trust_env: bool = True,\n-    http2: bool = False,\n-) -> ssl.SSLContext:\n-    return SSLConfig(\n-        cert=cert, verify=verify, trust_env=trust_env, http2=http2\n-    ).ssl_context\n-\n-\n-class SSLConfig:\n-    \"\"\"\n-    SSL Configuration.\n-    \"\"\"\n-\n+class SSLContext(ssl.SSLContext):",
      "comment": "@graingert Interesting...\r\n\r\nCan we progress that separately by opening a design discussion in `httpcore` for adding `pyopenssl` support?\r\nI'd like to understand that individual part better without risking this PR getting sidetracked.",
      "comment_id": 1771466178,
      "user": "lovelydinosaur",
      "created_at": "2024-09-23T13:40:01Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1771466178"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_api.py",
      "line": 50,
      "side": "RIGHT",
      "diff_hunk": "@@ -49,8 +47,7 @@ def request(\n     proxy: ProxyTypes | None = None,\n     timeout: TimeoutTypes = DEFAULT_TIMEOUT_CONFIG,\n     follow_redirects: bool = False,\n-    verify: VerifyTypes = True,\n-    cert: CertTypes | None = None,\n+    ssl_context: typing.Optional[SSLContext] = None,",
      "comment": "We should be using `typing.Optional[ssl.SSLContext]` here. (Yes it's a bit subtle)",
      "comment_id": 1777467423,
      "user": "lovelydinosaur",
      "created_at": "2024-09-26T17:11:39Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1777467423"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "tests/test_config.py",
      "line": 26,
      "side": "RIGHT",
      "diff_hunk": "@@ -9,48 +9,46 @@\n \n \n def test_load_ssl_config():\n-    context = httpx.create_ssl_context()\n+    context = httpx.SSLContext()\n     assert context.verify_mode == ssl.VerifyMode.CERT_REQUIRED\n     assert context.check_hostname is True\n \n \n def test_load_ssl_config_verify_non_existing_path():\n     with pytest.raises(IOError):\n-        httpx.create_ssl_context(verify=\"/path/to/nowhere\")\n+        httpx.SSLContext(verify=\"/path/to/nowhere\")\n \n \n-def test_load_ssl_config_verify_existing_file():\n-    context = httpx.create_ssl_context(verify=certifi.where())\n-    assert context.verify_mode == ssl.VerifyMode.CERT_REQUIRED\n-    assert context.check_hostname is True\n-\n+@pytest.mark.parametrize(\n+    \"trust_env, expected_keylog_filename\",\n+    [\n+        pytest.param(True, \"test\", id=\"With trasting env\"),\n+        pytest.param(False, None, id=\"Without trasting env\"),",
      "comment": "```suggestion\r\n        pytest.param(True, \"test\", id=\"With trusting env\"),\r\n        pytest.param(False, None, id=\"Without trusting env\"),\r\n```\r\n\r\n\ud83d\ude0e",
      "comment_id": 1787439662,
      "user": "lovelydinosaur",
      "created_at": "2024-10-04T09:29:07Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1787439662"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_api.py",
      "line": 51,
      "side": "RIGHT",
      "diff_hunk": "@@ -49,8 +48,7 @@ def request(\n     proxy: ProxyTypes | None = None,\n     timeout: TimeoutTypes = DEFAULT_TIMEOUT_CONFIG,\n     follow_redirects: bool = False,\n-    verify: VerifyTypes = True,\n-    cert: CertTypes | None = None,\n+    ssl_context: typing.Optional[ssl.SSLContext] = None,",
      "comment": "Do we need DEFAULT_SSL_CONTEXT instead of None?\r\nI can remember there are memory leak issues for high count instantiate of Client. Personally, I fixed this by using global ssl context varible used for all httpx.Client instances.\r\nref: https://github.com/encode/httpx/discussions/2785#discussioncomment-6539886",
      "comment_id": 1787817691,
      "user": "T-256",
      "created_at": "2024-10-04T14:34:58Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1787817691"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_api.py",
      "line": 51,
      "side": "RIGHT",
      "diff_hunk": "@@ -49,8 +48,7 @@ def request(\n     proxy: ProxyTypes | None = None,\n     timeout: TimeoutTypes = DEFAULT_TIMEOUT_CONFIG,\n     follow_redirects: bool = False,\n-    verify: VerifyTypes = True,\n-    cert: CertTypes | None = None,\n+    ssl_context: typing.Optional[ssl.SSLContext] = None,",
      "comment": "Let's add things incrementally, maybe it should be resolved in another PR?",
      "comment_id": 1787935057,
      "user": "karpetrosyan",
      "created_at": "2024-10-04T16:08:40Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1787935057"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_api.py",
      "line": 51,
      "side": "RIGHT",
      "diff_hunk": "@@ -49,8 +48,7 @@ def request(\n     proxy: ProxyTypes | None = None,\n     timeout: TimeoutTypes = DEFAULT_TIMEOUT_CONFIG,\n     follow_redirects: bool = False,\n-    verify: VerifyTypes = True,\n-    cert: CertTypes | None = None,\n+    ssl_context: typing.Optional[ssl.SSLContext] = None,",
      "comment": "(btw it's not blocking concern, we can do it by follow-up PR.)",
      "comment_id": 1787941707,
      "user": "T-256",
      "created_at": "2024-10-04T16:14:52Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1787941707"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_api.py",
      "line": 51,
      "side": "RIGHT",
      "diff_hunk": "@@ -49,8 +48,7 @@ def request(\n     proxy: ProxyTypes | None = None,\n     timeout: TimeoutTypes = DEFAULT_TIMEOUT_CONFIG,\n     follow_redirects: bool = False,\n-    verify: VerifyTypes = True,\n-    cert: CertTypes | None = None,\n+    ssl_context: typing.Optional[ssl.SSLContext] = None,",
      "comment": "Yes, please. I don't even remember a lot of things from this PR \ud83d\ude04\r\nOne year old PR is a bad practice... always!",
      "comment_id": 1787951248,
      "user": "karpetrosyan",
      "created_at": "2024-10-04T16:23:50Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1787951248"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_api.py",
      "line": 51,
      "side": "RIGHT",
      "diff_hunk": "@@ -49,8 +48,7 @@ def request(\n     proxy: ProxyTypes | None = None,\n     timeout: TimeoutTypes = DEFAULT_TIMEOUT_CONFIG,\n     follow_redirects: bool = False,\n-    verify: VerifyTypes = True,\n-    cert: CertTypes | None = None,\n+    ssl_context: typing.Optional[ssl.SSLContext] = None,",
      "comment": "We should use `None` here.\r\nWe should use with instantiating a singleton default when `None` is passed to the transport.\r\nTho we can deal with that as a follow-up.",
      "comment_id": 1787956942,
      "user": "lovelydinosaur",
      "created_at": "2024-10-04T16:29:05Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1787956942"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_client.py",
      "line": 691,
      "side": "RIGHT",
      "diff_hunk": "@@ -695,20 +683,18 @@ def __init__(\n \n     def _init_transport(\n         self,\n-        verify: VerifyTypes = True,\n-        cert: CertTypes | None = None,\n+        ssl_context: typing.Optional[ssl.SSLContext],\n         http1: bool = True,\n         http2: bool = False,\n         limits: Limits = DEFAULT_LIMITS,\n-        transport: BaseTransport | None = None,\n         trust_env: bool = True,\n+        transport: typing.Optional[BaseTransport] = None,",
      "comment": "why this change? lets keep it consistent with others.",
      "comment_id": 1789284375,
      "user": "T-256",
      "created_at": "2024-10-06T22:47:39Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1789284375"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_config.py",
      "line": 70,
      "side": "RIGHT",
      "diff_hunk": "@@ -46,151 +45,103 @@ class UnsetType:\n UNSET = UnsetType()\n \n \n-def create_ssl_context(\n-    cert: CertTypes | None = None,\n-    verify: VerifyTypes = True,\n-    trust_env: bool = True,\n-    http2: bool = False,\n-) -> ssl.SSLContext:\n-    return SSLConfig(\n-        cert=cert, verify=verify, trust_env=trust_env, http2=http2\n-    ).ssl_context\n-\n-\n-class SSLConfig:\n-    \"\"\"\n-    SSL Configuration.\n-    \"\"\"\n-\n+class SSLContext(ssl.SSLContext):\n     DEFAULT_CA_BUNDLE_PATH = Path(certifi.where())\n \n     def __init__(\n         self,\n-        *,\n-        cert: CertTypes | None = None,\n         verify: VerifyTypes = True,\n+        cert: CertTypes | None = None,\n         trust_env: bool = True,\n-        http2: bool = False,\n     ) -> None:\n-        self.cert = cert\n         self.verify = verify\n+        set_minimum_tls_version_1_2(self)\n+        self.options |= ssl.OP_NO_COMPRESSION\n+        self.set_ciphers(DEFAULT_CIPHERS)\n         self.trust_env = trust_env\n-        self.http2 = http2\n-        self.ssl_context = self.load_ssl_context()\n \n-    def load_ssl_context(self) -> ssl.SSLContext:\n+        keylogfile = os.environ.get(\"SSLKEYLOGFILE\")\n+        if keylogfile and self.trust_env:\n+            self.keylog_filename = keylogfile\n+\n         logger.debug(\n-            \"load_ssl_context verify=%r cert=%r trust_env=%r http2=%r\",\n-            self.verify,\n-            self.cert,\n-            self.trust_env,\n-            self.http2,\n+            \"load_ssl_context verify=%r cert=%r\",\n+            verify,\n+            cert,",
      "comment": "```suggestion\r\n            \"load_ssl_context verify=%r cert=%r trust_env=%r\",\r\n            verify,\r\n            cert,\r\n            trust_env,\r\n```",
      "comment_id": 1789288418,
      "user": "T-256",
      "created_at": "2024-10-06T23:00:08Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1789288418"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_config.py",
      "line": 57,
      "side": "RIGHT",
      "diff_hunk": "@@ -46,151 +45,103 @@ class UnsetType:\n UNSET = UnsetType()\n \n \n-def create_ssl_context(\n-    cert: CertTypes | None = None,\n-    verify: VerifyTypes = True,\n-    trust_env: bool = True,\n-    http2: bool = False,\n-) -> ssl.SSLContext:\n-    return SSLConfig(\n-        cert=cert, verify=verify, trust_env=trust_env, http2=http2\n-    ).ssl_context\n-\n-\n-class SSLConfig:\n-    \"\"\"\n-    SSL Configuration.\n-    \"\"\"\n-\n+class SSLContext(ssl.SSLContext):\n     DEFAULT_CA_BUNDLE_PATH = Path(certifi.where())\n \n     def __init__(\n         self,\n-        *,\n-        cert: CertTypes | None = None,\n         verify: VerifyTypes = True,\n+        cert: CertTypes | None = None,\n         trust_env: bool = True,\n-        http2: bool = False,\n     ) -> None:\n-        self.cert = cert\n         self.verify = verify",
      "comment": "Since httpx.SSLContext is now subclass of ssl.SSLContext, let's make these fields private.",
      "comment_id": 1789289649,
      "user": "T-256",
      "created_at": "2024-10-06T23:04:29Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1789289649"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_config.py",
      "line": 136,
      "side": "RIGHT",
      "diff_hunk": "@@ -46,151 +45,103 @@ class UnsetType:\n UNSET = UnsetType()\n \n \n-def create_ssl_context(\n-    cert: CertTypes | None = None,\n-    verify: VerifyTypes = True,\n-    trust_env: bool = True,\n-    http2: bool = False,\n-) -> ssl.SSLContext:\n-    return SSLConfig(\n-        cert=cert, verify=verify, trust_env=trust_env, http2=http2\n-    ).ssl_context\n-\n-\n-class SSLConfig:\n-    \"\"\"\n-    SSL Configuration.\n-    \"\"\"\n-\n+class SSLContext(ssl.SSLContext):\n     DEFAULT_CA_BUNDLE_PATH = Path(certifi.where())\n \n     def __init__(\n         self,\n-        *,\n-        cert: CertTypes | None = None,\n         verify: VerifyTypes = True,\n+        cert: CertTypes | None = None,\n         trust_env: bool = True,\n-        http2: bool = False,\n     ) -> None:\n-        self.cert = cert\n         self.verify = verify\n+        set_minimum_tls_version_1_2(self)\n+        self.options |= ssl.OP_NO_COMPRESSION\n+        self.set_ciphers(DEFAULT_CIPHERS)\n         self.trust_env = trust_env\n-        self.http2 = http2\n-        self.ssl_context = self.load_ssl_context()\n \n-    def load_ssl_context(self) -> ssl.SSLContext:\n+        keylogfile = os.environ.get(\"SSLKEYLOGFILE\")\n+        if keylogfile and self.trust_env:\n+            self.keylog_filename = keylogfile\n+\n         logger.debug(\n-            \"load_ssl_context verify=%r cert=%r trust_env=%r http2=%r\",\n-            self.verify,\n-            self.cert,\n-            self.trust_env,\n-            self.http2,\n+            \"load_ssl_context verify=%r cert=%r\",\n+            verify,\n+            cert,\n         )\n \n-        if self.verify:\n-            return self.load_ssl_context_verify()\n-        return self.load_ssl_context_no_verify()\n-\n-    def load_ssl_context_no_verify(self) -> ssl.SSLContext:\n-        \"\"\"\n-        Return an SSL context for unverified connections.\n-        \"\"\"\n-        context = self._create_default_ssl_context()\n-        context.check_hostname = False\n-        context.verify_mode = ssl.CERT_NONE\n-        self._load_client_certs(context)\n-        return context\n+        if not verify:\n+            self.check_hostname = False\n+            self.verify_mode = ssl.CERT_NONE\n+            self._load_client_certs(cert)\n+            return\n \n-    def load_ssl_context_verify(self) -> ssl.SSLContext:\n-        \"\"\"\n-        Return an SSL context for verified connections.\n-        \"\"\"\n-        if self.trust_env and self.verify is True:\n-            ca_bundle = get_ca_bundle_from_env()\n-            if ca_bundle is not None:\n-                self.verify = ca_bundle\n-\n-        if isinstance(self.verify, ssl.SSLContext):\n-            # Allow passing in our own SSLContext object that's pre-configured.\n-            context = self.verify\n-            self._load_client_certs(context)\n-            return context\n-        elif isinstance(self.verify, bool):\n+        if isinstance(verify, bool):\n             ca_bundle_path = self.DEFAULT_CA_BUNDLE_PATH\n-        elif Path(self.verify).exists():\n-            ca_bundle_path = Path(self.verify)\n+        elif Path(verify).exists():\n+            ca_bundle_path = Path(verify)\n         else:\n             raise IOError(\n                 \"Could not find a suitable TLS CA certificate bundle, \"\n-                \"invalid path: {}\".format(self.verify)\n+                \"invalid path: {}\".format(verify)\n             )\n \n-        context = self._create_default_ssl_context()\n-        context.verify_mode = ssl.CERT_REQUIRED\n-        context.check_hostname = True\n+        self.verify_mode = ssl.CERT_REQUIRED\n+        self.check_hostname = True\n \n         # Signal to server support for PHA in TLS 1.3. Raises an\n         # AttributeError if only read-only access is implemented.\n         try:\n-            context.post_handshake_auth = True\n+            self.post_handshake_auth = True\n         except AttributeError:  # pragma: no cover\n             pass\n \n         # Disable using 'commonName' for SSLContext.check_hostname\n         # when the 'subjectAltName' extension isn't available.\n         try:\n-            context.hostname_checks_common_name = False\n+            self.hostname_checks_common_name = False\n         except AttributeError:  # pragma: no cover\n             pass\n \n         if ca_bundle_path.is_file():\n             cafile = str(ca_bundle_path)\n             logger.debug(\"load_verify_locations cafile=%r\", cafile)\n-            context.load_verify_locations(cafile=cafile)\n+            self.load_verify_locations(cafile=cafile)\n         elif ca_bundle_path.is_dir():\n             capath = str(ca_bundle_path)\n             logger.debug(\"load_verify_locations capath=%r\", capath)\n-            context.load_verify_locations(capath=capath)\n+            self.load_verify_locations(capath=capath)\n \n-        self._load_client_certs(context)\n+        self._load_client_certs(cert)\n \n-        return context\n-\n-    def _create_default_ssl_context(self) -> ssl.SSLContext:\n+    def _load_client_certs(self, cert: typing.Optional[CertTypes] = None) -> None:\n         \"\"\"\n-        Creates the default SSLContext object that's used for both verified\n-        and unverified connections.\n+        Loads client certificates into our SSLContext object\n         \"\"\"\n-        context = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT)\n-        set_minimum_tls_version_1_2(context)\n-        context.options |= ssl.OP_NO_COMPRESSION\n-        context.set_ciphers(DEFAULT_CIPHERS)\n-\n-        if ssl.HAS_ALPN:\n-            alpn_idents = [\"http/1.1\", \"h2\"] if self.http2 else [\"http/1.1\"]\n-            context.set_alpn_protocols(alpn_idents)\n+        if cert is not None:\n+            if isinstance(cert, str):\n+                self.load_cert_chain(certfile=cert)\n+            elif isinstance(cert, tuple) and len(cert) == 2:\n+                self.load_cert_chain(certfile=cert[0], keyfile=cert[1])\n+            elif isinstance(cert, tuple) and len(cert) == 3:\n+                self.load_cert_chain(\n+                    certfile=cert[0],\n+                    keyfile=cert[1],\n+                    password=cert[2],\n+                )\n \n-        keylogfile = os.environ.get(\"SSLKEYLOGFILE\")\n-        if keylogfile and self.trust_env:\n-            context.keylog_filename = keylogfile\n+    def __repr__(self) -> str:\n+        class_name = self.__class__.__name__\n \n-        return context\n+        return f\"{class_name}(verify={self.verify!r})\"",
      "comment": "```suggestion\r\n        return f\"{class_name}(verify={self.verify!r}, trust_env={self.trust_env!r})\"\r\n```",
      "comment_id": 1789290923,
      "user": "T-256",
      "created_at": "2024-10-06T23:08:18Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1789290923"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_transports/default.py",
      "line": 140,
      "side": "RIGHT",
      "diff_hunk": "@@ -136,8 +136,8 @@ def __init__(\n         retries: int = 0,\n         socket_options: typing.Iterable[SOCKET_OPTION] | None = None,\n     ) -> None:\n-        ssl_context = create_ssl_context(verify=verify, cert=cert, trust_env=trust_env)\n         proxy = Proxy(url=proxy) if isinstance(proxy, (str, URL)) else proxy\n+        ssl_context = ssl_context or SSLContext()",
      "comment": "```suggestion\r\n        ssl_context = ssl_context or SSLContext(trust_env=trust_env)\r\n```",
      "comment_id": 1789295895,
      "user": "T-256",
      "created_at": "2024-10-06T23:23:27Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1789295895"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_transports/default.py",
      "line": 279,
      "side": "RIGHT",
      "diff_hunk": "@@ -265,20 +265,18 @@ async def aclose(self) -> None:\n class AsyncHTTPTransport(AsyncBaseTransport):\n     def __init__(\n         self,\n-        verify: VerifyTypes = True,\n-        cert: CertTypes | None = None,\n+        ssl_context: ssl.SSLContext | None = None,\n         http1: bool = True,\n         http2: bool = False,\n         limits: Limits = DEFAULT_LIMITS,\n-        trust_env: bool = True,\n         proxy: ProxyTypes | None = None,\n         uds: str | None = None,\n         local_address: str | None = None,\n         retries: int = 0,\n         socket_options: typing.Iterable[SOCKET_OPTION] | None = None,\n     ) -> None:\n-        ssl_context = create_ssl_context(verify=verify, cert=cert, trust_env=trust_env)\n         proxy = Proxy(url=proxy) if isinstance(proxy, (str, URL)) else proxy\n+        ssl_context = ssl_context or SSLContext()",
      "comment": "```suggestion\r\n        ssl_context = ssl_context or SSLContext(trust_env=trust_env)\r\n```",
      "comment_id": 1789296103,
      "user": "T-256",
      "created_at": "2024-10-06T23:24:09Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1789296103"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_client.py",
      "line": 686,
      "side": "RIGHT",
      "diff_hunk": "@@ -695,20 +683,18 @@ def __init__(\n \n     def _init_transport(\n         self,\n-        verify: VerifyTypes = True,\n-        cert: CertTypes | None = None,\n+        ssl_context: typing.Optional[ssl.SSLContext],",
      "comment": "```suggestion\r\n        ssl_context: ssl.SSLContext | None = None,\r\n```",
      "comment_id": 1790216656,
      "user": "lovelydinosaur",
      "created_at": "2024-10-07T13:21:22Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1790216656"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_client.py",
      "line": 707,
      "side": "RIGHT",
      "diff_hunk": "@@ -718,16 +704,14 @@ def _init_transport(\n     def _init_proxy_transport(\n         self,\n         proxy: Proxy,\n-        verify: VerifyTypes = True,\n-        cert: CertTypes | None = None,\n+        ssl_context: typing.Optional[ssl.SSLContext] = None,",
      "comment": "```suggestion\r\n        ssl_context: ssl.SSLContext | None = None,\r\n```",
      "comment_id": 1790218211,
      "user": "lovelydinosaur",
      "created_at": "2024-10-07T13:22:18Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1790218211"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "tests/test_config.py",
      "line": 31,
      "side": "RIGHT",
      "diff_hunk": "@@ -9,48 +9,46 @@\n \n \n def test_load_ssl_config():\n-    context = httpx.create_ssl_context()\n+    context = httpx.SSLContext()\n     assert context.verify_mode == ssl.VerifyMode.CERT_REQUIRED\n     assert context.check_hostname is True\n \n \n def test_load_ssl_config_verify_non_existing_path():\n     with pytest.raises(IOError):\n-        httpx.create_ssl_context(verify=\"/path/to/nowhere\")\n+        httpx.SSLContext(verify=\"/path/to/nowhere\")\n \n \n-def test_load_ssl_config_verify_existing_file():\n-    context = httpx.create_ssl_context(verify=certifi.where())\n-    assert context.verify_mode == ssl.VerifyMode.CERT_REQUIRED\n-    assert context.check_hostname is True\n-\n+@pytest.mark.parametrize(\n+    \"trust_env, expected_keylog_filename\",\n+    [\n+        pytest.param(True, \"test\", id=\"With trusting env\"),\n+        pytest.param(False, None, id=\"Without trusting env\"),\n+    ],\n+)\n+def test_load_ssl_with_keylog(\n+    monkeypatch: t.Any, trust_env: bool, expected_keylog_filename: t.Union[str, None]\n+) -> None:",
      "comment": "```suggestion\r\ndef test_load_ssl_with_keylog(\r\n    monkeypatch: typing.Any,\r\n    trust_env: bool,\r\n    expected_keylog_filename: typing.Union[str, None]\r\n) -> None:\r\n```",
      "comment_id": 1790224400,
      "user": "lovelydinosaur",
      "created_at": "2024-10-07T13:25:48Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1790224400"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_api.py",
      "line": 51,
      "side": "RIGHT",
      "diff_hunk": "@@ -49,8 +48,7 @@ def request(\n     proxy: ProxyTypes | None = None,\n     timeout: TimeoutTypes = DEFAULT_TIMEOUT_CONFIG,\n     follow_redirects: bool = False,\n-    verify: VerifyTypes = True,\n-    cert: CertTypes | None = None,\n+    ssl_context: typing.Optional[ssl.SSLContext] = None,",
      "comment": "```suggestion\r\n    ssl_context: ssl.SSLContext | None = None,\r\n```",
      "comment_id": 1790225935,
      "user": "lovelydinosaur",
      "created_at": "2024-10-07T13:26:40Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1790225935"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_api.py",
      "line": 135,
      "side": "RIGHT",
      "diff_hunk": "@@ -141,8 +132,7 @@ def stream(\n     proxy: ProxyTypes | None = None,\n     timeout: TimeoutTypes = DEFAULT_TIMEOUT_CONFIG,\n     follow_redirects: bool = False,\n-    verify: VerifyTypes = True,\n-    cert: CertTypes | None = None,\n+    ssl_context: typing.Optional[ssl.SSLContext] = None,",
      "comment": "```suggestion\r\n    ssl_context: ssl.SSLContext | None = None,\r\n```",
      "comment_id": 1790226408,
      "user": "lovelydinosaur",
      "created_at": "2024-10-07T13:26:57Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1790226408"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_api.py",
      "line": 179,
      "side": "RIGHT",
      "diff_hunk": "@@ -187,8 +176,7 @@ def get(\n     auth: AuthTypes | None = None,\n     proxy: ProxyTypes | None = None,\n     follow_redirects: bool = False,\n-    cert: CertTypes | None = None,\n-    verify: VerifyTypes = True,\n+    ssl_context: typing.Optional[ssl.SSLContext] = None,",
      "comment": "```suggestion\r\n    ssl_context: ssl.SSLContext | None = None,\r\n```",
      "comment_id": 1790226941,
      "user": "lovelydinosaur",
      "created_at": "2024-10-07T13:27:14Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1790226941"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_api.py",
      "line": 215,
      "side": "RIGHT",
      "diff_hunk": "@@ -225,8 +212,7 @@ def options(\n     auth: AuthTypes | None = None,\n     proxy: ProxyTypes | None = None,\n     follow_redirects: bool = False,\n-    cert: CertTypes | None = None,\n-    verify: VerifyTypes = True,\n+    ssl_context: typing.Optional[ssl.SSLContext] = None,",
      "comment": "```suggestion\r\n    ssl_context: ssl.SSLContext | None = None,\r\n```",
      "comment_id": 1790227449,
      "user": "lovelydinosaur",
      "created_at": "2024-10-07T13:27:31Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1790227449"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_api.py",
      "line": 251,
      "side": "RIGHT",
      "diff_hunk": "@@ -263,8 +248,7 @@ def head(\n     auth: AuthTypes | None = None,\n     proxy: ProxyTypes | None = None,\n     follow_redirects: bool = False,\n-    cert: CertTypes | None = None,\n-    verify: VerifyTypes = True,\n+    ssl_context: typing.Optional[ssl.SSLContext] = None,",
      "comment": "```suggestion\r\n    ssl_context: ssl.SSLContext | None = None,\r\n```",
      "comment_id": 1790227923,
      "user": "lovelydinosaur",
      "created_at": "2024-10-07T13:27:47Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1790227923"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_api.py",
      "line": 291,
      "side": "RIGHT",
      "diff_hunk": "@@ -305,8 +288,7 @@ def post(\n     auth: AuthTypes | None = None,\n     proxy: ProxyTypes | None = None,\n     follow_redirects: bool = False,\n-    cert: CertTypes | None = None,\n-    verify: VerifyTypes = True,\n+    ssl_context: typing.Optional[ssl.SSLContext] = None,",
      "comment": "```suggestion\r\n    ssl_context: ssl.SSLContext | None = None,\r\n```",
      "comment_id": 1790228710,
      "user": "lovelydinosaur",
      "created_at": "2024-10-07T13:28:10Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1790228710"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_api.py",
      "line": 332,
      "side": "RIGHT",
      "diff_hunk": "@@ -348,8 +329,7 @@ def put(\n     auth: AuthTypes | None = None,\n     proxy: ProxyTypes | None = None,\n     follow_redirects: bool = False,\n-    cert: CertTypes | None = None,\n-    verify: VerifyTypes = True,\n+    ssl_context: typing.Optional[ssl.SSLContext] = None,",
      "comment": "```suggestion\r\n    ssl_context: ssl.SSLContext | None = None,\r\n```",
      "comment_id": 1790229202,
      "user": "lovelydinosaur",
      "created_at": "2024-10-07T13:28:25Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1790229202"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_api.py",
      "line": 373,
      "side": "RIGHT",
      "diff_hunk": "@@ -391,8 +370,7 @@ def patch(\n     auth: AuthTypes | None = None,\n     proxy: ProxyTypes | None = None,\n     follow_redirects: bool = False,\n-    cert: CertTypes | None = None,\n-    verify: VerifyTypes = True,\n+    ssl_context: typing.Optional[ssl.SSLContext] = None,",
      "comment": "```suggestion\r\n    ssl_context: ssl.SSLContext | None = None,\r\n```",
      "comment_id": 1790229610,
      "user": "lovelydinosaur",
      "created_at": "2024-10-07T13:28:41Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1790229610"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_api.py",
      "line": 411,
      "side": "RIGHT",
      "diff_hunk": "@@ -430,9 +407,8 @@ def delete(\n     auth: AuthTypes | None = None,\n     proxy: ProxyTypes | None = None,\n     follow_redirects: bool = False,\n-    cert: CertTypes | None = None,\n-    verify: VerifyTypes = True,\n     timeout: TimeoutTypes = DEFAULT_TIMEOUT_CONFIG,\n+    ssl_context: typing.Optional[ssl.SSLContext] = None,",
      "comment": "```suggestion\r\n    ssl_context: ssl.SSLContext | None = None,\r\n```",
      "comment_id": 1790230064,
      "user": "lovelydinosaur",
      "created_at": "2024-10-07T13:28:57Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1790230064"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_client.py",
      "line": 691,
      "side": "RIGHT",
      "diff_hunk": "@@ -695,20 +683,18 @@ def __init__(\n \n     def _init_transport(\n         self,\n-        verify: VerifyTypes = True,\n-        cert: CertTypes | None = None,\n+        ssl_context: typing.Optional[ssl.SSLContext],\n         http1: bool = True,\n         http2: bool = False,\n         limits: Limits = DEFAULT_LIMITS,\n-        transport: BaseTransport | None = None,\n         trust_env: bool = True,\n+        transport: typing.Optional[BaseTransport] = None,",
      "comment": "```suggestion\r\n        transport: BaseTransport | None = None,\r\n        trust_env: bool = True,\r\n```",
      "comment_id": 1790231357,
      "user": "lovelydinosaur",
      "created_at": "2024-10-07T13:29:40Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1790231357"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_client.py",
      "line": 691,
      "side": "RIGHT",
      "diff_hunk": "@@ -695,20 +683,18 @@ def __init__(\n \n     def _init_transport(\n         self,\n-        verify: VerifyTypes = True,\n-        cert: CertTypes | None = None,\n+        ssl_context: typing.Optional[ssl.SSLContext],\n         http1: bool = True,\n         http2: bool = False,\n         limits: Limits = DEFAULT_LIMITS,\n-        transport: BaseTransport | None = None,\n         trust_env: bool = True,\n+        transport: typing.Optional[BaseTransport] = None,",
      "comment": "```suggestion\r\n        transport: BaseTransport | None = None,\r\n        trust_env: bool = True,\r\n```",
      "comment_id": 1790234890,
      "user": "lovelydinosaur",
      "created_at": "2024-10-07T13:31:40Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1790234890"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "tests/test_config.py",
      "line": 31,
      "side": "RIGHT",
      "diff_hunk": "@@ -9,48 +9,46 @@\n \n \n def test_load_ssl_config():\n-    context = httpx.create_ssl_context()\n+    context = httpx.SSLContext()\n     assert context.verify_mode == ssl.VerifyMode.CERT_REQUIRED\n     assert context.check_hostname is True\n \n \n def test_load_ssl_config_verify_non_existing_path():\n     with pytest.raises(IOError):\n-        httpx.create_ssl_context(verify=\"/path/to/nowhere\")\n+        httpx.SSLContext(verify=\"/path/to/nowhere\")\n \n \n-def test_load_ssl_config_verify_existing_file():\n-    context = httpx.create_ssl_context(verify=certifi.where())\n-    assert context.verify_mode == ssl.VerifyMode.CERT_REQUIRED\n-    assert context.check_hostname is True\n-\n+@pytest.mark.parametrize(\n+    \"trust_env, expected_keylog_filename\",\n+    [\n+        pytest.param(True, \"test\", id=\"With trusting env\"),\n+        pytest.param(False, None, id=\"Without trusting env\"),\n+    ],\n+)\n+def test_load_ssl_with_keylog(\n+    monkeypatch: t.Any, trust_env: bool, expected_keylog_filename: t.Union[str, None]\n+) -> None:",
      "comment": "`typing.Union[str, None]` -> `str | None`\r\n\r\nIMO, we could also use add these type annotations in separated PR which includes annotating other tests?",
      "comment_id": 1790238999,
      "user": "T-256",
      "created_at": "2024-10-07T13:34:02Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1790238999"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "tests/test_config.py",
      "line": 32,
      "side": "RIGHT",
      "diff_hunk": "@@ -9,48 +9,48 @@\n \n \n def test_load_ssl_config():\n-    context = httpx.create_ssl_context()\n+    context = httpx.SSLContext()\n     assert context.verify_mode == ssl.VerifyMode.CERT_REQUIRED\n     assert context.check_hostname is True\n \n \n def test_load_ssl_config_verify_non_existing_path():\n     with pytest.raises(IOError):\n-        httpx.create_ssl_context(verify=\"/path/to/nowhere\")\n+        httpx.SSLContext(verify=\"/path/to/nowhere\")\n \n \n-def test_load_ssl_config_verify_existing_file():\n-    context = httpx.create_ssl_context(verify=certifi.where())\n-    assert context.verify_mode == ssl.VerifyMode.CERT_REQUIRED\n-    assert context.check_hostname is True\n-\n+@pytest.mark.parametrize(\n+    \"trust_env, expected_keylog_filename\",\n+    [\n+        pytest.param(True, \"test\", id=\"With trusting env\"),\n+        pytest.param(False, None, id=\"Without trusting env\"),\n+    ],\n+)\n+def test_load_ssl_with_keylog(\n+    monkeypatch: typing.Any,\n+    trust_env: bool,\n+    expected_keylog_filename: typing.Union[str, None]",
      "comment": "```suggestion\r\n    expected_keylog_filename: typing.Union[str, None],\r\n```",
      "comment_id": 1790250479,
      "user": "lovelydinosaur",
      "created_at": "2024-10-07T13:40:33Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1790250479"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "tests/test_config.py",
      "line": 36,
      "side": "RIGHT",
      "diff_hunk": "@@ -9,48 +9,48 @@\n \n \n def test_load_ssl_config():\n-    context = httpx.create_ssl_context()\n+    context = httpx.SSLContext()\n     assert context.verify_mode == ssl.VerifyMode.CERT_REQUIRED\n     assert context.check_hostname is True\n \n \n def test_load_ssl_config_verify_non_existing_path():\n     with pytest.raises(IOError):\n-        httpx.create_ssl_context(verify=\"/path/to/nowhere\")\n+        httpx.SSLContext(verify=\"/path/to/nowhere\")\n \n \n-def test_load_ssl_config_verify_existing_file():\n-    context = httpx.create_ssl_context(verify=certifi.where())\n-    assert context.verify_mode == ssl.VerifyMode.CERT_REQUIRED\n-    assert context.check_hostname is True\n-\n+@pytest.mark.parametrize(\n+    \"trust_env, expected_keylog_filename\",\n+    [\n+        pytest.param(True, \"test\", id=\"With trusting env\"),\n+        pytest.param(False, None, id=\"Without trusting env\"),\n+    ],\n+)\n+def test_load_ssl_with_keylog(\n+    monkeypatch: typing.Any,\n+    trust_env: bool,\n+    expected_keylog_filename: typing.Union[str, None],\n+) -> None:\n+    monkeypatch.setenv(\"SSLKEYLOGFILE\", \"test\")\n+    context = httpx.SSLContext(trust_env=trust_env)\n+    assert context.keylog_filename == expected_keylog_filename",
      "comment": "```suggestion\r\ndef test_load_ssl_with_keylog(\r\n    monkeypatch: typing.Any,\r\n    expected_keylog_filename: typing.Union[str, None],\r\n) -> None:\r\n    monkeypatch.setenv(\"SSLKEYLOGFILE\", \"test\")\r\n    context = httpx.SSLContext()\r\n    assert context.keylog_filename == expected_keylog_filename\r\n```",
      "comment_id": 1791561015,
      "user": "lovelydinosaur",
      "created_at": "2024-10-08T09:45:45Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1791561015"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_client.py",
      "line": 673,
      "side": "LEFT",
      "diff_hunk": "@@ -664,25 +656,21 @@ def __init__(\n         proxy_map = self._get_proxy_map(proxy, allow_env_proxies)\n \n         self._transport = self._init_transport(\n-            verify=verify,\n-            cert=cert,\n+            ssl_context=ssl_context,\n             http1=http1,\n             http2=http2,\n             limits=limits,\n             transport=transport,\n-            trust_env=trust_env,",
      "comment": "This is in the right direction... dropping `trust_env` out of the `httpx.SSLContext()` API.",
      "comment_id": 1791568594,
      "user": "lovelydinosaur",
      "created_at": "2024-10-08T09:50:46Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1791568594"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "httpx/_config.py",
      "line": 64,
      "side": "RIGHT",
      "diff_hunk": "@@ -46,151 +45,103 @@ class UnsetType:\n UNSET = UnsetType()\n \n \n-def create_ssl_context(\n-    cert: CertTypes | None = None,\n-    verify: VerifyTypes = True,\n-    trust_env: bool = True,\n-    http2: bool = False,\n-) -> ssl.SSLContext:\n-    return SSLConfig(\n-        cert=cert, verify=verify, trust_env=trust_env, http2=http2\n-    ).ssl_context\n-\n-\n-class SSLConfig:\n-    \"\"\"\n-    SSL Configuration.\n-    \"\"\"\n-\n+class SSLContext(ssl.SSLContext):\n     DEFAULT_CA_BUNDLE_PATH = Path(certifi.where())\n \n     def __init__(\n         self,\n-        *,\n-        cert: CertTypes | None = None,\n         verify: VerifyTypes = True,\n+        cert: CertTypes | None = None,\n         trust_env: bool = True,\n-        http2: bool = False,\n     ) -> None:\n-        self.cert = cert\n         self.verify = verify\n+        set_minimum_tls_version_1_2(self)\n+        self.options |= ssl.OP_NO_COMPRESSION\n+        self.set_ciphers(DEFAULT_CIPHERS)\n         self.trust_env = trust_env\n-        self.http2 = http2\n-        self.ssl_context = self.load_ssl_context()\n \n-    def load_ssl_context(self) -> ssl.SSLContext:\n+        keylogfile = os.environ.get(\"SSLKEYLOGFILE\")\n+        if keylogfile and self.trust_env:",
      "comment": "Yes we're applying this unilaterally. (In line with `ssl.create_default_context()`, `urllib3`, and `requests`)",
      "comment_id": 1791571187,
      "user": "lovelydinosaur",
      "created_at": "2024-10-08T09:52:28Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1791571187"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3022,
      "file_path": "tests/test_config.py",
      "line": 28,
      "side": "RIGHT",
      "diff_hunk": "@@ -9,48 +9,40 @@\n \n \n def test_load_ssl_config():\n-    context = httpx.create_ssl_context()\n+    context = httpx.SSLContext()\n     assert context.verify_mode == ssl.VerifyMode.CERT_REQUIRED\n     assert context.check_hostname is True\n \n \n def test_load_ssl_config_verify_non_existing_path():\n     with pytest.raises(IOError):\n-        httpx.create_ssl_context(verify=\"/path/to/nowhere\")\n+        httpx.SSLContext(verify=\"/path/to/nowhere\")\n \n \n-def test_load_ssl_config_verify_existing_file():\n-    context = httpx.create_ssl_context(verify=certifi.where())\n-    assert context.verify_mode == ssl.VerifyMode.CERT_REQUIRED\n-    assert context.check_hostname is True\n+def test_load_ssl_with_keylog(\n+    monkeypatch: typing.Any,\n+    expected_keylog_filename: typing.Union[str, None],\n+) -> None:\n+    monkeypatch.setenv(\"SSLKEYLOGFILE\", \"test\")\n+    context = httpx.SSLContext()\n+    assert context.keylog_filename == expected_keylog_filename",
      "comment": "```suggestion\r\ndef test_load_ssl_with_keylog(monkeypatch: typing.Any) -> None:\r\n    monkeypatch.setenv(\"SSLKEYLOGFILE\", \"test\")\r\n    context = httpx.SSLContext()\r\n    assert context.keylog_filename == \"test\"\r\n```",
      "comment_id": 1791576267,
      "user": "lovelydinosaur",
      "created_at": "2024-10-08T09:55:50Z",
      "url": "https://github.com/encode/httpx/pull/3022#discussion_r1791576267"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3335,
      "file_path": "httpx/_config.py",
      "line": 49,
      "side": "LEFT",
      "diff_hunk": "@@ -46,92 +45,41 @@ class UnsetType:\n \n \n class SSLContext(ssl.SSLContext):\n-    DEFAULT_CA_BUNDLE_PATH = Path(certifi.where())",
      "comment": "Let's keep it, avoid new `certifi.where()` call per instantization.",
      "comment_id": 1791986336,
      "user": "T-256",
      "created_at": "2024-10-08T14:25:39Z",
      "url": "https://github.com/encode/httpx/pull/3335#discussion_r1791986336"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3335,
      "file_path": "httpx/_config.py",
      "line": 56,
      "side": "RIGHT",
      "diff_hunk": "@@ -46,92 +45,41 @@ class UnsetType:\n \n \n class SSLContext(ssl.SSLContext):\n-    DEFAULT_CA_BUNDLE_PATH = Path(certifi.where())\n-\n     def __init__(\n         self,\n-        verify: VerifyTypes = True,\n-        cert: CertTypes | None = None,\n+        verify: bool = True,\n     ) -> None:\n-        self.verify = verify\n-        set_minimum_tls_version_1_2(self)\n-        self.options |= ssl.OP_NO_COMPRESSION\n-        self.set_ciphers(DEFAULT_CIPHERS)\n-\n-        keylogfile = os.environ.get(\"SSLKEYLOGFILE\")\n-        if keylogfile:\n-            self.keylog_filename = keylogfile\n-\n-        logger.debug(\n-            \"load_ssl_context verify=%r cert=%r\",\n-            verify,\n-            cert,\n-        )\n+        super().__init__()\n+        self._verify = verify\n \n+        # Our SSL setup here is similar to the stdlib `ssl.create_default_context()`\n+        # implementation, except with `certifi` used for certificate verification.",
      "comment": "when `verify=False`, we actually don't load certifi. let's move this comment to down.",
      "comment_id": 1791997641,
      "user": "T-256",
      "created_at": "2024-10-08T14:31:02Z",
      "url": "https://github.com/encode/httpx/pull/3335#discussion_r1791997641"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3335,
      "file_path": "httpx/_config.py",
      "line": 77,
      "side": "RIGHT",
      "diff_hunk": "@@ -46,92 +45,41 @@ class UnsetType:\n \n \n class SSLContext(ssl.SSLContext):\n-    DEFAULT_CA_BUNDLE_PATH = Path(certifi.where())\n-\n     def __init__(\n         self,\n-        verify: VerifyTypes = True,\n-        cert: CertTypes | None = None,\n+        verify: bool = True,\n     ) -> None:\n-        self.verify = verify\n-        set_minimum_tls_version_1_2(self)\n-        self.options |= ssl.OP_NO_COMPRESSION\n-        self.set_ciphers(DEFAULT_CIPHERS)\n-\n-        keylogfile = os.environ.get(\"SSLKEYLOGFILE\")\n-        if keylogfile:\n-            self.keylog_filename = keylogfile\n-\n-        logger.debug(\n-            \"load_ssl_context verify=%r cert=%r\",\n-            verify,\n-            cert,\n-        )\n+        super().__init__()\n+        self._verify = verify\n \n+        # Our SSL setup here is similar to the stdlib `ssl.create_default_context()`\n+        # implementation, except with `certifi` used for certificate verification.\n         if not verify:\n             self.check_hostname = False\n             self.verify_mode = ssl.CERT_NONE\n-            self._load_client_certs(cert)\n             return\n \n-        if isinstance(verify, bool):\n-            ca_bundle_path = self.DEFAULT_CA_BUNDLE_PATH\n-        elif Path(verify).exists():\n-            ca_bundle_path = Path(verify)\n-        else:\n-            raise IOError(\n-                \"Could not find a suitable TLS CA certificate bundle, \"\n-                \"invalid path: {}\".format(verify)\n-            )\n-\n         self.verify_mode = ssl.CERT_REQUIRED\n         self.check_hostname = True\n \n-        # Signal to server support for PHA in TLS 1.3. Raises an\n-        # AttributeError if only read-only access is implemented.\n-        try:\n-            self.post_handshake_auth = True\n-        except AttributeError:  # pragma: no cover\n-            pass\n-\n-        # Disable using 'commonName' for SSLContext.check_hostname\n-        # when the 'subjectAltName' extension isn't available.\n-        try:\n-            self.hostname_checks_common_name = False\n-        except AttributeError:  # pragma: no cover\n-            pass\n-\n-        if ca_bundle_path.is_file():\n-            cafile = str(ca_bundle_path)\n-            logger.debug(\"load_verify_locations cafile=%r\", cafile)\n-            self.load_verify_locations(cafile=cafile)\n-        elif ca_bundle_path.is_dir():\n-            capath = str(ca_bundle_path)\n-            logger.debug(\"load_verify_locations capath=%r\", capath)\n-            self.load_verify_locations(capath=capath)\n-\n-        self._load_client_certs(cert)\n-\n-    def _load_client_certs(self, cert: typing.Optional[CertTypes] = None) -> None:\n-        \"\"\"\n-        Loads client certificates into our SSLContext object\n-        \"\"\"\n-        if cert is not None:\n-            if isinstance(cert, str):\n-                self.load_cert_chain(certfile=cert)\n-            elif isinstance(cert, tuple) and len(cert) == 2:\n-                self.load_cert_chain(certfile=cert[0], keyfile=cert[1])\n-            elif isinstance(cert, tuple) and len(cert) == 3:\n-                self.load_cert_chain(\n-                    certfile=cert[0],\n-                    keyfile=cert[1],\n-                    password=cert[2],\n-                )\n+        # Use stricter verify flags where possible.\n+        if hasattr(ssl, \"VERIFY_X509_PARTIAL_CHAIN\"):  # pragma: nocover\n+            self.verify_flags |= ssl.VERIFY_X509_PARTIAL_CHAIN\n+        if hasattr(ssl, \"VERIFY_X509_STRICT\"):  # pragma: nocover\n+            self.verify_flags |= ssl.VERIFY_X509_STRICT\n+\n+        # Default to `certifi` for certificiate verification.\n+        self.load_verify_locations(cafile=certifi.where())\n+\n+        # OpenSSL keylog file support.\n+        if hasattr(self, \"keylog_filename\"):\n+            keylogfile = os.environ.get(\"SSLKEYLOGFILE\")\n+            if keylogfile and not sys.flags.ignore_environment:",
      "comment": "Do we need to document `sys.flags.ignore_environment` behavior at here?",
      "comment_id": 1792003687,
      "user": "T-256",
      "created_at": "2024-10-08T14:34:25Z",
      "url": "https://github.com/encode/httpx/pull/3335#discussion_r1792003687"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3335,
      "file_path": "httpx/_config.py",
      "line": 60,
      "side": "LEFT",
      "diff_hunk": "@@ -46,92 +45,41 @@ class UnsetType:\n \n \n class SSLContext(ssl.SSLContext):\n-    DEFAULT_CA_BUNDLE_PATH = Path(certifi.where())\n-\n     def __init__(\n         self,\n-        verify: VerifyTypes = True,\n-        cert: CertTypes | None = None,\n+        verify: bool = True,\n     ) -> None:\n-        self.verify = verify\n-        set_minimum_tls_version_1_2(self)\n-        self.options |= ssl.OP_NO_COMPRESSION\n-        self.set_ciphers(DEFAULT_CIPHERS)\n-",
      "comment": "We've been following `urllib3`'s lead on SSL, and have [some out of date defaults here](https://github.com/urllib3/urllib3/pull/2705).",
      "comment_id": 1794964425,
      "user": "lovelydinosaur",
      "created_at": "2024-10-10T08:29:07Z",
      "url": "https://github.com/encode/httpx/pull/3335#discussion_r1794964425"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3335,
      "file_path": "httpx/_config.py",
      "line": 49,
      "side": "LEFT",
      "diff_hunk": "@@ -46,92 +45,41 @@ class UnsetType:\n \n \n class SSLContext(ssl.SSLContext):\n-    DEFAULT_CA_BUNDLE_PATH = Path(certifi.where())",
      "comment": "Calling `certifi.where()` at the point the certifi certificates are loaded seems reasonable to me.",
      "comment_id": 1795109143,
      "user": "lovelydinosaur",
      "created_at": "2024-10-10T10:05:43Z",
      "url": "https://github.com/encode/httpx/pull/3335#discussion_r1795109143"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3335,
      "file_path": "httpx/_config.py",
      "line": 102,
      "side": "LEFT",
      "diff_hunk": "@@ -46,92 +45,41 @@ class UnsetType:\n \n \n class SSLContext(ssl.SSLContext):\n-    DEFAULT_CA_BUNDLE_PATH = Path(certifi.where())\n-\n     def __init__(\n         self,\n-        verify: VerifyTypes = True,\n-        cert: CertTypes | None = None,\n+        verify: bool = True,\n     ) -> None:\n-        self.verify = verify\n-        set_minimum_tls_version_1_2(self)\n-        self.options |= ssl.OP_NO_COMPRESSION\n-        self.set_ciphers(DEFAULT_CIPHERS)\n-\n-        keylogfile = os.environ.get(\"SSLKEYLOGFILE\")\n-        if keylogfile:\n-            self.keylog_filename = keylogfile\n-\n-        logger.debug(\n-            \"load_ssl_context verify=%r cert=%r\",\n-            verify,\n-            cert,\n-        )\n+        super().__init__()\n+        self._verify = verify\n \n+        # Our SSL setup here is similar to the stdlib `ssl.create_default_context()`\n+        # implementation, except with `certifi` used for certificate verification.\n         if not verify:\n             self.check_hostname = False\n             self.verify_mode = ssl.CERT_NONE\n-            self._load_client_certs(cert)\n             return\n \n-        if isinstance(verify, bool):\n-            ca_bundle_path = self.DEFAULT_CA_BUNDLE_PATH\n-        elif Path(verify).exists():\n-            ca_bundle_path = Path(verify)\n-        else:\n-            raise IOError(\n-                \"Could not find a suitable TLS CA certificate bundle, \"\n-                \"invalid path: {}\".format(verify)\n-            )\n-\n         self.verify_mode = ssl.CERT_REQUIRED\n         self.check_hostname = True\n \n-        # Signal to server support for PHA in TLS 1.3. Raises an\n-        # AttributeError if only read-only access is implemented.\n-        try:\n-            self.post_handshake_auth = True\n-        except AttributeError:  # pragma: no cover\n-            pass\n-\n-        # Disable using 'commonName' for SSLContext.check_hostname\n-        # when the 'subjectAltName' extension isn't available.\n-        try:\n-            self.hostname_checks_common_name = False\n-        except AttributeError:  # pragma: no cover\n-            pass",
      "comment": "Ah... there's possibly useful context to talk through for both `post_handshake_auth`, and for `hostname_checks_common_name`. I'm going to suggest that we start by matching the `stdlib` behaviour, and have a design discussion review on the SSL settings prior to 1.0.\r\n\r\nUsing commonName fallback has been deprecated for some time...\r\n\r\n* https://stackoverflow.com/questions/43665243/invalid-self-signed-ssl-cert-subject-alternative-name-missing\r\n* https://github.com/encode/httpx/discussions/2978\r\n* https://github.com/encode/httpx/discussions/3125\r\n\r\nThis is not enforced in `ssl.create_default_context()`, and perhaps we don't need to either if the user is working with self-signed certs may be their responsibility.\r\n\r\nWrt. PHA, this was introduced in `urllib3` based on this ticket...\r\n\r\nhttps://github.com/urllib3/urllib3/issues/1634\r\n\r\nIt's not obvious to me that \"client cert authentication based on HTTP request parameters like method or path\" is desirable.",
      "comment_id": 1795147435,
      "user": "lovelydinosaur",
      "created_at": "2024-10-10T10:21:25Z",
      "url": "https://github.com/encode/httpx/pull/3335#discussion_r1795147435"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3335,
      "file_path": "httpx/_config.py",
      "line": 77,
      "side": "RIGHT",
      "diff_hunk": "@@ -46,92 +45,41 @@ class UnsetType:\n \n \n class SSLContext(ssl.SSLContext):\n-    DEFAULT_CA_BUNDLE_PATH = Path(certifi.where())\n-\n     def __init__(\n         self,\n-        verify: VerifyTypes = True,\n-        cert: CertTypes | None = None,\n+        verify: bool = True,\n     ) -> None:\n-        self.verify = verify\n-        set_minimum_tls_version_1_2(self)\n-        self.options |= ssl.OP_NO_COMPRESSION\n-        self.set_ciphers(DEFAULT_CIPHERS)\n-\n-        keylogfile = os.environ.get(\"SSLKEYLOGFILE\")\n-        if keylogfile:\n-            self.keylog_filename = keylogfile\n-\n-        logger.debug(\n-            \"load_ssl_context verify=%r cert=%r\",\n-            verify,\n-            cert,\n-        )\n+        super().__init__()\n+        self._verify = verify\n \n+        # Our SSL setup here is similar to the stdlib `ssl.create_default_context()`\n+        # implementation, except with `certifi` used for certificate verification.\n         if not verify:\n             self.check_hostname = False\n             self.verify_mode = ssl.CERT_NONE\n-            self._load_client_certs(cert)\n             return\n \n-        if isinstance(verify, bool):\n-            ca_bundle_path = self.DEFAULT_CA_BUNDLE_PATH\n-        elif Path(verify).exists():\n-            ca_bundle_path = Path(verify)\n-        else:\n-            raise IOError(\n-                \"Could not find a suitable TLS CA certificate bundle, \"\n-                \"invalid path: {}\".format(verify)\n-            )\n-\n         self.verify_mode = ssl.CERT_REQUIRED\n         self.check_hostname = True\n \n-        # Signal to server support for PHA in TLS 1.3. Raises an\n-        # AttributeError if only read-only access is implemented.\n-        try:\n-            self.post_handshake_auth = True\n-        except AttributeError:  # pragma: no cover\n-            pass\n-\n-        # Disable using 'commonName' for SSLContext.check_hostname\n-        # when the 'subjectAltName' extension isn't available.\n-        try:\n-            self.hostname_checks_common_name = False\n-        except AttributeError:  # pragma: no cover\n-            pass\n-\n-        if ca_bundle_path.is_file():\n-            cafile = str(ca_bundle_path)\n-            logger.debug(\"load_verify_locations cafile=%r\", cafile)\n-            self.load_verify_locations(cafile=cafile)\n-        elif ca_bundle_path.is_dir():\n-            capath = str(ca_bundle_path)\n-            logger.debug(\"load_verify_locations capath=%r\", capath)\n-            self.load_verify_locations(capath=capath)\n-\n-        self._load_client_certs(cert)\n-\n-    def _load_client_certs(self, cert: typing.Optional[CertTypes] = None) -> None:\n-        \"\"\"\n-        Loads client certificates into our SSLContext object\n-        \"\"\"\n-        if cert is not None:\n-            if isinstance(cert, str):\n-                self.load_cert_chain(certfile=cert)\n-            elif isinstance(cert, tuple) and len(cert) == 2:\n-                self.load_cert_chain(certfile=cert[0], keyfile=cert[1])\n-            elif isinstance(cert, tuple) and len(cert) == 3:\n-                self.load_cert_chain(\n-                    certfile=cert[0],\n-                    keyfile=cert[1],\n-                    password=cert[2],\n-                )\n+        # Use stricter verify flags where possible.\n+        if hasattr(ssl, \"VERIFY_X509_PARTIAL_CHAIN\"):  # pragma: nocover\n+            self.verify_flags |= ssl.VERIFY_X509_PARTIAL_CHAIN\n+        if hasattr(ssl, \"VERIFY_X509_STRICT\"):  # pragma: nocover\n+            self.verify_flags |= ssl.VERIFY_X509_STRICT\n+\n+        # Default to `certifi` for certificiate verification.\n+        self.load_verify_locations(cafile=certifi.where())\n+\n+        # OpenSSL keylog file support.\n+        if hasattr(self, \"keylog_filename\"):\n+            keylogfile = os.environ.get(\"SSLKEYLOGFILE\")\n+            if keylogfile and not sys.flags.ignore_environment:",
      "comment": "Maybe. We're just following `stdlib` behavior here. Can take a final review on that once we've dealt with getting the API clean-up in.",
      "comment_id": 1795162041,
      "user": "lovelydinosaur",
      "created_at": "2024-10-10T10:30:10Z",
      "url": "https://github.com/encode/httpx/pull/3335#discussion_r1795162041"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3335,
      "file_path": "httpx/_config.py",
      "line": 29,
      "side": "RIGHT",
      "diff_hunk": "@@ -46,92 +22,41 @@ class UnsetType:\n \n \n class SSLContext(ssl.SSLContext):\n-    DEFAULT_CA_BUNDLE_PATH = Path(certifi.where())\n-\n     def __init__(\n         self,\n-        verify: VerifyTypes = True,\n-        cert: CertTypes | None = None,\n+        verify: bool = True,\n     ) -> None:\n-        self.verify = verify\n-        set_minimum_tls_version_1_2(self)\n-        self.options |= ssl.OP_NO_COMPRESSION\n-        self.set_ciphers(DEFAULT_CIPHERS)\n-\n-        keylogfile = os.environ.get(\"SSLKEYLOGFILE\")\n-        if keylogfile:\n-            self.keylog_filename = keylogfile\n-\n-        logger.debug(\n-            \"load_ssl_context verify=%r cert=%r\",\n-            verify,\n-            cert,\n-        )\n+        super().__init__()",
      "comment": "```suggestion\r\n        # ssl.SSLContext sets OP_NO_SSLv2, OP_NO_SSLv3, OP_NO_COMPRESSION,\r\n        # OP_CIPHER_SERVER_PREFERENCE, OP_SINGLE_DH_USE and OP_SINGLE_ECDH_USE\r\n        # by default. (from `ssl.create_default_context`)\r\n        super().__init__()\r\n```",
      "comment_id": 1795338325,
      "user": "T-256",
      "created_at": "2024-10-10T12:34:23Z",
      "url": "https://github.com/encode/httpx/pull/3335#discussion_r1795338325"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3050,
      "file_path": "httpx/_client.py",
      "line": 683,
      "side": "RIGHT",
      "diff_hunk": "@@ -682,7 +679,13 @@ def __init__(\n             if proxy:\n                 raise RuntimeError(\"Use either `proxy` or 'proxies', not both.\")\n \n-        allow_env_proxies = trust_env and app is None and transport is None\n+        if app:\n+            raise RuntimeError(",
      "comment": "I don't know what's deprecation policy but are you sure raise error here? isn't it breaking change for those using `app`?\r\n\r\nI'd recommend keep docs, type-hints and use warning instead. then we can schedule the removal version and mention it in warning message (?)",
      "comment_id": 1451676418,
      "user": "T-256",
      "created_at": "2024-01-14T07:53:50Z",
      "url": "https://github.com/encode/httpx/pull/3050#discussion_r1451676418"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3050,
      "file_path": "httpx/_client.py",
      "line": 683,
      "side": "RIGHT",
      "diff_hunk": "@@ -682,7 +679,13 @@ def __init__(\n             if proxy:\n                 raise RuntimeError(\"Use either `proxy` or 'proxies', not both.\")\n \n-        allow_env_proxies = trust_env and app is None and transport is None\n+        if app:\n+            raise RuntimeError(",
      "comment": "> I don't know what's deprecation policy but are you sure raise error here? isn't it breaking change for those using `app`?\r\n\r\nIt looks good to me if this breaking change is for 1.0. But do we want to keep these additional errors in 1.0?",
      "comment_id": 1451678340,
      "user": "T-256",
      "created_at": "2024-01-14T08:06:37Z",
      "url": "https://github.com/encode/httpx/pull/3050#discussion_r1451678340"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3050,
      "file_path": "httpx/_client.py",
      "line": 683,
      "side": "RIGHT",
      "diff_hunk": "@@ -682,7 +679,13 @@ def __init__(\n             if proxy:\n                 raise RuntimeError(\"Use either `proxy` or 'proxies', not both.\")\n \n-        allow_env_proxies = trust_env and app is None and transport is None\n+        if app:\n+            raise RuntimeError(",
      "comment": "> It looks good to me if this breaking change is for 1.0. But do we want to keep these additional errors in 1.0?\r\n\r\nSo...  we'll need to have consistency around this.\r\n\r\nOptions might be...\r\n\r\n* Include old argument styles for 1.0 with explicit errors, to aid migrations. Clean up later and drop the errors in a 1.1 release.\r\n* Don't include old argument styles.",
      "comment_id": 1452183188,
      "user": "lovelydinosaur",
      "created_at": "2024-01-15T10:20:51Z",
      "url": "https://github.com/encode/httpx/pull/3050#discussion_r1452183188"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3050,
      "file_path": "httpx/_client.py",
      "line": 683,
      "side": "RIGHT",
      "diff_hunk": "@@ -682,7 +679,13 @@ def __init__(\n             if proxy:\n                 raise RuntimeError(\"Use either `proxy` or 'proxies', not both.\")\n \n-        allow_env_proxies = trust_env and app is None and transport is None\n+        if app:\n+            raise RuntimeError(",
      "comment": "At there, I'm more fan of what done for `proxies`:\r\nhttps://github.com/encode/httpx/blob/419d3a9d80d0c4072f6cb58eeb306148ae89e2e9/httpx/_client.py#L676-L681\r\n\r\nSo, what I can suggest at here is:\r\n1. deprecate and mention scheduled removal if any (e.g. \"The 'proxies' argument is now deprecated and it will remove in v1.0.0\")\r\n2. minor/patch release\r\n3. clear all deprecations in major release",
      "comment_id": 1452230760,
      "user": "T-256",
      "created_at": "2024-01-15T10:58:47Z",
      "url": "https://github.com/encode/httpx/pull/3050#discussion_r1452230760"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3050,
      "file_path": "httpx/_client.py",
      "line": 683,
      "side": "RIGHT",
      "diff_hunk": "@@ -682,7 +679,13 @@ def __init__(\n             if proxy:\n                 raise RuntimeError(\"Use either `proxy` or 'proxies', not both.\")\n \n-        allow_env_proxies = trust_env and app is None and transport is None\n+        if app:\n+            raise RuntimeError(",
      "comment": "I think now is good time to discuss about deprecation policy, since there are few `1.0 proposal` PRs those trying to directly remove APIs without any deprecation.",
      "comment_id": 1452235919,
      "user": "T-256",
      "created_at": "2024-01-15T11:03:24Z",
      "url": "https://github.com/encode/httpx/pull/3050#discussion_r1452235919"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3050,
      "file_path": "httpx/_client.py",
      "line": 683,
      "side": "RIGHT",
      "diff_hunk": "@@ -682,7 +679,13 @@ def __init__(\n             if proxy:\n                 raise RuntimeError(\"Use either `proxy` or 'proxies', not both.\")\n \n-        allow_env_proxies = trust_env and app is None and transport is None\n+        if app:\n+            raise RuntimeError(",
      "comment": "https://github.com/encode/httpx/pull/3069 would be a good place for us to discuss how we'd like to approach this.",
      "comment_id": 1466121446,
      "user": "lovelydinosaur",
      "created_at": "2024-01-25T09:58:45Z",
      "url": "https://github.com/encode/httpx/pull/3050#discussion_r1466121446"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3050,
      "file_path": "httpx/_client.py",
      "line": 683,
      "side": "RIGHT",
      "diff_hunk": "@@ -682,7 +679,13 @@ def __init__(\n             if proxy:\n                 raise RuntimeError(\"Use either `proxy` or 'proxies', not both.\")\n \n-        allow_env_proxies = trust_env and app is None and transport is None\n+        if app:\n+            raise RuntimeError(",
      "comment": "Via #3071 and since v1.0 is major upgrade, I'm now more interested to directly drop instead of deprecation.\r\n",
      "comment_id": 1466369932,
      "user": "T-256",
      "created_at": "2024-01-25T13:19:15Z",
      "url": "https://github.com/encode/httpx/pull/3050#discussion_r1466369932"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3312,
      "file_path": "tests/test_utils.py",
      "line": 319,
      "side": "RIGHT",
      "diff_hunk": "@@ -300,6 +301,24 @@ def test_url_matches(pattern, url, expected):\n     assert pattern.matches(httpx.URL(url)) == expected\n \n \n+@pytest.mark.parametrize(\n+    [\"value\", \"expected\"],\n+    [\n+        (b\"value\", b\"value\"),\n+        (b\"success\", b\"success\"),\n+    ],\n+)\n+def test_normalize_header_value(value, expected):\n+    assert normalize_header_value(value) == expected\n+\n+\n+def test_normalize_header_incorrect_value():\n+    with pytest.raises(\n+        TypeError, match=f\"Header value must be str or bytes, not {type(None)}\"\n+    ):\n+        normalize_header_value(None)  # type: ignore",
      "comment": "Could we have these tests in `tests/client/test_headers.py` with the testing against `httpx.Header(...)` instead of against the internal API?",
      "comment_id": 1777193345,
      "user": "lovelydinosaur",
      "created_at": "2024-09-26T14:31:17Z",
      "url": "https://github.com/encode/httpx/pull/3312#discussion_r1777193345"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3250,
      "file_path": "httpx/_urlparse.py",
      "line": 163,
      "side": "RIGHT",
      "diff_hunk": "@@ -160,7 +160,12 @@ def urlparse(url: str = \"\", **kwargs: str | None) -> ParseResult:\n     # If a URL includes any ASCII control characters including \\t, \\r, \\n,\n     # then treat it as invalid.\n     if any(char.isascii() and not char.isprintable() for char in url):\n-        raise InvalidURL(\"Invalid non-printable ASCII character in URL\")\n+        char = [char for char in url if char.isascii() and not char.isprintable()][0]",
      "comment": "Could this be done lazily without materializing the list?",
      "comment_id": 1692499301,
      "user": "adriangb",
      "created_at": "2024-07-26T04:31:08Z",
      "url": "https://github.com/encode/httpx/pull/3250#discussion_r1692499301"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3250,
      "file_path": "httpx/_urlparse.py",
      "line": 215,
      "side": "RIGHT",
      "diff_hunk": "@@ -205,9 +210,15 @@ def urlparse(url: str = \"\", **kwargs: str | None) -> ParseResult:\n             # If a component includes any ASCII control characters including \\t, \\r, \\n,\n             # then treat it as invalid.\n             if any(char.isascii() and not char.isprintable() for char in value):\n-                raise InvalidURL(\n-                    f\"Invalid non-printable ASCII character in URL component '{key}'\"\n+                char = [\n+                    char for char in value if char.isascii() and not char.isprintable()\n+                ][0]",
      "comment": "Same here. This could be a small utility function as well (it's duplicated in 2 places, I'm okay keeping it duplicated).",
      "comment_id": 1692499591,
      "user": "adriangb",
      "created_at": "2024-07-26T04:31:43Z",
      "url": "https://github.com/encode/httpx/pull/3250#discussion_r1692499591"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3250,
      "file_path": "httpx/_urlparse.py",
      "line": 163,
      "side": "RIGHT",
      "diff_hunk": "@@ -160,7 +160,12 @@ def urlparse(url: str = \"\", **kwargs: str | None) -> ParseResult:\n     # If a URL includes any ASCII control characters including \\t, \\r, \\n,\n     # then treat it as invalid.\n     if any(char.isascii() and not char.isprintable() for char in url):\n-        raise InvalidURL(\"Invalid non-printable ASCII character in URL\")\n+        char = [char for char in url if char.isascii() and not char.isprintable()][0]",
      "comment": "It can be done lazily by materializing a generator yes. \u270c\ufe0f",
      "comment_id": 1692707828,
      "user": "lovelydinosaur",
      "created_at": "2024-07-26T08:35:24Z",
      "url": "https://github.com/encode/httpx/pull/3250#discussion_r1692707828"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3139,
      "file_path": "httpx/_decoders.py",
      "line": 167,
      "side": "RIGHT",
      "diff_hunk": "@@ -140,6 +140,41 @@ def flush(self) -> bytes:\n             raise DecodingError(str(exc)) from exc\n \n \n+class ZStandardDecoder(ContentDecoder):\n+    \"\"\"\n+    Handle 'zstd' RFC 8878 decoding.\n+\n+    Requires `pip install zstandard`.\n+    Can be installed as a dependency of httpx using `pip install httpx[zstd]`.\n+    \"\"\"\n+\n+    def __init__(self) -> None:\n+        if zstd is False:  # pragma: no cover\n+            raise ImportError(\n+                \"Using 'ZStandardDecoder', ...\"\n+                \"Make sure to install httpx using `pip install httpx[zstd]`.\"\n+            ) from None\n+\n+        self.decompressor = zstd.ZstdDecompressor().decompressobj()\n+\n+    def decode(self, data: bytes) -> bytes:\n+        try:\n+            data_parts = [self.decompressor.decompress(data)]\n+            while self.decompressor.eof and self.decompressor.unused_data:\n+                unused_data = self.decompressor.unused_data\n+                self.decompressor = zstd.ZstdDecompressor().decompressobj()\n+                data_parts.append(self.decompressor.decompress(unused_data))",
      "comment": "I may be wrong about this one. But wouldn't read_across_frames=True simplify this logic (since it will allow to read multiple frames)?",
      "comment_id": 1518812853,
      "user": "Zaczero",
      "created_at": "2024-03-10T10:19:09Z",
      "url": "https://github.com/encode/httpx/pull/3139#discussion_r1518812853"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3139,
      "file_path": "httpx/_compat.py",
      "line": 35,
      "side": "RIGHT",
      "diff_hunk": "@@ -17,6 +18,23 @@\n     except ImportError:\n         brotli = None\n \n+# Zstandard support is optional\n+try:\n+    import zstandard as zstd\n+except (AttributeError, ImportError, ValueError):  # Defensive:\n+    zstd = False\n+else:\n+    # The package 'zstandard' added the 'eof' property starting\n+    # in v0.18.0 which we require to ensure a complete and\n+    # valid zstd stream was fed into the ZstdDecoder.\n+    # See: https://github.com/urllib3/urllib3/pull/2624\n+    _zstd_version = tuple(\n+        map(int, re.search(r\"^([0-9]+)\\.([0-9]+)\", zstd.__version__).groups())  # type: ignore[union-attr]\n+    )\n+    if _zstd_version < (0, 18):  # Defensive:\n+        zstd = False",
      "comment": "This is just out of my curiosity, feel free to not answer it.\r\nWhy do we need this code if we already specify zstandard>=0.18.0, wouldn't package manager guarantee it already?",
      "comment_id": 1518813076,
      "user": "Zaczero",
      "created_at": "2024-03-10T10:20:32Z",
      "url": "https://github.com/encode/httpx/pull/3139#discussion_r1518813076"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3139,
      "file_path": "httpx/_compat.py",
      "line": 35,
      "side": "RIGHT",
      "diff_hunk": "@@ -17,6 +18,23 @@\n     except ImportError:\n         brotli = None\n \n+# Zstandard support is optional\n+try:\n+    import zstandard as zstd\n+except (AttributeError, ImportError, ValueError):  # Defensive:\n+    zstd = False\n+else:\n+    # The package 'zstandard' added the 'eof' property starting\n+    # in v0.18.0 which we require to ensure a complete and\n+    # valid zstd stream was fed into the ZstdDecoder.\n+    # See: https://github.com/urllib3/urllib3/pull/2624\n+    _zstd_version = tuple(\n+        map(int, re.search(r\"^([0-9]+)\\.([0-9]+)\", zstd.__version__).groups())  # type: ignore[union-attr]\n+    )\n+    if _zstd_version < (0, 18):  # Defensive:\n+        zstd = False",
      "comment": " zstandard is an optional install. So if your `requirements.txt` just has `httpx` and not `httpx[zstd]`, and you have `zstandard==[old version]` somehow, this would prevent breakage.",
      "comment_id": 1518815892,
      "user": "mbeijen",
      "created_at": "2024-03-10T10:30:56Z",
      "url": "https://github.com/encode/httpx/pull/3139#discussion_r1518815892"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3139,
      "file_path": "httpx/_decoders.py",
      "line": 165,
      "side": "RIGHT",
      "diff_hunk": "@@ -140,6 +140,43 @@ def flush(self) -> bytes:\n             raise DecodingError(str(exc)) from exc\n \n \n+class ZStandardDecoder(ContentDecoder):\n+    \"\"\"\n+    Handle 'zstd' RFC 8878 decoding.\n+\n+    Requires `pip install zstandard`.\n+    Can be installed as a dependency of httpx using `pip install httpx[zstd]`.\n+    \"\"\"\n+\n+    def __init__(self) -> None:\n+        if zstd is None:  # pragma: no cover\n+            raise ImportError(\n+                \"Using 'ZStandardDecoder', ...\"\n+                \"Make sure to install httpx using `pip install httpx[zstd]`.\"\n+            ) from None\n+\n+        self.decompressor = zstd.ZstdDecompressor().decompressobj()\n+\n+    def decode(self, data: bytes) -> bytes:\n+        assert zstd is not None\n+        output = io.BytesIO()\n+        try:\n+            output.write(self.decompressor.decompress(data))",
      "comment": "One less function call :slightly_smiling_face:\r\n\r\n```suggestion\r\n        try:\r\n            output = io.BytesIO(self.decompressor.decompress(data))\r\n```",
      "comment_id": 1518817642,
      "user": "Zaczero",
      "created_at": "2024-03-10T10:40:54Z",
      "url": "https://github.com/encode/httpx/pull/3139#discussion_r1518817642"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3139,
      "file_path": "httpx/_decoders.py",
      "line": 167,
      "side": "RIGHT",
      "diff_hunk": "@@ -140,6 +140,41 @@ def flush(self) -> bytes:\n             raise DecodingError(str(exc)) from exc\n \n \n+class ZStandardDecoder(ContentDecoder):\n+    \"\"\"\n+    Handle 'zstd' RFC 8878 decoding.\n+\n+    Requires `pip install zstandard`.\n+    Can be installed as a dependency of httpx using `pip install httpx[zstd]`.\n+    \"\"\"\n+\n+    def __init__(self) -> None:\n+        if zstd is False:  # pragma: no cover\n+            raise ImportError(\n+                \"Using 'ZStandardDecoder', ...\"\n+                \"Make sure to install httpx using `pip install httpx[zstd]`.\"\n+            ) from None\n+\n+        self.decompressor = zstd.ZstdDecompressor().decompressobj()\n+\n+    def decode(self, data: bytes) -> bytes:\n+        try:\n+            data_parts = [self.decompressor.decompress(data)]\n+            while self.decompressor.eof and self.decompressor.unused_data:\n+                unused_data = self.decompressor.unused_data\n+                self.decompressor = zstd.ZstdDecompressor().decompressobj()\n+                data_parts.append(self.decompressor.decompress(unused_data))",
      "comment": "I'm not exactly sure what happens, but if I try to use `read_across_frames` I get decoding errors. Also, please note I've copied this logic from urllib3 so it is \"Best Practice\" :-) or at least proven to be working.\r\n\r\nI would be open for suggestions of course!",
      "comment_id": 1518821040,
      "user": "mbeijen",
      "created_at": "2024-03-10T10:59:41Z",
      "url": "https://github.com/encode/httpx/pull/3139#discussion_r1518821040"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3139,
      "file_path": "httpx/_compat.py",
      "line": 38,
      "side": "RIGHT",
      "diff_hunk": "@@ -17,6 +20,24 @@\n     except ImportError:\n         brotli = None\n \n+# Zstandard support is optional\n+zstd: Optional[ModuleType] = None\n+try:\n+    import zstandard as zstd\n+except (AttributeError, ImportError, ValueError):  # Defensive:\n+    zstd = None\n+else:\n+    # The package 'zstandard' added the 'eof' property starting\n+    # in v0.18.0 which we require to ensure a complete and\n+    # valid zstd stream was fed into the ZstdDecoder.\n+    # See: https://github.com/urllib3/urllib3/pull/2624\n+    _zstd_version = tuple(\n+        map(int, re.search(r\"^([0-9]+)\\.([0-9]+)\", zstd.__version__).groups())  # type: ignore[union-attr]\n+    )\n+    if _zstd_version < (0, 18):  # Defensive:\n+        zstd = None",
      "comment": "This might be overly defensive? Can we pin >= 0.18 in the requirements instead?",
      "comment_id": 1524997549,
      "user": "lovelydinosaur",
      "created_at": "2024-03-14T14:38:46Z",
      "url": "https://github.com/encode/httpx/pull/3139#discussion_r1524997549"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3139,
      "file_path": "httpx/_compat.py",
      "line": 38,
      "side": "RIGHT",
      "diff_hunk": "@@ -17,6 +20,24 @@\n     except ImportError:\n         brotli = None\n \n+# Zstandard support is optional\n+zstd: Optional[ModuleType] = None\n+try:\n+    import zstandard as zstd\n+except (AttributeError, ImportError, ValueError):  # Defensive:\n+    zstd = None\n+else:\n+    # The package 'zstandard' added the 'eof' property starting\n+    # in v0.18.0 which we require to ensure a complete and\n+    # valid zstd stream was fed into the ZstdDecoder.\n+    # See: https://github.com/urllib3/urllib3/pull/2624\n+    _zstd_version = tuple(\n+        map(int, re.search(r\"^([0-9]+)\\.([0-9]+)\", zstd.__version__).groups())  # type: ignore[union-attr]\n+    )\n+    if _zstd_version < (0, 18):  # Defensive:\n+        zstd = None",
      "comment": "I've asked a similar question already and got this answer:\r\n\r\nhttps://github.com/encode/httpx/pull/3139#discussion_r1518815892\r\n\r\n> zstandard is an optional install. So if your `requirements.txt` just has `httpx` and not `httpx[zstd]`, and you have `zstandard==[old version]` somehow, this would prevent breakage.\r\n\r\nBasically the idea is that the optional dependency pin is optional and does not guarantee that the given dependency version will be installed. This check would only be redundant if we made zstandard a hard dependency (one not requiring httpx[zstandard]).\r\n\r\nThe same check [exists](https://github.com/urllib3/urllib3/blob/733f638a2faa02b4ff8a9f3b5668949d39396b8b/src/urllib3/response.py#L28-L43) in urllib3.\r\n\r\nPersonally, I +1 the extra safety at the cost of minimal startup delay. :slightly_smiling_face: ",
      "comment_id": 1525232707,
      "user": "Zaczero",
      "created_at": "2024-03-14T17:07:51Z",
      "url": "https://github.com/encode/httpx/pull/3139#discussion_r1525232707"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3139,
      "file_path": "httpx/_compat.py",
      "line": 38,
      "side": "RIGHT",
      "diff_hunk": "@@ -17,6 +20,24 @@\n     except ImportError:\n         brotli = None\n \n+# Zstandard support is optional\n+zstd: Optional[ModuleType] = None\n+try:\n+    import zstandard as zstd\n+except (AttributeError, ImportError, ValueError):  # Defensive:\n+    zstd = None\n+else:\n+    # The package 'zstandard' added the 'eof' property starting\n+    # in v0.18.0 which we require to ensure a complete and\n+    # valid zstd stream was fed into the ZstdDecoder.\n+    # See: https://github.com/urllib3/urllib3/pull/2624\n+    _zstd_version = tuple(\n+        map(int, re.search(r\"^([0-9]+)\\.([0-9]+)\", zstd.__version__).groups())  # type: ignore[union-attr]\n+    )\n+    if _zstd_version < (0, 18):  # Defensive:\n+        zstd = None",
      "comment": "Okay yep. Main thing is coverage needs addressing.",
      "comment_id": 1525274642,
      "user": "lovelydinosaur",
      "created_at": "2024-03-14T17:39:31Z",
      "url": "https://github.com/encode/httpx/pull/3139#discussion_r1525274642"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3245,
      "file_path": "httpx/_api.py",
      "line": 23,
      "side": "RIGHT",
      "diff_hunk": "@@ -18,9 +18,9 @@\n     RequestData,\n     RequestFiles,\n     TimeoutTypes,\n-    URLTypes,\n     VerifyTypes,\n )\n+from ._urls import URL",
      "comment": "I'd only use that if it was required to avoid a cyclical dependency.",
      "comment_id": 1688184272,
      "user": "lovelydinosaur",
      "created_at": "2024-07-23T14:36:53Z",
      "url": "https://github.com/encode/httpx/pull/3245#discussion_r1688184272"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3064,
      "file_path": "tests/test_utils.py",
      "line": 242,
      "side": "RIGHT",
      "diff_hunk": "@@ -234,21 +233,39 @@ def test_not_same_origin():\n \n \n def test_is_https_redirect():\n-    url = httpx.URL(\"http://example.com\")\n-    location = httpx.URL(\"https://example.com\")\n-    assert is_https_redirect(url, location)\n+    url = httpx.URL(\"https://example.com\")\n+    request = httpx.Request(\n+        \"GET\", \"http://example.com\", headers={\"Authorization\": \"empty\"}\n+    )\n+\n+    client = httpx.Client()\n+    headers = client._redirect_headers(request, url, \"GET\")",
      "comment": "Hrm. Is there a way around that allows us to test this against public API?",
      "comment_id": 1453218472,
      "user": "lovelydinosaur",
      "created_at": "2024-01-16T10:19:53Z",
      "url": "https://github.com/encode/httpx/pull/3064#discussion_r1453218472"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3064,
      "file_path": "tests/test_utils.py",
      "line": 242,
      "side": "RIGHT",
      "diff_hunk": "@@ -234,21 +233,39 @@ def test_not_same_origin():\n \n \n def test_is_https_redirect():\n-    url = httpx.URL(\"http://example.com\")\n-    location = httpx.URL(\"https://example.com\")\n-    assert is_https_redirect(url, location)\n+    url = httpx.URL(\"https://example.com\")\n+    request = httpx.Request(\n+        \"GET\", \"http://example.com\", headers={\"Authorization\": \"empty\"}\n+    )\n+\n+    client = httpx.Client()\n+    headers = client._redirect_headers(request, url, \"GET\")",
      "comment": "Might be that we're okay with leaning on implementation details a little bit, certainly not a terrible trade-off to make. (I think we already have some instances of this in our test cases right?)",
      "comment_id": 1453220170,
      "user": "lovelydinosaur",
      "created_at": "2024-01-16T10:21:13Z",
      "url": "https://github.com/encode/httpx/pull/3064#discussion_r1453220170"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3064,
      "file_path": "tests/test_utils.py",
      "line": 242,
      "side": "RIGHT",
      "diff_hunk": "@@ -234,21 +233,39 @@ def test_not_same_origin():\n \n \n def test_is_https_redirect():\n-    url = httpx.URL(\"http://example.com\")\n-    location = httpx.URL(\"https://example.com\")\n-    assert is_https_redirect(url, location)\n+    url = httpx.URL(\"https://example.com\")\n+    request = httpx.Request(\n+        \"GET\", \"http://example.com\", headers={\"Authorization\": \"empty\"}\n+    )\n+\n+    client = httpx.Client()\n+    headers = client._redirect_headers(request, url, \"GET\")",
      "comment": "Also, we already use private-accessors in another test:\r\nhttps://github.com/encode/httpx/blob/4f6edf36e93fd9f83ff95b065718fd6bd0c4d3c5/tests/client/test_proxies.py#L333-L338",
      "comment_id": 1453483159,
      "user": "T-256",
      "created_at": "2024-01-16T14:11:05Z",
      "url": "https://github.com/encode/httpx/pull/3064#discussion_r1453483159"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 1574,
      "file_path": "httpx/_client.py",
      "line": 736,
      "side": "RIGHT",
      "diff_hunk": "@@ -731,6 +731,14 @@ def request(\n \n         [0]: /advanced/#merging-of-configuration\n         \"\"\"\n+        if cookies is not None:\n+            message = (\n+                \"Setting per-request cookies=... is being deprecated, because \"",
      "comment": "This would be better as\r\n```suggestion\r\n                \"Setting per-request cookies=<...> is being deprecated, because \"\r\n```\r\nfor better language flow, I think (and similar to the `content=<...>` message in https://github.com/encode/httpx/pull/1573/files#diff-361c5daf6e5401b987414c619d003a20ea88f74fe9da50cbb2da72c337aeb290R152)",
      "comment_id": 613343499,
      "user": "StephenBrown2",
      "created_at": "2021-04-14T15:18:40Z",
      "url": "https://github.com/encode/httpx/pull/1574#discussion_r613343499"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3045,
      "file_path": "httpx/_auth.py",
      "line": 285,
      "side": "RIGHT",
      "diff_hunk": "@@ -280,7 +280,7 @@ def digest(data: bytes) -> bytes:\n \n         qop = self._resolve_qop(challenge.qop, request=request)\n         if qop is None:\n-            digest_data = [HA1, challenge.nonce, HA2]\n+            digest_data = [challenge.nonce, HA2]\n         else:\n             digest_data = [challenge.nonce, nc_value, cnonce, qop, HA2]",
      "comment": "Looking at the curl code, seems like this change should be the other way around(?)...\r\n\r\n```python\r\n        if qop is None:\r\n            digest_data = [HA1, challenge.nonce, HA2]\r\n        else:\r\n            digest_data = [HA1, challenge.nonce, nc_value, cnonce, qop, HA2]\r\n```\r\n\r\nAnd then...\r\n\r\n```python\r\n        format_args = {\r\n            \"username\": self._username,\r\n            \"realm\": challenge.realm,\r\n            \"nonce\": challenge.nonce,\r\n            \"uri\": path,\r\n            \"response\": digest(key_digest),\r\n            \"algorithm\": challenge.algorithm.encode(),\r\n        }\r\n```",
      "comment_id": 1444454071,
      "user": "lovelydinosaur",
      "created_at": "2024-01-08T11:01:34Z",
      "url": "https://github.com/encode/httpx/pull/3045#discussion_r1444454071"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3045,
      "file_path": "httpx/_auth.py",
      "line": 285,
      "side": "RIGHT",
      "diff_hunk": "@@ -280,7 +280,7 @@ def digest(data: bytes) -> bytes:\n \n         qop = self._resolve_qop(challenge.qop, request=request)\n         if qop is None:\n-            digest_data = [HA1, challenge.nonce, HA2]\n+            digest_data = [challenge.nonce, HA2]\n         else:\n             digest_data = [challenge.nonce, nc_value, cnonce, qop, HA2]",
      "comment": "Yeah, initially I wanted to avoid touching the other branch (so I don't break it accidentally), but I agree the curl way is more readable. I also add the comments.",
      "comment_id": 1444577834,
      "user": "the-ress",
      "created_at": "2024-01-08T12:50:09Z",
      "url": "https://github.com/encode/httpx/pull/3045#discussion_r1444577834"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3045,
      "file_path": "tests/test_auth.py",
      "line": 242,
      "side": "RIGHT",
      "diff_hunk": "@@ -140,3 +140,168 @@ def test_digest_auth_setting_cookie_in_request():\n     )\n     with pytest.raises(StopIteration):\n         flow.send(response)\n+\n+\n+def test_digest_auth_rfc_2069():\n+    # Example from https://datatracker.ietf.org/doc/html/rfc2069#section-2.4\n+    # with corrected response from https://www.rfc-editor.org/errata/eid749\n+\n+    auth = httpx.DigestAuth(username=\"Mufasa\", password=\"CircleOfLife\")\n+    request = httpx.Request(\"GET\", \"https://www.example.com/dir/index.html\")\n+\n+    # The initial request should not include an auth header.\n+    flow = auth.sync_auth_flow(request)\n+    request = next(flow)\n+    assert \"Authorization\" not in request.headers\n+\n+    # If a 401 response is returned, then a digest auth request is made.\n+    headers = {\n+        \"WWW-Authenticate\": (\n+            'Digest realm=\"testrealm@host.com\", '\n+            'nonce=\"dcd98b7102dd2f0e8b11d0f600bfb0c093\", '\n+            'opaque=\"5ccc069c403ebaf9f0171e9517f40e41\"'\n+        )\n+    }\n+    response = httpx.Response(\n+        content=b\"Auth required\", status_code=401, headers=headers, request=request\n+    )\n+    request = flow.send(response)\n+    assert request.headers[\"Authorization\"].startswith(\"Digest\")\n+    assert 'username=\"Mufasa\"' in request.headers[\"Authorization\"]\n+    assert 'realm=\"testrealm@host.com\"' in request.headers[\"Authorization\"]\n+    assert (\n+        'nonce=\"dcd98b7102dd2f0e8b11d0f600bfb0c093\"' in request.headers[\"Authorization\"]\n+    )\n+    assert 'uri=\"/dir/index.html\"' in request.headers[\"Authorization\"]\n+    assert (\n+        'opaque=\"5ccc069c403ebaf9f0171e9517f40e41\"' in request.headers[\"Authorization\"]\n+    )\n+    assert (\n+        'response=\"1949323746fe6a43ef61f9606e7febea\"'\n+        in request.headers[\"Authorization\"]\n+    )\n+\n+    # No other requests are made.\n+    response = httpx.Response(content=b\"Hello, world!\", status_code=200)\n+    with pytest.raises(StopIteration):\n+        flow.send(response)\n+\n+\n+def test_digest_auth_rfc_7616_md5(monkeypatch):\n+    # Example from https://datatracker.ietf.org/doc/html/rfc7616#section-3.9.1\n+\n+    def mock_get_client_nonce(nonce_count: int, nonce: bytes) -> bytes:\n+        return \"f2/wE4q74E6zIJEtWaHKaf5wv/H5QzzpXusqGemxURZJ\".encode()\n+\n+    auth = httpx.DigestAuth(username=\"Mufasa\", password=\"Circle of Life\")\n+    monkeypatch.setattr(auth, \"_get_client_nonce\", mock_get_client_nonce)\n+\n+    request = httpx.Request(\"GET\", \"https://www.example.com/dir/index.html\")\n+\n+    # The initial request should not include an auth header.\n+    flow = auth.sync_auth_flow(request)\n+    request = next(flow)\n+    assert \"Authorization\" not in request.headers\n+\n+    # If a 401 response is returned, then a digest auth request is made.\n+    headers = {\n+        \"WWW-Authenticate\": (\n+            'Digest realm=\"http-auth@example.org\", '\n+            'qop=\"auth, auth-int\", '\n+            \"algorithm=MD5, \"\n+            'nonce=\"7ypf/xlj9XXwfDPEoM4URrv/xwf94BcCAzFZH4GiTo0v\", '\n+            'opaque=\"FQhe/qaU925kfnzjCev0ciny7QMkPqMAFRtzCUYo5tdS\"'\n+        )\n+    }\n+    response = httpx.Response(\n+        content=b\"Auth required\", status_code=401, headers=headers, request=request\n+    )\n+    request = flow.send(response)\n+    assert request.headers[\"Authorization\"].startswith(\"Digest\")\n+    assert 'username=\"Mufasa\"' in request.headers[\"Authorization\"]\n+    assert 'realm=\"http-auth@example.org\"' in request.headers[\"Authorization\"]\n+    assert 'uri=\"/dir/index.html\"' in request.headers[\"Authorization\"]\n+    assert \"algorithm=MD5\" in request.headers[\"Authorization\"]\n+    assert (\n+        'nonce=\"7ypf/xlj9XXwfDPEoM4URrv/xwf94BcCAzFZH4GiTo0v\"'\n+        in request.headers[\"Authorization\"]\n+    )\n+    assert \"nc=00000001\" in request.headers[\"Authorization\"]\n+    assert (\n+        'cnonce=\"f2/wE4q74E6zIJEtWaHKaf5wv/H5QzzpXusqGemxURZJ\"'\n+        in request.headers[\"Authorization\"]\n+    )\n+    assert \"qop=auth\" in request.headers[\"Authorization\"]\n+    assert (\n+        'opaque=\"FQhe/qaU925kfnzjCev0ciny7QMkPqMAFRtzCUYo5tdS\"'\n+        in request.headers[\"Authorization\"]\n+    )\n+    assert (\n+        'response=\"8ca523f5e9506fed4657c9700eebdbec\"'\n+        in request.headers[\"Authorization\"]\n+    )",
      "comment": "These tests are fantastic, thanks!\r\n\r\nAre we able to do a literal `assert request.headers[\"Authorization\"] == ...` test here or is the ordering that we're returning different to the example in the RFC?",
      "comment_id": 1445998305,
      "user": "lovelydinosaur",
      "created_at": "2024-01-09T11:56:24Z",
      "url": "https://github.com/encode/httpx/pull/3045#discussion_r1445998305"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3045,
      "file_path": "tests/test_auth.py",
      "line": 242,
      "side": "RIGHT",
      "diff_hunk": "@@ -140,3 +140,168 @@ def test_digest_auth_setting_cookie_in_request():\n     )\n     with pytest.raises(StopIteration):\n         flow.send(response)\n+\n+\n+def test_digest_auth_rfc_2069():\n+    # Example from https://datatracker.ietf.org/doc/html/rfc2069#section-2.4\n+    # with corrected response from https://www.rfc-editor.org/errata/eid749\n+\n+    auth = httpx.DigestAuth(username=\"Mufasa\", password=\"CircleOfLife\")\n+    request = httpx.Request(\"GET\", \"https://www.example.com/dir/index.html\")\n+\n+    # The initial request should not include an auth header.\n+    flow = auth.sync_auth_flow(request)\n+    request = next(flow)\n+    assert \"Authorization\" not in request.headers\n+\n+    # If a 401 response is returned, then a digest auth request is made.\n+    headers = {\n+        \"WWW-Authenticate\": (\n+            'Digest realm=\"testrealm@host.com\", '\n+            'nonce=\"dcd98b7102dd2f0e8b11d0f600bfb0c093\", '\n+            'opaque=\"5ccc069c403ebaf9f0171e9517f40e41\"'\n+        )\n+    }\n+    response = httpx.Response(\n+        content=b\"Auth required\", status_code=401, headers=headers, request=request\n+    )\n+    request = flow.send(response)\n+    assert request.headers[\"Authorization\"].startswith(\"Digest\")\n+    assert 'username=\"Mufasa\"' in request.headers[\"Authorization\"]\n+    assert 'realm=\"testrealm@host.com\"' in request.headers[\"Authorization\"]\n+    assert (\n+        'nonce=\"dcd98b7102dd2f0e8b11d0f600bfb0c093\"' in request.headers[\"Authorization\"]\n+    )\n+    assert 'uri=\"/dir/index.html\"' in request.headers[\"Authorization\"]\n+    assert (\n+        'opaque=\"5ccc069c403ebaf9f0171e9517f40e41\"' in request.headers[\"Authorization\"]\n+    )\n+    assert (\n+        'response=\"1949323746fe6a43ef61f9606e7febea\"'\n+        in request.headers[\"Authorization\"]\n+    )\n+\n+    # No other requests are made.\n+    response = httpx.Response(content=b\"Hello, world!\", status_code=200)\n+    with pytest.raises(StopIteration):\n+        flow.send(response)\n+\n+\n+def test_digest_auth_rfc_7616_md5(monkeypatch):\n+    # Example from https://datatracker.ietf.org/doc/html/rfc7616#section-3.9.1\n+\n+    def mock_get_client_nonce(nonce_count: int, nonce: bytes) -> bytes:\n+        return \"f2/wE4q74E6zIJEtWaHKaf5wv/H5QzzpXusqGemxURZJ\".encode()\n+\n+    auth = httpx.DigestAuth(username=\"Mufasa\", password=\"Circle of Life\")\n+    monkeypatch.setattr(auth, \"_get_client_nonce\", mock_get_client_nonce)\n+\n+    request = httpx.Request(\"GET\", \"https://www.example.com/dir/index.html\")\n+\n+    # The initial request should not include an auth header.\n+    flow = auth.sync_auth_flow(request)\n+    request = next(flow)\n+    assert \"Authorization\" not in request.headers\n+\n+    # If a 401 response is returned, then a digest auth request is made.\n+    headers = {\n+        \"WWW-Authenticate\": (\n+            'Digest realm=\"http-auth@example.org\", '\n+            'qop=\"auth, auth-int\", '\n+            \"algorithm=MD5, \"\n+            'nonce=\"7ypf/xlj9XXwfDPEoM4URrv/xwf94BcCAzFZH4GiTo0v\", '\n+            'opaque=\"FQhe/qaU925kfnzjCev0ciny7QMkPqMAFRtzCUYo5tdS\"'\n+        )\n+    }\n+    response = httpx.Response(\n+        content=b\"Auth required\", status_code=401, headers=headers, request=request\n+    )\n+    request = flow.send(response)\n+    assert request.headers[\"Authorization\"].startswith(\"Digest\")\n+    assert 'username=\"Mufasa\"' in request.headers[\"Authorization\"]\n+    assert 'realm=\"http-auth@example.org\"' in request.headers[\"Authorization\"]\n+    assert 'uri=\"/dir/index.html\"' in request.headers[\"Authorization\"]\n+    assert \"algorithm=MD5\" in request.headers[\"Authorization\"]\n+    assert (\n+        'nonce=\"7ypf/xlj9XXwfDPEoM4URrv/xwf94BcCAzFZH4GiTo0v\"'\n+        in request.headers[\"Authorization\"]\n+    )\n+    assert \"nc=00000001\" in request.headers[\"Authorization\"]\n+    assert (\n+        'cnonce=\"f2/wE4q74E6zIJEtWaHKaf5wv/H5QzzpXusqGemxURZJ\"'\n+        in request.headers[\"Authorization\"]\n+    )\n+    assert \"qop=auth\" in request.headers[\"Authorization\"]\n+    assert (\n+        'opaque=\"FQhe/qaU925kfnzjCev0ciny7QMkPqMAFRtzCUYo5tdS\"'\n+        in request.headers[\"Authorization\"]\n+    )\n+    assert (\n+        'response=\"8ca523f5e9506fed4657c9700eebdbec\"'\n+        in request.headers[\"Authorization\"]\n+    )",
      "comment": "I wouldn't want to depend on the exact order since it'd make the test more fragile with little benefit.",
      "comment_id": 1446681951,
      "user": "the-ress",
      "created_at": "2024-01-09T22:21:31Z",
      "url": "https://github.com/encode/httpx/pull/3045#discussion_r1446681951"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3042,
      "file_path": "httpx/_client.py",
      "line": 1316,
      "side": "RIGHT",
      "diff_hunk": "@@ -1311,6 +1313,8 @@ class AsyncClient(BaseClient):\n     An asynchronous HTTP client, with connection pooling, HTTP/2, redirects,\n     cookie persistence, etc.\n \n+    It can be shared between threads.",
      "comment": "```suggestion\r\n    It can be shared between tasks.\r\n```",
      "comment_id": 1442512639,
      "user": "karpetrosyan",
      "created_at": "2024-01-05T05:41:11Z",
      "url": "https://github.com/encode/httpx/pull/3042#discussion_r1442512639"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 3042,
      "file_path": "httpx/_client.py",
      "line": 1552,
      "side": "RIGHT",
      "diff_hunk": "@@ -1544,6 +1548,15 @@ async def request(\n \n         [0]: /advanced/#merging-of-configuration\n         \"\"\"\n+\n+        if cookies is not None:",
      "comment": "```suggestion\r\n        if cookies is not None:  # pragma: no cover\r\n```",
      "comment_id": 1442513863,
      "user": "karpetrosyan",
      "created_at": "2024-01-05T05:44:18Z",
      "url": "https://github.com/encode/httpx/pull/3042#discussion_r1442513863"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2990,
      "file_path": "httpx/_urlparse.py",
      "line": 454,
      "side": "RIGHT",
      "diff_hunk": "@@ -449,6 +448,32 @@ def quote(string: str, safe: str = \"/\") -> str:\n     )\n \n \n+def quote(string: str, safe: str = \"/\") -> str:\n+    \"\"\"\n+    Use percent-encoding to quote a string, omitting existing '%xx' escape sequences.\n+    \"\"\"",
      "comment": "@tomchristie - perhaps this could do with some code comments?\r\n\r\nAn example of what we're ending up with here is...\r\n\r\n```python\r\ninput = \"abc%20de f\"\r\noutput = \"\".join([percent_encoded(\"abc\"), \"%20\", percent_encoded(\"de f\")])\r\nassert quote(input) == output",
      "comment_id": 1420326415,
      "user": "lovelydinosaur",
      "created_at": "2023-12-08T11:44:52Z",
      "url": "https://github.com/encode/httpx/pull/2990#discussion_r1420326415"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2990,
      "file_path": "httpx/_urlparse.py",
      "line": 497,
      "side": "RIGHT",
      "diff_hunk": "@@ -464,4 +489,9 @@ def urlencode(items: typing.List[typing.Tuple[str, str]]) -> str:\n     - https://github.com/encode/httpx/issues/2721\n     - https://docs.python.org/3/library/urllib.parse.html#urllib.parse.urlencode\n     \"\"\"\n-    return \"&\".join([quote(k, safe=\"\") + \"=\" + quote(v, safe=\"\") for k, v in items])\n+    return \"&\".join(\n+        [\n+            percent_encoded(k, safe=\"\") + \"=\" + percent_encoded(v, safe=\"\")\n+            for k, v in items\n+        ]\n+    )",
      "comment": "Comment to the gallery...\r\n\r\nWe've switch from `quote` to `percent_encoded` here, because we *always* want to percent encode items that are being provided as form inputs, with `params = {}`.\r\n",
      "comment_id": 1420329649,
      "user": "lovelydinosaur",
      "created_at": "2023-12-08T11:48:11Z",
      "url": "https://github.com/encode/httpx/pull/2990#discussion_r1420329649"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2990,
      "file_path": "httpx/_urlparse.py",
      "line": 454,
      "side": "RIGHT",
      "diff_hunk": "@@ -449,6 +448,32 @@ def quote(string: str, safe: str = \"/\") -> str:\n     )\n \n \n+def quote(string: str, safe: str = \"/\") -> str:\n+    \"\"\"\n+    Use percent-encoding to quote a string, omitting existing '%xx' escape sequences.\n+    \"\"\"",
      "comment": "I see code comments more difficult to read than that simple straightforward description.\r\nPerhaps `safe` parameter needs more info here(?)",
      "comment_id": 1421693032,
      "user": "T-256",
      "created_at": "2023-12-10T06:35:15Z",
      "url": "https://github.com/encode/httpx/pull/2990#discussion_r1421693032"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2990,
      "file_path": "httpx/_urlparse.py",
      "line": 443,
      "side": "RIGHT",
      "diff_hunk": "@@ -432,13 +432,12 @@ def is_safe(string: str, safe: str = \"/\") -> bool:\n         if char not in NON_ESCAPED_CHARS:\n             return False\n \n-    # Any '%' characters must be valid '%xx' escape sequences.\n-    return string.count(\"%\") == len(PERCENT_ENCODED_REGEX.findall(string))\n+    return True\n \n \n-def quote(string: str, safe: str = \"/\") -> str:\n+def percent_encoded(string: str, safe: str = \"/\") -> str:\n     \"\"\"\n-    Use percent-encoding to quote a string if required.\n+    Use percent-encoding to quote a string.\n     \"\"\"\n     if is_safe(string, safe=safe):\n         return string",
      "comment": "I suggest use minimal filtering instead of separated one-time function.\r\n```py\r\n    if all(char in NON_ESCAPED_CHARS for char in string):\r\n        return string\r\n```\r\n\r\nI also noticed `NON_ESCAPED_CHARS` name in `is_safe` function has different value than it is in `percent_encoded`.",
      "comment_id": 1421693601,
      "user": "T-256",
      "created_at": "2023-12-10T06:41:37Z",
      "url": "https://github.com/encode/httpx/pull/2990#discussion_r1421693601"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2990,
      "file_path": "httpx/_urlparse.py",
      "line": 474,
      "side": "RIGHT",
      "diff_hunk": "@@ -449,6 +448,32 @@ def quote(string: str, safe: str = \"/\") -> str:\n     )\n \n \n+def quote(string: str, safe: str = \"/\") -> str:\n+    \"\"\"\n+    Use percent-encoding to quote a string, omitting existing '%xx' escape sequences.\n+    \"\"\"\n+    parts = []\n+    current_position = 0\n+    for match in re.finditer(PERCENT_ENCODED_REGEX, string):\n+        start_position, end_position = match.start(), match.end()\n+        matched_text = match.group(0)\n+        # Add any text up to the '%xx' escape sequence.\n+        if start_position != current_position:\n+            leading_text = string[current_position:start_position]\n+            parts.append(percent_encoded(leading_text, safe=safe))\n+\n+        # Add the '%xx' escape sequence.\n+        parts.append(matched_text)\n+        current_position = end_position\n+\n+    # Add any text after the final '%xx' escape sequence.\n+    if current_position != len(string):\n+        trailing_text = string[current_position:]\n+        parts.append(percent_encoded(trailing_text, safe=safe))\n+\n+    return \"\".join(parts)",
      "comment": "`parts` could be string. `parts +=` instead of `parts.append`",
      "comment_id": 1421694231,
      "user": "T-256",
      "created_at": "2023-12-10T06:48:27Z",
      "url": "https://github.com/encode/httpx/pull/2990#discussion_r1421694231"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2990,
      "file_path": "httpx/_urlparse.py",
      "line": 443,
      "side": "RIGHT",
      "diff_hunk": "@@ -432,13 +432,12 @@ def is_safe(string: str, safe: str = \"/\") -> bool:\n         if char not in NON_ESCAPED_CHARS:\n             return False\n \n-    # Any '%' characters must be valid '%xx' escape sequences.\n-    return string.count(\"%\") == len(PERCENT_ENCODED_REGEX.findall(string))\n+    return True\n \n \n-def quote(string: str, safe: str = \"/\") -> str:\n+def percent_encoded(string: str, safe: str = \"/\") -> str:\n     \"\"\"\n-    Use percent-encoding to quote a string if required.\n+    Use percent-encoding to quote a string.\n     \"\"\"\n     if is_safe(string, safe=safe):\n         return string",
      "comment": "Sure. I'd like to treat performance improvements separately to this.\r\nI'll followup with some further work once we've got the behavioural changes in.",
      "comment_id": 1422158955,
      "user": "lovelydinosaur",
      "created_at": "2023-12-11T09:21:54Z",
      "url": "https://github.com/encode/httpx/pull/2990#discussion_r1422158955"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2990,
      "file_path": "httpx/_urlparse.py",
      "line": 474,
      "side": "RIGHT",
      "diff_hunk": "@@ -449,6 +448,32 @@ def quote(string: str, safe: str = \"/\") -> str:\n     )\n \n \n+def quote(string: str, safe: str = \"/\") -> str:\n+    \"\"\"\n+    Use percent-encoding to quote a string, omitting existing '%xx' escape sequences.\n+    \"\"\"\n+    parts = []\n+    current_position = 0\n+    for match in re.finditer(PERCENT_ENCODED_REGEX, string):\n+        start_position, end_position = match.start(), match.end()\n+        matched_text = match.group(0)\n+        # Add any text up to the '%xx' escape sequence.\n+        if start_position != current_position:\n+            leading_text = string[current_position:start_position]\n+            parts.append(percent_encoded(leading_text, safe=safe))\n+\n+        # Add the '%xx' escape sequence.\n+        parts.append(matched_text)\n+        current_position = end_position\n+\n+    # Add any text after the final '%xx' escape sequence.\n+    if current_position != len(string):\n+        trailing_text = string[current_position:]\n+        parts.append(percent_encoded(trailing_text, safe=safe))\n+\n+    return \"\".join(parts)",
      "comment": "It could, yes. I've gone with this pattern because in the past we've seen performance issues with the `+=` pattern, that a `.append() ... \"\".join()` pattern resolves. Might not be the case in this context, since we've got strict limits on the allowable string lengths in URLs. But... let's handle any performance related considerations separately to the behavioural fixes.",
      "comment_id": 1422164211,
      "user": "lovelydinosaur",
      "created_at": "2023-12-11T09:25:09Z",
      "url": "https://github.com/encode/httpx/pull/2990#discussion_r1422164211"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2990,
      "file_path": "httpx/_transports/asgi.py",
      "line": 3,
      "side": "LEFT",
      "diff_hunk": "@@ -1,7 +1,5 @@\n import typing\n \n-import sniffio",
      "comment": "`_utils.py` also imports sniffio as top-level.\r\nCould we consider add httpcore's [`current_async_library`](https://github.com/encode/httpcore/blob/2fcd062df71555cc7de55774c6dc137551eb8692/httpcore/_synchronization.py#L21) in httpx?",
      "comment_id": 1423025866,
      "user": "T-256",
      "created_at": "2023-12-11T19:27:01Z",
      "url": "https://github.com/encode/httpx/pull/2990#discussion_r1423025866"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2990,
      "file_path": "httpx/_urlparse.py",
      "line": 264,
      "side": "RIGHT",
      "diff_hunk": "@@ -263,7 +263,7 @@ def urlparse(url: str = \"\", **kwargs: typing.Optional[str]) -> ParseResult:\n     # We also exclude '/' because it is more robust to replace it with a percent\n     # encoding despite it not being a requirement of the spec.",
      "comment": "Doesn't this comment become incorrect with this change?",
      "comment_id": 1424884213,
      "user": "jkseppan",
      "created_at": "2023-12-13T06:01:36Z",
      "url": "https://github.com/encode/httpx/pull/2990#discussion_r1424884213"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2990,
      "file_path": "httpx/_urlparse.py",
      "line": 449,
      "side": "RIGHT",
      "diff_hunk": "@@ -449,6 +446,39 @@ def quote(string: str, safe: str = \"/\") -> str:\n     )\n \n \n+def quote(string: str, safe: str = \"/\") -> str:",
      "comment": "I believe we should also add unit tests for these functions, rather than simply testing them with `httpx.URL`. \r\nThis would be a more robust approach, in my opinion.\r\n",
      "comment_id": 1425186493,
      "user": "karpetrosyan",
      "created_at": "2023-12-13T10:55:47Z",
      "url": "https://github.com/encode/httpx/pull/2990#discussion_r1425186493"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2990,
      "file_path": "httpx/_urlparse.py",
      "line": 449,
      "side": "RIGHT",
      "diff_hunk": "@@ -449,6 +446,39 @@ def quote(string: str, safe: str = \"/\") -> str:\n     )\n \n \n+def quote(string: str, safe: str = \"/\") -> str:",
      "comment": "IMO current approach is fine.\r\n\r\n> This is clearly a code-smell, because our test cases ought to be tests against our public API, rather than testing implementation details. Perhaps there's some cases where it's a necessary hack, but... perhaps not?\r\n\r\n\r\nfrom https://github.com/encode/httpx/issues/2492#issue-1478857204",
      "comment_id": 1425847949,
      "user": "T-256",
      "created_at": "2023-12-13T20:23:49Z",
      "url": "https://github.com/encode/httpx/pull/2990#discussion_r1425847949"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2990,
      "file_path": "httpx/_urlparse.py",
      "line": 449,
      "side": "RIGHT",
      "diff_hunk": "@@ -449,6 +446,39 @@ def quote(string: str, safe: str = \"/\") -> str:\n     )\n \n \n+def quote(string: str, safe: str = \"/\") -> str:",
      "comment": "I agree testing the public API should be sufficient unless something private is particularly expensive to test via the public API.",
      "comment_id": 1426015469,
      "user": "zanieb",
      "created_at": "2023-12-14T00:20:26Z",
      "url": "https://github.com/encode/httpx/pull/2990#discussion_r1426015469"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2990,
      "file_path": "httpx/_urlparse.py",
      "line": 449,
      "side": "RIGHT",
      "diff_hunk": "@@ -449,6 +446,39 @@ def quote(string: str, safe: str = \"/\") -> str:\n     )\n \n \n+def quote(string: str, safe: str = \"/\") -> str:",
      "comment": "It's a matter of preference, but if we encounter regression in our `httpx.URL` tests, we will go through these functions and find the method that isn't working properly, which is why unit tests are useful.",
      "comment_id": 1426181241,
      "user": "karpetrosyan",
      "created_at": "2023-12-14T05:14:41Z",
      "url": "https://github.com/encode/httpx/pull/2990#discussion_r1426181241"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2998,
      "file_path": "tests/test_decoders.py",
      "line": 278,
      "side": "RIGHT",
      "diff_hunk": "@@ -262,6 +273,11 @@ def test_line_decoder_crnl():\n     assert list(response.iter_lines()) == [\"12345\", \"foo bar baz\"]\n \n \n+@pytest.mark.parametrize([\"text\", \"expected\"], [(\"\", [])])\n+def test_line_decoding_edge_cases(text: str, expected: typing.List[str]) -> None:\n+    assert httpx._decoders.LineDecoder().decode(text) == expected",
      "comment": "This was the only way I could figure out how to cover an empty input to `LineDecoder.decode`.",
      "comment_id": 1422619715,
      "user": "jamesbraza",
      "created_at": "2023-12-11T15:08:15Z",
      "url": "https://github.com/encode/httpx/pull/2998#discussion_r1422619715"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2879,
      "file_path": "httpx/_transports/default.py",
      "line": 52,
      "side": "RIGHT",
      "diff_hunk": "@@ -47,7 +49,7 @@\n     WriteTimeout,\n )\n from .._models import Request, Response\n-from .._types import AsyncByteStream, CertTypes, SyncByteStream, VerifyTypes\n+from .._types import AsyncByteStream, CertTypes, ProxyTypes, SyncByteStream, VerifyTypes",
      "comment": "```suggestion\r\nfrom .._types import AsyncByteStream, CertTypes, ProxyTypes, SyncByteStream, VerifyTypes\r\nfrom .._urls import URL\r\n```",
      "comment_id": 1353991206,
      "user": "T-256",
      "created_at": "2023-10-11T04:16:34Z",
      "url": "https://github.com/encode/httpx/pull/2879#discussion_r1353991206"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2879,
      "file_path": "httpx/_types.py",
      "line": 82,
      "side": "RIGHT",
      "diff_hunk": "@@ -78,7 +78,8 @@\n     Tuple[Optional[float], Optional[float], Optional[float], Optional[float]],\n     \"Timeout\",\n ]\n-ProxiesTypes = Union[URLTypes, \"Proxy\", Dict[URLTypes, Union[None, URLTypes, \"Proxy\"]]]\n+ProxyTypes = Union[URLTypes, \"Proxy\"]\n+ProxiesTypes = Union[URLTypes, ProxyTypes, Dict[URLTypes, Union[None, ProxyTypes]]]",
      "comment": "```suggestion\r\nProxiesTypes = Union[ProxyTypes, Dict[URLTypes, Union[None, ProxyTypes]]]\r\n```",
      "comment_id": 1356443220,
      "user": "karpetrosyan",
      "created_at": "2023-10-12T08:03:28Z",
      "url": "https://github.com/encode/httpx/pull/2879#discussion_r1356443220"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2723,
      "file_path": "httpx/_urlparse.py",
      "line": 267,
      "side": "RIGHT",
      "diff_hunk": "@@ -260,8 +260,11 @@ def urlparse(url: str = \"\", **kwargs: typing.Optional[str]) -> ParseResult:\n     # For 'path' we need to drop ? and # from the GEN_DELIMS set.\n     parsed_path: str = quote(path, safe=SUB_DELIMS + \":/[]@\")\n     # For 'query' we need to drop '#' from the GEN_DELIMS set.\n+    # We also exclude '/' because it is more robust to replace it with a percent\n+    # encoding despite it not being a requirement of the spec and include '%' to\n+    # prevent duplicate encoding of previously quoted items.\n     parsed_query: typing.Optional[str] = (\n-        None if query is None else quote(query, safe=SUB_DELIMS + \":/?[]@\")\n+        None if query is None else quote(query, safe=SUB_DELIMS + \":?[]@%\")",
      "comment": "I was surprised that I needed to include `%` but it was needed to to prevent spaces converted to `%20` from being changed to `%2520` in tests.",
      "comment_id": 1207129278,
      "user": "zanieb",
      "created_at": "2023-05-26T17:39:17Z",
      "url": "https://github.com/encode/httpx/pull/2723#discussion_r1207129278"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2723,
      "file_path": "httpx/_urlparse.py",
      "line": 267,
      "side": "RIGHT",
      "diff_hunk": "@@ -260,8 +260,11 @@ def urlparse(url: str = \"\", **kwargs: typing.Optional[str]) -> ParseResult:\n     # For 'path' we need to drop ? and # from the GEN_DELIMS set.\n     parsed_path: str = quote(path, safe=SUB_DELIMS + \":/[]@\")\n     # For 'query' we need to drop '#' from the GEN_DELIMS set.\n+    # We also exclude '/' because it is more robust to replace it with a percent\n+    # encoding despite it not being a requirement of the spec and include '%' to\n+    # prevent duplicate encoding of previously quoted items.\n     parsed_query: typing.Optional[str] = (\n-        None if query is None else quote(query, safe=SUB_DELIMS + \":/?[]@\")\n+        None if query is None else quote(query, safe=SUB_DELIMS + \":?[]@%\")",
      "comment": "Let me know if there's additional test coverage that would make sense for this or if we've revealed another issue.",
      "comment_id": 1207129907,
      "user": "zanieb",
      "created_at": "2023-05-26T17:39:50Z",
      "url": "https://github.com/encode/httpx/pull/2723#discussion_r1207129907"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2723,
      "file_path": "httpx/_urlparse.py",
      "line": 267,
      "side": "RIGHT",
      "diff_hunk": "@@ -260,8 +260,11 @@ def urlparse(url: str = \"\", **kwargs: typing.Optional[str]) -> ParseResult:\n     # For 'path' we need to drop ? and # from the GEN_DELIMS set.\n     parsed_path: str = quote(path, safe=SUB_DELIMS + \":/[]@\")\n     # For 'query' we need to drop '#' from the GEN_DELIMS set.\n+    # We also exclude '/' because it is more robust to replace it with a percent\n+    # encoding despite it not being a requirement of the spec and include '%' to\n+    # prevent duplicate encoding of previously quoted items.\n     parsed_query: typing.Optional[str] = (\n-        None if query is None else quote(query, safe=SUB_DELIMS + \":/?[]@\")\n+        None if query is None else quote(query, safe=SUB_DELIMS + \":?[]@%\")",
      "comment": "I'll rephrase: Can you drop that bit from this pull request? It seems unrelated so it'd make sense to consider it separately.",
      "comment_id": 1207742623,
      "user": "lovelydinosaur",
      "created_at": "2023-05-27T07:09:05Z",
      "url": "https://github.com/encode/httpx/pull/2723#discussion_r1207742623"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2723,
      "file_path": "httpx/_urlparse.py",
      "line": 267,
      "side": "RIGHT",
      "diff_hunk": "@@ -260,8 +260,11 @@ def urlparse(url: str = \"\", **kwargs: typing.Optional[str]) -> ParseResult:\n     # For 'path' we need to drop ? and # from the GEN_DELIMS set.\n     parsed_path: str = quote(path, safe=SUB_DELIMS + \":/[]@\")\n     # For 'query' we need to drop '#' from the GEN_DELIMS set.\n+    # We also exclude '/' because it is more robust to replace it with a percent\n+    # encoding despite it not being a requirement of the spec and include '%' to\n+    # prevent duplicate encoding of previously quoted items.\n     parsed_query: typing.Optional[str] = (\n-        None if query is None else quote(query, safe=SUB_DELIMS + \":/?[]@\")\n+        None if query is None else quote(query, safe=SUB_DELIMS + \":?[]@%\")",
      "comment": "Yeah I can but then the _other_ change will make the tests fail i.e. this pull request will introduce a incorrect behavior. I don't understand why yet. I'll adjust my commit so you can see it in CI.",
      "comment_id": 1209579869,
      "user": "zanieb",
      "created_at": "2023-05-29T22:40:17Z",
      "url": "https://github.com/encode/httpx/pull/2723#discussion_r1209579869"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2723,
      "file_path": "httpx/_urlparse.py",
      "line": 267,
      "side": "RIGHT",
      "diff_hunk": "@@ -260,8 +260,11 @@ def urlparse(url: str = \"\", **kwargs: typing.Optional[str]) -> ParseResult:\n     # For 'path' we need to drop ? and # from the GEN_DELIMS set.\n     parsed_path: str = quote(path, safe=SUB_DELIMS + \":/[]@\")\n     # For 'query' we need to drop '#' from the GEN_DELIMS set.\n+    # We also exclude '/' because it is more robust to replace it with a percent\n+    # encoding despite it not being a requirement of the spec and include '%' to\n+    # prevent duplicate encoding of previously quoted items.\n     parsed_query: typing.Optional[str] = (\n-        None if query is None else quote(query, safe=SUB_DELIMS + \":/?[]@\")\n+        None if query is None else quote(query, safe=SUB_DELIMS + \":?[]@%\")",
      "comment": "> I can but then the other change will make the tests fail i.e. this pull request will introduce a incorrect behavior.\r\n\r\nOkay. That would be worthwhile, we can review from there.",
      "comment_id": 1219265789,
      "user": "lovelydinosaur",
      "created_at": "2023-06-06T09:21:42Z",
      "url": "https://github.com/encode/httpx/pull/2723#discussion_r1219265789"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2723,
      "file_path": "httpx/_urlparse.py",
      "line": 266,
      "side": "RIGHT",
      "diff_hunk": "@@ -260,8 +260,10 @@ def urlparse(url: str = \"\", **kwargs: typing.Optional[str]) -> ParseResult:\n     # For 'path' we need to drop ? and # from the GEN_DELIMS set.\n     parsed_path: str = quote(path, safe=SUB_DELIMS + \":/[]@\")\n     # For 'query' we need to drop '#' from the GEN_DELIMS set.\n+    # We also exclude '/' because it is more robust to replace it with a percent\n+    # encoding despite it not being a requirement of the spec.\n     parsed_query: typing.Optional[str] = (\n-        None if query is None else quote(query, safe=SUB_DELIMS + \":/?[]@\")\n+        None if query is None else quote(query, safe=SUB_DELIMS + \":?[]@\")",
      "comment": "This change was unnecessary. The fix was the changes below. We dont need to change an already encoded query to encode slashes. We do need to handle it when passed in as separate parameter.",
      "comment_id": 1414362854,
      "user": "elupus",
      "created_at": "2023-12-04T19:10:14Z",
      "url": "https://github.com/encode/httpx/pull/2723#discussion_r1414362854"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2909,
      "file_path": "httpx/_multipart.py",
      "line": 202,
      "side": "RIGHT",
      "diff_hunk": "@@ -200,7 +199,7 @@ def __init__(\n         boundary: typing.Optional[bytes] = None,\n     ) -> None:\n         if boundary is None:\n-            boundary = binascii.hexlify(os.urandom(16))\n+            boundary = ''.join([f'{byte:02x}' for byte in(os.urandom(16)])",
      "comment": "Oh, I see python 2 user \ud83d\ude04 \r\n```suggestion\r\n            boundary = os.urandom(16).hex()\r\n```",
      "comment_id": 1376184439,
      "user": "T-256",
      "created_at": "2023-10-30T13:03:24Z",
      "url": "https://github.com/encode/httpx/pull/2909#discussion_r1376184439"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2854,
      "file_path": "httpx/_config.py",
      "line": 135,
      "side": "LEFT",
      "diff_hunk": "@@ -128,11 +127,10 @@ def load_ssl_context_verify(self) -> ssl.SSLContext:\n \n         # Signal to server support for PHA in TLS 1.3. Raises an\n         # AttributeError if only read-only access is implemented.\n-        if sys.version_info >= (3, 8):  # pragma: no cover\n-            try:\n-                context.post_handshake_auth = True\n-            except AttributeError:  # pragma: no cover\n-                pass",
      "comment": "Python 3.7 no longer supported https://github.com/encode/httpx/pull/2813",
      "comment_id": 1330177340,
      "user": "T-256",
      "created_at": "2023-09-19T13:58:04Z",
      "url": "https://github.com/encode/httpx/pull/2854#discussion_r1330177340"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2854,
      "file_path": "httpx/_config.py",
      "line": 135,
      "side": "LEFT",
      "diff_hunk": "@@ -128,11 +127,10 @@ def load_ssl_context_verify(self) -> ssl.SSLContext:\n \n         # Signal to server support for PHA in TLS 1.3. Raises an\n         # AttributeError if only read-only access is implemented.\n-        if sys.version_info >= (3, 8):  # pragma: no cover\n-            try:\n-                context.post_handshake_auth = True\n-            except AttributeError:  # pragma: no cover\n-                pass",
      "comment": "Yep, with 3.8+, the condition `sys.version_info >= (3, 8)` is always true.",
      "comment_id": 1330191242,
      "user": "hugovk",
      "created_at": "2023-09-19T14:05:37Z",
      "url": "https://github.com/encode/httpx/pull/2854#discussion_r1330191242"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2852,
      "file_path": "tests/models/test_responses.py",
      "line": 311,
      "side": "RIGHT",
      "diff_hunk": "@@ -298,6 +298,22 @@ def test_response_force_encoding():\n     assert response.encoding == \"iso-8859-1\"\n \n \n+def test_response_force_encoding_after_text_accessed():\n+    response = httpx.Response(\n+        200,\n+        content=b\"Hello, world!\",\n+    )\n+    assert response.status_code == 200\n+    assert response.reason_phrase == \"OK\"\n+    assert response.text == \"Hello, world!\"\n+    assert response.encoding == \"utf-8\"\n+\n+    response.encoding = \"UTF8\"",
      "comment": "I'd expect the `ValueError` to be raised here. I don't think we need the conditional \"don't raise if the encoding is being set but the resulting codec won't change\".\n\nSimplicity over complexity where possible.",
      "comment_id": 1328064919,
      "user": "lovelydinosaur",
      "created_at": "2023-09-17T08:52:20Z",
      "url": "https://github.com/encode/httpx/pull/2852#discussion_r1328064919"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2716,
      "file_path": "httpx/_transports/default.py",
      "line": 31,
      "side": "RIGHT",
      "diff_hunk": "@@ -28,6 +28,7 @@\n from types import TracebackType\n \n import httpcore\n+from httpcore.backends.base import SOCKET_OPTION",
      "comment": "Let's not import that type definition, it's not documented public API.\r\n\r\nIt's be okay to instead define the socket option types here.\r\n\r\n```python\r\nSOCKET_OPTION = typing.Union[\r\n    typing.Tuple[int, int, int],\r\n    typing.Tuple[int, int, typing.Union[bytes, bytearray]],\r\n    typing.Tuple[int, int, None, int],\r\n]\r\n```",
      "comment_id": 1203667142,
      "user": "lovelydinosaur",
      "created_at": "2023-05-24T08:14:28Z",
      "url": "https://github.com/encode/httpx/pull/2716#discussion_r1203667142"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2669,
      "file_path": "tests/test_asgi.py",
      "line": 6,
      "side": "RIGHT",
      "diff_hunk": "@@ -3,6 +3,7 @@\n import pytest\n \n import httpx\n+from httpx._transports.asgi import ASGITransport",
      "comment": "We don't need the import from private API space here.\r\nWe can use `httpx.ASGITransport`, same as the other tests.",
      "comment_id": 1172572391,
      "user": "lovelydinosaur",
      "created_at": "2023-04-20T13:14:57Z",
      "url": "https://github.com/encode/httpx/pull/2669#discussion_r1172572391"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2669,
      "file_path": "tests/test_asgi.py",
      "line": 203,
      "side": "RIGHT",
      "diff_hunk": "@@ -191,3 +192,12 @@ async def read_body(scope, receive, send):\n \n     assert response.status_code == 200\n     assert disconnect\n+\n+\n+@pytest.mark.anyio\n+async def test_asgi_exc_no_raise():\n+    transport = ASGITransport(app=raise_exc, raise_app_exceptions=False)\n+    async with httpx.AsyncClient(app=raise_exc, transport=transport) as client:\n+        response = await client.get(\"http://www.example.org/\")\n+\n+        assert response.status_code == 500",
      "comment": "What's the behaviour of this test before the code change?",
      "comment_id": 1172573305,
      "user": "lovelydinosaur",
      "created_at": "2023-04-20T13:15:40Z",
      "url": "https://github.com/encode/httpx/pull/2669#discussion_r1172573305"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2669,
      "file_path": "tests/test_asgi.py",
      "line": 200,
      "side": "RIGHT",
      "diff_hunk": "@@ -191,3 +192,12 @@ async def read_body(scope, receive, send):\n \n     assert response.status_code == 200\n     assert disconnect\n+\n+\n+@pytest.mark.anyio\n+async def test_asgi_exc_no_raise():\n+    transport = ASGITransport(app=raise_exc, raise_app_exceptions=False)\n+    async with httpx.AsyncClient(app=raise_exc, transport=transport) as client:",
      "comment": "Since we're specifying `transport=...` the `app=...` parameter is redundant.\r\n\r\n```suggestion\r\n    async with httpx.AsyncClient(transport=transport) as client:\r\n```",
      "comment_id": 1172575027,
      "user": "lovelydinosaur",
      "created_at": "2023-04-20T13:16:42Z",
      "url": "https://github.com/encode/httpx/pull/2669#discussion_r1172575027"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2669,
      "file_path": "tests/test_asgi.py",
      "line": 203,
      "side": "RIGHT",
      "diff_hunk": "@@ -191,3 +192,12 @@ async def read_body(scope, receive, send):\n \n     assert response.status_code == 200\n     assert disconnect\n+\n+\n+@pytest.mark.anyio\n+async def test_asgi_exc_no_raise():\n+    transport = ASGITransport(app=raise_exc, raise_app_exceptions=False)\n+    async with httpx.AsyncClient(app=raise_exc, transport=transport) as client:\n+        response = await client.get(\"http://www.example.org/\")\n+\n+        assert response.status_code == 500",
      "comment": "Test failed with `RuntimeError` being raised from `raise_exc`",
      "comment_id": 1172689472,
      "user": "Nnonexistent",
      "created_at": "2023-04-20T14:32:59Z",
      "url": "https://github.com/encode/httpx/pull/2669#discussion_r1172689472"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2680,
      "file_path": "httpx/_utils.py",
      "line": 467,
      "side": "RIGHT",
      "diff_hunk": "@@ -464,14 +464,14 @@ def __eq__(self, other: typing.Any) -> bool:\n def is_ipv4_hostname(hostname: str) -> bool:\n     try:\n         ipaddress.IPv4Address(hostname.split(\"/\")[0])\n-    except:\n+    except ValueError:",
      "comment": "Not sure if an IndexError is required too.\r\nMaybe we can replace it with `except Exception`.",
      "comment_id": 1176504865,
      "user": "aminalaee",
      "created_at": "2023-04-25T13:16:56Z",
      "url": "https://github.com/encode/httpx/pull/2680#discussion_r1176504865"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2680,
      "file_path": "httpx/_utils.py",
      "line": 467,
      "side": "RIGHT",
      "diff_hunk": "@@ -464,14 +464,14 @@ def __eq__(self, other: typing.Any) -> bool:\n def is_ipv4_hostname(hostname: str) -> bool:\n     try:\n         ipaddress.IPv4Address(hostname.split(\"/\")[0])\n-    except:\n+    except ValueError:",
      "comment": "Technically speaking `except Exception` would guarantee the same behaviour?",
      "comment_id": 1176696378,
      "user": "michaeloliverx",
      "created_at": "2023-04-25T15:32:49Z",
      "url": "https://github.com/encode/httpx/pull/2680#discussion_r1176696378"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2680,
      "file_path": "httpx/_utils.py",
      "line": 467,
      "side": "RIGHT",
      "diff_hunk": "@@ -464,14 +464,14 @@ def __eq__(self, other: typing.Any) -> bool:\n def is_ipv4_hostname(hostname: str) -> bool:\n     try:\n         ipaddress.IPv4Address(hostname.split(\"/\")[0])\n-    except:\n+    except ValueError:",
      "comment": "Yeah, and the only reason we allowed this was because of the ruff issue. Flake8 would have caught this too.",
      "comment_id": 1176767295,
      "user": "aminalaee",
      "created_at": "2023-04-25T16:29:46Z",
      "url": "https://github.com/encode/httpx/pull/2680#discussion_r1176767295"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2570,
      "file_path": "tests/test_decoders.py",
      "line": 335,
      "side": "LEFT",
      "diff_hunk": "@@ -214,125 +213,55 @@ async def iterator() -> typing.AsyncIterator[bytes]:\n \n \n def test_text_decoder_empty_cases():\n-    decoder = TextDecoder()\n-    assert decoder.flush() == \"\"\n+    response = httpx.Response(200, content=b\"\")\n+    assert response.text == \"\"\n \n-    decoder = TextDecoder()\n-    assert decoder.decode(b\"\") == \"\"\n-    assert decoder.flush() == \"\"\n+    response = httpx.Response(200, content=[b\"\"])\n+    response.read()\n+    assert response.text == \"\"\n \n \n def test_line_decoder_nl():\n-    decoder = LineDecoder()\n-    assert decoder.decode(\"\") == []\n-    assert decoder.decode(\"a\\n\\nb\\nc\") == [\"a\\n\", \"\\n\", \"b\\n\"]\n-    assert decoder.flush() == [\"c\"]\n+    response = httpx.Response(200, content=[b\"\"])\n+    assert list(response.iter_lines()) == []\n \n-    decoder = LineDecoder()\n-    assert decoder.decode(\"\") == []\n-    assert decoder.decode(\"a\\n\\nb\\nc\\n\") == [\"a\\n\", \"\\n\", \"b\\n\", \"c\\n\"]\n-    assert decoder.flush() == []\n+    response = httpx.Response(200, content=[b\"\", b\"a\\n\\nb\\nc\"])\n+    assert list(response.iter_lines()) == [\"a\\n\", \"\\n\", \"b\\n\", \"c\"]\n \n     # Issue #1033\n-    decoder = LineDecoder()\n-    assert decoder.decode(\"\") == []\n-    assert decoder.decode(\"12345\\n\") == [\"12345\\n\"]\n-    assert decoder.decode(\"foo \") == []\n-    assert decoder.decode(\"bar \") == []\n-    assert decoder.decode(\"baz\\n\") == [\"foo bar baz\\n\"]\n-    assert decoder.flush() == []\n+    response = httpx.Response(\n+        200, content=[b\"\", b\"12345\\n\", b\"foo \", b\"bar \", b\"baz\\n\"]\n+    )\n+    assert list(response.iter_lines()) == [\"12345\\n\", \"foo bar baz\\n\"]\n \n \n def test_line_decoder_cr():\n-    decoder = LineDecoder()\n-    assert decoder.decode(\"\") == []\n-    assert decoder.decode(\"a\\r\\rb\\rc\") == [\"a\\n\", \"\\n\", \"b\\n\"]\n-    assert decoder.flush() == [\"c\"]\n+    response = httpx.Response(200, content=[b\"\", b\"a\\r\\rb\\rc\"])\n+    assert list(response.iter_lines()) == [\"a\\n\", \"\\n\", \"b\\n\", \"c\"]\n \n-    decoder = LineDecoder()\n-    assert decoder.decode(\"\") == []\n-    assert decoder.decode(\"a\\r\\rb\\rc\\r\") == [\"a\\n\", \"\\n\", \"b\\n\"]\n-    assert decoder.flush() == [\"c\\n\"]\n+    response = httpx.Response(200, content=[b\"\", b\"a\\r\\rb\\rc\\r\"])\n+    assert list(response.iter_lines()) == [\"a\\n\", \"\\n\", \"b\\n\", \"c\\n\"]\n \n     # Issue #1033\n-    decoder = LineDecoder()\n-    assert decoder.decode(\"\") == []\n-    assert decoder.decode(\"12345\\r\") == []\n-    assert decoder.decode(\"foo \") == [\"12345\\n\"]\n-    assert decoder.decode(\"bar \") == []\n-    assert decoder.decode(\"baz\\r\") == []\n-    assert decoder.flush() == [\"foo bar baz\\n\"]\n+    response = httpx.Response(\n+        200, content=[b\"\", b\"12345\\r\", b\"foo \", b\"bar \", b\"baz\\r\"]\n+    )\n+    assert list(response.iter_lines()) == [\"12345\\n\", \"foo bar baz\\n\"]\n \n \n def test_line_decoder_crnl():\n-    decoder = LineDecoder()\n-    assert decoder.decode(\"\") == []\n-    assert decoder.decode(\"a\\r\\n\\r\\nb\\r\\nc\") == [\"a\\n\", \"\\n\", \"b\\n\"]\n-    assert decoder.flush() == [\"c\"]\n-\n-    decoder = LineDecoder()\n-    assert decoder.decode(\"\") == []\n-    assert decoder.decode(\"a\\r\\n\\r\\nb\\r\\nc\\r\\n\") == [\"a\\n\", \"\\n\", \"b\\n\", \"c\\n\"]\n-    assert decoder.flush() == []\n-\n-    decoder = LineDecoder()\n-    assert decoder.decode(\"\") == []\n-    assert decoder.decode(\"a\\r\") == []\n-    assert decoder.decode(\"\\n\\r\\nb\\r\\nc\") == [\"a\\n\", \"\\n\", \"b\\n\"]\n-    assert decoder.flush() == [\"c\"]\n+    response = httpx.Response(200, content=[b\"\", b\"a\\r\\n\\r\\nb\\r\\nc\"])\n+    assert list(response.iter_lines()) == [\"a\\n\", \"\\n\", \"b\\n\", \"c\"]\n+\n+    response = httpx.Response(200, content=[b\"\", b\"a\\r\\n\\r\\nb\\r\\nc\\r\\n\"])\n+    assert list(response.iter_lines()) == [\"a\\n\", \"\\n\", \"b\\n\", \"c\\n\"]\n+\n+    response = httpx.Response(200, content=[b\"\", b\"a\\r\", b\"\\n\\r\\nb\\r\\nc\"])\n+    assert list(response.iter_lines()) == [\"a\\n\", \"\\n\", \"b\\n\", \"c\"]\n \n     # Issue #1033\n-    decoder = LineDecoder()\n-    assert decoder.decode(\"\") == []\n-    assert decoder.decode(\"12345\\r\\n\") == [\"12345\\n\"]\n-    assert decoder.decode(\"foo \") == []\n-    assert decoder.decode(\"bar \") == []\n-    assert decoder.decode(\"baz\\r\\n\") == [\"foo bar baz\\n\"]\n-    assert decoder.flush() == []\n-\n-\n-def test_byte_chunker():\n-    decoder = ByteChunker()\n-    assert decoder.decode(b\"1234567\") == [b\"1234567\"]\n-    assert decoder.decode(b\"89\") == [b\"89\"]\n-    assert decoder.flush() == []\n-\n-    decoder = ByteChunker(chunk_size=3)\n-    assert decoder.decode(b\"1234567\") == [b\"123\", b\"456\"]\n-    assert decoder.decode(b\"89\") == [b\"789\"]\n-    assert decoder.flush() == []\n-\n-    decoder = ByteChunker(chunk_size=3)\n-    assert decoder.decode(b\"123456\") == [b\"123\", b\"456\"]\n-    assert decoder.decode(b\"789\") == [b\"789\"]\n-    assert decoder.flush() == []\n-\n-    decoder = ByteChunker(chunk_size=3)\n-    assert decoder.decode(b\"123456\") == [b\"123\", b\"456\"]\n-    assert decoder.decode(b\"78\") == []\n-    assert decoder.flush() == [b\"78\"]\n-\n-\n-def test_text_chunker():\n-    decoder = TextChunker()\n-    assert decoder.decode(\"1234567\") == [\"1234567\"]\n-    assert decoder.decode(\"89\") == [\"89\"]\n-    assert decoder.flush() == []\n-\n-    decoder = TextChunker(chunk_size=3)\n-    assert decoder.decode(\"1234567\") == [\"123\", \"456\"]\n-    assert decoder.decode(\"89\") == [\"789\"]\n-    assert decoder.flush() == []\n-\n-    decoder = TextChunker(chunk_size=3)\n-    assert decoder.decode(\"123456\") == [\"123\", \"456\"]\n-    assert decoder.decode(\"789\") == [\"789\"]\n-    assert decoder.flush() == []\n-\n-    decoder = TextChunker(chunk_size=3)\n-    assert decoder.decode(\"123456\") == [\"123\", \"456\"]\n-    assert decoder.decode(\"78\") == []\n-    assert decoder.flush() == [\"78\"]",
      "comment": "Most of these cases we actually already well covered in `test_responses.py`, so I figured I'd expand those a bit to match coverage, and drop these.",
      "comment_id": 1096597170,
      "user": "florimondmanca",
      "created_at": "2023-02-04T23:07:37Z",
      "url": "https://github.com/encode/httpx/pull/2570#discussion_r1096597170"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2648,
      "file_path": "tests/client/test_async_client.py",
      "line": 87,
      "side": "RIGHT",
      "diff_hunk": "@@ -84,7 +84,7 @@ async def test_access_content_stream_response(server):\n \n     assert response.status_code == 200\n     with pytest.raises(httpx.ResponseNotRead):\n-        response.content\n+        response.content  # noqa: B018",
      "comment": "It's on the rules page: https://beta.ruff.rs/docs/rules/#flake8-bugbear-b\r\n\r\nShould I add a comment here or ignore it globally?",
      "comment_id": 1158258821,
      "user": "Kludex",
      "created_at": "2023-04-05T09:22:34Z",
      "url": "https://github.com/encode/httpx/pull/2648#discussion_r1158258821"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2547,
      "file_path": "httpx/_client.py",
      "line": 1013,
      "side": "RIGHT",
      "diff_hunk": "@@ -1010,10 +1010,13 @@ def _send_single_request(self, request: Request) -> Response:\n         self.cookies.extract_cookies(response)\n         response.default_encoding = self._default_encoding\n \n-        status = f\"{response.status_code} {response.reason_phrase}\"\n-        response_line = f\"{response.http_version} {status}\"\n-        logger.debug(\n-            'HTTP Request: %s %s \"%s\"', request.method, request.url, response_line\n+        logger.info(",
      "comment": "Based on the other python packages, I don't recall any other python client having an INFO log message level. :thinking: ",
      "comment_id": 1141706645,
      "user": "Kludex",
      "created_at": "2023-03-20T07:17:57Z",
      "url": "https://github.com/encode/httpx/pull/2547#discussion_r1141706645"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2547,
      "file_path": "httpx/_client.py",
      "line": 1013,
      "side": "RIGHT",
      "diff_hunk": "@@ -1010,10 +1010,13 @@ def _send_single_request(self, request: Request) -> Response:\n         self.cookies.extract_cookies(response)\n         response.default_encoding = self._default_encoding\n \n-        status = f\"{response.status_code} {response.reason_phrase}\"\n-        response_line = f\"{response.http_version} {status}\"\n-        logger.debug(\n-            'HTTP Request: %s %s \"%s\"', request.method, request.url, response_line\n+        logger.info(",
      "comment": "It could yes, though it's less useful to have logging if everything is at the same level.\r\nI like the different \"show me nothing\", \"show me the requests\", \"show me the debug\" lighting levels.\r\n\r\n(Similar: gunicorn, uvicorn using INFO for requests, and DEBUG for other stuffs)",
      "comment_id": 1141930995,
      "user": "lovelydinosaur",
      "created_at": "2023-03-20T10:45:16Z",
      "url": "https://github.com/encode/httpx/pull/2547#discussion_r1141930995"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2423,
      "file_path": "tests/models/test_responses.py",
      "line": 642,
      "side": "RIGHT",
      "diff_hunk": "@@ -639,7 +639,7 @@ def test_iter_lines():\n         content=b\"Hello,\\nworld!\",\n     )\n     content = [line for line in response.iter_lines()]\n-    assert content == [\"Hello,\\n\", \"world!\"]\n+    assert content == [\"Hello,\", \"world!\"]",
      "comment": "These test changes show that currently this PR introduces a behavior change. I assume we don't want behavior to change, i.e. for `\\n` to stay in the yielded lines? Does that mean relying on `splitlines(keep_ends=True)`... ?",
      "comment_id": 1014794251,
      "user": "florimondmanca",
      "created_at": "2022-11-06T09:13:57Z",
      "url": "https://github.com/encode/httpx/pull/2423#discussion_r1014794251"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2423,
      "file_path": "tests/models/test_responses.py",
      "line": 642,
      "side": "RIGHT",
      "diff_hunk": "@@ -639,7 +639,7 @@ def test_iter_lines():\n         content=b\"Hello,\\nworld!\",\n     )\n     content = [line for line in response.iter_lines()]\n-    assert content == [\"Hello,\\n\", \"world!\"]\n+    assert content == [\"Hello,\", \"world!\"]",
      "comment": "Oh, I didn't realise there was a `keepends` flag, let me rework the patch with that. ",
      "comment_id": 1027299829,
      "user": "giannitedesco",
      "created_at": "2022-11-20T14:30:48Z",
      "url": "https://github.com/encode/httpx/pull/2423#discussion_r1027299829"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2423,
      "file_path": "tests/models/test_responses.py",
      "line": 642,
      "side": "RIGHT",
      "diff_hunk": "@@ -639,7 +639,7 @@ def test_iter_lines():\n         content=b\"Hello,\\nworld!\",\n     )\n     content = [line for line in response.iter_lines()]\n-    assert content == [\"Hello,\\n\", \"world!\"]\n+    assert content == [\"Hello,\", \"world!\"]",
      "comment": "Right, the issue is that `keepends` ~converts~ doesn't convert them all to `\\n`",
      "comment_id": 1027510876,
      "user": "giannitedesco",
      "created_at": "2022-11-21T03:23:35Z",
      "url": "https://github.com/encode/httpx/pull/2423#discussion_r1027510876"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2423,
      "file_path": "httpx/_decoders.py",
      "line": 286,
      "side": "RIGHT",
      "diff_hunk": "@@ -267,57 +267,23 @@ def __init__(self) -> None:\n         self.buffer = \"\"\n \n     def decode(self, text: str) -> typing.List[str]:\n-        lines = []\n-\n-        if text and self.buffer and self.buffer[-1] == \"\\r\":\n-            if text.startswith(\"\\n\"):\n-                # Handle the case where we have an \"\\r\\n\" split across\n-                # our previous input, and our new chunk.\n-                lines.append(self.buffer[:-1] + \"\\n\")\n-                self.buffer = \"\"\n-                text = text[1:]\n-            else:\n-                # Handle the case where we have \"\\r\" at the end of our\n-                # previous input.\n-                lines.append(self.buffer[:-1] + \"\\n\")\n-                self.buffer = \"\"\n-\n-        while text:\n-            num_chars = len(text)\n-            for idx in range(num_chars):\n-                char = text[idx]\n-                next_char = None if idx + 1 == num_chars else text[idx + 1]\n-                if char == \"\\n\":\n-                    lines.append(self.buffer + text[: idx + 1])\n-                    self.buffer = \"\"\n-                    text = text[idx + 1 :]\n-                    break\n-                elif char == \"\\r\" and next_char == \"\\n\":\n-                    lines.append(self.buffer + text[:idx] + \"\\n\")\n-                    self.buffer = \"\"\n-                    text = text[idx + 2 :]\n-                    break\n-                elif char == \"\\r\" and next_char is not None:\n-                    lines.append(self.buffer + text[:idx] + \"\\n\")\n-                    self.buffer = \"\"\n-                    text = text[idx + 1 :]\n-                    break\n-                elif next_char is None:\n-                    self.buffer += text\n-                    text = \"\"\n-                    break\n+        if self.buffer:\n+            text = self.buffer + text\n+\n+        if not text:\n+            return []\n+\n+        lines = text.splitlines(True)\n+        if text.endswith(\"\\n\"):\n+            self.buffer = \"\"\n+        else:\n+            remainder = lines.pop()\n+            self.buffer = remainder\n \n         return lines\n \n     def flush(self) -> typing.List[str]:\n-        if self.buffer.endswith(\"\\r\"):\n-            # Handle the case where we had a trailing '\\r', which could have\n-            # been a '\\r\\n' pair.\n-            lines = [self.buffer[:-1] + \"\\n\"]\n-        elif self.buffer:\n-            lines = [self.buffer]\n-        else:\n-            lines = []\n+        lines = self.buffer.splitlines(True)",
      "comment": "I guess we'll want to drop the `True` here, given review comments.",
      "comment_id": 1044351555,
      "user": "lovelydinosaur",
      "created_at": "2022-12-09T11:18:54Z",
      "url": "https://github.com/encode/httpx/pull/2423#discussion_r1044351555"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2423,
      "file_path": "httpx/_decoders.py",
      "line": 276,
      "side": "RIGHT",
      "diff_hunk": "@@ -267,57 +267,23 @@ def __init__(self) -> None:\n         self.buffer = \"\"\n \n     def decode(self, text: str) -> typing.List[str]:\n-        lines = []\n-\n-        if text and self.buffer and self.buffer[-1] == \"\\r\":\n-            if text.startswith(\"\\n\"):\n-                # Handle the case where we have an \"\\r\\n\" split across\n-                # our previous input, and our new chunk.\n-                lines.append(self.buffer[:-1] + \"\\n\")\n-                self.buffer = \"\"\n-                text = text[1:]\n-            else:\n-                # Handle the case where we have \"\\r\" at the end of our\n-                # previous input.\n-                lines.append(self.buffer[:-1] + \"\\n\")\n-                self.buffer = \"\"\n-\n-        while text:\n-            num_chars = len(text)\n-            for idx in range(num_chars):\n-                char = text[idx]\n-                next_char = None if idx + 1 == num_chars else text[idx + 1]\n-                if char == \"\\n\":\n-                    lines.append(self.buffer + text[: idx + 1])\n-                    self.buffer = \"\"\n-                    text = text[idx + 1 :]\n-                    break\n-                elif char == \"\\r\" and next_char == \"\\n\":\n-                    lines.append(self.buffer + text[:idx] + \"\\n\")\n-                    self.buffer = \"\"\n-                    text = text[idx + 2 :]\n-                    break\n-                elif char == \"\\r\" and next_char is not None:\n-                    lines.append(self.buffer + text[:idx] + \"\\n\")\n-                    self.buffer = \"\"\n-                    text = text[idx + 1 :]\n-                    break\n-                elif next_char is None:\n-                    self.buffer += text\n-                    text = \"\"\n-                    break\n+        if self.buffer:\n+            text = self.buffer + text\n+\n+        if not text:\n+            return []\n+\n+        lines = text.splitlines(True)",
      "comment": "I guess we'll want to drop the `True` here, given review comments.",
      "comment_id": 1044351913,
      "user": "lovelydinosaur",
      "created_at": "2022-12-09T11:19:22Z",
      "url": "https://github.com/encode/httpx/pull/2423#discussion_r1044351913"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2423,
      "file_path": "httpx/_decoders.py",
      "line": 277,
      "side": "RIGHT",
      "diff_hunk": "@@ -267,57 +267,23 @@ def __init__(self) -> None:\n         self.buffer = \"\"\n \n     def decode(self, text: str) -> typing.List[str]:\n-        lines = []\n-\n-        if text and self.buffer and self.buffer[-1] == \"\\r\":\n-            if text.startswith(\"\\n\"):\n-                # Handle the case where we have an \"\\r\\n\" split across\n-                # our previous input, and our new chunk.\n-                lines.append(self.buffer[:-1] + \"\\n\")\n-                self.buffer = \"\"\n-                text = text[1:]\n-            else:\n-                # Handle the case where we have \"\\r\" at the end of our\n-                # previous input.\n-                lines.append(self.buffer[:-1] + \"\\n\")\n-                self.buffer = \"\"\n-\n-        while text:\n-            num_chars = len(text)\n-            for idx in range(num_chars):\n-                char = text[idx]\n-                next_char = None if idx + 1 == num_chars else text[idx + 1]\n-                if char == \"\\n\":\n-                    lines.append(self.buffer + text[: idx + 1])\n-                    self.buffer = \"\"\n-                    text = text[idx + 1 :]\n-                    break\n-                elif char == \"\\r\" and next_char == \"\\n\":\n-                    lines.append(self.buffer + text[:idx] + \"\\n\")\n-                    self.buffer = \"\"\n-                    text = text[idx + 2 :]\n-                    break\n-                elif char == \"\\r\" and next_char is not None:\n-                    lines.append(self.buffer + text[:idx] + \"\\n\")\n-                    self.buffer = \"\"\n-                    text = text[idx + 1 :]\n-                    break\n-                elif next_char is None:\n-                    self.buffer += text\n-                    text = \"\"\n-                    break\n+        if self.buffer:\n+            text = self.buffer + text\n+\n+        if not text:\n+            return []\n+\n+        lines = text.splitlines(True)\n+        if text.endswith(\"\\n\"):",
      "comment": "I *think* this would then be something like...\r\n\r\n```python\r\nlastline = lines[-1]\r\nif lastline and text and lastline[-1] != text[-1]:\r\n    # The final line ends with a different character to the input\r\n    # text, so it must have ended in a newline.\r\n    ...\r\nelse:\r\n    #\u00a0The final line *didn't* end with a newline, so push it back\r\n    # into the buffer.\r\n    ...\r\n```",
      "comment_id": 1044355428,
      "user": "lovelydinosaur",
      "created_at": "2022-12-09T11:24:02Z",
      "url": "https://github.com/encode/httpx/pull/2423#discussion_r1044355428"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2423,
      "file_path": "httpx/_decoders.py",
      "line": 272,
      "side": "RIGHT",
      "diff_hunk": "@@ -266,57 +266,23 @@ def __init__(self) -> None:\n         self.buffer = \"\"\n \n     def decode(self, text: str) -> typing.List[str]:\n-        lines = []\n-\n-        if text and self.buffer and self.buffer[-1] == \"\\r\":\n-            if text.startswith(\"\\n\"):\n-                # Handle the case where we have an \"\\r\\n\" split across\n-                # our previous input, and our new chunk.\n-                lines.append(self.buffer[:-1] + \"\\n\")\n-                self.buffer = \"\"\n-                text = text[1:]\n-            else:\n-                # Handle the case where we have \"\\r\" at the end of our\n-                # previous input.\n-                lines.append(self.buffer[:-1] + \"\\n\")\n-                self.buffer = \"\"\n-\n-        while text:\n-            num_chars = len(text)\n-            for idx in range(num_chars):\n-                char = text[idx]\n-                next_char = None if idx + 1 == num_chars else text[idx + 1]\n-                if char == \"\\n\":\n-                    lines.append(self.buffer + text[: idx + 1])\n-                    self.buffer = \"\"\n-                    text = text[idx + 1 :]\n-                    break\n-                elif char == \"\\r\" and next_char == \"\\n\":\n-                    lines.append(self.buffer + text[:idx] + \"\\n\")\n-                    self.buffer = \"\"\n-                    text = text[idx + 2 :]\n-                    break\n-                elif char == \"\\r\" and next_char is not None:\n-                    lines.append(self.buffer + text[:idx] + \"\\n\")\n-                    self.buffer = \"\"\n-                    text = text[idx + 1 :]\n-                    break\n-                elif next_char is None:\n-                    self.buffer += text\n-                    text = \"\"\n-                    break\n+        if self.buffer:\n+            text = self.buffer + text\n+\n+        if text.endswith(\"\\r\"):",
      "comment": "`splitlines()` will teat a trailing `\"\\r\"` as a newline because it is designed around the assumption that that input is the whole string.. in the case of the line decoder, when a trailing `\"\\r\"` is received, we need to buffer it up in case the next character in the input stream is an `\"\\n\"` (ie. it was part of an interrupted `\"\\r\\n\"` sequence)",
      "comment_id": 1045052360,
      "user": "giannitedesco",
      "created_at": "2022-12-10T10:10:21Z",
      "url": "https://github.com/encode/httpx/pull/2423#discussion_r1045052360"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2423,
      "file_path": "httpx/_decoders.py",
      "line": 272,
      "side": "RIGHT",
      "diff_hunk": "@@ -266,57 +266,23 @@ def __init__(self) -> None:\n         self.buffer = \"\"\n \n     def decode(self, text: str) -> typing.List[str]:\n-        lines = []\n-\n-        if text and self.buffer and self.buffer[-1] == \"\\r\":\n-            if text.startswith(\"\\n\"):\n-                # Handle the case where we have an \"\\r\\n\" split across\n-                # our previous input, and our new chunk.\n-                lines.append(self.buffer[:-1] + \"\\n\")\n-                self.buffer = \"\"\n-                text = text[1:]\n-            else:\n-                # Handle the case where we have \"\\r\" at the end of our\n-                # previous input.\n-                lines.append(self.buffer[:-1] + \"\\n\")\n-                self.buffer = \"\"\n-\n-        while text:\n-            num_chars = len(text)\n-            for idx in range(num_chars):\n-                char = text[idx]\n-                next_char = None if idx + 1 == num_chars else text[idx + 1]\n-                if char == \"\\n\":\n-                    lines.append(self.buffer + text[: idx + 1])\n-                    self.buffer = \"\"\n-                    text = text[idx + 1 :]\n-                    break\n-                elif char == \"\\r\" and next_char == \"\\n\":\n-                    lines.append(self.buffer + text[:idx] + \"\\n\")\n-                    self.buffer = \"\"\n-                    text = text[idx + 2 :]\n-                    break\n-                elif char == \"\\r\" and next_char is not None:\n-                    lines.append(self.buffer + text[:idx] + \"\\n\")\n-                    self.buffer = \"\"\n-                    text = text[idx + 1 :]\n-                    break\n-                elif next_char is None:\n-                    self.buffer += text\n-                    text = \"\"\n-                    break\n+        if self.buffer:\n+            text = self.buffer + text\n+\n+        if text.endswith(\"\\r\"):",
      "comment": "well, if we go with this version then it needs a comment :)",
      "comment_id": 1045052847,
      "user": "giannitedesco",
      "created_at": "2022-12-10T10:11:49Z",
      "url": "https://github.com/encode/httpx/pull/2423#discussion_r1045052847"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2423,
      "file_path": "httpx/_decoders.py",
      "line": 278,
      "side": "RIGHT",
      "diff_hunk": "@@ -267,57 +267,23 @@ def __init__(self) -> None:\n         self.buffer = \"\"\n \n     def decode(self, text: str) -> typing.List[str]:\n-        lines = []\n-\n-        if text and self.buffer and self.buffer[-1] == \"\\r\":\n-            if text.startswith(\"\\n\"):\n-                # Handle the case where we have an \"\\r\\n\" split across\n-                # our previous input, and our new chunk.\n-                lines.append(self.buffer[:-1] + \"\\n\")\n-                self.buffer = \"\"\n-                text = text[1:]\n-            else:\n-                # Handle the case where we have \"\\r\" at the end of our\n-                # previous input.\n-                lines.append(self.buffer[:-1] + \"\\n\")\n-                self.buffer = \"\"\n-\n-        while text:\n-            num_chars = len(text)\n-            for idx in range(num_chars):\n-                char = text[idx]\n-                next_char = None if idx + 1 == num_chars else text[idx + 1]\n-                if char == \"\\n\":\n-                    lines.append(self.buffer + text[: idx + 1])\n-                    self.buffer = \"\"\n-                    text = text[idx + 1 :]\n-                    break\n-                elif char == \"\\r\" and next_char == \"\\n\":\n-                    lines.append(self.buffer + text[:idx] + \"\\n\")\n-                    self.buffer = \"\"\n-                    text = text[idx + 2 :]\n-                    break\n-                elif char == \"\\r\" and next_char is not None:\n-                    lines.append(self.buffer + text[:idx] + \"\\n\")\n-                    self.buffer = \"\"\n-                    text = text[idx + 1 :]\n-                    break\n-                elif next_char is None:\n-                    self.buffer += text\n-                    text = \"\"\n-                    break\n+        if self.buffer:\n+            text = self.buffer + text\n+\n+        if text.endswith(\"\\r\"):\n+            self.buffer = text\n+            return []\n+\n+        lines = text.splitlines()\n+        if text.endswith(\"\\n\") or not lines:",
      "comment": "The `if text.endswith(\"\\n\")` doesn't seems sufficient to me.\r\n\r\nOther newline endings [are also available](https://docs.python.org/3/library/stdtypes.html#str.splitlines).\r\n\r\nI'd assume that our test cases aren't covering this sufficiently.",
      "comment_id": 1045653671,
      "user": "lovelydinosaur",
      "created_at": "2022-12-12T10:32:26Z",
      "url": "https://github.com/encode/httpx/pull/2423#discussion_r1045653671"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2423,
      "file_path": "httpx/_decoders.py",
      "line": 272,
      "side": "RIGHT",
      "diff_hunk": "@@ -266,57 +266,23 @@ def __init__(self) -> None:\n         self.buffer = \"\"\n \n     def decode(self, text: str) -> typing.List[str]:\n-        lines = []\n-\n-        if text and self.buffer and self.buffer[-1] == \"\\r\":\n-            if text.startswith(\"\\n\"):\n-                # Handle the case where we have an \"\\r\\n\" split across\n-                # our previous input, and our new chunk.\n-                lines.append(self.buffer[:-1] + \"\\n\")\n-                self.buffer = \"\"\n-                text = text[1:]\n-            else:\n-                # Handle the case where we have \"\\r\" at the end of our\n-                # previous input.\n-                lines.append(self.buffer[:-1] + \"\\n\")\n-                self.buffer = \"\"\n-\n-        while text:\n-            num_chars = len(text)\n-            for idx in range(num_chars):\n-                char = text[idx]\n-                next_char = None if idx + 1 == num_chars else text[idx + 1]\n-                if char == \"\\n\":\n-                    lines.append(self.buffer + text[: idx + 1])\n-                    self.buffer = \"\"\n-                    text = text[idx + 1 :]\n-                    break\n-                elif char == \"\\r\" and next_char == \"\\n\":\n-                    lines.append(self.buffer + text[:idx] + \"\\n\")\n-                    self.buffer = \"\"\n-                    text = text[idx + 2 :]\n-                    break\n-                elif char == \"\\r\" and next_char is not None:\n-                    lines.append(self.buffer + text[:idx] + \"\\n\")\n-                    self.buffer = \"\"\n-                    text = text[idx + 1 :]\n-                    break\n-                elif next_char is None:\n-                    self.buffer += text\n-                    text = \"\"\n-                    break\n+        if self.buffer:\n+            text = self.buffer + text\n+\n+        if text.endswith(\"\\r\"):",
      "comment": "Gotcha. Yes a comment would be good. I assume that `\"\\r\\n\"` is the *only* two-character newline sequence, right?",
      "comment_id": 1045679676,
      "user": "lovelydinosaur",
      "created_at": "2022-12-12T10:57:22Z",
      "url": "https://github.com/encode/httpx/pull/2423#discussion_r1045679676"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2423,
      "file_path": "httpx/_decoders.py",
      "line": 278,
      "side": "RIGHT",
      "diff_hunk": "@@ -267,57 +267,23 @@ def __init__(self) -> None:\n         self.buffer = \"\"\n \n     def decode(self, text: str) -> typing.List[str]:\n-        lines = []\n-\n-        if text and self.buffer and self.buffer[-1] == \"\\r\":\n-            if text.startswith(\"\\n\"):\n-                # Handle the case where we have an \"\\r\\n\" split across\n-                # our previous input, and our new chunk.\n-                lines.append(self.buffer[:-1] + \"\\n\")\n-                self.buffer = \"\"\n-                text = text[1:]\n-            else:\n-                # Handle the case where we have \"\\r\" at the end of our\n-                # previous input.\n-                lines.append(self.buffer[:-1] + \"\\n\")\n-                self.buffer = \"\"\n-\n-        while text:\n-            num_chars = len(text)\n-            for idx in range(num_chars):\n-                char = text[idx]\n-                next_char = None if idx + 1 == num_chars else text[idx + 1]\n-                if char == \"\\n\":\n-                    lines.append(self.buffer + text[: idx + 1])\n-                    self.buffer = \"\"\n-                    text = text[idx + 1 :]\n-                    break\n-                elif char == \"\\r\" and next_char == \"\\n\":\n-                    lines.append(self.buffer + text[:idx] + \"\\n\")\n-                    self.buffer = \"\"\n-                    text = text[idx + 2 :]\n-                    break\n-                elif char == \"\\r\" and next_char is not None:\n-                    lines.append(self.buffer + text[:idx] + \"\\n\")\n-                    self.buffer = \"\"\n-                    text = text[idx + 1 :]\n-                    break\n-                elif next_char is None:\n-                    self.buffer += text\n-                    text = \"\"\n-                    break\n+        if self.buffer:\n+            text = self.buffer + text\n+\n+        if text.endswith(\"\\r\"):\n+            self.buffer = text\n+            return []\n+\n+        lines = text.splitlines()\n+        if text.endswith(\"\\n\") or not lines:",
      "comment": "The behaviour when using any of the other \"funny money\" line-endings is not really incorrect output, but that trailing lines will be delayed until the subsequent call or until finalization when one or two lines will be returned.",
      "comment_id": 1045694536,
      "user": "giannitedesco",
      "created_at": "2022-12-12T11:13:20Z",
      "url": "https://github.com/encode/httpx/pull/2423#discussion_r1045694536"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2423,
      "file_path": "httpx/_decoders.py",
      "line": 278,
      "side": "RIGHT",
      "diff_hunk": "@@ -267,57 +267,23 @@ def __init__(self) -> None:\n         self.buffer = \"\"\n \n     def decode(self, text: str) -> typing.List[str]:\n-        lines = []\n-\n-        if text and self.buffer and self.buffer[-1] == \"\\r\":\n-            if text.startswith(\"\\n\"):\n-                # Handle the case where we have an \"\\r\\n\" split across\n-                # our previous input, and our new chunk.\n-                lines.append(self.buffer[:-1] + \"\\n\")\n-                self.buffer = \"\"\n-                text = text[1:]\n-            else:\n-                # Handle the case where we have \"\\r\" at the end of our\n-                # previous input.\n-                lines.append(self.buffer[:-1] + \"\\n\")\n-                self.buffer = \"\"\n-\n-        while text:\n-            num_chars = len(text)\n-            for idx in range(num_chars):\n-                char = text[idx]\n-                next_char = None if idx + 1 == num_chars else text[idx + 1]\n-                if char == \"\\n\":\n-                    lines.append(self.buffer + text[: idx + 1])\n-                    self.buffer = \"\"\n-                    text = text[idx + 1 :]\n-                    break\n-                elif char == \"\\r\" and next_char == \"\\n\":\n-                    lines.append(self.buffer + text[:idx] + \"\\n\")\n-                    self.buffer = \"\"\n-                    text = text[idx + 2 :]\n-                    break\n-                elif char == \"\\r\" and next_char is not None:\n-                    lines.append(self.buffer + text[:idx] + \"\\n\")\n-                    self.buffer = \"\"\n-                    text = text[idx + 1 :]\n-                    break\n-                elif next_char is None:\n-                    self.buffer += text\n-                    text = \"\"\n-                    break\n+        if self.buffer:\n+            text = self.buffer + text\n+\n+        if text.endswith(\"\\r\"):\n+            self.buffer = text\n+            return []\n+\n+        lines = text.splitlines()\n+        if text.endswith(\"\\n\") or not lines:",
      "comment": "Well, unless you consider splitting on those other line endings is, itself, as a bug/unintended. (I had no idea, fwiw)",
      "comment_id": 1045695695,
      "user": "giannitedesco",
      "created_at": "2022-12-12T11:14:38Z",
      "url": "https://github.com/encode/httpx/pull/2423#discussion_r1045695695"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2423,
      "file_path": "httpx/_decoders.py",
      "line": 283,
      "side": "RIGHT",
      "diff_hunk": "@@ -267,57 +267,23 @@ def __init__(self) -> None:\n         self.buffer = \"\"\n \n     def decode(self, text: str) -> typing.List[str]:\n-        lines = []\n-\n-        if text and self.buffer and self.buffer[-1] == \"\\r\":\n-            if text.startswith(\"\\n\"):\n-                # Handle the case where we have an \"\\r\\n\" split across\n-                # our previous input, and our new chunk.\n-                lines.append(self.buffer[:-1] + \"\\n\")\n-                self.buffer = \"\"\n-                text = text[1:]\n-            else:\n-                # Handle the case where we have \"\\r\" at the end of our\n-                # previous input.\n-                lines.append(self.buffer[:-1] + \"\\n\")\n-                self.buffer = \"\"\n-\n-        while text:\n-            num_chars = len(text)\n-            for idx in range(num_chars):\n-                char = text[idx]\n-                next_char = None if idx + 1 == num_chars else text[idx + 1]\n-                if char == \"\\n\":\n-                    lines.append(self.buffer + text[: idx + 1])\n-                    self.buffer = \"\"\n-                    text = text[idx + 1 :]\n-                    break\n-                elif char == \"\\r\" and next_char == \"\\n\":\n-                    lines.append(self.buffer + text[:idx] + \"\\n\")\n-                    self.buffer = \"\"\n-                    text = text[idx + 2 :]\n-                    break\n-                elif char == \"\\r\" and next_char is not None:\n-                    lines.append(self.buffer + text[:idx] + \"\\n\")\n-                    self.buffer = \"\"\n-                    text = text[idx + 1 :]\n-                    break\n-                elif next_char is None:\n-                    self.buffer += text\n-                    text = \"\"\n-                    break\n+        if self.buffer:\n+            text = self.buffer + text\n+\n+        if text.endswith(\"\\r\"):\n+            self.buffer = text\n+            return []\n+\n+        lines = text.splitlines()\n+        if text.endswith(\"\\n\") or not lines:\n+            self.buffer = \"\"\n+        else:\n+            self.buffer = lines.pop()\n \n         return lines",
      "comment": "```suggestion\r\n        # See https://docs.python.org/3/library/stdtypes.html#str.splitlines\r\n        NEWLINE_CHARS = \"\\n\\r\\x0b\\x0c\\x1c\\x1d\\x1e\\x85\\u2028\\u2029\"\r\n\r\n        if self.buffer:\r\n            # If we have some buffered text from the previous pass,\r\n            # then we include it before handling the input.\r\n            text = self.buffer + text\r\n            self.buffer = \"\"\r\n\r\n        if not text:\r\n            return []\r\n        elif text[-1] == \"\\r\":\r\n            # If the last character is \"\\r\", then we might be about to see \"\\r\\n\"\r\n            # newline seperator. We buffer the text input and return.\r\n            self.buffer = text\r\n            return []\r\n        elif text[-1] in NEWLINE_CHARS:\r\n            # If the last character is a newline separator then we can simply split\r\n            # the text into lines and return them. There is no remaining portion\r\n            # to be dealt with on the next pass.\r\n            return text.splitlines()\r\n        else:\r\n            # If the last character is not a newline seperator, then the final portion\r\n            # from `splitlines()` is incomplete and needs to be buffered for the next\r\n            # pass.\r\n            lines = text.splitlines()\r\n            self.buffer = lines.pop()\r\n            return lines\r\n```",
      "comment_id": 1050940196,
      "user": "lovelydinosaur",
      "created_at": "2022-12-16T16:30:16Z",
      "url": "https://github.com/encode/httpx/pull/2423#discussion_r1050940196"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2423,
      "file_path": "httpx/_decoders.py",
      "line": 276,
      "side": "RIGHT",
      "diff_hunk": "@@ -267,57 +267,34 @@ def __init__(self) -> None:\n         self.buffer = \"\"\n \n     def decode(self, text: str) -> typing.List[str]:\n-        lines = []\n-\n-        if text and self.buffer and self.buffer[-1] == \"\\r\":\n-            if text.startswith(\"\\n\"):\n-                # Handle the case where we have an \"\\r\\n\" split across\n-                # our previous input, and our new chunk.\n-                lines.append(self.buffer[:-1] + \"\\n\")\n-                self.buffer = \"\"\n-                text = text[1:]\n-            else:\n-                # Handle the case where we have \"\\r\" at the end of our\n-                # previous input.\n-                lines.append(self.buffer[:-1] + \"\\n\")\n-                self.buffer = \"\"\n-\n-        while text:\n-            num_chars = len(text)\n-            for idx in range(num_chars):\n-                char = text[idx]\n-                next_char = None if idx + 1 == num_chars else text[idx + 1]\n-                if char == \"\\n\":\n-                    lines.append(self.buffer + text[: idx + 1])\n-                    self.buffer = \"\"\n-                    text = text[idx + 1 :]\n-                    break\n-                elif char == \"\\r\" and next_char == \"\\n\":\n-                    lines.append(self.buffer + text[:idx] + \"\\n\")\n-                    self.buffer = \"\"\n-                    text = text[idx + 2 :]\n-                    break\n-                elif char == \"\\r\" and next_char is not None:\n-                    lines.append(self.buffer + text[:idx] + \"\\n\")\n-                    self.buffer = \"\"\n-                    text = text[idx + 1 :]\n-                    break\n-                elif next_char is None:\n-                    self.buffer += text\n-                    text = \"\"\n-                    break\n+        # See https://docs.python.org/3/library/stdtypes.html#str.splitlines\n+        NEWLINE_CHARS = \"\\n\\r\\x0b\\x0c\\x1c\\x1d\\x1e\\x85\\u2028\\u2029\"\n+\n+        if self.buffer:\n+            # If we have some buffered text from the previous pass,\n+            # then we include it before handling the input.\n+            text = self.buffer + text",
      "comment": "Please correct me if I'm wrong, but it seems like if we do the following:\r\n\r\n```python\r\nfor i in range(N):\r\n    decoder.decode(\"a\")\r\n```\r\n\r\nthen we will have N string concatenations here (e.g. N string concatenations gives us O(N**2)). I'm curious if there any way to avoid it?\r\n\r\nE.g. may be we can use a `list` as a buffer, and do `''.join(buffer)` (it's O(N)) when newline is coming?",
      "comment_id": 1066603902,
      "user": "cdeler",
      "created_at": "2023-01-11T06:07:22Z",
      "url": "https://github.com/encode/httpx/pull/2423#discussion_r1066603902"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2423,
      "file_path": "httpx/_decoders.py",
      "line": 276,
      "side": "RIGHT",
      "diff_hunk": "@@ -267,57 +267,34 @@ def __init__(self) -> None:\n         self.buffer = \"\"\n \n     def decode(self, text: str) -> typing.List[str]:\n-        lines = []\n-\n-        if text and self.buffer and self.buffer[-1] == \"\\r\":\n-            if text.startswith(\"\\n\"):\n-                # Handle the case where we have an \"\\r\\n\" split across\n-                # our previous input, and our new chunk.\n-                lines.append(self.buffer[:-1] + \"\\n\")\n-                self.buffer = \"\"\n-                text = text[1:]\n-            else:\n-                # Handle the case where we have \"\\r\" at the end of our\n-                # previous input.\n-                lines.append(self.buffer[:-1] + \"\\n\")\n-                self.buffer = \"\"\n-\n-        while text:\n-            num_chars = len(text)\n-            for idx in range(num_chars):\n-                char = text[idx]\n-                next_char = None if idx + 1 == num_chars else text[idx + 1]\n-                if char == \"\\n\":\n-                    lines.append(self.buffer + text[: idx + 1])\n-                    self.buffer = \"\"\n-                    text = text[idx + 1 :]\n-                    break\n-                elif char == \"\\r\" and next_char == \"\\n\":\n-                    lines.append(self.buffer + text[:idx] + \"\\n\")\n-                    self.buffer = \"\"\n-                    text = text[idx + 2 :]\n-                    break\n-                elif char == \"\\r\" and next_char is not None:\n-                    lines.append(self.buffer + text[:idx] + \"\\n\")\n-                    self.buffer = \"\"\n-                    text = text[idx + 1 :]\n-                    break\n-                elif next_char is None:\n-                    self.buffer += text\n-                    text = \"\"\n-                    break\n+        # See https://docs.python.org/3/library/stdtypes.html#str.splitlines\n+        NEWLINE_CHARS = \"\\n\\r\\x0b\\x0c\\x1c\\x1d\\x1e\\x85\\u2028\\u2029\"\n+\n+        if self.buffer:\n+            # If we have some buffered text from the previous pass,\n+            # then we include it before handling the input.\n+            text = self.buffer + text",
      "comment": "Well observed.\r\n\r\nThis will run slowly...\r\n\r\n```python\r\nclass LineDecoder:\r\n    \"\"\"\r\n    Handles incrementally reading lines from text.\r\n    Uses universal line decoding, supporting any of `\\n`, `\\r`, or `\\r\\n`\r\n    as line endings, normalizing to `\\n`.\r\n    \"\"\"\r\n\r\n    def __init__(self):\r\n        self.buffer = \"\"\r\n\r\n    def decode(self, text):\r\n        # See https://docs.python.org/3/library/stdtypes.html#str.splitlines\r\n        NEWLINE_CHARS = \"\\n\\r\\x0b\\x0c\\x1c\\x1d\\x1e\\x85\\u2028\\u2029\"\r\n\r\n        if self.buffer:\r\n            # If we have some buffered text from the previous pass,\r\n            # then we include it before handling the input.\r\n            text = self.buffer + text\r\n            self.buffer = \"\"\r\n\r\n        if not text:\r\n            return []\r\n\r\n        lines = text.splitlines()\r\n\r\n        if text[-1] == \"\\r\":\r\n            # If the last character is \"\\r\", then we might be on the boundary of\r\n            # a \"\\r\\n\" sequence. We reassemble and buffer the final portion of the input.\r\n            self.buffer = lines.pop() + \"\\r\"\r\n        elif text[-1] not in NEWLINE_CHARS:\r\n            # If the last character is not a newline seperator, then the final portion\r\n            # from `splitlines()` is incomplete and needs to be buffered for the next\r\n            # pass.\r\n            self.buffer = lines.pop()\r\n\r\n        return lines\r\n\r\n    def flush(self):\r\n        lines = self.buffer.splitlines()\r\n        self.buffer = \"\"\r\n        return lines\r\n\r\n\r\nl = LineDecoder()\r\nfor i in range(100_000):\r\n    l.decode(\"a\")\r\nl.flush()\r\n```",
      "comment_id": 1066796498,
      "user": "lovelydinosaur",
      "created_at": "2023-01-11T10:00:50Z",
      "url": "https://github.com/encode/httpx/pull/2423#discussion_r1066796498"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2423,
      "file_path": "httpx/_decoders.py",
      "line": 276,
      "side": "RIGHT",
      "diff_hunk": "@@ -267,57 +267,34 @@ def __init__(self) -> None:\n         self.buffer = \"\"\n \n     def decode(self, text: str) -> typing.List[str]:\n-        lines = []\n-\n-        if text and self.buffer and self.buffer[-1] == \"\\r\":\n-            if text.startswith(\"\\n\"):\n-                # Handle the case where we have an \"\\r\\n\" split across\n-                # our previous input, and our new chunk.\n-                lines.append(self.buffer[:-1] + \"\\n\")\n-                self.buffer = \"\"\n-                text = text[1:]\n-            else:\n-                # Handle the case where we have \"\\r\" at the end of our\n-                # previous input.\n-                lines.append(self.buffer[:-1] + \"\\n\")\n-                self.buffer = \"\"\n-\n-        while text:\n-            num_chars = len(text)\n-            for idx in range(num_chars):\n-                char = text[idx]\n-                next_char = None if idx + 1 == num_chars else text[idx + 1]\n-                if char == \"\\n\":\n-                    lines.append(self.buffer + text[: idx + 1])\n-                    self.buffer = \"\"\n-                    text = text[idx + 1 :]\n-                    break\n-                elif char == \"\\r\" and next_char == \"\\n\":\n-                    lines.append(self.buffer + text[:idx] + \"\\n\")\n-                    self.buffer = \"\"\n-                    text = text[idx + 2 :]\n-                    break\n-                elif char == \"\\r\" and next_char is not None:\n-                    lines.append(self.buffer + text[:idx] + \"\\n\")\n-                    self.buffer = \"\"\n-                    text = text[idx + 1 :]\n-                    break\n-                elif next_char is None:\n-                    self.buffer += text\n-                    text = \"\"\n-                    break\n+        # See https://docs.python.org/3/library/stdtypes.html#str.splitlines\n+        NEWLINE_CHARS = \"\\n\\r\\x0b\\x0c\\x1c\\x1d\\x1e\\x85\\u2028\\u2029\"\n+\n+        if self.buffer:\n+            # If we have some buffered text from the previous pass,\n+            # then we include it before handling the input.\n+            text = self.buffer + text",
      "comment": "I feel we should re-write the `flush(...)`\r\n\r\n```diff\r\n    def flush(self) -> typing.List[str]:\r\n        if self.buffer or self.trailing_cr:\r\n-            return [\"\".join(self.buffer)]\r\n+            result = [\"\".join(self.buffer)]\r\n+            self.buffer = []\r\n+            return result\r\n        return []\r\n```",
      "comment_id": 1067360513,
      "user": "cdeler",
      "created_at": "2023-01-11T19:04:30Z",
      "url": "https://github.com/encode/httpx/pull/2423#discussion_r1067360513"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2423,
      "file_path": "httpx/_decoders.py",
      "line": 311,
      "side": "RIGHT",
      "diff_hunk": "@@ -259,67 +259,56 @@ class LineDecoder:\n     \"\"\"\n     Handles incrementally reading lines from text.\n \n-    Uses universal line decoding, supporting any of `\\n`, `\\r`, or `\\r\\n`\n-    as line endings, normalizing to `\\n`.\n+    Has the same behaviour as the stdllib splitlines, but handling the input iteratively.\n     \"\"\"\n \n     def __init__(self) -> None:\n-        self.buffer = \"\"\n+        self.buffer: typing.List[str] = []\n+        self.trailing_cr: bool = False\n \n     def decode(self, text: str) -> typing.List[str]:\n-        lines = []\n-\n-        if text and self.buffer and self.buffer[-1] == \"\\r\":\n-            if text.startswith(\"\\n\"):\n-                # Handle the case where we have an \"\\r\\n\" split across\n-                # our previous input, and our new chunk.\n-                lines.append(self.buffer[:-1] + \"\\n\")\n-                self.buffer = \"\"\n-                text = text[1:]\n-            else:\n-                # Handle the case where we have \"\\r\" at the end of our\n-                # previous input.\n-                lines.append(self.buffer[:-1] + \"\\n\")\n-                self.buffer = \"\"\n-\n-        while text:\n-            num_chars = len(text)\n-            for idx in range(num_chars):\n-                char = text[idx]\n-                next_char = None if idx + 1 == num_chars else text[idx + 1]\n-                if char == \"\\n\":\n-                    lines.append(self.buffer + text[: idx + 1])\n-                    self.buffer = \"\"\n-                    text = text[idx + 1 :]\n-                    break\n-                elif char == \"\\r\" and next_char == \"\\n\":\n-                    lines.append(self.buffer + text[:idx] + \"\\n\")\n-                    self.buffer = \"\"\n-                    text = text[idx + 2 :]\n-                    break\n-                elif char == \"\\r\" and next_char is not None:\n-                    lines.append(self.buffer + text[:idx] + \"\\n\")\n-                    self.buffer = \"\"\n-                    text = text[idx + 1 :]\n-                    break\n-                elif next_char is None:\n-                    self.buffer += text\n-                    text = \"\"\n-                    break\n+        # See https://docs.python.org/3/library/stdtypes.html#str.splitlines\n+        NEWLINE_CHARS = \"\\n\\r\\x0b\\x0c\\x1c\\x1d\\x1e\\x85\\u2028\\u2029\"\n+\n+        # We always push a trailing `\\r` into the next decode iteration.\n+        if self.trailing_cr:\n+            text = \"\\r\" + text\n+            self.trailing_cr = False\n+        if text.endswith(\"\\r\"):\n+            self.trailing_cr = True\n+            text = text[:-1]\n+\n+        if not text:\n+            return []\n+\n+        trailing_newline = text[-1] in NEWLINE_CHARS\n+        lines = text.splitlines()\n+\n+        if len(lines) == 1 and not trailing_newline:\n+            # No new lines, buffer the input and continue.\n+            self.buffer.append(lines[0])\n+            return []\n+\n+        if self.buffer:\n+            # Include any existing buffer in the first portion of the\n+            # splitlines result.\n+            lines = [\"\".join(self.buffer) + lines[0]] + lines[1:]\n+            self.buffer = []\n+\n+        if not trailing_newline:\n+            # If the last segment of splitlines is not newline terminated,\n+            # then drop it from our output and start a new buffer.\n+            self.buffer = [lines.pop()]\n \n         return lines\n \n     def flush(self) -> typing.List[str]:\n-        if self.buffer.endswith(\"\\r\"):\n-            # Handle the case where we had a trailing '\\r', which could have\n-            # been a '\\r\\n' pair.\n-            lines = [self.buffer[:-1] + \"\\n\"]\n-        elif self.buffer:\n-            lines = [self.buffer]\n-        else:\n-            lines = []\n-        self.buffer = \"\"\n-        return lines\n+        if self.buffer or self.trailing_cr:\n+            lines = [\"\".join(self.buffer)]\n+            self.buffer = []\n+            self.trailing_cr = False\n+            return lines\n+        return []",
      "comment": "```suggestion\r\n        if not self.buffer and not self.trailing_cr:\r\n            return []\r\n            \r\n        lines = [\"\".join(self.buffer)]\r\n        self.buffer = []\r\n        self.trailing_cr = False\r\n        return lines\r\n```",
      "comment_id": 1068912857,
      "user": "cdeler",
      "created_at": "2023-01-13T04:24:11Z",
      "url": "https://github.com/encode/httpx/pull/2423#discussion_r1068912857"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2572,
      "file_path": "httpx/_urlparse.py",
      "line": 208,
      "side": "LEFT",
      "diff_hunk": "@@ -195,18 +195,6 @@ def urlparse(url: str = \"\", **kwargs: typing.Optional[str]) -> ParseResult:\n     # -------------------------------------------------------------\n \n     for key, value in kwargs.items():\n-        if key not in (\n-            \"scheme\",\n-            \"authority\",\n-            \"path\",\n-            \"query\",\n-            \"fragment\",\n-            \"userinfo\",\n-            \"host\",\n-            \"port\",\n-        ):\n-            raise TypeError(f\"'{key}' is an invalid keyword argument for urlparse()\")",
      "comment": "We were already doing this check in `URL.__init__()`, so in the wild it would indeed be redundant as we use `httpx.URL()`, so I dropped this check.",
      "comment_id": 1096670353,
      "user": "florimondmanca",
      "created_at": "2023-02-05T11:44:24Z",
      "url": "https://github.com/encode/httpx/pull/2572#discussion_r1096670353"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2571,
      "file_path": "tests/test_exceptions.py",
      "line": 63,
      "side": "LEFT",
      "diff_hunk": "@@ -16,66 +14,39 @@ def test_httpcore_all_exceptions_mapped() -> None:\n     All exception classes exposed by HTTPCore are properly mapped to an HTTPX-specific\n     exception class.\n     \"\"\"\n-    not_mapped = [\n-        value\n-        for name, value in vars(httpcore).items()\n+    expected_mapped_httpcore_exceptions = {\n+        value.__name__\n+        for _, value in vars(httpcore).items()\n         if isinstance(value, type)\n         and issubclass(value, Exception)\n-        and value not in HTTPCORE_EXC_MAP\n         and value is not httpcore.ConnectionNotAvailable\n-    ]\n+    }\n \n-    if not_mapped:  # pragma: no cover\n-        pytest.fail(f\"Unmapped httpcore exceptions: {not_mapped}\")\n+    httpx_exceptions = {\n+        value.__name__\n+        for _, value in vars(httpx).items()\n+        if isinstance(value, type) and issubclass(value, Exception)\n+    }\n \n+    unmapped_exceptions = expected_mapped_httpcore_exceptions - httpx_exceptions\n \n-def test_httpcore_exception_mapping(server: \"TestServer\") -> None:\n-    \"\"\"\n-    HTTPCore exception mapping works as expected.\n-    \"\"\"\n-\n-    def connect_failed(*args, **kwargs):\n-        raise httpcore.ConnectError()\n-\n-    class TimeoutStream:\n-        def __iter__(self):\n-            raise httpcore.ReadTimeout()\n-\n-        def close(self):\n-            pass\n-\n-    with mock.patch(\n-        \"httpcore.ConnectionPool.handle_request\", side_effect=connect_failed\n-    ):\n-        with pytest.raises(httpx.ConnectError):\n-            httpx.get(server.url)\n+    if unmapped_exceptions:  # pragma: no cover\n+        pytest.fail(f\"Unmapped httpcore exceptions: {unmapped_exceptions}\")\n \n-    with mock.patch(\n-        \"httpcore.ConnectionPool.handle_request\",\n-        return_value=httpcore.Response(\n-            200, headers=[], content=TimeoutStream(), extensions={}\n-        ),\n-    ):\n-        with pytest.raises(httpx.ReadTimeout):\n-            httpx.get(server.url)\n \n-\n-def test_httpx_exceptions_exposed() -> None:",
      "comment": "The purpose of this test was to verify that we don't leave any exception class defined in `httpx/_exceptions.py` out of `httpx/__init__.py`.\r\n\r\nAs such, it's impossible to perform without reaching to `httpx._exceptions`.\r\n\r\nBut I also thought it wasn't very useful after all, so I am dropping it in this PR.",
      "comment_id": 1096665736,
      "user": "florimondmanca",
      "created_at": "2023-02-05T11:12:07Z",
      "url": "https://github.com/encode/httpx/pull/2571#discussion_r1096665736"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2535,
      "file_path": "httpx/_auth.py",
      "line": 155,
      "side": "RIGHT",
      "diff_hunk": "@@ -141,6 +142,34 @@ def _build_auth_header(\n         return f\"Basic {token}\"\n \n \n+class NetRCAuth(Auth):\n+    \"\"\"\n+    Use a 'netrc' file to lookup basic auth credentials based on the url host.\n+    \"\"\"\n+\n+    def __init__(self, file: typing.Optional[str]):\n+        self._netrc_info = netrc.netrc(file)\n+\n+    def auth_flow(self, request: Request) -> typing.Generator[Request, Response, None]:\n+        auth_info = self._netrc_info.authenticators(request.url.host)\n+        if auth_info is None or auth_info[2] is None:",
      "comment": "I'm sorry, may I ask you why you are checking the password for `None`?",
      "comment_id": 1066615132,
      "user": "cdeler",
      "created_at": "2023-01-11T06:26:56Z",
      "url": "https://github.com/encode/httpx/pull/2535#discussion_r1066615132"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2535,
      "file_path": "httpx/_auth.py",
      "line": 155,
      "side": "RIGHT",
      "diff_hunk": "@@ -141,6 +142,34 @@ def _build_auth_header(\n         return f\"Basic {token}\"\n \n \n+class NetRCAuth(Auth):\n+    \"\"\"\n+    Use a 'netrc' file to lookup basic auth credentials based on the url host.\n+    \"\"\"\n+\n+    def __init__(self, file: typing.Optional[str]):\n+        self._netrc_info = netrc.netrc(file)\n+\n+    def auth_flow(self, request: Request) -> typing.Generator[Request, Response, None]:\n+        auth_info = self._netrc_info.authenticators(request.url.host)\n+        if auth_info is None or auth_info[2] is None:",
      "comment": "I'm asking since previously you were checking \r\n```python\r\nif credentials is not None:\r\n    ...\r\n```\r\n\r\nand python documentation says ([link](https://docs.python.org/3/library/netrc.html#netrc.netrc.authenticators)) that \r\n> If the netrc file did not contain an entry for the given host, return the tuple associated with the \u2018default\u2019 entry. If neither matching host nor default entry is available, return None.\r\n\r\n",
      "comment_id": 1066617055,
      "user": "cdeler",
      "created_at": "2023-01-11T06:28:42Z",
      "url": "https://github.com/encode/httpx/pull/2535#discussion_r1066617055"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2535,
      "file_path": "httpx/_auth.py",
      "line": 155,
      "side": "RIGHT",
      "diff_hunk": "@@ -141,6 +142,34 @@ def _build_auth_header(\n         return f\"Basic {token}\"\n \n \n+class NetRCAuth(Auth):\n+    \"\"\"\n+    Use a 'netrc' file to lookup basic auth credentials based on the url host.\n+    \"\"\"\n+\n+    def __init__(self, file: typing.Optional[str]):\n+        self._netrc_info = netrc.netrc(file)\n+\n+    def auth_flow(self, request: Request) -> typing.Generator[Request, Response, None]:\n+        auth_info = self._netrc_info.authenticators(request.url.host)\n+        if auth_info is None or auth_info[2] is None:",
      "comment": "Thanks @cdeler.\r\n\r\nThe simple version of \"why\" is because I was blindly following our previous implementation details here.\r\n\r\nI've had a look into it, and I don't think the password can be `None` here, but I think it can be the empty string. It looks to me like Python 3.11+ will allow the empty string for a missing password field in the `netrc` file, and the previous versions will raise a parse error for missing passwords.\r\n\r\nI think the robust thing to do here would be...\r\n\r\n```python\r\nif auth_info is None or not auth_info[2]:\r\n```",
      "comment_id": 1066942807,
      "user": "lovelydinosaur",
      "created_at": "2023-01-11T12:38:35Z",
      "url": "https://github.com/encode/httpx/pull/2535#discussion_r1066942807"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2535,
      "file_path": "httpx/_auth.py",
      "line": 155,
      "side": "RIGHT",
      "diff_hunk": "@@ -141,6 +142,34 @@ def _build_auth_header(\n         return f\"Basic {token}\"\n \n \n+class NetRCAuth(Auth):\n+    \"\"\"\n+    Use a 'netrc' file to lookup basic auth credentials based on the url host.\n+    \"\"\"\n+\n+    def __init__(self, file: typing.Optional[str]):\n+        self._netrc_info = netrc.netrc(file)\n+\n+    def auth_flow(self, request: Request) -> typing.Generator[Request, Response, None]:\n+        auth_info = self._netrc_info.authenticators(request.url.host)\n+        if auth_info is None or auth_info[2] is None:",
      "comment": "```suggestion\r\n        if auth_info is None or not auth_info[2]:\r\n```",
      "comment_id": 1066943032,
      "user": "lovelydinosaur",
      "created_at": "2023-01-11T12:38:49Z",
      "url": "https://github.com/encode/httpx/pull/2535#discussion_r1066943032"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2535,
      "file_path": "httpx/_auth.py",
      "line": 155,
      "side": "RIGHT",
      "diff_hunk": "@@ -141,6 +142,34 @@ def _build_auth_header(\n         return f\"Basic {token}\"\n \n \n+class NetRCAuth(Auth):\n+    \"\"\"\n+    Use a 'netrc' file to lookup basic auth credentials based on the url host.\n+    \"\"\"\n+\n+    def __init__(self, file: typing.Optional[str]):\n+        self._netrc_info = netrc.netrc(file)\n+\n+    def auth_flow(self, request: Request) -> typing.Generator[Request, Response, None]:\n+        auth_info = self._netrc_info.authenticators(request.url.host)\n+        if auth_info is None or auth_info[2] is None:",
      "comment": "Demo'ing the behaviour of Python `netrc` with an empty password entry...\r\n\r\n```python\r\nimport netrc\r\nimport tempfile\r\n\r\n\r\nwith tempfile.NamedTemporaryFile(delete=False) as t:\r\n    t.write(b\"machine example.com\\nlogin user\\n\")\r\n    t.close()\r\n    n = netrc.netrc(t.name)\r\n    print(n.authenticators(\"example.com\"))\r\n```\r\n\r\nPython 3.11:\r\n\r\n```shell\r\n$ python3.11 ./example.py \r\n('user', '', '')\r\n```\r\n\r\nPython 3.10:\r\n\r\n```shell\r\n$ python3.10 ./example.py \r\nTraceback (most recent call last):\r\n  File \"/Users/tomchristie/Temp/./example.py\", line 8, in <module>\r\n    n = netrc.netrc(t.name)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/netrc.py\", line 31, in __init__\r\n    self._parse(file, fp, default_netrc)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/netrc.py\", line 82, in _parse\r\n    raise NetrcParseError(\r\nnetrc.NetrcParseError: malformed machine entry example.com terminated by '' (/var/folders/8s/dk9369g11yzdnsfkvbtljcjm0000gn/T/tmpipnocc0x, line 3)\r\n```",
      "comment_id": 1066955362,
      "user": "lovelydinosaur",
      "created_at": "2023-01-11T12:52:11Z",
      "url": "https://github.com/encode/httpx/pull/2535#discussion_r1066955362"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2523,
      "file_path": "tests/models/test_queryparams.py",
      "line": 91,
      "side": "RIGHT",
      "diff_hunk": "@@ -87,6 +87,11 @@ def test_empty_query_params():\n     assert str(q) == \"a=\"\n \n \n+def test_invalid_query_params():\n+    with pytest.raises(TypeError):",
      "comment": "Seems nice to ensure that the formatted message is correct\r\n```suggestion\r\n    with pytest.raises(TypeError, match=r\"Expected str, int, float, bool, or None\\. Got 'bytes'\\.\"):\r\n```",
      "comment_id": 1059507467,
      "user": "zanieb",
      "created_at": "2022-12-30T20:01:02Z",
      "url": "https://github.com/encode/httpx/pull/2523#discussion_r1059507467"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2523,
      "file_path": "tests/models/test_queryparams.py",
      "line": 92,
      "side": "RIGHT",
      "diff_hunk": "@@ -89,7 +89,7 @@ def test_empty_query_params():\n \n def test_invalid_query_params():\n     with pytest.raises(\n-        TypeError, match=r\"Expected str, int, float, bool, or None\\. Got 'bytes'\\.\"\n+        TypeError, match=r\"Expected str, int, float, bool, or None. Got 'bytes'.\"",
      "comment": "You don't need the `r` if you aren't escaping the periods \u2014 this is doing a `re.search` so technically the `.` will be a wildcard but \ud83e\udd37\u200d\u2640\ufe0f it's not likely to cause you any problems. You could use `re.escape` if you really wanted to be correct.",
      "comment_id": 1059518254,
      "user": "zanieb",
      "created_at": "2022-12-30T20:58:04Z",
      "url": "https://github.com/encode/httpx/pull/2523#discussion_r1059518254"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2512,
      "file_path": "tests/client/test_auth.py",
      "line": 155,
      "side": "RIGHT",
      "diff_hunk": "@@ -152,7 +152,7 @@ async def async_auth_flow(\n         yield request\n \n \n-@pytest.mark.asyncio\n+@pytest.mark.anyio",
      "comment": "is there a reason these were not marked `@pytest.mark.usefixtures(\"async_environment\")` ?",
      "comment_id": 1053443351,
      "user": "graingert",
      "created_at": "2022-12-20T15:27:14Z",
      "url": "https://github.com/encode/httpx/pull/2512#discussion_r1053443351"
    },
    {
      "repo": "encode/httpx",
      "pr_number": 2512,
      "file_path": "tests/client/test_auth.py",
      "line": 155,
      "side": "RIGHT",
      "diff_hunk": "@@ -152,7 +152,7 @@ async def async_auth_flow(\n         yield request\n \n \n-@pytest.mark.asyncio\n+@pytest.mark.anyio",
      "comment": "ah some of these tests used `asyncio.Lock` so only worked on asyncio. I changed it to `anyio.Lock` and they pass on trio or asyncio now",
      "comment_id": 1055414437,
      "user": "graingert",
      "created_at": "2022-12-22T12:35:16Z",
      "url": "https://github.com/encode/httpx/pull/2512#discussion_r1055414437"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22943,
      "file_path": "scripts/conformance.py",
      "line": 465,
      "side": "RIGHT",
      "diff_hunk": "@@ -462,6 +462,7 @@ def collect_ty_diagnostics(\n             f\"--python-version={python_version}\",\n             \"--output-format=gitlab\",\n             \"--ignore=assert-type-unspellable-subtype\",\n+            \"--error=invalid-legacy-positional-parameter\",",
      "comment": "this is necessary because I made the default severity of the lint `Level::Warn` rather than `Level::Error` (since it won't fail at runtime)",
      "comment_id": 2741872858,
      "user": "AlexWaygood",
      "created_at": "2026-01-29T14:15:48Z",
      "url": "https://github.com/astral-sh/ruff/pull/22943#discussion_r2741872858"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22943,
      "file_path": "scripts/conformance.py",
      "line": 465,
      "side": "RIGHT",
      "diff_hunk": "@@ -462,6 +462,7 @@ def collect_ty_diagnostics(\n             f\"--python-version={python_version}\",\n             \"--output-format=gitlab\",\n             \"--ignore=assert-type-unspellable-subtype\",\n+            \"--error=invalid-legacy-positional-parameter\",",
      "comment": "I feel like we should set up our conformance suite integration such that warnings count as errors for conformance suite purposes in general.",
      "comment_id": 2748483779,
      "user": "carljm",
      "created_at": "2026-01-30T23:59:41Z",
      "url": "https://github.com/astral-sh/ruff/pull/22943#discussion_r2748483779"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22943,
      "file_path": "scripts/conformance.py",
      "line": 465,
      "side": "RIGHT",
      "diff_hunk": "@@ -462,6 +462,7 @@ def collect_ty_diagnostics(\n             f\"--python-version={python_version}\",\n             \"--output-format=gitlab\",\n             \"--ignore=assert-type-unspellable-subtype\",\n+            \"--error=invalid-legacy-positional-parameter\",",
      "comment": "For example, the number of false positives we report might go up due to some `unused-ignore-comment` diagnostics on comments in the spec that aren't meant to be suppression comments at all, such as https://github.com/python/typing/blob/08f929ba70397b5c898a55bbb73c8b4e2e8e4fbb/conformance/tests/directives_type_ignore_file1.py#L11. (Related: https://github.com/astral-sh/ty/issues/881.)",
      "comment_id": 2749311445,
      "user": "AlexWaygood",
      "created_at": "2026-01-31T10:04:33Z",
      "url": "https://github.com/astral-sh/ruff/pull/22943#discussion_r2749311445"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21369,
      "file_path": "crates/ruff_python_formatter/resources/test/fixtures/ruff/parentheses/call_chains.py",
      "line": 266,
      "side": "RIGHT",
      "diff_hunk": "@@ -216,3 +216,57 @@\n     .baz()\n )\n \n+# Note in preview we split at `pl` which some\n+# folks may dislike. (Similarly with common\n+# `np` and `pd` invocations).\n+\n+expr = (\n+    pl.scan_parquet(\"/data/pypi-parquet/*.parquet\")\n+    .filter(\n+        [\n+            pl.col(\"path\").str.contains(\n+                r\"\\.(asm|c|cc|cpp|cxx|h|hpp|rs|[Ff][0-9]{0,2}(?:or)?|go)$\"\n+            ),\n+            ~pl.col(\"path\").str.contains(r\"(^|/)test(|s|ing)\"),\n+            ~pl.col(\"path\").str.contains(\"/site-packages/\", literal=True),\n+        ]\n+    )\n+    .with_columns(\n+        month=pl.col(\"uploaded_on\").dt.truncate(\"1mo\"),\n+        ext=pl.col(\"path\")\n+        .str.extract(pattern=r\"\\.([a-z0-9]+)$\", group_index=1)\n+        .str.replace_all(pattern=r\"cxx|cpp|cc|c|hpp|h\", value=\"C/C++\")\n+        .str.replace_all(pattern=\"^f.*$\", value=\"Fortran\")\n+        .str.replace(\"rs\", \"Rust\", literal=True)\n+        .str.replace(\"go\", \"Go\", literal=True)\n+        .str.replace(\"asm\", \"Assembly\", literal=True)\n+        .replace({\"\": None}),\n+    )\n+    .group_by([\"month\", \"ext\"])\n+    .agg(project_count=pl.col(\"project_name\").n_unique())\n+    .drop_nulls([\"ext\"])\n+    .sort([\"month\", \"project_count\"], descending=True)\n+)\n+\n+def indentation_matching_for_loop_in_preview():\n+    if make_this:\n+        if more_nested_because_line_length:\n+            identical_hidden_layer_sizes = all(",
      "comment": "Variable identical_hidden_layer_sizes is not used.\n```suggestion\n            all(\n```",
      "comment_id": 2615739046,
      "user": "Copilot",
      "created_at": "2025-12-12T22:26:47Z",
      "url": "https://github.com/astral-sh/ruff/pull/21369#discussion_r2615739046"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21369,
      "file_path": "crates/ruff_python_formatter/resources/test/fixtures/ruff/parentheses/call_chains.py",
      "line": 221,
      "side": "RIGHT",
      "diff_hunk": "@@ -216,3 +216,57 @@\n     .baz()\n )\n \n+# Note in preview we split at `pl` which some\n+# folks may dislike. (Similarly with common\n+# `np` and `pd` invocations).",
      "comment": "It might be worth explaining why we do it regardless. I'm sure it will come up in a future issue and I would much appreciate if I came across a comment in code (or test) explaining why we do split after `np`",
      "comment_id": 2618339047,
      "user": "MichaReiser",
      "created_at": "2025-12-15T07:46:41Z",
      "url": "https://github.com/astral-sh/ruff/pull/21369#discussion_r2618339047"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22746,
      "file_path": "scripts/conformance.py",
      "line": 385,
      "side": "RIGHT",
      "diff_hunk": "@@ -327,6 +380,10 @@ def collect_expected_diagnostics(test_files: Sequence[Path]) -> list[Diagnostic]\n                         ),\n                         source=Source.EXPECTED,\n                         optional=error.group(\"optional\") is not None,\n+                        tag=f\"{file.name}:{error.group('tag')}\"\n+                        if error.group(\"tag\")\n+                        else None,",
      "comment": "Ruff doesn't do a good job right now at formatting nested if-else expressions. But we can help it a bit to make this more readable by adding parentheses\r\n```suggestion\r\n                        tag=(\r\n                        \tf\"{file.name}:{error.group('tag')}\"\r\n                        \tif error.group(\"tag\")\r\n                        \telse None\r\n                      \t),\r\n```",
      "comment_id": 2712875649,
      "user": "MichaReiser",
      "created_at": "2026-01-21T14:40:17Z",
      "url": "https://github.com/astral-sh/ruff/pull/22746#discussion_r2712875649"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22746,
      "file_path": "scripts/conformance.py",
      "line": 471,
      "side": "RIGHT",
      "diff_hunk": "@@ -366,59 +423,77 @@ def collect_ty_diagnostics(\n     ]\n \n \n-def group_diagnostics_by_key(\n-    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+def group_diagnostics_by_key_or_tag(\n+    old: list[Diagnostic],\n+    new: list[Diagnostic],\n+    expected: list[Diagnostic],\n ) -> list[GroupedDiagnostics]:\n+    # propagate tags from expected diagnostics to old and new diagnostics\n+    tagged_lines = {\n+        (d.location.path.name, d.location.positions.begin.line): d.tag\n+        for d in expected\n+        if d.tag is not None\n+    }\n+\n+    for diag in old:\n+        diag.tag = tagged_lines.get(\n+            (diag.location.path.name, diag.location.positions.begin.line), None\n+        )\n+\n+    for diag in new:\n+        diag.tag = tagged_lines.get(\n+            (diag.location.path.name, diag.location.positions.begin.line), None\n+        )\n+\n     diagnostics = [\n         *old,\n         *new,\n         *expected,\n     ]\n \n-    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n-\n-    grouped = []\n-    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+    # group diagnostics either by a path and a line or a path and a tag\n+    diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+    grouped_diagnostics = []\n+    for key, group in groupby(diagnostics, key=attrgetter(\"key\")):\n         group = list(group)\n-        sources: Source = reduce(or_, (diag.source for diag in group))\n-        grouped.append(\n-            GroupedDiagnostics(\n-                key=key,\n-                sources=sources,\n-                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n-                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n-                expected=next(\n-                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n-                ),\n-            )\n+        old_group = list(filter(lambda diag: diag.source == Source.OLD, group))\n+        new_group = list(filter(lambda diag: diag.source == Source.NEW, group))\n+        expected_group = list(\n+            filter(lambda diag: diag.source == Source.EXPECTED, group)\n+        )\n+\n+        grouped = GroupedDiagnostics(\n+            key=key,\n+            sources={d.source for d in group},\n+            old=old_group,\n+            new=new_group,\n+            expected=expected_group,\n         )",
      "comment": "Nit: I would use a regular loop here to avoid iterating `diagnostics` multiple times and also because I find it slightly more readable. But maybe that's the Rust dev in me.\r\n\r\n```py\r\n\t\t\t\told_diagnostics: list[Diagnostic] = []\r\n        new_diagnostics: list[Diagnostic] = []\r\n        expected_diagnostics: list[Diagnostic] = []\r\n        sources: set[Source] = set()\r\n\r\n        for diag in group:\r\n            sources.add(diag.source)\r\n            match diag.source:\r\n                case Source.OLD:\r\n                    old_diagnostics.append(diag)\r\n                case Source.NEW:\r\n                    new_diagnostics.append(diag)\r\n                case Source.EXPECTED:\r\n                    expected_diagnostics.append(diag)\r\n\r\n        grouped = GroupedDiagnostics(\r\n            key=key,\r\n            sources=sources,\r\n            old=old_diagnostics,\r\n            new=new_diagnostics,\r\n            expected=expected_diagnostics,\r\n        )\r\n```",
      "comment_id": 2712882781,
      "user": "MichaReiser",
      "created_at": "2026-01-21T14:41:54Z",
      "url": "https://github.com/astral-sh/ruff/pull/22746#discussion_r2712882781"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22746,
      "file_path": "scripts/conformance.py",
      "line": 482,
      "side": "RIGHT",
      "diff_hunk": "@@ -366,59 +423,77 @@ def collect_ty_diagnostics(\n     ]\n \n \n-def group_diagnostics_by_key(\n-    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+def group_diagnostics_by_key_or_tag(\n+    old: list[Diagnostic],\n+    new: list[Diagnostic],\n+    expected: list[Diagnostic],\n ) -> list[GroupedDiagnostics]:\n+    # propagate tags from expected diagnostics to old and new diagnostics\n+    tagged_lines = {\n+        (d.location.path.name, d.location.positions.begin.line): d.tag\n+        for d in expected\n+        if d.tag is not None\n+    }\n+\n+    for diag in old:\n+        diag.tag = tagged_lines.get(\n+            (diag.location.path.name, diag.location.positions.begin.line), None\n+        )\n+\n+    for diag in new:\n+        diag.tag = tagged_lines.get(\n+            (diag.location.path.name, diag.location.positions.begin.line), None\n+        )\n+\n     diagnostics = [\n         *old,\n         *new,\n         *expected,\n     ]\n \n-    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n-\n-    grouped = []\n-    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+    # group diagnostics either by a path and a line or a path and a tag\n+    diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+    grouped_diagnostics = []\n+    for key, group in groupby(diagnostics, key=attrgetter(\"key\")):\n         group = list(group)\n-        sources: Source = reduce(or_, (diag.source for diag in group))\n-        grouped.append(\n-            GroupedDiagnostics(\n-                key=key,\n-                sources=sources,\n-                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n-                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n-                expected=next(\n-                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n-                ),\n-            )\n+        old_group = list(filter(lambda diag: diag.source == Source.OLD, group))\n+        new_group = list(filter(lambda diag: diag.source == Source.NEW, group))\n+        expected_group = list(\n+            filter(lambda diag: diag.source == Source.EXPECTED, group)\n+        )\n+\n+        grouped = GroupedDiagnostics(\n+            key=key,\n+            sources={d.source for d in group},\n+            old=old_group,\n+            new=new_group,\n+            expected=expected_group,\n         )\n+        grouped_diagnostics.append(grouped)\n \n-    return grouped\n+    return grouped_diagnostics\n \n \n def compute_stats(\n     grouped_diagnostics: list[GroupedDiagnostics],\n     source: Source,\n ) -> Statistics:\n     if source == source.EXPECTED:\n-        # ty currently raises a false positive here due to incomplete enum.Flag support\n-        # see https://github.com/astral-sh/ty/issues/876\n-        num_errors = sum(\n-            1\n-            for g in grouped_diagnostics\n-            if source.EXPECTED in g.sources  # ty:ignore[unsupported-operator]\n-        )\n+        num_errors = sum(1 for g in grouped_diagnostics if source.EXPECTED in g.sources)",
      "comment": "It's a bit tricky to say what `num_errors` means in the presence of `E[tag+]` and `E?` where a single expected tag can have multiple errors and some errors are entirely optional. \r\n\r\nBut it also seems that we never call `compute_stats` with `Source::Expected`. \r\n\r\nShould we remove this code and make `compute_stats` take a boolean argument instead (new_diagnostics)?",
      "comment_id": 2712891753,
      "user": "MichaReiser",
      "created_at": "2026-01-21T14:44:00Z",
      "url": "https://github.com/astral-sh/ruff/pull/22746#discussion_r2712891753"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22746,
      "file_path": "scripts/conformance.py",
      "line": 430,
      "side": "RIGHT",
      "diff_hunk": "@@ -366,59 +423,77 @@ def collect_ty_diagnostics(\n     ]\n \n \n-def group_diagnostics_by_key(\n-    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+def group_diagnostics_by_key_or_tag(\n+    old: list[Diagnostic],\n+    new: list[Diagnostic],\n+    expected: list[Diagnostic],\n ) -> list[GroupedDiagnostics]:",
      "comment": "GitHub doesn't really allow me to comment on that line but you could simplify your code a good amount by removing `None` from `old`, `new` and `expected`. Unless I miss a place where you create a `GroupedDiagnostics` instance where those fields are `None`",
      "comment_id": 2712920374,
      "user": "MichaReiser",
      "created_at": "2026-01-21T14:50:32Z",
      "url": "https://github.com/astral-sh/ruff/pull/22746#discussion_r2712920374"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22746,
      "file_path": "scripts/conformance.py",
      "line": 278,
      "side": "RIGHT",
      "diff_hunk": "@@ -241,17 +238,76 @@ def change(self) -> Change:\n \n     @property\n     def optional(self) -> bool:\n-        return self.expected is not None and self.expected.optional\n+        return bool(self.expected) and all(\n+            diagnostic.optional for diagnostic in self.expected\n+        )\n+\n+    @property\n+    def multi(self) -> bool:\n+        return bool(self.expected) and all(\n+            diagnostic.multi for diagnostic in self.expected\n+        )\n+\n+    def diagnostics_by_source(self, source: Source) -> list[Diagnostic]:\n+        match source:\n+            case Source.NEW:\n+                return self.new or []\n+            case Source.OLD:\n+                return self.old or []\n+            case Source.EXPECTED:\n+                return self.expected or []\n+            case _:\n+                raise ValueError(f\"Invalid source: {source}\")\n+\n+    def classify(self, source: Source) -> Classification:\n+        if source in self.sources and Source.EXPECTED in self.sources:\n+            assert self.expected is not None\n+            distinct_lines = len(\n+                {\n+                    diagnostic.location.positions.begin.line\n+                    for diagnostic in self.diagnostics_by_source(source)\n+                }\n+            )\n+            expected_max = len(self.expected) if self.multi else 1\n+\n+            if 1 <= distinct_lines <= expected_max:\n+                return Classification.TRUE_POSITIVE\n+            else:\n+                return Classification.FALSE_POSITIVE\n+\n+        elif source in self.sources and Source.EXPECTED not in self.sources:",
      "comment": "```suggestion\r\n        elif source in self.sources:\r\n```\r\n\r\nI believe the second part is implicitly given by not taking the first if branch\r\n\r\nIt might also be more readable if you nested the conditions like so:\r\n\r\n```py\r\nif source in self.sources:\r\n\tif Source.EXPECTED in self.sources:\r\n\t\tdistinct_lines = ... \r\n\r\n\telse:\r\n\t\treturn Classification.FalsePositive\r\n\r\nelif Source.EXPECTED in self.sources:\r\n\treturn Classification.FALSE_NEGATIVE\r\n\r\nelse: \r\n\treturn Classification.TRUE_NEGATIVE\t\r\n```",
      "comment_id": 2712931785,
      "user": "MichaReiser",
      "created_at": "2026-01-21T14:53:15Z",
      "url": "https://github.com/astral-sh/ruff/pull/22746#discussion_r2712931785"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22746,
      "file_path": "scripts/conformance.py",
      "line": 273,
      "side": "RIGHT",
      "diff_hunk": "@@ -241,17 +238,76 @@ def change(self) -> Change:\n \n     @property\n     def optional(self) -> bool:\n-        return self.expected is not None and self.expected.optional\n+        return bool(self.expected) and all(\n+            diagnostic.optional for diagnostic in self.expected\n+        )\n+\n+    @property\n+    def multi(self) -> bool:\n+        return bool(self.expected) and all(\n+            diagnostic.multi for diagnostic in self.expected\n+        )\n+\n+    def diagnostics_by_source(self, source: Source) -> list[Diagnostic]:\n+        match source:\n+            case Source.NEW:\n+                return self.new or []\n+            case Source.OLD:\n+                return self.old or []\n+            case Source.EXPECTED:\n+                return self.expected or []\n+            case _:\n+                raise ValueError(f\"Invalid source: {source}\")\n+\n+    def classify(self, source: Source) -> Classification:\n+        if source in self.sources and Source.EXPECTED in self.sources:\n+            assert self.expected is not None\n+            distinct_lines = len(\n+                {\n+                    diagnostic.location.positions.begin.line\n+                    for diagnostic in self.diagnostics_by_source(source)\n+                }\n+            )\n+            expected_max = len(self.expected) if self.multi else 1\n+\n+            if 1 <= distinct_lines <= expected_max:",
      "comment": "I can see how classifying too many diagnostics as false positives can be a bit confusing but I think it's the right thing. There's at least one false positive. \r\n\r\nIt might be better to count them as both a true positive and false positive, but I suspect that this doesn't play well with how we use those numbers later on.",
      "comment_id": 2712936781,
      "user": "MichaReiser",
      "created_at": "2026-01-21T14:54:28Z",
      "url": "https://github.com/astral-sh/ruff/pull/22746#discussion_r2712936781"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22746,
      "file_path": "scripts/conformance.py",
      "line": 273,
      "side": "RIGHT",
      "diff_hunk": "@@ -241,17 +238,76 @@ def change(self) -> Change:\n \n     @property\n     def optional(self) -> bool:\n-        return self.expected is not None and self.expected.optional\n+        return bool(self.expected) and all(\n+            diagnostic.optional for diagnostic in self.expected\n+        )\n+\n+    @property\n+    def multi(self) -> bool:\n+        return bool(self.expected) and all(\n+            diagnostic.multi for diagnostic in self.expected\n+        )\n+\n+    def diagnostics_by_source(self, source: Source) -> list[Diagnostic]:\n+        match source:\n+            case Source.NEW:\n+                return self.new or []\n+            case Source.OLD:\n+                return self.old or []\n+            case Source.EXPECTED:\n+                return self.expected or []\n+            case _:\n+                raise ValueError(f\"Invalid source: {source}\")\n+\n+    def classify(self, source: Source) -> Classification:\n+        if source in self.sources and Source.EXPECTED in self.sources:\n+            assert self.expected is not None\n+            distinct_lines = len(\n+                {\n+                    diagnostic.location.positions.begin.line\n+                    for diagnostic in self.diagnostics_by_source(source)\n+                }\n+            )\n+            expected_max = len(self.expected) if self.multi else 1\n+\n+            if 1 <= distinct_lines <= expected_max:",
      "comment": "Makes sense. One advantage of grouping them like this is that you'll be able to see why an error that appears to match a line in the conformance suite is a false positive if another line with the same tag has a diagnostic.",
      "comment_id": 2714696511,
      "user": "WillDuke",
      "created_at": "2026-01-21T23:24:01Z",
      "url": "https://github.com/astral-sh/ruff/pull/22746#discussion_r2714696511"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22746,
      "file_path": "scripts/conformance.py",
      "line": 265,
      "side": "RIGHT",
      "diff_hunk": "@@ -241,17 +238,79 @@ def change(self) -> Change:\n \n     @property\n     def optional(self) -> bool:\n-        return self.expected is not None and self.expected.optional\n+        return bool(self.expected) and all(\n+            diagnostic.optional for diagnostic in self.expected\n+        )\n+\n+    @property\n+    def multi(self) -> bool:\n+        return bool(self.expected) and all(\n+            diagnostic.multi for diagnostic in self.expected\n+        )\n+\n+    def diagnostics_by_source(self, source: Source) -> list[Diagnostic]:\n+        match source:\n+            case Source.NEW:\n+                return self.new\n+            case Source.OLD:\n+                return self.old\n+            case Source.EXPECTED:\n+                return self.expected\n+            case _:\n+                raise ValueError(f\"Invalid source: {source}\")\n+\n+    def classify(self, source: Source) -> Classification:\n+        if source in self.sources:\n+            if Source.EXPECTED in self.sources:\n+                assert self.expected is not None",
      "comment": "I'm surprised that ty doesn't catch this but `self.expected` should never be `None` here",
      "comment_id": 2715756019,
      "user": "MichaReiser",
      "created_at": "2026-01-22T08:05:02Z",
      "url": "https://github.com/astral-sh/ruff/pull/22746#discussion_r2715756019"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22746,
      "file_path": "scripts/conformance.py",
      "line": 260,
      "side": "RIGHT",
      "diff_hunk": "@@ -241,17 +238,79 @@ def change(self) -> Change:\n \n     @property\n     def optional(self) -> bool:\n-        return self.expected is not None and self.expected.optional\n+        return bool(self.expected) and all(\n+            diagnostic.optional for diagnostic in self.expected\n+        )\n+\n+    @property\n+    def multi(self) -> bool:\n+        return bool(self.expected) and all(\n+            diagnostic.multi for diagnostic in self.expected\n+        )\n+\n+    def diagnostics_by_source(self, source: Source) -> list[Diagnostic]:\n+        match source:\n+            case Source.NEW:\n+                return self.new\n+            case Source.OLD:\n+                return self.old\n+            case Source.EXPECTED:\n+                return self.expected\n+            case _:\n+                raise ValueError(f\"Invalid source: {source}\")",
      "comment": "I'm surprised that this last `case` is needed here? Does ty complain without it?",
      "comment_id": 2715758499,
      "user": "MichaReiser",
      "created_at": "2026-01-22T08:05:55Z",
      "url": "https://github.com/astral-sh/ruff/pull/22746#discussion_r2715758499"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22746,
      "file_path": "scripts/conformance.py",
      "line": 487,
      "side": "RIGHT",
      "diff_hunk": "@@ -366,64 +423,88 @@ def collect_ty_diagnostics(\n     ]\n \n \n-def group_diagnostics_by_key(\n-    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+def group_diagnostics_by_key_or_tag(\n+    old: list[Diagnostic],\n+    new: list[Diagnostic],\n+    expected: list[Diagnostic],\n ) -> list[GroupedDiagnostics]:\n+    # propagate tags from expected diagnostics to old and new diagnostics\n+    tagged_lines = {\n+        (d.location.path.name, d.location.positions.begin.line): d.tag\n+        for d in expected\n+        if d.tag is not None\n+    }\n+\n+    for diag in old:\n+        diag.tag = tagged_lines.get(\n+            (diag.location.path.name, diag.location.positions.begin.line), None\n+        )\n+\n+    for diag in new:\n+        diag.tag = tagged_lines.get(\n+            (diag.location.path.name, diag.location.positions.begin.line), None\n+        )\n+\n     diagnostics = [\n         *old,\n         *new,\n         *expected,\n     ]\n \n-    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n-\n-    grouped = []\n-    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n-        group = list(group)\n-        sources: Source = reduce(or_, (diag.source for diag in group))\n-        grouped.append(\n-            GroupedDiagnostics(\n-                key=key,\n-                sources=sources,\n-                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n-                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n-                expected=next(\n-                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n-                ),\n-            )\n+    # group diagnostics by a key which may be a path and a line or a path and a tag\n+    diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+    grouped_diagnostics = []\n+    for key, group in groupby(diagnostics, key=attrgetter(\"key\")):\n+        old_diagnostics: list[Diagnostic] = []\n+        new_diagnostics: list[Diagnostic] = []\n+        expected_diagnostics: list[Diagnostic] = []\n+        sources: set[Source] = set()\n+\n+        for diag in group:\n+            sources.add(diag.source)\n+            match diag.source:\n+                case Source.OLD:\n+                    old_diagnostics.append(diag)\n+                case Source.NEW:\n+                    new_diagnostics.append(diag)\n+                case Source.EXPECTED:\n+                    expected_diagnostics.append(diag)\n+\n+        grouped = GroupedDiagnostics(\n+            key=key,\n+            sources=sources,\n+            old=old_diagnostics,\n+            new=new_diagnostics,\n+            expected=expected_diagnostics,\n         )\n+        grouped_diagnostics.append(grouped)\n \n-    return grouped\n+    return grouped_diagnostics\n \n \n def compute_stats(\n     grouped_diagnostics: list[GroupedDiagnostics],\n-    source: Source,\n+    ty_version: Literal[\"new\", \"old\"],",
      "comment": "This is nice. Does Python allow us to define a union over `Source.NEW | Source.OLD` or is this something Python doesn't support?",
      "comment_id": 2715761401,
      "user": "MichaReiser",
      "created_at": "2026-01-22T08:06:50Z",
      "url": "https://github.com/astral-sh/ruff/pull/22746#discussion_r2715761401"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22746,
      "file_path": "scripts/conformance.py",
      "line": 487,
      "side": "RIGHT",
      "diff_hunk": "@@ -366,64 +423,88 @@ def collect_ty_diagnostics(\n     ]\n \n \n-def group_diagnostics_by_key(\n-    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+def group_diagnostics_by_key_or_tag(\n+    old: list[Diagnostic],\n+    new: list[Diagnostic],\n+    expected: list[Diagnostic],\n ) -> list[GroupedDiagnostics]:\n+    # propagate tags from expected diagnostics to old and new diagnostics\n+    tagged_lines = {\n+        (d.location.path.name, d.location.positions.begin.line): d.tag\n+        for d in expected\n+        if d.tag is not None\n+    }\n+\n+    for diag in old:\n+        diag.tag = tagged_lines.get(\n+            (diag.location.path.name, diag.location.positions.begin.line), None\n+        )\n+\n+    for diag in new:\n+        diag.tag = tagged_lines.get(\n+            (diag.location.path.name, diag.location.positions.begin.line), None\n+        )\n+\n     diagnostics = [\n         *old,\n         *new,\n         *expected,\n     ]\n \n-    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n-\n-    grouped = []\n-    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n-        group = list(group)\n-        sources: Source = reduce(or_, (diag.source for diag in group))\n-        grouped.append(\n-            GroupedDiagnostics(\n-                key=key,\n-                sources=sources,\n-                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n-                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n-                expected=next(\n-                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n-                ),\n-            )\n+    # group diagnostics by a key which may be a path and a line or a path and a tag\n+    diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+    grouped_diagnostics = []\n+    for key, group in groupby(diagnostics, key=attrgetter(\"key\")):\n+        old_diagnostics: list[Diagnostic] = []\n+        new_diagnostics: list[Diagnostic] = []\n+        expected_diagnostics: list[Diagnostic] = []\n+        sources: set[Source] = set()\n+\n+        for diag in group:\n+            sources.add(diag.source)\n+            match diag.source:\n+                case Source.OLD:\n+                    old_diagnostics.append(diag)\n+                case Source.NEW:\n+                    new_diagnostics.append(diag)\n+                case Source.EXPECTED:\n+                    expected_diagnostics.append(diag)\n+\n+        grouped = GroupedDiagnostics(\n+            key=key,\n+            sources=sources,\n+            old=old_diagnostics,\n+            new=new_diagnostics,\n+            expected=expected_diagnostics,\n         )\n+        grouped_diagnostics.append(grouped)\n \n-    return grouped\n+    return grouped_diagnostics\n \n \n def compute_stats(\n     grouped_diagnostics: list[GroupedDiagnostics],\n-    source: Source,\n+    ty_version: Literal[\"new\", \"old\"],",
      "comment": "I tried the same thing, but unfortunately it does not!",
      "comment_id": 2718728396,
      "user": "WillDuke",
      "created_at": "2026-01-22T21:37:33Z",
      "url": "https://github.com/astral-sh/ruff/pull/22746#discussion_r2718728396"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22746,
      "file_path": "scripts/conformance.py",
      "line": 260,
      "side": "RIGHT",
      "diff_hunk": "@@ -241,17 +238,79 @@ def change(self) -> Change:\n \n     @property\n     def optional(self) -> bool:\n-        return self.expected is not None and self.expected.optional\n+        return bool(self.expected) and all(\n+            diagnostic.optional for diagnostic in self.expected\n+        )\n+\n+    @property\n+    def multi(self) -> bool:\n+        return bool(self.expected) and all(\n+            diagnostic.multi for diagnostic in self.expected\n+        )\n+\n+    def diagnostics_by_source(self, source: Source) -> list[Diagnostic]:\n+        match source:\n+            case Source.NEW:\n+                return self.new\n+            case Source.OLD:\n+                return self.old\n+            case Source.EXPECTED:\n+                return self.expected\n+            case _:\n+                raise ValueError(f\"Invalid source: {source}\")",
      "comment": "Just defensive programming while I was writing this!",
      "comment_id": 2718740704,
      "user": "WillDuke",
      "created_at": "2026-01-22T21:41:08Z",
      "url": "https://github.com/astral-sh/ruff/pull/22746#discussion_r2718740704"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22746,
      "file_path": "scripts/conformance.py",
      "line": 107,
      "side": "RIGHT",
      "diff_hunk": "@@ -105,6 +103,15 @@ def into_title(self) -> str:\n                 return \"True positives removed\"\n \n \n+@dataclass(kw_only=True, slots=True)\n+class Evaluation:",
      "comment": "This class feels pretty heavy only to support the case where one group has both true and false positives. \r\n\r\nI was wondering if we could change `classify` to return an iterable of `(Classification, int)` instead. Most groups return exactly one, with the exception of the `many` case where ty emits too many diagnostics, in which case we return two.",
      "comment_id": 2721493099,
      "user": "MichaReiser",
      "created_at": "2026-01-23T14:43:12Z",
      "url": "https://github.com/astral-sh/ruff/pull/22746#discussion_r2721493099"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22746,
      "file_path": "scripts/conformance.py",
      "line": 107,
      "side": "RIGHT",
      "diff_hunk": "@@ -105,6 +103,15 @@ def into_title(self) -> str:\n                 return \"True positives removed\"\n \n \n+@dataclass(kw_only=True, slots=True)\n+class Evaluation:",
      "comment": "With the last set of changes, we now count the diagnostics individually. So if ty emits 5 diagnostics on the same line where a \"# E\" is present, we're counting them all as true positives. Similarly, if ty raises 3 diagnostics on one line of a tagged group (no '+') and 1 on each of the other lines, we count the 3 diagnostics as true positives and the remainder as false positives. \n\nHappy to keep iterating on it though if this doesn't make sense. ",
      "comment_id": 2723118794,
      "user": "WillDuke",
      "created_at": "2026-01-23T22:54:13Z",
      "url": "https://github.com/astral-sh/ruff/pull/22746#discussion_r2723118794"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22234,
      "file_path": "crates/ruff_linter/resources/test/fixtures/refurb/FURB180.py",
      "line": 92,
      "side": "RIGHT",
      "diff_hunk": "@@ -56,3 +56,37 @@ def foo(self): pass\n class A7(B0, abc.ABC, B1):\n     @abstractmethod\n     def foo(self): pass\n+\n+\n+# Regression tests for https://github.com/astral-sh/ruff/issues/17162\n+class A8(abc.ABC, metaclass=ABCMeta):  # FURB180\n+    @abstractmethod\n+    def foo(self):\n+        pass\n+\n+\n+def a9():\n+    from abc import ABC\n+\n+    class A9(ABC, metaclass=ABCMeta):  # FURB180\n+        @abstractmethod\n+        def foo(self):\n+            pass\n+\n+\n+def a10():\n+    from abc import ABC as ABCAlternativeName\n+\n+    class A10(ABCAlternativeName, metaclass=ABCMeta):  # FURB180\n+        @abstractmethod\n+        def foo(self):\n+            pass\n+\n+\n+class MyMetaClass(abc.ABC): ...\n+\n+\n+class A11(MyMetaClass, metaclass=ABCMeta):  # FURB180\n+    @abstractmethod\n+    def foo(self):\n+        pass",
      "comment": "Could you add a test case for the unsafe comment deletion? I think something like this example would work well:\n\n```py\nclass C(\n    other_kwarg=1,\n    # comment\n    metaclass=abc.ABCMeta,\n):\n    pass\n```",
      "comment_id": 2699959682,
      "user": "ntBre",
      "created_at": "2026-01-16T21:02:50Z",
      "url": "https://github.com/astral-sh/ruff/pull/22234#discussion_r2699959682"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22234,
      "file_path": "crates/ruff_linter/resources/test/fixtures/refurb/FURB180.py",
      "line": 92,
      "side": "RIGHT",
      "diff_hunk": "@@ -56,3 +56,37 @@ def foo(self): pass\n class A7(B0, abc.ABC, B1):\n     @abstractmethod\n     def foo(self): pass\n+\n+\n+# Regression tests for https://github.com/astral-sh/ruff/issues/17162\n+class A8(abc.ABC, metaclass=ABCMeta):  # FURB180\n+    @abstractmethod\n+    def foo(self):\n+        pass\n+\n+\n+def a9():\n+    from abc import ABC\n+\n+    class A9(ABC, metaclass=ABCMeta):  # FURB180\n+        @abstractmethod\n+        def foo(self):\n+            pass\n+\n+\n+def a10():\n+    from abc import ABC as ABCAlternativeName\n+\n+    class A10(ABCAlternativeName, metaclass=ABCMeta):  # FURB180\n+        @abstractmethod\n+        def foo(self):\n+            pass\n+\n+\n+class MyMetaClass(abc.ABC): ...\n+\n+\n+class A11(MyMetaClass, metaclass=ABCMeta):  # FURB180\n+    @abstractmethod\n+    def foo(self):\n+        pass",
      "comment": "Sure, the test case was added: https://github.com/astral-sh/ruff/pull/22234/changes/9c429186ed3eb060415d1b67f4c5ae96ad64722b",
      "comment_id": 2700644360,
      "user": "akawd",
      "created_at": "2026-01-17T05:03:33Z",
      "url": "https://github.com/astral-sh/ruff/pull/22234#discussion_r2700644360"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22320,
      "file_path": "crates/ruff_linter/resources/test/fixtures/pyupgrade/UP045_py39.py",
      "line": 15,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,15 @@\n+\"\"\"\n+Regression test for https://github.com/astral-sh/ruff/issues/20096\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from typing import Optional, cast, TypeAlias\n+\n+\n+x: Optional[str]             # UP045\n+x: \"Optional[str]\"           # UP045\n+cast(\"Optional[str]\", None)  # UP045\n+cast(Optional[str], None)    # okay, str | None is a runtime error\n+x: TypeAlias = \"Optional[str]\"  # UP045\n+x: TypeAlias = Optional[str]  # okay",
      "comment": "Can we add some tests for \"Complex\" string annotations (an annotation that uses implicit string concatenation). ",
      "comment_id": 2661017993,
      "user": "MichaReiser",
      "created_at": "2026-01-05T10:28:32Z",
      "url": "https://github.com/astral-sh/ruff/pull/22320#discussion_r2661017993"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22320,
      "file_path": "crates/ruff_linter/resources/test/fixtures/pyupgrade/UP045_py39.py",
      "line": 15,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,15 @@\n+\"\"\"\n+Regression test for https://github.com/astral-sh/ruff/issues/20096\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from typing import Optional, cast, TypeAlias\n+\n+\n+x: Optional[str]             # UP045\n+x: \"Optional[str]\"           # UP045\n+cast(\"Optional[str]\", None)  # UP045\n+cast(Optional[str], None)    # okay, str | None is a runtime error\n+x: TypeAlias = \"Optional[str]\"  # UP045\n+x: TypeAlias = Optional[str]  # okay",
      "comment": "Interestingly, the complex cases don't emit diagnostics even with my change. Claude thinks this is a bug, but I'll have to dig into it a bit more.\n\nty also emits an [implicit-concatenated-string-type-annotation](https://docs.astral.sh/ty/reference/rules/#implicit-concatenated-string-type-annotation) diagnostic on the new test cases, so I could possibly just replace `in_string_type_definition` with `in_simple_string_type_definition` if we want to filter out the complex cases intentionally.",
      "comment_id": 2684195946,
      "user": "ntBre",
      "created_at": "2026-01-12T23:07:35Z",
      "url": "https://github.com/astral-sh/ruff/pull/22320#discussion_r2684195946"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22320,
      "file_path": "crates/ruff_linter/resources/test/fixtures/pyupgrade/UP045_py39.py",
      "line": 15,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,15 @@\n+\"\"\"\n+Regression test for https://github.com/astral-sh/ruff/issues/20096\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from typing import Optional, cast, TypeAlias\n+\n+\n+x: Optional[str]             # UP045\n+x: \"Optional[str]\"           # UP045\n+cast(\"Optional[str]\", None)  # UP045\n+cast(Optional[str], None)    # okay, str | None is a runtime error\n+x: TypeAlias = \"Optional[str]\"  # UP045\n+x: TypeAlias = Optional[str]  # okay",
      "comment": "Yeah, this is a bit surprising. Let me know if you want me to take a closer look",
      "comment_id": 2711917554,
      "user": "MichaReiser",
      "created_at": "2026-01-21T10:23:07Z",
      "url": "https://github.com/astral-sh/ruff/pull/22320#discussion_r2711917554"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22320,
      "file_path": "crates/ruff_linter/resources/test/fixtures/pyupgrade/UP045_py39.py",
      "line": 15,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,15 @@\n+\"\"\"\n+Regression test for https://github.com/astral-sh/ruff/issues/20096\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from typing import Optional, cast, TypeAlias\n+\n+\n+x: Optional[str]             # UP045\n+x: \"Optional[str]\"           # UP045\n+cast(\"Optional[str]\", None)  # UP045\n+cast(Optional[str], None)    # okay, str | None is a runtime error\n+x: TypeAlias = \"Optional[str]\"  # UP045\n+x: TypeAlias = Optional[str]  # okay",
      "comment": "I think I'll just close this for now, unless you think it's worth landing without looking into the (arguably separate) issue with concatenated annotations. This is a pretty niche issue in any case.",
      "comment_id": 2713264436,
      "user": "ntBre",
      "created_at": "2026-01-21T16:06:15Z",
      "url": "https://github.com/astral-sh/ruff/pull/22320#discussion_r2713264436"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22320,
      "file_path": "crates/ruff_linter/resources/test/fixtures/pyupgrade/UP045_py39.py",
      "line": 15,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,15 @@\n+\"\"\"\n+Regression test for https://github.com/astral-sh/ruff/issues/20096\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from typing import Optional, cast, TypeAlias\n+\n+\n+x: Optional[str]             # UP045\n+x: \"Optional[str]\"           # UP045\n+cast(\"Optional[str]\", None)  # UP045\n+cast(Optional[str], None)    # okay, str | None is a runtime error\n+x: TypeAlias = \"Optional[str]\"  # UP045\n+x: TypeAlias = Optional[str]  # okay",
      "comment": "What you have here seems better than what we had before and should cover the majority of stringified type annotations",
      "comment_id": 2715700468,
      "user": "MichaReiser",
      "created_at": "2026-01-22T07:45:36Z",
      "url": "https://github.com/astral-sh/ruff/pull/22320#discussion_r2715700468"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22320,
      "file_path": "crates/ruff_linter/resources/test/fixtures/pyupgrade/UP045_py39.py",
      "line": 17,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,35 @@\n+\"\"\"\n+Regression test for https://github.com/astral-sh/ruff/issues/20096\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from typing import Optional, cast, TypeAlias\n+\n+\n+x: Optional[str]             # UP045\n+x: \"Optional[str]\"           # UP045\n+cast(\"Optional[str]\", None)  # UP045\n+cast(Optional[str], None)    # okay, str | None is a runtime error\n+x: TypeAlias = \"Optional[str]\"  # UP045\n+x: TypeAlias = Optional[str]  # okay\n+\n+# complex (implicitly concatenated) annotations",
      "comment": "Let's add a TODO comment here to make it clear, that the rule isn't currently handling implicitly concatenated strings.",
      "comment_id": 2715701759,
      "user": "MichaReiser",
      "created_at": "2026-01-22T07:46:04Z",
      "url": "https://github.com/astral-sh/ruff/pull/22320#discussion_r2715701759"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22320,
      "file_path": "crates/ruff_linter/resources/test/fixtures/pyupgrade/UP045_py39.py",
      "line": 15,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,15 @@\n+\"\"\"\n+Regression test for https://github.com/astral-sh/ruff/issues/20096\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from typing import Optional, cast, TypeAlias\n+\n+\n+x: Optional[str]             # UP045\n+x: \"Optional[str]\"           # UP045\n+cast(\"Optional[str]\", None)  # UP045\n+cast(Optional[str], None)    # okay, str | None is a runtime error\n+x: TypeAlias = \"Optional[str]\"  # UP045\n+x: TypeAlias = Optional[str]  # okay",
      "comment": "Sorry, I only just realized that my comment probably made it sound like I had left a bug in the PR. When Claude and I looked into it, the bug appears to be upstream of the rule itself in our complex string annotation parsing, or at least in how that parsing interacts with our semantic model. I think the rule itself is behaving correctly with the annotations it's asked to check.\r\n\r\nThanks for taking another look!",
      "comment_id": 2717344978,
      "user": "ntBre",
      "created_at": "2026-01-22T15:14:30Z",
      "url": "https://github.com/astral-sh/ruff/pull/22320#discussion_r2717344978"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22798,
      "file_path": "crates/ruff_linter/resources/test/fixtures/flake8_pyi/PYI034.py",
      "line": 330,
      "side": "RIGHT",
      "diff_hunk": "@@ -323,6 +323,13 @@ def __iadd__(self, other: \"UsesStringizedAnnotations\") -> \"typing.Self\":\n         return self\n \n \n+class UsesStringizedForwardReferences:\n+    def __new__(cls) -> \"UsesStringizedForwardReferences\": ...       # PYI034\n+    def __enter__(self) -> \"UsesStringizedForwardReferences\": ...    # PYI034\n+    async def __aenter__(self) -> \"UsesStringizedForwardReferences\": ...  # PYI034\n+    def __iadd__(self, other) -> \"UsesStringizedForwardReferences\": ...  # PYI034",
      "comment": "nit: could you possibly move this to the end of the fixture file? It's very hard to review the diff to the snapshot when all the line numbers change \ud83d\ude06",
      "comment_id": 2716597174,
      "user": "AlexWaygood",
      "created_at": "2026-01-22T11:55:47Z",
      "url": "https://github.com/astral-sh/ruff/pull/22798#discussion_r2716597174"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21110,
      "file_path": "crates/ruff_python_formatter/resources/test/fixtures/ruff/newlines.py",
      "line": 374,
      "side": "RIGHT",
      "diff_hunk": "@@ -335,3 +335,40 @@ def overload4():\n     # trailing comment\n \n def overload4(a: int): ...\n+\n+\n+# In preview, we preserve these newlines at the start of functions:\n+def preserved1():\n+\n+    return 1\n+\n+def preserved2():\n+\n+    pass\n+\n+\n+# But we still discard these newlines:\n+def removed1():\n+\n+    \"Docstring\"\n+\n+    return 1\n+\n+\n+def removed2():\n+\n+    # Comment\n+\n+    return 1\n+\n+\n+def removed3():\n+\n+    ...\n+\n+\n+# And we discard empty lines after the first:\n+def partially_preserved1():\n+\n+\n+    return 1",
      "comment": "Can we add more tests to this, specifically tests including comments or cases where the first element is a function on its own?",
      "comment_id": 2473686964,
      "user": "MichaReiser",
      "created_at": "2025-10-29T15:07:52Z",
      "url": "https://github.com/astral-sh/ruff/pull/21110#discussion_r2473686964"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22671,
      "file_path": "scripts/conformance.py",
      "line": 300,
      "side": "RIGHT",
      "diff_hunk": "@@ -297,8 +297,12 @@ def total(self) -> int:\n \n def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n     diagnostics: list[Diagnostic] = []\n-    for file in path.resolve().rglob(\"*.py\"):\n-        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+    for entry in path.iterdir():",
      "comment": "I'm surprised that Python's globbing doesn't support multiple extensions, e.g. `.{py,pyi}`",
      "comment_id": 2702308348,
      "user": "MichaReiser",
      "created_at": "2026-01-18T10:45:52Z",
      "url": "https://github.com/astral-sh/ruff/pull/22671#discussion_r2702308348"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22671,
      "file_path": "scripts/conformance.py",
      "line": 303,
      "side": "RIGHT",
      "diff_hunk": "@@ -297,8 +297,12 @@ def total(self) -> int:\n \n def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n     diagnostics: list[Diagnostic] = []\n-    for file in path.resolve().rglob(\"*.py\"):\n-        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+    for entry in path.iterdir():\n+        if not entry.is_file():\n+            continue\n+        if entry.suffix not in {\".py\", \".pyi\"}:",
      "comment": "Should we skip files starting with an `_`? I assume it's not strictly necessary, since they never contain any comments matching the error pattern but it feels unnecessary",
      "comment_id": 2702309581,
      "user": "MichaReiser",
      "created_at": "2026-01-18T10:47:07Z",
      "url": "https://github.com/astral-sh/ruff/pull/22671#discussion_r2702309581"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22671,
      "file_path": "scripts/conformance.py",
      "line": 355,
      "side": "RIGHT",
      "diff_hunk": "@@ -345,7 +349,11 @@ def collect_ty_diagnostics(\n             f\"--python-version={python_version}\",\n             \"--output-format=gitlab\",\n             \"--exit-zero\",\n-            tests_path,\n+            *(\n+                str(path)\n+                for path in Path(tests_path).iterdir()\n+                if path.suffix in {\".py\", \".pyi\"} and not path.name.startswith(\"_\")",
      "comment": "Should we create a helper method that, given a path, returns whether this is a conformance test file?",
      "comment_id": 2702310372,
      "user": "MichaReiser",
      "created_at": "2026-01-18T10:47:47Z",
      "url": "https://github.com/astral-sh/ruff/pull/22671#discussion_r2702310372"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22647,
      "file_path": "scripts/conformance.py",
      "line": 118,
      "side": "RIGHT",
      "diff_hunk": "@@ -101,6 +103,21 @@ def into_title(self) -> str:\n                 return \"True positives removed\"\n \n \n+class Change(StrEnum):\n+    ADDED = auto()\n+    REMOVED = auto()\n+    UNCHANGED = auto()\n+\n+    def into_title(self) -> str:\n+        match self:\n+            case Change.ADDED:\n+                return \"Added (Optional)\"\n+            case Change.REMOVED:\n+                return \"Removed (Optional)\"\n+            case Change.UNCHANGED:\n+                return \"Unchanged (Optional)\"",
      "comment": "```suggestion\r\n                return \"Optional Diagnostics Added\"\r\n            case Change.REMOVED:\r\n                return \"Optional Diagnostics Removed\"\r\n            case Change.UNCHANGED:\r\n                return \"Optional Diagnostics Unchanged\"\r\n```",
      "comment_id": 2701042575,
      "user": "AlexWaygood",
      "created_at": "2026-01-17T12:23:32Z",
      "url": "https://github.com/astral-sh/ruff/pull/22647#discussion_r2701042575"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22647,
      "file_path": "scripts/conformance.py",
      "line": 156,
      "side": "RIGHT",
      "diff_hunk": "@@ -136,8 +153,11 @@ class Diagnostic:\n     fingerprint: str | None\n     location: Location\n     source: Source\n+    optional: bool",
      "comment": "It doesn't need to be in this PR, but it might be good if we could add some docs for the fields on this class at some point. `optional` is fairly clear, but I'd have to study the script a bit to figure out what e.g. `fingerprint` is \ud83d\ude04",
      "comment_id": 2701043523,
      "user": "AlexWaygood",
      "created_at": "2026-01-17T12:24:34Z",
      "url": "https://github.com/astral-sh/ruff/pull/22647#discussion_r2701043523"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22647,
      "file_path": "scripts/conformance.py",
      "line": 452,
      "side": "RIGHT",
      "diff_hunk": "@@ -397,13 +431,25 @@ def render_grouped_diagnostics(\n     format: Literal[\"diff\", \"github\"] = \"diff\",\n ) -> str:\n     if changed_only:\n-        grouped = [diag for diag in grouped if diag.changed]\n+        grouped = [\n+            diag for diag in grouped if diag.change in (Change.ADDED, Change.REMOVED)\n+        ]\n \n-    sorted_by_class = sorted(\n-        grouped,\n+    g1, g2 = tee(grouped)\n+    required, optional = (\n+        filterfalse(attrgetter(\"optional\"), g1),\n+        filter(attrgetter(\"optional\"), g2),\n+    )\n+    required = sorted(\n+        required,\n         key=attrgetter(\"classification\"),\n         reverse=True,\n     )\n+    optional = sorted(\n+        optional,\n+        key=attrgetter(\"change\"),\n+        reverse=True,\n+    )",
      "comment": "I think in this case, you can do this in a slightly more readable way without the `tee` ;)\r\n\r\nWhat about something like\r\n\r\n```diff\r\ndiff --git a/scripts/conformance.py b/scripts/conformance.py\r\nindex f8e7124d8a..383c5eb6bf 100644\r\n--- a/scripts/conformance.py\r\n+++ b/scripts/conformance.py\r\n@@ -435,19 +435,17 @@ def render_grouped_diagnostics(\r\n             diag for diag in grouped if diag.change in (Change.ADDED, Change.REMOVED)\r\n         ]\r\n \r\n-    g1, g2 = tee(grouped)\r\n-    required, optional = (\r\n-        filterfalse(attrgetter(\"optional\"), g1),\r\n-        filter(attrgetter(\"optional\"), g2),\r\n-    )\r\n-    required = sorted(\r\n-        required,\r\n-        key=attrgetter(\"classification\"),\r\n+    get_change = attrgetter(\"change\")\r\n+    get_classification = attrgetter(\"classification\")\r\n+\r\n+    optional_diagnostics = sorted(\r\n+        (diag for diag in grouped if diag.optional),\r\n+        key=get_change,\r\n         reverse=True,\r\n     )\r\n-    optional = sorted(\r\n-        optional,\r\n-        key=attrgetter(\"change\"),\r\n+    required_diagnostics = sorted(\r\n+        (diag for diag in grouped if not diag.optional),\r\n+        key=get_classification,\r\n         reverse=True,\r\n     )\r\n \r\n@@ -466,8 +464,8 @@ def render_grouped_diagnostics(\r\n \r\n     lines = []\r\n     for group, diagnostics in chain(\r\n-        groupby(required, key=attrgetter(\"classification\")),\r\n-        groupby(optional, key=attrgetter(\"change\")),\r\n+        groupby(required_diagnostics, key=get_classification),\r\n+        groupby(optional_diagnostics, key=get_change),\r\n     ):\r\n         lines.append(f\"### {group.into_title()}\")\r\n         lines.extend([\"\", \"<details>\", \"\"])\r\n```",
      "comment_id": 2701049489,
      "user": "AlexWaygood",
      "created_at": "2026-01-17T12:30:40Z",
      "url": "https://github.com/astral-sh/ruff/pull/22647#discussion_r2701049489"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22647,
      "file_path": "scripts/conformance.py",
      "line": 156,
      "side": "RIGHT",
      "diff_hunk": "@@ -136,8 +153,11 @@ class Diagnostic:\n     fingerprint: str | None\n     location: Location\n     source: Source\n+    optional: bool",
      "comment": "Makes sense. Regarding the fingerprint, I'm just preserving the data returned from `ty` in the gitlab output. I assume it provides a unique identifier for that diagnostic, but I don't actually know! Maybe it would be best to drop it since it is not used.",
      "comment_id": 2701064831,
      "user": "WillDuke",
      "created_at": "2026-01-17T12:45:05Z",
      "url": "https://github.com/astral-sh/ruff/pull/22647#discussion_r2701064831"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22114,
      "file_path": "crates/ruff_linter/resources/test/fixtures/ruff/RUF068.py",
      "line": 23,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,34 @@\n+import typing\n+\n+\n+class A: ...\n+\n+\n+class B: ...\n+\n+\n+# Good\n+__all__ = \"A\" + \"B\"\n+__all__: list[str] = [\"A\", \"B\"]\n+__all__: typing.Any = (\"A\", \"B\")\n+__all__ = [\"A\", \"B\"]\n+__all__ += [\"A\", \"B\"]\n+__all__.extend([\"A\", \"B\"])\n+\n+# Bad\n+__all__: list[str] = [\"A\", \"B\", \"A\"]\n+__all__: typing.Any = (\"A\", \"B\", \"B\")\n+__all__ = [\"A\", \"A\", \"B\"]\n+__all__ = [\"A\", \"B\", \"A\"]\n+__all__ = [\"A\", \"A\", \"B\", \"B\"]",
      "comment": "Let's throw in one multi-line case without comments.",
      "comment_id": 2656479847,
      "user": "ntBre",
      "created_at": "2026-01-01T16:38:35Z",
      "url": "https://github.com/astral-sh/ruff/pull/22114#discussion_r2656479847"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22114,
      "file_path": "crates/ruff_linter/resources/test/fixtures/ruff/RUF069.py",
      "line": 32,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,33 @@\n+import typing\n+\n+\n+class A: ...\n+\n+\n+class B: ...\n+\n+\n+# Good\n+__all__ = \"A\" + \"B\"\n+__all__: list[str] = [\"A\", \"B\"]\n+__all__: typing.Any = (\"A\", \"B\")\n+__all__ = [\"A\", \"B\"]\n+__all__ += [\"A\", \"B\"]\n+__all__.extend([\"A\", \"B\"])\n+\n+# Bad\n+__all__: list[str] = [\"A\", \"B\", \"A\"]\n+__all__: typing.Any = (\"A\", \"B\", \"B\")\n+__all__ = [\"A\", \"B\", \"A\"]\n+__all__ = [\"A\", \"A\", \"B\", \"B\"]\n+__all__ += [\"B\", \"B\"]\n+__all__.extend([\"B\", \"B\"])\n+\n+# Bad, unsafe\n+__all__ = [\n+    \"A\",\n+    \"A\",\n+    \"B\",\n+    # Comment\n+    \"B\",",
      "comment": "Can we add an end-of-line comment too? And maybe one after the deleted element, just to check which comments get deleted.\n\n\n```suggestion\n    \"B\",  # 2\n    # 3\n```",
      "comment_id": 2683974284,
      "user": "ntBre",
      "created_at": "2026-01-12T21:31:27Z",
      "url": "https://github.com/astral-sh/ruff/pull/22114#discussion_r2683974284"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22114,
      "file_path": "crates/ruff_linter/resources/test/fixtures/ruff/RUF069.py",
      "line": 15,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,41 @@\n+import typing\n+\n+\n+class A: ...\n+\n+\n+class B: ...\n+\n+\n+# Good\n+__all__ = \"A\" + \"B\"\n+__all__: list[str] = [\"A\", \"B\"]\n+__all__: typing.Any = (\"A\", \"B\")\n+__all__ = [\"A\", \"B\"]\n+__all__ = [A, \"A\", \"B\"]",
      "comment": "I think this is actually what I meant in https://github.com/astral-sh/ruff/pull/22114#discussion_r2656477043:\n\n\n```suggestion\n__all__ = [A, \"B\", \"B\"]\n```\n\nSorry for being quite pedantic, I just want to have one test case for the `continue` vs `return` choice.",
      "comment_id": 2699643146,
      "user": "ntBre",
      "created_at": "2026-01-16T19:01:58Z",
      "url": "https://github.com/astral-sh/ruff/pull/22114#discussion_r2699643146"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 1577,
      "file_path": "sqlmodel/_compat.py",
      "line": 224,
      "side": "RIGHT",
      "diff_hunk": "@@ -221,7 +221,13 @@ def get_field_metadata(field: Any) -> Any:\n         return FakeMetadata()\n \n     def post_init_field_info(field_info: FieldInfo) -> None:\n-        return None\n+        if IS_PYDANTIC_V2:",
      "comment": "This function is declared inside the `if IS_PYDANTIC_V2:` block. So, this condition looks not necessary here.\r\nAm I missing something?",
      "comment_id": 2392413881,
      "user": "YuriiMotov",
      "created_at": "2025-09-30T17:59:10Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/1577#discussion_r2392413881"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 1577,
      "file_path": "sqlmodel/main.py",
      "line": 445,
      "side": "RIGHT",
      "diff_hunk": "@@ -387,43 +395,65 @@ def Field(\n     schema_extra: Optional[Dict[str, Any]] = None,\n ) -> Any:\n     current_schema_extra = schema_extra or {}\n-    field_info = FieldInfo(\n-        default,\n-        default_factory=default_factory,\n-        alias=alias,\n-        title=title,\n-        description=description,\n-        exclude=exclude,\n-        include=include,\n-        const=const,\n-        gt=gt,\n-        ge=ge,\n-        lt=lt,\n-        le=le,\n-        multiple_of=multiple_of,\n-        max_digits=max_digits,\n-        decimal_places=decimal_places,\n-        min_items=min_items,\n-        max_items=max_items,\n-        unique_items=unique_items,\n-        min_length=min_length,\n-        max_length=max_length,\n-        allow_mutation=allow_mutation,\n-        regex=regex,\n-        discriminator=discriminator,\n-        repr=repr,\n-        primary_key=primary_key,\n-        foreign_key=foreign_key,\n-        ondelete=ondelete,\n-        unique=unique,\n-        nullable=nullable,\n-        index=index,\n-        sa_type=sa_type,\n-        sa_column=sa_column,\n-        sa_column_args=sa_column_args,\n-        sa_column_kwargs=sa_column_kwargs,\n+    field_info_kwargs = {\n+        \"alias\": alias,\n+        \"title\": title,\n+        \"description\": description,\n+        \"exclude\": exclude,\n+        \"include\": include,\n+        \"const\": const,\n+        \"gt\": gt,\n+        \"ge\": ge,\n+        \"lt\": lt,\n+        \"le\": le,\n+        \"multiple_of\": multiple_of,\n+        \"max_digits\": max_digits,\n+        \"decimal_places\": decimal_places,\n+        \"min_items\": min_items,\n+        \"max_items\": max_items,\n+        \"unique_items\": unique_items,\n+        \"min_length\": min_length,\n+        \"max_length\": max_length,\n+        \"allow_mutation\": allow_mutation,\n+        \"regex\": regex,\n+        \"discriminator\": discriminator,\n+        \"repr\": repr,\n+        \"primary_key\": primary_key,\n+        \"foreign_key\": foreign_key,\n+        \"ondelete\": ondelete,\n+        \"unique\": unique,\n+        \"nullable\": nullable,\n+        \"index\": index,\n+        \"sa_type\": sa_type,\n+        \"sa_column\": sa_column,\n+        \"sa_column_args\": sa_column_args,\n+        \"sa_column_kwargs\": sa_column_kwargs,\n         **current_schema_extra,\n-    )\n+    }\n+    if IS_PYDANTIC_V2:\n+        # Add Pydantic v2 specific parameters\n+        field_info_kwargs.update(\n+            {\n+                \"validation_alias\": validation_alias,\n+                \"serialization_alias\": serialization_alias,\n+            }\n+        )\n+        field_info = FieldInfo(\n+            default,\n+            default_factory=default_factory,\n+            **field_info_kwargs,\n+        )",
      "comment": "Looks like we can move this outside of `if..else..` block to avoid duplication",
      "comment_id": 2392422553,
      "user": "YuriiMotov",
      "created_at": "2025-09-30T18:03:12Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/1577#discussion_r2392422553"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 1577,
      "file_path": "tests/test_aliases.py",
      "line": 12,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,176 @@\n+from typing import Type, Union\n+\n+import pytest\n+from pydantic import VERSION, BaseModel, ValidationError\n+from pydantic import Field as PField\n+from sqlmodel import Field, SQLModel\n+\n+# -----------------------------------------------------------------------------------\n+# Models\n+\n+\n+class PydanticUser(BaseModel):",
      "comment": "We should think whether we want to keep parameterizing tests with Pydantic models.\r\nInitially I parametrized tests with both, Pydantic and SQLModel models to show the difference in behavior between SQLModel and Pydantic models.\r\n\r\nIf it's useful to keep this parametrization in repo? \ud83e\udd14\r\nTests would be a bit simpler without it, but on the other hand this way we ensure it works the same as with Pydantic model",
      "comment_id": 2392444806,
      "user": "YuriiMotov",
      "created_at": "2025-09-30T18:11:37Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/1577#discussion_r2392444806"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 1577,
      "file_path": "tests/test_aliases.py",
      "line": 9,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,176 @@\n+from typing import Type, Union\n+\n+import pytest\n+from pydantic import VERSION, BaseModel, ValidationError\n+from pydantic import Field as PField\n+from sqlmodel import Field, SQLModel\n+\n+# -----------------------------------------------------------------------------------\n+# Models",
      "comment": "We will need to remove unnecessary comments in final version",
      "comment_id": 2392446448,
      "user": "YuriiMotov",
      "created_at": "2025-09-30T18:12:18Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/1577#discussion_r2392446448"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 1577,
      "file_path": "tests/test_aliases.py",
      "line": 154,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,176 @@\n+from typing import Type, Union\n+\n+import pytest\n+from pydantic import VERSION, BaseModel, ValidationError\n+from pydantic import Field as PField\n+from sqlmodel import Field, SQLModel\n+\n+# -----------------------------------------------------------------------------------\n+# Models\n+\n+\n+class PydanticUser(BaseModel):\n+    full_name: str = PField(alias=\"fullName\")\n+\n+\n+class SQLModelUser(SQLModel):\n+    full_name: str = Field(alias=\"fullName\")\n+\n+\n+# Models with config (validate_by_name=True)\n+\n+\n+if VERSION.startswith(\"2.\"):\n+\n+    class PydanticUserWithConfig(PydanticUser):\n+        model_config = {\"validate_by_name\": True}\n+\n+    class SQLModelUserWithConfig(SQLModelUser):\n+        model_config = {\"validate_by_name\": True}\n+\n+else:\n+\n+    class PydanticUserWithConfig(PydanticUser):\n+        class Config:\n+            allow_population_by_field_name = True\n+\n+    class SQLModelUserWithConfig(SQLModelUser):\n+        class Config:\n+            allow_population_by_field_name = True\n+\n+\n+# -----------------------------------------------------------------------------------\n+# Tests\n+\n+# Test validate by name\n+\n+\n+@pytest.mark.parametrize(\"model\", [PydanticUser, SQLModelUser])\n+def test_create_with_field_name(model: Union[Type[PydanticUser], Type[SQLModelUser]]):\n+    with pytest.raises(ValidationError):\n+        model(full_name=\"Alice\")\n+\n+\n+@pytest.mark.parametrize(\"model\", [PydanticUserWithConfig, SQLModelUserWithConfig])\n+def test_create_with_field_name_with_config(\n+    model: Union[Type[PydanticUserWithConfig], Type[SQLModelUserWithConfig]],\n+):\n+    user = model(full_name=\"Alice\")\n+    assert user.full_name == \"Alice\"\n+\n+\n+# Test validate by alias\n+\n+\n+@pytest.mark.parametrize(\n+    \"model\",\n+    [PydanticUser, SQLModelUser, PydanticUserWithConfig, SQLModelUserWithConfig],\n+)\n+def test_create_with_alias(\n+    model: Union[\n+        Type[PydanticUser],\n+        Type[SQLModelUser],\n+        Type[PydanticUserWithConfig],\n+        Type[SQLModelUserWithConfig],\n+    ],\n+):\n+    user = model(fullName=\"Bob\")  # using alias\n+    assert user.full_name == \"Bob\"\n+\n+\n+# Test validate by name and alias\n+\n+\n+@pytest.mark.parametrize(\"model\", [PydanticUserWithConfig, SQLModelUserWithConfig])\n+def test_create_with_both_prefers_alias(\n+    model: Union[Type[PydanticUserWithConfig], Type[SQLModelUserWithConfig]],\n+):\n+    user = model(full_name=\"IGNORED\", fullName=\"Charlie\")\n+    assert user.full_name == \"Charlie\"  # alias should take precedence\n+\n+\n+# Test serialize\n+\n+\n+@pytest.mark.parametrize(\"model\", [PydanticUser, SQLModelUser])\n+def test_dict_default_uses_field_names(\n+    model: Union[Type[PydanticUser], Type[SQLModelUser]],\n+):\n+    user = model(fullName=\"Dana\")\n+    data = user.dict()\n+    assert \"full_name\" in data\n+    assert \"fullName\" not in data\n+    assert data[\"full_name\"] == \"Dana\"\n+\n+\n+# Test serialize by alias\n+\n+\n+@pytest.mark.parametrize(\"model\", [PydanticUser, SQLModelUser])\n+def test_dict_default_uses_aliases(\n+    model: Union[Type[PydanticUser], Type[SQLModelUser]],\n+):\n+    user = model(fullName=\"Dana\")\n+    data = user.dict(by_alias=True)\n+    assert \"fullName\" in data\n+    assert \"full_name\" not in data\n+    assert data[\"fullName\"] == \"Dana\"\n+\n+\n+# Test json by alias\n+\n+\n+@pytest.mark.parametrize(\"model\", [PydanticUser, SQLModelUser])\n+def test_json_by_alias(\n+    model: Union[Type[PydanticUser], Type[SQLModelUser]],\n+):\n+    user = model(fullName=\"Frank\")\n+    json_data = user.json(by_alias=True)\n+    assert ('\"fullName\":\"Frank\"' in json_data) or ('\"fullName\": \"Frank\"' in json_data)\n+    assert \"full_name\" not in json_data\n+\n+\n+# Pydantic v2 specific models - only define if we're running Pydantic v2\n+if VERSION.startswith(\"2.\"):\n+\n+    class PydanticUserV2(BaseModel):\n+        first_name: str = PField(\n+            validation_alias=\"firstName\", serialization_alias=\"f_name\"\n+        )\n+\n+    class SQLModelUserV2(SQLModel):\n+        first_name: str = Field(\n+            validation_alias=\"firstName\", serialization_alias=\"f_name\"\n+        )\n+else:\n+    # Dummy classes for Pydantic v1 to prevent import errors\n+    PydanticUserV2 = None\n+    SQLModelUserV2 = None\n+\n+\n+@pytest.mark.skipif(\n+    not VERSION.startswith(\"2.\"),\n+    reason=\"validation_alias and serialization_alias are not supported in Pydantic v1\",\n+)",
      "comment": "```suggestion\r\n@needs_pydanticv2\r\n```\r\n\r\nWe should use `@needs_pydanticv2` decorator as [it's usually done](https://github.com/fastapi/sqlmodel/blob/69b20188bdfec20bcda11c2b73be3e3d571047a8/tests/test_annotated_uuid.py#L9) in this code base\r\n```\r\nfrom tests.conftest import needs_pydanticv2\r\n```\r\n",
      "comment_id": 2392454534,
      "user": "YuriiMotov",
      "created_at": "2025-09-30T18:16:14Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/1577#discussion_r2392454534"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 1577,
      "file_path": "tests/test_aliases.py",
      "line": 148,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,176 @@\n+from typing import Type, Union\n+\n+import pytest\n+from pydantic import VERSION, BaseModel, ValidationError\n+from pydantic import Field as PField\n+from sqlmodel import Field, SQLModel\n+\n+# -----------------------------------------------------------------------------------\n+# Models\n+\n+\n+class PydanticUser(BaseModel):\n+    full_name: str = PField(alias=\"fullName\")\n+\n+\n+class SQLModelUser(SQLModel):\n+    full_name: str = Field(alias=\"fullName\")\n+\n+\n+# Models with config (validate_by_name=True)\n+\n+\n+if VERSION.startswith(\"2.\"):\n+\n+    class PydanticUserWithConfig(PydanticUser):\n+        model_config = {\"validate_by_name\": True}\n+\n+    class SQLModelUserWithConfig(SQLModelUser):\n+        model_config = {\"validate_by_name\": True}\n+\n+else:\n+\n+    class PydanticUserWithConfig(PydanticUser):\n+        class Config:\n+            allow_population_by_field_name = True\n+\n+    class SQLModelUserWithConfig(SQLModelUser):\n+        class Config:\n+            allow_population_by_field_name = True\n+\n+\n+# -----------------------------------------------------------------------------------\n+# Tests\n+\n+# Test validate by name\n+\n+\n+@pytest.mark.parametrize(\"model\", [PydanticUser, SQLModelUser])\n+def test_create_with_field_name(model: Union[Type[PydanticUser], Type[SQLModelUser]]):\n+    with pytest.raises(ValidationError):\n+        model(full_name=\"Alice\")\n+\n+\n+@pytest.mark.parametrize(\"model\", [PydanticUserWithConfig, SQLModelUserWithConfig])\n+def test_create_with_field_name_with_config(\n+    model: Union[Type[PydanticUserWithConfig], Type[SQLModelUserWithConfig]],\n+):\n+    user = model(full_name=\"Alice\")\n+    assert user.full_name == \"Alice\"\n+\n+\n+# Test validate by alias\n+\n+\n+@pytest.mark.parametrize(\n+    \"model\",\n+    [PydanticUser, SQLModelUser, PydanticUserWithConfig, SQLModelUserWithConfig],\n+)\n+def test_create_with_alias(\n+    model: Union[\n+        Type[PydanticUser],\n+        Type[SQLModelUser],\n+        Type[PydanticUserWithConfig],\n+        Type[SQLModelUserWithConfig],\n+    ],\n+):\n+    user = model(fullName=\"Bob\")  # using alias\n+    assert user.full_name == \"Bob\"\n+\n+\n+# Test validate by name and alias\n+\n+\n+@pytest.mark.parametrize(\"model\", [PydanticUserWithConfig, SQLModelUserWithConfig])\n+def test_create_with_both_prefers_alias(\n+    model: Union[Type[PydanticUserWithConfig], Type[SQLModelUserWithConfig]],\n+):\n+    user = model(full_name=\"IGNORED\", fullName=\"Charlie\")\n+    assert user.full_name == \"Charlie\"  # alias should take precedence\n+\n+\n+# Test serialize\n+\n+\n+@pytest.mark.parametrize(\"model\", [PydanticUser, SQLModelUser])\n+def test_dict_default_uses_field_names(\n+    model: Union[Type[PydanticUser], Type[SQLModelUser]],\n+):\n+    user = model(fullName=\"Dana\")\n+    data = user.dict()\n+    assert \"full_name\" in data\n+    assert \"fullName\" not in data\n+    assert data[\"full_name\"] == \"Dana\"\n+\n+\n+# Test serialize by alias\n+\n+\n+@pytest.mark.parametrize(\"model\", [PydanticUser, SQLModelUser])\n+def test_dict_default_uses_aliases(\n+    model: Union[Type[PydanticUser], Type[SQLModelUser]],\n+):\n+    user = model(fullName=\"Dana\")\n+    data = user.dict(by_alias=True)\n+    assert \"fullName\" in data\n+    assert \"full_name\" not in data\n+    assert data[\"fullName\"] == \"Dana\"\n+\n+\n+# Test json by alias\n+\n+\n+@pytest.mark.parametrize(\"model\", [PydanticUser, SQLModelUser])\n+def test_json_by_alias(\n+    model: Union[Type[PydanticUser], Type[SQLModelUser]],\n+):\n+    user = model(fullName=\"Frank\")\n+    json_data = user.json(by_alias=True)\n+    assert ('\"fullName\":\"Frank\"' in json_data) or ('\"fullName\": \"Frank\"' in json_data)\n+    assert \"full_name\" not in json_data\n+\n+\n+# Pydantic v2 specific models - only define if we're running Pydantic v2\n+if VERSION.startswith(\"2.\"):\n+\n+    class PydanticUserV2(BaseModel):\n+        first_name: str = PField(\n+            validation_alias=\"firstName\", serialization_alias=\"f_name\"\n+        )\n+\n+    class SQLModelUserV2(SQLModel):\n+        first_name: str = Field(\n+            validation_alias=\"firstName\", serialization_alias=\"f_name\"\n+        )\n+else:\n+    # Dummy classes for Pydantic v1 to prevent import errors\n+    PydanticUserV2 = None\n+    SQLModelUserV2 = None",
      "comment": "We also need to test the logic with `RuntimeError` for Pydantic V1\r\n\r\n```py\r\n        if validation_alias:\r\n            raise RuntimeError(\"validation_alias is not supported in Pydantic v1\")\r\n        if serialization_alias:\r\n            raise RuntimeError(\"serialization_alias is not supported in Pydantic v1\")\r\n```",
      "comment_id": 2392464519,
      "user": "YuriiMotov",
      "created_at": "2025-09-30T18:21:10Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/1577#discussion_r2392464519"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 1577,
      "file_path": "sqlmodel/main.py",
      "line": 218,
      "side": "RIGHT",
      "diff_hunk": "@@ -215,6 +215,8 @@ def Field(\n     *,\n     default_factory: Optional[NoArgAnyCallable] = None,\n     alias: Optional[str] = None,\n+    validation_alias: Optional[str] = None,",
      "comment": "Pydantic's `validation_alias` type annotation is wider (`validation_alias: str | AliasPath | AliasChoices | None`), but I think it's fine if we only support `str` for now and extend it later",
      "comment_id": 2392474638,
      "user": "YuriiMotov",
      "created_at": "2025-09-30T18:26:07Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/1577#discussion_r2392474638"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 1577,
      "file_path": "sqlmodel/_compat.py",
      "line": 224,
      "side": "RIGHT",
      "diff_hunk": "@@ -221,7 +221,13 @@ def get_field_metadata(field: Any) -> Any:\n         return FakeMetadata()\n \n     def post_init_field_info(field_info: FieldInfo) -> None:\n-        return None\n+        if IS_PYDANTIC_V2:",
      "comment": "Added for backward compatibility with Pydantic v1.\nWithout this, Pydantic tests v1 would fail.\n\n",
      "comment_id": 2392903040,
      "user": "ravishan16",
      "created_at": "2025-09-30T21:46:55Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/1577#discussion_r2392903040"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 1577,
      "file_path": "sqlmodel/main.py",
      "line": 445,
      "side": "RIGHT",
      "diff_hunk": "@@ -387,43 +395,65 @@ def Field(\n     schema_extra: Optional[Dict[str, Any]] = None,\n ) -> Any:\n     current_schema_extra = schema_extra or {}\n-    field_info = FieldInfo(\n-        default,\n-        default_factory=default_factory,\n-        alias=alias,\n-        title=title,\n-        description=description,\n-        exclude=exclude,\n-        include=include,\n-        const=const,\n-        gt=gt,\n-        ge=ge,\n-        lt=lt,\n-        le=le,\n-        multiple_of=multiple_of,\n-        max_digits=max_digits,\n-        decimal_places=decimal_places,\n-        min_items=min_items,\n-        max_items=max_items,\n-        unique_items=unique_items,\n-        min_length=min_length,\n-        max_length=max_length,\n-        allow_mutation=allow_mutation,\n-        regex=regex,\n-        discriminator=discriminator,\n-        repr=repr,\n-        primary_key=primary_key,\n-        foreign_key=foreign_key,\n-        ondelete=ondelete,\n-        unique=unique,\n-        nullable=nullable,\n-        index=index,\n-        sa_type=sa_type,\n-        sa_column=sa_column,\n-        sa_column_args=sa_column_args,\n-        sa_column_kwargs=sa_column_kwargs,\n+    field_info_kwargs = {\n+        \"alias\": alias,\n+        \"title\": title,\n+        \"description\": description,\n+        \"exclude\": exclude,\n+        \"include\": include,\n+        \"const\": const,\n+        \"gt\": gt,\n+        \"ge\": ge,\n+        \"lt\": lt,\n+        \"le\": le,\n+        \"multiple_of\": multiple_of,\n+        \"max_digits\": max_digits,\n+        \"decimal_places\": decimal_places,\n+        \"min_items\": min_items,\n+        \"max_items\": max_items,\n+        \"unique_items\": unique_items,\n+        \"min_length\": min_length,\n+        \"max_length\": max_length,\n+        \"allow_mutation\": allow_mutation,\n+        \"regex\": regex,\n+        \"discriminator\": discriminator,\n+        \"repr\": repr,\n+        \"primary_key\": primary_key,\n+        \"foreign_key\": foreign_key,\n+        \"ondelete\": ondelete,\n+        \"unique\": unique,\n+        \"nullable\": nullable,\n+        \"index\": index,\n+        \"sa_type\": sa_type,\n+        \"sa_column\": sa_column,\n+        \"sa_column_args\": sa_column_args,\n+        \"sa_column_kwargs\": sa_column_kwargs,\n         **current_schema_extra,\n-    )\n+    }\n+    if IS_PYDANTIC_V2:\n+        # Add Pydantic v2 specific parameters\n+        field_info_kwargs.update(\n+            {\n+                \"validation_alias\": validation_alias,\n+                \"serialization_alias\": serialization_alias,\n+            }\n+        )\n+        field_info = FieldInfo(\n+            default,\n+            default_factory=default_factory,\n+            **field_info_kwargs,\n+        )",
      "comment": "Good catch! I've fixed this by moving it outside the if/else block to avoid duplication.",
      "comment_id": 2392937768,
      "user": "ravishan16",
      "created_at": "2025-09-30T22:10:10Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/1577#discussion_r2392937768"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 1577,
      "file_path": "tests/test_aliases.py",
      "line": 12,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,176 @@\n+from typing import Type, Union\n+\n+import pytest\n+from pydantic import VERSION, BaseModel, ValidationError\n+from pydantic import Field as PField\n+from sqlmodel import Field, SQLModel\n+\n+# -----------------------------------------------------------------------------------\n+# Models\n+\n+\n+class PydanticUser(BaseModel):",
      "comment": "Personally, I think it\u2019s useful to keep the parametrization for now, since it acts as a kind of integration test between the two libraries.",
      "comment_id": 2393050952,
      "user": "ravishan16",
      "created_at": "2025-09-30T23:26:05Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/1577#discussion_r2393050952"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 1577,
      "file_path": "sqlmodel/_compat.py",
      "line": 224,
      "side": "RIGHT",
      "diff_hunk": "@@ -221,7 +221,13 @@ def get_field_metadata(field: Any) -> Any:\n         return FakeMetadata()\n \n     def post_init_field_info(field_info: FieldInfo) -> None:\n-        return None\n+        if IS_PYDANTIC_V2:",
      "comment": "I just checked it with Python 3.11 and Pydantic 1.10.22 - tests passed.\r\nCould you please check it one more time?\r\n```py\r\n    def post_init_field_info(field_info: FieldInfo) -> None:\r\n        if field_info.alias and not field_info.validation_alias:\r\n            field_info.validation_alias = field_info.alias\r\n        if field_info.alias and not field_info.serialization_alias:\r\n            field_info.serialization_alias = field_info.alias\r\n```",
      "comment_id": 2405320135,
      "user": "YuriiMotov",
      "created_at": "2025-10-06T08:27:43Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/1577#discussion_r2405320135"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 1577,
      "file_path": "tests/test_aliases.py",
      "line": 152,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,178 @@\n+from typing import Type, Union\n+\n+import pytest\n+from pydantic import VERSION, BaseModel, ValidationError\n+from pydantic import Field as PField\n+from sqlmodel import Field, SQLModel\n+\n+from tests.conftest import needs_pydanticv2\n+\n+\"\"\"\n+Alias tests for SQLModel and Pydantic compatibility\n+\"\"\"\n+\n+\n+class PydanticUser(BaseModel):\n+    full_name: str = PField(alias=\"fullName\")\n+\n+\n+class SQLModelUser(SQLModel):\n+    full_name: str = Field(alias=\"fullName\")\n+\n+\n+# Models with config (validate_by_name=True)\n+\n+\n+if VERSION.startswith(\"2.\"):\n+\n+    class PydanticUserWithConfig(PydanticUser):\n+        model_config = {\"validate_by_name\": True}\n+\n+    class SQLModelUserWithConfig(SQLModelUser):\n+        model_config = {\"validate_by_name\": True}\n+\n+else:\n+\n+    class PydanticUserWithConfig(PydanticUser):\n+        class Config:\n+            allow_population_by_field_name = True\n+\n+    class SQLModelUserWithConfig(SQLModelUser):\n+        class Config:\n+            allow_population_by_field_name = True\n+\n+\n+@pytest.mark.parametrize(\"model\", [PydanticUser, SQLModelUser])\n+def test_create_with_field_name(model: Union[Type[PydanticUser], Type[SQLModelUser]]):\n+    with pytest.raises(ValidationError):\n+        model(full_name=\"Alice\")\n+\n+\n+@pytest.mark.parametrize(\"model\", [PydanticUserWithConfig, SQLModelUserWithConfig])\n+def test_create_with_field_name_with_config(\n+    model: Union[Type[PydanticUserWithConfig], Type[SQLModelUserWithConfig]],\n+):\n+    user = model(full_name=\"Alice\")\n+    assert user.full_name == \"Alice\"\n+\n+\n+@pytest.mark.parametrize(\n+    \"model\",\n+    [PydanticUser, SQLModelUser, PydanticUserWithConfig, SQLModelUserWithConfig],\n+)\n+def test_create_with_alias(\n+    model: Union[\n+        Type[PydanticUser],\n+        Type[SQLModelUser],\n+        Type[PydanticUserWithConfig],\n+        Type[SQLModelUserWithConfig],\n+    ],\n+):\n+    user = model(fullName=\"Bob\")  # using alias\n+    assert user.full_name == \"Bob\"\n+\n+\n+@pytest.mark.parametrize(\"model\", [PydanticUserWithConfig, SQLModelUserWithConfig])\n+def test_create_with_both_prefers_alias(\n+    model: Union[Type[PydanticUserWithConfig], Type[SQLModelUserWithConfig]],\n+):\n+    user = model(full_name=\"IGNORED\", fullName=\"Charlie\")\n+    assert user.full_name == \"Charlie\"  # alias should take precedence\n+\n+\n+@pytest.mark.parametrize(\"model\", [PydanticUser, SQLModelUser])\n+def test_dict_default_uses_field_names(\n+    model: Union[Type[PydanticUser], Type[SQLModelUser]],\n+):\n+    user = model(fullName=\"Dana\")\n+    if VERSION.startswith(\"2.\"):\n+        data = user.model_dump()\n+    else:\n+        data = user.dict()\n+    assert \"full_name\" in data\n+    assert \"fullName\" not in data\n+    assert data[\"full_name\"] == \"Dana\"\n+\n+\n+@pytest.mark.parametrize(\"model\", [PydanticUser, SQLModelUser])\n+def test_dict_default_uses_aliases(\n+    model: Union[Type[PydanticUser], Type[SQLModelUser]],\n+):\n+    user = model(fullName=\"Dana\")\n+    if VERSION.startswith(\"2.\"):\n+        data = user.model_dump(by_alias=True)\n+    else:\n+        data = user.dict(by_alias=True)\n+    assert \"fullName\" in data\n+    assert \"full_name\" not in data\n+    assert data[\"fullName\"] == \"Dana\"\n+\n+\n+@pytest.mark.parametrize(\"model\", [PydanticUser, SQLModelUser])\n+def test_json_by_alias(\n+    model: Union[Type[PydanticUser], Type[SQLModelUser]],\n+):\n+    user = model(fullName=\"Frank\")\n+    if VERSION.startswith(\"2.\"):\n+        json_data = user.model_dump_json(by_alias=True)\n+    else:\n+        json_data = user.json(by_alias=True)\n+    assert ('\"fullName\":\"Frank\"' in json_data) or ('\"fullName\": \"Frank\"' in json_data)\n+    assert \"full_name\" not in json_data\n+\n+\n+if VERSION.startswith(\"2.\"):\n+\n+    class PydanticUserV2(BaseModel):\n+        first_name: str = PField(\n+            validation_alias=\"firstName\", serialization_alias=\"f_name\"\n+        )\n+\n+    class SQLModelUserV2(SQLModel):\n+        first_name: str = Field(\n+            validation_alias=\"firstName\", serialization_alias=\"f_name\"\n+        )\n+else:\n+    # Dummy classes for Pydantic v1 to prevent import errors\n+    PydanticUserV2 = None\n+    SQLModelUserV2 = None\n+\n+\n+def test_validation_alias_runtimeerror_pydantic_v1():\n+    if VERSION.startswith(\"2.\"):\n+        pytest.skip(\"Only relevant for Pydantic v1\")\n+    with pytest.raises(\n+        RuntimeError, match=\"validation_alias is not supported in Pydantic v1\"\n+    ):\n+        Field(validation_alias=\"foo\")\n+\n+\n+def test_serialization_alias_runtimeerror_pydantic_v1():\n+    if VERSION.startswith(\"2.\"):\n+        pytest.skip(\"Only relevant for Pydantic v1\")",
      "comment": "```suggestion\r\n@needs_pydanticv1\r\ndef test_serialization_alias_runtimeerror_pydantic_v1():\r\n```",
      "comment_id": 2405353223,
      "user": "YuriiMotov",
      "created_at": "2025-10-06T08:40:39Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/1577#discussion_r2405353223"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 1577,
      "file_path": "tests/test_aliases.py",
      "line": 143,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,178 @@\n+from typing import Type, Union\n+\n+import pytest\n+from pydantic import VERSION, BaseModel, ValidationError\n+from pydantic import Field as PField\n+from sqlmodel import Field, SQLModel\n+\n+from tests.conftest import needs_pydanticv2\n+\n+\"\"\"\n+Alias tests for SQLModel and Pydantic compatibility\n+\"\"\"\n+\n+\n+class PydanticUser(BaseModel):\n+    full_name: str = PField(alias=\"fullName\")\n+\n+\n+class SQLModelUser(SQLModel):\n+    full_name: str = Field(alias=\"fullName\")\n+\n+\n+# Models with config (validate_by_name=True)\n+\n+\n+if VERSION.startswith(\"2.\"):\n+\n+    class PydanticUserWithConfig(PydanticUser):\n+        model_config = {\"validate_by_name\": True}\n+\n+    class SQLModelUserWithConfig(SQLModelUser):\n+        model_config = {\"validate_by_name\": True}\n+\n+else:\n+\n+    class PydanticUserWithConfig(PydanticUser):\n+        class Config:\n+            allow_population_by_field_name = True\n+\n+    class SQLModelUserWithConfig(SQLModelUser):\n+        class Config:\n+            allow_population_by_field_name = True\n+\n+\n+@pytest.mark.parametrize(\"model\", [PydanticUser, SQLModelUser])\n+def test_create_with_field_name(model: Union[Type[PydanticUser], Type[SQLModelUser]]):\n+    with pytest.raises(ValidationError):\n+        model(full_name=\"Alice\")\n+\n+\n+@pytest.mark.parametrize(\"model\", [PydanticUserWithConfig, SQLModelUserWithConfig])\n+def test_create_with_field_name_with_config(\n+    model: Union[Type[PydanticUserWithConfig], Type[SQLModelUserWithConfig]],\n+):\n+    user = model(full_name=\"Alice\")\n+    assert user.full_name == \"Alice\"\n+\n+\n+@pytest.mark.parametrize(\n+    \"model\",\n+    [PydanticUser, SQLModelUser, PydanticUserWithConfig, SQLModelUserWithConfig],\n+)\n+def test_create_with_alias(\n+    model: Union[\n+        Type[PydanticUser],\n+        Type[SQLModelUser],\n+        Type[PydanticUserWithConfig],\n+        Type[SQLModelUserWithConfig],\n+    ],\n+):\n+    user = model(fullName=\"Bob\")  # using alias\n+    assert user.full_name == \"Bob\"\n+\n+\n+@pytest.mark.parametrize(\"model\", [PydanticUserWithConfig, SQLModelUserWithConfig])\n+def test_create_with_both_prefers_alias(\n+    model: Union[Type[PydanticUserWithConfig], Type[SQLModelUserWithConfig]],\n+):\n+    user = model(full_name=\"IGNORED\", fullName=\"Charlie\")\n+    assert user.full_name == \"Charlie\"  # alias should take precedence\n+\n+\n+@pytest.mark.parametrize(\"model\", [PydanticUser, SQLModelUser])\n+def test_dict_default_uses_field_names(\n+    model: Union[Type[PydanticUser], Type[SQLModelUser]],\n+):\n+    user = model(fullName=\"Dana\")\n+    if VERSION.startswith(\"2.\"):\n+        data = user.model_dump()\n+    else:\n+        data = user.dict()\n+    assert \"full_name\" in data\n+    assert \"fullName\" not in data\n+    assert data[\"full_name\"] == \"Dana\"\n+\n+\n+@pytest.mark.parametrize(\"model\", [PydanticUser, SQLModelUser])\n+def test_dict_default_uses_aliases(\n+    model: Union[Type[PydanticUser], Type[SQLModelUser]],\n+):\n+    user = model(fullName=\"Dana\")\n+    if VERSION.startswith(\"2.\"):\n+        data = user.model_dump(by_alias=True)\n+    else:\n+        data = user.dict(by_alias=True)\n+    assert \"fullName\" in data\n+    assert \"full_name\" not in data\n+    assert data[\"fullName\"] == \"Dana\"\n+\n+\n+@pytest.mark.parametrize(\"model\", [PydanticUser, SQLModelUser])\n+def test_json_by_alias(\n+    model: Union[Type[PydanticUser], Type[SQLModelUser]],\n+):\n+    user = model(fullName=\"Frank\")\n+    if VERSION.startswith(\"2.\"):\n+        json_data = user.model_dump_json(by_alias=True)\n+    else:\n+        json_data = user.json(by_alias=True)\n+    assert ('\"fullName\":\"Frank\"' in json_data) or ('\"fullName\": \"Frank\"' in json_data)\n+    assert \"full_name\" not in json_data\n+\n+\n+if VERSION.startswith(\"2.\"):\n+\n+    class PydanticUserV2(BaseModel):\n+        first_name: str = PField(\n+            validation_alias=\"firstName\", serialization_alias=\"f_name\"\n+        )\n+\n+    class SQLModelUserV2(SQLModel):\n+        first_name: str = Field(\n+            validation_alias=\"firstName\", serialization_alias=\"f_name\"\n+        )\n+else:\n+    # Dummy classes for Pydantic v1 to prevent import errors\n+    PydanticUserV2 = None\n+    SQLModelUserV2 = None\n+\n+\n+def test_validation_alias_runtimeerror_pydantic_v1():\n+    if VERSION.startswith(\"2.\"):\n+        pytest.skip(\"Only relevant for Pydantic v1\")",
      "comment": "```suggestion\r\n@needs_pydanticv1\r\ndef test_validation_alias_runtimeerror_pydantic_v1():\r\n```",
      "comment_id": 2405354804,
      "user": "YuriiMotov",
      "created_at": "2025-10-06T08:41:19Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/1577#discussion_r2405354804"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 1577,
      "file_path": "tests/test_aliases.py",
      "line": 8,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,178 @@\n+from typing import Type, Union\n+\n+import pytest\n+from pydantic import VERSION, BaseModel, ValidationError\n+from pydantic import Field as PField\n+from sqlmodel import Field, SQLModel\n+\n+from tests.conftest import needs_pydanticv2",
      "comment": "```suggestion\r\nfrom tests.conftest import needs_pydanticv1, needs_pydanticv2\r\n```",
      "comment_id": 2405458711,
      "user": "YuriiMotov",
      "created_at": "2025-10-06T09:20:17Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/1577#discussion_r2405458711"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 1577,
      "file_path": "sqlmodel/_compat.py",
      "line": 224,
      "side": "RIGHT",
      "diff_hunk": "@@ -221,7 +221,13 @@ def get_field_metadata(field: Any) -> Any:\n         return FakeMetadata()\n \n     def post_init_field_info(field_info: FieldInfo) -> None:\n-        return None\n+        if IS_PYDANTIC_V2:",
      "comment": "You are right. I checked and tests passed. Thanks.",
      "comment_id": 2415362542,
      "user": "ravishan16",
      "created_at": "2025-10-09T01:42:32Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/1577#discussion_r2415362542"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 997,
      "file_path": "tests/test_update.py",
      "line": 1,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,23 @@\n+from sqlmodel import Field, SQLModel, create_engine",
      "comment": "```suggestion\r\nfrom sqlmodel import Field, SQLModel\r\n```",
      "comment_id": 2292048896,
      "user": "YuriiMotov",
      "created_at": "2025-08-21T20:17:51Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/997#discussion_r2292048896"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 997,
      "file_path": "tests/test_update.py",
      "line": 14,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,23 @@\n+from sqlmodel import Field, SQLModel, create_engine\n+\n+\n+def test_sqlmodel_update():\n+    class Organization(SQLModel, table=True):\n+        id: int = Field(default=None, primary_key=True)\n+        name: str\n+        headquarters: str\n+\n+    class OrganizationUpdate(SQLModel):\n+        name: str\n+\n+    engine = create_engine(\"sqlite:///\", echo=True)\n+    SQLModel.metadata.create_all(engine)",
      "comment": "```suggestion\r\n```\r\n\r\nWe don't need this for this test",
      "comment_id": 2292050955,
      "user": "YuriiMotov",
      "created_at": "2025-08-21T20:18:52Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/997#discussion_r2292050955"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 1560,
      "file_path": "sqlmodel/_compat.py",
      "line": 126,
      "side": "RIGHT",
      "diff_hunk": "@@ -123,7 +123,7 @@ def init_pydantic_private_attrs(new_object: InstanceOrType[\"SQLModel\"]) -> None:\n         object.__setattr__(new_object, \"__pydantic_private__\", None)\n \n     def get_annotations(class_dict: Dict[str, Any]) -> Dict[str, Any]:\n-        return class_dict.get(\"__annotations__\", {})\n+        return class_dict.get(\"__annotations__\", {})  # type: ignore[no-any-return]",
      "comment": "As on L425, where we're already ignoring it, the error is \r\n> Returning Any from function declared to return \"dict[str, Any]\"  [no-any-return]\r\n",
      "comment_id": 2352561220,
      "user": "svlandeg",
      "created_at": "2025-09-16T13:41:07Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/1560#discussion_r2352561220"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 1578,
      "file_path": "sqlmodel/_compat.py",
      "line": 127,
      "side": "RIGHT",
      "diff_hunk": "@@ -123,7 +124,20 @@ def init_pydantic_private_attrs(new_object: InstanceOrType[\"SQLModel\"]) -> None:\n         object.__setattr__(new_object, \"__pydantic_private__\", None)\n \n     def get_annotations(class_dict: Dict[str, Any]) -> Dict[str, Any]:\n-        return class_dict.get(\"__annotations__\", {})  # type: ignore[no-any-return]\n+        raw_annotations: Dict[str, Any] = class_dict.get(\"__annotations__\", {})",
      "comment": "Note that https://github.com/fastapi/sqlmodel/discussions/1594 proposes an alternative implementation, using `class_dict.get(\"__annotate_func__\")`",
      "comment_id": 2414049442,
      "user": "svlandeg",
      "created_at": "2025-10-08T14:24:35Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/1578#discussion_r2414049442"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 1307,
      "file_path": "tests/test_select_gen.py",
      "line": 15,
      "side": "RIGHT",
      "diff_hunk": "@@ -10,8 +11,8 @@\n @needs_py39\n def test_select_gen() -> None:\n     result = subprocess.run(\n-        [sys.executable, \"scripts/generate_select.py\"],\n-        env={\"CHECK_JINJA\": \"1\"},\n+        [sys.executable, Path(\"scripts\") / \"generate_select.py\"],\n+        env={**os.environ, \"CHECK_JINJA\": \"1\"},",
      "comment": "The `env` stuff is basically the same fix as what was done in https://github.com/fastapi/sqlmodel/pull/969, but shortened.\r\n\r\n(we can probably also just keep the current way it's done on `main` - as long as the things in `os.environ` are passed to `env` as well)",
      "comment_id": 2354865365,
      "user": "svlandeg",
      "created_at": "2025-09-17T09:17:42Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/1307#discussion_r2354865365"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 1340,
      "file_path": "sqlmodel/_compat.py",
      "line": 113,
      "side": "RIGHT",
      "diff_hunk": "@@ -103,7 +103,14 @@ def set_config_value(\n         model.model_config[parameter] = value  # type: ignore[literal-required]\n \n     def get_model_fields(model: InstanceOrType[BaseModel]) -> Dict[str, \"FieldInfo\"]:\n-        return model.model_fields\n+        # TODO: refactor the usage of this function to always pass the class\n+        # not the instance, and then remove this extra check\n+        # this is for compatibility with Pydantic v3\n+        if isinstance(model, type):\n+            use_model = model\n+        else:\n+            use_model = model.__class__\n+        return use_model.model_fields",
      "comment": "I was checking the error from Pydantic and I saw that they will deprecate accessing the `model_fields` from the instance in Pydantic v3, they should be accessed from the class (which makes sense).\r\n\r\nI updated this logic here to handle that future use case. In a subsequent PR we can refactor the internal usage of this function to only pass the class and not the instance.",
      "comment_id": 2061539455,
      "user": "tiangolo",
      "created_at": "2025-04-26T18:37:04Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/1340#discussion_r2061539455"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 1340,
      "file_path": "sqlmodel/main.py",
      "line": 480,
      "side": "RIGHT",
      "diff_hunk": "@@ -477,7 +477,7 @@ def Relationship(\n class SQLModelMetaclass(ModelMetaclass, DeclarativeMeta):\n     __sqlmodel_relationships__: Dict[str, RelationshipInfo]\n     model_config: SQLModelConfig\n-    model_fields: Dict[str, FieldInfo]  # type: ignore[assignment]\n+    model_fields: ClassVar[Dict[str, FieldInfo]]",
      "comment": "As the idea is that this should now be only in the class and not instances, I think it's good to type it right away. It won't affect at runtime if people are using it, but will start warning them in their linters, so they can start preparing for this change in Pydantic v3.",
      "comment_id": 2061539665,
      "user": "tiangolo",
      "created_at": "2025-04-26T18:38:09Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/1340#discussion_r2061539665"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 1340,
      "file_path": "sqlmodel/main.py",
      "line": 842,
      "side": "RIGHT",
      "diff_hunk": "@@ -839,7 +839,7 @@ def __tablename__(cls) -> str:\n         return cls.__name__.lower()\n \n     @classmethod\n-    def model_validate(\n+    def model_validate(  # type: ignore[override]",
      "comment": "I changed this to not use `kwargs` and instead ignore the type here, that way people will keep having autocompletion for the params, and kwargs won't swallow extra / invald params.\r\n\r\nSeeing this, I realize it was probably not the best idea to include the extra `update` here. :thinking: \r\n\r\nAlso, it's probably not needed for most use cases, maybe it would make sense to remove those docs, explain how to extend data to validate an object using just dicts, then deprecate using it, and finally at some point dropping the `update`... \r\n\r\nBut anyway, for now, it probably works to just `# type: ignore` and handle it on our side so that final users get the best developer experience / editor support.",
      "comment_id": 2061540780,
      "user": "tiangolo",
      "created_at": "2025-04-26T18:42:19Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/1340#discussion_r2061540780"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 1340,
      "file_path": "sqlmodel/main.py",
      "line": 867,
      "side": "RIGHT",
      "diff_hunk": "@@ -863,20 +863,24 @@ def model_dump(\n         mode: Union[Literal[\"json\", \"python\"], str] = \"python\",\n         include: Union[IncEx, None] = None,\n         exclude: Union[IncEx, None] = None,\n-        context: Union[Dict[str, Any], None] = None,\n-        by_alias: bool = False,\n+        context: Union[Any, None] = None,\n+        by_alias: Union[bool, None] = None,",
      "comment": "I like to keep the signature as close to the latest Pydantic, and then do the extra work internally to support the older versions, but this way people can get used to the new syntax and/or use it if they can already upgrade to the latest version.",
      "comment_id": 2061541161,
      "user": "tiangolo",
      "created_at": "2025-04-26T18:43:29Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/1340#discussion_r2061541161"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 1340,
      "file_path": "sqlmodel/main.py",
      "line": 903,
      "side": "RIGHT",
      "diff_hunk": "@@ -896,7 +900,7 @@ def model_dump(\n             return super().dict(\n                 include=include,\n                 exclude=exclude,\n-                by_alias=by_alias,\n+                by_alias=by_alias or False,",
      "comment": "With this trick, we avoid the type when `by_alias` is `None`. :sweat_smile: ",
      "comment_id": 2061541310,
      "user": "tiangolo",
      "created_at": "2025-04-26T18:44:11Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/1340#discussion_r2061541310"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 1340,
      "file_path": "sqlmodel/main.py",
      "line": 877,
      "side": "RIGHT",
      "diff_hunk": "@@ -863,20 +863,25 @@ def model_dump(\n         mode: Union[Literal[\"json\", \"python\"], str] = \"python\",\n         include: Union[IncEx, None] = None,\n         exclude: Union[IncEx, None] = None,\n-        context: Union[Dict[str, Any], None] = None,\n-        by_alias: bool = False,\n+        context: Union[Any, None] = None,\n+        by_alias: Union[bool, None] = None,\n         exclude_unset: bool = False,\n         exclude_defaults: bool = False,\n         exclude_none: bool = False,\n         round_trip: bool = False,\n         warnings: Union[bool, Literal[\"none\", \"warn\", \"error\"]] = True,\n+        fallback: Union[Callable[[Any], Any], None] = None,\n         serialize_as_any: bool = False,\n     ) -> Dict[str, Any]:\n+        if PYDANTIC_MINOR_VERSION < (2, 11):\n+            by_alias = by_alias or False",
      "comment": "With this we can handle older Pydantic versions but keep the most recent signature.",
      "comment_id": 2061546410,
      "user": "tiangolo",
      "created_at": "2025-04-26T18:58:36Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/1340#discussion_r2061546410"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 1255,
      "file_path": "sqlmodel/_compat.py",
      "line": 28,
      "side": "RIGHT",
      "diff_hunk": "@@ -25,7 +25,8 @@\n \n # Reassign variable to make it reexported for mypy\n PYDANTIC_VERSION = P_VERSION\n-IS_PYDANTIC_V2 = PYDANTIC_VERSION.startswith(\"2.\")\n+PYDANTIC_MINOR_VERSION = tuple(int(i) for i in P_VERSION.split(\".\", 2)[:2])",
      "comment": "Yes, this is much cleaner. \r\n\r\nI don't think the `maxsplit` parameter for `split()` makes any difference here, as we're not keeping the patch version part anyway, so I think we could might as well do \r\n```suggestion\r\nPYDANTIC_MINOR_VERSION = tuple(int(i) for i in P_VERSION.split(\".\")[:2])\r\n```\r\n\r\nright?",
      "comment_id": 1963713371,
      "user": "svlandeg",
      "created_at": "2025-02-20T14:50:08Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/1255#discussion_r1963713371"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 1255,
      "file_path": "sqlmodel/_compat.py",
      "line": 29,
      "side": "RIGHT",
      "diff_hunk": "@@ -25,7 +25,8 @@\n \n # Reassign variable to make it reexported for mypy\n PYDANTIC_VERSION = P_VERSION\n-IS_PYDANTIC_V2 = PYDANTIC_VERSION.startswith(\"2.\")\n+PYDANTIC_MINOR_VERSION = tuple(int(i) for i in P_VERSION.split(\".\", 2)[:2])\n+IS_PYDANTIC_V2 = PYDANTIC_MINOR_VERSION[0] >= 2",
      "comment": "To keep the semantics the same as before, and to avoid confusion of the semantics of this variable in the future when there would be a v3, I suggest to test for equality:\r\n```suggestion\r\nIS_PYDANTIC_V2 = PYDANTIC_MINOR_VERSION[0] == 2\r\n```",
      "comment_id": 1963715064,
      "user": "svlandeg",
      "created_at": "2025-02-20T14:50:55Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/1255#discussion_r1963715064"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 1255,
      "file_path": "sqlmodel/_compat.py",
      "line": 29,
      "side": "RIGHT",
      "diff_hunk": "@@ -25,7 +25,8 @@\n \n # Reassign variable to make it reexported for mypy\n PYDANTIC_VERSION = P_VERSION\n-IS_PYDANTIC_V2 = PYDANTIC_VERSION.startswith(\"2.\")\n+PYDANTIC_MINOR_VERSION = tuple(int(i) for i in P_VERSION.split(\".\", 2)[:2])\n+IS_PYDANTIC_V2 = PYDANTIC_MINOR_VERSION[0] >= 2",
      "comment": "@svlandeg I agree that the naming would be confusing with the check `>= 2`. I was thinking more about the idea of this variable: this flag is used to check what API to expect, and in all/most cases, it is expected to check whether the version is 2+ or not, rather than whether it is exactly 2.\r\nAnyways, it's not critical for today, just a thought for the future. Thanks for reviewing the PR!",
      "comment_id": 1986205776,
      "user": "asiunov",
      "created_at": "2025-03-09T03:08:43Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/1255#discussion_r1986205776"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 983,
      "file_path": "sqlmodel/main.py",
      "line": 710,
      "side": "RIGHT",
      "diff_hunk": "@@ -643,7 +704,11 @@ def get_column_from_field(field: Any) -> Column:  # type: ignore\n         unique = False\n     if foreign_key:\n         assert isinstance(foreign_key, str)\n-        args.append(ForeignKey(foreign_key))\n+        foreign_key_obj = ForeignKey(foreign_key)\n+        ondelete = getattr(field_info, \"ondelete\", Undefined)\n+        if isinstance(ondelete, str):\n+            foreign_key_obj.ondelete = ondelete",
      "comment": "Here we could better use the same trick of creating args and kwargs outside before creating the `ForeignKey` object and then create it once with everything. Instead of setting the `foreign_key_obj.ondelete = ondelete` afterwards.\r\n\r\nJust because SQLAlchemy could do some additional magic apart from setting the attribute when instantiating the object. It currently doesn't do anything, but it wouldn't be surprising if it did at some point, so better be safe. :nerd_face: ",
      "comment_id": 1638664752,
      "user": "tiangolo",
      "created_at": "2024-06-13T17:51:31Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/983#discussion_r1638664752"
    },
    {
      "repo": "tiangolo/sqlmodel",
      "pr_number": 983,
      "file_path": "sqlmodel/sql/_relationship_types.py",
      "line": 3,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,3 @@\n+from typing_extensions import Literal\n+\n+OnDeleteType = Literal[\"CASCADE\", \"SET NULL\", \"RESTRICT\", \"NO ACTION\"]",
      "comment": "Nice these literals!\r\n\r\nAs this is used in a single place, I'm thinking that maybe this type could be created there in the same `main.py`. As it's not imported anywhere else, and we don't have any other types to put here, at least yet.\r\n\r\nAlso to keep the directory structure as similar to SQLAlchemy's as possible.",
      "comment_id": 1638667721,
      "user": "tiangolo",
      "created_at": "2024-06-13T17:54:13Z",
      "url": "https://github.com/fastapi/sqlmodel/pull/983#discussion_r1638667721"
    },
    {
      "repo": "pallets/flask",
      "pr_number": 2635,
      "file_path": "flask/app.py",
      "line": 1975,
      "side": "RIGHT",
      "diff_hunk": "@@ -1963,8 +1970,15 @@ def create_url_adapter(self, request):\n            URL adapter is created for the application context.\n         \"\"\"\n         if request is not None:\n-            return self.url_map.bind_to_environ(request.environ,\n+            rv = self.url_map.bind_to_environ(request.environ,\n                 server_name=self.config['SERVER_NAME'])\n+            # If subdomain matching is not enabled (which is the default",
      "comment": "Note that this line is missing a closing parenthesis",
      "comment_id": 169991222,
      "user": "Derrreks",
      "created_at": "2018-02-22T15:21:01Z",
      "url": "https://github.com/pallets/flask/pull/2635#discussion_r169991222"
    },
    {
      "repo": "pallets/flask",
      "pr_number": 5127,
      "file_path": "src/flask/_static_mixin.py",
      "line": 21,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,63 @@\n+from __future__ import annotations\n+\n+import os\n+import typing as t\n+\n+from .helpers import send_from_directory\n+\n+if t.TYPE_CHECKING:  # pragma: no cover\n+    from .wrappers import Response\n+\n+\n+class ScaffoldMixin(t.Protocol):\n+    has_static_folder: bool\n+    root_path: str\n+    static_folder: str | None\n+\n+    def get_send_file_max_age(self, filename: str | None) -> int | None:\n+        ...\n+\n+\n+class _StaticMixin:",
      "comment": "Remove, implement in Blueprint and Flask (dupe is ok).\r\nNote \"This isn't Sansio, hence dupe in docstring\"",
      "comment_id": 1221931650,
      "user": "pgjones",
      "created_at": "2023-06-07T17:10:57Z",
      "url": "https://github.com/pallets/flask/pull/5127#discussion_r1221931650"
    },
    {
      "repo": "pallets/flask",
      "pr_number": 5127,
      "file_path": "src/flask/app.py",
      "line": 229,
      "side": "RIGHT",
      "diff_hunk": "@@ -365,124 +226,18 @@ def __init__(\n         root_path: str | None = None,\n     ):\n         super().__init__(\n-            import_name=import_name,\n-            static_folder=static_folder,\n-            static_url_path=static_url_path,\n-            template_folder=template_folder,\n-            root_path=root_path,\n+            import_name,",
      "comment": "It's annoying to write, but these should be passed as keyword arguments, I don't like to rely on positions.",
      "comment_id": 1299206826,
      "user": "davidism",
      "created_at": "2023-08-19T14:41:44Z",
      "url": "https://github.com/pallets/flask/pull/5127#discussion_r1299206826"
    },
    {
      "repo": "pallets/flask",
      "pr_number": 4992,
      "file_path": "src/flask/config.py",
      "line": 249,
      "side": "RIGHT",
      "diff_hunk": "@@ -245,7 +246,7 @@ def from_file(\n             app.config.from_file(\"config.json\", load=json.load)\n \n             import toml\n-            app.config.from_file(\"config.toml\", load=toml.load)\n+            app.config.from_file(\"config.toml\", load=toml.load, text=True)",
      "comment": "this parameter is a absolute lie - when true you take away the text and drop a buffer\r\n\r\nthis should be called `binary`\r\n\r\nand it should use the open mode ",
      "comment_id": 1114374958,
      "user": "RonnyPfannschmidt",
      "created_at": "2023-02-22T14:08:32Z",
      "url": "https://github.com/pallets/flask/pull/4992#discussion_r1114374958"
    },
    {
      "repo": "pallets/flask",
      "pr_number": 4992,
      "file_path": "src/flask/config.py",
      "line": 270,
      "side": "RIGHT",
      "diff_hunk": "@@ -256,12 +257,18 @@ def from_file(\n         :param silent: Ignore the file if it doesn't exist.\n         :return: ``True`` if the file was loaded successfully.\n \n+        .. versionchanged:: 2.3.0\n+            Added optional boolean argument \"text\" (False by default).\n+            You can now use .toml config file by specifying \"text=True\".\n+\n         .. versionadded:: 2.0\n         \"\"\"\n         filename = os.path.join(self.root_path, filename)\n \n         try:\n             with open(filename) as f:\n+                if text:",
      "comment": "This should change what `open` does, not try to access `buffer` after the fact (someone else pointed out this wasn't documented as a public API).\r\n\r\nAlso, it's backwards, `text=True`, the default, should result in a text file, `False` should result in a binary file.",
      "comment_id": 1114375396,
      "user": "davidism",
      "created_at": "2023-02-22T14:08:51Z",
      "url": "https://github.com/pallets/flask/pull/4992#discussion_r1114375396"
    },
    {
      "repo": "pallets/flask",
      "pr_number": 4992,
      "file_path": "src/flask/config.py",
      "line": 249,
      "side": "RIGHT",
      "diff_hunk": "@@ -245,7 +246,7 @@ def from_file(\n             app.config.from_file(\"config.json\", load=json.load)\n \n             import toml\n-            app.config.from_file(\"config.toml\", load=toml.load)\n+            app.config.from_file(\"config.toml\", load=toml.load, text=True)",
      "comment": "This still uses the `toml` library, it needs to change to `tomllib` to be a useful example of the parameter.",
      "comment_id": 1114380087,
      "user": "davidism",
      "created_at": "2023-02-22T14:11:52Z",
      "url": "https://github.com/pallets/flask/pull/4992#discussion_r1114380087"
    },
    {
      "repo": "pallets/flask",
      "pr_number": 4840,
      "file_path": "tests/test_templating.py",
      "line": 117,
      "side": "RIGHT",
      "diff_hunk": "@@ -114,6 +114,18 @@ def test_escaping_without_template_filename(app, client, req_ctx):\n     assert flask.render_template(\"mail.txt\", foo=\"<test>\") == \"<test> Mail\"\n \n \n+def test_escaping_svg(app, client):",
      "comment": "Don't need this test, it's just testing that Jinja works. At most, you wouuld test that `app.select_jinja_autoescape` returned `True` for a `.svg` filename.",
      "comment_id": 998530715,
      "user": "davidism",
      "created_at": "2022-10-18T17:39:09Z",
      "url": "https://github.com/pallets/flask/pull/4840#discussion_r998530715"
    },
    {
      "repo": "pallets/flask",
      "pr_number": 4692,
      "file_path": "src/flask/json/provider.py",
      "line": 106,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,310 @@\n+from __future__ import annotations\n+\n+import dataclasses\n+import decimal\n+import json\n+import typing as t\n+import uuid\n+import weakref\n+from datetime import date\n+\n+from werkzeug.http import http_date\n+\n+from ..globals import request\n+\n+if t.TYPE_CHECKING:  # pragma: no cover\n+    from ..app import Flask\n+    from ..wrappers import Response\n+\n+\n+class JSONProvider:\n+    \"\"\"A standard set of JSON operations for an application. Subclasses\n+    of this can be used to customize JSON behavior or use different\n+    JSON libraries.\n+\n+    To implement a provider for a specific library, subclass this base\n+    class and implement at least :meth:`dumps` and :meth:`loads`. All\n+    other methods have default implementations.\n+\n+    To use a different provider, either subclass ``Flask`` and set\n+    :attr:`~flask.Flask.json_provider_class` to a provider class, or set\n+    :attr:`app.json <flask.Flask.json>` to an instance of the class.\n+\n+    :param app: An application instance. This will be stored as a\n+        :class:`weakref.proxy` on the :attr:`_app` attribute.\n+\n+    .. versionadded:: 2.2\n+    \"\"\"\n+\n+    def __init__(self, app: Flask) -> None:\n+        self._app = weakref.proxy(app)\n+\n+    def dumps(self, obj: t.Any, **kwargs: t.Any) -> str:\n+        \"\"\"Serialize data as JSON.\n+\n+        :param obj: The data to serialize.\n+        :param kwargs: May be passed to the underlying JSON library.\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def dump(self, obj: t.Any, fp: t.IO[str], **kwargs: t.Any) -> None:\n+        \"\"\"Serialize data as JSON and write to a file.\n+\n+        :param obj: The data to serialize.\n+        :param fp: A file opened for writing text. Should use the UTF-8\n+            encoding to be valid JSON.\n+        :param kwargs: May be passed to the underlying JSON library.\n+        \"\"\"\n+        fp.write(self.dumps(obj, **kwargs))\n+\n+    def loads(self, s: str | bytes, **kwargs: t.Any) -> t.Any:\n+        \"\"\"Deserialize data as JSON.\n+\n+        :param s: Text or UTF-8 bytes.\n+        :param kwargs: May be passed to the underlying JSON library.\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def load(self, fp: t.IO[t.AnyStr], **kwargs: t.Any) -> t.Any:\n+        \"\"\"Deserialize data as JSON read from a file.\n+\n+        :param fp: A file opened for reading text or UTF-8 bytes.\n+        :param kwargs: May be passed to the underlying JSON library.\n+        \"\"\"\n+        return self.loads(fp.read(), **kwargs)\n+\n+    def _prepare_response_obj(\n+        self, args: t.Tuple[t.Any, ...], kwargs: t.Dict[str, t.Any]\n+    ) -> t.Any:\n+        if args and kwargs:\n+            raise TypeError(\"app.json.response() takes either args or kwargs, not both\")\n+\n+        if not args and not kwargs:\n+            return None\n+\n+        if len(args) == 1:\n+            return args[0]\n+\n+        return args or kwargs\n+\n+    def response(self, *args: t.Any, **kwargs: t.Any) -> Response:\n+        \"\"\"Serialize the given arguments as JSON, and return a\n+        :class:`~flask.Response` object with the ``application/json``\n+        mimetype.\n+\n+        The :func:`~flask.json.jsonify` function calls this method for\n+        the current application.\n+\n+        Either positional or keyword arguments can be given, not both.\n+        If no arguments are given, ``None`` is serialized.\n+\n+        :param args: A single value to serialize, or multiple values to\n+            treat as a list to serialize.\n+        :param kwargs: Treat as a dict to serialize.\n+        \"\"\"\n+        obj = self._prepare_response_obj(args, kwargs)\n+        return self._app.response_class(self.dumps(obj), mimetype=\"application/json\")",
      "comment": "Worth a cls default_mimetype here to save extensions having to override this method and as matching with responses?",
      "comment_id": 920306595,
      "user": "pgjones",
      "created_at": "2022-07-13T16:55:07Z",
      "url": "https://github.com/pallets/flask/pull/4692#discussion_r920306595"
    },
    {
      "repo": "pallets/flask",
      "pr_number": 4692,
      "file_path": "src/flask/json/provider.py",
      "line": 106,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,310 @@\n+from __future__ import annotations\n+\n+import dataclasses\n+import decimal\n+import json\n+import typing as t\n+import uuid\n+import weakref\n+from datetime import date\n+\n+from werkzeug.http import http_date\n+\n+from ..globals import request\n+\n+if t.TYPE_CHECKING:  # pragma: no cover\n+    from ..app import Flask\n+    from ..wrappers import Response\n+\n+\n+class JSONProvider:\n+    \"\"\"A standard set of JSON operations for an application. Subclasses\n+    of this can be used to customize JSON behavior or use different\n+    JSON libraries.\n+\n+    To implement a provider for a specific library, subclass this base\n+    class and implement at least :meth:`dumps` and :meth:`loads`. All\n+    other methods have default implementations.\n+\n+    To use a different provider, either subclass ``Flask`` and set\n+    :attr:`~flask.Flask.json_provider_class` to a provider class, or set\n+    :attr:`app.json <flask.Flask.json>` to an instance of the class.\n+\n+    :param app: An application instance. This will be stored as a\n+        :class:`weakref.proxy` on the :attr:`_app` attribute.\n+\n+    .. versionadded:: 2.2\n+    \"\"\"\n+\n+    def __init__(self, app: Flask) -> None:\n+        self._app = weakref.proxy(app)\n+\n+    def dumps(self, obj: t.Any, **kwargs: t.Any) -> str:\n+        \"\"\"Serialize data as JSON.\n+\n+        :param obj: The data to serialize.\n+        :param kwargs: May be passed to the underlying JSON library.\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def dump(self, obj: t.Any, fp: t.IO[str], **kwargs: t.Any) -> None:\n+        \"\"\"Serialize data as JSON and write to a file.\n+\n+        :param obj: The data to serialize.\n+        :param fp: A file opened for writing text. Should use the UTF-8\n+            encoding to be valid JSON.\n+        :param kwargs: May be passed to the underlying JSON library.\n+        \"\"\"\n+        fp.write(self.dumps(obj, **kwargs))\n+\n+    def loads(self, s: str | bytes, **kwargs: t.Any) -> t.Any:\n+        \"\"\"Deserialize data as JSON.\n+\n+        :param s: Text or UTF-8 bytes.\n+        :param kwargs: May be passed to the underlying JSON library.\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def load(self, fp: t.IO[t.AnyStr], **kwargs: t.Any) -> t.Any:\n+        \"\"\"Deserialize data as JSON read from a file.\n+\n+        :param fp: A file opened for reading text or UTF-8 bytes.\n+        :param kwargs: May be passed to the underlying JSON library.\n+        \"\"\"\n+        return self.loads(fp.read(), **kwargs)\n+\n+    def _prepare_response_obj(\n+        self, args: t.Tuple[t.Any, ...], kwargs: t.Dict[str, t.Any]\n+    ) -> t.Any:\n+        if args and kwargs:\n+            raise TypeError(\"app.json.response() takes either args or kwargs, not both\")\n+\n+        if not args and not kwargs:\n+            return None\n+\n+        if len(args) == 1:\n+            return args[0]\n+\n+        return args or kwargs\n+\n+    def response(self, *args: t.Any, **kwargs: t.Any) -> Response:\n+        \"\"\"Serialize the given arguments as JSON, and return a\n+        :class:`~flask.Response` object with the ``application/json``\n+        mimetype.\n+\n+        The :func:`~flask.json.jsonify` function calls this method for\n+        the current application.\n+\n+        Either positional or keyword arguments can be given, not both.\n+        If no arguments are given, ``None`` is serialized.\n+\n+        :param args: A single value to serialize, or multiple values to\n+            treat as a list to serialize.\n+        :param kwargs: Treat as a dict to serialize.\n+        \"\"\"\n+        obj = self._prepare_response_obj(args, kwargs)\n+        return self._app.response_class(self.dumps(obj), mimetype=\"application/json\")",
      "comment": "Discussed this more here: https://github.com/pallets/flask/pull/1728#issuecomment-1181806155\r\n\r\nI'm not sure this should actually be configurable at all. The original issue seemed to be about a specific type of response, not all JSON responses. *Maybe* if you want your whole API to have a different vendor type, like GitHub does, but even GitHub applies different mimetypes to different parts. APIs complex enough to use vendor types usually have versioning as well, so they still wouldn't apply globally.\r\n\r\nI did originally have this as a `JSONProvider.mimetype` attribute, but I ended up moving all existing behavior to `DefaultProvider` and keeping the base very simple.",
      "comment_id": 920311369,
      "user": "davidism",
      "created_at": "2022-07-13T17:00:28Z",
      "url": "https://github.com/pallets/flask/pull/4692#discussion_r920311369"
    },
    {
      "repo": "pallets/flask",
      "pr_number": 4692,
      "file_path": "src/flask/json/provider.py",
      "line": 106,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,310 @@\n+from __future__ import annotations\n+\n+import dataclasses\n+import decimal\n+import json\n+import typing as t\n+import uuid\n+import weakref\n+from datetime import date\n+\n+from werkzeug.http import http_date\n+\n+from ..globals import request\n+\n+if t.TYPE_CHECKING:  # pragma: no cover\n+    from ..app import Flask\n+    from ..wrappers import Response\n+\n+\n+class JSONProvider:\n+    \"\"\"A standard set of JSON operations for an application. Subclasses\n+    of this can be used to customize JSON behavior or use different\n+    JSON libraries.\n+\n+    To implement a provider for a specific library, subclass this base\n+    class and implement at least :meth:`dumps` and :meth:`loads`. All\n+    other methods have default implementations.\n+\n+    To use a different provider, either subclass ``Flask`` and set\n+    :attr:`~flask.Flask.json_provider_class` to a provider class, or set\n+    :attr:`app.json <flask.Flask.json>` to an instance of the class.\n+\n+    :param app: An application instance. This will be stored as a\n+        :class:`weakref.proxy` on the :attr:`_app` attribute.\n+\n+    .. versionadded:: 2.2\n+    \"\"\"\n+\n+    def __init__(self, app: Flask) -> None:\n+        self._app = weakref.proxy(app)\n+\n+    def dumps(self, obj: t.Any, **kwargs: t.Any) -> str:\n+        \"\"\"Serialize data as JSON.\n+\n+        :param obj: The data to serialize.\n+        :param kwargs: May be passed to the underlying JSON library.\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def dump(self, obj: t.Any, fp: t.IO[str], **kwargs: t.Any) -> None:\n+        \"\"\"Serialize data as JSON and write to a file.\n+\n+        :param obj: The data to serialize.\n+        :param fp: A file opened for writing text. Should use the UTF-8\n+            encoding to be valid JSON.\n+        :param kwargs: May be passed to the underlying JSON library.\n+        \"\"\"\n+        fp.write(self.dumps(obj, **kwargs))\n+\n+    def loads(self, s: str | bytes, **kwargs: t.Any) -> t.Any:\n+        \"\"\"Deserialize data as JSON.\n+\n+        :param s: Text or UTF-8 bytes.\n+        :param kwargs: May be passed to the underlying JSON library.\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def load(self, fp: t.IO[t.AnyStr], **kwargs: t.Any) -> t.Any:\n+        \"\"\"Deserialize data as JSON read from a file.\n+\n+        :param fp: A file opened for reading text or UTF-8 bytes.\n+        :param kwargs: May be passed to the underlying JSON library.\n+        \"\"\"\n+        return self.loads(fp.read(), **kwargs)\n+\n+    def _prepare_response_obj(\n+        self, args: t.Tuple[t.Any, ...], kwargs: t.Dict[str, t.Any]\n+    ) -> t.Any:\n+        if args and kwargs:\n+            raise TypeError(\"app.json.response() takes either args or kwargs, not both\")\n+\n+        if not args and not kwargs:\n+            return None\n+\n+        if len(args) == 1:\n+            return args[0]\n+\n+        return args or kwargs\n+\n+    def response(self, *args: t.Any, **kwargs: t.Any) -> Response:\n+        \"\"\"Serialize the given arguments as JSON, and return a\n+        :class:`~flask.Response` object with the ``application/json``\n+        mimetype.\n+\n+        The :func:`~flask.json.jsonify` function calls this method for\n+        the current application.\n+\n+        Either positional or keyword arguments can be given, not both.\n+        If no arguments are given, ``None`` is serialized.\n+\n+        :param args: A single value to serialize, or multiple values to\n+            treat as a list to serialize.\n+        :param kwargs: Treat as a dict to serialize.\n+        \"\"\"\n+        obj = self._prepare_response_obj(args, kwargs)\n+        return self._app.response_class(self.dumps(obj), mimetype=\"application/json\")",
      "comment": "@davidism, maybe add your orjson example to the documentation?",
      "comment_id": 920417062,
      "user": "Yourun-proger",
      "created_at": "2022-07-13T19:11:17Z",
      "url": "https://github.com/pallets/flask/pull/4692#discussion_r920417062"
    },
    {
      "repo": "pallets/flask",
      "pr_number": 4692,
      "file_path": "src/flask/json/provider.py",
      "line": 146,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,310 @@\n+from __future__ import annotations\n+\n+import dataclasses\n+import decimal\n+import json\n+import typing as t\n+import uuid\n+import weakref\n+from datetime import date\n+\n+from werkzeug.http import http_date\n+\n+from ..globals import request\n+\n+if t.TYPE_CHECKING:  # pragma: no cover\n+    from ..app import Flask\n+    from ..wrappers import Response\n+\n+\n+class JSONProvider:\n+    \"\"\"A standard set of JSON operations for an application. Subclasses\n+    of this can be used to customize JSON behavior or use different\n+    JSON libraries.\n+\n+    To implement a provider for a specific library, subclass this base\n+    class and implement at least :meth:`dumps` and :meth:`loads`. All\n+    other methods have default implementations.\n+\n+    To use a different provider, either subclass ``Flask`` and set\n+    :attr:`~flask.Flask.json_provider_class` to a provider class, or set\n+    :attr:`app.json <flask.Flask.json>` to an instance of the class.\n+\n+    :param app: An application instance. This will be stored as a\n+        :class:`weakref.proxy` on the :attr:`_app` attribute.\n+\n+    .. versionadded:: 2.2\n+    \"\"\"\n+\n+    def __init__(self, app: Flask) -> None:\n+        self._app = weakref.proxy(app)\n+\n+    def dumps(self, obj: t.Any, **kwargs: t.Any) -> str:\n+        \"\"\"Serialize data as JSON.\n+\n+        :param obj: The data to serialize.\n+        :param kwargs: May be passed to the underlying JSON library.\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def dump(self, obj: t.Any, fp: t.IO[str], **kwargs: t.Any) -> None:\n+        \"\"\"Serialize data as JSON and write to a file.\n+\n+        :param obj: The data to serialize.\n+        :param fp: A file opened for writing text. Should use the UTF-8\n+            encoding to be valid JSON.\n+        :param kwargs: May be passed to the underlying JSON library.\n+        \"\"\"\n+        fp.write(self.dumps(obj, **kwargs))\n+\n+    def loads(self, s: str | bytes, **kwargs: t.Any) -> t.Any:\n+        \"\"\"Deserialize data as JSON.\n+\n+        :param s: Text or UTF-8 bytes.\n+        :param kwargs: May be passed to the underlying JSON library.\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def load(self, fp: t.IO[t.AnyStr], **kwargs: t.Any) -> t.Any:\n+        \"\"\"Deserialize data as JSON read from a file.\n+\n+        :param fp: A file opened for reading text or UTF-8 bytes.\n+        :param kwargs: May be passed to the underlying JSON library.\n+        \"\"\"\n+        return self.loads(fp.read(), **kwargs)\n+\n+    def _prepare_response_obj(\n+        self, args: t.Tuple[t.Any, ...], kwargs: t.Dict[str, t.Any]\n+    ) -> t.Any:\n+        if args and kwargs:\n+            raise TypeError(\"app.json.response() takes either args or kwargs, not both\")\n+\n+        if not args and not kwargs:\n+            return None\n+\n+        if len(args) == 1:\n+            return args[0]\n+\n+        return args or kwargs\n+\n+    def response(self, *args: t.Any, **kwargs: t.Any) -> Response:\n+        \"\"\"Serialize the given arguments as JSON, and return a\n+        :class:`~flask.Response` object with the ``application/json``\n+        mimetype.\n+\n+        The :func:`~flask.json.jsonify` function calls this method for\n+        the current application.\n+\n+        Either positional or keyword arguments can be given, not both.\n+        If no arguments are given, ``None`` is serialized.\n+\n+        :param args: A single value to serialize, or multiple values to\n+            treat as a list to serialize.\n+        :param kwargs: Treat as a dict to serialize.\n+        \"\"\"\n+        obj = self._prepare_response_obj(args, kwargs)\n+        return self._app.response_class(self.dumps(obj), mimetype=\"application/json\")\n+\n+\n+def _default(o: t.Any) -> t.Any:\n+    if isinstance(o, date):\n+        return http_date(o)\n+\n+    if isinstance(o, (decimal.Decimal, uuid.UUID)):\n+        return str(o)\n+\n+    if dataclasses and dataclasses.is_dataclass(o):\n+        return dataclasses.asdict(o)\n+\n+    if hasattr(o, \"__html__\"):\n+        return str(o.__html__())\n+\n+    raise TypeError(f\"Object of type {type(o).__name__} is not JSON serializable\")\n+\n+\n+class DefaultJSONProvider(JSONProvider):\n+    \"\"\"Provide JSON operations using Python's built-in :mod:`json`\n+    library. Serializes the following additional data types:\n+\n+    -   :class:`datetime.datetime` and :class:`datetime.date` are\n+        serialized to :rfc:`822` strings. This is the same as the HTTP\n+        date format.\n+    -   :class:`uuid.UUID` is serialized to a string.\n+    -   :class:`dataclasses.dataclass` is passed to\n+        :func:`dataclasses.asdict`.\n+    -   :class:`~markupsafe.Markup` (or any object with a ``__html__``\n+        method) will call the ``__html__`` method to get a string.\n+    \"\"\"\n+\n+    default: t.Callable[[t.Any], t.Any] = staticmethod(\n+        _default\n+    )  # type: ignore[assignment]\n+    \"\"\"Apply this function to any object that :meth:`json.dumps` does\n+    not know how to serialize. It should return a valid JSON type or\n+    raise a ``TypeError``.\n+    \"\"\"\n+",
      "comment": "What do you think about adding a dict_to_object hook here as well (for the loads side). Allows something like this,\r\n```python\r\nclass MoneyJSONProvider(DefaultJSONProvider):\r\n\r\n        @staticmethod\r\n        def default(object_):\r\n            if isinstance(object_, date):\r\n                return http_date(object_)\r\n            if isinstance(object_, (Decimal, UUID)):\r\n                return str(object_)\r\n            if is_dataclass(object_):\r\n                return asdict(object_)\r\n            if hasattr(object_, \"__html__\"):\r\n                return str(object_.__html__())\r\n            if isinstance(object_, Money):\r\n                return {'amount': object_.amount, 'currency': object_.currency}\r\n\r\n            raise TypeError(f\"Object of type {type(object_).__name__} is not JSON serializable\")\r\n\r\n        @staticmethod\r\n        def dict_to_object(dict_):\r\n            if 'amount' in dict_ and 'currency' in dict_:\r\n                return Money(Decimal(dict_['amount']), dict_['currency'])\r\n            else:\r\n                return dict_ \r\n```",
      "comment_id": 923352802,
      "user": "pgjones",
      "created_at": "2022-07-18T13:07:59Z",
      "url": "https://github.com/pallets/flask/pull/4692#discussion_r923352802"
    },
    {
      "repo": "pallets/flask",
      "pr_number": 4692,
      "file_path": "src/flask/json/provider.py",
      "line": 146,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,310 @@\n+from __future__ import annotations\n+\n+import dataclasses\n+import decimal\n+import json\n+import typing as t\n+import uuid\n+import weakref\n+from datetime import date\n+\n+from werkzeug.http import http_date\n+\n+from ..globals import request\n+\n+if t.TYPE_CHECKING:  # pragma: no cover\n+    from ..app import Flask\n+    from ..wrappers import Response\n+\n+\n+class JSONProvider:\n+    \"\"\"A standard set of JSON operations for an application. Subclasses\n+    of this can be used to customize JSON behavior or use different\n+    JSON libraries.\n+\n+    To implement a provider for a specific library, subclass this base\n+    class and implement at least :meth:`dumps` and :meth:`loads`. All\n+    other methods have default implementations.\n+\n+    To use a different provider, either subclass ``Flask`` and set\n+    :attr:`~flask.Flask.json_provider_class` to a provider class, or set\n+    :attr:`app.json <flask.Flask.json>` to an instance of the class.\n+\n+    :param app: An application instance. This will be stored as a\n+        :class:`weakref.proxy` on the :attr:`_app` attribute.\n+\n+    .. versionadded:: 2.2\n+    \"\"\"\n+\n+    def __init__(self, app: Flask) -> None:\n+        self._app = weakref.proxy(app)\n+\n+    def dumps(self, obj: t.Any, **kwargs: t.Any) -> str:\n+        \"\"\"Serialize data as JSON.\n+\n+        :param obj: The data to serialize.\n+        :param kwargs: May be passed to the underlying JSON library.\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def dump(self, obj: t.Any, fp: t.IO[str], **kwargs: t.Any) -> None:\n+        \"\"\"Serialize data as JSON and write to a file.\n+\n+        :param obj: The data to serialize.\n+        :param fp: A file opened for writing text. Should use the UTF-8\n+            encoding to be valid JSON.\n+        :param kwargs: May be passed to the underlying JSON library.\n+        \"\"\"\n+        fp.write(self.dumps(obj, **kwargs))\n+\n+    def loads(self, s: str | bytes, **kwargs: t.Any) -> t.Any:\n+        \"\"\"Deserialize data as JSON.\n+\n+        :param s: Text or UTF-8 bytes.\n+        :param kwargs: May be passed to the underlying JSON library.\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def load(self, fp: t.IO[t.AnyStr], **kwargs: t.Any) -> t.Any:\n+        \"\"\"Deserialize data as JSON read from a file.\n+\n+        :param fp: A file opened for reading text or UTF-8 bytes.\n+        :param kwargs: May be passed to the underlying JSON library.\n+        \"\"\"\n+        return self.loads(fp.read(), **kwargs)\n+\n+    def _prepare_response_obj(\n+        self, args: t.Tuple[t.Any, ...], kwargs: t.Dict[str, t.Any]\n+    ) -> t.Any:\n+        if args and kwargs:\n+            raise TypeError(\"app.json.response() takes either args or kwargs, not both\")\n+\n+        if not args and not kwargs:\n+            return None\n+\n+        if len(args) == 1:\n+            return args[0]\n+\n+        return args or kwargs\n+\n+    def response(self, *args: t.Any, **kwargs: t.Any) -> Response:\n+        \"\"\"Serialize the given arguments as JSON, and return a\n+        :class:`~flask.Response` object with the ``application/json``\n+        mimetype.\n+\n+        The :func:`~flask.json.jsonify` function calls this method for\n+        the current application.\n+\n+        Either positional or keyword arguments can be given, not both.\n+        If no arguments are given, ``None`` is serialized.\n+\n+        :param args: A single value to serialize, or multiple values to\n+            treat as a list to serialize.\n+        :param kwargs: Treat as a dict to serialize.\n+        \"\"\"\n+        obj = self._prepare_response_obj(args, kwargs)\n+        return self._app.response_class(self.dumps(obj), mimetype=\"application/json\")\n+\n+\n+def _default(o: t.Any) -> t.Any:\n+    if isinstance(o, date):\n+        return http_date(o)\n+\n+    if isinstance(o, (decimal.Decimal, uuid.UUID)):\n+        return str(o)\n+\n+    if dataclasses and dataclasses.is_dataclass(o):\n+        return dataclasses.asdict(o)\n+\n+    if hasattr(o, \"__html__\"):\n+        return str(o.__html__())\n+\n+    raise TypeError(f\"Object of type {type(o).__name__} is not JSON serializable\")\n+\n+\n+class DefaultJSONProvider(JSONProvider):\n+    \"\"\"Provide JSON operations using Python's built-in :mod:`json`\n+    library. Serializes the following additional data types:\n+\n+    -   :class:`datetime.datetime` and :class:`datetime.date` are\n+        serialized to :rfc:`822` strings. This is the same as the HTTP\n+        date format.\n+    -   :class:`uuid.UUID` is serialized to a string.\n+    -   :class:`dataclasses.dataclass` is passed to\n+        :func:`dataclasses.asdict`.\n+    -   :class:`~markupsafe.Markup` (or any object with a ``__html__``\n+        method) will call the ``__html__`` method to get a string.\n+    \"\"\"\n+\n+    default: t.Callable[[t.Any], t.Any] = staticmethod(\n+        _default\n+    )  # type: ignore[assignment]\n+    \"\"\"Apply this function to any object that :meth:`json.dumps` does\n+    not know how to serialize. It should return a valid JSON type or\n+    raise a ``TypeError``.\n+    \"\"\"\n+",
      "comment": "I left it out for a few reasons. `object_hook` is not as consistently supported by different libraries as `default` is, and I didn't want to put a perceived requirement for it on all other providers. You usually want to perform validation when deserializing, and that gets very messy trying to cram it all in `object_hook` along with proper error collection. Instead, any project should use a serialization library, leaving the provider to only handle the JSON and basic types.",
      "comment_id": 923395880,
      "user": "davidism",
      "created_at": "2022-07-18T13:49:03Z",
      "url": "https://github.com/pallets/flask/pull/4692#discussion_r923395880"
    },
    {
      "repo": "pallets/flask",
      "pr_number": 1728,
      "file_path": "tests/test_basic.py",
      "line": 1024,
      "side": "RIGHT",
      "diff_hunk": "@@ -1029,6 +1029,18 @@ def test_jsonify_prettyprint():\n         assert rv.data == pretty_response\n \n \n+def test_jsonify_mimetype():\n+    app = flask.Flask(__name__)\n+    app.config.update({\"JSONIFY_MIMETYPE\": 'application/vnd.api+json'})",
      "comment": "why not just `app.config['JSONIFY_MIMETYPE'] = '...'`?\n",
      "comment_id": 52982620,
      "user": "ThiefMaster",
      "created_at": "2016-02-16T08:57:16Z",
      "url": "https://github.com/pallets/flask/pull/1728#discussion_r52982620"
    },
    {
      "repo": "pallets/flask",
      "pr_number": 1728,
      "file_path": "tests/test_basic.py",
      "line": 1024,
      "side": "RIGHT",
      "diff_hunk": "@@ -1029,6 +1029,18 @@ def test_jsonify_prettyprint():\n         assert rv.data == pretty_response\n \n \n+def test_jsonify_mimetype():\n+    app = flask.Flask(__name__)\n+    app.config.update({\"JSONIFY_MIMETYPE\": 'application/vnd.api+json'})",
      "comment": "I am following the conventions used in similar tests. For example: https://github.com/mitsuhiko/flask/pull/1728/files#diff-9e9ddedef65dec3da86063a067839e65R1021\n",
      "comment_id": 53270820,
      "user": "sloria",
      "created_at": "2016-02-18T04:40:35Z",
      "url": "https://github.com/pallets/flask/pull/1728#discussion_r53270820"
    },
    {
      "repo": "pallets/flask",
      "pr_number": 4682,
      "file_path": "src/flask/globals.py",
      "line": 73,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,59 +1,107 @@\n import typing as t\n-from functools import partial\n+from contextvars import ContextVar\n \n from werkzeug.local import LocalProxy\n-from werkzeug.local import LocalStack\n \n if t.TYPE_CHECKING:  # pragma: no cover\n     from .app import Flask\n     from .ctx import _AppCtxGlobals\n+    from .ctx import AppContext\n+    from .ctx import RequestContext\n     from .sessions import SessionMixin\n     from .wrappers import Request\n \n-_request_ctx_err_msg = \"\"\"\\\n-Working outside of request context.\n \n-This typically means that you attempted to use functionality that needed\n-an active HTTP request.  Consult the documentation on testing for\n-information about how to avoid this problem.\\\n-\"\"\"\n-_app_ctx_err_msg = \"\"\"\\\n+class _FakeStack:\n+    def __init__(self, name: str, cv: ContextVar[t.Any]) -> None:\n+        self.name = name\n+        self.cv = cv\n+\n+    def _warn(self):\n+        import warnings\n+\n+        warnings.warn(\n+            f\"'_{self.name}_ctx_stack' is deprecated and will be\"\n+            \" removed in Flask 2.3. Use 'g' to store data, or\"\n+            f\" '{self.name}_ctx' to access the current context.\",\n+            DeprecationWarning,\n+            stacklevel=3,\n+        )\n+\n+    def push(self, obj: t.Any) -> None:\n+        self._warn()\n+        self.cv.set(obj)\n+\n+    def pop(self) -> t.Any:\n+        self._warn()\n+        ctx = self.cv.get(None)\n+        self.cv.set(None)\n+        return ctx\n+\n+    @property\n+    def top(self) -> t.Optional[t.Any]:\n+        self._warn()\n+        return self.cv.get(None)\n+\n+\n+_no_app_msg = \"\"\"\\\n Working outside of application context.\n \n This typically means that you attempted to use functionality that needed\n-to interface with the current application object in some way. To solve\n-this, set up an application context with app.app_context().  See the\n-documentation for more information.\\\n+the current application. To solve this, set up an application context\n+with app.app_context(). See the documentation for more information.\\\n \"\"\"\n+_cv_app: ContextVar[\"AppContext\"] = ContextVar(\"flask.app_ctx\")\n+__app_ctx_stack = _FakeStack(\"app\", _cv_app)\n+app_ctx: \"AppContext\" = LocalProxy(  # type: ignore[assignment]\n+    _cv_app, unbound_message=_no_app_msg\n+)\n+current_app: \"Flask\" = LocalProxy(  # type: ignore[assignment]\n+    _cv_app, \"app\", unbound_message=_no_app_msg\n+)\n+g: \"_AppCtxGlobals\" = LocalProxy(  # type: ignore[assignment]\n+    _cv_app, \"g\", unbound_message=_no_app_msg\n+)\n \n+_no_req_msg = \"\"\"\\\n+Working outside of request context.\n \n-def _lookup_req_object(name):\n-    top = _request_ctx_stack.top\n-    if top is None:\n-        raise RuntimeError(_request_ctx_err_msg)\n-    return getattr(top, name)\n+This typically means that you attempted to use functionality that needed\n+an active HTTP request. Consult the documentation on testing for\n+information about how to avoid this problem.\\\n+\"\"\"\n+_cv_req: ContextVar[\"RequestContext\"] = ContextVar(\"flask.request_ctx\")",
      "comment": "Can this be `_cv_request`? I need a `_cv_websocket` as well in Quart and `_cv_web` isn't very clear.",
      "comment_id": 917122523,
      "user": "pgjones",
      "created_at": "2022-07-08T20:33:54Z",
      "url": "https://github.com/pallets/flask/pull/4682#discussion_r917122523"
    },
    {
      "repo": "pallets/flask",
      "pr_number": 4682,
      "file_path": "src/flask/globals.py",
      "line": 73,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,59 +1,107 @@\n import typing as t\n-from functools import partial\n+from contextvars import ContextVar\n \n from werkzeug.local import LocalProxy\n-from werkzeug.local import LocalStack\n \n if t.TYPE_CHECKING:  # pragma: no cover\n     from .app import Flask\n     from .ctx import _AppCtxGlobals\n+    from .ctx import AppContext\n+    from .ctx import RequestContext\n     from .sessions import SessionMixin\n     from .wrappers import Request\n \n-_request_ctx_err_msg = \"\"\"\\\n-Working outside of request context.\n \n-This typically means that you attempted to use functionality that needed\n-an active HTTP request.  Consult the documentation on testing for\n-information about how to avoid this problem.\\\n-\"\"\"\n-_app_ctx_err_msg = \"\"\"\\\n+class _FakeStack:\n+    def __init__(self, name: str, cv: ContextVar[t.Any]) -> None:\n+        self.name = name\n+        self.cv = cv\n+\n+    def _warn(self):\n+        import warnings\n+\n+        warnings.warn(\n+            f\"'_{self.name}_ctx_stack' is deprecated and will be\"\n+            \" removed in Flask 2.3. Use 'g' to store data, or\"\n+            f\" '{self.name}_ctx' to access the current context.\",\n+            DeprecationWarning,\n+            stacklevel=3,\n+        )\n+\n+    def push(self, obj: t.Any) -> None:\n+        self._warn()\n+        self.cv.set(obj)\n+\n+    def pop(self) -> t.Any:\n+        self._warn()\n+        ctx = self.cv.get(None)\n+        self.cv.set(None)\n+        return ctx\n+\n+    @property\n+    def top(self) -> t.Optional[t.Any]:\n+        self._warn()\n+        return self.cv.get(None)\n+\n+\n+_no_app_msg = \"\"\"\\\n Working outside of application context.\n \n This typically means that you attempted to use functionality that needed\n-to interface with the current application object in some way. To solve\n-this, set up an application context with app.app_context().  See the\n-documentation for more information.\\\n+the current application. To solve this, set up an application context\n+with app.app_context(). See the documentation for more information.\\\n \"\"\"\n+_cv_app: ContextVar[\"AppContext\"] = ContextVar(\"flask.app_ctx\")\n+__app_ctx_stack = _FakeStack(\"app\", _cv_app)\n+app_ctx: \"AppContext\" = LocalProxy(  # type: ignore[assignment]\n+    _cv_app, unbound_message=_no_app_msg\n+)\n+current_app: \"Flask\" = LocalProxy(  # type: ignore[assignment]\n+    _cv_app, \"app\", unbound_message=_no_app_msg\n+)\n+g: \"_AppCtxGlobals\" = LocalProxy(  # type: ignore[assignment]\n+    _cv_app, \"g\", unbound_message=_no_app_msg\n+)\n \n+_no_req_msg = \"\"\"\\\n+Working outside of request context.\n \n-def _lookup_req_object(name):\n-    top = _request_ctx_stack.top\n-    if top is None:\n-        raise RuntimeError(_request_ctx_err_msg)\n-    return getattr(top, name)\n+This typically means that you attempted to use functionality that needed\n+an active HTTP request. Consult the documentation on testing for\n+information about how to avoid this problem.\\\n+\"\"\"\n+_cv_req: ContextVar[\"RequestContext\"] = ContextVar(\"flask.request_ctx\")",
      "comment": "Oh yeah, I used the shorter name while I was working and forgot to change it.",
      "comment_id": 917165481,
      "user": "davidism",
      "created_at": "2022-07-08T22:14:47Z",
      "url": "https://github.com/pallets/flask/pull/4682#discussion_r917165481"
    },
    {
      "repo": "pallets/flask",
      "pr_number": 4576,
      "file_path": "src/flask/helpers.py",
      "line": 811,
      "side": "RIGHT",
      "diff_hunk": "@@ -788,3 +789,23 @@ def _split_blueprint_path(name: str) -> t.List[str]:\n         out.extend(_split_blueprint_path(name.rpartition(\".\")[0]))\n \n     return out\n+\n+\n+def abort(\n+    status: t.Union[int, \"Response\"], *args: t.Any, **kwargs: t.Any\n+) -> \"te.NoReturn\":\n+    \"\"\"Raises an :py:exc:`HTTPException` for the given status code or WSGI\n+    application using the :attr:`flask.Flask.aborter` instance.\n+\n+    If a status code is given, it will be looked up in the list of\n+    exceptions and will raise that exception.  If passed a WSGI application,\n+    it will wrap it in a proxy WSGI exception and raise that::\n+\n+       abort(404)  # 404 Not Found\n+       abort(Response('Hello World'))\n+\n+    .. versionchanged:: 2.2\n+       Call :attr:`flask.Flask.aborter` on the :data:`~flask.current_app`\n+       rather than exposing :class:`werkzeug.exceptions.abort` directly.\n+    \"\"\"\n+    current_app.aborter(status, *args, **kwargs)  # type: ignore[misc]",
      "comment": "The typing error here is: `src/flask/helpers.py:794: error: Implicit return in function which does not return  [misc]`.\r\n\r\nThe `aborter` is an instance of the `Aborter` class, which is [properly annotated to `NoReturn`](https://github.com/pallets/werkzeug/blob/main/src/werkzeug/exceptions.py#L851-L853) on it's `__call__` function. This seems like potentially a type-checking bug, unless I'm missing something",
      "comment_id": 863028116,
      "user": "dzcode",
      "created_at": "2022-05-02T17:15:04Z",
      "url": "https://github.com/pallets/flask/pull/4576#discussion_r863028116"
    },
    {
      "repo": "pallets/flask",
      "pr_number": 4560,
      "file_path": "src/flask/scaffold.py",
      "line": 736,
      "side": "RIGHT",
      "diff_hunk": "@@ -728,13 +714,32 @@ def _get_exc_class_and_code(\n         \"\"\"\n         exc_class: t.Type[Exception]\n         if isinstance(exc_class_or_code, int):\n-            exc_class = default_exceptions[exc_class_or_code]\n+            try:\n+                exc_class = default_exceptions[exc_class_or_code]\n+            except KeyError:\n+                raise KeyError(\n+                    f\"'{exc_class_or_code}' is not a recognized HTTP error\"\n+                    \" code. Use a subclass of HTTPException with that code\"\n+                    \" instead.\"\n+                ) from None\n         else:\n             exc_class = exc_class_or_code\n \n+        if isinstance(exc_class, Exception):\n+            raise ValueError(\n+                \"Tried to register a handler for an Exception instance\"\n+                f\" {exc_class!r}. Handlers can only be\"\n+                \" registered for Exception classes or HTTP error codes.\"\n+            )\n+\n+        # make sure `exc_class` is a class before checking subclass\n+        assert isinstance(",
      "comment": "This assertion is used to avoid the potential TypeError in `issubclass(exc_class, Exception)` since I found error msg `TypeError: issubclass() arg 1 must be a class` might be confusing to users. If it's deleted, we should be also fine.",
      "comment_id": 861203989,
      "user": "qingpeng9802",
      "created_at": "2022-04-28T18:33:04Z",
      "url": "https://github.com/pallets/flask/pull/4560#discussion_r861203989"
    },
    {
      "repo": "django/django",
      "pr_number": 20614,
      "file_path": "tests/migrations/test_operations.py",
      "line": 5615,
      "side": "RIGHT",
      "diff_hunk": "@@ -5605,6 +5605,15 @@ def test_run_sql_params(self):\n             )\n         self.assertTableNotExists(\"i_love_ponies\")\n \n+    def test_run_sql_elidable(self):\n+        operation = migrations.RunSQL(migrations.RunSQL.noop, elidable=True)\n+        self.assertEqual(operation.describe(), \"Raw SQL operation\")\n+        definition = operation.deconstruct()\n+        self.assertEqual(definition[0], \"RunSQL\")\n+        self.assertEqual(definition[1], [])\n+        self.assertEqual(sorted(definition[2]), [\"elidable\", \"sql\"])\n+        self.assertTrue(definition[2][\"elidable\"])",
      "comment": "Probs don't need an explicit test for just `elidable` param.\r\nIs there already a test somewhere that checks _all_ serialized params for RunSQL? I'd probs ensure we have one that does that, and include elidable as part of its check.",
      "comment_id": 2744906431,
      "user": "jarekwg",
      "created_at": "2026-01-30T06:51:36Z",
      "url": "https://github.com/django/django/pull/20614#discussion_r2744906431"
    },
    {
      "repo": "django/django",
      "pr_number": 20614,
      "file_path": "tests/migrations/test_operations.py",
      "line": 5615,
      "side": "RIGHT",
      "diff_hunk": "@@ -5605,6 +5605,15 @@ def test_run_sql_params(self):\n             )\n         self.assertTableNotExists(\"i_love_ponies\")\n \n+    def test_run_sql_elidable(self):\n+        operation = migrations.RunSQL(migrations.RunSQL.noop, elidable=True)\n+        self.assertEqual(operation.describe(), \"Raw SQL operation\")\n+        definition = operation.deconstruct()\n+        self.assertEqual(definition[0], \"RunSQL\")\n+        self.assertEqual(definition[1], [])\n+        self.assertEqual(sorted(definition[2]), [\"elidable\", \"sql\"])\n+        self.assertTrue(definition[2][\"elidable\"])",
      "comment": "Thanks for the feedback! I've updated the PR to remove the separate test case. I found that test_run_sqland and test_run_python\r\nwere already creating elidable operations for reduction testing, so I added assertions there to verify they also deconstruct correctly with the elidable keyword.",
      "comment_id": 2744939359,
      "user": "SnippyCodes",
      "created_at": "2026-01-30T07:01:14Z",
      "url": "https://github.com/django/django/pull/20614#discussion_r2744939359"
    },
    {
      "repo": "django/django",
      "pr_number": 20614,
      "file_path": "tests/migrations/test_operations.py",
      "line": 5544,
      "side": "RIGHT",
      "diff_hunk": "@@ -5543,6 +5536,13 @@ def test_run_sql(self):\n         elidable_operation = migrations.RunSQL(\"SELECT 1 FROM void;\", elidable=True)\n         self.assertEqual(elidable_operation.reduce(operation, []), [operation])\n \n+        # Test elidable deconstruction\n+        definition = elidable_operation.deconstruct()\n+        self.assertEqual(definition[0], \"RunSQL\")\n+        self.assertEqual(definition[1], [])\n+        self.assertEqual(sorted(definition[2]), [\"elidable\", \"sql\"])\n+        self.assertTrue(definition[2][\"elidable\"])",
      "comment": "One assertion for this is fine, also please use `assertIs` for first-party values:\n```suggestion\n        self.assertIs(definition[2][\"elidable\"], True)\n```",
      "comment_id": 2746661936,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-30T14:51:21Z",
      "url": "https://github.com/django/django/pull/20614#discussion_r2746661936"
    },
    {
      "repo": "django/django",
      "pr_number": 20576,
      "file_path": "tests/admin_views/models.py",
      "line": 859,
      "side": "RIGHT",
      "diff_hunk": "@@ -856,7 +856,7 @@ def __str__(self):\n \n class MainPrepopulated(models.Model):\n     name = models.CharField(max_length=100)\n-    pubdate = models.DateField()\n+    pubdate = models.DateTimeField()  # DateTimeField will render a <fieldset>.",
      "comment": "It seems like the existing tests are failing(`admin_views.tests.SeleniumTests.test_populate_existing_object`) due to changing the field type. \r\nOf course, we could fix the tests, but instead of changing the field type, what do you think about using the `Course` model that was previously created for the fieldsets tests?\r\nI think we would just need to add `fieldsets` to the `ModelAdmin`.\r\n\r\nhttps://github.com/django/django/blob/68d110f1fe593b7a368486c41cd062563a74fe0a/tests/admin_views/models.py#L626-L639",
      "comment_id": 2723411328,
      "user": "Antoliny0919",
      "created_at": "2026-01-24T01:46:50Z",
      "url": "https://github.com/django/django/pull/20576#discussion_r2723411328"
    },
    {
      "repo": "django/django",
      "pr_number": 20576,
      "file_path": "tests/admin_views/tests.py",
      "line": 7083,
      "side": "RIGHT",
      "diff_hunk": "@@ -7079,6 +7079,16 @@ def test_use_fieldset_fields_render(self):\n             legend = fieldset.find_element(By.TAG_NAME, \"legend\")\n             self.assertEqual(legend.text, expected_legend_tags_text[index])\n \n+    @screenshot_cases([\"desktop_size\", \"mobile_size\", \"rtl\", \"dark\", \"high_contrast\"])\n+    def test_use_fieldset_fields_horizontal(self):",
      "comment": "What do you think about including something that conveys \"multiple fields\"? \ud83e\udd14 I think it would be a bit clearer (like the \"multiline\" being added in CSS).",
      "comment_id": 2723428257,
      "user": "Antoliny0919",
      "created_at": "2026-01-24T02:07:31Z",
      "url": "https://github.com/django/django/pull/20576#discussion_r2723428257"
    },
    {
      "repo": "django/django",
      "pr_number": 20576,
      "file_path": "tests/admin_views/models.py",
      "line": 859,
      "side": "RIGHT",
      "diff_hunk": "@@ -856,7 +856,7 @@ def __str__(self):\n \n class MainPrepopulated(models.Model):\n     name = models.CharField(max_length=100)\n-    pubdate = models.DateField()\n+    pubdate = models.DateTimeField()  # DateTimeField will render a <fieldset>.",
      "comment": "Sure thing. I was taking a shortcut and figured I'd hear a better idea :)",
      "comment_id": 2723572133,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-24T04:02:36Z",
      "url": "https://github.com/django/django/pull/20576#discussion_r2723572133"
    },
    {
      "repo": "django/django",
      "pr_number": 20576,
      "file_path": "tests/admin_views/tests.py",
      "line": 7103,
      "side": "RIGHT",
      "diff_hunk": "@@ -7079,6 +7079,16 @@ def test_use_fieldset_fields_render(self):\n             legend = fieldset.find_element(By.TAG_NAME, \"legend\")\n             self.assertEqual(legend.text, expected_legend_tags_text[index])\n \n+    @screenshot_cases([\"desktop_size\", \"mobile_size\", \"rtl\", \"dark\", \"high_contrast\"])\n+    def test_use_fieldset_fields_horizontal(self):\n+        self.admin_login(\n+            username=\"super\", password=\"secret\", login_url=reverse(\"admin:index\")\n+        )\n+        self.selenium.get(\n+            self.live_server_url + reverse(\"admin:admin_views_mainprepopulated_add\")\n+        )\n+        self.take_screenshot(\"horizontal_fieldset\")",
      "comment": "I think we should verify that the multiline container contains fieldset tags and at least 2 fields before taking the screenshot :)\r\n```diff --git a/tests/admin_views/tests.py b/tests/admin_views/tests.py\r\nindex 8ac5571f9d..8290c56138 100644\r\n--- a/tests/admin_views/tests.py\r\n+++ b/tests/admin_views/tests.py\r\n@@ -7081,12 +7081,23 @@ class SeleniumTests(AdminSeleniumTestCase):\r\n \r\n     @screenshot_cases([\"desktop_size\", \"mobile_size\", \"rtl\", \"dark\", \"high_contrast\"])\r\n     def test_use_fieldset_fields_horizontal(self):\r\n+        from selenium.webdriver.common.by import By\r\n+\r\n         self.admin_login(\r\n             username=\"super\", password=\"secret\", login_url=reverse(\"admin:index\")\r\n         )\r\n         self.selenium.get(\r\n             self.live_server_url + reverse(\"admin:admin_views_mainprepopulated_add\")\r\n         )\r\n+        multiline = self.selenium.find_element(\r\n+            By.CSS_SELECTOR, \"#content-main .field-pubdate .form-multiline\",\r\n+        )\r\n+        # Get direct children (div, fieldset) of multiline\r\n+        divs = multiline.find_elements(By.CSS_SELECTOR, \":scope > div\")\r\n+        fieldsets = multiline.find_elements(By.CSS_SELECTOR, \":scope > fieldset\")\r\n+        fields = divs + fieldsets\r\n+        self.assertGreater(len(fields), 1)\r\n+        self.assertGreater(len(fieldsets), 0)\r\n         self.take_screenshot(\"horizontal_fieldset\")\r\n \r\n     @screenshot_cases([\"desktop_size\", \"mobile_size\", \"rtl\", \"dark\", \"high_contrast\"])\r\n```\r\nI'm not sure if using `:scope` is the best approach here \ud83e\udd72",
      "comment_id": 2724089473,
      "user": "Antoliny0919",
      "created_at": "2026-01-24T12:18:11Z",
      "url": "https://github.com/django/django/pull/20576#discussion_r2724089473"
    },
    {
      "repo": "django/django",
      "pr_number": 20346,
      "file_path": "tests/model_fields/test_decimalfield.py",
      "line": 145,
      "side": "RIGHT",
      "diff_hunk": "@@ -140,3 +140,12 @@ def test_roundtrip_with_trailing_zeros(self):\n         obj = Foo.objects.create(a=\"bar\", d=Decimal(\"8.320\"))\n         obj.refresh_from_db()\n         self.assertEqual(obj.d.compare_total(Decimal(\"8.320\")), Decimal(\"0\"))\n+\n+    def test_sqlite_integer_precision_bypass(self):\n+        if connection.vendor != \"sqlite\":",
      "comment": "Thanks for the review.\r\n1- I removed that block so the test runs for all backends.\r\n2- I added Simon as co-author since I took his fix from the discussions under the ticket \r\n3- I also added another test for the else block ",
      "comment_id": 2705970001,
      "user": "Samriddha9619",
      "created_at": "2026-01-19T20:14:02Z",
      "url": "https://github.com/django/django/pull/20346#discussion_r2705970001"
    },
    {
      "repo": "django/django",
      "pr_number": 20346,
      "file_path": "django/db/backends/sqlite3/operations.py",
      "line": 334,
      "side": "RIGHT",
      "diff_hunk": "@@ -310,15 +330,21 @@ def get_decimalfield_converter(self, expression):\n             )\n \n             def converter(value, expression, connection):\n-                if value is not None:\n+                if isinstance(value, int):\n+                    return decimal.Decimal(value)",
      "comment": "I don't think it's acceptable to skip the `quantize()` call. This test passes on main and fails on your branch:\n\n```py\n    def test_roundtrip_integer_with_trailing_zeros(self):\n        obj = Foo.objects.create(a=\"bar\", d=Decimal(\"8\"))\n        obj.refresh_from_db()\n        self.assertEqual(obj.d.compare_total(Decimal(\"8.000\")), Decimal(\"0\"))\n```\n\n```py\n======================================================================\nFAIL: test_roundtrip_integer_with_trailing_zeros (model_fields.test_decimalfield.DecimalFieldTests.test_roundtrip_integer_with_trailing_zeros)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/Users/jwalls/django/tests/model_fields/test_decimalfield.py\", line 148, in test_roundtrip_integer_with_trailing_zeros\n    self.assertEqual(obj.d.compare_total(Decimal(\"8.000\")), Decimal(\"0\"))\n    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: Decimal('1') != Decimal('0')\n\n----------------------------------------------------------------------\n```",
      "comment_id": 2709057178,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-20T16:11:26Z",
      "url": "https://github.com/django/django/pull/20346#discussion_r2709057178"
    },
    {
      "repo": "django/django",
      "pr_number": 20346,
      "file_path": "tests/model_fields/models.py",
      "line": 720,
      "side": "RIGHT",
      "diff_hunk": "@@ -728,3 +714,7 @@ class Meta:\n                 F(\"a\"), name=\"Generated model unique constraint virtual a\"\n             ),\n         ]\n+\n+\n+class HighPrecision(models.Model):\n+    d = models.DecimalField(max_digits=16, decimal_places=0)",
      "comment": "Please remove this model and reuse `BigD`. We can't add 1x model per test due to the overhead of creating tables.",
      "comment_id": 2709059049,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-20T16:11:54Z",
      "url": "https://github.com/django/django/pull/20346#discussion_r2709059049"
    },
    {
      "repo": "django/django",
      "pr_number": 20346,
      "file_path": "django/db/backends/sqlite3/operations.py",
      "line": 347,
      "side": "RIGHT",
      "diff_hunk": "@@ -310,15 +330,21 @@ def get_decimalfield_converter(self, expression):\n             )\n \n             def converter(value, expression, connection):\n-                if value is not None:\n+                if isinstance(value, int):\n+                    return decimal.Decimal(value)\n+\n+                elif value is not None:\n                     return create_decimal(value).quantize(\n                         quantize_value, context=expression.output_field.context\n                     )\n \n         else:\n \n             def converter(value, expression, connection):\n-                if value is not None:\n+                if isinstance(value, int):\n+                    return decimal.Decimal(value)\n+\n+                elif value is not None:",
      "comment": "We try to make surgical edits when touching longstanding code. Removing the newline and changing `elif` -> `if` will create a smaller diff.",
      "comment_id": 2709062650,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-20T16:12:48Z",
      "url": "https://github.com/django/django/pull/20346#discussion_r2709062650"
    },
    {
      "repo": "django/django",
      "pr_number": 20346,
      "file_path": "tests/model_fields/test_decimalfield.py",
      "line": 160,
      "side": "RIGHT",
      "diff_hunk": "@@ -140,3 +141,20 @@ def test_roundtrip_with_trailing_zeros(self):\n         obj = Foo.objects.create(a=\"bar\", d=Decimal(\"8.320\"))\n         obj.refresh_from_db()\n         self.assertEqual(obj.d.compare_total(Decimal(\"8.320\")), Decimal(\"0\"))\n+\n+    def test_large_integer_precision(self):\n+        large_int_val = Decimal(\"9999999999999999\")\n+        obj = BigD.objects.create(large_int=large_int_val, d=Decimal(\"0\"))\n+        obj.refresh_from_db()\n+        self.assertEqual(obj.large_int, large_int_val)\n+\n+    def test_large_integer_precision_aggregation(self):\n+        large_int_val = Decimal(\"9999999999999999\")\n+        BigD.objects.create(large_int=large_int_val, d=Decimal(\"0\"))\n+        result = BigD.objects.aggregate(max_val=Max(\"large_int\"))\n+        self.assertEqual(result[\"max_val\"], large_int_val)\n+\n+    def test_roundtrip_integer_with_trailing_zeros(self):\n+        obj = Foo.objects.create(a=\"bar\", d=Decimal(\"8\"))\n+        obj.refresh_from_db()\n+        self.assertEqual(obj.d.compare_total(Decimal(\"8.000\")), Decimal(\"0\"))",
      "comment": "SQL databases usually have a NUMERIC type with (max_digits, and decimal_places) which takes care of adding the missing precision to `Decimal(\"8\")`. This isn't the case on MongoDB, and similar to SQLite, manual quantizing is necessary to make this test pass:\n```diff\ndiff --git a/django_mongodb_backend/operations.py b/django_mongodb_backend/operations.py\nindex e446e2c4..f4b0857f 100644\n--- a/django_mongodb_backend/operations.py\n+++ b/django_mongodb_backend/operations.py\n@@ -171,18 +171,20 @@ class DatabaseOperations(GISOperations, BaseDatabaseOperations):\n     def convert_decimalfield_value(self, value, expression, connection):\n         if value is not None:\n             # from Decimal128 to decimal.Decimal()\n             try:\n                 value = value.to_decimal()\n             except AttributeError:\n                 # `value` could be an integer in the case of an annotation\n                 # like ExpressionWrapper(Value(1), output_field=DecimalField().\n                 return Decimal(value)\n+            quantize_value = Decimal(1).scaleb(-expression.output_field.decimal_places)\n+            value = value.quantize(quantize_value, context=expression.output_field.context)\n         return value\n```\nI tend to think this is unnecessary overhead and this test (which bypasses validation anyway) should be skipped on MongoDB. Raising the issue here in case any patch authors have thoughts. Thanks!",
      "comment_id": 2750110696,
      "user": "timgraham",
      "created_at": "2026-01-31T22:12:44Z",
      "url": "https://github.com/django/django/pull/20346#discussion_r2750110696"
    },
    {
      "repo": "django/django",
      "pr_number": 19923,
      "file_path": "tests/admin_views/models.py",
      "line": 1200,
      "side": "RIGHT",
      "diff_hunk": "@@ -1190,3 +1190,7 @@ def __str__(self):\n class CamelCaseRelatedModel(models.Model):\n     m2m = models.ManyToManyField(CamelCaseModel, related_name=\"m2m\")\n     fk = models.ForeignKey(CamelCaseModel, on_delete=models.CASCADE, related_name=\"fk\")\n+    # Add another relation for ticket #36468\n+    fk2 = models.ForeignKey(",
      "comment": "We need an additional relation, because the select which triggered the popup is already [ignored](https://github.com/django/django/pull/19923/files#diff-e01b712e0ef2ea7a3d38e6482034814eef4dea24166eb6484b5d1ce204980d27R104) and updated [elsewhere](https://github.com/django/django/pull/19923/files#diff-e01b712e0ef2ea7a3d38e6482034814eef4dea24166eb6484b5d1ce204980d27L132) in the RelatedObjectLookup.js.",
      "comment_id": 2410029612,
      "user": "maqnius",
      "created_at": "2025-10-07T09:39:43Z",
      "url": "https://github.com/django/django/pull/19923#discussion_r2410029612"
    },
    {
      "repo": "django/django",
      "pr_number": 20538,
      "file_path": "django/contrib/admin/options.py",
      "line": 1162,
      "side": "RIGHT",
      "diff_hunk": "@@ -1154,8 +1154,12 @@ def construct_search(field_name):\n                 except FieldDoesNotExist:\n                     # Use valid query lookups.\n                     if prev_field and prev_field.get_lookup(path_part):\n-                        if path_part == \"exact\" and not isinstance(\n-                            prev_field, (models.CharField, models.TextField)\n+                        if (\n+                            path_part == \"exact\"\n+                            and not isinstance(\n+                                prev_field, (models.CharField, models.TextField)\n+                            )\n+                            and not prev_field.primary_key",
      "comment": "I think the problem has little to do with whether or not a primary key is targeted. The cast will be harmful in all case where the field is indexed for example.\r\n\r\nA better solution here would likely be to attempt `prev_field.to_python(...)` with the search term an in cases where the value is valid (no exception is raised) then don't perform a `Cast`.\r\n\r\nIn other words, `prev_field.to_python(\"123\")` (assuming it's an `IntegerField` instance) passes and returns `123` so we know that `field__exact=\"123\"` doesn't require a cast while `prev_field.to_python(\"foo\")` raises a `ValueError` so we know we must cast.",
      "comment_id": 2692557078,
      "user": "charettes",
      "created_at": "2026-01-15T00:46:41Z",
      "url": "https://github.com/django/django/pull/20538#discussion_r2692557078"
    },
    {
      "repo": "django/django",
      "pr_number": 20538,
      "file_path": "tests/admin_changelist/tests.py",
      "line": 866,
      "side": "RIGHT",
      "diff_hunk": "@@ -856,6 +856,25 @@ def test_custom_lookup_with_pk_shortcut(self):\n         cl = m.get_changelist_instance(request)\n         self.assertCountEqual(cl.queryset, [abcd])\n \n+    def test_pk_exact_lookup_does_not_use_cast(self):\n+        \"\"\"Primary key exact lookups should not use Cast to preserve index usage.\"\"\"\n+        child = Child.objects.create(name=\"Test\", age=10)\n+        m = admin.ModelAdmin(Child, custom_site)\n+        m.search_fields = [\"pk__exact\"]\n+\n+        request = self.factory.get(\"/\", data={SEARCH_VAR: str(child.pk)})",
      "comment": "This is re-introducing the admin crash problem ticket-26001 fixed which you can demonstrate by doing\n\n\n```suggestion\n        request = self.factory.get(\"/\", data={SEARCH_VAR: \"foo\"})\n```\n\nA cast (or an exclusion of terms) is necessary when dealing with string that are not string representation of integers.",
      "comment_id": 2692575770,
      "user": "charettes",
      "created_at": "2026-01-15T00:57:41Z",
      "url": "https://github.com/django/django/pull/20538#discussion_r2692575770"
    },
    {
      "repo": "django/django",
      "pr_number": 20538,
      "file_path": "django/contrib/admin/options.py",
      "line": 1162,
      "side": "RIGHT",
      "diff_hunk": "@@ -1154,8 +1154,12 @@ def construct_search(field_name):\n                 except FieldDoesNotExist:\n                     # Use valid query lookups.\n                     if prev_field and prev_field.get_lookup(path_part):\n-                        if path_part == \"exact\" and not isinstance(\n-                            prev_field, (models.CharField, models.TextField)\n+                        if (\n+                            path_part == \"exact\"\n+                            and not isinstance(\n+                                prev_field, (models.CharField, models.TextField)\n+                            )\n+                            and not prev_field.primary_key",
      "comment": "Thanks. I was trying to keep things narrow and I've never messed around at this level of admin. I just pushed a tweak that uses `to_python`. I'm a bit over my head, but hopefully this addresses your concern.",
      "comment_id": 2692610146,
      "user": "mlissner",
      "created_at": "2026-01-15T01:19:16Z",
      "url": "https://github.com/django/django/pull/20538#discussion_r2692610146"
    },
    {
      "repo": "django/django",
      "pr_number": 20538,
      "file_path": "django/contrib/admin/options.py",
      "line": 1166,
      "side": "RIGHT",
      "diff_hunk": "@@ -1157,6 +1157,13 @@ def construct_search(field_name):\n                         if path_part == \"exact\" and not isinstance(\n                             prev_field, (models.CharField, models.TextField)\n                         ):\n+                            # Skip Cast if search_term is valid for this field\n+                            # to preserve index usage.\n+                            try:\n+                                prev_field.to_python(search_term)\n+                                return field_name, None\n+                            except ValidationError:\n+                                pass",
      "comment": "This doesn't account for multiple search terms (see `smart_split(search_term)` below) where some might need cast and others don't.\n\nFor example, if your search for `foo 123` the resulting query should be\n\n```python\nalias(\n    int_field_str=Cast(\"int_field\", TextField())\n).filter(\n    Q(int_field_str=\"foo\") | Q(int_field=\"123\")\n)\n```\n\nwith your current path it'll be doing\n\n```python\nalias(\n    int_field_str=Cast(\"int_field\", TextField())\n).filter(\n    Q(int_field_str=\"foo\") | Q(int_field_str=123)\n)\n```\n\nSee how #18765 changed the logic to no longer be search term `bit` specific and how we might need to restore it.",
      "comment_id": 2694820830,
      "user": "charettes",
      "created_at": "2026-01-15T15:21:46Z",
      "url": "https://github.com/django/django/pull/20538#discussion_r2694820830"
    },
    {
      "repo": "django/django",
      "pr_number": 20538,
      "file_path": "django/contrib/admin/options.py",
      "line": 1166,
      "side": "RIGHT",
      "diff_hunk": "@@ -1157,6 +1157,13 @@ def construct_search(field_name):\n                         if path_part == \"exact\" and not isinstance(\n                             prev_field, (models.CharField, models.TextField)\n                         ):\n+                            # Skip Cast if search_term is valid for this field\n+                            # to preserve index usage.\n+                            try:\n+                                prev_field.to_python(search_term)\n+                                return field_name, None\n+                            except ValidationError:\n+                                pass",
      "comment": "Got it. You know, Claude suggested this yesterday and I thought it would be too invasive, but your suggestion on code.djangoproject.org is even better, and I've just pushed that, including another test to prevent regression.\r\n\r\nThank you again.",
      "comment_id": 2695860117,
      "user": "mlissner",
      "created_at": "2026-01-15T20:32:03Z",
      "url": "https://github.com/django/django/pull/20538#discussion_r2695860117"
    },
    {
      "repo": "django/django",
      "pr_number": 20538,
      "file_path": "django/contrib/admin/options.py",
      "line": 1166,
      "side": "RIGHT",
      "diff_hunk": "@@ -1157,6 +1157,13 @@ def construct_search(field_name):\n                         if path_part == \"exact\" and not isinstance(\n                             prev_field, (models.CharField, models.TextField)\n                         ):\n+                            # Skip Cast if search_term is valid for this field\n+                            # to preserve index usage.\n+                            try:\n+                                prev_field.to_python(search_term)\n+                                return field_name, None\n+                            except ValidationError:\n+                                pass",
      "comment": "Given that, could I ask you to update the PR body \"solution\" part?",
      "comment_id": 2698951633,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-16T15:26:44Z",
      "url": "https://github.com/django/django/pull/20538#discussion_r2698951633"
    },
    {
      "repo": "django/django",
      "pr_number": 20538,
      "file_path": "django/contrib/admin/options.py",
      "line": 1194,
      "side": "RIGHT",
      "diff_hunk": "@@ -1177,30 +1177,37 @@ def construct_search(field_name):\n         may_have_duplicates = False\n         search_fields = self.get_search_fields(request)\n         if search_fields and search_term:\n-            str_aliases = {}\n             orm_lookups = []\n             for field in search_fields:\n-                lookup, str_alias = construct_search(str(field))\n-                orm_lookups.append(lookup)\n-                if str_alias:\n-                    str_aliases[lookup] = str_alias\n-\n-            if str_aliases:\n-                queryset = queryset.alias(**str_aliases)\n+                orm_lookups.append(construct_search(str(field)))\n \n             term_queries = []\n             for bit in smart_split(search_term):\n                 if bit.startswith(('\"', \"'\")) and bit[0] == bit[-1]:\n                     bit = unescape_string_literal(bit)\n-                or_queries = models.Q.create(\n-                    [(orm_lookup, bit) for orm_lookup in orm_lookups],\n-                    connector=models.Q.OR,\n-                )\n-                term_queries.append(or_queries)\n-            queryset = queryset.filter(models.Q.create(term_queries))\n+                # Build lookups for this term, skipping invalid field/value\n+                # combinations to preserve index usage.\n+                bit_lookups = []\n+                for orm_lookup, validate_field in orm_lookups:\n+                    if validate_field is not None:\n+                        try:\n+                            validate_field.to_python(bit)",
      "comment": "You could re-assign `bit` here to use the proper expected type\n\n\n```suggestion\n                            but = validate_field.to_python(bit)\n```",
      "comment_id": 2723384880,
      "user": "charettes",
      "created_at": "2026-01-24T01:27:20Z",
      "url": "https://github.com/django/django/pull/20538#discussion_r2723384880"
    },
    {
      "repo": "django/django",
      "pr_number": 20538,
      "file_path": "django/contrib/admin/options.py",
      "line": 1194,
      "side": "RIGHT",
      "diff_hunk": "@@ -1179,30 +1177,37 @@ def construct_search(field_name):\n         may_have_duplicates = False\n         search_fields = self.get_search_fields(request)\n         if search_fields and search_term:\n-            str_aliases = {}\n             orm_lookups = []\n             for field in search_fields:\n-                lookup, str_alias = construct_search(str(field))\n-                orm_lookups.append(lookup)\n-                if str_alias:\n-                    str_aliases[lookup] = str_alias\n-\n-            if str_aliases:\n-                queryset = queryset.alias(**str_aliases)\n+                orm_lookups.append(construct_search(str(field)))\n \n             term_queries = []\n             for bit in smart_split(search_term):\n                 if bit.startswith(('\"', \"'\")) and bit[0] == bit[-1]:\n                     bit = unescape_string_literal(bit)\n-                or_queries = models.Q.create(\n-                    [(orm_lookup, bit) for orm_lookup in orm_lookups],\n-                    connector=models.Q.OR,\n-                )\n-                term_queries.append(or_queries)\n-            queryset = queryset.filter(models.Q.create(term_queries))\n+                # Build lookups for this term, skipping invalid field/value\n+                # combinations to preserve index usage.\n+                bit_lookups = []\n+                for orm_lookup, validate_field in orm_lookups:\n+                    if validate_field is not None:\n+                        try:\n+                            bit = validate_field.to_python(bit)",
      "comment": "Did you give some thought to Simon's suggestion to use `.formfield()`? I expect some behavior differences, e.g.:\n\n```py\nIn [1]: from django.db.models.fields import BooleanField\n\nIn [2]: f = BooleanField()\n\nIn [3]: f.to_python('false')\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\nCell In[3], line 1\n----> 1 f.to_python('false')\n\nFile ~/django/django/db/models/fields/__init__.py:1179, in BooleanField.to_python(self, value)\n   1177 if value in (\"f\", \"False\", \"0\"):\n   1178     return False\n-> 1179 raise exceptions.ValidationError(\n   1180     self.error_messages[\"invalid_nullable\" if self.null else \"invalid\"],\n   1181     code=\"invalid\",\n   1182     params={\"value\": value},\n   1183 )\n\nValidationError: ['\u201cfalse\u201d value must be either True or False.']\n\nIn [4]: ff = f.formfield()\n\nIn [5]: ff.to_python('false')\nOut[5]: False\n\nIn [6]: ff.to_python('garbage')\nOut[6]: True\n```\n\nCan you look into this and add a test that will fail if the model field is used instead of the form field?",
      "comment_id": 2737719890,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-28T17:26:01Z",
      "url": "https://github.com/django/django/pull/20538#discussion_r2737719890"
    },
    {
      "repo": "django/django",
      "pr_number": 20538,
      "file_path": "django/contrib/admin/options.py",
      "line": 1194,
      "side": "RIGHT",
      "diff_hunk": "@@ -1179,30 +1177,37 @@ def construct_search(field_name):\n         may_have_duplicates = False\n         search_fields = self.get_search_fields(request)\n         if search_fields and search_term:\n-            str_aliases = {}\n             orm_lookups = []\n             for field in search_fields:\n-                lookup, str_alias = construct_search(str(field))\n-                orm_lookups.append(lookup)\n-                if str_alias:\n-                    str_aliases[lookup] = str_alias\n-\n-            if str_aliases:\n-                queryset = queryset.alias(**str_aliases)\n+                orm_lookups.append(construct_search(str(field)))\n \n             term_queries = []\n             for bit in smart_split(search_term):\n                 if bit.startswith(('\"', \"'\")) and bit[0] == bit[-1]:\n                     bit = unescape_string_literal(bit)\n-                or_queries = models.Q.create(\n-                    [(orm_lookup, bit) for orm_lookup in orm_lookups],\n-                    connector=models.Q.OR,\n-                )\n-                term_queries.append(or_queries)\n-            queryset = queryset.filter(models.Q.create(term_queries))\n+                # Build lookups for this term, skipping invalid field/value\n+                # combinations to preserve index usage.\n+                bit_lookups = []\n+                for orm_lookup, validate_field in orm_lookups:\n+                    if validate_field is not None:\n+                        try:\n+                            bit = validate_field.to_python(bit)",
      "comment": "Sorry, I missed their comment about this (and, again, I'm trying not to make many changes).\r\n\r\nI think it makes sense to be more lenient here and use formfields instead of model fields. I just pushed a tweak and a test accordingly. I don't have tests working locally, so I'll update Track once tests are set to go.\r\n\r\nThanks for the review.",
      "comment_id": 2739394523,
      "user": "mlissner",
      "created_at": "2026-01-29T01:45:37Z",
      "url": "https://github.com/django/django/pull/20538#discussion_r2739394523"
    },
    {
      "repo": "django/django",
      "pr_number": 20538,
      "file_path": "django/contrib/admin/options.py",
      "line": 1196,
      "side": "RIGHT",
      "diff_hunk": "@@ -1179,30 +1177,43 @@ def construct_search(field_name):\n         may_have_duplicates = False\n         search_fields = self.get_search_fields(request)\n         if search_fields and search_term:\n-            str_aliases = {}\n             orm_lookups = []\n             for field in search_fields:\n-                lookup, str_alias = construct_search(str(field))\n-                orm_lookups.append(lookup)\n-                if str_alias:\n-                    str_aliases[lookup] = str_alias\n-\n-            if str_aliases:\n-                queryset = queryset.alias(**str_aliases)\n+                orm_lookups.append(construct_search(str(field)))\n \n             term_queries = []\n             for bit in smart_split(search_term):\n                 if bit.startswith(('\"', \"'\")) and bit[0] == bit[-1]:\n                     bit = unescape_string_literal(bit)\n-                or_queries = models.Q.create(\n-                    [(orm_lookup, bit) for orm_lookup in orm_lookups],\n-                    connector=models.Q.OR,\n-                )\n-                term_queries.append(or_queries)\n-            queryset = queryset.filter(models.Q.create(term_queries))\n+                # Build lookups for this term, skipping invalid field/value\n+                # combinations to preserve index usage.\n+                bit_lookups = []\n+                for orm_lookup, validate_field in orm_lookups:\n+                    if validate_field is not None:\n+                        formfield = validate_field.formfield()\n+                        try:\n+                            if formfield is not None:\n+                                bit = formfield.to_python(bit)",
      "comment": "I think we shouldn't shadow the outer loop variable.\n\nI found that with two fields, e.g. IntegerField and JSONField (and with search fields `[\"int__exact\", \"json__exact\"]`), a bit that was only valid as an int but not as json (e.g. \"3.\"), translated into valid `Q` objects for both fields instead of only the int field.\n\nThe `bit` from the `IntegerField.to_python()` result was still `3` on the second loop iteration, which became a valid input to `JSONField.to_python()`.\n\nCan you address & add a test?",
      "comment_id": 2742232366,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-29T15:30:27Z",
      "url": "https://github.com/django/django/pull/20538#discussion_r2742232366"
    },
    {
      "repo": "django/django",
      "pr_number": 20538,
      "file_path": "django/contrib/admin/options.py",
      "line": 1202,
      "side": "RIGHT",
      "diff_hunk": "@@ -1179,30 +1177,45 @@ def construct_search(field_name):\n         may_have_duplicates = False\n         search_fields = self.get_search_fields(request)\n         if search_fields and search_term:\n-            str_aliases = {}\n             orm_lookups = []\n             for field in search_fields:\n-                lookup, str_alias = construct_search(str(field))\n-                orm_lookups.append(lookup)\n-                if str_alias:\n-                    str_aliases[lookup] = str_alias\n-\n-            if str_aliases:\n-                queryset = queryset.alias(**str_aliases)\n+                orm_lookups.append(construct_search(str(field)))\n \n             term_queries = []\n             for bit in smart_split(search_term):\n                 if bit.startswith(('\"', \"'\")) and bit[0] == bit[-1]:\n                     bit = unescape_string_literal(bit)\n-                or_queries = models.Q.create(\n-                    [(orm_lookup, bit) for orm_lookup in orm_lookups],\n-                    connector=models.Q.OR,\n-                )\n-                term_queries.append(or_queries)\n-            queryset = queryset.filter(models.Q.create(term_queries))\n+                # Build lookups for this term, skipping invalid field/value\n+                # combinations to preserve index usage.\n+                bit_lookups = []\n+                for orm_lookup, validate_field in orm_lookups:\n+                    if validate_field is not None:\n+                        formfield = validate_field.formfield()\n+                        try:\n+                            if formfield is not None:\n+                                value = formfield.to_python(bit)\n+                            else:\n+                                # Fallback for fields without form fields (e.g.\n+                                # AutoField).\n+                                value = validate_field.to_python(bit)\n+                        except ValidationError:\n+                            # Skip this lookup for invalid values.\n+                            continue",
      "comment": "We could return some some sort of sentinel here that signals it's okay to skip this `orm_lookup` when calculating `may_have_duplicates` below, but this is just a subtle optimization opportunity to avoid a `distinct()` later on, and I'm comfortable leaving it for someone else in a follow-up \ud83d\udc4d .",
      "comment_id": 2742731412,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-29T17:24:03Z",
      "url": "https://github.com/django/django/pull/20538#discussion_r2742731412"
    },
    {
      "repo": "django/django",
      "pr_number": 20608,
      "file_path": "tests/utils_tests/test_html.py",
      "line": 104,
      "side": "LEFT",
      "diff_hunk": "@@ -100,10 +100,9 @@ def test_strip_tags(self):\n             (3, 11): (3, 11, 14),\n             (3, 10): (3, 10, 19),\n             (3, 9): (3, 9, 24),\n+            # Not fixed in 3.8.\n+            (3, 8): (3, 8, math.inf),\n         }\n-        htmlparser_fixed_security = (",
      "comment": "This is defined below (conflict incorrectly resolved?)",
      "comment_id": 2738790009,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-28T21:58:32Z",
      "url": "https://github.com/django/django/pull/20608#discussion_r2738790009"
    },
    {
      "repo": "django/django",
      "pr_number": 20574,
      "file_path": "django/db/migrations/state.py",
      "line": 195,
      "side": "RIGHT",
      "diff_hunk": "@@ -192,9 +192,10 @@ def alter_model_options(self, app_label, model_name, options, option_keys=None):\n     def remove_model_options(self, app_label, model_name, option_name, value_to_remove):\n         model_state = self.models[app_label, model_name]\n         if objs := model_state.options.get(option_name):\n-            model_state.options[option_name] = [\n-                obj for obj in objs if tuple(obj) != tuple(value_to_remove)\n-            ]\n+            new_value = [obj for obj in objs if tuple(obj) != tuple(value_to_remove)]",
      "comment": "I haven't been able to come up with a test triggering the same problem as reported, but there is a type inconsistency here.",
      "comment_id": 2721704994,
      "user": "MarkusH",
      "created_at": "2026-01-23T15:30:31Z",
      "url": "https://github.com/django/django/pull/20574#discussion_r2721704994"
    },
    {
      "repo": "django/django",
      "pr_number": 20574,
      "file_path": "django/db/migrations/state.py",
      "line": 343,
      "side": "RIGHT",
      "diff_hunk": "@@ -339,10 +340,10 @@ def rename_field(self, app_label, model_name, old_name, new_name):\n         options = model_state.options\n         for option in (\"index_together\", \"unique_together\"):\n             if option in options:\n-                options[option] = [\n-                    [new_name if n == old_name else n for n in together]\n+                options[option] = set(",
      "comment": "```suggestion\n                options[option] = {\n```",
      "comment_id": 2721706428,
      "user": "MarkusH",
      "created_at": "2026-01-23T15:30:51Z",
      "url": "https://github.com/django/django/pull/20574#discussion_r2721706428"
    },
    {
      "repo": "django/django",
      "pr_number": 20574,
      "file_path": "django/db/migrations/state.py",
      "line": 195,
      "side": "RIGHT",
      "diff_hunk": "@@ -192,9 +192,10 @@ def alter_model_options(self, app_label, model_name, options, option_keys=None):\n     def remove_model_options(self, app_label, model_name, option_name, value_to_remove):\n         model_state = self.models[app_label, model_name]\n         if objs := model_state.options.get(option_name):\n-            model_state.options[option_name] = [\n-                obj for obj in objs if tuple(obj) != tuple(value_to_remove)\n-            ]\n+            new_value = [obj for obj in objs if tuple(obj) != tuple(value_to_remove)]",
      "comment": "I added some minimal coverage like this:\n```diff\ndiff --git a/tests/migrations/test_operations.py b/tests/migrations/test_operations.py\nindex 28d65a7ecf..d3a2f732f2 100644\n--- a/tests/migrations/test_operations.py\n+++ b/tests/migrations/test_operations.py\n@@ -3974,6 +3974,7 @@ class OperationTests(OperationTestBase):\n         )\n         new_state = project_state.clone()\n         operation.state_forwards(app_label, new_state)\n+        self.assertIsInstance(new_state.models[app_label, \"pony\"].options[\"index_together\"], set)\n         # Rename index.\n         with connection.schema_editor() as editor:\n             operation.database_forwards(app_label, editor, project_state, new_state)\n```\n\nUnless you think that's testing an implementation detail? Poking around, we have other assertions against `new_state.models`.",
      "comment_id": 2737017476,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-28T14:53:53Z",
      "url": "https://github.com/django/django/pull/20574#discussion_r2737017476"
    },
    {
      "repo": "django/django",
      "pr_number": 20574,
      "file_path": "tests/migrations/test_autodetector.py",
      "line": 5596,
      "side": "RIGHT",
      "diff_hunk": "@@ -5590,6 +5590,54 @@ def test_remove_composite_pk(self):\n             preserve_default=True,\n         )\n \n+    def test_does_not_crash_after_rename_on_unique_together(self):\n+        \"\"\"\n+        Regression test for https://code.djangoproject.com/ticket/36878\n+        \"\"\"",
      "comment": "You can chop, we just leave ticket references in the blame these days.",
      "comment_id": 2737019284,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-28T14:54:19Z",
      "url": "https://github.com/django/django/pull/20574#discussion_r2737019284"
    },
    {
      "repo": "django/django",
      "pr_number": 20574,
      "file_path": "django/db/migrations/state.py",
      "line": 195,
      "side": "RIGHT",
      "diff_hunk": "@@ -192,9 +192,10 @@ def alter_model_options(self, app_label, model_name, options, option_keys=None):\n     def remove_model_options(self, app_label, model_name, option_name, value_to_remove):\n         model_state = self.models[app_label, model_name]\n         if objs := model_state.options.get(option_name):\n-            model_state.options[option_name] = [\n-                obj for obj in objs if tuple(obj) != tuple(value_to_remove)\n-            ]\n+            new_value = [obj for obj in objs if tuple(obj) != tuple(value_to_remove)]",
      "comment": "If this assertion feels like a non sequitur, we could pull it out into a new test method.",
      "comment_id": 2737068892,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-28T15:04:51Z",
      "url": "https://github.com/django/django/pull/20574#discussion_r2737068892"
    },
    {
      "repo": "django/django",
      "pr_number": 20021,
      "file_path": "django/core/management/base.py",
      "line": 304,
      "side": "RIGHT",
      "diff_hunk": "@@ -298,6 +298,11 @@ def create_parser(self, prog_name, subcommand, **kwargs):\n         parse the arguments to this command.\n         \"\"\"\n         kwargs.setdefault(\"formatter_class\", DjangoHelpFormatter)\n+        \n+        # Enable suggest_on_error for Python 3.14+\n+        if sys.version_info >= (3, 14):\n+            kwargs.setdefault(\"suggest_on_error\", True)",
      "comment": "Use `PY314` from `django.utils.version`\r\n```suggestion\r\n        if PY314:\r\n            kwargs.setdefault(\"suggest_on_error\", True)\r\n```",
      "comment_id": 2476883281,
      "user": "felixxm",
      "created_at": "2025-10-30T08:40:07Z",
      "url": "https://github.com/django/django/pull/20021#discussion_r2476883281"
    },
    {
      "repo": "django/django",
      "pr_number": 20021,
      "file_path": "django/core/management/commands/makemessages.py",
      "line": 393,
      "side": "RIGHT",
      "diff_hunk": "@@ -382,10 +383,15 @@ def handle(self, *args, **options):\n             self.invoked_for_django = True\n         else:\n             if self.settings_available:\n-                self.locale_paths.extend(settings.LOCALE_PATHS)\n+                for path in settings.LOCALE_PATHS:\n+                    locale_path = os.path.abspath(path)\n+                    if locale_path not in self.locale_paths:\n+                        self.locale_paths.append(locale_path)\n             # Allow to run makemessages inside an app dir\n             if os.path.isdir(\"locale\"):\n-                self.locale_paths.append(os.path.abspath(\"locale\"))\n+                locale_path = os.path.abspath(\"locale\")\n+                if locale_path not in self.locale_paths:\n+                    self.locale_paths.append(locale_path)",
      "comment": "I think you mixed this patch with some unrelated changes in `makemessages`, please remove them.",
      "comment_id": 2476889835,
      "user": "felixxm",
      "created_at": "2025-10-30T08:42:03Z",
      "url": "https://github.com/django/django/pull/20021#discussion_r2476889835"
    },
    {
      "repo": "django/django",
      "pr_number": 20021,
      "file_path": "django/core/management/base.py",
      "line": 62,
      "side": "RIGHT",
      "diff_hunk": "@@ -57,6 +58,9 @@ def __init__(\n     ):\n         self.missing_args_message = missing_args_message\n         self.called_from_command_line = called_from_command_line\n+        # Enable suggest_on_error for Python 3.14+\n+        if PY314 and called_from_command_line:",
      "comment": "I just spotted that the default has flipped to `True` on Python 3.15: https://github.com/python/cpython/commit/d2f3cfd38457204a2c47162de6f89ee3dd22fa99\n\nTherefore, let's change the check to:\n```suggestion\n        if PY314 and not PY315 and called_from_command_line:\n            # Adopt Python 3.15 default early.\n```\n\nThis way, when we drop Python 3.14 support, it should be clear that we can remove this block.",
      "comment_id": 2485983581,
      "user": "adamchainz",
      "created_at": "2025-11-03T10:21:17Z",
      "url": "https://github.com/django/django/pull/20021#discussion_r2485983581"
    },
    {
      "repo": "django/django",
      "pr_number": 20021,
      "file_path": "tests/user_commands/tests.py",
      "line": 463,
      "side": "RIGHT",
      "diff_hunk": "@@ -455,6 +457,80 @@ def test_outputwrapper_flush(self):\n         self.assertIs(mocked_flush.called, True)\n \n \n+class SuggestOnErrorTests(SimpleTestCase):\n+    \"\"\"\n+    Tests for argparse suggest_on_error feature on Python 3.14+.\n+    \"\"\"",
      "comment": "These tests do not need a dedicated test case and can be placed at the end of `CommandTests`",
      "comment_id": 2485988570,
      "user": "adamchainz",
      "created_at": "2025-11-03T10:23:18Z",
      "url": "https://github.com/django/django/pull/20021#discussion_r2485988570"
    },
    {
      "repo": "django/django",
      "pr_number": 20021,
      "file_path": "tests/user_commands/tests.py",
      "line": 531,
      "side": "RIGHT",
      "diff_hunk": "@@ -455,6 +457,80 @@ def test_outputwrapper_flush(self):\n         self.assertIs(mocked_flush.called, True)\n \n \n+class SuggestOnErrorTests(SimpleTestCase):\n+    \"\"\"\n+    Tests for argparse suggest_on_error feature on Python 3.14+.\n+    \"\"\"\n+\n+    def test_parser_kwargs_suggest_on_error_on_python_314_plus(self):\n+        \"\"\"\n+        CommandParser sets suggest_on_error=True on Python 3.14+.\n+        \"\"\"\n+        command = BaseCommand()\n+        command._called_from_command_line = True  # ADD THIS LINE\n+        parser = command.create_parser(\"prog_name\", \"subcommand\")\n+\n+        if PY314:\n+            self.assertTrue(\n+                getattr(parser, \"suggest_on_error\", False),\n+                \"Parser should have suggest_on_error=True on Python 3.14+\",\n+            )\n+\n+    @unittest.skipUnless(PY314, \"Requires Python 3.14+\")\n+    def test_custom_suggest_on_error_respected(self):\n+        \"\"\"\n+        Explicit suggest_on_error=False is respected.\n+        \"\"\"\n+        command = BaseCommand()\n+        command._called_from_command_line = True  # ADD THIS LINE\n+        parser = command.create_parser(\n+            \"prog_name\", \"subcommand\", suggest_on_error=False\n+        )\n+        self.assertFalse(\n+            parser.suggest_on_error,\n+            \"Explicit suggest_on_error=False is respected\",\n+        )\n+\n+    @unittest.skipUnless(PY314, \"Requires Python 3.14+\")\n+    def test_misspelled_option_suggests_correct_option(self):\n+        \"\"\"\n+        On Python 3.14+, misspelled options trigger suggestions when available.\n+        \"\"\"\n+        command = BaseCommand()\n+        command._called_from_command_line = True\n+        parser = command.create_parser(\"django-admin\", \"test\")\n+\n+        err = StringIO()\n+        with mock.patch(\"sys.stderr\", err):\n+            with self.assertRaises(SystemExit) as cm:\n+                parser.parse_args([\"--verbositty\", \"2\"])\n+        self.assertEqual(cm.exception.code, 2)\n+\n+        error_output = err.getvalue().lower()\n+        # Ensure it failed for the right reason\n+        self.assertIn(\"unrecognized arguments\", error_output)\n+\n+        # On Python 3.14+, suggestions *may* appear depending on environment\n+        if \"did you mean\" in error_output:\n+            self.assertIn(\"--verbosity\", error_output)\n+\n+    def test_suggest_on_error_works_with_management_commands(self):\n+        \"\"\"\n+        Management commands have suggest_on_error on Python 3.14+.\n+        \"\"\"\n+        from .management.commands.dance import Command as DanceCommand\n+\n+        dance_cmd = DanceCommand()\n+        dance_cmd._called_from_command_line = True  # ADD THIS LINE\n+        parser = dance_cmd.create_parser(\"django-admin\", \"dance\")\n+\n+        if PY314:\n+            self.assertTrue(\n+                getattr(parser, \"suggest_on_error\", False),\n+                \"Management command parsers should have suggest_on_error=True\",\n+            )",
      "comment": "Some fixes for your tests:\n\n1. Use `unittest.skipUnless` on all tests.\n2. Use `not PY315` per the above comments about the default chanigng on Python 3.15.\n3. Drop the `# ADD THIS LINE` comments, which look like they came from someone's review?\n4. Drop the behaviour tests - they are not valuable, as they only test argparse's behaviour, which we can depend on.\n\n```suggestion\n    @unittest.skipUnless(PY314 and not PY315, \"Requires Python 3.14\")\n    def test_suggest_on_error_defaults_true(self):\n        \"\"\"\n        CommandParser sets suggest_on_error=True on Python 3.14+.\n        \"\"\"\n        command = BaseCommand()\n        command._called_from_command_line = True\n        parser = command.create_parser(\"prog_name\", \"subcommand\")\n\n        self.assertTrue(parser.suggest_on_error)\n\n    @unittest.skipUnless(PY314 and not PY315, \"Requires Python 3.14+\")\n    def test_suggest_on_error_custom(self):\n        \"\"\"\n        Explicit suggest_on_error=False is respected.\n        \"\"\"\n        command = BaseCommand()\n        command._called_from_command_line = True\n        parser = command.create_parser(\n            \"prog_name\", \"subcommand\", suggest_on_error=False\n        )\n        self.assertFalse(parser.suggest_on_error)\n```",
      "comment_id": 2485998264,
      "user": "adamchainz",
      "created_at": "2025-11-03T10:27:03Z",
      "url": "https://github.com/django/django/pull/20021#discussion_r2485998264"
    },
    {
      "repo": "django/django",
      "pr_number": 20021,
      "file_path": "tests/user_commands/tests.py",
      "line": 515,
      "side": "RIGHT",
      "diff_hunk": "@@ -455,6 +457,80 @@ def test_outputwrapper_flush(self):\n         self.assertIs(mocked_flush.called, True)\n \n \n+class SuggestOnErrorTests(SimpleTestCase):\n+    \"\"\"\n+    Tests for argparse suggest_on_error feature on Python 3.14+.\n+    \"\"\"\n+\n+    def test_parser_kwargs_suggest_on_error_on_python_314_plus(self):\n+        \"\"\"\n+        CommandParser sets suggest_on_error=True on Python 3.14+.\n+        \"\"\"\n+        command = BaseCommand()\n+        command._called_from_command_line = True  # ADD THIS LINE\n+        parser = command.create_parser(\"prog_name\", \"subcommand\")\n+\n+        if PY314:\n+            self.assertTrue(\n+                getattr(parser, \"suggest_on_error\", False),\n+                \"Parser should have suggest_on_error=True on Python 3.14+\",\n+            )\n+\n+    @unittest.skipUnless(PY314, \"Requires Python 3.14+\")\n+    def test_custom_suggest_on_error_respected(self):\n+        \"\"\"\n+        Explicit suggest_on_error=False is respected.\n+        \"\"\"\n+        command = BaseCommand()\n+        command._called_from_command_line = True  # ADD THIS LINE\n+        parser = command.create_parser(\n+            \"prog_name\", \"subcommand\", suggest_on_error=False\n+        )\n+        self.assertFalse(\n+            parser.suggest_on_error,\n+            \"Explicit suggest_on_error=False is respected\",\n+        )\n+\n+    @unittest.skipUnless(PY314, \"Requires Python 3.14+\")\n+    def test_misspelled_option_suggests_correct_option(self):\n+        \"\"\"\n+        On Python 3.14+, misspelled options trigger suggestions when available.\n+        \"\"\"\n+        command = BaseCommand()\n+        command._called_from_command_line = True\n+        parser = command.create_parser(\"django-admin\", \"test\")\n+\n+        err = StringIO()\n+        with mock.patch(\"sys.stderr\", err):\n+            with self.assertRaises(SystemExit) as cm:\n+                parser.parse_args([\"--verbositty\", \"2\"])\n+        self.assertEqual(cm.exception.code, 2)\n+\n+        error_output = err.getvalue().lower()\n+        # Ensure it failed for the right reason\n+        self.assertIn(\"unrecognized arguments\", error_output)\n+\n+        # On Python 3.14+, suggestions *may* appear depending on environment\n+        if \"did you mean\" in error_output:\n+            self.assertIn(\"--verbosity\", error_output)",
      "comment": "This test was not testing the correct output, or expecting the right correction. `suggest_on_error` does not affect mistyped options. It affects mistyped option *choices* and subparser names.\n\nThe way I'm testing it manually is:\n\n```\n$ python -m django check --fail-level EROR\n...\n__main__.py check: error: argument --fail-level: invalid choice: 'EROR', maybe you meant 'ERROR'? (choose from CRITICAL, ERROR, WARNING, INFO, DEBUG)\n```\n\nNote the \"maybe you meant 'ERROR'?\"\n\nI don't think argparse *has* any feature to suggest for mistyped options at current. Did you see this assertion pass on any environment?",
      "comment_id": 2486050288,
      "user": "adamchainz",
      "created_at": "2025-11-03T10:46:47Z",
      "url": "https://github.com/django/django/pull/20021#discussion_r2486050288"
    },
    {
      "repo": "django/django",
      "pr_number": 20021,
      "file_path": "tests/user_commands/tests.py",
      "line": 515,
      "side": "RIGHT",
      "diff_hunk": "@@ -455,6 +457,80 @@ def test_outputwrapper_flush(self):\n         self.assertIs(mocked_flush.called, True)\n \n \n+class SuggestOnErrorTests(SimpleTestCase):\n+    \"\"\"\n+    Tests for argparse suggest_on_error feature on Python 3.14+.\n+    \"\"\"\n+\n+    def test_parser_kwargs_suggest_on_error_on_python_314_plus(self):\n+        \"\"\"\n+        CommandParser sets suggest_on_error=True on Python 3.14+.\n+        \"\"\"\n+        command = BaseCommand()\n+        command._called_from_command_line = True  # ADD THIS LINE\n+        parser = command.create_parser(\"prog_name\", \"subcommand\")\n+\n+        if PY314:\n+            self.assertTrue(\n+                getattr(parser, \"suggest_on_error\", False),\n+                \"Parser should have suggest_on_error=True on Python 3.14+\",\n+            )\n+\n+    @unittest.skipUnless(PY314, \"Requires Python 3.14+\")\n+    def test_custom_suggest_on_error_respected(self):\n+        \"\"\"\n+        Explicit suggest_on_error=False is respected.\n+        \"\"\"\n+        command = BaseCommand()\n+        command._called_from_command_line = True  # ADD THIS LINE\n+        parser = command.create_parser(\n+            \"prog_name\", \"subcommand\", suggest_on_error=False\n+        )\n+        self.assertFalse(\n+            parser.suggest_on_error,\n+            \"Explicit suggest_on_error=False is respected\",\n+        )\n+\n+    @unittest.skipUnless(PY314, \"Requires Python 3.14+\")\n+    def test_misspelled_option_suggests_correct_option(self):\n+        \"\"\"\n+        On Python 3.14+, misspelled options trigger suggestions when available.\n+        \"\"\"\n+        command = BaseCommand()\n+        command._called_from_command_line = True\n+        parser = command.create_parser(\"django-admin\", \"test\")\n+\n+        err = StringIO()\n+        with mock.patch(\"sys.stderr\", err):\n+            with self.assertRaises(SystemExit) as cm:\n+                parser.parse_args([\"--verbositty\", \"2\"])\n+        self.assertEqual(cm.exception.code, 2)\n+\n+        error_output = err.getvalue().lower()\n+        # Ensure it failed for the right reason\n+        self.assertIn(\"unrecognized arguments\", error_output)\n+\n+        # On Python 3.14+, suggestions *may* appear depending on environment\n+        if \"did you mean\" in error_output:\n+            self.assertIn(\"--verbosity\", error_output)",
      "comment": "You're right @adamchainz,  I apologize for the confusion. I did not actually test this on Python 3.14+. I was working on Python 3.12 and wrote tests based on the ticket description, which turned out to be incorrect about what `suggest_on_error` does.\r\n\r\nThank you for the clarification that it only affects:\r\n- Mistyped argument **choices** (like `--fail-level EROR` \u2192 `ERROR`)  \r\n- Mistyped **subparser names** (like `python manage.py chek` \u2192 `check`)\r\n\r\nAnd NOT mistyped option flags (like `--verbositty`).\r\n\r\nI've removed the incorrect behavioral test. The remaining two tests just verify that the setting is applied correctly, which is appropriate since we're relying on argparse's implementation.\r\n\r\nThe ticket description should probably be updated to reflect the actual behavior of this feature.",
      "comment_id": 2496146732,
      "user": "kihuni",
      "created_at": "2025-11-05T21:00:55Z",
      "url": "https://github.com/django/django/pull/20021#discussion_r2496146732"
    },
    {
      "repo": "django/django",
      "pr_number": 20021,
      "file_path": "django/core/management/base.py",
      "line": 61,
      "side": "RIGHT",
      "diff_hunk": "@@ -57,6 +58,8 @@ def __init__(\n     ):\n         self.missing_args_message = missing_args_message\n         self.called_from_command_line = called_from_command_line\n+        if PY314 and not PY315 and called_from_command_line:",
      "comment": "Curious why you decided to add `and called_from_command_line` here, couldn't we opt in everywhere?",
      "comment_id": 2499714689,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-06T16:14:50Z",
      "url": "https://github.com/django/django/pull/20021#discussion_r2499714689"
    },
    {
      "repo": "django/django",
      "pr_number": 20021,
      "file_path": "django/core/management/base.py",
      "line": 61,
      "side": "RIGHT",
      "diff_hunk": "@@ -57,6 +58,8 @@ def __init__(\n     ):\n         self.missing_args_message = missing_args_message\n         self.called_from_command_line = called_from_command_line\n+        if PY314 and not PY315 and called_from_command_line:",
      "comment": "Hi @jacobtylerwalls, I added the `called_from_command_line` check because I hit a pickling error in CI on Python 3.14:\r\n```\r\nPicklingError(\"Can't pickle local object <function ArgumentParser.__init__.<locals>.identity>\")\r\n```\r\n\r\nThis broke the parallel test runner. I thought limiting it to command-line usage would avoid the issue.\r\n\r\nI see the issues persist after removing `called_from_command_line`.\r\n\r\nI would really appreciate your guidance on this one.",
      "comment_id": 2500426927,
      "user": "kihuni",
      "created_at": "2025-11-06T18:59:13Z",
      "url": "https://github.com/django/django/pull/20021#discussion_r2500426927"
    },
    {
      "repo": "django/django",
      "pr_number": 20021,
      "file_path": "django/core/management/base.py",
      "line": 61,
      "side": "RIGHT",
      "diff_hunk": "@@ -57,6 +58,8 @@ def __init__(\n     ):\n         self.missing_args_message = missing_args_message\n         self.called_from_command_line = called_from_command_line\n+        if PY314 and not PY315 and called_from_command_line:",
      "comment": "I don't see the error, but I'm happy to look into it if you have steps to reproduce. Sometimes the parallel test runner spews irrelevant information underneath advice like:\r\n```\r\nIn order to see the traceback, you should install tblib:\r\n\r\n    python -m pip install tblib\r\n```",
      "comment_id": 2500531071,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-06T19:30:11Z",
      "url": "https://github.com/django/django/pull/20021#discussion_r2500531071"
    },
    {
      "repo": "django/django",
      "pr_number": 20021,
      "file_path": "django/core/management/base.py",
      "line": 61,
      "side": "RIGHT",
      "diff_hunk": "@@ -57,6 +58,8 @@ def __init__(\n     ):\n         self.missing_args_message = missing_args_message\n         self.called_from_command_line = called_from_command_line\n+        if PY314 and not PY315 and called_from_command_line:",
      "comment": "Can you update the failing test? (test_invalid_choice_db_option)",
      "comment_id": 2500531932,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-06T19:30:27Z",
      "url": "https://github.com/django/django/pull/20021#discussion_r2500531932"
    },
    {
      "repo": "django/django",
      "pr_number": 20021,
      "file_path": "django/core/management/base.py",
      "line": 61,
      "side": "RIGHT",
      "diff_hunk": "@@ -57,6 +58,8 @@ def __init__(\n     ):\n         self.missing_args_message = missing_args_message\n         self.called_from_command_line = called_from_command_line\n+        if PY314 and not PY315 and called_from_command_line:",
      "comment": "Here's the full traceback from the Windows Python 3.14 CI run:\r\n```\r\nmultiprocessing.pool.MaybeEncodingError: Error sending result: '<multiprocessing.pool.ExceptionWithTraceback object at 0x000002587F365940>'. \r\nReason: 'PicklingError(\"Can't pickle local object <function ArgumentParser.__init__.<locals>.identity at 0x000002587F4075E0>\")'\r\n```\r\n\r\nThis is happening in Django's parallel test runner when it tries to pickle test objects that contain `ArgumentParser` instances with `suggest_on_error=True`.\r\n\r\nThe error occurs consistently on Python 3.14 when `suggest_on_error=True` is enabled unconditionally. That's why I limited it to `called_from_command_line`. It still provides the feature to end users while avoiding the multiprocessing issue in tests.\r\n\r\nIs there a better approach to handle this?",
      "comment_id": 2501696847,
      "user": "kihuni",
      "created_at": "2025-11-07T05:05:12Z",
      "url": "https://github.com/django/django/pull/20021#discussion_r2501696847"
    },
    {
      "repo": "django/django",
      "pr_number": 20021,
      "file_path": "django/core/management/base.py",
      "line": 61,
      "side": "RIGHT",
      "diff_hunk": "@@ -57,6 +58,8 @@ def __init__(\n     ):\n         self.missing_args_message = missing_args_message\n         self.called_from_command_line = called_from_command_line\n+        if PY314 and not PY315 and called_from_command_line:",
      "comment": "I'm not convinced that's relevant yet. I think we should start with fixing the test failure in `test_invalid_choice_db_option`.",
      "comment_id": 2503793045,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-07T14:20:34Z",
      "url": "https://github.com/django/django/pull/20021#discussion_r2503793045"
    },
    {
      "repo": "django/django",
      "pr_number": 20021,
      "file_path": "django/core/management/base.py",
      "line": 61,
      "side": "RIGHT",
      "diff_hunk": "@@ -57,6 +58,8 @@ def __init__(\n     ):\n         self.missing_args_message = missing_args_message\n         self.called_from_command_line = called_from_command_line\n+        if PY314 and not PY315 and called_from_command_line:",
      "comment": "Hi @jacobtylerwalls, could you help me understand which specific test failure I should focus on? The CI logs show pickling errors, but I'm not seeing a clear test failure message.\r\n\r\nCause when I enable `suggest_on_error` everywhere, i get \r\n<img width=\"758\" height=\"871\" alt=\"Screenshot from 2025-11-14 11-54-28\" src=\"https://github.com/user-attachments/assets/46d4ae55-b7d1-472d-a915-0ff5db338e70\" />     But when i limit to when `called_from_command_line=True` when user runs command in the command line, the tests passes.\r\n",
      "comment_id": 2529500850,
      "user": "kihuni",
      "created_at": "2025-11-15T03:24:07Z",
      "url": "https://github.com/django/django/pull/20021#discussion_r2529500850"
    },
    {
      "repo": "django/django",
      "pr_number": 20021,
      "file_path": "django/core/management/base.py",
      "line": 61,
      "side": "RIGHT",
      "diff_hunk": "@@ -57,6 +58,8 @@ def __init__(\n     ):\n         self.missing_args_message = missing_args_message\n         self.called_from_command_line = called_from_command_line\n+        if PY314 and not PY315 and called_from_command_line:",
      "comment": "It's possible you haven't scrolled up far enough, I can't tell.\r\n\r\nHere's the error, it's just an update needed for the expected string (based on the python version):\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/jwalls/django/tests/admin_scripts/tests.py\", line 2490, in test_invalid_choice_db_option\r\n    with self.assertRaisesRegex(CommandError, expected_error):\r\n         ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nAssertionError: \"Error: argument --database: invalid choice: 'deflaut' \\(choose from '?default'?, '?other'?\\)\" does not match \"Error: argument --database: invalid choice: 'deflaut', maybe you meant 'default'? (choose from default, other)\"\r\n\r\n----------------------------------------------------------------------\r\n```",
      "comment_id": 2529885390,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-15T13:22:42Z",
      "url": "https://github.com/django/django/pull/20021#discussion_r2529885390"
    },
    {
      "repo": "django/django",
      "pr_number": 20021,
      "file_path": "django/core/management/base.py",
      "line": 61,
      "side": "RIGHT",
      "diff_hunk": "@@ -57,6 +58,8 @@ def __init__(\n     ):\n         self.missing_args_message = missing_args_message\n         self.called_from_command_line = called_from_command_line\n+        if PY314 and not PY315 and called_from_command_line:",
      "comment": "@jacobtylerwalls Thank you for pinpointing the exact issue! \r\n\r\nThe test `test_invalid_choice_db_option` expects the error message format without suggestions, but on Python 3.14 with `suggest_on_error=True`, argparse now adds `\", maybe you meant 'default'?\"` to the message.\r\n\r\nI've updated the test to use a conditional regex pattern based on Python versions:\r\n- **Python 3.14**: Expects `\"invalid choice: 'deflaut', maybe you meant 'default'? (choose from default, other)\"`\r\n- **Other versions**: Expects `\"invalid choice: 'deflaut' (choose from 'default', 'other')\"`\r\n\r\nI added `PY314` and `PY315` imports to `tests/admin_scripts/tests.py`, updated `expected_error` to be conditional based on Python version, and adjusted the regex to match the new format on Python 3.14.\r\n\r\nDoes this approach look good, or would you prefer a different way to handle version differences?",
      "comment_id": 2531737909,
      "user": "kihuni",
      "created_at": "2025-11-16T08:27:28Z",
      "url": "https://github.com/django/django/pull/20021#discussion_r2531737909"
    },
    {
      "repo": "django/django",
      "pr_number": 20021,
      "file_path": "tests/admin_scripts/tests.py",
      "line": 2450,
      "side": "RIGHT",
      "diff_hunk": "@@ -2446,10 +2446,19 @@ def test_precedence(self):\n \n class CommandDBOptionChoiceTests(SimpleTestCase):\n     def test_invalid_choice_db_option(self):\n-        expected_error = (\n-            r\"Error: argument --database: invalid choice: 'deflaut' \"\n-            r\"\\(choose from '?default'?, '?other'?\\)\"\n-        )\n+        # Update expected error based on Python version\n+        if PY314 and not PY315:",
      "comment": "We're expecting this change in all versions after 3.14. 3.15 alpha 2 comes out tomorrow and should have this change. So please go ahead and update this.\n```suggestion\n        if PY314:\n```",
      "comment_id": 2534568137,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-17T15:38:46Z",
      "url": "https://github.com/django/django/pull/20021#discussion_r2534568137"
    },
    {
      "repo": "django/django",
      "pr_number": 20021,
      "file_path": "tests/admin_scripts/tests.py",
      "line": 2457,
      "side": "RIGHT",
      "diff_hunk": "@@ -2446,10 +2446,19 @@ def test_precedence(self):\n \n class CommandDBOptionChoiceTests(SimpleTestCase):\n     def test_invalid_choice_db_option(self):\n-        expected_error = (\n-            r\"Error: argument --database: invalid choice: 'deflaut' \"\n-            r\"\\(choose from '?default'?, '?other'?\\)\"\n-        )\n+        # Update expected error based on Python version\n+        if PY314 and not PY315:\n+            # Python 3.14 includes suggestions\n+            expected_error = (\n+                r\"Error: argument --database: invalid choice: 'deflaut', \"\n+                r\"maybe you meant 'default'\\? \\(choose from default, other\\)\"\n+            )\n+        else:\n+            # Python < 3.14 or >= 3.15",
      "comment": "Thank you @jacobtylerwalls \ud83d\ude4f\r\n\r\nI\u2019ve updated the condition to use only if `PY314:` as suggested, and removed the outdated comments. Please let me know if you'd like me to make any other adjustments.",
      "comment_id": 2534894303,
      "user": "kihuni",
      "created_at": "2025-11-17T17:09:23Z",
      "url": "https://github.com/django/django/pull/20021#discussion_r2534894303"
    },
    {
      "repo": "django/django",
      "pr_number": 20021,
      "file_path": "django/core/management/base.py",
      "line": 62,
      "side": "RIGHT",
      "diff_hunk": "@@ -57,6 +58,8 @@ def __init__(\n     ):\n         self.missing_args_message = missing_args_message\n         self.called_from_command_line = called_from_command_line\n+        if PY314:\n+            kwargs.setdefault(\"suggest_on_error\", True)",
      "comment": "Hey @kihuni. My idea here was to keep the `and not PY315` but just lose the called from command line bit. The idea being that when we eventually drop python 3.14, we will know this code is _only_ needed on 3.14, and we can just remove the code instead of deindenting it at the time. Does that make sense?",
      "comment_id": 2535161483,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-17T18:44:52Z",
      "url": "https://github.com/django/django/pull/20021#discussion_r2535161483"
    },
    {
      "repo": "django/django",
      "pr_number": 20021,
      "file_path": "tests/user_commands/tests.py",
      "line": 459,
      "side": "RIGHT",
      "diff_hunk": "@@ -454,6 +456,28 @@ def test_outputwrapper_flush(self):\n         self.assertIn(\"Working...\", out.getvalue())\n         self.assertIs(mocked_flush.called, True)\n \n+    @unittest.skipUnless(PY314, \"Requires Python 3.14\")",
      "comment": "That means adding `and not PY315` here again as well (and below)",
      "comment_id": 2535162362,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-17T18:45:15Z",
      "url": "https://github.com/django/django/pull/20021#discussion_r2535162362"
    },
    {
      "repo": "django/django",
      "pr_number": 20021,
      "file_path": "django/core/management/base.py",
      "line": 62,
      "side": "RIGHT",
      "diff_hunk": "@@ -57,6 +58,8 @@ def __init__(\n     ):\n         self.missing_args_message = missing_args_message\n         self.called_from_command_line = called_from_command_line\n+        if PY314:\n+            kwargs.setdefault(\"suggest_on_error\", True)",
      "comment": "Hey @jacobtylerwalls, I see the confusion now! I misunderstood your earlier comment about changing to `if PY314:`. I thought you meant it should apply everywhere.\r\n\r\n**In `base.py`:**\r\n```python\r\nif PY314 and not PY315:\r\n    kwargs.setdefault(\"suggest_on_error\", True)\r\n```\r\n\r\nThis way, when Django drops Python 3.14 support, we know to remove this entire block (since 3.15+ has it as default).\r\n\r\n**Regarding `command._called_from_command_line = True`:**\r\n\r\nI had added the `called_from_command_line` check in `base.py` because I was getting an `error message format`  from the `test_invalid_choice_db_option`. After fixing the `admin_scripts` test expectations, those errors were resolved. I'll now remove the `command._called_from_command_line = True` lines from the tests.\r\n\r\nLet me update this now. Thanks for your patience in clarifying!",
      "comment_id": 2544605469,
      "user": "kihuni",
      "created_at": "2025-11-20T07:06:18Z",
      "url": "https://github.com/django/django/pull/20021#discussion_r2544605469"
    },
    {
      "repo": "django/django",
      "pr_number": 18555,
      "file_path": "tests/queries/test_qs_combinators.py",
      "line": 478,
      "side": "RIGHT",
      "diff_hunk": "@@ -450,6 +459,27 @@ def test_union_in_subquery(self):\n             [8, 1],\n         )\n \n+    @skipUnlessDBFeature(\"supports_select_intersection\")\n+    def test_intersection_in_nested_subquery(self):\n+        tag = Tag.objects.create(name=\"tag\")\n+        note = Note.objects.create(tag=tag)\n+        annotation = Annotation.objects.create(tag=tag)\n+        tags = Tag.objects.order_by()\n+        tags = tags.filter(id=OuterRef(\"tag_id\")).intersection(\n+            tags.filter(id=OuterRef(OuterRef(\"tag_id\")))\n+        )\n+        qs = Note.objects.filter(\n+            Exists(\n+                Annotation.objects.filter(\n+                    Exists(tags),\n+                    notes__in=OuterRef(\"pk\"),\n+                )\n+            )\n+        )",
      "comment": "Spent 90% of the time trying to come up with a semi-realistic use case with the currently existing models.\r\n\r\nReviewers might find [this real-life example easier to grok](https://forum.djangoproject.com/t/filtering-model-instances-m2m-field-to-exactly-match-another-m2m-field-or-list/34533/5)",
      "comment_id": 1749434671,
      "user": "charettes",
      "created_at": "2024-09-09T01:21:45Z",
      "url": "https://github.com/django/django/pull/18555#discussion_r1749434671"
    },
    {
      "repo": "django/django",
      "pr_number": 18555,
      "file_path": "tests/queries/test_qs_combinators.py",
      "line": 470,
      "side": "RIGHT",
      "diff_hunk": "@@ -450,6 +459,27 @@ def test_union_in_subquery(self):\n             [8, 1],\n         )\n \n+    @skipUnlessDBFeature(\"supports_select_intersection\")\n+    def test_intersection_in_nested_subquery(self):\n+        tag = Tag.objects.create(name=\"tag\")\n+        note = Note.objects.create(tag=tag)\n+        annotation = Annotation.objects.create(tag=tag)\n+        tags = Tag.objects.order_by()\n+        tags = tags.filter(id=OuterRef(\"tag_id\")).intersection(\n+            tags.filter(id=OuterRef(OuterRef(\"tag_id\")))\n+        )",
      "comment": "Previously the SQL would be\r\n\r\n```sql\r\nSELECT *\r\nFROM \"queries_note\"\r\nWHERE EXISTS\r\n    (SELECT 1 AS \"a\"\r\n     FROM \"queries_annotation\" V0\r\n     INNER JOIN \"queries_annotation_notes\" V2 ON (V0.\"id\" = V2.\"annotation_id\")\r\n     WHERE (EXISTS\r\n              (SELECT U0.\"id\"\r\n               FROM \"queries_tag\" U0\r\n               WHERE U0.\"id\" = (\"queries_annotation\".\"tag_id\") INTERSECT\r\n                 SELECT U0.\"id\"\r\n                 FROM \"queries_tag\" U0 WHERE U0.\"id\" = (\"queries_note\".\"tag_id\")\r\n               LIMIT 1)\r\n            AND V2.\"note_id\" IN (\"queries_note\".\"id\"))\r\n     LIMIT 1)\r\n```\r\n\r\nWhich would crash because of the `U0.\"id\" = (\"queries_annotation\".\"tag_id\")` part.\r\n\r\nThis was due to the `tags.filter(id=OuterRef(\"tag_id\"))` part resolving to `\"queries_annotation\".\"tag_id\"` before `\"queries_annotation\"` was re-labelled to `V0` when the `Exists(Annotation...)` annotation was resolved in the `Note.objects.filter` clause.\r\n\r\nThe changes to `sql.Query.change_aliases` make sure that when this happens the re-label of `{\"queries_annotation\": \"V0\"}` percolate all the way to `tags` in order to replace the `WHERE` clause to `U0.\"id\" = (\"V0\".\"tag_id\")`.",
      "comment_id": 1749438173,
      "user": "charettes",
      "created_at": "2024-09-09T01:29:17Z",
      "url": "https://github.com/django/django/pull/18555#discussion_r1749438173"
    },
    {
      "repo": "django/django",
      "pr_number": 18555,
      "file_path": "django/db/models/sql/query.py",
      "line": 1025,
      "side": "RIGHT",
      "diff_hunk": "@@ -1021,6 +1021,17 @@ def change_aliases(self, change_map):\n                 if alias == old_alias:\n                     table_aliases[pos] = new_alias\n                     break\n+\n+        # 3. Rename the external aliases of combined queries.",
      "comment": "I guess the comment here could be adjusted to mention that we're doing that both for direct and combined queries external aliases.",
      "comment_id": 1750891809,
      "user": "charettes",
      "created_at": "2024-09-09T20:28:28Z",
      "url": "https://github.com/django/django/pull/18555#discussion_r1750891809"
    },
    {
      "repo": "django/django",
      "pr_number": 18555,
      "file_path": "django/db/models/sql/query.py",
      "line": 1033,
      "side": "RIGHT",
      "diff_hunk": "@@ -1021,6 +1021,17 @@ def change_aliases(self, change_map):\n                 if alias == old_alias:\n                     table_aliases[pos] = new_alias\n                     break\n+\n+        # 3. Rename the external aliases of combined queries.\n+        for combined_query in self.combined_queries:\n+            external_change_map = {\n+                alias: aliased\n+                for alias, aliased in change_map.items()\n+                if alias in combined_query.external_aliases\n+            }\n+            if external_change_map:\n+                combined_query.change_aliases(external_change_map)",
      "comment": "`change_aliases()` is already no-op for empty maps, so I would skip this check:\r\n```suggestion\r\n            combined_query.change_aliases(external_change_map)\r\n```",
      "comment_id": 1798298343,
      "user": "felixxm",
      "created_at": "2024-10-13T12:21:01Z",
      "url": "https://github.com/django/django/pull/18555#discussion_r1798298343"
    },
    {
      "repo": "django/django",
      "pr_number": 18555,
      "file_path": "django/db/models/sql/query.py",
      "line": 1033,
      "side": "RIGHT",
      "diff_hunk": "@@ -1021,6 +1021,17 @@ def change_aliases(self, change_map):\n                 if alias == old_alias:\n                     table_aliases[pos] = new_alias\n                     break\n+\n+        # 3. Rename the external aliases of combined queries.\n+        for combined_query in self.combined_queries:\n+            external_change_map = {\n+                alias: aliased\n+                for alias, aliased in change_map.items()\n+                if alias in combined_query.external_aliases\n+            }\n+            if external_change_map:\n+                combined_query.change_aliases(external_change_map)",
      "comment": "Thanks for the review @felixxm as usual \ud83d\ude47 \ud83c\udfc5 I forgot about f3d10546a850df4fe3796f972d5b7e16adf52f54.",
      "comment_id": 1798332490,
      "user": "charettes",
      "created_at": "2024-10-13T13:19:20Z",
      "url": "https://github.com/django/django/pull/18555#discussion_r1798332490"
    },
    {
      "repo": "django/django",
      "pr_number": 18555,
      "file_path": "tests/queries/test_qs_combinators.py",
      "line": 475,
      "side": "RIGHT",
      "diff_hunk": "@@ -450,6 +459,27 @@ def test_union_in_subquery(self):\n             [8, 1],\n         )\n \n+    @skipUnlessDBFeature(\"supports_select_intersection\")\n+    def test_intersection_in_nested_subquery(self):\n+        tag = Tag.objects.create(name=\"tag\")\n+        note = Note.objects.create(tag=tag)\n+        annotation = Annotation.objects.create(tag=tag)\n+        tags = Tag.objects.order_by()\n+        tags = tags.filter(id=OuterRef(\"tag_id\")).intersection(\n+            tags.filter(id=OuterRef(OuterRef(\"tag_id\")))\n+        )\n+        qs = Note.objects.filter(\n+            Exists(\n+                Annotation.objects.filter(\n+                    Exists(tags),\n+                    notes__in=OuterRef(\"pk\"),",
      "comment": "@charettes Is `notes__in=OuterRef(\"pk\")` correct here? I believe `notes=OuterRef(\"pk\"),` works just as well. Emanuel found that we'd need to special case this on MongoDB: https://github.com/mongodb/django-mongodb-backend/pull/474#discussion_r2728665613.",
      "comment_id": 2729349464,
      "user": "timgraham",
      "created_at": "2026-01-26T21:36:29Z",
      "url": "https://github.com/django/django/pull/18555#discussion_r2729349464"
    },
    {
      "repo": "django/django",
      "pr_number": 18555,
      "file_path": "tests/queries/test_qs_combinators.py",
      "line": 475,
      "side": "RIGHT",
      "diff_hunk": "@@ -450,6 +459,27 @@ def test_union_in_subquery(self):\n             [8, 1],\n         )\n \n+    @skipUnlessDBFeature(\"supports_select_intersection\")\n+    def test_intersection_in_nested_subquery(self):\n+        tag = Tag.objects.create(name=\"tag\")\n+        note = Note.objects.create(tag=tag)\n+        annotation = Annotation.objects.create(tag=tag)\n+        tags = Tag.objects.order_by()\n+        tags = tags.filter(id=OuterRef(\"tag_id\")).intersection(\n+            tags.filter(id=OuterRef(OuterRef(\"tag_id\")))\n+        )\n+        qs = Note.objects.filter(\n+            Exists(\n+                Annotation.objects.filter(\n+                    Exists(tags),\n+                    notes__in=OuterRef(\"pk\"),",
      "comment": "@timgraham It seems odd that we'd use `notes__in=OuterRef(\"pk\")` over `notes__in=[OuterRef(\"pk\")]` or `notes=OuterRef(\"pk\")`, its very much a SQL'ism that likely only works because we wrap the right-hand-side with `(...)` and that it's interpreted as a singleton tuple which is allowed with `IN`.\r\n\r\nI've tested that\r\n\r\n```diff\r\ndiff --git a/tests/queries/test_qs_combinators.py b/tests/queries/test_qs_combinators.py\r\nindex e329d0c4f0..6866d07d03 100644\r\n--- a/tests/queries/test_qs_combinators.py\r\n+++ b/tests/queries/test_qs_combinators.py\r\n@@ -515,7 +515,7 @@ class QuerySetSetOperationTests(TestCase):\r\n             Exists(\r\n                 Annotation.objects.filter(\r\n                     Exists(tags),\r\n-                    notes__in=OuterRef(\"pk\"),\r\n+                    notes=OuterRef(\"pk\"),\r\n                 )\r\n             )\r\n         )\r\n```\r\n\r\npasses and remains a valid regression test so we just likely adjust it.",
      "comment_id": 2729774278,
      "user": "charettes",
      "created_at": "2026-01-27T00:24:16Z",
      "url": "https://github.com/django/django/pull/18555#discussion_r2729774278"
    },
    {
      "repo": "django/django",
      "pr_number": 18555,
      "file_path": "tests/queries/test_qs_combinators.py",
      "line": 475,
      "side": "RIGHT",
      "diff_hunk": "@@ -450,6 +459,27 @@ def test_union_in_subquery(self):\n             [8, 1],\n         )\n \n+    @skipUnlessDBFeature(\"supports_select_intersection\")\n+    def test_intersection_in_nested_subquery(self):\n+        tag = Tag.objects.create(name=\"tag\")\n+        note = Note.objects.create(tag=tag)\n+        annotation = Annotation.objects.create(tag=tag)\n+        tags = Tag.objects.order_by()\n+        tags = tags.filter(id=OuterRef(\"tag_id\")).intersection(\n+            tags.filter(id=OuterRef(OuterRef(\"tag_id\")))\n+        )\n+        qs = Note.objects.filter(\n+            Exists(\n+                Annotation.objects.filter(\n+                    Exists(tags),\n+                    notes__in=OuterRef(\"pk\"),",
      "comment": "Thanks for confirming: https://github.com/django/django/pull/20592.",
      "comment_id": 2729916772,
      "user": "timgraham",
      "created_at": "2026-01-27T01:45:48Z",
      "url": "https://github.com/django/django/pull/18555#discussion_r2729916772"
    },
    {
      "repo": "django/django",
      "pr_number": 20528,
      "file_path": "tests/admin_views/tests.py",
      "line": 7099,
      "side": "RIGHT",
      "diff_hunk": "@@ -7085,6 +7085,25 @@ def test_pagination_layout(self):\n         self.assertTrue(show_all.is_displayed())\n         self.take_screenshot(\"pagination\")\n \n+    @screenshot_cases([\"desktop_size\", \"mobile_size\", \"rtl\", \"dark\", \"high_contrast\"])\n+    def test_changelist_filter_sidebar_with_long_verbose_fields(self):\n+        from selenium.webdriver.common.by import By\n+\n+        self.admin_login(\n+            username=\"super\", password=\"secret\", login_url=reverse(\"admin:index\")\n+        )\n+        Person.objects.create(\n+            name=\"very very very very very very very very very \"\n+            \"loooooooooooooooooooooooooooooooooooooooooong name\",\n+            gender=2,\n+        )",
      "comment": "I seem to be failing to reproduce the issue \ud83e\udd72\r\nThe filter sidebar size should increase.",
      "comment_id": 2723442809,
      "user": "Antoliny0919",
      "created_at": "2026-01-24T02:22:53Z",
      "url": "https://github.com/django/django/pull/20528#discussion_r2723442809"
    },
    {
      "repo": "django/django",
      "pr_number": 19840,
      "file_path": "tests/cache/tests.py",
      "line": 1195,
      "side": "RIGHT",
      "diff_hunk": "@@ -1165,6 +1165,14 @@ def test_get_or_set_racing(self):\n             cache_add.return_value = False\n             self.assertEqual(cache.get_or_set(\"key\", \"default\"), \"default\")\n \n+    async def test_async_impl(self):\n+        if hasattr(cache, \"get_many\"):\n+            self.assertTrue(hasattr(cache, \"aget_many\"))\n+        if hasattr(cache, \"set_many\"):\n+            self.assertTrue(hasattr(cache, \"aset_many\"))\n+        if hasattr(cache, \"delete_many\"):\n+            self.assertTrue(hasattr(cache, \"adelete_many\"))\n+",
      "comment": "These tests will pass even without your changes applied as these methods are always present they were just the wrong implementations. You could assert against their nature instead and the test doesn't need to be `async`\r\n\r\n```suggestion\r\n    def test_async_impl(self):\r\n        if cache.get_many is not BaseCache.get_many:\r\n            self.assertIs(cache.aget_many, aget_many)\r\n        ...\r\n```",
      "comment_id": 2337540430,
      "user": "charettes",
      "created_at": "2025-09-10T18:09:21Z",
      "url": "https://github.com/django/django/pull/19840#discussion_r2337540430"
    },
    {
      "repo": "django/django",
      "pr_number": 19840,
      "file_path": "django/core/cache/backends/db.py",
      "line": 59,
      "side": "RIGHT",
      "diff_hunk": "@@ -39,6 +53,18 @@ class CacheEntry:\n \n         self.cache_model_class = CacheEntry\n \n+    def __init_subclass__(cls, **kwargs):\n+        # If a subclass implements a specialized *_many method,\n+        # it should use that method for the async implementation,\n+        # vs the version inherited from BaseCache.",
      "comment": "This should be defined on `BaseCache` and not `BaseDatabaseCache`. As pointed out in the ticket the memcached and redis backend are also affected.",
      "comment_id": 2337545639,
      "user": "charettes",
      "created_at": "2025-09-10T18:11:43Z",
      "url": "https://github.com/django/django/pull/19840#discussion_r2337545639"
    },
    {
      "repo": "django/django",
      "pr_number": 19840,
      "file_path": "django/core/cache/backends/db.py",
      "line": 62,
      "side": "RIGHT",
      "diff_hunk": "@@ -39,6 +53,18 @@ class CacheEntry:\n \n         self.cache_model_class = CacheEntry\n \n+    def __init_subclass__(cls, **kwargs):\n+        # If a subclass implements a specialized *_many method,\n+        # it should use that method for the async implementation,\n+        # vs the version inherited from BaseCache.\n+        super().__init_subclass__(**kwargs)\n+        if hasattr(cls, \"get_many\"):\n+            setattr(cls, \"aget_many\", aget_impl)",
      "comment": "This should not add anything if the subclass has its own specialized `aget_many`",
      "comment_id": 2337547753,
      "user": "charettes",
      "created_at": "2025-09-10T18:12:45Z",
      "url": "https://github.com/django/django/pull/19840#discussion_r2337547753"
    },
    {
      "repo": "django/django",
      "pr_number": 19840,
      "file_path": "django/core/cache/backends/db.py",
      "line": 59,
      "side": "RIGHT",
      "diff_hunk": "@@ -39,6 +53,18 @@ class CacheEntry:\n \n         self.cache_model_class = CacheEntry\n \n+    def __init_subclass__(cls, **kwargs):\n+        # If a subclass implements a specialized *_many method,\n+        # it should use that method for the async implementation,\n+        # vs the version inherited from BaseCache.",
      "comment": "Based on ticket comments, I had thought the goal was to only apply this change to subclasses of BaseDatabaseCache.  Moving these to BaseCache is ultimately cleaner. ",
      "comment_id": 2341904537,
      "user": "eevelweezel",
      "created_at": "2025-09-11T17:43:41Z",
      "url": "https://github.com/django/django/pull/19840#discussion_r2341904537"
    },
    {
      "repo": "django/django",
      "pr_number": 19840,
      "file_path": "django/core/cache/backends/db.py",
      "line": 62,
      "side": "RIGHT",
      "diff_hunk": "@@ -39,6 +53,18 @@ class CacheEntry:\n \n         self.cache_model_class = CacheEntry\n \n+    def __init_subclass__(cls, **kwargs):\n+        # If a subclass implements a specialized *_many method,\n+        # it should use that method for the async implementation,\n+        # vs the version inherited from BaseCache.\n+        super().__init_subclass__(**kwargs)\n+        if hasattr(cls, \"get_many\"):\n+            setattr(cls, \"aget_many\", aget_impl)",
      "comment": "Adding these directly to BaseCache vs trying to apply them conditionally via __init_subclass__ is ultimately cleaner. ",
      "comment_id": 2341910418,
      "user": "eevelweezel",
      "created_at": "2025-09-11T17:45:30Z",
      "url": "https://github.com/django/django/pull/19840#discussion_r2341910418"
    },
    {
      "repo": "django/django",
      "pr_number": 19840,
      "file_path": "tests/cache/tests.py",
      "line": 1195,
      "side": "RIGHT",
      "diff_hunk": "@@ -1165,6 +1165,14 @@ def test_get_or_set_racing(self):\n             cache_add.return_value = False\n             self.assertEqual(cache.get_or_set(\"key\", \"default\"), \"default\")\n \n+    async def test_async_impl(self):\n+        if hasattr(cache, \"get_many\"):\n+            self.assertTrue(hasattr(cache, \"aget_many\"))\n+        if hasattr(cache, \"set_many\"):\n+            self.assertTrue(hasattr(cache, \"aset_many\"))\n+        if hasattr(cache, \"delete_many\"):\n+            self.assertTrue(hasattr(cache, \"adelete_many\"))\n+",
      "comment": "The object ID ultimately doesn't match, so an is comparison fails.  I've settled on comparing pickles of the methods and asserting that calling the async method calls the sync version.   ",
      "comment_id": 2342346623,
      "user": "eevelweezel",
      "created_at": "2025-09-11T21:08:45Z",
      "url": "https://github.com/django/django/pull/19840#discussion_r2342346623"
    },
    {
      "repo": "django/django",
      "pr_number": 19840,
      "file_path": "tests/cache/tests.py",
      "line": 1168,
      "side": "RIGHT",
      "diff_hunk": "@@ -1165,6 +1165,16 @@ def test_get_or_set_racing(self):\n             cache_add.return_value = False\n             self.assertEqual(cache.get_or_set(\"key\", \"default\"), \"default\")\n \n+    def test_async_impl(self):",
      "comment": "I would suggest a more verbose test method name. You could do away with the comment in that case.",
      "comment_id": 2500125796,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-06T17:47:14Z",
      "url": "https://github.com/django/django/pull/19840#discussion_r2500125796"
    },
    {
      "repo": "django/django",
      "pr_number": 19840,
      "file_path": "tests/cache/tests.py",
      "line": 1172,
      "side": "RIGHT",
      "diff_hunk": "@@ -1165,6 +1165,16 @@ def test_get_or_set_racing(self):\n             cache_add.return_value = False\n             self.assertEqual(cache.get_or_set(\"key\", \"default\"), \"default\")\n \n+    def test_async_impl(self):\n+        # Assert that the inherited method matches the BaseCache\n+        # implementation.\n+        methods = [\"aget_many\", \"aset_many\", \"adelete_many\"]\n+        if isinstance(cache, BaseCache):",
      "comment": "This is always False, so the assertions aren't running.",
      "comment_id": 2500132376,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-06T17:48:56Z",
      "url": "https://github.com/django/django/pull/19840#discussion_r2500132376"
    },
    {
      "repo": "django/django",
      "pr_number": 18934,
      "file_path": "tests/admin_views/tests.py",
      "line": 6370,
      "side": "RIGHT",
      "diff_hunk": "@@ -6366,6 +6367,7 @@ def test_list_editable_popups(self):\n         self.wait_for_text(\"#content h1\", \"Add section\")\n         self.selenium.find_element(By.ID, \"id_name\").send_keys(\"new section\")\n         self.selenium.find_element(By.XPATH, '//input[@value=\"Save\"]').click()\n+",
      "comment": "```suggestion\r\n```\r\n(revert any unrelated changes)",
      "comment_id": 2048692595,
      "user": "sarahboyce",
      "created_at": "2025-04-17T10:40:50Z",
      "url": "https://github.com/django/django/pull/18934#discussion_r2048692595"
    },
    {
      "repo": "django/django",
      "pr_number": 18934,
      "file_path": "django/contrib/admin/options.py",
      "line": 1412,
      "side": "RIGHT",
      "diff_hunk": "@@ -1398,12 +1401,39 @@ def response_add(self, request, obj, post_url_continue=None):\n             else:\n                 attr = obj._meta.pk.attname\n             value = obj.serializable_value(attr)\n-            popup_response_data = json.dumps(\n-                {\n-                    \"value\": str(value),\n-                    \"obj\": str(obj),\n-                }\n-            )\n+            popup_response = {\n+                \"value\": str(value),\n+                \"obj\": str(obj),\n+            }\n+\n+            # Find the optgroup for the new item, if available\n+            source_model_name = request.POST.get(SOURCE_MODEL_VAR)\n+\n+            if source_model_name:",
      "comment": "I can remove this whole `if` block without any test failures.",
      "comment_id": 2524979779,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-13T21:24:19Z",
      "url": "https://github.com/django/django/pull/18934#discussion_r2524979779"
    },
    {
      "repo": "django/django",
      "pr_number": 18934,
      "file_path": "django/contrib/admin/options.py",
      "line": 1422,
      "side": "RIGHT",
      "diff_hunk": "@@ -1398,12 +1401,39 @@ def response_add(self, request, obj, post_url_continue=None):\n             else:\n                 attr = obj._meta.pk.attname\n             value = obj.serializable_value(attr)\n-            popup_response_data = json.dumps(\n-                {\n-                    \"value\": str(value),\n-                    \"obj\": str(obj),\n-                }\n-            )\n+            popup_response = {\n+                \"value\": str(value),\n+                \"obj\": str(obj),\n+            }\n+\n+            # Find the optgroup for the new item, if available\n+            source_model_name = request.POST.get(SOURCE_MODEL_VAR)\n+\n+            if source_model_name:\n+                app_label, model_name = source_model_name.split(\".\", 1)\n+                try:\n+                    source_model = apps.get_model(app_label, model_name)\n+                except LookupError:\n+                    msg = _('The app \"%s\" could not be found.') % source_model_name\n+                    self.message_user(request, msg, messages.ERROR)\n+                else:\n+                    form_class = self.admin_site._registry[source_model].form\n+                    if (\n+                        hasattr(form_class, \"_meta\")",
      "comment": "For the `ModelForm` I was working with, it was a bare `ModelForm`, so it didn't have `_meta`, but rather the admin had `opts`. I don't think we can depend on having a `_meta`, although I'm not so familiar with this part of the framework.\r\n\r\n```py\r\n(Pdb) self.admin_site._registry[source_model].form\r\n<class 'django.forms.models.ModelForm'>\r\n(Pdb) self.admin_site._registry[source_model].form._meta\r\n*** AttributeError: type object 'ModelForm' has no attribute '_meta'\r\n(Pdb) self.admin_site._registry[source_model].opts.fields\r\n(<django.db.models.fields.AutoField: id>, <django.db.models.fields.CharField: name>)\r\n```\r\n\r\nFor reference, I was applying Sarah's earlier example with the `Group` model like this, without declaring a form:\r\n```py\r\n@admin.register(Group)\r\nclass GroupAdmin(admin.ModelAdmin):\r\n    search_fields = (\"name\",)\r\n    ordering = (\"name\",)\r\n    filter_horizontal = (\"permissions\",)\r\n\r\n    def formfield_for_manytomany(self, db_field, request=None, **kwargs):\r\n        if db_field.name == \"permissions\":\r\n            qs = kwargs.get(\"queryset\", db_field.remote_field.model.objects)\r\n            # Avoid a major performance hit resolving permission names which\r\n            # triggers a content_type load:\r\n            kwargs[\"queryset\"] = qs.select_related(\"content_type\")\r\n            return GroupedModelChoiceField(\r\n                queryset=kwargs[\"queryset\"],\r\n                choices_groupby='content_type',\r\n                widget=FilteredSelectMultiple(verbose_name=\"User permissions\", is_stacked=False),\r\n            )\r\n        return super().formfield_for_manytomany(db_field, request=request, **kwargs)\r\n```\r\n\r\n",
      "comment_id": 2673740455,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-08T20:13:33Z",
      "url": "https://github.com/django/django/pull/18934#discussion_r2673740455"
    },
    {
      "repo": "django/django",
      "pr_number": 18934,
      "file_path": "tests/admin_widgets/tests.py",
      "line": 1365,
      "side": "RIGHT",
      "diff_hunk": "@@ -1354,14 +1355,14 @@ def execute_basic_operations(self, mode, field_name):\n         self.assertSelectOptions(\n             to_box,\n             [\n-                str(self.lisa.id),\n-                str(self.peter.id),\n                 str(self.arthur.id),\n                 str(self.bob.id),\n                 str(self.cliff.id),\n                 str(self.jason.id),\n                 str(self.jenny.id),\n                 str(self.john.id),\n+                str(self.lisa.id),\n+                str(self.peter.id),",
      "comment": "Can you tell me a little bit about why these are moving?",
      "comment_id": 2673834553,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-08T20:42:59Z",
      "url": "https://github.com/django/django/pull/18934#discussion_r2673834553"
    },
    {
      "repo": "django/django",
      "pr_number": 18934,
      "file_path": "tests/admin_widgets/tests.py",
      "line": 1365,
      "side": "RIGHT",
      "diff_hunk": "@@ -1354,14 +1355,14 @@ def execute_basic_operations(self, mode, field_name):\n         self.assertSelectOptions(\n             to_box,\n             [\n-                str(self.lisa.id),\n-                str(self.peter.id),\n                 str(self.arthur.id),\n                 str(self.bob.id),\n                 str(self.cliff.id),\n                 str(self.jason.id),\n                 str(self.jenny.id),\n                 str(self.john.id),\n+                str(self.lisa.id),\n+                str(self.peter.id),",
      "comment": "I believe SelectBox sorts items by optgroup first, then alphabetically within each group. These test updates reflect that sorted order.",
      "comment_id": 2701363408,
      "user": "seanhelvey",
      "created_at": "2026-01-17T19:14:55Z",
      "url": "https://github.com/django/django/pull/18934#discussion_r2701363408"
    },
    {
      "repo": "django/django",
      "pr_number": 18934,
      "file_path": "django/contrib/admin/options.py",
      "line": 1422,
      "side": "RIGHT",
      "diff_hunk": "@@ -1398,12 +1401,39 @@ def response_add(self, request, obj, post_url_continue=None):\n             else:\n                 attr = obj._meta.pk.attname\n             value = obj.serializable_value(attr)\n-            popup_response_data = json.dumps(\n-                {\n-                    \"value\": str(value),\n-                    \"obj\": str(obj),\n-                }\n-            )\n+            popup_response = {\n+                \"value\": str(value),\n+                \"obj\": str(obj),\n+            }\n+\n+            # Find the optgroup for the new item, if available\n+            source_model_name = request.POST.get(SOURCE_MODEL_VAR)\n+\n+            if source_model_name:\n+                app_label, model_name = source_model_name.split(\".\", 1)\n+                try:\n+                    source_model = apps.get_model(app_label, model_name)\n+                except LookupError:\n+                    msg = _('The app \"%s\" could not be found.') % source_model_name\n+                    self.message_user(request, msg, messages.ERROR)\n+                else:\n+                    form_class = self.admin_site._registry[source_model].form\n+                    if (\n+                        hasattr(form_class, \"_meta\")",
      "comment": "Awesome. So further up in response_add we're operating on a model instance, and _meta is guaranteed. Here though the admin's form_class doesn't reliably expose _meta. Reworked to check form.fields directly instead. Also added isinstance() to detect optgroups vs flat choices (the original assumed all choices were optgroups). Added a test that should cover the bare ModelAdmin case.",
      "comment_id": 2701366559,
      "user": "seanhelvey",
      "created_at": "2026-01-17T19:21:00Z",
      "url": "https://github.com/django/django/pull/18934#discussion_r2701366559"
    },
    {
      "repo": "django/django",
      "pr_number": 18934,
      "file_path": "django/contrib/admin/options.py",
      "line": 1422,
      "side": "RIGHT",
      "diff_hunk": "@@ -1398,12 +1401,39 @@ def response_add(self, request, obj, post_url_continue=None):\n             else:\n                 attr = obj._meta.pk.attname\n             value = obj.serializable_value(attr)\n-            popup_response_data = json.dumps(\n-                {\n-                    \"value\": str(value),\n-                    \"obj\": str(obj),\n-                }\n-            )\n+            popup_response = {\n+                \"value\": str(value),\n+                \"obj\": str(obj),\n+            }\n+\n+            # Find the optgroup for the new item, if available\n+            source_model_name = request.POST.get(SOURCE_MODEL_VAR)\n+\n+            if source_model_name:\n+                app_label, model_name = source_model_name.split(\".\", 1)\n+                try:\n+                    source_model = apps.get_model(app_label, model_name)\n+                except LookupError:\n+                    msg = _('The app \"%s\" could not be found.') % source_model_name\n+                    self.message_user(request, msg, messages.ERROR)\n+                else:\n+                    form_class = self.admin_site._registry[source_model].form\n+                    if (\n+                        hasattr(form_class, \"_meta\")",
      "comment": "This still didn't work for me. I found success with `get_form(request)()`:\r\n\r\n```py\r\n                    form = self.admin_site._registry[source_model].get_form(request)()\r\n                    field = form.fields[self.opts.verbose_name_plural]\r\n                    if hasattr(field, \"choices\"):\r\n...\r\n\r\n```",
      "comment_id": 2705304857,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-19T15:53:41Z",
      "url": "https://github.com/django/django/pull/18934#discussion_r2705304857"
    },
    {
      "repo": "django/django",
      "pr_number": 18934,
      "file_path": "django/contrib/admin/options.py",
      "line": 1436,
      "side": "RIGHT",
      "diff_hunk": "@@ -1398,12 +1401,43 @@ def response_add(self, request, obj, post_url_continue=None):\n             else:\n                 attr = obj._meta.pk.attname\n             value = obj.serializable_value(attr)\n-            popup_response_data = json.dumps(\n-                {\n-                    \"value\": str(value),\n-                    \"obj\": str(obj),\n-                }\n-            )\n+            popup_response = {\n+                \"value\": str(value),\n+                \"obj\": str(obj),\n+            }\n+\n+            # Find the optgroup for the new item, if available\n+            source_model_name = request.POST.get(SOURCE_MODEL_VAR)\n+\n+            if source_model_name:\n+                app_label, model_name = source_model_name.split(\".\", 1)\n+                try:\n+                    source_model = apps.get_model(app_label, model_name)\n+                except LookupError:\n+                    msg = _('The app \"%s\" could not be found.') % source_model_name\n+                    self.message_user(request, msg, messages.ERROR)\n+                else:\n+                    form_class = self.admin_site._registry[source_model].form\n+                    try:\n+                        form = form_class()\n+                    except (AttributeError, TypeError):\n+                        form = None\n+                    if form and self.opts.verbose_name_plural in form.fields:\n+                        field = form.fields[self.opts.verbose_name_plural]\n+                        if hasattr(field, \"choices\"):\n+                            for option_value, option_label in field.choices:\n+                                # Check if this is an optgroup\n+                                # (label is a list/tuple of choices)\n+                                if isinstance(option_label, (list, tuple)):\n+                                    # It's an optgroup:\n+                                    # (group_name, [(value, label), ...])\n+                                    optgroup_label = option_value\n+                                    for choice_value, choice_display in option_label:\n+                                        if choice_display == str(obj):\n+                                            popup_response[\"optgroup\"] = optgroup_label\n+                                            break\n+\n+            popup_response_data = json.dumps(popup_response)",
      "comment": "With the GroupAdmin I was testing with [earlier](https://github.com/django/django/pull/18934#discussion_r2673740455), and with the fix from [comment](https://github.com/django/django/pull/18934#discussion_r2705304857), I was able to reach this line with a matched `popup_response[\"optgroup\"]`, but then `dumps()` failed with:\r\n\r\n```py\r\n(Pdb) option_value\r\n<ContentType: Myapp | song>\r\n\r\nTypeError: Object of type ContentType is not JSON serializable\r\n```",
      "comment_id": 2705316314,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-19T15:57:01Z",
      "url": "https://github.com/django/django/pull/18934#discussion_r2705316314"
    },
    {
      "repo": "django/django",
      "pr_number": 18934,
      "file_path": "django/contrib/admin/options.py",
      "line": 1422,
      "side": "RIGHT",
      "diff_hunk": "@@ -1398,12 +1401,39 @@ def response_add(self, request, obj, post_url_continue=None):\n             else:\n                 attr = obj._meta.pk.attname\n             value = obj.serializable_value(attr)\n-            popup_response_data = json.dumps(\n-                {\n-                    \"value\": str(value),\n-                    \"obj\": str(obj),\n-                }\n-            )\n+            popup_response = {\n+                \"value\": str(value),\n+                \"obj\": str(obj),\n+            }\n+\n+            # Find the optgroup for the new item, if available\n+            source_model_name = request.POST.get(SOURCE_MODEL_VAR)\n+\n+            if source_model_name:\n+                app_label, model_name = source_model_name.split(\".\", 1)\n+                try:\n+                    source_model = apps.get_model(app_label, model_name)\n+                except LookupError:\n+                    msg = _('The app \"%s\" could not be found.') % source_model_name\n+                    self.message_user(request, msg, messages.ERROR)\n+                else:\n+                    form_class = self.admin_site._registry[source_model].form\n+                    if (\n+                        hasattr(form_class, \"_meta\")",
      "comment": "Ah ok thank you. I applied the patch in the example / demo project here for easy reference it seems to be working for me now but let me know if I am still missing something. https://github.com/seanhelvey/ticket_13883",
      "comment_id": 2705397046,
      "user": "seanhelvey",
      "created_at": "2026-01-19T16:22:19Z",
      "url": "https://github.com/django/django/pull/18934#discussion_r2705397046"
    },
    {
      "repo": "django/django",
      "pr_number": 18934,
      "file_path": "django/contrib/admin/options.py",
      "line": 1436,
      "side": "RIGHT",
      "diff_hunk": "@@ -1398,12 +1401,43 @@ def response_add(self, request, obj, post_url_continue=None):\n             else:\n                 attr = obj._meta.pk.attname\n             value = obj.serializable_value(attr)\n-            popup_response_data = json.dumps(\n-                {\n-                    \"value\": str(value),\n-                    \"obj\": str(obj),\n-                }\n-            )\n+            popup_response = {\n+                \"value\": str(value),\n+                \"obj\": str(obj),\n+            }\n+\n+            # Find the optgroup for the new item, if available\n+            source_model_name = request.POST.get(SOURCE_MODEL_VAR)\n+\n+            if source_model_name:\n+                app_label, model_name = source_model_name.split(\".\", 1)\n+                try:\n+                    source_model = apps.get_model(app_label, model_name)\n+                except LookupError:\n+                    msg = _('The app \"%s\" could not be found.') % source_model_name\n+                    self.message_user(request, msg, messages.ERROR)\n+                else:\n+                    form_class = self.admin_site._registry[source_model].form\n+                    try:\n+                        form = form_class()\n+                    except (AttributeError, TypeError):\n+                        form = None\n+                    if form and self.opts.verbose_name_plural in form.fields:\n+                        field = form.fields[self.opts.verbose_name_plural]\n+                        if hasattr(field, \"choices\"):\n+                            for option_value, option_label in field.choices:\n+                                # Check if this is an optgroup\n+                                # (label is a list/tuple of choices)\n+                                if isinstance(option_label, (list, tuple)):\n+                                    # It's an optgroup:\n+                                    # (group_name, [(value, label), ...])\n+                                    optgroup_label = option_value\n+                                    for choice_value, choice_display in option_label:\n+                                        if choice_display == str(obj):\n+                                            popup_response[\"optgroup\"] = optgroup_label\n+                                            break\n+\n+            popup_response_data = json.dumps(popup_response)",
      "comment": "Pushed a commit to add the fix from the linked comment above and the failure here before seeing the suggestion to avoid using dumps entirely. Hopefully this will get it into a working state and then I am open to any changes :)",
      "comment_id": 2705399122,
      "user": "seanhelvey",
      "created_at": "2026-01-19T16:23:01Z",
      "url": "https://github.com/django/django/pull/18934#discussion_r2705399122"
    },
    {
      "repo": "django/django",
      "pr_number": 18934,
      "file_path": "django/contrib/admin/options.py",
      "line": 1424,
      "side": "RIGHT",
      "diff_hunk": "@@ -1398,12 +1401,45 @@ def response_add(self, request, obj, post_url_continue=None):\n             else:\n                 attr = obj._meta.pk.attname\n             value = obj.serializable_value(attr)\n-            popup_response_data = json.dumps(\n-                {\n-                    \"value\": str(value),\n-                    \"obj\": str(obj),\n-                }\n-            )\n+            popup_response = {\n+                \"value\": str(value),\n+                \"obj\": str(obj),\n+            }\n+\n+            # Find the optgroup for the new item, if available\n+            source_model_name = request.POST.get(SOURCE_MODEL_VAR)\n+\n+            if source_model_name:\n+                app_label, model_name = source_model_name.split(\".\", 1)\n+                try:\n+                    source_model = apps.get_model(app_label, model_name)\n+                except LookupError:\n+                    msg = _('The app \"%s\" could not be found.') % source_model_name\n+                    self.message_user(request, msg, messages.ERROR)\n+                else:\n+                    source_admin = self.admin_site._registry[source_model]\n+                    try:\n+                        form = source_admin.get_form(request)()\n+                    except (AttributeError, TypeError):\n+                        form = None",
      "comment": "This is uncovered, I think we can chop it and avoid testing `if form` below.",
      "comment_id": 2705475114,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-19T16:39:51Z",
      "url": "https://github.com/django/django/pull/18934#discussion_r2705475114"
    },
    {
      "repo": "django/django",
      "pr_number": 18934,
      "file_path": "django/contrib/admin/options.py",
      "line": 1436,
      "side": "RIGHT",
      "diff_hunk": "@@ -1398,12 +1401,43 @@ def response_add(self, request, obj, post_url_continue=None):\n             else:\n                 attr = obj._meta.pk.attname\n             value = obj.serializable_value(attr)\n-            popup_response_data = json.dumps(\n-                {\n-                    \"value\": str(value),\n-                    \"obj\": str(obj),\n-                }\n-            )\n+            popup_response = {\n+                \"value\": str(value),\n+                \"obj\": str(obj),\n+            }\n+\n+            # Find the optgroup for the new item, if available\n+            source_model_name = request.POST.get(SOURCE_MODEL_VAR)\n+\n+            if source_model_name:\n+                app_label, model_name = source_model_name.split(\".\", 1)\n+                try:\n+                    source_model = apps.get_model(app_label, model_name)\n+                except LookupError:\n+                    msg = _('The app \"%s\" could not be found.') % source_model_name\n+                    self.message_user(request, msg, messages.ERROR)\n+                else:\n+                    form_class = self.admin_site._registry[source_model].form\n+                    try:\n+                        form = form_class()\n+                    except (AttributeError, TypeError):\n+                        form = None\n+                    if form and self.opts.verbose_name_plural in form.fields:\n+                        field = form.fields[self.opts.verbose_name_plural]\n+                        if hasattr(field, \"choices\"):\n+                            for option_value, option_label in field.choices:\n+                                # Check if this is an optgroup\n+                                # (label is a list/tuple of choices)\n+                                if isinstance(option_label, (list, tuple)):\n+                                    # It's an optgroup:\n+                                    # (group_name, [(value, label), ...])\n+                                    optgroup_label = option_value\n+                                    for choice_value, choice_display in option_label:\n+                                        if choice_display == str(obj):\n+                                            popup_response[\"optgroup\"] = optgroup_label\n+                                            break\n+\n+            popup_response_data = json.dumps(popup_response)",
      "comment": "It works! \ud83c\udf89\n\nMy bad, I didn't notice we were using `dumps()` before (never mind!)\n\nA test case to cover this bare `ModelForm` case would be grand.",
      "comment_id": 2705505873,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-19T16:45:30Z",
      "url": "https://github.com/django/django/pull/18934#discussion_r2705505873"
    },
    {
      "repo": "django/django",
      "pr_number": 18934,
      "file_path": "django/contrib/admin/options.py",
      "line": 1424,
      "side": "RIGHT",
      "diff_hunk": "@@ -1398,12 +1401,45 @@ def response_add(self, request, obj, post_url_continue=None):\n             else:\n                 attr = obj._meta.pk.attname\n             value = obj.serializable_value(attr)\n-            popup_response_data = json.dumps(\n-                {\n-                    \"value\": str(value),\n-                    \"obj\": str(obj),\n-                }\n-            )\n+            popup_response = {\n+                \"value\": str(value),\n+                \"obj\": str(obj),\n+            }\n+\n+            # Find the optgroup for the new item, if available\n+            source_model_name = request.POST.get(SOURCE_MODEL_VAR)\n+\n+            if source_model_name:\n+                app_label, model_name = source_model_name.split(\".\", 1)\n+                try:\n+                    source_model = apps.get_model(app_label, model_name)\n+                except LookupError:\n+                    msg = _('The app \"%s\" could not be found.') % source_model_name\n+                    self.message_user(request, msg, messages.ERROR)\n+                else:\n+                    source_admin = self.admin_site._registry[source_model]\n+                    try:\n+                        form = source_admin.get_form(request)()\n+                    except (AttributeError, TypeError):\n+                        form = None",
      "comment": "Thanks! Removed the try/except. Regarding your bare ModelForm test comment below - test_popup_add_POST_without_optgroups should cover the bare ModelForm case (uses default admin without custom optgroups form). Did you have a specific scenario in mind that's not covered?",
      "comment_id": 2705536514,
      "user": "seanhelvey",
      "created_at": "2026-01-19T16:55:23Z",
      "url": "https://github.com/django/django/pull/18934#discussion_r2705536514"
    },
    {
      "repo": "django/django",
      "pr_number": 18934,
      "file_path": "django/contrib/admin/options.py",
      "line": 1424,
      "side": "RIGHT",
      "diff_hunk": "@@ -1398,12 +1401,45 @@ def response_add(self, request, obj, post_url_continue=None):\n             else:\n                 attr = obj._meta.pk.attname\n             value = obj.serializable_value(attr)\n-            popup_response_data = json.dumps(\n-                {\n-                    \"value\": str(value),\n-                    \"obj\": str(obj),\n-                }\n-            )\n+            popup_response = {\n+                \"value\": str(value),\n+                \"obj\": str(obj),\n+            }\n+\n+            # Find the optgroup for the new item, if available\n+            source_model_name = request.POST.get(SOURCE_MODEL_VAR)\n+\n+            if source_model_name:\n+                app_label, model_name = source_model_name.split(\".\", 1)\n+                try:\n+                    source_model = apps.get_model(app_label, model_name)\n+                except LookupError:\n+                    msg = _('The app \"%s\" could not be found.') % source_model_name\n+                    self.message_user(request, msg, messages.ERROR)\n+                else:\n+                    source_admin = self.admin_site._registry[source_model]\n+                    try:\n+                        form = source_admin.get_form(request)()\n+                    except (AttributeError, TypeError):\n+                        form = None",
      "comment": "Yeah, I was imagining a test case that did have custom optgroups, along the lines of my example that overrode `formfield_for_manytomany`: https://github.com/django/django/pull/18934#discussion_r2673740455.",
      "comment_id": 2705626533,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-19T17:30:47Z",
      "url": "https://github.com/django/django/pull/18934#discussion_r2705626533"
    },
    {
      "repo": "django/django",
      "pr_number": 18934,
      "file_path": "django/contrib/admin/options.py",
      "line": 1427,
      "side": "RIGHT",
      "diff_hunk": "@@ -1398,12 +1401,42 @@ def response_add(self, request, obj, post_url_continue=None):\n             else:\n                 attr = obj._meta.pk.attname\n             value = obj.serializable_value(attr)\n-            popup_response_data = json.dumps(\n-                {\n-                    \"value\": str(value),\n-                    \"obj\": str(obj),\n-                }\n-            )\n+            popup_response = {\n+                \"value\": str(value),\n+                \"obj\": str(obj),\n+            }\n+\n+            # Find the optgroup for the new item, if available\n+            source_model_name = request.POST.get(SOURCE_MODEL_VAR)\n+\n+            if source_model_name:\n+                app_label, model_name = source_model_name.split(\".\", 1)\n+                try:\n+                    source_model = apps.get_model(app_label, model_name)\n+                except LookupError:\n+                    msg = _('The app \"%s\" could not be found.') % source_model_name\n+                    self.message_user(request, msg, messages.ERROR)\n+                else:\n+                    source_admin = self.admin_site._registry[source_model]\n+                    form = source_admin.get_form(request)()\n+                    if self.opts.verbose_name_plural in form.fields:\n+                        field = form.fields[self.opts.verbose_name_plural]\n+                        if hasattr(field, \"choices\"):\n+                            for option_value, option_label in field.choices:\n+                                # Check if this is an optgroup\n+                                # (label is a list/tuple of choices)",
      "comment": "Do we have this usage documented anywhere nearby `ModelChoiceIterator` or `ChoiceField.choices`? Poking around, all I see is that `choices` yields 2-tuples.\n\nI'm feeling like we might want a release note & treat this as a new feature.\n\nSarah's example from the ticket looks like it was riffing on Simon's comment here: https://code.djangoproject.com/ticket/27331#comment:7",
      "comment_id": 2706159390,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-19T21:43:42Z",
      "url": "https://github.com/django/django/pull/18934#discussion_r2706159390"
    },
    {
      "repo": "django/django",
      "pr_number": 18934,
      "file_path": "django/contrib/admin/options.py",
      "line": 1424,
      "side": "RIGHT",
      "diff_hunk": "@@ -1398,12 +1401,45 @@ def response_add(self, request, obj, post_url_continue=None):\n             else:\n                 attr = obj._meta.pk.attname\n             value = obj.serializable_value(attr)\n-            popup_response_data = json.dumps(\n-                {\n-                    \"value\": str(value),\n-                    \"obj\": str(obj),\n-                }\n-            )\n+            popup_response = {\n+                \"value\": str(value),\n+                \"obj\": str(obj),\n+            }\n+\n+            # Find the optgroup for the new item, if available\n+            source_model_name = request.POST.get(SOURCE_MODEL_VAR)\n+\n+            if source_model_name:\n+                app_label, model_name = source_model_name.split(\".\", 1)\n+                try:\n+                    source_model = apps.get_model(app_label, model_name)\n+                except LookupError:\n+                    msg = _('The app \"%s\" could not be found.') % source_model_name\n+                    self.message_user(request, msg, messages.ERROR)\n+                else:\n+                    source_admin = self.admin_site._registry[source_model]\n+                    try:\n+                        form = source_admin.get_form(request)()\n+                    except (AttributeError, TypeError):\n+                        form = None",
      "comment": "I see. Nope the opposite of hazy! Ok that test has been added running through CI now.",
      "comment_id": 2706173846,
      "user": "seanhelvey",
      "created_at": "2026-01-19T21:52:30Z",
      "url": "https://github.com/django/django/pull/18934#discussion_r2706173846"
    },
    {
      "repo": "django/django",
      "pr_number": 18934,
      "file_path": "django/contrib/admin/options.py",
      "line": 1427,
      "side": "RIGHT",
      "diff_hunk": "@@ -1398,12 +1401,42 @@ def response_add(self, request, obj, post_url_continue=None):\n             else:\n                 attr = obj._meta.pk.attname\n             value = obj.serializable_value(attr)\n-            popup_response_data = json.dumps(\n-                {\n-                    \"value\": str(value),\n-                    \"obj\": str(obj),\n-                }\n-            )\n+            popup_response = {\n+                \"value\": str(value),\n+                \"obj\": str(obj),\n+            }\n+\n+            # Find the optgroup for the new item, if available\n+            source_model_name = request.POST.get(SOURCE_MODEL_VAR)\n+\n+            if source_model_name:\n+                app_label, model_name = source_model_name.split(\".\", 1)\n+                try:\n+                    source_model = apps.get_model(app_label, model_name)\n+                except LookupError:\n+                    msg = _('The app \"%s\" could not be found.') % source_model_name\n+                    self.message_user(request, msg, messages.ERROR)\n+                else:\n+                    source_admin = self.admin_site._registry[source_model]\n+                    form = source_admin.get_form(request)()\n+                    if self.opts.verbose_name_plural in form.fields:\n+                        field = form.fields[self.opts.verbose_name_plural]\n+                        if hasattr(field, \"choices\"):\n+                            for option_value, option_label in field.choices:\n+                                # Check if this is an optgroup\n+                                # (label is a list/tuple of choices)",
      "comment": "Interesting ok I made an attempt at a release note let me know what you think!",
      "comment_id": 2706203425,
      "user": "seanhelvey",
      "created_at": "2026-01-19T22:11:19Z",
      "url": "https://github.com/django/django/pull/18934#discussion_r2706203425"
    },
    {
      "repo": "django/django",
      "pr_number": 18934,
      "file_path": "django/contrib/admin/options.py",
      "line": 1430,
      "side": "RIGHT",
      "diff_hunk": "@@ -1398,12 +1401,42 @@ def response_add(self, request, obj, post_url_continue=None):\n             else:\n                 attr = obj._meta.pk.attname\n             value = obj.serializable_value(attr)\n-            popup_response_data = json.dumps(\n-                {\n-                    \"value\": str(value),\n-                    \"obj\": str(obj),\n-                }\n-            )\n+            popup_response = {\n+                \"value\": str(value),\n+                \"obj\": str(obj),\n+            }\n+\n+            # Find the optgroup for the new item, if available\n+            source_model_name = request.POST.get(SOURCE_MODEL_VAR)\n+\n+            if source_model_name:\n+                app_label, model_name = source_model_name.split(\".\", 1)\n+                try:\n+                    source_model = apps.get_model(app_label, model_name)\n+                except LookupError:\n+                    msg = _('The app \"%s\" could not be found.') % source_model_name\n+                    self.message_user(request, msg, messages.ERROR)\n+                else:\n+                    source_admin = self.admin_site._registry[source_model]\n+                    form = source_admin.get_form(request)()\n+                    if self.opts.verbose_name_plural in form.fields:\n+                        field = form.fields[self.opts.verbose_name_plural]\n+                        if hasattr(field, \"choices\"):\n+                            for option_value, option_label in field.choices:\n+                                # Check if this is an optgroup\n+                                # (label is a list/tuple of choices)\n+                                if isinstance(option_label, (list, tuple)):",
      "comment": "The doc you linked says any sequence is accepted, so I think we can use `not field._choices_is_value()` instead.",
      "comment_id": 2709139905,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-20T16:30:31Z",
      "url": "https://github.com/django/django/pull/18934#discussion_r2709139905"
    },
    {
      "repo": "django/django",
      "pr_number": 18934,
      "file_path": "tests/admin_widgets/tests.py",
      "line": 1365,
      "side": "RIGHT",
      "diff_hunk": "@@ -1354,14 +1355,14 @@ def execute_basic_operations(self, mode, field_name):\n         self.assertSelectOptions(\n             to_box,\n             [\n-                str(self.lisa.id),\n-                str(self.peter.id),\n                 str(self.arthur.id),\n                 str(self.bob.id),\n                 str(self.cliff.id),\n                 str(self.jason.id),\n                 str(self.jenny.id),\n                 str(self.john.id),\n+                str(self.lisa.id),\n+                str(self.peter.id),",
      "comment": "I looked a little closer -- this test doesn't involve optgroups, so I wouldn't really expect it to change. I traced it to the new calls to `sort()`. What do you think, should we gate the new `sort()` calls only if there are any groups?",
      "comment_id": 2709285985,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-20T17:09:09Z",
      "url": "https://github.com/django/django/pull/18934#discussion_r2709285985"
    },
    {
      "repo": "django/django",
      "pr_number": 18934,
      "file_path": "django/contrib/admin/options.py",
      "line": 1430,
      "side": "RIGHT",
      "diff_hunk": "@@ -1398,12 +1401,42 @@ def response_add(self, request, obj, post_url_continue=None):\n             else:\n                 attr = obj._meta.pk.attname\n             value = obj.serializable_value(attr)\n-            popup_response_data = json.dumps(\n-                {\n-                    \"value\": str(value),\n-                    \"obj\": str(obj),\n-                }\n-            )\n+            popup_response = {\n+                \"value\": str(value),\n+                \"obj\": str(obj),\n+            }\n+\n+            # Find the optgroup for the new item, if available\n+            source_model_name = request.POST.get(SOURCE_MODEL_VAR)\n+\n+            if source_model_name:\n+                app_label, model_name = source_model_name.split(\".\", 1)\n+                try:\n+                    source_model = apps.get_model(app_label, model_name)\n+                except LookupError:\n+                    msg = _('The app \"%s\" could not be found.') % source_model_name\n+                    self.message_user(request, msg, messages.ERROR)\n+                else:\n+                    source_admin = self.admin_site._registry[source_model]\n+                    form = source_admin.get_form(request)()\n+                    if self.opts.verbose_name_plural in form.fields:\n+                        field = form.fields[self.opts.verbose_name_plural]\n+                        if hasattr(field, \"choices\"):\n+                            for option_value, option_label in field.choices:\n+                                # Check if this is an optgroup\n+                                # (label is a list/tuple of choices)\n+                                if isinstance(option_label, (list, tuple)):",
      "comment": "Isn't field._choices_is_value() only on model fields, not form fields? ",
      "comment_id": 2713653472,
      "user": "seanhelvey",
      "created_at": "2026-01-21T17:40:03Z",
      "url": "https://github.com/django/django/pull/18934#discussion_r2713653472"
    },
    {
      "repo": "django/django",
      "pr_number": 18934,
      "file_path": "django/contrib/admin/options.py",
      "line": 1430,
      "side": "RIGHT",
      "diff_hunk": "@@ -1398,12 +1401,42 @@ def response_add(self, request, obj, post_url_continue=None):\n             else:\n                 attr = obj._meta.pk.attname\n             value = obj.serializable_value(attr)\n-            popup_response_data = json.dumps(\n-                {\n-                    \"value\": str(value),\n-                    \"obj\": str(obj),\n-                }\n-            )\n+            popup_response = {\n+                \"value\": str(value),\n+                \"obj\": str(obj),\n+            }\n+\n+            # Find the optgroup for the new item, if available\n+            source_model_name = request.POST.get(SOURCE_MODEL_VAR)\n+\n+            if source_model_name:\n+                app_label, model_name = source_model_name.split(\".\", 1)\n+                try:\n+                    source_model = apps.get_model(app_label, model_name)\n+                except LookupError:\n+                    msg = _('The app \"%s\" could not be found.') % source_model_name\n+                    self.message_user(request, msg, messages.ERROR)\n+                else:\n+                    source_admin = self.admin_site._registry[source_model]\n+                    form = source_admin.get_form(request)()\n+                    if self.opts.verbose_name_plural in form.fields:\n+                        field = form.fields[self.opts.verbose_name_plural]\n+                        if hasattr(field, \"choices\"):\n+                            for option_value, option_label in field.choices:\n+                                # Check if this is an optgroup\n+                                # (label is a list/tuple of choices)\n+                                if isinstance(option_label, (list, tuple)):",
      "comment": "Good call. How about:\n```suggestion\n                                if isinstance(option_label, Iterable) and not isinstance(\n                                    option_label, str\n                                ):\n```",
      "comment_id": 2713783710,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-21T18:15:28Z",
      "url": "https://github.com/django/django/pull/18934#discussion_r2713783710"
    },
    {
      "repo": "django/django",
      "pr_number": 18934,
      "file_path": "django/contrib/admin/options.py",
      "line": 1424,
      "side": "RIGHT",
      "diff_hunk": "@@ -1398,12 +1401,44 @@ def response_add(self, request, obj, post_url_continue=None):\n             else:\n                 attr = obj._meta.pk.attname\n             value = obj.serializable_value(attr)\n-            popup_response_data = json.dumps(\n-                {\n-                    \"value\": str(value),\n-                    \"obj\": str(obj),\n-                }\n-            )\n+            popup_response = {\n+                \"value\": str(value),\n+                \"obj\": str(obj),\n+            }\n+\n+            # Find the optgroup for the new item, if available\n+            source_model_name = request.POST.get(SOURCE_MODEL_VAR)\n+\n+            if source_model_name:\n+                app_label, model_name = source_model_name.split(\".\", 1)\n+                try:\n+                    source_model = apps.get_model(app_label, model_name)\n+                except LookupError:\n+                    msg = _('The app \"%s\" could not be found.') % source_model_name\n+                    self.message_user(request, msg, messages.ERROR)\n+                else:\n+                    source_admin = self.admin_site._registry[source_model]\n+                    form = source_admin.get_form(request)()\n+                    if self.opts.verbose_name_plural in form.fields:\n+                        field = form.fields[self.opts.verbose_name_plural]\n+                        if hasattr(field, \"choices\"):",
      "comment": "I don't think this attr is ever missing, so I think you can remove that and deindent (and reflow the lines so that `optgroup_label` isn't all by itself).",
      "comment_id": 2713822416,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-21T18:25:56Z",
      "url": "https://github.com/django/django/pull/18934#discussion_r2713822416"
    },
    {
      "repo": "django/django",
      "pr_number": 18934,
      "file_path": "django/contrib/admin/options.py",
      "line": 1430,
      "side": "RIGHT",
      "diff_hunk": "@@ -1398,12 +1401,42 @@ def response_add(self, request, obj, post_url_continue=None):\n             else:\n                 attr = obj._meta.pk.attname\n             value = obj.serializable_value(attr)\n-            popup_response_data = json.dumps(\n-                {\n-                    \"value\": str(value),\n-                    \"obj\": str(obj),\n-                }\n-            )\n+            popup_response = {\n+                \"value\": str(value),\n+                \"obj\": str(obj),\n+            }\n+\n+            # Find the optgroup for the new item, if available\n+            source_model_name = request.POST.get(SOURCE_MODEL_VAR)\n+\n+            if source_model_name:\n+                app_label, model_name = source_model_name.split(\".\", 1)\n+                try:\n+                    source_model = apps.get_model(app_label, model_name)\n+                except LookupError:\n+                    msg = _('The app \"%s\" could not be found.') % source_model_name\n+                    self.message_user(request, msg, messages.ERROR)\n+                else:\n+                    source_admin = self.admin_site._registry[source_model]\n+                    form = source_admin.get_form(request)()\n+                    if self.opts.verbose_name_plural in form.fields:\n+                        field = form.fields[self.opts.verbose_name_plural]\n+                        if hasattr(field, \"choices\"):\n+                            for option_value, option_label in field.choices:\n+                                # Check if this is an optgroup\n+                                # (label is a list/tuple of choices)\n+                                if isinstance(option_label, (list, tuple)):",
      "comment": "I rechecked, and it looks like these get normalized to 2-tuples somewhere no matter how they're initially coded up, so we can leave this as is. \ud83d\udc4d ",
      "comment_id": 2715148391,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-22T03:15:13Z",
      "url": "https://github.com/django/django/pull/18934#discussion_r2715148391"
    },
    {
      "repo": "django/django",
      "pr_number": 20543,
      "file_path": "django/db/migrations/graph.py",
      "line": 315,
      "side": "RIGHT",
      "diff_hunk": "@@ -306,9 +306,11 @@ def _nodes_and_edges(self):\n \n     def _generate_plan(self, nodes, at_end):\n         plan = []\n+        seen = set()  # Use set for O(1) membership testing instead of O(n)\n         for node in nodes:\n             for migration in self.forwards_plan(node):\n-                if migration not in plan and (at_end or migration not in nodes):\n+                if migration not in seen and (at_end or migration not in nodes):\n+                    seen.add(migration)\n                     plan.append(migration)\n         return plan",
      "comment": "How does it compare to using `django.utils.datastructures.OrderedSet`?\r\n\r\n```python\r\n    def _generate_plan(self, nodes, at_end):\r\n        plan = OrderedSet()\r\n        for node in nodes:\r\n            for migration in self.forwards_plan(node):\r\n                if migration not in plan and (at_end or migration not in nodes):\r\n                    plan.append(migration)\r\n        return list(plan)\r\n```\r\n\r\nIt's backed by using a dictionary where both `key in d` and `d[key] = value` are O(1), but then we do need to convert to a list at the end.",
      "comment_id": 2697815979,
      "user": "ngnpope",
      "created_at": "2026-01-16T10:02:02Z",
      "url": "https://github.com/django/django/pull/20543#discussion_r2697815979"
    },
    {
      "repo": "django/django",
      "pr_number": 20543,
      "file_path": "django/db/migrations/graph.py",
      "line": 312,
      "side": "RIGHT",
      "diff_hunk": "@@ -306,9 +306,11 @@ def _nodes_and_edges(self):\n \n     def _generate_plan(self, nodes, at_end):\n         plan = []\n+        seen = set()  # Use set for O(1) membership testing instead of O(n)\n         for node in nodes:\n             for migration in self.forwards_plan(node):\n-                if migration not in plan and (at_end or migration not in nodes):\n+                if migration not in seen and (at_end or migration not in nodes):",
      "comment": "How big does `nodes` typically get? I wonder if we can also optimise the `migration not in nodes` check?\r\n\r\nNot a blocker for this PR.",
      "comment_id": 2698958376,
      "user": "LilyFirefly",
      "created_at": "2026-01-16T15:28:07Z",
      "url": "https://github.com/django/django/pull/20543#discussion_r2698958376"
    },
    {
      "repo": "django/django",
      "pr_number": 20543,
      "file_path": "django/db/migrations/graph.py",
      "line": 315,
      "side": "RIGHT",
      "diff_hunk": "@@ -306,9 +306,11 @@ def _nodes_and_edges(self):\n \n     def _generate_plan(self, nodes, at_end):\n         plan = []\n+        seen = set()  # Use set for O(1) membership testing instead of O(n)\n         for node in nodes:\n             for migration in self.forwards_plan(node):\n-                if migration not in plan and (at_end or migration not in nodes):\n+                if migration not in seen and (at_end or migration not in nodes):\n+                    seen.add(migration)\n                     plan.append(migration)\n         return plan",
      "comment": "Thank you @ngnpope for this idea. I created a minimal benchmark script and added the results in the ticket: https://code.djangoproject.com/ticket/36869\n\ntl;dr your proposal performs as well as the proposed changed.",
      "comment_id": 2699241200,
      "user": "nessita",
      "created_at": "2026-01-16T16:48:25Z",
      "url": "https://github.com/django/django/pull/20543#discussion_r2699241200"
    },
    {
      "repo": "django/django",
      "pr_number": 20543,
      "file_path": "django/db/migrations/graph.py",
      "line": 315,
      "side": "RIGHT",
      "diff_hunk": "@@ -306,9 +306,11 @@ def _nodes_and_edges(self):\n \n     def _generate_plan(self, nodes, at_end):\n         plan = []\n+        seen = set()  # Use set for O(1) membership testing instead of O(n)\n         for node in nodes:\n             for migration in self.forwards_plan(node):\n-                if migration not in plan and (at_end or migration not in nodes):\n+                if migration not in seen and (at_end or migration not in nodes):\n+                    seen.add(migration)\n                     plan.append(migration)\n         return plan",
      "comment": "Great idea @ngnpope!  It feels like a much cleaner approach only having to maintain a single collection.  And thanks @nessita for benchmarking to validate that performance remains equivalent \ud83d\ude4f \r\n\r\nI'll update the PR with this suggestion",
      "comment_id": 2699470655,
      "user": "jfysh-kraken",
      "created_at": "2026-01-16T18:00:14Z",
      "url": "https://github.com/django/django/pull/20543#discussion_r2699470655"
    },
    {
      "repo": "django/django",
      "pr_number": 20543,
      "file_path": "django/db/migrations/graph.py",
      "line": 312,
      "side": "RIGHT",
      "diff_hunk": "@@ -306,9 +306,11 @@ def _nodes_and_edges(self):\n \n     def _generate_plan(self, nodes, at_end):\n         plan = []\n+        seen = set()  # Use set for O(1) membership testing instead of O(n)\n         for node in nodes:\n             for migration in self.forwards_plan(node):\n-                if migration not in plan and (at_end or migration not in nodes):\n+                if migration not in seen and (at_end or migration not in nodes):",
      "comment": "If I understand the code correctly, `nodes` should be the total number of migrations across all apps in `INSTALLED_APPS`.  And you are absolutely correct, `nodes` appears to be a list (looking at `make_state`) and there would be benefit to switching this out for a data structure that supports constant-time membership testing.\r\n\r\nI have made a small change to address this.",
      "comment_id": 2699544169,
      "user": "jfysh-kraken",
      "created_at": "2026-01-16T18:25:03Z",
      "url": "https://github.com/django/django/pull/20543#discussion_r2699544169"
    },
    {
      "repo": "django/django",
      "pr_number": 20543,
      "file_path": "django/db/migrations/graph.py",
      "line": 314,
      "side": "RIGHT",
      "diff_hunk": "@@ -305,12 +306,13 @@ def _nodes_and_edges(self):\n         )\n \n     def _generate_plan(self, nodes, at_end):\n-        plan = []\n+        plan = OrderedSet()\n+        nodes_set = set(nodes)  # Convert to set for O(1) membership testing\n         for node in nodes:\n             for migration in self.forwards_plan(node):\n-                if migration not in plan and (at_end or migration not in nodes):\n-                    plan.append(migration)\n-        return plan\n+                if migration not in plan and (at_end or migration not in nodes_set):\n+                    plan.add(migration)\n+        return list(plan)",
      "comment": "My Opinion: I think no need to covert to list here . in `make_state` we are iterating over plan. I don't know but this may reduce some overhead",
      "comment_id": 2702410193,
      "user": "p-r-a-v-i-n",
      "created_at": "2026-01-18T13:22:59Z",
      "url": "https://github.com/django/django/pull/20543#discussion_r2702410193"
    },
    {
      "repo": "django/django",
      "pr_number": 20543,
      "file_path": "django/db/migrations/graph.py",
      "line": 314,
      "side": "RIGHT",
      "diff_hunk": "@@ -305,12 +306,13 @@ def _nodes_and_edges(self):\n         )\n \n     def _generate_plan(self, nodes, at_end):\n-        plan = []\n+        plan = OrderedSet()\n+        nodes_set = set(nodes)  # Convert to set for O(1) membership testing\n         for node in nodes:\n             for migration in self.forwards_plan(node):\n-                if migration not in plan and (at_end or migration not in nodes):\n-                    plan.append(migration)\n-        return plan\n+                if migration not in plan and (at_end or migration not in nodes_set):\n+                    plan.add(migration)\n+        return list(plan)",
      "comment": "Maybe I'm being overly risk-averse, but I would prefer to maintain the same interface - a function that returns a `list`.  I don't think the cost to convert from OrderedSet to list is likely to be material enough to worry about?",
      "comment_id": 2702890311,
      "user": "jfysh-kraken",
      "created_at": "2026-01-19T00:11:01Z",
      "url": "https://github.com/django/django/pull/20543#discussion_r2702890311"
    },
    {
      "repo": "django/django",
      "pr_number": 20543,
      "file_path": "django/db/migrations/graph.py",
      "line": 314,
      "side": "RIGHT",
      "diff_hunk": "@@ -305,12 +306,13 @@ def _nodes_and_edges(self):\n         )\n \n     def _generate_plan(self, nodes, at_end):\n-        plan = []\n+        plan = OrderedSet()\n+        nodes_set = set(nodes)  # Convert to set for O(1) membership testing\n         for node in nodes:\n             for migration in self.forwards_plan(node):\n-                if migration not in plan and (at_end or migration not in nodes):\n-                    plan.append(migration)\n-        return plan\n+                if migration not in plan and (at_end or migration not in nodes_set):\n+                    plan.add(migration)\n+        return list(plan)",
      "comment": "if i understand correctly , `list` take O(n). if there is large plan then converting it list might add some overhead. Although this is just my assumptions , i might be totally wrong here.",
      "comment_id": 2703521706,
      "user": "p-r-a-v-i-n",
      "created_at": "2026-01-19T07:24:00Z",
      "url": "https://github.com/django/django/pull/20543#discussion_r2703521706"
    },
    {
      "repo": "django/django",
      "pr_number": 20543,
      "file_path": "django/db/migrations/graph.py",
      "line": 314,
      "side": "RIGHT",
      "diff_hunk": "@@ -305,12 +306,13 @@ def _nodes_and_edges(self):\n         )\n \n     def _generate_plan(self, nodes, at_end):\n-        plan = []\n+        plan = OrderedSet()\n+        nodes_set = set(nodes)  # Convert to set for O(1) membership testing\n         for node in nodes:\n             for migration in self.forwards_plan(node):\n-                if migration not in plan and (at_end or migration not in nodes):\n-                    plan.append(migration)\n-        return plan\n+                if migration not in plan and (at_end or migration not in nodes_set):\n+                    plan.add(migration)\n+        return list(plan)",
      "comment": "It's a private method only used in one place, so agreed that we could just change this to return the `OrderedSet` directly. I'd only suggested the making it a list again to maintain the existing return type, but it's not compulsory - I just hadn't looked.",
      "comment_id": 2704257088,
      "user": "ngnpope",
      "created_at": "2026-01-19T10:56:28Z",
      "url": "https://github.com/django/django/pull/20543#discussion_r2704257088"
    },
    {
      "repo": "django/django",
      "pr_number": 20543,
      "file_path": "django/db/migrations/graph.py",
      "line": 314,
      "side": "RIGHT",
      "diff_hunk": "@@ -305,12 +306,13 @@ def _nodes_and_edges(self):\n         )\n \n     def _generate_plan(self, nodes, at_end):\n-        plan = []\n+        plan = OrderedSet()\n+        nodes_set = set(nodes)  # Convert to set for O(1) membership testing\n         for node in nodes:\n             for migration in self.forwards_plan(node):\n-                if migration not in plan and (at_end or migration not in nodes):\n-                    plan.append(migration)\n-        return plan\n+                if migration not in plan and (at_end or migration not in nodes_set):\n+                    plan.add(migration)\n+        return list(plan)",
      "comment": "If we were to go down this path, `_generate_plan` is already so narrowly scoped that we could inline it into `make_state` and avoid the extra method call altogether. That said, this is starting to feel like yak shaving.\r\n\r\nI think we should merge the changes that demonstrate a clear and measurable performance improvement, and avoid those that do not move the benchmark numbers. This is consistent with Django's general practice of avoiding optimization-driven code changes unless the gains are documented and significant.",
      "comment_id": 2705540449,
      "user": "nessita",
      "created_at": "2026-01-19T16:56:31Z",
      "url": "https://github.com/django/django/pull/20543#discussion_r2705540449"
    },
    {
      "repo": "django/django",
      "pr_number": 20309,
      "file_path": "django/db/models/expressions.py",
      "line": 32,
      "side": "RIGHT",
      "diff_hunk": "@@ -29,7 +29,10 @@ def as_sqlite(self, compiler, connection, **extra_context):\n         sql, params = self.as_sql(compiler, connection, **extra_context)\n         try:\n             if self.output_field.get_internal_type() == \"DecimalField\":\n-                sql = \"(CAST(%s AS NUMERIC))\" % sql\n+                if isinstance(self, Value) and isinstance(self.value, Decimal):",
      "comment": "Instead of adding an if/else switch here, did you consider just implementing `Value.as_sqlite()` and removing this mixin from that class?",
      "comment_id": 2698862894,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-16T15:03:45Z",
      "url": "https://github.com/django/django/pull/20309#discussion_r2698862894"
    },
    {
      "repo": "django/django",
      "pr_number": 20309,
      "file_path": "tests/expressions/test_sqlite.py",
      "line": 29,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,37 @@\n+from decimal import Decimal\n+from unittest import skipUnless\n+\n+from django.db import connection\n+from django.db.models import DecimalField, F, Value\n+from django.db.models.sql import Query\n+from django.test import TestCase\n+\n+from .models import Number\n+\n+\n+class SQLiteDecimalExpressionsTests(TestCase):\n+    # Ensures the test only runs on SQLite.\n+    @skipUnless(connection.vendor == \"sqlite\", \"SQLite-only test\")\n+    def test_literal_value_decimal_cast_to_real(self):\n+        expr = Value(Decimal(\"3.0\"), output_field=DecimalField())\n+\n+        # We must use compiler.compile() so that Django looks for 'as_sqlite'\n+        compiler = connection.ops.compiler(\"SQLCompiler\")(Query(None), connection, None)\n+        sql, params = compiler.compile(expr.resolve_expression(Query(None)))\n+\n+        # Verify that the SQL is generated with CAST(... AS REAL)\n+        self.assertIn(\"CAST(\", sql)\n+        self.assertIn(\"AS REAL)\", sql)\n+        self.assertNotIn(\"AS NUMERIC\", sql)\n+\n+    def test_decimal_division_behavior(self):\n+        \"\"\"\n+        Verify that division with a literal Decimal value preserves precision",
      "comment": "We avoid preambles like \"verify\" in test docstrings.",
      "comment_id": 2698880947,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-16T15:08:01Z",
      "url": "https://github.com/django/django/pull/20309#discussion_r2698880947"
    },
    {
      "repo": "django/django",
      "pr_number": 20309,
      "file_path": "tests/expressions/test_sqlite.py",
      "line": 25,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,37 @@\n+from decimal import Decimal\n+from unittest import skipUnless\n+\n+from django.db import connection\n+from django.db.models import DecimalField, F, Value\n+from django.db.models.sql import Query\n+from django.test import TestCase\n+\n+from .models import Number\n+\n+\n+class SQLiteDecimalExpressionsTests(TestCase):\n+    # Ensures the test only runs on SQLite.\n+    @skipUnless(connection.vendor == \"sqlite\", \"SQLite-only test\")\n+    def test_literal_value_decimal_cast_to_real(self):\n+        expr = Value(Decimal(\"3.0\"), output_field=DecimalField())\n+\n+        # We must use compiler.compile() so that Django looks for 'as_sqlite'\n+        compiler = connection.ops.compiler(\"SQLCompiler\")(Query(None), connection, None)\n+        sql, params = compiler.compile(expr.resolve_expression(Query(None)))\n+\n+        # Verify that the SQL is generated with CAST(... AS REAL)\n+        self.assertIn(\"CAST(\", sql)\n+        self.assertIn(\"AS REAL)\", sql)\n+        self.assertNotIn(\"AS NUMERIC\", sql)",
      "comment": "As @bkline mentioned, we usually avoid asserting over the underlying SQL. The other test should be sufficient.",
      "comment_id": 2698885469,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-16T15:09:08Z",
      "url": "https://github.com/django/django/pull/20309#discussion_r2698885469"
    },
    {
      "repo": "django/django",
      "pr_number": 20309,
      "file_path": "tests/expressions/test_sqlite.py",
      "line": 30,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,37 @@\n+from decimal import Decimal\n+from unittest import skipUnless\n+\n+from django.db import connection\n+from django.db.models import DecimalField, F, Value\n+from django.db.models.sql import Query\n+from django.test import TestCase\n+\n+from .models import Number\n+\n+\n+class SQLiteDecimalExpressionsTests(TestCase):\n+    # Ensures the test only runs on SQLite.\n+    @skipUnless(connection.vendor == \"sqlite\", \"SQLite-only test\")\n+    def test_literal_value_decimal_cast_to_real(self):\n+        expr = Value(Decimal(\"3.0\"), output_field=DecimalField())\n+\n+        # We must use compiler.compile() so that Django looks for 'as_sqlite'\n+        compiler = connection.ops.compiler(\"SQLCompiler\")(Query(None), connection, None)\n+        sql, params = compiler.compile(expr.resolve_expression(Query(None)))\n+\n+        # Verify that the SQL is generated with CAST(... AS REAL)\n+        self.assertIn(\"CAST(\", sql)\n+        self.assertIn(\"AS REAL)\", sql)\n+        self.assertNotIn(\"AS NUMERIC\", sql)\n+\n+    def test_decimal_division_behavior(self):\n+        \"\"\"\n+        Verify that division with a literal Decimal value preserves precision\n+        on SQLite (i.e., it behaves like float division, not integer division).",
      "comment": "I think we want this test to run on all backends. So instead of placing it in a `test_sqlite` file, can you place it right after the test that was added for this casting behavior in the first place, i.e. right after `test_filter_decimal_annotation()`?",
      "comment_id": 2698895065,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-16T15:11:40Z",
      "url": "https://github.com/django/django/pull/20309#discussion_r2698895065"
    },
    {
      "repo": "django/django",
      "pr_number": 20309,
      "file_path": "tests/expressions/test_sqlite.py",
      "line": 37,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,37 @@\n+from decimal import Decimal\n+from unittest import skipUnless\n+\n+from django.db import connection\n+from django.db.models import DecimalField, F, Value\n+from django.db.models.sql import Query\n+from django.test import TestCase\n+\n+from .models import Number\n+\n+\n+class SQLiteDecimalExpressionsTests(TestCase):\n+    # Ensures the test only runs on SQLite.\n+    @skipUnless(connection.vendor == \"sqlite\", \"SQLite-only test\")\n+    def test_literal_value_decimal_cast_to_real(self):\n+        expr = Value(Decimal(\"3.0\"), output_field=DecimalField())\n+\n+        # We must use compiler.compile() so that Django looks for 'as_sqlite'\n+        compiler = connection.ops.compiler(\"SQLCompiler\")(Query(None), connection, None)\n+        sql, params = compiler.compile(expr.resolve_expression(Query(None)))\n+\n+        # Verify that the SQL is generated with CAST(... AS REAL)\n+        self.assertIn(\"CAST(\", sql)\n+        self.assertIn(\"AS REAL)\", sql)\n+        self.assertNotIn(\"AS NUMERIC\", sql)\n+\n+    def test_decimal_division_behavior(self):\n+        \"\"\"\n+        Verify that division with a literal Decimal value preserves precision\n+        on SQLite (i.e., it behaves like float division, not integer division).\n+        \"\"\"\n+        Number.objects.create(integer=2)\n+        # Expected behavior: Result is ~0.6667 (float/real division)\n+        obj = Number.objects.annotate(\n+            val=F(\"integer\") / Value(Decimal(\"3.0\"), output_field=DecimalField())\n+        ).first()\n+        self.assertAlmostEqual(obj.val, Decimal(\"0.6666666666666667\"), places=4)",
      "comment": "If we're only comparing four places, I'd adjust the second arg to use four places.",
      "comment_id": 2698915464,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-16T15:17:03Z",
      "url": "https://github.com/django/django/pull/20309#discussion_r2698915464"
    },
    {
      "repo": "django/django",
      "pr_number": 20309,
      "file_path": "tests/expressions/test_sqlite.py",
      "line": 30,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,37 @@\n+from decimal import Decimal\n+from unittest import skipUnless\n+\n+from django.db import connection\n+from django.db.models import DecimalField, F, Value\n+from django.db.models.sql import Query\n+from django.test import TestCase\n+\n+from .models import Number\n+\n+\n+class SQLiteDecimalExpressionsTests(TestCase):\n+    # Ensures the test only runs on SQLite.\n+    @skipUnless(connection.vendor == \"sqlite\", \"SQLite-only test\")\n+    def test_literal_value_decimal_cast_to_real(self):\n+        expr = Value(Decimal(\"3.0\"), output_field=DecimalField())\n+\n+        # We must use compiler.compile() so that Django looks for 'as_sqlite'\n+        compiler = connection.ops.compiler(\"SQLCompiler\")(Query(None), connection, None)\n+        sql, params = compiler.compile(expr.resolve_expression(Query(None)))\n+\n+        # Verify that the SQL is generated with CAST(... AS REAL)\n+        self.assertIn(\"CAST(\", sql)\n+        self.assertIn(\"AS REAL)\", sql)\n+        self.assertNotIn(\"AS NUMERIC\", sql)\n+\n+    def test_decimal_division_behavior(self):\n+        \"\"\"\n+        Verify that division with a literal Decimal value preserves precision\n+        on SQLite (i.e., it behaves like float division, not integer division).",
      "comment": "Hi, @jacobtylerwalls. The original ticket was not specific to SQLite (the original reporter was using PostgreSQL), but the ticket was repurposed (based on an interpretation of what @sarahboyce intended in the comment she posted December 20, 2024 when the ticket was accepted) narrowing it to the SQLite back end. Should we restore the broader focus of the ticket? I don't believe the new test will pass with PostgreSQL unless the PR is expanded (in the direction of the previous PR which was rejected).",
      "comment_id": 2699035518,
      "user": "bkline",
      "created_at": "2026-01-16T15:48:22Z",
      "url": "https://github.com/django/django/pull/20309#discussion_r2699035518"
    },
    {
      "repo": "django/django",
      "pr_number": 20309,
      "file_path": "tests/expressions/test_sqlite.py",
      "line": 30,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,37 @@\n+from decimal import Decimal\n+from unittest import skipUnless\n+\n+from django.db import connection\n+from django.db.models import DecimalField, F, Value\n+from django.db.models.sql import Query\n+from django.test import TestCase\n+\n+from .models import Number\n+\n+\n+class SQLiteDecimalExpressionsTests(TestCase):\n+    # Ensures the test only runs on SQLite.\n+    @skipUnless(connection.vendor == \"sqlite\", \"SQLite-only test\")\n+    def test_literal_value_decimal_cast_to_real(self):\n+        expr = Value(Decimal(\"3.0\"), output_field=DecimalField())\n+\n+        # We must use compiler.compile() so that Django looks for 'as_sqlite'\n+        compiler = connection.ops.compiler(\"SQLCompiler\")(Query(None), connection, None)\n+        sql, params = compiler.compile(expr.resolve_expression(Query(None)))\n+\n+        # Verify that the SQL is generated with CAST(... AS REAL)\n+        self.assertIn(\"CAST(\", sql)\n+        self.assertIn(\"AS REAL)\", sql)\n+        self.assertNotIn(\"AS NUMERIC\", sql)\n+\n+    def test_decimal_division_behavior(self):\n+        \"\"\"\n+        Verify that division with a literal Decimal value preserves precision\n+        on SQLite (i.e., it behaves like float division, not integer division).",
      "comment": "This test as written passes on postgres (and currently runs on postgres; there is no skip decorator on it).",
      "comment_id": 2699104906,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-16T16:07:05Z",
      "url": "https://github.com/django/django/pull/20309#discussion_r2699104906"
    },
    {
      "repo": "django/django",
      "pr_number": 20309,
      "file_path": "django/db/models/expressions.py",
      "line": 1185,
      "side": "RIGHT",
      "diff_hunk": "@@ -1182,6 +1182,18 @@ def as_sql(self, compiler, connection):\n             return \"NULL\", []\n         return \"%s\", [val]\n \n+    def as_sqlite(self, compiler, connection, **extra_context):",
      "comment": "Thanks, I think this is a good idea for now given that we are checking `self.value` below. We may find similar edge cases with the other expressions that would involve different kinds of introspection `self.cases`, `self.source_expressions`, etc., but we can cross that bridge later.",
      "comment_id": 2704961829,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-19T14:18:05Z",
      "url": "https://github.com/django/django/pull/20309#discussion_r2704961829"
    },
    {
      "repo": "django/django",
      "pr_number": 20466,
      "file_path": "tests/sites_framework/migrations/0002_alter_customarticle_managers_and_more.py",
      "line": 1,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,62 @@\n+# Generated by Django 6.1.dev20251225180557 on 2025-12-25 18:51",
      "comment": "Since these are tests, instead of adding new migration files, you can just edit the prior migrations.",
      "comment_id": 2682732352,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-12T15:19:39Z",
      "url": "https://github.com/django/django/pull/20466#discussion_r2682732352"
    },
    {
      "repo": "django/django",
      "pr_number": 20466,
      "file_path": "scripts/check_migrations.py",
      "line": 31,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,37 @@\n+import sys\n+from pathlib import Path\n+\n+import django\n+from django.apps import apps\n+from django.conf import settings\n+from django.core.management import call_command\n+\n+\n+def main():\n+    repo_root = Path(__file__).resolve().parent.parent\n+    sys.path[:0] = [str(repo_root / \"tests\"), str(repo_root)]\n+    from runtests import ALWAYS_INSTALLED_APPS, get_apps_to_install, get_test_modules\n+\n+    if not settings.configured:\n+        settings.configure(\n+            INSTALLED_APPS=ALWAYS_INSTALLED_APPS,\n+            MIGRATION_MODULES={},\n+        )\n+        django.setup()\n+\n+    test_modules = list(get_test_modules(gis_enabled=False))\n+    installed_apps = list(ALWAYS_INSTALLED_APPS)\n+    for app in get_apps_to_install(test_modules):\n+        # Check against the list to prevent duplicate error\n+        if app not in installed_apps:\n+            installed_apps.append(app)\n+    apps.set_installed_apps(installed_apps)\n+\n+    try:\n+        call_command(\"makemigrations\", \"--check\", verbosity=1)",
      "comment": "Running this locally I got unexpected success because the database was `django.db.backends.dummy`.",
      "comment_id": 2682775826,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-12T15:29:37Z",
      "url": "https://github.com/django/django/pull/20466#discussion_r2682775826"
    },
    {
      "repo": "django/django",
      "pr_number": 20466,
      "file_path": "scripts/check_migrations.py",
      "line": 31,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,37 @@\n+import sys\n+from pathlib import Path\n+\n+import django\n+from django.apps import apps\n+from django.conf import settings\n+from django.core.management import call_command\n+\n+\n+def main():\n+    repo_root = Path(__file__).resolve().parent.parent\n+    sys.path[:0] = [str(repo_root / \"tests\"), str(repo_root)]\n+    from runtests import ALWAYS_INSTALLED_APPS, get_apps_to_install, get_test_modules\n+\n+    if not settings.configured:\n+        settings.configure(\n+            INSTALLED_APPS=ALWAYS_INSTALLED_APPS,\n+            MIGRATION_MODULES={},\n+        )\n+        django.setup()\n+\n+    test_modules = list(get_test_modules(gis_enabled=False))\n+    installed_apps = list(ALWAYS_INSTALLED_APPS)\n+    for app in get_apps_to_install(test_modules):\n+        # Check against the list to prevent duplicate error\n+        if app not in installed_apps:\n+            installed_apps.append(app)\n+    apps.set_installed_apps(installed_apps)\n+\n+    try:\n+        call_command(\"makemigrations\", \"--check\", verbosity=1)",
      "comment": "I'd add a comment that the reason we aren't using `check=True` is that `--check` directly calls `sys.exit(1)` instead of raising `CommandError`, which was a little unintuitive for me.",
      "comment_id": 2682825236,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-12T15:40:48Z",
      "url": "https://github.com/django/django/pull/20466#discussion_r2682825236"
    },
    {
      "repo": "django/django",
      "pr_number": 20466,
      "file_path": "scripts/check_migrations.py",
      "line": 17,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,31 @@\n+import sys\n+from pathlib import Path\n+\n+\n+def main():\n+    repo_root = Path(__file__).resolve().parent.parent\n+    sys.path[:0] = [str(repo_root / \"tests\"), str(repo_root)]\n+\n+    from runtests import ALWAYS_INSTALLED_APPS, get_apps_to_install, get_test_modules\n+\n+    import django\n+    from django.apps import apps\n+    from django.core.management import call_command\n+\n+    django.setup()\n+\n+    test_modules = list(get_test_modules(gis_enabled=False))",
      "comment": "I think we should run with `gis_enabled=True`, as I have several more failures:\n\n```py\nMigrations for 'gis_migrations':\n  tests/gis_tests/gis_migrations/migrations/0003_remove_household_family_delete_heatmap_and_more.py\n    - Remove field family from household\n    - Delete model Heatmap\n    - Remove field neighborhood from household\n    - Delete model Family\n    - Delete model Household\n    - Delete model Neighborhood\nMigrations for 'rasterapp':\n  tests/gis_tests/rasterapp/migrations/0003_alter_rastermodel_id_alter_rasterrelatedmodel_id.py\n    ~ Alter field id on rastermodel\n    ~ Alter field id on rasterrelatedmodel\n```\n\nYou could borrow the CI config in the `postgis.yml`.",
      "comment_id": 2704899086,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-19T14:02:54Z",
      "url": "https://github.com/django/django/pull/20466#discussion_r2704899086"
    },
    {
      "repo": "django/django",
      "pr_number": 20466,
      "file_path": "scripts/check_migrations.py",
      "line": 27,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,31 @@\n+import sys\n+from pathlib import Path\n+\n+\n+def main():\n+    repo_root = Path(__file__).resolve().parent.parent\n+    sys.path[:0] = [str(repo_root / \"tests\"), str(repo_root)]\n+\n+    from runtests import ALWAYS_INSTALLED_APPS, get_apps_to_install, get_test_modules\n+\n+    import django\n+    from django.apps import apps\n+    from django.core.management import call_command\n+\n+    django.setup()\n+\n+    test_modules = list(get_test_modules(gis_enabled=False))\n+    installed_apps = list(ALWAYS_INSTALLED_APPS)\n+    for app in get_apps_to_install(test_modules):\n+        # Check against the list to prevent duplicate error\n+        if app not in installed_apps:\n+            installed_apps.append(app)\n+    apps.set_installed_apps(installed_apps)\n+\n+    # Note: We don't use check=True here because --check calls sys.exit(1)\n+    # instead of raising CommandError when migrations are missing\n+    call_command(\"makemigrations\", \"--check\", verbosity=1)",
      "comment": "verbosity=3 would be a nice convenience, so that when this fails on CI you don't have to re-run it manually to get the changes.",
      "comment_id": 2704904951,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-19T14:04:20Z",
      "url": "https://github.com/django/django/pull/20466#discussion_r2704904951"
    },
    {
      "repo": "django/django",
      "pr_number": 20466,
      "file_path": "scripts/check_migrations.py",
      "line": 17,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,31 @@\n+import sys\n+from pathlib import Path\n+\n+\n+def main():\n+    repo_root = Path(__file__).resolve().parent.parent\n+    sys.path[:0] = [str(repo_root / \"tests\"), str(repo_root)]\n+\n+    from runtests import ALWAYS_INSTALLED_APPS, get_apps_to_install, get_test_modules\n+\n+    import django\n+    from django.apps import apps\n+    from django.core.management import call_command\n+\n+    django.setup()\n+\n+    test_modules = list(get_test_modules(gis_enabled=False))",
      "comment": "Ah, well, on second look it looks like for the `gis_migrations` case there might be too much variance between GIS backends, plus the dynamic way the tests in `test_operations.py` are written. So maybe it's okay to leave this as is.\n\nWe could still fold in the fixes for `rasterapp`, though? \ud83e\udd14 ",
      "comment_id": 2704922159,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-19T14:08:33Z",
      "url": "https://github.com/django/django/pull/20466#discussion_r2704922159"
    },
    {
      "repo": "django/django",
      "pr_number": 20466,
      "file_path": "scripts/check_migrations.py",
      "line": 17,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,31 @@\n+import sys\n+from pathlib import Path\n+\n+\n+def main():\n+    repo_root = Path(__file__).resolve().parent.parent\n+    sys.path[:0] = [str(repo_root / \"tests\"), str(repo_root)]\n+\n+    from runtests import ALWAYS_INSTALLED_APPS, get_apps_to_install, get_test_modules\n+\n+    import django\n+    from django.apps import apps\n+    from django.core.management import call_command\n+\n+    django.setup()\n+\n+    test_modules = list(get_test_modules(gis_enabled=True))",
      "comment": "Looks like we can't do this, see https://github.com/django/django/pull/20466#discussion_r2704922159.",
      "comment_id": 2705421314,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-19T16:29:53Z",
      "url": "https://github.com/django/django/pull/20466#discussion_r2705421314"
    },
    {
      "repo": "django/django",
      "pr_number": 20466,
      "file_path": "scripts/check_migrations.py",
      "line": 17,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,31 @@\n+import sys\n+from pathlib import Path\n+\n+\n+def main():\n+    repo_root = Path(__file__).resolve().parent.parent\n+    sys.path[:0] = [str(repo_root / \"tests\"), str(repo_root)]\n+\n+    from runtests import ALWAYS_INSTALLED_APPS, get_apps_to_install, get_test_modules\n+\n+    import django\n+    from django.apps import apps\n+    from django.core.management import call_command\n+\n+    django.setup()\n+\n+    test_modules = list(get_test_modules(gis_enabled=True))",
      "comment": "Yes , I was trying if this is possible . Migrations can be fixed by adding \r\n\r\n```\r\n\r\nif connection.features.supports_raster:\r\n    ops.insert(1, migrations.DeleteModel(name=\"Heatmap\"))\r\n\r\n```\r\n\r\nas Heatmap has \"supports_raster\"  in 0002 migration file \r\nand changing a test case (checking the deleted model) . \r\n\r\nShould I add this or let ``` gis_enabled = False ``` ?",
      "comment_id": 2705445137,
      "user": "Skyiesac",
      "created_at": "2026-01-19T16:34:22Z",
      "url": "https://github.com/django/django/pull/20466#discussion_r2705445137"
    },
    {
      "repo": "django/django",
      "pr_number": 20466,
      "file_path": "scripts/check_migrations.py",
      "line": 17,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,31 @@\n+import sys\n+from pathlib import Path\n+\n+\n+def main():\n+    repo_root = Path(__file__).resolve().parent.parent\n+    sys.path[:0] = [str(repo_root / \"tests\"), str(repo_root)]\n+\n+    from runtests import ALWAYS_INSTALLED_APPS, get_apps_to_install, get_test_modules\n+\n+    import django\n+    from django.apps import apps\n+    from django.core.management import call_command\n+\n+    django.setup()\n+\n+    test_modules = list(get_test_modules(gis_enabled=True))",
      "comment": "Can you tell me a little more about that idea? My understanding was that we don't have a python model for `Neighborhood` anywhere, so I would expect the `makemigrations --check` to be a poor fit for that test paradigm.",
      "comment_id": 2705526307,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-19T16:51:44Z",
      "url": "https://github.com/django/django/pull/20466#discussion_r2705526307"
    },
    {
      "repo": "django/django",
      "pr_number": 20466,
      "file_path": "scripts/check_migrations.py",
      "line": 17,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,31 @@\n+import sys\n+from pathlib import Path\n+\n+\n+def main():\n+    repo_root = Path(__file__).resolve().parent.parent\n+    sys.path[:0] = [str(repo_root / \"tests\"), str(repo_root)]\n+\n+    from runtests import ALWAYS_INSTALLED_APPS, get_apps_to_install, get_test_modules\n+\n+    import django\n+    from django.apps import apps\n+    from django.core.management import call_command\n+\n+    django.setup()\n+\n+    test_modules = list(get_test_modules(gis_enabled=True))",
      "comment": "Oh, my bad , Got it ! I've removed gis tests entirely . Thankyou for clarity .",
      "comment_id": 2705568329,
      "user": "Skyiesac",
      "created_at": "2026-01-19T17:06:47Z",
      "url": "https://github.com/django/django/pull/20466#discussion_r2705568329"
    },
    {
      "repo": "django/django",
      "pr_number": 20515,
      "file_path": "tests/gis_tests/gdal_tests/test_driver.py",
      "line": 12,
      "side": "RIGHT",
      "diff_hunk": "@@ -9,7 +9,7 @@\n     \"MapInfo File\",\n     \"S57\",\n     \"DGN\",\n-    \"Memory\",\n+    \"Memory\" if GDAL_VERSION <= (3, 10) else \"MEM\",",
      "comment": "@smithdc1 do you know what GDAL 3.10.3 returns, is it `(3, 10)` or `(3, 10, 3)`?",
      "comment_id": 2699531729,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-16T18:20:02Z",
      "url": "https://github.com/django/django/pull/20515#discussion_r2699531729"
    },
    {
      "repo": "django/django",
      "pr_number": 20515,
      "file_path": "tests/gis_tests/gdal_tests/test_driver.py",
      "line": 12,
      "side": "RIGHT",
      "diff_hunk": "@@ -9,7 +9,7 @@\n     \"MapInfo File\",\n     \"S57\",\n     \"DGN\",\n-    \"Memory\",\n+    \"Memory\" if GDAL_VERSION <= (3, 10) else \"MEM\",",
      "comment": "Ah. Well, 3.11.3 returns `(3, 11, 3)` so I'm going to take a punt that the 3.10 series does the same thing. \r\n\r\nHappy to provide a fix but I'm wondering if it is worth it? It's only deprecated, not removed. So anyone running 3.10 will still see the tests pass but may notice a log. But given logs are silenced by default, most folk will have trouble spotting it. \r\n\r\n",
      "comment_id": 2701111277,
      "user": "smithdc1",
      "created_at": "2026-01-17T13:34:19Z",
      "url": "https://github.com/django/django/pull/20515#discussion_r2701111277"
    },
    {
      "repo": "django/django",
      "pr_number": 20515,
      "file_path": "tests/gis_tests/gdal_tests/test_driver.py",
      "line": 12,
      "side": "RIGHT",
      "diff_hunk": "@@ -9,7 +9,7 @@\n     \"MapInfo File\",\n     \"S57\",\n     \"DGN\",\n-    \"Memory\",\n+    \"Memory\" if GDAL_VERSION <= (3, 10) else \"MEM\",",
      "comment": "I'm not so concerned about unnecessary use of `Memory`, I'm moreso wondering about too early use of `MEM` on 3.10.x. Does it still work on 3.10.x? From your link, it looks like not.",
      "comment_id": 2703121789,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-19T03:46:28Z",
      "url": "https://github.com/django/django/pull/20515#discussion_r2703121789"
    },
    {
      "repo": "django/django",
      "pr_number": 20380,
      "file_path": "tests/model_state/test_memory_leak.py",
      "line": 20,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,33 @@\n+import gc\n+import weakref\n+\n+from django.db import models\n+from django.test import SimpleTestCase\n+\n+\n+class CycleParent(models.Model):\n+    pass\n+\n+\n+class CycleChild(models.Model):\n+    parent = models.OneToOneField(CycleParent, on_delete=models.CASCADE)\n+\n+\n+class ModelStateMemoryTests(SimpleTestCase):\n+    def test_cycle_collection(self):\n+        \"\"\"\n+        Ensure that OneToOneField cycles are collected by the GC.\n+        \"\"\"",
      "comment": "You can remove this in favor of a more descriptive method title mentioning one_to_one (or o2o) fields.",
      "comment_id": 2599623908,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-08T18:11:52Z",
      "url": "https://github.com/django/django/pull/20380#discussion_r2599623908"
    },
    {
      "repo": "django/django",
      "pr_number": 20380,
      "file_path": "tests/model_state/test_memory_leak.py",
      "line": 32,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,33 @@\n+import gc\n+import weakref\n+\n+from django.db import models\n+from django.test import SimpleTestCase\n+\n+\n+class CycleParent(models.Model):\n+    pass\n+\n+\n+class CycleChild(models.Model):\n+    parent = models.OneToOneField(CycleParent, on_delete=models.CASCADE)\n+\n+\n+class ModelStateMemoryTests(SimpleTestCase):\n+    def test_cycle_collection(self):\n+        \"\"\"\n+        Ensure that OneToOneField cycles are collected by the GC.\n+        \"\"\"\n+        p = CycleParent()\n+        c = CycleChild(parent=p)\n+\n+        p_ref = weakref.ref(p)\n+        c_ref = weakref.ref(c)\n+\n+        del p\n+        del c\n+\n+        gc.collect()\n+\n+        self.assertIsNone(p_ref(), \"Parent should be garbage collected\")",
      "comment": "This test doesn't fail when I revert your changes. You might be able to draw inspiration from this:\n\nhttps://github.com/django/django/blob/334308efae8e0c7b1523d5583af32b674a098eba/tests/select_related/tests.py#L63-L72",
      "comment_id": 2599634613,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-08T18:16:11Z",
      "url": "https://github.com/django/django/pull/20380#discussion_r2599634613"
    },
    {
      "repo": "django/django",
      "pr_number": 20380,
      "file_path": "tests/model_state/test_memory_leak.py",
      "line": 16,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,33 @@\n+import gc\n+import weakref\n+\n+from django.db import models\n+from django.test import SimpleTestCase\n+\n+\n+class CycleParent(models.Model):\n+    pass\n+\n+\n+class CycleChild(models.Model):\n+    parent = models.OneToOneField(CycleParent, on_delete=models.CASCADE)\n+\n+\n+class ModelStateMemoryTests(SimpleTestCase):",
      "comment": "Please move this test to tests/model_regress/test_state.py",
      "comment_id": 2599643979,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-08T18:18:46Z",
      "url": "https://github.com/django/django/pull/20380#discussion_r2599643979"
    },
    {
      "repo": "django/django",
      "pr_number": 20380,
      "file_path": "tests/model_state/test_memory_leak.py",
      "line": 32,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,33 @@\n+import gc\n+import weakref\n+\n+from django.db import models\n+from django.test import SimpleTestCase\n+\n+\n+class CycleParent(models.Model):\n+    pass\n+\n+\n+class CycleChild(models.Model):\n+    parent = models.OneToOneField(CycleParent, on_delete=models.CASCADE)\n+\n+\n+class ModelStateMemoryTests(SimpleTestCase):\n+    def test_cycle_collection(self):\n+        \"\"\"\n+        Ensure that OneToOneField cycles are collected by the GC.\n+        \"\"\"\n+        p = CycleParent()\n+        c = CycleChild(parent=p)\n+\n+        p_ref = weakref.ref(p)\n+        c_ref = weakref.ref(c)\n+\n+        del p\n+        del c\n+\n+        gc.collect()\n+\n+        self.assertIsNone(p_ref(), \"Parent should be garbage collected\")",
      "comment": "I am really sorry, I saw the test passing and got excited and forgot to check for the main scenario I was writing the test.\r\nI have written the test again and verified it is failing now before the fix and passing after the fix ",
      "comment_id": 2599764195,
      "user": "Samriddha9619",
      "created_at": "2025-12-08T18:53:04Z",
      "url": "https://github.com/django/django/pull/20380#discussion_r2599764195"
    },
    {
      "repo": "django/django",
      "pr_number": 20380,
      "file_path": "tests/model_regress/test_state.py",
      "line": 21,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,7 +1,36 @@\n+import gc\n+\n from django.db.models.base import ModelState, ModelStateFieldsCacheDescriptor\n from django.test import SimpleTestCase\n \n+from .models import CycleChild, CycleParent\n+\n \n class ModelStateTests(SimpleTestCase):\n     def test_fields_cache_descriptor(self):\n         self.assertIsInstance(ModelState.fields_cache, ModelStateFieldsCacheDescriptor)\n+\n+    def test_one_to_one_field_cycle_collection(self):\n+        \"\"\"\n+        Ensure OneToOneField intermediate state does not create reference\n+        cycles during model initialization.\n+        \"\"\"\n+        self.addCleanup(gc.set_debug, gc.get_debug())\n+        gc.set_debug(gc.DEBUG_SAVEALL)\n+\n+        gc.collect()",
      "comment": "Please use the `garbage_collect()` helper from `django.test.utils`.",
      "comment_id": 2600172943,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-08T21:17:48Z",
      "url": "https://github.com/django/django/pull/20380#discussion_r2600172943"
    },
    {
      "repo": "django/django",
      "pr_number": 20380,
      "file_path": "tests/model_regress/test_state.py",
      "line": 18,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,7 +1,36 @@\n+import gc\n+\n from django.db.models.base import ModelState, ModelStateFieldsCacheDescriptor\n from django.test import SimpleTestCase\n \n+from .models import CycleChild, CycleParent\n+\n \n class ModelStateTests(SimpleTestCase):\n     def test_fields_cache_descriptor(self):\n         self.assertIsInstance(ModelState.fields_cache, ModelStateFieldsCacheDescriptor)\n+\n+    def test_one_to_one_field_cycle_collection(self):\n+        \"\"\"\n+        Ensure OneToOneField intermediate state does not create reference\n+        cycles during model initialization.\n+        \"\"\"",
      "comment": "```suggestion\n```\nThanks for the method name change. Now we can lose the docstring (we avoid \"Ensure ...\" preambles anyway.)",
      "comment_id": 2600180316,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-08T21:21:01Z",
      "url": "https://github.com/django/django/pull/20380#discussion_r2600180316"
    },
    {
      "repo": "django/django",
      "pr_number": 20380,
      "file_path": "tests/model_regress/test_state.py",
      "line": 33,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,7 +1,36 @@\n+import gc\n+\n from django.db.models.base import ModelState, ModelStateFieldsCacheDescriptor\n from django.test import SimpleTestCase\n \n+from .models import CycleChild, CycleParent\n+\n \n class ModelStateTests(SimpleTestCase):\n     def test_fields_cache_descriptor(self):\n         self.assertIsInstance(ModelState.fields_cache, ModelStateFieldsCacheDescriptor)\n+\n+    def test_one_to_one_field_cycle_collection(self):\n+        \"\"\"\n+        Ensure OneToOneField intermediate state does not create reference\n+        cycles during model initialization.\n+        \"\"\"\n+        self.addCleanup(gc.set_debug, gc.get_debug())\n+        gc.set_debug(gc.DEBUG_SAVEALL)\n+\n+        gc.collect()\n+        del gc.garbage[:]\n+\n+        p = CycleParent()\n+        c = CycleChild(parent=p)\n+\n+        p_id = id(p)\n+\n+        del p\n+        del c\n+\n+        gc.collect()\n+\n+        leaked = [obj for obj in gc.garbage if id(obj) == p_id]\n+\n+        self.assertEqual(leaked, [])",
      "comment": "After this test I get this warning:\n```py\nException ignored in GC shutdown:\nResourceWarning: gc: 7995 uncollectable objects at shutdown; use gc.set_debug(gc.DEBUG_UNCOLLECTABLE) to list them\n```",
      "comment_id": 2600197392,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-08T21:28:08Z",
      "url": "https://github.com/django/django/pull/20380#discussion_r2600197392"
    },
    {
      "repo": "django/django",
      "pr_number": 20380,
      "file_path": "django/db/models/base.py",
      "line": 499,
      "side": "RIGHT",
      "diff_hunk": "@@ -494,6 +494,10 @@ def __getstate__(self):\n         state.pop(\"peers\", None)\n         return state\n \n+    def __del__(self):\n+        if hasattr(self, \"_fields_cache\"):\n+            del self._fields_cache",
      "comment": "My question was getting at, under what conditions would `_fields_cache` be missing?",
      "comment_id": 2602570060,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-09T13:04:39Z",
      "url": "https://github.com/django/django/pull/20380#discussion_r2602570060"
    },
    {
      "repo": "django/django",
      "pr_number": 20380,
      "file_path": "tests/model_regress/test_state.py",
      "line": 9,
      "side": "RIGHT",
      "diff_hunk": "@@ -5,3 +5,15 @@\n class ModelStateTests(SimpleTestCase):\n     def test_fields_cache_descriptor(self):\n         self.assertIsInstance(ModelState.fields_cache, ModelStateFieldsCacheDescriptor)\n+\n+    def test_model_state_del_clears_cache(self):",
      "comment": "This test is testing the implementation instead of the underlying issue. It would be better to polish the test you had before.",
      "comment_id": 2602572322,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-09T13:05:13Z",
      "url": "https://github.com/django/django/pull/20380#discussion_r2602572322"
    },
    {
      "repo": "django/django",
      "pr_number": 20380,
      "file_path": "django/db/models/base.py",
      "line": 501,
      "side": "RIGHT",
      "diff_hunk": "@@ -494,6 +494,12 @@ def __getstate__(self):\n         state.pop(\"peers\", None)\n         return state\n \n+    def __del__(self):\n+        try:\n+            self.fields_cache.clear()\n+        except AttributeError:\n+            pass",
      "comment": "Thanks for explaining -- in that case these lines are reachable, so we need a test to cover them.",
      "comment_id": 2608258954,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-10T21:24:50Z",
      "url": "https://github.com/django/django/pull/20380#discussion_r2608258954"
    },
    {
      "repo": "django/django",
      "pr_number": 20380,
      "file_path": "tests/model_regress/test_state.py",
      "line": 33,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,7 +1,36 @@\n+import gc\n+\n from django.db.models.base import ModelState, ModelStateFieldsCacheDescriptor\n from django.test import SimpleTestCase\n \n+from .models import CycleChild, CycleParent\n+\n \n class ModelStateTests(SimpleTestCase):\n     def test_fields_cache_descriptor(self):\n         self.assertIsInstance(ModelState.fields_cache, ModelStateFieldsCacheDescriptor)\n+\n+    def test_one_to_one_field_cycle_collection(self):\n+        \"\"\"\n+        Ensure OneToOneField intermediate state does not create reference\n+        cycles during model initialization.\n+        \"\"\"\n+        self.addCleanup(gc.set_debug, gc.get_debug())\n+        gc.set_debug(gc.DEBUG_SAVEALL)\n+\n+        gc.collect()\n+        del gc.garbage[:]\n+\n+        p = CycleParent()\n+        c = CycleChild(parent=p)\n+\n+        p_id = id(p)\n+\n+        del p\n+        del c\n+\n+        gc.collect()\n+\n+        leaked = [obj for obj in gc.garbage if id(obj) == p_id]\n+\n+        self.assertEqual(leaked, [])",
      "comment": "Again, thanks very much for explaining. Now we need to register this as a cleanup earlier (right after garbage_collect, with `self.addCleanup`) because if the assertion fails for some reason I get the warning again.",
      "comment_id": 2608269458,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-10T21:27:57Z",
      "url": "https://github.com/django/django/pull/20380#discussion_r2608269458"
    },
    {
      "repo": "django/django",
      "pr_number": 20380,
      "file_path": "tests/model_regress/test_state.py",
      "line": 16,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,7 +1,38 @@\n+import gc\n+\n from django.db.models.base import ModelState, ModelStateFieldsCacheDescriptor\n from django.test import SimpleTestCase\n+from django.test.utils import garbage_collect\n+\n+from .models import Worker, WorkerProfile\n \n \n class ModelStateTests(SimpleTestCase):\n     def test_fields_cache_descriptor(self):\n         self.assertIsInstance(ModelState.fields_cache, ModelStateFieldsCacheDescriptor)\n+\n+    def test_model_state_del_no_cache(self):\n+        state = ModelState()\n+        self.assertFalse(hasattr(state, \"_fields_cache\"))",
      "comment": "@jacobtylerwalls\r\n\r\nRegarding `_fields_cache` assignment: That was an error in my test code. The internal storage key is actually `fields_cache` . I have corrected the test to check state.`__dict__` for the correct key `(\"fields_cache\")`.\r\n\r\nReliance on `del state` alone was causing coverage gaps. I have updated the test to explicitly call state.`__del__()`. This forces the execution of the except AttributeError: pass block, ensuring 100% coverage for the teardown logic.",
      "comment_id": 2622977356,
      "user": "Samriddha9619",
      "created_at": "2025-12-16T11:55:39Z",
      "url": "https://github.com/django/django/pull/20380#discussion_r2622977356"
    },
    {
      "repo": "django/django",
      "pr_number": 20380,
      "file_path": "tests/model_regress/test_state.py",
      "line": 16,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,7 +1,38 @@\n+import gc\n+\n from django.db.models.base import ModelState, ModelStateFieldsCacheDescriptor\n from django.test import SimpleTestCase\n+from django.test.utils import garbage_collect\n+\n+from .models import Worker, WorkerProfile\n \n \n class ModelStateTests(SimpleTestCase):\n     def test_fields_cache_descriptor(self):\n         self.assertIsInstance(ModelState.fields_cache, ModelStateFieldsCacheDescriptor)\n+\n+    def test_model_state_del_no_cache(self):\n+        state = ModelState()\n+        self.assertFalse(hasattr(state, \"_fields_cache\"))",
      "comment": "The except is still not covered. Also, please fetch the changes I pushed to your branch; you've force-pushed over them. TIP: you can use `--force-with-lease` when force pushing to ensure you get an error if there are missing changes to fetch first.",
      "comment_id": 2623360618,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-16T13:49:42Z",
      "url": "https://github.com/django/django/pull/20380#discussion_r2623360618"
    },
    {
      "repo": "django/django",
      "pr_number": 20380,
      "file_path": "tests/model_regress/test_state.py",
      "line": 16,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,7 +1,37 @@\n+import gc\n+\n from django.db.models.base import ModelState, ModelStateFieldsCacheDescriptor\n from django.test import SimpleTestCase\n+from django.test.utils import garbage_collect\n+\n+from .models import Worker, WorkerProfile\n \n \n class ModelStateTests(SimpleTestCase):\n     def test_fields_cache_descriptor(self):\n         self.assertIsInstance(ModelState.fields_cache, ModelStateFieldsCacheDescriptor)\n+\n+    def test_model_state_del_no_cache(self):\n+        state = ModelState()\n+        del state",
      "comment": "Sorry, I guess this whole test method can go now, now that we're not trying to exercise some separate code path.",
      "comment_id": 2632529639,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-18T20:27:59Z",
      "url": "https://github.com/django/django/pull/20380#discussion_r2632529639"
    },
    {
      "repo": "django/django",
      "pr_number": 20524,
      "file_path": "django/contrib/admin/utils.py",
      "line": 572,
      "side": "RIGHT",
      "diff_hunk": "@@ -552,21 +552,21 @@ def construct_change_message(form, formsets, add):\n     Translations are deactivated so that strings are stored untranslated.\n     Translation happens later on LogEntry access.\n     \"\"\"\n+    change_message = []\n+    if add:\n+        change_message.append({\"added\": {}})\n     # Evaluating `form.changed_data` prior to disabling translations is\n     # required to avoid fields affected by localization from being included\n     # incorrectly, e.g. where date formats differ such as MM/DD/YYYY vs\n     # DD/MM/YYYY.\n-    changed_data = form.changed_data\n-    with translation_override(None):\n-        # Deactivate translations while fetching verbose_name for form\n-        # field labels and using `field_name`, if verbose_name is not provided.\n-        # Translations will happen later on LogEntry access.\n-        changed_field_labels = _get_changed_field_labels_from_form(form, changed_data)\n-\n-    change_message = []\n-    if add:\n-        change_message.append({\"added\": {}})\n-    elif form.changed_data:\n+    elif changed_data := form.changed_data:\n+        with translation_override(None):\n+            # Deactivate translations while fetching verbose_name for form\n+            # field labels and using `field_name`, if verbose_name is not\n+            # provided. Translations will happen later on LogEntry access.\n+            changed_field_labels = _get_changed_field_labels_from_form(\n+                form, changed_data\n+            )\n         change_message.append({\"changed\": {\"fields\": changed_field_labels}})\n     if formsets:\n         with translation_override(None):",
      "comment": "Thought: Given you mention `translation_override` is expensive, is there any way to combine these calls? It'd be nice to avoid needing to call it twice in the same call to `construct_change_message`.\n\nIt might end up being far more complex than is useful - just a thought.",
      "comment_id": 2691385394,
      "user": "RealOrangeOne",
      "created_at": "2026-01-14T17:31:03Z",
      "url": "https://github.com/django/django/pull/20524#discussion_r2691385394"
    },
    {
      "repo": "django/django",
      "pr_number": 20524,
      "file_path": "django/contrib/admin/utils.py",
      "line": 572,
      "side": "RIGHT",
      "diff_hunk": "@@ -552,21 +552,21 @@ def construct_change_message(form, formsets, add):\n     Translations are deactivated so that strings are stored untranslated.\n     Translation happens later on LogEntry access.\n     \"\"\"\n+    change_message = []\n+    if add:\n+        change_message.append({\"added\": {}})\n     # Evaluating `form.changed_data` prior to disabling translations is\n     # required to avoid fields affected by localization from being included\n     # incorrectly, e.g. where date formats differ such as MM/DD/YYYY vs\n     # DD/MM/YYYY.\n-    changed_data = form.changed_data\n-    with translation_override(None):\n-        # Deactivate translations while fetching verbose_name for form\n-        # field labels and using `field_name`, if verbose_name is not provided.\n-        # Translations will happen later on LogEntry access.\n-        changed_field_labels = _get_changed_field_labels_from_form(form, changed_data)\n-\n-    change_message = []\n-    if add:\n-        change_message.append({\"added\": {}})\n-    elif form.changed_data:\n+    elif changed_data := form.changed_data:\n+        with translation_override(None):\n+            # Deactivate translations while fetching verbose_name for form\n+            # field labels and using `field_name`, if verbose_name is not\n+            # provided. Translations will happen later on LogEntry access.\n+            changed_field_labels = _get_changed_field_labels_from_form(\n+                form, changed_data\n+            )\n         change_message.append({\"changed\": {\"fields\": changed_field_labels}})\n     if formsets:\n         with translation_override(None):",
      "comment": "I can't see a way of doing that which isn't more complex than I think it's worth\u2026",
      "comment_id": 2696443224,
      "user": "adamchainz",
      "created_at": "2026-01-16T00:31:53Z",
      "url": "https://github.com/django/django/pull/20524#discussion_r2696443224"
    },
    {
      "repo": "django/django",
      "pr_number": 19478,
      "file_path": "django/db/models/sql/query.py",
      "line": 2526,
      "side": "RIGHT",
      "diff_hunk": "@@ -2519,10 +2519,13 @@ def set_values(self, fields):\n                         annotation_names.append(f)\n                         selected[f] = f\n                     elif f in self.annotations:\n-                        raise FieldError(\n-                            f\"Cannot select the '{f}' alias. Use annotate() to \"\n-                            \"promote it.\"\n-                        )\n+                        if f not in self.annotation_select:\n+                            raise FieldError(\n+                                f\"Cannot select the '{f}' alias. Use annotate() to \"\n+                                \"promote it.\"\n+                            )",
      "comment": "Does it need to have the guard on line 2522? If it gets into the block of code in lines 2521-2526, would the guard always evaluate to true, because the condition on line 2518 has already evaluated to false?",
      "comment_id": 2118815930,
      "user": "ontowhee",
      "created_at": "2025-06-01T07:02:29Z",
      "url": "https://github.com/django/django/pull/19478#discussion_r2118815930"
    },
    {
      "repo": "django/django",
      "pr_number": 19478,
      "file_path": "django/db/models/sql/query.py",
      "line": 2526,
      "side": "RIGHT",
      "diff_hunk": "@@ -2519,10 +2519,13 @@ def set_values(self, fields):\n                         annotation_names.append(f)\n                         selected[f] = f\n                     elif f in self.annotations:\n-                        raise FieldError(\n-                            f\"Cannot select the '{f}' alias. Use annotate() to \"\n-                            \"promote it.\"\n-                        )\n+                        if f not in self.annotation_select:\n+                            raise FieldError(\n+                                f\"Cannot select the '{f}' alias. Use annotate() to \"\n+                                \"promote it.\"\n+                            )",
      "comment": "I initially misunderstood the context and ended up heading in the wrong direction. I updated it by improving the error message instead.",
      "comment_id": 2694959382,
      "user": "JaeHyuckSa",
      "created_at": "2026-01-15T15:55:50Z",
      "url": "https://github.com/django/django/pull/19478#discussion_r2694959382"
    },
    {
      "repo": "django/django",
      "pr_number": 19478,
      "file_path": "django/db/models/sql/query.py",
      "line": 2526,
      "side": "RIGHT",
      "diff_hunk": "@@ -2519,10 +2519,13 @@ def set_values(self, fields):\n                         annotation_names.append(f)\n                         selected[f] = f\n                     elif f in self.annotations:\n-                        raise FieldError(\n-                            f\"Cannot select the '{f}' alias. Use annotate() to \"\n-                            \"promote it.\"\n-                        )\n+                        if f not in self.annotation_select:\n+                            raise FieldError(\n+                                f\"Cannot select the '{f}' alias. Use annotate() to \"\n+                                \"promote it.\"\n+                            )",
      "comment": "> Is the desired solution for the ticket to have a [more helpful error message](https://code.djangoproject.com/ticket/36352#comment:11)? Or is it to allow the annotation to be referenced without raising the FieldError?\r\n> Does this require a release note for the bug fix?\r\n\r\nI\u2019ve improved the error message, and I don\u2019t think this needs a documentation update, so I left that out. (It\u2019s categorized as Cleanup/optimization, not a bug.)\r\n\r\nRef. \r\n- https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/submitting-patches/#:~:text=If%20the%20code%20adds%20a%20new%20feature%2C%20or%20modifies%20the%20behavior%20of%20an%20existing%20feature%2C%20the%20change%20should%20also%20contain%20documentation.\r\n- https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/submitting-patches/#:~:text=If%20it%E2%80%99s%20a%20bug%20that%20qualifies%20for%20a%20backport%20to%20the%20stable%20version%20of%20Django%2C%20is%20there%20a%20release%20note%20in%20docs/releases/A.B.C.txt%3F%20Bug%20fixes%20that%20will%20be%20applied%20only%20to%20the%20main%20branch%20don%E2%80%99t%20need%20a%20release%20note.",
      "comment_id": 2694989640,
      "user": "JaeHyuckSa",
      "created_at": "2026-01-15T16:00:33Z",
      "url": "https://github.com/django/django/pull/19478#discussion_r2694989640"
    },
    {
      "repo": "django/django",
      "pr_number": 20461,
      "file_path": "django/db/backends/postgresql/features.py",
      "line": 11,
      "side": "RIGHT",
      "diff_hunk": "@@ -8,6 +8,7 @@\n \n class DatabaseFeatures(BaseDatabaseFeatures):\n     minimum_database_version = (15,)\n+    max_query_params = 2**16 - 1",
      "comment": "Since the limit is only present when server-side cursors are used we should make this value a `cached_property`  that introspects `self.connection.settings_dict[\"OPTIONS\"].get(\"server_side_binding\")` and return this value if its enabled and `None` otherwise.",
      "comment_id": 2647115752,
      "user": "charettes",
      "created_at": "2025-12-25T15:21:16Z",
      "url": "https://github.com/django/django/pull/20461#discussion_r2647115752"
    },
    {
      "repo": "django/django",
      "pr_number": 20461,
      "file_path": "tests/backends/postgresql/test_operations.py",
      "line": 83,
      "side": "RIGHT",
      "diff_hunk": "@@ -78,3 +78,26 @@ def test_prepare_join_on_clause_different_types(self):\n         self.assertEqual(\n             rhs_expr, Cast(Col(book_table, book_fk_field), author_id_field)\n         )\n+\n+    def test_bulk_batch_size(self):",
      "comment": "And then we should add a test with and without server-side cursors enabled.",
      "comment_id": 2647116043,
      "user": "charettes",
      "created_at": "2025-12-25T15:21:51Z",
      "url": "https://github.com/django/django/pull/20461#discussion_r2647116043"
    },
    {
      "repo": "django/django",
      "pr_number": 20461,
      "file_path": "django/db/backends/postgresql/features.py",
      "line": 11,
      "side": "RIGHT",
      "diff_hunk": "@@ -8,6 +8,7 @@\n \n class DatabaseFeatures(BaseDatabaseFeatures):\n     minimum_database_version = (15,)\n+    max_query_params = 2**16 - 1",
      "comment": "@charettes Thanks for the review! I've updated `max_query_params` to a `cached_property` that returns the limit only when `server_side_binding` is enabled.",
      "comment_id": 2648167464,
      "user": "JaeHyuckSa",
      "created_at": "2025-12-26T13:08:49Z",
      "url": "https://github.com/django/django/pull/20461#discussion_r2648167464"
    },
    {
      "repo": "django/django",
      "pr_number": 20461,
      "file_path": "tests/backends/postgresql/test_operations.py",
      "line": 83,
      "side": "RIGHT",
      "diff_hunk": "@@ -78,3 +78,26 @@ def test_prepare_join_on_clause_different_types(self):\n         self.assertEqual(\n             rhs_expr, Cast(Col(book_table, book_fk_field), author_id_field)\n         )\n+\n+    def test_bulk_batch_size(self):",
      "comment": "I've updated the test to cover both cases (with and without server-side binding).",
      "comment_id": 2648171063,
      "user": "JaeHyuckSa",
      "created_at": "2025-12-26T13:10:43Z",
      "url": "https://github.com/django/django/pull/20461#discussion_r2648171063"
    },
    {
      "repo": "django/django",
      "pr_number": 20461,
      "file_path": "django/db/backends/postgresql/operations.py",
      "line": 408,
      "side": "RIGHT",
      "diff_hunk": "@@ -403,3 +404,16 @@ def prepare_join_on_clause(self, lhs_table, lhs_field, rhs_table, rhs_field):\n             rhs_expr = Cast(rhs_expr, lhs_field)\n \n         return lhs_expr, rhs_expr\n+\n+    def bulk_batch_size(self, fields, objs):",
      "comment": "We have similar code for oracle & sqlite. I wonder if we should hoist all 3 implementations up to a single implementation on the superclass.\n\nWe might then one day implement a sane default for MySQL, which surely won't let you send a query of unbounded size even if the parameters have no strict upper limit.",
      "comment_id": 2651096279,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-29T14:28:51Z",
      "url": "https://github.com/django/django/pull/20461#discussion_r2651096279"
    },
    {
      "repo": "django/django",
      "pr_number": 20461,
      "file_path": "django/db/backends/postgresql/operations.py",
      "line": 408,
      "side": "RIGHT",
      "diff_hunk": "@@ -403,3 +404,16 @@ def prepare_join_on_clause(self, lhs_table, lhs_field, rhs_table, rhs_field):\n             rhs_expr = Cast(rhs_expr, lhs_field)\n \n         return lhs_expr, rhs_expr\n+\n+    def bulk_batch_size(self, fields, objs):",
      "comment": "I didn\u2019t realize this code was shared across backends. Refactoring it as you suggested does make things cleaner. Thanks , @jacobtylerwalls ",
      "comment_id": 2653106577,
      "user": "JaeHyuckSa",
      "created_at": "2025-12-30T14:19:10Z",
      "url": "https://github.com/django/django/pull/20461#discussion_r2653106577"
    },
    {
      "repo": "django/django",
      "pr_number": 20461,
      "file_path": "django/db/backends/postgresql/features.py",
      "line": 156,
      "side": "RIGHT",
      "diff_hunk": "@@ -151,6 +151,12 @@ def uses_server_side_binding(self):\n         options = self.connection.settings_dict[\"OPTIONS\"]\n         return is_psycopg3 and options.get(\"server_side_binding\") is True\n \n+    @cached_property\n+    def max_query_params(self):\n+        if self.connection.settings_dict[\"OPTIONS\"].get(\"server_side_binding\"):",
      "comment": "I would just delegate to `uses_server_side_binding` since the added logic is slightly different.",
      "comment_id": 2653120083,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-30T14:26:02Z",
      "url": "https://github.com/django/django/pull/20461#discussion_r2653120083"
    },
    {
      "repo": "django/django",
      "pr_number": 20461,
      "file_path": "tests/backends/postgresql/test_operations.py",
      "line": 95,
      "side": "RIGHT",
      "diff_hunk": "@@ -78,3 +79,49 @@ def test_prepare_join_on_clause_different_types(self):\n         self.assertEqual(\n             rhs_expr, Cast(Col(book_table, book_fk_field), author_id_field)\n         )\n+\n+    def test_bulk_batch_size(self):\n+        objects = range(2**16)\n+        max_query_params = 2**16 - 1\n+        first_name_field = Person._meta.get_field(\"first_name\")\n+        last_name_field = Person._meta.get_field(\"last_name\")\n+        composite_pk = models.CompositePrimaryKey(\"first_name\", \"last_name\")\n+        composite_pk.fields = [first_name_field, last_name_field]\n+\n+        self.assertEqual(connection.ops.bulk_batch_size([], objects), len(objects))\n+\n+        # Without server-side binding.\n+        with mock.patch.object(\n+            type(connection.features),\n+            \"max_query_params\",\n+            new_callable=mock.PropertyMock,\n+            return_value=None,\n+        ):",
      "comment": "Then we could just mock `uses_server_side_binding`, which I think would improve the test a little bit.",
      "comment_id": 2653134752,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-30T14:33:04Z",
      "url": "https://github.com/django/django/pull/20461#discussion_r2653134752"
    },
    {
      "repo": "django/django",
      "pr_number": 20461,
      "file_path": "tests/backends/postgresql/test_operations.py",
      "line": 89,
      "side": "RIGHT",
      "diff_hunk": "@@ -78,3 +79,49 @@ def test_prepare_join_on_clause_different_types(self):\n         self.assertEqual(\n             rhs_expr, Cast(Col(book_table, book_fk_field), author_id_field)\n         )\n+\n+    def test_bulk_batch_size(self):\n+        objects = range(2**16)\n+        max_query_params = 2**16 - 1\n+        first_name_field = Person._meta.get_field(\"first_name\")\n+        last_name_field = Person._meta.get_field(\"last_name\")\n+        composite_pk = models.CompositePrimaryKey(\"first_name\", \"last_name\")\n+        composite_pk.fields = [first_name_field, last_name_field]",
      "comment": "I'm starting to wonder if we should hoist these tests out of the oracle & sqlite tests and just have one base test. Then the postgres specific test only needs to test the postgres-specific part where we switch on server-side binding...",
      "comment_id": 2653144535,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-30T14:38:04Z",
      "url": "https://github.com/django/django/pull/20461#discussion_r2653144535"
    },
    {
      "repo": "django/django",
      "pr_number": 20461,
      "file_path": "tests/backends/postgresql/test_operations.py",
      "line": 127,
      "side": "RIGHT",
      "diff_hunk": "@@ -78,3 +79,49 @@ def test_prepare_join_on_clause_different_types(self):\n         self.assertEqual(\n             rhs_expr, Cast(Col(book_table, book_fk_field), author_id_field)\n         )\n+\n+    def test_bulk_batch_size(self):\n+        objects = range(2**16)\n+        max_query_params = 2**16 - 1\n+        first_name_field = Person._meta.get_field(\"first_name\")\n+        last_name_field = Person._meta.get_field(\"last_name\")\n+        composite_pk = models.CompositePrimaryKey(\"first_name\", \"last_name\")\n+        composite_pk.fields = [first_name_field, last_name_field]\n+\n+        self.assertEqual(connection.ops.bulk_batch_size([], objects), len(objects))\n+\n+        # Without server-side binding.\n+        with mock.patch.object(\n+            type(connection.features),\n+            \"max_query_params\",\n+            new_callable=mock.PropertyMock,\n+            return_value=None,\n+        ):\n+            self.assertEqual(\n+                connection.ops.bulk_batch_size([first_name_field], objects),\n+                len(objects),\n+            )\n+\n+        # With server-side binding.\n+        with mock.patch.object(\n+            type(connection.features),\n+            \"max_query_params\",\n+            new_callable=mock.PropertyMock,\n+            return_value=max_query_params,\n+        ):\n+            self.assertEqual(\n+                connection.ops.bulk_batch_size([first_name_field], objects),\n+                max_query_params,\n+            )\n+            self.assertEqual(\n+                connection.ops.bulk_batch_size(\n+                    [first_name_field, last_name_field], objects\n+                ),\n+                max_query_params // 2,\n+            )\n+            self.assertEqual(\n+                connection.ops.bulk_batch_size(\n+                    [composite_pk, first_name_field], objects\n+                ),\n+                max_query_params // 3,\n+            )",
      "comment": "... rather than reiterate all of these assertions.",
      "comment_id": 2653145712,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-30T14:38:46Z",
      "url": "https://github.com/django/django/pull/20461#discussion_r2653145712"
    },
    {
      "repo": "django/django",
      "pr_number": 20461,
      "file_path": "django/db/backends/base/operations.py",
      "line": 85,
      "side": "RIGHT",
      "diff_hunk": "@@ -78,7 +79,19 @@ def bulk_batch_size(self, fields, objs):\n         are the fields going to be inserted in the batch, the objs contains\n         all the objects to be inserted.\n         \"\"\"\n-        return len(objs)\n+        if self.connection.features.max_query_params is None or not fields:\n+            return len(objs)\n+\n+        from django.db.models import CompositePrimaryKey",
      "comment": "I initially added this because I thought it was causing a circular import, but I had mixed it up with another file. I\u2019ve now moved the import to the top.",
      "comment_id": 2653163511,
      "user": "JaeHyuckSa",
      "created_at": "2025-12-30T14:48:22Z",
      "url": "https://github.com/django/django/pull/20461#discussion_r2653163511"
    },
    {
      "repo": "django/django",
      "pr_number": 20461,
      "file_path": "tests/composite_pk/tests.py",
      "line": 153,
      "side": "RIGHT",
      "diff_hunk": "@@ -149,14 +149,14 @@ def test_in_bulk(self):\n \n     def test_in_bulk_batching(self):\n         Comment.objects.all().delete()\n-        batching_required = connection.features.max_query_params is not None\n-        expected_queries = 2 if batching_required else 1\n+        connection.features.__dict__.pop(\"max_query_params\", None)",
      "comment": "Can you tell me a little bit about this change? Was there a test isolation problem? What is the state of `connection.features` after this test?",
      "comment_id": 2653168624,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-30T14:51:01Z",
      "url": "https://github.com/django/django/pull/20461#discussion_r2653168624"
    },
    {
      "repo": "django/django",
      "pr_number": 20461,
      "file_path": "tests/composite_pk/tests.py",
      "line": 153,
      "side": "RIGHT",
      "diff_hunk": "@@ -149,14 +149,14 @@ def test_in_bulk(self):\n \n     def test_in_bulk_batching(self):\n         Comment.objects.all().delete()\n-        batching_required = connection.features.max_query_params is not None\n-        expected_queries = 2 if batching_required else 1\n+        connection.features.__dict__.pop(\"max_query_params\", None)",
      "comment": "This came up because `max_query_params` was changed to a `cached_property`. Once it calculates a value, it stores it in `__dict__` and just uses that stored value from then on. So when you try to mock the value in tests, if there's already a cached value, the mock won't take effect. You need to clear the cache with `.pop()` before applying the mock for it to work properly, which is why I made that change",
      "comment_id": 2691162416,
      "user": "JaeHyuckSa",
      "created_at": "2026-01-14T16:27:29Z",
      "url": "https://github.com/django/django/pull/20461#discussion_r2691162416"
    },
    {
      "repo": "django/django",
      "pr_number": 20461,
      "file_path": "tests/backends/base/test_operations.py",
      "line": 227,
      "side": "RIGHT",
      "diff_hunk": "@@ -234,3 +222,38 @@ def test_execute_sql_flush_statements(self):\n                 self.assertEqual(author.pk, 1)\n                 book = Book.objects.create(author=author)\n                 self.assertEqual(book.pk, 1)\n+\n+\n+class BulkBatchSizeMixin:",
      "comment": "I tweaked the test so it could still run on MySQL, allowing me to move this test into one of the above test cases and remove the mixin \ud83d\udc4d ",
      "comment_id": 2694225682,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-15T12:39:20Z",
      "url": "https://github.com/django/django/pull/20461#discussion_r2694225682"
    },
    {
      "repo": "django/django",
      "pr_number": 20461,
      "file_path": "tests/backends/postgresql/test_operations.py",
      "line": 95,
      "side": "RIGHT",
      "diff_hunk": "@@ -78,3 +79,49 @@ def test_prepare_join_on_clause_different_types(self):\n         self.assertEqual(\n             rhs_expr, Cast(Col(book_table, book_fk_field), author_id_field)\n         )\n+\n+    def test_bulk_batch_size(self):\n+        objects = range(2**16)\n+        max_query_params = 2**16 - 1\n+        first_name_field = Person._meta.get_field(\"first_name\")\n+        last_name_field = Person._meta.get_field(\"last_name\")\n+        composite_pk = models.CompositePrimaryKey(\"first_name\", \"last_name\")\n+        composite_pk.fields = [first_name_field, last_name_field]\n+\n+        self.assertEqual(connection.ops.bulk_batch_size([], objects), len(objects))\n+\n+        # Without server-side binding.\n+        with mock.patch.object(\n+            type(connection.features),\n+            \"max_query_params\",\n+            new_callable=mock.PropertyMock,\n+            return_value=None,\n+        ):",
      "comment": "Now that the tests run on all backends, I think we no longer need this test.",
      "comment_id": 2694263000,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-15T12:50:31Z",
      "url": "https://github.com/django/django/pull/20461#discussion_r2694263000"
    },
    {
      "repo": "django/django",
      "pr_number": 20461,
      "file_path": "tests/backends/postgresql/test_operations.py",
      "line": 95,
      "side": "RIGHT",
      "diff_hunk": "@@ -78,3 +79,49 @@ def test_prepare_join_on_clause_different_types(self):\n         self.assertEqual(\n             rhs_expr, Cast(Col(book_table, book_fk_field), author_id_field)\n         )\n+\n+    def test_bulk_batch_size(self):\n+        objects = range(2**16)\n+        max_query_params = 2**16 - 1\n+        first_name_field = Person._meta.get_field(\"first_name\")\n+        last_name_field = Person._meta.get_field(\"last_name\")\n+        composite_pk = models.CompositePrimaryKey(\"first_name\", \"last_name\")\n+        composite_pk.fields = [first_name_field, last_name_field]\n+\n+        self.assertEqual(connection.ops.bulk_batch_size([], objects), len(objects))\n+\n+        # Without server-side binding.\n+        with mock.patch.object(\n+            type(connection.features),\n+            \"max_query_params\",\n+            new_callable=mock.PropertyMock,\n+            return_value=None,\n+        ):",
      "comment": "(Actually, just to ensure we had _something_ I replaced it with a simpler one. Our CI configuration tests with server side params enabled & not enabled.)",
      "comment_id": 2694443445,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-15T13:42:35Z",
      "url": "https://github.com/django/django/pull/20461#discussion_r2694443445"
    },
    {
      "repo": "django/django",
      "pr_number": 20461,
      "file_path": "tests/backends/base/test_operations.py",
      "line": 227,
      "side": "RIGHT",
      "diff_hunk": "@@ -234,3 +222,38 @@ def test_execute_sql_flush_statements(self):\n                 self.assertEqual(author.pk, 1)\n                 book = Book.objects.create(author=author)\n                 self.assertEqual(book.pk, 1)\n+\n+\n+class BulkBatchSizeMixin:",
      "comment": "Thank you for letting me know. I didn\u2019t realize there was a way to do that. I hadn\u2019t thought of writing it in SimpleDatabaseOperationTests.",
      "comment_id": 2694499746,
      "user": "JaeHyuckSa",
      "created_at": "2026-01-15T13:58:02Z",
      "url": "https://github.com/django/django/pull/20461#discussion_r2694499746"
    },
    {
      "repo": "django/django",
      "pr_number": 20460,
      "file_path": "tests/queries/tests.py",
      "line": 2284,
      "side": "RIGHT",
      "diff_hunk": "@@ -2280,6 +2280,17 @@ def test_ticket8597(self):\n             [item_ab],\n         )\n \n+    @skipUnlessDBFeature(\"interprets_empty_strings_as_nulls\")\n+    def test_empty_string_iexact_lookup(self):",
      "comment": "Thanks, this location next to case-insensitive tests makes sense, but we could also move it to `NullQueriesTests`. What do you prefer?",
      "comment_id": 2683458860,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-12T18:41:26Z",
      "url": "https://github.com/django/django/pull/20460#discussion_r2683458860"
    },
    {
      "repo": "django/django",
      "pr_number": 20460,
      "file_path": "tests/queries/tests.py",
      "line": 2284,
      "side": "RIGHT",
      "diff_hunk": "@@ -2280,6 +2280,17 @@ def test_ticket8597(self):\n             [item_ab],\n         )\n \n+    @skipUnlessDBFeature(\"interprets_empty_strings_as_nulls\")\n+    def test_empty_string_iexact_lookup(self):",
      "comment": "Thanks for the suggestion! I'll move it to `NullQueriesTests`.  Since both `None` and empty strings are converted to `IS NULL`,  it makes sense to keep them together.",
      "comment_id": 2690832225,
      "user": "JaeHyuckSa",
      "created_at": "2026-01-14T15:05:08Z",
      "url": "https://github.com/django/django/pull/20460#discussion_r2690832225"
    },
    {
      "repo": "django/django",
      "pr_number": 20519,
      "file_path": "django/db/backends/base/creation.py",
      "line": 364,
      "side": "RIGHT",
      "diff_hunk": "@@ -361,24 +361,43 @@ def mark_expected_failures_and_skips(self):\n         # Only load unittest if we're actually testing.\n         from unittest import expectedFailure, skip\n \n+        def get_test(module_or_class, class_or_method, test_name):",
      "comment": "There is no memory leak when using an inline method in this case?",
      "comment_id": 2677999156,
      "user": "timgraham",
      "created_at": "2026-01-10T00:20:18Z",
      "url": "https://github.com/django/django/pull/20519#discussion_r2677999156"
    },
    {
      "repo": "django/django",
      "pr_number": 20519,
      "file_path": "tests/backends/base/test_creation.py",
      "line": 337,
      "side": "RIGHT",
      "diff_hunk": "@@ -333,6 +334,13 @@ def test_mark_expected_failures_and_skips(self):\n                 \"backends.base.test_creation.skip_test_function\",\n             },\n         }\n+",
      "comment": "I prefer not to add blank lines under the guideline that comments are better to separate any blocks that would otherwise seem to need blank lines.",
      "comment_id": 2678002877,
      "user": "timgraham",
      "created_at": "2026-01-10T00:24:21Z",
      "url": "https://github.com/django/django/pull/20519#discussion_r2678002877"
    },
    {
      "repo": "django/django",
      "pr_number": 20519,
      "file_path": "tests/backends/base/test_creation.py",
      "line": 340,
      "side": "RIGHT",
      "diff_hunk": "@@ -333,6 +334,13 @@ def test_mark_expected_failures_and_skips(self):\n                 \"backends.base.test_creation.skip_test_function\",\n             },\n         }\n+\n+        # Emulate the scenario where the parent module for\n+        # backends.base.test_creation has not been imported yet.\n+        # This case has been problematic.",
      "comment": "As all bugs are ;-) (I think this sentence isn't needed.)",
      "comment_id": 2678003330,
      "user": "timgraham",
      "created_at": "2026-01-10T00:24:53Z",
      "url": "https://github.com/django/django/pull/20519#discussion_r2678003330"
    },
    {
      "repo": "django/django",
      "pr_number": 20519,
      "file_path": "django/db/backends/base/creation.py",
      "line": 368,
      "side": "RIGHT",
      "diff_hunk": "@@ -361,24 +361,43 @@ def mark_expected_failures_and_skips(self):\n         # Only load unittest if we're actually testing.\n         from unittest import expectedFailure, skip\n \n+        def get_test(module_or_class, class_or_method, test_name):\n+            try:\n+                test_case = import_string(module_or_class)\n+            except ImportError:\n+                # If import_string couldn't chop the rightmost term",
      "comment": "I think it's more important to say something high-level like \"When running a subset of tests in a module, tests in that module's other submodules won't be imported by the test runner and must be manually imported to avoid a crash. The ImportError could be ignored but the second import catches nonexistent tests.\" I leave it to you about whether or not to include the implementation details you've described but it seems like it may be uneeded archaic details (about the implementation of `import_string()`, for example). ",
      "comment_id": 2678004314,
      "user": "timgraham",
      "created_at": "2026-01-10T00:26:03Z",
      "url": "https://github.com/django/django/pull/20519#discussion_r2678004314"
    },
    {
      "repo": "django/django",
      "pr_number": 20519,
      "file_path": "django/db/backends/base/creation.py",
      "line": 369,
      "side": "RIGHT",
      "diff_hunk": "@@ -361,24 +361,43 @@ def mark_expected_failures_and_skips(self):\n         # Only load unittest if we're actually testing.\n         from unittest import expectedFailure, skip\n \n+        def get_test(module_or_class, class_or_method, test_name):\n+            try:\n+                test_case = import_string(module_or_class)\n+            except ImportError:\n+                # If import_string couldn't chop the rightmost term\n+                # and getattr() it from the remainder, it's likely that",
      "comment": "The comment wrapping looks too short to me (less than 79 characters.)",
      "comment_id": 2678010177,
      "user": "timgraham",
      "created_at": "2026-01-10T00:32:11Z",
      "url": "https://github.com/django/django/pull/20519#discussion_r2678010177"
    },
    {
      "repo": "django/django",
      "pr_number": 20519,
      "file_path": "django/db/backends/base/creation.py",
      "line": 374,
      "side": "RIGHT",
      "diff_hunk": "@@ -361,24 +361,43 @@ def mark_expected_failures_and_skips(self):\n         # Only load unittest if we're actually testing.\n         from unittest import expectedFailure, skip\n \n+        def get_test(module_or_class, class_or_method, test_name):\n+            try:\n+                test_case = import_string(module_or_class)\n+            except ImportError:\n+                # If import_string couldn't chop the rightmost term\n+                # and getattr() it from the remainder, it's likely that\n+                # the rightmost term was a submodule, and the remainder\n+                # is a module that hasn't been imported yet.\n+                # Import the test_method by importing test_name.\n+                test_method = import_string(test_name)\n+                test_case = sys.modules.get(test_method.__module__)",
      "comment": "Oh good point. Let me look a little closer next week.",
      "comment_id": 2678202475,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-10T02:18:40Z",
      "url": "https://github.com/django/django/pull/20519#discussion_r2678202475"
    },
    {
      "repo": "django/django",
      "pr_number": 20519,
      "file_path": "django/db/backends/base/creation.py",
      "line": 364,
      "side": "RIGHT",
      "diff_hunk": "@@ -361,24 +361,43 @@ def mark_expected_failures_and_skips(self):\n         # Only load unittest if we're actually testing.\n         from unittest import expectedFailure, skip\n \n+        def get_test(module_or_class, class_or_method, test_name):",
      "comment": "This gets called only once per test worker, so I don't think it's a meaningful number of objects. I just didn't want to clutter the module scope. Maybe I'll end up needing to hoist it up to the module to target it in a test.\r\n\r\nIt was a little early to request your review, but I'm glad I did as I need to look into your comment about unnecessary decoration.",
      "comment_id": 2678210208,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-10T02:23:34Z",
      "url": "https://github.com/django/django/pull/20519#discussion_r2678210208"
    },
    {
      "repo": "django/django",
      "pr_number": 20519,
      "file_path": "django/db/backends/base/creation.py",
      "line": 368,
      "side": "RIGHT",
      "diff_hunk": "@@ -361,24 +361,43 @@ def mark_expected_failures_and_skips(self):\n         # Only load unittest if we're actually testing.\n         from unittest import expectedFailure, skip\n \n+        def get_test(module_or_class, class_or_method, test_name):\n+            try:\n+                test_case = import_string(module_or_class)\n+            except ImportError:\n+                # If import_string couldn't chop the rightmost term",
      "comment": "You wrote \"import_string() sometimes raises ImportError given a module.\" However, I think the comment should give an example of what \"sometimes\" means.",
      "comment_id": 2683850272,
      "user": "timgraham",
      "created_at": "2026-01-12T20:44:09Z",
      "url": "https://github.com/django/django/pull/20519#discussion_r2683850272"
    },
    {
      "repo": "django/django",
      "pr_number": 20519,
      "file_path": "django/db/backends/base/creation.py",
      "line": 368,
      "side": "RIGHT",
      "diff_hunk": "@@ -361,24 +361,43 @@ def mark_expected_failures_and_skips(self):\n         # Only load unittest if we're actually testing.\n         from unittest import expectedFailure, skip\n \n+        def get_test(module_or_class, class_or_method, test_name):\n+            try:\n+                test_case = import_string(module_or_class)\n+            except ImportError:\n+                # If import_string couldn't chop the rightmost term",
      "comment": "Ah right. Let me know if [eff69c8](https://github.com/django/django/pull/20519/commits/eff69c8fdf351ef4612cbc1782e01be274176d25) still doesn't strike the right balance.",
      "comment_id": 2684060029,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-12T22:03:09Z",
      "url": "https://github.com/django/django/pull/20519#discussion_r2684060029"
    },
    {
      "repo": "django/django",
      "pr_number": 20519,
      "file_path": "django/db/backends/base/creation.py",
      "line": 371,
      "side": "RIGHT",
      "diff_hunk": "@@ -361,24 +361,46 @@ def mark_expected_failures_and_skips(self):\n         # Only load unittest if we're actually testing.\n         from unittest import expectedFailure, skip\n \n+        def get_test(module_or_class, class_or_method, test_name):\n+            try:\n+                test_frame = import_string(module_or_class)\n+            except ImportError:\n+                # When given a submodule, import_string() can raise ImportError\n+                # if its parent module has not been already imported during\n+                # test discovery. Since the parallel test runner spawns workers\n+                # with a fresh import cache and then applies these skips, do",
      "comment": "\"do not expect\" seems like a verbiage change from the previous sentence. Here's my suggestion:\r\n\r\nimport_string() can raise ImportError if a submodule's parent module hasn't already been imported during test discovery. This can happen in at least two cases:\r\n1. When running a subset of tests in a module, tests in that module's other submodules won't be imported by the test runner.\r\n2. When the parallel test runner spawns workers with an empty import cache.\r\n\r\nPlease correct any details that I got wrong.\r\n",
      "comment_id": 2684070759,
      "user": "timgraham",
      "created_at": "2026-01-12T22:07:39Z",
      "url": "https://github.com/django/django/pull/20519#discussion_r2684070759"
    },
    {
      "repo": "django/django",
      "pr_number": 20519,
      "file_path": "django/db/backends/base/creation.py",
      "line": 370,
      "side": "RIGHT",
      "diff_hunk": "@@ -361,24 +361,46 @@ def mark_expected_failures_and_skips(self):\n         # Only load unittest if we're actually testing.\n         from unittest import expectedFailure, skip\n \n+        def get_test(module_or_class, class_or_method, test_name):\n+            try:\n+                test_frame = import_string(module_or_class)\n+            except ImportError:\n+                # When given a submodule, import_string() can raise ImportError\n+                # if its parent module has not been already imported during\n+                # test discovery. Since the parallel test runner spawns workers",
      "comment": "I didn't know about the problem for the parallel test runner, and I think it's important to also describe the case I mentioned where it's not involved.",
      "comment_id": 2684073892,
      "user": "timgraham",
      "created_at": "2026-01-12T22:08:53Z",
      "url": "https://github.com/django/django/pull/20519#discussion_r2684073892"
    },
    {
      "repo": "django/django",
      "pr_number": 20519,
      "file_path": "django/db/backends/base/creation.py",
      "line": 393,
      "side": "RIGHT",
      "diff_hunk": "@@ -361,24 +361,46 @@ def mark_expected_failures_and_skips(self):\n         # Only load unittest if we're actually testing.\n         from unittest import expectedFailure, skip\n \n+        def get_test(module_or_class, class_or_method, test_name):\n+            try:\n+                test_frame = import_string(module_or_class)\n+            except ImportError:\n+                # When given a submodule, import_string() can raise ImportError\n+                # if its parent module has not been already imported during\n+                # test discovery. Since the parallel test runner spawns workers\n+                # with a fresh import cache and then applies these skips, do\n+                # not expect import_string() to succeed for submodules.\n+                # test_name is likely a class, so import that instead.\n+                test_to_skip = import_string(test_name)\n+                test_frame = sys.modules.get(test_to_skip.__module__)\n+            else:\n+                test_to_skip = getattr(test_frame, class_or_method)\n+            return test_frame, test_to_skip\n+\n         for test_name in self.connection.features.django_test_expected_failures:\n-            test_case_name, _, test_method_name = test_name.rpartition(\".\")\n+            module_or_class_name, _, class_or_method_name = test_name.rpartition(\".\")\n             test_app = test_name.split(\".\")[0]\n             # Importing a test app that isn't installed raises RuntimeError.\n             if test_app in settings.INSTALLED_APPS:\n-                test_case = import_string(test_case_name)\n-                test_method = getattr(test_case, test_method_name)\n-                setattr(test_case, test_method_name, expectedFailure(test_method))\n+                test_frame, test_to_skip = get_test(\n+                    module_or_class_name, class_or_method_name, test_name\n+                )\n+                setattr(test_frame, class_or_method_name, expectedFailure(test_to_skip))\n         for reason, tests in self.connection.features.django_test_skips.items():\n             for test_name in tests:\n-                test_case_name, _, test_method_name = test_name.rpartition(\".\")\n+                module_or_class_name, _, class_or_method_name = test_name.rpartition(\n+                    \".\"\n+                )",
      "comment": "I hate this formatting and how other lines have to be multi-lined. I don't normally advocate for abbreviations, but I'm a bit tempted to abbreviate \"class\" -> \"cls\" if it avoids that.",
      "comment_id": 2684105502,
      "user": "timgraham",
      "created_at": "2026-01-12T22:21:54Z",
      "url": "https://github.com/django/django/pull/20519#discussion_r2684105502"
    },
    {
      "repo": "django/django",
      "pr_number": 20519,
      "file_path": "django/db/backends/base/creation.py",
      "line": 393,
      "side": "RIGHT",
      "diff_hunk": "@@ -361,24 +361,46 @@ def mark_expected_failures_and_skips(self):\n         # Only load unittest if we're actually testing.\n         from unittest import expectedFailure, skip\n \n+        def get_test(module_or_class, class_or_method, test_name):\n+            try:\n+                test_frame = import_string(module_or_class)\n+            except ImportError:\n+                # When given a submodule, import_string() can raise ImportError\n+                # if its parent module has not been already imported during\n+                # test discovery. Since the parallel test runner spawns workers\n+                # with a fresh import cache and then applies these skips, do\n+                # not expect import_string() to succeed for submodules.\n+                # test_name is likely a class, so import that instead.\n+                test_to_skip = import_string(test_name)\n+                test_frame = sys.modules.get(test_to_skip.__module__)\n+            else:\n+                test_to_skip = getattr(test_frame, class_or_method)\n+            return test_frame, test_to_skip\n+\n         for test_name in self.connection.features.django_test_expected_failures:\n-            test_case_name, _, test_method_name = test_name.rpartition(\".\")\n+            module_or_class_name, _, class_or_method_name = test_name.rpartition(\".\")\n             test_app = test_name.split(\".\")[0]\n             # Importing a test app that isn't installed raises RuntimeError.\n             if test_app in settings.INSTALLED_APPS:\n-                test_case = import_string(test_case_name)\n-                test_method = getattr(test_case, test_method_name)\n-                setattr(test_case, test_method_name, expectedFailure(test_method))\n+                test_frame, test_to_skip = get_test(\n+                    module_or_class_name, class_or_method_name, test_name\n+                )\n+                setattr(test_frame, class_or_method_name, expectedFailure(test_to_skip))\n         for reason, tests in self.connection.features.django_test_skips.items():\n             for test_name in tests:\n-                test_case_name, _, test_method_name = test_name.rpartition(\".\")\n+                module_or_class_name, _, class_or_method_name = test_name.rpartition(\n+                    \".\"\n+                )",
      "comment": "I could also parenthesize the left-hand side. Fully agree about the lonely `\".\"`",
      "comment_id": 2684338072,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-13T00:16:14Z",
      "url": "https://github.com/django/django/pull/20519#discussion_r2684338072"
    },
    {
      "repo": "django/django",
      "pr_number": 20519,
      "file_path": "django/db/backends/base/creation.py",
      "line": 371,
      "side": "RIGHT",
      "diff_hunk": "@@ -361,24 +361,46 @@ def mark_expected_failures_and_skips(self):\n         # Only load unittest if we're actually testing.\n         from unittest import expectedFailure, skip\n \n+        def get_test(module_or_class, class_or_method, test_name):\n+            try:\n+                test_frame = import_string(module_or_class)\n+            except ImportError:\n+                # When given a submodule, import_string() can raise ImportError\n+                # if its parent module has not been already imported during\n+                # test discovery. Since the parallel test runner spawns workers\n+                # with a fresh import cache and then applies these skips, do",
      "comment": "I like this. I applied your suggestion and removed the passive voice to fit on one fewer line.",
      "comment_id": 2687229683,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-13T16:45:13Z",
      "url": "https://github.com/django/django/pull/20519#discussion_r2687229683"
    },
    {
      "repo": "django/django",
      "pr_number": 20519,
      "file_path": "django/db/backends/base/creation.py",
      "line": 393,
      "side": "RIGHT",
      "diff_hunk": "@@ -361,24 +361,46 @@ def mark_expected_failures_and_skips(self):\n         # Only load unittest if we're actually testing.\n         from unittest import expectedFailure, skip\n \n+        def get_test(module_or_class, class_or_method, test_name):\n+            try:\n+                test_frame = import_string(module_or_class)\n+            except ImportError:\n+                # When given a submodule, import_string() can raise ImportError\n+                # if its parent module has not been already imported during\n+                # test discovery. Since the parallel test runner spawns workers\n+                # with a fresh import cache and then applies these skips, do\n+                # not expect import_string() to succeed for submodules.\n+                # test_name is likely a class, so import that instead.\n+                test_to_skip = import_string(test_name)\n+                test_frame = sys.modules.get(test_to_skip.__module__)\n+            else:\n+                test_to_skip = getattr(test_frame, class_or_method)\n+            return test_frame, test_to_skip\n+\n         for test_name in self.connection.features.django_test_expected_failures:\n-            test_case_name, _, test_method_name = test_name.rpartition(\".\")\n+            module_or_class_name, _, class_or_method_name = test_name.rpartition(\".\")\n             test_app = test_name.split(\".\")[0]\n             # Importing a test app that isn't installed raises RuntimeError.\n             if test_app in settings.INSTALLED_APPS:\n-                test_case = import_string(test_case_name)\n-                test_method = getattr(test_case, test_method_name)\n-                setattr(test_case, test_method_name, expectedFailure(test_method))\n+                test_frame, test_to_skip = get_test(\n+                    module_or_class_name, class_or_method_name, test_name\n+                )\n+                setattr(test_frame, class_or_method_name, expectedFailure(test_to_skip))\n         for reason, tests in self.connection.features.django_test_skips.items():\n             for test_name in tests:\n-                test_case_name, _, test_method_name = test_name.rpartition(\".\")\n+                module_or_class_name, _, class_or_method_name = test_name.rpartition(\n+                    \".\"\n+                )",
      "comment": "Took the opportunity to factor out helper methods to address this and also remove nested functions at the same time \ud83d\udc4d ",
      "comment_id": 2687231817,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-13T16:45:48Z",
      "url": "https://github.com/django/django/pull/20519#discussion_r2687231817"
    },
    {
      "repo": "django/django",
      "pr_number": 20519,
      "file_path": "django/db/backends/base/creation.py",
      "line": 377,
      "side": "RIGHT",
      "diff_hunk": "@@ -353,32 +353,53 @@ def _destroy_test_db(self, test_database_name, verbosity):\n                 \"DROP DATABASE %s\" % self.connection.ops.quote_name(test_database_name)\n             )\n \n-    def mark_expected_failures_and_skips(self):\n+    @staticmethod\n+    def _get_test_and_frame(module_or_class, class_or_method, test_name):\n         \"\"\"\n-        Mark tests in Django's test suite which are expected failures on this\n-        database and test which should be skipped on this database.\n+        Import and return a test to skip and the module or class containing it.\n         \"\"\"\n+        try:\n+            test_frame = import_string(module_or_class)\n+        except ImportError:\n+            # import_string() can raise ImportError if a submodule's parent\n+            # module hasn't already been imported during test discovery.\n+            # This can happen in at least two cases:\n+            # 1. When running a subset of tests in a module, the test runner\n+            #    won't import tests in that module's other submodules.\n+            # 2. When the parallel test runner spawns workers with an empty\n+            #    import cache.\n+            test_to_skip = import_string(test_name)\n+            test_frame = sys.modules.get(test_to_skip.__module__)\n+        else:\n+            test_to_skip = getattr(test_frame, class_or_method)\n+        return test_frame, test_to_skip\n+\n+    def _skip_test(self, test_name, reason=None):",
      "comment": "skip -> mark (since \"mark\" could be skip or expected failure)\r\nreason -> skip_reason",
      "comment_id": 2687727509,
      "user": "timgraham",
      "created_at": "2026-01-13T19:08:52Z",
      "url": "https://github.com/django/django/pull/20519#discussion_r2687727509"
    },
    {
      "repo": "django/django",
      "pr_number": 20519,
      "file_path": "django/db/backends/base/creation.py",
      "line": 393,
      "side": "RIGHT",
      "diff_hunk": "@@ -353,32 +353,53 @@ def _destroy_test_db(self, test_database_name, verbosity):\n                 \"DROP DATABASE %s\" % self.connection.ops.quote_name(test_database_name)\n             )\n \n-    def mark_expected_failures_and_skips(self):\n+    @staticmethod\n+    def _get_test_and_frame(module_or_class, class_or_method, test_name):\n         \"\"\"\n-        Mark tests in Django's test suite which are expected failures on this\n-        database and test which should be skipped on this database.\n+        Import and return a test to skip and the module or class containing it.\n         \"\"\"\n+        try:\n+            test_frame = import_string(module_or_class)\n+        except ImportError:\n+            # import_string() can raise ImportError if a submodule's parent\n+            # module hasn't already been imported during test discovery.\n+            # This can happen in at least two cases:\n+            # 1. When running a subset of tests in a module, the test runner\n+            #    won't import tests in that module's other submodules.\n+            # 2. When the parallel test runner spawns workers with an empty\n+            #    import cache.\n+            test_to_skip = import_string(test_name)\n+            test_frame = sys.modules.get(test_to_skip.__module__)\n+        else:\n+            test_to_skip = getattr(test_frame, class_or_method)\n+        return test_frame, test_to_skip\n+\n+    def _skip_test(self, test_name, reason=None):\n         # Only load unittest if we're actually testing.\n         from unittest import expectedFailure, skip\n \n+        module_or_class_name, _, name_to_skip = test_name.rpartition(\".\")\n+        test_app = test_name.split(\".\")[0]\n+        # Importing a test app that isn't installed raises RuntimeError.\n+        if test_app in settings.INSTALLED_APPS:\n+            test_frame, test_to_skip = self._get_test_and_frame(\n+                module_or_class_name, name_to_skip, test_name\n+            )\n+            if reason:\n+                setattr(test_frame, name_to_skip, skip(reason)(test_to_skip))\n+            else:\n+                setattr(test_frame, name_to_skip, expectedFailure(test_to_skip))\n+\n+    def mark_expected_failures_and_skips(self):",
      "comment": "Perhaps it's better to follow the pattern in the rest of this file (at least with destroy_test_db/_destroy_test_db) and put the helper methods after the method that calls them. Otherwise, it seems to give more prominence to the helpers.",
      "comment_id": 2687750804,
      "user": "timgraham",
      "created_at": "2026-01-13T19:13:42Z",
      "url": "https://github.com/django/django/pull/20519#discussion_r2687750804"
    },
    {
      "repo": "django/django",
      "pr_number": 20519,
      "file_path": "django/db/backends/base/creation.py",
      "line": 368,
      "side": "RIGHT",
      "diff_hunk": "@@ -358,27 +358,48 @@ def mark_expected_failures_and_skips(self):\n         Mark tests in Django's test suite which are expected failures on this\n         database and test which should be skipped on this database.\n         \"\"\"\n-        # Only load unittest if we're actually testing.\n-        from unittest import expectedFailure, skip\n-\n         for test_name in self.connection.features.django_test_expected_failures:\n-            test_case_name, _, test_method_name = test_name.rpartition(\".\")\n-            test_app = test_name.split(\".\")[0]\n-            # Importing a test app that isn't installed raises RuntimeError.\n-            if test_app in settings.INSTALLED_APPS:\n-                test_case = import_string(test_case_name)\n-                test_method = getattr(test_case, test_method_name)\n-                setattr(test_case, test_method_name, expectedFailure(test_method))\n+            self._mark_test(test_name)\n         for reason, tests in self.connection.features.django_test_skips.items():\n             for test_name in tests:\n-                test_case_name, _, test_method_name = test_name.rpartition(\".\")\n-                test_app = test_name.split(\".\")[0]\n-                # Importing a test app that isn't installed raises\n-                # RuntimeError.\n-                if test_app in settings.INSTALLED_APPS:\n-                    test_case = import_string(test_case_name)\n-                    test_method = getattr(test_case, test_method_name)\n-                    setattr(test_case, test_method_name, skip(reason)(test_method))\n+                self._mark_test(test_name, reason)\n+\n+    def _mark_test(self, test_name, skip_reason=None):\n+        # Only load unittest if we're actually testing.",
      "comment": "We could clean this up to follow comment guidelines (no \"we\"):  \"if we're actually\" -> \"during\"",
      "comment_id": 2688397807,
      "user": "timgraham",
      "created_at": "2026-01-13T23:03:09Z",
      "url": "https://github.com/django/django/pull/20519#discussion_r2688397807"
    },
    {
      "repo": "django/django",
      "pr_number": 20519,
      "file_path": "django/db/backends/base/creation.py",
      "line": 377,
      "side": "RIGHT",
      "diff_hunk": "@@ -358,27 +358,48 @@ def mark_expected_failures_and_skips(self):\n         Mark tests in Django's test suite which are expected failures on this\n         database and test which should be skipped on this database.\n         \"\"\"\n-        # Only load unittest if we're actually testing.\n-        from unittest import expectedFailure, skip\n-\n         for test_name in self.connection.features.django_test_expected_failures:\n-            test_case_name, _, test_method_name = test_name.rpartition(\".\")\n-            test_app = test_name.split(\".\")[0]\n-            # Importing a test app that isn't installed raises RuntimeError.\n-            if test_app in settings.INSTALLED_APPS:\n-                test_case = import_string(test_case_name)\n-                test_method = getattr(test_case, test_method_name)\n-                setattr(test_case, test_method_name, expectedFailure(test_method))\n+            self._mark_test(test_name)\n         for reason, tests in self.connection.features.django_test_skips.items():\n             for test_name in tests:\n-                test_case_name, _, test_method_name = test_name.rpartition(\".\")\n-                test_app = test_name.split(\".\")[0]\n-                # Importing a test app that isn't installed raises\n-                # RuntimeError.\n-                if test_app in settings.INSTALLED_APPS:\n-                    test_case = import_string(test_case_name)\n-                    test_method = getattr(test_case, test_method_name)\n-                    setattr(test_case, test_method_name, skip(reason)(test_method))\n+                self._mark_test(test_name, reason)\n+\n+    def _mark_test(self, test_name, skip_reason=None):\n+        # Only load unittest if we're actually testing.\n+        from unittest import expectedFailure, skip\n+\n+        module_or_class_name, _, name_to_mark = test_name.rpartition(\".\")\n+        test_app = test_name.split(\".\")[0]\n+        # Importing a test app that isn't installed raises RuntimeError.\n+        if test_app in settings.INSTALLED_APPS:\n+            test_frame, test_to_mark = self._get_test_and_frame(\n+                module_or_class_name, name_to_mark, test_name\n+            )",
      "comment": "Does it still make sense to make this a method (vs. inline the logic) even though it's only called in one place?",
      "comment_id": 2688401895,
      "user": "timgraham",
      "created_at": "2026-01-13T23:05:22Z",
      "url": "https://github.com/django/django/pull/20519#discussion_r2688401895"
    },
    {
      "repo": "django/django",
      "pr_number": 20519,
      "file_path": "django/db/backends/base/creation.py",
      "line": 377,
      "side": "RIGHT",
      "diff_hunk": "@@ -358,27 +358,48 @@ def mark_expected_failures_and_skips(self):\n         Mark tests in Django's test suite which are expected failures on this\n         database and test which should be skipped on this database.\n         \"\"\"\n-        # Only load unittest if we're actually testing.\n-        from unittest import expectedFailure, skip\n-\n         for test_name in self.connection.features.django_test_expected_failures:\n-            test_case_name, _, test_method_name = test_name.rpartition(\".\")\n-            test_app = test_name.split(\".\")[0]\n-            # Importing a test app that isn't installed raises RuntimeError.\n-            if test_app in settings.INSTALLED_APPS:\n-                test_case = import_string(test_case_name)\n-                test_method = getattr(test_case, test_method_name)\n-                setattr(test_case, test_method_name, expectedFailure(test_method))\n+            self._mark_test(test_name)\n         for reason, tests in self.connection.features.django_test_skips.items():\n             for test_name in tests:\n-                test_case_name, _, test_method_name = test_name.rpartition(\".\")\n-                test_app = test_name.split(\".\")[0]\n-                # Importing a test app that isn't installed raises\n-                # RuntimeError.\n-                if test_app in settings.INSTALLED_APPS:\n-                    test_case = import_string(test_case_name)\n-                    test_method = getattr(test_case, test_method_name)\n-                    setattr(test_case, test_method_name, skip(reason)(test_method))\n+                self._mark_test(test_name, reason)\n+\n+    def _mark_test(self, test_name, skip_reason=None):\n+        # Only load unittest if we're actually testing.\n+        from unittest import expectedFailure, skip\n+\n+        module_or_class_name, _, name_to_mark = test_name.rpartition(\".\")\n+        test_app = test_name.split(\".\")[0]\n+        # Importing a test app that isn't installed raises RuntimeError.\n+        if test_app in settings.INSTALLED_APPS:\n+            test_frame, test_to_mark = self._get_test_and_frame(\n+                module_or_class_name, name_to_mark, test_name\n+            )",
      "comment": "Ah, good point! Didn't notice that opportunity after the latest refactor \ud83d\udc4d ",
      "comment_id": 2688503727,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-14T00:06:46Z",
      "url": "https://github.com/django/django/pull/20519#discussion_r2688503727"
    },
    {
      "repo": "django/django",
      "pr_number": 20519,
      "file_path": "django/db/backends/base/creation.py",
      "line": 368,
      "side": "RIGHT",
      "diff_hunk": "@@ -358,27 +358,48 @@ def mark_expected_failures_and_skips(self):\n         Mark tests in Django's test suite which are expected failures on this\n         database and test which should be skipped on this database.\n         \"\"\"\n-        # Only load unittest if we're actually testing.\n-        from unittest import expectedFailure, skip\n-\n         for test_name in self.connection.features.django_test_expected_failures:\n-            test_case_name, _, test_method_name = test_name.rpartition(\".\")\n-            test_app = test_name.split(\".\")[0]\n-            # Importing a test app that isn't installed raises RuntimeError.\n-            if test_app in settings.INSTALLED_APPS:\n-                test_case = import_string(test_case_name)\n-                test_method = getattr(test_case, test_method_name)\n-                setattr(test_case, test_method_name, expectedFailure(test_method))\n+            self._mark_test(test_name)\n         for reason, tests in self.connection.features.django_test_skips.items():\n             for test_name in tests:\n-                test_case_name, _, test_method_name = test_name.rpartition(\".\")\n-                test_app = test_name.split(\".\")[0]\n-                # Importing a test app that isn't installed raises\n-                # RuntimeError.\n-                if test_app in settings.INSTALLED_APPS:\n-                    test_case = import_string(test_case_name)\n-                    test_method = getattr(test_case, test_method_name)\n-                    setattr(test_case, test_method_name, skip(reason)(test_method))\n+                self._mark_test(test_name, reason)\n+\n+    def _mark_test(self, test_name, skip_reason=None):\n+        # Only load unittest if we're actually testing.",
      "comment": "Yeah, I undid some edits that were no longer opportunistic once lines started moving around again, but now this is opportunistic \ud83d\udc4d ",
      "comment_id": 2688505614,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-14T00:08:03Z",
      "url": "https://github.com/django/django/pull/20519#discussion_r2688505614"
    },
    {
      "repo": "django/django",
      "pr_number": 20519,
      "file_path": "django/db/backends/base/creation.py",
      "line": 376,
      "side": "RIGHT",
      "diff_hunk": "@@ -358,27 +358,39 @@ def mark_expected_failures_and_skips(self):\n         Mark tests in Django's test suite which are expected failures on this\n         database and test which should be skipped on this database.\n         \"\"\"\n-        # Only load unittest if we're actually testing.\n-        from unittest import expectedFailure, skip\n-\n         for test_name in self.connection.features.django_test_expected_failures:\n-            test_case_name, _, test_method_name = test_name.rpartition(\".\")\n-            test_app = test_name.split(\".\")[0]\n-            # Importing a test app that isn't installed raises RuntimeError.\n-            if test_app in settings.INSTALLED_APPS:\n-                test_case = import_string(test_case_name)\n-                test_method = getattr(test_case, test_method_name)\n-                setattr(test_case, test_method_name, expectedFailure(test_method))\n+            self._mark_test(test_name)\n         for reason, tests in self.connection.features.django_test_skips.items():\n             for test_name in tests:\n-                test_case_name, _, test_method_name = test_name.rpartition(\".\")\n-                test_app = test_name.split(\".\")[0]\n-                # Importing a test app that isn't installed raises\n-                # RuntimeError.\n-                if test_app in settings.INSTALLED_APPS:\n-                    test_case = import_string(test_case_name)\n-                    test_method = getattr(test_case, test_method_name)\n-                    setattr(test_case, test_method_name, skip(reason)(test_method))\n+                self._mark_test(test_name, reason)\n+\n+    def _mark_test(self, test_name, skip_reason=None):\n+        # Only load unittest if we're actually testing.\n+        from unittest import expectedFailure, skip\n+\n+        module_or_class_name, _, name_to_mark = test_name.rpartition(\".\")\n+        test_app = test_name.split(\".\")[0]\n+        # Importing a test app that isn't installed raises RuntimeError.\n+        if test_app in settings.INSTALLED_APPS:\n+            try:\n+                test_frame = import_string(module_or_class_name)",
      "comment": "Now that we lost the other helper's docstring suggesting what a \"frame\" is, I'm hoping this is sort of self-explanatory if in the 1-helper version you see it's the thing having `setattr()` run on it. (?)",
      "comment_id": 2688517458,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-14T00:16:02Z",
      "url": "https://github.com/django/django/pull/20519#discussion_r2688517458"
    },
    {
      "repo": "django/django",
      "pr_number": 20409,
      "file_path": "django/core/serializers/xml_serializer.py",
      "line": 398,
      "side": "RIGHT",
      "diff_hunk": "@@ -387,6 +394,8 @@ def m2m_convert(n):\n         try:\n             for c in node.getElementsByTagName(\"object\"):\n                 values.append(m2m_convert(c))\n+        except UnexpectedNode:\n+            raise",
      "comment": "There might be another one of these to filter out and raise, e.g. in `save_deferred_fields()`.",
      "comment_id": 2621138310,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-15T23:05:33Z",
      "url": "https://github.com/django/django/pull/20409#discussion_r2621138310"
    },
    {
      "repo": "django/django",
      "pr_number": 20409,
      "file_path": "django/core/serializers/xml_serializer.py",
      "line": 38,
      "side": "RIGHT",
      "diff_hunk": "@@ -35,6 +35,10 @@ def fast_cache_clearing():\n             minidom._in_document = original_fn\n \n \n+class UnexpectedNode(SuspiciousOperation):",
      "comment": "I'd be more general, `SuspiciousXML` or even `SuspiciousSerialization`.\r\n\r\nBut note that, currently, AFAICT, all `SuspiciousOperation` child classes in the Django source are defined in `exceptions.py` files -- and except for those in `contrib` apps, all are in `django.core.exceptions`. I suspect we could do without a child class at all.",
      "comment_id": 2637307610,
      "user": "shaib",
      "created_at": "2025-12-20T20:12:11Z",
      "url": "https://github.com/django/django/pull/20409#discussion_r2637307610"
    },
    {
      "repo": "django/django",
      "pr_number": 20409,
      "file_path": "django/core/serializers/xml_serializer.py",
      "line": 321,
      "side": "RIGHT",
      "diff_hunk": "@@ -301,7 +305,10 @@ def _handle_object(self, node):\n                 if field_node.getElementsByTagName(\"None\"):\n                     value = None\n                 else:\n-                    value = field.to_python(getInnerText(field_node).strip())\n+                    if natural_nodes := field_node.getElementsByTagName(\"natural\"):\n+                        value = field.to_python(getInnerText(natural_nodes[0]).strip())\n+                    else:\n+                        value = field.to_python(getInnerText(field_node).strip())",
      "comment": "Why give special treatment to `natural` nodes inside fields which aren't relation fields?",
      "comment_id": 2637316329,
      "user": "shaib",
      "created_at": "2025-12-20T20:24:01Z",
      "url": "https://github.com/django/django/pull/20409#discussion_r2637316329"
    },
    {
      "repo": "django/django",
      "pr_number": 20409,
      "file_path": "django/core/serializers/xml_serializer.py",
      "line": 397,
      "side": "RIGHT",
      "diff_hunk": "@@ -387,6 +394,8 @@ def m2m_convert(n):\n         try:\n             for c in node.getElementsByTagName(\"object\"):\n                 values.append(m2m_convert(c))\n+        except UnexpectedNode:",
      "comment": "This is a good idea IMO, but, regardless of whether we need a specific exception subclass for this fix, I'd go with\r\n```suggestion\r\n        except SuspiciousOperation:\r\n```",
      "comment_id": 2637380214,
      "user": "shaib",
      "created_at": "2025-12-20T21:34:57Z",
      "url": "https://github.com/django/django/pull/20409#discussion_r2637380214"
    },
    {
      "repo": "django/django",
      "pr_number": 20409,
      "file_path": "django/core/serializers/xml_serializer.py",
      "line": 452,
      "side": "RIGHT",
      "diff_hunk": "@@ -415,17 +424,15 @@ def _get_model_from_node(self, node, attr):\n             )\n \n \n+def check_element_type(element):\n+    if element.childNodes:\n+        raise UnexpectedNode\n+    return element.nodeType in (element.TEXT_NODE, element.CDATA_SECTION_NODE)\n+\n+\n def getInnerText(node):\n-    \"\"\"Get the inner text of a DOM node and any children one level deep.\"\"\"\n-    # inspired by\n-    # https://mail.python.org/pipermail/xml-sig/2005-March/011022.html\n     return \"\".join(\n-        [\n-            element.data\n-            for child in node.childNodes\n-            for element in (child, *child.childNodes)\n-            if element.nodeType in (element.TEXT_NODE, element.CDATA_SECTION_NODE)\n-        ]\n+        [child.data for child in node.childNodes if check_element_type(child)]",
      "comment": "This doesn't look equivalent... the original took children and grandchildren, this drops grandchildren.",
      "comment_id": 2637385012,
      "user": "shaib",
      "created_at": "2025-12-20T21:41:38Z",
      "url": "https://github.com/django/django/pull/20409#discussion_r2637385012"
    },
    {
      "repo": "django/django",
      "pr_number": 20409,
      "file_path": "tests/serializers/test_deserialization.py",
      "line": 156,
      "side": "RIGHT",
      "diff_hunk": "@@ -155,35 +153,6 @@ def build_crafted_xml(depth, leaf_text_len):\n                 </django-objects>\n             \"\"\"\n \n-        def deserialize(crafted_xml):\n-            iterator = XMLDeserializer(crafted_xml)\n-            garbage_collect()\n-\n-            start_time = time.perf_counter()\n-            result = list(iterator)\n-            end_time = time.perf_counter()\n-\n-            self.assertEqual(len(result), 1)\n-            self.assertIsInstance(result[0].object, models.Model)\n-            return end_time - start_time\n-\n-        def assertFactor(label, params, factor=2):\n-            factors = []\n-            prev_time = None\n-            for depth, length in params:\n-                crafted_xml = build_crafted_xml(depth, length)\n-                elapsed = deserialize(crafted_xml)\n-                if prev_time is not None:\n-                    factors.append(elapsed / prev_time)\n-                prev_time = elapsed\n-\n-            with self.subTest(label):\n-                # Assert based on the average factor to reduce test flakiness.\n-                self.assertLessEqual(sum(factors) / len(factors), factor)\n-\n-        assertFactor(\n-            \"varying depth, varying length\",\n-            [(50, 2000), (100, 4000), (200, 8000), (400, 16000), (800, 32000)],\n-            2,\n-        )\n-        assertFactor(\"constant depth, varying length\", [(100, 1), (100, 1000)], 2)\n+        crafted_xml = build_crafted_xml(100, 1000)",
      "comment": "`build_crafted_xml` seems redundant as a function if it only gets called once. I'd simplify it away.",
      "comment_id": 2637387739,
      "user": "shaib",
      "created_at": "2025-12-20T21:46:15Z",
      "url": "https://github.com/django/django/pull/20409#discussion_r2637387739"
    },
    {
      "repo": "django/django",
      "pr_number": 20409,
      "file_path": "django/core/serializers/xml_serializer.py",
      "line": 452,
      "side": "RIGHT",
      "diff_hunk": "@@ -415,17 +424,15 @@ def _get_model_from_node(self, node, attr):\n             )\n \n \n+def check_element_type(element):\n+    if element.childNodes:\n+        raise UnexpectedNode\n+    return element.nodeType in (element.TEXT_NODE, element.CDATA_SECTION_NODE)\n+\n+\n def getInnerText(node):\n-    \"\"\"Get the inner text of a DOM node and any children one level deep.\"\"\"\n-    # inspired by\n-    # https://mail.python.org/pipermail/xml-sig/2005-March/011022.html\n     return \"\".join(\n-        [\n-            element.data\n-            for child in node.childNodes\n-            for element in (child, *child.childNodes)\n-            if element.nodeType in (element.TEXT_NODE, element.CDATA_SECTION_NODE)\n-        ]\n+        [child.data for child in node.childNodes if check_element_type(child)]",
      "comment": "Right, I should have left notes about this to aid the review. This change is possible _only_ in virtue of the other change you noticed, where I'm specializing `<natural>` nodes.\r\n\r\nThat's the only location where we need to drill two levels deep, e.g. from:\r\n\r\n```\r\n<field>\r\n    <natural> <!-- child -->\r\n        some content <!-- grandchild text node -->\r\n```\r\n\r\nGrandchild text nodes don't need to be visited if I instead call the helper on `<natural>` nodes. That dramatically simplifies checking for unexpected children.",
      "comment_id": 2637409898,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-20T22:23:01Z",
      "url": "https://github.com/django/django/pull/20409#discussion_r2637409898"
    },
    {
      "repo": "django/django",
      "pr_number": 20409,
      "file_path": "django/core/serializers/xml_serializer.py",
      "line": 452,
      "side": "RIGHT",
      "diff_hunk": "@@ -415,17 +424,15 @@ def _get_model_from_node(self, node, attr):\n             )\n \n \n+def check_element_type(element):\n+    if element.childNodes:\n+        raise UnexpectedNode\n+    return element.nodeType in (element.TEXT_NODE, element.CDATA_SECTION_NODE)\n+\n+\n def getInnerText(node):\n-    \"\"\"Get the inner text of a DOM node and any children one level deep.\"\"\"\n-    # inspired by\n-    # https://mail.python.org/pipermail/xml-sig/2005-March/011022.html\n     return \"\".join(\n-        [\n-            element.data\n-            for child in node.childNodes\n-            for element in (child, *child.childNodes)\n-            if element.nodeType in (element.TEXT_NODE, element.CDATA_SECTION_NODE)\n-        ]\n+        [child.data for child in node.childNodes if check_element_type(child)]",
      "comment": "Elsewhere, `<field>` nodes are interrogated when we _don't_ expect `<natural>` serialization, e.g.\r\n\r\n```\r\n<field>\r\n    some content <!-- child text node -->\r\n```",
      "comment_id": 2637410265,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-20T22:24:05Z",
      "url": "https://github.com/django/django/pull/20409#discussion_r2637410265"
    },
    {
      "repo": "django/django",
      "pr_number": 20409,
      "file_path": "django/core/serializers/xml_serializer.py",
      "line": 321,
      "side": "RIGHT",
      "diff_hunk": "@@ -301,7 +305,10 @@ def _handle_object(self, node):\n                 if field_node.getElementsByTagName(\"None\"):\n                     value = None\n                 else:\n-                    value = field.to_python(getInnerText(field_node).strip())\n+                    if natural_nodes := field_node.getElementsByTagName(\"natural\"):\n+                        value = field.to_python(getInnerText(natural_nodes[0]).strip())\n+                    else:\n+                        value = field.to_python(getInnerText(field_node).strip())",
      "comment": "Thanks for the review. Sorry I didn't share more of my findings. I think throughout the serializer natural nodes are specialized. The ones that are relations, in the branches just before this, are specialized inside `_handle_m2m_field_node`. Unless I'm missing your meaning.",
      "comment_id": 2637410877,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-20T22:25:14Z",
      "url": "https://github.com/django/django/pull/20409#discussion_r2637410877"
    },
    {
      "repo": "django/django",
      "pr_number": 20409,
      "file_path": "django/core/serializers/xml_serializer.py",
      "line": 398,
      "side": "RIGHT",
      "diff_hunk": "@@ -387,6 +394,8 @@ def m2m_convert(n):\n         try:\n             for c in node.getElementsByTagName(\"object\"):\n                 values.append(m2m_convert(c))\n+        except UnexpectedNode:\n+            raise",
      "comment": "I checked again and couldn't find any other places to update.",
      "comment_id": 2640284799,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-22T15:36:18Z",
      "url": "https://github.com/django/django/pull/20409#discussion_r2640284799"
    },
    {
      "repo": "django/django",
      "pr_number": 20409,
      "file_path": "django/core/serializers/xml_serializer.py",
      "line": 321,
      "side": "RIGHT",
      "diff_hunk": "@@ -301,7 +305,10 @@ def _handle_object(self, node):\n                 if field_node.getElementsByTagName(\"None\"):\n                     value = None\n                 else:\n-                    value = field.to_python(getInnerText(field_node).strip())\n+                    if natural_nodes := field_node.getElementsByTagName(\"natural\"):\n+                        value = field.to_python(getInnerText(natural_nodes[0]).strip())\n+                    else:\n+                        value = field.to_python(getInnerText(field_node).strip())",
      "comment": "@shaib I think the key point here is that `<natural>` is not limited to FK or M2M values. It can also appear on direct fields when those fields participate in the object's own natural key (e.g. fixtures produced with `dumpdata --natural-primary`, as Jacob patiently explained to me when we chatted). In that sense, handling `<natural>` outside the relation branches is expected, not special-casing.\n\nWith that understanding, the change looks good to me. The only remaining concern is test coverage: reverting this change (but leaving `getInnerText` as modified in this commit) would not currently cause any serializer tests to fail, so it would be good to see a few targeted tests added to lock this behavior in.",
      "comment_id": 2669527349,
      "user": "nessita",
      "created_at": "2026-01-07T17:57:58Z",
      "url": "https://github.com/django/django/pull/20409#discussion_r2669527349"
    },
    {
      "repo": "django/django",
      "pr_number": 20409,
      "file_path": "django/core/serializers/xml_serializer.py",
      "line": 321,
      "side": "RIGHT",
      "diff_hunk": "@@ -301,7 +305,10 @@ def _handle_object(self, node):\n                 if field_node.getElementsByTagName(\"None\"):\n                     value = None\n                 else:\n-                    value = field.to_python(getInnerText(field_node).strip())\n+                    if natural_nodes := field_node.getElementsByTagName(\"natural\"):\n+                        value = field.to_python(getInnerText(natural_nodes[0]).strip())\n+                    else:\n+                        value = field.to_python(getInnerText(field_node).strip())",
      "comment": "@shaib @nessita Thanks for pushing me to test my understanding here. I tried dumping fixture9, and noticed it was different from the `fixture9.xml` in the repo. Ouch.\r\n\r\nI think this all boils down to an invalid fixture. Upon correcting the fixture, I was able to remove this specialization.\r\n\r\n> In that sense, handling <natural> outside the relation branches is expected, not special-casing.\r\n\r\nI managed to talk @nessita into this when we we chatted, but it was wrong -- I think I was not prepared for the fixture to turn out to be invalid in *two* ways, not only whether or not `<natural>` should be present, but whether the definitions of the fields were to be trusted (missing `rel=ManyToOne...`).\r\n\r\nI wonder if it was just a copy-paste mistake in 35cc439.",
      "comment_id": 2670043573,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-07T20:54:01Z",
      "url": "https://github.com/django/django/pull/20409#discussion_r2670043573"
    },
    {
      "repo": "django/django",
      "pr_number": 20409,
      "file_path": "django/core/serializers/xml_serializer.py",
      "line": 446,
      "side": "RIGHT",
      "diff_hunk": "@@ -439,17 +441,15 @@ def _get_model_from_node(self, node, attr):\n             )\n \n \n+def check_element_type(element):\n+    if element.childNodes:\n+        raise SuspiciousOperation",
      "comment": "Could we perhaps add a message to this exception, since we are re-raising it in `_handle_m2m_field_node`? To aid debugging/understanding the error.",
      "comment_id": 2683642861,
      "user": "nessita",
      "created_at": "2026-01-12T19:40:04Z",
      "url": "https://github.com/django/django/pull/20409#discussion_r2683642861"
    },
    {
      "repo": "django/django",
      "pr_number": 20526,
      "file_path": "django/db/models/fields/__init__.py",
      "line": 1037,
      "side": "RIGHT",
      "diff_hunk": "@@ -1032,9 +1032,9 @@ def _get_default(self):\n         if self.has_db_default():\n             from django.db.models.expressions import DatabaseDefault\n \n-            return lambda: DatabaseDefault(\n-                self._db_default_expression, output_field=self\n-            )\n+            default = DatabaseDefault(self._db_default_expression, output_field=self)\n+\n+            return lambda: default",
      "comment": "This is safe to do as these are always resolved at query time and `resolve_expression` returns a resolved copy already.",
      "comment_id": 2680539696,
      "user": "charettes",
      "created_at": "2026-01-11T23:50:25Z",
      "url": "https://github.com/django/django/pull/20526#discussion_r2680539696"
    },
    {
      "repo": "django/django",
      "pr_number": 20526,
      "file_path": "django/db/models/fields/__init__.py",
      "line": 1037,
      "side": "RIGHT",
      "diff_hunk": "@@ -1032,9 +1032,9 @@ def _get_default(self):\n         if self.has_db_default():\n             from django.db.models.expressions import DatabaseDefault\n \n-            return lambda: DatabaseDefault(\n-                self._db_default_expression, output_field=self\n-            )\n+            default = DatabaseDefault(self._db_default_expression, output_field=self)\n+\n+            return lambda: default",
      "comment": "Thanks. Just checking: \"always resolved\" meaning if they can't be replaced by the `DEFAULT` keyword, right? I think we optimized out some resolving in #20493.",
      "comment_id": 2682688701,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-12T15:09:59Z",
      "url": "https://github.com/django/django/pull/20526#discussion_r2682688701"
    },
    {
      "repo": "django/django",
      "pr_number": 20526,
      "file_path": "django/db/models/fields/__init__.py",
      "line": 1037,
      "side": "RIGHT",
      "diff_hunk": "@@ -1032,9 +1032,9 @@ def _get_default(self):\n         if self.has_db_default():\n             from django.db.models.expressions import DatabaseDefault\n \n-            return lambda: DatabaseDefault(\n-                self._db_default_expression, output_field=self\n-            )\n+            default = DatabaseDefault(self._db_default_expression, output_field=self)\n+\n+            return lambda: default",
      "comment": "@jacobtylerwalls The optimization made in https://github.com/django/django/pull/20493 assume that `DatabaseDefault.resolve_expression` will never have to be called during `bulk_create` which seems safe as all databases we support require that a _pure_ expression (most of the time a constant) be used as `db_default` and resolving mainly turns field references to `Col` objects.\r\n\r\nI assume folks would get a weird crash if they try doing something like `db_default=F(\"other_field\")` after https://github.com/django/django/pull/20493 as you'll try a compile an unresolved expression but that seems expected.\r\n\r\nGiven `DatabaseDefault.as_sql` is idempotent the cloning that `resolve_expression` performs is not  necessary in fact I'm not sure what calls `DatabaseDefault.resolve_expression` anymore to be honest, maybe some logic in `Model.save` prior to calling `sql.Query._insert`? [The coverage report](https://djangoci.com/view/%C2%ADCoverage/job/django-coverage/HTML_20Coverage_20Report/z_d81526da7cfdb7e4_expressions_py.html#t1295) points at the `not for_save` branch being completely dead so I guess we could prune it and raise a `ValueError` if `not for_save`?",
      "comment_id": 2682809720,
      "user": "charettes",
      "created_at": "2026-01-12T15:36:44Z",
      "url": "https://github.com/django/django/pull/20526#discussion_r2682809720"
    },
    {
      "repo": "django/django",
      "pr_number": 20526,
      "file_path": "django/db/models/fields/__init__.py",
      "line": 1037,
      "side": "RIGHT",
      "diff_hunk": "@@ -1032,9 +1032,9 @@ def _get_default(self):\n         if self.has_db_default():\n             from django.db.models.expressions import DatabaseDefault\n \n-            return lambda: DatabaseDefault(\n-                self._db_default_expression, output_field=self\n-            )\n+            default = DatabaseDefault(self._db_default_expression, output_field=self)\n+\n+            return lambda: default",
      "comment": "> I assume folks would get a weird crash if they try doing something like db_default=F(\"other_field\")\r\n\r\nWe have a system check covering that \ud83d\udc4d \r\n\r\n> ERRORS:\r\napp.Person.name: (fields.E012) F(pk) cannot be used in db_default.",
      "comment_id": 2682841250,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-12T15:44:46Z",
      "url": "https://github.com/django/django/pull/20526#discussion_r2682841250"
    },
    {
      "repo": "django/django",
      "pr_number": 20526,
      "file_path": "django/db/models/fields/__init__.py",
      "line": 1037,
      "side": "RIGHT",
      "diff_hunk": "@@ -1032,9 +1032,9 @@ def _get_default(self):\n         if self.has_db_default():\n             from django.db.models.expressions import DatabaseDefault\n \n-            return lambda: DatabaseDefault(\n-                self._db_default_expression, output_field=self\n-            )\n+            default = DatabaseDefault(self._db_default_expression, output_field=self)\n+\n+            return lambda: default",
      "comment": "FWIW the only remaining `DatabaseDefault.resolve_expression` calls are coming from constraint validation.\r\n\r\nIn light of ticket-36847 this made me realize that expression resolving during constraint validation should potentially pass `for_save=True` as well \ud83e\udd14 ?",
      "comment_id": 2683039063,
      "user": "charettes",
      "created_at": "2026-01-12T16:35:31Z",
      "url": "https://github.com/django/django/pull/20526#discussion_r2683039063"
    },
    {
      "repo": "django/django",
      "pr_number": 20526,
      "file_path": "django/db/models/fields/__init__.py",
      "line": 1037,
      "side": "RIGHT",
      "diff_hunk": "@@ -1032,9 +1032,9 @@ def _get_default(self):\n         if self.has_db_default():\n             from django.db.models.expressions import DatabaseDefault\n \n-            return lambda: DatabaseDefault(\n-                self._db_default_expression, output_field=self\n-            )\n+            default = DatabaseDefault(self._db_default_expression, output_field=self)\n+\n+            return lambda: default",
      "comment": "Hm, I haven't dealt with an expression cloning / for_save ticket, so unfortunately I'd probably need a bit more context.",
      "comment_id": 2683520085,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-12T19:02:48Z",
      "url": "https://github.com/django/django/pull/20526#discussion_r2683520085"
    },
    {
      "repo": "django/django",
      "pr_number": 20184,
      "file_path": "django/contrib/admin/options.py",
      "line": 2053,
      "side": "RIGHT",
      "diff_hunk": "@@ -2050,10 +2050,7 @@ def changelist_view(self, request, extra_context=None):\n             # me back\" button on the action confirmation page.\n             return HttpResponseRedirect(request.get_full_path())\n \n-        # If we're allowing changelist editing, we need to construct a formset\n-        # for the changelist given all the fields to be edited. Then we'll\n-        # use the formset to validate/process POSTed data.\n-        formset = cl.formset = None\n+        formset = None",
      "comment": "It looks like the lint issue is occurring in this part.\r\nRemoving that line should resolve it!",
      "comment_id": 2539961196,
      "user": "Antoliny0919",
      "created_at": "2025-11-18T23:39:05Z",
      "url": "https://github.com/django/django/pull/20184#discussion_r2539961196"
    },
    {
      "repo": "django/django",
      "pr_number": 20184,
      "file_path": "django/contrib/admin/options.py",
      "line": 2053,
      "side": "RIGHT",
      "diff_hunk": "@@ -2050,10 +2050,7 @@ def changelist_view(self, request, extra_context=None):\n             # me back\" button on the action confirmation page.\n             return HttpResponseRedirect(request.get_full_path())\n \n-        # If we're allowing changelist editing, we need to construct a formset\n-        # for the changelist given all the fields to be edited. Then we'll\n-        # use the formset to validate/process POSTed data.\n-        formset = cl.formset = None\n+        formset = None",
      "comment": "oh boy! that was a stupid oversight sorry \ud83d\ude05\r\ni mistook options for main ig ",
      "comment_id": 2540434159,
      "user": "Rudraksha-007",
      "created_at": "2025-11-19T03:55:35Z",
      "url": "https://github.com/django/django/pull/20184#discussion_r2540434159"
    },
    {
      "repo": "django/django",
      "pr_number": 20184,
      "file_path": "django/contrib/admin/views/main.py",
      "line": 67,
      "side": "RIGHT",
      "diff_hunk": "@@ -64,6 +64,7 @@ def __init__(self, *args, **kwargs):\n \n class ChangeList:\n     search_form_class = ChangeListSearchForm\n+    formset = None  # noqa: F841",
      "comment": "yep, i thought that snakeoil fix (#type:ignore) would work here \r\ni have removed this \u2705\r\nit passes the test now ",
      "comment_id": 2540435119,
      "user": "Rudraksha-007",
      "created_at": "2025-11-19T03:56:20Z",
      "url": "https://github.com/django/django/pull/20184#discussion_r2540435119"
    },
    {
      "repo": "django/django",
      "pr_number": 20184,
      "file_path": "django/contrib/admin/options.py",
      "line": 2056,
      "side": "LEFT",
      "diff_hunk": "@@ -2050,11 +2050,6 @@ def changelist_view(self, request, extra_context=None):\n             # me back\" button on the action confirmation page.\n             return HttpResponseRedirect(request.get_full_path())\n \n-        # If we're allowing changelist editing, we need to construct a formset\n-        # for the changelist given all the fields to be edited. Then we'll\n-        # use the formset to validate/process POSTed data.\n-        formset = cl.formset = None",
      "comment": "I could see value in leaving `cl.formset = None` here to just avoid any pollution from prior calls. @Antoliny0919 wdyt?",
      "comment_id": 2624610523,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-16T20:17:09Z",
      "url": "https://github.com/django/django/pull/20184#discussion_r2624610523"
    },
    {
      "repo": "django/django",
      "pr_number": 20184,
      "file_path": "django/contrib/admin/options.py",
      "line": 2056,
      "side": "LEFT",
      "diff_hunk": "@@ -2050,11 +2050,6 @@ def changelist_view(self, request, extra_context=None):\n             # me back\" button on the action confirmation page.\n             return HttpResponseRedirect(request.get_full_path())\n \n-        # If we're allowing changelist editing, we need to construct a formset\n-        # for the changelist given all the fields to be edited. Then we'll\n-        # use the formset to validate/process POSTed data.\n-        formset = cl.formset = None",
      "comment": "Never mind -- `cl` is an instance. `cl` meaning \"changelist\", who knew? \ud83d\ude04 ",
      "comment_id": 2624645464,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-16T20:31:27Z",
      "url": "https://github.com/django/django/pull/20184#discussion_r2624645464"
    },
    {
      "repo": "django/django",
      "pr_number": 20416,
      "file_path": "django/contrib/admin/sites.py",
      "line": 174,
      "side": "RIGHT",
      "diff_hunk": "@@ -170,6 +170,8 @@ def get_model_admin(self, model):\n         try:\n             return self._registry[model]\n         except KeyError:\n+            if isinstance(model, str):\n+                raise NotRegistered(f\"The model {model!r} is not registered.\")",
      "comment": "Sorry, looking more closely I see that `get_model_admin` is documented to take a class, not a string. It might be better to solve this closer to the source of the erroring check, by checking `isinstance(..., str)` before calling `get_model_admin()`.",
      "comment_id": 2624299381,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-16T18:25:24Z",
      "url": "https://github.com/django/django/pull/20416#discussion_r2624299381"
    },
    {
      "repo": "django/django",
      "pr_number": 20416,
      "file_path": "django/contrib/admin/sites.py",
      "line": 174,
      "side": "RIGHT",
      "diff_hunk": "@@ -170,6 +170,8 @@ def get_model_admin(self, model):\n         try:\n             return self._registry[model]\n         except KeyError:\n+            if isinstance(model, str):\n+                raise NotRegistered(f\"The model {model!r} is not registered.\")",
      "comment": "The error came up when the `ForeignKey`'s `to` field was defined as a string, and the app was not registered. I have pushed a fix that checks the type before `get_model_admin` is called. I found that a similar call was made in `options.py`. Please let me know if it looks okay, will add regression tests if it does. Thanks!",
      "comment_id": 2649177008,
      "user": "parth-paradkar",
      "created_at": "2025-12-27T14:59:37Z",
      "url": "https://github.com/django/django/pull/20416#discussion_r2649177008"
    },
    {
      "repo": "django/django",
      "pr_number": 20416,
      "file_path": "django/contrib/admin/checks.py",
      "line": 251,
      "side": "RIGHT",
      "diff_hunk": "@@ -236,6 +236,19 @@ def _check_autocomplete_fields_item(self, obj, field_name, label):\n                     id=\"admin.E038\",\n                 )\n             try:\n+                if isinstance(field.remote_field.model, str):\n+                    return [\n+                        checks.Error(\n+                            'An admin for model \"%s\" has to be registered '\n+                            \"to be referenced by %s.autocomplete_fields.\"\n+                            % (\n+                                field.remote_field.model,\n+                                type(obj).__name__,\n+                            ),\n+                            obj=obj.__class__,\n+                            id=\"admin.E039\",\n+                        )\n+                    ]",
      "comment": "```suggestion\n                    raise NotRegistered\n```",
      "comment_id": 2651524728,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-29T18:30:42Z",
      "url": "https://github.com/django/django/pull/20416#discussion_r2651524728"
    },
    {
      "repo": "django/django",
      "pr_number": 20416,
      "file_path": "django/contrib/admin/options.py",
      "line": 258,
      "side": "RIGHT",
      "diff_hunk": "@@ -254,6 +254,8 @@ def get_field_queryset(self, db, db_field, request):\n         (return None in that case).\n         \"\"\"\n         try:\n+            if isinstance(db_field.remote_field.model, str):\n+                return None",
      "comment": "I'm not sure this is realistic, wouldn't we have failed well before this?",
      "comment_id": 2651533053,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-29T18:36:07Z",
      "url": "https://github.com/django/django/pull/20416#discussion_r2651533053"
    },
    {
      "repo": "django/django",
      "pr_number": 20416,
      "file_path": "django/contrib/admin/options.py",
      "line": 258,
      "side": "RIGHT",
      "diff_hunk": "@@ -254,6 +254,8 @@ def get_field_queryset(self, db, db_field, request):\n         (return None in that case).\n         \"\"\"\n         try:\n+            if isinstance(db_field.remote_field.model, str):\n+                return None",
      "comment": "Hmm ok. I'll remove the change in this PR and check separately.",
      "comment_id": 2652607799,
      "user": "parth-paradkar",
      "created_at": "2025-12-30T09:35:45Z",
      "url": "https://github.com/django/django/pull/20416#discussion_r2652607799"
    },
    {
      "repo": "django/django",
      "pr_number": 20416,
      "file_path": "django/contrib/admin/sites.py",
      "line": 174,
      "side": "RIGHT",
      "diff_hunk": "@@ -170,6 +170,8 @@ def get_model_admin(self, model):\n         try:\n             return self._registry[model]\n         except KeyError:\n+            if isinstance(model, str):\n+                raise NotRegistered(f\"The model {model!r} is not registered.\")",
      "comment": "Yep, looks great! I pushed minor formatting edits to reduce space. Be sure to do a hard reset back several commits and then pull my latest changes.\r\n\r\nA regression test would be great, thanks.",
      "comment_id": 2673927890,
      "user": "jacobtylerwalls",
      "created_at": "2026-01-08T21:14:10Z",
      "url": "https://github.com/django/django/pull/20416#discussion_r2673927890"
    },
    {
      "repo": "django/django",
      "pr_number": 20462,
      "file_path": "django/contrib/postgres/constraints.py",
      "line": 157,
      "side": "RIGHT",
      "diff_hunk": "@@ -131,6 +133,28 @@ def remove_sql(self, model, schema_editor):\n             schema_editor.quote_name(self.name),\n         )\n \n+    def check_supported(self, schema_editor):\n+        if self.include and self.index_type.lower() == \"hash\":\n+            raise NotSupportedError(\n+                \"Covering exclusion constraints using an Hash index \"\n+                \"are not supported.\"\n+            )\n+\n+        if len(self.expressions) > 1 and self.index_type.lower() == \"hash\":\n+            raise NotSupportedError(\n+                \"Composite exclusion constraints using an Hash index \"\n+                \"are not supported.\"\n+            )\n+\n+        if (\n+            self.index_type.lower() == \"hash\"\n+            and self.expressions[0][1] != RangeOperators.EQUAL\n+        ):\n+            raise NotSupportedError(\n+                \"Exclusion constraints using an Hash index only \"\n+                \"supports the EQUAL operator.\"\n+            )\n+",
      "comment": "Why not do all of this validation up in `__init__` ? It seems weird to validate the type there, but then delay this extra validation until SQL creation.",
      "comment_id": 2651856092,
      "user": "adamchainz",
      "created_at": "2025-12-29T22:10:26Z",
      "url": "https://github.com/django/django/pull/20462#discussion_r2651856092"
    },
    {
      "repo": "django/django",
      "pr_number": 20462,
      "file_path": "django/contrib/postgres/constraints.py",
      "line": 157,
      "side": "RIGHT",
      "diff_hunk": "@@ -131,6 +133,28 @@ def remove_sql(self, model, schema_editor):\n             schema_editor.quote_name(self.name),\n         )\n \n+    def check_supported(self, schema_editor):\n+        if self.include and self.index_type.lower() == \"hash\":\n+            raise NotSupportedError(\n+                \"Covering exclusion constraints using an Hash index \"\n+                \"are not supported.\"\n+            )\n+\n+        if len(self.expressions) > 1 and self.index_type.lower() == \"hash\":\n+            raise NotSupportedError(\n+                \"Composite exclusion constraints using an Hash index \"\n+                \"are not supported.\"\n+            )\n+\n+        if (\n+            self.index_type.lower() == \"hash\"\n+            and self.expressions[0][1] != RangeOperators.EQUAL\n+        ):\n+            raise NotSupportedError(\n+                \"Exclusion constraints using an Hash index only \"\n+                \"supports the EQUAL operator.\"\n+            )\n+",
      "comment": "Hi Adam, I added it like that because this is how it was done before support for PostgreSQL 14 was dropped:\r\n\r\nhttps://github.com/django/django/commit/b049bec7cfe9b5854584d240addb44fa1e9375a5#diff-a4e33792faf48c1d998ea65d49b65a6ab695efad8c2b74891180e055342e92eeL117-L141\r\n\r\nLooking at other constraints (such as UniqueConstraint) I think the checks in `__init__` are focusing more on types rather then database support or potential runtime errors so I don't this is the right place for these types of checks.\r\n\r\nIt seems like these checks are better off as Django checks. See for example similar checks in `UniqueConstraint`:\r\n\r\nhttps://github.com/django/django/blob/ccf74f7dc771313b41e7c2912a71f9c5b0ae5e1d/django/db/models/constraints.py#L338\r\n\r\nTo recap the options here:\r\n\r\n1. No checks (there are currently no checks for the other index types)\r\n2. Raise `ValueError` in `__init__`\r\n3. Issue Django checks in `check()` (consistent with other constraints)\r\n4. Check at runtime when producing the SQL (as it used to be, and how it's currently implemented in this PR)",
      "comment_id": 2652498439,
      "user": "hakib",
      "created_at": "2025-12-30T08:26:49Z",
      "url": "https://github.com/django/django/pull/20462#discussion_r2652498439"
    },
    {
      "repo": "django/django",
      "pr_number": 20462,
      "file_path": "django/contrib/postgres/constraints.py",
      "line": 157,
      "side": "RIGHT",
      "diff_hunk": "@@ -131,6 +133,28 @@ def remove_sql(self, model, schema_editor):\n             schema_editor.quote_name(self.name),\n         )\n \n+    def check_supported(self, schema_editor):\n+        if self.include and self.index_type.lower() == \"hash\":\n+            raise NotSupportedError(\n+                \"Covering exclusion constraints using an Hash index \"\n+                \"are not supported.\"\n+            )\n+\n+        if len(self.expressions) > 1 and self.index_type.lower() == \"hash\":\n+            raise NotSupportedError(\n+                \"Composite exclusion constraints using an Hash index \"\n+                \"are not supported.\"\n+            )\n+\n+        if (\n+            self.index_type.lower() == \"hash\"\n+            and self.expressions[0][1] != RangeOperators.EQUAL\n+        ):\n+            raise NotSupportedError(\n+                \"Exclusion constraints using an Hash index only \"\n+                \"supports the EQUAL operator.\"\n+            )\n+",
      "comment": "I'd prefer the 2nd option (`ValueError` in `__init__()`) previous checks were implemented in `check_support()` because they required database connection to check feature flags (`schema_editor.connection`). New checks are not database dependent.",
      "comment_id": 2659676600,
      "user": "felixxm",
      "created_at": "2026-01-04T13:44:01Z",
      "url": "https://github.com/django/django/pull/20462#discussion_r2659676600"
    },
    {
      "repo": "django/django",
      "pr_number": 20462,
      "file_path": "django/contrib/postgres/constraints.py",
      "line": 157,
      "side": "RIGHT",
      "diff_hunk": "@@ -131,6 +133,28 @@ def remove_sql(self, model, schema_editor):\n             schema_editor.quote_name(self.name),\n         )\n \n+    def check_supported(self, schema_editor):\n+        if self.include and self.index_type.lower() == \"hash\":\n+            raise NotSupportedError(\n+                \"Covering exclusion constraints using an Hash index \"\n+                \"are not supported.\"\n+            )\n+\n+        if len(self.expressions) > 1 and self.index_type.lower() == \"hash\":\n+            raise NotSupportedError(\n+                \"Composite exclusion constraints using an Hash index \"\n+                \"are not supported.\"\n+            )\n+\n+        if (\n+            self.index_type.lower() == \"hash\"\n+            and self.expressions[0][1] != RangeOperators.EQUAL\n+        ):\n+            raise NotSupportedError(\n+                \"Exclusion constraints using an Hash index only \"\n+                \"supports the EQUAL operator.\"\n+            )\n+",
      "comment": "I've added a commit to move the checks to `__init__.py` as `ValueError` + updated the tests. \r\n\r\nLet me know if we are good to go and I'll squash.",
      "comment_id": 2661652476,
      "user": "hakib",
      "created_at": "2026-01-05T14:13:58Z",
      "url": "https://github.com/django/django/pull/20462#discussion_r2661652476"
    },
    {
      "repo": "django/django",
      "pr_number": 20462,
      "file_path": "django/contrib/postgres/constraints.py",
      "line": 157,
      "side": "RIGHT",
      "diff_hunk": "@@ -131,6 +133,28 @@ def remove_sql(self, model, schema_editor):\n             schema_editor.quote_name(self.name),\n         )\n \n+    def check_supported(self, schema_editor):\n+        if self.include and self.index_type.lower() == \"hash\":\n+            raise NotSupportedError(\n+                \"Covering exclusion constraints using an Hash index \"\n+                \"are not supported.\"\n+            )\n+\n+        if len(self.expressions) > 1 and self.index_type.lower() == \"hash\":\n+            raise NotSupportedError(\n+                \"Composite exclusion constraints using an Hash index \"\n+                \"are not supported.\"\n+            )\n+\n+        if (\n+            self.index_type.lower() == \"hash\"\n+            and self.expressions[0][1] != RangeOperators.EQUAL\n+        ):\n+            raise NotSupportedError(\n+                \"Exclusion constraints using an Hash index only \"\n+                \"supports the EQUAL operator.\"\n+            )\n+",
      "comment": "@hakib Thanks for updates :+1: I will move it forward during the weekend.",
      "comment_id": 2669546431,
      "user": "felixxm",
      "created_at": "2026-01-07T18:02:35Z",
      "url": "https://github.com/django/django/pull/20462#discussion_r2669546431"
    },
    {
      "repo": "django/django",
      "pr_number": 20462,
      "file_path": "django/contrib/postgres/constraints.py",
      "line": 40,
      "side": "RIGHT",
      "diff_hunk": "@@ -36,7 +37,7 @@ def __init__(\n         violation_error_code=None,\n         violation_error_message=None,\n     ):\n-        if index_type and index_type.lower() not in {\"gist\", \"spgist\"}:\n+        if index_type and index_type.lower() not in {\"gist\", \"spgist\", \"hash\"}:",
      "comment": "```suggestion\n        if index_type and index_type.lower() not in {\"gist\", \"hash\", \"spgist\"}:\n```\nI suggest to preserve the alphabetic order for code readability.",
      "comment_id": 2672447704,
      "user": "pauloxnet",
      "created_at": "2026-01-08T13:52:46Z",
      "url": "https://github.com/django/django/pull/20462#discussion_r2672447704"
    },
    {
      "repo": "django/django",
      "pr_number": 20462,
      "file_path": "django/contrib/postgres/constraints.py",
      "line": 42,
      "side": "RIGHT",
      "diff_hunk": "@@ -36,7 +37,7 @@ def __init__(\n         violation_error_code=None,\n         violation_error_message=None,\n     ):\n-        if index_type and index_type.lower() not in {\"gist\", \"spgist\"}:\n+        if index_type and index_type.lower() not in {\"gist\", \"spgist\", \"hash\"}:\n             raise ValueError(\n                 \"Exclusion constraints only support GiST or SP-GiST indexes.\"",
      "comment": "```suggestion\n                \"Exclusion constraints only support GiST, Hash or SP-GiST indexes.\"\n```",
      "comment_id": 2672464036,
      "user": "pauloxnet",
      "created_at": "2026-01-08T13:57:26Z",
      "url": "https://github.com/django/django/pull/20462#discussion_r2672464036"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "tests/model_fields/test_jsonfield.py",
      "line": 240,
      "side": "RIGHT",
      "diff_hunk": "@@ -224,6 +236,7 @@ def test_null(self):\n         self.assertIsNone(obj.value)\n \n     @skipUnlessDBFeature(\"supports_primitives_in_json_field\")\n+    @ignore_warnings(category=RemovedInDjango70Warning)",
      "comment": "```suggestion\n    # RemovedInDjango70Warning.\n    @ignore_warnings(category=RemovedInDjango70Warning)\n```",
      "comment_id": 2383012280,
      "user": "jacobtylerwalls",
      "created_at": "2025-09-26T17:23:46Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2383012280"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "tests/model_fields/test_jsonfield.py",
      "line": 242,
      "side": "RIGHT",
      "diff_hunk": "@@ -224,6 +236,7 @@ def test_null(self):\n         self.assertIsNone(obj.value)\n \n     @skipUnlessDBFeature(\"supports_primitives_in_json_field\")\n+    @ignore_warnings(category=RemovedInDjango70Warning)\n     def test_json_null_different_from_sql_null(self):\n         json_null = NullableJSONModel.objects.create(value=Value(None, JSONField()))",
      "comment": "I take it you intentionally left this as `Value(None, JSONField()` rather than `JSONNull()` to ensure we still had some coverage?",
      "comment_id": 2383014941,
      "user": "jacobtylerwalls",
      "created_at": "2025-09-26T17:25:14Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2383014941"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "tests/model_fields/test_jsonfield.py",
      "line": 1263,
      "side": "RIGHT",
      "diff_hunk": "@@ -1235,3 +1251,112 @@ def test_literal_annotation_filtering(self):\n             data__foo=\"bar\"\n         )\n         self.assertQuerySetEqual(qs, all_objects)\n+\n+\n+class JSONNullTests(TestCase):\n+    def test_repr(self):\n+        self.assertEqual(repr(JSONNull()), \"JSONNull()\")\n+\n+    def test_save_load(self):\n+        obj = NullableJSONModel(value=JSONNull())\n+        obj.save()\n+        obj.refresh_from_db()",
      "comment": "Thanks to #19285 we can drop these calls, at least for the top level.\n\n```suggestion\n```",
      "comment_id": 2383026088,
      "user": "jacobtylerwalls",
      "created_at": "2025-09-26T17:31:10Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2383026088"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "tests/model_fields/test_jsonfield.py",
      "line": 1272,
      "side": "RIGHT",
      "diff_hunk": "@@ -1235,3 +1251,112 @@ def test_literal_annotation_filtering(self):\n             data__foo=\"bar\"\n         )\n         self.assertQuerySetEqual(qs, all_objects)\n+\n+\n+class JSONNullTests(TestCase):\n+    def test_repr(self):\n+        self.assertEqual(repr(JSONNull()), \"JSONNull()\")\n+\n+    def test_save_load(self):\n+        obj = NullableJSONModel(value=JSONNull())\n+        obj.save()\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_create(self):\n+        obj = NullableJSONModel.objects.create(value=JSONNull())",
      "comment": "Would it be better to write these tests with `JSONModel` or `JSONNullDefaultModel`? These tests would pass if the `JSONNull()` got dropped somewhere and a row got created anyway.",
      "comment_id": 2383031239,
      "user": "jacobtylerwalls",
      "created_at": "2025-09-26T17:33:16Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2383031239"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "tests/model_fields/test_jsonfield.py",
      "line": 1280,
      "side": "RIGHT",
      "diff_hunk": "@@ -1235,3 +1251,112 @@ def test_literal_annotation_filtering(self):\n             data__foo=\"bar\"\n         )\n         self.assertQuerySetEqual(qs, all_objects)\n+\n+\n+class JSONNullTests(TestCase):\n+    def test_repr(self):\n+        self.assertEqual(repr(JSONNull()), \"JSONNull()\")\n+\n+    def test_save_load(self):\n+        obj = NullableJSONModel(value=JSONNull())\n+        obj.save()\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_create(self):\n+        obj = NullableJSONModel.objects.create(value=JSONNull())\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_update(self):\n+        obj = NullableJSONModel.objects.create(value={\"key\": \"value\"})\n+        NullableJSONModel.objects.update(value=JSONNull())\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_filter(self):\n+        obj = NullableJSONModel.objects.create(value=JSONNull())\n+        found = NullableJSONModel.objects.get(value=JSONNull())\n+        self.assertEqual(obj, found)",
      "comment": "I'm not sure what this assertion is testing.\n```suggestion\n```",
      "comment_id": 2383045557,
      "user": "jacobtylerwalls",
      "created_at": "2025-09-26T17:38:40Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2383045557"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "tests/model_fields/test_jsonfield.py",
      "line": 1286,
      "side": "RIGHT",
      "diff_hunk": "@@ -1235,3 +1251,112 @@ def test_literal_annotation_filtering(self):\n             data__foo=\"bar\"\n         )\n         self.assertQuerySetEqual(qs, all_objects)\n+\n+\n+class JSONNullTests(TestCase):\n+    def test_repr(self):\n+        self.assertEqual(repr(JSONNull()), \"JSONNull()\")\n+\n+    def test_save_load(self):\n+        obj = NullableJSONModel(value=JSONNull())\n+        obj.save()\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_create(self):\n+        obj = NullableJSONModel.objects.create(value=JSONNull())\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_update(self):\n+        obj = NullableJSONModel.objects.create(value={\"key\": \"value\"})\n+        NullableJSONModel.objects.update(value=JSONNull())\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_filter(self):\n+        obj = NullableJSONModel.objects.create(value=JSONNull())\n+        found = NullableJSONModel.objects.get(value=JSONNull())\n+        self.assertEqual(obj, found)\n+        self.assertSequenceEqual(",
      "comment": "Can we add examples that return results? Especially if we end up avoiding using `JSONNull()` in `test_json_null_different_from_sql_null`.",
      "comment_id": 2383049416,
      "user": "jacobtylerwalls",
      "created_at": "2025-09-26T17:40:01Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2383049416"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "tests/model_fields/test_jsonfield.py",
      "line": 1289,
      "side": "RIGHT",
      "diff_hunk": "@@ -1235,3 +1251,112 @@ def test_literal_annotation_filtering(self):\n             data__foo=\"bar\"\n         )\n         self.assertQuerySetEqual(qs, all_objects)\n+\n+\n+class JSONNullTests(TestCase):\n+    def test_repr(self):\n+        self.assertEqual(repr(JSONNull()), \"JSONNull()\")\n+\n+    def test_save_load(self):\n+        obj = NullableJSONModel(value=JSONNull())\n+        obj.save()\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_create(self):\n+        obj = NullableJSONModel.objects.create(value=JSONNull())\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_update(self):\n+        obj = NullableJSONModel.objects.create(value={\"key\": \"value\"})\n+        NullableJSONModel.objects.update(value=JSONNull())\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_filter(self):\n+        obj = NullableJSONModel.objects.create(value=JSONNull())\n+        found = NullableJSONModel.objects.get(value=JSONNull())\n+        self.assertEqual(obj, found)\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.exclude(value=JSONNull()), []\n+        )\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__isnull=True), []\n+        )\n+\n+    def test_case_expression(self):\n+        with self.subTest(\"JSONNull() in When.then\"):",
      "comment": "I would just explode these out into separate test methods, since the tests don't share state.",
      "comment_id": 2383050855,
      "user": "jacobtylerwalls",
      "created_at": "2025-09-26T17:40:39Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2383050855"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "tests/model_fields/test_jsonfield.py",
      "line": 1288,
      "side": "RIGHT",
      "diff_hunk": "@@ -1235,3 +1251,112 @@ def test_literal_annotation_filtering(self):\n             data__foo=\"bar\"\n         )\n         self.assertQuerySetEqual(qs, all_objects)\n+\n+\n+class JSONNullTests(TestCase):\n+    def test_repr(self):\n+        self.assertEqual(repr(JSONNull()), \"JSONNull()\")\n+\n+    def test_save_load(self):\n+        obj = NullableJSONModel(value=JSONNull())\n+        obj.save()\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_create(self):\n+        obj = NullableJSONModel.objects.create(value=JSONNull())\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_update(self):\n+        obj = NullableJSONModel.objects.create(value={\"key\": \"value\"})\n+        NullableJSONModel.objects.update(value=JSONNull())\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_filter(self):\n+        obj = NullableJSONModel.objects.create(value=JSONNull())\n+        found = NullableJSONModel.objects.get(value=JSONNull())\n+        self.assertEqual(obj, found)\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.exclude(value=JSONNull()), []\n+        )\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__isnull=True), []\n+        )\n+\n+    def test_case_expression(self):",
      "comment": "I think a couple tests with a custom encoder that can serialize `JSONNull` would be good.",
      "comment_id": 2383077021,
      "user": "jacobtylerwalls",
      "created_at": "2025-09-26T17:48:15Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2383077021"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "tests/model_fields/test_jsonfield.py",
      "line": 1332,
      "side": "RIGHT",
      "diff_hunk": "@@ -1235,3 +1251,112 @@ def test_literal_annotation_filtering(self):\n             data__foo=\"bar\"\n         )\n         self.assertQuerySetEqual(qs, all_objects)\n+\n+\n+class JSONNullTests(TestCase):\n+    def test_repr(self):\n+        self.assertEqual(repr(JSONNull()), \"JSONNull()\")\n+\n+    def test_save_load(self):\n+        obj = NullableJSONModel(value=JSONNull())\n+        obj.save()\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_create(self):\n+        obj = NullableJSONModel.objects.create(value=JSONNull())\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_update(self):\n+        obj = NullableJSONModel.objects.create(value={\"key\": \"value\"})\n+        NullableJSONModel.objects.update(value=JSONNull())\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_filter(self):\n+        obj = NullableJSONModel.objects.create(value=JSONNull())\n+        found = NullableJSONModel.objects.get(value=JSONNull())\n+        self.assertEqual(obj, found)\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.exclude(value=JSONNull()), []\n+        )\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__isnull=True), []\n+        )\n+\n+    def test_case_expression(self):\n+        with self.subTest(\"JSONNull() in When.then\"):\n+            obj = NullableJSONModel.objects.create(value={\"key\": \"value\"})\n+            NullableJSONModel.objects.filter(pk=obj.pk).update(\n+                value=Case(\n+                    When(value={\"key\": \"value\"}, then=JSONNull()),\n+                )\n+            )\n+            obj.refresh_from_db()\n+            self.assertIsNone(obj.value)\n+        with self.subTest(\"JSONNull() in When.condition\"):\n+            obj = NullableJSONModel.objects.create(value=JSONNull())\n+            NullableJSONModel.objects.filter(pk=obj.pk).update(\n+                value=Case(\n+                    When(\n+                        value=JSONNull(),\n+                        then=Value({\"key\": \"replaced\"}, output_field=JSONField()),\n+                    )\n+                ),\n+            )\n+            obj.refresh_from_db()\n+            self.assertEqual(obj.value, {\"key\": \"replaced\"})\n+\n+    def test_key_transform_exact_filter(self):\n+        obj = NullableJSONModel.objects.create(value={\"key\": None})\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__key=JSONNull()),\n+            [obj],\n+        )",
      "comment": "Do we want to show here that `=None` behaves the same?",
      "comment_id": 2383081074,
      "user": "jacobtylerwalls",
      "created_at": "2025-09-26T17:49:20Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2383081074"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "django/db/models/fields/json.py",
      "line": 343,
      "side": "RIGHT",
      "diff_hunk": "@@ -311,14 +334,27 @@ def process_rhs(self, compiler, connection):\n \n \n class JSONExact(lookups.Exact):\n+    # RemovedInDjango70Warning: When the deprecation period is over, remove\n+    # the following line.\n     can_use_none_as_rhs = True\n \n     def process_rhs(self, compiler, connection):\n+        if self.rhs is None and isinstance(self.lhs, expressions.Col):\n+            warnings.warn(\n+                \"Using None as the right-hand side of an exact lookup on JSONField \"\n+                \"to mean JSON scalar 'null' is deprecated. Use JSONNull() instead.\",",
      "comment": "I think there will be enough unsuspecting users that weren't trying to select on top-level scalar null that we could also point users toward `__isnull=True` for key or index absence.",
      "comment_id": 2383094898,
      "user": "jacobtylerwalls",
      "created_at": "2025-09-26T17:53:16Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2383094898"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "tests/model_fields/test_jsonfield.py",
      "line": 1384,
      "side": "RIGHT",
      "diff_hunk": "@@ -1235,3 +1251,112 @@ def test_literal_annotation_filtering(self):\n             data__foo=\"bar\"\n         )\n         self.assertQuerySetEqual(qs, all_objects)\n+\n+\n+class JSONNullTests(TestCase):\n+    def test_repr(self):\n+        self.assertEqual(repr(JSONNull()), \"JSONNull()\")\n+\n+    def test_save_load(self):\n+        obj = NullableJSONModel(value=JSONNull())\n+        obj.save()\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_create(self):\n+        obj = NullableJSONModel.objects.create(value=JSONNull())\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_update(self):\n+        obj = NullableJSONModel.objects.create(value={\"key\": \"value\"})\n+        NullableJSONModel.objects.update(value=JSONNull())\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_filter(self):\n+        obj = NullableJSONModel.objects.create(value=JSONNull())\n+        found = NullableJSONModel.objects.get(value=JSONNull())\n+        self.assertEqual(obj, found)\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.exclude(value=JSONNull()), []\n+        )\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__isnull=True), []\n+        )\n+\n+    def test_case_expression(self):\n+        with self.subTest(\"JSONNull() in When.then\"):\n+            obj = NullableJSONModel.objects.create(value={\"key\": \"value\"})\n+            NullableJSONModel.objects.filter(pk=obj.pk).update(\n+                value=Case(\n+                    When(value={\"key\": \"value\"}, then=JSONNull()),\n+                )\n+            )\n+            obj.refresh_from_db()\n+            self.assertIsNone(obj.value)\n+        with self.subTest(\"JSONNull() in When.condition\"):\n+            obj = NullableJSONModel.objects.create(value=JSONNull())\n+            NullableJSONModel.objects.filter(pk=obj.pk).update(\n+                value=Case(\n+                    When(\n+                        value=JSONNull(),\n+                        then=Value({\"key\": \"replaced\"}, output_field=JSONField()),\n+                    )\n+                ),\n+            )\n+            obj.refresh_from_db()\n+            self.assertEqual(obj.value, {\"key\": \"replaced\"})\n+\n+    def test_key_transform_exact_filter(self):\n+        obj = NullableJSONModel.objects.create(value={\"key\": None})\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__key=JSONNull()),\n+            [obj],\n+        )\n+\n+    @skipUnlessDBFeature(\"supports_table_check_constraints\")\n+    def test_constraint_validation(self):\n+        constraint = CheckConstraint(\n+            condition=~Q(value=JSONNull()), name=\"check_not_json_null\"\n+        )\n+        constraint.validate(NullableJSONModel, NullableJSONModel(value={\"key\": None}))\n+        msg = f\"Constraint \u201c{constraint.name}\u201d is violated.\"\n+        with self.assertRaisesMessage(ValidationError, msg):\n+            constraint.validate(NullableJSONModel, NullableJSONModel(value=JSONNull()))\n+\n+    def test_constraint_validation_key_transform(self):\n+        constraint = CheckConstraint(\n+            condition=Q(value__has_key=\"name\") & ~Q(value__name=JSONNull()),\n+            name=\"check_value_name_not_json_null\",\n+        )\n+        constraint.validate(\n+            NullableJSONModel, NullableJSONModel(value={\"name\": \"Django\"})\n+        )\n+        msg = f\"Constraint \u201c{constraint.name}\u201d is violated.\"\n+        with self.assertRaisesMessage(ValidationError, msg):\n+            constraint.validate(\n+                NullableJSONModel, NullableJSONModel(value={\"name\": None})\n+            )\n+\n+    def test_default(self):\n+        obj = JSONNullDefaultModel.objects.create()\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+        self.assertSequenceEqual(\n+            JSONNullDefaultModel.objects.filter(value__isnull=True), []\n+        )\n+\n+\n+class JSONExactNoneDeprecationTests(TestCase):",
      "comment": "```suggestion\n# RemovedInDjango70Warning.\nclass JSONExactNoneDeprecationTests(TestCase):\n```",
      "comment_id": 2383102668,
      "user": "jacobtylerwalls",
      "created_at": "2025-09-26T17:56:14Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2383102668"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "django/db/models/fields/json.py",
      "line": 153,
      "side": "RIGHT",
      "diff_hunk": "@@ -148,6 +150,27 @@ def formfield(self, **kwargs):\n         )\n \n \n+class JSONNull(expressions.Expression):",
      "comment": "@charettes can you advise on why we wouldn't subclass `Value`?\n\nIt makes me wonder about auditing all the places we do `isinstance(..., Value)`. This site seems like something we'd need to teach about `JSONNull`:\n\nhttps://github.com/django/django/blob/be581ff473e8ade6365975db2df602f295a4cb4b/django/db/backends/sqlite3/schema.py#L321-L323",
      "comment_id": 2383179163,
      "user": "jacobtylerwalls",
      "created_at": "2025-09-26T18:33:08Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2383179163"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "django/db/models/fields/json.py",
      "line": 159,
      "side": "RIGHT",
      "diff_hunk": "@@ -148,6 +150,27 @@ def formfield(self, **kwargs):\n         )\n \n \n+class JSONNull(expressions.Expression):\n+    \"\"\"Represent JSON `null` primitive.\"\"\"\n+\n+    allowed_default = True\n+\n+    def __init__(self):\n+        super().__init__(output_field=JSONField())",
      "comment": "More subclass friendly?\n```suggestion\n    output_field = JSONField()\n```",
      "comment_id": 2383191955,
      "user": "jacobtylerwalls",
      "created_at": "2025-09-26T18:38:18Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2383191955"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "django/db/models/fields/json.py",
      "line": 342,
      "side": "RIGHT",
      "diff_hunk": "@@ -311,14 +334,27 @@ def process_rhs(self, compiler, connection):\n \n \n class JSONExact(lookups.Exact):\n+    # RemovedInDjango70Warning: When the deprecation period is over, remove\n+    # the following line.\n     can_use_none_as_rhs = True\n \n     def process_rhs(self, compiler, connection):\n+        if self.rhs is None and isinstance(self.lhs, expressions.Col):",
      "comment": "Hey @jacobtylerwalls! Thanks for the thorough review! Concerning `ArrayField(JSONField(), ...)`, the problem is: Top level `None` in `ArrayField(JSONField())` saves as JSON `null` instead of SQL `NULL` (which is the case for top-level None in standalone `JSONField`s). Deprecating `json__0=None` without changing how `[None]` saves in `ArrayField(JSONField(), ...)` would introduce an asymmetry.\r\n\r\n```diff\r\ndiff --git a/tests/postgres_tests/test_array.py b/tests/postgres_tests/test_array.py\r\nindex 392b8f946c..0cb6a223a7 100644\r\n--- a/tests/postgres_tests/test_array.py\r\n+++ b/tests/postgres_tests/test_array.py\r\n@@ -203,6 +203,23 @@ class TestSaveLoad(PostgreSQLTestCase):\r\n         )\r\n         self.assertEqual(instance.field_nested, [[None, None], [None, None]])\r\n \r\n+    def test_top_level_none_for_json_base_field(self):\r\n+        obj = OtherTypesArrayModel.objects.create(json=[None, None])\r\n+        # Current behaviour (bug or design?): JSON null is always saved and\r\n+        # queried for, instead of SQL, as one might expect from the docs at\r\n+        # https://docs.djangoproject.com/en/6.0/topics/db/queries/#storing-and-querying-for-none.\r\n+        # Not sure if this should change, but changing the filter case without\r\n+        # changing how None saves would result in an assymetry: None\r\n+        # would be saved as one value (JSON null) and queried as a different\r\n+        # value (SQL NULL).\r\n+        self.assertSequenceEqual(\r\n+            OtherTypesArrayModel.objects.filter(json__0=None), [obj]\r\n+        )\r\n+        # This fails because json=[None, None] does not map to [NULL, NULL].\r\n+        self.assertSequenceEqual(\r\n+            OtherTypesArrayModel.objects.filter(json__0__isnull=True), [obj]\r\n+        )\r\n+\r\n ```\r\nHow should I proceed?",
      "comment_id": 2414255848,
      "user": "cliffordgama",
      "created_at": "2025-10-08T15:26:40Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2414255848"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "django/db/models/fields/json.py",
      "line": 153,
      "side": "RIGHT",
      "diff_hunk": "@@ -148,6 +150,27 @@ def formfield(self, **kwargs):\n         )\n \n \n+class JSONNull(expressions.Expression):",
      "comment": "Caught up with Simon at Django on the Med and he confirmed that we want `Value` because `JSONNull` is acting like a literal value here.",
      "comment_id": 2415745393,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-09T06:54:52Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2415745393"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "django/db/models/fields/json.py",
      "line": 342,
      "side": "RIGHT",
      "diff_hunk": "@@ -311,14 +334,27 @@ def process_rhs(self, compiler, connection):\n \n \n class JSONExact(lookups.Exact):\n+    # RemovedInDjango70Warning: When the deprecation period is over, remove\n+    # the following line.\n     can_use_none_as_rhs = True\n \n     def process_rhs(self, compiler, connection):\n+        if self.rhs is None and isinstance(self.lhs, expressions.Col):",
      "comment": "1. Re: emitting the warning, catching up with Simon he also suggested removing the `and isinstance(self.lhs, expressions.Col)`, since other left-hand-sides are possible that might yield `JSONNull`,  like a `Case` expression. This makes sure the warning appears before the behavior changes.\n\n2. Re your test: I'm understanding that you're asking whether we should change the current patch, which does have a behavior change in Django 7 for filtering, to *not* have that change so that we keep the symmetry with saving. But as you point out, the current Django 6 behavior for saving JSON subfields in ArrayField contradicts the docs. I chatted with Simon and we agreed we should consider the status quo behavior for saving as a bug, since it doesn't match the docs and what standalone JSONField does. We could fix that in a distinct commit here (with a deprecation cycle).",
      "comment_id": 2415808000,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-09T07:21:07Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2415808000"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "tests/model_fields/test_jsonfield.py",
      "line": 1288,
      "side": "RIGHT",
      "diff_hunk": "@@ -1235,3 +1251,112 @@ def test_literal_annotation_filtering(self):\n             data__foo=\"bar\"\n         )\n         self.assertQuerySetEqual(qs, all_objects)\n+\n+\n+class JSONNullTests(TestCase):\n+    def test_repr(self):\n+        self.assertEqual(repr(JSONNull()), \"JSONNull()\")\n+\n+    def test_save_load(self):\n+        obj = NullableJSONModel(value=JSONNull())\n+        obj.save()\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_create(self):\n+        obj = NullableJSONModel.objects.create(value=JSONNull())\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_update(self):\n+        obj = NullableJSONModel.objects.create(value={\"key\": \"value\"})\n+        NullableJSONModel.objects.update(value=JSONNull())\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_filter(self):\n+        obj = NullableJSONModel.objects.create(value=JSONNull())\n+        found = NullableJSONModel.objects.get(value=JSONNull())\n+        self.assertEqual(obj, found)\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.exclude(value=JSONNull()), []\n+        )\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__isnull=True), []\n+        )\n+\n+    def test_case_expression(self):",
      "comment": "I added a test with a custom encoder. Let me know if it suffices.",
      "comment_id": 2455255484,
      "user": "cliffordgama",
      "created_at": "2025-10-23T14:01:21Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2455255484"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "tests/model_fields/test_jsonfield.py",
      "line": 1272,
      "side": "RIGHT",
      "diff_hunk": "@@ -1235,3 +1251,112 @@ def test_literal_annotation_filtering(self):\n             data__foo=\"bar\"\n         )\n         self.assertQuerySetEqual(qs, all_objects)\n+\n+\n+class JSONNullTests(TestCase):\n+    def test_repr(self):\n+        self.assertEqual(repr(JSONNull()), \"JSONNull()\")\n+\n+    def test_save_load(self):\n+        obj = NullableJSONModel(value=JSONNull())\n+        obj.save()\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_create(self):\n+        obj = NullableJSONModel.objects.create(value=JSONNull())",
      "comment": "I'm not sure that I understand the rationale for the proposal very well.",
      "comment_id": 2455264929,
      "user": "cliffordgama",
      "created_at": "2025-10-23T14:04:12Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2455264929"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "django/db/models/fields/json.py",
      "line": 153,
      "side": "RIGHT",
      "diff_hunk": "@@ -148,6 +150,27 @@ def formfield(self, **kwargs):\n         )\n \n \n+class JSONNull(expressions.Expression):",
      "comment": "Hey @jacobtylerwalls, please advise on how I can subclass `Value` without making `JSONNull()` a mere alias of `Value(None, JSONField())`, since the latter still has bugs like ticket-36445, which was closed in favour of introducing `JSONNull()` and under-promoting `Value(None, JSONField())`.\r\n\r\n(I added `model_fields.test_jsonfield.JSONNullTests.test_bulk_update` to ensure the bug isn't carried over to `JSONNull()` as well.)",
      "comment_id": 2455306538,
      "user": "cliffordgama",
      "created_at": "2025-10-23T14:16:45Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2455306538"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "tests/model_fields/test_jsonfield.py",
      "line": 1272,
      "side": "RIGHT",
      "diff_hunk": "@@ -1235,3 +1251,112 @@ def test_literal_annotation_filtering(self):\n             data__foo=\"bar\"\n         )\n         self.assertQuerySetEqual(qs, all_objects)\n+\n+\n+class JSONNullTests(TestCase):\n+    def test_repr(self):\n+        self.assertEqual(repr(JSONNull()), \"JSONNull()\")\n+\n+    def test_save_load(self):\n+        obj = NullableJSONModel(value=JSONNull())\n+        obj.save()\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_create(self):\n+        obj = NullableJSONModel.objects.create(value=JSONNull())",
      "comment": "Sorry for the shorthand. Consider if I changed the first line of this test to:\r\n```py\r\nobj = NullableJSONModel.objects.create()\r\n```\r\nThe test would still pass, since both SQL NULL and json null load into python as `None`, meaning we didn't really test that json null got persisted.\r\n\r\nIf we use a model that doesn't allow SQL NULL, that wouldn't be possible.",
      "comment_id": 2461662841,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-24T18:48:47Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2461662841"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "django/db/models/fields/json.py",
      "line": 153,
      "side": "RIGHT",
      "diff_hunk": "@@ -148,6 +150,27 @@ def formfield(self, **kwargs):\n         )\n \n \n+class JSONNull(expressions.Expression):",
      "comment": "Thanks for reminding me about that ticket. When I retested the test you supplied there against this branch, it still passes (when rewriting it to use JSONNull) even when JSONNull subclasses `Value`.",
      "comment_id": 2461714448,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-24T19:09:50Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2461714448"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "tests/postgres_tests/test_array.py",
      "line": 1600,
      "side": "RIGHT",
      "diff_hunk": "@@ -1577,3 +1578,30 @@ def test_array_with_choices_display_for_field(self):\n             self.empty_value,\n         )\n         self.assertEqual(display_value, self.empty_value)\n+\n+\n+class TestJSONFieldQuerying(PostgreSQLTestCase):\n+    def test_saving_and_querying_for_sql_null(self):\n+        obj = OtherTypesArrayModel.objects.create(json=[None, None])\n+        self.assertSequenceEqual(\n+            OtherTypesArrayModel.objects.filter(json__1__isnull=True), [obj]\n+        )\n+\n+    def test_saving_and_querying_for_nested_none(self):\n+        obj = OtherTypesArrayModel.objects.create(json=[[None, 1], [None, 2]])\n+        self.assertSequenceEqual(\n+            OtherTypesArrayModel.objects.filter(json__1__0=None), [obj]\n+        )\n+        self.assertSequenceEqual(\n+            OtherTypesArrayModel.objects.filter(json__1__0__isnull=True), []\n+        )\n+\n+    # RemovedInDjango70Warning.\n+    def test_exact_none_deprecation_warning(self):",
      "comment": "Might be nice to rewrite this slightly to assert against the query result so we can keep the test after we remove the deprecation warning.\n\nWhen playing with that locally, I noticed that the behavior of this query already changes in this PR, when I would have expected it to change only in the future/django 7 like JSONField. Do you know why this is?\n\nI don't have an opinion yet if we have to have a deprecation period for this kind of change, if it turns out to be more of a bugfix.",
      "comment_id": 2461791260,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-24T19:44:51Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2461791260"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "tests/model_fields/test_jsonfield.py",
      "line": 1355,
      "side": "RIGHT",
      "diff_hunk": "@@ -1241,3 +1257,166 @@ def test_literal_annotation_filtering(self):\n             data__foo=\"bar\"\n         )\n         self.assertQuerySetEqual(qs, all_objects)\n+\n+\n+class JSONNullTests(TestCase):\n+    def test_repr(self):\n+        self.assertEqual(repr(JSONNull()), \"JSONNull()\")\n+\n+    def test_save_load(self):\n+        obj = NullableJSONModel(value=JSONNull())\n+        obj.save()\n+        self.assertIsNone(obj.value)\n+\n+    def test_create(self):\n+        obj = NullableJSONModel.objects.create(value=JSONNull())\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_update(self):\n+        obj = NullableJSONModel.objects.create(value={\"key\": \"value\"})\n+        NullableJSONModel.objects.update(value=JSONNull())\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_filter(self):\n+        json_null = NullableJSONModel.objects.create(value=JSONNull())\n+        sql_null = NullableJSONModel.objects.create(value=None)\n+        self.assertSequenceEqual(\n+            [json_null], NullableJSONModel.objects.filter(value=JSONNull())\n+        )\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__isnull=True), [sql_null]\n+        )\n+\n+    def test_bulk_update(self):\n+        obj1 = NullableJSONModel.objects.create(value={\"k\": \"1st\"})\n+        obj2 = NullableJSONModel.objects.create(value={\"k\": \"2nd\"})\n+        obj1.value = JSONNull()\n+        obj2.value = JSONNull()\n+        NullableJSONModel.objects.bulk_update([obj1, obj2], fields=[\"value\"])\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value=JSONNull()),\n+            [obj1, obj2],\n+        )\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__isnull=True), []\n+        )\n+\n+    def test_case_expression_with_jsonnull_then(self):\n+        obj = NullableJSONModel.objects.create(value={\"key\": \"value\"})\n+        NullableJSONModel.objects.filter(pk=obj.pk).update(\n+            value=Case(\n+                When(value={\"key\": \"value\"}, then=JSONNull()),\n+            )\n+        )\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_case_expr_with_jsonnull_condition(self):\n+        obj = NullableJSONModel.objects.create(value=JSONNull())\n+        NullableJSONModel.objects.filter(pk=obj.pk).update(\n+            value=Case(\n+                When(\n+                    value=JSONNull(),\n+                    then=Value({\"key\": \"replaced\"}, output_field=JSONField()),\n+                )\n+            ),\n+        )\n+        obj.refresh_from_db()\n+        self.assertEqual(obj.value, {\"key\": \"replaced\"})\n+\n+    def test_key_transform_exact_filter(self):\n+        obj = NullableJSONModel.objects.create(value={\"key\": None})\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__key=JSONNull()),\n+            [obj],\n+        )\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__key=None), [obj]\n+        )\n+\n+    def test_index_lookup(self):\n+        obj = NullableJSONModel.objects.create(value=[\"a\", \"b\", None, 3])\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__2=JSONNull()), [obj]\n+        )\n+        self.assertSequenceEqual(NullableJSONModel.objects.filter(value__2=None), [obj])\n+\n+    @skipUnlessDBFeature(\"supports_table_check_constraints\")\n+    def test_constraint_validation(self):\n+        constraint = CheckConstraint(\n+            condition=~Q(value=JSONNull()), name=\"check_not_json_null\"\n+        )\n+        constraint.validate(NullableJSONModel, NullableJSONModel(value={\"key\": None}))\n+        msg = f\"Constraint \u201c{constraint.name}\u201d is violated.\"\n+        with self.assertRaisesMessage(ValidationError, msg):\n+            constraint.validate(NullableJSONModel, NullableJSONModel(value=JSONNull()))\n+\n+    def test_constraint_validation_key_transform(self):",
      "comment": "```suggestion\n    @skipUnlessDBFeature(\"supports_table_check_constraints\")\n    def test_constraint_validation_key_transform(self):\n```",
      "comment_id": 2461815733,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-24T19:49:37Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2461815733"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "tests/postgres_tests/test_array.py",
      "line": 1600,
      "side": "RIGHT",
      "diff_hunk": "@@ -1577,3 +1578,30 @@ def test_array_with_choices_display_for_field(self):\n             self.empty_value,\n         )\n         self.assertEqual(display_value, self.empty_value)\n+\n+\n+class TestJSONFieldQuerying(PostgreSQLTestCase):\n+    def test_saving_and_querying_for_sql_null(self):\n+        obj = OtherTypesArrayModel.objects.create(json=[None, None])\n+        self.assertSequenceEqual(\n+            OtherTypesArrayModel.objects.filter(json__1__isnull=True), [obj]\n+        )\n+\n+    def test_saving_and_querying_for_nested_none(self):\n+        obj = OtherTypesArrayModel.objects.create(json=[[None, 1], [None, 2]])\n+        self.assertSequenceEqual(\n+            OtherTypesArrayModel.objects.filter(json__1__0=None), [obj]\n+        )\n+        self.assertSequenceEqual(\n+            OtherTypesArrayModel.objects.filter(json__1__0__isnull=True), []\n+        )\n+\n+    # RemovedInDjango70Warning.\n+    def test_exact_none_deprecation_warning(self):",
      "comment": "The behaviour is changing per 7165ee4f8970886251f6061b4ff05a1e1f452c51, and I changed it now as a breaking change, instead of introducing a deprecation, because I couldn't see a deprecation path.",
      "comment_id": 2461846389,
      "user": "cliffordgama",
      "created_at": "2025-10-24T20:02:25Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2461846389"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "tests/postgres_tests/test_array.py",
      "line": 1600,
      "side": "RIGHT",
      "diff_hunk": "@@ -1577,3 +1578,30 @@ def test_array_with_choices_display_for_field(self):\n             self.empty_value,\n         )\n         self.assertEqual(display_value, self.empty_value)\n+\n+\n+class TestJSONFieldQuerying(PostgreSQLTestCase):\n+    def test_saving_and_querying_for_sql_null(self):\n+        obj = OtherTypesArrayModel.objects.create(json=[None, None])\n+        self.assertSequenceEqual(\n+            OtherTypesArrayModel.objects.filter(json__1__isnull=True), [obj]\n+        )\n+\n+    def test_saving_and_querying_for_nested_none(self):\n+        obj = OtherTypesArrayModel.objects.create(json=[[None, 1], [None, 2]])\n+        self.assertSequenceEqual(\n+            OtherTypesArrayModel.objects.filter(json__1__0=None), [obj]\n+        )\n+        self.assertSequenceEqual(\n+            OtherTypesArrayModel.objects.filter(json__1__0__isnull=True), []\n+        )\n+\n+    # RemovedInDjango70Warning.\n+    def test_exact_none_deprecation_warning(self):",
      "comment": " I only made the change for array=[None, ...] when saving, so if it seems that it has also changed for selects, then that was unintentional and I'd have to investigate.",
      "comment_id": 2461854843,
      "user": "cliffordgama",
      "created_at": "2025-10-24T20:05:22Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2461854843"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "tests/postgres_tests/test_array.py",
      "line": 1600,
      "side": "RIGHT",
      "diff_hunk": "@@ -1577,3 +1578,30 @@ def test_array_with_choices_display_for_field(self):\n             self.empty_value,\n         )\n         self.assertEqual(display_value, self.empty_value)\n+\n+\n+class TestJSONFieldQuerying(PostgreSQLTestCase):\n+    def test_saving_and_querying_for_sql_null(self):\n+        obj = OtherTypesArrayModel.objects.create(json=[None, None])\n+        self.assertSequenceEqual(\n+            OtherTypesArrayModel.objects.filter(json__1__isnull=True), [obj]\n+        )\n+\n+    def test_saving_and_querying_for_nested_none(self):\n+        obj = OtherTypesArrayModel.objects.create(json=[[None, 1], [None, 2]])\n+        self.assertSequenceEqual(\n+            OtherTypesArrayModel.objects.filter(json__1__0=None), [obj]\n+        )\n+        self.assertSequenceEqual(\n+            OtherTypesArrayModel.objects.filter(json__1__0__isnull=True), []\n+        )\n+\n+    # RemovedInDjango70Warning.\n+    def test_exact_none_deprecation_warning(self):",
      "comment": "> Might be nice to rewrite this slightly to assert against the query result so we can keep the test after we remove the deprecation warning.\r\n\r\nYes, I agree. It is only like that is because `None` should be matching JSON `null` in the query, but there is currently no way to store a JSON `null` top-level element of an array per[ this thread](https://github.com/django/django/pull/19794#issuecomment-3437321628).",
      "comment_id": 2461865802,
      "user": "cliffordgama",
      "created_at": "2025-10-24T20:11:32Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2461865802"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "django/db/models/fields/json.py",
      "line": 153,
      "side": "RIGHT",
      "diff_hunk": "@@ -148,6 +150,27 @@ def formfield(self, **kwargs):\n         )\n \n \n+class JSONNull(expressions.Expression):",
      "comment": "Thanks :+1:, I managed to get it passing locally as well by keeping the `as_sql()` override.",
      "comment_id": 2461895360,
      "user": "cliffordgama",
      "created_at": "2025-10-24T20:27:48Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2461895360"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "django/db/models/fields/json.py",
      "line": 156,
      "side": "RIGHT",
      "diff_hunk": "@@ -148,6 +150,27 @@ def formfield(self, **kwargs):\n         )\n \n \n+class JSONNull(expressions.Value):\n+    \"\"\"Represent JSON `null` primitive.\"\"\"\n+\n+    def __repr__(self):\n+        return f\"{self.__class__.__name__}()\"\n+\n+    def __init__(self):",
      "comment": "(Sorry to bikeshed here, but I'd place `__init__` first.)",
      "comment_id": 2462868721,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-25T14:41:53Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2462868721"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "django/db/models/fields/json.py",
      "line": 597,
      "side": "RIGHT",
      "diff_hunk": "@@ -552,7 +594,7 @@ def process_rhs(self, compiler, connection):\n \n     def as_oracle(self, compiler, connection):\n         rhs, rhs_params = super().process_rhs(compiler, connection)\n-        if rhs_params == [\"null\"]:\n+        if tuple(rhs_params) == (\"null\",):",
      "comment": "This seems like it was missed in ticket-35972, I'll open a PR against main if so.",
      "comment_id": 2466256686,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-27T16:08:57Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2466256686"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "tests/model_fields/test_jsonfield.py",
      "line": 1392,
      "side": "RIGHT",
      "diff_hunk": "@@ -1241,3 +1258,162 @@ def test_literal_annotation_filtering(self):\n             data__foo=\"bar\"\n         )\n         self.assertQuerySetEqual(qs, all_objects)\n+\n+\n+@skipUnlessDBFeature(\"supports_primitives_in_json_field\")\n+class JSONNullTests(TestCase):\n+    def test_repr(self):\n+        self.assertEqual(repr(JSONNull()), \"JSONNull()\")\n+\n+    def test_save_load(self):\n+        obj = JSONModel(value=JSONNull())\n+        obj.save()\n+        self.assertIsNone(obj.value)\n+\n+    def test_create(self):\n+        obj = JSONModel.objects.create(value=JSONNull())\n+        self.assertIsNone(obj.value)\n+\n+    def test_update(self):\n+        obj = JSONModel.objects.create(value={\"key\": \"value\"})\n+        JSONModel.objects.update(value=JSONNull())\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_filter(self):\n+        json_null = NullableJSONModel.objects.create(value=JSONNull())\n+        sql_null = NullableJSONModel.objects.create(value=None)\n+        self.assertSequenceEqual(\n+            [json_null], NullableJSONModel.objects.filter(value=JSONNull())\n+        )\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__isnull=True), [sql_null]\n+        )\n+\n+    def test_bulk_update(self):\n+        obj1 = NullableJSONModel.objects.create(value={\"k\": \"1st\"})\n+        obj2 = NullableJSONModel.objects.create(value={\"k\": \"2nd\"})\n+        obj1.value = JSONNull()\n+        obj2.value = JSONNull()\n+        NullableJSONModel.objects.bulk_update([obj1, obj2], fields=[\"value\"])\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value=JSONNull()),\n+            [obj1, obj2],\n+        )\n+\n+    def test_case_expression_with_jsonnull_then(self):\n+        obj = JSONModel.objects.create(value={\"key\": \"value\"})\n+        JSONModel.objects.filter(pk=obj.pk).update(\n+            value=Case(\n+                When(value={\"key\": \"value\"}, then=JSONNull()),\n+            )\n+        )\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value)\n+\n+    def test_case_expr_with_jsonnull_condition(self):\n+        obj = NullableJSONModel.objects.create(value=JSONNull())\n+        NullableJSONModel.objects.filter(pk=obj.pk).update(\n+            value=Case(\n+                When(\n+                    value=JSONNull(),\n+                    then=Value({\"key\": \"replaced\"}, output_field=JSONField()),\n+                )\n+            ),\n+        )\n+        obj.refresh_from_db()\n+        self.assertEqual(obj.value, {\"key\": \"replaced\"})\n+\n+    def test_key_transform_exact_filter(self):\n+        obj = NullableJSONModel.objects.create(value={\"key\": None})\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__key=JSONNull()),\n+            [obj],\n+        )\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__key=None), [obj]\n+        )\n+\n+    def test_index_lookup(self):\n+        obj = NullableJSONModel.objects.create(value=[\"a\", \"b\", None, 3])\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__2=JSONNull()), [obj]\n+        )\n+        self.assertSequenceEqual(NullableJSONModel.objects.filter(value__2=None), [obj])\n+\n+    def test_filter_in(self):\n+        obj = NullableJSONModel.objects.create(value=JSONNull())\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__in=[JSONNull()]),\n+            [obj],\n+        )\n+\n+    @skipUnlessDBFeature(\"supports_table_check_constraints\")\n+    def test_constraint_validation(self):\n+        constraint = CheckConstraint(\n+            condition=~Q(value=JSONNull()), name=\"check_not_json_null\"\n+        )\n+        constraint.validate(NullableJSONModel, NullableJSONModel(value={\"key\": None}))\n+        msg = f\"Constraint \u201c{constraint.name}\u201d is violated.\"\n+        with self.assertRaisesMessage(ValidationError, msg):\n+            constraint.validate(NullableJSONModel, NullableJSONModel(value=JSONNull()))\n+\n+    @skipUnlessDBFeature(\"supports_table_check_constraints\")\n+    def test_constraint_validation_key_transform(self):\n+        constraint = CheckConstraint(\n+            condition=Q(value__has_key=\"name\") & ~Q(value__name=JSONNull()),\n+            name=\"check_value_name_not_json_null\",\n+        )\n+        constraint.validate(\n+            NullableJSONModel, NullableJSONModel(value={\"name\": \"Django\"})\n+        )\n+        msg = f\"Constraint \u201c{constraint.name}\u201d is violated.\"\n+        with self.assertRaisesMessage(ValidationError, msg):\n+            constraint.validate(\n+                NullableJSONModel, NullableJSONModel(value={\"name\": None})\n+            )\n+\n+    def test_default(self):\n+        obj = JSONNullDefaultModel.objects.create()\n+        self.assertIsNone(obj.value)\n+\n+    def test_custom_jsonnull_encoder(self):\n+        obj = JSONNullDefaultModel.objects.create(\n+            value={\"name\": JSONNull(), \"array\": [1, JSONNull()]}\n+        )\n+        obj.refresh_from_db()\n+        self.assertIsNone(obj.value[\"name\"])\n+        self.assertEqual(obj.value[\"array\"], [1, None])\n+\n+\n+# RemovedInDjango70Warning.\n+@skipUnlessDBFeature(\"supports_primitives_in_json_field\")\n+class JSONExactNoneDeprecationTests(TestCase):\n+    def setUp(self):",
      "comment": "We usually use `setUpTestData` to avoid recreating an object every time.",
      "comment_id": 2469220343,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-28T11:40:43Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2469220343"
    },
    {
      "repo": "django/django",
      "pr_number": 19794,
      "file_path": "tests/model_fields/test_jsonfield.py",
      "line": 1263,
      "side": "RIGHT",
      "diff_hunk": "@@ -1235,3 +1251,112 @@ def test_literal_annotation_filtering(self):\n             data__foo=\"bar\"\n         )\n         self.assertQuerySetEqual(qs, all_objects)\n+\n+\n+class JSONNullTests(TestCase):\n+    def test_repr(self):\n+        self.assertEqual(repr(JSONNull()), \"JSONNull()\")\n+\n+    def test_save_load(self):\n+        obj = NullableJSONModel(value=JSONNull())\n+        obj.save()\n+        obj.refresh_from_db()",
      "comment": "Spotted this pattern elsewhere and realized why it's here, it's testing the \"load\" of the save/load cycle. So we could have left it in.",
      "comment_id": 2499754162,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-06T16:23:12Z",
      "url": "https://github.com/django/django/pull/19794#discussion_r2499754162"
    },
    {
      "repo": "django/django",
      "pr_number": 19440,
      "file_path": "django/contrib/admin/checks.py",
      "line": 342,
      "side": "RIGHT",
      "diff_hunk": "@@ -333,9 +333,13 @@ def _check_fields(self, obj):\n             ]\n         fields = flatten(obj.fields)\n         if len(fields) != len(set(fields)):\n+            duplicated_fields = list(\n+                sorted({field for field in fields if fields.count(field) > 1})\n+            )\n             return [\n                 checks.Error(\n                     \"The value of 'fields' contains duplicate field(s).\",\n+                    hint=\"Remove duplicates of these field(s): %s.\" % duplicated_fields,",
      "comment": "Favor the usage of a `collections.Counter` for these kind of things to avoid scanning `fields` multiple times\r\n\r\n```suggestion\r\n        fields = Counter(flatten(obj.fields))\r\n        if duplicate_fields := [field for field, count in fields.items() if count > 1]:\r\n            return [\r\n                checks.Error(\r\n                    \"The value of 'fields' contains duplicate field(s).\",\r\n                    hint=\"Remove duplicates of these field(s): %s.\" % duplicated_fields,\r\n```",
      "comment_id": 2070838534,
      "user": "charettes",
      "created_at": "2025-05-01T21:35:00Z",
      "url": "https://github.com/django/django/pull/19440#discussion_r2070838534"
    },
    {
      "repo": "django/django",
      "pr_number": 19440,
      "file_path": "django/contrib/admin/checks.py",
      "line": 337,
      "side": "RIGHT",
      "diff_hunk": "@@ -331,11 +331,14 @@ def _check_fields(self, obj):\n                     id=\"admin.E005\",\n                 )\n             ]\n-        fields = flatten(obj.fields)\n-        if len(fields) != len(set(fields)):\n+        field_counts = collections.Counter(flatten(obj.fields))\n+        if duplicate_fields := sorted(\n+            [field for field, count in field_counts.items() if count > 1]\n+        ):",
      "comment": "You can do without the list comprehension here:\r\n```suggestion\r\n        if duplicate_fields := sorted(\r\n            field for field, count in field_counts.items() if count > 1\r\n        ):\r\n```",
      "comment_id": 2072438910,
      "user": "g-nie",
      "created_at": "2025-05-03T17:56:36Z",
      "url": "https://github.com/django/django/pull/19440#discussion_r2072438910"
    },
    {
      "repo": "django/django",
      "pr_number": 19440,
      "file_path": "django/contrib/admin/checks.py",
      "line": 337,
      "side": "RIGHT",
      "diff_hunk": "@@ -331,11 +331,14 @@ def _check_fields(self, obj):\n                     id=\"admin.E005\",\n                 )\n             ]\n-        fields = flatten(obj.fields)\n-        if len(fields) != len(set(fields)):\n+        field_counts = collections.Counter(flatten(obj.fields))\n+        if duplicate_fields := sorted(\n+            [field for field, count in field_counts.items() if count > 1]\n+        ):",
      "comment": "One more thought, do we need the sorting? If you find it useful for displaying purposes, maybe an idea is to do it only when there are duplicated fields, which has the benefit of not doing extra work when there's no error, i.e:\r\n\r\n```python\r\nif duplicate_fields := [field for field, count in field_counts.items() if count > 1]:\r\n    return [\r\n        checks.Error(\r\n            \"The value of 'fields' contains duplicate field(s).\",\r\n            hint=\"Remove duplicates of these field(s): %s.\" % sorted(duplicate_fields),\r\n            obj=obj.__class__,\r\n            id=\"admin.E006\",\r\n        )\r\n    ]\r\n```",
      "comment_id": 2072441042,
      "user": "g-nie",
      "created_at": "2025-05-03T18:11:03Z",
      "url": "https://github.com/django/django/pull/19440#discussion_r2072441042"
    },
    {
      "repo": "django/django",
      "pr_number": 19440,
      "file_path": "django/contrib/admin/checks.py",
      "line": 337,
      "side": "RIGHT",
      "diff_hunk": "@@ -331,11 +331,14 @@ def _check_fields(self, obj):\n                     id=\"admin.E005\",\n                 )\n             ]\n-        fields = flatten(obj.fields)\n-        if len(fields) != len(set(fields)):\n+        field_counts = collections.Counter(flatten(obj.fields))\n+        if duplicate_fields := sorted(\n+            [field for field, count in field_counts.items() if count > 1]\n+        ):",
      "comment": "@g-nie yeah it was partly just for consistent ordering for the sake of the tests but sorting can happen only on errors to reduce possibly unneeded work, thanks for the suggestion",
      "comment_id": 2072474080,
      "user": "Safrone",
      "created_at": "2025-05-03T22:05:14Z",
      "url": "https://github.com/django/django/pull/19440#discussion_r2072474080"
    },
    {
      "repo": "django/django",
      "pr_number": 19440,
      "file_path": "django/contrib/admin/checks.py",
      "line": 342,
      "side": "RIGHT",
      "diff_hunk": "@@ -331,11 +331,15 @@ def _check_fields(self, obj):\n                     id=\"admin.E005\",\n                 )\n             ]\n-        fields = flatten(obj.fields)\n-        if len(fields) != len(set(fields)):\n+        field_counts = collections.Counter(flatten(obj.fields))\n+        if duplicate_fields := [\n+            field for field, count in field_counts.items() if count > 1\n+        ]:\n             return [\n                 checks.Error(\n                     \"The value of 'fields' contains duplicate field(s).\",\n+                    hint=\"Remove duplicates of these field(s): %s.\"\n+                    % sorted(duplicate_fields),",
      "comment": "I'm not sure it's worth calling `sorted`\r\nI also think I would prefer the hint to say `\"Remove duplicates of 'example_field_1', 'example_field_2'.\"`",
      "comment_id": 2077742615,
      "user": "sarahboyce",
      "created_at": "2025-05-07T14:17:13Z",
      "url": "https://github.com/django/django/pull/19440#discussion_r2077742615"
    },
    {
      "repo": "django/django",
      "pr_number": 19440,
      "file_path": "django/contrib/admin/checks.py",
      "line": 342,
      "side": "RIGHT",
      "diff_hunk": "@@ -331,11 +331,15 @@ def _check_fields(self, obj):\n                     id=\"admin.E005\",\n                 )\n             ]\n-        fields = flatten(obj.fields)\n-        if len(fields) != len(set(fields)):\n+        field_counts = collections.Counter(flatten(obj.fields))\n+        if duplicate_fields := [\n+            field for field, count in field_counts.items() if count > 1\n+        ]:\n             return [\n                 checks.Error(\n                     \"The value of 'fields' contains duplicate field(s).\",\n+                    hint=\"Remove duplicates of these field(s): %s.\"\n+                    % sorted(duplicate_fields),",
      "comment": "Removed the sorting, though I found a slight inaccuracy in the messaging when there are multiple duplicates across fieldsets so I added extra checks to only raise the error for the fields in the current fieldset and added a test for that",
      "comment_id": 2078747195,
      "user": "Safrone",
      "created_at": "2025-05-08T01:29:23Z",
      "url": "https://github.com/django/django/pull/19440#discussion_r2078747195"
    },
    {
      "repo": "django/django",
      "pr_number": 20496,
      "file_path": "django/tasks/base.py",
      "line": 190,
      "side": "RIGHT",
      "diff_hunk": "@@ -187,7 +187,7 @@ class TaskResult:\n     # Time the task was last attempted to be run.\n     last_attempted_at: Optional[datetime]\n \n-    args: list  # Arguments to pass to the task function.\n+    args: list[Any]  # Arguments to pass to the task function.",
      "comment": "Question: is `list` not just a shorthand for `list[Any]`? Does this improve the type signature at all?",
      "comment_id": 2661333487,
      "user": "RealOrangeOne",
      "created_at": "2026-01-05T12:27:40Z",
      "url": "https://github.com/django/django/pull/20496#discussion_r2661333487"
    },
    {
      "repo": "django/django",
      "pr_number": 20496,
      "file_path": "django/tasks/base.py",
      "line": 190,
      "side": "RIGHT",
      "diff_hunk": "@@ -187,7 +187,7 @@ class TaskResult:\n     # Time the task was last attempted to be run.\n     last_attempted_at: Optional[datetime]\n \n-    args: list  # Arguments to pass to the task function.\n+    args: list[Any]  # Arguments to pass to the task function.",
      "comment": "Implicitly, yes, but it's often helpful to be explicit.\n\nIn addition, when enabling mypy's [`--disallow-any-generics`](https://mypy.readthedocs.io/en/stable/command_line.html#cmdoption-mypy-disallow-any-generics), which is included when using the [`--strict`](https://mypy.readthedocs.io/en/stable/command_line.html#cmdoption-mypy-strict) flag, implicit type parameters will error.",
      "comment_id": 2662329817,
      "user": "ngnpope",
      "created_at": "2026-01-05T17:50:24Z",
      "url": "https://github.com/django/django/pull/20496#discussion_r2662329817"
    },
    {
      "repo": "django/django",
      "pr_number": 20321,
      "file_path": "django/db/models/sql/compiler.py",
      "line": 1628,
      "side": "RIGHT",
      "diff_hunk": "@@ -1621,9 +1621,15 @@ def execute_sql(\n             cursor = self.connection.cursor()\n         try:\n             cursor.execute(sql, params)\n-        except Exception:\n+        except Exception as e:\n             # Might fail for server-side cursors (e.g. connection closed)\n-            cursor.close()\n+            try:\n+                cursor.close()\n+            except Exception:",
      "comment": "```suggestion\n            except DatabaseError:\n```",
      "comment_id": 2624538705,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-16T19:49:43Z",
      "url": "https://github.com/django/django/pull/20321#discussion_r2624538705"
    },
    {
      "repo": "django/django",
      "pr_number": 20321,
      "file_path": "django/db/models/sql/compiler.py",
      "line": 1633,
      "side": "RIGHT",
      "diff_hunk": "@@ -1621,9 +1621,15 @@ def execute_sql(\n             cursor = self.connection.cursor()\n         try:\n             cursor.execute(sql, params)\n-        except Exception:\n+        except Exception as e:\n             # Might fail for server-side cursors (e.g. connection closed)",
      "comment": "Please move this comment into the new inner except.",
      "comment_id": 2624540432,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-16T19:50:24Z",
      "url": "https://github.com/django/django/pull/20321#discussion_r2624540432"
    },
    {
      "repo": "django/django",
      "pr_number": 20321,
      "file_path": "tests/queries/test_execute_sql.py",
      "line": 37,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,37 @@\n+from unittest import mock\n+\n+from django.db import DatabaseError, connection, models\n+from django.db.models.sql.compiler import SQLCompiler\n+from django.db.models.sql.query import Query\n+from django.test import TestCase\n+\n+\n+class DummyModel(models.Model):\n+    class Meta:\n+        app_label = \"tests\"\n+        managed = False\n+\n+\n+class ExecuteSqlCursorCloseErrorTests(TestCase):\n+    def test_execute_sql_raises_original_exception_when_close_fails(self):\n+        # Build compiler\n+        query = Query(DummyModel)\n+        compiler = SQLCompiler(query, connection, None)\n+\n+        # Fake cursor\n+        cursor = mock.MagicMock()\n+\n+        # Step 1: execution fails\n+        execute_err = DatabaseError(\"execute failed\")\n+        cursor.execute.side_effect = execute_err\n+\n+        # Step 2: closing fails\n+        cursor.close.side_effect = DatabaseError(\"close failed\")\n+\n+        # Patch connection.cursor() to use our fake cursor\n+        with mock.patch.object(connection, \"cursor\", return_value=cursor):\n+            with self.assertRaises(DatabaseError) as ctx:\n+                compiler.execute_sql(\"SELECT 1\", [])\n+\n+        # Must be *execute* error, not close error\n+        assert str(ctx.exception) == \"execute failed\"",
      "comment": "Assert against `ctx.exception.__cause__` and `__suppress_context__` similar to `test_fetch_mode_raise_forward` and others.",
      "comment_id": 2624545645,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-16T19:52:12Z",
      "url": "https://github.com/django/django/pull/20321#discussion_r2624545645"
    },
    {
      "repo": "django/django",
      "pr_number": 20321,
      "file_path": "tests/queries/test_execute_sql.py",
      "line": 17,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,37 @@\n+from unittest import mock\n+\n+from django.db import DatabaseError, connection, models\n+from django.db.models.sql.compiler import SQLCompiler\n+from django.db.models.sql.query import Query\n+from django.test import TestCase\n+\n+\n+class DummyModel(models.Model):\n+    class Meta:\n+        app_label = \"tests\"\n+        managed = False\n+\n+\n+class ExecuteSqlCursorCloseErrorTests(TestCase):\n+    def test_execute_sql_raises_original_exception_when_close_fails(self):\n+        # Build compiler",
      "comment": "You can remove most of these comments. You can leave the execution fails & closing fails comments if you like, as that's the interesting part -- the fact that we need nested failures.",
      "comment_id": 2624547551,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-16T19:52:53Z",
      "url": "https://github.com/django/django/pull/20321#discussion_r2624547551"
    },
    {
      "repo": "django/django",
      "pr_number": 20321,
      "file_path": "tests/queries/test_execute_sql.py",
      "line": 16,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,37 @@\n+from unittest import mock\n+\n+from django.db import DatabaseError, connection, models\n+from django.db.models.sql.compiler import SQLCompiler\n+from django.db.models.sql.query import Query\n+from django.test import TestCase\n+\n+\n+class DummyModel(models.Model):\n+    class Meta:\n+        app_label = \"tests\"\n+        managed = False\n+\n+\n+class ExecuteSqlCursorCloseErrorTests(TestCase):\n+    def test_execute_sql_raises_original_exception_when_close_fails(self):",
      "comment": "The original exception was already raised, it just had extra context, so I'd suggest tweaking this title.",
      "comment_id": 2624566342,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-16T20:00:04Z",
      "url": "https://github.com/django/django/pull/20321#discussion_r2624566342"
    },
    {
      "repo": "django/django",
      "pr_number": 20321,
      "file_path": "tests/queries/test_execute_sql.py",
      "line": 12,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,37 @@\n+from unittest import mock\n+\n+from django.db import DatabaseError, connection, models\n+from django.db.models.sql.compiler import SQLCompiler\n+from django.db.models.sql.query import Query\n+from django.test import TestCase\n+\n+\n+class DummyModel(models.Model):\n+    class Meta:\n+        app_label = \"tests\"\n+        managed = False",
      "comment": "You can remove the model:\n\n```diff\ndiff --git a/tests/queries/test_execute_sql.py b/tests/queries/test_execute_sql.py\nindex 6e9e578093..213ac21f53 100644\n--- a/tests/queries/test_execute_sql.py\n+++ b/tests/queries/test_execute_sql.py\n@@ -1,21 +1,15 @@\n from unittest import mock\n \n-from django.db import DatabaseError, connection, models\n+from django.db import DatabaseError, connection\n from django.db.models.sql.compiler import SQLCompiler\n from django.db.models.sql.query import Query\n from django.test import TestCase\n \n \n-class DummyModel(models.Model):\n-    class Meta:\n-        app_label = \"tests\"\n-        managed = False\n-\n-\n class ExecuteSqlCursorCloseErrorTests(TestCase):\n     def test_execute_sql_raises_original_exception_when_close_fails(self):\n         # Build compiler\n-        query = Query(DummyModel)\n+        query = Query(None)\n         compiler = SQLCompiler(query, connection, None)\n \n         # Fake cursor\n```",
      "comment_id": 2624575895,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-16T20:03:33Z",
      "url": "https://github.com/django/django/pull/20321#discussion_r2624575895"
    },
    {
      "repo": "django/django",
      "pr_number": 20471,
      "file_path": "django/contrib/admin/widgets.py",
      "line": 125,
      "side": "RIGHT",
      "diff_hunk": "@@ -122,6 +122,7 @@ class AdminRadioSelect(forms.RadioSelect):\n \n class AdminFileWidget(forms.ClearableFileInput):\n     template_name = \"admin/widgets/clearable_file_input.html\"\n+    use_fieldset = True",
      "comment": "The AdminFileWidget class now explicitly sets use_fieldset to True, but there's no test coverage to verify this attribute value. Since the base ClearableFileInput has test coverage for use_fieldset in test_clearablefileinput.py, consider adding a similar test for AdminFileWidget to prevent future regressions of this attribute.",
      "comment_id": 2649172415,
      "user": "Copilot",
      "created_at": "2025-12-27T14:49:03Z",
      "url": "https://github.com/django/django/pull/20471#discussion_r2649172415"
    },
    {
      "repo": "django/django",
      "pr_number": 20471,
      "file_path": "tests/admin_widgets/tests.py",
      "line": 597,
      "side": "RIGHT",
      "diff_hunk": "@@ -582,6 +594,9 @@ def test_attrs(self):\n \n @override_settings(ROOT_URLCONF=\"admin_widgets.urls\")\n class AdminFileWidgetTests(TestDataMixin, TestCase):\n+    def setUp(self):",
      "comment": "I'll revert the changes to this file, as the admin's rendering of file fields with a `<fieldset>` was already covered in `test_use_fieldset_fields_render()`:\n```py\n======================================================================\nFAIL: test_use_fieldset_fields_render (admin_views.tests.SeleniumTests.test_use_fieldset_fields_render)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/Users/jwalls/django/tests/admin_views/tests.py\", line 6960, in test_use_fieldset_fields_render\n    self.assertEqual(legend.text, expected_legend_tags_text[index])\n    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: 'Difficulty:' != 'Materials:'\n- Difficulty:\n+ Materials:\n\n\n----------------------------------------------------------------------\nRan 1 test in 10.571s\n```",
      "comment_id": 2655423077,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-31T13:43:04Z",
      "url": "https://github.com/django/django/pull/20471#discussion_r2655423077"
    },
    {
      "repo": "django/django",
      "pr_number": 20288,
      "file_path": "tests/signals/tests.py",
      "line": 681,
      "side": "RIGHT",
      "diff_hunk": "@@ -645,3 +646,238 @@ async def test_asend_robust_only_async_receivers(self):\n \n         result = await signal.asend_robust(self.__class__)\n         self.assertEqual(result, [(async_handler, 1)])\n+\n+\n+class TestReceiversContextVarsSharing(SimpleTestCase):\n+    def setUp(self):\n+        self.ctx_var = contextvars.ContextVar(\"test_var\", default=0)\n+\n+        class CtxSyncHandler:\n+            def __init__(self, ctx_var):\n+                self.ctx_var = ctx_var\n+                self.values = []\n+\n+            def __call__(self, **kwargs):\n+                val = self.ctx_var.get()\n+                self.ctx_var.set(val + 1)\n+                self.values.append(self.ctx_var.get())\n+                return self.ctx_var.get()\n+\n+        class CtxAsyncHandler:\n+            def __init__(self, ctx_var):\n+                self.ctx_var = ctx_var\n+                self.values = []\n+                markcoroutinefunction(self)\n+\n+            async def __call__(self, **kwargs):\n+                val = self.ctx_var.get()\n+                self.ctx_var.set(val + 1)\n+                self.values.append(self.ctx_var.get())\n+                return self.ctx_var.get()\n+\n+        self.CtxSyncHandler = CtxSyncHandler\n+        self.CtxAsyncHandler = CtxAsyncHandler\n+\n+    async def test_asend_correct_contextvars_sharing_async_receivers(self):",
      "comment": "I tried using `.subTest` to reduce the number of tests for `asend/asend_robust` and `send/send_robust`, but the error output became unreadable.\r\n\r\n```\r\n  File \"/Users/m.havelya/work/django/django/test/testcases.py\", line 87, in is_pickable\r\n    pickle.loads(pickle.dumps(obj))\r\n    ^^^^^^^^^^^^^^^\r\n\r\n   ....\r\n\r\n  File \"/Users/m.havelya/work/django/django/test/testcases.py\", line 325, in __getstate__\r\n    if key in pickable_state or not is_pickable(value):\r\n    ^^^^^^^\r\n  File \"/Users/m.havelya/work/django/django/test/testcases.py\", line 87, in is_pickable\r\n    pickle.loads(pickle.dumps(obj))\r\n    ^^^^^^^^^^^^^^^\r\n  File \"/Users/m.havelya/work/django/django/test/testcases.py\", line 325, in __getstate__\r\n    if key in pickable_state or not is_pickable(value):\r\n    ^^^^^^^\r\nRecursionError: maximum recursion depth exceeded\r\n\r\n----------------------------------------------------------------------\r\nRan 31 tests in 0.439s\r\n\r\nFAILED (errors=1)\r\n```\r\n\r\nSo I dropped the idea. Using separate tests is more readable.",
      "comment_id": 2549721274,
      "user": "Arfey",
      "created_at": "2025-11-21T13:12:05Z",
      "url": "https://github.com/django/django/pull/20288#discussion_r2549721274"
    },
    {
      "repo": "django/django",
      "pr_number": 20288,
      "file_path": "django/dispatch/dispatcher.py",
      "line": 26,
      "side": "RIGHT",
      "diff_hunk": "@@ -22,26 +23,41 @@ def _make_id(target):\n NO_RECEIVERS = object()\n \n \n-async def _gather(*coros):\n-    if len(coros) == 0:\n-        return []\n+async def _run_parallel(*coroutines):",
      "comment": "Summary:\r\n\r\n- Removed the `len(coros) == 1` optimization. It doesn\u2019t appear to provide any real benefit.\r\n\r\n```\r\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\r\n\u2503                        Benchmark \u2503 Min     \u2503 Max     \u2503 Mean    \u2503 Min (+)         \u2503 Max (+)         \u2503 Mean (+)        \u2503\r\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\r\n\u2502 TaskGroup instead of create_task \u2502 0.001   \u2502 0.001   \u2502 0.001   \u2502 0.001 (1.1x)    \u2502 0.002 (-1.1x)   \u2502 0.001 (1.1x)    \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n- All coroutines now share the same context, which is restored to the current context after completion.\r\n\r\n\r\nbtw:\r\n\r\n- `_restore_context` is just a copy-paste from the `asgiref` library",
      "comment_id": 2589828115,
      "user": "Arfey",
      "created_at": "2025-12-04T16:48:06Z",
      "url": "https://github.com/django/django/pull/20288#discussion_r2589828115"
    },
    {
      "repo": "django/django",
      "pr_number": 20288,
      "file_path": "django/dispatch/dispatcher.py",
      "line": 319,
      "side": "RIGHT",
      "diff_hunk": "@@ -290,14 +308,12 @@ def sync_send():\n             async def sync_send():\n                 return []\n \n-        responses, async_responses = await _gather(\n-            sync_send(),\n-            _gather(\n-                *(\n-                    receiver(signal=self, sender=sender, **named)\n-                    for receiver in async_receivers\n-                )\n-            ),\n+        responses = await sync_send()\n+        async_responses = await _run_parallel(\n+            *(\n+                receiver(signal=self, sender=sender, **named)\n+                for receiver in async_receivers\n+            )",
      "comment": "Should we update the docstrings for `asend()`/`asend_robust()` now? It reads, \"If any receivers are asynchronous, they are grouped and executed concurrently...\", which is a little bit ambiguous, but if \"they\" is \"all the receivers\", then they're not all concurrent anymore.",
      "comment_id": 2627697128,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-17T16:17:01Z",
      "url": "https://github.com/django/django/pull/20288#discussion_r2627697128"
    },
    {
      "repo": "django/django",
      "pr_number": 20288,
      "file_path": "django/dispatch/dispatcher.py",
      "line": 33,
      "side": "RIGHT",
      "diff_hunk": "@@ -22,26 +23,42 @@ def _make_id(target):\n NO_RECEIVERS = object()\n \n \n-async def _gather(*coros):\n+async def _run_parallel(*coros):\n+    \"\"\"\n+    Execute multiple asynchronous coroutines in parallel,\n+    sharing the current context between them.\n+    \"\"\"\n+    context = contextvars.copy_context()\n+\n+    def _restore_context(context):",
      "comment": "I see we're coping this from `asgiref`. Is that only because it's marked as private? We could ask Carlton if he minds us importing it and/or if he wants to make it public in the next release.",
      "comment_id": 2627755315,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-17T16:31:33Z",
      "url": "https://github.com/django/django/pull/20288#discussion_r2627755315"
    },
    {
      "repo": "django/django",
      "pr_number": 20288,
      "file_path": "django/dispatch/dispatcher.py",
      "line": 33,
      "side": "RIGHT",
      "diff_hunk": "@@ -22,26 +23,42 @@ def _make_id(target):\n NO_RECEIVERS = object()\n \n \n-async def _gather(*coros):\n+async def _run_parallel(*coros):\n+    \"\"\"\n+    Execute multiple asynchronous coroutines in parallel,\n+    sharing the current context between them.\n+    \"\"\"\n+    context = contextvars.copy_context()\n+\n+    def _restore_context(context):",
      "comment": "Also, it would be nice to lift this up out of the function definition.",
      "comment_id": 2627809498,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-17T16:47:28Z",
      "url": "https://github.com/django/django/pull/20288#discussion_r2627809498"
    },
    {
      "repo": "django/django",
      "pr_number": 20288,
      "file_path": "django/dispatch/dispatcher.py",
      "line": 319,
      "side": "RIGHT",
      "diff_hunk": "@@ -290,14 +308,12 @@ def sync_send():\n             async def sync_send():\n                 return []\n \n-        responses, async_responses = await _gather(\n-            sync_send(),\n-            _gather(\n-                *(\n-                    receiver(signal=self, sender=sender, **named)\n-                    for receiver in async_receivers\n-                )\n-            ),\n+        responses = await sync_send()\n+        async_responses = await _run_parallel(\n+            *(\n+                receiver(signal=self, sender=sender, **named)\n+                for receiver in async_receivers\n+            )",
      "comment": "Never mind, looking at the context right before this, I guess \"they\" means the asynchronous ones, so this should be fine.",
      "comment_id": 2633063601,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-18T23:57:19Z",
      "url": "https://github.com/django/django/pull/20288#discussion_r2633063601"
    },
    {
      "repo": "django/django",
      "pr_number": 20288,
      "file_path": "django/dispatch/dispatcher.py",
      "line": 33,
      "side": "RIGHT",
      "diff_hunk": "@@ -22,26 +23,42 @@ def _make_id(target):\n NO_RECEIVERS = object()\n \n \n-async def _gather(*coros):\n+async def _run_parallel(*coros):\n+    \"\"\"\n+    Execute multiple asynchronous coroutines in parallel,\n+    sharing the current context between them.\n+    \"\"\"\n+    context = contextvars.copy_context()\n+\n+    def _restore_context(context):",
      "comment": "- Is that only because it's marked as private? - yes \ud83d\ude0c\r\n\r\n- We could ask Carlton if he minds us importing it and/or if he wants to make it public in the next release - it's much more robust to keep them separate\r\n",
      "comment_id": 2640493507,
      "user": "Arfey",
      "created_at": "2025-12-22T16:40:43Z",
      "url": "https://github.com/django/django/pull/20288#discussion_r2640493507"
    },
    {
      "repo": "django/django",
      "pr_number": 20288,
      "file_path": "django/dispatch/dispatcher.py",
      "line": 33,
      "side": "RIGHT",
      "diff_hunk": "@@ -22,26 +23,42 @@ def _make_id(target):\n NO_RECEIVERS = object()\n \n \n-async def _gather(*coros):\n+async def _run_parallel(*coros):\n+    \"\"\"\n+    Execute multiple asynchronous coroutines in parallel,\n+    sharing the current context between them.\n+    \"\"\"\n+    context = contextvars.copy_context()\n+\n+    def _restore_context(context):",
      "comment": "- Also, it would be nice to lift this up out of the function definition.\r\n\r\ndone \ud83d\ude42",
      "comment_id": 2640505886,
      "user": "Arfey",
      "created_at": "2025-12-22T16:45:53Z",
      "url": "https://github.com/django/django/pull/20288#discussion_r2640505886"
    },
    {
      "repo": "django/django",
      "pr_number": 20445,
      "file_path": "django/db/models/fields/__init__.py",
      "line": 1734,
      "side": "RIGHT",
      "diff_hunk": "@@ -1715,40 +1715,46 @@ def __init__(\n \n     def check(self, **kwargs):\n         errors = super().check(**kwargs)\n+        databases = kwargs.get(\"databases\") or []\n \n         digits_errors = [\n-            *self._check_decimal_places(),\n-            *self._check_max_digits(),\n+            *self._check_decimal_places(databases),\n+            *self._check_max_digits(databases),\n         ]\n         if not digits_errors:\n             errors.extend(self._check_decimal_places_and_max_digits(**kwargs))\n         else:\n             errors.extend(digits_errors)\n         return errors\n \n-    def _check_decimal_places(self):\n+    def _check_decimal_places(self, databases):\n         if self.decimal_places is None:\n-            if (\n-                not connection.features.supports_no_precision_decimalfield\n-                and \"supports_no_precision_decimalfield\"\n-                not in self.model._meta.required_db_features\n-            ):\n-                return [\n-                    checks.Error(\n-                        \"DecimalFields must define a 'decimal_places' attribute.\",\n-                        obj=self,\n-                        id=\"fields.E130\",\n-                    )\n-                ]\n-            elif self.max_digits is not None:\n-                return [\n-                    checks.Error(\n-                        \"DecimalField\u2019s max_digits and decimal_places must both \"\n-                        \"be defined or both omitted.\",\n-                        obj=self,\n-                        id=\"fields.E135\",\n-                    ),\n-                ]\n+            for db in databases:\n+                if not router.allow_migrate_model(db, self.model):\n+                    continue",
      "comment": "Any thoughts about moving this skip to `check`? I wonder if it might be worth adjusting `Model.check` to eventually skip delegation when `not router.allow_migrate_model(db, self.model)`.",
      "comment_id": 2644234920,
      "user": "charettes",
      "created_at": "2025-12-23T20:49:33Z",
      "url": "https://github.com/django/django/pull/20445#discussion_r2644234920"
    },
    {
      "repo": "django/django",
      "pr_number": 20445,
      "file_path": "django/db/models/fields/__init__.py",
      "line": 1777,
      "side": "RIGHT",
      "diff_hunk": "@@ -1764,29 +1770,32 @@ def _check_decimal_places(self):\n                 ]\n         return []\n \n-    def _check_max_digits(self):\n+    def _check_max_digits(self, databases):\n         if self.max_digits is None:\n-            if (\n-                not connection.features.supports_no_precision_decimalfield\n-                and \"supports_no_precision_decimalfield\"\n-                not in self.model._meta.required_db_features\n-            ):\n-                return [\n-                    checks.Error(\n-                        \"DecimalFields must define a 'max_digits' attribute.\",\n-                        obj=self,\n-                        id=\"fields.E132\",\n-                    )\n-                ]\n-            elif self.decimal_places is not None:\n-                return [\n-                    checks.Error(\n-                        \"DecimalField\u2019s max_digits and decimal_places must both \"\n-                        \"be defined or both omitted.\",\n-                        obj=self,\n-                        id=\"fields.E135\",\n-                    ),\n-                ]\n+            for db in databases:\n+                if not router.allow_migrate_model(db, self.model):\n+                    continue",
      "comment": "I think you're missing `connection = connections[db]` here?",
      "comment_id": 2644235375,
      "user": "charettes",
      "created_at": "2025-12-23T20:49:50Z",
      "url": "https://github.com/django/django/pull/20445#discussion_r2644235375"
    },
    {
      "repo": "django/django",
      "pr_number": 20445,
      "file_path": "django/db/models/fields/__init__.py",
      "line": 1734,
      "side": "RIGHT",
      "diff_hunk": "@@ -1715,40 +1715,46 @@ def __init__(\n \n     def check(self, **kwargs):\n         errors = super().check(**kwargs)\n+        databases = kwargs.get(\"databases\") or []\n \n         digits_errors = [\n-            *self._check_decimal_places(),\n-            *self._check_max_digits(),\n+            *self._check_decimal_places(databases),\n+            *self._check_max_digits(databases),\n         ]\n         if not digits_errors:\n             errors.extend(self._check_decimal_places_and_max_digits(**kwargs))\n         else:\n             errors.extend(digits_errors)\n         return errors\n \n-    def _check_decimal_places(self):\n+    def _check_decimal_places(self, databases):\n         if self.decimal_places is None:\n-            if (\n-                not connection.features.supports_no_precision_decimalfield\n-                and \"supports_no_precision_decimalfield\"\n-                not in self.model._meta.required_db_features\n-            ):\n-                return [\n-                    checks.Error(\n-                        \"DecimalFields must define a 'decimal_places' attribute.\",\n-                        obj=self,\n-                        id=\"fields.E130\",\n-                    )\n-                ]\n-            elif self.max_digits is not None:\n-                return [\n-                    checks.Error(\n-                        \"DecimalField\u2019s max_digits and decimal_places must both \"\n-                        \"be defined or both omitted.\",\n-                        obj=self,\n-                        id=\"fields.E135\",\n-                    ),\n-                ]\n+            for db in databases:\n+                if not router.allow_migrate_model(db, self.model):\n+                    continue",
      "comment": "The second branch (when `decimal_places` is not `None`) should always run, that's why I put it here.",
      "comment_id": 2644249178,
      "user": "felixxm",
      "created_at": "2025-12-23T20:58:21Z",
      "url": "https://github.com/django/django/pull/20445#discussion_r2644249178"
    },
    {
      "repo": "django/django",
      "pr_number": 20312,
      "file_path": "tests/backends/base/test_base.py",
      "line": 470,
      "side": "RIGHT",
      "diff_hunk": "@@ -436,3 +437,34 @@ def test_multi_database_init_connection_state_called_once(self):\n                             len(mocked_check_database_version_supported.mock_calls),\n                             after_first_calls,\n                         )\n+\n+\n+class DummyCursor:\n+    def __init__(self, statement=None):\n+        self.statement = statement\n+\n+\n+class DummyConnection:\n+    vendor = \"sqlite\"\n+    alias = \"default\"\n+    features = MagicMock()\n+    settings_dict = {}\n+\n+    def __init__(self):\n+        self.ops = DummyBackendOperations(self)\n+        self.queries_log = []\n+\n+\n+class DummyBackendOperations(BaseDatabaseOperations):\n+    \"\"\"\n+    Simulates a backend with the fallback behavior implemented in the patch:\n+    - If cursor.statement is truthy, return it.\n+    - If cursor.statement is None, fallback to the base implementation.\n+    \"\"\"\n+\n+    def __init__(self, connection):\n+        super().__init__(connection)\n+\n+    def last_executed_query(self, cursor, sql, params):\n+        statement = getattr(cursor, \"statement\", None)\n+        return statement or super().last_executed_query(cursor, sql, params)",
      "comment": "This is not a regression test. TBH it is not a test at all.",
      "comment_id": 2566338258,
      "user": "felixxm",
      "created_at": "2025-11-26T19:57:10Z",
      "url": "https://github.com/django/django/pull/20312#discussion_r2566338258"
    },
    {
      "repo": "django/django",
      "pr_number": 20312,
      "file_path": "tests/backends/oracle/test_last_executed_query.py",
      "line": 35,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,35 @@\n+from unittest import skipUnless\n+\n+from django.db import connection\n+from django.db.backends.base.operations import BaseDatabaseOperations\n+from django.test import TestCase\n+\n+# Oracle backend import guard.\n+# Importing oracle.operations will fail on\n+# environments without the oracledb driver.\n+try:\n+    from django.db.backends.oracle.operations import DatabaseOperations\n+except Exception:\n+    DatabaseOperations = None\n+\n+\n+@skipUnless(connection.vendor == \"oracle\", \"Oracle-specific tests\")\n+class TestLastExecutedQueryFallback(TestCase):\n+    def test_last_executed_query_fallback(self):\n+        if DatabaseOperations is None:\n+            self.skipTest(\"Oracle backend not available\")\n+\n+        class FakeCursor:\n+            # Simulates a cursor lacking the `statement` attribute\n+            pass\n+\n+        cursor = FakeCursor()\n+        sql = \"SELECT 1\"\n+        params = []\n+\n+        ops = DatabaseOperations(None)\n+\n+        # Expected fallback from BaseDatabaseOperations\n+        expected = BaseDatabaseOperations(None).last_executed_query(cursor, sql, params)\n+\n+        self.assertEqual(ops.last_executed_query(cursor, sql, params), expected)",
      "comment": "Why do you need a new instance of `DatabaseOperations`? Also it should be enough to check that result of `last_executed_query()` is not `None`.\r\n```suggestion\r\n\r\n@skipUnless(connection.vendor == \"oracle\", \"Oracle specific tests\")\r\nclass TestLastExecutedQueryFallback(TestCase):\r\n    def test_last_executed_query_fallback(self):\r\n        class FakeCursor:\r\n            # Simulates a cursor lacking the `statement` attribute\r\n            pass\r\n\r\n        cursor = FakeCursor()\r\n        sql = \"SELECT 1\"\r\n        params = []\r\n\r\n        self.assertIsNotNone(connection.ops.last_executed_query(cursor, sql, params))\r\n```\r\n\r\nWe could also use a real cursor with an invalid SQL statement, e.g.\r\n```\r\nwith connection.cursor() as cursor:\r\n    sql = \"INVALID SQL\"\r\n    params = []\r\n    cursor.execute(sql, params)\r\n    self.assertEqual(connection.ops.last_executed_query(cursor, sql, params), sql)\r\n```",
      "comment_id": 2567491066,
      "user": "felixxm",
      "created_at": "2025-11-27T07:52:28Z",
      "url": "https://github.com/django/django/pull/20312#discussion_r2567491066"
    },
    {
      "repo": "django/django",
      "pr_number": 20312,
      "file_path": "tests/backends/oracle/test_last_executed_query.py",
      "line": 21,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,21 @@\n+from django.db import connection\n+from django.test import TestCase\n+from django.test.utils import skipUnless\n+\n+\n+@skipUnless(connection.vendor == \"oracle\", \"Oracle-specific tests\")\n+class TestLastExecutedQueryFallback(TestCase):\n+    def test_last_executed_query_fallback(self):\n+        # Use a real Oracle cursor and force an error\n+        with connection.cursor() as cursor:\n+            sql = \"INVALID SQL\"\n+            params = []\n+            try:\n+                cursor.execute(sql, params)\n+            except Exception:\n+                pass\n+\n+            # The result MUST NOT be None and MUST fall back to the SQL string\n+            result = connection.ops.last_executed_query(cursor, sql, params)\n+            self.assertIsNotNone(result)\n+            self.assertEqual(result, sql)",
      "comment": "`assertIsNotNone` is redundant\r\n```suggestion\r\n            self.assertEqual(result, sql)\r\n```",
      "comment_id": 2580489793,
      "user": "felixxm",
      "created_at": "2025-12-02T10:06:22Z",
      "url": "https://github.com/django/django/pull/20312#discussion_r2580489793"
    },
    {
      "repo": "django/django",
      "pr_number": 20312,
      "file_path": "tests/backends/oracle/test_last_executed_query.py",
      "line": 8,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,21 @@\n+from django.db import connection\n+from django.test import TestCase\n+from django.test.utils import skipUnless\n+\n+\n+@skipUnless(connection.vendor == \"oracle\", \"Oracle-specific tests\")\n+class TestLastExecutedQueryFallback(TestCase):\n+    def test_last_executed_query_fallback(self):",
      "comment": "As far as I'm aware, this test should pass for all database backends, so we can move it to the \r\n`tests.backends.base.test_operations.DatabaseOperationTests` class.",
      "comment_id": 2580496057,
      "user": "felixxm",
      "created_at": "2025-12-02T10:08:10Z",
      "url": "https://github.com/django/django/pull/20312#discussion_r2580496057"
    },
    {
      "repo": "django/django",
      "pr_number": 20312,
      "file_path": "django/db/backends/oracle/operations.py",
      "line": 355,
      "side": "RIGHT",
      "diff_hunk": "@@ -352,7 +352,7 @@ def last_executed_query(self, cursor, sql, params):\n                 statement = statement.replace(\n                     key, force_str(params[key], errors=\"replace\")\n                 )\n-        return statement\n+        return statement or super().last_executed_query(cursor, sql, params)",
      "comment": "I think we should use `super().last_executed_query()` only when `statement` is None\r\n```suggestion\r\n        return super().last_executed_query(cursor, sql, params) if statement is None else statement \r\n```",
      "comment_id": 2580501766,
      "user": "felixxm",
      "created_at": "2025-12-02T10:09:38Z",
      "url": "https://github.com/django/django/pull/20312#discussion_r2580501766"
    },
    {
      "repo": "django/django",
      "pr_number": 20312,
      "file_path": "tests/backends/base/test_operations.py",
      "line": 180,
      "side": "RIGHT",
      "diff_hunk": "@@ -171,6 +171,20 @@ class DatabaseOperationTests(TestCase):\n     def setUp(self):\n         self.ops = BaseDatabaseOperations(connection=connection)\n \n+    def test_last_executed_query_fallback(self):\n+        with connection.cursor() as cursor:\n+            sql = \"INVALID SQL\"\n+            params = []\n+\n+            try:\n+                cursor.execute(sql, params)",
      "comment": "This test pass even without a fix, you need to `close()` the cursor to have a proper regression test.\r\n```suggestion\r\n                cursor.close()\r\n                cursor.execute(sql, params)\r\n```",
      "comment_id": 2648658295,
      "user": "felixxm",
      "created_at": "2025-12-26T20:51:17Z",
      "url": "https://github.com/django/django/pull/20312#discussion_r2648658295"
    },
    {
      "repo": "django/django",
      "pr_number": 20437,
      "file_path": "tests/lookup/tests.py",
      "line": 1864,
      "side": "RIGHT",
      "diff_hunk": "@@ -1855,6 +1855,14 @@ def test_in_lookup_in_filter(self):\n                     Season.objects.filter(In(F(\"year\"), years)).order_by(\"pk\"), seasons\n                 )\n \n+    def test_in_lookup_in_filter_text_field(self):\n+        self.assertSequenceEqual(\n+            Season.objects.filter(\n+                In(F(\"nulled_text_field\"), [F(\"nulled_text_field\"), \"special_value\"])\n+            ),\n+            [self.s2],\n+        )",
      "comment": "This test crashes on Oracle because blob fields (e.g. `NCLOB`) are not allowed in the `IN()` clause. As far as I'm aware `TextFields` are unnecessary here, see #20453.",
      "comment_id": 2644244696,
      "user": "felixxm",
      "created_at": "2025-12-23T20:55:32Z",
      "url": "https://github.com/django/django/pull/20437#discussion_r2644244696"
    },
    {
      "repo": "django/django",
      "pr_number": 19793,
      "file_path": "django/db/models/sql/query.py",
      "line": 1442,
      "side": "RIGHT",
      "diff_hunk": "@@ -1430,11 +1431,15 @@ def build_lookup(self, lookups, lhs, rhs):\n                 return\n \n         lookup = lookup_class(lhs, rhs)\n-        # Interpret '__exact=None' as the sql 'is NULL'; otherwise, reject all\n-        # uses of None as a query value unless the lookup supports it.\n+        # Interpret '__exact=None' as the SQL 'IS NULL'. For '__iexact=None' on\n+        # KeyTransform, interpret it as '__exact=None' instead of 'IS NULL'\n+        # (#36508). For all other cases, reject the use of None as a query\n+        # value unless the lookup explicitly supports it.\n         if lookup.rhs is None and not lookup.can_use_none_as_rhs:\n             if lookup_name not in (\"exact\", \"iexact\"):\n                 raise ValueError(\"Cannot use None as a query value\")\n+            if lookup_name == \"iexact\" and isinstance(lhs, KeyTransform):\n+                return lhs.get_lookup(\"exact\")(lhs, None)",
      "comment": "Having had another think, it would be preferable if we don't add json specific logic here\r\nCan we not update `django.db.models.fields.json.KeyTransformIExact` somehow?",
      "comment_id": 2379114138,
      "user": "sarahboyce",
      "created_at": "2025-09-25T13:32:35Z",
      "url": "https://github.com/django/django/pull/19793#discussion_r2379114138"
    },
    {
      "repo": "django/django",
      "pr_number": 19793,
      "file_path": "django/db/models/sql/query.py",
      "line": 1442,
      "side": "RIGHT",
      "diff_hunk": "@@ -1430,11 +1431,15 @@ def build_lookup(self, lookups, lhs, rhs):\n                 return\n \n         lookup = lookup_class(lhs, rhs)\n-        # Interpret '__exact=None' as the sql 'is NULL'; otherwise, reject all\n-        # uses of None as a query value unless the lookup supports it.\n+        # Interpret '__exact=None' as the SQL 'IS NULL'. For '__iexact=None' on\n+        # KeyTransform, interpret it as '__exact=None' instead of 'IS NULL'\n+        # (#36508). For all other cases, reject the use of None as a query\n+        # value unless the lookup explicitly supports it.\n         if lookup.rhs is None and not lookup.can_use_none_as_rhs:\n             if lookup_name not in (\"exact\", \"iexact\"):\n                 raise ValueError(\"Cannot use None as a query value\")\n+            if lookup_name == \"iexact\" and isinstance(lhs, KeyTransform):\n+                return lhs.get_lookup(\"exact\")(lhs, None)",
      "comment": "I share your sentiment @sarahboyce, this should be implemented at the `KeyTransformIExact` level and not hardcoded here. The reason why `exact` and `iexact` are here today is due to how they must span `OUTER JOIN` when targeting a reverse relationship, I hope to remove it entirely with #16817.",
      "comment_id": 2379578552,
      "user": "charettes",
      "created_at": "2025-09-25T15:37:16Z",
      "url": "https://github.com/django/django/pull/19793#discussion_r2379578552"
    },
    {
      "repo": "django/django",
      "pr_number": 19793,
      "file_path": "django/db/models/fields/json.py",
      "line": 668,
      "side": "RIGHT",
      "diff_hunk": "@@ -665,7 +665,19 @@ def as_oracle(self, compiler, connection):\n class KeyTransformIExact(\n     CaseInsensitiveMixin, KeyTransformTextLookupMixin, lookups.IExact\n ):\n-    pass\n+    can_use_none_as_rhs = True",
      "comment": "I'm interested if you can tell me a little bit about this part. I see `KeyTransformExact` doesn't have it (at least not until Django 7.)\n\nI also see this logic that doesn't treat `exact` and `iexact` equivalently:\n\nhttps://github.com/django/django/blob/e49e14fd9032feb7a8cf254658ac4e74a4ffb712/django/db/models/sql/query.py#L1442-L1451",
      "comment_id": 2636417393,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-19T22:00:12Z",
      "url": "https://github.com/django/django/pull/19793#discussion_r2636417393"
    },
    {
      "repo": "django/django",
      "pr_number": 19793,
      "file_path": "django/db/models/fields/json.py",
      "line": 668,
      "side": "RIGHT",
      "diff_hunk": "@@ -665,7 +665,19 @@ def as_oracle(self, compiler, connection):\n class KeyTransformIExact(\n     CaseInsensitiveMixin, KeyTransformTextLookupMixin, lookups.IExact\n ):\n-    pass\n+    can_use_none_as_rhs = True",
      "comment": "> I see KeyTransformExact doesn't have it (at least not until Django 7.)\r\n\r\n`KeyTransformExact` does have the flag set to `True`; it is defined in `JSONExact` which `KeyTransformExact` subclasses. The change in Django 7 will remove the flag from `JSONExact` but keep it in `KeyTransformExact`, i.e. move it from the superclass to the subclass.\r\n\r\n> I also see this logic that doesn't treat `exact` and `iexact` equivalently:\r\n\r\nGood catch, seems like a bug? Test failure for iexact:\r\n```diff\r\ndiff --git a/tests/queries/tests.py b/tests/queries/tests.py\r\nindex 51d1915c97..24e9c51593 100644\r\n--- a/tests/queries/tests.py\r\n+++ b/tests/queries/tests.py\r\n@@ -2280,6 +2280,17 @@ class ComparisonTests(TestCase):\r\n            [item_ab],\r\n        )\r\n\r\n+    @skipUnlessDBFeature(\"interprets_empty_strings_as_nulls\")\r\n+    def test_empty_string_is_null(self):\r\n+        obj = NullableName.objects.create(name=None)\r\n+        obj1 = NullableName.objects.create(name=\"\")\r\n+        cases = [{\"name__exact\": \"\"}, {\"name__iexact\": \"\"}]\r\n+        for lookup in cases:\r\n+            with self.subTest(lookup):\r\n+                self.assertSequenceEqual(\r\n+                    NullableName.objects.filter(**lookup), [obj, obj1]\r\n+                )\r\n+\r\n\r\nclass ExistsSql(TestCase):\r\n    def test_exists(self):\r\n```\r\nThe queries are:\r\n```sql\r\nSELECT \"queries_nullablename\".\"id\",\r\n       \"queries_nullablename\".\"name\"\r\nFROM   \"queries_nullablename\"\r\nWHERE  \"queries_nullablename\".\"name\" IS NULL\r\nORDER  BY \"queries_nullablename\".\"id\" ASC;\r\n\r\nSELECT \"queries_nullablename\".\"id\",\r\n       \"queries_nullablename\".\"name\"\r\nFROM   \"queries_nullablename\"\r\nWHERE  Upper(\"queries_nullablename\".\"name\") = Upper()\r\nORDER  BY \"queries_nullablename\".\"id\" ASC;  \r\n```",
      "comment_id": 2637064874,
      "user": "cliffordgama",
      "created_at": "2025-12-20T11:42:39Z",
      "url": "https://github.com/django/django/pull/19793#discussion_r2637064874"
    },
    {
      "repo": "django/django",
      "pr_number": 19793,
      "file_path": "django/db/models/fields/json.py",
      "line": 668,
      "side": "RIGHT",
      "diff_hunk": "@@ -665,7 +665,19 @@ def as_oracle(self, compiler, connection):\n class KeyTransformIExact(\n     CaseInsensitiveMixin, KeyTransformTextLookupMixin, lookups.IExact\n ):\n-    pass\n+    can_use_none_as_rhs = True",
      "comment": "Re: empty strings as null: https://code.djangoproject.com/ticket/29222#comment:7 seems relevant.",
      "comment_id": 2637685073,
      "user": "cliffordgama",
      "created_at": "2025-12-21T08:47:03Z",
      "url": "https://github.com/django/django/pull/19793#discussion_r2637685073"
    },
    {
      "repo": "django/django",
      "pr_number": 19793,
      "file_path": "django/db/models/fields/json.py",
      "line": 668,
      "side": "RIGHT",
      "diff_hunk": "@@ -665,7 +665,19 @@ def as_oracle(self, compiler, connection):\n class KeyTransformIExact(\n     CaseInsensitiveMixin, KeyTransformTextLookupMixin, lookups.IExact\n ):\n-    pass\n+    can_use_none_as_rhs = True",
      "comment": "Super helpful. Could I interest you in filing a Trac ticket with your findings?",
      "comment_id": 2640666900,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-22T17:34:19Z",
      "url": "https://github.com/django/django/pull/19793#discussion_r2640666900"
    },
    {
      "repo": "django/django",
      "pr_number": 20443,
      "file_path": "tests/backends/tests.py",
      "line": 1023,
      "side": "RIGHT",
      "diff_hunk": "@@ -1017,3 +1018,19 @@ def test_many_to_many(self):\n         intermediary_model.objects.create(from_object_id=obj.id, to_object_id=12345)\n         self.assertEqual(obj.related_objects.count(), 1)\n         self.assertEqual(intermediary_model.objects.count(), 2)\n+\n+\n+class SQLiteMaxQueryParamsTests(TestCase):",
      "comment": "Refer to 358fd21c47cdf7bda520ce73c5cfd82bba57827b and where the original test was added in `tests/backends/sqlite/test_features.py`.\r\n\r\nThe way this is currently defined the test is running on all backend when it should be SQLite specific.\r\n\r\nhttps://github.com/django/django/commit/358fd21c47cdf7bda520ce73c5cfd82bba57827b#diff-bcc4842c09fdfbe9666b5d295f56846226e72148b015e0c19d4fc4ebd570b202R14-R22",
      "comment_id": 2640428474,
      "user": "charettes",
      "created_at": "2025-12-22T16:16:00Z",
      "url": "https://github.com/django/django/pull/20443#discussion_r2640428474"
    },
    {
      "repo": "django/django",
      "pr_number": 20443,
      "file_path": "tests/backends/sqlite/test_features.py",
      "line": 42,
      "side": "RIGHT",
      "diff_hunk": "@@ -29,3 +30,13 @@ def test_max_query_params_respects_variable_limit(self):\n         finally:\n             connection.connection.setlimit(limit_name, current_limit)\n         self.assertEqual(connection.features.max_query_params, current_limit)\n+\n+    def test_max_query_params_without_established_connection(self):\n+        new_connection = connection.copy()\n+        new_connection.settings_dict = copy.deepcopy(connection.settings_dict)\n+        self.assertIsNone(new_connection.connection)\n+        try:\n+            result = new_connection.features.max_query_params\n+            self.assertIsInstance(result, int)\n+        finally:\n+            new_connection.close()",
      "comment": "I'm getting the following, when using an in-memory database:\n\n```py\nResourceWarning: unclosed database in <sqlite3.Connection object at 0x10c710f40>\n```\nIt goes away with:\n```suggestion\n            new_connection._close()\n```",
      "comment_id": 2641021306,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-22T19:36:54Z",
      "url": "https://github.com/django/django/pull/20443#discussion_r2641021306"
    },
    {
      "repo": "django/django",
      "pr_number": 20443,
      "file_path": "tests/backends/sqlite/test_features.py",
      "line": 42,
      "side": "RIGHT",
      "diff_hunk": "@@ -29,3 +30,13 @@ def test_max_query_params_respects_variable_limit(self):\n         finally:\n             connection.connection.setlimit(limit_name, current_limit)\n         self.assertEqual(connection.features.max_query_params, current_limit)\n+\n+    def test_max_query_params_without_established_connection(self):\n+        new_connection = connection.copy()\n+        new_connection.settings_dict = copy.deepcopy(connection.settings_dict)\n+        self.assertIsNone(new_connection.connection)\n+        try:\n+            result = new_connection.features.max_query_params\n+            self.assertIsInstance(result, int)\n+        finally:\n+            new_connection.close()",
      "comment": "Thanks for the suggestion. I've updated the code to use `_close()` to ensure the underlying connection is properly closed in all environments.",
      "comment_id": 2641632711,
      "user": "guro-Ishiguro",
      "created_at": "2025-12-23T01:03:10Z",
      "url": "https://github.com/django/django/pull/20443#discussion_r2641632711"
    },
    {
      "repo": "django/django",
      "pr_number": 20055,
      "file_path": "django/core/management/commands/inspectdb.py",
      "line": 370,
      "side": "RIGHT",
      "diff_hunk": "@@ -359,21 +359,11 @@ def get_field_type(self, connection, table_name, row):\n         if field_type in {\"CharField\", \"TextField\"} and row.collation:\n             field_params[\"db_collation\"] = row.collation\n \n-        if field_type == \"DecimalField\":\n-            if row.precision is None or row.scale is None:\n-                field_notes.append(\n-                    \"max_digits and decimal_places have been guessed, as this \"\n-                    \"database handles decimal fields as float\"\n-                )\n-                field_params[\"max_digits\"] = (\n-                    row.precision if row.precision is not None else 10\n-                )\n-                field_params[\"decimal_places\"] = (\n-                    row.scale if row.scale is not None else 5\n-                )\n-            else:\n-                field_params[\"max_digits\"] = row.precision\n-                field_params[\"decimal_places\"] = row.scale\n+        if field_type == \"DecimalField\" and (\n+            row.precision is not None or row.scale is not None\n+        ):\n+            field_params[\"max_digits\"] = row.precision\n+            field_params[\"decimal_places\"] = row.scale",
      "comment": "I see below that we have a check to ensure that we don't have one of `max_digits` and `decimal_places` set to `None` when the other is not `None`. I think it's worth at least a comment here that we're OK with generating such an invalid field, since there's nothing more correct we can do. Alternatively we could raise a warning or an error here, but maybe that's excessive for something that might never happen.",
      "comment_id": 2530196486,
      "user": "LilyFirefly",
      "created_at": "2025-11-15T21:48:14Z",
      "url": "https://github.com/django/django/pull/20055#discussion_r2530196486"
    },
    {
      "repo": "django/django",
      "pr_number": 20055,
      "file_path": "django/db/models/fields/__init__.py",
      "line": 1804,
      "side": "RIGHT",
      "diff_hunk": "@@ -1725,54 +1725,84 @@ def check(self, **kwargs):\n         return errors\n \n     def _check_decimal_places(self):\n-        try:\n-            decimal_places = int(self.decimal_places)\n-            if decimal_places < 0:\n-                raise ValueError()\n-        except TypeError:\n-            return [\n-                checks.Error(\n-                    \"DecimalFields must define a 'decimal_places' attribute.\",\n-                    obj=self,\n-                    id=\"fields.E130\",\n-                )\n-            ]\n-        except ValueError:\n-            return [\n-                checks.Error(\n-                    \"'decimal_places' must be a non-negative integer.\",\n-                    obj=self,\n-                    id=\"fields.E131\",\n-                )\n-            ]\n+        if self.decimal_places is None:\n+            if not (\n+                connection.features.supports_no_precision_decimalfield\n+                or \"supports_no_precision_decimalfield\"\n+                in self.model._meta.required_db_features\n+            ):\n+                return [\n+                    checks.Error(\n+                        \"DecimalFields must define a 'decimal_places' attribute.\",\n+                        obj=self,\n+                        id=\"fields.E130\",\n+                    )\n+                ]\n+            elif self.max_digits is not None:\n+                return [\n+                    checks.Error(\n+                        \"DecimalField\u2019s max_digits and decimal_places must both \"\n+                        \"be defined or both omitted.\",\n+                        obj=self,\n+                        id=\"fields.E135\",\n+                    ),\n+                ]\n         else:\n-            return []\n+            try:\n+                decimal_places = int(self.decimal_places)\n+                if decimal_places < 0:\n+                    raise ValueError()\n+            except ValueError:\n+                return [\n+                    checks.Error(\n+                        \"'decimal_places' must be a non-negative integer.\",\n+                        obj=self,\n+                        id=\"fields.E131\",\n+                    )\n+                ]\n+        return []\n \n     def _check_max_digits(self):\n-        try:\n-            max_digits = int(self.max_digits)\n-            if max_digits <= 0:\n-                raise ValueError()\n-        except TypeError:\n-            return [\n-                checks.Error(\n-                    \"DecimalFields must define a 'max_digits' attribute.\",\n-                    obj=self,\n-                    id=\"fields.E132\",\n-                )\n-            ]\n-        except ValueError:\n-            return [\n-                checks.Error(\n-                    \"'max_digits' must be a positive integer.\",\n-                    obj=self,\n-                    id=\"fields.E133\",\n-                )\n-            ]\n+        if self.max_digits is None:\n+            if not (\n+                connection.features.supports_no_precision_decimalfield\n+                or \"supports_no_precision_decimalfield\"\n+                in self.model._meta.required_db_features\n+            ):\n+                return [\n+                    checks.Error(\n+                        \"DecimalFields must define a 'max_digits' attribute.\",\n+                        obj=self,\n+                        id=\"fields.E132\",\n+                    )\n+                ]\n+            elif self.decimal_places is not None:\n+                return [\n+                    checks.Error(\n+                        \"DecimalField\u2019s max_digits and decimal_places must both \"\n+                        \"be defined or both omitted.\",\n+                        obj=self,\n+                        id=\"fields.E135\",\n+                    ),\n+                ]\n         else:\n-            return []\n+            try:\n+                max_digits = int(self.max_digits)\n+                if max_digits <= 0:\n+                    raise ValueError()\n+            except ValueError:\n+                return [\n+                    checks.Error(\n+                        \"'max_digits' must be a positive integer.\",\n+                        obj=self,\n+                        id=\"fields.E133\",\n+                    )\n+                ]\n+        return []\n \n     def _check_decimal_places_and_max_digits(self, **kwargs):\n+        if self.decimal_places is None and self.max_digits is None:",
      "comment": "If only one of these was `None`, this check would raise a `TypeError`. Maybe this code isn't reachable in that case, but this version is safe even if it becomes reachable:\r\n\r\n```suggestion\r\n        if self.decimal_places is None or self.max_digits is None:\r\n```",
      "comment_id": 2530212189,
      "user": "LilyFirefly",
      "created_at": "2025-11-15T21:53:13Z",
      "url": "https://github.com/django/django/pull/20055#discussion_r2530212189"
    },
    {
      "repo": "django/django",
      "pr_number": 20055,
      "file_path": "django/db/models/fields/__init__.py",
      "line": 1733,
      "side": "RIGHT",
      "diff_hunk": "@@ -1725,54 +1725,84 @@ def check(self, **kwargs):\n         return errors\n \n     def _check_decimal_places(self):\n-        try:\n-            decimal_places = int(self.decimal_places)\n-            if decimal_places < 0:\n-                raise ValueError()\n-        except TypeError:\n-            return [\n-                checks.Error(\n-                    \"DecimalFields must define a 'decimal_places' attribute.\",\n-                    obj=self,\n-                    id=\"fields.E130\",\n-                )\n-            ]\n-        except ValueError:\n-            return [\n-                checks.Error(\n-                    \"'decimal_places' must be a non-negative integer.\",\n-                    obj=self,\n-                    id=\"fields.E131\",\n-                )\n-            ]\n+        if self.decimal_places is None:\n+            if not (\n+                connection.features.supports_no_precision_decimalfield\n+                or \"supports_no_precision_decimalfield\"\n+                in self.model._meta.required_db_features\n+            ):",
      "comment": "I think it's a bit clearer to distribute the `not` here:\r\n\r\n```suggestion\r\n            if (\r\n                not connection.features.supports_no_precision_decimalfield\r\n                and \"supports_no_precision_decimalfield\"\r\n                not in self.model._meta.required_db_features\r\n            ):\r\n```",
      "comment_id": 2533683815,
      "user": "LilyFirefly",
      "created_at": "2025-11-17T11:19:50Z",
      "url": "https://github.com/django/django/pull/20055#discussion_r2533683815"
    },
    {
      "repo": "django/django",
      "pr_number": 20055,
      "file_path": "django/db/models/fields/__init__.py",
      "line": 1771,
      "side": "RIGHT",
      "diff_hunk": "@@ -1725,54 +1725,84 @@ def check(self, **kwargs):\n         return errors\n \n     def _check_decimal_places(self):\n-        try:\n-            decimal_places = int(self.decimal_places)\n-            if decimal_places < 0:\n-                raise ValueError()\n-        except TypeError:\n-            return [\n-                checks.Error(\n-                    \"DecimalFields must define a 'decimal_places' attribute.\",\n-                    obj=self,\n-                    id=\"fields.E130\",\n-                )\n-            ]\n-        except ValueError:\n-            return [\n-                checks.Error(\n-                    \"'decimal_places' must be a non-negative integer.\",\n-                    obj=self,\n-                    id=\"fields.E131\",\n-                )\n-            ]\n+        if self.decimal_places is None:\n+            if not (\n+                connection.features.supports_no_precision_decimalfield\n+                or \"supports_no_precision_decimalfield\"\n+                in self.model._meta.required_db_features\n+            ):\n+                return [\n+                    checks.Error(\n+                        \"DecimalFields must define a 'decimal_places' attribute.\",\n+                        obj=self,\n+                        id=\"fields.E130\",\n+                    )\n+                ]\n+            elif self.max_digits is not None:\n+                return [\n+                    checks.Error(\n+                        \"DecimalField\u2019s max_digits and decimal_places must both \"\n+                        \"be defined or both omitted.\",\n+                        obj=self,\n+                        id=\"fields.E135\",\n+                    ),\n+                ]\n         else:\n-            return []\n+            try:\n+                decimal_places = int(self.decimal_places)\n+                if decimal_places < 0:\n+                    raise ValueError()\n+            except ValueError:\n+                return [\n+                    checks.Error(\n+                        \"'decimal_places' must be a non-negative integer.\",\n+                        obj=self,\n+                        id=\"fields.E131\",\n+                    )\n+                ]\n+        return []\n \n     def _check_max_digits(self):\n-        try:\n-            max_digits = int(self.max_digits)\n-            if max_digits <= 0:\n-                raise ValueError()\n-        except TypeError:\n-            return [\n-                checks.Error(\n-                    \"DecimalFields must define a 'max_digits' attribute.\",\n-                    obj=self,\n-                    id=\"fields.E132\",\n-                )\n-            ]\n-        except ValueError:\n-            return [\n-                checks.Error(\n-                    \"'max_digits' must be a positive integer.\",\n-                    obj=self,\n-                    id=\"fields.E133\",\n-                )\n-            ]\n+        if self.max_digits is None:\n+            if not (\n+                connection.features.supports_no_precision_decimalfield\n+                or \"supports_no_precision_decimalfield\"\n+                in self.model._meta.required_db_features\n+            ):",
      "comment": "And here:\r\n\r\n```suggestion\r\n            if (\r\n                not connection.features.supports_no_precision_decimalfield\r\n                and \"supports_no_precision_decimalfield\"\r\n                not in self.model._meta.required_db_features\r\n            ):\r\n```",
      "comment_id": 2533688406,
      "user": "LilyFirefly",
      "created_at": "2025-11-17T11:21:23Z",
      "url": "https://github.com/django/django/pull/20055#discussion_r2533688406"
    },
    {
      "repo": "django/django",
      "pr_number": 20055,
      "file_path": "django/db/models/fields/__init__.py",
      "line": 1740,
      "side": "RIGHT",
      "diff_hunk": "@@ -1725,54 +1725,84 @@ def check(self, **kwargs):\n         return errors\n \n     def _check_decimal_places(self):\n-        try:\n-            decimal_places = int(self.decimal_places)\n-            if decimal_places < 0:\n-                raise ValueError()\n-        except TypeError:\n-            return [\n-                checks.Error(\n-                    \"DecimalFields must define a 'decimal_places' attribute.\",\n-                    obj=self,\n-                    id=\"fields.E130\",\n-                )\n-            ]\n-        except ValueError:\n-            return [\n-                checks.Error(\n-                    \"'decimal_places' must be a non-negative integer.\",\n-                    obj=self,\n-                    id=\"fields.E131\",\n-                )\n-            ]\n+        if self.decimal_places is None:\n+            if (\n+                not connection.features.supports_no_precision_decimalfield\n+                and \"supports_no_precision_decimalfield\"\n+                not in self.model._meta.required_db_features\n+            ):\n+                return [\n+                    checks.Error(\n+                        \"DecimalFields must define a 'decimal_places' attribute.\",\n+                        obj=self,\n+                        id=\"fields.E130\",\n+                    )\n+                ]",
      "comment": "I believe this should be a database dependent check similar to `_check_db_collation()`, not only examining the default database.",
      "comment_id": 2640150213,
      "user": "timgraham",
      "created_at": "2025-12-22T15:01:31Z",
      "url": "https://github.com/django/django/pull/20055#discussion_r2640150213"
    },
    {
      "repo": "django/django",
      "pr_number": 20055,
      "file_path": "django/db/models/fields/__init__.py",
      "line": 1740,
      "side": "RIGHT",
      "diff_hunk": "@@ -1725,54 +1725,84 @@ def check(self, **kwargs):\n         return errors\n \n     def _check_decimal_places(self):\n-        try:\n-            decimal_places = int(self.decimal_places)\n-            if decimal_places < 0:\n-                raise ValueError()\n-        except TypeError:\n-            return [\n-                checks.Error(\n-                    \"DecimalFields must define a 'decimal_places' attribute.\",\n-                    obj=self,\n-                    id=\"fields.E130\",\n-                )\n-            ]\n-        except ValueError:\n-            return [\n-                checks.Error(\n-                    \"'decimal_places' must be a non-negative integer.\",\n-                    obj=self,\n-                    id=\"fields.E131\",\n-                )\n-            ]\n+        if self.decimal_places is None:\n+            if (\n+                not connection.features.supports_no_precision_decimalfield\n+                and \"supports_no_precision_decimalfield\"\n+                not in self.model._meta.required_db_features\n+            ):\n+                return [\n+                    checks.Error(\n+                        \"DecimalFields must define a 'decimal_places' attribute.\",\n+                        obj=self,\n+                        id=\"fields.E130\",\n+                    )\n+                ]",
      "comment": "Agreed, checking `connection` directly prevents mixing different database backends.",
      "comment_id": 2640161406,
      "user": "charettes",
      "created_at": "2025-12-22T15:04:22Z",
      "url": "https://github.com/django/django/pull/20055#discussion_r2640161406"
    },
    {
      "repo": "django/django",
      "pr_number": 20420,
      "file_path": "django/core/serializers/xml_serializer.py",
      "line": 323,
      "side": "LEFT",
      "diff_hunk": "@@ -316,43 +330,41 @@ def _handle_fk_field_node(self, node, field):\n         \"\"\"\n         Handle a <field> node for a ForeignKey\n         \"\"\"\n+        model = field.remote_field.model\n+        if hasattr(model._default_manager, \"get_by_natural_key\"):\n+            keys = node.getElementsByTagName(\"natural\")\n+            if keys:\n+                # If there are 'natural' subelements, it must be a natural\n+                # key\n+                field_value = [\n+                    None if k.getElementsByTagName(\"None\") else getInnerText(k).strip()\n+                    for k in keys\n+                ]\n+                try:\n+                    obj = model._default_manager.db_manager(self.db).get_by_natural_key(\n+                        *field_value\n+                    )\n+                except ObjectDoesNotExist:\n+                    if self.handle_forward_references:\n+                        return base.DEFER_FIELD\n+                    else:\n+                        raise\n+                obj_pk = getattr(obj, field.remote_field.field_name)\n+                # If this is a natural foreign key to an object that\n+                # has a FK/O2O as the foreign key, use the FK value\n+                if field.remote_field.model._meta.pk.remote_field:\n+                    obj_pk = obj_pk.pk\n+                return obj_pk\n+\n         # Check if there is a child node named 'None', returning None if so.\n         if node.getElementsByTagName(\"None\"):\n             return None\n-        else:\n-            model = field.remote_field.model",
      "comment": "For the sake of having a much smaller diff, I would suggest leaving most of this where it was and just fix the condition. (e.g. lift up `keys = ...` and then check `... and not keys: return None`, perhaps improving the variable name `keys`).\n\nYour version has a terser if condition, but this code is touched so infrequently that I think we should prioritize a smaller diff and more understandable blames over more readable if conditions.",
      "comment_id": 2636251555,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-19T20:41:23Z",
      "url": "https://github.com/django/django/pull/20420#discussion_r2636251555"
    },
    {
      "repo": "django/django",
      "pr_number": 20420,
      "file_path": "tests/serializers/models/natural.py",
      "line": 103,
      "side": "RIGHT",
      "diff_hunk": "@@ -91,3 +91,28 @@ class PostToOptOutSubclassUser(models.Model):\n     subscribers = models.ManyToManyField(\n         SubclassNaturalKeyOptOutUser, related_name=\"subscribed_posts\", blank=True\n     )\n+\n+\n+class NaturalKeyWithNullableFieldManager(models.Manager):\n+    def get_by_natural_key(self, name, optional_id):\n+        return self.get(name=name, optional_id=optional_id)\n+\n+\n+class NaturalKeyWithNullableField(models.Model):\n+    name = models.CharField(max_length=100)\n+    optional_id = models.UUIDField(null=True, blank=True)",
      "comment": "So my example of `UUIDField` on the ticket was just what I happened to be working with (and show a really nice validation problem when attempting the roundtrip). However, it's not very realistic as a member of a natural key (and doesn't work well with the yaml serializer). Could I ask you to try another field type?",
      "comment_id": 2636265959,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-19T20:47:16Z",
      "url": "https://github.com/django/django/pull/20420#discussion_r2636265959"
    },
    {
      "repo": "django/django",
      "pr_number": 20420,
      "file_path": "tests/serializers/test_natural.py",
      "line": 324,
      "side": "RIGHT",
      "diff_hunk": "@@ -280,6 +282,48 @@ def natural_key_opt_out_test(self, format):\n     )\n \n \n+def nullable_natural_key_fk_test(self, format):\n+    # Create objects with None in natural key\n+    target_with_none = NaturalKeyWithNullableField.objects.create(\n+        name=\"test_none\",\n+        optional_id=None,\n+    )\n+    target_with_uuid = NaturalKeyWithNullableField.objects.create(\n+        name=\"test_uuid\",\n+        optional_id=\"12345678-1234-5678-1234-567812345678\",\n+    )\n+    fk_to_none = FKToNaturalKeyWithNullable.objects.create(\n+        ref=target_with_none,\n+        data=\"points_to_none\",\n+    )\n+    fk_to_uuid = FKToNaturalKeyWithNullable.objects.create(\n+        ref=target_with_uuid,\n+        data=\"points_to_uuid\",\n+    )\n+    # Serialize\n+    objects = [target_with_none, target_with_uuid, fk_to_none, fk_to_uuid]\n+    serialized = serializers.serialize(\n+        format,\n+        objects,\n+        use_natural_foreign_keys=True,\n+        use_natural_primary_keys=True,\n+    )\n+    # Delete and deserialize\n+    FKToNaturalKeyWithNullable.objects.all().delete()\n+    NaturalKeyWithNullableField.objects.all().delete()\n+    for obj in serializers.deserialize(format, serialized):\n+        obj.save()\n+    # Verify\n+    restored_fk_none = FKToNaturalKeyWithNullable.objects.get(data=\"points_to_none\")\n+    restored_fk_uuid = FKToNaturalKeyWithNullable.objects.get(data=\"points_to_uuid\")\n+    self.assertIsNone(restored_fk_none.ref.optional_id)\n+    self.assertEqual(restored_fk_none.ref.name, \"test_none\")\n+    self.assertEqual(\n+        str(restored_fk_uuid.ref.optional_id),\n+        \"12345678-1234-5678-1234-567812345678\",\n+    )",
      "comment": "Do we need the saving part, or can we just assert against the unsaved object and still capture the underlying issue?",
      "comment_id": 2636273458,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-19T20:50:17Z",
      "url": "https://github.com/django/django/pull/20420#discussion_r2636273458"
    },
    {
      "repo": "django/django",
      "pr_number": 20420,
      "file_path": "tests/serializers/test_natural.py",
      "line": 286,
      "side": "RIGHT",
      "diff_hunk": "@@ -280,6 +282,48 @@ def natural_key_opt_out_test(self, format):\n     )\n \n \n+def nullable_natural_key_fk_test(self, format):\n+    # Create objects with None in natural key",
      "comment": "You can remove these comments to blend in with the surrounding style.",
      "comment_id": 2636274402,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-19T20:50:42Z",
      "url": "https://github.com/django/django/pull/20420#discussion_r2636274402"
    },
    {
      "repo": "django/django",
      "pr_number": 20420,
      "file_path": "tests/serializers/models/natural.py",
      "line": 114,
      "side": "RIGHT",
      "diff_hunk": "@@ -91,3 +91,28 @@ class PostToOptOutSubclassUser(models.Model):\n     subscribers = models.ManyToManyField(\n         SubclassNaturalKeyOptOutUser, related_name=\"subscribed_posts\", blank=True\n     )\n+\n+\n+class NaturalKeyWithNullableFieldManager(models.Manager):\n+    def get_by_natural_key(self, name, optional_id):\n+        return self.get(name=name, optional_id=optional_id)\n+\n+\n+class NaturalKeyWithNullableField(models.Model):\n+    name = models.CharField(max_length=100)\n+    optional_id = models.UUIDField(null=True, blank=True)\n+\n+    objects = NaturalKeyWithNullableFieldManager()\n+\n+    class Meta:\n+        unique_together = [[\"name\", \"optional_id\"]]\n+\n+    def natural_key(self):\n+        return (self.name, self.optional_id)\n+\n+\n+class FKToNaturalKeyWithNullable(models.Model):",
      "comment": "Great models -- trying your tests against main shows all kinds of nasty failures \ud83d\udc4d ",
      "comment_id": 2636293382,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-19T20:59:48Z",
      "url": "https://github.com/django/django/pull/20420#discussion_r2636293382"
    },
    {
      "repo": "django/django",
      "pr_number": 20420,
      "file_path": "django/core/serializers/xml_serializer.py",
      "line": 323,
      "side": "LEFT",
      "diff_hunk": "@@ -316,43 +330,41 @@ def _handle_fk_field_node(self, node, field):\n         \"\"\"\n         Handle a <field> node for a ForeignKey\n         \"\"\"\n+        model = field.remote_field.model\n+        if hasattr(model._default_manager, \"get_by_natural_key\"):\n+            keys = node.getElementsByTagName(\"natural\")\n+            if keys:\n+                # If there are 'natural' subelements, it must be a natural\n+                # key\n+                field_value = [\n+                    None if k.getElementsByTagName(\"None\") else getInnerText(k).strip()\n+                    for k in keys\n+                ]\n+                try:\n+                    obj = model._default_manager.db_manager(self.db).get_by_natural_key(\n+                        *field_value\n+                    )\n+                except ObjectDoesNotExist:\n+                    if self.handle_forward_references:\n+                        return base.DEFER_FIELD\n+                    else:\n+                        raise\n+                obj_pk = getattr(obj, field.remote_field.field_name)\n+                # If this is a natural foreign key to an object that\n+                # has a FK/O2O as the foreign key, use the FK value\n+                if field.remote_field.model._meta.pk.remote_field:\n+                    obj_pk = obj_pk.pk\n+                return obj_pk\n+\n         # Check if there is a child node named 'None', returning None if so.\n         if node.getElementsByTagName(\"None\"):\n             return None\n-        else:\n-            model = field.remote_field.model",
      "comment": "I tried to simplify the logic, but ended up making a bigger diff than needed. Your suggestion is the better approach.\r\nI've updated it.",
      "comment_id": 2636857798,
      "user": "2ykwang",
      "created_at": "2025-12-20T06:31:34Z",
      "url": "https://github.com/django/django/pull/20420#discussion_r2636857798"
    },
    {
      "repo": "django/django",
      "pr_number": 20420,
      "file_path": "tests/serializers/models/natural.py",
      "line": 103,
      "side": "RIGHT",
      "diff_hunk": "@@ -91,3 +91,28 @@ class PostToOptOutSubclassUser(models.Model):\n     subscribers = models.ManyToManyField(\n         SubclassNaturalKeyOptOutUser, related_name=\"subscribed_posts\", blank=True\n     )\n+\n+\n+class NaturalKeyWithNullableFieldManager(models.Manager):\n+    def get_by_natural_key(self, name, optional_id):\n+        return self.get(name=name, optional_id=optional_id)\n+\n+\n+class NaturalKeyWithNullableField(models.Model):\n+    name = models.CharField(max_length=100)\n+    optional_id = models.UUIDField(null=True, blank=True)",
      "comment": "Agreed. Changed to `CharField` it's more generalizable across all serializers and aligns with other natural key fields in the test suite.\r\ne.g. (`NaturalKeyAnchor.data`, `NaturalKeyThing.key`).",
      "comment_id": 2636857867,
      "user": "2ykwang",
      "created_at": "2025-12-20T06:31:45Z",
      "url": "https://github.com/django/django/pull/20420#discussion_r2636857867"
    },
    {
      "repo": "django/django",
      "pr_number": 20420,
      "file_path": "tests/serializers/test_natural.py",
      "line": 324,
      "side": "RIGHT",
      "diff_hunk": "@@ -280,6 +282,48 @@ def natural_key_opt_out_test(self, format):\n     )\n \n \n+def nullable_natural_key_fk_test(self, format):\n+    # Create objects with None in natural key\n+    target_with_none = NaturalKeyWithNullableField.objects.create(\n+        name=\"test_none\",\n+        optional_id=None,\n+    )\n+    target_with_uuid = NaturalKeyWithNullableField.objects.create(\n+        name=\"test_uuid\",\n+        optional_id=\"12345678-1234-5678-1234-567812345678\",\n+    )\n+    fk_to_none = FKToNaturalKeyWithNullable.objects.create(\n+        ref=target_with_none,\n+        data=\"points_to_none\",\n+    )\n+    fk_to_uuid = FKToNaturalKeyWithNullable.objects.create(\n+        ref=target_with_uuid,\n+        data=\"points_to_uuid\",\n+    )\n+    # Serialize\n+    objects = [target_with_none, target_with_uuid, fk_to_none, fk_to_uuid]\n+    serialized = serializers.serialize(\n+        format,\n+        objects,\n+        use_natural_foreign_keys=True,\n+        use_natural_primary_keys=True,\n+    )\n+    # Delete and deserialize\n+    FKToNaturalKeyWithNullable.objects.all().delete()\n+    NaturalKeyWithNullableField.objects.all().delete()\n+    for obj in serializers.deserialize(format, serialized):\n+        obj.save()\n+    # Verify\n+    restored_fk_none = FKToNaturalKeyWithNullable.objects.get(data=\"points_to_none\")\n+    restored_fk_uuid = FKToNaturalKeyWithNullable.objects.get(data=\"points_to_uuid\")\n+    self.assertIsNone(restored_fk_none.ref.optional_id)\n+    self.assertEqual(restored_fk_none.ref.name, \"test_none\")\n+    self.assertEqual(\n+        str(restored_fk_uuid.ref.optional_id),\n+        \"12345678-1234-5678-1234-567812345678\",\n+    )",
      "comment": "From a serialization perspective, the responsibility is data transformation `save()` is a separate concern. I'll simplify the test. Thanks!",
      "comment_id": 2636857926,
      "user": "2ykwang",
      "created_at": "2025-12-20T06:31:51Z",
      "url": "https://github.com/django/django/pull/20420#discussion_r2636857926"
    },
    {
      "repo": "django/django",
      "pr_number": 20422,
      "file_path": "tests/user_commands/tests.py",
      "line": 475,
      "side": "RIGHT",
      "diff_hunk": "@@ -470,6 +470,38 @@ def test_suggest_on_error_explicit_false(self):\n         )\n         self.assertFalse(parser.suggest_on_error)\n \n+    @unittest.skipUnless(PY314, \"Only relevant for Python 3.14+\")\n+    def test_color_enabled_by_default(self):\n+        \"\"\"Argparse color should be enabled by default.\"\"\"",
      "comment": "When the docstrings communicate the same information as the method names we chop them.",
      "comment_id": 2636058702,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-19T19:25:31Z",
      "url": "https://github.com/django/django/pull/20422#discussion_r2636058702"
    },
    {
      "repo": "django/django",
      "pr_number": 20422,
      "file_path": "django/core/management/base.py",
      "line": 70,
      "side": "RIGHT",
      "diff_hunk": "@@ -58,8 +58,16 @@ def __init__(\n     ):\n         self.missing_args_message = missing_args_message\n         self.called_from_command_line = called_from_command_line\n-        if PY314 and not PY315:\n-            kwargs.setdefault(\"suggest_on_error\", True)\n+        if PY314:\n+            if not PY315:\n+                kwargs.setdefault(\"suggest_on_error\", True)\n+            if (\n+                os.environ.get(\"NO_COLOR\")\n+                or os.environ.get(\"DJANGO_COLORS\") == \"nocolor\"\n+            ):\n+                kwargs.setdefault(\"color\", False)\n+            else:\n+                kwargs.setdefault(\"color\", True)",
      "comment": "```suggestion\n```\nI don't think we need to bother; color is already True by default since https://github.com/python/cpython/pull/136809 (3.14)",
      "comment_id": 2636073803,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-19T19:31:45Z",
      "url": "https://github.com/django/django/pull/20422#discussion_r2636073803"
    },
    {
      "repo": "django/django",
      "pr_number": 20422,
      "file_path": "django/core/management/base.py",
      "line": 65,
      "side": "RIGHT",
      "diff_hunk": "@@ -58,8 +58,16 @@ def __init__(\n     ):\n         self.missing_args_message = missing_args_message\n         self.called_from_command_line = called_from_command_line\n-        if PY314 and not PY315:\n-            kwargs.setdefault(\"suggest_on_error\", True)\n+        if PY314:\n+            if not PY315:\n+                kwargs.setdefault(\"suggest_on_error\", True)\n+            if (\n+                os.environ.get(\"NO_COLOR\")",
      "comment": "This environment variable hasn't been set yet; we also need to check for an incoming ``--no-color`` argument instead. Please add a test.",
      "comment_id": 2636085507,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-19T19:35:45Z",
      "url": "https://github.com/django/django/pull/20422#discussion_r2636085507"
    },
    {
      "repo": "django/django",
      "pr_number": 20422,
      "file_path": "tests/user_commands/tests.py",
      "line": 498,
      "side": "RIGHT",
      "diff_hunk": "@@ -470,6 +470,34 @@ def test_suggest_on_error_explicit_false(self):\n         )\n         self.assertFalse(parser.suggest_on_error)\n \n+    @unittest.skipUnless(PY314, \"Only relevant for Python 3.14+\")\n+    def test_color_enabled_by_default(self):\n+        with mock.patch.dict(os.environ, {}, clear=True):\n+            command = BaseCommand()\n+            parser = command.create_parser(\"prog_name\", \"subcommand\")\n+            self.assertTrue(parser.color)\n+\n+    @unittest.skipUnless(PY314, \"Only relevant for Python 3.14+\")\n+    def test_color_disabled_with_django_colors_nocolor(self):\n+        with mock.patch.dict(os.environ, {\"DJANGO_COLORS\": \"nocolor\"}):\n+            command = BaseCommand()\n+            parser = command.create_parser(\"prog_name\", \"subcommand\")\n+            self.assertFalse(parser.color)\n+\n+    @unittest.skipUnless(PY314, \"Only relevant for Python 3.14+\")\n+    def test_force_color_does_not_affect_argparse_color(self):\n+        with mock.patch.dict(os.environ, {}, clear=True):\n+            command = BaseCommand(force_color=True)\n+            parser = command.create_parser(\"prog_name\", \"subcommand\")\n+            self.assertTrue(parser.color)\n+\n+    @unittest.skipUnless(PY314, \"Only relevant for Python 3.14+\")\n+    def test_no_color_flag_disables_color(self):\n+        with mock.patch.object(sys, \"argv\", [\"manage.py\", \"mycommand\", \"--no-color\"]):\n+            command = BaseCommand()\n+            parser = command.create_parser(\"prog\", \"sub\")",
      "comment": "Very minor: I'd use \"manage.py\" and \"mycommand\" to match the mock.",
      "comment_id": 2640394776,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-22T16:02:55Z",
      "url": "https://github.com/django/django/pull/20422#discussion_r2640394776"
    },
    {
      "repo": "django/django",
      "pr_number": 20422,
      "file_path": "tests/user_commands/tests.py",
      "line": 499,
      "side": "RIGHT",
      "diff_hunk": "@@ -470,6 +470,34 @@ def test_suggest_on_error_explicit_false(self):\n         )\n         self.assertFalse(parser.suggest_on_error)\n \n+    @unittest.skipUnless(PY314, \"Only relevant for Python 3.14+\")\n+    def test_color_enabled_by_default(self):\n+        with mock.patch.dict(os.environ, {}, clear=True):\n+            command = BaseCommand()\n+            parser = command.create_parser(\"prog_name\", \"subcommand\")\n+            self.assertTrue(parser.color)\n+\n+    @unittest.skipUnless(PY314, \"Only relevant for Python 3.14+\")\n+    def test_color_disabled_with_django_colors_nocolor(self):\n+        with mock.patch.dict(os.environ, {\"DJANGO_COLORS\": \"nocolor\"}):\n+            command = BaseCommand()\n+            parser = command.create_parser(\"prog_name\", \"subcommand\")\n+            self.assertFalse(parser.color)\n+\n+    @unittest.skipUnless(PY314, \"Only relevant for Python 3.14+\")\n+    def test_force_color_does_not_affect_argparse_color(self):\n+        with mock.patch.dict(os.environ, {}, clear=True):\n+            command = BaseCommand(force_color=True)\n+            parser = command.create_parser(\"prog_name\", \"subcommand\")\n+            self.assertTrue(parser.color)\n+\n+    @unittest.skipUnless(PY314, \"Only relevant for Python 3.14+\")\n+    def test_no_color_flag_disables_color(self):\n+        with mock.patch.object(sys, \"argv\", [\"manage.py\", \"mycommand\", \"--no-color\"]):\n+            command = BaseCommand()\n+            parser = command.create_parser(\"prog\", \"sub\")\n+            self.assertIs(parser.color, False)",
      "comment": "I'd make all 4 assertions use either assertIs or not.",
      "comment_id": 2640395530,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-22T16:03:12Z",
      "url": "https://github.com/django/django/pull/20422#discussion_r2640395530"
    },
    {
      "repo": "django/django",
      "pr_number": 19938,
      "file_path": "tests/file_storage/models.py",
      "line": 46,
      "side": "RIGHT",
      "diff_hunk": "@@ -37,6 +38,14 @@ def __call__(self):\n         return self\n \n \n+class LazyTempStorage(LazyObject):\n+    def _setup(self):\n+        self._wrapped = temp_storage\n+\n+\n+lazy_temp_storage = LazyTempStorage()",
      "comment": "Do we need this module level definition? Could we instead just instantiate in the usage below?\n```python\nlazy_storage = models.FileField(storage=LazyTempStorage(), upload_to=\"tests\")\n```",
      "comment_id": 2432832589,
      "user": "nessita",
      "created_at": "2025-10-15T14:38:27Z",
      "url": "https://github.com/django/django/pull/19938#discussion_r2432832589"
    },
    {
      "repo": "django/django",
      "pr_number": 19938,
      "file_path": "tests/file_storage/tests.py",
      "line": 1273,
      "side": "RIGHT",
      "diff_hunk": "@@ -1267,3 +1268,11 @@ def test_nonexistent_backend(self):\n         )\n         with self.assertRaisesMessage(InvalidStorageError, msg):\n             test_storages[\"invalid_backend\"]\n+\n+\n+class LazyObjectHandlingTests(SimpleTestCase):",
      "comment": "Small nitpick:\n```suggestion\nclass StorageLazyObjectTests(SimpleTestCase):\n```",
      "comment_id": 2432842254,
      "user": "nessita",
      "created_at": "2025-10-15T14:41:04Z",
      "url": "https://github.com/django/django/pull/19938#discussion_r2432842254"
    },
    {
      "repo": "django/django",
      "pr_number": 19480,
      "file_path": "tests/postgres_tests/test_dependency_checks.py",
      "line": 1,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,177 @@\n+from django.core import checks",
      "comment": "Not sure `test_dependency_check` is the most adequate name here. Maybe `test_installed_app_checks`?",
      "comment_id": 2094674654,
      "user": "charettes",
      "created_at": "2025-05-19T00:32:42Z",
      "url": "https://github.com/django/django/pull/19480#discussion_r2094674654"
    },
    {
      "repo": "django/django",
      "pr_number": 19480,
      "file_path": "django/contrib/postgres/search.py",
      "line": 34,
      "side": "RIGHT",
      "diff_hunk": "@@ -29,7 +31,7 @@ def as_sql(self, qn, connection):\n         return \"%s @@ %s\" % (lhs, rhs), params\n \n \n-class SearchVectorField(Field):\n+class SearchVectorField(CheckPostgresInstalledMixin, Field):",
      "comment": "Was wondering if we should add to the check `SearchQueryField`. I know it's not documented but is used within `SearchQuery` which is documented?",
      "comment_id": 2140419548,
      "user": "sarahboyce",
      "created_at": "2025-06-11T14:55:43Z",
      "url": "https://github.com/django/django/pull/19480#discussion_r2140419548"
    },
    {
      "repo": "django/django",
      "pr_number": 19480,
      "file_path": "django/contrib/postgres/search.py",
      "line": 34,
      "side": "RIGHT",
      "diff_hunk": "@@ -29,7 +31,7 @@ def as_sql(self, qn, connection):\n         return \"%s @@ %s\" % (lhs, rhs), params\n \n \n-class SearchVectorField(Field):\n+class SearchVectorField(CheckPostgresInstalledMixin, Field):",
      "comment": "Thanks for the review! Makes sense \ud83d\udc4d\ud83c\udffe. I'll add the check accordingly",
      "comment_id": 2142572357,
      "user": "cliffordgama",
      "created_at": "2025-06-12T12:10:24Z",
      "url": "https://github.com/django/django/pull/19480#discussion_r2142572357"
    },
    {
      "repo": "django/django",
      "pr_number": 20426,
      "file_path": "tests/forms_tests/tests/test_forms.py",
      "line": 5353,
      "side": "RIGHT",
      "diff_hunk": "@@ -5343,10 +5350,7 @@ def my_function(method, post_data):\n                 return \"VALID: %r\" % sorted(form.cleaned_data.items())\n \n             t = Template(\n-                '<form method=\"post\">'\n-                \"{{ form }}\"\n-                '<input type=\"submit\" required>'\n-                \"</form>\"\n+                '<form method=\"post\">{{ form }}<input type=\"submit\" required></form>'",
      "comment": "Please revert -- we don't mix in unrelated refactors.",
      "comment_id": 2636040965,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-19T19:18:21Z",
      "url": "https://github.com/django/django/pull/20426#discussion_r2636040965"
    },
    {
      "repo": "django/django",
      "pr_number": 20395,
      "file_path": "tests/migrations/test_autodetector.py",
      "line": 5595,
      "side": "RIGHT",
      "diff_hunk": "@@ -5543,6 +5543,78 @@ def test_remove_composite_pk(self):\n             preserve_default=True,\n         )\n \n+    def test_m2m_target_change_generates_remove_and_add(self):\n+        # Initial state: Author has M2M to Book\n+        before = [\n+            ModelState(\n+                \"testapp\",\n+                \"Book\",\n+                [(\"id\", models.AutoField(primary_key=True))],\n+            ),\n+            ModelState(\n+                \"testapp\",\n+                \"Magazine\",\n+                [(\"id\", models.AutoField(primary_key=True))],\n+            ),\n+            ModelState(\n+                \"testapp\",\n+                \"Author\",\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                    (\"readings\", models.ManyToManyField(\"testapp.Book\")),\n+                ],\n+            ),\n+        ]\n+\n+        # After state: Author has M2M to Magazine (same field name 'readings')\n+        after = [\n+            ModelState(\n+                \"testapp\",\n+                \"Book\",\n+                [(\"id\", models.AutoField(primary_key=True))],\n+            ),\n+            ModelState(\n+                \"testapp\",\n+                \"Magazine\",\n+                [(\"id\", models.AutoField(primary_key=True))],\n+            ),\n+            ModelState(\n+                \"testapp\",\n+                \"Author\",\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                    (\"readings\", models.ManyToManyField(\"testapp.Magazine\")),\n+                ],\n+            ),\n+        ]\n+\n+        changes = self.get_changes(before, after)\n+\n+        # We expect 1 migration for testapp\n+        self.assertIn(\"testapp\", changes)\n+        self.assertEqual(len(changes[\"testapp\"]), 1)",
      "comment": "use `assertNumberMigrations()` (see surrounding tests)",
      "comment_id": 2616891657,
      "user": "cliffordgama",
      "created_at": "2025-12-14T10:05:11Z",
      "url": "https://github.com/django/django/pull/20395#discussion_r2616891657"
    },
    {
      "repo": "django/django",
      "pr_number": 20395,
      "file_path": "tests/migrations/test_autodetector.py",
      "line": 5581,
      "side": "RIGHT",
      "diff_hunk": "@@ -5543,6 +5543,75 @@ def test_remove_composite_pk(self):\n             preserve_default=True,\n         )\n \n+    def test_m2m_target_change_generates_remove_and_add(self):\n+        # Initial state: Author has M2M to Book\n+        before = [\n+            ModelState(\n+                \"testapp\",\n+                \"Book\",\n+                [(\"id\", models.AutoField(primary_key=True))],\n+            ),\n+            ModelState(\n+                \"testapp\",\n+                \"Magazine\",\n+                [(\"id\", models.AutoField(primary_key=True))],\n+            ),\n+            ModelState(\n+                \"testapp\",\n+                \"Author\",\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                    (\"readings\", models.ManyToManyField(\"testapp.Book\")),\n+                ],\n+            ),\n+        ]\n+\n+        # After state: Author has M2M to Magazine (same field name 'readings')\n+        after = [\n+            ModelState(\n+                \"testapp\",\n+                \"Book\",\n+                [(\"id\", models.AutoField(primary_key=True))],\n+            ),\n+            ModelState(\n+                \"testapp\",\n+                \"Magazine\",\n+                [(\"id\", models.AutoField(primary_key=True))],\n+            ),\n+            ModelState(\n+                \"testapp\",\n+                \"Author\",\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                    (\"readings\", models.ManyToManyField(\"testapp.Magazine\")),\n+                ],\n+            ),\n+        ]\n+\n+        changes = self.get_changes(before, after)\n+\n+        self.assertNumberMigrations(changes, \"testapp\", 1)\n+\n+        self.assertOperationTypes(changes, \"testapp\", 0, [\"RemoveField\", \"AddField\"])\n+\n+        self.assertOperationAttributes(\n+            changes,\n+            \"testapp\",\n+            0,\n+            0,\n+            name=\"readings\",\n+            model_name=\"author\",\n+        )\n+\n+        self.assertOperationAttributes(",
      "comment": "Nit: I'd remove the blank lines; the file is already too long as it is \n```suggestion\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RemoveField\", \"AddField\"])\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            0,\n            name=\"readings\",\n            model_name=\"author\",\n        )\n        self.assertOperationAttributes(\n```",
      "comment_id": 2617252239,
      "user": "cliffordgama",
      "created_at": "2025-12-14T15:31:41Z",
      "url": "https://github.com/django/django/pull/20395#discussion_r2617252239"
    },
    {
      "repo": "django/django",
      "pr_number": 20395,
      "file_path": "tests/migrations/test_autodetector.py",
      "line": 5567,
      "side": "RIGHT",
      "diff_hunk": "@@ -5543,6 +5543,69 @@ def test_remove_composite_pk(self):\n             preserve_default=True,\n         )\n \n+    def test_m2m_target_change_generates_remove_and_add(self):\n+        # Initial state: Author has M2M to Book\n+        before = [\n+            ModelState(\n+                \"testapp\",\n+                \"Book\",\n+                [(\"id\", models.AutoField(primary_key=True))],\n+            ),\n+            ModelState(\n+                \"testapp\",\n+                \"Magazine\",\n+                [(\"id\", models.AutoField(primary_key=True))],\n+            ),\n+            ModelState(\n+                \"testapp\",\n+                \"Author\",\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                    (\"readings\", models.ManyToManyField(\"testapp.Book\")),\n+                ],\n+            ),\n+        ]",
      "comment": "I've just discovered these model states or similar are predefined, e.g. `self.author_with_m2m`. Can you switch to those here and, where appropriate, in `after` as well.",
      "comment_id": 2619428551,
      "user": "cliffordgama",
      "created_at": "2025-12-15T13:24:47Z",
      "url": "https://github.com/django/django/pull/20395#discussion_r2619428551"
    },
    {
      "repo": "django/django",
      "pr_number": 20395,
      "file_path": "tests/migrations/test_autodetector.py",
      "line": 5562,
      "side": "RIGHT",
      "diff_hunk": "@@ -5543,6 +5554,39 @@ def test_remove_composite_pk(self):\n             preserve_default=True,\n         )\n \n+    def test_m2m_target_change_generates_remove_and_add(self):\n+        # Initial state: Author has M2M to Publisher\n+        before = [\n+            self.publisher,\n+            self.magazine,\n+            self.author_with_m2m,",
      "comment": "```suggestion\n        before = [\n            self.publisher,\n            self.other_publisher,\n            self.author_with_m2m,  # m2m to self.publisher.\n```",
      "comment_id": 2630938677,
      "user": "cliffordgama",
      "created_at": "2025-12-18T12:49:00Z",
      "url": "https://github.com/django/django/pull/20395#discussion_r2630938677"
    },
    {
      "repo": "django/django",
      "pr_number": 20395,
      "file_path": "tests/migrations/test_autodetector.py",
      "line": 754,
      "side": "RIGHT",
      "diff_hunk": "@@ -741,6 +741,17 @@ class AutodetectorTests(BaseAutodetectorTests):\n             (\"publishers\", models.ManyToManyField(\"testapp.Publisher\", blank=True)),\n         ],\n     )\n+    magazine = ModelState(\n+        \"testapp\", \"Magazine\", [(\"id\", models.AutoField(primary_key=True))]\n+    )\n+    author_with_m2m_magazine = ModelState(\n+        \"testapp\",\n+        \"Author\",\n+        [\n+            (\"id\", models.AutoField(primary_key=True)),\n+            (\"publishers\", models.ManyToManyField(\"testapp.Magazine\")),\n+        ],\n+    )",
      "comment": "\nI think we only need a variable for something we need to use more than once.\n```suggestion\n    other_publisher = ModelState(\n        \"testapp\",\n        \"OtherPublisher\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n        ],\n    )\n```",
      "comment_id": 2630966200,
      "user": "cliffordgama",
      "created_at": "2025-12-18T12:55:15Z",
      "url": "https://github.com/django/django/pull/20395#discussion_r2630966200"
    },
    {
      "repo": "django/django",
      "pr_number": 20395,
      "file_path": "tests/migrations/test_autodetector.py",
      "line": 5569,
      "side": "RIGHT",
      "diff_hunk": "@@ -5543,6 +5554,39 @@ def test_remove_composite_pk(self):\n             preserve_default=True,\n         )\n \n+    def test_m2m_target_change_generates_remove_and_add(self):\n+        # Initial state: Author has M2M to Publisher\n+        before = [\n+            self.publisher,\n+            self.magazine,\n+            self.author_with_m2m,\n+        ]\n+        # After state: Author has M2M to Magazine\n+        after = [\n+            self.publisher,\n+            self.magazine,\n+            self.author_with_m2m_magazine,\n+        ]",
      "comment": "```suggestion\n        after = [\n            self.publisher,\n            self.other_publisher,\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    # Repoint m2m to self.other_publisher.\n                    (\"publishers\", models.ManyToManyField(\"testapp.OtherPublisher\")),\n                ],\n            ),\n        ]\n```",
      "comment_id": 2630975903,
      "user": "cliffordgama",
      "created_at": "2025-12-18T12:57:20Z",
      "url": "https://github.com/django/django/pull/20395#discussion_r2630975903"
    },
    {
      "repo": "django/django",
      "pr_number": 20395,
      "file_path": "tests/migrations/test_autodetector.py",
      "line": 5559,
      "side": "RIGHT",
      "diff_hunk": "@@ -5543,6 +5550,46 @@ def test_remove_composite_pk(self):\n             preserve_default=True,\n         )\n \n+    def test_m2m_target_change_generates_remove_and_add(self):\n+        before = [\n+            self.publisher,\n+            self.other_publisher,\n+            self.author_with_m2m,  # m2m to self.publisher.\n+        ]\n+        # After state: Author has M2M to Magazine",
      "comment": "And also if I could bother you to move the test, as suggested [here](https://github.com/django/django/pull/20395#pullrequestreview-3592776256).\r\n\r\nOtherwise this should be good to go.",
      "comment_id": 2632590554,
      "user": "cliffordgama",
      "created_at": "2025-12-18T20:52:41Z",
      "url": "https://github.com/django/django/pull/20395#discussion_r2632590554"
    },
    {
      "repo": "django/django",
      "pr_number": 20423,
      "file_path": "django/db/models/constraints.py",
      "line": 278,
      "side": "LEFT",
      "diff_hunk": "@@ -274,8 +274,6 @@ def __init__(\n         violation_error_code=None,\n         violation_error_message=None,\n     ):\n-        if not name:\n-            raise ValueError(\"A unique constraint must be named.\")",
      "comment": "This change also means that passing any falsey values to `name` may not raise an error. Notably, it now accepts a blank string (`\"\"`).",
      "comment_id": 2631199687,
      "user": "jonbiemond",
      "created_at": "2025-12-18T13:55:47Z",
      "url": "https://github.com/django/django/pull/20423#discussion_r2631199687"
    },
    {
      "repo": "django/django",
      "pr_number": 20423,
      "file_path": "django/db/models/constraints.py",
      "line": 278,
      "side": "LEFT",
      "diff_hunk": "@@ -274,8 +274,6 @@ def __init__(\n         violation_error_code=None,\n         violation_error_message=None,\n     ):\n-        if not name:\n-            raise ValueError(\"A unique constraint must be named.\")",
      "comment": "SQLite doesn't mind an empty string, but PostgreSQL does.",
      "comment_id": 2631364049,
      "user": "jonbiemond",
      "created_at": "2025-12-18T14:41:35Z",
      "url": "https://github.com/django/django/pull/20423#discussion_r2631364049"
    },
    {
      "repo": "django/django",
      "pr_number": 20423,
      "file_path": "django/db/models/constraints.py",
      "line": 278,
      "side": "LEFT",
      "diff_hunk": "@@ -274,8 +274,6 @@ def __init__(\n         violation_error_code=None,\n         violation_error_message=None,\n     ):\n-        if not name:\n-            raise ValueError(\"A unique constraint must be named.\")",
      "comment": "Why not leave this as `if not name` to continue raising helpful errors if `None` is explicitly passed?",
      "comment_id": 2631597896,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-18T15:42:19Z",
      "url": "https://github.com/django/django/pull/20423#discussion_r2631597896"
    },
    {
      "repo": "django/django",
      "pr_number": 20423,
      "file_path": "django/db/models/constraints.py",
      "line": 278,
      "side": "LEFT",
      "diff_hunk": "@@ -274,8 +274,6 @@ def __init__(\n         violation_error_code=None,\n         violation_error_message=None,\n     ):\n-        if not name:\n-            raise ValueError(\"A unique constraint must be named.\")",
      "comment": "Thanks for the speedy review! I think that's a good idea! (I just checked what we were doing in other classes.) I'll change it back. How do you feel about the new error message?\r\n> A unique constraint name must not be blank.",
      "comment_id": 2631888378,
      "user": "jonbiemond",
      "created_at": "2025-12-18T17:10:01Z",
      "url": "https://github.com/django/django/pull/20423#discussion_r2631888378"
    },
    {
      "repo": "django/django",
      "pr_number": 20423,
      "file_path": "django/db/models/constraints.py",
      "line": 278,
      "side": "LEFT",
      "diff_hunk": "@@ -274,8 +274,6 @@ def __init__(\n         violation_error_code=None,\n         violation_error_message=None,\n     ):\n-        if not name:\n-            raise ValueError(\"A unique constraint must be named.\")",
      "comment": "That said, we do tend to avoid changes unless it's clearly better. This one's borderline for me, since blank is still \"not named\" \ud83e\udd14 ",
      "comment_id": 2631925837,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-18T17:21:49Z",
      "url": "https://github.com/django/django/pull/20423#discussion_r2631925837"
    },
    {
      "repo": "django/django",
      "pr_number": 19609,
      "file_path": "tests/test_utils/tests.py",
      "line": 2093,
      "side": "RIGHT",
      "diff_hunk": "@@ -2145,6 +2090,35 @@ def hook():\n         self.assertIsInstance(raised_exception, MyException)\n         self.assertEqual(str(raised_exception), \"robust callback\")\n \n+    def test_execute_robust_with_callback_as_partial(self):",
      "comment": "Thanks. Just leaving a note that I had a small look at reducing duplication with the above test via `subTest`, and it wasn't a great fit given the need to pop from `run_on_commit` managed via the `captureOnCommitCallbacks` context manager. So nothing to change as far as I'm concerned.",
      "comment_id": 2624847233,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-16T21:44:55Z",
      "url": "https://github.com/django/django/pull/19609#discussion_r2624847233"
    },
    {
      "repo": "django/django",
      "pr_number": 19609,
      "file_path": "tests/test_utils/tests.py",
      "line": 2116,
      "side": "RIGHT",
      "diff_hunk": "@@ -2145,6 +2090,35 @@ def hook():\n         self.assertIsInstance(raised_exception, MyException)\n         self.assertEqual(str(raised_exception), \"robust callback\")\n \n+    def test_execute_robust_with_callback_as_partial(self):\n+        class MyException(Exception):\n+            pass\n+\n+        def hook():\n+            self.callback_called = True\n+            raise MyException(\"robust callback\")\n+\n+        hook_partial = partial(hook)\n+\n+        with self.assertLogs(\"django.test\", \"ERROR\") as cm:\n+            with self.captureOnCommitCallbacks(execute=True) as callbacks:\n+                transaction.on_commit(hook_partial, robust=True)\n+\n+        self.assertEqual(len(callbacks), 1)\n+        self.assertIs(self.callback_called, True)\n+\n+        log_record = cm.records[0]\n+        self.assertRegex(\n+            log_record.getMessage(),\n+            r\"Error calling functools\\.partial\\(<function CaptureOnCommitCallbacksTests\"\n+            r\"\\.test_execute_robust_with_callback_as_partial\\.<locals>\\.hook\"\n+            r\" at .+>\\) in on_commit\\(\\) \\(robust callback\\)\\.\",\n+        )",
      "comment": "Django tends to assert very precisely on exception messages (for [another view](https://github.com/python/cpython/pull/98100#discussion_r1759570358), see Brett Cannon) -- but here I  think with so much escaping readability suffers. I would suggest:\n\n```suggestion\n        self.assertEqual(\n            log_record.getMessage(),\n            f\"Error calling {hook_partial} in on_commit() (robust callback).\",\n        )\n```\nand similar for the other test.",
      "comment_id": 2624857552,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-16T21:48:16Z",
      "url": "https://github.com/django/django/pull/19609#discussion_r2624857552"
    },
    {
      "repo": "django/django",
      "pr_number": 19609,
      "file_path": "django/db/backends/base/base.py",
      "line": 745,
      "side": "RIGHT",
      "diff_hunk": "@@ -741,11 +741,8 @@ def on_commit(self, func, robust=False):\n                 try:\n                     func()\n                 except Exception as e:\n-                    logger.error(\n-                        f\"Error calling {func.__qualname__} in on_commit() (%s).\",\n-                        e,\n-                        exc_info=True,\n-                    )\n+                    name = getattr(func, \"__qualname__\", func)\n+                    logger.exception(\"Error calling %s in on_commit() (%s).\", name, e)",
      "comment": "Updated the tests, `test_robust_if_no_transaction_with_callback_as_partial` should cover this",
      "comment_id": 2635026410,
      "user": "krishna-holvi",
      "created_at": "2025-12-19T12:58:53Z",
      "url": "https://github.com/django/django/pull/19609#discussion_r2635026410"
    },
    {
      "repo": "django/django",
      "pr_number": 20411,
      "file_path": "django/db/backends/base/schema.py",
      "line": 1676,
      "side": "RIGHT",
      "diff_hunk": "@@ -1669,8 +1669,11 @@ def _field_indexes_sql(self, model, field):\n         return output\n \n     def _field_should_be_altered(self, old_field, new_field, ignore=None):\n-        if (not (old_field.concrete or old_field.many_to_many)) and (\n-            not (new_field.concrete or new_field.many_to_many)\n+        if (\n+            not old_field.concrete\n+            and not new_field.concrete\n+            and not old_field.many_to_many\n+            and not new_field.many_to_many",
      "comment": "Reverted \ud83d\udc4d\ud83c\udffe. I'd changed it so it's easier to read.",
      "comment_id": 2621047456,
      "user": "cliffordgama",
      "created_at": "2025-12-15T22:18:44Z",
      "url": "https://github.com/django/django/pull/20411#discussion_r2621047456"
    },
    {
      "repo": "django/django",
      "pr_number": 20411,
      "file_path": "django/db/backends/base/schema.py",
      "line": 1716,
      "side": "RIGHT",
      "diff_hunk": "@@ -1708,19 +1695,28 @@ def _field_should_be_altered(self, old_field, new_field, ignore=None):\n         ):\n             old_kwargs.pop(\"to\", None)\n             new_kwargs.pop(\"to\", None)\n-        # db_default can take many form but result in the same SQL.\n+        # db_default can take many forms but result in the same SQL.\n         if (\n             old_kwargs.get(\"db_default\")\n             and new_kwargs.get(\"db_default\")\n             and self.db_default_sql(old_field) == self.db_default_sql(new_field)\n         ):\n             old_kwargs.pop(\"db_default\")\n             new_kwargs.pop(\"db_default\")\n-        return (\n-            old_field.concrete\n-            and new_field.concrete\n-            and (self.quote_name(old_field.column) != self.quote_name(new_field.column))\n-        ) or (old_path, old_args, old_kwargs) != (new_path, new_args, new_kwargs)\n+\n+        old_schema_name = (\n+            self.quote_name(old_field.column)\n+            if not old_field.many_to_many\n+            else old_field.attname\n+        )\n+        new_schema_name = (\n+            self.quote_name(new_field.column)\n+            if not new_field.many_to_many\n+            else new_field.attname\n+        )",
      "comment": "@cliffordgama After having a look at #20412, do you have an opinion about how to factor this? I think the multiple returns helps readability.",
      "comment_id": 2621146065,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-15T23:10:08Z",
      "url": "https://github.com/django/django/pull/20411#discussion_r2621146065"
    },
    {
      "repo": "django/django",
      "pr_number": 20287,
      "file_path": "django/contrib/admindocs/utils.py",
      "line": 268,
      "side": "RIGHT",
      "diff_hunk": "@@ -265,4 +265,4 @@ def remove_non_capturing_groups(pattern):\n \n \n def strip_p_tags(value):\n-    return mark_safe(value.replace(\"<p>\", \"\").replace(\"</p>\", \"\"))\n+    return SafeString(value.replace(\"<p>\", \"\").replace(\"</p>\", \"\"))",
      "comment": "On second thought, can you revert this also? It's the only one that's not a a literal string (in other words, where we can't guarantee the type is exactly `str`. There could be a lazy object with a custom `replace` method that relies on this somehow.",
      "comment_id": 2627620439,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-17T15:55:48Z",
      "url": "https://github.com/django/django/pull/20287#discussion_r2627620439"
    },
    {
      "repo": "django/django",
      "pr_number": 20287,
      "file_path": "django/contrib/admindocs/utils.py",
      "line": 268,
      "side": "RIGHT",
      "diff_hunk": "@@ -265,4 +265,4 @@ def remove_non_capturing_groups(pattern):\n \n \n def strip_p_tags(value):\n-    return mark_safe(value.replace(\"<p>\", \"\").replace(\"</p>\", \"\"))\n+    return SafeString(value.replace(\"<p>\", \"\").replace(\"</p>\", \"\"))",
      "comment": "thanks for catching this.  Yes there might case where title(value) could be None or ",
      "comment_id": 2627702376,
      "user": "p-r-a-v-i-n",
      "created_at": "2025-12-17T16:18:21Z",
      "url": "https://github.com/django/django/pull/20287#discussion_r2627702376"
    },
    {
      "repo": "django/django",
      "pr_number": 20287,
      "file_path": "django/contrib/admindocs/utils.py",
      "line": 268,
      "side": "RIGHT",
      "diff_hunk": "@@ -265,4 +265,4 @@ def remove_non_capturing_groups(pattern):\n \n \n def strip_p_tags(value):\n-    return mark_safe(value.replace(\"<p>\", \"\").replace(\"</p>\", \"\"))\n+    return SafeString(value.replace(\"<p>\", \"\").replace(\"</p>\", \"\"))",
      "comment": "something could have been done but i'm not confident\r\n```\r\ndef strip_p_tags(value):\r\n    if value is None:\r\n        return \"\"\r\n    return SafeString(str(value).replace(\"<p>\", \"\").replace(\"</p>\", \"\"))\r\n```",
      "comment_id": 2627725051,
      "user": "p-r-a-v-i-n",
      "created_at": "2025-12-17T16:23:07Z",
      "url": "https://github.com/django/django/pull/20287#discussion_r2627725051"
    },
    {
      "repo": "django/django",
      "pr_number": 20067,
      "file_path": "django/db/models/fields/related_descriptors.py",
      "line": 1290,
      "side": "RIGHT",
      "diff_hunk": "@@ -1280,8 +1281,15 @@ def add(self, *objs, through_defaults=None):\n                         self.source_field_name,\n                         *objs,\n                         through_defaults=through_defaults,\n+                        using=db,\n+                        raw=raw,\n                     )\n \n+        def add(self, *objs, through_defaults=None):\n+            self._remove_prefetched_objects()\n+            db = router.db_for_write(self.through, instance=self.instance)",
      "comment": "I take it you prefer the explicitness of doing this here versus relying on the `or ...` in `_add_base()`?",
      "comment_id": 2624700249,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-16T20:53:09Z",
      "url": "https://github.com/django/django/pull/20067#discussion_r2624700249"
    },
    {
      "repo": "django/django",
      "pr_number": 20067,
      "file_path": "django/db/models/fields/related_descriptors.py",
      "line": 1290,
      "side": "RIGHT",
      "diff_hunk": "@@ -1280,8 +1281,15 @@ def add(self, *objs, through_defaults=None):\n                         self.source_field_name,\n                         *objs,\n                         through_defaults=through_defaults,\n+                        using=db,\n+                        raw=raw,\n                     )\n \n+        def add(self, *objs, through_defaults=None):\n+            self._remove_prefetched_objects()\n+            db = router.db_for_write(self.through, instance=self.instance)",
      "comment": "I wanted to keep all hook consistent also with other related descriptors.",
      "comment_id": 2627751636,
      "user": "felixxm",
      "created_at": "2025-12-17T16:30:21Z",
      "url": "https://github.com/django/django/pull/20067#discussion_r2627751636"
    },
    {
      "repo": "django/django",
      "pr_number": 19870,
      "file_path": "django/db/models/constraints.py",
      "line": 410,
      "side": "RIGHT",
      "diff_hunk": "@@ -406,7 +406,8 @@ def check(self, model, connection):\n             errors.append(\n                 checks.Warning(\n                     f\"{connection.display_name} does not support unique constraints \"\n-                    \"with nulls distinct.\",\n+                    \"with nulls distinct. They are ignored for databases besides \"\n+                    \"PostgreSQL 15+.\",",
      "comment": "I would maybe have:\r\n```python\r\n                    f\"{connection.display_name} does not support the \"\r\n                    \"'nulls_distinct' parameter for unique constraints.\",\r\n```\r\nAs I think we want to emphasizing that having values for `nulls_distinct` set is not supported (rather than this being interpreted that nulls_distinct=False might be valid \r\n\r\nNote that any changes to the warning needs to be updated in the system check docs:\r\n```diff\r\n--- a/docs/ref/checks.txt\r\n+++ b/docs/ref/checks.txt\r\n@@ -438,8 +438,8 @@ Models\r\n   expression and won't be validated during the model ``full_clean()``.\r\n * **models.W046**: ``<database>`` does not support comments on tables\r\n   (``db_table_comment``).\r\n-* **models.W047**: ``<database>`` does not support unique constraints with\r\n-  nulls distinct.\r\n+* **models.W047**: ``<database>`` does not support the 'nulls_distinct'\r\n+  parameter for unique constraints.\r\n * **models.E048**: ``constraints/indexes/unique_together`` refers to a\r\n   ``CompositePrimaryKey`` ``<field name>``, but ``CompositePrimaryKey``\\s are\r\n   not supported for that option.\r\n```\r\n",
      "comment_id": 2362529323,
      "user": "sarahboyce",
      "created_at": "2025-09-19T10:56:05Z",
      "url": "https://github.com/django/django/pull/19870#discussion_r2362529323"
    },
    {
      "repo": "django/django",
      "pr_number": 19870,
      "file_path": "django/db/models/constraints.py",
      "line": 410,
      "side": "RIGHT",
      "diff_hunk": "@@ -406,7 +406,8 @@ def check(self, model, connection):\n             errors.append(\n                 checks.Warning(\n                     f\"{connection.display_name} does not support unique constraints \"\n-                    \"with nulls distinct.\",\n+                    \"with nulls distinct. They are ignored for databases besides \"\n+                    \"PostgreSQL 15+.\",",
      "comment": "parameter -> argument\r\nor perhaps \"does not support UniqueConstraint.nulls_distinct.\"",
      "comment_id": 2362588094,
      "user": "timgraham",
      "created_at": "2025-09-19T11:26:22Z",
      "url": "https://github.com/django/django/pull/19870#discussion_r2362588094"
    },
    {
      "repo": "django/django",
      "pr_number": 20377,
      "file_path": "django/core/serializers/xml_serializer.py",
      "line": 424,
      "side": "RIGHT",
      "diff_hunk": "@@ -421,7 +421,7 @@ def getInnerText(node):\n     return \"\".join(inner_text_list)\n \n \n-def getInnerTextList(node):\n+def getInnerTextList(node, max_depth=1, _depth=0):",
      "comment": "Limit max recursion depth to 1 level because fixtures only need text one level deep",
      "comment_id": 2594826777,
      "user": "p-r-a-v-i-n",
      "created_at": "2025-12-06T12:52:05Z",
      "url": "https://github.com/django/django/pull/20377#discussion_r2594826777"
    },
    {
      "repo": "django/django",
      "pr_number": 20377,
      "file_path": "django/core/serializers/xml_serializer.py",
      "line": 427,
      "side": "RIGHT",
      "diff_hunk": "@@ -421,8 +421,11 @@ def getInnerText(node):\n     return \"\".join(inner_text_list)\n \n \n-def getInnerTextList(node):\n-    \"\"\"Return a list of the inner texts of a DOM node (recursively).\"\"\"\n+def getInnerTextList(node, max_depth=1, _depth=0):\n+    \"\"\"\n+    Return inner text from a DOM node, limited by max_depth.\n+    This avoids collecting deeply nested text.",
      "comment": "Thanks, but `getInnerText()` is still not correct.",
      "comment_id": 2602644048,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-09T13:22:33Z",
      "url": "https://github.com/django/django/pull/20377#discussion_r2602644048"
    },
    {
      "repo": "django/django",
      "pr_number": 20377,
      "file_path": "django/core/serializers/xml_serializer.py",
      "line": 427,
      "side": "RIGHT",
      "diff_hunk": "@@ -421,8 +421,11 @@ def getInnerText(node):\n     return \"\".join(inner_text_list)\n \n \n-def getInnerTextList(node):\n-    \"\"\"Return a list of the inner texts of a DOM node (recursively).\"\"\"\n+def getInnerTextList(node, max_depth=1, _depth=0):\n+    \"\"\"\n+    Return inner text from a DOM node, limited by max_depth.\n+    This avoids collecting deeply nested text.",
      "comment": "my bad, i missed it. thank for catching it. I have updated now",
      "comment_id": 2602717132,
      "user": "p-r-a-v-i-n",
      "created_at": "2025-12-09T13:38:45Z",
      "url": "https://github.com/django/django/pull/20377#discussion_r2602717132"
    },
    {
      "repo": "django/django",
      "pr_number": 20377,
      "file_path": "tests/fixtures/tests.py",
      "line": 530,
      "side": "RIGHT",
      "diff_hunk": "@@ -520,6 +521,14 @@ def test_loading_and_dumping(self):\n             natural_foreign_keys=True,\n         )\n \n+    def test_deeply_nested_elements(self):\n+        \"\"\"Text inside deeply-nested tags is skipped.\"\"\"\n+        management.call_command(\n+            \"loaddata\", \"invalid_deeply_nested_elements.xml\", verbosity=0\n+        )\n+        person = Person.objects.get(pk=1)\n+        self.assertEqual(person.name, \"Django\")  # not \"Django pony\"",
      "comment": "I find that surprising. I'd expect `Django <em>pony</em>` or a `SuspiciousOperation`.",
      "comment_id": 2616789793,
      "user": "shaib",
      "created_at": "2025-12-14T07:14:56Z",
      "url": "https://github.com/django/django/pull/20377#discussion_r2616789793"
    },
    {
      "repo": "django/django",
      "pr_number": 20281,
      "file_path": "tests/migrations/test_loader.py",
      "line": 687,
      "side": "RIGHT",
      "diff_hunk": "@@ -682,8 +682,10 @@ def test_loading_order_does_not_create_circular_dependency(self):\n         with tempfile.NamedTemporaryFile(\n             mode=\"w\", encoding=\"utf-8\", suffix=\".py\", dir=tests_dir, delete=False\n         ) as test_settings:\n+            self.addCleanup(os.remove, test_settings.name)\n             for attr, value in settings._wrapped.__dict__.items():\n-                if attr.isupper():\n+                # Only write literal values.",
      "comment": "\"... so that any settings that reference a value that needs an import are omitted.\"",
      "comment_id": 2548187936,
      "user": "timgraham",
      "created_at": "2025-11-21T01:00:43Z",
      "url": "https://github.com/django/django/pull/20281#discussion_r2548187936"
    },
    {
      "repo": "django/django",
      "pr_number": 20374,
      "file_path": "django/db/models/sql/query.py",
      "line": 2362,
      "side": "RIGHT",
      "diff_hunk": "@@ -2338,6 +2345,34 @@ def add_ordering(self, *ordering):\n         else:\n             self.default_ordering = False\n \n+    @property\n+    def orderby_issubset_groupby(self):\n+        \"\"\"\n+        Return true if order_by of query is subset of group_by.\n+        \"\"\"\n+        # we don't want to harm original query, so we need to clone it\n+        q = self.clone()\n+        if q.group_by is False:\n+            # there is no use case for that, but we need to be sure\n+            return False\n+        if q.group_by in (None, True):\n+            # it seems like there is no aggregation at all (None)\n+            # or there are all required groupbies(True) generated automatically\n+            # from models fields - so, it is safe to clear ordering\n+            return True",
      "comment": "No need to clone immediately for these two check as it's a potentially costly operation.\r\n\r\n```suggestion\r\n        if self.group_by is False:\r\n            # there is no use case for that, but we need to be sure\r\n            return False\r\n        if self.group_by in (None, True):\r\n            # it seems like there is no aggregation at all (None)\r\n            # or there are all required groupbies(True) generated automatically\r\n            # from models fields - so, it is safe to clear ordering\r\n            return True\r\n        q = self.clone()\r\n```",
      "comment_id": 2593529018,
      "user": "charettes",
      "created_at": "2025-12-05T18:11:52Z",
      "url": "https://github.com/django/django/pull/20374#discussion_r2593529018"
    },
    {
      "repo": "django/django",
      "pr_number": 20374,
      "file_path": "django/db/models/sql/query.py",
      "line": 2372,
      "side": "RIGHT",
      "diff_hunk": "@@ -2338,6 +2345,34 @@ def add_ordering(self, *ordering):\n         else:\n             self.default_ordering = False\n \n+    @property\n+    def orderby_issubset_groupby(self):\n+        \"\"\"\n+        Return true if order_by of query is subset of group_by.\n+        \"\"\"\n+        # we don't want to harm original query, so we need to clone it\n+        q = self.clone()\n+        if q.group_by is False:\n+            # there is no use case for that, but we need to be sure\n+            return False\n+        if q.group_by in (None, True):\n+            # it seems like there is no aggregation at all (None)\n+            # or there are all required groupbies(True) generated automatically\n+            # from models fields - so, it is safe to clear ordering\n+            return True\n+        order_by_set = set(\n+            [\n+                (\n+                    order_by.resolve_expression(q)\n+                    if hasattr(order_by, \"resolve_expression\")\n+                    else F(order_by).resolve_expression(q)\n+                )\n+                for order_by in q.order_by\n+            ]\n+        ).union(q.extra_order_by)",
      "comment": "You can also avoid the clone if `not (q.order_by or q.extra_order_by)` I believe as an empty set is necessarily a subset of the group by.",
      "comment_id": 2593533846,
      "user": "charettes",
      "created_at": "2025-12-05T18:13:47Z",
      "url": "https://github.com/django/django/pull/20374#discussion_r2593533846"
    },
    {
      "repo": "django/django",
      "pr_number": 20374,
      "file_path": "django/db/models/sql/query.py",
      "line": 544,
      "side": "RIGHT",
      "diff_hunk": "@@ -534,7 +534,14 @@ def get_aggregation(self, using, aggregate_exprs):\n             # Queries with distinct_fields need ordering and when a limit is\n             # applied we must take the slice from the ordered query. Otherwise\n             # no need for ordering.\n-            inner_query.clear_ordering(force=False)\n+            if (\n+                not inner_query.is_sliced\n+                and not inner_query.distinct_fields\n+                and inner_query.orderby_issubset_groupby\n+            ):\n+                inner_query.order_by = ()\n+                inner_query.extra_order_by = ()\n+                inner_query.default_ordering = False",
      "comment": "Should we still delegate to `clear_ordering` with `force=True` instead?\r\n\r\n```suggestion\r\n            if (\r\n                not inner_query.is_sliced\r\n                and not inner_query.distinct_fields\r\n                and inner_query.orderby_issubset_groupby\r\n            ):\r\n                inner_query.clear_ordering(force=True)\r\n```",
      "comment_id": 2594028927,
      "user": "charettes",
      "created_at": "2025-12-05T21:26:04Z",
      "url": "https://github.com/django/django/pull/20374#discussion_r2594028927"
    },
    {
      "repo": "django/django",
      "pr_number": 20374,
      "file_path": "django/db/models/sql/query.py",
      "line": 2374,
      "side": "RIGHT",
      "diff_hunk": "@@ -2338,6 +2345,34 @@ def add_ordering(self, *ordering):\n         else:\n             self.default_ordering = False\n \n+    @property\n+    def orderby_issubset_groupby(self):\n+        \"\"\"\n+        Return true if order_by of query is subset of group_by.\n+        \"\"\"\n+        # we don't want to harm original query, so we need to clone it\n+        q = self.clone()\n+        if q.group_by is False:\n+            # there is no use case for that, but we need to be sure\n+            return False\n+        if q.group_by in (None, True):\n+            # it seems like there is no aggregation at all (None)\n+            # or there are all required groupbies(True) generated automatically\n+            # from models fields - so, it is safe to clear ordering\n+            return True\n+        order_by_set = set(\n+            [\n+                (\n+                    order_by.resolve_expression(q)\n+                    if hasattr(order_by, \"resolve_expression\")\n+                    else F(order_by).resolve_expression(q)\n+                )\n+                for order_by in q.order_by\n+            ]\n+        ).union(q.extra_order_by)\n+        group_by_set = set([group_by.resolve_expression(q) for group_by in q.group_by])\n+        return order_by_set.issubset(group_by_set)",
      "comment": "Do we have members of `group_by` which are not already resolved? I thought they must be already resolved since they are plucked from `Query.select` and `annotations`.\r\n\r\nhttps://github.com/django/django/blob/020e5799ad74cfafd469f032cd05767c9d670a16/django/db/models/sql/query.py#L2383-L2391\r\n\r\nassuming this is the case it means it could be reduced to\r\n\r\n```suggestion\r\n        return order_by_set.issubset(self.group_by)\r\n```",
      "comment_id": 2594039459,
      "user": "charettes",
      "created_at": "2025-12-05T21:31:57Z",
      "url": "https://github.com/django/django/pull/20374#discussion_r2594039459"
    },
    {
      "repo": "django/django",
      "pr_number": 20374,
      "file_path": "tests/aggregation_regress/tests.py",
      "line": 194,
      "side": "RIGHT",
      "diff_hunk": "@@ -171,6 +171,56 @@ def assertObjectAttrs(self, obj, **kwargs):\n         for attr, value in kwargs.items():\n             self.assertEqual(getattr(obj, attr), value)\n \n+    def test_count_preserve_group_by(self):\n+        # new release of the same book\n+        Book.objects.create(\n+            isbn=\"113235613\",\n+            name=self.b4.name,\n+            pages=self.b4.pages,\n+            rating=4.0,\n+            price=Decimal(\"39.69\"),\n+            contact=self.a5,\n+            publisher=self.p3,\n+            pubdate=datetime.date(2018, 11, 3),\n+        )\n+        qs = Book.objects.values(\"contact__name\", \"publisher__name\").annotate(\n+            publications=Count(\"id\")\n+        )\n+        self.assertEqual(qs.count(), 6)\n+        self.assertEqual(qs.count(), len(qs))\n+        # before ticket 26434 had been solved .count() was returning also 6\n+        self.assertEqual(qs.order_by(\"id\").count(), 7)\n+        # before ticket 26434 had been solved .count() was not equal to len(qs)\n+        self.assertEqual(qs.order_by(\"id\").count(), len(qs.order_by(\"id\")))",
      "comment": "I feel like the last assertion is enough as it keeps the ticket focused on the problem at play (mismatch in length between count and `len`) but mergers will\r\n\r\n```suggestion\r\n        self.assertEqual(qs.order_by(\"id\").count(), len(qs.order_by(\"id\")))\r\n```\r\n\r\n",
      "comment_id": 2596683385,
      "user": "charettes",
      "created_at": "2025-12-07T23:07:09Z",
      "url": "https://github.com/django/django/pull/20374#discussion_r2596683385"
    },
    {
      "repo": "django/django",
      "pr_number": 20374,
      "file_path": "tests/aggregation_regress/tests.py",
      "line": 223,
      "side": "RIGHT",
      "diff_hunk": "@@ -171,6 +171,56 @@ def assertObjectAttrs(self, obj, **kwargs):\n         for attr, value in kwargs.items():\n             self.assertEqual(getattr(obj, attr), value)\n \n+    def test_count_preserve_group_by(self):\n+        # new release of the same book\n+        Book.objects.create(\n+            isbn=\"113235613\",\n+            name=self.b4.name,\n+            pages=self.b4.pages,\n+            rating=4.0,\n+            price=Decimal(\"39.69\"),\n+            contact=self.a5,\n+            publisher=self.p3,\n+            pubdate=datetime.date(2018, 11, 3),\n+        )\n+        qs = Book.objects.values(\"contact__name\", \"publisher__name\").annotate(\n+            publications=Count(\"id\")\n+        )\n+        self.assertEqual(qs.count(), 6)\n+        self.assertEqual(qs.count(), len(qs))\n+        # before ticket 26434 had been solved .count() was returning also 6\n+        self.assertEqual(qs.order_by(\"id\").count(), 7)\n+        # before ticket 26434 had been solved .count() was not equal to len(qs)\n+        self.assertEqual(qs.order_by(\"id\").count(), len(qs.order_by(\"id\")))\n+\n+    def test_aggregate_preserve_group_by(self):\n+        # new release of the same book\n+        Book.objects.create(\n+            isbn=\"113235613\",\n+            name=self.b4.name,\n+            pages=self.b4.pages,\n+            rating=4.0,\n+            price=Decimal(\"39.69\"),\n+            contact=self.a5,\n+            publisher=self.p3,\n+            pubdate=datetime.date(2018, 11, 3),\n+        )\n+        qs = Book.objects.values(\"contact__name\", \"publisher__name\").annotate(\n+            publications=Count(\"id\")\n+        )\n+        self.assertEqual(qs.count(), 6)\n+        self.assertEqual(\n+            qs.aggregate(Avg(\"rating\"))[\"rating__avg\"], (12.5 + 3 * 4.0) / 6\n+        )\n+        # before ticket 26434 had been solved .count() was returning also 6\n+        self.assertEqual(qs.order_by(\"id\").count(), 7)\n+        # before ticket 26434 had been solved .aggregate(Avg(...))\n+        # was returning also (12.5 + 3 * 4.0) / 6)\n+        self.assertEqual(\n+            qs.order_by(\"id\").aggregate(Avg(\"rating\"))[\"rating__avg\"],\n+            (12.5 + 4 * 4.0) / 7,\n+        )\n+",
      "comment": "Similar story here and you can drop the ticket reference as it's _baked_ into the commit message and easily referenceable through blame.\r\n\r\n```suggestion\r\n        self.assertEqual(\r\n            qs.order_by(\"id\").aggregate(Avg(\"rating\"))[\"rating__avg\"],\r\n            (12.5 + 4 * 4.0) / 7,\r\n        )\r\n\r\n```",
      "comment_id": 2596684186,
      "user": "charettes",
      "created_at": "2025-12-07T23:08:41Z",
      "url": "https://github.com/django/django/pull/20374#discussion_r2596684186"
    },
    {
      "repo": "django/django",
      "pr_number": 20374,
      "file_path": "django/db/models/sql/query.py",
      "line": 2362,
      "side": "RIGHT",
      "diff_hunk": "@@ -2338,6 +2343,34 @@ def add_ordering(self, *ordering):\n         else:\n             self.default_ordering = False\n \n+    @property\n+    def orderby_issubset_groupby(self):\n+        \"\"\"\n+        Return true if order_by of query is subset of group_by.\n+        \"\"\"\n+        if self.group_by is False:\n+            # there is no use case for that, but we need to be sure\n+            return False\n+        if self.group_by in (None, True):\n+            # it seems like there is no aggregation at all (None)\n+            # or there are all required groupbies(True) generated automatically\n+            # from models fields - so, it is safe to clear ordering\n+            return True\n+        if not (self.order_by or self.group_by):\n+            # empty set is necessarily a subset of the group by\n+            return True\n+        # we don't want to harm original query, so we need to clone it",
      "comment": "Maybe harm is too harsh and we want to use _pollute_ but I'm not native speaker ",
      "comment_id": 2596684991,
      "user": "charettes",
      "created_at": "2025-12-07T23:09:32Z",
      "url": "https://github.com/django/django/pull/20374#discussion_r2596684991"
    },
    {
      "repo": "django/django",
      "pr_number": 20374,
      "file_path": "django/db/models/sql/query.py",
      "line": 2353,
      "side": "RIGHT",
      "diff_hunk": "@@ -2338,6 +2343,34 @@ def add_ordering(self, *ordering):\n         else:\n             self.default_ordering = False\n \n+    @property\n+    def orderby_issubset_groupby(self):\n+        \"\"\"\n+        Return true if order_by of query is subset of group_by.\n+        \"\"\"\n+        if self.group_by is False:\n+            # there is no use case for that, but we need to be sure\n+            return False",
      "comment": "If there is no use case it means it's dead code from its inception and thus and it can be removed.\r\n\r\n`Query.group_by` appears to be `None | True | tuple[Compilable]`",
      "comment_id": 2596686043,
      "user": "charettes",
      "created_at": "2025-12-07T23:10:54Z",
      "url": "https://github.com/django/django/pull/20374#discussion_r2596686043"
    },
    {
      "repo": "django/django",
      "pr_number": 20374,
      "file_path": "django/db/models/sql/query.py",
      "line": 2357,
      "side": "RIGHT",
      "diff_hunk": "@@ -2338,6 +2343,34 @@ def add_ordering(self, *ordering):\n         else:\n             self.default_ordering = False\n \n+    @property\n+    def orderby_issubset_groupby(self):\n+        \"\"\"\n+        Return true if order_by of query is subset of group_by.\n+        \"\"\"\n+        if self.group_by is False:\n+            # there is no use case for that, but we need to be sure\n+            return False\n+        if self.group_by in (None, True):\n+            # it seems like there is no aggregation at all (None)\n+            # or there are all required groupbies(True) generated automatically\n+            # from models fields - so, it is safe to clear ordering",
      "comment": "Minor but the _it is safe to clear ordering_ assumes the sole caller will always be for order clearing purpose.\r\n\r\nMaybe something along the lines of \r\n\r\n```\r\nWhen grouping by all fields the order by is necessarily a subset of the group by.\r\n```",
      "comment_id": 2596689748,
      "user": "charettes",
      "created_at": "2025-12-07T23:15:31Z",
      "url": "https://github.com/django/django/pull/20374#discussion_r2596689748"
    },
    {
      "repo": "django/django",
      "pr_number": 20374,
      "file_path": "django/db/models/sql/query.py",
      "line": 2353,
      "side": "RIGHT",
      "diff_hunk": "@@ -2338,6 +2343,34 @@ def add_ordering(self, *ordering):\n         else:\n             self.default_ordering = False\n \n+    @property\n+    def orderby_issubset_groupby(self):\n+        \"\"\"\n+        Return true if order_by of query is subset of group_by.\n+        \"\"\"\n+        if self.group_by is False:\n+            # there is no use case for that, but we need to be sure\n+            return False",
      "comment": "That's an [unrelated flaky test that we're addressing](https://code.djangoproject.com/ticket/36769#comment:9) as soon as Python issues a patch release of 3.12.",
      "comment_id": 2615132395,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-12T18:05:54Z",
      "url": "https://github.com/django/django/pull/20374#discussion_r2615132395"
    },
    {
      "repo": "django/django",
      "pr_number": 20374,
      "file_path": "django/db/models/sql/query.py",
      "line": 2370,
      "side": "RIGHT",
      "diff_hunk": "@@ -2338,6 +2343,33 @@ def add_ordering(self, *ordering):\n         else:\n             self.default_ordering = False\n \n+    @property\n+    def orderby_issubset_groupby(self):\n+        \"\"\"\n+        Return true if order_by of query is subset of group_by.\n+        \"\"\"\n+        if self.group_by is False:\n+            return False\n+        if self.group_by in (None, True):\n+            # it seems like there is no aggregation at all (None)\n+            # or there are all groupbies(True) generated automatically from\n+            # models fields - the order by is necessarilly a subset of them\n+            return True\n+        if not (self.order_by or self.group_by):\n+            # empty set is necessarily a subset of the group by\n+            return True\n+        # we don't want to pollute original query(joins)\n+        q = self.clone()\n+        order_by_set = {\n+            (\n+                order_by.resolve_expression(q)\n+                if hasattr(order_by, \"resolve_expression\")\n+                else F(order_by).resolve_expression(q)\n+            )\n+            for order_by in q.order_by\n+        }.union(q.extra_order_by)",
      "comment": "`extra_order_by` is composed of strings, don't we need to resolve them to `Col`s as well?",
      "comment_id": 2615244857,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-12T18:50:47Z",
      "url": "https://github.com/django/django/pull/20374#discussion_r2615244857"
    },
    {
      "repo": "django/django",
      "pr_number": 20374,
      "file_path": "django/db/models/sql/query.py",
      "line": 2370,
      "side": "RIGHT",
      "diff_hunk": "@@ -2338,6 +2343,33 @@ def add_ordering(self, *ordering):\n         else:\n             self.default_ordering = False\n \n+    @property\n+    def orderby_issubset_groupby(self):\n+        \"\"\"\n+        Return true if order_by of query is subset of group_by.\n+        \"\"\"\n+        if self.group_by is False:\n+            return False\n+        if self.group_by in (None, True):\n+            # it seems like there is no aggregation at all (None)\n+            # or there are all groupbies(True) generated automatically from\n+            # models fields - the order by is necessarilly a subset of them\n+            return True\n+        if not (self.order_by or self.group_by):\n+            # empty set is necessarily a subset of the group by\n+            return True\n+        # we don't want to pollute original query(joins)\n+        q = self.clone()\n+        order_by_set = {\n+            (\n+                order_by.resolve_expression(q)\n+                if hasattr(order_by, \"resolve_expression\")\n+                else F(order_by).resolve_expression(q)\n+            )\n+            for order_by in q.order_by\n+        }.union(q.extra_order_by)",
      "comment": "@jacobtylerwalls they can't; `extra_order_by` is this weird thing coming from `extra(order_by)` which is raw SQL.\r\n\r\nI've been trying to kill it #16681 for reference.",
      "comment_id": 2615249469,
      "user": "charettes",
      "created_at": "2025-12-12T18:52:41Z",
      "url": "https://github.com/django/django/pull/20374#discussion_r2615249469"
    },
    {
      "repo": "django/django",
      "pr_number": 20374,
      "file_path": "django/db/models/sql/query.py",
      "line": 2370,
      "side": "RIGHT",
      "diff_hunk": "@@ -2338,6 +2343,33 @@ def add_ordering(self, *ordering):\n         else:\n             self.default_ordering = False\n \n+    @property\n+    def orderby_issubset_groupby(self):\n+        \"\"\"\n+        Return true if order_by of query is subset of group_by.\n+        \"\"\"\n+        if self.group_by is False:\n+            return False\n+        if self.group_by in (None, True):\n+            # it seems like there is no aggregation at all (None)\n+            # or there are all groupbies(True) generated automatically from\n+            # models fields - the order by is necessarilly a subset of them\n+            return True\n+        if not (self.order_by or self.group_by):\n+            # empty set is necessarily a subset of the group by\n+            return True\n+        # we don't want to pollute original query(joins)\n+        q = self.clone()\n+        order_by_set = {\n+            (\n+                order_by.resolve_expression(q)\n+                if hasattr(order_by, \"resolve_expression\")\n+                else F(order_by).resolve_expression(q)\n+            )\n+            for order_by in q.order_by\n+        }.union(q.extra_order_by)",
      "comment": "Not sure if `extra(select)` ever end up in `group_by` but we are definitely more interested in `q.extra_order_by.values()` here.\r\n\r\nMaybe we could return `False` the moment `bool(q.extra_order_by)` until we finally kill it?",
      "comment_id": 2615262628,
      "user": "charettes",
      "created_at": "2025-12-12T18:57:15Z",
      "url": "https://github.com/django/django/pull/20374#discussion_r2615262628"
    },
    {
      "repo": "django/django",
      "pr_number": 20374,
      "file_path": "django/db/models/sql/query.py",
      "line": 2370,
      "side": "RIGHT",
      "diff_hunk": "@@ -2338,6 +2343,33 @@ def add_ordering(self, *ordering):\n         else:\n             self.default_ordering = False\n \n+    @property\n+    def orderby_issubset_groupby(self):\n+        \"\"\"\n+        Return true if order_by of query is subset of group_by.\n+        \"\"\"\n+        if self.group_by is False:\n+            return False\n+        if self.group_by in (None, True):\n+            # it seems like there is no aggregation at all (None)\n+            # or there are all groupbies(True) generated automatically from\n+            # models fields - the order by is necessarilly a subset of them\n+            return True\n+        if not (self.order_by or self.group_by):\n+            # empty set is necessarily a subset of the group by\n+            return True\n+        # we don't want to pollute original query(joins)\n+        q = self.clone()\n+        order_by_set = {\n+            (\n+                order_by.resolve_expression(q)\n+                if hasattr(order_by, \"resolve_expression\")\n+                else F(order_by).resolve_expression(q)\n+            )\n+            for order_by in q.order_by\n+        }.union(q.extra_order_by)",
      "comment": "Yes, makes sense. extra(order_by) is just the weird legacy case we need to account for. I'll update the patch accordingly.",
      "comment_id": 2615276838,
      "user": "michalnik",
      "created_at": "2025-12-12T19:03:08Z",
      "url": "https://github.com/django/django/pull/20374#discussion_r2615276838"
    },
    {
      "repo": "django/django",
      "pr_number": 20374,
      "file_path": "django/db/models/sql/query.py",
      "line": 2370,
      "side": "RIGHT",
      "diff_hunk": "@@ -2338,6 +2343,33 @@ def add_ordering(self, *ordering):\n         else:\n             self.default_ordering = False\n \n+    @property\n+    def orderby_issubset_groupby(self):\n+        \"\"\"\n+        Return true if order_by of query is subset of group_by.\n+        \"\"\"\n+        if self.group_by is False:\n+            return False\n+        if self.group_by in (None, True):\n+            # it seems like there is no aggregation at all (None)\n+            # or there are all groupbies(True) generated automatically from\n+            # models fields - the order by is necessarilly a subset of them\n+            return True\n+        if not (self.order_by or self.group_by):\n+            # empty set is necessarily a subset of the group by\n+            return True\n+        # we don't want to pollute original query(joins)\n+        q = self.clone()\n+        order_by_set = {\n+            (\n+                order_by.resolve_expression(q)\n+                if hasattr(order_by, \"resolve_expression\")\n+                else F(order_by).resolve_expression(q)\n+            )\n+            for order_by in q.order_by\n+        }.union(q.extra_order_by)",
      "comment": "The test case I just pushed gives me this:\r\n```py\r\n(Pdb) print(q.extra_order_by)\r\n['id']\r\n```",
      "comment_id": 2615277284,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-12T19:03:20Z",
      "url": "https://github.com/django/django/pull/20374#discussion_r2615277284"
    },
    {
      "repo": "django/django",
      "pr_number": 20374,
      "file_path": "django/db/models/sql/query.py",
      "line": 2370,
      "side": "RIGHT",
      "diff_hunk": "@@ -2338,6 +2343,33 @@ def add_ordering(self, *ordering):\n         else:\n             self.default_ordering = False\n \n+    @property\n+    def orderby_issubset_groupby(self):\n+        \"\"\"\n+        Return true if order_by of query is subset of group_by.\n+        \"\"\"\n+        if self.group_by is False:\n+            return False\n+        if self.group_by in (None, True):\n+            # it seems like there is no aggregation at all (None)\n+            # or there are all groupbies(True) generated automatically from\n+            # models fields - the order by is necessarilly a subset of them\n+            return True\n+        if not (self.order_by or self.group_by):\n+            # empty set is necessarily a subset of the group by\n+            return True\n+        # we don't want to pollute original query(joins)\n+        q = self.clone()\n+        order_by_set = {\n+            (\n+                order_by.resolve_expression(q)\n+                if hasattr(order_by, \"resolve_expression\")\n+                else F(order_by).resolve_expression(q)\n+            )\n+            for order_by in q.order_by\n+        }.union(q.extra_order_by)",
      "comment": "> this weird thing coming from extra(order_by) which is raw SQL.\r\n\r\nOh, then I guess my test isn't realistic.",
      "comment_id": 2615290542,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-12T19:08:49Z",
      "url": "https://github.com/django/django/pull/20374#discussion_r2615290542"
    },
    {
      "repo": "django/django",
      "pr_number": 20374,
      "file_path": "django/db/models/sql/query.py",
      "line": 2370,
      "side": "RIGHT",
      "diff_hunk": "@@ -2338,6 +2343,33 @@ def add_ordering(self, *ordering):\n         else:\n             self.default_ordering = False\n \n+    @property\n+    def orderby_issubset_groupby(self):\n+        \"\"\"\n+        Return true if order_by of query is subset of group_by.\n+        \"\"\"\n+        if self.group_by is False:\n+            return False\n+        if self.group_by in (None, True):\n+            # it seems like there is no aggregation at all (None)\n+            # or there are all groupbies(True) generated automatically from\n+            # models fields - the order by is necessarilly a subset of them\n+            return True\n+        if not (self.order_by or self.group_by):\n+            # empty set is necessarily a subset of the group by\n+            return True\n+        # we don't want to pollute original query(joins)\n+        q = self.clone()\n+        order_by_set = {\n+            (\n+                order_by.resolve_expression(q)\n+                if hasattr(order_by, \"resolve_expression\")\n+                else F(order_by).resolve_expression(q)\n+            )\n+            for order_by in q.order_by\n+        }.union(q.extra_order_by)",
      "comment": "You're completely right, it's a not a `dict` at all, it's a collection of raw SQL (I got confused with `extra_select`)\r\n\r\nIn this case the `.values()` doesn't make sense but unless we want to invest time in enabling this optimization in the presence of `extra(order_by)` (which is documented to be deprecated) it might be better to opt-out by returning `False` immediately in its presence.",
      "comment_id": 2615293375,
      "user": "charettes",
      "created_at": "2025-12-12T19:09:48Z",
      "url": "https://github.com/django/django/pull/20374#discussion_r2615293375"
    },
    {
      "repo": "django/django",
      "pr_number": 20374,
      "file_path": "django/db/models/sql/query.py",
      "line": 2371,
      "side": "RIGHT",
      "diff_hunk": "@@ -2338,6 +2343,32 @@ def add_ordering(self, *ordering):\n         else:\n             self.default_ordering = False\n \n+    @property\n+    def orderby_issubset_groupby(self):\n+        if self.extra_order_by:\n+            # Raw SQL from extra(order_by=...) can't be reliably compared\n+            # against resolved OrderBy/Col expressions. Treat as not a subset.\n+            return False\n+        if self.group_by in (None, True):\n+            # There is either no aggregation at all (None), or the group by\n+            # is generated automatically from model fields (True), in which\n+            # case the order by is necessarily a subset of them.\n+            return True\n+        if not (self.order_by or self.group_by):\n+            # An empty set is always a subset of something.\n+            return True\n+        # Don't pollute the original query (might disrupt joins).\n+        q = self.clone()\n+        order_by_set = {\n+            (\n+                order_by.resolve_expression(q)\n+                if hasattr(order_by, \"resolve_expression\")\n+                else F(order_by).resolve_expression(q)\n+            )\n+            for order_by in q.order_by\n+        }\n+        return order_by_set.issubset(self.group_by)\n+\n     def clear_ordering(self, force=False, clear_default=True):\n         \"\"\"\n         Remove any ordering settings if the current query allows it without",
      "comment": "Is there a reason you didn't opt for:\n```suggestion\n            if inner_query.orderby_issubset_groupby:\n                inner_query.clear_ordering(force=False)\n```",
      "comment_id": 2615411906,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-12T19:56:47Z",
      "url": "https://github.com/django/django/pull/20374#discussion_r2615411906"
    },
    {
      "repo": "django/django",
      "pr_number": 20374,
      "file_path": "django/db/models/sql/query.py",
      "line": 542,
      "side": "RIGHT",
      "diff_hunk": "@@ -534,7 +534,12 @@ def get_aggregation(self, using, aggregate_exprs):\n             # Queries with distinct_fields need ordering and when a limit is\n             # applied we must take the slice from the ordered query. Otherwise\n             # no need for ordering.\n-            inner_query.clear_ordering(force=False)\n+            if (\n+                not inner_query.is_sliced\n+                and not inner_query.distinct_fields\n+                and inner_query.orderby_issubset_groupby\n+            ):\n+                inner_query.clear_ordering(force=True)",
      "comment": "Github chewed up my last comment, leaving it again.\n\nIs there reason you didn't opt for:\n```suggestion\n            if inner_query.orderby_issubset_groupby:\n                inner_query.clear_ordering(force=False)\n```\n",
      "comment_id": 2615415580,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-12T19:58:13Z",
      "url": "https://github.com/django/django/pull/20374#discussion_r2615415580"
    },
    {
      "repo": "django/django",
      "pr_number": 20374,
      "file_path": "django/db/models/sql/query.py",
      "line": 542,
      "side": "RIGHT",
      "diff_hunk": "@@ -534,7 +534,12 @@ def get_aggregation(self, using, aggregate_exprs):\n             # Queries with distinct_fields need ordering and when a limit is\n             # applied we must take the slice from the ordered query. Otherwise\n             # no need for ordering.\n-            inner_query.clear_ordering(force=False)\n+            if (\n+                not inner_query.is_sliced\n+                and not inner_query.distinct_fields\n+                and inner_query.orderby_issubset_groupby\n+            ):\n+                inner_query.clear_ordering(force=True)",
      "comment": "I followed blindly the comments (line: 534) hoping that is true:\r\n```python\r\n# Queries with distinct_fields need ordering and when a limit is\r\n# applied we must take the slice from the ordered query. Otherwise\r\n# no need for ordering.\r\n```",
      "comment_id": 2615426796,
      "user": "michalnik",
      "created_at": "2025-12-12T20:03:27Z",
      "url": "https://github.com/django/django/pull/20374#discussion_r2615426796"
    },
    {
      "repo": "django/django",
      "pr_number": 20374,
      "file_path": "django/db/models/sql/query.py",
      "line": 542,
      "side": "RIGHT",
      "diff_hunk": "@@ -534,7 +534,12 @@ def get_aggregation(self, using, aggregate_exprs):\n             # Queries with distinct_fields need ordering and when a limit is\n             # applied we must take the slice from the ordered query. Otherwise\n             # no need for ordering.\n-            inner_query.clear_ordering(force=False)\n+            if (\n+                not inner_query.is_sliced\n+                and not inner_query.distinct_fields\n+                and inner_query.orderby_issubset_groupby\n+            ):\n+                inner_query.clear_ordering(force=True)",
      "comment": "For sure, I just mean the `clear_ordering()` actually handles that for you and also checks `select_for_update`. See the beginning of `clear_ordering()`.\r\n\r\nJust so that we don't actually introduce any drift against that implementation, I'll roll this in for you \ud83d\udc4d ",
      "comment_id": 2615539927,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-12T20:47:41Z",
      "url": "https://github.com/django/django/pull/20374#discussion_r2615539927"
    },
    {
      "repo": "django/django",
      "pr_number": 20374,
      "file_path": "tests/aggregation_regress/tests.py",
      "line": 207,
      "side": "RIGHT",
      "diff_hunk": "@@ -171,6 +171,44 @@ def assertObjectAttrs(self, obj, **kwargs):\n         for attr, value in kwargs.items():\n             self.assertEqual(getattr(obj, attr), value)\n \n+    def test_count_preserve_group_by(self):\n+        # new release of the same book\n+        Book.objects.create(\n+            isbn=\"113235613\",\n+            name=self.b4.name,\n+            pages=self.b4.pages,\n+            rating=4.0,\n+            price=Decimal(\"39.69\"),\n+            contact=self.a5,\n+            publisher=self.p3,\n+            pubdate=datetime.date(2018, 11, 3),\n+        )\n+        qs = Book.objects.values(\"contact__name\", \"publisher__name\").annotate(\n+            publications=Count(\"id\")\n+        )\n+        self.assertEqual(qs.order_by(\"id\").count(), len(qs.order_by(\"id\")))\n+        self.assertEqual(qs.extra(order_by=[\"id\"]).count(), len(qs.order_by(\"id\")))\n+\n+    def test_aggregate_preserve_group_by(self):\n+        # new release of the same book\n+        Book.objects.create(\n+            isbn=\"113235613\",\n+            name=self.b4.name,\n+            pages=self.b4.pages,\n+            rating=4.0,\n+            price=Decimal(\"39.69\"),\n+            contact=self.a5,\n+            publisher=self.p3,\n+            pubdate=datetime.date(2018, 11, 3),\n+        )\n+        qs = Book.objects.values(\"contact__name\", \"publisher__name\").annotate(\n+            publications=Count(\"id\")\n+        )\n+        self.assertEqual(",
      "comment": "I was about to change this to assertAlmostEqual just out of paranoia about the decimal value, when I realized I think the other test is sufficient. I think Simon suggested something along those lines. I'll remove it.",
      "comment_id": 2615551547,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-12T20:52:17Z",
      "url": "https://github.com/django/django/pull/20374#discussion_r2615551547"
    },
    {
      "repo": "django/django",
      "pr_number": 20374,
      "file_path": "django/db/models/sql/query.py",
      "line": 2356,
      "side": "RIGHT",
      "diff_hunk": "@@ -2338,6 +2339,32 @@ def add_ordering(self, *ordering):\n         else:\n             self.default_ordering = False\n \n+    @property\n+    def orderby_issubset_groupby(self):\n+        if self.extra_order_by:\n+            # Raw SQL from extra(order_by=...) can't be reliably compared\n+            # against resolved OrderBy/Col expressions. Treat as not a subset.\n+            return False\n+        if self.group_by in (None, True):\n+            # There is either no aggregation at all (None), or the group by\n+            # is generated automatically from model fields (True), in which\n+            # case the order by is necessarily a subset of them.\n+            return True\n+        if not (self.order_by or self.group_by):\n+            # An empty set is always a subset of something.\n+            return True",
      "comment": "Just noticed from the coverage report that this line is not covered. When would `self.group_by` be an empty iterable?",
      "comment_id": 2615898688,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-13T00:26:12Z",
      "url": "https://github.com/django/django/pull/20374#discussion_r2615898688"
    },
    {
      "repo": "django/django",
      "pr_number": 20374,
      "file_path": "django/db/models/sql/query.py",
      "line": 2356,
      "side": "RIGHT",
      "diff_hunk": "@@ -2338,6 +2339,32 @@ def add_ordering(self, *ordering):\n         else:\n             self.default_ordering = False\n \n+    @property\n+    def orderby_issubset_groupby(self):\n+        if self.extra_order_by:\n+            # Raw SQL from extra(order_by=...) can't be reliably compared\n+            # against resolved OrderBy/Col expressions. Treat as not a subset.\n+            return False\n+        if self.group_by in (None, True):\n+            # There is either no aggregation at all (None), or the group by\n+            # is generated automatically from model fields (True), in which\n+            # case the order by is necessarily a subset of them.\n+            return True\n+        if not (self.order_by or self.group_by):\n+            # An empty set is always a subset of something.\n+            return True",
      "comment": "With the current call site never as we only get in this branch `if isinstance(self.group_by, tuple)` which means the queryset was previously annotated with an aggregate expression and that there was at least one value passed to `values` to specify the group by prior to that.\r\n\r\nIt might be handy if `orderby_issubset_groupby` is eventually called from other context but in with this single call site it's not.",
      "comment_id": 2615969005,
      "user": "charettes",
      "created_at": "2025-12-13T01:33:34Z",
      "url": "https://github.com/django/django/pull/20374#discussion_r2615969005"
    },
    {
      "repo": "django/django",
      "pr_number": 20374,
      "file_path": "django/db/models/sql/query.py",
      "line": 2356,
      "side": "RIGHT",
      "diff_hunk": "@@ -2338,6 +2339,32 @@ def add_ordering(self, *ordering):\n         else:\n             self.default_ordering = False\n \n+    @property\n+    def orderby_issubset_groupby(self):\n+        if self.extra_order_by:\n+            # Raw SQL from extra(order_by=...) can't be reliably compared\n+            # against resolved OrderBy/Col expressions. Treat as not a subset.\n+            return False\n+        if self.group_by in (None, True):\n+            # There is either no aggregation at all (None), or the group by\n+            # is generated automatically from model fields (True), in which\n+            # case the order by is necessarily a subset of them.\n+            return True\n+        if not (self.order_by or self.group_by):\n+            # An empty set is always a subset of something.\n+            return True",
      "comment": "Shouldn't we also short-circuit if `self.order_by` is empty and `self.group_by` is non-empty?\n\nAnd if `self.order_by` is non-empty but `self.group_by` is empty, we could also short-circuit but return `False`.",
      "comment_id": 2616006109,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-13T02:42:03Z",
      "url": "https://github.com/django/django/pull/20374#discussion_r2616006109"
    },
    {
      "repo": "django/django",
      "pr_number": 20374,
      "file_path": "django/db/models/sql/query.py",
      "line": 2356,
      "side": "RIGHT",
      "diff_hunk": "@@ -2338,6 +2339,32 @@ def add_ordering(self, *ordering):\n         else:\n             self.default_ordering = False\n \n+    @property\n+    def orderby_issubset_groupby(self):\n+        if self.extra_order_by:\n+            # Raw SQL from extra(order_by=...) can't be reliably compared\n+            # against resolved OrderBy/Col expressions. Treat as not a subset.\n+            return False\n+        if self.group_by in (None, True):\n+            # There is either no aggregation at all (None), or the group by\n+            # is generated automatically from model fields (True), in which\n+            # case the order by is necessarily a subset of them.\n+            return True\n+        if not (self.order_by or self.group_by):\n+            # An empty set is always a subset of something.\n+            return True",
      "comment": "> Shouldn't we also short-circuit if self.order_by is empty and self.group_by is non-empty?\r\n\r\nWe could yes to be coherent with the method name but if there is no ordering there is little value in clearing it.\r\n\r\n> And if self.order_by is non-empty but self.group_by is empty, we could also short-circuit but return False.\r\n\r\nSame story IMO, we have to choose a side.\r\n\r\nDo we want to make the function caller aware (make choices knowing it will only be called in the context of `get_aggregation`) or make it general purposed with all the optimization possible but harder to exercise through code coverage.",
      "comment_id": 2616021621,
      "user": "charettes",
      "created_at": "2025-12-13T03:10:40Z",
      "url": "https://github.com/django/django/pull/20374#discussion_r2616021621"
    },
    {
      "repo": "django/django",
      "pr_number": 20374,
      "file_path": "django/db/models/sql/query.py",
      "line": 2356,
      "side": "RIGHT",
      "diff_hunk": "@@ -2338,6 +2339,32 @@ def add_ordering(self, *ordering):\n         else:\n             self.default_ordering = False\n \n+    @property\n+    def orderby_issubset_groupby(self):\n+        if self.extra_order_by:\n+            # Raw SQL from extra(order_by=...) can't be reliably compared\n+            # against resolved OrderBy/Col expressions. Treat as not a subset.\n+            return False\n+        if self.group_by in (None, True):\n+            # There is either no aggregation at all (None), or the group by\n+            # is generated automatically from model fields (True), in which\n+            # case the order by is necessarily a subset of them.\n+            return True\n+        if not (self.order_by or self.group_by):\n+            # An empty set is always a subset of something.\n+            return True",
      "comment": "Thanks, I'm just not understanding why `group_by` is in this condition; your earlier suggestion had `extra_order_by` there instead.\r\n\r\n> We could yes to be coherent with the method name but if there is no ordering there is little value in clearing it.\r\n\r\nIn that case, I think we should either change this to\r\n```py\r\nif not order_by:\r\n    # Empty sets are always subsets, but there's no point in clearing an ordering.\r\n    return False\r\n```\r\nOr just remove it altogether.\r\n\r\nIf we do that suggested tweak, then we're not doing all possible optimizations but we are doing the most likely one.",
      "comment_id": 2616350203,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-13T14:54:03Z",
      "url": "https://github.com/django/django/pull/20374#discussion_r2616350203"
    },
    {
      "repo": "django/django",
      "pr_number": 20374,
      "file_path": "django/db/models/sql/query.py",
      "line": 2356,
      "side": "RIGHT",
      "diff_hunk": "@@ -2338,6 +2339,32 @@ def add_ordering(self, *ordering):\n         else:\n             self.default_ordering = False\n \n+    @property\n+    def orderby_issubset_groupby(self):\n+        if self.extra_order_by:\n+            # Raw SQL from extra(order_by=...) can't be reliably compared\n+            # against resolved OrderBy/Col expressions. Treat as not a subset.\n+            return False\n+        if self.group_by in (None, True):\n+            # There is either no aggregation at all (None), or the group by\n+            # is generated automatically from model fields (True), in which\n+            # case the order by is necessarily a subset of them.\n+            return True\n+        if not (self.order_by or self.group_by):\n+            # An empty set is always a subset of something.\n+            return True",
      "comment": "I'm good with switching it to solely checking `not order_by` \ud83d\udc4d ",
      "comment_id": 2616364210,
      "user": "charettes",
      "created_at": "2025-12-13T15:22:56Z",
      "url": "https://github.com/django/django/pull/20374#discussion_r2616364210"
    },
    {
      "repo": "django/django",
      "pr_number": 20362,
      "file_path": "django/db/backends/base/schema.py",
      "line": 918,
      "side": "RIGHT",
      "diff_hunk": "@@ -906,6 +910,15 @@ def alter_field(self, model, old_field, new_field, strict=False):\n             else:\n                 new_field_sql = new_field.generated_sql(self.connection)\n                 modifying_generated_field = old_field_sql != new_field_sql\n+                db_features = self.connection.features\n+                # Some databases (e.g. Oracle) don't allow altering a data type\n+                # for generated columns.\n+                if (\n+                    not modifying_generated_field\n+                    and old_type != new_type",
      "comment": "I think we have to check for collation here also. Any other suffixes to check?",
      "comment_id": 2615501017,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-12T20:32:08Z",
      "url": "https://github.com/django/django/pull/20362#discussion_r2615501017"
    },
    {
      "repo": "django/django",
      "pr_number": 20362,
      "file_path": "django/db/backends/base/schema.py",
      "line": 918,
      "side": "RIGHT",
      "diff_hunk": "@@ -906,6 +910,15 @@ def alter_field(self, model, old_field, new_field, strict=False):\n             else:\n                 new_field_sql = new_field.generated_sql(self.connection)\n                 modifying_generated_field = old_field_sql != new_field_sql\n+                db_features = self.connection.features\n+                # Some databases (e.g. Oracle) don't allow altering a data type\n+                # for generated columns.\n+                if (\n+                    not modifying_generated_field\n+                    and old_type != new_type",
      "comment": "Actually, changing only `db_collation` works fine (I've checked on `VIRTUAL` generated field).",
      "comment_id": 2615614395,
      "user": "felixxm",
      "created_at": "2025-12-12T21:23:05Z",
      "url": "https://github.com/django/django/pull/20362#discussion_r2615614395"
    },
    {
      "repo": "django/django",
      "pr_number": 20362,
      "file_path": "django/db/backends/base/schema.py",
      "line": 918,
      "side": "RIGHT",
      "diff_hunk": "@@ -906,6 +910,15 @@ def alter_field(self, model, old_field, new_field, strict=False):\n             else:\n                 new_field_sql = new_field.generated_sql(self.connection)\n                 modifying_generated_field = old_field_sql != new_field_sql\n+                db_features = self.connection.features\n+                # Some databases (e.g. Oracle) don't allow altering a data type\n+                # for generated columns.\n+                if (\n+                    not modifying_generated_field\n+                    and old_type != new_type",
      "comment": "Oh good to know. Tried to test, but it wanted me to set `MAX_STRING_SIZE=EXTENDED` and that turned out to be impossibly complicated.",
      "comment_id": 2615770931,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-12T22:44:25Z",
      "url": "https://github.com/django/django/pull/20362#discussion_r2615770931"
    },
    {
      "repo": "django/django",
      "pr_number": 20362,
      "file_path": "django/db/backends/base/schema.py",
      "line": 918,
      "side": "RIGHT",
      "diff_hunk": "@@ -906,6 +910,15 @@ def alter_field(self, model, old_field, new_field, strict=False):\n             else:\n                 new_field_sql = new_field.generated_sql(self.connection)\n                 modifying_generated_field = old_field_sql != new_field_sql\n+                db_features = self.connection.features\n+                # Some databases (e.g. Oracle) don't allow altering a data type\n+                # for generated columns.\n+                if (\n+                    not modifying_generated_field\n+                    and old_type != new_type",
      "comment": "Yes it is complicated :disappointed: \r\n\r\nTBH, I don't see us adding more and more conditions here. I would treat this check as \"the best effort\", there are too many database-specific caveats around altering `GeneratedField`. IMO, `DatabaseError` is fine here when someone hits the edge.",
      "comment_id": 2616181976,
      "user": "felixxm",
      "created_at": "2025-12-13T08:51:26Z",
      "url": "https://github.com/django/django/pull/20362#discussion_r2616181976"
    },
    {
      "repo": "django/django",
      "pr_number": 20282,
      "file_path": "django/db/models/functions/uuid.py",
      "line": 40,
      "side": "LEFT",
      "diff_hunk": "@@ -37,13 +37,27 @@ def as_oracle(self, compiler, connection, **extra_context):\n \n class UUID7(Func):\n     function = \"UUIDV7\"\n-    arity = 0",
      "comment": "I don't think setting this to `arity = 1` (as suggested in https://github.com/django/django/pull/20101#discussion_r2608226079) makes sense, so I've just removed it again. The problem is that setting `arity = 1` means that `shift` becomes required, rather than optional. I'm open to suggestions if there's something I've missed here.",
      "comment_id": 2608390592,
      "user": "LilyFirefly",
      "created_at": "2025-12-10T22:09:22Z",
      "url": "https://github.com/django/django/pull/20282#discussion_r2608390592"
    },
    {
      "repo": "django/django",
      "pr_number": 20282,
      "file_path": "django/db/models/functions/uuid.py",
      "line": 40,
      "side": "LEFT",
      "diff_hunk": "@@ -37,13 +37,27 @@ def as_oracle(self, compiler, connection, **extra_context):\n \n class UUID7(Func):\n     function = \"UUIDV7\"\n-    arity = 0",
      "comment": "Makes sense, I forgot we wouldn't have access to the connection's feature flags to do the switch inside `__init__()`. EDIT: I'll propose something below.",
      "comment_id": 2612172223,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-11T21:47:12Z",
      "url": "https://github.com/django/django/pull/20282#discussion_r2612172223"
    },
    {
      "repo": "django/django",
      "pr_number": 20282,
      "file_path": "django/db/models/functions/uuid.py",
      "line": 40,
      "side": "LEFT",
      "diff_hunk": "@@ -37,13 +37,27 @@ def as_oracle(self, compiler, connection, **extra_context):\n \n class UUID7(Func):\n     function = \"UUIDV7\"\n-    arity = 0",
      "comment": "I think we can leverage the order of operations in the `Func` constructor to avoid reimplementing args checking from scratch. This works to get us args checking:\n\n```diff\ndiff --git a/django/db/models/functions/uuid.py b/django/db/models/functions/uuid.py\nindex c41f58d11b..7059798ff3 100644\n--- a/django/db/models/functions/uuid.py\n+++ b/django/db/models/functions/uuid.py\n@@ -37,13 +37,16 @@ class UUID4(Func):\n \n class UUID7(Func):\n     function = \"UUIDV7\"\n+    arity = 1\n     output_field = UUIDField()\n \n     def __init__(self, shift=None, **extra):\n-        if shift is None:\n-            super().__init__(**extra)\n-        else:\n-            super().__init__(shift, **extra)\n+        super().__init__(shift, **extra)\n+\n+    def _parse_expressions(self, *expressions):\n+        if expressions[0] is None:\n+            expressions = expressions[1:]\n+        return super()._parse_expressions(*expressions)\n \n     def as_sql(self, compiler, connection, **extra_context):\n         if not connection.features.supports_uuid7_function:\n```",
      "comment_id": 2612209395,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-11T22:02:18Z",
      "url": "https://github.com/django/django/pull/20282#discussion_r2612209395"
    },
    {
      "repo": "django/django",
      "pr_number": 20282,
      "file_path": "tests/db_functions/models.py",
      "line": 64,
      "side": "RIGHT",
      "diff_hunk": "@@ -59,3 +59,8 @@ class FloatModel(models.Model):\n \n class UUIDModel(models.Model):\n     uuid = models.UUIDField(null=True)\n+\n+\n+class UUIDDurationModel(models.Model):",
      "comment": "I'd probably just add `shift` to the existing model, unless you think that makes the test harder to grok?",
      "comment_id": 2612217387,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-11T22:04:12Z",
      "url": "https://github.com/django/django/pull/20282#discussion_r2612217387"
    },
    {
      "repo": "django/django",
      "pr_number": 20282,
      "file_path": "tests/db_functions/test_uuid.py",
      "line": 43,
      "side": "RIGHT",
      "diff_hunk": "@@ -31,6 +32,29 @@ def test_uuid7(self):\n         self.assertEqual(m1.uuid.version, 7)\n         self.assertNotEqual(m1.uuid, m2.uuid)\n \n+    @skipUnlessDBFeature(\"supports_uuid7_function_shift\")\n+    def test_uuid7_shift(self):\n+        now = datetime.now(timezone.utc)\n+        past = datetime(2005, 11, 16, tzinfo=timezone.utc)\n+        shift = past - now\n+        m = UUIDModel.objects.create(uuid=UUID7(shift))\n+        m.refresh_from_db()\n+        self.assertIsInstance(m.uuid, uuid.UUID)\n+        self.assertEqual(m.uuid.version, 7)",
      "comment": "I'd chop, we have other save & reload tests.\n```suggestion\n```",
      "comment_id": 2612234718,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-11T22:09:06Z",
      "url": "https://github.com/django/django/pull/20282#discussion_r2612234718"
    },
    {
      "repo": "django/django",
      "pr_number": 20282,
      "file_path": "tests/db_functions/test_uuid.py",
      "line": 41,
      "side": "RIGHT",
      "diff_hunk": "@@ -31,6 +32,29 @@ def test_uuid7(self):\n         self.assertEqual(m1.uuid.version, 7)\n         self.assertNotEqual(m1.uuid, m2.uuid)\n \n+    @skipUnlessDBFeature(\"supports_uuid7_function_shift\")\n+    def test_uuid7_shift(self):\n+        now = datetime.now(timezone.utc)\n+        past = datetime(2005, 11, 16, tzinfo=timezone.utc)\n+        shift = past - now\n+        m = UUIDModel.objects.create(uuid=UUID7(shift))\n+        m.refresh_from_db()\n+        self.assertIsInstance(m.uuid, uuid.UUID)\n+        self.assertEqual(m.uuid.version, 7)\n+        self.assertTrue(str(m.uuid).startswith(\"0107965e-e400\"))",
      "comment": "I wonder if this is a timezone difference making the calculation a bit off somewhere.\r\n\r\nIt would be useful to see what the uuid actually starts with - if it's only different in the last hexdigit, I think it may be just a precision/timing difference.",
      "comment_id": 2612291190,
      "user": "LilyFirefly",
      "created_at": "2025-12-11T22:36:41Z",
      "url": "https://github.com/django/django/pull/20282#discussion_r2612291190"
    },
    {
      "repo": "django/django",
      "pr_number": 20282,
      "file_path": "tests/db_functions/test_uuid.py",
      "line": 41,
      "side": "RIGHT",
      "diff_hunk": "@@ -31,6 +32,29 @@ def test_uuid7(self):\n         self.assertEqual(m1.uuid.version, 7)\n         self.assertNotEqual(m1.uuid, m2.uuid)\n \n+    @skipUnlessDBFeature(\"supports_uuid7_function_shift\")\n+    def test_uuid7_shift(self):\n+        now = datetime.now(timezone.utc)\n+        past = datetime(2005, 11, 16, tzinfo=timezone.utc)\n+        shift = past - now\n+        m = UUIDModel.objects.create(uuid=UUID7(shift))\n+        m.refresh_from_db()\n+        self.assertIsInstance(m.uuid, uuid.UUID)\n+        self.assertEqual(m.uuid.version, 7)\n+        self.assertTrue(str(m.uuid).startswith(\"0107965e-e400\"))",
      "comment": "Sure thing. Maybe worth tweaking the assertion so it's always there.\r\n\r\n```py\r\nAssertionError: False is not true : 0107965e-e40a-7380-b548-5e3709b94f41\r\nAssertionError: False is not true : 0107965e-e406-72a3-9cf2-ac4f75411952\r\n```\r\n",
      "comment_id": 2612315206,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-11T22:49:43Z",
      "url": "https://github.com/django/django/pull/20282#discussion_r2612315206"
    },
    {
      "repo": "django/django",
      "pr_number": 20282,
      "file_path": "tests/db_functions/test_uuid.py",
      "line": 41,
      "side": "RIGHT",
      "diff_hunk": "@@ -31,6 +32,29 @@ def test_uuid7(self):\n         self.assertEqual(m1.uuid.version, 7)\n         self.assertNotEqual(m1.uuid, m2.uuid)\n \n+    @skipUnlessDBFeature(\"supports_uuid7_function_shift\")\n+    def test_uuid7_shift(self):\n+        now = datetime.now(timezone.utc)\n+        past = datetime(2005, 11, 16, tzinfo=timezone.utc)\n+        shift = past - now\n+        m = UUIDModel.objects.create(uuid=UUID7(shift))\n+        m.refresh_from_db()\n+        self.assertIsInstance(m.uuid, uuid.UUID)\n+        self.assertEqual(m.uuid.version, 7)\n+        self.assertTrue(str(m.uuid).startswith(\"0107965e-e400\"))",
      "comment": "Yeah, looks like it's just a precision issue in the last hexdigit of the timestamp part.",
      "comment_id": 2612316993,
      "user": "LilyFirefly",
      "created_at": "2025-12-11T22:50:45Z",
      "url": "https://github.com/django/django/pull/20282#discussion_r2612316993"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "tests/db_functions/test_uuid.py",
      "line": 20,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,64 @@\n+import uuid\n+\n+from django.db import NotSupportedError, connection\n+from django.db.models.functions import UUID4, UUID7\n+from django.test import TestCase\n+from django.test.testcases import skipIfDBFeature, skipUnlessDBFeature\n+\n+from .models import UUIDModel\n+\n+\n+class TestUUID(TestCase):\n+    @skipUnlessDBFeature(\"supports_uuid4_function\")\n+    def test_uuid4(self):\n+        m1 = UUIDModel.objects.create()\n+        m2 = UUIDModel.objects.create()\n+        UUIDModel.objects.update(uuid=UUID4())\n+        m1.refresh_from_db()\n+        m2.refresh_from_db()\n+        self.assertIsInstance(m1.uuid, uuid.UUID)\n+        self.assertEqual(m1.uuid.version, 4)",
      "comment": "We could do, but I don't think there's a big benefit either way.",
      "comment_id": 2542938281,
      "user": "LilyFirefly",
      "created_at": "2025-11-19T17:23:46Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2542938281"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/db/backends/sqlite3/_functions.py",
      "line": 8,
      "side": "RIGHT",
      "diff_hunk": "@@ -5,6 +5,7 @@\n import functools\n import random\n import statistics\n+import uuid",
      "comment": "Do `from uuid import uuid4, uuid7` to avoid the attribute lookup in the functions - micro optimisation but since these functions can be called many times in a query I think it makes sense. uuid7 will need guarding with PY314.",
      "comment_id": 2543474426,
      "user": "adamchainz",
      "created_at": "2025-11-19T20:31:40Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2543474426"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/contrib/postgres/functions.py",
      "line": 15,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,9 +1,17 @@\n-from django.db.models import DateTimeField, Func, UUIDField\n+import warnings\n \n+from django.db.models import DateTimeField, Func\n+from django.db.models.functions import UUID4\n+from django.utils.deprecation import RemovedInDjango70Warning\n \n-class RandomUUID(Func):\n-    template = \"GEN_RANDOM_UUID()\"\n-    output_field = UUIDField()\n+\n+class RandomUUID(UUID4):\n+    def __init__(self, *args, **kwargs):\n+        warnings.warn(\n+            \"Use django.db.models.functions.UUID4 instead.\",\n+            RemovedInDjango70Warning,\n+        )\n+        super().__init__(*args, **kwargs)",
      "comment": "Since `RandomUUID()` may have been used in migrations as a `db_default`, doesn't it have to remain for use in historical migrations?",
      "comment_id": 2546238662,
      "user": "cliffordgama",
      "created_at": "2025-11-20T14:17:03Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2546238662"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/contrib/postgres/functions.py",
      "line": 15,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,9 +1,17 @@\n-from django.db.models import DateTimeField, Func, UUIDField\n+import warnings\n \n+from django.db.models import DateTimeField, Func\n+from django.db.models.functions import UUID4\n+from django.utils.deprecation import RemovedInDjango70Warning\n \n-class RandomUUID(Func):\n-    template = \"GEN_RANDOM_UUID()\"\n-    output_field = UUIDField()\n+\n+class RandomUUID(UUID4):\n+    def __init__(self, *args, **kwargs):\n+        warnings.warn(\n+            \"Use django.db.models.functions.UUID4 instead.\",\n+            RemovedInDjango70Warning,\n+        )\n+        super().__init__(*args, **kwargs)",
      "comment": "Regardless of the above, I'd also add \"RandomUUID is deprecated\" to the deprecation message, and a `skip_file_prefixes=django_file_prefixes()` to the `warn`, so a user is pointed to where they need to change.\r\n",
      "comment_id": 2546262213,
      "user": "cliffordgama",
      "created_at": "2025-11-20T14:23:50Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2546262213"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/contrib/postgres/functions.py",
      "line": 15,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,9 +1,17 @@\n-from django.db.models import DateTimeField, Func, UUIDField\n+import warnings\n \n+from django.db.models import DateTimeField, Func\n+from django.db.models.functions import UUID4\n+from django.utils.deprecation import RemovedInDjango70Warning\n \n-class RandomUUID(Func):\n-    template = \"GEN_RANDOM_UUID()\"\n-    output_field = UUIDField()\n+\n+class RandomUUID(UUID4):\n+    def __init__(self, *args, **kwargs):\n+        warnings.warn(\n+            \"Use django.db.models.functions.UUID4 instead.\",\n+            RemovedInDjango70Warning,\n+        )\n+        super().__init__(*args, **kwargs)",
      "comment": "Yeah, I'm not entirely happy with this approach.\r\n\r\nFor `Field` subclasses, we can move the deprecation to the checks framework with `system_check_deprecated_details` (and later `system_check_removed_details`), but these aren't set up for expressions and I think it would be out of scope for this PR to add something like that.",
      "comment_id": 2547432896,
      "user": "LilyFirefly",
      "created_at": "2025-11-20T19:35:34Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2547432896"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/contrib/postgres/functions.py",
      "line": 15,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,9 +1,17 @@\n-from django.db.models import DateTimeField, Func, UUIDField\n+import warnings\n \n+from django.db.models import DateTimeField, Func\n+from django.db.models.functions import UUID4\n+from django.utils.deprecation import RemovedInDjango70Warning\n \n-class RandomUUID(Func):\n-    template = \"GEN_RANDOM_UUID()\"\n-    output_field = UUIDField()\n+\n+class RandomUUID(UUID4):\n+    def __init__(self, *args, **kwargs):\n+        warnings.warn(\n+            \"Use django.db.models.functions.UUID4 instead.\",\n+            RemovedInDjango70Warning,\n+        )\n+        super().__init__(*args, **kwargs)",
      "comment": "If this is merged, django-upgrade can gain a fixer for `RandomUUID` -> `UUID4`, which would affect migrations too. That would help users of django-upgrade, at least.",
      "comment_id": 2554405884,
      "user": "adamchainz",
      "created_at": "2025-11-23T23:22:45Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2554405884"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/contrib/postgres/functions.py",
      "line": 15,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,9 +1,17 @@\n-from django.db.models import DateTimeField, Func, UUIDField\n+import warnings\n \n+from django.db.models import DateTimeField, Func\n+from django.db.models.functions import UUID4\n+from django.utils.deprecation import RemovedInDjango70Warning\n \n-class RandomUUID(Func):\n-    template = \"GEN_RANDOM_UUID()\"\n-    output_field = UUIDField()\n+\n+class RandomUUID(UUID4):\n+    def __init__(self, *args, **kwargs):\n+        warnings.warn(\n+            \"Use django.db.models.functions.UUID4 instead.\",\n+            RemovedInDjango70Warning,\n+        )\n+        super().__init__(*args, **kwargs)",
      "comment": "I like the  Adam's proposal.\nWhat others think?\nHow do you think we want to solve this RandomUUID question here?",
      "comment_id": 2572907198,
      "user": "pauloxnet",
      "created_at": "2025-11-29T09:00:32Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2572907198"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/db/backends/oracle/features.py",
      "line": 230,
      "side": "RIGHT",
      "diff_hunk": "@@ -224,3 +224,7 @@ def bare_select_suffix(self):\n     def supports_tuple_lookups(self):\n         # Support is known to be missing on 23.2 but available on 23.4.\n         return self.connection.oracle_version >= (23, 4)\n+\n+    @cached_property\n+    def supports_uuid4_function(self):\n+        return self.connection.oracle_version >= (23, 4)",
      "comment": "```suggestion\r\n        return self.connection.oracle_version >= (23, 9)\r\n```",
      "comment_id": 2573669121,
      "user": "felixxm",
      "created_at": "2025-11-30T13:52:17Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2573669121"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/db/models/functions/uuid.py",
      "line": 31,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,63 @@\n+from django.db import NotSupportedError\n+from django.db.models.expressions import Func\n+from django.db.models.fields import UUIDField\n+\n+\n+class UUID4(Func):\n+    function = \"UUIDV4\"\n+    output_field = UUIDField()\n+\n+    def as_sql(self, compiler, connection, **extra_context):\n+        if connection.features.supports_uuid4_function:\n+            return super().as_sql(compiler, connection, **extra_context)\n+        raise NotSupportedError(\"UUID4 is not supported on this database backend.\")\n+\n+    def as_postgresql(self, compiler, connection, **extra_context):\n+        if connection.features.is_postgresql_18:\n+            return self.as_sql(compiler, connection, **extra_context)\n+        return self.as_sql(\n+            compiler, connection, function=\"GEN_RANDOM_UUID\", **extra_context\n+        )\n+\n+    def as_mysql(self, compiler, connection, **extra_context):\n+        if not connection.features.supports_uuid4_function:\n+            if connection.mysql_is_mariadb:\n+                raise NotSupportedError(\"UUID4 requires MariaDB version 11.7 or later.\")\n+            raise NotSupportedError(\"UUID4 is not supported on MySQL.\")\n+        return self.as_sql(compiler, connection, function=\"UUID_V4\", **extra_context)\n+\n+    def as_oracle(self, compiler, connection, **extra_context):\n+        if not connection.features.supports_uuid4_function:\n+            raise NotSupportedError(\"UUID4 requires Oracle version 23ai or later.\")",
      "comment": "```suggestion\r\n            raise NotSupportedError(\"UUID4 requires Oracle version 26ai or later.\")\r\n```",
      "comment_id": 2606165338,
      "user": "pauloxnet",
      "created_at": "2025-12-10T10:51:20Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2606165338"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/db/models/functions/uuid.py",
      "line": 35,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,63 @@\n+from django.db import NotSupportedError\n+from django.db.models.expressions import Func\n+from django.db.models.fields import UUIDField\n+\n+\n+class UUID4(Func):\n+    function = \"UUIDV4\"\n+    output_field = UUIDField()\n+\n+    def as_sql(self, compiler, connection, **extra_context):\n+        if connection.features.supports_uuid4_function:\n+            return super().as_sql(compiler, connection, **extra_context)\n+        raise NotSupportedError(\"UUID4 is not supported on this database backend.\")\n+\n+    def as_postgresql(self, compiler, connection, **extra_context):\n+        if connection.features.is_postgresql_18:\n+            return self.as_sql(compiler, connection, **extra_context)\n+        return self.as_sql(\n+            compiler, connection, function=\"GEN_RANDOM_UUID\", **extra_context\n+        )\n+\n+    def as_mysql(self, compiler, connection, **extra_context):\n+        if not connection.features.supports_uuid4_function:\n+            if connection.mysql_is_mariadb:\n+                raise NotSupportedError(\"UUID4 requires MariaDB version 11.7 or later.\")\n+            raise NotSupportedError(\"UUID4 is not supported on MySQL.\")\n+        return self.as_sql(compiler, connection, function=\"UUID_V4\", **extra_context)\n+\n+    def as_oracle(self, compiler, connection, **extra_context):\n+        if not connection.features.supports_uuid4_function:\n+            raise NotSupportedError(\"UUID4 requires Oracle version 23ai or later.\")\n+        return self.as_sql(compiler, connection, function=\"UUID\", **extra_context)",
      "comment": "I know that based on the docs using `UUID` is OK, but what if we explicitly use the version number `UUID(4)`?\r\n\r\n> UUID can optionally take as input a version_specifier of NUMBER type. UUID(0) and UUID(4) are equivalent to UUID() in that in both cases a version 4 variant 1 UUID is returned.\r\n> Versions other than 4 and 0 return an error.\r\n\r\nSo, if in the future the `UUID` will change or be deprecated (like `GEN_RANDOM_UUID` in PG) we don't have to change nothing here.\r\n",
      "comment_id": 2606191096,
      "user": "pauloxnet",
      "created_at": "2025-12-10T10:58:48Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2606191096"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/db/models/functions/uuid.py",
      "line": 31,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,63 @@\n+from django.db import NotSupportedError\n+from django.db.models.expressions import Func\n+from django.db.models.fields import UUIDField\n+\n+\n+class UUID4(Func):\n+    function = \"UUIDV4\"\n+    output_field = UUIDField()\n+\n+    def as_sql(self, compiler, connection, **extra_context):\n+        if connection.features.supports_uuid4_function:\n+            return super().as_sql(compiler, connection, **extra_context)\n+        raise NotSupportedError(\"UUID4 is not supported on this database backend.\")\n+\n+    def as_postgresql(self, compiler, connection, **extra_context):\n+        if connection.features.is_postgresql_18:\n+            return self.as_sql(compiler, connection, **extra_context)\n+        return self.as_sql(\n+            compiler, connection, function=\"GEN_RANDOM_UUID\", **extra_context\n+        )\n+\n+    def as_mysql(self, compiler, connection, **extra_context):\n+        if not connection.features.supports_uuid4_function:\n+            if connection.mysql_is_mariadb:\n+                raise NotSupportedError(\"UUID4 requires MariaDB version 11.7 or later.\")\n+            raise NotSupportedError(\"UUID4 is not supported on MySQL.\")\n+        return self.as_sql(compiler, connection, function=\"UUID_V4\", **extra_context)\n+\n+    def as_oracle(self, compiler, connection, **extra_context):\n+        if not connection.features.supports_uuid4_function:\n+            raise NotSupportedError(\"UUID4 requires Oracle version 23ai or later.\")",
      "comment": "26ai is not a precise error message. `UUID` was added in `23.9` so between `23ai` (`23.4`) and 26ai (`23.26`).",
      "comment_id": 2606212734,
      "user": "felixxm",
      "created_at": "2025-12-10T11:05:54Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2606212734"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/db/models/functions/uuid.py",
      "line": 31,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,63 @@\n+from django.db import NotSupportedError\n+from django.db.models.expressions import Func\n+from django.db.models.fields import UUIDField\n+\n+\n+class UUID4(Func):\n+    function = \"UUIDV4\"\n+    output_field = UUIDField()\n+\n+    def as_sql(self, compiler, connection, **extra_context):\n+        if connection.features.supports_uuid4_function:\n+            return super().as_sql(compiler, connection, **extra_context)\n+        raise NotSupportedError(\"UUID4 is not supported on this database backend.\")\n+\n+    def as_postgresql(self, compiler, connection, **extra_context):\n+        if connection.features.is_postgresql_18:\n+            return self.as_sql(compiler, connection, **extra_context)\n+        return self.as_sql(\n+            compiler, connection, function=\"GEN_RANDOM_UUID\", **extra_context\n+        )\n+\n+    def as_mysql(self, compiler, connection, **extra_context):\n+        if not connection.features.supports_uuid4_function:\n+            if connection.mysql_is_mariadb:\n+                raise NotSupportedError(\"UUID4 requires MariaDB version 11.7 or later.\")\n+            raise NotSupportedError(\"UUID4 is not supported on MySQL.\")\n+        return self.as_sql(compiler, connection, function=\"UUID_V4\", **extra_context)\n+\n+    def as_oracle(self, compiler, connection, **extra_context):\n+        if not connection.features.supports_uuid4_function:\n+            raise NotSupportedError(\"UUID4 requires Oracle version 23ai or later.\")",
      "comment": "I was trying to update the error message based on this comment of yours:\r\nhttps://github.com/django/django/pull/20101#issuecomment-3592316394\r\n\r\n> It seems that UUID is not available in Oracle 23ai, maybe only in 26ai (23.9)\r\n\r\nfrom which I understand that the sentence:\r\n\r\n> \"UUID4 requires Oracle version 23ai or later.\"\r\n\r\nis not correct.\r\n\r\nI don't know exactly how the Oracle version number works, so please update it :-)",
      "comment_id": 2606226940,
      "user": "pauloxnet",
      "created_at": "2025-12-10T11:10:34Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2606226940"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/db/models/functions/uuid.py",
      "line": 31,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,63 @@\n+from django.db import NotSupportedError\n+from django.db.models.expressions import Func\n+from django.db.models.fields import UUIDField\n+\n+\n+class UUID4(Func):\n+    function = \"UUIDV4\"\n+    output_field = UUIDField()\n+\n+    def as_sql(self, compiler, connection, **extra_context):\n+        if connection.features.supports_uuid4_function:\n+            return super().as_sql(compiler, connection, **extra_context)\n+        raise NotSupportedError(\"UUID4 is not supported on this database backend.\")\n+\n+    def as_postgresql(self, compiler, connection, **extra_context):\n+        if connection.features.is_postgresql_18:\n+            return self.as_sql(compiler, connection, **extra_context)\n+        return self.as_sql(\n+            compiler, connection, function=\"GEN_RANDOM_UUID\", **extra_context\n+        )\n+\n+    def as_mysql(self, compiler, connection, **extra_context):\n+        if not connection.features.supports_uuid4_function:\n+            if connection.mysql_is_mariadb:\n+                raise NotSupportedError(\"UUID4 requires MariaDB version 11.7 or later.\")\n+            raise NotSupportedError(\"UUID4 is not supported on MySQL.\")\n+        return self.as_sql(compiler, connection, function=\"UUID_V4\", **extra_context)\n+\n+    def as_oracle(self, compiler, connection, **extra_context):\n+        if not connection.features.supports_uuid4_function:\n+            raise NotSupportedError(\"UUID4 requires Oracle version 23ai or later.\")",
      "comment": "> ... `UUID` was added in `23.9` ...\r\n\r\nMaybe this error message is more precise?\r\n\r\n> \"UUID4 requires Oracle version 23.9 or later.\"",
      "comment_id": 2606236361,
      "user": "pauloxnet",
      "created_at": "2025-12-10T11:13:35Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2606236361"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/db/models/functions/uuid.py",
      "line": 31,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,63 @@\n+from django.db import NotSupportedError\n+from django.db.models.expressions import Func\n+from django.db.models.fields import UUIDField\n+\n+\n+class UUID4(Func):\n+    function = \"UUIDV4\"\n+    output_field = UUIDField()\n+\n+    def as_sql(self, compiler, connection, **extra_context):\n+        if connection.features.supports_uuid4_function:\n+            return super().as_sql(compiler, connection, **extra_context)\n+        raise NotSupportedError(\"UUID4 is not supported on this database backend.\")\n+\n+    def as_postgresql(self, compiler, connection, **extra_context):\n+        if connection.features.is_postgresql_18:\n+            return self.as_sql(compiler, connection, **extra_context)\n+        return self.as_sql(\n+            compiler, connection, function=\"GEN_RANDOM_UUID\", **extra_context\n+        )\n+\n+    def as_mysql(self, compiler, connection, **extra_context):\n+        if not connection.features.supports_uuid4_function:\n+            if connection.mysql_is_mariadb:\n+                raise NotSupportedError(\"UUID4 requires MariaDB version 11.7 or later.\")\n+            raise NotSupportedError(\"UUID4 is not supported on MySQL.\")\n+        return self.as_sql(compiler, connection, function=\"UUID_V4\", **extra_context)\n+\n+    def as_oracle(self, compiler, connection, **extra_context):\n+        if not connection.features.supports_uuid4_function:\n+            raise NotSupportedError(\"UUID4 requires Oracle version 23ai or later.\")",
      "comment": "> I don't know exactly how the Oracle version number works, so please update it :-)\r\n\r\nno one knows :upside_down_face: ",
      "comment_id": 2606259798,
      "user": "felixxm",
      "created_at": "2025-12-10T11:21:27Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2606259798"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/db/models/functions/uuid.py",
      "line": 31,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,63 @@\n+from django.db import NotSupportedError\n+from django.db.models.expressions import Func\n+from django.db.models.fields import UUIDField\n+\n+\n+class UUID4(Func):\n+    function = \"UUIDV4\"\n+    output_field = UUIDField()\n+\n+    def as_sql(self, compiler, connection, **extra_context):\n+        if connection.features.supports_uuid4_function:\n+            return super().as_sql(compiler, connection, **extra_context)\n+        raise NotSupportedError(\"UUID4 is not supported on this database backend.\")\n+\n+    def as_postgresql(self, compiler, connection, **extra_context):\n+        if connection.features.is_postgresql_18:\n+            return self.as_sql(compiler, connection, **extra_context)\n+        return self.as_sql(\n+            compiler, connection, function=\"GEN_RANDOM_UUID\", **extra_context\n+        )\n+\n+    def as_mysql(self, compiler, connection, **extra_context):\n+        if not connection.features.supports_uuid4_function:\n+            if connection.mysql_is_mariadb:\n+                raise NotSupportedError(\"UUID4 requires MariaDB version 11.7 or later.\")\n+            raise NotSupportedError(\"UUID4 is not supported on MySQL.\")\n+        return self.as_sql(compiler, connection, function=\"UUID_V4\", **extra_context)\n+\n+    def as_oracle(self, compiler, connection, **extra_context):\n+        if not connection.features.supports_uuid4_function:\n+            raise NotSupportedError(\"UUID4 requires Oracle version 23ai or later.\")",
      "comment": "I'd use the same wording that I proposed in #20362, so `23ai/26ai (23.9)`, e.g.\r\n\r\n_\"UUID4 requires Oracle version 23ai/26ai (23.9) or later.\"_",
      "comment_id": 2606549024,
      "user": "felixxm",
      "created_at": "2025-12-10T12:51:57Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2606549024"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "tests/postgres_tests/test_functions.py",
      "line": 32,
      "side": "LEFT",
      "diff_hunk": "@@ -29,11 +29,7 @@ def test_transaction_now(self):\n \n \n class TestRandomUUID(PostgreSQLTestCase):\n-    def test_random_uuid(self):",
      "comment": "Let's keep the new test for the deprecation warning but also keep the old test since it's still supported for now. All you should have to do is `warnings.catch_warnings(action=\"ignore\", category=...)`",
      "comment_id": 2607840889,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-10T18:49:52Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2607840889"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/db/models/functions/uuid.py",
      "line": 7,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,63 @@\n+from django.db import NotSupportedError\n+from django.db.models.expressions import Func\n+from django.db.models.fields import UUIDField\n+\n+\n+class UUID4(Func):\n+    function = \"UUIDV4\"",
      "comment": "We should take the opportunity to add `arity` on new built-in functions:\n```suggestion\n    function = \"UUIDV4\"\n    arity = 0\n```",
      "comment_id": 2607897034,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-10T19:09:33Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2607897034"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/db/models/functions/uuid.py",
      "line": 41,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,63 @@\n+from django.db import NotSupportedError\n+from django.db.models.expressions import Func\n+from django.db.models.fields import UUIDField\n+\n+\n+class UUID4(Func):\n+    function = \"UUIDV4\"\n+    output_field = UUIDField()\n+\n+    def as_sql(self, compiler, connection, **extra_context):\n+        if connection.features.supports_uuid4_function:\n+            return super().as_sql(compiler, connection, **extra_context)\n+        raise NotSupportedError(\"UUID4 is not supported on this database backend.\")\n+\n+    def as_postgresql(self, compiler, connection, **extra_context):\n+        if connection.features.is_postgresql_18:\n+            return self.as_sql(compiler, connection, **extra_context)\n+        return self.as_sql(\n+            compiler, connection, function=\"GEN_RANDOM_UUID\", **extra_context\n+        )\n+\n+    def as_mysql(self, compiler, connection, **extra_context):\n+        if not connection.features.supports_uuid4_function:\n+            if connection.mysql_is_mariadb:\n+                raise NotSupportedError(\"UUID4 requires MariaDB version 11.7 or later.\")\n+            raise NotSupportedError(\"UUID4 is not supported on MySQL.\")\n+        return self.as_sql(compiler, connection, function=\"UUID_V4\", **extra_context)\n+\n+    def as_oracle(self, compiler, connection, **extra_context):\n+        if not connection.features.supports_uuid4_function:\n+            raise NotSupportedError(\"UUID4 requires Oracle version 23ai or later.\")\n+        return self.as_sql(compiler, connection, function=\"UUID\", **extra_context)\n+\n+\n+class UUID7(Func):\n+    function = \"UUIDV7\"\n+    output_field = UUIDField()",
      "comment": "We should take a decision about whether to add `arity = 0` here also, even though postgres has an optional `shift` argument, or leave it without. I don't think the shift is part of our cross-db abstraction here, so I'm okay with `arity = 0` and forcing people to subclass `Func` themselves if they want a shift. WDYT?",
      "comment_id": 2607897466,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-10T19:09:42Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2607897466"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/contrib/postgres/functions.py",
      "line": 15,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,9 +1,17 @@\n-from django.db.models import DateTimeField, Func, UUIDField\n+import warnings\n \n+from django.db.models import DateTimeField, Func\n+from django.db.models.functions import UUID4\n+from django.utils.deprecation import RemovedInDjango70Warning\n \n-class RandomUUID(Func):\n-    template = \"GEN_RANDOM_UUID()\"\n-    output_field = UUIDField()\n+\n+class RandomUUID(UUID4):\n+    def __init__(self, *args, **kwargs):\n+        warnings.warn(\n+            \"Use django.db.models.functions.UUID4 instead.\",\n+            RemovedInDjango70Warning,\n+        )\n+        super().__init__(*args, **kwargs)",
      "comment": "We definitely need to keep the class forever. Whether to deprecate it is, I think, bound up with Lily's point about `system_check_deprecated_details` gaining support for inspecting `db_default`. If we don't want to do that in this PR, then we should remove the deprecation.",
      "comment_id": 2607952140,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-10T19:28:20Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2607952140"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/contrib/postgres/functions.py",
      "line": 13,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,9 +1,18 @@\n-from django.db.models import DateTimeField, Func, UUIDField\n+import warnings\n \n+from django.db.models import DateTimeField, Func\n+from django.db.models.functions import UUID4\n+from django.utils.deprecation import RemovedInDjango70Warning, django_file_prefixes\n \n-class RandomUUID(Func):\n-    template = \"GEN_RANDOM_UUID()\"\n-    output_field = UUIDField()\n+\n+class RandomUUID(UUID4):\n+    def __init__(self, *args, **kwargs):\n+        warnings.warn(\n+            \"RandomUUID is deprecated. Use django.db.models.functions.UUID4 instead.\",\n+            RemovedInDjango70Warning,\n+            skip_file_prefixes=django_file_prefixes(),",
      "comment": "This will be pretty noisy for existing migrations. Did you look into whether you could `catch_warnings` anywhere inside the migration loader?",
      "comment_id": 2607960300,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-10T19:31:17Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2607960300"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/db/models/functions/uuid.py",
      "line": 54,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,63 @@\n+from django.db import NotSupportedError\n+from django.db.models.expressions import Func\n+from django.db.models.fields import UUIDField\n+\n+\n+class UUID4(Func):\n+    function = \"UUIDV4\"\n+    output_field = UUIDField()\n+\n+    def as_sql(self, compiler, connection, **extra_context):\n+        if connection.features.supports_uuid4_function:\n+            return super().as_sql(compiler, connection, **extra_context)\n+        raise NotSupportedError(\"UUID4 is not supported on this database backend.\")\n+\n+    def as_postgresql(self, compiler, connection, **extra_context):\n+        if connection.features.is_postgresql_18:\n+            return self.as_sql(compiler, connection, **extra_context)\n+        return self.as_sql(\n+            compiler, connection, function=\"GEN_RANDOM_UUID\", **extra_context\n+        )\n+\n+    def as_mysql(self, compiler, connection, **extra_context):\n+        if not connection.features.supports_uuid4_function:\n+            if connection.mysql_is_mariadb:\n+                raise NotSupportedError(\"UUID4 requires MariaDB version 11.7 or later.\")\n+            raise NotSupportedError(\"UUID4 is not supported on MySQL.\")\n+        return self.as_sql(compiler, connection, function=\"UUID_V4\", **extra_context)\n+\n+    def as_oracle(self, compiler, connection, **extra_context):\n+        if not connection.features.supports_uuid4_function:\n+            raise NotSupportedError(\"UUID4 requires Oracle version 23ai or later.\")\n+        return self.as_sql(compiler, connection, function=\"UUID\", **extra_context)\n+\n+\n+class UUID7(Func):\n+    function = \"UUIDV7\"\n+    output_field = UUIDField()\n+\n+    def as_sql(self, compiler, connection, **extra_context):\n+        if connection.features.supports_uuid7_function:\n+            return super().as_sql(compiler, connection, **extra_context)\n+        raise NotSupportedError(\"UUID7 is not supported on this database backend.\")\n+\n+    def as_postgresql(self, compiler, connection, **extra_context):\n+        if connection.features.supports_uuid7_function:\n+            return self.as_sql(compiler, connection, **extra_context)\n+        raise NotSupportedError(\"UUID7 requires PostgreSQL version 18 or later.\")\n+\n+    def as_sqlite(self, compiler, connection, **extra_context):",
      "comment": "This will help when grepping for `PY314` three years from now. :-)\n```suggestion\n    # PY314: When dropping support for 3.14, remove the entire method.\n    def as_sqlite(self, compiler, connection, **extra_context):\n```\n",
      "comment_id": 2608161415,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-10T20:47:41Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2608161415"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/db/backends/sqlite3/_functions.py",
      "line": 90,
      "side": "RIGHT",
      "diff_hunk": "@@ -81,6 +86,8 @@ def register(connection):\n     connection.create_aggregate(\"VAR_POP\", 1, VarPop)\n     connection.create_aggregate(\"VAR_SAMP\", 1, VarSamp)\n     connection.create_aggregate(\"ANY_VALUE\", 1, AnyValue)\n+    connection.create_function(\"UUIDV4\", 0, _sqlite_uuid4)\n+    connection.create_function(\"UUIDV7\", 0, _sqlite_uuid7)",
      "comment": "I think we should try to avoid adding an invalid function when Python > 3.14 is used. Maybe:\r\n```python\r\ndef _sqlite_uuid7():\r\n    if PY314:\r\n        from uuid import uuid7\r\n\r\n        return uuid7().hex\r\n    else:\r\n        return None\r\n```\r\nor raise an error in `else...`.",
      "comment_id": 2608200027,
      "user": "felixxm",
      "created_at": "2025-12-10T21:01:47Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2608200027"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/db/models/functions/uuid.py",
      "line": 53,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,63 @@\n+from django.db import NotSupportedError\n+from django.db.models.expressions import Func\n+from django.db.models.fields import UUIDField\n+\n+\n+class UUID4(Func):\n+    function = \"UUIDV4\"\n+    output_field = UUIDField()\n+\n+    def as_sql(self, compiler, connection, **extra_context):\n+        if connection.features.supports_uuid4_function:\n+            return super().as_sql(compiler, connection, **extra_context)\n+        raise NotSupportedError(\"UUID4 is not supported on this database backend.\")\n+\n+    def as_postgresql(self, compiler, connection, **extra_context):\n+        if connection.features.is_postgresql_18:\n+            return self.as_sql(compiler, connection, **extra_context)\n+        return self.as_sql(\n+            compiler, connection, function=\"GEN_RANDOM_UUID\", **extra_context\n+        )\n+\n+    def as_mysql(self, compiler, connection, **extra_context):\n+        if not connection.features.supports_uuid4_function:\n+            if connection.mysql_is_mariadb:\n+                raise NotSupportedError(\"UUID4 requires MariaDB version 11.7 or later.\")\n+            raise NotSupportedError(\"UUID4 is not supported on MySQL.\")\n+        return self.as_sql(compiler, connection, function=\"UUID_V4\", **extra_context)\n+\n+    def as_oracle(self, compiler, connection, **extra_context):\n+        if not connection.features.supports_uuid4_function:\n+            raise NotSupportedError(\"UUID4 requires Oracle version 23ai or later.\")\n+        return self.as_sql(compiler, connection, function=\"UUID\", **extra_context)\n+\n+\n+class UUID7(Func):\n+    function = \"UUIDV7\"\n+    output_field = UUIDField()\n+\n+    def as_sql(self, compiler, connection, **extra_context):\n+        if connection.features.supports_uuid7_function:\n+            return super().as_sql(compiler, connection, **extra_context)\n+        raise NotSupportedError(\"UUID7 is not supported on this database backend.\")\n+\n+    def as_postgresql(self, compiler, connection, **extra_context):\n+        if connection.features.supports_uuid7_function:\n+            return self.as_sql(compiler, connection, **extra_context)\n+        raise NotSupportedError(\"UUID7 requires PostgreSQL version 18 or later.\")\n+\n+    def as_sqlite(self, compiler, connection, **extra_context):\n+        if connection.features.supports_uuid7_function:\n+            return self.as_sql(compiler, connection, **extra_context)\n+        raise NotSupportedError(\n+            \"UUID7 on sqlite requires Python version 3.14 or later.\"",
      "comment": "```suggestion\r\n            \"UUID7 on SQLite requires Python version 3.14 or later.\"\r\n```",
      "comment_id": 2608205772,
      "user": "felixxm",
      "created_at": "2025-12-10T21:04:17Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2608205772"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/contrib/postgres/functions.py",
      "line": 5,
      "side": "LEFT",
      "diff_hunk": "@@ -1,9 +1,18 @@\n-from django.db.models import DateTimeField, Func, UUIDField\n+import warnings\n \n+from django.db.models import DateTimeField, Func\n+from django.db.models.functions import UUID4\n+from django.utils.deprecation import RemovedInDjango70Warning, django_file_prefixes\n \n-class RandomUUID(Func):\n-    template = \"GEN_RANDOM_UUID()\"",
      "comment": "I don't think it's worth removing these class attributes -- we're breaking backward compatibility for subclasses that might be using different `template` or `function` or `output_field`.",
      "comment_id": 2608208743,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-10T21:05:37Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2608208743"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/contrib/postgres/functions.py",
      "line": 10,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,9 +1,18 @@\n-from django.db.models import DateTimeField, Func, UUIDField\n+import warnings\n \n+from django.db.models import DateTimeField, Func\n+from django.db.models.functions import UUID4\n+from django.utils.deprecation import RemovedInDjango70Warning, django_file_prefixes\n \n-class RandomUUID(Func):\n-    template = \"GEN_RANDOM_UUID()\"\n-    output_field = UUIDField()\n+\n+class RandomUUID(UUID4):\n+    def __init__(self, *args, **kwargs):\n+        warnings.warn(",
      "comment": "Reminder for when you've finalized the deprecation strategy: Please add a `# RemovedInDjango70Warning` with instructions about how to alter the code.",
      "comment_id": 2608211211,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-10T21:06:44Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2608211211"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/db/models/functions/uuid.py",
      "line": 41,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,63 @@\n+from django.db import NotSupportedError\n+from django.db.models.expressions import Func\n+from django.db.models.fields import UUIDField\n+\n+\n+class UUID4(Func):\n+    function = \"UUIDV4\"\n+    output_field = UUIDField()\n+\n+    def as_sql(self, compiler, connection, **extra_context):\n+        if connection.features.supports_uuid4_function:\n+            return super().as_sql(compiler, connection, **extra_context)\n+        raise NotSupportedError(\"UUID4 is not supported on this database backend.\")\n+\n+    def as_postgresql(self, compiler, connection, **extra_context):\n+        if connection.features.is_postgresql_18:\n+            return self.as_sql(compiler, connection, **extra_context)\n+        return self.as_sql(\n+            compiler, connection, function=\"GEN_RANDOM_UUID\", **extra_context\n+        )\n+\n+    def as_mysql(self, compiler, connection, **extra_context):\n+        if not connection.features.supports_uuid4_function:\n+            if connection.mysql_is_mariadb:\n+                raise NotSupportedError(\"UUID4 requires MariaDB version 11.7 or later.\")\n+            raise NotSupportedError(\"UUID4 is not supported on MySQL.\")\n+        return self.as_sql(compiler, connection, function=\"UUID_V4\", **extra_context)\n+\n+    def as_oracle(self, compiler, connection, **extra_context):\n+        if not connection.features.supports_uuid4_function:\n+            raise NotSupportedError(\"UUID4 requires Oracle version 23ai or later.\")\n+        return self.as_sql(compiler, connection, function=\"UUID\", **extra_context)\n+\n+\n+class UUID7(Func):\n+    function = \"UUIDV7\"\n+    output_field = UUIDField()",
      "comment": "Oh, I just saw #20282. In that case let's definitely use `arity=0` here and then in #20282 we can change to `arity=1` and then raise in `__init__` for providing `shift` on unsupported backends before delegating to the super's arity checks.",
      "comment_id": 2608226079,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-10T21:12:57Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2608226079"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/contrib/postgres/functions.py",
      "line": 13,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,9 +1,18 @@\n-from django.db.models import DateTimeField, Func, UUIDField\n+import warnings\n \n+from django.db.models import DateTimeField, Func\n+from django.db.models.functions import UUID4\n+from django.utils.deprecation import RemovedInDjango70Warning, django_file_prefixes\n \n-class RandomUUID(Func):\n-    template = \"GEN_RANDOM_UUID()\"\n-    output_field = UUIDField()\n+\n+class RandomUUID(UUID4):\n+    def __init__(self, *args, **kwargs):\n+        warnings.warn(\n+            \"RandomUUID is deprecated. Use django.db.models.functions.UUID4 instead.\",\n+            RemovedInDjango70Warning,\n+            skip_file_prefixes=django_file_prefixes(),",
      "comment": "Since we probably don't want to do that, I think I prefer either the system check or not deprecating.",
      "comment_id": 2608234157,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-10T21:16:04Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2608234157"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/contrib/postgres/functions.py",
      "line": 13,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,9 +1,18 @@\n-from django.db.models import DateTimeField, Func, UUIDField\n+import warnings\n \n+from django.db.models import DateTimeField, Func\n+from django.db.models.functions import UUID4\n+from django.utils.deprecation import RemovedInDjango70Warning, django_file_prefixes\n \n-class RandomUUID(Func):\n-    template = \"GEN_RANDOM_UUID()\"\n-    output_field = UUIDField()\n+\n+class RandomUUID(UUID4):\n+    def __init__(self, *args, **kwargs):\n+        warnings.warn(\n+            \"RandomUUID is deprecated. Use django.db.models.functions.UUID4 instead.\",\n+            RemovedInDjango70Warning,\n+            skip_file_prefixes=django_file_prefixes(),",
      "comment": "I think I'll just remove the deprecation. We could document the new alternative, but anything more than that is too much hassle without the check infrastructure, which I don't want to add. This could be revisited if someone else adds it. Should we have a ticket for that?",
      "comment_id": 2608251263,
      "user": "LilyFirefly",
      "created_at": "2025-12-10T21:22:46Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2608251263"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/contrib/postgres/functions.py",
      "line": 13,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,9 +1,18 @@\n-from django.db.models import DateTimeField, Func, UUIDField\n+import warnings\n \n+from django.db.models import DateTimeField, Func\n+from django.db.models.functions import UUID4\n+from django.utils.deprecation import RemovedInDjango70Warning, django_file_prefixes\n \n-class RandomUUID(Func):\n-    template = \"GEN_RANDOM_UUID()\"\n-    output_field = UUIDField()\n+\n+class RandomUUID(UUID4):\n+    def __init__(self, *args, **kwargs):\n+        warnings.warn(\n+            \"RandomUUID is deprecated. Use django.db.models.functions.UUID4 instead.\",\n+            RemovedInDjango70Warning,\n+            skip_file_prefixes=django_file_prefixes(),",
      "comment": "> Should we have a ticket for that?\r\n\r\nSure, but I would say a new-features issue: \"make it possible to deprecate expressions\". Let me know if you don't have a cycle for that, I could toss something quickly over the wall.",
      "comment_id": 2608288501,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-10T21:34:23Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2608288501"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/db/backends/sqlite3/_functions.py",
      "line": 90,
      "side": "RIGHT",
      "diff_hunk": "@@ -81,6 +86,8 @@ def register(connection):\n     connection.create_aggregate(\"VAR_POP\", 1, VarPop)\n     connection.create_aggregate(\"VAR_SAMP\", 1, VarSamp)\n     connection.create_aggregate(\"ANY_VALUE\", 1, AnyValue)\n+    connection.create_function(\"UUIDV4\", 0, _sqlite_uuid4)\n+    connection.create_function(\"UUIDV7\", 0, _sqlite_uuid7)",
      "comment": "I put an `if PY314:` guard around registering the function instead.",
      "comment_id": 2608356932,
      "user": "LilyFirefly",
      "created_at": "2025-12-10T21:57:46Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2608356932"
    },
    {
      "repo": "django/django",
      "pr_number": 20101,
      "file_path": "django/contrib/postgres/functions.py",
      "line": 5,
      "side": "LEFT",
      "diff_hunk": "@@ -1,9 +1,18 @@\n-from django.db.models import DateTimeField, Func, UUIDField\n+import warnings\n \n+from django.db.models import DateTimeField, Func\n+from django.db.models.functions import UUID4\n+from django.utils.deprecation import RemovedInDjango70Warning, django_file_prefixes\n \n-class RandomUUID(Func):\n-    template = \"GEN_RANDOM_UUID()\"",
      "comment": "This is moot since I've removed the deprecation, but otherwise a good point.",
      "comment_id": 2608364593,
      "user": "LilyFirefly",
      "created_at": "2025-12-10T21:59:59Z",
      "url": "https://github.com/django/django/pull/20101#discussion_r2608364593"
    },
    {
      "repo": "django/django",
      "pr_number": 20009,
      "file_path": "django/db/models/fields/json.py",
      "line": 416,
      "side": "RIGHT",
      "diff_hunk": "@@ -373,6 +373,69 @@ class JSONIContains(CaseInsensitiveMixin, lookups.IContains):\n     pass\n \n \n+class JSONIn(lookups.In):\n+    def resolve_expression_parameter(self, compiler, connection, sql, param):\n+        sql, params = super().resolve_expression_parameter(\n+            compiler,\n+            connection,\n+            sql,\n+            param,\n+        )\n+        if (\n+            not hasattr(param, \"as_sql\")\n+            and not connection.features.has_native_json_field\n+        ):\n+            if connection.vendor == \"oracle\":\n+                value = json.loads(param)\n+                sql = \"%s(JSON_OBJECT('value' VALUE %%s FORMAT JSON), '$.value')\"\n+                if isinstance(value, (list, dict)):\n+                    sql %= \"JSON_QUERY\"\n+                else:\n+                    sql %= \"JSON_VALUE\"\n+            elif connection.vendor == \"mysql\" or (\n+                connection.vendor == \"sqlite\"\n+                and params[0] not in connection.ops.jsonfield_datatype_values\n+            ):\n+                sql = \"JSON_EXTRACT(%s, '$')\"\n+        if connection.vendor == \"mysql\" and connection.mysql_is_mariadb:\n+            sql = \"JSON_UNQUOTE(%s)\" % sql\n+        return sql, params\n+\n+    def process_lhs(self, compiler, connection):\n+        sql, params = super().process_lhs(compiler, connection)\n+        if isinstance(self.lhs, KeyTransform):\n+            return sql, params\n+        if connection.vendor in \"mysql\":\n+            sql = \"JSON_EXTRACT(%s, '$')\" % sql\n+        elif connection.vendor == \"oracle\":\n+            if connection.features.supports_primitives_in_json_field:\n+                template = (\n+                    \"COALESCE(\"\n+                    \"JSON_VALUE(%s, '$'),\"\n+                    \"JSON_QUERY(%s, '$' DISALLOW SCALARS)\"\n+                    \")\"",
      "comment": "This looks subtly different from the logic in `KeyTransform.as_oracle`. Can you share more about how you derived this code, or why we wouldn't factor out the duplication?",
      "comment_id": 2477965161,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-30T12:46:14Z",
      "url": "https://github.com/django/django/pull/20009#discussion_r2477965161"
    },
    {
      "repo": "django/django",
      "pr_number": 20009,
      "file_path": "django/db/models/fields/json.py",
      "line": 416,
      "side": "RIGHT",
      "diff_hunk": "@@ -373,6 +373,69 @@ class JSONIContains(CaseInsensitiveMixin, lookups.IContains):\n     pass\n \n \n+class JSONIn(lookups.In):\n+    def resolve_expression_parameter(self, compiler, connection, sql, param):\n+        sql, params = super().resolve_expression_parameter(\n+            compiler,\n+            connection,\n+            sql,\n+            param,\n+        )\n+        if (\n+            not hasattr(param, \"as_sql\")\n+            and not connection.features.has_native_json_field\n+        ):\n+            if connection.vendor == \"oracle\":\n+                value = json.loads(param)\n+                sql = \"%s(JSON_OBJECT('value' VALUE %%s FORMAT JSON), '$.value')\"\n+                if isinstance(value, (list, dict)):\n+                    sql %= \"JSON_QUERY\"\n+                else:\n+                    sql %= \"JSON_VALUE\"\n+            elif connection.vendor == \"mysql\" or (\n+                connection.vendor == \"sqlite\"\n+                and params[0] not in connection.ops.jsonfield_datatype_values\n+            ):\n+                sql = \"JSON_EXTRACT(%s, '$')\"\n+        if connection.vendor == \"mysql\" and connection.mysql_is_mariadb:\n+            sql = \"JSON_UNQUOTE(%s)\" % sql\n+        return sql, params\n+\n+    def process_lhs(self, compiler, connection):\n+        sql, params = super().process_lhs(compiler, connection)\n+        if isinstance(self.lhs, KeyTransform):\n+            return sql, params\n+        if connection.vendor in \"mysql\":\n+            sql = \"JSON_EXTRACT(%s, '$')\" % sql\n+        elif connection.vendor == \"oracle\":\n+            if connection.features.supports_primitives_in_json_field:\n+                template = (\n+                    \"COALESCE(\"\n+                    \"JSON_VALUE(%s, '$'),\"\n+                    \"JSON_QUERY(%s, '$' DISALLOW SCALARS)\"\n+                    \")\"",
      "comment": "I factored out the duplication. LMKWYT!\r\n\r\n(The code was derived taking inspiration from `KeyTransform.as_{vendor}` methods, replacing JSON paths with the root path for JSON `Col`s.)",
      "comment_id": 2479215592,
      "user": "cliffordgama",
      "created_at": "2025-10-30T19:02:49Z",
      "url": "https://github.com/django/django/pull/20009#discussion_r2479215592"
    },
    {
      "repo": "django/django",
      "pr_number": 20009,
      "file_path": "django/db/backends/oracle/features.py",
      "line": 132,
      "side": "RIGHT",
      "diff_hunk": "@@ -127,6 +127,9 @@ def django_test_skips(self):\n             \"Oracle doesn't support casting filters to NUMBER.\": {\n                 \"lookup.tests.LookupQueryingTests.test_aggregate_combined_lookup\",\n             },\n+            \"Oracle doesn't support JSON null scalar extraction for IN queries\": {\n+                \"model_fields.test_jsonfield.JSONNullTests.test_filter_in\"\n+            },",
      "comment": "@jacobtylerwalls you were right about there being no way to extract a JSON scalar `null` value in Oracle. Oracle's JSON/SQL functions will convert it to SQL NULL. The way key transform exact handles this is by changing `__key__exact=JSONNull()` to \"does the JSON value have the key and is the key NULL. https://github.com/django/django/blob/main/django/db/models/fields/json.py#L594-L597\n\nPerhaps a potential hack in this case would be to rewrite the `IN` clause as an `OR` chain, for example replacing\n`IN (v1, v2, \u2026)` with\n`JSON_EQUAL(..., v1) OR JSON_EQUAL(..., v2) OR ...` although I'm not sure if it's worth it or not. \n\n",
      "comment_id": 2479337244,
      "user": "cliffordgama",
      "created_at": "2025-10-30T19:53:12Z",
      "url": "https://github.com/django/django/pull/20009#discussion_r2479337244"
    },
    {
      "repo": "django/django",
      "pr_number": 20009,
      "file_path": "django/db/backends/oracle/features.py",
      "line": 132,
      "side": "RIGHT",
      "diff_hunk": "@@ -127,6 +127,9 @@ def django_test_skips(self):\n             \"Oracle doesn't support casting filters to NUMBER.\": {\n                 \"lookup.tests.LookupQueryingTests.test_aggregate_combined_lookup\",\n             },\n+            \"Oracle doesn't support JSON null scalar extraction for IN queries\": {\n+                \"model_fields.test_jsonfield.JSONNullTests.test_filter_in\"\n+            },",
      "comment": "Cool. I'll sit with this, but at a glance yeah a skip looks better than a feature flag for this.\n\n> rewrite the IN clause as an OR chain\n\nThat seems akin to what we did with composite pk's but I agree would be pretty overkill for a niche use case. Happy to skip as you suggest here.",
      "comment_id": 2479579117,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-30T21:29:44Z",
      "url": "https://github.com/django/django/pull/20009#discussion_r2479579117"
    },
    {
      "repo": "django/django",
      "pr_number": 20009,
      "file_path": "django/db/models/fields/json.py",
      "line": 382,
      "side": "RIGHT",
      "diff_hunk": "@@ -373,16 +373,106 @@ class JSONIContains(CaseInsensitiveMixin, lookups.IContains):\n     pass\n \n \n+class ProcessJSONLHSMixin:\n+    def _get_json_path(self, connection, key_transforms):\n+        if key_transforms is None:\n+            json_path = \"$\"\n+        else:\n+            json_path = connection.ops.compile_json_path(key_transforms)\n+        return json_path",
      "comment": "```suggestion\n        if key_transforms is None:\n            return \"$\"\n        return connection.ops.compile_json_path(key_transforms)\n```",
      "comment_id": 2480388626,
      "user": "cliffordgama",
      "created_at": "2025-10-31T07:33:28Z",
      "url": "https://github.com/django/django/pull/20009#discussion_r2480388626"
    },
    {
      "repo": "django/django",
      "pr_number": 20009,
      "file_path": "tests/model_fields/test_jsonfield.py",
      "line": 1021,
      "side": "RIGHT",
      "diff_hunk": "@@ -1010,6 +1010,19 @@ def test_key_iexact(self):\n             NullableJSONModel.objects.filter(value__foo__iexact='\"BaR\"').exists(), False\n         )\n \n+    def test_in(self):\n+        tests = [\n+            ([[]], [self.objs[1]]),\n+            ([{}], [self.objs[2]]),\n+            ([{\"a\": \"b\", \"c\": 14}], [self.objs[3]]),\n+            ([[1, [2]]], [self.objs[5]]),\n+        ]\n+        for lookup_value, expected in tests:\n+            with self.subTest(value__in=lookup_value), transaction.atomic():",
      "comment": "I saw that Simon added it for `test__key__in` in 089deb82b9ac2d002af36fd36f288368cdac4b53 (seemingly intentionally? there is no comment), and I thought that maybe it is needed to isolate failures, say in the event of a database error. I'm not too sure, tho'.",
      "comment_id": 2482867388,
      "user": "cliffordgama",
      "created_at": "2025-10-31T22:56:13Z",
      "url": "https://github.com/django/django/pull/20009#discussion_r2482867388"
    },
    {
      "repo": "django/django",
      "pr_number": 20009,
      "file_path": "tests/model_fields/test_jsonfield.py",
      "line": 1021,
      "side": "RIGHT",
      "diff_hunk": "@@ -1010,6 +1010,19 @@ def test_key_iexact(self):\n             NullableJSONModel.objects.filter(value__foo__iexact='\"BaR\"').exists(), False\n         )\n \n+    def test_in(self):\n+        tests = [\n+            ([[]], [self.objs[1]]),\n+            ([{}], [self.objs[2]]),\n+            ([{\"a\": \"b\", \"c\": 14}], [self.objs[3]]),\n+            ([[1, [2]]], [self.objs[5]]),\n+        ]\n+        for lookup_value, expected in tests:\n+            with self.subTest(value__in=lookup_value), transaction.atomic():",
      "comment": "> maybe it is needed to isolate failures\r\n\r\nAh, maybe. But then this would fall more on the side of _debugging code_ then? I think we can remove it here.",
      "comment_id": 2488387985,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-04T01:53:53Z",
      "url": "https://github.com/django/django/pull/20009#discussion_r2488387985"
    },
    {
      "repo": "django/django",
      "pr_number": 20009,
      "file_path": "django/db/backends/oracle/features.py",
      "line": 132,
      "side": "RIGHT",
      "diff_hunk": "@@ -127,6 +127,9 @@ def django_test_skips(self):\n             \"Oracle doesn't support casting filters to NUMBER.\": {\n                 \"lookup.tests.LookupQueryingTests.test_aggregate_combined_lookup\",\n             },\n+            \"Oracle doesn't support JSON null scalar extraction for IN queries\": {\n+                \"model_fields.test_jsonfield.JSONNullTests.test_filter_in\"\n+            },",
      "comment": "I also struggled with that, and that explains why I played with `__in=[JSONNull(), None]` -- all I wanted was a second parameter, and `None` was the easiest thing to provide. (But now I see an issue with providing `None` there is that the behavior isn't the same on all backends, so let's remove it.)  I think it would be better to use `JSONObject(k=Value(\"v\")` for now. \r\n\r\nIf you think there is a separate issue here where we can improve handling of list/dict params we should probably get a new ticket triaged for it and evaluate it on each backend.\r\n\r\nI also think I'm missing a `clone = self.rhs.copy()` before mutating the source expressions.",
      "comment_id": 2494543502,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-05T13:39:15Z",
      "url": "https://github.com/django/django/pull/20009#discussion_r2494543502"
    },
    {
      "repo": "django/django",
      "pr_number": 20009,
      "file_path": "django/db/backends/oracle/features.py",
      "line": 132,
      "side": "RIGHT",
      "diff_hunk": "@@ -127,6 +127,9 @@ def django_test_skips(self):\n             \"Oracle doesn't support casting filters to NUMBER.\": {\n                 \"lookup.tests.LookupQueryingTests.test_aggregate_combined_lookup\",\n             },\n+            \"Oracle doesn't support JSON null scalar extraction for IN queries\": {\n+                \"model_fields.test_jsonfield.JSONNullTests.test_filter_in\"\n+            },",
      "comment": "I do see this now with `JSONObject`:\r\n\r\n> django.db.utils.DatabaseError: ORA-22848: cannot use NCLOB type as comparison key\r\nHelp: https://docs.oracle.com/error-help/db/ora-22848/\r\n\r\nDo you have an idea about this?",
      "comment_id": 2494562062,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-05T13:42:48Z",
      "url": "https://github.com/django/django/pull/20009#discussion_r2494562062"
    },
    {
      "repo": "django/django",
      "pr_number": 20009,
      "file_path": "django/db/backends/oracle/features.py",
      "line": 132,
      "side": "RIGHT",
      "diff_hunk": "@@ -127,6 +127,9 @@ def django_test_skips(self):\n             \"Oracle doesn't support casting filters to NUMBER.\": {\n                 \"lookup.tests.LookupQueryingTests.test_aggregate_combined_lookup\",\n             },\n+            \"Oracle doesn't support JSON null scalar extraction for IN queries\": {\n+                \"model_fields.test_jsonfield.JSONNullTests.test_filter_in\"\n+            },",
      "comment": "> Do you have an idea about this?\r\n\r\nI will look into it \ud83d\udc4d\ud83c\udffe ",
      "comment_id": 2494598393,
      "user": "cliffordgama",
      "created_at": "2025-11-05T13:50:26Z",
      "url": "https://github.com/django/django/pull/20009#discussion_r2494598393"
    },
    {
      "repo": "django/django",
      "pr_number": 20009,
      "file_path": "django/db/backends/oracle/features.py",
      "line": 132,
      "side": "RIGHT",
      "diff_hunk": "@@ -127,6 +127,9 @@ def django_test_skips(self):\n             \"Oracle doesn't support casting filters to NUMBER.\": {\n                 \"lookup.tests.LookupQueryingTests.test_aggregate_combined_lookup\",\n             },\n+            \"Oracle doesn't support JSON null scalar extraction for IN queries\": {\n+                \"model_fields.test_jsonfield.JSONNullTests.test_filter_in\"\n+            },",
      "comment": "Grepping for the NCLOB stuff, I see this `== \"TextField\"` that maybe should be `in [\"TextField\", \"JSONField\"]`?\r\n\r\nhttps://github.com/django/django/blob/6d4d99b3cef4a6d931de02f89a493fb345dc438e/django/db/models/functions/comparison.py#L88-L91",
      "comment_id": 2500562079,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-06T19:39:52Z",
      "url": "https://github.com/django/django/pull/20009#discussion_r2500562079"
    },
    {
      "repo": "django/django",
      "pr_number": 20009,
      "file_path": "django/db/backends/oracle/features.py",
      "line": 132,
      "side": "RIGHT",
      "diff_hunk": "@@ -127,6 +127,9 @@ def django_test_skips(self):\n             \"Oracle doesn't support casting filters to NUMBER.\": {\n                 \"lookup.tests.LookupQueryingTests.test_aggregate_combined_lookup\",\n             },\n+            \"Oracle doesn't support JSON null scalar extraction for IN queries\": {\n+                \"model_fields.test_jsonfield.JSONNullTests.test_filter_in\"\n+            },",
      "comment": "> Grepping for the NCLOB stuff, I see this == \"TextField\" that maybe should be in [\"TextField\", \"JSONField\"]?\r\n\r\nI'd say yes, since Django saves `JSONField` as NCLOB on Oracle, although given how strict Oracle is with LOBs we'd probably want to add a few extra tests to ensure that nothing breaks?\r\n\r\n> I do see this now with `JSONObject`:\r\n\r\nFrom a glance at the queries, I think it's because `JSONObject` is explicitly `RETURNING CLOB` and LOB cannot be used for comparison. Although I'm not sure that we want to add support for `JSONObject()` seeing as there are no tests related to `JSONField`?",
      "comment_id": 2508344373,
      "user": "cliffordgama",
      "created_at": "2025-11-09T21:47:30Z",
      "url": "https://github.com/django/django/pull/20009#discussion_r2508344373"
    },
    {
      "repo": "django/django",
      "pr_number": 20009,
      "file_path": "django/db/backends/oracle/features.py",
      "line": 132,
      "side": "RIGHT",
      "diff_hunk": "@@ -127,6 +127,9 @@ def django_test_skips(self):\n             \"Oracle doesn't support casting filters to NUMBER.\": {\n                 \"lookup.tests.LookupQueryingTests.test_aggregate_combined_lookup\",\n             },\n+            \"Oracle doesn't support JSON null scalar extraction for IN queries\": {\n+                \"model_fields.test_jsonfield.JSONNullTests.test_filter_in\"\n+            },",
      "comment": "Hello, @jacobtylerwalls \ud83d\udc4b\ud83c\udffe \r\n\r\nRE: your `as_oracle` to allow \"OR ... IS NULL\"\r\n\r\nI struggled to make it work in a way I was happy with. For context, the following are some specific things that I had to take into account.\r\n1. The `IS NULL` check is not the same for top-level `JSONField` and for keys, (which were overlooked). To make sure a key truly has \"null\" as a value, we need to first perform a `JSON_EXISTS` (`__haskey` lookup) on the key and then do a `JSON_VALUE(...) IS NULL` (`__isnull`). This is because `JSON_VALUE(..) IS NULL` will also be true for nonexistent keys.\r\n3. Because of the above, we also have to handle `None` values (at least when lhs is a keytransform), since they are synonymous with `JSONNull()` inside an array or json document.\r\n4. `JSON_VALUE(...)` is fine for the case when what the LHS returns is a JSON scalar value, otherwise we have to use `JSON_QUERY(...)` using `COALESCE(...)` as is done when processing other oracle lhs in these lookups.\r\n5. As you identified in the patch, we need to handle cases when `JSONNull()/None` may be the only values in the iterable rhs.\r\n\r\nGiven that this is non-trivial, can we skip the test on oracle for now and follow up with this in a separate work, if we choose to follow up? If so, I'd like to try chaining `OR` instead of doing an `IN () OR ... IS NULL` in a follow up.\r\n\r\n(RE: The extra commit:\r\n\r\nThinking of an alternative solution \ud83e\udd14. Do you have any idea why `FieldGetDbPrepValueIterableMixin` does not call any `get_db_prep_value()` despite its name seemingly promising that it will? ... I understand values are already \"prepped\" by the time they are wrapped in `ExpressionList`, but this doesn't mean much for JSONField since it doesn't override `get_prep_value()`...)",
      "comment_id": 2511974291,
      "user": "cliffordgama",
      "created_at": "2025-11-10T21:08:15Z",
      "url": "https://github.com/django/django/pull/20009#discussion_r2511974291"
    },
    {
      "repo": "django/django",
      "pr_number": 20009,
      "file_path": "django/db/backends/oracle/features.py",
      "line": 132,
      "side": "RIGHT",
      "diff_hunk": "@@ -127,6 +127,9 @@ def django_test_skips(self):\n             \"Oracle doesn't support casting filters to NUMBER.\": {\n                 \"lookup.tests.LookupQueryingTests.test_aggregate_combined_lookup\",\n             },\n+            \"Oracle doesn't support JSON null scalar extraction for IN queries\": {\n+                \"model_fields.test_jsonfield.JSONNullTests.test_filter_in\"\n+            },",
      "comment": "> Given that this is non-trivial, can we skip the test on oracle for now and follow up with this in a separate work, if we choose to follow up? If so, I'd like to try chaining OR instead of doing an IN () OR ... IS NULL in a follow up.\r\n\r\nFor sure. This is getting pretty baroque. Let's wait until someone needs it. Thanks for giving it a look though.",
      "comment_id": 2512145486,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-10T22:14:42Z",
      "url": "https://github.com/django/django/pull/20009#discussion_r2512145486"
    },
    {
      "repo": "django/django",
      "pr_number": 20009,
      "file_path": "django/db/models/lookups.py",
      "line": 309,
      "side": "RIGHT",
      "diff_hunk": "@@ -304,11 +304,9 @@ def get_prep_lookup(self):\n         if contains_expr:\n             return ExpressionList(\n                 *[\n-                    # Expression defaults `str` to field references while\n-                    # lookups default them to literal values.\n                     (\n                         Value(prep_value, self.lhs.output_field)\n-                        if isinstance(prep_value, str)\n+                        if not hasattr(prep_value, \"as_sql\")",
      "comment": "Can you say more about how you worked out this should check for compilables and not resolvables? I get no failures when I change this to `\"resolve_expression\"`.",
      "comment_id": 2519512397,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-12T19:21:26Z",
      "url": "https://github.com/django/django/pull/20009#discussion_r2519512397"
    },
    {
      "repo": "django/django",
      "pr_number": 20009,
      "file_path": "django/db/backends/oracle/features.py",
      "line": 132,
      "side": "RIGHT",
      "diff_hunk": "@@ -127,6 +127,9 @@ def django_test_skips(self):\n             \"Oracle doesn't support casting filters to NUMBER.\": {\n                 \"lookup.tests.LookupQueryingTests.test_aggregate_combined_lookup\",\n             },\n+            \"Oracle doesn't support JSON null scalar extraction for IN queries\": {\n+                \"model_fields.test_jsonfield.JSONNullTests.test_filter_in\"\n+            },",
      "comment": "> Do you have any idea why FieldGetDbPrepValueIterableMixin does not call any get_db_prep_value() despite its name seemingly promising that it will?\r\n\r\nIt inherits from `FieldGetDbPrepValueMixin`, which does provide a `get_db_prep_lookup()` that calls `get_db_prep_value()`, looks relevant?\r\n",
      "comment_id": 2519527706,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-12T19:27:29Z",
      "url": "https://github.com/django/django/pull/20009#discussion_r2519527706"
    },
    {
      "repo": "django/django",
      "pr_number": 20009,
      "file_path": "django/db/models/lookups.py",
      "line": 309,
      "side": "RIGHT",
      "diff_hunk": "@@ -304,11 +304,9 @@ def get_prep_lookup(self):\n         if contains_expr:\n             return ExpressionList(\n                 *[\n-                    # Expression defaults `str` to field references while\n-                    # lookups default them to literal values.\n                     (\n                         Value(prep_value, self.lhs.output_field)\n-                        if isinstance(prep_value, str)\n+                        if not hasattr(prep_value, \"as_sql\")",
      "comment": "That was a mistake, it should check for resolvables instead. Thanks.",
      "comment_id": 2526149643,
      "user": "cliffordgama",
      "created_at": "2025-11-14T07:31:07Z",
      "url": "https://github.com/django/django/pull/20009#discussion_r2526149643"
    },
    {
      "repo": "django/django",
      "pr_number": 20009,
      "file_path": "django/db/backends/oracle/features.py",
      "line": 132,
      "side": "RIGHT",
      "diff_hunk": "@@ -127,6 +127,9 @@ def django_test_skips(self):\n             \"Oracle doesn't support casting filters to NUMBER.\": {\n                 \"lookup.tests.LookupQueryingTests.test_aggregate_combined_lookup\",\n             },\n+            \"Oracle doesn't support JSON null scalar extraction for IN queries\": {\n+                \"model_fields.test_jsonfield.JSONNullTests.test_filter_in\"\n+            },",
      "comment": "The issue is that once an iterable RHS is wrapped in `ExpressionList`, its elements become expressions, so `get_db_prep_value()` is no longer called for literal values.\r\n\r\n>  looks relevant?\r\n\r\nI thought `get_db_prep_lookup()` might help, but it can\u2019t, because the RHS has already been turned into expressions by the time that method runs.\r\n\r\nUsing `Value(..., output_field=...)` ensures that `get_db_prep_value()` [is applied to every value](https://github.com/django/django/blob/0eec2a163a4b2ea4e82c53ae1d7b71bf43ac911d/django/db/models/expressions.py#L1171) in the RHS.",
      "comment_id": 2526227582,
      "user": "cliffordgama",
      "created_at": "2025-11-14T07:47:34Z",
      "url": "https://github.com/django/django/pull/20009#discussion_r2526227582"
    },
    {
      "repo": "django/django",
      "pr_number": 20009,
      "file_path": "django/db/models/lookups.py",
      "line": 293,
      "side": "RIGHT",
      "diff_hunk": "@@ -287,33 +287,26 @@ class FieldGetDbPrepValueIterableMixin(FieldGetDbPrepValueMixin):\n     def get_prep_lookup(self):\n         if hasattr(self.rhs, \"resolve_expression\"):\n             return self.rhs\n-        contains_expr = False\n+        if any(hasattr(value, \"resolve_expression\") for value in self.rhs):\n+            return ExpressionList(",
      "comment": "We can return early since direct values will be prepped at compilation time by `Value.as_sql()`; there's no need to prep them here.",
      "comment_id": 2526242219,
      "user": "cliffordgama",
      "created_at": "2025-11-14T07:51:29Z",
      "url": "https://github.com/django/django/pull/20009#discussion_r2526242219"
    },
    {
      "repo": "django/django",
      "pr_number": 20009,
      "file_path": "django/db/backends/oracle/features.py",
      "line": 132,
      "side": "RIGHT",
      "diff_hunk": "@@ -127,6 +127,9 @@ def django_test_skips(self):\n             \"Oracle doesn't support casting filters to NUMBER.\": {\n                 \"lookup.tests.LookupQueryingTests.test_aggregate_combined_lookup\",\n             },\n+            \"Oracle doesn't support JSON null scalar extraction for IN queries\": {\n+                \"model_fields.test_jsonfield.JSONNullTests.test_filter_in\"\n+            },",
      "comment": "> This is getting pretty baroque. Let's wait until someone needs it.\r\n\r\n@jacobtylerwalls I decided to give it another shot with fresh eyes. It seems like the logic becomes simplified if we just `exact` and chain everything instead of special-casing `JSONNull()` alone.\r\n",
      "comment_id": 2598600249,
      "user": "cliffordgama",
      "created_at": "2025-12-08T13:14:10Z",
      "url": "https://github.com/django/django/pull/20009#discussion_r2598600249"
    },
    {
      "repo": "django/django",
      "pr_number": 20009,
      "file_path": "django/db/models/lookups.py",
      "line": 298,
      "side": "RIGHT",
      "diff_hunk": "@@ -287,33 +287,26 @@ class FieldGetDbPrepValueIterableMixin(FieldGetDbPrepValueMixin):\n     def get_prep_lookup(self):\n         if hasattr(self.rhs, \"resolve_expression\"):\n             return self.rhs\n-        contains_expr = False\n+        if any(hasattr(value, \"resolve_expression\") for value in self.rhs):\n+            return ExpressionList(\n+                *[\n+                    (\n+                        value\n+                        if hasattr(value, \"resolve_expression\")\n+                        else Value(value, self.lhs.output_field)",
      "comment": "Somewhere in this vicinity we might keep the comment \"an expression will be handled by the database...\"",
      "comment_id": 2603509196,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-09T16:57:02Z",
      "url": "https://github.com/django/django/pull/20009#discussion_r2603509196"
    },
    {
      "repo": "django/django",
      "pr_number": 20009,
      "file_path": "django/db/models/fields/json.py",
      "line": 476,
      "side": "RIGHT",
      "diff_hunk": "@@ -375,16 +375,121 @@ class JSONIContains(CaseInsensitiveMixin, lookups.IContains):\n     pass\n \n \n+class ProcessJSONLHSMixin:\n+    def _get_json_path(self, connection, key_transforms):\n+        if key_transforms is None:\n+            return \"$\"\n+        return connection.ops.compile_json_path(key_transforms)\n+\n+    def _process_as_oracle(self, sql, params, connection, key_transforms=None):\n+        json_path = self._get_json_path(connection, key_transforms)\n+        if connection.features.supports_primitives_in_json_field:\n+            template = (\n+                \"COALESCE(\"\n+                \"JSON_VALUE(%s, q'\\uffff%s\\uffff'),\"\n+                \"JSON_QUERY(%s, q'\\uffff%s\\uffff' DISALLOW SCALARS)\"\n+                \")\"\n+            )\n+        else:\n+            template = (\n+                \"COALESCE(\"\n+                \"JSON_QUERY(%s, q'\\uffff%s\\uffff'),\"\n+                \"JSON_VALUE(%s, q'\\uffff%s\\uffff')\"\n+                \")\"\n+            )\n+        # Add paths directly into SQL because path expressions cannot be passed\n+        # as bind variables on Oracle. Use a custom delimiter to prevent the\n+        # JSON path from escaping the SQL literal. Each key in the JSON path is\n+        # passed through json.dumps() with ensure_ascii=True (the default),\n+        # which converts the delimiter into the escaped \\uffff format. This\n+        # ensures that the delimiter is not present in the JSON path.\n+        sql = template % ((sql, json_path) * 2)\n+        return sql, params * 2\n+\n+    def _process_as_sqlite(self, sql, params, connection, key_transforms=None):\n+        json_path = self._get_json_path(connection, key_transforms)\n+        datatype_values = \",\".join(\n+            [repr(value) for value in connection.ops.jsonfield_datatype_values]\n+        )\n+        return (\n+            \"(CASE WHEN JSON_TYPE(%s, %%s) IN (%s) \"\n+            \"THEN JSON_TYPE(%s, %%s) ELSE JSON_EXTRACT(%s, %%s) END)\"\n+        ) % (sql, datatype_values, sql, sql), (*params, json_path) * 3\n+\n+    def _process_as_mysql(self, sql, params, connection, key_transforms=None):\n+        json_path = self._get_json_path(connection, key_transforms)\n+        return \"JSON_EXTRACT(%s, %%s)\" % sql, (*params, json_path)\n+\n+\n+class JSONIn(ProcessJSONLHSMixin, lookups.In):\n+    def resolve_expression_parameter(self, compiler, connection, sql, param):\n+        sql, params = super().resolve_expression_parameter(\n+            compiler,\n+            connection,\n+            sql,\n+            param,\n+        )\n+        if not connection.features.has_native_json_field and (\n+            not hasattr(param, \"as_sql\") or isinstance(param, expressions.Value)\n+        ):\n+            if connection.vendor == \"oracle\":\n+                value = param.value if hasattr(param, \"value\") else json.loads(param)\n+                sql = \"%s(JSON_OBJECT('value' VALUE %%s FORMAT JSON), '$.value')\"\n+                if isinstance(value, (list, dict)):\n+                    sql %= \"JSON_QUERY\"\n+                else:\n+                    sql %= \"JSON_VALUE\"\n+            elif connection.vendor == \"mysql\" or (\n+                connection.vendor == \"sqlite\"\n+                and params[0] not in connection.ops.jsonfield_datatype_values\n+            ):\n+                sql = \"JSON_EXTRACT(%s, '$')\"\n+        if connection.vendor == \"mysql\" and connection.mysql_is_mariadb:\n+            sql = \"JSON_UNQUOTE(%s)\" % sql\n+        return sql, params\n+\n+    def process_lhs(self, compiler, connection):\n+        sql, params = super().process_lhs(compiler, connection)\n+        if isinstance(self.lhs, KeyTransform):\n+            return sql, params\n+        if connection.vendor == \"mysql\":\n+            return self._process_as_mysql(sql, params, connection)\n+        elif connection.vendor == \"oracle\":\n+            return self._process_as_oracle(sql, params, connection)\n+        elif connection.vendor == \"sqlite\":\n+            return self._process_as_sqlite(sql, params, connection)\n+        return sql, params\n+\n+    def as_oracle(self, compiler, connection):\n+        if (\n+            connection.features.supports_primitives_in_json_field\n+            and isinstance(self.rhs, expressions.ExpressionList)\n+            and JSONNull() in self.rhs.get_source_expressions()\n+        ):\n+            exact_lookup = self.lhs.get_lookup(\"exact\")\n+            sql_parts = []\n+            all_params = ()\n+            for expr in self.rhs.get_source_expressions():\n+                lookup = exact_lookup(self.lhs, expr)\n+                sql, params = lookup.as_oracle(compiler, connection)\n+                sql_parts.append(f\"({sql})\")\n+                all_params += params",
      "comment": "Let's avoid the `+` and favor `*`, given how much trouble we've had with `TypeError` for tuple + list. It *shouldn't* be an issue now, but who knows what subclasses are doing in the wild...",
      "comment_id": 2603519947,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-09T17:00:16Z",
      "url": "https://github.com/django/django/pull/20009#discussion_r2603519947"
    },
    {
      "repo": "django/django",
      "pr_number": 20009,
      "file_path": "tests/model_fields/test_jsonfield.py",
      "line": 1329,
      "side": "RIGHT",
      "diff_hunk": "@@ -1297,6 +1322,22 @@ def test_filter(self):\n             NullableJSONModel.objects.filter(value__isnull=True), [sql_null]\n         )\n \n+    def test_filter_in(self):\n+        obj = NullableJSONModel.objects.create(value=JSONNull())\n+        obj2 = NullableJSONModel.objects.create(value=[1])\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__in=[JSONNull(), [1], \"foo\"]),",
      "comment": "Playing with this test case (using `In()` rather than `__in`) revealed ticket-36787, but we don't have to block on that here.",
      "comment_id": 2604162492,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-09T20:07:43Z",
      "url": "https://github.com/django/django/pull/20009#discussion_r2604162492"
    },
    {
      "repo": "django/django",
      "pr_number": 19993,
      "file_path": "tests/gis_tests/geoapp/tests.py",
      "line": 303,
      "side": "RIGHT",
      "diff_hunk": "@@ -269,6 +272,47 @@ def test_empty_geometries(self):\n             self.assertEqual(feature.geom.srid, g.srid)\n \n \n+class SaveLoadTests(TestCase):\n+    def test_multilinestringfield(self):\n+        geom = MultiLineString(\n+            LineString((0, 0), (1, 1), (5, 5)),\n+            LineString((0, 0), (0, 5), (5, 5), (5, 0), (0, 0)),\n+        )\n+        obj = Lines.objects.create(geom=geom)\n+        obj.refresh_from_db()\n+        self.assertEqual(obj.geom.tuple, geom.tuple)\n+\n+    def test_multilinestring_with_linearring(self):\n+        geom = MultiLineString(\n+            LineString((0, 0), (1, 1), (5, 5)),\n+            LinearRing((0, 0), (0, 5), (5, 5), (5, 0), (0, 0)),\n+        )\n+        obj = Lines.objects.create(geom=geom)\n+        obj.refresh_from_db()\n+        self.assertEqual(obj.geom.tuple, geom.tuple)\n+        self.assertEqual(obj.geom[1].__class__.__name__, \"LineString\")\n+        self.assertEqual(obj.geom[0].tuple, geom[0].tuple)\n+        # LinearRings are transformed to LineString.\n+        self.assertEqual(obj.geom[1].__class__.__name__, \"LineString\")\n+        self.assertEqual(obj.geom[1].tuple, geom[1].tuple)\n+\n+    def test_multipointfield(self):\n+        geom = MultiPoint(Point(1, 1), Point(0, 0))\n+        obj = Points.objects.create(geom=geom)",
      "comment": "It is a bug in MariaDB, see 5a2490a19d81f6f2666dd90ce79e5724b9a20a39.",
      "comment_id": 2458949436,
      "user": "felixxm",
      "created_at": "2025-10-24T05:33:29Z",
      "url": "https://github.com/django/django/pull/19993#discussion_r2458949436"
    },
    {
      "repo": "django/django",
      "pr_number": 19993,
      "file_path": "tests/gis_tests/geoapp/tests.py",
      "line": 303,
      "side": "RIGHT",
      "diff_hunk": "@@ -269,6 +272,47 @@ def test_empty_geometries(self):\n             self.assertEqual(feature.geom.srid, g.srid)\n \n \n+class SaveLoadTests(TestCase):\n+    def test_multilinestringfield(self):\n+        geom = MultiLineString(\n+            LineString((0, 0), (1, 1), (5, 5)),\n+            LineString((0, 0), (0, 5), (5, 5), (5, 0), (0, 0)),\n+        )\n+        obj = Lines.objects.create(geom=geom)\n+        obj.refresh_from_db()\n+        self.assertEqual(obj.geom.tuple, geom.tuple)\n+\n+    def test_multilinestring_with_linearring(self):\n+        geom = MultiLineString(\n+            LineString((0, 0), (1, 1), (5, 5)),\n+            LinearRing((0, 0), (0, 5), (5, 5), (5, 0), (0, 0)),\n+        )\n+        obj = Lines.objects.create(geom=geom)\n+        obj.refresh_from_db()\n+        self.assertEqual(obj.geom.tuple, geom.tuple)\n+        self.assertEqual(obj.geom[1].__class__.__name__, \"LineString\")\n+        self.assertEqual(obj.geom[0].tuple, geom[0].tuple)\n+        # LinearRings are transformed to LineString.\n+        self.assertEqual(obj.geom[1].__class__.__name__, \"LineString\")\n+        self.assertEqual(obj.geom[1].tuple, geom[1].tuple)\n+\n+    def test_multipointfield(self):\n+        geom = MultiPoint(Point(1, 1), Point(0, 0))\n+        obj = Points.objects.create(geom=geom)",
      "comment": "Any pointer about Oracle failure?\r\n```\r\nTraceback (most recent call last):\r\n  File \"/tests/gis_tests/geoapp/tests.py\", line 315, in test_geometrycollectionfield\r\n    self.assertEqual(obj.geom, geom)\r\n    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\r\nAssertionError: <GeometryCollection object at 0x782e1769da40> != <GeometryCollection object at 0x782e1769dea0>\r\n```",
      "comment_id": 2467450335,
      "user": "timgraham",
      "created_at": "2025-10-27T23:35:53Z",
      "url": "https://github.com/django/django/pull/19993#discussion_r2467450335"
    },
    {
      "repo": "django/django",
      "pr_number": 19993,
      "file_path": "tests/gis_tests/geoapp/tests.py",
      "line": 303,
      "side": "RIGHT",
      "diff_hunk": "@@ -269,6 +272,47 @@ def test_empty_geometries(self):\n             self.assertEqual(feature.geom.srid, g.srid)\n \n \n+class SaveLoadTests(TestCase):\n+    def test_multilinestringfield(self):\n+        geom = MultiLineString(\n+            LineString((0, 0), (1, 1), (5, 5)),\n+            LineString((0, 0), (0, 5), (5, 5), (5, 0), (0, 0)),\n+        )\n+        obj = Lines.objects.create(geom=geom)\n+        obj.refresh_from_db()\n+        self.assertEqual(obj.geom.tuple, geom.tuple)\n+\n+    def test_multilinestring_with_linearring(self):\n+        geom = MultiLineString(\n+            LineString((0, 0), (1, 1), (5, 5)),\n+            LinearRing((0, 0), (0, 5), (5, 5), (5, 0), (0, 0)),\n+        )\n+        obj = Lines.objects.create(geom=geom)\n+        obj.refresh_from_db()\n+        self.assertEqual(obj.geom.tuple, geom.tuple)\n+        self.assertEqual(obj.geom[1].__class__.__name__, \"LineString\")\n+        self.assertEqual(obj.geom[0].tuple, geom[0].tuple)\n+        # LinearRings are transformed to LineString.\n+        self.assertEqual(obj.geom[1].__class__.__name__, \"LineString\")\n+        self.assertEqual(obj.geom[1].tuple, geom[1].tuple)\n+\n+    def test_multipointfield(self):\n+        geom = MultiPoint(Point(1, 1), Point(0, 0))\n+        obj = Points.objects.create(geom=geom)",
      "comment": "@felixxm ^^ can you speculate on the cause offhand? Some way that Oracle normalizes geometries perhaps?",
      "comment_id": 2562380114,
      "user": "timgraham",
      "created_at": "2025-11-26T01:34:55Z",
      "url": "https://github.com/django/django/pull/19993#discussion_r2562380114"
    },
    {
      "repo": "django/django",
      "pr_number": 19993,
      "file_path": "tests/gis_tests/geoapp/tests.py",
      "line": 303,
      "side": "RIGHT",
      "diff_hunk": "@@ -269,6 +272,47 @@ def test_empty_geometries(self):\n             self.assertEqual(feature.geom.srid, g.srid)\n \n \n+class SaveLoadTests(TestCase):\n+    def test_multilinestringfield(self):\n+        geom = MultiLineString(\n+            LineString((0, 0), (1, 1), (5, 5)),\n+            LineString((0, 0), (0, 5), (5, 5), (5, 0), (0, 0)),\n+        )\n+        obj = Lines.objects.create(geom=geom)\n+        obj.refresh_from_db()\n+        self.assertEqual(obj.geom.tuple, geom.tuple)\n+\n+    def test_multilinestring_with_linearring(self):\n+        geom = MultiLineString(\n+            LineString((0, 0), (1, 1), (5, 5)),\n+            LinearRing((0, 0), (0, 5), (5, 5), (5, 0), (0, 0)),\n+        )\n+        obj = Lines.objects.create(geom=geom)\n+        obj.refresh_from_db()\n+        self.assertEqual(obj.geom.tuple, geom.tuple)\n+        self.assertEqual(obj.geom[1].__class__.__name__, \"LineString\")\n+        self.assertEqual(obj.geom[0].tuple, geom[0].tuple)\n+        # LinearRings are transformed to LineString.\n+        self.assertEqual(obj.geom[1].__class__.__name__, \"LineString\")\n+        self.assertEqual(obj.geom[1].tuple, geom[1].tuple)\n+\n+    def test_multipointfield(self):\n+        geom = MultiPoint(Point(1, 1), Point(0, 0))\n+        obj = Points.objects.create(geom=geom)",
      "comment": "> @felixxm ^^ can you speculate on the cause offhand? Some way that Oracle normalizes geometries perhaps?\r\n\r\nTBH, I have no idea, but I can try to investigate it.",
      "comment_id": 2565342366,
      "user": "felixxm",
      "created_at": "2025-11-26T14:59:13Z",
      "url": "https://github.com/django/django/pull/19993#discussion_r2565342366"
    },
    {
      "repo": "django/django",
      "pr_number": 19993,
      "file_path": "tests/gis_tests/geoapp/tests.py",
      "line": 303,
      "side": "RIGHT",
      "diff_hunk": "@@ -269,6 +272,47 @@ def test_empty_geometries(self):\n             self.assertEqual(feature.geom.srid, g.srid)\n \n \n+class SaveLoadTests(TestCase):\n+    def test_multilinestringfield(self):\n+        geom = MultiLineString(\n+            LineString((0, 0), (1, 1), (5, 5)),\n+            LineString((0, 0), (0, 5), (5, 5), (5, 0), (0, 0)),\n+        )\n+        obj = Lines.objects.create(geom=geom)\n+        obj.refresh_from_db()\n+        self.assertEqual(obj.geom.tuple, geom.tuple)\n+\n+    def test_multilinestring_with_linearring(self):\n+        geom = MultiLineString(\n+            LineString((0, 0), (1, 1), (5, 5)),\n+            LinearRing((0, 0), (0, 5), (5, 5), (5, 0), (0, 0)),\n+        )\n+        obj = Lines.objects.create(geom=geom)\n+        obj.refresh_from_db()\n+        self.assertEqual(obj.geom.tuple, geom.tuple)\n+        self.assertEqual(obj.geom[1].__class__.__name__, \"LineString\")\n+        self.assertEqual(obj.geom[0].tuple, geom[0].tuple)\n+        # LinearRings are transformed to LineString.\n+        self.assertEqual(obj.geom[1].__class__.__name__, \"LineString\")\n+        self.assertEqual(obj.geom[1].tuple, geom[1].tuple)\n+\n+    def test_multipointfield(self):\n+        geom = MultiPoint(Point(1, 1), Point(0, 0))\n+        obj = Points.objects.create(geom=geom)",
      "comment": "It seems that the database is reversing the direction of the polygon:\r\n\r\n> (Pdb) geom.wkt\r\n'GEOMETRYCOLLECTION (POINT (2 2), LINESTRING (0 0, 2 2), POLYGON ((0 0, 0 5, 5 5, 5 0, 0 0)))'\r\n(Pdb) obj.geom.wkt\r\n'GEOMETRYCOLLECTION (POINT (2 2), LINESTRING (0 0, 2 2), POLYGON ((0 0, 5 0, 5 5, 0 5, 0 0)))'\r\n\r\nWe could change the test to:\r\n\r\n```python\r\n        self.assertIs(obj.geom.equals(geom), True)\r\n```\r\n\r\nI think this is ok as there are other tests in the file which use `.equals()` method for comparing geometries?",
      "comment_id": 2603683934,
      "user": "smithdc1",
      "created_at": "2025-12-09T17:44:58Z",
      "url": "https://github.com/django/django/pull/19993#discussion_r2603683934"
    },
    {
      "repo": "django/django",
      "pr_number": 19993,
      "file_path": "tests/gis_tests/geoapp/models.py",
      "line": 115,
      "side": "RIGHT",
      "diff_hunk": "@@ -102,3 +102,15 @@ class ManyPointModel(NamedModel):\n     point1 = models.PointField()\n     point2 = models.PointField()\n     point3 = models.PointField(srid=3857)\n+\n+\n+class Points(models.Model):\n+    geom = models.MultiPointField()\n+\n+\n+class Lines(models.Model):\n+    geom = models.MultiLineStringField()\n+\n+\n+class GeometryCollections(models.Model):",
      "comment": "Should this be singular and not plural? `GeometryCollections(geom=single_collection)` doesn't seem right, although I get `Lines(geom=lines)`.",
      "comment_id": 2604431680,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-09T21:29:07Z",
      "url": "https://github.com/django/django/pull/19993#discussion_r2604431680"
    },
    {
      "repo": "django/django",
      "pr_number": 19993,
      "file_path": "tests/gis_tests/geoapp/models.py",
      "line": 115,
      "side": "RIGHT",
      "diff_hunk": "@@ -102,3 +102,15 @@ class ManyPointModel(NamedModel):\n     point1 = models.PointField()\n     point2 = models.PointField()\n     point3 = models.PointField(srid=3857)\n+\n+\n+class Points(models.Model):\n+    geom = models.MultiPointField()\n+\n+\n+class Lines(models.Model):\n+    geom = models.MultiLineStringField()\n+\n+\n+class GeometryCollections(models.Model):",
      "comment": "I didn't want to collide with `geos.GeometryCollection`. Could go with `GeometryCollectionModel`.",
      "comment_id": 2604549893,
      "user": "timgraham",
      "created_at": "2025-12-09T22:15:19Z",
      "url": "https://github.com/django/django/pull/19993#discussion_r2604549893"
    },
    {
      "repo": "django/django",
      "pr_number": 20370,
      "file_path": "django/core/files/base.py",
      "line": 84,
      "side": "RIGHT",
      "diff_hunk": "@@ -78,26 +78,27 @@ def __iter__(self):\n         for chunk in self.chunks():\n             for line in chunk.splitlines(True):\n                 if buffer_:\n-                    if endswith_cr(buffer_) and not equals_lf(line):\n+                    if endswith_cr(buffer_[-1]) and not equals_lf(line):\n                         # Line split after a \\r newline; yield buffer_.\n-                        yield buffer_\n-                        # Continue with line.\n+                        yield type(buffer_[0])().join(buffer_)\n+                        buffer_ = None",
      "comment": "Let's try to keep the type consistent. Will require changes above & below.\n```suggestion\n                        buffer_ = []\n```",
      "comment_id": 2590154473,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-04T18:34:19Z",
      "url": "https://github.com/django/django/pull/20370#discussion_r2590154473"
    },
    {
      "repo": "django/django",
      "pr_number": 20086,
      "file_path": "django/contrib/admin/templatetags/base.py",
      "line": 20,
      "side": "RIGHT",
      "diff_hunk": "@@ -9,11 +10,20 @@ class InclusionAdminNode(InclusionNode):\n     or globally.\n     \"\"\"\n \n-    def __init__(self, parser, token, func, template_name, takes_context=True):\n+    def __init__(self, name, parser, token, func, template_name, takes_context=True):\n         self.template_name = template_name\n         params, varargs, varkw, defaults, kwonly, kwonly_defaults, _ = getfullargspec(\n             func\n         )\n+        if takes_context:",
      "comment": "Thought: This pattern is used quite a bit in this file - should it be extracted into a function?",
      "comment_id": 2526810573,
      "user": "RealOrangeOne",
      "created_at": "2025-11-14T09:56:00Z",
      "url": "https://github.com/django/django/pull/20086#discussion_r2526810573"
    },
    {
      "repo": "django/django",
      "pr_number": 20086,
      "file_path": "django/template/library.py",
      "line": 423,
      "side": "RIGHT",
      "diff_hunk": "@@ -391,22 +412,13 @@ def parse_bits(\n     defaults,\n     kwonly,\n     kwonly_defaults,\n-    takes_context,\n     name,\n ):\n     \"\"\"\n     Parse bits for template tag helpers simple_tag and inclusion_tag, in\n     particular by detecting syntax errors and by extracting positional and\n     keyword arguments.",
      "comment": "Should we clarify here that `context` should already be popped?",
      "comment_id": 2589982688,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-04T17:34:59Z",
      "url": "https://github.com/django/django/pull/20086#discussion_r2589982688"
    },
    {
      "repo": "django/django",
      "pr_number": 20074,
      "file_path": "tests/model_fields/test_integerfield.py",
      "line": 201,
      "side": "LEFT",
      "diff_hunk": "@@ -198,7 +198,7 @@ def test_invalid_value(self):\n             (TypeError, {}),\n             (TypeError, set()),\n             (TypeError, object()),\n-            (TypeError, complex()),",
      "comment": "This was raising \"The database backend does not accept 0 as a value for AutoField\" for me on MySQL. `complex() == 0` is `True`. (`IntegerFieldTests` is subclassed by auto field test cases.)",
      "comment_id": 2510541734,
      "user": "cliffordgama",
      "created_at": "2025-11-10T13:16:34Z",
      "url": "https://github.com/django/django/pull/20074#discussion_r2510541734"
    },
    {
      "repo": "django/django",
      "pr_number": 20074,
      "file_path": "django/db/models/fields/__init__.py",
      "line": 2851,
      "side": "RIGHT",
      "diff_hunk": "@@ -2810,10 +2810,13 @@ def validate(self, value, model_instance):\n         pass\n \n     def get_db_prep_value(self, value, connection, prepared=False):\n-        if not prepared:\n-            value = self.get_prep_value(value)\n-            value = connection.ops.validate_autopk_value(value)\n-        return value\n+        return value if prepared else self.get_prep_value(value)\n+\n+    def get_db_prep_save(self, value, connection):\n+        if hasattr(value, \"as_sql\"):\n+            return value\n+        value = connection.ops.validate_autopk_value(value)",
      "comment": "This no longer feeds get_prep_value() into validate_autopk_value(). Should we instead gather the result of `get_db_prep_value` and validate _that_?",
      "comment_id": 2586658762,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-03T21:26:44Z",
      "url": "https://github.com/django/django/pull/20074#discussion_r2586658762"
    },
    {
      "repo": "django/django",
      "pr_number": 20074,
      "file_path": "tests/model_fields/test_integerfield.py",
      "line": 201,
      "side": "LEFT",
      "diff_hunk": "@@ -198,7 +198,7 @@ def test_invalid_value(self):\n             (TypeError, {}),\n             (TypeError, set()),\n             (TypeError, object()),\n-            (TypeError, complex()),",
      "comment": "The original test with `complex()` passes with my suggestion to validate the prepped value.",
      "comment_id": 2586706569,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-03T21:43:00Z",
      "url": "https://github.com/django/django/pull/20074#discussion_r2586706569"
    },
    {
      "repo": "django/django",
      "pr_number": 19688,
      "file_path": "tests/admin_changelist/test_date_hierarchy.py",
      "line": 103,
      "side": "RIGHT",
      "diff_hunk": "@@ -95,3 +95,11 @@ def test_invalid_params(self):\n                 self.assertRaises(IncorrectLookupParameters),\n             ):\n                 self.assertDateParams(invalid_query, None, None)\n+\n+    @override_settings(ROOT_URLCONF=\"admin_changelist.urls\")\n+    def test_label_in_hierarchy(self):\n+        self.client.force_login(self.superuser)\n+        Event.objects.create(date=datetime(2017, 1, 1))\n+        response = self.client.get(\"/admin/admin_changelist/event/\")",
      "comment": "I see there\u2019s other date_hierarchy tests elsewhere but this seems like the right place and right test. But - shouldn\u2019t this be using `reverse` rather than hard-coded URL like other tests?\n\n",
      "comment_id": 2588936250,
      "user": "thibaudcolas",
      "created_at": "2025-12-04T12:46:06Z",
      "url": "https://github.com/django/django/pull/19688#discussion_r2588936250"
    },
    {
      "repo": "django/django",
      "pr_number": 19688,
      "file_path": "tests/admin_changelist/test_date_hierarchy.py",
      "line": 106,
      "side": "RIGHT",
      "diff_hunk": "@@ -95,3 +96,11 @@ def test_invalid_params(self):\n                 self.assertRaises(IncorrectLookupParameters),\n             ):\n                 self.assertDateParams(invalid_query, None, None)\n+\n+    @override_settings(ROOT_URLCONF=\"admin_changelist.urls\")\n+    def test_label_in_hierarchy(self):\n+        self.client.force_login(self.superuser)\n+        Event.objects.create(date=datetime(2017, 1, 1))\n+        response = self.client.get(reverse(\"admin:admin_changelist_event_changelist\"))\n+        self.assertEqual(response.status_code, 200)\n+        self.assertContains(response, \"Filter by\")",
      "comment": "I think instead of testing the template we could test that the view provided the dynamic content.",
      "comment_id": 2590577711,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-04T21:06:56Z",
      "url": "https://github.com/django/django/pull/19688#discussion_r2590577711"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "django/core/serializers/python.py",
      "line": 55,
      "side": "RIGHT",
      "diff_hunk": "@@ -34,8 +34,32 @@ def end_object(self, obj):\n \n     def get_dump_object(self, obj):\n         data = {\"model\": str(obj._meta)}\n-        if not self.use_natural_primary_keys or not hasattr(obj, \"natural_key\"):\n+\n+        pk_included = True\n+\n+        if self.use_natural_primary_keys:\n+            natural_key_func = getattr(obj, \"natural_key\", None)\n+\n+            if callable(natural_key_func):\n+                natural_key_value = None\n+                try:\n+                    natural_key_value = natural_key_func()\n+                except Exception:\n+                    pass\n+\n+                is_opt_out = (\n+                    natural_key_value is None\n+                    or not natural_key_value\n+                    or natural_key_value == (obj.pk,)\n+                    or not isinstance(natural_key_value, tuple)\n+                )",
      "comment": "This handles a lot of variance. Can we just use implicit booleanness? (`not natural_key_value`). This whole block will likely become shorter.",
      "comment_id": 2441068956,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-17T19:46:27Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2441068956"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "django/core/serializers/python.py",
      "line": 48,
      "side": "RIGHT",
      "diff_hunk": "@@ -34,8 +34,32 @@ def end_object(self, obj):\n \n     def get_dump_object(self, obj):\n         data = {\"model\": str(obj._meta)}\n-        if not self.use_natural_primary_keys or not hasattr(obj, \"natural_key\"):\n+\n+        pk_included = True\n+\n+        if self.use_natural_primary_keys:\n+            natural_key_func = getattr(obj, \"natural_key\", None)\n+\n+            if callable(natural_key_func):\n+                natural_key_value = None\n+                try:\n+                    natural_key_value = natural_key_func()\n+                except Exception:\n+                    pass",
      "comment": "Let's remove the except (if the callable raises, that's not our problem).",
      "comment_id": 2441069386,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-17T19:46:41Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2441069386"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "django/core/serializers/python.py",
      "line": 88,
      "side": "RIGHT",
      "diff_hunk": "@@ -57,7 +81,16 @@ def handle_fk_field(self, obj, field):\n         ):\n             related = getattr(obj, field.name)\n             if related:\n-                value = related.natural_key()\n+                natural_key_value = related.natural_key()\n+\n+                is_opt_out = (natural_key_value is None) or (\n+                    natural_key_value == (related.pk,)\n+                )",
      "comment": "This isn't identical with the other check, where non-tuples or 0-tuples are handled.",
      "comment_id": 2441072154,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-17T19:48:17Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2441072154"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "django/core/serializers/xml_serializer.py",
      "line": 59,
      "side": "RIGHT",
      "diff_hunk": "@@ -53,7 +53,32 @@ def start_object(self, obj):\n \n         self.indent(1)\n         attrs = {\"model\": str(obj._meta)}\n-        if not self.use_natural_primary_keys or not hasattr(obj, \"natural_key\"):\n+\n+        pk_included = True\n+\n+        if self.use_natural_primary_keys:",
      "comment": "The duplication here is starting to show that we should factor out a helper as I mused on the ticket :-)",
      "comment_id": 2441073531,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-17T19:49:09Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2441073531"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "django/core/serializers/python.py",
      "line": 53,
      "side": "RIGHT",
      "diff_hunk": "@@ -34,8 +34,32 @@ def end_object(self, obj):\n \n     def get_dump_object(self, obj):\n         data = {\"model\": str(obj._meta)}\n-        if not self.use_natural_primary_keys or not hasattr(obj, \"natural_key\"):\n+\n+        pk_included = True\n+\n+        if self.use_natural_primary_keys:\n+            natural_key_func = getattr(obj, \"natural_key\", None)\n+\n+            if callable(natural_key_func):\n+                natural_key_value = None\n+                try:\n+                    natural_key_value = natural_key_func()\n+                except Exception:\n+                    pass\n+\n+                is_opt_out = (\n+                    natural_key_value is None\n+                    or not natural_key_value\n+                    or natural_key_value == (obj.pk,)",
      "comment": "A 1-tuple of just `\"pk\"` feels like a little bit of a hack / contradiction. If we don't add support for this, then we can just use implicit booleanness (truthiness).",
      "comment_id": 2441075243,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-17T19:50:06Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2441075243"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "django/core/serializers/xml_serializer.py",
      "line": 140,
      "side": "RIGHT",
      "diff_hunk": "@@ -113,12 +138,17 @@ def handle_fk_field(self, obj, field):\n             ):\n                 related = getattr(obj, field.name)\n                 # If related object has a natural key, use it\n-                related = related.natural_key()\n-                # Iterable natural keys are rolled out as subelements\n-                for key_value in related:\n-                    self.xml.startElement(\"natural\", {})\n-                    self.xml.characters(str(key_value))\n-                    self.xml.endElement(\"natural\")\n+                natural_key_value = related.natural_key()\n+                is_pk_tuple = natural_key_value == (related.pk,)\n+\n+                if natural_key_value is None or is_pk_tuple:\n+                    self.xml.characters(str(related_att))\n+                else:\n+                    # Iterable natural keys are rolled out as subelements\n+                    for key_value in natural_key_value:\n+                        self.xml.startElement(\"natural\", {})\n+                        self.xml.characters(str(key_value))\n+                        self.xml.endElement(\"natural\")\n             else:\n                 self.xml.characters(str(related_att))",
      "comment": "There's another `hasattr(..., \"natural_key\")` to update in `build_instance` in django/core/serializers.py.",
      "comment_id": 2441078639,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-17T19:51:50Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2441078639"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "django/core/serializers/python.py",
      "line": 59,
      "side": "RIGHT",
      "diff_hunk": "@@ -57,7 +81,16 @@ def handle_fk_field(self, obj, field):\n         ):\n             related = getattr(obj, field.name)\n             if related:\n-                value = related.natural_key()\n+                natural_key_value = related.natural_key()\n+\n+                is_opt_out = (natural_key_value is None) or (\n+                    natural_key_value == (related.pk,)\n+                )\n+\n+                if is_opt_out:\n+                    value = self._value_from_field(obj, field)\n+                else:\n+                    value = natural_key_value\n             else:\n                 value = None\n         else:",
      "comment": "There's a 3rd location in this file with a similar need (`handle_m2m_field`).",
      "comment_id": 2441081478,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-17T19:53:15Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2441081478"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "tests/serializers/models/natural.py",
      "line": 80,
      "side": "RIGHT",
      "diff_hunk": "@@ -74,3 +75,43 @@ class FKAsPKNoNaturalKey(models.Model):\n \n     def natural_key(self):\n         raise NotImplementedError(\"This method was not expected to be called.\")\n+\n+\n+class PKTupleOptOutUser(AbstractBaseUser):",
      "comment": "Let's replace these models with a single one using a 0-tuple. Suggest renaming the model to mention subclass, not the tuple detail.",
      "comment_id": 2441094991,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-17T20:00:18Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2441094991"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "tests/serializers/test_natural.py",
      "line": 266,
      "side": "RIGHT",
      "diff_hunk": "@@ -250,6 +256,78 @@ def fk_as_pk_natural_key_not_called(self, format):\n         self.assertEqual(obj.object.pk, o1.pk)\n \n \n+def natural_key_opt_out_test(self, format):\n+    # Refs #35729\n+    \"\"\"\n+    Tests natural key opt-out scenarios for models inheriting\n+    from AbstractBaseUser.\n+\n+    Verifies that when natural_key() returns None or (self.pk,):\n+    \"\"\"",
      "comment": "This is a fun one: our coding style docs advises against \"tests...\" \"verifies that...\" [preambles](https://docs.djangoproject.com/en/5.2/internals/contributing/writing-code/coding-style/#:~:text=preambles%20such%20as%20%E2%80%9CTests%20that%E2%80%9D%20or%20%E2%80%9CEnsures%20that%E2%80%9D.).",
      "comment_id": 2441098431,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-17T20:02:06Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2441098431"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "tests/serializers/test_natural.py",
      "line": 321,
      "side": "RIGHT",
      "diff_hunk": "@@ -250,6 +256,78 @@ def fk_as_pk_natural_key_not_called(self, format):\n         self.assertEqual(obj.object.pk, o1.pk)\n \n \n+def natural_key_opt_out_test(self, format):\n+    # Refs #35729\n+    \"\"\"\n+    Tests natural key opt-out scenarios for models inheriting\n+    from AbstractBaseUser.\n+\n+    Verifies that when natural_key() returns None or (self.pk,):\n+    \"\"\"\n+\n+    user1 = PKTupleOptOutUser.objects.create(email=\"user1@example.com\")\n+    post1 = PostToPKTupleUser.objects.create(author=user1, title=\"Post 1 (PK Opt-out)\")\n+\n+    user2 = NoneOptOutUser.objects.create(email=\"user2@example.com\")\n+    post2 = PostToNoneUser.objects.create(author=user2, title=\"Post 2 (None Opt-out)\")\n+    scenarios = [\n+        (user1, post1),\n+        (user2, post2),\n+    ]\n+\n+    is_json = format == \"json\"\n+\n+    for user, post in scenarios:\n+        user_pk = user.pk\n+\n+        user_data_str = serializers.serialize(\n+            format, [user], indent=2, use_natural_primary_keys=True\n+        )\n+\n+        if is_json:\n+            user_data = json.loads(user_data_str)\n+            serialized_user = user_data[0]\n+\n+            self.assertEqual(\n+                serialized_user.get(\"pk\"),\n+                user_pk,\n+                f\"[{user.__class__.__name__}] PK must be included \"\n+                f\"in the primary object after patch.\",\n+            )\n+\n+        post_data_str = serializers.serialize(\n+            format,\n+            [post],\n+            indent=2,\n+            use_natural_foreign_keys=True,\n+        )\n+\n+        if is_json:\n+            post_data = json.loads(post_data_str)\n+            serialized_post = post_data[0]\n+\n+            self.assertEqual(\n+                serialized_post[\"fields\"][\"author\"],\n+                user_pk,\n+                f\"[{user.__class__.__name__}] FK must be serialized as PK integer\"\n+                f\", not a natural key list.\",\n+            )\n+\n+        try:\n+            list(serializers.deserialize(format, user_data_str))\n+        except Exception as e:\n+            self.fail(\n+                f\"Deserialization of {user.__class__.__name__} failed in {format}: {e}\"\n+            )",
      "comment": "We let exceptions escape naturally instead of `self.fail`ing. If you need additional messaging, use the last argument to `self.assert...`",
      "comment_id": 2441099917,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-17T20:02:50Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2441099917"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "tests/serializers/test_natural.py",
      "line": 313,
      "side": "RIGHT",
      "diff_hunk": "@@ -250,6 +256,78 @@ def fk_as_pk_natural_key_not_called(self, format):\n         self.assertEqual(obj.object.pk, o1.pk)\n \n \n+def natural_key_opt_out_test(self, format):\n+    # Refs #35729\n+    \"\"\"\n+    Tests natural key opt-out scenarios for models inheriting\n+    from AbstractBaseUser.\n+\n+    Verifies that when natural_key() returns None or (self.pk,):\n+    \"\"\"\n+\n+    user1 = PKTupleOptOutUser.objects.create(email=\"user1@example.com\")\n+    post1 = PostToPKTupleUser.objects.create(author=user1, title=\"Post 1 (PK Opt-out)\")\n+\n+    user2 = NoneOptOutUser.objects.create(email=\"user2@example.com\")\n+    post2 = PostToNoneUser.objects.create(author=user2, title=\"Post 2 (None Opt-out)\")\n+    scenarios = [\n+        (user1, post1),\n+        (user2, post2),\n+    ]\n+\n+    is_json = format == \"json\"\n+\n+    for user, post in scenarios:\n+        user_pk = user.pk\n+\n+        user_data_str = serializers.serialize(\n+            format, [user], indent=2, use_natural_primary_keys=True\n+        )\n+\n+        if is_json:\n+            user_data = json.loads(user_data_str)\n+            serialized_user = user_data[0]\n+\n+            self.assertEqual(\n+                serialized_user.get(\"pk\"),\n+                user_pk,\n+                f\"[{user.__class__.__name__}] PK must be included \"\n+                f\"in the primary object after patch.\",\n+            )\n+\n+        post_data_str = serializers.serialize(\n+            format,\n+            [post],\n+            indent=2,\n+            use_natural_foreign_keys=True,\n+        )\n+\n+        if is_json:\n+            post_data = json.loads(post_data_str)\n+            serialized_post = post_data[0]\n+\n+            self.assertEqual(\n+                serialized_post[\"fields\"][\"author\"],\n+                user_pk,\n+                f\"[{user.__class__.__name__}] FK must be serialized as PK integer\"\n+                f\", not a natural key list.\",",
      "comment": "Only running these tests if `is_json` kind of cuts against the design of the surrounding tests, which test export & import instead of asserting on the exact output.\n\nWhile I like the explicitness of this, I wonder if it would \"blend in\" to demonstrate a roundtrip instead. This would involve demonstrating when deserializing that objects serialized with integer pk's instead of natural keys cause pk collisions when those integer pk's already exist, but not otherwise.",
      "comment_id": 2441121730,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-17T20:12:13Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2441121730"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "django/core/serializers/xml_serializer.py",
      "line": 59,
      "side": "RIGHT",
      "diff_hunk": "@@ -53,7 +53,32 @@ def start_object(self, obj):\n \n         self.indent(1)\n         attrs = {\"model\": str(obj._meta)}\n-        if not self.use_natural_primary_keys or not hasattr(obj, \"natural_key\"):\n+\n+        pk_included = True\n+\n+        if self.use_natural_primary_keys:",
      "comment": "Thanks for the suggestion!\r\nSince all format serializers are likely to follow that pattern,\r\nI'm planning to factor out the helper into `django/core/serializers/base.py` as a method of the base Serializer.\r\nDoes that location sound reasonable to you?",
      "comment_id": 2459797774,
      "user": "rimi0108",
      "created_at": "2025-10-24T10:55:48Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2459797774"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "django/core/serializers/python.py",
      "line": 53,
      "side": "RIGHT",
      "diff_hunk": "@@ -34,8 +34,32 @@ def end_object(self, obj):\n \n     def get_dump_object(self, obj):\n         data = {\"model\": str(obj._meta)}\n-        if not self.use_natural_primary_keys or not hasattr(obj, \"natural_key\"):\n+\n+        pk_included = True\n+\n+        if self.use_natural_primary_keys:\n+            natural_key_func = getattr(obj, \"natural_key\", None)\n+\n+            if callable(natural_key_func):\n+                natural_key_value = None\n+                try:\n+                    natural_key_value = natural_key_func()\n+                except Exception:\n+                    pass\n+\n+                is_opt_out = (\n+                    natural_key_value is None\n+                    or not natural_key_value\n+                    or natural_key_value == (obj.pk,)",
      "comment": "I'd like to confirm the opt-out contract.\r\n\r\n```\r\nclass PKTupleOptOutUser(AbstractBaseUser):\r\n    email = models.EmailField(unique=False, null=True, blank=True)\r\n    USERNAME_FIELD = \"email\"\r\n    REQUIRED_FIELDS = []\r\n\r\n    class Manager(models.Manager):\r\n        def get_by_natural_key(self, pk_value):\r\n            return self.get(pk=pk_value)\r\n\r\n    objects = Manager()\r\n\r\n    def natural_key(self):\r\n        return (self.pk,)\r\n```\r\n\r\nWould it be correct to treat only returning a falsy value (such as None or an empty tuple ()) as the explicit opt-out signal, and not treat cases like (self.pk,) \u2014 as in the example above \u2014 as opt-out?",
      "comment_id": 2459927371,
      "user": "rimi0108",
      "created_at": "2025-10-24T11:30:03Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2459927371"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "django/core/serializers/python.py",
      "line": 53,
      "side": "RIGHT",
      "diff_hunk": "@@ -34,8 +34,32 @@ def end_object(self, obj):\n \n     def get_dump_object(self, obj):\n         data = {\"model\": str(obj._meta)}\n-        if not self.use_natural_primary_keys or not hasattr(obj, \"natural_key\"):\n+\n+        pk_included = True\n+\n+        if self.use_natural_primary_keys:\n+            natural_key_func = getattr(obj, \"natural_key\", None)\n+\n+            if callable(natural_key_func):\n+                natural_key_value = None\n+                try:\n+                    natural_key_value = natural_key_func()\n+                except Exception:\n+                    pass\n+\n+                is_opt_out = (\n+                    natural_key_value is None\n+                    or not natural_key_value\n+                    or natural_key_value == (obj.pk,)",
      "comment": "That's right. As we progress the review, we might even remove support for `None`, since it could cause problems with uses like `.get_by_natural_key(*obj.natural_key())`.",
      "comment_id": 2461619895,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-24T18:35:10Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2461619895"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "django/core/serializers/python.py",
      "line": 53,
      "side": "RIGHT",
      "diff_hunk": "@@ -34,8 +34,32 @@ def end_object(self, obj):\n \n     def get_dump_object(self, obj):\n         data = {\"model\": str(obj._meta)}\n-        if not self.use_natural_primary_keys or not hasattr(obj, \"natural_key\"):\n+\n+        pk_included = True\n+\n+        if self.use_natural_primary_keys:\n+            natural_key_func = getattr(obj, \"natural_key\", None)\n+\n+            if callable(natural_key_func):\n+                natural_key_value = None\n+                try:\n+                    natural_key_value = natural_key_func()\n+                except Exception:\n+                    pass\n+\n+                is_opt_out = (\n+                    natural_key_value is None\n+                    or not natural_key_value\n+                    or natural_key_value == (obj.pk,)",
      "comment": "Got it, that makes sense.\r\nI\u2019ll keep the current None handling for this PR, and we can revisit its removal in a follow-up change if needed!",
      "comment_id": 2463322746,
      "user": "rimi0108",
      "created_at": "2025-10-26T08:30:50Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2463322746"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "django/core/serializers/base.py",
      "line": 232,
      "side": "RIGHT",
      "diff_hunk": "@@ -209,6 +209,29 @@ def getvalue(self):\n         if callable(getattr(self.stream, \"getvalue\", None)):\n             return self.stream.getvalue()\n \n+    def _should_include_pk(self, obj):\n+        \"\"\"\n+        Determines whether the Primary Key (PK) should be included in the\n+        serialized data for the given object, especially when\n+        use_natural_primary_keys=True is set.\n+        \"\"\"\n+        pk_included = True\n+\n+        if self.use_natural_primary_keys:\n+            natural_key_func = getattr(obj, \"natural_key\", None)\n+\n+            if callable(natural_key_func):\n+                natural_key_value = natural_key_func()\n+\n+                is_opt_out = not natural_key_value or not isinstance(\n+                    natural_key_value, tuple\n+                )\n+\n+                if not is_opt_out:\n+                    pk_included = False\n+",
      "comment": "Could this logic be flipped around and cleaned up w/ Demorgan's Law?\r\n\r\n```\r\nif callable(natural_key_func):\r\n    natural_key_value = natural_key_func()\r\n    if natural_key_value and isinstance(natural_key_value, tuple):\r\n        pk_included = False\r\n```",
      "comment_id": 2479716275,
      "user": "seanhelvey",
      "created_at": "2025-10-30T22:55:33Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2479716275"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "django/core/serializers/base.py",
      "line": 375,
      "side": "RIGHT",
      "diff_hunk": "@@ -317,12 +369,13 @@ def build_instance(Model, data, db):\n         obj = Model(**data)\n         obj._state.db = db\n         natural_key = obj.natural_key()\n-        try:\n-            data[Model._meta.pk.attname] = Model._meta.pk.to_python(\n-                default_manager.db_manager(db).get_by_natural_key(*natural_key).pk\n-            )\n-        except Model.DoesNotExist:\n-            pass\n+        if natural_key is not None:\n+            try:\n+                data[Model._meta.pk.attname] = Model._meta.pk.to_python(\n+                    default_manager.db_manager(db).get_by_natural_key(*natural_key).pk",
      "comment": "Maybe less complex if the value passed into to_python is processed first and then passed in?\r\n\r\n```\r\nobj = default_manager.db_manager(db).get_by_natural_key(*natural_key)\r\ndata[Model._meta.pk.attname] = Model._meta.pk.to_python(obj.pk)\r\n```",
      "comment_id": 2525223828,
      "user": "seanhelvey",
      "created_at": "2025-11-13T23:13:10Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2525223828"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "django/core/serializers/base.py",
      "line": 253,
      "side": "RIGHT",
      "diff_hunk": "@@ -209,6 +209,54 @@ def getvalue(self):\n         if callable(getattr(self.stream, \"getvalue\", None)):\n             return self.stream.getvalue()\n \n+    def _should_include_pk(self, obj):\n+        \"\"\"\n+        Determines whether the Primary Key (PK) should be included in the\n+        serialized data for the given object, especially when\n+        use_natural_primary_keys=True is set.\n+        \"\"\"\n+        if not self.use_natural_primary_keys:\n+            return True\n+\n+        return not self._resolve_natural_key(obj)\n+\n+    def _resolve_fk_natural_key(self, obj, field):\n+        \"\"\"\n+        Return the natural key for a ForeignKey's related object when valid.\n+\n+        When natural foreign keys are enabled, the helper attempts to use the\n+        related object's natural key; otherwise it falls back to the PK.\n+        \"\"\"\n+        if not self.use_natural_foreign_keys:\n+            return None\n+\n+        if not self._model_supports_natural_key(field.remote_field.model):\n+            return None\n+\n+        related = getattr(obj, field.name, None)\n+        return self._resolve_natural_key(related)\n+\n+    def _resolve_natural_key(self, obj):\n+        \"\"\"\n+        Return a natural key tuple for the given object when available.\n+        \"\"\"\n+        if not (obj and callable(getattr(obj, \"natural_key\", None))):\n+            return None\n+\n+        natural_key_value = obj.natural_key()\n+        if self._is_natural_key_opt_out(natural_key_value):\n+            return None\n+\n+        return natural_key_value\n+\n+    def _is_natural_key_opt_out(self, natural_key_value):\n+        return not natural_key_value or not isinstance(natural_key_value, tuple)",
      "comment": "```suggestion\n        return not natural_key_value\n```\n\nLists worked before via duck-typing, so let's make sure we don't remove support.\n\nGiven that, you probably don't need this helper.",
      "comment_id": 2539329637,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-18T18:56:29Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2539329637"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "django/core/serializers/base.py",
      "line": 369,
      "side": "RIGHT",
      "diff_hunk": "@@ -309,20 +357,23 @@ def build_instance(Model, data, db):\n     \"\"\"\n     default_manager = Model._meta.default_manager\n     pk = data.get(Model._meta.pk.attname)\n+    natural_key_method = getattr(Model, \"natural_key\", None)\n     if (\n         pk is None\n         and hasattr(default_manager, \"get_by_natural_key\")\n-        and hasattr(Model, \"natural_key\")\n+        and callable(natural_key_method)\n     ):\n         obj = Model(**data)\n         obj._state.db = db\n         natural_key = obj.natural_key()\n-        try:\n-            data[Model._meta.pk.attname] = Model._meta.pk.to_python(\n-                default_manager.db_manager(db).get_by_natural_key(*natural_key).pk\n-            )\n-        except Model.DoesNotExist:\n-            pass\n+        if natural_key:",
      "comment": "Carrying over a thread from the last review, since we're not suggesting to support `None`, we don't have to touch these lines. Let's pursue that and revert the changes to these lines.",
      "comment_id": 2539425972,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-18T19:30:13Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2539425972"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "django/core/serializers/python.py",
      "line": 64,
      "side": "RIGHT",
      "diff_hunk": "@@ -52,43 +56,42 @@ def handle_field(self, obj, field):\n         self._current[field.name] = self._value_from_field(obj, field)\n \n     def handle_fk_field(self, obj, field):\n-        if self.use_natural_foreign_keys and hasattr(\n-            field.remote_field.model, \"natural_key\"\n-        ):\n-            related = getattr(obj, field.name)\n-            if related:\n-                value = related.natural_key()\n+        if self.use_natural_foreign_keys:\n+            natural_key_value = self._resolve_fk_natural_key(obj, field)\n+            if natural_key_value is not None:\n+                value = natural_key_value\n             else:\n-                value = None\n+                value = self._value_from_field(obj, field)",
      "comment": "I think this simplifies to the following since `_resolve_fk_natural_key()` also checks `self.use_natural_foreign_keys`:\n```suggestion\n        if natural_key_value := self._resolve_fk_natural_key(obj, field):\n            value = natural_key_value\n```",
      "comment_id": 2539481919,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-18T19:46:29Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2539481919"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "django/core/serializers/xml_serializer.py",
      "line": 156,
      "side": "LEFT",
      "diff_hunk": "@@ -133,36 +132,39 @@ def handle_m2m_field(self, obj, field):\n         \"\"\"\n         if field.remote_field.through._meta.auto_created:\n             self._start_relational_field(field)\n-            if self.use_natural_foreign_keys and hasattr(",
      "comment": "Can you reduce this diff by keeping the original if else branches and just replacing the first if with `if use_natural`?",
      "comment_id": 2539555687,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-18T20:06:03Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2539555687"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "django/core/serializers/python.py",
      "line": 68,
      "side": "RIGHT",
      "diff_hunk": "@@ -52,43 +56,42 @@ def handle_field(self, obj, field):\n         self._current[field.name] = self._value_from_field(obj, field)\n \n     def handle_fk_field(self, obj, field):\n-        if self.use_natural_foreign_keys and hasattr(\n-            field.remote_field.model, \"natural_key\"\n-        ):\n-            related = getattr(obj, field.name)\n-            if related:\n-                value = related.natural_key()\n+        if self.use_natural_foreign_keys:\n+            natural_key_value = self._resolve_fk_natural_key(obj, field)\n+            if natural_key_value is not None:\n+                value = natural_key_value\n             else:\n-                value = None\n+                value = self._value_from_field(obj, field)\n         else:\n             value = self._value_from_field(obj, field)\n         self._current[field.name] = value\n \n     def handle_m2m_field(self, obj, field):\n         if field.remote_field.through._meta.auto_created:\n-            if self.use_natural_foreign_keys and hasattr(\n-                field.remote_field.model, \"natural_key\"\n-            ):\n+            use_natural = (",
      "comment": "Same here, it would be helpful to mix in fewer refactors here. Let's keep the if/else branches as they were and only insert the changes you need.",
      "comment_id": 2539558074,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-18T20:06:36Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2539558074"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "django/core/serializers/xml_serializer.py",
      "line": 59,
      "side": "RIGHT",
      "diff_hunk": "@@ -53,7 +53,10 @@ def start_object(self, obj):\n \n         self.indent(1)\n         attrs = {\"model\": str(obj._meta)}\n-        if not self.use_natural_primary_keys or not hasattr(obj, \"natural_key\"):\n+\n+        pk_included = self._should_include_pk(obj)\n+\n+        if pk_included:",
      "comment": "This method name is named well enough to just avoid the assignment:\n```suggestion\n        if self._should_include_pk(obj):\n```",
      "comment_id": 2539562562,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-18T20:07:42Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2539562562"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "django/core/serializers/python.py",
      "line": 42,
      "side": "RIGHT",
      "diff_hunk": "@@ -34,8 +34,12 @@ def end_object(self, obj):\n \n     def get_dump_object(self, obj):\n         data = {\"model\": str(obj._meta)}\n-        if not self.use_natural_primary_keys or not hasattr(obj, \"natural_key\"):\n+\n+        pk_included = self._should_include_pk(obj)\n+\n+        if pk_included:\n             data[\"pk\"] = self._value_from_field(obj, obj._meta.pk)\n+",
      "comment": "```suggestion\n        if pk_included := self._should_include_pk(obj):\n            data[\"pk\"] = self._value_from_field(obj, obj._meta.pk)\n```",
      "comment_id": 2539569451,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-18T20:08:59Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2539569451"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "tests/serializers/test_natural.py",
      "line": 263,
      "side": "RIGHT",
      "diff_hunk": "@@ -250,6 +253,51 @@ def fk_as_pk_natural_key_not_called(self, format):\n         self.assertEqual(obj.object.pk, o1.pk)\n \n \n+def natural_key_opt_out_test(self, format):\n+    \"\"\"\n+    When a subclass of AbstractBaseUser opts out of natural key serialization\n+    by returning an empty tuple, both FK and M2M relations serialize as\n+    integer PKs and can be deserialized without error.\n+    \"\"\"\n+    user1 = SubclassNaturalKeyOptOutUser.objects.create(email=\"user2@example.com\")\n+    user2 = SubclassNaturalKeyOptOutUser.objects.create(email=\"user3@example.com\")",
      "comment": "left-hand side doesn't match the numbers in the email addresses.",
      "comment_id": 2539658297,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-18T20:41:27Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2539658297"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "django/core/serializers/base.py",
      "line": 374,
      "side": "RIGHT",
      "diff_hunk": "@@ -309,20 +357,23 @@ def build_instance(Model, data, db):\n     \"\"\"\n     default_manager = Model._meta.default_manager\n     pk = data.get(Model._meta.pk.attname)\n+    natural_key_method = getattr(Model, \"natural_key\", None)\n     if (\n         pk is None\n         and hasattr(default_manager, \"get_by_natural_key\")\n-        and hasattr(Model, \"natural_key\")\n+        and callable(natural_key_method)\n     ):\n         obj = Model(**data)\n         obj._state.db = db\n         natural_key = obj.natural_key()\n-        try:\n-            data[Model._meta.pk.attname] = Model._meta.pk.to_python(\n-                default_manager.db_manager(db).get_by_natural_key(*natural_key).pk\n-            )\n-        except Model.DoesNotExist:\n-            pass\n+        if natural_key:\n+            try:\n+                existing = default_manager.db_manager(db).get_by_natural_key(\n+                    *natural_key\n+                )\n+                data[Model._meta.pk.attname] = Model._meta.pk.to_python(existing.pk)",
      "comment": "If we're taking the opportunity to factor this line out, then let's move it out of the `try` and under an `else`.",
      "comment_id": 2539681336,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-18T21:03:14Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2539681336"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "tests/serializers/test_natural.py",
      "line": 296,
      "side": "RIGHT",
      "diff_hunk": "@@ -250,6 +253,51 @@ def fk_as_pk_natural_key_not_called(self, format):\n         self.assertEqual(obj.object.pk, o1.pk)\n \n \n+def natural_key_opt_out_test(self, format):\n+    \"\"\"\n+    When a subclass of AbstractBaseUser opts out of natural key serialization\n+    by returning an empty tuple, both FK and M2M relations serialize as\n+    integer PKs and can be deserialized without error.\n+    \"\"\"\n+    user1 = SubclassNaturalKeyOptOutUser.objects.create(email=\"user2@example.com\")\n+    user2 = SubclassNaturalKeyOptOutUser.objects.create(email=\"user3@example.com\")\n+\n+    post = PostToOptOutSubclassUser.objects.create(\n+        author=user1, title=\"Post 2 (Subclass Opt-out)\"\n+    )\n+    post.subscribers.add(user1, user2)\n+\n+    user_data = serializers.serialize(format, [user1], use_natural_primary_keys=True)\n+    post_data = serializers.serialize(format, [post], use_natural_foreign_keys=True)\n+\n+    list(serializers.deserialize(format, user_data))\n+    deserialized_posts = list(serializers.deserialize(format, post_data))\n+\n+    post_obj = deserialized_posts[0].object\n+    self.assertEqual(user1.email, post_obj.author.email)\n+    self.assertEqual(\n+        sorted([user1.email, user2.email]),\n+        sorted(post_obj.subscribers.values_list(\"email\", flat=True)),\n+    )\n+\n+\n+def deserialize_natural_key_then_opted_out(self, format):\n+    \"\"\"\n+    Deserialization remains backward compatible:\n+    data serialized with natural keys continues to load correctly\n+    after the model opts out by returning empty tuple from natural_key().\n+    \"\"\"\n+    user = NaturalKeyOptOut.objects.create(name=\"example\")\n+    serialized = serializers.serialize(format, [user], use_natural_primary_keys=True)\n+\n+    def optout_natural_key(self):\n+        return ()\n+\n+    NaturalKeyOptOut.natural_key = optout_natural_key",
      "comment": "This test doesn't seem very realistic -- I don't think we want to set the expectation that you can change the natural key and load data you dumped before without problems. I think we can just remove it; then we don't have to worry about polishing this test. (I was about to suggest things like making sure state was restored after the test, since we're altering the class during the test, but it will be nice not to worry about that!)\n\nIf that leaves a coverage gap somewhere, let me know, and we can develop a different test.",
      "comment_id": 2539690375,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-18T21:10:40Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2539690375"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "django/core/serializers/xml_serializer.py",
      "line": 143,
      "side": "RIGHT",
      "diff_hunk": "@@ -133,12 +130,19 @@ def handle_m2m_field(self, obj, field):\n         \"\"\"\n         if field.remote_field.through._meta.auto_created:\n             self._start_relational_field(field)\n-            if self.use_natural_foreign_keys and hasattr(\n-                field.remote_field.model, \"natural_key\"\n-            ):\n+\n+            use_natural = (\n+                self.use_natural_foreign_keys\n+                and self._model_supports_natural_key(field.remote_field.model)\n+            )\n+\n+            if use_natural:\n                 # If the objects in the m2m have a natural key, use it\n                 def handle_m2m(value):\n-                    natural = value.natural_key()\n+                    natural = self._resolve_natural_key(value)\n+                    if not natural:",
      "comment": "If we move this boolean test into your helper, then we can avoid adding this branch and ensure we aren't missing analogous changes for the `queryset_iterator` nested function.",
      "comment_id": 2582370428,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-02T18:30:52Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2582370428"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "django/core/serializers/base.py",
      "line": 228,
      "side": "RIGHT",
      "diff_hunk": "@@ -209,6 +209,51 @@ def getvalue(self):\n         if callable(getattr(self.stream, \"getvalue\", None)):\n             return self.stream.getvalue()\n \n+    def _should_include_pk(self, obj):\n+        \"\"\"\n+        Determines whether the Primary Key (PK) should be included in the\n+        serialized data for the given object, especially when\n+        use_natural_primary_keys=True is set.\n+        \"\"\"\n+        if not self.use_natural_primary_keys:\n+            return True\n+\n+        return not self._resolve_natural_key(obj)\n+\n+    def _resolve_fk_natural_key(self, obj, field):\n+        \"\"\"\n+        Return the natural key for a ForeignKey's related object when valid.\n+\n+        When natural foreign keys are enabled, the helper attempts to use the\n+        related object's natural key; otherwise it falls back to the PK.",
      "comment": "I'm not seeing where this fallback behavior is implemented. I'll take a quick stab at simplifying the helpers a little further.",
      "comment_id": 2582486319,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-02T19:08:45Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2582486319"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "django/core/serializers/python.py",
      "line": 64,
      "side": "RIGHT",
      "diff_hunk": "@@ -52,43 +56,42 @@ def handle_field(self, obj, field):\n         self._current[field.name] = self._value_from_field(obj, field)\n \n     def handle_fk_field(self, obj, field):\n-        if self.use_natural_foreign_keys and hasattr(\n-            field.remote_field.model, \"natural_key\"\n-        ):\n-            related = getattr(obj, field.name)\n-            if related:\n-                value = related.natural_key()\n+        if self.use_natural_foreign_keys:\n+            natural_key_value = self._resolve_fk_natural_key(obj, field)\n+            if natural_key_value is not None:\n+                value = natural_key_value\n             else:\n-                value = None\n+                value = self._value_from_field(obj, field)",
      "comment": "Thanks for implementing this simplification. What I'm now realizing is that we lack test cases for when natural keys are defined and `self.use_natural_foreign_keys` is False. To avoid having to add coverage for your helper where you check that condition, I'm just going to shuffle that responsibility back out of the helper so we can leave it where it was (and not have the responsibility for testing new added lines of code). Pardon me!",
      "comment_id": 2582599958,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-02T19:54:43Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2582599958"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "django/core/serializers/base.py",
      "line": 235,
      "side": "RIGHT",
      "diff_hunk": "@@ -209,6 +209,31 @@ def getvalue(self):\n         if callable(getattr(self.stream, \"getvalue\", None)):\n             return self.stream.getvalue()\n \n+    def _resolve_fk_natural_key(self, obj, field):\n+        \"\"\"\n+        Return the natural key for a ForeignKey's related object, or None if\n+        not supported.\n+        \"\"\"\n+        if not self._model_supports_natural_key(field.remote_field.model):\n+            return None\n+\n+        related = getattr(obj, field.name, None)\n+        try:\n+            return related.natural_key()\n+        except AttributeError:\n+            return None\n+\n+    def _instance_supports_natural_key(self, obj):\n+        \"\"\"Return True if the instance defines a non-empty natural_key().\"\"\"\n+        try:\n+            return bool(obj.natural_key())\n+        except AttributeError:\n+            return False\n+\n+    def _model_supports_natural_key(self, model):\n+        \"\"\"Return True if the model defines a non-empty natural_key().\"\"\"\n+        return self._instance_supports_natural_key(model())",
      "comment": "I went too far. I now see why you needed to call `natural_key()` on each object during the many-to-many iterator. Will restore your helper for that. \ud83d\ude05 ",
      "comment_id": 2585419830,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-03T14:48:00Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2585419830"
    },
    {
      "repo": "django/django",
      "pr_number": 19919,
      "file_path": "django/core/serializers/base.py",
      "line": 235,
      "side": "RIGHT",
      "diff_hunk": "@@ -209,6 +209,31 @@ def getvalue(self):\n         if callable(getattr(self.stream, \"getvalue\", None)):\n             return self.stream.getvalue()\n \n+    def _resolve_fk_natural_key(self, obj, field):\n+        \"\"\"\n+        Return the natural key for a ForeignKey's related object, or None if\n+        not supported.\n+        \"\"\"\n+        if not self._model_supports_natural_key(field.remote_field.model):\n+            return None\n+\n+        related = getattr(obj, field.name, None)\n+        try:\n+            return related.natural_key()\n+        except AttributeError:\n+            return None\n+\n+    def _instance_supports_natural_key(self, obj):\n+        \"\"\"Return True if the instance defines a non-empty natural_key().\"\"\"\n+        try:\n+            return bool(obj.natural_key())\n+        except AttributeError:\n+            return False\n+\n+    def _model_supports_natural_key(self, model):\n+        \"\"\"Return True if the model defines a non-empty natural_key().\"\"\"\n+        return self._instance_supports_natural_key(model())",
      "comment": "Got it \u2014 thank you for all your help and guidance throughout this PR. \ud83d\ude0a",
      "comment_id": 2592128347,
      "user": "rimi0108",
      "created_at": "2025-12-05T10:11:30Z",
      "url": "https://github.com/django/django/pull/19919#discussion_r2592128347"
    },
    {
      "repo": "django/django",
      "pr_number": 20369,
      "file_path": "django/db/backends/postgresql/compiler.py",
      "line": 35,
      "side": "RIGHT",
      "diff_hunk": "@@ -29,7 +29,8 @@ class SQLCompiler(BaseSQLCompiler):\n     def quote_name_unless_alias(self, name):\n         if \"$\" in name:\n             raise ValueError(\n-                \"Dollar signs are not permitted in column aliases on PostgreSQL.\"\n+                \"Dollar signs are not permitted in column aliases on \"\n+                f\"{self.connection.display_name}.\"\n             )\n         return super().quote_name_unless_alias(name)",
      "comment": "Now that we have a flag for this, I suppose we could just move this into the top of the method in the base class in `django/db/models/sql/compiler.py`?\n\n```python\n        if \"$\" in name and self.connection.features.prohibits_dollar_signs_in_column_aliases:\n            raise ValueError(\n                \"Dollar signs are not permitted in column aliases on \"\n                f\"{self.connection.display_name}.\"\n            )",
      "comment_id": 2588577393,
      "user": "ngnpope",
      "created_at": "2025-12-04T11:02:32Z",
      "url": "https://github.com/django/django/pull/20369#discussion_r2588577393"
    },
    {
      "repo": "django/django",
      "pr_number": 20369,
      "file_path": "django/db/backends/base/features.py",
      "line": 420,
      "side": "RIGHT",
      "diff_hunk": "@@ -415,6 +415,10 @@ class BaseDatabaseFeatures:\n     # Does the Round() database function round to even?\n     rounds_to_even = False\n \n+    # Should dollar signs be prohibited in column aliases to prevent SQL\n+    # injection?\n+    prohibits_dollar_signs_in_column_aliases = False",
      "comment": "Maybe `prohibits_dollar_char_in_column_aliases` or `prohibits_dollar_in_column_aliases`?",
      "comment_id": 2588583262,
      "user": "ngnpope",
      "created_at": "2025-12-04T11:04:00Z",
      "url": "https://github.com/django/django/pull/20369#discussion_r2588583262"
    },
    {
      "repo": "django/django",
      "pr_number": 20369,
      "file_path": "django/db/backends/base/features.py",
      "line": 420,
      "side": "RIGHT",
      "diff_hunk": "@@ -415,6 +415,10 @@ class BaseDatabaseFeatures:\n     # Does the Round() database function round to even?\n     rounds_to_even = False\n \n+    # Should dollar signs be prohibited in column aliases to prevent SQL\n+    # injection?\n+    prohibits_dollar_signs_in_column_aliases = False",
      "comment": "I think it's okay. It's called a dollar sign, not a dollar character. :-D",
      "comment_id": 2589449087,
      "user": "timgraham",
      "created_at": "2025-12-04T15:06:09Z",
      "url": "https://github.com/django/django/pull/20369#discussion_r2589449087"
    },
    {
      "repo": "django/django",
      "pr_number": 20329,
      "file_path": "django/test/runner.py",
      "line": 604,
      "side": "RIGHT",
      "diff_hunk": "@@ -579,38 +586,30 @@ def run(self, result):\n                 self.debug_mode,\n                 self.used_aliases,\n             ],\n-        )\n-        args = [\n-            (self.runner_class, index, subsuite, self.failfast, self.buffer)\n-            for index, subsuite in enumerate(self.subsuites)\n-        ]\n-        # Don't buffer in the main process to avoid error propagation issues.\n-        result.buffer = False\n-\n-        test_results = pool.imap_unordered(self.run_subsuite.__func__, args)\n-\n-        while True:\n-            if result.shouldStop:\n-                pool.terminate()\n-                break\n-\n-            try:\n-                subsuite_index, events = test_results.next(timeout=0.1)\n-            except multiprocessing.TimeoutError as err:\n-                if counter.value < 0:\n-                    err.add_note(\"ERROR: _init_worker failed, see prior traceback\")\n+        ) as pool:\n+            test_results = pool.imap_unordered(self.run_subsuite.__func__, args)\n+\n+            while True:\n+                if result.shouldStop:\n+                    pool.terminate()\n+                    break\n+\n+                try:\n+                    subsuite_index, events = test_results.next(timeout=0.1)\n+                except multiprocessing.TimeoutError as err:\n+                    if counter.value < 0:\n+                        err.add_note(\"ERROR: _init_worker failed, see prior traceback\")\n+                        raise\n+                    continue\n+                except StopIteration:",
      "comment": "I think we don't need the `pool.close()` on the line below this now.",
      "comment_id": 2582973946,
      "user": "LilyFirefly",
      "created_at": "2025-12-02T22:28:57Z",
      "url": "https://github.com/django/django/pull/20329#discussion_r2582973946"
    },
    {
      "repo": "django/django",
      "pr_number": 20329,
      "file_path": "django/test/runner.py",
      "line": 604,
      "side": "RIGHT",
      "diff_hunk": "@@ -579,38 +586,30 @@ def run(self, result):\n                 self.debug_mode,\n                 self.used_aliases,\n             ],\n-        )\n-        args = [\n-            (self.runner_class, index, subsuite, self.failfast, self.buffer)\n-            for index, subsuite in enumerate(self.subsuites)\n-        ]\n-        # Don't buffer in the main process to avoid error propagation issues.\n-        result.buffer = False\n-\n-        test_results = pool.imap_unordered(self.run_subsuite.__func__, args)\n-\n-        while True:\n-            if result.shouldStop:\n-                pool.terminate()\n-                break\n-\n-            try:\n-                subsuite_index, events = test_results.next(timeout=0.1)\n-            except multiprocessing.TimeoutError as err:\n-                if counter.value < 0:\n-                    err.add_note(\"ERROR: _init_worker failed, see prior traceback\")\n+        ) as pool:\n+            test_results = pool.imap_unordered(self.run_subsuite.__func__, args)\n+\n+            while True:\n+                if result.shouldStop:\n+                    pool.terminate()\n+                    break\n+\n+                try:\n+                    subsuite_index, events = test_results.next(timeout=0.1)\n+                except multiprocessing.TimeoutError as err:\n+                    if counter.value < 0:\n+                        err.add_note(\"ERROR: _init_worker failed, see prior traceback\")\n+                        raise\n+                    continue\n+                except StopIteration:",
      "comment": "I get the following when I try that (given that we only `break` and not `raise`):\r\n\r\n```py\r\n  File \"/Users/jwalls/django/django/test/runner.py\", line 611, in run\r\n    pool.join()\r\n    ~~~~~~~~~^^\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/multiprocessing/pool.py\", line 662, in join\r\n    raise ValueError(\"Pool is still running\")\r\nValueError: Pool is still running\r\n```",
      "comment_id": 2583021202,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-02T22:54:42Z",
      "url": "https://github.com/django/django/pull/20329#discussion_r2583021202"
    },
    {
      "repo": "django/django",
      "pr_number": 20182,
      "file_path": "tests/auth_tests/test_validators.py",
      "line": 215,
      "side": "RIGHT",
      "diff_hunk": "@@ -202,24 +202,26 @@ def test_validate(self):\n         self.assertEqual(cm.exception.messages, [expected_error % \"username\"])\n         self.assertEqual(cm.exception.error_list[0].code, \"password_too_similar\")\n \n-        with self.assertRaises(ValidationError) as cm:\n+        msg = expected_error % \"email address\"\n+        with self.assertRaisesMessage(ValidationError, msg):\n             UserAttributeSimilarityValidator().validate(\"example.com\", user=user)\n-        self.assertEqual(cm.exception.messages, [expected_error % \"email address\"])\n \n-        with self.assertRaises(ValidationError) as cm:\n+        msg = expected_error % \"first name\"\n+        with self.assertRaisesMessage(ValidationError, msg):\n             UserAttributeSimilarityValidator(\n                 user_attributes=[\"first_name\"],\n                 max_similarity=0.3,\n             ).validate(\"testclient\", user=user)\n-        self.assertEqual(cm.exception.messages, [expected_error % \"first name\"])\n+",
      "comment": "Try not to introduce new newlines; it bloats the diff.",
      "comment_id": 2582715526,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-02T20:40:21Z",
      "url": "https://github.com/django/django/pull/20182#discussion_r2582715526"
    },
    {
      "repo": "django/django",
      "pr_number": 20182,
      "file_path": "tests/auth_tests/test_validators.py",
      "line": 215,
      "side": "RIGHT",
      "diff_hunk": "@@ -202,24 +202,26 @@ def test_validate(self):\n         self.assertEqual(cm.exception.messages, [expected_error % \"username\"])\n         self.assertEqual(cm.exception.error_list[0].code, \"password_too_similar\")\n \n-        with self.assertRaises(ValidationError) as cm:\n+        msg = expected_error % \"email address\"\n+        with self.assertRaisesMessage(ValidationError, msg):\n             UserAttributeSimilarityValidator().validate(\"example.com\", user=user)\n-        self.assertEqual(cm.exception.messages, [expected_error % \"email address\"])\n \n-        with self.assertRaises(ValidationError) as cm:\n+        msg = expected_error % \"first name\"\n+        with self.assertRaisesMessage(ValidationError, msg):\n             UserAttributeSimilarityValidator(\n                 user_attributes=[\"first_name\"],\n                 max_similarity=0.3,\n             ).validate(\"testclient\", user=user)\n-        self.assertEqual(cm.exception.messages, [expected_error % \"first name\"])\n+",
      "comment": "done , removed the newline and also added a fullstop",
      "comment_id": 2584662143,
      "user": "Skyiesac",
      "created_at": "2025-12-03T11:03:05Z",
      "url": "https://github.com/django/django/pull/20182#discussion_r2584662143"
    },
    {
      "repo": "django/django",
      "pr_number": 19277,
      "file_path": "tests/bulk_create/models.py",
      "line": 157,
      "side": "RIGHT",
      "diff_hunk": "@@ -147,3 +148,10 @@ class RelatedModel(models.Model):\n class DbDefaultModel(models.Model):\n     name = models.CharField(max_length=10)\n     created_at = models.DateTimeField(db_default=Now())\n+\n+\n+class DbDefaultPrimaryKey(models.Model):\n+    id = models.UUIDField(primary_key=True, db_default=Func(function=\"gen_random_uuid\"))\n+\n+    class Meta:\n+        required_db_vendor = \"postgresql\"",
      "comment": "The problem is not Postgres specific; use the following model instead\r\n\r\n```suggestion\r\nclass DbDefaultPrimaryKeyModel(models.Model):\r\n    id = models.DateTimeField(primary_key=True, db_default=Now())\r\n```",
      "comment_id": 1997775535,
      "user": "charettes",
      "created_at": "2025-03-16T23:53:32Z",
      "url": "https://github.com/django/django/pull/19277#discussion_r1997775535"
    },
    {
      "repo": "django/django",
      "pr_number": 19277,
      "file_path": "tests/bulk_create/tests.py",
      "line": 880,
      "side": "RIGHT",
      "diff_hunk": "@@ -866,3 +869,12 @@ def test_db_default_field_excluded(self):\n             ctx[0][\"sql\"].count(created_at_quoted_name),\n             2 if connection.features.can_return_rows_from_bulk_insert else 1,\n         )\n+\n+    @unittest.skipUnless(connection.vendor == \"postgresql\", \"PostgreSQL specific SQL\")\n+    def test_db_default_primary_key(self):\n+        obj1, obj2 = DbDefaultPrimaryKey.objects.bulk_create(\n+            [DbDefaultPrimaryKey(), DbDefaultPrimaryKey()]\n+        )\n+        self.assertIsInstance(obj1.id, UUID)\n+        self.assertIsInstance(obj2.id, UUID)\n+        self.assertNotEqual(obj1.id, obj2.id)",
      "comment": "```suggestion\r\n    @skipUnlessDBFeature(\"can_return_rows_from_bulk_insert\")\r\n    def test_db_default_primary_key(self):\r\n        (obj,) = DbDefaultPrimaryKeyModel.objects.bulk_create(\r\n            [DbDefaultPrimaryKeyModel()]\r\n        )\r\n        self.assertIsInstance(obj.id, datetime)\r\n```",
      "comment_id": 1997780377,
      "user": "charettes",
      "created_at": "2025-03-17T00:08:11Z",
      "url": "https://github.com/django/django/pull/19277#discussion_r1997780377"
    },
    {
      "repo": "django/django",
      "pr_number": 19277,
      "file_path": "django/db/models/base.py",
      "line": 688,
      "side": "RIGHT",
      "diff_hunk": "@@ -685,6 +685,7 @@ def _is_pk_set(self, meta=None):\n         pk_val = self._get_pk_val(meta)\n         return not (\n             pk_val is None\n+            or isinstance(pk_val, DatabaseDefault)",
      "comment": "I think we should avoid adding more to this function particularly in the context of ticket-36259.\r\n\r\nI'd suggest altering the `bulk_create` usage instead\r\n\r\n```diff\r\ndiff --git a/django/db/models/query.py b/django/db/models/query.py\r\nindex 175073b961..9f2332a60e 100644\r\n--- a/django/db/models/query.py\r\n+++ b/django/db/models/query.py\r\n@@ -23,7 +23,7 @@\r\n from django.db.models import AutoField, DateField, DateTimeField, Field, sql\r\n from django.db.models.constants import LOOKUP_SEP, OnConflict\r\n from django.db.models.deletion import Collector\r\n-from django.db.models.expressions import Case, F, Value, When\r\n+from django.db.models.expressions import Case, DatabaseDefault, F, Value, When\r\n from django.db.models.functions import Cast, Trunc\r\n from django.db.models.query_utils import FilteredRelation, Q\r\n from django.db.models.sql.constants import GET_ITERATOR_CHUNK_SIZE, ROW_COUNT\r\n@@ -789,7 +789,10 @@ def bulk_create(\r\n         objs = list(objs)\r\n         self._prepare_for_bulk_create(objs)\r\n         with transaction.atomic(using=self.db, savepoint=False):\r\n-            objs_without_pk, objs_with_pk = partition(lambda o: o._is_pk_set(), objs)\r\n+            objs_without_pk, objs_with_pk = partition(\r\n+                lambda o: o._is_pk_set() and not isinstance(o.pk, DatabaseDefault),\r\n+                objs,\r\n+            )\r\n             if objs_with_pk:\r\n                 returned_columns = self._batched_insert(\r\n                     objs_with_pk,\r\n```",
      "comment_id": 1997781682,
      "user": "charettes",
      "created_at": "2025-03-17T00:12:27Z",
      "url": "https://github.com/django/django/pull/19277#discussion_r1997781682"
    },
    {
      "repo": "django/django",
      "pr_number": 19277,
      "file_path": "django/db/models/query.py",
      "line": 795,
      "side": "RIGHT",
      "diff_hunk": "@@ -789,7 +789,10 @@ def bulk_create(\n         objs = list(objs)\n         self._prepare_for_bulk_create(objs)\n         with transaction.atomic(using=self.db, savepoint=False):\n-            objs_without_pk, objs_with_pk = partition(lambda o: o._is_pk_set(), objs)\n+            objs_without_pk, objs_with_pk = partition(\n+                lambda o: o._is_pk_set() and not isinstance(o.pk, DatabaseDefault),\n+                objs,\n+            )",
      "comment": "Yes, this change makes sense to avoid looping twice. Should I push it to my branch?",
      "comment_id": 1998859153,
      "user": "mitya57",
      "created_at": "2025-03-17T14:25:52Z",
      "url": "https://github.com/django/django/pull/19277#discussion_r1998859153"
    },
    {
      "repo": "django/django",
      "pr_number": 19277,
      "file_path": "django/db/models/query.py",
      "line": 795,
      "side": "RIGHT",
      "diff_hunk": "@@ -789,7 +789,10 @@ def bulk_create(\n         objs = list(objs)\n         self._prepare_for_bulk_create(objs)\n         with transaction.atomic(using=self.db, savepoint=False):\n-            objs_without_pk, objs_with_pk = partition(lambda o: o._is_pk_set(), objs)\n+            objs_without_pk, objs_with_pk = partition(\n+                lambda o: o._is_pk_set() and not isinstance(o.pk, DatabaseDefault),\n+                objs,\n+            )",
      "comment": "I think you could yes, worst case it gets removed at merge time. Push it as a separate commit starting with `Refs #36260 -- <past tense message>` \ud83d\ude47 ",
      "comment_id": 1998864090,
      "user": "charettes",
      "created_at": "2025-03-17T14:28:06Z",
      "url": "https://github.com/django/django/pull/19277#discussion_r1998864090"
    },
    {
      "repo": "django/django",
      "pr_number": 19277,
      "file_path": "django/db/models/query.py",
      "line": 684,
      "side": "RIGHT",
      "diff_hunk": "@@ -670,11 +670,20 @@ async def acreate(self, **kwargs):\n     acreate.alters_data = True\n \n     def _prepare_for_bulk_create(self, objs):\n+        objs_with_pk, objs_without_pk = [], []\n         for obj in objs:\n-            if not obj._is_pk_set():\n-                # Populate new PK values.\n+            if isinstance(obj.pk, DatabaseDefault):\n+                objs_without_pk.append(obj)\n+            elif obj._is_pk_set():\n+                objs_with_pk.append(obj)\n+            else:\n                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)\n+                if obj._is_pk_set():\n+                    objs_with_pk.append(obj)\n+                else:\n+                    objs_without_pk.append(obj)",
      "comment": "```suggestion\r\n                objs_without_pk.append(obj)\r\n```\r\nI'm not sure if I've missed something but I can change to the following without any bulk_create test failures. Are we missing a test for this or can it be simplified?",
      "comment_id": 2011588141,
      "user": "sarahboyce",
      "created_at": "2025-03-25T08:40:53Z",
      "url": "https://github.com/django/django/pull/19277#discussion_r2011588141"
    },
    {
      "repo": "django/django",
      "pr_number": 19277,
      "file_path": "django/db/models/query.py",
      "line": 684,
      "side": "RIGHT",
      "diff_hunk": "@@ -670,11 +670,20 @@ async def acreate(self, **kwargs):\n     acreate.alters_data = True\n \n     def _prepare_for_bulk_create(self, objs):\n+        objs_with_pk, objs_without_pk = [], []\n         for obj in objs:\n-            if not obj._is_pk_set():\n-                # Populate new PK values.\n+            if isinstance(obj.pk, DatabaseDefault):\n+                objs_without_pk.append(obj)\n+            elif obj._is_pk_set():\n+                objs_with_pk.append(obj)\n+            else:\n                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)\n+                if obj._is_pk_set():\n+                    objs_with_pk.append(obj)\n+                else:\n+                    objs_without_pk.append(obj)",
      "comment": "@sarahboyce through a bit of git-blame if found the test that would fail.\r\n\r\n`get_pk_value_on_save` was introduced in 8adc59038cdc6ce4f9170e4de2d716d940e136b3 to [expose a hook for these reasons](https://code.djangoproject.com/ticket/23617#comment:11).\r\n\r\nWhether or not this hook is still relevant today now that we support `db_default` is up for discussion but if you run the `model_fields.test_uuid` suite you'll notice that this code is exercised and thus likely used in the wild.",
      "comment_id": 2012212921,
      "user": "charettes",
      "created_at": "2025-03-25T14:17:48Z",
      "url": "https://github.com/django/django/pull/19277#discussion_r2012212921"
    },
    {
      "repo": "django/django",
      "pr_number": 19519,
      "file_path": "tests/aggregation_regress/tests.py",
      "line": 196,
      "side": "RIGHT",
      "diff_hunk": "@@ -171,6 +171,56 @@ def assertObjectAttrs(self, obj, **kwargs):\n         for attr, value in kwargs.items():\n             self.assertEqual(getattr(obj, attr), value)\n \n+    def test_count_preserve_group_by(self):\n+        # new release of the same book\n+        Book.objects.create(\n+            isbn=\"113235613\",\n+            name=self.b4.name,\n+            pages=self.b4.pages,\n+            rating=4.0,\n+            price=Decimal(\"39.69\"),\n+            contact=self.a5,\n+            publisher=self.p3,\n+            pubdate=datetime.date(2018, 11, 3),\n+        )\n+        qs = Book.objects.values(\"contact__name\", \"publisher__name\").annotate(\n+            publications=Count(\"id\")\n+        )\n+        self.assertEqual(qs.count(), 6)\n+        self.assertEqual(qs.count(), len(qs))\n+        # before ticket 26434 had been solved .count() was returning also 6\n+        self.assertEqual(qs.order_by(\"id\").count(), 7)\n+        # before ticket 26434 had been solved .count() was not equal to len(qs)\n+        self.assertEqual(qs.order_by(\"id\").count(), len(qs.order_by(\"id\")))\n+\n+    def test_aggregate_preserve_group_by(self):",
      "comment": "Counts are aggregates, and this patch doesn't introduce specialization for `Count`, so I think just having the prior test with count is sufficient.",
      "comment_id": 2440361876,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-17T15:18:06Z",
      "url": "https://github.com/django/django/pull/19519#discussion_r2440361876"
    },
    {
      "repo": "django/django",
      "pr_number": 19519,
      "file_path": "tests/aggregation_regress/tests.py",
      "line": 194,
      "side": "RIGHT",
      "diff_hunk": "@@ -171,6 +171,56 @@ def assertObjectAttrs(self, obj, **kwargs):\n         for attr, value in kwargs.items():\n             self.assertEqual(getattr(obj, attr), value)\n \n+    def test_count_preserve_group_by(self):\n+        # new release of the same book\n+        Book.objects.create(\n+            isbn=\"113235613\",\n+            name=self.b4.name,\n+            pages=self.b4.pages,\n+            rating=4.0,\n+            price=Decimal(\"39.69\"),\n+            contact=self.a5,\n+            publisher=self.p3,\n+            pubdate=datetime.date(2018, 11, 3),\n+        )\n+        qs = Book.objects.values(\"contact__name\", \"publisher__name\").annotate(\n+            publications=Count(\"id\")\n+        )\n+        self.assertEqual(qs.count(), 6)\n+        self.assertEqual(qs.count(), len(qs))\n+        # before ticket 26434 had been solved .count() was returning also 6\n+        self.assertEqual(qs.order_by(\"id\").count(), 7)\n+        # before ticket 26434 had been solved .count() was not equal to len(qs)\n+        self.assertEqual(qs.order_by(\"id\").count(), len(qs.order_by(\"id\")))",
      "comment": "```suggestion\n        self.assertEqual(qs.count(), Book.objects.count() - 1)\n        self.assertEqual(qs.order_by(\"id\").count(), Book.objects.count())\n```",
      "comment_id": 2440384837,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-17T15:26:13Z",
      "url": "https://github.com/django/django/pull/19519#discussion_r2440384837"
    },
    {
      "repo": "django/django",
      "pr_number": 19519,
      "file_path": "django/db/models/sql/query.py",
      "line": 2319,
      "side": "RIGHT",
      "diff_hunk": "@@ -2311,7 +2311,13 @@ def clear_ordering(self, force=False, clear_default=True):\n         query (not even the model's default).\n         \"\"\"\n         if not force and (\n-            self.is_sliced or self.distinct_fields or self.select_for_update\n+            self.is_sliced\n+            or self.distinct_fields\n+            or self.select_for_update\n+            or (\n+                isinstance(self.group_by, tuple)\n+                and not set(self.order_by + self.extra_order_by).issubset(self.group_by)",
      "comment": "No test fails when I remove the `self.extra_order_by`, so let's add coverage.",
      "comment_id": 2440386611,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-17T15:26:53Z",
      "url": "https://github.com/django/django/pull/19519#discussion_r2440386611"
    },
    {
      "repo": "django/django",
      "pr_number": 19519,
      "file_path": "django/db/models/sql/query.py",
      "line": 2319,
      "side": "RIGHT",
      "diff_hunk": "@@ -2311,7 +2311,13 @@ def clear_ordering(self, force=False, clear_default=True):\n         query (not even the model's default).\n         \"\"\"\n         if not force and (\n-            self.is_sliced or self.distinct_fields or self.select_for_update\n+            self.is_sliced\n+            or self.distinct_fields\n+            or self.select_for_update\n+            or (\n+                isinstance(self.group_by, tuple)\n+                and not set(self.order_by + self.extra_order_by).issubset(self.group_by)",
      "comment": "Just to confirm regarding the missing coverage for self.extra_order_by: \r\n\r\nDo you have a specific scenario in mind that I should turn into a test? \r\nI assume you mean a case where extra_order_by affects the count/aggregation differently than order_by alone - I can add a dedicated test for that if that\u2019s what you meant.",
      "comment_id": 2440622157,
      "user": "michalnik",
      "created_at": "2025-10-17T17:00:38Z",
      "url": "https://github.com/django/django/pull/19519#discussion_r2440622157"
    },
    {
      "repo": "django/django",
      "pr_number": 19519,
      "file_path": "tests/aggregation_regress/tests.py",
      "line": 194,
      "side": "RIGHT",
      "diff_hunk": "@@ -171,6 +171,56 @@ def assertObjectAttrs(self, obj, **kwargs):\n         for attr, value in kwargs.items():\n             self.assertEqual(getattr(obj, attr), value)\n \n+    def test_count_preserve_group_by(self):\n+        # new release of the same book\n+        Book.objects.create(\n+            isbn=\"113235613\",\n+            name=self.b4.name,\n+            pages=self.b4.pages,\n+            rating=4.0,\n+            price=Decimal(\"39.69\"),\n+            contact=self.a5,\n+            publisher=self.p3,\n+            pubdate=datetime.date(2018, 11, 3),\n+        )\n+        qs = Book.objects.values(\"contact__name\", \"publisher__name\").annotate(\n+            publications=Count(\"id\")\n+        )\n+        self.assertEqual(qs.count(), 6)\n+        self.assertEqual(qs.count(), len(qs))\n+        # before ticket 26434 had been solved .count() was returning also 6\n+        self.assertEqual(qs.order_by(\"id\").count(), 7)\n+        # before ticket 26434 had been solved .count() was not equal to len(qs)\n+        self.assertEqual(qs.order_by(\"id\").count(), len(qs.order_by(\"id\")))",
      "comment": "I don\u2019t have time to adjust the test right now, but I\u2019ll make sure to address it in the next update.",
      "comment_id": 2440652178,
      "user": "michalnik",
      "created_at": "2025-10-17T17:15:20Z",
      "url": "https://github.com/django/django/pull/19519#discussion_r2440652178"
    },
    {
      "repo": "django/django",
      "pr_number": 19519,
      "file_path": "django/db/models/sql/query.py",
      "line": 2319,
      "side": "RIGHT",
      "diff_hunk": "@@ -2311,7 +2311,13 @@ def clear_ordering(self, force=False, clear_default=True):\n         query (not even the model's default).\n         \"\"\"\n         if not force and (\n-            self.is_sliced or self.distinct_fields or self.select_for_update\n+            self.is_sliced\n+            or self.distinct_fields\n+            or self.select_for_update\n+            or (\n+                isinstance(self.group_by, tuple)\n+                and not set(self.order_by + self.extra_order_by).issubset(self.group_by)",
      "comment": "Right, just having a test that varies only in that instead of calling `.order_by()` it calls `.extra(order_by=...`.",
      "comment_id": 2440684480,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-17T17:30:26Z",
      "url": "https://github.com/django/django/pull/19519#discussion_r2440684480"
    },
    {
      "repo": "django/django",
      "pr_number": 19519,
      "file_path": "django/db/models/sql/query.py",
      "line": 2354,
      "side": "RIGHT",
      "diff_hunk": "@@ -2346,7 +2346,15 @@ def clear_ordering(self, force=False, clear_default=True):\n         query (not even the model's default).\n         \"\"\"\n         if not force and (\n-            self.is_sliced or self.distinct_fields or self.select_for_update\n+            self.is_sliced\n+            or self.distinct_fields\n+            or self.select_for_update\n+            or (\n+                isinstance(self.group_by, tuple)\n+                and not set(*self.order_by, *self.extra_order_by).issubset(",
      "comment": "Isn't this missing a `[...]` wrapping?\n\n\n```suggestion\n                and not set([*self.order_by, *self.extra_order_by]).issubset(\n```\n\n```\n\u279c  Workspace python --version\nPython 3.13.5\n\u279c  Workspace python\nPython 3.13.5 (main, Jun 14 2025, 15:09:19) [Clang 14.0.0 (clang-1400.0.29.202)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> set(*[1,2,3], *[4,5,6])\nTraceback (most recent call last):\n  File \"<python-input-0>\", line 1, in <module>\n    set(*[1,2,3], *[4,5,6])\n    ~~~^^^^^^^^^^^^^^^^^^^^\nTypeError: set expected at most 1 argument, got 6\n```",
      "comment_id": 2466412464,
      "user": "charettes",
      "created_at": "2025-10-27T16:57:50Z",
      "url": "https://github.com/django/django/pull/19519#discussion_r2466412464"
    },
    {
      "repo": "django/django",
      "pr_number": 19519,
      "file_path": "django/db/models/sql/query.py",
      "line": 2354,
      "side": "RIGHT",
      "diff_hunk": "@@ -2346,7 +2346,15 @@ def clear_ordering(self, force=False, clear_default=True):\n         query (not even the model's default).\n         \"\"\"\n         if not force and (\n-            self.is_sliced or self.distinct_fields or self.select_for_update\n+            self.is_sliced\n+            or self.distinct_fields\n+            or self.select_for_update\n+            or (\n+                isinstance(self.group_by, tuple)\n+                and not set(*self.order_by, *self.extra_order_by).issubset(",
      "comment": "Or maybe use a set literal directly?\n\n\n```suggestion\n                and not {*self.order_by, *self.extra_order_by}.issubset(\n```",
      "comment_id": 2466417097,
      "user": "charettes",
      "created_at": "2025-10-27T16:59:25Z",
      "url": "https://github.com/django/django/pull/19519#discussion_r2466417097"
    },
    {
      "repo": "django/django",
      "pr_number": 19519,
      "file_path": "django/db/models/sql/query.py",
      "line": 2354,
      "side": "RIGHT",
      "diff_hunk": "@@ -2346,7 +2346,15 @@ def clear_ordering(self, force=False, clear_default=True):\n         query (not even the model's default).\n         \"\"\"\n         if not force and (\n-            self.is_sliced or self.distinct_fields or self.select_for_update\n+            self.is_sliced\n+            or self.distinct_fields\n+            or self.select_for_update\n+            or (\n+                isinstance(self.group_by, tuple)\n+                and not set(*self.order_by, *self.extra_order_by).issubset(",
      "comment": "(I naively assumed that if tests passed when removing the `tuple` I would be okay.)",
      "comment_id": 2466435673,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-27T17:06:35Z",
      "url": "https://github.com/django/django/pull/19519#discussion_r2466435673"
    },
    {
      "repo": "django/django",
      "pr_number": 19519,
      "file_path": "django/db/models/sql/query.py",
      "line": 2354,
      "side": "RIGHT",
      "diff_hunk": "@@ -2346,7 +2346,15 @@ def clear_ordering(self, force=False, clear_default=True):\n         query (not even the model's default).\n         \"\"\"\n         if not force and (\n-            self.is_sliced or self.distinct_fields or self.select_for_update\n+            self.is_sliced\n+            or self.distinct_fields\n+            or self.select_for_update\n+            or (\n+                isinstance(self.group_by, tuple)\n+                and not set(*self.order_by, *self.extra_order_by).issubset(",
      "comment": "I suspect it does because there's only one member of `order_by` present which turned into single argument being provided?",
      "comment_id": 2466439300,
      "user": "charettes",
      "created_at": "2025-10-27T17:07:55Z",
      "url": "https://github.com/django/django/pull/19519#discussion_r2466439300"
    },
    {
      "repo": "django/django",
      "pr_number": 19519,
      "file_path": "django/db/models/sql/query.py",
      "line": 2355,
      "side": "RIGHT",
      "diff_hunk": "@@ -2346,7 +2346,13 @@ def clear_ordering(self, force=False, clear_default=True):\n         query (not even the model's default).\n         \"\"\"\n         if not force and (\n-            self.is_sliced or self.distinct_fields or self.select_for_update\n+            self.is_sliced\n+            or self.distinct_fields\n+            or self.select_for_update\n+            or (\n+                isinstance(self.group_by, tuple)\n+                and not {*self.order_by, *self.extra_order_by}.issubset(self.group_by)\n+            )",
      "comment": "This caused a [regression](https://djangoci.com/job/django-oracle/database=oracle19,label=oracle,python=python3.14/lastCompletedBuild/testReport/) on Oracle but not only. `group_by` is a tuple of `Col` expressions, `order_by` is a list of strings, so `{*self.order_by, *self.extra_order_by}` will never be a subset of `group_by`.\r\n\r\nPreviously:\r\n\r\n```sql\r\nSELECT\r\n   ...\r\nFROM \"AGGREGATION_REGRESS_BOOK\"\r\nWHERE \"AGGREGATION_REGRESS_BOOK\".\"ID\" IN (\r\n    SELECT MAX(U0.\"ID\") AS \"ID__MAX\"\r\n   FROM \"AGGREGATION_REGRESS_BOOK\" U0\r\n   GROUP BY U0.\"CONTACT_ID\"\r\n)\r\nORDER BY \"AGGREGATION_REGRESS_BOOK\".\"ID\" ASC\r\n```\r\nwith this patch\r\n```sql\r\nSELECT\r\n   ...\r\nFROM \"AGGREGATION_REGRESS_BOOK\"\r\nWHERE \"AGGREGATION_REGRESS_BOOK\".\"ID\" IN (\r\n    SELECT MAX(U0.\"ID\") AS \"ID__MAX\"\r\n   FROM \"AGGREGATION_REGRESS_BOOK\" U0\r\n   GROUP BY U0.\"CONTACT_ID\"\r\n   ORDER BY U0.\"CONTACT_ID\" ASC\r\n)\r\nORDER BY \"AGGREGATION_REGRESS_BOOK\".\"ID\" ASC\r\n```",
      "comment_id": 2468153859,
      "user": "felixxm",
      "created_at": "2025-10-28T07:08:56Z",
      "url": "https://github.com/django/django/pull/19519#discussion_r2468153859"
    },
    {
      "repo": "django/django",
      "pr_number": 19519,
      "file_path": "django/db/models/sql/query.py",
      "line": 2355,
      "side": "RIGHT",
      "diff_hunk": "@@ -2346,7 +2346,13 @@ def clear_ordering(self, force=False, clear_default=True):\n         query (not even the model's default).\n         \"\"\"\n         if not force and (\n-            self.is_sliced or self.distinct_fields or self.select_for_update\n+            self.is_sliced\n+            or self.distinct_fields\n+            or self.select_for_update\n+            or (\n+                isinstance(self.group_by, tuple)\n+                and not {*self.order_by, *self.extra_order_by}.issubset(self.group_by)\n+            )",
      "comment": "As far as I'm aware, we would need to translate string aliases (`order_by` and `extra_order_by`) to `Col` expressions but this may be expensive here (or at least non-trivial).",
      "comment_id": 2468339798,
      "user": "felixxm",
      "created_at": "2025-10-28T08:06:05Z",
      "url": "https://github.com/django/django/pull/19519#discussion_r2468339798"
    },
    {
      "repo": "django/django",
      "pr_number": 20340,
      "file_path": "django/utils/inspect.py",
      "line": 126,
      "side": "RIGHT",
      "diff_hunk": "@@ -98,3 +107,27 @@ def is_module_level_function(func):\n         return False\n \n     return True\n+\n+\n+@contextmanager\n+def leave_deferred_annotations_unevaluated():\n+    \"\"\"\n+    inspect.getfullargspec eagerly evaluates type annotations. To add\n+    compatibility with Python 3.14+ deferred evaluation, patch the module-level\n+    helper to provide the annotation_format that we are using elsewhere.\n+\n+    This private helper could be removed when there is an upstream solution for\n+    https://github.com/python/cpython/issues/141560.\n+    \"\"\"\n+    with lock:",
      "comment": "Nit: For non-3.14 versions, this adds a lock around evaluting arguments. It's probably not huge, but in especially hot contexts it adds unnecessary extra work and could impact throughput. Perhaps worth a short-circuit for non-3.14 to avoid the lock?\n\n```python\nif not PY314:\n    yield\nelse:\n    with lock:\n        ...\n```",
      "comment_id": 2576958451,
      "user": "RealOrangeOne",
      "created_at": "2025-12-01T12:51:35Z",
      "url": "https://github.com/django/django/pull/20340#discussion_r2576958451"
    },
    {
      "repo": "django/django",
      "pr_number": 20340,
      "file_path": "django/utils/inspect.py",
      "line": 16,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,11 +1,20 @@\n import functools\n import inspect\n+import threading\n+from contextlib import contextmanager\n \n from django.utils.version import PY314\n \n if PY314:\n     import annotationlib\n \n+    safe_signature_from_callable = functools.partial(\n+        inspect._signature_from_callable,\n+        annotation_format=annotationlib.Format.FORWARDREF,\n+    )\n+\n+lock = threading.Lock()",
      "comment": "Should this `lock` also be inside the `if PY314:` guard?",
      "comment_id": 2578207946,
      "user": "nessita",
      "created_at": "2025-12-01T18:34:27Z",
      "url": "https://github.com/django/django/pull/20340#discussion_r2578207946"
    },
    {
      "repo": "django/django",
      "pr_number": 20340,
      "file_path": "django/utils/inspect.py",
      "line": 126,
      "side": "RIGHT",
      "diff_hunk": "@@ -98,3 +107,29 @@ def is_module_level_function(func):\n         return False\n \n     return True\n+\n+\n+@contextmanager\n+def leave_deferred_annotations_unevaluated():\n+    \"\"\"\n+    inspect.getfullargspec eagerly evaluates type annotations. To add\n+    compatibility with Python 3.14+ deferred evaluation, patch the module-level\n+    helper to provide the annotation_format that we are using elsewhere.\n+\n+    This private helper could be removed when there is an upstream solution for\n+    https://github.com/python/cpython/issues/141560.\n+    \"\"\"\n+    if not PY314:\n+        yield\n+        return\n+    with lock:\n+        helper_was_already_replaced = False",
      "comment": "I would suggest using the slightly shorter:\n\n```suggestion\n        helper_was_replaced = False\n```",
      "comment_id": 2578372192,
      "user": "nessita",
      "created_at": "2025-12-01T19:39:25Z",
      "url": "https://github.com/django/django/pull/20340#discussion_r2578372192"
    },
    {
      "repo": "django/django",
      "pr_number": 20340,
      "file_path": "django/utils/inspect.py",
      "line": 113,
      "side": "RIGHT",
      "diff_hunk": "@@ -98,3 +107,29 @@ def is_module_level_function(func):\n         return False\n \n     return True\n+\n+\n+@contextmanager\n+def leave_deferred_annotations_unevaluated():",
      "comment": "How about:\n\n```suggestion\ndef lazy_deferred_annotations():\n```\n",
      "comment_id": 2578376018,
      "user": "nessita",
      "created_at": "2025-12-01T19:40:59Z",
      "url": "https://github.com/django/django/pull/20340#discussion_r2578376018"
    },
    {
      "repo": "django/django",
      "pr_number": 20340,
      "file_path": "django/utils/inspect.py",
      "line": 126,
      "side": "RIGHT",
      "diff_hunk": "@@ -98,3 +107,29 @@ def is_module_level_function(func):\n         return False\n \n     return True\n+\n+\n+@contextmanager\n+def leave_deferred_annotations_unevaluated():\n+    \"\"\"\n+    inspect.getfullargspec eagerly evaluates type annotations. To add\n+    compatibility with Python 3.14+ deferred evaluation, patch the module-level\n+    helper to provide the annotation_format that we are using elsewhere.\n+\n+    This private helper could be removed when there is an upstream solution for\n+    https://github.com/python/cpython/issues/141560.\n+    \"\"\"\n+    if not PY314:\n+        yield\n+        return\n+    with lock:\n+        helper_was_already_replaced = False",
      "comment": "Good call -- I had it like this, then changed it, because from the perspective of the finally block, it could be unclear what \"replaced\" means: replaced before the context was entered, or during the `try`? \r\n\r\nBut this isn't necessarily helping, so I'll revert.",
      "comment_id": 2579083832,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-01T23:35:09Z",
      "url": "https://github.com/django/django/pull/20340#discussion_r2579083832"
    },
    {
      "repo": "django/django",
      "pr_number": 20340,
      "file_path": "django/utils/inspect.py",
      "line": 13,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,11 +1,19 @@\n import functools\n import inspect\n+import threading\n+from contextlib import contextmanager\n \n from django.utils.version import PY314\n \n if PY314:\n     import annotationlib\n \n+    lock = threading.Lock()\n+    safe_signature_from_callable = functools.partial(\n+        inspect._signature_from_callable,",
      "comment": "No reason to have to `getattr` for this below, since we're relying on it here. This helper hasn't changed in python for a long time, it's likely pretty stable.",
      "comment_id": 2579133267,
      "user": "jacobtylerwalls",
      "created_at": "2025-12-02T00:08:30Z",
      "url": "https://github.com/django/django/pull/20340#discussion_r2579133267"
    },
    {
      "repo": "django/django",
      "pr_number": 20350,
      "file_path": "django/core/exceptions.py",
      "line": 61,
      "side": "RIGHT",
      "diff_hunk": "@@ -58,7 +58,7 @@ class DisallowedHost(SuspiciousOperation):\n \n \n class DisallowedRedirect(SuspiciousOperation):\n-    \"\"\"Redirect to scheme not in allowed list\"\"\"\n+    \"\"\"Redirect too long or scheme not in allowed list\"\"\"",
      "comment": "Since we are changing this line, could you please add the missing ending dot? Thanks!",
      "comment_id": 2577192175,
      "user": "nessita",
      "created_at": "2025-12-01T13:56:38Z",
      "url": "https://github.com/django/django/pull/20350#discussion_r2577192175"
    },
    {
      "repo": "django/django",
      "pr_number": 19405,
      "file_path": "django/contrib/auth/management/__init__.py",
      "line": 154,
      "side": "RIGHT",
      "diff_hunk": "@@ -108,6 +109,108 @@ def create_permissions(\n             print(\"Adding permission '%s'\" % perm)\n \n \n+class RenamePermission(migrations.RunPython):\n+    def __init__(self, app_label, old_model, new_model):\n+        self.app_label = app_label\n+        self.old_model = old_model\n+        self.new_model = new_model\n+        super(RenamePermission, self).__init__(\n+            self.rename_forward, self.rename_backward\n+        )\n+\n+    def _rename(self, apps, schema_editor, old_model, new_model):\n+        Permission = apps.get_model(\"auth\", \"Permission\")\n+        ContentType = apps.get_model(\"contenttypes\", \"ContentType\")\n+\n+        db = schema_editor.connection.alias\n+\n+        if not router.allow_migrate_model(db, Permission):\n+            return\n+\n+        try:\n+            ctypes = ContentType.objects.filter(\n+                app_label=self.app_label, model__icontains=self.old_model\n+            )\n+\n+            permissions = Permission.objects.filter(content_type__in=ctypes)\n+\n+        except Permission.DoesNotExist:\n+            pass\n+        else:\n+            for permission in permissions:\n+                permission.codename = (\n+                    permission.codename.split(\"_\")[0] + \"_\" + self.new_model\n+                )\n+\n+                new_str = permission.name.split()[:-1]\n+\n+                permission.name = \" \".join(new_str) + \" \" + self.new_model\n+                try:\n+                    with transaction.atomic(using=db):\n+                        permission.save(update_fields={\"name\", \"codename\"})\n+                except IntegrityError:\n+                    # Gracefully fallback if a stale content type causes a\n+                    # conflict as update_contenttypes will take care of asking the\n+                    # user what should be done next.\n+                    permission.model = old_model\n+            else:\n+                # Clear the cache as the `get_by_natual_key()` call will cache\n+                # the renamed ContentType instance by its old model name.\n+                # Permission.objects.clear_cache()\n+                pass\n+\n+    def rename_forward(self, apps, schema_editor):\n+        self._rename(apps, schema_editor, self.old_model, self.new_model)\n+\n+    def rename_backward(self, apps, schema_editor):\n+        self._rename(apps, schema_editor, self.new_model, self.old_model)",
      "comment": "Can update this to \"pass\" without any failures, so this is missing a test",
      "comment_id": 2089084240,
      "user": "sarahboyce",
      "created_at": "2025-05-14T14:25:31Z",
      "url": "https://github.com/django/django/pull/19405#discussion_r2089084240"
    },
    {
      "repo": "django/django",
      "pr_number": 19405,
      "file_path": "django/contrib/auth/management/__init__.py",
      "line": 178,
      "side": "RIGHT",
      "diff_hunk": "@@ -108,6 +109,108 @@ def create_permissions(\n             print(\"Adding permission '%s'\" % perm)\n \n \n+class RenamePermission(migrations.RunPython):\n+    def __init__(self, app_label, old_model, new_model):\n+        self.app_label = app_label\n+        self.old_model = old_model\n+        self.new_model = new_model\n+        super(RenamePermission, self).__init__(\n+            self.rename_forward, self.rename_backward\n+        )\n+\n+    def _rename(self, apps, schema_editor, old_model, new_model):\n+        Permission = apps.get_model(\"auth\", \"Permission\")\n+        ContentType = apps.get_model(\"contenttypes\", \"ContentType\")\n+\n+        db = schema_editor.connection.alias\n+\n+        if not router.allow_migrate_model(db, Permission):\n+            return\n+\n+        try:\n+            ctypes = ContentType.objects.filter(\n+                app_label=self.app_label, model__icontains=self.old_model\n+            )\n+\n+            permissions = Permission.objects.filter(content_type__in=ctypes)\n+\n+        except Permission.DoesNotExist:\n+            pass\n+        else:\n+            for permission in permissions:\n+                permission.codename = (\n+                    permission.codename.split(\"_\")[0] + \"_\" + self.new_model\n+                )\n+\n+                new_str = permission.name.split()[:-1]\n+\n+                permission.name = \" \".join(new_str) + \" \" + self.new_model\n+                try:\n+                    with transaction.atomic(using=db):\n+                        permission.save(update_fields={\"name\", \"codename\"})\n+                except IntegrityError:\n+                    # Gracefully fallback if a stale content type causes a\n+                    # conflict as update_contenttypes will take care of asking the\n+                    # user what should be done next.\n+                    permission.model = old_model\n+            else:\n+                # Clear the cache as the `get_by_natual_key()` call will cache\n+                # the renamed ContentType instance by its old model name.\n+                # Permission.objects.clear_cache()\n+                pass\n+\n+    def rename_forward(self, apps, schema_editor):\n+        self._rename(apps, schema_editor, self.old_model, self.new_model)\n+\n+    def rename_backward(self, apps, schema_editor):\n+        self._rename(apps, schema_editor, self.new_model, self.old_model)\n+\n+\n+def update_permissions(\n+    plan,\n+    verbosity=2,\n+    interactive=True,\n+    using=DEFAULT_DB_ALIAS,\n+    apps=global_apps,\n+    **kwargs,\n+):\n+    \"\"\"\n+    Insert a `RenameContentType` operation after every planned `RenameModel`\n+    operation.\n+    \"\"\"\n+    if plan is None:\n+        return\n+\n+    # Determine whether or not the ContentType model is available.\n+    try:\n+        ContentType = apps.get_model(\"contenttypes\", \"ContentType\")\n+        apps.get_model(\"auth\", \"Permission\")\n+    except LookupError:\n+        available = False\n+    else:\n+        if not router.allow_migrate_model(using, ContentType):\n+            return\n+        available = True\n+\n+    if not available:\n+        return",
      "comment": "without this the tests (the global ones) do fail, since at some point the function called while permission model is not defined.\r\nI added tests that you asked for and pushed them, if I did something wrong or misunderstood you let me know",
      "comment_id": 2122358303,
      "user": "artirix1927",
      "created_at": "2025-06-03T00:02:16Z",
      "url": "https://github.com/django/django/pull/19405#discussion_r2122358303"
    },
    {
      "repo": "django/django",
      "pr_number": 19405,
      "file_path": "tests/auth_tests/test_management.py",
      "line": 1549,
      "side": "RIGHT",
      "diff_hunk": "@@ -1522,6 +1531,186 @@ def test_permission_with_proxy_content_type_created(self):\n         )\n \n \n+class RenamePermissionTests(TestCase):\n+\n+    def setUp(self):\n+        ct, _ = ContentType.objects.get_or_create(\n+            app_label=\"auth_tests\", model=\"OldModel\"\n+        )\n+\n+        Permission.objects.get_or_create(\n+            codename=\"change_oldmodel\", name=\"Can change oldmodel\", content_type=ct\n+        )\n+        pre_migrate.connect(update_permissions)\n+\n+    def tearDown(self):\n+        pre_migrate.disconnect(update_permissions)\n+\n+    def test_fake_migration_plan(self):",
      "comment": "Thank you for the updates!\r\n\r\nI just realized we might want the testing to be closer to what was done in f179113e6cbc8ba0a8d4e87e1d4410fb61d63e75 where we call migrate and migrate back to zero to check everything was undone. I think this is cleaner than having some of the \"setup\" and \"clean state\" that exists in some of the tests. Would you mind taking a look?",
      "comment_id": 2145084652,
      "user": "sarahboyce",
      "created_at": "2025-06-13T13:21:51Z",
      "url": "https://github.com/django/django/pull/19405#discussion_r2145084652"
    },
    {
      "repo": "django/django",
      "pr_number": 19405,
      "file_path": "django/contrib/auth/management/__init__.py",
      "line": 125,
      "side": "RIGHT",
      "diff_hunk": "@@ -108,6 +109,90 @@ def create_permissions(\n             print(\"Adding permission '%s'\" % perm)\n \n \n+class RenamePermission(migrations.RunPython):\n+    def __init__(self, app_label, old_model, new_model):\n+        self.app_label = app_label\n+        self.old_model = old_model\n+        self.new_model = new_model\n+        super(RenamePermission, self).__init__(\n+            self.rename_forward, self.rename_backward\n+        )\n+\n+    def _rename(self, apps, schema_editor, old_model, new_model):\n+        ContentType = apps.get_model(\"contenttypes\", \"ContentType\")\n+        from django.contrib.auth.models import Permission  # real live model import",
      "comment": "Can you explain why this shouldn't be `Permission = apps.get_model(\"auth\", \"Permission\")`?\r\nI see there are some test failures but the direct import feels wrong",
      "comment_id": 2215677607,
      "user": "sarahboyce",
      "created_at": "2025-07-18T10:19:15Z",
      "url": "https://github.com/django/django/pull/19405#discussion_r2215677607"
    },
    {
      "repo": "django/django",
      "pr_number": 19405,
      "file_path": "tests/auth_tests/test_management.py",
      "line": 1753,
      "side": "RIGHT",
      "diff_hunk": "@@ -1522,6 +1528,208 @@ def test_permission_with_proxy_content_type_created(self):\n         )\n \n \n+@override_settings(\n+    MIGRATION_MODULES=dict(\n+        settings.MIGRATION_MODULES,\n+        auth_tests=\"auth_tests.operations_migrations\",\n+    ),\n+)\n+class PermissionRenameOperationsTests(TransactionTestCase):\n+    available_apps = [\n+        \"django.contrib.contenttypes\",\n+        \"django.contrib.auth\",\n+        \"auth_tests\",\n+    ]\n+\n+    def setUp(self):\n+        app_config = apps.get_app_config(\"auth_tests\")\n+        models.signals.post_migrate.connect(\n+            self.assertOperationsInjected, sender=app_config\n+        )\n+\n+    def tearDown(self):\n+        app_config = apps.get_app_config(\"auth_tests\")\n+        models.signals.post_migrate.disconnect(\n+            self.assertOperationsInjected, sender=app_config\n+        )\n+\n+    def assertOperationsInjected(self, plan, **kwargs):\n+        for migration, _backward in plan:\n+            operations = iter(migration.operations)\n+            for operation in operations:\n+                if isinstance(operation, migrations.RenameModel):\n+                    next_operation = next(operations)\n+                    self.assertTrue(\n+                        any(isinstance(op, RenamePermission) for op in operations),\n+                        \"RenamePermission not found in injected operations\",\n+                    )\n+                    self.assertEqual(next_operation.app_label, migration.app_label)\n+                    self.assertEqual(next_operation.old_model, operation.old_name_lower)\n+                    self.assertEqual(next_operation.new_model, operation.new_name_lower)\n+\n+    def test_permission_rename(self):\n+        ct = ContentType.objects.create(app_label=\"auth_tests\", model=\"oldmodel\")\n+        Permission.objects.create(\n+            codename=\"change_oldmodel\",\n+            name=\"Can change oldmodel\",\n+            content_type=ct,\n+        )\n+\n+        self.assertTrue(Permission.objects.filter(name=\"Can change oldmodel\").exists())\n+        self.assertFalse(Permission.objects.filter(name=\"Can change newmodel\").exists())\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertFalse(Permission.objects.filter(name=\"Can change oldmodel\").exists())\n+        self.assertTrue(Permission.objects.filter(name=\"Can change newmodel\").exists())\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertTrue(Permission.objects.filter(name=\"Can change oldmodel\").exists())\n+        self.assertFalse(Permission.objects.filter(name=\"Can change newmodel\").exists())\n+\n+    def test_rename_skipped_if_router_disallows(self):\n+        ct = ContentType.objects.create(app_label=\"auth_tests\", model=\"oldmodel\")\n+        Permission.objects.create(\n+            codename=\"change_oldmodel\",\n+            name=\"Can change oldmodel\",\n+            content_type=ct,\n+        )\n+\n+        with mock.patch(\"django.db.router.allow_migrate_model\", return_value=False):\n+            call_command(\n+                \"migrate\",\n+                \"auth_tests\",\n+                database=\"default\",\n+                interactive=False,\n+                verbosity=0,\n+            )\n+\n+        self.assertTrue(Permission.objects.filter(codename=\"change_oldmodel\").exists())\n+        self.assertFalse(Permission.objects.filter(codename=\"change_newmodel\").exists())\n+\n+    def test_rename_backward_does_nothing_if_no_permissions(self):\n+        Permission.objects.filter(content_type__app_label=\"auth_tests\").delete()\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+\n+        self.assertFalse(\n+            Permission.objects.filter(\n+                codename__in=[\"change_oldmodel\", \"change_newmodel\"]\n+            ).exists()\n+        )\n+\n+    def test_rename_permission_integrity_error(self):\n+        ct = ContentType.objects.create(app_label=\"auth_tests\", model=\"oldmodel\")\n+        Permission.objects.create(\n+            codename=\"change_newmodel\",\n+            name=\"Can change newmodel\",\n+            content_type=ct,\n+        )\n+        Permission.objects.create(\n+            codename=\"change_oldmodel\",\n+            name=\"Can change oldmodel\",\n+            content_type=ct,\n+        )\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+\n+        self.assertTrue(\n+            Permission.objects.filter(\n+                codename=\"change_oldmodel\",\n+                name=\"Can change oldmodel\",\n+            ).exists()\n+        )\n+        self.assertEqual(\n+            Permission.objects.filter(\n+                codename=\"change_newmodel\",\n+                name=\"Can change newmodel\",\n+            ).count(),\n+            1,\n+        )\n+\n+    def test_multiple_permission_types(self):\n+        \"\"\"Tests that all permission types (add/change/delete/view) are renamed\"\"\"\n+        ct = ContentType.objects.create(app_label=\"auth_tests\", model=\"oldmodel\")\n+        for action in [\"add\", \"change\", \"delete\", \"view\"]:\n+            Permission.objects.create(\n+                codename=f\"{action}_oldmodel\",\n+                name=f\"Can {action} oldmodel\",\n+                content_type=ct,\n+            )\n+\n+        call_command(\"migrate\", \"auth_tests\", verbosity=0)\n+\n+        for action in [\"add\", \"change\", \"delete\", \"view\"]:\n+            self.assertFalse(\n+                Permission.objects.filter(codename=f\"{action}_oldmodel\").exists()\n+            )\n+            self.assertTrue(\n+                Permission.objects.filter(codename=f\"{action}_newmodel\").exists()\n+            )\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        for action in [\"add\", \"change\", \"delete\", \"view\"]:\n+            self.assertTrue(\n+                Permission.objects.filter(codename=f\"{action}_oldmodel\").exists()\n+            )\n+            self.assertFalse(\n+                Permission.objects.filter(codename=f\"{action}_newmodel\").exists()\n+            )\n+\n+    def test_case_sensitivity(self):\n+        \"\"\"Tests that renaming works with mixed-case model names\"\"\"\n+        ct = ContentType.objects.create(app_label=\"auth_tests\", model=\"OldModel\")\n+        Permission.objects.create(\n+            codename=\"change_oldmodel\",\n+            name=\"Can change OldModel\",\n+            content_type=ct,\n+        )\n+\n+        call_command(\"migrate\", \"auth_tests\", verbosity=0)\n+        self.assertTrue(Permission.objects.filter(codename=\"change_newmodel\").exists())\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+",
      "comment": "```suggestion\r\n        self.assertFalse(Permission.objects.filter(codename=\"change_newmodel\").exists())\r\n\r\n```\r\nWhen I add the following assertion to check this has reversed, this fails. Can you look into this?",
      "comment_id": 2215681390,
      "user": "sarahboyce",
      "created_at": "2025-07-18T10:20:32Z",
      "url": "https://github.com/django/django/pull/19405#discussion_r2215681390"
    },
    {
      "repo": "django/django",
      "pr_number": 19405,
      "file_path": "django/contrib/auth/management/__init__.py",
      "line": 125,
      "side": "RIGHT",
      "diff_hunk": "@@ -108,6 +109,90 @@ def create_permissions(\n             print(\"Adding permission '%s'\" % perm)\n \n \n+class RenamePermission(migrations.RunPython):\n+    def __init__(self, app_label, old_model, new_model):\n+        self.app_label = app_label\n+        self.old_model = old_model\n+        self.new_model = new_model\n+        super(RenamePermission, self).__init__(\n+            self.rename_forward, self.rename_backward\n+        )\n+\n+    def _rename(self, apps, schema_editor, old_model, new_model):\n+        ContentType = apps.get_model(\"contenttypes\", \"ContentType\")\n+        from django.contrib.auth.models import Permission  # real live model import",
      "comment": "Okay, so as I understand, the tests don't pass because the apps.get_model() function returns frozen (`__fake__`) model. Which does not have foreign key relationships yet and does not have content_type field at all. I tried to fix this by avoiding the foreign keys and content types to find the needed permissions, but the permission class has its own ordering that involves contenttypes so this does not work too. I can call .order_by() inside the operation, which will clear the ordering and wont raise errors and will allow me to query for permissions by codename strings and etc, while  avoiding contettypes. Other than that, the he only way I could do this is by using raw sql inside the migration operation which seems even worse than using live model.",
      "comment_id": 2217998863,
      "user": "artirix1927",
      "created_at": "2025-07-20T23:23:05Z",
      "url": "https://github.com/django/django/pull/19405#discussion_r2217998863"
    },
    {
      "repo": "django/django",
      "pr_number": 19405,
      "file_path": "tests/auth_tests/test_management.py",
      "line": 1753,
      "side": "RIGHT",
      "diff_hunk": "@@ -1522,6 +1528,208 @@ def test_permission_with_proxy_content_type_created(self):\n         )\n \n \n+@override_settings(\n+    MIGRATION_MODULES=dict(\n+        settings.MIGRATION_MODULES,\n+        auth_tests=\"auth_tests.operations_migrations\",\n+    ),\n+)\n+class PermissionRenameOperationsTests(TransactionTestCase):\n+    available_apps = [\n+        \"django.contrib.contenttypes\",\n+        \"django.contrib.auth\",\n+        \"auth_tests\",\n+    ]\n+\n+    def setUp(self):\n+        app_config = apps.get_app_config(\"auth_tests\")\n+        models.signals.post_migrate.connect(\n+            self.assertOperationsInjected, sender=app_config\n+        )\n+\n+    def tearDown(self):\n+        app_config = apps.get_app_config(\"auth_tests\")\n+        models.signals.post_migrate.disconnect(\n+            self.assertOperationsInjected, sender=app_config\n+        )\n+\n+    def assertOperationsInjected(self, plan, **kwargs):\n+        for migration, _backward in plan:\n+            operations = iter(migration.operations)\n+            for operation in operations:\n+                if isinstance(operation, migrations.RenameModel):\n+                    next_operation = next(operations)\n+                    self.assertTrue(\n+                        any(isinstance(op, RenamePermission) for op in operations),\n+                        \"RenamePermission not found in injected operations\",\n+                    )\n+                    self.assertEqual(next_operation.app_label, migration.app_label)\n+                    self.assertEqual(next_operation.old_model, operation.old_name_lower)\n+                    self.assertEqual(next_operation.new_model, operation.new_name_lower)\n+\n+    def test_permission_rename(self):\n+        ct = ContentType.objects.create(app_label=\"auth_tests\", model=\"oldmodel\")\n+        Permission.objects.create(\n+            codename=\"change_oldmodel\",\n+            name=\"Can change oldmodel\",\n+            content_type=ct,\n+        )\n+\n+        self.assertTrue(Permission.objects.filter(name=\"Can change oldmodel\").exists())\n+        self.assertFalse(Permission.objects.filter(name=\"Can change newmodel\").exists())\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertFalse(Permission.objects.filter(name=\"Can change oldmodel\").exists())\n+        self.assertTrue(Permission.objects.filter(name=\"Can change newmodel\").exists())\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertTrue(Permission.objects.filter(name=\"Can change oldmodel\").exists())\n+        self.assertFalse(Permission.objects.filter(name=\"Can change newmodel\").exists())\n+\n+    def test_rename_skipped_if_router_disallows(self):\n+        ct = ContentType.objects.create(app_label=\"auth_tests\", model=\"oldmodel\")\n+        Permission.objects.create(\n+            codename=\"change_oldmodel\",\n+            name=\"Can change oldmodel\",\n+            content_type=ct,\n+        )\n+\n+        with mock.patch(\"django.db.router.allow_migrate_model\", return_value=False):\n+            call_command(\n+                \"migrate\",\n+                \"auth_tests\",\n+                database=\"default\",\n+                interactive=False,\n+                verbosity=0,\n+            )\n+\n+        self.assertTrue(Permission.objects.filter(codename=\"change_oldmodel\").exists())\n+        self.assertFalse(Permission.objects.filter(codename=\"change_newmodel\").exists())\n+\n+    def test_rename_backward_does_nothing_if_no_permissions(self):\n+        Permission.objects.filter(content_type__app_label=\"auth_tests\").delete()\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+\n+        self.assertFalse(\n+            Permission.objects.filter(\n+                codename__in=[\"change_oldmodel\", \"change_newmodel\"]\n+            ).exists()\n+        )\n+\n+    def test_rename_permission_integrity_error(self):\n+        ct = ContentType.objects.create(app_label=\"auth_tests\", model=\"oldmodel\")\n+        Permission.objects.create(\n+            codename=\"change_newmodel\",\n+            name=\"Can change newmodel\",\n+            content_type=ct,\n+        )\n+        Permission.objects.create(\n+            codename=\"change_oldmodel\",\n+            name=\"Can change oldmodel\",\n+            content_type=ct,\n+        )\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+\n+        self.assertTrue(\n+            Permission.objects.filter(\n+                codename=\"change_oldmodel\",\n+                name=\"Can change oldmodel\",\n+            ).exists()\n+        )\n+        self.assertEqual(\n+            Permission.objects.filter(\n+                codename=\"change_newmodel\",\n+                name=\"Can change newmodel\",\n+            ).count(),\n+            1,\n+        )\n+\n+    def test_multiple_permission_types(self):\n+        \"\"\"Tests that all permission types (add/change/delete/view) are renamed\"\"\"\n+        ct = ContentType.objects.create(app_label=\"auth_tests\", model=\"oldmodel\")\n+        for action in [\"add\", \"change\", \"delete\", \"view\"]:\n+            Permission.objects.create(\n+                codename=f\"{action}_oldmodel\",\n+                name=f\"Can {action} oldmodel\",\n+                content_type=ct,\n+            )\n+\n+        call_command(\"migrate\", \"auth_tests\", verbosity=0)\n+\n+        for action in [\"add\", \"change\", \"delete\", \"view\"]:\n+            self.assertFalse(\n+                Permission.objects.filter(codename=f\"{action}_oldmodel\").exists()\n+            )\n+            self.assertTrue(\n+                Permission.objects.filter(codename=f\"{action}_newmodel\").exists()\n+            )\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        for action in [\"add\", \"change\", \"delete\", \"view\"]:\n+            self.assertTrue(\n+                Permission.objects.filter(codename=f\"{action}_oldmodel\").exists()\n+            )\n+            self.assertFalse(\n+                Permission.objects.filter(codename=f\"{action}_newmodel\").exists()\n+            )\n+\n+    def test_case_sensitivity(self):\n+        \"\"\"Tests that renaming works with mixed-case model names\"\"\"\n+        ct = ContentType.objects.create(app_label=\"auth_tests\", model=\"OldModel\")\n+        Permission.objects.create(\n+            codename=\"change_oldmodel\",\n+            name=\"Can change OldModel\",\n+            content_type=ct,\n+        )\n+\n+        call_command(\"migrate\", \"auth_tests\", verbosity=0)\n+        self.assertTrue(Permission.objects.filter(codename=\"change_newmodel\").exists())\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+",
      "comment": "I looked into this and to be honest that test is not needed and it didnt pass the test because when creating a contenttype for this test I put the \"model=\" with camel case which wont ever happen in real app since django manages the names by itself and lowers all the names of the models to lower case automatically.",
      "comment_id": 2218015036,
      "user": "artirix1927",
      "created_at": "2025-07-21T00:16:10Z",
      "url": "https://github.com/django/django/pull/19405#discussion_r2218015036"
    },
    {
      "repo": "django/django",
      "pr_number": 19405,
      "file_path": "django/contrib/auth/management/__init__.py",
      "line": 138,
      "side": "RIGHT",
      "diff_hunk": "@@ -108,6 +109,90 @@ def create_permissions(\n             print(\"Adding permission '%s'\" % perm)\n \n \n+class RenamePermission(migrations.RunPython):\n+    def __init__(self, app_label, old_model, new_model):\n+        self.app_label = app_label\n+        self.old_model = old_model\n+        self.new_model = new_model\n+        super(RenamePermission, self).__init__(\n+            self.rename_forward, self.rename_backward\n+        )\n+\n+    def _rename(self, apps, schema_editor, old_model, new_model):\n+        ContentType = apps.get_model(\"contenttypes\", \"ContentType\")\n+        from django.contrib.auth.models import Permission  # real live model import\n+\n+        db = schema_editor.connection.alias\n+        if not router.allow_migrate_model(db, Permission):\n+            return\n+\n+        ctypes = ContentType.objects.filter(\n+            app_label=self.app_label, model__icontains=old_model\n+        )\n+        permissions = Permission.objects.filter(\n+            content_type_id__in=[ct.id for ct in ctypes]\n+        )\n+        for permission in permissions:\n+            prefix = permission.codename.split(\"_\")[0]\n+            new_codename = f\"{prefix}_{new_model}\"\n+            new_name = f\"Can {prefix} {new_model}\"",
      "comment": "```suggestion\r\n            default_verbose_name = camel_case_to_spaces(new_model)\r\n            new_name = f\"Can {prefix} {default_verbose_name}\"\r\n```\r\nI think the Permission name uses the model verbose_name which I'm not sure we can access but we can probably do a better job of building. This will need some test updates :+1: ",
      "comment_id": 2218547038,
      "user": "sarahboyce",
      "created_at": "2025-07-21T08:45:53Z",
      "url": "https://github.com/django/django/pull/19405#discussion_r2218547038"
    },
    {
      "repo": "django/django",
      "pr_number": 19405,
      "file_path": "django/contrib/auth/management/__init__.py",
      "line": 156,
      "side": "RIGHT",
      "diff_hunk": "@@ -108,6 +109,90 @@ def create_permissions(\n             print(\"Adding permission '%s'\" % perm)\n \n \n+class RenamePermission(migrations.RunPython):\n+    def __init__(self, app_label, old_model, new_model):\n+        self.app_label = app_label\n+        self.old_model = old_model\n+        self.new_model = new_model\n+        super(RenamePermission, self).__init__(\n+            self.rename_forward, self.rename_backward\n+        )\n+\n+    def _rename(self, apps, schema_editor, old_model, new_model):\n+        ContentType = apps.get_model(\"contenttypes\", \"ContentType\")\n+        from django.contrib.auth.models import Permission  # real live model import\n+\n+        db = schema_editor.connection.alias\n+        if not router.allow_migrate_model(db, Permission):\n+            return\n+\n+        ctypes = ContentType.objects.filter(\n+            app_label=self.app_label, model__icontains=old_model\n+        )\n+        permissions = Permission.objects.filter(\n+            content_type_id__in=[ct.id for ct in ctypes]\n+        )\n+        for permission in permissions:\n+            prefix = permission.codename.split(\"_\")[0]\n+            new_codename = f\"{prefix}_{new_model}\"\n+            new_name = f\"Can {prefix} {new_model}\"\n+            if permission.codename != new_codename or permission.name != new_name:\n+                permission.codename = new_codename\n+                permission.name = new_name\n+                try:\n+                    with transaction.atomic(using=db):\n+                        permission.save(update_fields={\"name\", \"codename\"})\n+                except IntegrityError:\n+                    # Skip conflicting permissions.\n+                    pass\n+\n+    def rename_forward(self, apps, schema_editor):\n+        self._rename(apps, schema_editor, self.old_model, self.new_model)\n+\n+    def rename_backward(self, apps, schema_editor):\n+        self._rename(apps, schema_editor, self.new_model, self.old_model)\n+\n+\n+def inject_permission_rename_operations(",
      "comment": "Why not to keep the same naming pattern? `inject_*_operations` sounds unnecessary\r\n```suggestion\r\ndef rename_permissions(\r\n```",
      "comment_id": 2218592266,
      "user": "felixxm",
      "created_at": "2025-07-21T09:07:17Z",
      "url": "https://github.com/django/django/pull/19405#discussion_r2218592266"
    },
    {
      "repo": "django/django",
      "pr_number": 19405,
      "file_path": "django/contrib/auth/management/__init__.py",
      "line": 178,
      "side": "RIGHT",
      "diff_hunk": "@@ -108,6 +109,90 @@ def create_permissions(\n             print(\"Adding permission '%s'\" % perm)\n \n \n+class RenamePermission(migrations.RunPython):\n+    def __init__(self, app_label, old_model, new_model):\n+        self.app_label = app_label\n+        self.old_model = old_model\n+        self.new_model = new_model\n+        super(RenamePermission, self).__init__(\n+            self.rename_forward, self.rename_backward\n+        )\n+\n+    def _rename(self, apps, schema_editor, old_model, new_model):\n+        ContentType = apps.get_model(\"contenttypes\", \"ContentType\")\n+        from django.contrib.auth.models import Permission  # real live model import\n+\n+        db = schema_editor.connection.alias\n+        if not router.allow_migrate_model(db, Permission):\n+            return\n+\n+        ctypes = ContentType.objects.filter(\n+            app_label=self.app_label, model__icontains=old_model\n+        )\n+        permissions = Permission.objects.filter(\n+            content_type_id__in=[ct.id for ct in ctypes]\n+        )\n+        for permission in permissions:\n+            prefix = permission.codename.split(\"_\")[0]\n+            new_codename = f\"{prefix}_{new_model}\"\n+            new_name = f\"Can {prefix} {new_model}\"\n+            if permission.codename != new_codename or permission.name != new_name:\n+                permission.codename = new_codename\n+                permission.name = new_name\n+                try:\n+                    with transaction.atomic(using=db):\n+                        permission.save(update_fields={\"name\", \"codename\"})\n+                except IntegrityError:\n+                    # Skip conflicting permissions.\n+                    pass\n+\n+    def rename_forward(self, apps, schema_editor):\n+        self._rename(apps, schema_editor, self.old_model, self.new_model)\n+\n+    def rename_backward(self, apps, schema_editor):\n+        self._rename(apps, schema_editor, self.new_model, self.old_model)\n+\n+\n+def inject_permission_rename_operations(\n+    plan,\n+    verbosity=2,\n+    interactive=True,\n+    using=DEFAULT_DB_ALIAS,\n+    apps=global_apps,\n+    **kwargs,\n+):\n+    \"\"\"\n+    Insert a `RenamePermissionType` operation after every planned `RenameModel`\n+    operation.\n+    \"\"\"\n+    try:\n+        Permission = apps.get_model(\"auth\", \"Permission\")\n+    except LookupError:\n+        available = False\n+    else:\n+        if not router.allow_migrate_model(using, Permission):\n+            return\n+        available = True\n+\n+    if not available:\n+        return",
      "comment": "Do we need the `available` temporary variable?\r\n```suggestion\r\n    try:\r\n        Permission = apps.get_model(\"auth\", \"Permission\")\r\n    except LookupError:\r\n        return\r\n    else:\r\n        if not router.allow_migrate_model(using, Permission):\r\n            return\r\n\r\n```",
      "comment_id": 2218595401,
      "user": "felixxm",
      "created_at": "2025-07-21T09:08:53Z",
      "url": "https://github.com/django/django/pull/19405#discussion_r2218595401"
    },
    {
      "repo": "django/django",
      "pr_number": 19405,
      "file_path": "django/contrib/auth/management/__init__.py",
      "line": 131,
      "side": "RIGHT",
      "diff_hunk": "@@ -108,6 +109,108 @@ def create_permissions(\n             print(\"Adding permission '%s'\" % perm)\n \n \n+class RenamePermission(migrations.RunPython):\n+    def __init__(self, app_label, old_model, new_model):\n+        self.app_label = app_label\n+        self.old_model = old_model\n+        self.new_model = new_model\n+        super(RenamePermission, self).__init__(\n+            self.rename_forward, self.rename_backward\n+        )\n+\n+    def _rename(self, apps, schema_editor, old_model, new_model):\n+        Permission = apps.get_model(\"auth\", \"Permission\")\n+        ContentType = apps.get_model(\"contenttypes\", \"ContentType\")\n+\n+        db = schema_editor.connection.alias\n+\n+        if not router.allow_migrate_model(db, Permission):\n+            return",
      "comment": "Just to clarify \u2014 for the missing test, do you mean I should assert that the RenamePermission operation is not in the migration plan when the router disallows it? I want to make sure I understand what you expecting from the test. Sorry for being a little bit slow",
      "comment_id": 2253337693,
      "user": "artirix1927",
      "created_at": "2025-08-05T06:57:22Z",
      "url": "https://github.com/django/django/pull/19405#discussion_r2253337693"
    },
    {
      "repo": "django/django",
      "pr_number": 19405,
      "file_path": "django/contrib/auth/management/__init__.py",
      "line": 131,
      "side": "RIGHT",
      "diff_hunk": "@@ -108,6 +109,108 @@ def create_permissions(\n             print(\"Adding permission '%s'\" % perm)\n \n \n+class RenamePermission(migrations.RunPython):\n+    def __init__(self, app_label, old_model, new_model):\n+        self.app_label = app_label\n+        self.old_model = old_model\n+        self.new_model = new_model\n+        super(RenamePermission, self).__init__(\n+            self.rename_forward, self.rename_backward\n+        )\n+\n+    def _rename(self, apps, schema_editor, old_model, new_model):\n+        Permission = apps.get_model(\"auth\", \"Permission\")\n+        ContentType = apps.get_model(\"contenttypes\", \"ContentType\")\n+\n+        db = schema_editor.connection.alias\n+\n+        if not router.allow_migrate_model(db, Permission):\n+            return",
      "comment": "I think in this case, the `RenamePermission` is in the plan but `router.allow_migrate_model(db, Permission)` returns `False` and so no permission renames, or queries, should take place",
      "comment_id": 2253388157,
      "user": "sarahboyce",
      "created_at": "2025-08-05T07:21:11Z",
      "url": "https://github.com/django/django/pull/19405#discussion_r2253388157"
    },
    {
      "repo": "django/django",
      "pr_number": 19405,
      "file_path": "django/contrib/auth/management/__init__.py",
      "line": 131,
      "side": "RIGHT",
      "diff_hunk": "@@ -108,6 +109,108 @@ def create_permissions(\n             print(\"Adding permission '%s'\" % perm)\n \n \n+class RenamePermission(migrations.RunPython):\n+    def __init__(self, app_label, old_model, new_model):\n+        self.app_label = app_label\n+        self.old_model = old_model\n+        self.new_model = new_model\n+        super(RenamePermission, self).__init__(\n+            self.rename_forward, self.rename_backward\n+        )\n+\n+    def _rename(self, apps, schema_editor, old_model, new_model):\n+        Permission = apps.get_model(\"auth\", \"Permission\")\n+        ContentType = apps.get_model(\"contenttypes\", \"ContentType\")\n+\n+        db = schema_editor.connection.alias\n+\n+        if not router.allow_migrate_model(db, Permission):\n+            return",
      "comment": "Do you think it would be appropriate to call the RenamePermission manually in this specific case? Because I cant find a way to write the test so the router allows migration at the stage of building the migration plan (so it can actually insert the rename operation) but disallows it in the runtime (when the actual rename happens).\r\n\r\n\r\n```python\r\n       op = RenamePermission(\r\n            app_label=\"auth_tests\",\r\n            old_model=\"OldModel\",\r\n            new_model=\"NewModel\",\r\n        )\r\n\r\n        with connection.schema_editor() as schema_editor:\r\n            op.rename_forward(apps, schema_editor)\r\n ",
      "comment_id": 2259205764,
      "user": "artirix1927",
      "created_at": "2025-08-07T06:20:45Z",
      "url": "https://github.com/django/django/pull/19405#discussion_r2259205764"
    },
    {
      "repo": "django/django",
      "pr_number": 19405,
      "file_path": "django/contrib/auth/management/__init__.py",
      "line": 131,
      "side": "RIGHT",
      "diff_hunk": "@@ -108,6 +109,108 @@ def create_permissions(\n             print(\"Adding permission '%s'\" % perm)\n \n \n+class RenamePermission(migrations.RunPython):\n+    def __init__(self, app_label, old_model, new_model):\n+        self.app_label = app_label\n+        self.old_model = old_model\n+        self.new_model = new_model\n+        super(RenamePermission, self).__init__(\n+            self.rename_forward, self.rename_backward\n+        )\n+\n+    def _rename(self, apps, schema_editor, old_model, new_model):\n+        Permission = apps.get_model(\"auth\", \"Permission\")\n+        ContentType = apps.get_model(\"contenttypes\", \"ContentType\")\n+\n+        db = schema_editor.connection.alias\n+\n+        if not router.allow_migrate_model(db, Permission):\n+            return",
      "comment": "Perhaps we just remove it :thinking: \r\nWe have copied this because it exists in `django.contrib.contenttypes.management.RenameContentType._rename` but this is also not covered by tests and may not be needed :thinking: ",
      "comment_id": 2259672062,
      "user": "sarahboyce",
      "created_at": "2025-08-07T09:18:48Z",
      "url": "https://github.com/django/django/pull/19405#discussion_r2259672062"
    },
    {
      "repo": "django/django",
      "pr_number": 19405,
      "file_path": "django/contrib/auth/management/__init__.py",
      "line": 147,
      "side": "RIGHT",
      "diff_hunk": "@@ -108,6 +110,84 @@ def create_permissions(\n             print(\"Adding permission '%s'\" % perm)\n \n \n+class RenamePermission(migrations.RunPython):\n+    def __init__(self, app_label, old_model, new_model):\n+        self.app_label = app_label\n+        self.old_model = old_model\n+        self.new_model = new_model\n+        super(RenamePermission, self).__init__(\n+            self.rename_forward, self.rename_backward\n+        )\n+\n+    def _rename(self, apps, schema_editor, old_model, new_model):\n+        ContentType = apps.get_model(\"contenttypes\", \"ContentType\")\n+        # Use the live Permission model instead of the frozen one, since frozen\n+        # models do not retain foreign key constraints.\n+        from django.contrib.auth.models import Permission\n+\n+        db = schema_editor.connection.alias\n+        ctypes = ContentType.objects.filter(\n+            app_label=self.app_label, model__icontains=old_model.lower()\n+        )\n+        for permission in Permission.objects.filter(\n+            content_type_id__in=ctypes.values(\"id\")\n+        ):\n+            prefix = permission.codename.split(\"_\")[0]\n+            default_verbose_name = camel_case_to_spaces(new_model)\n+\n+            new_codename = f\"{prefix}_{new_model.lower()}\"\n+            new_name = f\"Can {prefix} {default_verbose_name}\"\n+\n+            if permission.codename != new_codename or permission.name != new_name:\n+                permission.codename = new_codename\n+                permission.name = new_name\n+                try:\n+                    with transaction.atomic(using=db):\n+                        permission.save(update_fields={\"name\", \"codename\"})\n+                except IntegrityError:",
      "comment": "This could use a comment explaining under what circumstances `IntegrityError` happens.",
      "comment_id": 2566920018,
      "user": "timgraham",
      "created_at": "2025-11-27T01:35:20Z",
      "url": "https://github.com/django/django/pull/19405#discussion_r2566920018"
    },
    {
      "repo": "django/django",
      "pr_number": 19405,
      "file_path": "tests/auth_tests/test_management.py",
      "line": 1693,
      "side": "RIGHT",
      "diff_hunk": "@@ -1528,6 +1534,174 @@ def test_permission_with_proxy_content_type_created(self):\n         )\n \n \n+@override_settings(\n+    MIGRATION_MODULES=dict(\n+        settings.MIGRATION_MODULES,\n+        auth_tests=\"auth_tests.operations_migrations\",\n+    ),\n+)\n+class PermissionRenameOperationsTests(TransactionTestCase):\n+    available_apps = [\n+        \"django.contrib.contenttypes\",\n+        \"django.contrib.auth\",\n+        \"auth_tests\",\n+    ]\n+\n+    def setUp(self):\n+        app_config = apps.get_app_config(\"auth_tests\")\n+        models.signals.post_migrate.connect(\n+            self.assertOperationsInjected, sender=app_config\n+        )\n+        self.addCleanup(\n+            models.signals.post_migrate.disconnect,\n+            self.assertOperationsInjected,\n+            sender=app_config,\n+        )\n+\n+    def assertOperationsInjected(self, plan, **kwargs):\n+        for migration, _backward in plan:\n+            operations = iter(migration.operations)\n+            for operation in operations:\n+                if isinstance(operation, migrations.RenameModel):\n+                    next_operation = next(operations)\n+                    self.assertIsInstance(next_operation, RenamePermission)\n+                    self.assertEqual(next_operation.app_label, migration.app_label)\n+                    self.assertEqual(next_operation.old_model, operation.old_name)\n+                    self.assertEqual(next_operation.new_model, operation.new_name)\n+\n+    def test_permission_rename(self):\n+        ct = ContentType.objects.create(app_label=\"auth_tests\", model=\"oldmodel\")\n+        actions = [\"add\", \"change\", \"delete\", \"view\"]\n+        for action in actions:\n+            Permission.objects.create(\n+                codename=f\"{action}_oldmodel\",\n+                name=f\"Can {action} old model\",\n+                content_type=ct,\n+            )\n+\n+        call_command(\"migrate\", \"auth_tests\", verbosity=0)\n+        for action in actions:\n+            self.assertFalse(\n+                Permission.objects.filter(codename=f\"{action}_oldmodel\").exists()\n+            )\n+            self.assertTrue(\n+                Permission.objects.filter(codename=f\"{action}_newmodel\").exists()\n+            )\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+\n+        for action in actions:\n+            self.assertTrue(\n+                Permission.objects.filter(codename=f\"{action}_oldmodel\").exists()\n+            )\n+            self.assertFalse(\n+                Permission.objects.filter(codename=f\"{action}_newmodel\").exists()\n+            )\n+\n+    @mock.patch(\n+        \"django.db.router.allow_migrate_model\",\n+        return_value=False,\n+    )\n+    def test_rename_skipped_if_router_disallows(self, _):\n+        ct = ContentType.objects.create(app_label=\"auth_tests\", model=\"oldmodel\")\n+        Permission.objects.create(\n+            codename=\"change_oldmodel\",\n+            name=\"Can change old model\",\n+            content_type=ct,\n+        )\n+        # The rename operation should not be there when disallowed by router.\n+        app_config = apps.get_app_config(\"auth_tests\")\n+        models.signals.post_migrate.disconnect(\n+            self.assertOperationsInjected, sender=app_config\n+        )\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertTrue(Permission.objects.filter(codename=\"change_oldmodel\").exists())\n+        self.assertFalse(Permission.objects.filter(codename=\"change_newmodel\").exists())\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+\n+    def test_rename_backward_does_nothing_if_no_permissions(self):\n+        Permission.objects.filter(content_type__app_label=\"auth_tests\").delete()\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertFalse(\n+            Permission.objects.filter(\n+                codename__in=[\"change_oldmodel\", \"change_newmodel\"]\n+            ).exists()\n+        )\n+\n+    def test_rename_permission_conflict(self):\n+        ct = ContentType.objects.create(app_label=\"auth_tests\", model=\"oldmodel\")\n+        Permission.objects.create(\n+            codename=\"change_newmodel\",\n+            name=\"Can change new model\",\n+            content_type=ct,\n+        )\n+        Permission.objects.create(\n+            codename=\"change_oldmodel\",\n+            name=\"Can change old model\",\n+            content_type=ct,\n+        )\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertTrue(\n+            Permission.objects.filter(\n+                codename=\"change_oldmodel\",\n+                name=\"Can change old model\",\n+            ).exists()\n+        )\n+        self.assertEqual(\n+            Permission.objects.filter(\n+                codename=\"change_newmodel\",\n+                name=\"Can change new model\",\n+            ).count(),\n+            1,\n+        )",
      "comment": "This test could use an explanatory docstring and/or inline comments. It fails on Snowflake (because Snowflake doesn't support unique constraints, I think) and it's not obvious to me exactly what scenario the test is simulating. I'm guessing \"change_oldmodel\" can't be renamed to \"change_newmodel\" because it already exists, but there is no explanation of why the permission would already exist (related to my other comment).\r\n\r\nOn Snowflake, I've seen the test fail like this:\r\n```\r\n======================================================================\r\nFAIL: test_rename_permission_conflict (auth_tests.test_management.PermissionRenameOperationsTests.test_rename_permission_conflict)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/tim/code/django/tests/auth_tests/test_management.py\", line 1702, in test_rename_permission_conflict\r\n    self.assertTrue(\r\nAssertionError: False is not true\r\n```\r\nAnd then like this subsequently (I use `--keepdb`) so probably the test isn't isolated in this case.\r\n```\r\n======================================================================\r\nFAIL: test_rename_permission_conflict (auth_tests.test_management.PermissionRenameOperationsTests.test_rename_permission_conflict)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/tim/code/django/tests/auth_tests/test_management.py\", line 1708, in test_rename_permission_conflict\r\n    self.assertEqual(\r\nAssertionError: 2 != 1\r\n```",
      "comment_id": 2566927400,
      "user": "timgraham",
      "created_at": "2025-11-27T01:42:23Z",
      "url": "https://github.com/django/django/pull/19405#discussion_r2566927400"
    },
    {
      "repo": "django/django",
      "pr_number": 19405,
      "file_path": "tests/auth_tests/test_management.py",
      "line": 1693,
      "side": "RIGHT",
      "diff_hunk": "@@ -1528,6 +1534,174 @@ def test_permission_with_proxy_content_type_created(self):\n         )\n \n \n+@override_settings(\n+    MIGRATION_MODULES=dict(\n+        settings.MIGRATION_MODULES,\n+        auth_tests=\"auth_tests.operations_migrations\",\n+    ),\n+)\n+class PermissionRenameOperationsTests(TransactionTestCase):\n+    available_apps = [\n+        \"django.contrib.contenttypes\",\n+        \"django.contrib.auth\",\n+        \"auth_tests\",\n+    ]\n+\n+    def setUp(self):\n+        app_config = apps.get_app_config(\"auth_tests\")\n+        models.signals.post_migrate.connect(\n+            self.assertOperationsInjected, sender=app_config\n+        )\n+        self.addCleanup(\n+            models.signals.post_migrate.disconnect,\n+            self.assertOperationsInjected,\n+            sender=app_config,\n+        )\n+\n+    def assertOperationsInjected(self, plan, **kwargs):\n+        for migration, _backward in plan:\n+            operations = iter(migration.operations)\n+            for operation in operations:\n+                if isinstance(operation, migrations.RenameModel):\n+                    next_operation = next(operations)\n+                    self.assertIsInstance(next_operation, RenamePermission)\n+                    self.assertEqual(next_operation.app_label, migration.app_label)\n+                    self.assertEqual(next_operation.old_model, operation.old_name)\n+                    self.assertEqual(next_operation.new_model, operation.new_name)\n+\n+    def test_permission_rename(self):\n+        ct = ContentType.objects.create(app_label=\"auth_tests\", model=\"oldmodel\")\n+        actions = [\"add\", \"change\", \"delete\", \"view\"]\n+        for action in actions:\n+            Permission.objects.create(\n+                codename=f\"{action}_oldmodel\",\n+                name=f\"Can {action} old model\",\n+                content_type=ct,\n+            )\n+\n+        call_command(\"migrate\", \"auth_tests\", verbosity=0)\n+        for action in actions:\n+            self.assertFalse(\n+                Permission.objects.filter(codename=f\"{action}_oldmodel\").exists()\n+            )\n+            self.assertTrue(\n+                Permission.objects.filter(codename=f\"{action}_newmodel\").exists()\n+            )\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+\n+        for action in actions:\n+            self.assertTrue(\n+                Permission.objects.filter(codename=f\"{action}_oldmodel\").exists()\n+            )\n+            self.assertFalse(\n+                Permission.objects.filter(codename=f\"{action}_newmodel\").exists()\n+            )\n+\n+    @mock.patch(\n+        \"django.db.router.allow_migrate_model\",\n+        return_value=False,\n+    )\n+    def test_rename_skipped_if_router_disallows(self, _):\n+        ct = ContentType.objects.create(app_label=\"auth_tests\", model=\"oldmodel\")\n+        Permission.objects.create(\n+            codename=\"change_oldmodel\",\n+            name=\"Can change old model\",\n+            content_type=ct,\n+        )\n+        # The rename operation should not be there when disallowed by router.\n+        app_config = apps.get_app_config(\"auth_tests\")\n+        models.signals.post_migrate.disconnect(\n+            self.assertOperationsInjected, sender=app_config\n+        )\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertTrue(Permission.objects.filter(codename=\"change_oldmodel\").exists())\n+        self.assertFalse(Permission.objects.filter(codename=\"change_newmodel\").exists())\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+\n+    def test_rename_backward_does_nothing_if_no_permissions(self):\n+        Permission.objects.filter(content_type__app_label=\"auth_tests\").delete()\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertFalse(\n+            Permission.objects.filter(\n+                codename__in=[\"change_oldmodel\", \"change_newmodel\"]\n+            ).exists()\n+        )\n+\n+    def test_rename_permission_conflict(self):\n+        ct = ContentType.objects.create(app_label=\"auth_tests\", model=\"oldmodel\")\n+        Permission.objects.create(\n+            codename=\"change_newmodel\",\n+            name=\"Can change new model\",\n+            content_type=ct,\n+        )\n+        Permission.objects.create(\n+            codename=\"change_oldmodel\",\n+            name=\"Can change old model\",\n+            content_type=ct,\n+        )\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertTrue(\n+            Permission.objects.filter(\n+                codename=\"change_oldmodel\",\n+                name=\"Can change old model\",\n+            ).exists()\n+        )\n+        self.assertEqual(\n+            Permission.objects.filter(\n+                codename=\"change_newmodel\",\n+                name=\"Can change new model\",\n+            ).count(),\n+            1,\n+        )",
      "comment": "We need this test because previously the migration would create new permissions instead of renaming existing ones. This could leave unused permissions in the database, which might trigger integrity errors if duplicates exist.",
      "comment_id": 2567306885,
      "user": "artirix1927",
      "created_at": "2025-11-27T06:22:34Z",
      "url": "https://github.com/django/django/pull/19405#discussion_r2567306885"
    },
    {
      "repo": "django/django",
      "pr_number": 19405,
      "file_path": "tests/auth_tests/test_management.py",
      "line": 1693,
      "side": "RIGHT",
      "diff_hunk": "@@ -1528,6 +1534,174 @@ def test_permission_with_proxy_content_type_created(self):\n         )\n \n \n+@override_settings(\n+    MIGRATION_MODULES=dict(\n+        settings.MIGRATION_MODULES,\n+        auth_tests=\"auth_tests.operations_migrations\",\n+    ),\n+)\n+class PermissionRenameOperationsTests(TransactionTestCase):\n+    available_apps = [\n+        \"django.contrib.contenttypes\",\n+        \"django.contrib.auth\",\n+        \"auth_tests\",\n+    ]\n+\n+    def setUp(self):\n+        app_config = apps.get_app_config(\"auth_tests\")\n+        models.signals.post_migrate.connect(\n+            self.assertOperationsInjected, sender=app_config\n+        )\n+        self.addCleanup(\n+            models.signals.post_migrate.disconnect,\n+            self.assertOperationsInjected,\n+            sender=app_config,\n+        )\n+\n+    def assertOperationsInjected(self, plan, **kwargs):\n+        for migration, _backward in plan:\n+            operations = iter(migration.operations)\n+            for operation in operations:\n+                if isinstance(operation, migrations.RenameModel):\n+                    next_operation = next(operations)\n+                    self.assertIsInstance(next_operation, RenamePermission)\n+                    self.assertEqual(next_operation.app_label, migration.app_label)\n+                    self.assertEqual(next_operation.old_model, operation.old_name)\n+                    self.assertEqual(next_operation.new_model, operation.new_name)\n+\n+    def test_permission_rename(self):\n+        ct = ContentType.objects.create(app_label=\"auth_tests\", model=\"oldmodel\")\n+        actions = [\"add\", \"change\", \"delete\", \"view\"]\n+        for action in actions:\n+            Permission.objects.create(\n+                codename=f\"{action}_oldmodel\",\n+                name=f\"Can {action} old model\",\n+                content_type=ct,\n+            )\n+\n+        call_command(\"migrate\", \"auth_tests\", verbosity=0)\n+        for action in actions:\n+            self.assertFalse(\n+                Permission.objects.filter(codename=f\"{action}_oldmodel\").exists()\n+            )\n+            self.assertTrue(\n+                Permission.objects.filter(codename=f\"{action}_newmodel\").exists()\n+            )\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+\n+        for action in actions:\n+            self.assertTrue(\n+                Permission.objects.filter(codename=f\"{action}_oldmodel\").exists()\n+            )\n+            self.assertFalse(\n+                Permission.objects.filter(codename=f\"{action}_newmodel\").exists()\n+            )\n+\n+    @mock.patch(\n+        \"django.db.router.allow_migrate_model\",\n+        return_value=False,\n+    )\n+    def test_rename_skipped_if_router_disallows(self, _):\n+        ct = ContentType.objects.create(app_label=\"auth_tests\", model=\"oldmodel\")\n+        Permission.objects.create(\n+            codename=\"change_oldmodel\",\n+            name=\"Can change old model\",\n+            content_type=ct,\n+        )\n+        # The rename operation should not be there when disallowed by router.\n+        app_config = apps.get_app_config(\"auth_tests\")\n+        models.signals.post_migrate.disconnect(\n+            self.assertOperationsInjected, sender=app_config\n+        )\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertTrue(Permission.objects.filter(codename=\"change_oldmodel\").exists())\n+        self.assertFalse(Permission.objects.filter(codename=\"change_newmodel\").exists())\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+\n+    def test_rename_backward_does_nothing_if_no_permissions(self):\n+        Permission.objects.filter(content_type__app_label=\"auth_tests\").delete()\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertFalse(\n+            Permission.objects.filter(\n+                codename__in=[\"change_oldmodel\", \"change_newmodel\"]\n+            ).exists()\n+        )\n+\n+    def test_rename_permission_conflict(self):\n+        ct = ContentType.objects.create(app_label=\"auth_tests\", model=\"oldmodel\")\n+        Permission.objects.create(\n+            codename=\"change_newmodel\",\n+            name=\"Can change new model\",\n+            content_type=ct,\n+        )\n+        Permission.objects.create(\n+            codename=\"change_oldmodel\",\n+            name=\"Can change old model\",\n+            content_type=ct,\n+        )\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertTrue(\n+            Permission.objects.filter(\n+                codename=\"change_oldmodel\",\n+                name=\"Can change old model\",\n+            ).exists()\n+        )\n+        self.assertEqual(\n+            Permission.objects.filter(\n+                codename=\"change_newmodel\",\n+                name=\"Can change new model\",\n+            ).count(),\n+            1,\n+        )",
      "comment": "```======================================================================\r\nFAIL: test_rename_permission_conflict (auth_tests.test_management.PermissionRenameOperationsTests.test_rename_permission_conflict)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/tim/code/django/tests/auth_tests/test_management.py\", line 1708, in test_rename_permission_conflict\r\n    self.assertEqual(\r\nAssertionError: 2 != 1\r\n```\r\n\r\n\r\nYou get this only on the second run only?",
      "comment_id": 2567344147,
      "user": "artirix1927",
      "created_at": "2025-11-27T06:43:32Z",
      "url": "https://github.com/django/django/pull/19405#discussion_r2567344147"
    },
    {
      "repo": "django/django",
      "pr_number": 19405,
      "file_path": "tests/auth_tests/test_management.py",
      "line": 1693,
      "side": "RIGHT",
      "diff_hunk": "@@ -1528,6 +1534,174 @@ def test_permission_with_proxy_content_type_created(self):\n         )\n \n \n+@override_settings(\n+    MIGRATION_MODULES=dict(\n+        settings.MIGRATION_MODULES,\n+        auth_tests=\"auth_tests.operations_migrations\",\n+    ),\n+)\n+class PermissionRenameOperationsTests(TransactionTestCase):\n+    available_apps = [\n+        \"django.contrib.contenttypes\",\n+        \"django.contrib.auth\",\n+        \"auth_tests\",\n+    ]\n+\n+    def setUp(self):\n+        app_config = apps.get_app_config(\"auth_tests\")\n+        models.signals.post_migrate.connect(\n+            self.assertOperationsInjected, sender=app_config\n+        )\n+        self.addCleanup(\n+            models.signals.post_migrate.disconnect,\n+            self.assertOperationsInjected,\n+            sender=app_config,\n+        )\n+\n+    def assertOperationsInjected(self, plan, **kwargs):\n+        for migration, _backward in plan:\n+            operations = iter(migration.operations)\n+            for operation in operations:\n+                if isinstance(operation, migrations.RenameModel):\n+                    next_operation = next(operations)\n+                    self.assertIsInstance(next_operation, RenamePermission)\n+                    self.assertEqual(next_operation.app_label, migration.app_label)\n+                    self.assertEqual(next_operation.old_model, operation.old_name)\n+                    self.assertEqual(next_operation.new_model, operation.new_name)\n+\n+    def test_permission_rename(self):\n+        ct = ContentType.objects.create(app_label=\"auth_tests\", model=\"oldmodel\")\n+        actions = [\"add\", \"change\", \"delete\", \"view\"]\n+        for action in actions:\n+            Permission.objects.create(\n+                codename=f\"{action}_oldmodel\",\n+                name=f\"Can {action} old model\",\n+                content_type=ct,\n+            )\n+\n+        call_command(\"migrate\", \"auth_tests\", verbosity=0)\n+        for action in actions:\n+            self.assertFalse(\n+                Permission.objects.filter(codename=f\"{action}_oldmodel\").exists()\n+            )\n+            self.assertTrue(\n+                Permission.objects.filter(codename=f\"{action}_newmodel\").exists()\n+            )\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+\n+        for action in actions:\n+            self.assertTrue(\n+                Permission.objects.filter(codename=f\"{action}_oldmodel\").exists()\n+            )\n+            self.assertFalse(\n+                Permission.objects.filter(codename=f\"{action}_newmodel\").exists()\n+            )\n+\n+    @mock.patch(\n+        \"django.db.router.allow_migrate_model\",\n+        return_value=False,\n+    )\n+    def test_rename_skipped_if_router_disallows(self, _):\n+        ct = ContentType.objects.create(app_label=\"auth_tests\", model=\"oldmodel\")\n+        Permission.objects.create(\n+            codename=\"change_oldmodel\",\n+            name=\"Can change old model\",\n+            content_type=ct,\n+        )\n+        # The rename operation should not be there when disallowed by router.\n+        app_config = apps.get_app_config(\"auth_tests\")\n+        models.signals.post_migrate.disconnect(\n+            self.assertOperationsInjected, sender=app_config\n+        )\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertTrue(Permission.objects.filter(codename=\"change_oldmodel\").exists())\n+        self.assertFalse(Permission.objects.filter(codename=\"change_newmodel\").exists())\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+\n+    def test_rename_backward_does_nothing_if_no_permissions(self):\n+        Permission.objects.filter(content_type__app_label=\"auth_tests\").delete()\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertFalse(\n+            Permission.objects.filter(\n+                codename__in=[\"change_oldmodel\", \"change_newmodel\"]\n+            ).exists()\n+        )\n+\n+    def test_rename_permission_conflict(self):\n+        ct = ContentType.objects.create(app_label=\"auth_tests\", model=\"oldmodel\")\n+        Permission.objects.create(\n+            codename=\"change_newmodel\",\n+            name=\"Can change new model\",\n+            content_type=ct,\n+        )\n+        Permission.objects.create(\n+            codename=\"change_oldmodel\",\n+            name=\"Can change old model\",\n+            content_type=ct,\n+        )\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertTrue(\n+            Permission.objects.filter(\n+                codename=\"change_oldmodel\",\n+                name=\"Can change old model\",\n+            ).exists()\n+        )\n+        self.assertEqual(\n+            Permission.objects.filter(\n+                codename=\"change_newmodel\",\n+                name=\"Can change new model\",\n+            ).count(),\n+            1,\n+        )",
      "comment": "If I understand correctly, the test simulates a pre-Django 6.0 database where duplicate permissions exist after \"oldmodel\" was renamed to \"newmodel\". The ignoring of IntegrityError handles the case where \"newmodel\" is renamed back to \"oldmodel\". In this case, is there anything to update user-permission relations to point to the \"oldmodel\" permission? If not, it might be better not to let this issue pass silently.",
      "comment_id": 2572257963,
      "user": "timgraham",
      "created_at": "2025-11-28T18:07:12Z",
      "url": "https://github.com/django/django/pull/19405#discussion_r2572257963"
    },
    {
      "repo": "django/django",
      "pr_number": 19405,
      "file_path": "tests/auth_tests/test_management.py",
      "line": 1659,
      "side": "RIGHT",
      "diff_hunk": "@@ -1528,6 +1534,174 @@ def test_permission_with_proxy_content_type_created(self):\n         )\n \n \n+@override_settings(\n+    MIGRATION_MODULES=dict(\n+        settings.MIGRATION_MODULES,\n+        auth_tests=\"auth_tests.operations_migrations\",\n+    ),\n+)\n+class PermissionRenameOperationsTests(TransactionTestCase):\n+    available_apps = [\n+        \"django.contrib.contenttypes\",\n+        \"django.contrib.auth\",\n+        \"auth_tests\",\n+    ]\n+\n+    def setUp(self):\n+        app_config = apps.get_app_config(\"auth_tests\")\n+        models.signals.post_migrate.connect(\n+            self.assertOperationsInjected, sender=app_config\n+        )\n+        self.addCleanup(\n+            models.signals.post_migrate.disconnect,\n+            self.assertOperationsInjected,\n+            sender=app_config,\n+        )\n+\n+    def assertOperationsInjected(self, plan, **kwargs):\n+        for migration, _backward in plan:\n+            operations = iter(migration.operations)\n+            for operation in operations:\n+                if isinstance(operation, migrations.RenameModel):\n+                    next_operation = next(operations)\n+                    self.assertIsInstance(next_operation, RenamePermission)\n+                    self.assertEqual(next_operation.app_label, migration.app_label)\n+                    self.assertEqual(next_operation.old_model, operation.old_name)\n+                    self.assertEqual(next_operation.new_model, operation.new_name)\n+\n+    def test_permission_rename(self):\n+        ct = ContentType.objects.create(app_label=\"auth_tests\", model=\"oldmodel\")\n+        actions = [\"add\", \"change\", \"delete\", \"view\"]\n+        for action in actions:\n+            Permission.objects.create(\n+                codename=f\"{action}_oldmodel\",\n+                name=f\"Can {action} old model\",\n+                content_type=ct,\n+            )\n+\n+        call_command(\"migrate\", \"auth_tests\", verbosity=0)\n+        for action in actions:\n+            self.assertFalse(\n+                Permission.objects.filter(codename=f\"{action}_oldmodel\").exists()\n+            )\n+            self.assertTrue(\n+                Permission.objects.filter(codename=f\"{action}_newmodel\").exists()\n+            )\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+\n+        for action in actions:\n+            self.assertTrue(\n+                Permission.objects.filter(codename=f\"{action}_oldmodel\").exists()\n+            )\n+            self.assertFalse(\n+                Permission.objects.filter(codename=f\"{action}_newmodel\").exists()\n+            )\n+\n+    @mock.patch(\n+        \"django.db.router.allow_migrate_model\",\n+        return_value=False,\n+    )\n+    def test_rename_skipped_if_router_disallows(self, _):\n+        ct = ContentType.objects.create(app_label=\"auth_tests\", model=\"oldmodel\")\n+        Permission.objects.create(\n+            codename=\"change_oldmodel\",\n+            name=\"Can change old model\",\n+            content_type=ct,\n+        )\n+        # The rename operation should not be there when disallowed by router.\n+        app_config = apps.get_app_config(\"auth_tests\")\n+        models.signals.post_migrate.disconnect(\n+            self.assertOperationsInjected, sender=app_config\n+        )\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertTrue(Permission.objects.filter(codename=\"change_oldmodel\").exists())\n+        self.assertFalse(Permission.objects.filter(codename=\"change_newmodel\").exists())\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+\n+    def test_rename_backward_does_nothing_if_no_permissions(self):\n+        Permission.objects.filter(content_type__app_label=\"auth_tests\").delete()\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertFalse(\n+            Permission.objects.filter(\n+                codename__in=[\"change_oldmodel\", \"change_newmodel\"]\n+            ).exists()\n+        )",
      "comment": "I think this test isn't written properly because migrating to zero with no migrations applied doesn't do anything and so no `RenamePermission` operations are injected.",
      "comment_id": 2572260857,
      "user": "timgraham",
      "created_at": "2025-11-28T18:09:25Z",
      "url": "https://github.com/django/django/pull/19405#discussion_r2572260857"
    },
    {
      "repo": "django/django",
      "pr_number": 19405,
      "file_path": "tests/auth_tests/test_management.py",
      "line": 1659,
      "side": "RIGHT",
      "diff_hunk": "@@ -1528,6 +1534,174 @@ def test_permission_with_proxy_content_type_created(self):\n         )\n \n \n+@override_settings(\n+    MIGRATION_MODULES=dict(\n+        settings.MIGRATION_MODULES,\n+        auth_tests=\"auth_tests.operations_migrations\",\n+    ),\n+)\n+class PermissionRenameOperationsTests(TransactionTestCase):\n+    available_apps = [\n+        \"django.contrib.contenttypes\",\n+        \"django.contrib.auth\",\n+        \"auth_tests\",\n+    ]\n+\n+    def setUp(self):\n+        app_config = apps.get_app_config(\"auth_tests\")\n+        models.signals.post_migrate.connect(\n+            self.assertOperationsInjected, sender=app_config\n+        )\n+        self.addCleanup(\n+            models.signals.post_migrate.disconnect,\n+            self.assertOperationsInjected,\n+            sender=app_config,\n+        )\n+\n+    def assertOperationsInjected(self, plan, **kwargs):\n+        for migration, _backward in plan:\n+            operations = iter(migration.operations)\n+            for operation in operations:\n+                if isinstance(operation, migrations.RenameModel):\n+                    next_operation = next(operations)\n+                    self.assertIsInstance(next_operation, RenamePermission)\n+                    self.assertEqual(next_operation.app_label, migration.app_label)\n+                    self.assertEqual(next_operation.old_model, operation.old_name)\n+                    self.assertEqual(next_operation.new_model, operation.new_name)\n+\n+    def test_permission_rename(self):\n+        ct = ContentType.objects.create(app_label=\"auth_tests\", model=\"oldmodel\")\n+        actions = [\"add\", \"change\", \"delete\", \"view\"]\n+        for action in actions:\n+            Permission.objects.create(\n+                codename=f\"{action}_oldmodel\",\n+                name=f\"Can {action} old model\",\n+                content_type=ct,\n+            )\n+\n+        call_command(\"migrate\", \"auth_tests\", verbosity=0)\n+        for action in actions:\n+            self.assertFalse(\n+                Permission.objects.filter(codename=f\"{action}_oldmodel\").exists()\n+            )\n+            self.assertTrue(\n+                Permission.objects.filter(codename=f\"{action}_newmodel\").exists()\n+            )\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+\n+        for action in actions:\n+            self.assertTrue(\n+                Permission.objects.filter(codename=f\"{action}_oldmodel\").exists()\n+            )\n+            self.assertFalse(\n+                Permission.objects.filter(codename=f\"{action}_newmodel\").exists()\n+            )\n+\n+    @mock.patch(\n+        \"django.db.router.allow_migrate_model\",\n+        return_value=False,\n+    )\n+    def test_rename_skipped_if_router_disallows(self, _):\n+        ct = ContentType.objects.create(app_label=\"auth_tests\", model=\"oldmodel\")\n+        Permission.objects.create(\n+            codename=\"change_oldmodel\",\n+            name=\"Can change old model\",\n+            content_type=ct,\n+        )\n+        # The rename operation should not be there when disallowed by router.\n+        app_config = apps.get_app_config(\"auth_tests\")\n+        models.signals.post_migrate.disconnect(\n+            self.assertOperationsInjected, sender=app_config\n+        )\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertTrue(Permission.objects.filter(codename=\"change_oldmodel\").exists())\n+        self.assertFalse(Permission.objects.filter(codename=\"change_newmodel\").exists())\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+\n+    def test_rename_backward_does_nothing_if_no_permissions(self):\n+        Permission.objects.filter(content_type__app_label=\"auth_tests\").delete()\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertFalse(\n+            Permission.objects.filter(\n+                codename__in=[\"change_oldmodel\", \"change_newmodel\"]\n+            ).exists()\n+        )",
      "comment": "You were right \u2014 I\u2019ve updated the test and modified the migration so that when an IntegrityError occurs, duplicate user/group assignments are correctly transferred to the new permission and delete the old one. What\u2019s the best way to open a PR with these fixes? Should I still reference ticket #27489, or can I just submit the fixes directly?",
      "comment_id": 2572706254,
      "user": "artirix1927",
      "created_at": "2025-11-29T01:22:26Z",
      "url": "https://github.com/django/django/pull/19405#discussion_r2572706254"
    },
    {
      "repo": "django/django",
      "pr_number": 19405,
      "file_path": "tests/auth_tests/test_management.py",
      "line": 1659,
      "side": "RIGHT",
      "diff_hunk": "@@ -1528,6 +1534,174 @@ def test_permission_with_proxy_content_type_created(self):\n         )\n \n \n+@override_settings(\n+    MIGRATION_MODULES=dict(\n+        settings.MIGRATION_MODULES,\n+        auth_tests=\"auth_tests.operations_migrations\",\n+    ),\n+)\n+class PermissionRenameOperationsTests(TransactionTestCase):\n+    available_apps = [\n+        \"django.contrib.contenttypes\",\n+        \"django.contrib.auth\",\n+        \"auth_tests\",\n+    ]\n+\n+    def setUp(self):\n+        app_config = apps.get_app_config(\"auth_tests\")\n+        models.signals.post_migrate.connect(\n+            self.assertOperationsInjected, sender=app_config\n+        )\n+        self.addCleanup(\n+            models.signals.post_migrate.disconnect,\n+            self.assertOperationsInjected,\n+            sender=app_config,\n+        )\n+\n+    def assertOperationsInjected(self, plan, **kwargs):\n+        for migration, _backward in plan:\n+            operations = iter(migration.operations)\n+            for operation in operations:\n+                if isinstance(operation, migrations.RenameModel):\n+                    next_operation = next(operations)\n+                    self.assertIsInstance(next_operation, RenamePermission)\n+                    self.assertEqual(next_operation.app_label, migration.app_label)\n+                    self.assertEqual(next_operation.old_model, operation.old_name)\n+                    self.assertEqual(next_operation.new_model, operation.new_name)\n+\n+    def test_permission_rename(self):\n+        ct = ContentType.objects.create(app_label=\"auth_tests\", model=\"oldmodel\")\n+        actions = [\"add\", \"change\", \"delete\", \"view\"]\n+        for action in actions:\n+            Permission.objects.create(\n+                codename=f\"{action}_oldmodel\",\n+                name=f\"Can {action} old model\",\n+                content_type=ct,\n+            )\n+\n+        call_command(\"migrate\", \"auth_tests\", verbosity=0)\n+        for action in actions:\n+            self.assertFalse(\n+                Permission.objects.filter(codename=f\"{action}_oldmodel\").exists()\n+            )\n+            self.assertTrue(\n+                Permission.objects.filter(codename=f\"{action}_newmodel\").exists()\n+            )\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+\n+        for action in actions:\n+            self.assertTrue(\n+                Permission.objects.filter(codename=f\"{action}_oldmodel\").exists()\n+            )\n+            self.assertFalse(\n+                Permission.objects.filter(codename=f\"{action}_newmodel\").exists()\n+            )\n+\n+    @mock.patch(\n+        \"django.db.router.allow_migrate_model\",\n+        return_value=False,\n+    )\n+    def test_rename_skipped_if_router_disallows(self, _):\n+        ct = ContentType.objects.create(app_label=\"auth_tests\", model=\"oldmodel\")\n+        Permission.objects.create(\n+            codename=\"change_oldmodel\",\n+            name=\"Can change old model\",\n+            content_type=ct,\n+        )\n+        # The rename operation should not be there when disallowed by router.\n+        app_config = apps.get_app_config(\"auth_tests\")\n+        models.signals.post_migrate.disconnect(\n+            self.assertOperationsInjected, sender=app_config\n+        )\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertTrue(Permission.objects.filter(codename=\"change_oldmodel\").exists())\n+        self.assertFalse(Permission.objects.filter(codename=\"change_newmodel\").exists())\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+\n+    def test_rename_backward_does_nothing_if_no_permissions(self):\n+        Permission.objects.filter(content_type__app_label=\"auth_tests\").delete()\n+\n+        call_command(\n+            \"migrate\",\n+            \"auth_tests\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertFalse(\n+            Permission.objects.filter(\n+                codename__in=[\"change_oldmodel\", \"change_newmodel\"]\n+            ).exists()\n+        )",
      "comment": "You can use \"Refs #27489 -- ...\" since the fix hasn't made it to a stable release yet, otherwise, we would use a new ticket.",
      "comment_id": 2572766716,
      "user": "timgraham",
      "created_at": "2025-11-29T03:01:04Z",
      "url": "https://github.com/django/django/pull/19405#discussion_r2572766716"
    },
    {
      "repo": "django/django",
      "pr_number": 19947,
      "file_path": "tests/expressions/tests.py",
      "line": 2915,
      "side": "RIGHT",
      "diff_hunk": "@@ -2911,6 +2912,47 @@ def test_non_empty_group_by(self):\n         self.assertEqual(group_by_cols[0].output_field, expr.output_field)\n \n \n+class DateFieldTimedeltaTests(TestCase):",
      "comment": "I think this can just be a documentation PR. The tests are date field specific, but this file tests the general behavior of expressions.",
      "comment_id": 2432822889,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-15T14:35:51Z",
      "url": "https://github.com/django/django/pull/19947#discussion_r2432822889"
    },
    {
      "repo": "django/django",
      "pr_number": 19947,
      "file_path": "tests/expressions/tests.py",
      "line": 2941,
      "side": "RIGHT",
      "diff_hunk": "@@ -2911,6 +2912,47 @@ def test_non_empty_group_by(self):\n         self.assertEqual(group_by_cols[0].output_field, expr.output_field)\n \n \n+class DateFieldTimedeltaTests(TestCase):\n+    @classmethod\n+    def setUpTestData(cls):\n+        cls.test_date = datetime.date(2025, 10, 11)\n+        cls.experiment = Experiment.objects.create(\n+            name=\"DateField Test\",\n+            assigned=cls.test_date,\n+            start=datetime.datetime(2025, 10, 11, 18, 0, 0),\n+            end=datetime.datetime(2025, 10, 13, 18, 0, 0),\n+            completed=datetime.date(2025, 10, 13),\n+            estimated_time=datetime.timedelta(days=2),\n+        )\n+\n+    def test_datefield_timedelta_with_cast(self):\n+        \"\"\"Cast ensures date type when subtracting timedelta from DateField.\"\"\"\n+        qs = Experiment.objects.annotate(\n+            next_day=Cast(\n+                F(\"assigned\") + datetime.timedelta(days=1),\n+                output_field=DateField(),\n+            )\n+        )\n+        result = qs.first().next_day\n+        self.assertIsInstance(result, datetime.date)\n+        self.assertNotIsInstance(result, datetime.datetime)\n+        self.assertEqual(result, datetime.date(2025, 10, 12))\n+\n+    @unittest.skipUnless(connection.vendor == \"postgresql\", \"PostgreSQL only\")",
      "comment": "In general we don't add tests that memorialize database quirks. (If we did, we'd want to include mysql also.)",
      "comment_id": 2432824716,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-15T14:36:17Z",
      "url": "https://github.com/django/django/pull/19947#discussion_r2432824716"
    },
    {
      "repo": "django/django",
      "pr_number": 18637,
      "file_path": "tests/gis_tests/geoapp/models.py",
      "line": 37,
      "side": "RIGHT",
      "diff_hunk": "@@ -28,6 +28,13 @@ class Meta:\n         app_label = \"geoapp\"\n \n \n+class City3d(NamedModel):\n+    point = models.PointField(dim=3)\n+\n+    class Meta:\n+        app_label = \"geoapp\"\n+\n+",
      "comment": "Do you really need to create a new model? Isn't `ThreeDimensionalFeature` usable for the tests?",
      "comment_id": 1780482246,
      "user": "claudep",
      "created_at": "2024-09-30T06:05:15Z",
      "url": "https://github.com/django/django/pull/18637#discussion_r1780482246"
    },
    {
      "repo": "django/django",
      "pr_number": 20286,
      "file_path": "django/db/models/fields/related.py",
      "line": 1079,
      "side": "RIGHT",
      "diff_hunk": "@@ -1056,9 +1057,43 @@ def check(self, **kwargs):\n             *self._check_unique(),\n         ]\n \n+    def _check_on_delete_db_support(self, on_delete, feature_flag, databases):\n+        errors = []\n+        for db in databases:\n+            if not router.allow_migrate_model(db, self.model):\n+                continue\n+            connection = connections[db]\n+            if not (\n+                feature_flag in self.model._meta.required_db_features\n+                or getattr(connection.features, feature_flag)\n+            ):\n+                no_db_option_name = on_delete.__name__.removeprefix(\"DB_\")\n+                errors.append(\n+                    checks.Error(\n+                        f\"{connection.display_name} does not support a \"\n+                        f\"{on_delete.__name__}.\",\n+                        hint=f\"Change the on_delete rule to {no_db_option_name}.\",\n+                        obj=self,\n+                        id=\"fields.E324\",\n+                    ),\n+                )",
      "comment": "```suggestion\r\n            if (\r\n                feature_flag in self.model._meta.required_db_features\r\n                or getattr(connection.features, feature_flag)\r\n            ):\r\n                continue\r\n            no_db_option_name = on_delete.__name__.removeprefix(\"DB_\")\r\n            errors.append(\r\n                checks.Error(\r\n                    f\"{connection.display_name} does not support a \"\r\n                    f\"{on_delete.__name__}.\",\r\n                    hint=f\"Change the on_delete rule to {no_db_option_name}.\",\r\n                    obj=self,\r\n                    id=\"fields.E324\",\r\n                ),\r\n            )\r\n```",
      "comment_id": 2553904847,
      "user": "LilyFirefly",
      "created_at": "2025-11-23T09:20:24Z",
      "url": "https://github.com/django/django/pull/20286#discussion_r2553904847"
    },
    {
      "repo": "django/django",
      "pr_number": 20286,
      "file_path": "django/db/models/fields/related.py",
      "line": 1079,
      "side": "RIGHT",
      "diff_hunk": "@@ -1056,9 +1057,43 @@ def check(self, **kwargs):\n             *self._check_unique(),\n         ]\n \n+    def _check_on_delete_db_support(self, on_delete, feature_flag, databases):\n+        errors = []\n+        for db in databases:\n+            if not router.allow_migrate_model(db, self.model):\n+                continue\n+            connection = connections[db]\n+            if not (\n+                feature_flag in self.model._meta.required_db_features\n+                or getattr(connection.features, feature_flag)\n+            ):\n+                no_db_option_name = on_delete.__name__.removeprefix(\"DB_\")\n+                errors.append(\n+                    checks.Error(\n+                        f\"{connection.display_name} does not support a \"\n+                        f\"{on_delete.__name__}.\",\n+                        hint=f\"Change the on_delete rule to {no_db_option_name}.\",\n+                        obj=self,\n+                        id=\"fields.E324\",\n+                    ),\n+                )",
      "comment": "We could also make this helper method a generator and `yield` the `Error`s.",
      "comment_id": 2553909821,
      "user": "LilyFirefly",
      "created_at": "2025-11-23T09:22:25Z",
      "url": "https://github.com/django/django/pull/20286#discussion_r2553909821"
    },
    {
      "repo": "django/django",
      "pr_number": 20303,
      "file_path": "tests/expressions/tests.py",
      "line": 1575,
      "side": "RIGHT",
      "diff_hunk": "@@ -1571,6 +1571,24 @@ def test_get_expression_for_validation_only_one_source_expression(self):\n         with self.assertRaisesMessage(ValueError, msg):\n             expression.get_expression_for_validation()\n \n+    def test_replace_expressions_falsey(self):\n+        class AssignableExpression(Expression):",
      "comment": "I wanted to use `Expression` directly but it doesn't allow assignment of `source_expressions`\r\n\r\nhttps://github.com/django/django/blob/ec60df6d1ea8939a316d9b180faa0b4ef2e83606/django/db/models/expressions.py#L212-L213",
      "comment_id": 2554485923,
      "user": "charettes",
      "created_at": "2025-11-24T01:44:23Z",
      "url": "https://github.com/django/django/pull/20303#discussion_r2554485923"
    },
    {
      "repo": "django/django",
      "pr_number": 20303,
      "file_path": "tests/expressions/tests.py",
      "line": 1587,
      "side": "RIGHT",
      "diff_hunk": "@@ -1571,6 +1571,24 @@ def test_get_expression_for_validation_only_one_source_expression(self):\n         with self.assertRaisesMessage(ValueError, msg):\n             expression.get_expression_for_validation()\n \n+    def test_replace_expressions_falsey(self):\n+        class AssignableExpression(Expression):\n+            def __init__(self, *source_expressions):\n+                super().__init__()\n+                self.set_source_expressions(list(source_expressions))\n+\n+            def get_source_expressions(self):\n+                return self.source_expressions\n+\n+            def set_source_expressions(self, exprs):\n+                self.source_expressions = exprs\n+\n+        expression = AssignableExpression()\n+        falsey = Q()",
      "comment": "We could use any object with a `__bool__() -> False` but I figured it was simpler to use `Q` directly since it's the case that triggered the reported problem.",
      "comment_id": 2554486756,
      "user": "charettes",
      "created_at": "2025-11-24T01:45:50Z",
      "url": "https://github.com/django/django/pull/20303#discussion_r2554486756"
    },
    {
      "repo": "django/django",
      "pr_number": 20303,
      "file_path": "django/db/models/expressions.py",
      "line": 429,
      "side": "RIGHT",
      "diff_hunk": "@@ -426,7 +426,7 @@ def replace_expressions(self, replacements):\n         clone = self.copy()\n         clone.set_source_expressions(\n             [\n-                expr.replace_expressions(replacements) if expr else None\n+                None if expr is None else expr.replace_expressions(replacements)",
      "comment": "As mentionned in the commit message this would have caused objects such as `QuerySet` to be evaluated as replacement as well (since `QuerySet.__bool__` executes the query).",
      "comment_id": 2554487267,
      "user": "charettes",
      "created_at": "2025-11-24T01:46:42Z",
      "url": "https://github.com/django/django/pull/20303#discussion_r2554487267"
    },
    {
      "repo": "django/django",
      "pr_number": 20290,
      "file_path": "tests/template_tests/filter_tests/test_urlize.py",
      "line": 225,
      "side": "RIGHT",
      "diff_hunk": "@@ -218,6 +218,13 @@ def test_parenthesis(self):\n             'rel=\"nofollow\">https://en.wikipedia.org/wiki/Django_(web_framework)</a>)',\n         )\n \n+    def test_parenthesis_and_bracket(self):\n+        self.assertEqual(\n+            urlize(\"[(https://en.wikipedia.org/)]\"),\n+            '[(<a href=\"https://en.wikipedia.org/\" '\n+            'rel=\"nofollow\">https://en.wikipedia.org/</a>)]',",
      "comment": "Before, this final bracket at the end of the string was missing.",
      "comment_id": 2550143280,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-21T15:30:58Z",
      "url": "https://github.com/django/django/pull/20290#discussion_r2550143280"
    },
    {
      "repo": "django/django",
      "pr_number": 20045,
      "file_path": "django/utils/http.py",
      "line": 176,
      "side": "RIGHT",
      "diff_hunk": "@@ -169,11 +169,11 @@ def int_to_base36(i):\n         raise ValueError(\"Negative base36 conversion input.\")\n     if i < 36:\n         return char_set[i]\n-    b36 = \"\"\n+    b36 = []\n     while i != 0:\n         i, n = divmod(i, 36)\n-        b36 = char_set[n] + b36\n-    return b36\n+        b36.append(char_set[n])\n+    return \"\".join(reversed(b36))",
      "comment": "I get appending and finally reversing to avoid `insert()`, but in that case I think we need to update the `b36` variable name.",
      "comment_id": 2499937378,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-06T17:04:33Z",
      "url": "https://github.com/django/django/pull/20045#discussion_r2499937378"
    },
    {
      "repo": "django/django",
      "pr_number": 20045,
      "file_path": "django/utils/html.py",
      "line": 446,
      "side": "RIGHT",
      "diff_hunk": "@@ -442,7 +443,7 @@ def trim_punctuation(self, word):\n                     rstripped = middle.rstrip(closing)\n                     if rstripped != middle:\n                         strip = counts[closing] - counts[opening]\n-                        trail = middle[-strip:]\n+                        trail.appendleft(middle[-strip:])",
      "comment": "This seems like a wrong transformation - we don't want to append, but fully replace\u2026? So more like:\n\n```python\ntrail.clear()\ntrail.append(middle[-strip:])\n```",
      "comment_id": 2544009071,
      "user": "adamchainz",
      "created_at": "2025-11-20T01:07:42Z",
      "url": "https://github.com/django/django/pull/20045#discussion_r2544009071"
    },
    {
      "repo": "django/django",
      "pr_number": 20045,
      "file_path": "django/utils/html.py",
      "line": 446,
      "side": "RIGHT",
      "diff_hunk": "@@ -442,7 +443,7 @@ def trim_punctuation(self, word):\n                     rstripped = middle.rstrip(closing)\n                     if rstripped != middle:\n                         strip = counts[closing] - counts[opening]\n-                        trail = middle[-strip:]\n+                        trail.appendleft(middle[-strip:])",
      "comment": "Thanks @adamchainz, good eye.\n\nAlthough this is a logic change, I'm wondering if it fixed a bug.\n\nBefore (i.e. with #20045):\n```py\n>>> Urlizer()(\"([https://djangoproject.com])\")\n'([<a href=\"https://djangoproject.com\">https://djangoproject.com</a>])'\n```\n\n\nWith adding `trail.clear()` to \"fix\" the accidental change, wrong result, missing final \")\":\n```py\n>>> Urlizer()(\"([https://djangoproject.com])\")\n'([<a href=\"https://djangoproject.com\">https://djangoproject.com</a>]'\n```\n\nThe missing trailing character is also missing on stable/6.0.x\n\nDo you agree we should add a test case to capture?",
      "comment_id": 2547729992,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-20T21:31:55Z",
      "url": "https://github.com/django/django/pull/20045#discussion_r2547729992"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "django/db/backends/base/schema.py",
      "line": 1638,
      "side": "RIGHT",
      "diff_hunk": "@@ -1628,6 +1630,13 @@ def _rename_index_sql(self, model, old_name, new_name):\n             new_name=self.quote_name(new_name),\n         )\n \n+    def _create_on_delete_sql(self, model, field):\n+        if (on_delete_db := getattr(field.remote_field, \"on_delete\", None)) and hasattr(\n+            on_delete_db, \"on_delete_sql\"\n+        ):\n+            return on_delete_db.on_delete_sql(self)\n+        return \"\"",
      "comment": "I find this a bit easier to read:\r\n\r\n```suggestion\r\n        remote_field = field.remote_field\r\n        try:\r\n            on_delete_sql = remote_field.on_delete.on_delete_sql\r\n        except AttributeError:\r\n            return \"\"\r\n        return on_delete_sql(self)\r\n```",
      "comment_id": 2413496071,
      "user": "LilyFirefly",
      "created_at": "2025-10-08T11:09:24Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2413496071"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "django/db/models/deletion.py",
      "line": 349,
      "side": "RIGHT",
      "diff_hunk": "@@ -316,8 +347,13 @@ def collect(\n                 continue\n             field = related.field\n             on_delete = field.remote_field.on_delete\n-            if on_delete == DO_NOTHING:\n-                continue\n+            if on_delete in SKIP_COLLECTION:\n+                if self.force_collection and (\n+                    forced_on_delete := getattr(on_delete, \"forced_collector\", None)\n+                ):\n+                    on_delete = forced_on_delete",
      "comment": "```suggestion\r\n                if self.force_collection:\r\n                    on_delete = getattr(on_delete, \"forced_collector\", on_delete)\r\n```",
      "comment_id": 2413500954,
      "user": "LilyFirefly",
      "created_at": "2025-10-08T11:11:35Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2413500954"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "django/contrib/contenttypes/fields.py",
      "line": 153,
      "side": "RIGHT",
      "diff_hunk": "@@ -138,6 +139,19 @@ def _check_content_type_field(self):\n                         id=\"contenttypes.E004\",\n                     )\n                 ]\n+            elif isinstance(field.remote_field.on_delete, DatabaseOnDelete):\n+                return [\n+                    checks.Error(\n+                        f\"'{self.model._meta.object_name}.{self.ct_field}' cannot use \"\n+                        \"the database-level on_delete variant.\",\n+                        hint=\"Change the on_delete rule to non-database variant.\",\n+                        obj=self,\n+                        id=\"contenttypes.E006\",\n+                    )\n+                ]\n+                raise ValueError(\n+                    \"The GenericForeignKey  cannot use database-level on_delete option.\"",
      "comment": "```suggestion\n                    \"GenericForeignKey can't use database-level `on_delete` options.\"\n```",
      "comment_id": 2422052384,
      "user": "charettes",
      "created_at": "2025-10-10T20:45:28Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2422052384"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "tests/invalid_models_tests/test_relative_fields.py",
      "line": 6,
      "side": "RIGHT",
      "diff_hunk": "@@ -3,7 +3,8 @@\n from django.core.checks import Error\n from django.core.checks import Warning as DjangoWarning\n from django.db import connection, models\n-from django.test.testcases import SimpleTestCase\n+from django.test import skipUnlessDBFeature",
      "comment": "Not related to your changes but this module should be named `test_related_fields`, there's no such thing as a relative field \ud83d\ude05 ",
      "comment_id": 2422056473,
      "user": "charettes",
      "created_at": "2025-10-10T20:48:19Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2422056473"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "django/contrib/contenttypes/fields.py",
      "line": 153,
      "side": "RIGHT",
      "diff_hunk": "@@ -138,6 +139,19 @@ def _check_content_type_field(self):\n                         id=\"contenttypes.E004\",\n                     )\n                 ]\n+            elif isinstance(field.remote_field.on_delete, DatabaseOnDelete):\n+                return [\n+                    checks.Error(\n+                        f\"'{self.model._meta.object_name}.{self.ct_field}' cannot use \"\n+                        \"the database-level on_delete variant.\",\n+                        hint=\"Change the on_delete rule to non-database variant.\",\n+                        obj=self,\n+                        id=\"contenttypes.E006\",\n+                    )\n+                ]\n+                raise ValueError(\n+                    \"The GenericForeignKey  cannot use database-level on_delete option.\"",
      "comment": "Sorry, raising `ValueError` was the first idea :facepalm: Removed.",
      "comment_id": 2422100652,
      "user": "felixxm",
      "created_at": "2025-10-10T21:14:12Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2422100652"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "tests/invalid_models_tests/test_relative_fields.py",
      "line": 6,
      "side": "RIGHT",
      "diff_hunk": "@@ -3,7 +3,8 @@\n from django.core.checks import Error\n from django.core.checks import Warning as DjangoWarning\n from django.db import connection, models\n-from django.test.testcases import SimpleTestCase\n+from django.test import skipUnlessDBFeature",
      "comment": "Good catch :dart: Let's rename it when this is merged.",
      "comment_id": 2422101773,
      "user": "felixxm",
      "created_at": "2025-10-10T21:14:50Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2422101773"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "django/contrib/admin/utils.py",
      "line": 189,
      "side": "RIGHT",
      "diff_hunk": "@@ -185,6 +185,7 @@ def format_callback(obj):\n \n class NestedObjects(Collector):\n     def __init__(self, *args, **kwargs):\n+        kwargs.setdefault(\"force_collection\", True)\n         super().__init__(*args, **kwargs)",
      "comment": "Why not put this in the signature?\n```suggestion\n    def __init__(self, *args, force_collection=True, **kwargs):\n        super().__init__(*args, force_collection=force_collection, **kwargs)\n```",
      "comment_id": 2423173154,
      "user": "adamchainz",
      "created_at": "2025-10-11T22:20:35Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2423173154"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "django/db/models/base.py",
      "line": 2441,
      "side": "RIGHT",
      "diff_hunk": "@@ -2421,6 +2422,29 @@ def _check_long_column_names(cls, databases):\n \n         return errors\n \n+    @classmethod\n+    def _check_related_fields(cls):\n+        has_db_variant = False\n+        has_python_variant = False\n+        for rel in cls._meta.get_fields():\n+            if rel.related_model and not rel.auto_created:\n+                if not (on_delete := getattr(rel.remote_field, \"on_delete\", None)):\n+                    continue\n+                if isinstance(on_delete, DatabaseOnDelete):\n+                    has_db_variant = True\n+                else:\n+                    has_python_variant = True\n+                if has_db_variant and has_python_variant:\n+                    return [\n+                        checks.Error(\n+                            \"The model cannot have both related fields with \"\n+                            \"database-level and python-level on_delete variants.\",",
      "comment": "Grammar fix:\n\n```suggestion\n                            \"The model cannot have related fields with both \"\n                            \"database-level and python-level on_delete variants.\",\n```",
      "comment_id": 2423173800,
      "user": "adamchainz",
      "created_at": "2025-10-11T22:22:38Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2423173800"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "django/contrib/admin/utils.py",
      "line": 189,
      "side": "RIGHT",
      "diff_hunk": "@@ -185,6 +185,7 @@ def format_callback(obj):\n \n class NestedObjects(Collector):\n     def __init__(self, *args, **kwargs):\n+        kwargs.setdefault(\"force_collection\", True)\n         super().__init__(*args, **kwargs)",
      "comment": "No other reason than this originated from a local iteration session, we should change.",
      "comment_id": 2423740737,
      "user": "charettes",
      "created_at": "2025-10-12T11:56:05Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2423740737"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "tests/delete/tests.py",
      "line": 355,
      "side": "RIGHT",
      "diff_hunk": "@@ -304,6 +342,32 @@ def test_restrict_gfk_no_fast_delete(self):\n         self.assertFalse(GenericB2.objects.exists())\n         self.assertFalse(GenericDeleteBottom.objects.exists())\n \n+    @skipUnlessDBFeature(\"supports_on_delete_db_restrict\")\n+    def test_db_restrict(self):\n+        r = RelatedDbOptionParent.objects.create()\n+        restrict_db_obj = RestrictDbModel.objects.create(\n+            db_restrict=r, name=\"db_restrict\"\n+        )\n+        with self.assertRaises(IntegrityError), transaction.atomic():\n+            restrict_db_obj.db_restrict.delete()\n+\n+    @skipUnlessDBFeature(\"supports_on_delete_db_restrict\")\n+    def test_db_restrict_path_db_cascade_direct(self):",
      "comment": "I wouldn't be surprised if this failed on MySQL because it [has poor foreign key constraint deferral support](https://docs.djangoproject.com/en/5.2/topics/db/fixtures/#:~:text=constraints%20will%20be%20checked%20at%20the%20end%20of%20the%20transaction.%20Any%20relationships).\n\nhttps://github.com/django/django/blob/315dbe675df338ae66c8fa43274a76ecbed7ef67/django/db/backends/base/features.py#L122-L124\n\nI think we'll want to also include `can_defer_constraint_checks`?\n\n\n```suggestion\n    @skipUnlessDBFeatures(\"supports_on_delete_db_restrict\", \"can_defer_constraint_checks\")\n    def test_db_restrict_path_db_cascade_direct(self):\n```",
      "comment_id": 2423757673,
      "user": "charettes",
      "created_at": "2025-10-12T12:01:40Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2423757673"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "tests/delete/tests.py",
      "line": 355,
      "side": "RIGHT",
      "diff_hunk": "@@ -304,6 +342,32 @@ def test_restrict_gfk_no_fast_delete(self):\n         self.assertFalse(GenericB2.objects.exists())\n         self.assertFalse(GenericDeleteBottom.objects.exists())\n \n+    @skipUnlessDBFeature(\"supports_on_delete_db_restrict\")\n+    def test_db_restrict(self):\n+        r = RelatedDbOptionParent.objects.create()\n+        restrict_db_obj = RestrictDbModel.objects.create(\n+            db_restrict=r, name=\"db_restrict\"\n+        )\n+        with self.assertRaises(IntegrityError), transaction.atomic():\n+            restrict_db_obj.db_restrict.delete()\n+\n+    @skipUnlessDBFeature(\"supports_on_delete_db_restrict\")\n+    def test_db_restrict_path_db_cascade_direct(self):",
      "comment": "In other words, since MySQL doesn't support foreign key constraints check deferral its version of `DB_RESTRICT` is kind of an equivalent to our `PROTECTED`.",
      "comment_id": 2423767238,
      "user": "charettes",
      "created_at": "2025-10-12T12:05:00Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2423767238"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "tests/delete/tests.py",
      "line": 355,
      "side": "RIGHT",
      "diff_hunk": "@@ -304,6 +342,32 @@ def test_restrict_gfk_no_fast_delete(self):\n         self.assertFalse(GenericB2.objects.exists())\n         self.assertFalse(GenericDeleteBottom.objects.exists())\n \n+    @skipUnlessDBFeature(\"supports_on_delete_db_restrict\")\n+    def test_db_restrict(self):\n+        r = RelatedDbOptionParent.objects.create()\n+        restrict_db_obj = RestrictDbModel.objects.create(\n+            db_restrict=r, name=\"db_restrict\"\n+        )\n+        with self.assertRaises(IntegrityError), transaction.atomic():\n+            restrict_db_obj.db_restrict.delete()\n+\n+    @skipUnlessDBFeature(\"supports_on_delete_db_restrict\")\n+    def test_db_restrict_path_db_cascade_direct(self):",
      "comment": "More than happy to add this, I've scratched my head why it started to fail :sweat_smile: ",
      "comment_id": 2423771253,
      "user": "felixxm",
      "created_at": "2025-10-12T12:06:21Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2423771253"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "django/db/backends/base/operations.py",
      "line": 261,
      "side": "RIGHT",
      "diff_hunk": "@@ -254,6 +254,17 @@ def limit_offset_sql(self, low_mark, high_mark):\n             if sql\n         )\n \n+    def fk_on_delete_sql(self, operation):\n+        \"\"\"\n+        Return the SQL to make an ON DELETE statement during a CREATE TABLE\n+        statement.\n+        \"\"\"\n+        if operation in [\"CASCADE\", \"SET NULL\", \"RESTRICT\", \"SET DEFAULT\"]:",
      "comment": "```suggestion\r\n        if operation in {\"CASCADE\", \"RESTRICT\", \"SET DEFAULT\", \"SET NULL\"}:\r\n```\r\nIt's a micro optimization to have a faster check and ordered values to help to read the code.",
      "comment_id": 2425728916,
      "user": "pauloxnet",
      "created_at": "2025-10-13T09:41:49Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2425728916"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "django/db/backends/base/schema.py",
      "line": 1636,
      "side": "RIGHT",
      "diff_hunk": "@@ -1628,6 +1630,13 @@ def _rename_index_sql(self, model, old_name, new_name):\n             new_name=self.quote_name(new_name),\n         )\n \n+    def _create_on_delete_sql(self, model, field):\n+        remote_field = field.remote_field\n+        try:\n+            return remote_field.on_delete.on_delete_sql(self)",
      "comment": "```suggestion\r\n        try:\r\n            return field.remote_field.on_delete.on_delete_sql(self)\r\n```\r\nAnother micro optimization, removing a variable used only once.",
      "comment_id": 2425733340,
      "user": "pauloxnet",
      "created_at": "2025-10-13T09:43:45Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2425733340"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "django/db/backends/base/operations.py",
      "line": 261,
      "side": "RIGHT",
      "diff_hunk": "@@ -254,6 +254,17 @@ def limit_offset_sql(self, low_mark, high_mark):\n             if sql\n         )\n \n+    def fk_on_delete_sql(self, operation):\n+        \"\"\"\n+        Return the SQL to make an ON DELETE statement during a CREATE TABLE\n+        statement.\n+        \"\"\"\n+        if operation in [\"CASCADE\", \"SET NULL\", \"RESTRICT\", \"SET DEFAULT\"]:",
      "comment": "Is this actually faster for four elements? It could be that the cost of hashing outweighs the big O behaviour.",
      "comment_id": 2426014824,
      "user": "LilyFirefly",
      "created_at": "2025-10-13T11:19:03Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2426014824"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "django/db/backends/base/schema.py",
      "line": 1636,
      "side": "RIGHT",
      "diff_hunk": "@@ -1628,6 +1630,13 @@ def _rename_index_sql(self, model, old_name, new_name):\n             new_name=self.quote_name(new_name),\n         )\n \n+    def _create_on_delete_sql(self, model, field):\n+        remote_field = field.remote_field\n+        try:\n+            return remote_field.on_delete.on_delete_sql(self)",
      "comment": "If `field` doesn't have a `remote_field` attribute, the behaviour will change here.",
      "comment_id": 2426018577,
      "user": "LilyFirefly",
      "created_at": "2025-10-13T11:19:57Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2426018577"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "django/db/backends/base/schema.py",
      "line": 1636,
      "side": "RIGHT",
      "diff_hunk": "@@ -1628,6 +1630,13 @@ def _rename_index_sql(self, model, old_name, new_name):\n             new_name=self.quote_name(new_name),\n         )\n \n+    def _create_on_delete_sql(self, model, field):\n+        remote_field = field.remote_field\n+        try:\n+            return remote_field.on_delete.on_delete_sql(self)",
      "comment": "Yes, I wanted to limit catching the `AttributeError` to `on_delete`, that's why `.remote_field` is outside of `try... except`.",
      "comment_id": 2426042214,
      "user": "felixxm",
      "created_at": "2025-10-13T11:24:36Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2426042214"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "django/contrib/contenttypes/fields.py",
      "line": 147,
      "side": "RIGHT",
      "diff_hunk": "@@ -138,6 +139,16 @@ def _check_content_type_field(self):\n                         id=\"contenttypes.E004\",\n                     )\n                 ]\n+            elif isinstance(field.remote_field.on_delete, DatabaseOnDelete):\n+                return [\n+                    checks.Error(\n+                        f\"'{self.model._meta.object_name}.{self.ct_field}' cannot use \"\n+                        \"the database-level on_delete variant.\",\n+                        hint=\"Change the on_delete rule to non-database variant.\",",
      "comment": "To match the error\n```suggestion\n                        hint=\"Change the on_delete rule to the non-database variant.\",\n```",
      "comment_id": 2432993897,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-15T15:19:36Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2432993897"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "django/db/models/fields/related.py",
      "line": 1156,
      "side": "RIGHT",
      "diff_hunk": "@@ -1041,37 +1049,119 @@ def __class_getitem__(cls, *args, **kwargs):\n         return cls\n \n     def check(self, **kwargs):\n+        databases = kwargs.get(\"databases\") or []\n         return [\n             *super().check(**kwargs),\n-            *self._check_on_delete(),\n+            *self._check_on_delete(databases),\n             *self._check_unique(),\n         ]\n \n-    def _check_on_delete(self):\n+    def _check_on_delete(self, databases):\n         on_delete = getattr(self.remote_field, \"on_delete\", None)\n-        if on_delete == SET_NULL and not self.null:\n-            return [\n+        errors = []\n+        if on_delete in [DB_SET_NULL, SET_NULL] and not self.null:\n+            errors.append(\n                 checks.Error(\n-                    \"Field specifies on_delete=SET_NULL, but cannot be null.\",\n+                    f\"Field specifies on_delete={on_delete.__name__}, but cannot be \"\n+                    \"null.\",\n                     hint=(\n                         \"Set null=True argument on the field, or change the on_delete \"\n                         \"rule.\"\n                     ),\n                     obj=self,\n                     id=\"fields.E320\",\n                 )\n-            ]\n+            )\n         elif on_delete == SET_DEFAULT and not self.has_default():\n-            return [\n+            errors.append(\n                 checks.Error(\n                     \"Field specifies on_delete=SET_DEFAULT, but has no default value.\",\n                     hint=\"Set a default value, or change the on_delete rule.\",\n                     obj=self,\n                     id=\"fields.E321\",\n                 )\n-            ]\n-        else:\n-            return []\n+            )\n+        elif on_delete == DB_SET_DEFAULT:\n+            if self.db_default is NOT_PROVIDED:\n+                errors.append(\n+                    checks.Error(\n+                        \"Field specifies on_delete=DB_SET_DEFAULT, but has \"\n+                        \"no db_default value.\",\n+                        hint=\"Set a db_default value, or change the on_delete rule.\",\n+                        obj=self,\n+                        id=\"fields.E322\",\n+                    )\n+                )\n+            for db in databases:\n+                if not router.allow_migrate_model(db, self.model):\n+                    continue\n+                connection = connections[db]\n+                if not (\n+                    \"supports_on_delete_db_default\"\n+                    in self.model._meta.required_db_features\n+                    or connection.features.supports_on_delete_db_default\n+                ):\n+                    errors.append(\n+                        checks.Error(\n+                            f\"{connection.display_name} does not support a \"\n+                            \"DB_SET_DEFAULT.\",\n+                            hint=\"Change the on_delete rule to SET_DEFAULT.\",\n+                            obj=self,\n+                            id=\"fields.E324\",\n+                        ),\n+                    )\n+        elif on_delete == DB_RESTRICT:\n+            for db in databases:\n+                if not router.allow_migrate_model(db, self.model):\n+                    continue\n+                connection = connections[db]\n+                if not (\n+                    \"supports_on_delete_db_restrict\"\n+                    in self.model._meta.required_db_features\n+                    or connection.features.supports_on_delete_db_restrict\n+                ):\n+                    errors.append(\n+                        checks.Error(\n+                            f\"{connection.display_name} does not support a \"\n+                            \"DB_RESTRICT.\",\n+                            hint=\"Change the on_delete rule to RESTRICT.\",\n+                            obj=self,\n+                            id=\"fields.E324\",\n+                        ),\n+                    )\n+        elif not isinstance(self.remote_field.model, str):\n+            # Database and Python variants cannot be mixed in a chain of\n+            # model references.\n+            is_db_on_delete = isinstance(on_delete, DatabaseOnDelete)\n+            ref_model_related_fields = (\n+                ref_model_field.remote_field\n+                for ref_model_field in self.remote_field.model._meta.get_fields()\n+                if ref_model_field.related_model and not ref_model_field.auto_created\n+            )\n+\n+            for ref_remote_field in ref_model_related_fields:\n+                if (\n+                    ref_remote_field.on_delete is not None\n+                    and isinstance(ref_remote_field.on_delete, DatabaseOnDelete)\n+                    is not is_db_on_delete\n+                ):\n+                    on_delete_type = \"database\" if is_db_on_delete else \"python\"\n+                    ref_on_delete_type = \"python\" if is_db_on_delete else \"database\"\n+                    errors.append(\n+                        checks.Error(\n+                            f\"Field specifies {on_delete_type}-level on_delete \"\n+                            \"variant, but referenced model uses \"\n+                            f\"{ref_on_delete_type}-level variant.\",\n+                            hint=(\n+                                \"Use constantly database or python on_delete variants \"",
      "comment": "```suggestion\n                                \"Use either database or python on_delete variants uniformly\"\n```",
      "comment_id": 2433533607,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-15T18:14:35Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2433533607"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "django/db/models/fields/related.py",
      "line": 1134,
      "side": "RIGHT",
      "diff_hunk": "@@ -1041,37 +1049,119 @@ def __class_getitem__(cls, *args, **kwargs):\n         return cls\n \n     def check(self, **kwargs):\n+        databases = kwargs.get(\"databases\") or []\n         return [\n             *super().check(**kwargs),\n-            *self._check_on_delete(),\n+            *self._check_on_delete(databases),\n             *self._check_unique(),\n         ]\n \n-    def _check_on_delete(self):\n+    def _check_on_delete(self, databases):\n         on_delete = getattr(self.remote_field, \"on_delete\", None)\n-        if on_delete == SET_NULL and not self.null:\n-            return [\n+        errors = []\n+        if on_delete in [DB_SET_NULL, SET_NULL] and not self.null:\n+            errors.append(\n                 checks.Error(\n-                    \"Field specifies on_delete=SET_NULL, but cannot be null.\",\n+                    f\"Field specifies on_delete={on_delete.__name__}, but cannot be \"\n+                    \"null.\",\n                     hint=(\n                         \"Set null=True argument on the field, or change the on_delete \"\n                         \"rule.\"\n                     ),\n                     obj=self,\n                     id=\"fields.E320\",\n                 )\n-            ]\n+            )\n         elif on_delete == SET_DEFAULT and not self.has_default():\n-            return [\n+            errors.append(\n                 checks.Error(\n                     \"Field specifies on_delete=SET_DEFAULT, but has no default value.\",\n                     hint=\"Set a default value, or change the on_delete rule.\",\n                     obj=self,\n                     id=\"fields.E321\",\n                 )\n-            ]\n-        else:\n-            return []\n+            )\n+        elif on_delete == DB_SET_DEFAULT:\n+            if self.db_default is NOT_PROVIDED:\n+                errors.append(\n+                    checks.Error(\n+                        \"Field specifies on_delete=DB_SET_DEFAULT, but has \"\n+                        \"no db_default value.\",\n+                        hint=\"Set a db_default value, or change the on_delete rule.\",\n+                        obj=self,\n+                        id=\"fields.E322\",\n+                    )\n+                )\n+            for db in databases:\n+                if not router.allow_migrate_model(db, self.model):\n+                    continue\n+                connection = connections[db]\n+                if not (\n+                    \"supports_on_delete_db_default\"\n+                    in self.model._meta.required_db_features\n+                    or connection.features.supports_on_delete_db_default\n+                ):\n+                    errors.append(\n+                        checks.Error(\n+                            f\"{connection.display_name} does not support a \"\n+                            \"DB_SET_DEFAULT.\",\n+                            hint=\"Change the on_delete rule to SET_DEFAULT.\",\n+                            obj=self,\n+                            id=\"fields.E324\",\n+                        ),\n+                    )\n+        elif on_delete == DB_RESTRICT:\n+            for db in databases:\n+                if not router.allow_migrate_model(db, self.model):\n+                    continue\n+                connection = connections[db]\n+                if not (\n+                    \"supports_on_delete_db_restrict\"\n+                    in self.model._meta.required_db_features\n+                    or connection.features.supports_on_delete_db_restrict\n+                ):\n+                    errors.append(\n+                        checks.Error(\n+                            f\"{connection.display_name} does not support a \"\n+                            \"DB_RESTRICT.\",\n+                            hint=\"Change the on_delete rule to RESTRICT.\",\n+                            obj=self,\n+                            id=\"fields.E324\",\n+                        ),\n+                    )\n+        elif not isinstance(self.remote_field.model, str):\n+            # Database and Python variants cannot be mixed in a chain of\n+            # model references.\n+            is_db_on_delete = isinstance(on_delete, DatabaseOnDelete)\n+            ref_model_related_fields = (\n+                ref_model_field.remote_field\n+                for ref_model_field in self.remote_field.model._meta.get_fields()\n+                if ref_model_field.related_model and not ref_model_field.auto_created\n+            )\n+\n+            for ref_remote_field in ref_model_related_fields:\n+                if (\n+                    ref_remote_field.on_delete is not None\n+                    and isinstance(ref_remote_field.on_delete, DatabaseOnDelete)\n+                    is not is_db_on_delete\n+                ):\n+                    on_delete_type = \"database\" if is_db_on_delete else \"python\"\n+                    ref_on_delete_type = \"python\" if is_db_on_delete else \"database\"\n+                    errors.append(\n+                        checks.Error(",
      "comment": "Is there any harm in letting DO_NOTHING pass here when there are DB_* variants? (Since we're unlikely to create a DB_DO_NOTHING variant?) It might help folks migrate to DB_* when they have some models with `PROTECT`, i.e. they can migrate to `DO_NOTHING` and let the db integrity error bubble up.\r\n\r\nEDIT: shouldn't imply that PROTECT is equivalent to DO_NOTHING, as PROTECT is a little stricter.",
      "comment_id": 2433675875,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-15T19:13:14Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2433675875"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "django/db/models/fields/related.py",
      "line": 1134,
      "side": "RIGHT",
      "diff_hunk": "@@ -1041,37 +1049,119 @@ def __class_getitem__(cls, *args, **kwargs):\n         return cls\n \n     def check(self, **kwargs):\n+        databases = kwargs.get(\"databases\") or []\n         return [\n             *super().check(**kwargs),\n-            *self._check_on_delete(),\n+            *self._check_on_delete(databases),\n             *self._check_unique(),\n         ]\n \n-    def _check_on_delete(self):\n+    def _check_on_delete(self, databases):\n         on_delete = getattr(self.remote_field, \"on_delete\", None)\n-        if on_delete == SET_NULL and not self.null:\n-            return [\n+        errors = []\n+        if on_delete in [DB_SET_NULL, SET_NULL] and not self.null:\n+            errors.append(\n                 checks.Error(\n-                    \"Field specifies on_delete=SET_NULL, but cannot be null.\",\n+                    f\"Field specifies on_delete={on_delete.__name__}, but cannot be \"\n+                    \"null.\",\n                     hint=(\n                         \"Set null=True argument on the field, or change the on_delete \"\n                         \"rule.\"\n                     ),\n                     obj=self,\n                     id=\"fields.E320\",\n                 )\n-            ]\n+            )\n         elif on_delete == SET_DEFAULT and not self.has_default():\n-            return [\n+            errors.append(\n                 checks.Error(\n                     \"Field specifies on_delete=SET_DEFAULT, but has no default value.\",\n                     hint=\"Set a default value, or change the on_delete rule.\",\n                     obj=self,\n                     id=\"fields.E321\",\n                 )\n-            ]\n-        else:\n-            return []\n+            )\n+        elif on_delete == DB_SET_DEFAULT:\n+            if self.db_default is NOT_PROVIDED:\n+                errors.append(\n+                    checks.Error(\n+                        \"Field specifies on_delete=DB_SET_DEFAULT, but has \"\n+                        \"no db_default value.\",\n+                        hint=\"Set a db_default value, or change the on_delete rule.\",\n+                        obj=self,\n+                        id=\"fields.E322\",\n+                    )\n+                )\n+            for db in databases:\n+                if not router.allow_migrate_model(db, self.model):\n+                    continue\n+                connection = connections[db]\n+                if not (\n+                    \"supports_on_delete_db_default\"\n+                    in self.model._meta.required_db_features\n+                    or connection.features.supports_on_delete_db_default\n+                ):\n+                    errors.append(\n+                        checks.Error(\n+                            f\"{connection.display_name} does not support a \"\n+                            \"DB_SET_DEFAULT.\",\n+                            hint=\"Change the on_delete rule to SET_DEFAULT.\",\n+                            obj=self,\n+                            id=\"fields.E324\",\n+                        ),\n+                    )\n+        elif on_delete == DB_RESTRICT:\n+            for db in databases:\n+                if not router.allow_migrate_model(db, self.model):\n+                    continue\n+                connection = connections[db]\n+                if not (\n+                    \"supports_on_delete_db_restrict\"\n+                    in self.model._meta.required_db_features\n+                    or connection.features.supports_on_delete_db_restrict\n+                ):\n+                    errors.append(\n+                        checks.Error(\n+                            f\"{connection.display_name} does not support a \"\n+                            \"DB_RESTRICT.\",\n+                            hint=\"Change the on_delete rule to RESTRICT.\",\n+                            obj=self,\n+                            id=\"fields.E324\",\n+                        ),\n+                    )\n+        elif not isinstance(self.remote_field.model, str):\n+            # Database and Python variants cannot be mixed in a chain of\n+            # model references.\n+            is_db_on_delete = isinstance(on_delete, DatabaseOnDelete)\n+            ref_model_related_fields = (\n+                ref_model_field.remote_field\n+                for ref_model_field in self.remote_field.model._meta.get_fields()\n+                if ref_model_field.related_model and not ref_model_field.auto_created\n+            )\n+\n+            for ref_remote_field in ref_model_related_fields:\n+                if (\n+                    ref_remote_field.on_delete is not None\n+                    and isinstance(ref_remote_field.on_delete, DatabaseOnDelete)\n+                    is not is_db_on_delete\n+                ):\n+                    on_delete_type = \"database\" if is_db_on_delete else \"python\"\n+                    ref_on_delete_type = \"python\" if is_db_on_delete else \"database\"\n+                    errors.append(\n+                        checks.Error(",
      "comment": "I think we should also check for auto-created OneToOneField parent links via MTI:\n```py\nclass A(models.Model):\n    ...\n\nclass B(A):\n    ...\n\nclass C(models.Model):\n    b = models.ForeignKey(B, on_delete=models.DB_CASCADE)\n```\n```py\nIn [1]: a = A.objects.create()\n\nIn [2]: b = B.objects.create(a_ptr=a)\n\nIn [3]: c = C.objects.create(b=b)\n\nIn [4]: a.delete()\nOut[4]: (2, {'models.B': 1, 'models.A': 1})\n\nIn [5]: c.refresh_from_db()\n---------------------------------------------------------------------------\nDoesNotExist                              Traceback (most recent call last)\nCell In[5], line 1\n----> 1 c.refresh_from_db()\n\nFile ~/django/django/db/models/base.py:758, in Model.refresh_from_db(self, using, fields, from_queryset)\n    749 elif deferred_fields:\n    750     db_instance_qs = db_instance_qs.only(\n    751         *{\n    752             f.attname\n   (...)    755         }\n    756     )\n--> 758 db_instance = db_instance_qs.get()\n    759 non_loaded_fields = db_instance.get_deferred_fields()\n    760 for field in self._meta.fields:\n\nFile ~/django/django/db/models/query.py:637, in QuerySet.get(self, *args, **kwargs)\n    635     return clone._result_cache[0]\n    636 if not num:\n--> 637     raise self.model.DoesNotExist(\n    638         \"%s matching query does not exist.\" % self.model._meta.object_name\n    639     )\n    640 raise self.model.MultipleObjectsReturned(\n    641     \"get() returned more than one %s -- it returned %s!\"\n    642     % (\n   (...)    645     )\n    646 )\n\nDoesNotExist: C matching query does not exist.\n```",
      "comment_id": 2433808248,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-15T20:12:05Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2433808248"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "tests/schema/tests.py",
      "line": 447,
      "side": "RIGHT",
      "diff_hunk": "@@ -410,6 +412,41 @@ def test_inline_fk(self):\n             ]\n         )\n \n+    @skipUnlessDBFeature(\"can_create_inline_fk\")\n+    def test_inline_fk_db_on_delete(self):\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Author)\n+            editor.create_model(Book)\n+            editor.create_model(Note)\n+        self.assertForeignKeyNotExists(Note, \"book_id\", \"schema_book\")\n+        # Add a foreign key from model to the other.\n+        with (\n+            CaptureQueriesContext(connection) as ctx,\n+            connection.schema_editor() as editor,\n+        ):\n+            new_field = ForeignKey(Book, DB_CASCADE)\n+            new_field.set_attributes_from_name(\"book\")\n+            editor.add_field(Note, new_field)\n+        self.assertForeignKeyExists(Note, \"book_id\", \"schema_book\")\n+        # Creating a FK field with a constraint uses a single statement without\n+        # a deferred ALTER TABLE.\n+        self.assertFalse(\n+            [\n+                sql\n+                for sql in (str(statement) for statement in editor.deferred_sql)\n+                if sql.startswith(\"ALTER TABLE\") and \"ADD CONSTRAINT\" in sql\n+            ]\n+        )\n+        # ON DELETE clause is used.\n+        self.assertTrue(\n+            [\n+                True\n+                for capture_query in ctx.captured_queries\n+                if capture_query[\"sql\"].startswith(\"ALTER TABLE\")\n+                and \"ON DELETE\" in capture_query[\"sql\"]\n+            ]",
      "comment": "At your option, I find this a little more straightforward.\n```suggestion\n            any(\n                capture_query[\"sql\"].startswith(\"ALTER TABLE\")\n                and \"ON DELETE\" in capture_query[\"sql\"]\n                for capture_query in ctx.captured_queries\n            )\n```",
      "comment_id": 2433854740,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-15T20:28:42Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2433854740"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "tests/delete/tests.py",
      "line": 101,
      "side": "RIGHT",
      "diff_hunk": "@@ -76,18 +82,50 @@ def test_setnull(self):\n         a = A.objects.get(pk=a.pk)\n         self.assertIsNone(a.setnull)\n \n+    def test_db_setnull(self):\n+        a = create_related_db_option(\"db_setnull\")\n+        a.db_setnull.delete()\n+        a = RelatedDbOption.objects.get(pk=a.pk)\n+        self.assertIsNone(a.db_setnull)\n+\n     def test_setdefault(self):\n         a = create_a(\"setdefault\")\n         a.setdefault.delete()\n         a = A.objects.get(pk=a.pk)\n         self.assertEqual(self.DEFAULT, a.setdefault.pk)\n \n+    @skipUnlessDBFeature(\"supports_on_delete_db_default\")\n+    def test_db_setdefault(self):\n+        # Object cannot be created on the module initialization, use hardcoded\n+        # PKs instead.\n+        RelatedDbOptionParent.objects.all().delete()",
      "comment": "Good catch :dart: It's leftover from using the same model for both database and python `on_delete` variants.",
      "comment_id": 2433930628,
      "user": "felixxm",
      "created_at": "2025-10-15T20:59:56Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2433930628"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "django/db/backends/oracle/features.py",
      "line": 82,
      "side": "RIGHT",
      "diff_hunk": "@@ -78,6 +78,8 @@ class DatabaseFeatures(BaseDatabaseFeatures):\n     supports_json_field_contains = False\n     supports_json_negative_indexing = False\n     supports_collation_on_textfield = False\n+    supports_on_delete_db_default = False\n+    supports_on_delete_db_restrict = False",
      "comment": "> ... since they claim that their default behavior is roughly equivalent to restrict ...\r\n\r\nCan you share any links to Oracle docs?",
      "comment_id": 2433952996,
      "user": "felixxm",
      "created_at": "2025-10-15T21:09:39Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2433952996"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "django/db/backends/oracle/features.py",
      "line": 82,
      "side": "RIGHT",
      "diff_hunk": "@@ -78,6 +78,8 @@ class DatabaseFeatures(BaseDatabaseFeatures):\n     supports_json_field_contains = False\n     supports_json_negative_indexing = False\n     supports_collation_on_textfield = False\n+    supports_on_delete_db_default = False\n+    supports_on_delete_db_restrict = False",
      "comment": "I was looking at [this](https://docs.oracle.com/cd/B28359_01/server.111/b28318/data_int.htm#CNCPT1629), which I now realize is an old version:\r\n\r\n> No action: Disallows the update or deletion of referenced data. This differs from RESTRICT in that it is checked at the end of the statement, or at the end of the transaction if the constraint is deferred. (Oracle Database uses No Action as its default action.)\r\n\r\n... but that small difference is represented by our `PROTECT` and `DO NOTHING`, which Simon's reply below clarified for me. Nothing to add here, I think.",
      "comment_id": 2434077955,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-15T21:55:20Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2434077955"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "tests/invalid_models_tests/test_relative_fields.py",
      "line": 2441,
      "side": "RIGHT",
      "diff_hunk": "@@ -2259,3 +2283,162 @@ class FooBar(models.Model):\n                 ),\n             ],\n         )\n+\n+\n+@isolate_apps(\"invalid_models_tests\")\n+class DatabaseLevelOnDeleteTests(TestCase):\n+\n+    def test_db_set_default_support(self):\n+        class Parent(models.Model):\n+            pass\n+\n+        class Child(models.Model):\n+            parent = models.ForeignKey(\n+                Parent, models.DB_SET_DEFAULT, db_default=models.Value(1)\n+            )\n+\n+        field = Child._meta.get_field(\"parent\")\n+        expected = (\n+            []\n+            if connection.features.supports_on_delete_db_default\n+            else [\n+                Error(\n+                    f\"{connection.display_name} does not support a DB_SET_DEFAULT.\",\n+                    hint=\"Change the on_delete rule to SET_DEFAULT.\",\n+                    obj=field,\n+                    id=\"fields.E324\",\n+                )\n+            ]\n+        )\n+        self.assertEqual(field.check(databases=self.databases), expected)\n+\n+    def test_db_set_default_required_db_features(self):\n+        class Parent(models.Model):\n+            pass\n+\n+        class Child(models.Model):\n+            parent = models.ForeignKey(\n+                Parent, models.DB_SET_DEFAULT, db_default=models.Value(1)\n+            )\n+\n+            class Meta:\n+                required_db_features = {\"supports_on_delete_db_default\"}\n+\n+        field = Child._meta.get_field(\"parent\")\n+        self.assertEqual(field.check(databases=self.databases), [])\n+\n+    @skipUnlessDBFeature(\"supports_on_delete_db_default\")\n+    def test_db_set_default_no_db_default(self):\n+        class Parent(models.Model):\n+            pass\n+\n+        class Child(models.Model):\n+            parent = models.ForeignKey(Parent, models.DB_SET_DEFAULT)\n+\n+        field = Child._meta.get_field(\"parent\")\n+        self.assertEqual(\n+            field.check(databases=self.databases),\n+            [\n+                Error(\n+                    \"Field specifies on_delete=DB_SET_DEFAULT, but has no db_default \"\n+                    \"value.\",\n+                    hint=\"Set a db_default value, or change the on_delete rule.\",\n+                    obj=field,\n+                    id=\"fields.E322\",\n+                )\n+            ],\n+        )\n+\n+    def test_python_db_chain(self):\n+        class GrandParent(models.Model):\n+            pass\n+\n+        class Parent(models.Model):\n+            grand_parent = models.ForeignKey(GrandParent, models.DB_CASCADE)\n+\n+        class Child(models.Model):\n+            parent = models.ForeignKey(Parent, models.RESTRICT)\n+\n+        field = Child._meta.get_field(\"parent\")\n+        self.assertEqual(\n+            field.check(databases=self.databases),\n+            [\n+                Error(\n+                    \"Field specifies python-level on_delete variant, but referenced \"\n+                    \"model uses database-level variant.\",\n+                    hint=(\n+                        \"Use either database or python on_delete variants uniformly in \"\n+                        \"the references chain.\"\n+                    ),\n+                    obj=field,\n+                    id=\"fields.E323\",\n+                )\n+            ],\n+        )\n+\n+    def test_db_python_chain(self):\n+        class GrandParent(models.Model):\n+            pass\n+\n+        class Parent(models.Model):\n+            grand_parent = models.ForeignKey(GrandParent, models.CASCADE)\n+\n+        class Child(models.Model):\n+            parent = models.ForeignKey(Parent, models.DB_SET_NULL, null=True)\n+\n+        field = Child._meta.get_field(\"parent\")\n+        self.assertEqual(\n+            field.check(databases=self.databases),\n+            [\n+                Error(\n+                    \"Field specifies database-level on_delete variant, but referenced \"\n+                    \"model uses python-level variant.\",\n+                    hint=(\n+                        \"Use either database or python on_delete variants uniformly in \"\n+                        \"the references chain.\"\n+                    ),\n+                    obj=field,\n+                    id=\"fields.E323\",\n+                )\n+            ],\n+        )\n+\n+    def test_db_python_chain_auto_created(self):\n+        class GrandParent(models.Model):\n+            pass\n+\n+        class Parent(GrandParent):\n+            pass\n+\n+        class Child(models.Model):\n+            parent = models.ForeignKey(Parent, on_delete=models.DB_CASCADE)\n+\n+        field = Child._meta.get_field(\"parent\")\n+        self.assertEqual(\n+            field.check(databases=self.databases),\n+            [\n+                Error(\n+                    \"Field specifies database-level on_delete variant, but referenced \"\n+                    \"model uses python-level variant.\",\n+                    hint=(\n+                        \"Use either database or python on_delete variants uniformly in \"\n+                        \"the references chain.\"\n+                    ),\n+                    obj=field,\n+                    id=\"fields.E323\",\n+                )\n+            ],\n+        )\n+\n+    def test_db_do_nothing_chain(self):\n+        class GrandParent(models.Model):\n+            pass\n+\n+        class Parent(models.Model):\n+            grand_parent = models.ForeignKey(GrandParent, models.DO_NOTHING)\n+\n+        class Child(models.Model):\n+            parent = models.ForeignKey(Parent, models.DB_SET_NULL, null=True)",
      "comment": "Reversing these gives a failure:\n```diff\ndiff --git a/tests/invalid_models_tests/test_relative_fields.py b/tests/invalid_models_tests/test_relative_fields.py\nindex b86235c964..06c42c77b6 100644\n--- a/tests/invalid_models_tests/test_relative_fields.py\n+++ b/tests/invalid_models_tests/test_relative_fields.py\n@@ -2435,10 +2435,10 @@ class DatabaseLevelOnDeleteTests(TestCase):\n             pass\n \n         class Parent(models.Model):\n-            grand_parent = models.ForeignKey(GrandParent, models.DO_NOTHING)\n+            grand_parent = models.ForeignKey(GrandParent, models.DB_SET_NULL, null=True)\n \n         class Child(models.Model):\n-            parent = models.ForeignKey(Parent, models.DB_SET_NULL, null=True)\n+            parent = models.ForeignKey(Parent, models.DO_NOTHING)\n \n         field = Child._meta.get_field(\"parent\")\n         self.assertEqual(field.check(databases=self.databases), [])\n```\n```\nAssertionError: Lists differ: [<Error: level=40, msg='Field specifies py[245 chars]23'>] != []\n\nFirst list contains 1 additional elements.\nFirst extra element 0:\n<Error: level=40, msg='Field specifies python-level on_delete variant, but referenced model uses database-level variant.', hint='Use either database or python on_delete variants uniformly in the references chain.', obj=<django.db.models.fields.related.ForeignKey: parent>, id='fields.E323'>\n```",
      "comment_id": 2437223646,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-16T19:26:46Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2437223646"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "django/db/models/base.py",
      "line": 2469,
      "side": "RIGHT",
      "diff_hunk": "@@ -2455,6 +2456,29 @@ def _check_long_column_names(cls, databases):\n \n         return errors\n \n+    @classmethod\n+    def _check_related_fields(cls):\n+        has_db_variant = False\n+        has_python_variant = False\n+        for rel in cls._meta.get_fields():\n+            if rel.related_model and not rel.auto_created:\n+                if not (on_delete := getattr(rel.remote_field, \"on_delete\", None)):\n+                    continue\n+                if isinstance(on_delete, DatabaseOnDelete):\n+                    has_db_variant = True\n+                else:",
      "comment": "Same problem here, suspect we want something like:\n```suggestion\n                elif on_delete != DO_NOTHING:\n```",
      "comment_id": 2437441499,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-16T20:52:55Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2437441499"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "django/db/models/base.py",
      "line": 2469,
      "side": "RIGHT",
      "diff_hunk": "@@ -2455,6 +2456,29 @@ def _check_long_column_names(cls, databases):\n \n         return errors\n \n+    @classmethod\n+    def _check_related_fields(cls):\n+        has_db_variant = False\n+        has_python_variant = False\n+        for rel in cls._meta.get_fields():\n+            if rel.related_model and not rel.auto_created:\n+                if not (on_delete := getattr(rel.remote_field, \"on_delete\", None)):\n+                    continue\n+                if isinstance(on_delete, DatabaseOnDelete):\n+                    has_db_variant = True\n+                else:",
      "comment": "Fixed, I also fixed this system check for auto-created `OneToOneField`s.",
      "comment_id": 2438601824,
      "user": "felixxm",
      "created_at": "2025-10-17T06:59:43Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2438601824"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "tests/delete/tests.py",
      "line": 411,
      "side": "RIGHT",
      "diff_hunk": "@@ -359,6 +394,22 @@ def test_bulk(self):\n         self.assertNumQueries(5, s.delete)\n         self.assertFalse(S.objects.exists())\n \n+    def test_db_cascade(self):\n+        related_db_op = RelatedDbOptionParent.objects.create(\n+            p=RelatedDbOptionGrandParent.objects.create()\n+        )\n+        RelatedDbOption.objects.bulk_create(\n+            [\n+                RelatedDbOption(db_cascade=related_db_op)\n+                for _ in range(2 * GET_ITERATOR_CHUNK_SIZE)\n+            ]\n+        )\n+        with self.assertNumQueries(1):\n+            results = related_db_op.delete()\n+            self.assertEqual(results, (1, {\"delete.RelatedDbOptionParent\": 1}))\n+        self.assertFalse(RelatedDbOption.objects.exists())\n+        self.assertFalse(RelatedDbOptionParent.objects.exists())",
      "comment": "> Are some `@skipUnlessDBFeature(\"supports_on_delete_db_default\")` missing? I have these failures on MongoDB:\r\n\r\n`supports_on_delete_db_default` is only for `SET DEFAULT` support. We can add `supports_on_delete_db_null` and `supports_on_delete_db_cascade` feature flags if none of the remaining options are supported on some databases :thinking: ",
      "comment_id": 2545344719,
      "user": "felixxm",
      "created_at": "2025-11-20T10:29:01Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2545344719"
    },
    {
      "repo": "django/django",
      "pr_number": 19925,
      "file_path": "tests/delete/tests.py",
      "line": 411,
      "side": "RIGHT",
      "diff_hunk": "@@ -359,6 +394,22 @@ def test_bulk(self):\n         self.assertNumQueries(5, s.delete)\n         self.assertFalse(S.objects.exists())\n \n+    def test_db_cascade(self):\n+        related_db_op = RelatedDbOptionParent.objects.create(\n+            p=RelatedDbOptionGrandParent.objects.create()\n+        )\n+        RelatedDbOption.objects.bulk_create(\n+            [\n+                RelatedDbOption(db_cascade=related_db_op)\n+                for _ in range(2 * GET_ITERATOR_CHUNK_SIZE)\n+            ]\n+        )\n+        with self.assertNumQueries(1):\n+            results = related_db_op.delete()\n+            self.assertEqual(results, (1, {\"delete.RelatedDbOptionParent\": 1}))\n+        self.assertFalse(RelatedDbOption.objects.exists())\n+        self.assertFalse(RelatedDbOptionParent.objects.exists())",
      "comment": "Thanks for explaining. Here you are: https://github.com/django/django/pull/20286",
      "comment_id": 2548340448,
      "user": "timgraham",
      "created_at": "2025-11-21T02:34:26Z",
      "url": "https://github.com/django/django/pull/19925#discussion_r2548340448"
    },
    {
      "repo": "django/django",
      "pr_number": 20183,
      "file_path": "django/utils/html.py",
      "line": 79,
      "side": "RIGHT",
      "diff_hunk": "@@ -76,8 +76,9 @@ def escape(text):\n     ord(\"\\u2029\"): \"\\\\u2029\",\n }\n \n-# Escape every ASCII character with a value less than 32.\n+# Escape every ASCII character with a value less than 32 (C0) && 127-159(C1).",
      "comment": "Nit: Suggest keeping the comment as prose\n```suggestion\n# Escape every ASCII character with a value less than 32 (C0) and 127-159(C1).\n```",
      "comment_id": 2537169158,
      "user": "RealOrangeOne",
      "created_at": "2025-11-18T09:44:23Z",
      "url": "https://github.com/django/django/pull/20183#discussion_r2537169158"
    },
    {
      "repo": "django/django",
      "pr_number": 20183,
      "file_path": "tests/utils_tests/test_html.py",
      "line": 249,
      "side": "RIGHT",
      "diff_hunk": "@@ -244,6 +244,9 @@ def test_escapejs(self):\n                 \"paragraph separator:\\\\u2029and line separator:\\\\u2028\",\n             ),\n             (\"`\", \"\\\\u0060\"),\n+            (\"\\u007F\", \"\\u007F\"),       \n+            (\"\\u0080\", \"\\u0080\"),\n+            (\"\\u009F\",\"\\u009F\"),",
      "comment": "```suggestion\r\n            (\"\\u009F\", \"\\u009F\"),\r\n```",
      "comment_id": 2537179876,
      "user": "pauloxnet",
      "created_at": "2025-11-18T09:46:22Z",
      "url": "https://github.com/django/django/pull/20183#discussion_r2537179876"
    },
    {
      "repo": "django/django",
      "pr_number": 20183,
      "file_path": "tests/utils_tests/test_html.py",
      "line": 247,
      "side": "RIGHT",
      "diff_hunk": "@@ -244,6 +244,9 @@ def test_escapejs(self):\n                 \"paragraph separator:\\\\u2029and line separator:\\\\u2028\",\n             ),\n             (\"`\", \"\\\\u0060\"),\n+            (\"\\u007F\", \"\\u007F\"),       ",
      "comment": "```suggestion\r\n            (\"\\u007F\", \"\\u007F\"),\r\n```",
      "comment_id": 2537180633,
      "user": "pauloxnet",
      "created_at": "2025-11-18T09:46:29Z",
      "url": "https://github.com/django/django/pull/20183#discussion_r2537180633"
    },
    {
      "repo": "django/django",
      "pr_number": 20183,
      "file_path": "tests/utils_tests/test_html.py",
      "line": 249,
      "side": "RIGHT",
      "diff_hunk": "@@ -244,6 +244,9 @@ def test_escapejs(self):\n                 \"paragraph separator:\\\\u2029and line separator:\\\\u2028\",\n             ),\n             (\"`\", \"\\\\u0060\"),\n+            (\"\\u007f\", \"\\u007f\"),\n+            (\"\\u0080\", \"\\u0080\"),\n+            (\"\\u009f\", \"\\u009f\"),",
      "comment": "These should all be escaped\n```suggestion\n            (\"\\u007f\", \"\\\\u007F\"),\n            (\"\\u0080\", \"\\\\u0080\"),\n            (\"\\u009f\", \"\\\\u009F\"),\n```",
      "comment_id": 2539170208,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-18T17:55:21Z",
      "url": "https://github.com/django/django/pull/20183#discussion_r2539170208"
    },
    {
      "repo": "django/django",
      "pr_number": 20183,
      "file_path": "tests/utils_tests/test_html.py",
      "line": 249,
      "side": "RIGHT",
      "diff_hunk": "@@ -244,6 +244,9 @@ def test_escapejs(self):\n                 \"paragraph separator:\\\\u2029and line separator:\\\\u2028\",\n             ),\n             (\"`\", \"\\\\u0060\"),\n+            (\"\\u007f\", \"\\u007f\"),\n+            (\"\\u0080\", \"\\u0080\"),\n+            (\"\\u009f\", \"\\u009f\"),",
      "comment": "Major doubt I have checked my code works but during the implementation of test cases throws an ` Asserstion error` that similar test executed separately runs perfectly \r\nI have been stuck with this for a while",
      "comment_id": 2540597502,
      "user": "farthestmage",
      "created_at": "2025-11-19T05:30:23Z",
      "url": "https://github.com/django/django/pull/20183#discussion_r2540597502"
    },
    {
      "repo": "django/django",
      "pr_number": 20183,
      "file_path": "tests/utils_tests/test_html.py",
      "line": 249,
      "side": "RIGHT",
      "diff_hunk": "@@ -244,6 +244,9 @@ def test_escapejs(self):\n                 \"paragraph separator:\\\\u2029and line separator:\\\\u2028\",\n             ),\n             (\"`\", \"\\\\u0060\"),\n+            (\"\\u007f\", \"\\u007f\"),\n+            (\"\\u0080\", \"\\u0080\"),\n+            (\"\\u009f\", \"\\u009f\"),",
      "comment": "The test passes for me when I correct the expected results.",
      "comment_id": 2542253819,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-19T14:28:39Z",
      "url": "https://github.com/django/django/pull/20183#discussion_r2542253819"
    },
    {
      "repo": "django/django",
      "pr_number": 20183,
      "file_path": "django/utils/html.py",
      "line": 81,
      "side": "RIGHT",
      "diff_hunk": "@@ -76,8 +76,9 @@ def escape(text):\n     ord(\"\\u2029\"): \"\\\\u2029\",\n }\n \n-# Escape every ASCII character with a value less than 32.\n+# Escape every ASCII character with a value less than 32 (C0) and 127-159(C1).\n _js_escapes.update((ord(\"%c\" % z), \"\\\\u%04X\" % z) for z in range(32))\n+_js_escapes.update((ord(\"%c\" % z), \"\\\\u%04X\" % z) for z in range(0x7F, 0xA0))",
      "comment": "You can just do this once if you `from itertools import chain`. Also please don't include 127 with the C1 range, it's C0.\n```suggestion\n# Escape every ASCII character with a value less than 32 (C0), 127 (C0),\n# or 128-159(C1).\n_js_escapes.update(\n    (ord(\"%c\" % z), \"\\\\u%04X\" % z) for z in chain(range(32), range(0x7F, 0xA0))\n)\n```",
      "comment_id": 2542489782,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-19T15:21:21Z",
      "url": "https://github.com/django/django/pull/20183#discussion_r2542489782"
    },
    {
      "repo": "django/django",
      "pr_number": 20183,
      "file_path": "django/utils/html.py",
      "line": 80,
      "side": "RIGHT",
      "diff_hunk": "@@ -76,8 +77,11 @@ def escape(text):\n     ord(\"\\u2029\"): \"\\\\u2029\",\n }\n \n-# Escape every ASCII character with a value less than 32.\n-_js_escapes.update((ord(\"%c\" % z), \"\\\\u%04X\" % z) for z in range(32))\n+# Escape every ASCII character with a value less than 32 (C0) ,127(C0)",
      "comment": "```suggestion\n# Escape every ASCII character with a value less than 32 (C0), 127(C0)\n```",
      "comment_id": 2543233340,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-19T19:08:22Z",
      "url": "https://github.com/django/django/pull/20183#discussion_r2543233340"
    },
    {
      "repo": "django/django",
      "pr_number": 20183,
      "file_path": "django/utils/html.py",
      "line": 81,
      "side": "RIGHT",
      "diff_hunk": "@@ -76,8 +77,11 @@ def escape(text):\n     ord(\"\\u2029\"): \"\\\\u2029\",\n }\n \n-# Escape every ASCII character with a value less than 32.\n-_js_escapes.update((ord(\"%c\" % z), \"\\\\u%04X\" % z) for z in range(32))\n+# Escape every ASCII character with a value less than 32 (C0), 127(C0)\n+#  or 128-159(C1).",
      "comment": "```suggestion\n# Escape every ASCII character with a value less than 32 (C0), 127(C0),\n# or 128-159(C1).\n```",
      "comment_id": 2543235742,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-19T19:09:17Z",
      "url": "https://github.com/django/django/pull/20183#discussion_r2543235742"
    },
    {
      "repo": "django/django",
      "pr_number": 20043,
      "file_path": "django/db/migrations/loader.py",
      "line": 112,
      "side": "RIGHT",
      "diff_hunk": "@@ -109,11 +109,11 @@ def load_disk(self):\n                 if was_loaded:\n                     reload(module)\n             self.migrated_apps.add(app_config.label)\n-            migration_names = {\n+            migration_names = [",
      "comment": "I considered making this `sorted()`, but I wanted to take the lightest possible touch for now, and not risk making a Django 6.1 upgrade any more difficult for anyone.",
      "comment_id": 2487263895,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-03T17:15:27Z",
      "url": "https://github.com/django/django/pull/20043#discussion_r2487263895"
    },
    {
      "repo": "django/django",
      "pr_number": 20043,
      "file_path": "tests/migrations/test_loader.py",
      "line": 687,
      "side": "RIGHT",
      "diff_hunk": "@@ -649,6 +654,70 @@ def test_loading_package_without__file__(self):\n             test_module.__spec__.origin = module_origin\n             test_module.__spec__.has_location = module_has_location\n \n+    def test_loading_order_does_not_create_circular_dependency(self):\n+        \"\"\"\n+        Before, for these migrations:\n+        app1\n+        [ ] 0001_squashed_initial <- replaces app1.0001\n+        [ ] 0002_squashed_initial <- replaces app1.0001\n+            depends on app1.0001_squashed_initial & app2.0001_squashed_initial\n+        app2\n+        [ ] 0001_squashed_initial <- replaces app2.0001\n+\n+        When loading app1's migrations, if 0002_squashed_initial was first:\n+        {'0002_squashed_initial', '0001_initial', '0001_squashed_initial'}\n+        Then CircularDependencyError was raised, but it's resolvable as:\n+        {'0001_initial', '0001_squashed_initial', '0002_squashed_initial'}\n+        \"\"\"\n+        # Create a test settings file to provide to the subprocess.\n+        MIGRATION_MODULES = {\n+            \"app1\": \"migrations.test_migrations_squashed_replaced_order.app1\",\n+            \"app2\": \"migrations.test_migrations_squashed_replaced_order.app2\",\n+        }\n+        INSTALLED_APPS = [\n+            \"migrations.test_migrations_squashed_replaced_order.app1\",\n+            \"migrations.test_migrations_squashed_replaced_order.app2\",\n+        ]\n+        tests_dir = Path(__file__).parent.parent\n+        with tempfile.NamedTemporaryFile(\n+            mode=\"w\", encoding=\"utf-8\", suffix=\".py\", dir=tests_dir, delete=False\n+        ) as test_settings:\n+            for attr, value in settings._wrapped.__dict__.items():\n+                if attr.isupper():\n+                    test_settings.write(f\"{attr} = {value!r}\\n\")",
      "comment": "This is a bit brittle. I don't know that it's worth accounting for, but for MongoDB we have this patch:\r\n```diff\r\ndiff --git a/tests/runtests.py b/tests/runtests.py\r\nindex 500908978a..b72b9b7de1 100755\r\n--- a/tests/runtests.py\r\n+++ b/tests/runtests.py\r\n@@ -14,6 +14,7 @@ import warnings\r\n from pathlib import Path\r\n \r\n import django_mongodb_backend\r\n+from bson import ObjectId\r\n \r\n try:\r\n     import django\r\n@@ -247,7 +248,7 @@ def setup_collect_tests(start_at, start_after, test_labels=None):\r\n         }\r\n     ]\r\n     settings.LANGUAGE_CODE = \"en\"\r\n-    settings.SITE_ID = 1\r\n+    settings.SITE_ID = ObjectId(\"000000000000000000000001\")\r\n     settings.MIDDLEWARE = ALWAYS_MIDDLEWARE\r\n     settings.MIGRATION_MODULES = {\r\n         # This lets us skip creating migrations for the test models as many of\r\n```\r\nThe written migration file doesn't have the necessary import:\r\n```\r\n  File \"/home/tim/code/django/tests/tmpf6bu1e4h.py\", line 151, in <module>\r\n    SITE_ID = ObjectId('000000000000000000000001')\r\n              ^^^^^^^^\r\nNameError: name 'ObjectId' is not defined. Did you mean: 'object'?\r\n```",
      "comment_id": 2544080477,
      "user": "timgraham",
      "created_at": "2025-11-20T01:49:58Z",
      "url": "https://github.com/django/django/pull/20043#discussion_r2544080477"
    },
    {
      "repo": "django/django",
      "pr_number": 20043,
      "file_path": "tests/migrations/test_loader.py",
      "line": 687,
      "side": "RIGHT",
      "diff_hunk": "@@ -649,6 +654,70 @@ def test_loading_package_without__file__(self):\n             test_module.__spec__.origin = module_origin\n             test_module.__spec__.has_location = module_has_location\n \n+    def test_loading_order_does_not_create_circular_dependency(self):\n+        \"\"\"\n+        Before, for these migrations:\n+        app1\n+        [ ] 0001_squashed_initial <- replaces app1.0001\n+        [ ] 0002_squashed_initial <- replaces app1.0001\n+            depends on app1.0001_squashed_initial & app2.0001_squashed_initial\n+        app2\n+        [ ] 0001_squashed_initial <- replaces app2.0001\n+\n+        When loading app1's migrations, if 0002_squashed_initial was first:\n+        {'0002_squashed_initial', '0001_initial', '0001_squashed_initial'}\n+        Then CircularDependencyError was raised, but it's resolvable as:\n+        {'0001_initial', '0001_squashed_initial', '0002_squashed_initial'}\n+        \"\"\"\n+        # Create a test settings file to provide to the subprocess.\n+        MIGRATION_MODULES = {\n+            \"app1\": \"migrations.test_migrations_squashed_replaced_order.app1\",\n+            \"app2\": \"migrations.test_migrations_squashed_replaced_order.app2\",\n+        }\n+        INSTALLED_APPS = [\n+            \"migrations.test_migrations_squashed_replaced_order.app1\",\n+            \"migrations.test_migrations_squashed_replaced_order.app2\",\n+        ]\n+        tests_dir = Path(__file__).parent.parent\n+        with tempfile.NamedTemporaryFile(\n+            mode=\"w\", encoding=\"utf-8\", suffix=\".py\", dir=tests_dir, delete=False\n+        ) as test_settings:\n+            for attr, value in settings._wrapped.__dict__.items():\n+                if attr.isupper():\n+                    test_settings.write(f\"{attr} = {value!r}\\n\")",
      "comment": "Ah. A prior iteration tried to only write the few settings that appeared relevant for the test, until the geodjango tests revealed that the geo settings needed to be present in order connect to the db. I'll give it some thought. Maybe we can only write out settings values consisting of literals and end up with something slightly less brittle.",
      "comment_id": 2544230217,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-20T03:41:30Z",
      "url": "https://github.com/django/django/pull/20043#discussion_r2544230217"
    },
    {
      "repo": "django/django",
      "pr_number": 20267,
      "file_path": "django/db/models/fields/__init__.py",
      "line": 320,
      "side": "RIGHT",
      "diff_hunk": "@@ -316,11 +316,13 @@ def _check_choices(self):\n         if not self.choices:\n             return []\n \n-        if not isinstance(self.choices, Iterable) or isinstance(self.choices, str):\n+        if not isinstance(self.choices, Iterable) or isinstance(\n+            self.choices, (str, set, frozenset)",
      "comment": "Ahh I always assumed `frozenset` was subclass of `set` but it makes sense that it isn't (e.g. it doesn't have an `add` method).",
      "comment_id": 2539076876,
      "user": "charettes",
      "created_at": "2025-11-18T17:23:32Z",
      "url": "https://github.com/django/django/pull/20267#discussion_r2539076876"
    },
    {
      "repo": "django/django",
      "pr_number": 20267,
      "file_path": "django/db/models/fields/__init__.py",
      "line": 320,
      "side": "RIGHT",
      "diff_hunk": "@@ -316,11 +316,13 @@ def _check_choices(self):\n         if not self.choices:\n             return []\n \n-        if not isinstance(self.choices, Iterable) or isinstance(self.choices, str):\n+        if not isinstance(self.choices, Iterable) or isinstance(\n+            self.choices, (str, set, frozenset)",
      "comment": "> Ahh I always assumed `frozenset` was subclass of `set` ....\r\n\r\n![giphy](https://github.com/user-attachments/assets/f0d9f81c-df72-45f9-bbb7-422b18a4f081)\r\n\r\n:sweat_smile: ",
      "comment_id": 2539091817,
      "user": "felixxm",
      "created_at": "2025-11-18T17:28:32Z",
      "url": "https://github.com/django/django/pull/20267#discussion_r2539091817"
    },
    {
      "repo": "django/django",
      "pr_number": 20267,
      "file_path": "django/db/models/fields/__init__.py",
      "line": 320,
      "side": "RIGHT",
      "diff_hunk": "@@ -316,11 +316,13 @@ def _check_choices(self):\n         if not self.choices:\n             return []\n \n-        if not isinstance(self.choices, Iterable) or isinstance(self.choices, str):\n+        if not isinstance(self.choices, Iterable) or isinstance(\n+            self.choices, (str, set, frozenset)",
      "comment": "But they are both subclasses of [`collections.abc.Set`](https://docs.python.org/3/library/collections.abc.html#collections.abc.Set).",
      "comment_id": 2542498015,
      "user": "ngnpope",
      "created_at": "2025-11-19T15:23:35Z",
      "url": "https://github.com/django/django/pull/20267#discussion_r2542498015"
    },
    {
      "repo": "django/django",
      "pr_number": 20098,
      "file_path": "django/utils/feedgenerator.py",
      "line": 108,
      "side": "RIGHT",
      "diff_hunk": "@@ -95,11 +96,15 @@ def mimetype(self):\n         return self._mimetype\n \n     def __str__(self):\n-        data = [f'href=\"{self.url}\"']\n-        if self.mimetype is not None:\n-            data.append(f'type=\"{self.mimetype}\"')\n-        if self.media is not None:\n-            data.append(f'media=\"{self.media}\"')\n+        url = escape(iri_to_uri(self._url))\n+        mimetype = escape(self.mimetype) if self.mimetype is not None else None\n+        media = escape(self.media) if self.media is not None else None\n+\n+        data = [f'href=\"{url}\"']\n+        if mimetype is not None:\n+            data.append(f'type=\"{mimetype}\"')\n+        if media is not None:\n+            data.append(f'media=\"{media}\"')\n         return \" \".join(data)",
      "comment": "Suggestion: It might be nice to use `django.forms.utils.flatatt` here, since it handles both joining the values together and escaping them",
      "comment_id": 2534936310,
      "user": "RealOrangeOne",
      "created_at": "2025-11-17T17:24:03Z",
      "url": "https://github.com/django/django/pull/20098#discussion_r2534936310"
    },
    {
      "repo": "django/django",
      "pr_number": 20098,
      "file_path": "tests/utils_tests/test_feedgenerator.py",
      "line": 166,
      "side": "RIGHT",
      "diff_hunk": "@@ -159,3 +159,20 @@ def test_stylesheet_keeps_lazy_urls(self):\n             str(stylesheet), 'href=\"test.css\" type=\"text/css\" media=\"screen\"'\n         )\n         m.assert_called_once()\n+\n+    def test_stylesheet_attribute_escaping(self):\n+        \"\"\"\n+        Stylesheet.__str__() should escape attribute values.\n+        \"\"\"",
      "comment": "You can chop this docstring; we favor descriptive method names instead (which you have done).",
      "comment_id": 2535288327,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-17T19:31:59Z",
      "url": "https://github.com/django/django/pull/20098#discussion_r2535288327"
    },
    {
      "repo": "django/django",
      "pr_number": 20098,
      "file_path": "django/utils/feedgenerator.py",
      "line": 108,
      "side": "RIGHT",
      "diff_hunk": "@@ -95,11 +96,15 @@ def mimetype(self):\n         return self._mimetype\n \n     def __str__(self):\n-        data = [f'href=\"{self.url}\"']\n-        if self.mimetype is not None:\n-            data.append(f'type=\"{self.mimetype}\"')\n-        if self.media is not None:\n-            data.append(f'media=\"{self.media}\"')\n+        url = escape(iri_to_uri(self._url))\n+        mimetype = escape(self.mimetype) if self.mimetype is not None else None\n+        media = escape(self.media) if self.media is not None else None\n+\n+        data = [f'href=\"{url}\"']\n+        if mimetype is not None:\n+            data.append(f'type=\"{mimetype}\"')\n+        if media is not None:\n+            data.append(f'media=\"{media}\"')\n         return \" \".join(data)",
      "comment": "It is unfortunate to have to update the tests, but I agree with @RealOrangeOne that delegating to `flatatt` is nicer.\n\n@varunkasyap are you up for updating the tests? What hurts is that we don't have an `assertInXML` helper like we do for `assertInHTML`.",
      "comment_id": 2535368961,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-17T20:05:30Z",
      "url": "https://github.com/django/django/pull/20098#discussion_r2535368961"
    },
    {
      "repo": "django/django",
      "pr_number": 20098,
      "file_path": "django/utils/feedgenerator.py",
      "line": 104,
      "side": "RIGHT",
      "diff_hunk": "@@ -95,12 +96,13 @@ def mimetype(self):\n         return self._mimetype\n \n     def __str__(self):\n-        data = [f'href=\"{self.url}\"']\n-        if self.mimetype is not None:\n-            data.append(f'type=\"{self.mimetype}\"')\n-        if self.media is not None:\n-            data.append(f'media=\"{self.media}\"')\n-        return \" \".join(data)\n+        attrs = {\n+            \"href\": iri_to_uri(self._url),\n+            \"type\": self.mimetype,\n+            \"media\": self.media,\n+        }\n+        final_attrs = {k: v for k, v in attrs.items() if v is not None}",
      "comment": "Nit: Looks like `flatatt` excludes `None` already: \n\nhttps://github.com/django/django/blob/3c005b5f79bf6d71f3f4c3692ed670e1722b0fb6/django/forms/utils.py#L37",
      "comment_id": 2537299479,
      "user": "RealOrangeOne",
      "created_at": "2025-11-18T10:12:17Z",
      "url": "https://github.com/django/django/pull/20098#discussion_r2537299479"
    },
    {
      "repo": "django/django",
      "pr_number": 20094,
      "file_path": "django/db/migrations/loader.py",
      "line": 374,
      "side": "RIGHT",
      "diff_hunk": "@@ -372,6 +369,20 @@ def check_consistent_history(self, connection):\n                         )\n                     )\n \n+    def all_replaced_applied(self, migration_key, applied):\n+        \"\"\"\n+        Checks (recursively) whether all replaced migrations are applied.",
      "comment": "Just noting for others: the checking for cycles is done earlier, in `replace_migration()`. I can't find a way to cause recursion errors that wouldn't be caught there.",
      "comment_id": 2528961826,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-14T21:18:56Z",
      "url": "https://github.com/django/django/pull/20094#discussion_r2528961826"
    },
    {
      "repo": "django/django",
      "pr_number": 19370,
      "file_path": "django/dispatch/dispatcher.py",
      "line": 29,
      "side": "RIGHT",
      "diff_hunk": "@@ -21,6 +21,29 @@ def _make_id(target):\n # A marker for caching\n NO_RECEIVERS = object()\n \n+async def _gather(*coros):\n+    if len(coros) == 0:\n+        return []\n+\n+    if len(coros) == 1:\n+        return [await coros[0], ]",
      "comment": "```suggestion\r\n        return [await coros[0]]\r\n```",
      "comment_id": 2036839318,
      "user": "graingert",
      "created_at": "2025-04-10T08:44:35Z",
      "url": "https://github.com/django/django/pull/19370#discussion_r2036839318"
    },
    {
      "repo": "django/django",
      "pr_number": 19370,
      "file_path": "django/dispatch/dispatcher.py",
      "line": 44,
      "side": "RIGHT",
      "diff_hunk": "@@ -22,6 +22,28 @@ def _make_id(target):\n NO_RECEIVERS = object()\n \n \n+async def _gather(*coros):\n+    if len(coros) == 0:\n+        return []\n+\n+    if len(coros) == 1:\n+        return [await coros[0]]\n+\n+    async def run(i, coro):\n+        results[i] = await coro\n+\n+    try:\n+        async with asyncio.TaskGroup() as tg:\n+            results = [None] * len(coros)\n+            for i, coro in enumerate(coros):\n+                tg.create_task(run(i, coro))\n+        return results\n+    except BaseExceptionGroup as exception_group:\n+        if len(exception_group.exceptions) == 1:\n+            raise exception_group.exceptions[0]\n+        raise",
      "comment": "Can this be simplified to:\r\n\r\n```suggestion\r\nasync def gather_tasks(*coros):\r\n    async with asyncio.TaskGroup() as tg:\r\n        tasks = [tg.create_task(coro) for coro in coros]\r\n    return [task.result() for task in tasks]\r\n```\r\n\r\nunless there's reason for the error handling? (BaseExceptionGroup).\r\n\r\nAlso, it might be worth moving the function to a `utils.py` file, so that it can easily be reused by other places.",
      "comment_id": 2061248814,
      "user": "g-nie",
      "created_at": "2025-04-26T09:58:37Z",
      "url": "https://github.com/django/django/pull/19370#discussion_r2061248814"
    },
    {
      "repo": "django/django",
      "pr_number": 19370,
      "file_path": "django/dispatch/dispatcher.py",
      "line": 44,
      "side": "RIGHT",
      "diff_hunk": "@@ -22,6 +22,28 @@ def _make_id(target):\n NO_RECEIVERS = object()\n \n \n+async def _gather(*coros):\n+    if len(coros) == 0:\n+        return []\n+\n+    if len(coros) == 1:\n+        return [await coros[0]]\n+\n+    async def run(i, coro):\n+        results[i] = await coro\n+\n+    try:\n+        async with asyncio.TaskGroup() as tg:\n+            results = [None] * len(coros)\n+            for i, coro in enumerate(coros):\n+                tg.create_task(run(i, coro))\n+        return results\n+    except BaseExceptionGroup as exception_group:\n+        if len(exception_group.exceptions) == 1:\n+            raise exception_group.exceptions[0]\n+        raise",
      "comment": "Yes the error handling is important to maintain backwards compatibility with the old use of gather",
      "comment_id": 2061250947,
      "user": "graingert",
      "created_at": "2025-04-26T10:13:18Z",
      "url": "https://github.com/django/django/pull/19370#discussion_r2061250947"
    },
    {
      "repo": "django/django",
      "pr_number": 19370,
      "file_path": "django/dispatch/dispatcher.py",
      "line": 44,
      "side": "RIGHT",
      "diff_hunk": "@@ -22,6 +22,28 @@ def _make_id(target):\n NO_RECEIVERS = object()\n \n \n+async def _gather(*coros):\n+    if len(coros) == 0:\n+        return []\n+\n+    if len(coros) == 1:\n+        return [await coros[0]]\n+\n+    async def run(i, coro):\n+        results[i] = await coro\n+\n+    try:\n+        async with asyncio.TaskGroup() as tg:\n+            results = [None] * len(coros)\n+            for i, coro in enumerate(coros):\n+                tg.create_task(run(i, coro))\n+        return results\n+    except BaseExceptionGroup as exception_group:\n+        if len(exception_group.exceptions) == 1:\n+            raise exception_group.exceptions[0]\n+        raise",
      "comment": "Keeping an extra reference to tasks here will result in reference cycles in the traceback of any errors that will delay garbage collection if there's any errors in signals",
      "comment_id": 2061251178,
      "user": "graingert",
      "created_at": "2025-04-26T10:15:01Z",
      "url": "https://github.com/django/django/pull/19370#discussion_r2061251178"
    },
    {
      "repo": "django/django",
      "pr_number": 19370,
      "file_path": "django/dispatch/dispatcher.py",
      "line": 44,
      "side": "RIGHT",
      "diff_hunk": "@@ -22,6 +22,28 @@ def _make_id(target):\n NO_RECEIVERS = object()\n \n \n+async def _gather(*coros):\n+    if len(coros) == 0:\n+        return []\n+\n+    if len(coros) == 1:\n+        return [await coros[0]]\n+\n+    async def run(i, coro):\n+        results[i] = await coro\n+\n+    try:\n+        async with asyncio.TaskGroup() as tg:\n+            results = [None] * len(coros)\n+            for i, coro in enumerate(coros):\n+                tg.create_task(run(i, coro))\n+        return results\n+    except BaseExceptionGroup as exception_group:\n+        if len(exception_group.exceptions) == 1:\n+            raise exception_group.exceptions[0]\n+        raise",
      "comment": "To maintain backward compatibility, shouldn't the first exception be raised even when the exception group contains multiple exceptions?",
      "comment_id": 2089978989,
      "user": "hartungstenio",
      "created_at": "2025-05-15T01:02:35Z",
      "url": "https://github.com/django/django/pull/19370#discussion_r2089978989"
    },
    {
      "repo": "django/django",
      "pr_number": 19370,
      "file_path": "django/dispatch/dispatcher.py",
      "line": 44,
      "side": "RIGHT",
      "diff_hunk": "@@ -22,6 +22,28 @@ def _make_id(target):\n NO_RECEIVERS = object()\n \n \n+async def _gather(*coros):\n+    if len(coros) == 0:\n+        return []\n+\n+    if len(coros) == 1:\n+        return [await coros[0]]\n+\n+    async def run(i, coro):\n+        results[i] = await coro\n+\n+    try:\n+        async with asyncio.TaskGroup() as tg:\n+            results = [None] * len(coros)\n+            for i, coro in enumerate(coros):\n+                tg.create_task(run(i, coro))\n+        return results\n+    except BaseExceptionGroup as exception_group:\n+        if len(exception_group.exceptions) == 1:\n+            raise exception_group.exceptions[0]\n+        raise",
      "comment": "I'm half-inclined to think we could evolve this fallback away over the medium term...   but if I'm not expecting the exception group would be a weird one. \r\n\r\n> To maintain backward compatibility, shouldn't the first exception be raised even when the exception group contains multiple exceptions?\r\n\r\nFirst exception will cancel the other tasks, so I'm not sure how this comes up in real life. \ud83e\udd14 (I think the _unexpected_ raising of the group is better in this _unexpected_ case.)",
      "comment_id": 2502545819,
      "user": "carltongibson",
      "created_at": "2025-11-07T10:08:43Z",
      "url": "https://github.com/django/django/pull/19370#discussion_r2502545819"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/lazy.py",
      "line": 29,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,63 @@\n+from contextlib import contextmanager\n+\n+from asgiref.local import Local\n+\n+\n+def FETCH_ONE(instance, field, fetch_for_instances, **kwargs):\n+    fetch_for_instances((instance,))\n+\n+\n+def FETCH_PEERS(instance, field, fetch_for_instances, **kwargs):\n+    instances = [\n+        peer\n+        for weakref_peer in instance._state.peers\n+        if (peer := weakref_peer()) is not None\n+    ]\n+    if not instances:\n+        # Peers aren\u2019t tracked for QuerySets returning a single instance\n+        instances = (instance,)\n+\n+    fetch_for_instances(instances)\n+\n+\n+class LazyFieldAccess(Exception):\n+    \"\"\"Blocked lazy access of a model field.\"\"\"\n+\n+    pass\n+\n+\n+def RAISE(instance, field, fetch_for_instances, **kwargs):",
      "comment": "Any thoughts about including a `WARN` as well? It seems like the only remaining feature that doesn't make `django-seal` obsolete.",
      "comment_id": 1555162327,
      "user": "charettes",
      "created_at": "2024-04-08T02:31:54Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r1555162327"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/fields/related_descriptors.py",
      "line": 532,
      "side": "RIGHT",
      "diff_hunk": "@@ -509,16 +517,34 @@ def __get__(self, instance, cls=None):\n             if related_pk is None:\n                 rel_obj = None\n             else:\n-                filter_args = self.related.field.get_forward_related_filter(instance)\n-                try:\n-                    rel_obj = self.get_queryset(instance=instance).get(**filter_args)\n-                except self.related.related_model.DoesNotExist:\n-                    rel_obj = None\n+                peers = [\n+                    peer\n+                    for weakref_peer in instance._state.peers\n+                    if (peer := weakref_peer()) is not None and not self.is_cached(peer)\n+                ]\n+                if len(peers) > 1:\n+                    # If the instance was fetched with other instances then\n+                    # prefetch the field for all of them\n+                    qs = self.get_queryset()\n+                    prefetch_related_objects(\n+                        peers, Prefetch(self.related.get_accessor_name(), queryset=qs)\n+                    )\n+                    rel_obj = self.related.get_cached_value(instance)",
      "comment": "That seems wrong. Is this meant to go through the currently defined lazy mode as well?",
      "comment_id": 1555163802,
      "user": "charettes",
      "created_at": "2024-04-08T02:35:43Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r1555163802"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query_utils.py",
      "line": 222,
      "side": "RIGHT",
      "diff_hunk": "@@ -203,7 +204,22 @@ def __get__(self, instance, cls=None):\n                     raise AttributeError(\n                         \"Cannot read a generated field from an unsaved model.\"\n                     )\n-                instance.refresh_from_db(fields=[field_name])\n+\n+                def fetch_for_instances(instances):\n+                    if len(instances) == 1:\n+                        instances[0].refresh_from_db(fields=[field_name])\n+                    else:\n+                        attname = self.field.attname\n+                        value_by_pk = {\n+                            pk: value\n+                            for pk, value in self.field.model._base_manager.db_manager()\n+                            .filter(pk__in={i.pk for i in instances})\n+                            .values_list(\"pk\", attname)\n+                        }\n+                        for instance in instances:\n+                            setattr(instance, attname, value_by_pk[instance.pk])\n+\n+                get_lazy_mode()(instance, self.field, fetch_for_instances)",
      "comment": "I wonder if a better interface for lazy modes would be to simply accept `(fetcher: Fetcher, instance: Model)` where `Fetcher` would have two methods `fetch_one(instance: Model)` and `fetch_many(Iterable[Model])`.\r\n\r\nThat would avoid creating unnecessary closures on every attribute access and make lazy modes focused on retrieving peers and delegating to the object manipulating the instance.\r\n\r\nIn the case of `DeferredAttribute` that would mean adding these two methods\r\n\r\n```python\r\ndef fetch_one(self, instance):\r\n    instance.refresh_from_db(fields=[self.field.attname])\r\n\r\ndef fetch_many(self, instances):\r\n    attname = self.field.attname\r\n    value_by_pk = {\r\n        pk: value\r\n        for pk, value in self.field.model._base_manager.db_manager()\r\n        .filter(pk__in={i.pk for i in instances})\r\n        .values_list(\"pk\", attname)\r\n     }\r\n    for instance in instances:\r\n        setattr(instance, attname, value_by_pk[instance.pk])\r\n```\r\n\r\nAnd changing this call to `get_lazy_mode()(self, instance)`",
      "comment_id": 1555170810,
      "user": "charettes",
      "created_at": "2024-04-08T02:53:23Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r1555170810"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/lazy.py",
      "line": 1,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,63 @@\n+from contextlib import contextmanager",
      "comment": "Any thoughts about naming this module  `fetching` instead? It feels like _fetching_ ties it to `prefetch_related` and is not an already overloaded term within the code base compared the _lazy_.",
      "comment_id": 1555171742,
      "user": "charettes",
      "created_at": "2024-04-08T02:55:48Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r1555171742"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/fields/related_descriptors.py",
      "line": 266,
      "side": "RIGHT",
      "diff_hunk": "@@ -251,13 +252,20 @@ def __get__(self, instance, cls=None):\n             else:\n                 rel_obj = None\n             if rel_obj is None and has_value:\n-                rel_obj = self.get_object(instance)\n-                remote_field = self.field.remote_field\n-                # If this is a one-to-one relation, set the reverse accessor\n-                # cache on the related object to the current instance to avoid\n-                # an extra SQL query if it's accessed later on.\n-                if not remote_field.multiple:\n-                    remote_field.set_cached_value(rel_obj, instance)\n+\n+                def fetch_for_instances(instances):\n+                    if len(instances) == 1:\n+                        instance = instances[0]\n+                        setattr(instance, self.field.name, self.get_object(instance))\n+                    else:\n+                        prefetch_related_objects(\n+                            instances,\n+                            Prefetch(self.field.name, queryset=self.get_queryset()),\n+                        )\n+\n+                get_lazy_mode()(instance, self.field, fetch_for_instances)",
      "comment": "Ditto about the unnecessary closure creation and the `Fetcher` protocol idea discussed below.",
      "comment_id": 1555173108,
      "user": "charettes",
      "created_at": "2024-04-08T02:59:01Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r1555173108"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/fields/related_descriptors.py",
      "line": 532,
      "side": "RIGHT",
      "diff_hunk": "@@ -509,16 +517,34 @@ def __get__(self, instance, cls=None):\n             if related_pk is None:\n                 rel_obj = None\n             else:\n-                filter_args = self.related.field.get_forward_related_filter(instance)\n-                try:\n-                    rel_obj = self.get_queryset(instance=instance).get(**filter_args)\n-                except self.related.related_model.DoesNotExist:\n-                    rel_obj = None\n+                peers = [\n+                    peer\n+                    for weakref_peer in instance._state.peers\n+                    if (peer := weakref_peer()) is not None and not self.is_cached(peer)\n+                ]\n+                if len(peers) > 1:\n+                    # If the instance was fetched with other instances then\n+                    # prefetch the field for all of them\n+                    qs = self.get_queryset()\n+                    prefetch_related_objects(\n+                        peers, Prefetch(self.related.get_accessor_name(), queryset=qs)\n+                    )\n+                    rel_obj = self.related.get_cached_value(instance)",
      "comment": "Yes, I just didn\u2019t update this yet from Andreas\u2019 original PR copying from django-auto-prefetch, yet.",
      "comment_id": 1575930309,
      "user": "adamchainz",
      "created_at": "2024-04-23T09:17:49Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r1575930309"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/lazy.py",
      "line": 1,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,63 @@\n+from contextlib import contextmanager",
      "comment": "I tried describing the feature name a few different ways before settling on \u201clazy-loading modes\u201d as in the release notes. But \u201clazy-fetching modes\u201d could also be a candidate.\r\n\r\nHappy to rename the module, but I\u2019d prefer to keep it aligned with the name of the feature.\r\n\r\nI agree that \u201clazy\u201d is used in many ways - translations, \u201clazy relationships\u201d for passing a string to Foreign Keys, `SimpleLazyObject`... But it does also accurately describes how the data is lazily fetched by a field on access. Naming things is hard.",
      "comment_id": 1575936312,
      "user": "adamchainz",
      "created_at": "2024-04-23T09:22:04Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r1575936312"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/lazy.py",
      "line": 29,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,63 @@\n+from contextlib import contextmanager\n+\n+from asgiref.local import Local\n+\n+\n+def FETCH_ONE(instance, field, fetch_for_instances, **kwargs):\n+    fetch_for_instances((instance,))\n+\n+\n+def FETCH_PEERS(instance, field, fetch_for_instances, **kwargs):\n+    instances = [\n+        peer\n+        for weakref_peer in instance._state.peers\n+        if (peer := weakref_peer()) is not None\n+    ]\n+    if not instances:\n+        # Peers aren\u2019t tracked for QuerySets returning a single instance\n+        instances = (instance,)\n+\n+    fetch_for_instances(instances)\n+\n+\n+class LazyFieldAccess(Exception):\n+    \"\"\"Blocked lazy access of a model field.\"\"\"\n+\n+    pass\n+\n+\n+def RAISE(instance, field, fetch_for_instances, **kwargs):",
      "comment": "I was thinking of that, but then maybe users would want \u201cfetch peers\u201d + warnings, or log messages directly rather than warnings. Maybe something we can instead document the pattern for? Like:\r\n\r\n```python\r\ndef warn_then_fetch_peers(...):\r\n    logger.warning(\"fetching peers!\")\r\n    return FETCH_PEERS(...)\r\n\r\nset_default_lazy_mode(warn_then_fetch_peers)\r\n```",
      "comment_id": 1575939529,
      "user": "adamchainz",
      "created_at": "2024-04-23T09:24:29Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r1575939529"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query_utils.py",
      "line": 222,
      "side": "RIGHT",
      "diff_hunk": "@@ -203,7 +204,22 @@ def __get__(self, instance, cls=None):\n                     raise AttributeError(\n                         \"Cannot read a generated field from an unsaved model.\"\n                     )\n-                instance.refresh_from_db(fields=[field_name])\n+\n+                def fetch_for_instances(instances):\n+                    if len(instances) == 1:\n+                        instances[0].refresh_from_db(fields=[field_name])\n+                    else:\n+                        attname = self.field.attname\n+                        value_by_pk = {\n+                            pk: value\n+                            for pk, value in self.field.model._base_manager.db_manager()\n+                            .filter(pk__in={i.pk for i in instances})\n+                            .values_list(\"pk\", attname)\n+                        }\n+                        for instance in instances:\n+                            setattr(instance, attname, value_by_pk[instance.pk])\n+\n+                get_lazy_mode()(instance, self.field, fetch_for_instances)",
      "comment": "Yeah, that would be good. I originally tried an interface but it was hard for some reason. At least, it was confusing whether methods should live on the descriptors or fields. I will give it another try, making the descriptors the \u201cfetchers\u201d.\r\n\r\nFetch one and many have separate implementations here only for backwards compatibility. Since its introduction, `refresh_from_db()` [has been documented](https://docs.djangoproject.com/en/5.0/ref/models/instances/#django.db.models.Model.refresh_from_db) as the backing method for deferred attributes:\r\n\r\n> Note that when deferred fields are accessed, the loading of the deferred field\u2019s value happens through this method.\r\n\r\nThere are more backwards compatibility concerns for related descriptors, but there won\u2019t be for `GenericForeignKey` (TODO), so maybe we should only have one method.",
      "comment_id": 1575951348,
      "user": "adamchainz",
      "created_at": "2024-04-23T09:33:07Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r1575951348"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query_utils.py",
      "line": 222,
      "side": "RIGHT",
      "diff_hunk": "@@ -203,7 +204,22 @@ def __get__(self, instance, cls=None):\n                     raise AttributeError(\n                         \"Cannot read a generated field from an unsaved model.\"\n                     )\n-                instance.refresh_from_db(fields=[field_name])\n+\n+                def fetch_for_instances(instances):\n+                    if len(instances) == 1:\n+                        instances[0].refresh_from_db(fields=[field_name])\n+                    else:\n+                        attname = self.field.attname\n+                        value_by_pk = {\n+                            pk: value\n+                            for pk, value in self.field.model._base_manager.db_manager()\n+                            .filter(pk__in={i.pk for i in instances})\n+                            .values_list(\"pk\", attname)\n+                        }\n+                        for instance in instances:\n+                            setattr(instance, attname, value_by_pk[instance.pk])\n+\n+                get_lazy_mode()(instance, self.field, fetch_for_instances)",
      "comment": "I\u2019ve just updated to a new mode with a `Fetcher` protocol using only a single `fetch()` method. The protocol also comes with the requirement `.field` resolves to the target field, for the `RAISE` lazy mode, but that is already the case for `DeferredAttribute`, `ReverseOneToOneDescriptor`, and `ForwardManyToOneDescriptor`. We can add `field` to `GenericForeignKey`, although it will be a little weird since the field is its own descriptor.",
      "comment_id": 1579632361,
      "user": "adamchainz",
      "created_at": "2024-04-25T14:54:13Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r1579632361"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/lazy.py",
      "line": 1,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,63 @@\n+from contextlib import contextmanager",
      "comment": "What do you think about putting this module in `db.models.fields` ? The fetching code here is all called from, and calls back to, fields. It seems logical to me...",
      "comment_id": 1579648680,
      "user": "adamchainz",
      "created_at": "2024-04-25T15:00:27Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r1579648680"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/lazy.py",
      "line": 1,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,63 @@\n+from contextlib import contextmanager",
      "comment": "I\u2019ve tried that for now, and updated the release notes to avoid the word lazy. I went for \u201con-demand fetching\u201d, which makes sense to me. WDYT?",
      "comment_id": 1579823787,
      "user": "adamchainz",
      "created_at": "2024-04-25T16:47:19Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r1579823787"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/lazy.py",
      "line": 29,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,63 @@\n+from contextlib import contextmanager\n+\n+from asgiref.local import Local\n+\n+\n+def FETCH_ONE(instance, field, fetch_for_instances, **kwargs):\n+    fetch_for_instances((instance,))\n+\n+\n+def FETCH_PEERS(instance, field, fetch_for_instances, **kwargs):\n+    instances = [\n+        peer\n+        for weakref_peer in instance._state.peers\n+        if (peer := weakref_peer()) is not None\n+    ]\n+    if not instances:\n+        # Peers aren\u2019t tracked for QuerySets returning a single instance\n+        instances = (instance,)\n+\n+    fetch_for_instances(instances)\n+\n+\n+class LazyFieldAccess(Exception):\n+    \"\"\"Blocked lazy access of a model field.\"\"\"\n+\n+    pass\n+\n+\n+def RAISE(instance, field, fetch_for_instances, **kwargs):",
      "comment": "I see, that makes sense to separate non-terminal _surfacing_ of _leaks_ from the actual fetching implementation. I wonder if there is anything that we could provide to avoid such boiler plate though.",
      "comment_id": 1590395782,
      "user": "charettes",
      "created_at": "2024-05-05T19:34:29Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r1590395782"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query_utils.py",
      "line": 222,
      "side": "RIGHT",
      "diff_hunk": "@@ -203,7 +204,22 @@ def __get__(self, instance, cls=None):\n                     raise AttributeError(\n                         \"Cannot read a generated field from an unsaved model.\"\n                     )\n-                instance.refresh_from_db(fields=[field_name])\n+\n+                def fetch_for_instances(instances):\n+                    if len(instances) == 1:\n+                        instances[0].refresh_from_db(fields=[field_name])\n+                    else:\n+                        attname = self.field.attname\n+                        value_by_pk = {\n+                            pk: value\n+                            for pk, value in self.field.model._base_manager.db_manager()\n+                            .filter(pk__in={i.pk for i in instances})\n+                            .values_list(\"pk\", attname)\n+                        }\n+                        for instance in instances:\n+                            setattr(instance, attname, value_by_pk[instance.pk])\n+\n+                get_lazy_mode()(instance, self.field, fetch_for_instances)",
      "comment": "That looks great! Any reason for going with a single `fetch` method over two distinct `fetch_(one|many)`?\r\n\r\nI don't see it as a hard blocker but the fact that all `fetch` implementations have an `if` branch that is unnecessary from the caller's perspective (the mode handler knows which method it should call) seemed odd to me.",
      "comment_id": 1590396387,
      "user": "charettes",
      "created_at": "2024-05-05T19:38:03Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r1590396387"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/lazy.py",
      "line": 1,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,63 @@\n+from contextlib import contextmanager",
      "comment": "I think your proposed structure and wording under `fields.fetching` makes a lot of sense. Thank you for doing that!",
      "comment_id": 1590396565,
      "user": "charettes",
      "created_at": "2024-05-05T19:39:08Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r1590396565"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/fields/fetching.py",
      "line": 29,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,64 @@\n+from contextlib import contextmanager\n+\n+from asgiref.local import Local\n+\n+\n+def FETCH_ONE(fetcher, instance, **kwargs):\n+    fetcher.fetch((instance,))\n+\n+\n+def FETCH_PEERS(fetcher, instance, **kwargs):\n+    if instance._state.peers:\n+        instances = [\n+            peer\n+            for weakref_peer in instance._state.peers\n+            if (peer := weakref_peer()) is not None\n+        ]\n+    else:\n+        # Peers aren\u2019t tracked for QuerySets returning a single instance\n+        instances = (instance,)\n+\n+    fetcher.fetch(instances)\n+\n+\n+class LazyFieldAccess(Exception):\n+    \"\"\"Blocked lazy access of a model field.\"\"\"\n+\n+    pass\n+\n+\n+def RAISE(fetcher, instance, **kwargs):",
      "comment": "Should we get rid of the `**kwargs`? Were they meant to be used for something?",
      "comment_id": 1590501433,
      "user": "charettes",
      "created_at": "2024-05-06T03:25:11Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r1590501433"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/fields/fetching.py",
      "line": 29,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,64 @@\n+from contextlib import contextmanager\n+\n+from asgiref.local import Local\n+\n+\n+def FETCH_ONE(fetcher, instance, **kwargs):\n+    fetcher.fetch((instance,))\n+\n+\n+def FETCH_PEERS(fetcher, instance, **kwargs):\n+    if instance._state.peers:\n+        instances = [\n+            peer\n+            for weakref_peer in instance._state.peers\n+            if (peer := weakref_peer()) is not None\n+        ]\n+    else:\n+        # Peers aren\u2019t tracked for QuerySets returning a single instance\n+        instances = (instance,)\n+\n+    fetcher.fetch(instances)\n+\n+\n+class LazyFieldAccess(Exception):\n+    \"\"\"Blocked lazy access of a model field.\"\"\"\n+\n+    pass\n+\n+\n+def RAISE(fetcher, instance, **kwargs):",
      "comment": "I left them to leave space for future extensions, if users wrap the calls. But I guess we don\u2019t have that for the callables in `django.db.models.deletion`, so we can drop them.",
      "comment_id": 1592071411,
      "user": "adamchainz",
      "created_at": "2024-05-07T08:56:51Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r1592071411"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/fields/fetching.py",
      "line": 29,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,64 @@\n+from contextlib import contextmanager\n+\n+from asgiref.local import Local\n+\n+\n+def FETCH_ONE(fetcher, instance, **kwargs):\n+    fetcher.fetch((instance,))\n+\n+\n+def FETCH_PEERS(fetcher, instance, **kwargs):\n+    if instance._state.peers:\n+        instances = [\n+            peer\n+            for weakref_peer in instance._state.peers\n+            if (peer := weakref_peer()) is not None\n+        ]\n+    else:\n+        # Peers aren\u2019t tracked for QuerySets returning a single instance\n+        instances = (instance,)\n+\n+    fetcher.fetch(instances)\n+\n+\n+class LazyFieldAccess(Exception):\n+    \"\"\"Blocked lazy access of a model field.\"\"\"\n+\n+    pass\n+\n+\n+def RAISE(fetcher, instance, **kwargs):",
      "comment": "I'm not that familiar with the back compat policy but the kwargs feels unnecessary\r\nif I had a custom fetch mode I would understand if a feature release broke it due to signature change and it would break loud and fast\r\n",
      "comment_id": 1626056734,
      "user": "tolomea",
      "created_at": "2024-06-04T13:50:33Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r1626056734"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/fields/fetching.py",
      "line": 47,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,64 @@\n+from contextlib import contextmanager\n+\n+from asgiref.local import Local\n+\n+\n+def FETCH_ONE(fetcher, instance, **kwargs):\n+    fetcher.fetch((instance,))\n+\n+\n+def FETCH_PEERS(fetcher, instance, **kwargs):\n+    if instance._state.peers:\n+        instances = [\n+            peer\n+            for weakref_peer in instance._state.peers\n+            if (peer := weakref_peer()) is not None\n+        ]\n+    else:\n+        # Peers aren\u2019t tracked for QuerySets returning a single instance\n+        instances = (instance,)\n+\n+    fetcher.fetch(instances)\n+\n+\n+class LazyFieldAccess(Exception):\n+    \"\"\"Blocked lazy access of a model field.\"\"\"\n+\n+    pass\n+\n+\n+def RAISE(fetcher, instance, **kwargs):\n+    klass = instance.__class__.__qualname__\n+    field_name = fetcher.field.name\n+    raise LazyFieldAccess(f\"Lazy loading of {klass}.{field_name} blocked.\")\n+\n+\n+_default = FETCH_ONE\n+_local = Local()\n+\n+\n+def set_default_fetching_mode(mode):\n+    global _default\n+    if not callable(mode):  # TODO: verify signature\n+        raise TypeError(\"mode must be callable.\")\n+    _default = mode\n+\n+\n+@contextmanager\n+def lazy_mode(mode):",
      "comment": "Yes, but I went with `fetch_mode` to keep it shorter.",
      "comment_id": 1965336672,
      "user": "adamchainz",
      "created_at": "2025-02-21T11:36:54Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r1965336672"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query_utils.py",
      "line": 222,
      "side": "RIGHT",
      "diff_hunk": "@@ -203,7 +204,22 @@ def __get__(self, instance, cls=None):\n                     raise AttributeError(\n                         \"Cannot read a generated field from an unsaved model.\"\n                     )\n-                instance.refresh_from_db(fields=[field_name])\n+\n+                def fetch_for_instances(instances):\n+                    if len(instances) == 1:\n+                        instances[0].refresh_from_db(fields=[field_name])\n+                    else:\n+                        attname = self.field.attname\n+                        value_by_pk = {\n+                            pk: value\n+                            for pk, value in self.field.model._base_manager.db_manager()\n+                            .filter(pk__in={i.pk for i in instances})\n+                            .values_list(\"pk\", attname)\n+                        }\n+                        for instance in instances:\n+                            setattr(instance, attname, value_by_pk[instance.pk])\n+\n+                get_lazy_mode()(instance, self.field, fetch_for_instances)",
      "comment": "Now using `fetch_one` and `fetch_many`! No need for a branch.",
      "comment_id": 1965362281,
      "user": "adamchainz",
      "created_at": "2025-02-21T11:57:09Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r1965362281"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/fields/fetching.py",
      "line": 29,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,64 @@\n+from contextlib import contextmanager\n+\n+from asgiref.local import Local\n+\n+\n+def FETCH_ONE(fetcher, instance, **kwargs):\n+    fetcher.fetch((instance,))\n+\n+\n+def FETCH_PEERS(fetcher, instance, **kwargs):\n+    if instance._state.peers:\n+        instances = [\n+            peer\n+            for weakref_peer in instance._state.peers\n+            if (peer := weakref_peer()) is not None\n+        ]\n+    else:\n+        # Peers aren\u2019t tracked for QuerySets returning a single instance\n+        instances = (instance,)\n+\n+    fetcher.fetch(instances)\n+\n+\n+class LazyFieldAccess(Exception):\n+    \"\"\"Blocked lazy access of a model field.\"\"\"\n+\n+    pass\n+\n+\n+def RAISE(fetcher, instance, **kwargs):",
      "comment": "I got rid of `**kwargs`. If we document \u201cmake your own mode\u201d, we can encourage using them there.",
      "comment_id": 1965363855,
      "user": "adamchainz",
      "created_at": "2025-02-21T11:58:08Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r1965363855"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "tests/model_fields/test_fetch_mode.py",
      "line": 59,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,59 @@\n+from django.db.models import (\n+    FETCH_ONE,\n+    FETCH_PEERS,\n+    RAISE,\n+    fetch_mode,\n+    set_default_fetch_mode,\n+)\n+from django.db.models.fields.fetch_modes import get_fetch_mode\n+from django.test import SimpleTestCase\n+\n+\n+class FetchModeTests(SimpleTestCase):\n+    def test_set_default_fetch_mode(self):\n+        try:\n+            set_default_fetch_mode(RAISE)\n+            self.assertIs(get_fetch_mode(), RAISE)\n+        finally:\n+            set_default_fetch_mode(FETCH_ONE)\n+\n+    def test_set_default_non_callable(self):\n+        msg = \"'RAISE' is not a callable object\"\n+        with self.assertRaisesMessage(TypeError, msg):\n+            set_default_fetch_mode(\"RAISE\")\n+\n+    def test_set_default_incorrect_signature(self):\n+        msg = \"mode must have signature (fetcher, instance).\"\n+        with self.assertRaisesMessage(TypeError, msg):\n+            set_default_fetch_mode(lambda x: x)\n+\n+    def test_fetch_mode_context_manager(self):\n+        self.assertIs(get_fetch_mode(), FETCH_ONE)\n+\n+        with fetch_mode(RAISE):\n+            self.assertIs(get_fetch_mode(), RAISE)\n+\n+        self.assertIs(get_fetch_mode(), FETCH_ONE)\n+\n+    def test_fetch_mode_nested(self):\n+        with fetch_mode(RAISE):\n+            self.assertIs(get_fetch_mode(), RAISE)\n+            with fetch_mode(FETCH_PEERS):\n+                self.assertIs(get_fetch_mode(), FETCH_PEERS)\n+            self.assertIs(get_fetch_mode(), RAISE)\n+\n+    @fetch_mode(RAISE)\n+    def test_fetch_mode_decorator(self):\n+        self.assertIs(get_fetch_mode(), RAISE)\n+\n+    def test_fetch_mode_non_callable(self):\n+        msg = \"'RAISE' is not a callable object\"\n+        with self.assertRaisesMessage(TypeError, msg):\n+            with fetch_mode(\"RAISE\"):\n+                pass\n+\n+    def test_bad_argument(self):\n+        msg = \"mode must have signature (fetcher, instance).\"\n+        with self.assertRaisesMessage(TypeError, msg):\n+            with fetch_mode(lambda x: x):\n+                pass",
      "comment": "(this is not the correct file/line for this comment)\r\n\r\ndoes fetch modes work with manytomany fields? if so, are there any tests?  if not, I think we should document that as a limitation of fetch modes. ignore this comment if this is already covered somewhere that I missed.",
      "comment_id": 1983136003,
      "user": "pelme",
      "created_at": "2025-03-06T10:51:59Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r1983136003"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/fields/__init__.py",
      "line": 959,
      "side": "RIGHT",
      "diff_hunk": "@@ -954,7 +954,9 @@ def contribute_to_class(self, cls, name, private_only=False):\n         self.model = cls\n         cls._meta.add_field(self, private=private_only)\n         if self.column:\n-            setattr(cls, self.attname, self.descriptor_class(self))\n+            descriptor = self.descriptor_class(self)\n+            setattr(cls, self.attname, descriptor)\n+            descriptor.__set_name__(cls, self.name)",
      "comment": "Could we potentially avoid calling these magic methods directly? Why can't descriptors rely on `self.field.name` instead if they require it?",
      "comment_id": 1983704129,
      "user": "charettes",
      "created_at": "2025-03-06T16:38:10Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r1983704129"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query.py",
      "line": 151,
      "side": "RIGHT",
      "diff_hunk": "@@ -142,6 +145,16 @@ def __iter__(self):\n                 else:\n                     setattr(obj, field.name, rel_obj)\n \n+            # Create peers, but only for >1 result, to save memory.\n+            if first_obj is None:\n+                first_obj = obj\n+            else:",
      "comment": "Could we also avoid storing peers entirely when this feature is not enabled? If memory is a concern for `>1` I assume it might also be the case for the existing Django projects where this feature is not necessary?",
      "comment_id": 1983718536,
      "user": "charettes",
      "created_at": "2025-03-06T16:47:20Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r1983718536"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/fields/fetch_modes.py",
      "line": 59,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,63 @@\n+from contextlib import contextmanager\n+\n+from asgiref.local import Local\n+\n+from django.core.exceptions import FieldFetchBlocked\n+from django.utils.inspect import get_func_args\n+\n+\n+def FETCH_ONE(fetcher, instance):\n+    fetcher.fetch_one(instance)\n+\n+\n+def FETCH_PEERS(fetcher, instance):\n+    if instance._state.peers:\n+        instances = [\n+            peer\n+            for weakref_peer in instance._state.peers\n+            if (peer := weakref_peer()) is not None\n+        ]\n+        fetcher.fetch_many(instances)\n+    else:\n+        # Peers aren\u2019t tracked for QuerySets returning a single instance\n+        fetcher.fetch_one(instance)\n+\n+\n+def RAISE(fetcher, instance):\n+    klass = instance.__class__.__qualname__\n+    field_name = fetcher.name\n+    raise FieldFetchBlocked(f\"Fetching of {klass}.{field_name} blocked.\")\n+\n+\n+_default = FETCH_ONE\n+_local = Local()\n+\n+\n+def validate_mode(mode):\n+    if get_func_args(mode) != [\"fetcher\", \"instance\"]:\n+        raise TypeError(\"mode must have signature (fetcher, instance).\")\n+\n+\n+def set_default_fetch_mode(mode):\n+    global _default\n+    validate_mode(mode)\n+    _default = mode\n+\n+\n+@contextmanager\n+def fetch_mode(mode):\n+    validate_mode(mode)\n+\n+    orig = getattr(_local, \"mode\", None)\n+    _local.mode = mode\n+    try:\n+        yield\n+    finally:\n+        if orig is None:\n+            del _local.mode\n+        else:\n+            _local.mode = orig",
      "comment": "I've brought it before but the fact the fetch mode is a global variable as opposed to attributed to `QuerySet` instances is likely going to make ergonomics quite weird when wanting to mix modes for distinct querysets or enable it only in some cases.\r\n\r\nI believe the way database routing is implemented for example where both `QuerySet.using` and routers can be used to determine what database should be used is battle proven approach. If you then want to rely on global state decorators to advise your router decision you can but Django has lived quite well without having core support for that in database routers.\r\n\r\nThe lack of queryset method / hook prevents usage of this feature in declarative scenarios such as admins, CBV, or anything that takes a `QuerySet` as a parameter (form, serializer).\r\n\r\ne.g.\r\n\r\n```python\r\nclass BookView(ListView):\r\n   queryset = Book.objects.fetch_mode(FETCH_PEERS)\r\n\r\n\r\nclass AuthorManager(Manager):\r\n   def get_queryset(self):\r\n       return super().get_queryset().fetch_mode(RAISE)\r\n\r\nclass Author(Model):\r\n    objects = AuthorManager()\r\n```",
      "comment_id": 1983745446,
      "user": "charettes",
      "created_at": "2025-03-06T17:05:03Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r1983745446"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query.py",
      "line": 151,
      "side": "RIGHT",
      "diff_hunk": "@@ -142,6 +145,16 @@ def __iter__(self):\n                 else:\n                     setattr(obj, field.name, rel_obj)\n \n+            # Create peers, but only for >1 result, to save memory.\n+            if first_obj is None:\n+                first_obj = obj\n+            else:",
      "comment": "If the fetching mode was bound to the current queryset instance this whole logic could be delegated to the fetcher itself. That would allow the `FetchPeer` logic to handle the `list[weakref]` creation only when needed while avoiding its creation when dealing with other fetching modes.",
      "comment_id": 1983758471,
      "user": "charettes",
      "created_at": "2025-03-06T17:14:00Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r1983758471"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query.py",
      "line": 151,
      "side": "RIGHT",
      "diff_hunk": "@@ -142,6 +145,16 @@ def __iter__(self):\n                 else:\n                     setattr(obj, field.name, rel_obj)\n \n+            # Create peers, but only for >1 result, to save memory.\n+            if first_obj is None:\n+                first_obj = obj\n+            else:",
      "comment": "This is done.\r\n\r\nI also added peers during `RawModelIterable`, making fetch modes work for instances retrieved through raw queries.",
      "comment_id": 2021099404,
      "user": "adamchainz",
      "created_at": "2025-03-31T13:57:09Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2021099404"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "tests/model_fields/test_fetch_mode.py",
      "line": 59,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,59 @@\n+from django.db.models import (\n+    FETCH_ONE,\n+    FETCH_PEERS,\n+    RAISE,\n+    fetch_mode,\n+    set_default_fetch_mode,\n+)\n+from django.db.models.fields.fetch_modes import get_fetch_mode\n+from django.test import SimpleTestCase\n+\n+\n+class FetchModeTests(SimpleTestCase):\n+    def test_set_default_fetch_mode(self):\n+        try:\n+            set_default_fetch_mode(RAISE)\n+            self.assertIs(get_fetch_mode(), RAISE)\n+        finally:\n+            set_default_fetch_mode(FETCH_ONE)\n+\n+    def test_set_default_non_callable(self):\n+        msg = \"'RAISE' is not a callable object\"\n+        with self.assertRaisesMessage(TypeError, msg):\n+            set_default_fetch_mode(\"RAISE\")\n+\n+    def test_set_default_incorrect_signature(self):\n+        msg = \"mode must have signature (fetcher, instance).\"\n+        with self.assertRaisesMessage(TypeError, msg):\n+            set_default_fetch_mode(lambda x: x)\n+\n+    def test_fetch_mode_context_manager(self):\n+        self.assertIs(get_fetch_mode(), FETCH_ONE)\n+\n+        with fetch_mode(RAISE):\n+            self.assertIs(get_fetch_mode(), RAISE)\n+\n+        self.assertIs(get_fetch_mode(), FETCH_ONE)\n+\n+    def test_fetch_mode_nested(self):\n+        with fetch_mode(RAISE):\n+            self.assertIs(get_fetch_mode(), RAISE)\n+            with fetch_mode(FETCH_PEERS):\n+                self.assertIs(get_fetch_mode(), FETCH_PEERS)\n+            self.assertIs(get_fetch_mode(), RAISE)\n+\n+    @fetch_mode(RAISE)\n+    def test_fetch_mode_decorator(self):\n+        self.assertIs(get_fetch_mode(), RAISE)\n+\n+    def test_fetch_mode_non_callable(self):\n+        msg = \"'RAISE' is not a callable object\"\n+        with self.assertRaisesMessage(TypeError, msg):\n+            with fetch_mode(\"RAISE\"):\n+                pass\n+\n+    def test_bad_argument(self):\n+        msg = \"mode must have signature (fetcher, instance).\"\n+        with self.assertRaisesMessage(TypeError, msg):\n+            with fetch_mode(lambda x: x):\n+                pass",
      "comment": "They don't work with related managers from `ManyToManyField` or the reverse side of a `ForeignKey`. Some support could potentially be added later, at least for `.all()`, analogously with how `prefetch_related` can prefetch such querysets. The related manager classes would need to become \"fetcher\" classes and call the fetch mode like fields do, saving results back in the instances\u2019 `_prefetched_objects_cache` attrs.\r\n\r\nI think it's clear enough now that in the new fetch mode docs that we list what *is* supported, no need to explicitly call out related managers as unsupported.",
      "comment_id": 2021114339,
      "user": "adamchainz",
      "created_at": "2025-03-31T14:04:01Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2021114339"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/fields/__init__.py",
      "line": 959,
      "side": "RIGHT",
      "diff_hunk": "@@ -954,7 +954,9 @@ def contribute_to_class(self, cls, name, private_only=False):\n         self.model = cls\n         cls._meta.add_field(self, private=private_only)\n         if self.column:\n-            setattr(cls, self.attname, self.descriptor_class(self))\n+            descriptor = self.descriptor_class(self)\n+            setattr(cls, self.attname, descriptor)\n+            descriptor.__set_name__(cls, self.name)",
      "comment": "The aim here was to have the \"fetcher protocol\" include a consistent way to see the related field's name available, so the `RAISE` mode can rely on it.\r\n\r\nDescriptors did not consistently point at fields. At least `GenericForeignKey` is, oddly, its own descriptor. Adding a recursive `field` attribute there felt wrong, but it seemed to make sense to call [`__set_name__`](https://docs.python.org/3/reference/datamodel.html#object.__set_name__) here because it's the same point that Python would do so if the descriptor were placed in the class definition.\r\n\r\nIn retrospect, it's probably less invasive and more sensible to make `field` consistently available, and fix GFK to have a separate descriptor class. I'll give that a try now.",
      "comment_id": 2042249701,
      "user": "adamchainz",
      "created_at": "2025-04-14T14:23:12Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2042249701"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/fields/related_descriptors.py",
      "line": 138,
      "side": "RIGHT",
      "diff_hunk": "@@ -133,6 +134,9 @@ class Child(Model):\n     def __init__(self, field_with_rel):\n         self.field = field_with_rel\n \n+    def __set_name__(self, owner, name):\n+        self.name = name\n+",
      "comment": "Per above - trying with field.\r\n\r\nhttps://github.com/django/django/pull/17554#discussion_r2042249701",
      "comment_id": 2042251992,
      "user": "adamchainz",
      "created_at": "2025-04-14T14:24:22Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2042251992"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/fields/__init__.py",
      "line": 959,
      "side": "RIGHT",
      "diff_hunk": "@@ -954,7 +954,9 @@ def contribute_to_class(self, cls, name, private_only=False):\n         self.model = cls\n         cls._meta.add_field(self, private=private_only)\n         if self.column:\n-            setattr(cls, self.attname, self.descriptor_class(self))\n+            descriptor = self.descriptor_class(self)\n+            setattr(cls, self.attname, descriptor)\n+            descriptor.__set_name__(cls, self.name)",
      "comment": "Okay, I just pushed a fix. It required a new commit that splits a descriptor class from `GenericForeignKey`, which I also put into a new PR at #19381. Another minor hack it required is a new property in `ReverseOneToOneDescriptor` for `field`, with a docstring explaining the ducktyping.",
      "comment_id": 2042353629,
      "user": "adamchainz",
      "created_at": "2025-04-14T15:08:02Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2042353629"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query_utils.py",
      "line": 293,
      "side": "RIGHT",
      "diff_hunk": "@@ -241,6 +242,20 @@ def _check_parent_chain(self, instance):\n             return getattr(instance, link_field.attname)\n         return None\n \n+    def fetch_one(self, instance):\n+        instance.refresh_from_db(fields=[self.field.attname])\n+\n+    def fetch_many(self, instances):\n+        attname = self.field.attname\n+        value_by_pk = {\n+            pk: value\n+            for pk, value in self.field.model._base_manager.db_manager()\n+            .filter(pk__in={i.pk for i in instances})",
      "comment": "I wonder if the set comprehension here could have some performance impact for a large number of `instances`. I'm not sure what else could be done instead though.",
      "comment_id": 2062580287,
      "user": "g-nie",
      "created_at": "2025-04-27T09:53:06Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2062580287"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query_utils.py",
      "line": 293,
      "side": "RIGHT",
      "diff_hunk": "@@ -241,6 +242,20 @@ def _check_parent_chain(self, instance):\n             return getattr(instance, link_field.attname)\n         return None\n \n+    def fetch_one(self, instance):\n+        instance.refresh_from_db(fields=[self.field.attname])\n+\n+    def fetch_many(self, instances):\n+        attname = self.field.attname\n+        value_by_pk = {\n+            pk: value\n+            for pk, value in self.field.model._base_manager.db_manager()\n+            .filter(pk__in={i.pk for i in instances})",
      "comment": "It shouldn't take a meaningful amount of time. Yes, it's for N instances, but gathering their in-memory PKs will be way faster than the actual query.",
      "comment_id": 2067519711,
      "user": "adamchainz",
      "created_at": "2025-04-29T22:36:44Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2067519711"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/contrib/contenttypes/fields.py",
      "line": 165,
      "side": "RIGHT",
      "diff_hunk": "@@ -157,11 +156,19 @@ def get_content_type(self, obj=None, id=None, using=None, model=None):\n             # This should never happen. I love comments like this, don't you?\n             raise Exception(\"Impossible arguments to GFK.get_content_type!\")\n \n+\n+class GenericForeignKeyDescriptor:",
      "comment": "Just to make sure I understand the rationale behind this change.\r\n\r\nThe same thing could be achieved by adding a `field` property on `GenericForeignKey` that returns `self` but a distinct class is deemed preferable as it makes `GenericForeignKey` follow the pattern used by other related fields.\r\n\r\nIt might be worth _explaining_ the rationale behind these changes in the commit message and the benefits it provides for the followed commits in the patch set for future reference.",
      "comment_id": 2191302268,
      "user": "charettes",
      "created_at": "2025-07-08T01:37:18Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2191302268"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/contrib/contenttypes/fields.py",
      "line": 281,
      "side": "RIGHT",
      "diff_hunk": "@@ -257,7 +267,10 @@ def __get__(self, instance, cls=None):\n             except ObjectDoesNotExist:\n                 pass\n         self.field.set_cached_value(instance, rel_obj)\n-        return rel_obj\n+\n+    def fetch_many(self, instances):\n+        missing_instances = [i for i in instances if not self.field.is_cached(i)]",
      "comment": "Minor but it might be worth using a local reference for `self.field.is_cached` to avoid two attribute lookups per instance in a potentially large loop\r\n\r\n```python\r\nis_cached = self.field.is_cache\r\nmissing_instances = [i for i in instances if not is_cached(i)]\r\n```",
      "comment_id": 2191308766,
      "user": "charettes",
      "created_at": "2025-07-08T01:44:59Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2191308766"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/base.py",
      "line": 494,
      "side": "RIGHT",
      "diff_hunk": "@@ -473,6 +482,14 @@ class ModelState:\n     # on the actual save.\n     adding = True\n     fields_cache = ModelStateFieldsCacheDescriptor()\n+    fetch_mode = ModelStateFetchModeDescriptor()\n+    peers = ()\n+\n+    def __getstate__(self):\n+        state = self.__dict__.copy()\n+        state.pop(\"fetch_mode\", None)\n+        state.pop(\"peers\", None)",
      "comment": "Just to make sure I understand, does this mean that a pickled model instances, and by definition a queryset of such objects, looses its fetch mode affinity? For example would that mean that caching a queryset though pickling and then unpickling it would not preserve the specified mode (e.g. raising errors on attributes that would require fetching).\r\n\r\nMaybe that's by design but the lack of comments makes me wonder the purpose behind it.",
      "comment_id": 2191313616,
      "user": "charettes",
      "created_at": "2025-07-08T01:48:55Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2191313616"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/fetch_modes.py",
      "line": 61,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,52 @@\n+from django.core.exceptions import FieldFetchBlocked\n+\n+\n+class FetchMode:\n+    __slots__ = ()\n+\n+    track_peers = False\n+\n+    def fetch(self, fetcher, instance):\n+        raise NotImplementedError(\"Subclasses must implement this method.\")\n+\n+\n+class FetchOne(FetchMode):\n+    __slots__ = ()\n+\n+    def fetch(self, fetcher, instance):\n+        fetcher.fetch_one(instance)\n+\n+\n+FETCH_ONE = FetchOne()\n+\n+\n+class FetchPeers(FetchMode):\n+    __slots__ = ()\n+\n+    track_peers = True\n+\n+    def fetch(self, fetcher, instance):\n+        instances = [\n+            peer\n+            for peer_weakref in instance._state.peers\n+            if (peer := peer_weakref()) is not None\n+        ]\n+        if len(instances) > 1:\n+            fetcher.fetch_many(instances)\n+        else:\n+            fetcher.fetch_one(instance)\n+\n+\n+FETCH_PEERS = FetchPeers()\n+\n+\n+class Raise(FetchMode):\n+    __slots__ = ()\n+\n+    def fetch(self, fetcher, instance):\n+        klass = instance.__class__.__qualname__\n+        field_name = fetcher.field.name\n+        raise FieldFetchBlocked(f\"Fetching of {klass}.{field_name} blocked.\")\n+\n+\n+RAISE = Raise()",
      "comment": "Really like how this turned out, thanks for the tweaks!",
      "comment_id": 2191315057,
      "user": "charettes",
      "created_at": "2025-07-08T01:50:46Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2191315057"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/fields/related_descriptors.py",
      "line": 465,
      "side": "RIGHT",
      "diff_hunk": "@@ -441,16 +447,20 @@ def RelatedObjectDoesNotExist(self):\n     def is_cached(self, instance):\n         return self.related.is_cached(instance)\n \n-    def get_queryset(self, **hints):\n-        return self.related.related_model._base_manager.db_manager(hints=hints).all()\n+    def get_queryset(self, *, instance):\n+        return self.related.related_model._base_manager.db_manager(\n+            hints={\"instance\": instance}",
      "comment": "I appreciate that a clearer signature is used but it should be done in a distinct commit as it might be used under other circumstances in the wild.",
      "comment_id": 2191322664,
      "user": "charettes",
      "created_at": "2025-07-08T01:59:07Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2191322664"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/fields/related_descriptors.py",
      "line": 479,
      "side": "RIGHT",
      "diff_hunk": "@@ -441,16 +447,20 @@ def RelatedObjectDoesNotExist(self):\n     def is_cached(self, instance):\n         return self.related.is_cached(instance)\n \n-    def get_queryset(self, **hints):\n-        return self.related.related_model._base_manager.db_manager(hints=hints).all()\n+    def get_queryset(self, *, instance):\n+        return self.related.related_model._base_manager.db_manager(\n+            hints={\"instance\": instance}\n+        ).fetch_mode(instance._state.fetch_mode)\n \n     def get_prefetch_querysets(self, instances, querysets=None):\n         if querysets and len(querysets) != 1:\n             raise ValueError(\n                 \"querysets argument of get_prefetch_querysets() should have a length \"\n                 \"of 1.\"\n             )\n-        queryset = querysets[0] if querysets else self.get_queryset()\n+        queryset = (\n+            querysets[0] if querysets else self.get_queryset(instance=instances[0])\n+        )\n         queryset._add_hints(instance=instances[0])",
      "comment": "Aren't these two `instance` hint passing redundant?",
      "comment_id": 2191323414,
      "user": "charettes",
      "created_at": "2025-07-08T02:00:07Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2191323414"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/fetch_modes.py",
      "line": 49,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,52 @@\n+from django.core.exceptions import FieldFetchBlocked\n+\n+\n+class FetchMode:\n+    __slots__ = ()\n+\n+    track_peers = False\n+\n+    def fetch(self, fetcher, instance):\n+        raise NotImplementedError(\"Subclasses must implement this method.\")\n+\n+\n+class FetchOne(FetchMode):\n+    __slots__ = ()\n+\n+    def fetch(self, fetcher, instance):\n+        fetcher.fetch_one(instance)\n+\n+\n+FETCH_ONE = FetchOne()\n+\n+\n+class FetchPeers(FetchMode):\n+    __slots__ = ()\n+\n+    track_peers = True\n+\n+    def fetch(self, fetcher, instance):\n+        instances = [\n+            peer\n+            for peer_weakref in instance._state.peers\n+            if (peer := peer_weakref()) is not None\n+        ]\n+        if len(instances) > 1:\n+            fetcher.fetch_many(instances)\n+        else:\n+            fetcher.fetch_one(instance)\n+\n+\n+FETCH_PEERS = FetchPeers()\n+\n+\n+class Raise(FetchMode):\n+    __slots__ = ()\n+\n+    def fetch(self, fetcher, instance):\n+        klass = instance.__class__.__qualname__\n+        field_name = fetcher.field.name\n+        raise FieldFetchBlocked(f\"Fetching of {klass}.{field_name} blocked.\")",
      "comment": "Thanks for this tool, currently using it to debug a project. I noticed that with `RAISE`, the exception raised here will include context like:\r\n\r\n```py\r\nTraceback (most recent call last):\r\n  File \"/Users/jwalls/django/django/db/models/fields/related_descriptors.py\", line 231, in __get__\r\n    rel_obj = self.field.get_cached_value(instance)\r\n  File \"/Users/jwalls/django/django/db/models/fields/mixins.py\", line 21, in get_cached_value\r\n    return instance._state.fields_cache[self.cache_name]\r\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\r\nKeyError: 'nodegroup'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n```\r\n\r\nWhat do you think about raising this error `from None` to suppress that irrelevant context?",
      "comment_id": 2261482253,
      "user": "jacobtylerwalls",
      "created_at": "2025-08-07T21:44:57Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2261482253"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query.py",
      "line": 313,
      "side": "RIGHT",
      "diff_hunk": "@@ -291,6 +308,7 @@ def __init__(self, model=None, query=None, using=None, hints=None):\n         self._prefetch_done = False\n         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}\n         self._iterable_class = ModelIterable\n+        self._fetch_mode = FETCH_ONE",
      "comment": "When debugging a project earlier this summer, it was enormously helpful to be able to just monkey-patch this to `RAISE` and comb through failures.\r\n\r\nOne _interesting_ failure I wasn't able to solve looked like:\r\n\r\n```py\r\nrequest.user.userprofile\r\n```\r\n```py\r\ndjango.core.exceptions.FieldFetchBlocked: Fetching of User.userprofile blocked.\r\n```\r\n\r\nTo fix that, I'd need:\r\n- a custom user model\r\n- a custom model manager for that custom model that overrides `get_queryset()` to chain `.fetch_mode(models.FETCH_PEERS)`\r\n\r\nMy questions are:\r\n- that's the easiest way, right?\r\n- Is there a future where `django.contrib.auth` would ever use `.fetch_mode(models.FETCH_PEERS)` in `get_user()`? Since it's a contrib app? Or would that violate the mental model of \"django defaults to FETCH_ONE\"?\r\n\r\nOne query for a `.userprofile` per request isn't so bad, but the fact that I can't \"globally\" turn on RAISE mode without my tests immediately failing on this is actually what impacts me more.",
      "comment_id": 2311291480,
      "user": "jacobtylerwalls",
      "created_at": "2025-08-29T21:11:02Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2311291480"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query.py",
      "line": 1938,
      "side": "RIGHT",
      "diff_hunk": "@@ -1835,6 +1854,12 @@ def using(self, alias):\n         clone._db = alias\n         return clone\n \n+    def fetch_mode(self, fetch_mode):\n+        \"\"\"Set the fetch mode for the QuerySet.\"\"\"\n+        clone = self._chain()",
      "comment": "`.fetch_mode(None)` fails pretty far from the source of the problem, after the qs is evaluated:\r\n```py\r\n--> 134     if fetch_mode.track_peers:\r\n    135         peers.append(weak_ref(obj))\r\n    136         obj._state.peers = peers\r\n\r\nAttributeError: 'NoneType' object has no attribute 'track_peers'\r\n```\r\n\r\nCould we fail immediately inside this method, similar to how `.order_by(None)` immediately fails (rather than during query execution)?",
      "comment_id": 2319130510,
      "user": "jacobtylerwalls",
      "created_at": "2025-09-03T14:15:47Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2319130510"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query_utils.py",
      "line": 293,
      "side": "RIGHT",
      "diff_hunk": "@@ -281,6 +282,20 @@ def _check_parent_chain(self, instance):\n             return getattr(instance, link_field.attname)\n         return None\n \n+    def fetch_one(self, instance):\n+        instance.refresh_from_db(fields=[self.field.attname])\n+\n+    def fetch_many(self, instances):\n+        attname = self.field.attname\n+        value_by_pk = {\n+            pk: value\n+            for pk, value in self.field.model._base_manager.db_manager()\n+            .filter(pk__in={i.pk for i in instances})",
      "comment": "Somebody in the wild might have non-hashable primary keys, which as far as I know work just fine (outside of `in_bulk()` for obvious reasons).\r\n\r\nThe test model for custom pks even lacked a `__hash__` method until 8fd21b0da35697591e86f4eab0035c4360a45144.\r\n\r\nShould we use different data structures here to account for this?",
      "comment_id": 2319162023,
      "user": "jacobtylerwalls",
      "created_at": "2025-09-03T14:26:09Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2319162023"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/base.py",
      "line": 615,
      "side": "RIGHT",
      "diff_hunk": "@@ -595,7 +612,7 @@ def __init__(self, *args, **kwargs):\n         post_init.send(sender=cls, instance=self)\n \n     @classmethod\n-    def from_db(cls, db, field_names, values):\n+    def from_db(cls, db, field_names, values, *, fetch_mode=None):",
      "comment": "Should we document the ability to set `._state.fetch_mode` directly? I found this useful when I was downstream of a `RAISE`, but I needed to loosen the mode back to `FETCH_PEERS`, because there was no way to \"fix\" the underlying N+1 problem if the object I was dealing with came from `create()` (since there is no one-shot `create()` + `select_related()`).",
      "comment_id": 2319233714,
      "user": "jacobtylerwalls",
      "created_at": "2025-09-03T14:48:49Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2319233714"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/contrib/contenttypes/fields.py",
      "line": 165,
      "side": "RIGHT",
      "diff_hunk": "@@ -157,11 +156,19 @@ def get_content_type(self, obj=None, id=None, using=None, model=None):\n             # This should never happen. I love comments like this, don't you?\n             raise Exception(\"Impossible arguments to GFK.get_content_type!\")\n \n+\n+class GenericForeignKeyDescriptor:",
      "comment": "Yes, precisely. I added this explanation over in the separate PR (#19381):\r\n\r\n> This makes GenericForeignKey more similar to other fields which act as\r\ndescriptors, preparing it to add \u201cfetcher protocol\u201d support in a clear and\r\nconsistent way.\r\n\r\nI hope that is sufficient.",
      "comment_id": 2321568442,
      "user": "adamchainz",
      "created_at": "2025-09-04T10:30:01Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2321568442"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/contrib/contenttypes/fields.py",
      "line": 281,
      "side": "RIGHT",
      "diff_hunk": "@@ -257,7 +267,10 @@ def __get__(self, instance, cls=None):\n             except ObjectDoesNotExist:\n                 pass\n         self.field.set_cached_value(instance, rel_obj)\n-        return rel_obj\n+\n+    def fetch_many(self, instances):\n+        missing_instances = [i for i in instances if not self.field.is_cached(i)]",
      "comment": "Done, and similarly in `ForwardManyToOneDescriptor` and `ReverseOneToOneDescriptor` as well.",
      "comment_id": 2322151599,
      "user": "adamchainz",
      "created_at": "2025-09-04T13:25:30Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2322151599"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/fetch_modes.py",
      "line": 49,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,52 @@\n+from django.core.exceptions import FieldFetchBlocked\n+\n+\n+class FetchMode:\n+    __slots__ = ()\n+\n+    track_peers = False\n+\n+    def fetch(self, fetcher, instance):\n+        raise NotImplementedError(\"Subclasses must implement this method.\")\n+\n+\n+class FetchOne(FetchMode):\n+    __slots__ = ()\n+\n+    def fetch(self, fetcher, instance):\n+        fetcher.fetch_one(instance)\n+\n+\n+FETCH_ONE = FetchOne()\n+\n+\n+class FetchPeers(FetchMode):\n+    __slots__ = ()\n+\n+    track_peers = True\n+\n+    def fetch(self, fetcher, instance):\n+        instances = [\n+            peer\n+            for peer_weakref in instance._state.peers\n+            if (peer := peer_weakref()) is not None\n+        ]\n+        if len(instances) > 1:\n+            fetcher.fetch_many(instances)\n+        else:\n+            fetcher.fetch_one(instance)\n+\n+\n+FETCH_PEERS = FetchPeers()\n+\n+\n+class Raise(FetchMode):\n+    __slots__ = ()\n+\n+    def fetch(self, fetcher, instance):\n+        klass = instance.__class__.__qualname__\n+        field_name = fetcher.field.name\n+        raise FieldFetchBlocked(f\"Fetching of {klass}.{field_name} blocked.\")",
      "comment": "Done, and expanded the tests to check for this with extra assertions like:\r\n\r\n```python\r\nwith self.assertRaisesMessage(FieldFetchBlocked, msg) as cm:\r\n    ...\r\nself.assertIsNone(cm.exception.__cause__)\r\nself.assertTrue(cm.exception.__suppress_context__)\r\n```",
      "comment_id": 2322200121,
      "user": "adamchainz",
      "created_at": "2025-09-04T13:37:27Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2322200121"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/fields/related_descriptors.py",
      "line": 465,
      "side": "RIGHT",
      "diff_hunk": "@@ -441,16 +447,20 @@ def RelatedObjectDoesNotExist(self):\n     def is_cached(self, instance):\n         return self.related.is_cached(instance)\n \n-    def get_queryset(self, **hints):\n-        return self.related.related_model._base_manager.db_manager(hints=hints).all()\n+    def get_queryset(self, *, instance):\n+        return self.related.related_model._base_manager.db_manager(\n+            hints={\"instance\": instance}",
      "comment": "Good point, I have split the signature change into a new commit.",
      "comment_id": 2322874407,
      "user": "adamchainz",
      "created_at": "2025-09-04T17:16:38Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2322874407"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/base.py",
      "line": 615,
      "side": "RIGHT",
      "diff_hunk": "@@ -595,7 +612,7 @@ def __init__(self, *args, **kwargs):\n         post_init.send(sender=cls, instance=self)\n \n     @classmethod\n-    def from_db(cls, db, field_names, values):\n+    def from_db(cls, db, field_names, values, *, fetch_mode=None):",
      "comment": "I think we should leave it as a private API for now. Let's allow the tinkerers who read the source to play with it for the first release.\r\n\r\nBut at least we've documented two attributes of `Model._state` ([link](https://docs.djangoproject.com/en/5.2/ref/models/instances/#django.db.models.Model._state)) so there's precedent to document it more.",
      "comment_id": 2322888562,
      "user": "adamchainz",
      "created_at": "2025-09-04T17:21:04Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2322888562"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query.py",
      "line": 1938,
      "side": "RIGHT",
      "diff_hunk": "@@ -1835,6 +1854,12 @@ def using(self, alias):\n         clone._db = alias\n         return clone\n \n+    def fetch_mode(self, fetch_mode):\n+        \"\"\"Set the fetch mode for the QuerySet.\"\"\"\n+        clone = self._chain()",
      "comment": "I'm not sure if it's worth special-casing `None` here, given it's not documented\u2026 can we push this to a follow-up enhancement?",
      "comment_id": 2323492036,
      "user": "adamchainz",
      "created_at": "2025-09-04T20:35:06Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2323492036"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query_utils.py",
      "line": 293,
      "side": "RIGHT",
      "diff_hunk": "@@ -281,6 +282,20 @@ def _check_parent_chain(self, instance):\n             return getattr(instance, link_field.attname)\n         return None\n \n+    def fetch_one(self, instance):\n+        instance.refresh_from_db(fields=[self.field.attname])\n+\n+    def fetch_many(self, instances):\n+        attname = self.field.attname\n+        value_by_pk = {\n+            pk: value\n+            for pk, value in self.field.model._base_manager.db_manager()\n+            .filter(pk__in={i.pk for i in instances})",
      "comment": "After a quick search, I think Django already doesn't support unhashable primary keys. The deletion collector performs a similar set comprehension:\r\n\r\nhttps://github.com/django/django/blob/2722cb61ccae84f593e6d2c28814e3c628743994/django/db/models/deletion.py#L490\r\n\r\nSimilarly, model forms rely on hashing PKs:\r\n\r\nhttps://github.com/django/django/blob/2722cb61ccae84f593e6d2c28814e3c628743994/django/forms/models.py#L720\r\n\r\nAlso, because values must be unique in the database, that implies a hash function can be written.\r\n\r\nSo, I think this isn't a use case we need to worry about.\r\n\r\n",
      "comment_id": 2323509075,
      "user": "adamchainz",
      "created_at": "2025-09-04T20:43:00Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2323509075"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query.py",
      "line": 313,
      "side": "RIGHT",
      "diff_hunk": "@@ -291,6 +308,7 @@ def __init__(self, model=None, query=None, using=None, hints=None):\n         self._prefetch_done = False\n         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}\n         self._iterable_class = ModelIterable\n+        self._fetch_mode = FETCH_ONE",
      "comment": "> that's the easiest way, right?\r\n\r\nI think it would be slightly easier to subclass `ModelBackend` and replace `get_user` with a version that adds the `.fetch_mode()` call.\r\n\r\n> Is there a future where `django.contrib.auth` would ever use `.fetch_mode(models.FETCH_PEERS)` in `get_user()`? Since it's a contrib app? Or would that violate the mental model of \"django defaults to FETCH_ONE\"?\r\n\r\nI think there could be such a future. I'd hope we could also change the admin to use `models.FETCH_PEERS` by default.\r\n\r\n> One query for a `.userprofile` per request isn't so bad, but the fact that I can't \"globally\" turn on RAISE mode without my tests immediately failing on this is actually what impacts me more.\r\n\r\nYeah, I found a similar issue in a project I tried the same trick on. That one failed during migrations, because a data migration in a third-party app was relying on the default `FETCH_ONE` behaviour.\r\n\r\nI don't think there's a good API we can provide to allow enabling `RAISE` globally without breaking lots of hard-to-fix stuff. That's why I've come around to Simon's suggestion of going with the `QuerySet`-based API. We can always address adding a supported global switch later, but for now having the targeted API seems best.",
      "comment_id": 2323521780,
      "user": "adamchainz",
      "created_at": "2025-09-04T20:50:16Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2323521780"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/fetch_modes.py",
      "line": 23,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,52 @@\n+from django.core.exceptions import FieldFetchBlocked\n+\n+\n+class FetchMode:\n+    __slots__ = ()\n+\n+    track_peers = False\n+\n+    def fetch(self, fetcher, instance):\n+        raise NotImplementedError(\"Subclasses must implement this method.\")\n+\n+\n+class FetchOne(FetchMode):\n+    __slots__ = ()\n+\n+    def fetch(self, fetcher, instance):\n+        fetcher.fetch_one(instance)\n+\n+\n+FETCH_ONE = FetchOne()",
      "comment": "Last Q: did you consider making this an enum? (can totally punt...)",
      "comment_id": 2323557124,
      "user": "jacobtylerwalls",
      "created_at": "2025-09-04T21:11:08Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2323557124"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/fetch_modes.py",
      "line": 23,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,52 @@\n+from django.core.exceptions import FieldFetchBlocked\n+\n+\n+class FetchMode:\n+    __slots__ = ()\n+\n+    track_peers = False\n+\n+    def fetch(self, fetcher, instance):\n+        raise NotImplementedError(\"Subclasses must implement this method.\")\n+\n+\n+class FetchOne(FetchMode):\n+    __slots__ = ()\n+\n+    def fetch(self, fetcher, instance):\n+        fetcher.fetch_one(instance)\n+\n+\n+FETCH_ONE = FetchOne()",
      "comment": "I think initial designs used an enum or constants, but @charettes suggested to use callables, copying the `on_delete` modes like `CASCADE`. Not only is this more consistent, it opens the door for adding custom logic, like including logging or conditionally blocking fetches.",
      "comment_id": 2323750492,
      "user": "adamchainz",
      "created_at": "2025-09-04T23:35:54Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2323750492"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/base.py",
      "line": 494,
      "side": "RIGHT",
      "diff_hunk": "@@ -473,6 +482,14 @@ class ModelState:\n     # on the actual save.\n     adding = True\n     fields_cache = ModelStateFieldsCacheDescriptor()\n+    fetch_mode = ModelStateFetchModeDescriptor()\n+    peers = ()\n+\n+    def __getstate__(self):\n+        state = self.__dict__.copy()\n+        state.pop(\"fetch_mode\", None)\n+        state.pop(\"peers\", None)",
      "comment": "Yes, these state removals do mean that pickled instances lose their fetch modes.\r\n\r\nI added these lines because weakrefs aren't pickleable, copying from:\r\n\r\nhttps://github.com/adamchainz/django-auto-prefetch/blob/7f3b7aed5b1b029b7ddeb4d5fa9c0d76281b4fe4/src/auto_prefetch/__init__.py#L118-L126\r\n\r\nHowever, it's a good point that unpickled instances shouldn't lose their fetch modes. I guess the best we can do is drop peers only, so `FETCH_PEERS` won't work for unpickled instances. However, it will be propagated, so at least there's that.\r\n\r\nI'll change this and add a test for an unpickled instnace.",
      "comment_id": 2323760732,
      "user": "adamchainz",
      "created_at": "2025-09-04T23:47:16Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2323760732"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/base.py",
      "line": 494,
      "side": "RIGHT",
      "diff_hunk": "@@ -473,6 +482,14 @@ class ModelState:\n     # on the actual save.\n     adding = True\n     fields_cache = ModelStateFieldsCacheDescriptor()\n+    fetch_mode = ModelStateFetchModeDescriptor()\n+    peers = ()\n+\n+    def __getstate__(self):\n+        state = self.__dict__.copy()\n+        state.pop(\"fetch_mode\", None)\n+        state.pop(\"peers\", None)",
      "comment": "I ended up adding `__reduce__` methods to fetch modes to make them pickle more efficiently, in a new commit 30c72657ff9d3747eb5d4b59aacce9bac0acdbad.",
      "comment_id": 2324686265,
      "user": "adamchainz",
      "created_at": "2025-09-05T10:15:52Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2324686265"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/fetch_modes.py",
      "line": 23,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,52 @@\n+from django.core.exceptions import FieldFetchBlocked\n+\n+\n+class FetchMode:\n+    __slots__ = ()\n+\n+    track_peers = False\n+\n+    def fetch(self, fetcher, instance):\n+        raise NotImplementedError(\"Subclasses must implement this method.\")\n+\n+\n+class FetchOne(FetchMode):\n+    __slots__ = ()\n+\n+    def fetch(self, fetcher, instance):\n+        fetcher.fetch_one(instance)\n+\n+\n+FETCH_ONE = FetchOne()",
      "comment": "I'm down with the callables, but I just meant wrapping them in an enum like this, mostly just for discoverability:\r\n```py\r\nclass FetchModes(enum.Enum):\r\n    FETCH_ONE = FETCH_ONE\r\n    FETCH_PEERS = FETCH_PEERS\r\n    RAISE = RAISE\r\n```",
      "comment_id": 2324891799,
      "user": "jacobtylerwalls",
      "created_at": "2025-09-05T11:52:48Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2324891799"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/fetch_modes.py",
      "line": 23,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,52 @@\n+from django.core.exceptions import FieldFetchBlocked\n+\n+\n+class FetchMode:\n+    __slots__ = ()\n+\n+    track_peers = False\n+\n+    def fetch(self, fetcher, instance):\n+        raise NotImplementedError(\"Subclasses must implement this method.\")\n+\n+\n+class FetchOne(FetchMode):\n+    __slots__ = ()\n+\n+    def fetch(self, fetcher, instance):\n+        fetcher.fetch_one(instance)\n+\n+\n+FETCH_ONE = FetchOne()",
      "comment": "Oh, right\u2026 I'm not super convinced, but maybe others would be.\r\n\r\nCan we defer this one, too? I think we'd want the same pattern for the deletion modes as well, so it could be done as a cleanup affecting both.",
      "comment_id": 2324953376,
      "user": "adamchainz",
      "created_at": "2025-09-05T12:23:50Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2324953376"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query_utils.py",
      "line": 293,
      "side": "RIGHT",
      "diff_hunk": "@@ -281,6 +282,20 @@ def _check_parent_chain(self, instance):\n             return getattr(instance, link_field.attname)\n         return None\n \n+    def fetch_one(self, instance):\n+        instance.refresh_from_db(fields=[self.field.attname])\n+\n+    def fetch_many(self, instances):\n+        attname = self.field.attname\n+        value_by_pk = {\n+            pk: value\n+            for pk, value in self.field.model._base_manager.db_manager()\n+            .filter(pk__in={i.pk for i in instances})",
      "comment": "Any interest in observing database parameter limits, similar to how `in_bulk()` [does it](https://github.com/django/django/blob/0ddbe12ea99a2dc1b757dc2015ba8bb6bfd9d653/django/db/models/query.py#L1195), or factoring out some reusable private helper?",
      "comment_id": 2325273743,
      "user": "jacobtylerwalls",
      "created_at": "2025-09-05T14:34:00Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2325273743"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query_utils.py",
      "line": 293,
      "side": "RIGHT",
      "diff_hunk": "@@ -281,6 +282,20 @@ def _check_parent_chain(self, instance):\n             return getattr(instance, link_field.attname)\n         return None\n \n+    def fetch_one(self, instance):\n+        instance.refresh_from_db(fields=[self.field.attname])\n+\n+    def fetch_many(self, instances):\n+        attname = self.field.attname\n+        value_by_pk = {\n+            pk: value\n+            for pk, value in self.field.model._base_manager.db_manager()\n+            .filter(pk__in={i.pk for i in instances})",
      "comment": "Kind of a shame that `in_bulk` doesn't support `values` / `values_list` in the first place as that could have been\r\n\r\n```python\r\nvalue_by_pk = (\r\n    self.field.model._base_manager\r\n    .values_list(attname, flat=True).in_bulk({i.pk for i in instances})\r\n)\r\n```\r\n\r\nin all cases it seems that the `db_manager()` call is unnecessary?",
      "comment_id": 2325313394,
      "user": "charettes",
      "created_at": "2025-09-05T14:49:19Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2325313394"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query_utils.py",
      "line": 293,
      "side": "RIGHT",
      "diff_hunk": "@@ -281,6 +282,20 @@ def _check_parent_chain(self, instance):\n             return getattr(instance, link_field.attname)\n         return None\n \n+    def fetch_one(self, instance):\n+        instance.refresh_from_db(fields=[self.field.attname])\n+\n+    def fetch_many(self, instances):\n+        attname = self.field.attname\n+        value_by_pk = {\n+            pk: value\n+            for pk, value in self.field.model._base_manager.db_manager()\n+            .filter(pk__in={i.pk for i in instances})",
      "comment": "> Kind of a shame that in_bulk doesn't support values / values_list\r\n\r\nI'll open a new features ticket and link to the [sketched](https://github.com/django/django/pull/18497) implementation, given that we have this additional use in Django itself now.",
      "comment_id": 2325335804,
      "user": "jacobtylerwalls",
      "created_at": "2025-09-05T14:58:40Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2325335804"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query_utils.py",
      "line": 293,
      "side": "RIGHT",
      "diff_hunk": "@@ -281,6 +282,20 @@ def _check_parent_chain(self, instance):\n             return getattr(instance, link_field.attname)\n         return None\n \n+    def fetch_one(self, instance):\n+        instance.refresh_from_db(fields=[self.field.attname])\n+\n+    def fetch_many(self, instances):\n+        attname = self.field.attname\n+        value_by_pk = {\n+            pk: value\n+            for pk, value in self.field.model._base_manager.db_manager()\n+            .filter(pk__in={i.pk for i in instances})",
      "comment": "Also, not sure if the dict literal is faster but the `dict` function can be used as well\r\n\r\n```python\r\n        value_by_pk = dict(\r\n            self.field.model._base_manager\r\n            .filter(pk__in={i.pk for i in instances})\r\n            .values_list(\"pk\", attname)\r\n        )",
      "comment_id": 2325429976,
      "user": "charettes",
      "created_at": "2025-09-05T15:38:59Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2325429976"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query_utils.py",
      "line": 293,
      "side": "RIGHT",
      "diff_hunk": "@@ -281,6 +282,20 @@ def _check_parent_chain(self, instance):\n             return getattr(instance, link_field.attname)\n         return None\n \n+    def fetch_one(self, instance):\n+        instance.refresh_from_db(fields=[self.field.attname])\n+\n+    def fetch_many(self, instances):\n+        attname = self.field.attname\n+        value_by_pk = {\n+            pk: value\n+            for pk, value in self.field.model._base_manager.db_manager()\n+            .filter(pk__in={i.pk for i in instances})",
      "comment": "> Any interest in observing database parameter limits, similar to how `in_bulk()` [does it](https://github.com/django/django/blob/0ddbe12ea99a2dc1b757dc2015ba8bb6bfd9d653/django/db/models/query.py#L1195), or factoring out some reusable private helper?\r\n\r\nWell, yes, that seems great. I will take a look at how easy it would be to revive that old PR.\r\n\r\nParameter limit support is necessary, right? Otherwise, this will crash when there are enough instances to fetch, yes?\r\n\r\n> in all cases it seems that the `db_manager()` call is unnecessary?\r\n\r\nYup, good shout, updating.\r\n\r\n> Also, not sure if the dict literal is faster but the `dict` function can be used as well\r\n\r\n`dict()` is indeed faster:\r\n\r\n```\r\n$ uvx -p 3.14 ipython\r\n...\r\nPython 3.14.0rc1 (main, Aug  8 2025, 16:52:21) [Clang 20.1.4 ]\r\n...\r\n\r\nIn [1]: items = [(i, str(i)) for i in range(1000)]\r\n\r\nIn [2]: %timeit {k: v for k, v in items}\r\n22.1 \u03bcs \u00b1 208 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)\r\n\r\nIn [3]: %timeit dict(items)\r\n15.7 \u03bcs \u00b1 52.9 ns per loop (mean \u00b1 std. dev. of 7 runs, 100,000 loops each)\r\n```\r\n\r\nI'll update.\r\n",
      "comment_id": 2339752180,
      "user": "adamchainz",
      "created_at": "2025-09-11T09:31:05Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2339752180"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/fetch_modes.py",
      "line": 4,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,61 @@\n+from django.core.exceptions import FieldFetchBlocked\n+\n+\n+class FetchMode:",
      "comment": "I really love this pattern because it allows users to subclass and define their own fetch mode really easily. There are many variations that users will inevitably want:\r\n - fetch-many and then log\r\n - fetch-one but log if there are peers to fetch\r\n - add sentry event (ot whatever observability platform you're using) and do each of the above.\r\n - etc.\r\n\r\nAnd this pattern allows for all of the above without an ever-growing list of fetch-modes. Is it worth documenting this? We could add a simple how-to with an example.",
      "comment_id": 2344830617,
      "user": "tim-mccurrach",
      "created_at": "2025-09-12T16:30:47Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2344830617"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/fetch_modes.py",
      "line": 36,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,61 @@\n+from django.core.exceptions import FieldFetchBlocked\n+\n+\n+class FetchMode:\n+    __slots__ = ()\n+\n+    track_peers = False\n+\n+    def fetch(self, fetcher, instance):\n+        raise NotImplementedError(\"Subclasses must implement this method.\")\n+\n+\n+class FetchOne(FetchMode):\n+    __slots__ = ()\n+\n+    def fetch(self, fetcher, instance):\n+        fetcher.fetch_one(instance)\n+\n+    def __reduce__(self):\n+        return \"FETCH_ONE\"\n+\n+\n+FETCH_ONE = FetchOne()\n+\n+\n+class FetchPeers(FetchMode):\n+    __slots__ = ()\n+\n+    track_peers = True\n+\n+    def fetch(self, fetcher, instance):\n+        instances = [\n+            peer\n+            for peer_weakref in instance._state.peers\n+            if (peer := peer_weakref()) is not None\n+        ]",
      "comment": "In light of the comment above, I think the logic within`FetchPeers` will be useful for custom fetch-modes. I can imagine users only wanting to log/do-something in the case when `len(instances) > 1`. \n\nIt would be great to make `FetchPeers` more subclass-able without users needing to copy code. Two ideas that occur are:\n - Maybe give the `fetch` method a return value of `len(instances)`, so that you can call `super().fetch(...)` and vary the behaviour accordingly.\n - Alternatively, move lines 32-36 to a separate method, `get_peers`. This would make overwriting the `fetch` method easier. (This method could even go in the base `FetchMode` class). \n",
      "comment_id": 2344857151,
      "user": "tim-mccurrach",
      "created_at": "2025-09-12T16:35:36Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2344857151"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query.py",
      "line": 138,
      "side": "RIGHT",
      "diff_hunk": "@@ -122,10 +125,17 @@ def __iter__(self):\n             )\n             for field, related_objs in queryset._known_related_objects.items()\n         ]\n+        peers = []\n         for row in compiler.results_iter(results):\n             obj = model_cls.from_db(\n-                db, init_list, row[model_fields_start:model_fields_end]\n+                db,\n+                init_list,\n+                row[model_fields_start:model_fields_end],\n+                fetch_mode=fetch_mode,\n             )\n+            if fetch_mode.track_peers:\n+                peers.append(weak_ref(obj))\n+                obj._state.peers = peers",
      "comment": "This is really pernickety (sorry), but `peers` starts off life as a tuple and then changes to a list here. This makes it more difficult to use, since there are fewer assumptions you can make at the point of using it. Having said this, I appreciate that this is not part of the public API, so maybe that need not be a concern.",
      "comment_id": 2344869077,
      "user": "tim-mccurrach",
      "created_at": "2025-09-12T16:41:07Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2344869077"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/fetch_modes.py",
      "line": 4,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,61 @@\n+from django.core.exceptions import FieldFetchBlocked\n+\n+\n+class FetchMode:",
      "comment": "Yes, I'd like to document the pattern. I was thinking of leaving it out of the initial PR, but perhaps that's not worth it at this point!",
      "comment_id": 2345545473,
      "user": "adamchainz",
      "created_at": "2025-09-12T22:38:41Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2345545473"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query.py",
      "line": 138,
      "side": "RIGHT",
      "diff_hunk": "@@ -122,10 +125,17 @@ def __iter__(self):\n             )\n             for field, related_objs in queryset._known_related_objects.items()\n         ]\n+        peers = []\n         for row in compiler.results_iter(results):\n             obj = model_cls.from_db(\n-                db, init_list, row[model_fields_start:model_fields_end]\n+                db,\n+                init_list,\n+                row[model_fields_start:model_fields_end],\n+                fetch_mode=fetch_mode,\n             )\n+            if fetch_mode.track_peers:\n+                peers.append(weak_ref(obj))\n+                obj._state.peers = peers",
      "comment": "There are good reasons for this discrepancy\u2026\r\n\r\n1. We keep `peers` as a class-level variable in `ModelState` to avoid creating any values for new instances that don't need.\r\n2. For class-level variables, we want them to be immutable, hence a tuple.\r\n3. When peers are collected, though, they must be a list, so we can grow it as we instantiate the model instances here.\r\n\r\nIf we were using type hints, we would define `peers: Sequence[weak_ref[T]]`, so we'd know that code relied only on the common methods of a sequence, not any of the specifics of tuple or list.",
      "comment_id": 2345566633,
      "user": "adamchainz",
      "created_at": "2025-09-12T22:54:47Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2345566633"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/fetch_modes.py",
      "line": 4,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,61 @@\n+from django.core.exceptions import FieldFetchBlocked\n+\n+\n+class FetchMode:",
      "comment": "I think I will leave this as a follow-up, I haven't found the time to really hash out the details, and the 6.0 merge deadline is here\u2026 Also, it's probably best to leave it for tinkerers to explore for one version before we commit to a certain pattern.\r\n\r\nI'd also like to make another follow-up, to use `FETCH_PEERS` by default in the admin.",
      "comment_id": 2354993737,
      "user": "adamchainz",
      "created_at": "2025-09-17T10:06:06Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2354993737"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/fetch_modes.py",
      "line": 36,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,61 @@\n+from django.core.exceptions import FieldFetchBlocked\n+\n+\n+class FetchMode:\n+    __slots__ = ()\n+\n+    track_peers = False\n+\n+    def fetch(self, fetcher, instance):\n+        raise NotImplementedError(\"Subclasses must implement this method.\")\n+\n+\n+class FetchOne(FetchMode):\n+    __slots__ = ()\n+\n+    def fetch(self, fetcher, instance):\n+        fetcher.fetch_one(instance)\n+\n+    def __reduce__(self):\n+        return \"FETCH_ONE\"\n+\n+\n+FETCH_ONE = FetchOne()\n+\n+\n+class FetchPeers(FetchMode):\n+    __slots__ = ()\n+\n+    track_peers = True\n+\n+    def fetch(self, fetcher, instance):\n+        instances = [\n+            peer\n+            for peer_weakref in instance._state.peers\n+            if (peer := peer_weakref()) is not None\n+        ]",
      "comment": "I think best to leave this for a follow-up where we document how to make a custom fetch mode.",
      "comment_id": 2354996204,
      "user": "adamchainz",
      "created_at": "2025-09-17T10:07:07Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2354996204"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query_utils.py",
      "line": 293,
      "side": "RIGHT",
      "diff_hunk": "@@ -281,6 +282,20 @@ def _check_parent_chain(self, instance):\n             return getattr(instance, link_field.attname)\n         return None\n \n+    def fetch_one(self, instance):\n+        instance.refresh_from_db(fields=[self.field.attname])\n+\n+    def fetch_many(self, instances):\n+        attname = self.field.attname\n+        value_by_pk = {\n+            pk: value\n+            for pk, value in self.field.model._base_manager.db_manager()\n+            .filter(pk__in={i.pk for i in instances})",
      "comment": "I've just rebased and made this bit use `in_bulk()` now that support was added in 1820d35b17f0a95f4ce888971b9ca0c7a3697c83.",
      "comment_id": 2387381954,
      "user": "adamchainz",
      "created_at": "2025-09-29T10:01:04Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2387381954"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query_utils.py",
      "line": 293,
      "side": "RIGHT",
      "diff_hunk": "@@ -281,6 +282,20 @@ def _check_parent_chain(self, instance):\n             return getattr(instance, link_field.attname)\n         return None\n \n+    def fetch_one(self, instance):\n+        instance.refresh_from_db(fields=[self.field.attname])\n+\n+    def fetch_many(self, instances):\n+        attname = self.field.attname\n+        value_by_pk = {\n+            pk: value\n+            for pk, value in self.field.model._base_manager.db_manager()\n+            .filter(pk__in={i.pk for i in instances})",
      "comment": "Nice. Just checking: did you plan to use `dict()` here after all?",
      "comment_id": 2387808331,
      "user": "jacobtylerwalls",
      "created_at": "2025-09-29T12:44:07Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2387808331"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query_utils.py",
      "line": 293,
      "side": "RIGHT",
      "diff_hunk": "@@ -281,6 +282,20 @@ def _check_parent_chain(self, instance):\n             return getattr(instance, link_field.attname)\n         return None\n \n+    def fetch_one(self, instance):\n+        instance.refresh_from_db(fields=[self.field.attname])\n+\n+    def fetch_many(self, instances):\n+        attname = self.field.attname\n+        value_by_pk = {\n+            pk: value\n+            for pk, value in self.field.model._base_manager.db_manager()\n+            .filter(pk__in={i.pk for i in instances})",
      "comment": "Whoops, that's irrelevant now that we got `in_bulk` working :-), resolving",
      "comment_id": 2429953607,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-14T17:32:40Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2429953607"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query_utils.py",
      "line": 292,
      "side": "RIGHT",
      "diff_hunk": "@@ -281,6 +282,17 @@ def _check_parent_chain(self, instance):\n             return getattr(instance, link_field.attname)\n         return None\n \n+    def fetch_one(self, instance):\n+        instance.refresh_from_db(fields=[self.field.attname])\n+\n+    def fetch_many(self, instances):\n+        attname = self.field.attname\n+        value_by_pk = self.field.model._base_manager.values_list(attname).in_bulk(\n+            {i.pk for i in instances}\n+        )",
      "comment": "Earlier you removed a no-argument call to `db_manager()`, but now I wonder if we need to provide a db alias here.\r\n\r\nWith only an `\"other\"` connection, I see this works: the deferred field is fetched from `\"other\"`:\r\n```py\r\nIn [18]: qs = ResourceInstance.objects.defer('principaluser').using('other')\r\n\r\nIn [19]: for el in qs:\r\n    ...:     print(el.principaluser)\r\n    ...: \r\nadmin\r\n```\r\nversus this failure showing we tried to go to the default alias:\r\n```py\r\nIn [20]: qs = ResourceInstance.objects.defer('principaluser').using('other').fetch_mode(models.FETCH_PEERS)\r\n\r\nIn [21]: for el in qs:\r\n    ...:     print(el.principaluser)\r\n\r\nFile ~/django/django/db/models/query_utils.py:290, in DeferredAttribute.fetch_many(self, instances)\r\n    288 def fetch_many(self, instances):\r\n    289     attname = self.field.attname\r\n--> 290     value_by_pk = self.field.model._base_manager.values_list(attname).in_bulk(\r\n    291         {i.pk for i in instances}\r\n    292     )\r\n    293     for instance in instances:\r\n    294         setattr(instance, attname, value_by_pk[instance.pk])\r\n...\r\nFile ~/django/django/db/backends/dummy/base.py:21, in complain(*args, **kwargs)\r\n     20 def complain(*args, **kwargs):\r\n---> 21     raise ImproperlyConfigured(\r\n     22         \"settings.DATABASES is improperly configured. \"\r\n     23         \"Please supply the ENGINE value. Check \"\r\n     24         \"settings documentation for more details.\"\r\n     25     )\r\n\r\nImproperlyConfigured: settings.DATABASES is improperly configured. Please supply the ENGINE value. Check settings documentation for more details.\r\n```\r\n\r\nHow would the user provide an alias here?",
      "comment_id": 2430149841,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-14T19:01:12Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2430149841"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query_utils.py",
      "line": 292,
      "side": "RIGHT",
      "diff_hunk": "@@ -281,6 +282,17 @@ def _check_parent_chain(self, instance):\n             return getattr(instance, link_field.attname)\n         return None\n \n+    def fetch_one(self, instance):\n+        instance.refresh_from_db(fields=[self.field.attname])\n+\n+    def fetch_many(self, instances):\n+        attname = self.field.attname\n+        value_by_pk = self.field.model._base_manager.values_list(attname).in_bulk(\n+            {i.pk for i in instances}\n+        )",
      "comment": "@jacobtylerwalls we can safely assume that all instances are originating from the same database (otherwise they wouldn't be queryset peers) so doing\r\n\r\n```python\r\ndb = instances[0]._state.db\r\nvalue_by_pk = self.field.model._base_manager.using(db).values_list(attname).in_bulk(\r\n    {i.pk for i in instances}\r\n)\r\n```\r\n\r\nshould do here",
      "comment_id": 2430326751,
      "user": "charettes",
      "created_at": "2025-10-14T20:12:53Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2430326751"
    },
    {
      "repo": "django/django",
      "pr_number": 17554,
      "file_path": "django/db/models/query_utils.py",
      "line": 292,
      "side": "RIGHT",
      "diff_hunk": "@@ -281,6 +282,17 @@ def _check_parent_chain(self, instance):\n             return getattr(instance, link_field.attname)\n         return None\n \n+    def fetch_one(self, instance):\n+        instance.refresh_from_db(fields=[self.field.attname])\n+\n+    def fetch_many(self, instances):\n+        attname = self.field.attname\n+        value_by_pk = self.field.model._base_manager.values_list(attname).in_bulk(\n+            {i.pk for i in instances}\n+        )",
      "comment": "Yeah, that looks good. @jacobtylerwalls since you set \"ready to merge\", will you take care of this edit?",
      "comment_id": 2430476759,
      "user": "adamchainz",
      "created_at": "2025-10-14T21:12:10Z",
      "url": "https://github.com/django/django/pull/17554#discussion_r2430476759"
    },
    {
      "repo": "django/django",
      "pr_number": 19943,
      "file_path": "django/utils/text.py",
      "line": 406,
      "side": "RIGHT",
      "diff_hunk": "@@ -393,6 +393,22 @@ def compress_sequence(sequence, *, max_random_bytes=None):\n     yield buf.read()\n \n \n+async def acompress_sequence(sequence, *, max_random_bytes=None):\n+    buf = StreamingBuffer()\n+    filename = _get_random_filename(max_random_bytes) if max_random_bytes else None\n+    with GzipFile(\n+        filename=filename, mode=\"wb\", compresslevel=6, fileobj=buf, mtime=0\n+    ) as zfile:\n+        # Output headers...\n+        yield buf.read()\n+        async for item in sequence:\n+            zfile.write(item)\n+            data = buf.read()",
      "comment": "Is this blocked on ticket-36293? Without flushing here, your sample app from the ticket blocks. Or does this have independent value?",
      "comment_id": 2440793566,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-17T18:12:50Z",
      "url": "https://github.com/django/django/pull/19943#discussion_r2440793566"
    },
    {
      "repo": "django/django",
      "pr_number": 19943,
      "file_path": "django/utils/text.py",
      "line": 406,
      "side": "RIGHT",
      "diff_hunk": "@@ -393,6 +393,22 @@ def compress_sequence(sequence, *, max_random_bytes=None):\n     yield buf.read()\n \n \n+async def acompress_sequence(sequence, *, max_random_bytes=None):\n+    buf = StreamingBuffer()\n+    filename = _get_random_filename(max_random_bytes) if max_random_bytes else None\n+    with GzipFile(\n+        filename=filename, mode=\"wb\", compresslevel=6, fileobj=buf, mtime=0\n+    ) as zfile:\n+        # Output headers...\n+        yield buf.read()\n+        async for item in sequence:\n+            zfile.write(item)\n+            data = buf.read()",
      "comment": "This change has independent value because it fixes response correctness, even if they are buffered.",
      "comment_id": 2442432141,
      "user": "adamchainz",
      "created_at": "2025-10-18T13:52:05Z",
      "url": "https://github.com/django/django/pull/19943#discussion_r2442432141"
    },
    {
      "repo": "django/django",
      "pr_number": 20087,
      "file_path": "django/db/models/constraints.py",
      "line": 59,
      "side": "RIGHT",
      "diff_hunk": "@@ -51,9 +51,9 @@ def remove_sql(self, model, schema_editor):\n     def _expression_refs_exclude(cls, model, expression, exclude):\n         get_field = model._meta.get_field\n         for field_name, *__ in model._get_expr_references(expression):\n-            if field_name in exclude:\n-                return True\n             field = get_field(field_name)\n+            if field.name in exclude:\n+                return True",
      "comment": "We need to handle `pk` separately (ticket-27944):\r\n```python\r\nif field_name == \"pk\":\r\n    field = model._meta.pk\r\nelse:\r\n    field = get_field(field_name)\r\nif field.name in exclude:\r\n    return True\r\n```",
      "comment_id": 2519928779,
      "user": "felixxm",
      "created_at": "2025-11-12T21:52:37Z",
      "url": "https://github.com/django/django/pull/20087#discussion_r2519928779"
    },
    {
      "repo": "django/django",
      "pr_number": 20087,
      "file_path": "tests/model_forms/models.py",
      "line": 557,
      "side": "RIGHT",
      "diff_hunk": "@@ -543,3 +543,22 @@ class Meta:\n                 violation_error_message=\"Price must be greater than zero.\",\n             ),\n         ]\n+\n+\n+class AttnameConstraintsModel(models.Model):\n+    left = models.ForeignKey(\n+        \"self\", related_name=\"+\", null=True, on_delete=models.SET_NULL\n+    )\n+    right = models.ForeignKey(\n+        \"self\", related_name=\"+\", null=True, on_delete=models.SET_NULL\n+    )\n+\n+    class Meta:\n+        required_db_features = {\"supports_table_check_constraints\"}",
      "comment": "EDIT ~The same flag should be checked in `test_check_constraint_refs_excluded_field_attname`.~ Nvd",
      "comment_id": 2522180621,
      "user": "felixxm",
      "created_at": "2025-11-13T08:08:19Z",
      "url": "https://github.com/django/django/pull/20087#discussion_r2522180621"
    },
    {
      "repo": "django/django",
      "pr_number": 19912,
      "file_path": "django/contrib/postgres/fields/array.py",
      "line": 73,
      "side": "RIGHT",
      "diff_hunk": "@@ -67,7 +67,7 @@ def check(self, **kwargs):\n                 )\n             )\n         else:\n-            base_checks = self.base_field.check()\n+            base_checks = self.base_field.check(**kwargs)",
      "comment": "This is not strictly necessary here but it ensures `databases` is respected when specified.",
      "comment_id": 2389370509,
      "user": "charettes",
      "created_at": "2025-09-29T22:03:32Z",
      "url": "https://github.com/django/django/pull/19912#discussion_r2389370509"
    },
    {
      "repo": "django/django",
      "pr_number": 20059,
      "file_path": "tests/admin_scripts/tests.py",
      "line": 179,
      "side": "RIGHT",
      "diff_hunk": "@@ -192,11 +167,18 @@ def run_manage(\n         with open(test_manage_py, \"w\") as fp:\n             fp.write(manage_py_contents)\n \n-        return self.run_test(\n-            [\"./manage.py\", *args],\n-            settings_file,\n-            discover_formatters=discover_formatters,\n-        )\n+        return self.run_test([\"./manage.py\", *args], settings_file)\n+\n+    def assertInAfterFormatting(self, member, container, msg=None):\n+        if HAS_BLACK:\n+            import black\n+\n+            # Black does not have a stable API, but this is still less\n+            # fragile than attempting to filter out containing paths.\n+            member = black.format_str(member, mode=black.FileMode())\n+            container = black.format_str(container, mode=black.FileMode())",
      "comment": "Yep, missed a handful here. Most of the assertIn's are actually asserting against streams, not templated files, so black isn't relevant there.\n\nRe:\n> Moreover, I would rename this helper to assertInOutput \n\nI hear that, but the tricky part is we have an `assertOutput` already for streams. How's `assertInTemplatedFile`?",
      "comment_id": 2492115136,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-04T21:37:42Z",
      "url": "https://github.com/django/django/pull/20059#discussion_r2492115136"
    },
    {
      "repo": "django/django",
      "pr_number": 20059,
      "file_path": "tests/admin_scripts/tests.py",
      "line": 179,
      "side": "RIGHT",
      "diff_hunk": "@@ -192,11 +167,18 @@ def run_manage(\n         with open(test_manage_py, \"w\") as fp:\n             fp.write(manage_py_contents)\n \n-        return self.run_test(\n-            [\"./manage.py\", *args],\n-            settings_file,\n-            discover_formatters=discover_formatters,\n-        )\n+        return self.run_test([\"./manage.py\", *args], settings_file)\n+\n+    def assertInAfterFormatting(self, member, container, msg=None):\n+        if HAS_BLACK:\n+            import black\n+\n+            # Black does not have a stable API, but this is still less\n+            # fragile than attempting to filter out containing paths.\n+            member = black.format_str(member, mode=black.FileMode())\n+            container = black.format_str(container, mode=black.FileMode())",
      "comment": "We can't use it blindly, since the blind approach gives on an input of `<&>`:\n```py\nblack.parsing.InvalidInput: Cannot parse: 1:0: <&>\n```",
      "comment_id": 2492116915,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-04T21:38:31Z",
      "url": "https://github.com/django/django/pull/20059#discussion_r2492116915"
    },
    {
      "repo": "django/django",
      "pr_number": 20059,
      "file_path": "tests/admin_scripts/tests.py",
      "line": 179,
      "side": "RIGHT",
      "diff_hunk": "@@ -192,11 +167,18 @@ def run_manage(\n         with open(test_manage_py, \"w\") as fp:\n             fp.write(manage_py_contents)\n \n-        return self.run_test(\n-            [\"./manage.py\", *args],\n-            settings_file,\n-            discover_formatters=discover_formatters,\n-        )\n+        return self.run_test([\"./manage.py\", *args], settings_file)\n+\n+    def assertInAfterFormatting(self, member, container, msg=None):\n+        if HAS_BLACK:\n+            import black\n+\n+            # Black does not have a stable API, but this is still less\n+            # fragile than attempting to filter out containing paths.\n+            member = black.format_str(member, mode=black.FileMode())\n+            container = black.format_str(container, mode=black.FileMode())",
      "comment": "Given that, I kind of like `assertInAfterFormatting` or similar after all so that you don't end up with:\n```py\n            self.assertIn(\"class NewAppConfig(AppConfig)\", content)\n            self.assertInTemplatedFile(\"name = 'new_app'\", content)\n```",
      "comment_id": 2492128225,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-04T21:44:05Z",
      "url": "https://github.com/django/django/pull/20059#discussion_r2492128225"
    },
    {
      "repo": "django/django",
      "pr_number": 20059,
      "file_path": "tests/admin_scripts/tests.py",
      "line": 50,
      "side": "RIGHT",
      "diff_hunk": "@@ -49,6 +47,8 @@\n \n SYSTEM_CHECK_MSG = \"System check identified no issues\"\n \n+HAS_BLACK = shutil.which(\"black\")",
      "comment": "[Black is in the requirements.txt](https://github.com/django/django/blob/74564946c3b42a2ef7d087047e49873847a7e1d9/tests/requirements/py3.txt#L5) that the test instructions and CI jobs install. Unless you're explicitly wanting to support running tests in other contexts, I think the tests could assume black is installed and perhaps explicitly test the case where it doesn't by mocking `find_formatters()` to return `None`?\r\n\r\n(Just passing by after you mentioned this in my MR \u2014 I think your approach here to avoid changing the PATH is sensible!)",
      "comment_id": 2493634457,
      "user": "h4l",
      "created_at": "2025-11-05T09:18:58Z",
      "url": "https://github.com/django/django/pull/20059#discussion_r2493634457"
    },
    {
      "repo": "django/django",
      "pr_number": 20059,
      "file_path": "tests/admin_scripts/tests.py",
      "line": 50,
      "side": "RIGHT",
      "diff_hunk": "@@ -49,6 +47,8 @@\n \n SYSTEM_CHECK_MSG = \"System check identified no issues\"\n \n+HAS_BLACK = shutil.which(\"black\")",
      "comment": "Thank you @h4l for taking another look. Django explicitly support development environments (and Django general usage) without having `black` available, so we can't just assume `black` is installed.\n\nFor example, in our Jenkins CI we define one job that runs the tests without any external dependency (https://djangoci.com/job/main-no-requirements/).",
      "comment_id": 2510985184,
      "user": "nessita",
      "created_at": "2025-11-10T15:21:21Z",
      "url": "https://github.com/django/django/pull/20059#discussion_r2510985184"
    },
    {
      "repo": "django/django",
      "pr_number": 19366,
      "file_path": "django/core/handlers/asgi.py",
      "line": 202,
      "side": "RIGHT",
      "diff_hunk": "@@ -170,65 +170,41 @@ async def handle(self, scope, receive, send):\n             body_file = await self.read_body(receive)\n         except RequestAborted:\n             return\n-        # Request is complete and can be served.\n-        set_script_prefix(get_script_prefix(scope))\n-        await signals.request_started.asend(sender=self.__class__, scope=scope)\n-        # Get the request and check for basic issues.\n-        request, error_response = self.create_request(scope, body_file)\n-        if request is None:\n-            body_file.close()\n-            await self.send_response(error_response, send)\n-            await sync_to_async(error_response.close)()\n-            return\n \n-        async def process_request(request, send):\n-            response = await self.run_get_response(request)\n-            try:\n-                await self.send_response(response, send)\n-            except asyncio.CancelledError:\n-                # Client disconnected during send_response (ignore exception).\n+        with closing(body_file):\n+            # Request is complete and can be served.\n+            set_script_prefix(get_script_prefix(scope))\n+            await signals.request_started.asend(sender=self.__class__, scope=scope)\n+            # Get the request and check for basic issues.\n+            request, error_response = self.create_request(scope, body_file)\n+            if request is None:\n+                body_file.close()\n+                await self.send_response(error_response, send)\n+                await sync_to_async(error_response.close)()\n+                return\n+\n+            class RequestProcessed(Exception):\n                 pass\n \n-            return response\n-\n-        # Try to catch a disconnect while getting response.\n-        tasks = [\n-            # Check the status of these tasks and (optionally) terminate them\n-            # in this order. The listen_for_disconnect() task goes first\n-            # because it should not raise unexpected errors that would prevent\n-            # us from cancelling process_request().\n-            asyncio.create_task(self.listen_for_disconnect(receive)),\n-            asyncio.create_task(process_request(request, send)),\n-        ]\n-        await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)\n-        # Now wait on both tasks (they may have both finished by now).\n-        for task in tasks:\n-            if task.done():\n-                try:\n-                    task.result()\n-                except RequestAborted:\n-                    # Ignore client disconnects.\n-                    pass\n-                except AssertionError:\n-                    body_file.close()\n-                    raise\n-            else:\n-                # Allow views to handle cancellation.\n-                task.cancel()\n+            response = None\n+            try:\n                 try:\n-                    await task\n-                except asyncio.CancelledError:\n-                    # Task re-raised the CancelledError as expected.\n+                    async with asyncio.TaskGroup() as tg:\n+                        tg.create_task(self.listen_for_disconnect(receive))\n+                        response = await self.run_get_response(request)\n+                        await self.send_response(response, send)\n+                        raise RequestProcessed\n+                except* (RequestProcessed, RequestAborted):\n                     pass",
      "comment": "Wouldn't simply cancelling the background task achieve the same result, without triggering the exception handling mechanism. It would also make this code more readable, since it seems like the exception is being raised only to cancel the task.\r\n\r\n```python\r\n                    async with asyncio.TaskGroup() as tg:\r\n                        disconnect_task = tg.create_task(self.listen_for_disconnect(receive))\r\n                        response = await self.run_get_response(request)\r\n                        await self.send_response(response, send)\r\n                        disconnect_task.cancel()\r\n                except* RequestAborted:\r\n                    pass\r\n```",
      "comment_id": 2089932163,
      "user": "hartungstenio",
      "created_at": "2025-05-15T00:14:28Z",
      "url": "https://github.com/django/django/pull/19366#discussion_r2089932163"
    },
    {
      "repo": "django/django",
      "pr_number": 19366,
      "file_path": "django/core/handlers/asgi.py",
      "line": 202,
      "side": "RIGHT",
      "diff_hunk": "@@ -170,65 +170,41 @@ async def handle(self, scope, receive, send):\n             body_file = await self.read_body(receive)\n         except RequestAborted:\n             return\n-        # Request is complete and can be served.\n-        set_script_prefix(get_script_prefix(scope))\n-        await signals.request_started.asend(sender=self.__class__, scope=scope)\n-        # Get the request and check for basic issues.\n-        request, error_response = self.create_request(scope, body_file)\n-        if request is None:\n-            body_file.close()\n-            await self.send_response(error_response, send)\n-            await sync_to_async(error_response.close)()\n-            return\n \n-        async def process_request(request, send):\n-            response = await self.run_get_response(request)\n-            try:\n-                await self.send_response(response, send)\n-            except asyncio.CancelledError:\n-                # Client disconnected during send_response (ignore exception).\n+        with closing(body_file):\n+            # Request is complete and can be served.\n+            set_script_prefix(get_script_prefix(scope))\n+            await signals.request_started.asend(sender=self.__class__, scope=scope)\n+            # Get the request and check for basic issues.\n+            request, error_response = self.create_request(scope, body_file)\n+            if request is None:\n+                body_file.close()\n+                await self.send_response(error_response, send)\n+                await sync_to_async(error_response.close)()\n+                return\n+\n+            class RequestProcessed(Exception):\n                 pass\n \n-            return response\n-\n-        # Try to catch a disconnect while getting response.\n-        tasks = [\n-            # Check the status of these tasks and (optionally) terminate them\n-            # in this order. The listen_for_disconnect() task goes first\n-            # because it should not raise unexpected errors that would prevent\n-            # us from cancelling process_request().\n-            asyncio.create_task(self.listen_for_disconnect(receive)),\n-            asyncio.create_task(process_request(request, send)),\n-        ]\n-        await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)\n-        # Now wait on both tasks (they may have both finished by now).\n-        for task in tasks:\n-            if task.done():\n-                try:\n-                    task.result()\n-                except RequestAborted:\n-                    # Ignore client disconnects.\n-                    pass\n-                except AssertionError:\n-                    body_file.close()\n-                    raise\n-            else:\n-                # Allow views to handle cancellation.\n-                task.cancel()\n+            response = None\n+            try:\n                 try:\n-                    await task\n-                except asyncio.CancelledError:\n-                    # Task re-raised the CancelledError as expected.\n+                    async with asyncio.TaskGroup() as tg:\n+                        tg.create_task(self.listen_for_disconnect(receive))\n+                        response = await self.run_get_response(request)\n+                        await self.send_response(response, send)\n+                        raise RequestProcessed\n+                except* (RequestProcessed, RequestAborted):\n                     pass",
      "comment": "This is the recommended way to terminate a TaskGroup https://docs.python.org/3/library/asyncio-task.html#terminating-a-task-group",
      "comment_id": 2090087654,
      "user": "graingert",
      "created_at": "2025-05-15T02:30:44Z",
      "url": "https://github.com/django/django/pull/19366#discussion_r2090087654"
    },
    {
      "repo": "django/django",
      "pr_number": 19366,
      "file_path": "django/core/handlers/asgi.py",
      "line": 202,
      "side": "RIGHT",
      "diff_hunk": "@@ -170,65 +170,41 @@ async def handle(self, scope, receive, send):\n             body_file = await self.read_body(receive)\n         except RequestAborted:\n             return\n-        # Request is complete and can be served.\n-        set_script_prefix(get_script_prefix(scope))\n-        await signals.request_started.asend(sender=self.__class__, scope=scope)\n-        # Get the request and check for basic issues.\n-        request, error_response = self.create_request(scope, body_file)\n-        if request is None:\n-            body_file.close()\n-            await self.send_response(error_response, send)\n-            await sync_to_async(error_response.close)()\n-            return\n \n-        async def process_request(request, send):\n-            response = await self.run_get_response(request)\n-            try:\n-                await self.send_response(response, send)\n-            except asyncio.CancelledError:\n-                # Client disconnected during send_response (ignore exception).\n+        with closing(body_file):\n+            # Request is complete and can be served.\n+            set_script_prefix(get_script_prefix(scope))\n+            await signals.request_started.asend(sender=self.__class__, scope=scope)\n+            # Get the request and check for basic issues.\n+            request, error_response = self.create_request(scope, body_file)\n+            if request is None:\n+                body_file.close()\n+                await self.send_response(error_response, send)\n+                await sync_to_async(error_response.close)()\n+                return\n+\n+            class RequestProcessed(Exception):\n                 pass\n \n-            return response\n-\n-        # Try to catch a disconnect while getting response.\n-        tasks = [\n-            # Check the status of these tasks and (optionally) terminate them\n-            # in this order. The listen_for_disconnect() task goes first\n-            # because it should not raise unexpected errors that would prevent\n-            # us from cancelling process_request().\n-            asyncio.create_task(self.listen_for_disconnect(receive)),\n-            asyncio.create_task(process_request(request, send)),\n-        ]\n-        await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)\n-        # Now wait on both tasks (they may have both finished by now).\n-        for task in tasks:\n-            if task.done():\n-                try:\n-                    task.result()\n-                except RequestAborted:\n-                    # Ignore client disconnects.\n-                    pass\n-                except AssertionError:\n-                    body_file.close()\n-                    raise\n-            else:\n-                # Allow views to handle cancellation.\n-                task.cancel()\n+            response = None\n+            try:\n                 try:\n-                    await task\n-                except asyncio.CancelledError:\n-                    # Task re-raised the CancelledError as expected.\n+                    async with asyncio.TaskGroup() as tg:\n+                        tg.create_task(self.listen_for_disconnect(receive))\n+                        response = await self.run_get_response(request)\n+                        await self.send_response(response, send)\n+                        raise RequestProcessed\n+                except* (RequestProcessed, RequestAborted):\n                     pass",
      "comment": "The recommendation is to create an \"exception-raising task\".\r\n\r\nBut in this case, semantically, we don't want to end the TaskGroup (like, there are many tasks running, and want to finish them all), we want to stop listening for disconnections (which happens to be the only running task).",
      "comment_id": 2091086606,
      "user": "hartungstenio",
      "created_at": "2025-05-15T12:43:02Z",
      "url": "https://github.com/django/django/pull/19366#discussion_r2091086606"
    },
    {
      "repo": "django/django",
      "pr_number": 19366,
      "file_path": "django/core/handlers/asgi.py",
      "line": 185,
      "side": "RIGHT",
      "diff_hunk": "@@ -174,65 +174,41 @@ async def handle(self, scope, receive, send):\n             body_file = await self.read_body(receive)\n         except RequestAborted:\n             return\n-        # Request is complete and can be served.\n-        set_script_prefix(get_script_prefix(scope))\n-        await signals.request_started.asend(sender=self.__class__, scope=scope)\n-        # Get the request and check for basic issues.\n-        request, error_response = self.create_request(scope, body_file)\n-        if request is None:\n-            body_file.close()\n-            await self.send_response(error_response, send)\n-            await sync_to_async(error_response.close)()\n-            return\n \n-        async def process_request(request, send):\n-            response = await self.run_get_response(request)\n-            try:\n-                await self.send_response(response, send)\n-            except asyncio.CancelledError:\n-                # Client disconnected during send_response (ignore exception).\n+        with closing(body_file):\n+            # Request is complete and can be served.\n+            set_script_prefix(get_script_prefix(scope))\n+            await signals.request_started.asend(sender=self.__class__, scope=scope)\n+            # Get the request and check for basic issues.\n+            request, error_response = self.create_request(scope, body_file)\n+            if request is None:\n+                body_file.close()",
      "comment": "I want the body to close before waiting for send_response in this case ",
      "comment_id": 2504649015,
      "user": "graingert",
      "created_at": "2025-11-07T17:11:48Z",
      "url": "https://github.com/django/django/pull/19366#discussion_r2504649015"
    },
    {
      "repo": "django/django",
      "pr_number": 19366,
      "file_path": "django/core/handlers/asgi.py",
      "line": 201,
      "side": "RIGHT",
      "diff_hunk": "@@ -174,65 +174,41 @@ async def handle(self, scope, receive, send):\n             body_file = await self.read_body(receive)\n         except RequestAborted:\n             return\n-        # Request is complete and can be served.\n-        set_script_prefix(get_script_prefix(scope))\n-        await signals.request_started.asend(sender=self.__class__, scope=scope)\n-        # Get the request and check for basic issues.\n-        request, error_response = self.create_request(scope, body_file)\n-        if request is None:\n-            body_file.close()\n-            await self.send_response(error_response, send)\n-            await sync_to_async(error_response.close)()\n-            return\n \n-        async def process_request(request, send):\n-            response = await self.run_get_response(request)\n-            try:\n-                await self.send_response(response, send)\n-            except asyncio.CancelledError:\n-                # Client disconnected during send_response (ignore exception).\n+        with closing(body_file):\n+            # Request is complete and can be served.\n+            set_script_prefix(get_script_prefix(scope))\n+            await signals.request_started.asend(sender=self.__class__, scope=scope)\n+            # Get the request and check for basic issues.\n+            request, error_response = self.create_request(scope, body_file)\n+            if request is None:\n+                body_file.close()\n+                await self.send_response(error_response, send)\n+                await sync_to_async(error_response.close)()\n+                return\n+\n+            class RequestProcessed(Exception):\n                 pass\n \n-            return response\n-\n-        # Try to catch a disconnect while getting response.\n-        tasks = [\n-            # Check the status of these tasks and (optionally) terminate them\n-            # in this order. The listen_for_disconnect() task goes first\n-            # because it should not raise unexpected errors that would prevent\n-            # us from cancelling process_request().\n-            asyncio.create_task(self.listen_for_disconnect(receive)),\n-            asyncio.create_task(process_request(request, send)),\n-        ]\n-        await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)\n-        # Now wait on both tasks (they may have both finished by now).\n-        for task in tasks:\n-            if task.done():\n-                try:\n-                    task.result()\n-                except RequestAborted:\n-                    # Ignore client disconnects.\n-                    pass\n-                except AssertionError:\n-                    body_file.close()\n-                    raise\n-            else:\n-                # Allow views to handle cancellation.\n-                task.cancel()\n+            response = None\n+            try:\n                 try:\n-                    await task\n-                except asyncio.CancelledError:\n-                    # Task re-raised the CancelledError as expected.\n+                    async with asyncio.TaskGroup() as tg:\n+                        tg.create_task(self.listen_for_disconnect(receive))\n+                        response = await self.run_get_response(request)\n+                        await self.send_response(response, send)\n+                        raise RequestProcessed\n+                except* (RequestProcessed, RequestAborted):",
      "comment": "This CancelledError should be propagated because it comes from a manual cancel call unrelated to the TaskGroup. Generally you should not catch CancelledError and replace it unless you caused the cancellation ",
      "comment_id": 2505398546,
      "user": "graingert",
      "created_at": "2025-11-07T20:34:56Z",
      "url": "https://github.com/django/django/pull/19366#discussion_r2505398546"
    },
    {
      "repo": "django/django",
      "pr_number": 19366,
      "file_path": "django/core/handlers/asgi.py",
      "line": 201,
      "side": "RIGHT",
      "diff_hunk": "@@ -174,65 +174,41 @@ async def handle(self, scope, receive, send):\n             body_file = await self.read_body(receive)\n         except RequestAborted:\n             return\n-        # Request is complete and can be served.\n-        set_script_prefix(get_script_prefix(scope))\n-        await signals.request_started.asend(sender=self.__class__, scope=scope)\n-        # Get the request and check for basic issues.\n-        request, error_response = self.create_request(scope, body_file)\n-        if request is None:\n-            body_file.close()\n-            await self.send_response(error_response, send)\n-            await sync_to_async(error_response.close)()\n-            return\n \n-        async def process_request(request, send):\n-            response = await self.run_get_response(request)\n-            try:\n-                await self.send_response(response, send)\n-            except asyncio.CancelledError:\n-                # Client disconnected during send_response (ignore exception).\n+        with closing(body_file):\n+            # Request is complete and can be served.\n+            set_script_prefix(get_script_prefix(scope))\n+            await signals.request_started.asend(sender=self.__class__, scope=scope)\n+            # Get the request and check for basic issues.\n+            request, error_response = self.create_request(scope, body_file)\n+            if request is None:\n+                body_file.close()\n+                await self.send_response(error_response, send)\n+                await sync_to_async(error_response.close)()\n+                return\n+\n+            class RequestProcessed(Exception):\n                 pass\n \n-            return response\n-\n-        # Try to catch a disconnect while getting response.\n-        tasks = [\n-            # Check the status of these tasks and (optionally) terminate them\n-            # in this order. The listen_for_disconnect() task goes first\n-            # because it should not raise unexpected errors that would prevent\n-            # us from cancelling process_request().\n-            asyncio.create_task(self.listen_for_disconnect(receive)),\n-            asyncio.create_task(process_request(request, send)),\n-        ]\n-        await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)\n-        # Now wait on both tasks (they may have both finished by now).\n-        for task in tasks:\n-            if task.done():\n-                try:\n-                    task.result()\n-                except RequestAborted:\n-                    # Ignore client disconnects.\n-                    pass\n-                except AssertionError:\n-                    body_file.close()\n-                    raise\n-            else:\n-                # Allow views to handle cancellation.\n-                task.cancel()\n+            response = None\n+            try:\n                 try:\n-                    await task\n-                except asyncio.CancelledError:\n-                    # Task re-raised the CancelledError as expected.\n+                    async with asyncio.TaskGroup() as tg:\n+                        tg.create_task(self.listen_for_disconnect(receive))\n+                        response = await self.run_get_response(request)\n+                        await self.send_response(response, send)\n+                        raise RequestProcessed\n+                except* (RequestProcessed, RequestAborted):",
      "comment": "First of all, thank you for your MR. It will definitely help me in the future \ud83d\ude0c\r\n\r\nYou\u2019re right about the propagation. So, the better approach is probably to wrap everything in a `try/finally` block:\r\n\r\n```python \r\ntry:\r\n    ...\r\nfinally:\r\n    if response is None:\r\n        await signals.request_finished.asend(sender=self.__class__)\r\n    else:\r\n        await sync_to_async(response.close)()\r\n```\r\n\r\nThe `request_finished` signal is used for connection cleanup, and if I don\u2019t release the connection back to the pool, it could cause issues later.\r\n\r\nwhat do u think?",
      "comment_id": 2505601632,
      "user": "Arfey",
      "created_at": "2025-11-07T21:27:02Z",
      "url": "https://github.com/django/django/pull/19366#discussion_r2505601632"
    },
    {
      "repo": "django/django",
      "pr_number": 19366,
      "file_path": "django/core/handlers/asgi.py",
      "line": 201,
      "side": "RIGHT",
      "diff_hunk": "@@ -174,65 +174,41 @@ async def handle(self, scope, receive, send):\n             body_file = await self.read_body(receive)\n         except RequestAborted:\n             return\n-        # Request is complete and can be served.\n-        set_script_prefix(get_script_prefix(scope))\n-        await signals.request_started.asend(sender=self.__class__, scope=scope)\n-        # Get the request and check for basic issues.\n-        request, error_response = self.create_request(scope, body_file)\n-        if request is None:\n-            body_file.close()\n-            await self.send_response(error_response, send)\n-            await sync_to_async(error_response.close)()\n-            return\n \n-        async def process_request(request, send):\n-            response = await self.run_get_response(request)\n-            try:\n-                await self.send_response(response, send)\n-            except asyncio.CancelledError:\n-                # Client disconnected during send_response (ignore exception).\n+        with closing(body_file):\n+            # Request is complete and can be served.\n+            set_script_prefix(get_script_prefix(scope))\n+            await signals.request_started.asend(sender=self.__class__, scope=scope)\n+            # Get the request and check for basic issues.\n+            request, error_response = self.create_request(scope, body_file)\n+            if request is None:\n+                body_file.close()\n+                await self.send_response(error_response, send)\n+                await sync_to_async(error_response.close)()\n+                return\n+\n+            class RequestProcessed(Exception):\n                 pass\n \n-            return response\n-\n-        # Try to catch a disconnect while getting response.\n-        tasks = [\n-            # Check the status of these tasks and (optionally) terminate them\n-            # in this order. The listen_for_disconnect() task goes first\n-            # because it should not raise unexpected errors that would prevent\n-            # us from cancelling process_request().\n-            asyncio.create_task(self.listen_for_disconnect(receive)),\n-            asyncio.create_task(process_request(request, send)),\n-        ]\n-        await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)\n-        # Now wait on both tasks (they may have both finished by now).\n-        for task in tasks:\n-            if task.done():\n-                try:\n-                    task.result()\n-                except RequestAborted:\n-                    # Ignore client disconnects.\n-                    pass\n-                except AssertionError:\n-                    body_file.close()\n-                    raise\n-            else:\n-                # Allow views to handle cancellation.\n-                task.cancel()\n+            response = None\n+            try:\n                 try:\n-                    await task\n-                except asyncio.CancelledError:\n-                    # Task re-raised the CancelledError as expected.\n+                    async with asyncio.TaskGroup() as tg:\n+                        tg.create_task(self.listen_for_disconnect(receive))\n+                        response = await self.run_get_response(request)\n+                        await self.send_response(response, send)\n+                        raise RequestProcessed\n+                except* (RequestProcessed, RequestAborted):",
      "comment": "Oh I see yes this PR needs reverting and this try/finally applying",
      "comment_id": 2506875448,
      "user": "graingert",
      "created_at": "2025-11-08T12:00:36Z",
      "url": "https://github.com/django/django/pull/19366#discussion_r2506875448"
    },
    {
      "repo": "django/django",
      "pr_number": 19366,
      "file_path": "django/core/handlers/asgi.py",
      "line": 201,
      "side": "RIGHT",
      "diff_hunk": "@@ -174,65 +174,41 @@ async def handle(self, scope, receive, send):\n             body_file = await self.read_body(receive)\n         except RequestAborted:\n             return\n-        # Request is complete and can be served.\n-        set_script_prefix(get_script_prefix(scope))\n-        await signals.request_started.asend(sender=self.__class__, scope=scope)\n-        # Get the request and check for basic issues.\n-        request, error_response = self.create_request(scope, body_file)\n-        if request is None:\n-            body_file.close()\n-            await self.send_response(error_response, send)\n-            await sync_to_async(error_response.close)()\n-            return\n \n-        async def process_request(request, send):\n-            response = await self.run_get_response(request)\n-            try:\n-                await self.send_response(response, send)\n-            except asyncio.CancelledError:\n-                # Client disconnected during send_response (ignore exception).\n+        with closing(body_file):\n+            # Request is complete and can be served.\n+            set_script_prefix(get_script_prefix(scope))\n+            await signals.request_started.asend(sender=self.__class__, scope=scope)\n+            # Get the request and check for basic issues.\n+            request, error_response = self.create_request(scope, body_file)\n+            if request is None:\n+                body_file.close()\n+                await self.send_response(error_response, send)\n+                await sync_to_async(error_response.close)()\n+                return\n+\n+            class RequestProcessed(Exception):\n                 pass\n \n-            return response\n-\n-        # Try to catch a disconnect while getting response.\n-        tasks = [\n-            # Check the status of these tasks and (optionally) terminate them\n-            # in this order. The listen_for_disconnect() task goes first\n-            # because it should not raise unexpected errors that would prevent\n-            # us from cancelling process_request().\n-            asyncio.create_task(self.listen_for_disconnect(receive)),\n-            asyncio.create_task(process_request(request, send)),\n-        ]\n-        await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)\n-        # Now wait on both tasks (they may have both finished by now).\n-        for task in tasks:\n-            if task.done():\n-                try:\n-                    task.result()\n-                except RequestAborted:\n-                    # Ignore client disconnects.\n-                    pass\n-                except AssertionError:\n-                    body_file.close()\n-                    raise\n-            else:\n-                # Allow views to handle cancellation.\n-                task.cancel()\n+            response = None\n+            try:\n                 try:\n-                    await task\n-                except asyncio.CancelledError:\n-                    # Task re-raised the CancelledError as expected.\n+                    async with asyncio.TaskGroup() as tg:\n+                        tg.create_task(self.listen_for_disconnect(receive))\n+                        response = await self.run_get_response(request)\n+                        await self.send_response(response, send)\n+                        raise RequestProcessed\n+                except* (RequestProcessed, RequestAborted):",
      "comment": "Hmmm maybe not, I don't think external cancellation should fire the request_finished hook, @carltongibson thoughts?",
      "comment_id": 2506882555,
      "user": "graingert",
      "created_at": "2025-11-08T12:19:48Z",
      "url": "https://github.com/django/django/pull/19366#discussion_r2506882555"
    },
    {
      "repo": "django/django",
      "pr_number": 19366,
      "file_path": "django/core/handlers/asgi.py",
      "line": 201,
      "side": "RIGHT",
      "diff_hunk": "@@ -174,65 +174,41 @@ async def handle(self, scope, receive, send):\n             body_file = await self.read_body(receive)\n         except RequestAborted:\n             return\n-        # Request is complete and can be served.\n-        set_script_prefix(get_script_prefix(scope))\n-        await signals.request_started.asend(sender=self.__class__, scope=scope)\n-        # Get the request and check for basic issues.\n-        request, error_response = self.create_request(scope, body_file)\n-        if request is None:\n-            body_file.close()\n-            await self.send_response(error_response, send)\n-            await sync_to_async(error_response.close)()\n-            return\n \n-        async def process_request(request, send):\n-            response = await self.run_get_response(request)\n-            try:\n-                await self.send_response(response, send)\n-            except asyncio.CancelledError:\n-                # Client disconnected during send_response (ignore exception).\n+        with closing(body_file):\n+            # Request is complete and can be served.\n+            set_script_prefix(get_script_prefix(scope))\n+            await signals.request_started.asend(sender=self.__class__, scope=scope)\n+            # Get the request and check for basic issues.\n+            request, error_response = self.create_request(scope, body_file)\n+            if request is None:\n+                body_file.close()\n+                await self.send_response(error_response, send)\n+                await sync_to_async(error_response.close)()\n+                return\n+\n+            class RequestProcessed(Exception):\n                 pass\n \n-            return response\n-\n-        # Try to catch a disconnect while getting response.\n-        tasks = [\n-            # Check the status of these tasks and (optionally) terminate them\n-            # in this order. The listen_for_disconnect() task goes first\n-            # because it should not raise unexpected errors that would prevent\n-            # us from cancelling process_request().\n-            asyncio.create_task(self.listen_for_disconnect(receive)),\n-            asyncio.create_task(process_request(request, send)),\n-        ]\n-        await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)\n-        # Now wait on both tasks (they may have both finished by now).\n-        for task in tasks:\n-            if task.done():\n-                try:\n-                    task.result()\n-                except RequestAborted:\n-                    # Ignore client disconnects.\n-                    pass\n-                except AssertionError:\n-                    body_file.close()\n-                    raise\n-            else:\n-                # Allow views to handle cancellation.\n-                task.cancel()\n+            response = None\n+            try:\n                 try:\n-                    await task\n-                except asyncio.CancelledError:\n-                    # Task re-raised the CancelledError as expected.\n+                    async with asyncio.TaskGroup() as tg:\n+                        tg.create_task(self.listen_for_disconnect(receive))\n+                        response = await self.run_get_response(request)\n+                        await self.send_response(response, send)\n+                        raise RequestProcessed\n+                except* (RequestProcessed, RequestAborted):",
      "comment": "Yes, so we do want to fire request finished here. The client disconnected rather than completing the request, but we still need to clean up. \n\nI need to look at the tests here. The request finished signal not firing should trigger a failure. \ud83e\udd14 ",
      "comment_id": 2506906529,
      "user": "carltongibson",
      "created_at": "2025-11-08T13:00:19Z",
      "url": "https://github.com/django/django/pull/19366#discussion_r2506906529"
    },
    {
      "repo": "django/django",
      "pr_number": 19366,
      "file_path": "django/core/handlers/asgi.py",
      "line": 201,
      "side": "RIGHT",
      "diff_hunk": "@@ -174,65 +174,41 @@ async def handle(self, scope, receive, send):\n             body_file = await self.read_body(receive)\n         except RequestAborted:\n             return\n-        # Request is complete and can be served.\n-        set_script_prefix(get_script_prefix(scope))\n-        await signals.request_started.asend(sender=self.__class__, scope=scope)\n-        # Get the request and check for basic issues.\n-        request, error_response = self.create_request(scope, body_file)\n-        if request is None:\n-            body_file.close()\n-            await self.send_response(error_response, send)\n-            await sync_to_async(error_response.close)()\n-            return\n \n-        async def process_request(request, send):\n-            response = await self.run_get_response(request)\n-            try:\n-                await self.send_response(response, send)\n-            except asyncio.CancelledError:\n-                # Client disconnected during send_response (ignore exception).\n+        with closing(body_file):\n+            # Request is complete and can be served.\n+            set_script_prefix(get_script_prefix(scope))\n+            await signals.request_started.asend(sender=self.__class__, scope=scope)\n+            # Get the request and check for basic issues.\n+            request, error_response = self.create_request(scope, body_file)\n+            if request is None:\n+                body_file.close()\n+                await self.send_response(error_response, send)\n+                await sync_to_async(error_response.close)()\n+                return\n+\n+            class RequestProcessed(Exception):\n                 pass\n \n-            return response\n-\n-        # Try to catch a disconnect while getting response.\n-        tasks = [\n-            # Check the status of these tasks and (optionally) terminate them\n-            # in this order. The listen_for_disconnect() task goes first\n-            # because it should not raise unexpected errors that would prevent\n-            # us from cancelling process_request().\n-            asyncio.create_task(self.listen_for_disconnect(receive)),\n-            asyncio.create_task(process_request(request, send)),\n-        ]\n-        await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)\n-        # Now wait on both tasks (they may have both finished by now).\n-        for task in tasks:\n-            if task.done():\n-                try:\n-                    task.result()\n-                except RequestAborted:\n-                    # Ignore client disconnects.\n-                    pass\n-                except AssertionError:\n-                    body_file.close()\n-                    raise\n-            else:\n-                # Allow views to handle cancellation.\n-                task.cancel()\n+            response = None\n+            try:\n                 try:\n-                    await task\n-                except asyncio.CancelledError:\n-                    # Task re-raised the CancelledError as expected.\n+                    async with asyncio.TaskGroup() as tg:\n+                        tg.create_task(self.listen_for_disconnect(receive))\n+                        response = await self.run_get_response(request)\n+                        await self.send_response(response, send)\n+                        raise RequestProcessed\n+                except* (RequestProcessed, RequestAborted):",
      "comment": "I don't think we should revert the current MR (it doesn't break the existing codebase), but we need to find a proper approach for handling the cleanup. We can move this discussion elsewhere.",
      "comment_id": 2506952926,
      "user": "Arfey",
      "created_at": "2025-11-08T14:25:45Z",
      "url": "https://github.com/django/django/pull/19366#discussion_r2506952926"
    },
    {
      "repo": "django/django",
      "pr_number": 19366,
      "file_path": "django/core/handlers/asgi.py",
      "line": 201,
      "side": "RIGHT",
      "diff_hunk": "@@ -174,65 +174,41 @@ async def handle(self, scope, receive, send):\n             body_file = await self.read_body(receive)\n         except RequestAborted:\n             return\n-        # Request is complete and can be served.\n-        set_script_prefix(get_script_prefix(scope))\n-        await signals.request_started.asend(sender=self.__class__, scope=scope)\n-        # Get the request and check for basic issues.\n-        request, error_response = self.create_request(scope, body_file)\n-        if request is None:\n-            body_file.close()\n-            await self.send_response(error_response, send)\n-            await sync_to_async(error_response.close)()\n-            return\n \n-        async def process_request(request, send):\n-            response = await self.run_get_response(request)\n-            try:\n-                await self.send_response(response, send)\n-            except asyncio.CancelledError:\n-                # Client disconnected during send_response (ignore exception).\n+        with closing(body_file):\n+            # Request is complete and can be served.\n+            set_script_prefix(get_script_prefix(scope))\n+            await signals.request_started.asend(sender=self.__class__, scope=scope)\n+            # Get the request and check for basic issues.\n+            request, error_response = self.create_request(scope, body_file)\n+            if request is None:\n+                body_file.close()\n+                await self.send_response(error_response, send)\n+                await sync_to_async(error_response.close)()\n+                return\n+\n+            class RequestProcessed(Exception):\n                 pass\n \n-            return response\n-\n-        # Try to catch a disconnect while getting response.\n-        tasks = [\n-            # Check the status of these tasks and (optionally) terminate them\n-            # in this order. The listen_for_disconnect() task goes first\n-            # because it should not raise unexpected errors that would prevent\n-            # us from cancelling process_request().\n-            asyncio.create_task(self.listen_for_disconnect(receive)),\n-            asyncio.create_task(process_request(request, send)),\n-        ]\n-        await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)\n-        # Now wait on both tasks (they may have both finished by now).\n-        for task in tasks:\n-            if task.done():\n-                try:\n-                    task.result()\n-                except RequestAborted:\n-                    # Ignore client disconnects.\n-                    pass\n-                except AssertionError:\n-                    body_file.close()\n-                    raise\n-            else:\n-                # Allow views to handle cancellation.\n-                task.cancel()\n+            response = None\n+            try:\n                 try:\n-                    await task\n-                except asyncio.CancelledError:\n-                    # Task re-raised the CancelledError as expected.\n+                    async with asyncio.TaskGroup() as tg:\n+                        tg.create_task(self.listen_for_disconnect(receive))\n+                        response = await self.run_get_response(request)\n+                        await self.send_response(response, send)\n+                        raise RequestProcessed\n+                except* (RequestProcessed, RequestAborted):",
      "comment": "OK, so the signals.request_finished.asend is **still hit** on the tests checking cancellation. \r\n\r\ne.g. see `asgi.tests.ASGITest.test_asyncio_cancel_error`.\r\n\r\n@Arfey If you could add a test case demonstrating a change in behaviour then we can certainly tweak. ",
      "comment_id": 2506966218,
      "user": "carltongibson",
      "created_at": "2025-11-08T14:58:47Z",
      "url": "https://github.com/django/django/pull/19366#discussion_r2506966218"
    },
    {
      "repo": "django/django",
      "pr_number": 19366,
      "file_path": "django/core/handlers/asgi.py",
      "line": 201,
      "side": "RIGHT",
      "diff_hunk": "@@ -174,65 +174,41 @@ async def handle(self, scope, receive, send):\n             body_file = await self.read_body(receive)\n         except RequestAborted:\n             return\n-        # Request is complete and can be served.\n-        set_script_prefix(get_script_prefix(scope))\n-        await signals.request_started.asend(sender=self.__class__, scope=scope)\n-        # Get the request and check for basic issues.\n-        request, error_response = self.create_request(scope, body_file)\n-        if request is None:\n-            body_file.close()\n-            await self.send_response(error_response, send)\n-            await sync_to_async(error_response.close)()\n-            return\n \n-        async def process_request(request, send):\n-            response = await self.run_get_response(request)\n-            try:\n-                await self.send_response(response, send)\n-            except asyncio.CancelledError:\n-                # Client disconnected during send_response (ignore exception).\n+        with closing(body_file):\n+            # Request is complete and can be served.\n+            set_script_prefix(get_script_prefix(scope))\n+            await signals.request_started.asend(sender=self.__class__, scope=scope)\n+            # Get the request and check for basic issues.\n+            request, error_response = self.create_request(scope, body_file)\n+            if request is None:\n+                body_file.close()\n+                await self.send_response(error_response, send)\n+                await sync_to_async(error_response.close)()\n+                return\n+\n+            class RequestProcessed(Exception):\n                 pass\n \n-            return response\n-\n-        # Try to catch a disconnect while getting response.\n-        tasks = [\n-            # Check the status of these tasks and (optionally) terminate them\n-            # in this order. The listen_for_disconnect() task goes first\n-            # because it should not raise unexpected errors that would prevent\n-            # us from cancelling process_request().\n-            asyncio.create_task(self.listen_for_disconnect(receive)),\n-            asyncio.create_task(process_request(request, send)),\n-        ]\n-        await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)\n-        # Now wait on both tasks (they may have both finished by now).\n-        for task in tasks:\n-            if task.done():\n-                try:\n-                    task.result()\n-                except RequestAborted:\n-                    # Ignore client disconnects.\n-                    pass\n-                except AssertionError:\n-                    body_file.close()\n-                    raise\n-            else:\n-                # Allow views to handle cancellation.\n-                task.cancel()\n+            response = None\n+            try:\n                 try:\n-                    await task\n-                except asyncio.CancelledError:\n-                    # Task re-raised the CancelledError as expected.\n+                    async with asyncio.TaskGroup() as tg:\n+                        tg.create_task(self.listen_for_disconnect(receive))\n+                        response = await self.run_get_response(request)\n+                        await self.send_response(response, send)\n+                        raise RequestProcessed\n+                except* (RequestProcessed, RequestAborted):",
      "comment": "Thanks.\r\n\r\nThe `asgi.tests.ASGITest.test_asyncio_cancel_error` covers two cases:\r\n- The request cycle completes normally when no disconnect is sent.\r\n- The request cycle is interrupted when a disconnect occurs before the view responds.\r\n\r\nSo, it tests cancellation caused by a disconnection event. However, we discovered a potential cleanup issue if something higher up cancels `handle_request`.\r\n\r\n>  demonstrating a change in behaviour then we can certainly tweak.\r\n\r\nThe behaviour remains the same. We already had the same potential issue in the previous version of the code.\r\n\r\n> If you could add a test case\r\n\r\nSure",
      "comment_id": 2507328215,
      "user": "Arfey",
      "created_at": "2025-11-08T22:32:50Z",
      "url": "https://github.com/django/django/pull/19366#discussion_r2507328215"
    },
    {
      "repo": "django/django",
      "pr_number": 19366,
      "file_path": "django/core/handlers/asgi.py",
      "line": 201,
      "side": "RIGHT",
      "diff_hunk": "@@ -174,65 +174,41 @@ async def handle(self, scope, receive, send):\n             body_file = await self.read_body(receive)\n         except RequestAborted:\n             return\n-        # Request is complete and can be served.\n-        set_script_prefix(get_script_prefix(scope))\n-        await signals.request_started.asend(sender=self.__class__, scope=scope)\n-        # Get the request and check for basic issues.\n-        request, error_response = self.create_request(scope, body_file)\n-        if request is None:\n-            body_file.close()\n-            await self.send_response(error_response, send)\n-            await sync_to_async(error_response.close)()\n-            return\n \n-        async def process_request(request, send):\n-            response = await self.run_get_response(request)\n-            try:\n-                await self.send_response(response, send)\n-            except asyncio.CancelledError:\n-                # Client disconnected during send_response (ignore exception).\n+        with closing(body_file):\n+            # Request is complete and can be served.\n+            set_script_prefix(get_script_prefix(scope))\n+            await signals.request_started.asend(sender=self.__class__, scope=scope)\n+            # Get the request and check for basic issues.\n+            request, error_response = self.create_request(scope, body_file)\n+            if request is None:\n+                body_file.close()\n+                await self.send_response(error_response, send)\n+                await sync_to_async(error_response.close)()\n+                return\n+\n+            class RequestProcessed(Exception):\n                 pass\n \n-            return response\n-\n-        # Try to catch a disconnect while getting response.\n-        tasks = [\n-            # Check the status of these tasks and (optionally) terminate them\n-            # in this order. The listen_for_disconnect() task goes first\n-            # because it should not raise unexpected errors that would prevent\n-            # us from cancelling process_request().\n-            asyncio.create_task(self.listen_for_disconnect(receive)),\n-            asyncio.create_task(process_request(request, send)),\n-        ]\n-        await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)\n-        # Now wait on both tasks (they may have both finished by now).\n-        for task in tasks:\n-            if task.done():\n-                try:\n-                    task.result()\n-                except RequestAborted:\n-                    # Ignore client disconnects.\n-                    pass\n-                except AssertionError:\n-                    body_file.close()\n-                    raise\n-            else:\n-                # Allow views to handle cancellation.\n-                task.cancel()\n+            response = None\n+            try:\n                 try:\n-                    await task\n-                except asyncio.CancelledError:\n-                    # Task re-raised the CancelledError as expected.\n+                    async with asyncio.TaskGroup() as tg:\n+                        tg.create_task(self.listen_for_disconnect(receive))\n+                        response = await self.run_get_response(request)\n+                        await self.send_response(response, send)\n+                        raise RequestProcessed\n+                except* (RequestProcessed, RequestAborted):",
      "comment": "https://github.com/Arfey/django/pull/2/files\r\n\r\nIt's not a problem for now, since we don't use async database connections. The threaded version doesn't have this issue, as it handles connection closing both before and after each request. But for pure async connections, it could become an issue.",
      "comment_id": 2507350483,
      "user": "Arfey",
      "created_at": "2025-11-08T23:12:18Z",
      "url": "https://github.com/django/django/pull/19366#discussion_r2507350483"
    },
    {
      "repo": "django/django",
      "pr_number": 19366,
      "file_path": "django/core/handlers/asgi.py",
      "line": 201,
      "side": "RIGHT",
      "diff_hunk": "@@ -174,65 +174,41 @@ async def handle(self, scope, receive, send):\n             body_file = await self.read_body(receive)\n         except RequestAborted:\n             return\n-        # Request is complete and can be served.\n-        set_script_prefix(get_script_prefix(scope))\n-        await signals.request_started.asend(sender=self.__class__, scope=scope)\n-        # Get the request and check for basic issues.\n-        request, error_response = self.create_request(scope, body_file)\n-        if request is None:\n-            body_file.close()\n-            await self.send_response(error_response, send)\n-            await sync_to_async(error_response.close)()\n-            return\n \n-        async def process_request(request, send):\n-            response = await self.run_get_response(request)\n-            try:\n-                await self.send_response(response, send)\n-            except asyncio.CancelledError:\n-                # Client disconnected during send_response (ignore exception).\n+        with closing(body_file):\n+            # Request is complete and can be served.\n+            set_script_prefix(get_script_prefix(scope))\n+            await signals.request_started.asend(sender=self.__class__, scope=scope)\n+            # Get the request and check for basic issues.\n+            request, error_response = self.create_request(scope, body_file)\n+            if request is None:\n+                body_file.close()\n+                await self.send_response(error_response, send)\n+                await sync_to_async(error_response.close)()\n+                return\n+\n+            class RequestProcessed(Exception):\n                 pass\n \n-            return response\n-\n-        # Try to catch a disconnect while getting response.\n-        tasks = [\n-            # Check the status of these tasks and (optionally) terminate them\n-            # in this order. The listen_for_disconnect() task goes first\n-            # because it should not raise unexpected errors that would prevent\n-            # us from cancelling process_request().\n-            asyncio.create_task(self.listen_for_disconnect(receive)),\n-            asyncio.create_task(process_request(request, send)),\n-        ]\n-        await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)\n-        # Now wait on both tasks (they may have both finished by now).\n-        for task in tasks:\n-            if task.done():\n-                try:\n-                    task.result()\n-                except RequestAborted:\n-                    # Ignore client disconnects.\n-                    pass\n-                except AssertionError:\n-                    body_file.close()\n-                    raise\n-            else:\n-                # Allow views to handle cancellation.\n-                task.cancel()\n+            response = None\n+            try:\n                 try:\n-                    await task\n-                except asyncio.CancelledError:\n-                    # Task re-raised the CancelledError as expected.\n+                    async with asyncio.TaskGroup() as tg:\n+                        tg.create_task(self.listen_for_disconnect(receive))\n+                        response = await self.run_get_response(request)\n+                        await self.send_response(response, send)\n+                        raise RequestProcessed\n+                except* (RequestProcessed, RequestAborted):",
      "comment": "Great. I'll look at that. Thanks. \n\nNot sure how such a case would come up currently. We can likely discuss elsewhere as you say. ",
      "comment_id": 2507617647,
      "user": "carltongibson",
      "created_at": "2025-11-09T05:41:05Z",
      "url": "https://github.com/django/django/pull/19366#discussion_r2507617647"
    },
    {
      "repo": "django/django",
      "pr_number": 19517,
      "file_path": "django/db/backends/sqlite3/operations.py",
      "line": 160,
      "side": "RIGHT",
      "diff_hunk": "@@ -155,16 +156,15 @@ def _quote_params_for_last_executed_query(self, params):\n         \"\"\"\n         Only for last_executed_query! Don't use this to execute SQL queries!\n         \"\"\"\n-        # This function is limited both by SQLITE_LIMIT_VARIABLE_NUMBER (the\n-        # number of parameters, default = 999) and SQLITE_MAX_COLUMN (the\n-        # number of return values, default = 2000). Since Python's sqlite3\n-        # module doesn't expose the get_limit() C API, assume the default\n-        # limits are in effect and split the work in batches if needed.\n-        BATCH_SIZE = 999\n-        if len(params) > BATCH_SIZE:\n+        connection = self.connection.connection\n+        variable_limit = connection.getlimit(sqlite3.SQLITE_LIMIT_VARIABLE_NUMBER)",
      "comment": "This is now available as `self.connection.features.max_query_params` as of #19427.\r\n\r\nhttps://github.com/django/django/blob/26313bc21932d0d3af278ab387549d63b1f64575/django/db/backends/sqlite3/features.py#L144-L152\r\n\r\n```suggestion\r\n        variable_limit = self.connection.features.max_query_params\r\n```",
      "comment_id": 2124371771,
      "user": "laymonage",
      "created_at": "2025-06-03T16:24:54Z",
      "url": "https://github.com/django/django/pull/19517#discussion_r2124371771"
    },
    {
      "repo": "django/django",
      "pr_number": 19517,
      "file_path": "django/db/backends/sqlite3/operations.py",
      "line": 160,
      "side": "RIGHT",
      "diff_hunk": "@@ -155,16 +156,15 @@ def _quote_params_for_last_executed_query(self, params):\n         \"\"\"\n         Only for last_executed_query! Don't use this to execute SQL queries!\n         \"\"\"\n-        # This function is limited both by SQLITE_LIMIT_VARIABLE_NUMBER (the\n-        # number of parameters, default = 999) and SQLITE_MAX_COLUMN (the\n-        # number of return values, default = 2000). Since Python's sqlite3\n-        # module doesn't expose the get_limit() C API, assume the default\n-        # limits are in effect and split the work in batches if needed.\n-        BATCH_SIZE = 999\n-        if len(params) > BATCH_SIZE:\n+        connection = self.connection.connection\n+        variable_limit = connection.getlimit(sqlite3.SQLITE_LIMIT_VARIABLE_NUMBER)",
      "comment": "Thanks for pointing this out! I considered updating column_limit to use the same pattern (self.connection.features.max_columns), but decided to keep this PR focused on the immediate fix. ",
      "comment_id": 2128935590,
      "user": "myoungjinGo-BE",
      "created_at": "2025-06-05T14:00:41Z",
      "url": "https://github.com/django/django/pull/19517#discussion_r2128935590"
    },
    {
      "repo": "django/django",
      "pr_number": 20041,
      "file_path": "tests/bulk_create/tests.py",
      "line": 894,
      "side": "RIGHT",
      "diff_hunk": "@@ -884,6 +884,13 @@ def test_db_default_primary_key(self):\n         (obj,) = DbDefaultPrimaryKey.objects.bulk_create([DbDefaultPrimaryKey()])\n         self.assertIsInstance(obj.id, datetime)\n \n+    @skipUnlessDBFeature(\"can_return_rows_from_bulk_insert\")\n+    def test_db_expression_primary_key(self):\n+        (obj,) = DbDefaultPrimaryKey.objects.bulk_create(\n+            [DbDefaultPrimaryKey(id=Now())]\n+        )\n+        self.assertIsInstance(obj.id, datetime)",
      "comment": "Use `State` instead as the `DbDefaultPrimaryKey` model will only be created on backends that `supports_expression_defaults`.\r\n\r\n```suggestion\r\n    def test_db_expression_primary_key(self):\r\n        (obj,) = State.objects.bulk_create(\r\n            [State(two_letter_code=Value(\"CA\"))]\r\n        )\r\n        self.assertIsInstance(obj.two_letter_code, str)\r\n```",
      "comment_id": 2486575052,
      "user": "charettes",
      "created_at": "2025-11-03T13:55:46Z",
      "url": "https://github.com/django/django/pull/20041#discussion_r2486575052"
    },
    {
      "repo": "django/django",
      "pr_number": 20041,
      "file_path": "tests/bulk_create/tests.py",
      "line": 894,
      "side": "RIGHT",
      "diff_hunk": "@@ -884,6 +884,13 @@ def test_db_default_primary_key(self):\n         (obj,) = DbDefaultPrimaryKey.objects.bulk_create([DbDefaultPrimaryKey()])\n         self.assertIsInstance(obj.id, datetime)\n \n+    @skipUnlessDBFeature(\"can_return_rows_from_bulk_insert\")\n+    def test_db_expression_primary_key(self):\n+        (obj,) = DbDefaultPrimaryKey.objects.bulk_create(\n+            [DbDefaultPrimaryKey(id=Now())]\n+        )\n+        self.assertIsInstance(obj.id, datetime)",
      "comment": "With your suggested test my fix doesn't work, because `two_letter_code` is not in `State._meta.db_returning_fields` because it doesn't have a `db_default`.",
      "comment_id": 2486612576,
      "user": "us77ipis",
      "created_at": "2025-11-03T14:08:08Z",
      "url": "https://github.com/django/django/pull/20041#discussion_r2486612576"
    },
    {
      "repo": "django/django",
      "pr_number": 20041,
      "file_path": "tests/bulk_create/tests.py",
      "line": 894,
      "side": "RIGHT",
      "diff_hunk": "@@ -884,6 +884,13 @@ def test_db_default_primary_key(self):\n         (obj,) = DbDefaultPrimaryKey.objects.bulk_create([DbDefaultPrimaryKey()])\n         self.assertIsInstance(obj.id, datetime)\n \n+    @skipUnlessDBFeature(\"can_return_rows_from_bulk_insert\")\n+    def test_db_expression_primary_key(self):\n+        (obj,) = DbDefaultPrimaryKey.objects.bulk_create(\n+            [DbDefaultPrimaryKey(id=Now())]\n+        )\n+        self.assertIsInstance(obj.id, datetime)",
      "comment": "I see, so the follow up will address this case as well.\r\n\r\nMake sure to include `\"supports_expression_defaults\"` in the `skipUnlessDBFeature` instead then.",
      "comment_id": 2486620653,
      "user": "charettes",
      "created_at": "2025-11-03T14:10:48Z",
      "url": "https://github.com/django/django/pull/20041#discussion_r2486620653"
    },
    {
      "repo": "django/django",
      "pr_number": 20041,
      "file_path": "tests/bulk_create/tests.py",
      "line": 894,
      "side": "RIGHT",
      "diff_hunk": "@@ -884,6 +884,13 @@ def test_db_default_primary_key(self):\n         (obj,) = DbDefaultPrimaryKey.objects.bulk_create([DbDefaultPrimaryKey()])\n         self.assertIsInstance(obj.id, datetime)\n \n+    @skipUnlessDBFeature(\"can_return_rows_from_bulk_insert\")\n+    def test_db_expression_primary_key(self):\n+        (obj,) = DbDefaultPrimaryKey.objects.bulk_create(\n+            [DbDefaultPrimaryKey(id=Now())]\n+        )\n+        self.assertIsInstance(obj.id, datetime)",
      "comment": "Fixing the issue for the more general case would require to allow entirely overriding or at least specifying additional returning fields through an additional parameter of `bulk_create`. This way also expressions specified for other non-primary key fields could easily be returned.",
      "comment_id": 2486684981,
      "user": "us77ipis",
      "created_at": "2025-11-03T14:29:56Z",
      "url": "https://github.com/django/django/pull/20041#discussion_r2486684981"
    },
    {
      "repo": "django/django",
      "pr_number": 20041,
      "file_path": "tests/bulk_create/tests.py",
      "line": 894,
      "side": "RIGHT",
      "diff_hunk": "@@ -884,6 +884,13 @@ def test_db_default_primary_key(self):\n         (obj,) = DbDefaultPrimaryKey.objects.bulk_create([DbDefaultPrimaryKey()])\n         self.assertIsInstance(obj.id, datetime)\n \n+    @skipUnlessDBFeature(\"can_return_rows_from_bulk_insert\")\n+    def test_db_expression_primary_key(self):\n+        (obj,) = DbDefaultPrimaryKey.objects.bulk_create(\n+            [DbDefaultPrimaryKey(id=Now())]\n+        )\n+        self.assertIsInstance(obj.id, datetime)",
      "comment": "> This way also expressions specified for other non-primary key fields could easily be returned.\r\n\r\nI think there might be a way to efficiently flag all fields that have resolvable/compilable values assigned to them and automatically augment `returning_fields` with them so it can be done implicitly.\r\n\r\nThat's [the approach we've recently taken with `Model.save` for example](https://github.com/django/django/commit/94680437a45a71c70ca8bd2e68b72aa1e2eff337#diff-1c8b882c73bfda668d5451d4578c97191b0ebc0f088d0c0ba2296ab89b428c44R1105-R1158).",
      "comment_id": 2486867339,
      "user": "charettes",
      "created_at": "2025-11-03T15:23:17Z",
      "url": "https://github.com/django/django/pull/20041#discussion_r2486867339"
    },
    {
      "repo": "django/django",
      "pr_number": 20041,
      "file_path": "tests/bulk_create/tests.py",
      "line": 894,
      "side": "RIGHT",
      "diff_hunk": "@@ -884,6 +884,13 @@ def test_db_default_primary_key(self):\n         (obj,) = DbDefaultPrimaryKey.objects.bulk_create([DbDefaultPrimaryKey()])\n         self.assertIsInstance(obj.id, datetime)\n \n+    @skipUnlessDBFeature(\"can_return_rows_from_bulk_insert\")\n+    def test_db_expression_primary_key(self):\n+        (obj,) = DbDefaultPrimaryKey.objects.bulk_create(\n+            [DbDefaultPrimaryKey(id=Now())]\n+        )\n+        self.assertIsInstance(obj.id, datetime)",
      "comment": "That would be smart. Still a worst case scenario could be a `bulk_create` call with many objects on a model that has many fields, most of them specified as expressions, none of which are actually needed afterwards. It would be both unnecessary to look at all field values for all objects to check if they are expressions, and also to fetch all values again from the db. For such a case it would still be nice to override the returning fields manually to regain the little extra performance.",
      "comment_id": 2487007143,
      "user": "us77ipis",
      "created_at": "2025-11-03T16:00:23Z",
      "url": "https://github.com/django/django/pull/20041#discussion_r2487007143"
    },
    {
      "repo": "django/django",
      "pr_number": 20041,
      "file_path": "tests/bulk_create/tests.py",
      "line": 894,
      "side": "RIGHT",
      "diff_hunk": "@@ -884,6 +884,13 @@ def test_db_default_primary_key(self):\n         (obj,) = DbDefaultPrimaryKey.objects.bulk_create([DbDefaultPrimaryKey()])\n         self.assertIsInstance(obj.id, datetime)\n \n+    @skipUnlessDBFeature(\"can_return_rows_from_bulk_insert\")\n+    def test_db_expression_primary_key(self):\n+        (obj,) = DbDefaultPrimaryKey.objects.bulk_create(\n+            [DbDefaultPrimaryKey(id=Now())]\n+        )\n+        self.assertIsInstance(obj.id, datetime)",
      "comment": "> It would be both unnecessary to look at all field values for all objects to check if they are expressions\r\n\r\nThe thing is we already have to do that in order to call `resolve_expression` and `as_sql` on them and generate the proper SQL. See how `SQLInsertCompiler` does it.\r\n\r\n> also to fetch all values again from the db. For such a case it would still be nice to override the returning fields manually to regain the little extra performance.\r\n\r\nI do agree that granular control over `returning_fields` is desirable as even without implicitly `RETURNING` any field that makes use of expressions, as suggested above, the overfetching problem is already present for `db_returning` fields such as the ones that define `db_default` or are `GeneratedField`. You might not necessarily want fetch back db generated value on each `bulk_create` and in some cases the `pk` might not even be of interest.\r\n\r\nI think we agree that in an ideal state `returning_fields` could be overridable and it would default to `db_returning` fields + fields that had at least one instance with a provided expression.",
      "comment_id": 2487379150,
      "user": "charettes",
      "created_at": "2025-11-03T17:53:52Z",
      "url": "https://github.com/django/django/pull/20041#discussion_r2487379150"
    },
    {
      "repo": "django/django",
      "pr_number": 20060,
      "file_path": "tests/utils_tests/test_html.py",
      "line": 469,
      "side": "RIGHT",
      "diff_hunk": "@@ -455,6 +455,18 @@ def test_urlize(self):\n                 '<a href=\"mailto:idna-2008@%DE%89%DE%A8%DE%80%DE%A7%DE%83%DE%AA.ex'\n                 'ample.mv\">idna-2008@\u0789\u07a8\u0780\u07a7\u0783\u07aa.example.mv</a>',\n             ),\n+            (\n+                \"as.d8f.ghj8.gov\",\n+                '<a href=\"https://as.d8f.ghj8.gov\">as.d8f.ghj8.gov</a>',\n+            ),\n+            (\n+                \"a.b2c.com\",\n+                '<a href=\"https://a.b2c.com\">a.b2c.com</a>',\n+            ),\n+            (\n+                \"www.sub1.d2f.gov\",\n+                '<a href=\"https://www.sub1.d2f.gov\">www.sub1.d2f.gov</a>',\n+            ),",
      "comment": "I think we can just use one test here. Something like `host.djangoproject.com`.",
      "comment_id": 2495851342,
      "user": "jacobtylerwalls",
      "created_at": "2025-11-05T19:30:46Z",
      "url": "https://github.com/django/django/pull/20060#discussion_r2495851342"
    },
    {
      "repo": "django/django",
      "pr_number": 19614,
      "file_path": "django/db/models/query.py",
      "line": 803,
      "side": "RIGHT",
      "diff_hunk": "@@ -800,7 +801,11 @@ def bulk_create(\n         fields = [f for f in opts.concrete_fields if not f.generated]\n         objs = list(objs)\n         objs_with_pk, objs_without_pk = self._prepare_for_bulk_create(objs)",
      "comment": "It appears that this whole primary key assignment partition logic can also be elided if [we'd like to go further](https://github.com/django/django/compare/main...charettes:django:ticket-36490-simplify-bulk_create#diff-d58ef61559dc7af5fdf7b56fee13571a4d2948e784cd608f6afeacf3ac2fb195R793-R808)",
      "comment_id": 2180637311,
      "user": "charettes",
      "created_at": "2025-07-02T17:45:13Z",
      "url": "https://github.com/django/django/pull/19614#discussion_r2180637311"
    },
    {
      "repo": "django/django",
      "pr_number": 19989,
      "file_path": "django/db/backends/mysql/introspection.py",
      "line": 62,
      "side": "RIGHT",
      "diff_hunk": "@@ -53,6 +54,14 @@ class DatabaseIntrospection(BaseDatabaseIntrospection):\n         FIELD_TYPE.VAR_STRING: \"CharField\",\n     }\n \n+    @cached_property\n+    def on_delete_types(self):\n+        on_delete_types = super().on_delete_types\n+        if self.connection.mysql_is_mariadb:\n+            # On MariaDB \"NO ACTION\" and \"RESTRICT\" are synonyms.\n+            on_delete_types[\"RESTRICT\"] = DO_NOTHING",
      "comment": "https://mariadb.com/docs/server/ha-and-performance/optimization-and-tuning/optimization-and-indexes/foreign-keys#constraints",
      "comment_id": 2452129490,
      "user": "felixxm",
      "created_at": "2025-10-22T13:35:35Z",
      "url": "https://github.com/django/django/pull/19989#discussion_r2452129490"
    },
    {
      "repo": "django/django",
      "pr_number": 19989,
      "file_path": "django/db/backends/mysql/introspection.py",
      "line": 62,
      "side": "RIGHT",
      "diff_hunk": "@@ -53,6 +54,14 @@ class DatabaseIntrospection(BaseDatabaseIntrospection):\n         FIELD_TYPE.VAR_STRING: \"CharField\",\n     }\n \n+    @cached_property\n+    def on_delete_types(self):\n+        on_delete_types = super().on_delete_types\n+        if self.connection.mysql_is_mariadb:\n+            # On MariaDB \"NO ACTION\" and \"RESTRICT\" are synonyms.\n+            on_delete_types[\"RESTRICT\"] = DO_NOTHING",
      "comment": "I suspect it's the same on MySQL as neither support deferrable foreign key constraints which means that both `RETRICT` and `DO_NOTHING` are equivalent.\r\n\r\nNot sure we should translate it to `DO_NOTHING` though as when we do add support do `DB_RESTRICT` the value will be swapped back to `DO_NOTHING` on introspection.\r\n\r\nMaybe this is something we can revisit when we do add support for `DB_RESTRICT`.",
      "comment_id": 2452151882,
      "user": "charettes",
      "created_at": "2025-10-22T13:42:34Z",
      "url": "https://github.com/django/django/pull/19989#discussion_r2452151882"
    },
    {
      "repo": "django/django",
      "pr_number": 19989,
      "file_path": "django/db/backends/mysql/introspection.py",
      "line": 62,
      "side": "RIGHT",
      "diff_hunk": "@@ -53,6 +54,14 @@ class DatabaseIntrospection(BaseDatabaseIntrospection):\n         FIELD_TYPE.VAR_STRING: \"CharField\",\n     }\n \n+    @cached_property\n+    def on_delete_types(self):\n+        on_delete_types = super().on_delete_types\n+        if self.connection.mysql_is_mariadb:\n+            # On MariaDB \"NO ACTION\" and \"RESTRICT\" are synonyms.\n+            on_delete_types[\"RESTRICT\"] = DO_NOTHING",
      "comment": "> I suspect it's the same on MySQL as neither support deferrable foreign key constraints which means that both `RETRICT` and `DO_NOTHING` are equivalent.\r\n\r\nAt least on MySQL, `delete_rule` returns `NO ACTION` by default :)",
      "comment_id": 2452227636,
      "user": "felixxm",
      "created_at": "2025-10-22T14:05:56Z",
      "url": "https://github.com/django/django/pull/19989#discussion_r2452227636"
    },
    {
      "repo": "django/django",
      "pr_number": 19989,
      "file_path": "django/db/backends/mysql/introspection.py",
      "line": 62,
      "side": "RIGHT",
      "diff_hunk": "@@ -53,6 +54,14 @@ class DatabaseIntrospection(BaseDatabaseIntrospection):\n         FIELD_TYPE.VAR_STRING: \"CharField\",\n     }\n \n+    @cached_property\n+    def on_delete_types(self):\n+        on_delete_types = super().on_delete_types\n+        if self.connection.mysql_is_mariadb:\n+            # On MariaDB \"NO ACTION\" and \"RESTRICT\" are synonyms.\n+            on_delete_types[\"RESTRICT\"] = DO_NOTHING",
      "comment": "> Not sure we should translate it to `DO_NOTHING` though as when we do add support do `DB_RESTRICT` the value will be swapped back to `DO_NOTHING` on introspection.\r\n\r\nPreviously we translated all options to `DO_NOTHING`.",
      "comment_id": 2452234598,
      "user": "felixxm",
      "created_at": "2025-10-22T14:08:15Z",
      "url": "https://github.com/django/django/pull/19989#discussion_r2452234598"
    },
    {
      "repo": "django/django",
      "pr_number": 19989,
      "file_path": "django/core/management/commands/inspectdb.py",
      "line": 203,
      "side": "RIGHT",
      "diff_hunk": "@@ -191,6 +194,13 @@ def handle_inspection(self, options):\n                                 model_name.lower(),\n                                 att_name,\n                             )\n+                        if db_on_delete:\n+                            if isinstance(db_on_delete, DatabaseOnDelete):\n+                                extra_params[\"on_delete\"] = f\"models.{db_on_delete}\"\n+                            else:\n+                                extra_params[\"on_delete\"] = (\n+                                    f\"models.{db_on_delete.__qualname__}\"\n+                                )",
      "comment": "No tests fail when I alter this to assign an empty tuple, but maybe that's fine.",
      "comment_id": 2478017710,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-30T13:03:30Z",
      "url": "https://github.com/django/django/pull/19989#discussion_r2478017710"
    },
    {
      "repo": "django/django",
      "pr_number": 19989,
      "file_path": "django/core/management/commands/inspectdb.py",
      "line": 240,
      "side": "RIGHT",
      "diff_hunk": "@@ -227,8 +237,12 @@ def handle_inspection(self, options):\n                         \"\" if \".\" in field_type else \"models.\",\n                         field_type,\n                     )\n+                    on_delete_repr = extra_params.pop(\"on_delete\", None)",
      "comment": "```suggestion\n                    on_delete_qualname = extra_params.pop(\"on_delete\", None)\n```",
      "comment_id": 2478025450,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-30T13:05:47Z",
      "url": "https://github.com/django/django/pull/19989#discussion_r2478025450"
    },
    {
      "repo": "django/django",
      "pr_number": 19989,
      "file_path": "django/db/backends/mysql/introspection.py",
      "line": 62,
      "side": "RIGHT",
      "diff_hunk": "@@ -53,6 +54,14 @@ class DatabaseIntrospection(BaseDatabaseIntrospection):\n         FIELD_TYPE.VAR_STRING: \"CharField\",\n     }\n \n+    @cached_property\n+    def on_delete_types(self):\n+        on_delete_types = super().on_delete_types\n+        if self.connection.mysql_is_mariadb:\n+            # On MariaDB \"NO ACTION\" and \"RESTRICT\" are synonyms.\n+            on_delete_types[\"RESTRICT\"] = DO_NOTHING",
      "comment": "True, but if we have a choice between synonyms, why wouldn't we leave it as is and avoid swapping it out for `DO_NOTHING`? Maybe a project starts by introspecting a mariadb and plans to migrate to postgres later.\n\nIf we did that, couldn't we remove the property and just make this a class constant?",
      "comment_id": 2478099528,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-30T13:25:34Z",
      "url": "https://github.com/django/django/pull/19989#discussion_r2478099528"
    },
    {
      "repo": "django/django",
      "pr_number": 19989,
      "file_path": "django/db/backends/mysql/introspection.py",
      "line": 62,
      "side": "RIGHT",
      "diff_hunk": "@@ -53,6 +54,14 @@ class DatabaseIntrospection(BaseDatabaseIntrospection):\n         FIELD_TYPE.VAR_STRING: \"CharField\",\n     }\n \n+    @cached_property\n+    def on_delete_types(self):\n+        on_delete_types = super().on_delete_types\n+        if self.connection.mysql_is_mariadb:\n+            # On MariaDB \"NO ACTION\" and \"RESTRICT\" are synonyms.\n+            on_delete_types[\"RESTRICT\"] = DO_NOTHING",
      "comment": "> I suspect it's the same on MySQL as neither support deferrable foreign key constraints which means that both RETRICT and DO_NOTHING are equivalent.\r\n\r\nA subtle difference is [mentioned](https://dev.mysql.com/doc/refman/8.0/en/create-table-foreign-keys.html#foreign-key-referential-actions) between InnoDB and NDB:\r\n\r\n> [NDB](https://dev.mysql.com/doc/refman/8.0/en/mysql-cluster.html) supports deferred checks, and NO ACTION specifies a deferred check;",
      "comment_id": 2478113664,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-30T13:29:44Z",
      "url": "https://github.com/django/django/pull/19989#discussion_r2478113664"
    },
    {
      "repo": "django/django",
      "pr_number": 19989,
      "file_path": "django/db/backends/base/introspection.py",
      "line": 185,
      "side": "RIGHT",
      "diff_hunk": "@@ -169,8 +183,11 @@ def get_sequences(self, cursor, table_name, table_fields=()):\n \n     def get_relations(self, cursor, table_name):\n         \"\"\"\n-        Return a dictionary of {field_name: (field_name_other_table,\n-        other_table)} representing all foreign keys in the given table.\n+        Return a dictionary of\n+            {\n+                field_name: (field_name_other_table, other_table, db_on_delete)\n+            }\n+        representing all foreign keys in the given table.",
      "comment": "Do we need to mention this in the \"changes that may be needed in third-party database\nbackends\"?",
      "comment_id": 2478125018,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-30T13:33:10Z",
      "url": "https://github.com/django/django/pull/19989#discussion_r2478125018"
    },
    {
      "repo": "django/django",
      "pr_number": 19989,
      "file_path": "django/db/backends/mysql/introspection.py",
      "line": 62,
      "side": "RIGHT",
      "diff_hunk": "@@ -53,6 +54,14 @@ class DatabaseIntrospection(BaseDatabaseIntrospection):\n         FIELD_TYPE.VAR_STRING: \"CharField\",\n     }\n \n+    @cached_property\n+    def on_delete_types(self):\n+        on_delete_types = super().on_delete_types\n+        if self.connection.mysql_is_mariadb:\n+            # On MariaDB \"NO ACTION\" and \"RESTRICT\" are synonyms.\n+            on_delete_types[\"RESTRICT\"] = DO_NOTHING",
      "comment": "Thanks for that, TIL about NDB.\n\nI don't think we support the NDB storage engine though, at least we don't test against it and [we explicitly mark the MySQL backend as not supporting deferred constraint checks](https://github.com/search?q=repo%3Adjango%2Fdjango%20can_defer_constraint_checks&type=code).",
      "comment_id": 2478337721,
      "user": "charettes",
      "created_at": "2025-10-30T14:29:46Z",
      "url": "https://github.com/django/django/pull/19989#discussion_r2478337721"
    },
    {
      "repo": "django/django",
      "pr_number": 19989,
      "file_path": "django/core/management/commands/inspectdb.py",
      "line": 203,
      "side": "RIGHT",
      "diff_hunk": "@@ -191,6 +194,13 @@ def handle_inspection(self, options):\n                                 model_name.lower(),\n                                 att_name,\n                             )\n+                        if db_on_delete:\n+                            if isinstance(db_on_delete, DatabaseOnDelete):\n+                                extra_params[\"on_delete\"] = f\"models.{db_on_delete}\"\n+                            else:\n+                                extra_params[\"on_delete\"] = (\n+                                    f\"models.{db_on_delete.__qualname__}\"\n+                                )",
      "comment": "Right, currently it's unnecessary because the only `db_on_delete` that is not `DatabaseOnDelete` is actually `DO_NOTHING`.",
      "comment_id": 2480307147,
      "user": "felixxm",
      "created_at": "2025-10-31T06:43:55Z",
      "url": "https://github.com/django/django/pull/19989#discussion_r2480307147"
    },
    {
      "repo": "django/django",
      "pr_number": 19989,
      "file_path": "django/db/backends/base/introspection.py",
      "line": 29,
      "side": "RIGHT",
      "diff_hunk": "@@ -22,6 +24,18 @@ def __init__(self, connection):\n     def __del__(self):\n         del self.connection\n \n+    @cached_property\n+    def on_delete_types(self):\n+        from django.db.models import DB_CASCADE, DB_SET_DEFAULT, DB_SET_NULL, DO_NOTHING",
      "comment": "Nope, `django.db` could be an issue but `django.db.models` is fine.",
      "comment_id": 2480318514,
      "user": "felixxm",
      "created_at": "2025-10-31T06:51:08Z",
      "url": "https://github.com/django/django/pull/19989#discussion_r2480318514"
    },
    {
      "repo": "django/django",
      "pr_number": 19989,
      "file_path": "django/db/backends/mysql/introspection.py",
      "line": 62,
      "side": "RIGHT",
      "diff_hunk": "@@ -53,6 +54,14 @@ class DatabaseIntrospection(BaseDatabaseIntrospection):\n         FIELD_TYPE.VAR_STRING: \"CharField\",\n     }\n \n+    @cached_property\n+    def on_delete_types(self):\n+        on_delete_types = super().on_delete_types\n+        if self.connection.mysql_is_mariadb:\n+            # On MariaDB \"NO ACTION\" and \"RESTRICT\" are synonyms.\n+            on_delete_types[\"RESTRICT\"] = DO_NOTHING",
      "comment": "We can remove `on_delete_types` in MySQL backend, but this requires minor adjustments in introspection tests.\r\n\r\nUpdated.",
      "comment_id": 2480368385,
      "user": "felixxm",
      "created_at": "2025-10-31T07:21:23Z",
      "url": "https://github.com/django/django/pull/19989#discussion_r2480368385"
    },
    {
      "repo": "django/django",
      "pr_number": 20024,
      "file_path": "tests/utils_tests/test_inspect.py",
      "line": 117,
      "side": "RIGHT",
      "diff_hunk": "@@ -103,6 +114,17 @@ def test_func_accepts_kwargs(self):\n         self.assertIs(inspect.func_accepts_kwargs(Person.all_kinds), True)\n         self.assertIs(inspect.func_accepts_kwargs(Person().just_args), False)\n \n+    def test_regression_36696(self):",
      "comment": "Use descriptive name instead of the ticket number, e.g.\r\n```suggestion\r\n    def test_func_accepts_kwargs_deferred_annotations(self):\r\n```",
      "comment_id": 2477618244,
      "user": "felixxm",
      "created_at": "2025-10-30T11:14:56Z",
      "url": "https://github.com/django/django/pull/20024#discussion_r2477618244"
    },
    {
      "repo": "django/django",
      "pr_number": 20024,
      "file_path": "tests/utils_tests/test_inspect.py",
      "line": 122,
      "side": "RIGHT",
      "diff_hunk": "@@ -103,6 +114,17 @@ def test_func_accepts_kwargs(self):\n         self.assertIs(inspect.func_accepts_kwargs(Person.all_kinds), True)\n         self.assertIs(inspect.func_accepts_kwargs(Person().just_args), False)\n \n+    def test_regression_36696(self):\n+        # See https://code.djangoproject.com/ticket/36696\n+        # inspection failed with deferred annotations starting with python 3.14\n+        # however, we can only render this bug with Python 3.14, as earlier\n+        # versions would trigger the NameError on initialization of this\n+        # module.",
      "comment": "We normally don't include links to tickets in test comments, and avoid using `we` and similar. Maybe:\r\n```\r\n        # Inspection fails with deferred annotations with python 3.14+. Earlier\r\n        # Python versions trigger the NameError on module initialization.\r\n```",
      "comment_id": 2477633315,
      "user": "felixxm",
      "created_at": "2025-10-30T11:17:55Z",
      "url": "https://github.com/django/django/pull/20024#discussion_r2477633315"
    },
    {
      "repo": "django/django",
      "pr_number": 20024,
      "file_path": "tests/utils_tests/test_inspect.py",
      "line": 124,
      "side": "RIGHT",
      "diff_hunk": "@@ -103,6 +114,17 @@ def test_func_accepts_kwargs(self):\n         self.assertIs(inspect.func_accepts_kwargs(Person.all_kinds), True)\n         self.assertIs(inspect.func_accepts_kwargs(Person().just_args), False)\n \n+    def test_regression_36696(self):\n+        # See https://code.djangoproject.com/ticket/36696\n+        # inspection failed with deferred annotations starting with python 3.14\n+        # however, we can only render this bug with Python 3.14, as earlier\n+        # versions would trigger the NameError on initialization of this\n+        # module.\n+        if not PY314:\n+            return",
      "comment": "Uhh, I did not know about this feature before, nice to know :)",
      "comment_id": 2477699445,
      "user": "prauscher",
      "created_at": "2025-10-30T11:29:47Z",
      "url": "https://github.com/django/django/pull/20024#discussion_r2477699445"
    },
    {
      "repo": "django/django",
      "pr_number": 20024,
      "file_path": "tests/utils_tests/test_inspect.py",
      "line": 111,
      "side": "RIGHT",
      "diff_hunk": "@@ -103,6 +108,16 @@ def test_func_accepts_kwargs(self):\n         self.assertIs(inspect.func_accepts_kwargs(Person.all_kinds), True)\n         self.assertIs(inspect.func_accepts_kwargs(Person().just_args), False)\n \n+    @unittest.skipUnless(PY314, \"deferred annotations are python 3.14+ only\")",
      "comment": "```suggestion\r\n    @unittest.skipUnless(PY314, \"Deferred annotations are Python 3.14+ only\")\r\n```",
      "comment_id": 2480185369,
      "user": "felixxm",
      "created_at": "2025-10-31T05:14:14Z",
      "url": "https://github.com/django/django/pull/20024#discussion_r2480185369"
    },
    {
      "repo": "django/django",
      "pr_number": 19983,
      "file_path": "django/test/runner.py",
      "line": 574,
      "side": "RIGHT",
      "diff_hunk": "@@ -571,18 +571,25 @@ def run(self, result):\n \n         test_results = pool.imap_unordered(self.run_subsuite.__func__, args)\n \n+        timeout = 64  # ms",
      "comment": "I experiemented with another approach where I wrapped the body of `_init_worker` in a `try/except`, and set a flag (`init_failed`) in the except, and accessed it in the workers via a lock (shared state), but the bloating of the diff told me I was going in the wrong direction (?)",
      "comment_id": 2449843526,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-21T22:16:43Z",
      "url": "https://github.com/django/django/pull/19983#discussion_r2449843526"
    },
    {
      "repo": "django/django",
      "pr_number": 19983,
      "file_path": "django/test/runner.py",
      "line": 574,
      "side": "RIGHT",
      "diff_hunk": "@@ -571,18 +571,25 @@ def run(self, result):\n \n         test_results = pool.imap_unordered(self.run_subsuite.__func__, args)\n \n+        timeout = 64  # ms",
      "comment": "Actually, scratch that, I'll have to go back in that direction, in case user tests exceed this time limit. Bah. (EDIT: done \u2714\ufe0f )",
      "comment_id": 2449858046,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-21T22:21:23Z",
      "url": "https://github.com/django/django/pull/19983#discussion_r2449858046"
    },
    {
      "repo": "django/django",
      "pr_number": 19983,
      "file_path": "django/test/runner.py",
      "line": 598,
      "side": "RIGHT",
      "diff_hunk": "@@ -580,9 +590,12 @@ def run(self, result):\n \n             try:\n                 subsuite_index, events = test_results.next(timeout=0.1)\n-            except multiprocessing.TimeoutError:\n+            except multiprocessing.TimeoutError as err:\n+                if counter.value == -1:\n+                    err.add_note(\"_init_worker failed\")\n+                    raise\n                 continue\n-            except StopIteration:\n+            except (StopIteration, multiprocessing.TimeoutError):",
      "comment": "I assumed this was added to avoid this error, but I'm still getting it if I force an error inside `init_worker`:\n```\nResourceWarning: unclosed running multiprocessing pool <multiprocessing.pool.Pool state=RUN pool_size=16>\n```",
      "comment_id": 2467145848,
      "user": "nessita",
      "created_at": "2025-10-27T21:36:22Z",
      "url": "https://github.com/django/django/pull/19983#discussion_r2467145848"
    },
    {
      "repo": "django/django",
      "pr_number": 19983,
      "file_path": "django/test/runner.py",
      "line": 595,
      "side": "RIGHT",
      "diff_hunk": "@@ -580,9 +590,12 @@ def run(self, result):\n \n             try:\n                 subsuite_index, events = test_results.next(timeout=0.1)\n-            except multiprocessing.TimeoutError:\n+            except multiprocessing.TimeoutError as err:\n+                if counter.value == -1:\n+                    err.add_note(\"_init_worker failed\")",
      "comment": "Could we add a more prominent and explanatory note here?",
      "comment_id": 2467146325,
      "user": "nessita",
      "created_at": "2025-10-27T21:36:40Z",
      "url": "https://github.com/django/django/pull/19983#discussion_r2467146325"
    },
    {
      "repo": "django/django",
      "pr_number": 19983,
      "file_path": "django/test/runner.py",
      "line": 491,
      "side": "RIGHT",
      "diff_hunk": "@@ -480,6 +481,15 @@ def _init_worker(\n         )\n \n \n+def _safe_init_worker(init_worker, counter, *args, **kwargs):\n+    try:\n+        init_worker(counter, *args, **kwargs)\n+    except Exception:\n+        with counter.get_lock():\n+            counter.value = -1\n+        raise",
      "comment": "Is the goal of this new method to avoid indenting the original `_init_worker` one level in to add the try-except there?\n\nAlso, question: what happens if we create a `multiprocessing.Pool` of `N` threads: `t1`, `t2`, ..., `tN`, and (for example) `t2` executes super fast and crashes inside `_init_worker` before `t3` even starts initializing?\n\nSince `_safe_init_worker` sets `counter.value = -1` on failure, any thread still initializing could then read that shared value and end up with `_worker_id = 0`. That seems unintended and could lead to duplicate or invalid worker IDs. Furthermore, it could still raise but the code handling the exception does not see the counter in `-1`.",
      "comment_id": 2467358337,
      "user": "nessita",
      "created_at": "2025-10-27T23:01:40Z",
      "url": "https://github.com/django/django/pull/19983#discussion_r2467358337"
    },
    {
      "repo": "django/django",
      "pr_number": 19983,
      "file_path": "django/test/runner.py",
      "line": 595,
      "side": "RIGHT",
      "diff_hunk": "@@ -580,9 +590,12 @@ def run(self, result):\n \n             try:\n                 subsuite_index, events = test_results.next(timeout=0.1)\n-            except multiprocessing.TimeoutError:\n+            except multiprocessing.TimeoutError as err:\n+                if counter.value == -1:\n+                    err.add_note(\"_init_worker failed\")",
      "comment": "Natalia suggested something like \"see traceback above\"",
      "comment_id": 2469498583,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-28T13:02:56Z",
      "url": "https://github.com/django/django/pull/19983#discussion_r2469498583"
    },
    {
      "repo": "django/django",
      "pr_number": 19983,
      "file_path": "django/test/runner.py",
      "line": 491,
      "side": "RIGHT",
      "diff_hunk": "@@ -480,6 +481,15 @@ def _init_worker(\n         )\n \n \n+def _safe_init_worker(init_worker, counter, *args, **kwargs):\n+    try:\n+        init_worker(counter, *args, **kwargs)\n+    except Exception:\n+        with counter.get_lock():\n+            counter.value = -1\n+        raise",
      "comment": "> Is the goal of this new method to avoid indenting the original _init_worker one level in to add the try-except there?\r\n\r\nExactly, keep the diff smaller and (more importantly) reduce the number of statements under a try.\r\n\r\n> That seems unintended and could lead to duplicate or invalid worker IDs.\r\n\r\nThanks \ud83c\udfaf ",
      "comment_id": 2470762225,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-28T19:14:11Z",
      "url": "https://github.com/django/django/pull/19983#discussion_r2470762225"
    },
    {
      "repo": "django/django",
      "pr_number": 19991,
      "file_path": "tests/admin_scripts/tests.py",
      "line": 117,
      "side": "RIGHT",
      "diff_hunk": "@@ -112,7 +112,20 @@ def _ext_backend_paths(self):\n                 paths.append(os.path.dirname(backend_dir))\n         return paths\n \n-    def run_test(self, args, settings_file=None, apps=None, umask=-1):\n+    @cached_property\n+    def path_without_formatters(self):\n+        black_path = find_formatters()[\"black_path\"]",
      "comment": "If black exists in multiple places on the path, we'd have to iterate this algorithm to keep patching it out. Not sure it's worth it.",
      "comment_id": 2452927439,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-22T18:08:47Z",
      "url": "https://github.com/django/django/pull/19991#discussion_r2452927439"
    },
    {
      "repo": "django/django",
      "pr_number": 19991,
      "file_path": "tests/admin_scripts/tests.py",
      "line": 117,
      "side": "RIGHT",
      "diff_hunk": "@@ -112,7 +112,20 @@ def _ext_backend_paths(self):\n                 paths.append(os.path.dirname(backend_dir))\n         return paths\n \n-    def run_test(self, args, settings_file=None, apps=None, umask=-1):\n+    @cached_property\n+    def path_without_formatters(self):\n+        black_path = find_formatters()[\"black_path\"]",
      "comment": "I agree it's not worth it. But perhaps we could/should remove from the `PATH` every formatter returned from `find_formatters` to be truth to the param name?\nSuggestion below.\n",
      "comment_id": 2465783866,
      "user": "nessita",
      "created_at": "2025-10-27T13:57:27Z",
      "url": "https://github.com/django/django/pull/19991#discussion_r2465783866"
    },
    {
      "repo": "django/django",
      "pr_number": 19991,
      "file_path": "tests/admin_scripts/tests.py",
      "line": 122,
      "side": "RIGHT",
      "diff_hunk": "@@ -112,7 +112,20 @@ def _ext_backend_paths(self):\n                 paths.append(os.path.dirname(backend_dir))\n         return paths\n \n-    def run_test(self, args, settings_file=None, apps=None, umask=-1):\n+    @cached_property\n+    def path_without_formatters(self):\n+        black_path = find_formatters()[\"black_path\"]\n+        return os.pathsep.join(\n+            [\n+                path_component\n+                for path_component in os.environ.get(\"PATH\", \"\").split(os.pathsep)\n+                if os.path.commonpath([path_component, black_path]) == os.sep",
      "comment": "```suggestion\n                path_component\n                for path_component in os.environ.get(\"PATH\", \"\").split(os.pathsep)\n                for formmater_path in find_formatters().values()\n                if os.path.commonpath([path_component, formatter_path]) == os.sep\n```",
      "comment_id": 2465792187,
      "user": "nessita",
      "created_at": "2025-10-27T13:59:30Z",
      "url": "https://github.com/django/django/pull/19991#discussion_r2465792187"
    },
    {
      "repo": "django/django",
      "pr_number": 20003,
      "file_path": "tests/queries/test_explain.py",
      "line": 162,
      "side": "RIGHT",
      "diff_hunk": "@@ -159,9 +159,7 @@ def test_mysql_text_to_traditional(self):\n         self.assertEqual(len(captured_queries), 1)\n         self.assertIn(\"FORMAT=TRADITIONAL\", captured_queries[0][\"sql\"])\n \n-    @unittest.skipUnless(\n-        connection.vendor == \"mysql\", \"MariaDB and MySQL >= 8.0.18 specific.\"\n-    )\n+    @unittest.skipUnless(connection.vendor == \"mysql\", \"MySQL specific\")",
      "comment": "This test passes on mariadb, so I think we can remove the skip. Debatable whether we should fix in this PR, but we are removing the dynamic skip for this test elsewhere in the PR.",
      "comment_id": 2465570550,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-27T12:58:37Z",
      "url": "https://github.com/django/django/pull/20003#discussion_r2465570550"
    },
    {
      "repo": "django/django",
      "pr_number": 20003,
      "file_path": "tests/queries/test_explain.py",
      "line": 162,
      "side": "RIGHT",
      "diff_hunk": "@@ -159,9 +159,7 @@ def test_mysql_text_to_traditional(self):\n         self.assertEqual(len(captured_queries), 1)\n         self.assertIn(\"FORMAT=TRADITIONAL\", captured_queries[0][\"sql\"])\n \n-    @unittest.skipUnless(\n-        connection.vendor == \"mysql\", \"MariaDB and MySQL >= 8.0.18 specific.\"\n-    )\n+    @unittest.skipUnless(connection.vendor == \"mysql\", \"MySQL specific\")",
      "comment": "MySQL and MariaDB share the same vendor name `mysql`, this test is skipped on other vendors, not on MySQL or MariaDB.",
      "comment_id": 2465673708,
      "user": "felixxm",
      "created_at": "2025-10-27T13:27:51Z",
      "url": "https://github.com/django/django/pull/20003#discussion_r2465673708"
    },
    {
      "repo": "django/django",
      "pr_number": 16311,
      "file_path": "django/utils/text.py",
      "line": 320,
      "side": "RIGHT",
      "diff_hunk": "@@ -314,8 +316,27 @@ def phone2numeric(phone):\n     return \"\".join(char2number.get(c, c) for c in phone.lower())\n \n \n-def compress_string(s):\n-    return gzip_compress(s, compresslevel=6, mtime=0)\n+def _get_random_fname(max_random_bytes):\n+    return \"a\" * secrets.randbelow(max_random_bytes)",
      "comment": "```suggestion\r\n    return b\"a\" * secrets.randbelow(max_random_bytes)\r\n```",
      "comment_id": 1027364608,
      "user": "ngnpope",
      "created_at": "2022-11-20T22:29:52Z",
      "url": "https://github.com/django/django/pull/16311#discussion_r1027364608"
    },
    {
      "repo": "django/django",
      "pr_number": 16311,
      "file_path": "django/utils/text.py",
      "line": 335,
      "side": "RIGHT",
      "diff_hunk": "@@ -314,8 +316,27 @@ def phone2numeric(phone):\n     return \"\".join(char2number.get(c, c) for c in phone.lower())\n \n \n-def compress_string(s):\n-    return gzip_compress(s, compresslevel=6, mtime=0)\n+def _get_random_fname(max_random_bytes):\n+    return \"a\" * secrets.randbelow(max_random_bytes)\n+\n+\n+def compress_string(s, max_random_bytes=0):\n+    compressed_data = gzip_compress(s, compresslevel=6, mtime=0)\n+\n+    if max_random_bytes == 0:\n+        return compressed_data\n+\n+    header = bytearray(compressed_data[:10])\n+    header[3] = gzip.FNAME\n+\n+    return b\"\".join(\n+        [\n+            header,\n+            _get_random_fname(max_random_bytes).encode(\"ascii\"),",
      "comment": "```suggestion\r\n            _get_random_fname(max_random_bytes),\r\n```",
      "comment_id": 1027364663,
      "user": "ngnpope",
      "created_at": "2022-11-20T22:30:15Z",
      "url": "https://github.com/django/django/pull/16311#discussion_r1027364663"
    },
    {
      "repo": "django/django",
      "pr_number": 16311,
      "file_path": "django/utils/text.py",
      "line": 323,
      "side": "RIGHT",
      "diff_hunk": "@@ -314,8 +316,27 @@ def phone2numeric(phone):\n     return \"\".join(char2number.get(c, c) for c in phone.lower())\n \n \n-def compress_string(s):\n-    return gzip_compress(s, compresslevel=6, mtime=0)\n+def _get_random_fname(max_random_bytes):\n+    return \"a\" * secrets.randbelow(max_random_bytes)\n+\n+\n+def compress_string(s, max_random_bytes=0):",
      "comment": "Make this new argument keyword-only?\r\n```suggestion\r\ndef compress_string(s, *,  max_random_bytes=0):\r\n```",
      "comment_id": 1027364796,
      "user": "ngnpope",
      "created_at": "2022-11-20T22:31:19Z",
      "url": "https://github.com/django/django/pull/16311#discussion_r1027364796"
    },
    {
      "repo": "django/django",
      "pr_number": 16311,
      "file_path": "django/utils/text.py",
      "line": 356,
      "side": "RIGHT",
      "diff_hunk": "@@ -327,9 +348,15 @@ def read(self):\n \n \n # Like compress_string, but for iterators of strings.\n-def compress_sequence(sequence):\n+def compress_sequence(sequence, max_random_bytes=0):\n     buf = StreamingBuffer()\n-    with GzipFile(mode=\"wb\", compresslevel=6, fileobj=buf, mtime=0) as zfile:\n+    with GzipFile(\n+        mode=\"wb\",\n+        filename=_get_random_fname(max_random_bytes) if max_random_bytes else None,",
      "comment": "_This should be fine if provided with a `bytes` filename... At least if `typeshed` is correct..._",
      "comment_id": 1027365391,
      "user": "ngnpope",
      "created_at": "2022-11-20T22:34:43Z",
      "url": "https://github.com/django/django/pull/16311#discussion_r1027365391"
    },
    {
      "repo": "django/django",
      "pr_number": 16311,
      "file_path": "django/utils/text.py",
      "line": 335,
      "side": "RIGHT",
      "diff_hunk": "@@ -314,8 +316,27 @@ def phone2numeric(phone):\n     return \"\".join(char2number.get(c, c) for c in phone.lower())\n \n \n-def compress_string(s):\n-    return gzip_compress(s, compresslevel=6, mtime=0)\n+def _get_random_fname(max_random_bytes):\n+    return \"a\" * secrets.randbelow(max_random_bytes)\n+\n+\n+def compress_string(s, max_random_bytes=0):\n+    compressed_data = gzip_compress(s, compresslevel=6, mtime=0)\n+\n+    if max_random_bytes == 0:\n+        return compressed_data\n+\n+    header = bytearray(compressed_data[:10])\n+    header[3] = gzip.FNAME\n+\n+    return b\"\".join(\n+        [\n+            header,\n+            _get_random_fname(max_random_bytes).encode(\"ascii\"),",
      "comment": "This would also be less clunky styled as:\r\n```python\r\n    fname = _get_random_fname(max_random_bytes) + b\"\\x00\"\r\n    return b\"\".join([header, fname, compressed_data[10:]])\r\n```",
      "comment_id": 1027366036,
      "user": "ngnpope",
      "created_at": "2022-11-20T22:39:12Z",
      "url": "https://github.com/django/django/pull/16311#discussion_r1027366036"
    },
    {
      "repo": "django/django",
      "pr_number": 16311,
      "file_path": "django/utils/text.py",
      "line": 337,
      "side": "RIGHT",
      "diff_hunk": "@@ -314,8 +316,27 @@ def phone2numeric(phone):\n     return \"\".join(char2number.get(c, c) for c in phone.lower())\n \n \n-def compress_string(s):\n-    return gzip_compress(s, compresslevel=6, mtime=0)\n+def _get_random_fname(max_random_bytes):\n+    return \"a\" * secrets.randbelow(max_random_bytes)\n+\n+\n+def compress_string(s, max_random_bytes=0):\n+    compressed_data = gzip_compress(s, compresslevel=6, mtime=0)\n+\n+    if max_random_bytes == 0:\n+        return compressed_data\n+\n+    header = bytearray(compressed_data[:10])\n+    header[3] = gzip.FNAME\n+\n+    return b\"\".join(\n+        [\n+            header,\n+            _get_random_fname(max_random_bytes).encode(\"ascii\"),\n+            b\"\\x00\",\n+            compressed_data[10:],",
      "comment": "```suggestion\r\n            memoryview(compressed_data)[10:],\r\n```\r\n\r\nUsing `memoryview` seems to prevent creating a copy of bytes for the slice. At least in the test that I used:\r\n```sh\r\n$ /usr/bin/time -f \"%M\" python -c 'b = b\"0\" * 10**6; b\"\".join([b\"123\", b[10:]])'            \r\n11880\r\n$ /usr/bin/time -f \"%M\" python -c 'b = b\"0\" * 10**6; b\"\".join([b\"123\", memoryview(b)[10:]])'\r\n10820\r\n```",
      "comment_id": 1028463589,
      "user": "illia-v",
      "created_at": "2022-11-21T19:56:17Z",
      "url": "https://github.com/django/django/pull/16311#discussion_r1028463589"
    },
    {
      "repo": "django/django",
      "pr_number": 16311,
      "file_path": "django/utils/text.py",
      "line": 337,
      "side": "RIGHT",
      "diff_hunk": "@@ -314,8 +316,27 @@ def phone2numeric(phone):\n     return \"\".join(char2number.get(c, c) for c in phone.lower())\n \n \n-def compress_string(s):\n-    return gzip_compress(s, compresslevel=6, mtime=0)\n+def _get_random_fname(max_random_bytes):\n+    return \"a\" * secrets.randbelow(max_random_bytes)\n+\n+\n+def compress_string(s, max_random_bytes=0):\n+    compressed_data = gzip_compress(s, compresslevel=6, mtime=0)\n+\n+    if max_random_bytes == 0:\n+        return compressed_data\n+\n+    header = bytearray(compressed_data[:10])\n+    header[3] = gzip.FNAME\n+\n+    return b\"\".join(\n+        [\n+            header,\n+            _get_random_fname(max_random_bytes).encode(\"ascii\"),\n+            b\"\\x00\",\n+            compressed_data[10:],",
      "comment": "very nice! I thought about how to avoid potential copies but did not figure this out!",
      "comment_id": 1028528403,
      "user": "pelme",
      "created_at": "2022-11-21T21:25:06Z",
      "url": "https://github.com/django/django/pull/16311#discussion_r1028528403"
    },
    {
      "repo": "django/django",
      "pr_number": 16311,
      "file_path": "django/conf/global_settings.py",
      "line": 673,
      "side": "RIGHT",
      "diff_hunk": "@@ -666,3 +666,8 @@ def gettext_noop(s):\n SECURE_REFERRER_POLICY = \"same-origin\"\n SECURE_SSL_HOST = None\n SECURE_SSL_REDIRECT = False\n+\n+###################\n+# GZIP MIDDLEWARE #\n+###################\n+GZIP_MIDDLEWARE_RANDOM_BYTES = 100",
      "comment": "Playing devil's advocate for a moment here, and to ensure the discussion is had, do we really _need_ a new setting here? \r\nAssuming `100` is enough in the general case[^1] and is a _good_ default, we could also possibly consider having it be directly on the `GZipMiddleware` class and letting downstream users subclass that if they wish to change the value[^2]. Though there might be considerations around other related concepts, like using it via `decorator_from_middleware` which I've not given a second thought to...\r\n\r\n[^1]: I've skimmed the paper to understand where that value came from, but I'm not exactly used to reading papers, so there may be nuance that I've missed.\r\n[^2]: I wonder, under what scenarios one might wish to change it? Any ideas?",
      "comment_id": 1030207386,
      "user": "kezabelle",
      "created_at": "2022-11-23T09:31:13Z",
      "url": "https://github.com/django/django/pull/16311#discussion_r1030207386"
    },
    {
      "repo": "django/django",
      "pr_number": 16311,
      "file_path": "django/conf/global_settings.py",
      "line": 673,
      "side": "RIGHT",
      "diff_hunk": "@@ -666,3 +666,8 @@ def gettext_noop(s):\n SECURE_REFERRER_POLICY = \"same-origin\"\n SECURE_SSL_HOST = None\n SECURE_SSL_REDIRECT = False\n+\n+###################\n+# GZIP MIDDLEWARE #\n+###################\n+GZIP_MIDDLEWARE_RANDOM_BYTES = 100",
      "comment": "Totally agree, the number of random bytes should be a `GZipMiddleware` attribute.",
      "comment_id": 1030213682,
      "user": "felixxm",
      "created_at": "2022-11-23T09:37:26Z",
      "url": "https://github.com/django/django/pull/16311#discussion_r1030213682"
    },
    {
      "repo": "django/django",
      "pr_number": 16311,
      "file_path": "django/conf/global_settings.py",
      "line": 673,
      "side": "RIGHT",
      "diff_hunk": "@@ -666,3 +666,8 @@ def gettext_noop(s):\n SECURE_REFERRER_POLICY = \"same-origin\"\n SECURE_SSL_HOST = None\n SECURE_SSL_REDIRECT = False\n+\n+###################\n+# GZIP MIDDLEWARE #\n+###################\n+GZIP_MIDDLEWARE_RANDOM_BYTES = 100",
      "comment": "Good idea, I agree, using an attribute is much better!\r\n\r\nRegarding picking the number, I picked 100 because it was the highest (safest) number discussed the paper. I have reached out to the paper authors to get their opinion on a good number being default in Django but has not gotten any feedback from them (yet).",
      "comment_id": 1031794016,
      "user": "pelme",
      "created_at": "2022-11-24T19:46:11Z",
      "url": "https://github.com/django/django/pull/16311#discussion_r1031794016"
    },
    {
      "repo": "django/django",
      "pr_number": 16311,
      "file_path": "django/conf/global_settings.py",
      "line": 673,
      "side": "RIGHT",
      "diff_hunk": "@@ -666,3 +666,8 @@ def gettext_noop(s):\n SECURE_REFERRER_POLICY = \"same-origin\"\n SECURE_SSL_HOST = None\n SECURE_SSL_REDIRECT = False\n+\n+###################\n+# GZIP MIDDLEWARE #\n+###################\n+GZIP_MIDDLEWARE_RANDOM_BYTES = 100",
      "comment": "I think the only downside of using the highest number is the overhead of sending more bytes. However, the extra 50 bytes (average) does not feel that excessive to me. Especially since the other option may in many cases be to disable gzip altogether. ",
      "comment_id": 1031806426,
      "user": "pelme",
      "created_at": "2022-11-24T20:23:39Z",
      "url": "https://github.com/django/django/pull/16311#discussion_r1031806426"
    },
    {
      "repo": "django/django",
      "pr_number": 16311,
      "file_path": "django/utils/text.py",
      "line": 341,
      "side": "RIGHT",
      "diff_hunk": "@@ -314,8 +316,29 @@ def phone2numeric(phone):\n     return \"\".join(char2number.get(c, c) for c in phone.lower())\n \n \n-def compress_string(s):\n-    return gzip_compress(s, compresslevel=6, mtime=0)\n+def _get_random_fname(max_random_bytes):\n+    return b\"a\" * secrets.randbelow(max_random_bytes)\n+\n+\n+def compress_string(s, *, max_random_bytes=0):\n+    compressed_data = gzip_compress(s, compresslevel=6, mtime=0)\n+\n+    if max_random_bytes == 0:\n+        return compressed_data\n+\n+    header = bytearray(compressed_data[:10])\n+    header[3] = gzip.FNAME\n+\n+    fname = _get_random_fname(max_random_bytes)\n+\n+    return b\"\".join(\n+        [\n+            header,\n+            fname,\n+            b\"\\x00\",\n+            memoryview(compressed_data)[10:],\n+        ]\n+    )",
      "comment": "I guess you could create the `memoryview` earlier?\r\n\r\nAlso, `header[3]` contains flags, so we should, strictly speaking, use `header[3] |= gzip.FNAME`.\r\n\r\n```suggestion\r\n    compressed_view = memory_view(compressed_data)\r\n    header = bytearray(compressed_view[:10])\r\n    header[3] |= gzip.FNAME\r\n    fname = _get_random_fname(max_random_bytes)\r\n    return b\"\".join([header, fname, b\"\\x00\", compressed_view[10:]])\r\n```",
      "comment_id": 1032368266,
      "user": "ngnpope",
      "created_at": "2022-11-25T11:36:03Z",
      "url": "https://github.com/django/django/pull/16311#discussion_r1032368266"
    },
    {
      "repo": "django/django",
      "pr_number": 16311,
      "file_path": "django/utils/text.py",
      "line": 341,
      "side": "RIGHT",
      "diff_hunk": "@@ -314,8 +316,29 @@ def phone2numeric(phone):\n     return \"\".join(char2number.get(c, c) for c in phone.lower())\n \n \n-def compress_string(s):\n-    return gzip_compress(s, compresslevel=6, mtime=0)\n+def _get_random_fname(max_random_bytes):\n+    return b\"a\" * secrets.randbelow(max_random_bytes)\n+\n+\n+def compress_string(s, *, max_random_bytes=0):\n+    compressed_data = gzip_compress(s, compresslevel=6, mtime=0)\n+\n+    if max_random_bytes == 0:\n+        return compressed_data\n+\n+    header = bytearray(compressed_data[:10])\n+    header[3] = gzip.FNAME\n+\n+    fname = _get_random_fname(max_random_bytes)\n+\n+    return b\"\".join(\n+        [\n+            header,\n+            fname,\n+            b\"\\x00\",\n+            memoryview(compressed_data)[10:],\n+        ]\n+    )",
      "comment": "Also, not sure if it's cheaper, but we could do:\r\n```python\r\n    return header + fname + b\"\\x00\" + compressed_view[10:]\r\n```",
      "comment_id": 1032370390,
      "user": "ngnpope",
      "created_at": "2022-11-25T11:39:01Z",
      "url": "https://github.com/django/django/pull/16311#discussion_r1032370390"
    },
    {
      "repo": "django/django",
      "pr_number": 16311,
      "file_path": "django/utils/text.py",
      "line": 341,
      "side": "RIGHT",
      "diff_hunk": "@@ -314,8 +316,29 @@ def phone2numeric(phone):\n     return \"\".join(char2number.get(c, c) for c in phone.lower())\n \n \n-def compress_string(s):\n-    return gzip_compress(s, compresslevel=6, mtime=0)\n+def _get_random_fname(max_random_bytes):\n+    return b\"a\" * secrets.randbelow(max_random_bytes)\n+\n+\n+def compress_string(s, *, max_random_bytes=0):\n+    compressed_data = gzip_compress(s, compresslevel=6, mtime=0)\n+\n+    if max_random_bytes == 0:\n+        return compressed_data\n+\n+    header = bytearray(compressed_data[:10])\n+    header[3] = gzip.FNAME\n+\n+    fname = _get_random_fname(max_random_bytes)\n+\n+    return b\"\".join(\n+        [\n+            header,\n+            fname,\n+            b\"\\x00\",\n+            memoryview(compressed_data)[10:],\n+        ]\n+    )",
      "comment": "In my original code, I made put in `assert header[3] == 0` before setting the flag. I am not sure if other flags can potentially mess up the byte order of the filename or cause other problems. We do control the call to `gzip.compress` so there should never be anything else than 0 for the flags anyways. Do you have any input on that?\r\n\r\nI originally used + instead of \"\".join(...) but thought that maybe join would lead to fewer allocations since python can just the build the result directly instead of doing 3 concatenations. Have not measured it and have no idea if it is better or worse. The header, fname and null termination is tiny data and the memoryview avoids copies anyways (IIUC) so I'll change to +, it is cleaner!.",
      "comment_id": 1032581307,
      "user": "pelme",
      "created_at": "2022-11-25T16:13:29Z",
      "url": "https://github.com/django/django/pull/16311#discussion_r1032581307"
    },
    {
      "repo": "django/django",
      "pr_number": 16311,
      "file_path": "django/utils/text.py",
      "line": 341,
      "side": "RIGHT",
      "diff_hunk": "@@ -314,8 +316,29 @@ def phone2numeric(phone):\n     return \"\".join(char2number.get(c, c) for c in phone.lower())\n \n \n-def compress_string(s):\n-    return gzip_compress(s, compresslevel=6, mtime=0)\n+def _get_random_fname(max_random_bytes):\n+    return b\"a\" * secrets.randbelow(max_random_bytes)\n+\n+\n+def compress_string(s, *, max_random_bytes=0):\n+    compressed_data = gzip_compress(s, compresslevel=6, mtime=0)\n+\n+    if max_random_bytes == 0:\n+        return compressed_data\n+\n+    header = bytearray(compressed_data[:10])\n+    header[3] = gzip.FNAME\n+\n+    fname = _get_random_fname(max_random_bytes)\n+\n+    return b\"\".join(\n+        [\n+            header,\n+            fname,\n+            b\"\\x00\",\n+            memoryview(compressed_data)[10:],\n+        ]\n+    )",
      "comment": "Makes sense regarding the flags, thanks for the reasoning. Also, for the join or concat, whatever is faster \ud83d\ude42",
      "comment_id": 1032756943,
      "user": "ngnpope",
      "created_at": "2022-11-26T08:21:45Z",
      "url": "https://github.com/django/django/pull/16311#discussion_r1032756943"
    },
    {
      "repo": "django/django",
      "pr_number": 16311,
      "file_path": "tests/middleware/tests.py",
      "line": 995,
      "side": "RIGHT",
      "diff_hunk": "@@ -978,12 +979,57 @@ def test_compress_deterministic(self):\n         ConditionalGetMiddleware from recognizing conditional matches\n         on gzipped content).\n         \"\"\"\n-        r1 = GZipMiddleware(self.get_response)(self.req)\n-        r2 = GZipMiddleware(self.get_response)(self.req)\n+\n+        class DeterministicGZipMiddleware(GZipMiddleware):\n+            max_random_bytes = 0\n+\n+        r1 = DeterministicGZipMiddleware(self.get_response)(self.req)\n+        r2 = DeterministicGZipMiddleware(self.get_response)(self.req)\n         self.assertEqual(r1.content, r2.content)\n         self.assertEqual(self.get_mtime(r1.content), 0)\n         self.assertEqual(self.get_mtime(r2.content), 0)\n \n+    def test_random_bytes(self):\n+        \"\"\"\n+        A random number of bytes is added to mitigate the BREACH attack:\n+        https://breachattack.com/",
      "comment": "on a second thought, I cant really think of a better wording, leaving it as-is",
      "comment_id": 1032811344,
      "user": "pelme",
      "created_at": "2022-11-26T17:05:48Z",
      "url": "https://github.com/django/django/pull/16311#discussion_r1032811344"
    },
    {
      "repo": "django/django",
      "pr_number": 16311,
      "file_path": "django/middleware/gzip.py",
      "line": 16,
      "side": "RIGHT",
      "diff_hunk": "@@ -13,6 +13,8 @@ class GZipMiddleware(MiddlewareMixin):\n     on the Accept-Encoding header.\n     \"\"\"\n \n+    max_random_bytes = 100",
      "comment": "We should document `max_random_bytes` in the `GZipMiddleware` docs.",
      "comment_id": 1049292009,
      "user": "felixxm",
      "created_at": "2022-12-15T07:21:29Z",
      "url": "https://github.com/django/django/pull/16311#discussion_r1049292009"
    },
    {
      "repo": "django/django",
      "pr_number": 16311,
      "file_path": "django/utils/text.py",
      "line": 323,
      "side": "RIGHT",
      "diff_hunk": "@@ -314,8 +316,23 @@ def phone2numeric(phone):\n     return \"\".join(char2number.get(c, c) for c in phone.lower())\n \n \n-def compress_string(s):\n-    return gzip_compress(s, compresslevel=6, mtime=0)\n+def _get_random_filename(max_random_bytes):\n+    return b\"a\" * secrets.randbelow(max_random_bytes)\n+\n+\n+def compress_string(s, *, max_random_bytes=0):",
      "comment": "I would change the default to `None` (in both functions):\r\n```suggestion\r\ndef compress_string(s, *, max_random_bytes=None):\r\n```",
      "comment_id": 1049292752,
      "user": "felixxm",
      "created_at": "2022-12-15T07:22:39Z",
      "url": "https://github.com/django/django/pull/16311#discussion_r1049292752"
    },
    {
      "repo": "django/django",
      "pr_number": 16311,
      "file_path": "tests/middleware/tests.py",
      "line": 1004,
      "side": "RIGHT",
      "diff_hunk": "@@ -978,12 +979,57 @@ def test_compress_deterministic(self):\n         ConditionalGetMiddleware from recognizing conditional matches\n         on gzipped content).\n         \"\"\"\n-        r1 = GZipMiddleware(self.get_response)(self.req)\n-        r2 = GZipMiddleware(self.get_response)(self.req)\n+\n+        class DeterministicGZipMiddleware(GZipMiddleware):\n+            max_random_bytes = 0\n+\n+        r1 = DeterministicGZipMiddleware(self.get_response)(self.req)\n+        r2 = DeterministicGZipMiddleware(self.get_response)(self.req)\n         self.assertEqual(r1.content, r2.content)\n         self.assertEqual(self.get_mtime(r1.content), 0)\n         self.assertEqual(self.get_mtime(r2.content), 0)\n \n+    def test_random_bytes(self):\n+        \"\"\"\n+        A random number of bytes is added to mitigate the BREACH attack:\n+        https://breachattack.com/\n+        \"\"\"\n+        with mock.patch(\n+            \"django.utils.text.secrets.randbelow\", autospec=True, return_value=3\n+        ):\n+            r = GZipMiddleware(self.get_response)(self.req)\n+\n+        # The fourth byte of a gzip stream contains flags.\n+        self.assertEqual(r.content[3], gzip.FNAME)\n+        # Ensure we got a 3 byte filename \"aaa\" and a null byte",
      "comment": "To add more explanation, comments should state the expected behavior and omit prefixes like \"Ensure\" since all assertions ensure things. This guideline is from [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).\r\n",
      "comment_id": 1049294516,
      "user": "felixxm",
      "created_at": "2022-12-15T07:25:20Z",
      "url": "https://github.com/django/django/pull/16311#discussion_r1049294516"
    },
    {
      "repo": "django/django",
      "pr_number": 16311,
      "file_path": "django/conf/global_settings.py",
      "line": 673,
      "side": "RIGHT",
      "diff_hunk": "@@ -666,3 +666,8 @@ def gettext_noop(s):\n SECURE_REFERRER_POLICY = \"same-origin\"\n SECURE_SSL_HOST = None\n SECURE_SSL_REDIRECT = False\n+\n+###################\n+# GZIP MIDDLEWARE #\n+###################\n+GZIP_MIDDLEWARE_RANDOM_BYTES = 100",
      "comment": "For the record, after this PR was merged, I received this reply from Rafael Palacios, the author of the HTB paper:\r\n\r\n> We made our test using n=10 and n=100. Complexity for n=100 is huge, probably enough to use n=10 if there is an intrusion detection system, but safer in any scenario for n=100.",
      "comment_id": 2463728229,
      "user": "pelme",
      "created_at": "2025-10-26T10:49:24Z",
      "url": "https://github.com/django/django/pull/16311#discussion_r2463728229"
    },
    {
      "repo": "django/django",
      "pr_number": 19997,
      "file_path": "tests/update/tests.py",
      "line": 317,
      "side": "RIGHT",
      "diff_hunk": "@@ -311,6 +311,11 @@ def test_updating_non_conditional_field(self):\n         with self.assertRaisesMessage(TypeError, msg):\n             DataPoint.objects.update(is_active=~F(\"name\"))\n \n+    def test_disallowed_update_distinct_on(self):\n+        msg = \"Cannot call update() after .distinct(*fields).\"\n+        with self.assertRaisesMessage(TypeError, msg):\n+            DataPoint.objects.distinct(\"id\").update(is_active=False)",
      "comment": "A thought: how about a field other than the primary key? That would model more realistic usage.\n\nThen, for the same reason, since this this syntax is only available on Postgres, I think we should move this test to the `DistinctOnTests` (even though this error still rises even when attempted on other dbs).",
      "comment_id": 2462046066,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-24T21:35:18Z",
      "url": "https://github.com/django/django/pull/19997#discussion_r2462046066"
    },
    {
      "repo": "django/django",
      "pr_number": 19997,
      "file_path": "tests/update/tests.py",
      "line": 317,
      "side": "RIGHT",
      "diff_hunk": "@@ -311,6 +311,11 @@ def test_updating_non_conditional_field(self):\n         with self.assertRaisesMessage(TypeError, msg):\n             DataPoint.objects.update(is_active=~F(\"name\"))\n \n+    def test_disallowed_update_distinct_on(self):\n+        msg = \"Cannot call update() after .distinct(*fields).\"\n+        with self.assertRaisesMessage(TypeError, msg):\n+            DataPoint.objects.distinct(\"id\").update(is_active=False)",
      "comment": "Thanks for the feedback! \r\n\r\nTest was moved to `DistinctOnTests` and now performs a `distinct` on the `organization` field instead of `id` field. Please let me know if you spot any additional changes.",
      "comment_id": 2462113507,
      "user": "matthewshirley",
      "created_at": "2025-10-24T22:01:45Z",
      "url": "https://github.com/django/django/pull/19997#discussion_r2462113507"
    },
    {
      "repo": "django/django",
      "pr_number": 19986,
      "file_path": "tests/utils_tests/utils.py",
      "line": 20,
      "side": "RIGHT",
      "diff_hunk": "@@ -12,3 +14,25 @@ def on_macos_with_hfs():\n         parsed_macos_version = tuple(int(x) for x in macos_version.split(\".\"))\n         return parsed_macos_version < (10, 14)\n     return False\n+\n+\n+@contextmanager\n+def gc_debug_context():",
      "comment": "A context manager is the right approach, but I was expecting to factor out the guts of `test_release_memory_without_garbage_collection` and use it both here and there.",
      "comment_id": 2452975018,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-22T18:23:22Z",
      "url": "https://github.com/django/django/pull/19986#discussion_r2452975018"
    },
    {
      "repo": "django/django",
      "pr_number": 19998,
      "file_path": "tests/bulk_create/tests.py",
      "line": 893,
      "side": "RIGHT",
      "diff_hunk": "@@ -890,18 +890,20 @@ class BulkCreateTransactionTests(TransactionTestCase):\n     available_apps = [\"bulk_create\"]\n \n     def test_no_unnecessary_transaction(self):\n+        unused_id = getattr(Country.objects.last(), \"id\", 10) + 100",
      "comment": "Maybe\n\n\n```suggestion\n        # Find a serial ID that hasn't been used already and has enough of a buffer for the\n        # following `bulk_create` call without an explicit pk not to conflict.\n        unused_id = getattr(Country.objects.last(), \"id\", 10) + 100\n```",
      "comment_id": 2460487611,
      "user": "charettes",
      "created_at": "2025-10-24T13:31:20Z",
      "url": "https://github.com/django/django/pull/19998#discussion_r2460487611"
    },
    {
      "repo": "django/django",
      "pr_number": 19933,
      "file_path": "django/conf/locale/de_CH/formats.py",
      "line": 32,
      "side": "LEFT",
      "diff_hunk": "@@ -25,11 +25,9 @@\n     \"%d.%m.%Y %H:%M\",  # '25.10.2006 14:30'\n ]\n \n-# these are the separators for non-monetary numbers. For monetary numbers,\n-# the DECIMAL_SEPARATOR is a . (decimal point) and the THOUSAND_SEPARATOR is a\n-# ' (single quote).\n-# For details, please refer to the documentation and the following link:\n-# https://www.bk.admin.ch/bk/de/home/dokumentation/sprachen/hilfsmittel-textredaktion/schreibweisungen.html",
      "comment": "Did you come to a conclusion about the usefulness of that link?",
      "comment_id": 2453292979,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-22T20:35:57Z",
      "url": "https://github.com/django/django/pull/19933#discussion_r2453292979"
    },
    {
      "repo": "django/django",
      "pr_number": 19933,
      "file_path": "django/conf/locale/de_CH/formats.py",
      "line": 32,
      "side": "LEFT",
      "diff_hunk": "@@ -25,11 +25,9 @@\n     \"%d.%m.%Y %H:%M\",  # '25.10.2006 14:30'\n ]\n \n-# these are the separators for non-monetary numbers. For monetary numbers,\n-# the DECIMAL_SEPARATOR is a . (decimal point) and the THOUSAND_SEPARATOR is a\n-# ' (single quote).\n-# For details, please refer to the documentation and the following link:\n-# https://www.bk.admin.ch/bk/de/home/dokumentation/sprachen/hilfsmittel-textredaktion/schreibweisungen.html",
      "comment": "I removed the link because of your [comment:29](https://code.djangoproject.com/ticket/35095#comment:29) suggesting it, which convinced me. Also, the document is in German. In English, a [styleguide for English-language translators](https://www.bk.admin.ch/bk/en/home/dokumentation/languages/hilfsmittel-textredaktion.html) is the only similar document available.",
      "comment_id": 2454855104,
      "user": "annalauraw",
      "created_at": "2025-10-23T11:48:09Z",
      "url": "https://github.com/django/django/pull/19933#discussion_r2454855104"
    },
    {
      "repo": "django/django",
      "pr_number": 19982,
      "file_path": "tests/test_runner/test_parallel.py",
      "line": 172,
      "side": "RIGHT",
      "diff_hunk": "@@ -163,6 +166,13 @@ def test_picklable(self):\n         loaded_result = pickle.loads(pickle.dumps(result))\n         self.assertEqual(result.events, loaded_result.events)\n \n+    @unittest.skipIf(\n+        tblib and tblib_version >= (3, 2),\n+        \"tblib 3.2.0+ makes exception subclasses with __init__() and the \"\n+        \"default ___reduce__() picklable. Therefore, \"",
      "comment": "```suggestion\n        \"default __reduce__() picklable. Therefore, \"\n```",
      "comment_id": 2449470656,
      "user": "cliffordgama",
      "created_at": "2025-10-21T19:24:02Z",
      "url": "https://github.com/django/django/pull/19982#discussion_r2449470656"
    },
    {
      "repo": "django/django",
      "pr_number": 19593,
      "file_path": "django/core/servers/basehttp.py",
      "line": 193,
      "side": "RIGHT",
      "diff_hunk": "@@ -182,35 +183,27 @@ def address_string(self):\n         return self.client_address[0]\n \n     def log_message(self, format, *args):\n-        extra = {\n-            \"request\": self.request,\n-            \"server_time\": self.log_date_time_string(),\n-        }\n-        if args[1][0] == \"4\":\n+        if args[1][0] == \"4\" and args[0].startswith(\"\\x16\\x03\"):\n             # 0x16 = Handshake, 0x03 = SSL 3.0 or TLS 1.x\n-            if args[0].startswith(\"\\x16\\x03\"):\n-                extra[\"status_code\"] = 500\n-                logger.error(\n-                    \"You're accessing the development server over HTTPS, but \"\n-                    \"it only supports HTTP.\",\n-                    extra=extra,\n-                )\n-                return\n-\n-        if args[1].isdigit() and len(args[1]) == 3:\n+            format = (\n+                \"You're accessing the development server over HTTPS, but it only \"\n+                \"supports HTTP.\"\n+            )\n+            status_code = 500\n+            args = ()",
      "comment": "Issue: This concerns me slightly, as it overrides the `*args*` passed in to the function. I suspect duplicating the `log_message` call here again is fine.",
      "comment_id": 2416037696,
      "user": "RealOrangeOne",
      "created_at": "2025-10-09T08:50:47Z",
      "url": "https://github.com/django/django/pull/19593#discussion_r2416037696"
    },
    {
      "repo": "django/django",
      "pr_number": 19593,
      "file_path": "django/core/servers/basehttp.py",
      "line": 193,
      "side": "RIGHT",
      "diff_hunk": "@@ -182,35 +183,27 @@ def address_string(self):\n         return self.client_address[0]\n \n     def log_message(self, format, *args):\n-        extra = {\n-            \"request\": self.request,\n-            \"server_time\": self.log_date_time_string(),\n-        }\n-        if args[1][0] == \"4\":\n+        if args[1][0] == \"4\" and args[0].startswith(\"\\x16\\x03\"):\n             # 0x16 = Handshake, 0x03 = SSL 3.0 or TLS 1.x\n-            if args[0].startswith(\"\\x16\\x03\"):\n-                extra[\"status_code\"] = 500\n-                logger.error(\n-                    \"You're accessing the development server over HTTPS, but \"\n-                    \"it only supports HTTP.\",\n-                    extra=extra,\n-                )\n-                return\n-\n-        if args[1].isdigit() and len(args[1]) == 3:\n+            format = (\n+                \"You're accessing the development server over HTTPS, but it only \"\n+                \"supports HTTP.\"\n+            )\n+            status_code = 500\n+            args = ()",
      "comment": "But this was the case before, right? for this logging, the message and the args were completely ignored/dropped.",
      "comment_id": 2416527928,
      "user": "nessita",
      "created_at": "2025-10-09T11:51:18Z",
      "url": "https://github.com/django/django/pull/19593#discussion_r2416527928"
    },
    {
      "repo": "django/django",
      "pr_number": 19593,
      "file_path": "django/core/servers/basehttp.py",
      "line": 193,
      "side": "RIGHT",
      "diff_hunk": "@@ -182,35 +183,27 @@ def address_string(self):\n         return self.client_address[0]\n \n     def log_message(self, format, *args):\n-        extra = {\n-            \"request\": self.request,\n-            \"server_time\": self.log_date_time_string(),\n-        }\n-        if args[1][0] == \"4\":\n+        if args[1][0] == \"4\" and args[0].startswith(\"\\x16\\x03\"):\n             # 0x16 = Handshake, 0x03 = SSL 3.0 or TLS 1.x\n-            if args[0].startswith(\"\\x16\\x03\"):\n-                extra[\"status_code\"] = 500\n-                logger.error(\n-                    \"You're accessing the development server over HTTPS, but \"\n-                    \"it only supports HTTP.\",\n-                    extra=extra,\n-                )\n-                return\n-\n-        if args[1].isdigit() and len(args[1]) == 3:\n+            format = (\n+                \"You're accessing the development server over HTTPS, but it only \"\n+                \"supports HTTP.\"\n+            )\n+            status_code = 500\n+            args = ()",
      "comment": "I agree that we were ignoring the args earlier, but i think duplicating the **log_message** makes the code much cleaner. So here i agree with @RealOrangeOne approach here. \r\n\r\nApart from this, i think everything else is fine.",
      "comment_id": 2417485541,
      "user": "YashRaj1506",
      "created_at": "2025-10-09T17:37:10Z",
      "url": "https://github.com/django/django/pull/19593#discussion_r2417485541"
    },
    {
      "repo": "django/django",
      "pr_number": 19593,
      "file_path": "django/core/servers/basehttp.py",
      "line": 193,
      "side": "RIGHT",
      "diff_hunk": "@@ -182,35 +183,27 @@ def address_string(self):\n         return self.client_address[0]\n \n     def log_message(self, format, *args):\n-        extra = {\n-            \"request\": self.request,\n-            \"server_time\": self.log_date_time_string(),\n-        }\n-        if args[1][0] == \"4\":\n+        if args[1][0] == \"4\" and args[0].startswith(\"\\x16\\x03\"):\n             # 0x16 = Handshake, 0x03 = SSL 3.0 or TLS 1.x\n-            if args[0].startswith(\"\\x16\\x03\"):\n-                extra[\"status_code\"] = 500\n-                logger.error(\n-                    \"You're accessing the development server over HTTPS, but \"\n-                    \"it only supports HTTP.\",\n-                    extra=extra,\n-                )\n-                return\n-\n-        if args[1].isdigit() and len(args[1]) == 3:\n+            format = (\n+                \"You're accessing the development server over HTTPS, but it only \"\n+                \"supports HTTP.\"\n+            )\n+            status_code = 500\n+            args = ()",
      "comment": "Adding something like this makes sense:\r\n\r\n```\r\nif args[1][0] == \"4\" and args[0].startswith(\"\\x16\\x03\"):\r\n    log_message(\r\n        logger,\r\n        \"You're accessing the development server over HTTPS, but it only supports HTTP.\",\r\n        request=self.request,\r\n        status_code=500,\r\n        server_time=self.log_date_time_string(),\r\n    )\r\n    return\r\n```",
      "comment_id": 2417492514,
      "user": "YashRaj1506",
      "created_at": "2025-10-09T17:40:35Z",
      "url": "https://github.com/django/django/pull/19593#discussion_r2417492514"
    },
    {
      "repo": "django/django",
      "pr_number": 19593,
      "file_path": "django/core/servers/basehttp.py",
      "line": 193,
      "side": "RIGHT",
      "diff_hunk": "@@ -182,35 +183,27 @@ def address_string(self):\n         return self.client_address[0]\n \n     def log_message(self, format, *args):\n-        extra = {\n-            \"request\": self.request,\n-            \"server_time\": self.log_date_time_string(),\n-        }\n-        if args[1][0] == \"4\":\n+        if args[1][0] == \"4\" and args[0].startswith(\"\\x16\\x03\"):\n             # 0x16 = Handshake, 0x03 = SSL 3.0 or TLS 1.x\n-            if args[0].startswith(\"\\x16\\x03\"):\n-                extra[\"status_code\"] = 500\n-                logger.error(\n-                    \"You're accessing the development server over HTTPS, but \"\n-                    \"it only supports HTTP.\",\n-                    extra=extra,\n-                )\n-                return\n-\n-        if args[1].isdigit() and len(args[1]) == 3:\n+            format = (\n+                \"You're accessing the development server over HTTPS, but it only \"\n+                \"supports HTTP.\"\n+            )\n+            status_code = 500\n+            args = ()",
      "comment": "Thank you both for your comments, but after some further consideration I'll leave the single call to the helper as proposed.\r\n\r\nThe rationale is that, in the HTTPS case, we already override the log message, so overriding `args` feels not only correct but necessary, since the message does not interpolate them. Keeping the current structure to reuse the helper for consistency and clarity.",
      "comment_id": 2445782052,
      "user": "nessita",
      "created_at": "2025-10-20T18:25:39Z",
      "url": "https://github.com/django/django/pull/19593#discussion_r2445782052"
    },
    {
      "repo": "django/django",
      "pr_number": 19968,
      "file_path": "django/db/backends/postgresql/compiler.py",
      "line": 6,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,10 +1,10 @@\n-from django.db.models.sql.compiler import (\n+from django.db.models.sql.compiler import (  # isort:skip\n     SQLAggregateCompiler,\n     SQLCompiler,\n     SQLDeleteCompiler,\n+    SQLUpdateCompiler,",
      "comment": "(I imagine this is where this import first was before isort tried to alphabetize it.)",
      "comment_id": 2436261665,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-16T14:38:46Z",
      "url": "https://github.com/django/django/pull/19968#discussion_r2436261665"
    },
    {
      "repo": "django/django",
      "pr_number": 19968,
      "file_path": "django/db/backends/postgresql/compiler.py",
      "line": 6,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,10 +1,10 @@\n-from django.db.models.sql.compiler import (\n+from django.db.models.sql.compiler import (  # isort:skip\n     SQLAggregateCompiler,\n     SQLCompiler,\n     SQLDeleteCompiler,\n+    SQLUpdateCompiler,",
      "comment": "If we are going down this path (isort skip), I would rather we do:\n\n```python\nfrom django.db.models.sql.compiler import (\n    SQLAggregateCompiler,\n    SQLCompiler,\n    SQLDeleteCompiler,\n    SQLInsertCompiler as BaseSQLInsertCompiler,\n    SQLUpdateCompiler,\n)\n```",
      "comment_id": 2436470108,
      "user": "nessita",
      "created_at": "2025-10-16T15:27:23Z",
      "url": "https://github.com/django/django/pull/19968#discussion_r2436470108"
    },
    {
      "repo": "django/django",
      "pr_number": 19968,
      "file_path": "tests/contenttypes_tests/test_views.py",
      "line": 21,
      "side": "RIGHT",
      "diff_hunk": "@@ -17,9 +19,8 @@\n     ModelWithM2MToSite,\n     ModelWithNullFKToSite,\n     SchemeIncludedURL,\n+    UUIDModel,",
      "comment": "As before, if we are skipping, I would rather:\n```python\nfrom .models import (\n    Article,\n    Author,\n    FooWithBrokenAbsoluteUrl,\n    FooWithoutUrl,\n    FooWithUrl,\n    ModelWithM2MToSite,\n    ModelWithNullFKToSite,\n    SchemeIncludedURL,\n    Site as MockSite,\n    UUIDModel,\n)\n```",
      "comment_id": 2436476032,
      "user": "nessita",
      "created_at": "2025-10-16T15:28:49Z",
      "url": "https://github.com/django/django/pull/19968#discussion_r2436476032"
    },
    {
      "repo": "django/django",
      "pr_number": 19957,
      "file_path": "tests/gis_tests/geoapp/test_functions.py",
      "line": 950,
      "side": "RIGHT",
      "diff_hunk": "@@ -939,10 +939,19 @@ def test_geometry_type(self):\n             (\"POINT\", Point),\n             (\"LINESTRING\", LineString),\n             (\"POLYGON\", Polygon),\n-            (\"MULTIPOINT\", MultiPoint),\n             (\"MULTILINESTRING\", MultiLineString),\n             (\"MULTIPOLYGON\", MultiPolygon),\n         ]\n+        # GEOSWKTWriter_write() behavior was changed in GEOS 3.12+ to include\n+        # parentheses for sub-members. MariaDB doesn't accept WKT\n+        # representations with additional parentheses for MultiPoint. This is\n+        # an accepted bug (MDEV-36166) in MariaDB that should be fixed in the\n+        # future.\n+        if not connection.ops.mariadb or geos_version_tuple() < (3, 12):",
      "comment": "TIL that we set vendor name as booleans on `BaseSpatialOperations` instead of using features per-backend instead (I was expecting this to raise `AttributeError` or non-MySQL based backends).",
      "comment_id": 2432338490,
      "user": "charettes",
      "created_at": "2025-10-15T12:17:25Z",
      "url": "https://github.com/django/django/pull/19957#discussion_r2432338490"
    },
    {
      "repo": "django/django",
      "pr_number": 19955,
      "file_path": "django/db/models/query.py",
      "line": 2154,
      "side": "RIGHT",
      "diff_hunk": "@@ -2141,13 +2141,17 @@ def _check_operator_queryset(self, other, operator_):\n             raise TypeError(f\"Cannot use {operator_} operator with combined queryset.\")\n \n     def _check_ordering_first_last_queryset_aggregation(self, method):\n-        if isinstance(self.query.group_by, tuple) and not any(\n-            col.output_field is self.model._meta.pk for col in self.query.group_by\n-        ):\n-            raise TypeError(\n-                f\"Cannot use QuerySet.{method}() on an unordered queryset performing \"\n-                f\"aggregation. Add an ordering with order_by().\"\n-            )\n+        if isinstance(self.query.group_by, tuple):\n+            # Raise if the pk fields are not already in the group_by.\n+            if self.model._meta.pk not in {\n+                col.output_field for col in self.query.group_by\n+            } and set(self.model._meta.pk_fields).difference(\n+                {col.target for col in self.query.group_by}\n+            ):\n+                raise TypeError(\n+                    f\"Cannot use QuerySet.{method}() on an unordered queryset \"\n+                    \"performing aggregation. Add an ordering with order_by().\"\n+                )",
      "comment": "Could we avoid the extra indentation level and nested `if`, since all the checks need to hold as an `and`?\n\n```suggestion\n        if (\n            isinstance(self.query.group_by, tuple)\n            # Raise if the pk fields are not in the group_by.\n            and self.model._meta.pk\n            not in {col.output_field for col in self.query.group_by}\n            and set(self.model._meta.pk_fields).difference(\n                {col.target for col in self.query.group_by}\n            )\n        ):\n            raise TypeError(\n                f\"Cannot use QuerySet.{method}() on an unordered queryset performing \"\n                \"aggregation. Add an ordering with order_by().\"\n            )\n```",
      "comment_id": 2429980006,
      "user": "nessita",
      "created_at": "2025-10-14T17:44:16Z",
      "url": "https://github.com/django/django/pull/19955#discussion_r2429980006"
    },
    {
      "repo": "django/django",
      "pr_number": 19492,
      "file_path": "tests/admin_views/test_nav_sidebar.py",
      "line": 94,
      "side": "LEFT",
      "diff_hunk": "@@ -91,9 +91,7 @@ def test_sidebar_aria_current_page_missing_without_request_context_processor(sel\n         self.assertContains(\n             response, '<nav class=\"sticky\" id=\"nav-sidebar\" aria-label=\"Sidebar\">'\n         )\n-        # Does not include aria-current attribute.",
      "comment": "I would keep this comment\r\nThe test above has\r\n```\r\n      self.assertContains(\r\n            response, '<a href=\"%s\" aria-current=\"page\">Users</a>' % url\r\n        )\r\n```\r\nSo it's testing this Users link doesn't have aria-current=\"page\"",
      "comment_id": 2163215331,
      "user": "sarahboyce",
      "created_at": "2025-06-24T07:51:53Z",
      "url": "https://github.com/django/django/pull/19492#discussion_r2163215331"
    },
    {
      "repo": "django/django",
      "pr_number": 19884,
      "file_path": "django/db/models/base.py",
      "line": 2223,
      "side": "RIGHT",
      "diff_hunk": "@@ -2220,6 +2220,15 @@ def _check_local_fields(cls, fields, option):\n                             id=\"models.E048\",\n                         )\n                     )\n+                elif type(field) is models.ForeignObject:",
      "comment": "If we want to make this more resilient against subclassing, we could borrow the logic here:\r\n\r\nhttps://github.com/django/django/blob/7528979153355faa49c3e49ba3ea233f998a7583/django/db/models/base.py#L1379-L1382",
      "comment_id": 2363158285,
      "user": "jacobtylerwalls",
      "created_at": "2025-09-19T14:47:19Z",
      "url": "https://github.com/django/django/pull/19884#discussion_r2363158285"
    },
    {
      "repo": "django/django",
      "pr_number": 19884,
      "file_path": "django/db/models/base.py",
      "line": 2227,
      "side": "RIGHT",
      "diff_hunk": "@@ -2220,6 +2220,15 @@ def _check_local_fields(cls, fields, option):\n                             id=\"models.E048\",\n                         )\n                     )\n+                elif type(field) is models.ForeignObject:\n+                    errors.append(\n+                        checks.Error(\n+                            f\"{option!r} refers to a ForeignObject {field_name!r},\"\n+                            f\"but ForeignObject are not permitted in {option!r}.\",",
      "comment": "```suggestion\r\n                            f\"but ForeignObjects are not permitted in {option!r}.\",\r\n```",
      "comment_id": 2363159095,
      "user": "jacobtylerwalls",
      "created_at": "2025-09-19T14:47:31Z",
      "url": "https://github.com/django/django/pull/19884#discussion_r2363159095"
    },
    {
      "repo": "django/django",
      "pr_number": 19884,
      "file_path": "tests/foreign_object/tests.py",
      "line": 776,
      "side": "RIGHT",
      "diff_hunk": "@@ -771,6 +773,14 @@ def test_pickling_foreignobject(self):\n \n \n class ForeignObjectModelValidationTests(TestCase):\n+    def setUp(self):\n+        CustomerTab._meta.constraints = [",
      "comment": "I bet we could move these to `invalid_models_tests.test_models.ConstraintsTests` to avoid the hack, but I didn't look too closely.",
      "comment_id": 2363189936,
      "user": "jacobtylerwalls",
      "created_at": "2025-09-19T14:54:08Z",
      "url": "https://github.com/django/django/pull/19884#discussion_r2363189936"
    },
    {
      "repo": "django/django",
      "pr_number": 19884,
      "file_path": "tests/foreign_object/tests.py",
      "line": 776,
      "side": "RIGHT",
      "diff_hunk": "@@ -771,6 +773,14 @@ def test_pickling_foreignobject(self):\n \n \n class ForeignObjectModelValidationTests(TestCase):\n+    def setUp(self):\n+        CustomerTab._meta.constraints = [",
      "comment": "Looking again, that doesn't look very promising :-)",
      "comment_id": 2380036104,
      "user": "jacobtylerwalls",
      "created_at": "2025-09-25T18:48:04Z",
      "url": "https://github.com/django/django/pull/19884#discussion_r2380036104"
    },
    {
      "repo": "django/django",
      "pr_number": 19884,
      "file_path": "tests/foreign_object/tests.py",
      "line": 776,
      "side": "RIGHT",
      "diff_hunk": "@@ -771,6 +773,14 @@ def test_pickling_foreignobject(self):\n \n \n class ForeignObjectModelValidationTests(TestCase):\n+    def setUp(self):\n+        CustomerTab._meta.constraints = [",
      "comment": "yeah you want to avoid altering model definition at run time like this it's doomed to cause a ton of problems.",
      "comment_id": 2380040497,
      "user": "charettes",
      "created_at": "2025-09-25T18:49:42Z",
      "url": "https://github.com/django/django/pull/19884#discussion_r2380040497"
    },
    {
      "repo": "django/django",
      "pr_number": 19884,
      "file_path": "django/db/models/base.py",
      "line": 2227,
      "side": "RIGHT",
      "diff_hunk": "@@ -2220,6 +2220,18 @@ def _check_local_fields(cls, fields, option):\n                             id=\"models.E048\",\n                         )\n                     )\n+                elif (\n+                    isinstance(field.remote_field, ForeignObjectRel)\n+                    and field not in cls._meta.local_concrete_fields\n+                ):",
      "comment": "How about we narrow this down to:\r\n```suggestion\r\n                    and len(getattr(field, \"from_fields\", [])) > 1\r\n                ):\r\n```",
      "comment_id": 2388713164,
      "user": "nessita",
      "created_at": "2025-09-29T17:31:47Z",
      "url": "https://github.com/django/django/pull/19884#discussion_r2388713164"
    },
    {
      "repo": "django/django",
      "pr_number": 19884,
      "file_path": "django/db/models/base.py",
      "line": 2230,
      "side": "RIGHT",
      "diff_hunk": "@@ -2220,6 +2220,18 @@ def _check_local_fields(cls, fields, option):\n                             id=\"models.E048\",\n                         )\n                     )\n+                elif (\n+                    isinstance(field.remote_field, ForeignObjectRel)\n+                    and field not in cls._meta.local_concrete_fields\n+                ):\n+                    errors.append(\n+                        checks.Error(\n+                            f\"{option!r} refers to a ForeignObject {field_name!r}, \"\n+                            f\"but ForeignObjects are not permitted in {option!r}.\",",
      "comment": "With message:\r\n```suggestion\r\n                            f\"{option!r} refers to a ForeignObject {field_name!r} \"\r\n                            f\"with multiple target fields, which is not supported.\",\r\n```",
      "comment_id": 2388714563,
      "user": "nessita",
      "created_at": "2025-09-29T17:32:26Z",
      "url": "https://github.com/django/django/pull/19884#discussion_r2388714563"
    },
    {
      "repo": "django/django",
      "pr_number": 19884,
      "file_path": "tests/invalid_models_tests/test_models.py",
      "line": 216,
      "side": "RIGHT",
      "diff_hunk": "@@ -166,6 +166,34 @@ class Meta:\n             ],\n         )\n \n+    def test_pointing_to_foreign_object(self):\n+        class Reference(models.Model):\n+            reference_id = models.IntegerField(unique=True)\n+\n+        class ReferenceTab(models.Model):\n+            reference_id = models.IntegerField()\n+            reference = models.ForeignObject(\n+                Reference,\n+                from_fields=[\"reference_id\"],\n+                to_fields=[\"reference_id\"],\n+                on_delete=models.CASCADE,\n+            )\n+\n+            class Meta:\n+                unique_together = [[\"reference\"]]\n+\n+        self.assertEqual(\n+            ReferenceTab.check(),\n+            [\n+                Error(\n+                    \"'unique_together' refers to a ForeignObject 'reference', but \"\n+                    \"ForeignObjects are not permitted in 'unique_together'.\",\n+                    obj=ReferenceTab,\n+                    id=\"models.E049\",\n+                ),\n+            ],\n+        )\n+",
      "comment": "```suggestion\r\n        self.assertEqual(ReferenceTab.check(), [])\r\n\r\n    def test_pointing_to_foreign_object_multi_column(self):\r\n        class Reference(models.Model):\r\n            reference_id = models.IntegerField(unique=True)\r\n            code = models.CharField(max_length=1)\r\n\r\n        class ReferenceTabMultiple(models.Model):\r\n            reference_id = models.IntegerField()\r\n            code = models.CharField(max_length=1)\r\n            reference = models.ForeignObject(\r\n                Reference,\r\n                from_fields=[\"reference_id\", \"code\"],\r\n                to_fields=[\"reference_id\", \"code\"],\r\n                on_delete=models.CASCADE,\r\n            )\r\n\r\n            class Meta:\r\n                unique_together = [[\"reference\"]]\r\n\r\n        self.assertEqual(\r\n            ReferenceTabMultiple.check(),\r\n            [\r\n                Error(\r\n                    \"'unique_together' refers to a ForeignObject 'reference' \"\r\n                    \"with multiple target fields, which is not supported.\",\r\n                    obj=ReferenceTabMultiple,\r\n                    id=\"models.E049\",\r\n                ),\r\n            ],\r\n        )\r\n\r\n```",
      "comment_id": 2388798523,
      "user": "nessita",
      "created_at": "2025-09-29T18:03:52Z",
      "url": "https://github.com/django/django/pull/19884#discussion_r2388798523"
    },
    {
      "repo": "django/django",
      "pr_number": 19884,
      "file_path": "tests/foreign_object/tests.py",
      "line": 795,
      "side": "RIGHT",
      "diff_hunk": "@@ -770,16 +770,39 @@ def test_pickling_foreignobject(self):\n         self.assertIn(\"reverse_path_infos\", foreign_object_restored.__dict__)\n \n \n-class ForeignObjectModelValidationTests(TestCase):\n-    @skipUnlessDBFeature(\"supports_table_check_constraints\")\n+@isolate_apps(\"foreign_object\")\n+@skipUnlessDBFeature(\"supports_table_check_constraints\")\n+class ForeignObjectConstraintTests(TestCase):\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n+\n+        class CustomerTab(models.Model):\n+            customer_id = models.IntegerField()\n+            customer = models.ForeignObject(\n+                \"Customer\",\n+                from_fields=[\"customer_id\"],\n+                to_fields=[\"id\"],\n+                on_delete=models.CASCADE,\n+            )\n+\n+            class Meta:\n+                constraints = [\n+                    models.CheckConstraint(\n+                        condition=Q(customer__lt=1000),\n+                        name=\"customer_id_limit\",\n+                    ),\n+                ]",
      "comment": "I haven't fully understood why we need to move this class to inside the `setUpClass`  forcing us to do `self.CustomerTab` in the tests. Could you help me understand please?",
      "comment_id": 2403635600,
      "user": "nessita",
      "created_at": "2025-10-04T01:06:06Z",
      "url": "https://github.com/django/django/pull/19884#discussion_r2403635600"
    },
    {
      "repo": "django/django",
      "pr_number": 19884,
      "file_path": "tests/foreign_object/tests.py",
      "line": 795,
      "side": "RIGHT",
      "diff_hunk": "@@ -770,16 +770,39 @@ def test_pickling_foreignobject(self):\n         self.assertIn(\"reverse_path_infos\", foreign_object_restored.__dict__)\n \n \n-class ForeignObjectModelValidationTests(TestCase):\n-    @skipUnlessDBFeature(\"supports_table_check_constraints\")\n+@isolate_apps(\"foreign_object\")\n+@skipUnlessDBFeature(\"supports_table_check_constraints\")\n+class ForeignObjectConstraintTests(TestCase):\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n+\n+        class CustomerTab(models.Model):\n+            customer_id = models.IntegerField()\n+            customer = models.ForeignObject(\n+                \"Customer\",\n+                from_fields=[\"customer_id\"],\n+                to_fields=[\"id\"],\n+                on_delete=models.CASCADE,\n+            )\n+\n+            class Meta:\n+                constraints = [\n+                    models.CheckConstraint(\n+                        condition=Q(customer__lt=1000),\n+                        name=\"customer_id_limit\",\n+                    ),\n+                ]",
      "comment": "Oh! Now that we've adjusted the system check to affect only multicolumns, we could probably revert this change.\r\n\r\n(It was only moved up here to evade the system check, so we could still check the underlying code supporting n = 1 columns.)",
      "comment_id": 2403639667,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-04T01:13:05Z",
      "url": "https://github.com/django/django/pull/19884#discussion_r2403639667"
    },
    {
      "repo": "django/django",
      "pr_number": 19057,
      "file_path": "tests/custom_lookups/tests.py",
      "line": 253,
      "side": "RIGHT",
      "diff_hunk": "@@ -249,6 +249,28 @@ def test_custom_name_lookup(self):\n             self.assertSequenceEqual(qs1, [a1])\n             self.assertSequenceEqual(qs2, [a1])\n \n+    def test_custom_lookup_with_subquery(self):",
      "comment": "I like that `NotEqual` is now in our test suite, I checked the documented custom `AbsoluteValueLessThan` lookup that's in the docs and this also appears to be working :+1: ",
      "comment_id": 1946760102,
      "user": "sarahboyce",
      "created_at": "2025-02-07T15:55:44Z",
      "url": "https://github.com/django/django/pull/19057#discussion_r1946760102"
    },
    {
      "repo": "django/django",
      "pr_number": 19057,
      "file_path": "tests/lookup/tests.py",
      "line": 1067,
      "side": "RIGHT",
      "diff_hunk": "@@ -1064,6 +1064,16 @@ def test_regex_null(self):\n         Season.objects.create(year=2012, gt=None)\n         self.assertQuerySetEqual(Season.objects.filter(gt__regex=r\"^$\"), [])\n \n+    def test_regex_lookup_with_subquery(self):",
      "comment": "How do you feel about having a test which covers pretty much all registered lookups with a subquery (in a loop with `self.subTest`)?\r\nThere's a couple of line changes which we haven't strictly covered with a test and I wonder if having something like this might \"catch\" this issue in future lookups :thinking: \r\n",
      "comment_id": 1946777004,
      "user": "sarahboyce",
      "created_at": "2025-02-07T16:07:08Z",
      "url": "https://github.com/django/django/pull/19057#discussion_r1946777004"
    },
    {
      "repo": "django/django",
      "pr_number": 19057,
      "file_path": "tests/lookup/tests.py",
      "line": 1067,
      "side": "RIGHT",
      "diff_hunk": "@@ -1064,6 +1064,16 @@ def test_regex_null(self):\n         Season.objects.create(year=2012, gt=None)\n         self.assertQuerySetEqual(Season.objects.filter(gt__regex=r\"^$\"), [])\n \n+    def test_regex_lookup_with_subquery(self):",
      "comment": "Good idea. (Note to self: probably move this test to BasicExpressionsTests circa `test_in_subquery()`?)\r\n\r\nCould you let me know which line changes you don't see covered? I tried reverting each one individually (while leaving the rest of the PR) and at least one test failed for each, but I might be missing your meaning:\r\n\r\n| change | test |\r\n| ------------- | ------------- |\r\n| Col.as_sql | test_custom_implementation_year_exact |\r\n| HasKeyLookup.as_sql | queries.test_bulk_update.BulkUpdateTests.test_json_field |\r\n| Lookup.get_db_prep_lookup | custom_lookups.tests.LookupTests.test_custom_lookup_with_subquery |\r\n| IExact.process_rhs | admin_utils.test_logentry.LogEntryTests.test_logentry_change_message |\r\n| PatternLookup.process_rhs | model_forms.test_modelchoicefield.ModelChoiceFieldTests.test_queryset_none |\r\n| Regex.get_db_prep_lookup | (on mariadb:) lookups.tests.LookupTests.test_regex_lookup_with_subquery |\r\n| SearchVector.as_sql | (on postgres) postgres_tests.test_search.MultipleFieldsTest |\r\n| SearchVectorExact.as_sql |  (on postgres) postgres_tests.test_search.TestCombinations |\r\n| DistanceLookupFromFunction.as_sql | gis_tests.distapp.tests.DistanceTest |\r\n| DWithinLookup.process_rhs | gis_tests.distapp.tests.DistanceTest |\r\n| RelateLookup.process_rhs |  gis_tests.distapp.tests.DistanceTest |\r\n",
      "comment_id": 1947983073,
      "user": "jacobtylerwalls",
      "created_at": "2025-02-09T01:00:25Z",
      "url": "https://github.com/django/django/pull/19057#discussion_r1947983073"
    },
    {
      "repo": "django/django",
      "pr_number": 19057,
      "file_path": "tests/expressions/tests.py",
      "line": 752,
      "side": "RIGHT",
      "diff_hunk": "@@ -729,6 +729,24 @@ def test_in_subquery(self):\n         )\n         self.assertCountEqual(subquery_test2, [self.foobar_ltd])\n \n+    def test_lookups_subquery(self):",
      "comment": "I looked into doing something similar for JSONField, but it fails pretty badly, so this might be a follow-up ticket:\r\n\r\nRough test:\r\n```diff\r\ndiff --git a/tests/model_fields/test_jsonfield.py b/tests/model_fields/test_jsonfield.py\r\nindex 5a9cf9ad7a..32471d910e 100644\r\n--- a/tests/model_fields/test_jsonfield.py\r\n+++ b/tests/model_fields/test_jsonfield.py\r\n@@ -925,6 +925,20 @@ class TestQuerying(TestCase):\r\n             self.objs[3:5],\r\n         )\r\n \r\n+    @skipUnlessDBFeature(\"supports_json_field_contains\")\r\n+    def test_lookups_subquery(self):\r\n+        qs = NullableJSONModel.objects.values(\"value__foo\")[:1]\r\n+        for lookup in JSONField.get_lookups():\r\n+            with self.subTest(lookup=lookup):\r\n+                if lookup == \"isnull\":\r\n+                    continue  # not allowed, rhs must be a literal boolean.\r\n+                if lookup in {\"has_keys\", \"has_any_keys\"}:\r\n+                    rhs = [Subquery(qs)]\r\n+                else:\r\n+                    rhs = Subquery(qs)\r\n+                qs = NullableJSONModel.objects.filter(**{f\"value__foo__{lookup}\": rhs})\r\n+                self.assertGreaterEqual(len(qs), 0)\r\n+\r\n     @skipUnlessDBFeature(\"supports_json_field_contains\")\r\n     def test_array_key_contains(self):\r\n         tests = [\r\n```\r\n***\r\n```\r\n  File \"/usr/local/lib/python3.12/site-packages/psycopg/cursor.py\", line 97, in execute\r\n    raise ex.with_traceback(None)\r\ndjango.db.utils.ProgrammingError: the query has 5 placeholders but 19 parameters were passed\r\n```\r\n",
      "comment_id": 1948274147,
      "user": "jacobtylerwalls",
      "created_at": "2025-02-09T23:59:00Z",
      "url": "https://github.com/django/django/pull/19057#discussion_r1948274147"
    },
    {
      "repo": "django/django",
      "pr_number": 19057,
      "file_path": "tests/custom_lookups/tests.py",
      "line": 262,
      "side": "RIGHT",
      "diff_hunk": "@@ -249,6 +249,28 @@ def test_custom_name_lookup(self):\n             self.assertSequenceEqual(qs1, [a1])\n             self.assertSequenceEqual(qs2, [a1])\n \n+    def test_custom_lookup_with_subquery(self):\n+        class NotEqual(models.Lookup):\n+            lookup_name = \"ne\"\n+\n+            def as_sql(self, compiler, connection):\n+                lhs, lhs_params = self.process_lhs(compiler, connection)\n+                rhs, rhs_params = self.process_rhs(compiler, connection)\n+                params = lhs_params + rhs_params",
      "comment": "Did you use the `a + b` style over the `(*a, *b)` intentionally here? Seems inconsistent with the other tests and the docs...",
      "comment_id": 2061414027,
      "user": "browniebroke",
      "created_at": "2025-04-26T15:56:14Z",
      "url": "https://github.com/django/django/pull/19057#discussion_r2061414027"
    },
    {
      "repo": "django/django",
      "pr_number": 19057,
      "file_path": "tests/custom_lookups/tests.py",
      "line": 262,
      "side": "RIGHT",
      "diff_hunk": "@@ -249,6 +249,28 @@ def test_custom_name_lookup(self):\n             self.assertSequenceEqual(qs1, [a1])\n             self.assertSequenceEqual(qs2, [a1])\n \n+    def test_custom_lookup_with_subquery(self):\n+        class NotEqual(models.Lookup):\n+            lookup_name = \"ne\"\n+\n+            def as_sql(self, compiler, connection):\n+                lhs, lhs_params = self.process_lhs(compiler, connection)\n+                rhs, rhs_params = self.process_rhs(compiler, connection)\n+                params = lhs_params + rhs_params",
      "comment": "Great question. Yes, I did this intentionally to capture the scenario that failed with `TypeError`. If I adjust this test to the more resilient pattern with unpacking, I won't capture the issue.\r\n\r\nDo you think adding a comment would help clarify?\r\n\r\n> Seems inconsistent with the other tests and the docs...\r\n\r\nRe the docs, I've only documented the unpacking syntax in the release note (addressed to users supporting multiple versions of Django) since part of the point of this change is that the prior documented behavior (which was working in most cases) now works in the edge case with `Subquery`. So I wouldn't call it inconsistent with the docs.",
      "comment_id": 2061418361,
      "user": "jacobtylerwalls",
      "created_at": "2025-04-26T16:06:53Z",
      "url": "https://github.com/django/django/pull/19057#discussion_r2061418361"
    },
    {
      "repo": "django/django",
      "pr_number": 19057,
      "file_path": "tests/custom_lookups/tests.py",
      "line": 262,
      "side": "RIGHT",
      "diff_hunk": "@@ -249,6 +249,28 @@ def test_custom_name_lookup(self):\n             self.assertSequenceEqual(qs1, [a1])\n             self.assertSequenceEqual(qs2, [a1])\n \n+    def test_custom_lookup_with_subquery(self):\n+        class NotEqual(models.Lookup):\n+            lookup_name = \"ne\"\n+\n+            def as_sql(self, compiler, connection):\n+                lhs, lhs_params = self.process_lhs(compiler, connection)\n+                rhs, rhs_params = self.process_rhs(compiler, connection)\n+                params = lhs_params + rhs_params",
      "comment": "Ah got you. Thanks for the explanation. \r\n\r\n> Do you think adding a comment would help clarify?\r\n\r\nPerhaps... I stumbled on this issue in the review queue and had some basic context, so I can imagine someone else looking at the test in the future might think the same as I did? I might have been biased by the surrounding changes in the diff.",
      "comment_id": 2061434915,
      "user": "browniebroke",
      "created_at": "2025-04-26T16:14:21Z",
      "url": "https://github.com/django/django/pull/19057#discussion_r2061434915"
    },
    {
      "repo": "django/django",
      "pr_number": 19057,
      "file_path": "tests/custom_lookups/tests.py",
      "line": 262,
      "side": "RIGHT",
      "diff_hunk": "@@ -249,6 +249,28 @@ def test_custom_name_lookup(self):\n             self.assertSequenceEqual(qs1, [a1])\n             self.assertSequenceEqual(qs2, [a1])\n \n+    def test_custom_lookup_with_subquery(self):\n+        class NotEqual(models.Lookup):\n+            lookup_name = \"ne\"\n+\n+            def as_sql(self, compiler, connection):\n+                lhs, lhs_params = self.process_lhs(compiler, connection)\n+                rhs, rhs_params = self.process_rhs(compiler, connection)\n+                params = lhs_params + rhs_params",
      "comment": "Pushed a little comment in 6f1ada81c34df69290d80fa739c5956a04573bb3. Noticed there are merge conflicts, so I'll rebase in a sec.",
      "comment_id": 2061484297,
      "user": "jacobtylerwalls",
      "created_at": "2025-04-26T16:39:50Z",
      "url": "https://github.com/django/django/pull/19057#discussion_r2061484297"
    },
    {
      "repo": "django/django",
      "pr_number": 19057,
      "file_path": "tests/custom_lookups/tests.py",
      "line": 262,
      "side": "RIGHT",
      "diff_hunk": "@@ -249,6 +249,28 @@ def test_custom_name_lookup(self):\n             self.assertSequenceEqual(qs1, [a1])\n             self.assertSequenceEqual(qs2, [a1])\n \n+    def test_custom_lookup_with_subquery(self):\n+        class NotEqual(models.Lookup):\n+            lookup_name = \"ne\"\n+\n+            def as_sql(self, compiler, connection):\n+                lhs, lhs_params = self.process_lhs(compiler, connection)\n+                rhs, rhs_params = self.process_rhs(compiler, connection)\n+                params = lhs_params + rhs_params",
      "comment": "And if you're still at the sprints, raise your hand or come say hello!",
      "comment_id": 2061484552,
      "user": "jacobtylerwalls",
      "created_at": "2025-04-26T16:40:37Z",
      "url": "https://github.com/django/django/pull/19057#discussion_r2061484552"
    },
    {
      "repo": "django/django",
      "pr_number": 19057,
      "file_path": "tests/lookup/tests.py",
      "line": 1067,
      "side": "RIGHT",
      "diff_hunk": "@@ -1064,6 +1064,16 @@ def test_regex_null(self):\n         Season.objects.create(year=2012, gt=None)\n         self.assertQuerySetEqual(Season.objects.filter(gt__regex=r\"^$\"), [])\n \n+    def test_regex_lookup_with_subquery(self):",
      "comment": "The ones I don't get test failures for are:\r\n- `RelateLookup.process_rhs` (the `gis_tests.distapp.tests.DistanceTest` tests passed for me)\r\n- `Func.as_sql`\r\n\r\nAll other line changes I do get test failures when reverting",
      "comment_id": 2139974337,
      "user": "sarahboyce",
      "created_at": "2025-06-11T12:12:07Z",
      "url": "https://github.com/django/django/pull/19057#discussion_r2139974337"
    },
    {
      "repo": "django/django",
      "pr_number": 19057,
      "file_path": "tests/lookup/tests.py",
      "line": 1067,
      "side": "RIGHT",
      "diff_hunk": "@@ -1064,6 +1064,16 @@ def test_regex_null(self):\n         Season.objects.create(year=2012, gt=None)\n         self.assertQuerySetEqual(Season.objects.filter(gt__regex=r\"^$\"), [])\n \n+    def test_regex_lookup_with_subquery(self):",
      "comment": "Thanks, my results agree with yours now. I'll have a look at adding some coverage in the next few days \ud83d\udc4d ",
      "comment_id": 2142839900,
      "user": "jacobtylerwalls",
      "created_at": "2025-06-12T13:59:09Z",
      "url": "https://github.com/django/django/pull/19057#discussion_r2142839900"
    },
    {
      "repo": "django/django",
      "pr_number": 19057,
      "file_path": "tests/lookup/tests.py",
      "line": 1067,
      "side": "RIGHT",
      "diff_hunk": "@@ -1064,6 +1064,16 @@ def test_regex_null(self):\n         Season.objects.create(year=2012, gt=None)\n         self.assertQuerySetEqual(Season.objects.filter(gt__regex=r\"^$\"), [])\n \n+    def test_regex_lookup_with_subquery(self):",
      "comment": "I think the GIS changes are difficult to cover because the underlying lookups are already so resilient against either lists or tuples since ticket-31002. I think we can just treat these changes as code style cleanups. I did miss one for consistency: `GISLookup.get_db_prep_lookup`, just added.\r\n\r\nI added a case for `Func` (via `Lower`).",
      "comment_id": 2147399658,
      "user": "jacobtylerwalls",
      "created_at": "2025-06-15T01:56:14Z",
      "url": "https://github.com/django/django/pull/19057#discussion_r2147399658"
    },
    {
      "repo": "django/django",
      "pr_number": 19057,
      "file_path": "tests/expressions/tests.py",
      "line": 752,
      "side": "RIGHT",
      "diff_hunk": "@@ -729,6 +729,24 @@ def test_in_subquery(self):\n         )\n         self.assertCountEqual(subquery_test2, [self.foobar_ltd])\n \n+    def test_lookups_subquery(self):",
      "comment": "~This test found something to fix now, so I'll spruce it up and include it.~ Never mind, I had #19459 checked out, disregard. I'll just submit this test on Trac and get an opinion about the `# expected failures`.",
      "comment_id": 2178782419,
      "user": "jacobtylerwalls",
      "created_at": "2025-07-02T00:57:02Z",
      "url": "https://github.com/django/django/pull/19057#discussion_r2178782419"
    },
    {
      "repo": "django/django",
      "pr_number": 18259,
      "file_path": "django/contrib/staticfiles/finders.py",
      "line": 116,
      "side": "RIGHT",
      "diff_hunk": "@@ -113,17 +113,22 @@ def check(self, **kwargs):\n                 )\n         return errors\n \n-    def find(self, path, all=False):\n+    def find(self, path, fetch_all=False, **kwargs):",
      "comment": "I'm not familiarized in how deprecations are handled in python/django, but I think that adding `**kwargs` is overkill. Maybe add something like:\r\n\r\n```python\r\n    def find(self, path, fetch_all=False, all=None):\r\n        if all is not None:\r\n            # TODO: Add a warning or something\r\n            fetch_all = all\r\n```\r\n\r\nThis sadly doesn't fix shadowing `all`, but by adding a deprecation warning it will allow us to remove it in future versions.\r\n\r\nMaybe someone can give us some advice on how to handle this?",
      "comment_id": 1636888794,
      "user": "EnriqueSoria",
      "created_at": "2024-06-12T18:02:06Z",
      "url": "https://github.com/django/django/pull/18259#discussion_r1636888794"
    },
    {
      "repo": "django/django",
      "pr_number": 18259,
      "file_path": "django/contrib/staticfiles/finders.py",
      "line": 116,
      "side": "RIGHT",
      "diff_hunk": "@@ -113,17 +113,22 @@ def check(self, **kwargs):\n                 )\n         return errors\n \n-    def find(self, path, all=False):\n+    def find(self, path, fetch_all=False, **kwargs):",
      "comment": "Thanks for the feedback @EnriqueSoria. As per your suggestion I've added the deprecations warnings for the use of the `all` param in 03b3b1faabd1970ad846f1266ffb320b1ce0508b . On the other hand, I don't see why the use of `**kwargs` is an overkill in this case. I think it fits the case, removing the shadowing of the builtin `all` function and at the same time avoiding the errors for third party packages that may use or extend the existing finders API. I think  using `**kwargs` provide us the flexibility we need to keep the backwards compatibility. ",
      "comment_id": 1636958625,
      "user": "avallbona",
      "created_at": "2024-06-12T19:09:38Z",
      "url": "https://github.com/django/django/pull/18259#discussion_r1636958625"
    },
    {
      "repo": "django/django",
      "pr_number": 18259,
      "file_path": "django/contrib/staticfiles/finders.py",
      "line": 116,
      "side": "RIGHT",
      "diff_hunk": "@@ -113,17 +113,22 @@ def check(self, **kwargs):\n                 )\n         return errors\n \n-    def find(self, path, all=False):\n+    def find(self, path, fetch_all=False, **kwargs):",
      "comment": "When using a function that has `**kwargs` you don't know which parameters it does accept. That's why I dislike using it. \r\n\r\nBut I guess in this case is worth.",
      "comment_id": 1638038486,
      "user": "EnriqueSoria",
      "created_at": "2024-06-13T11:19:07Z",
      "url": "https://github.com/django/django/pull/18259#discussion_r1638038486"
    },
    {
      "repo": "django/django",
      "pr_number": 18259,
      "file_path": "django/contrib/staticfiles/finders.py",
      "line": 116,
      "side": "RIGHT",
      "diff_hunk": "@@ -113,17 +113,22 @@ def check(self, **kwargs):\n                 )\n         return errors\n \n-    def find(self, path, all=False):\n+    def find(self, path, fetch_all=False, **kwargs):",
      "comment": "Using `**kwargs` has the undesired side effect of allowing for arbitrary named arguments and the silently ignoring them. This is something that we should avoid so I see two options:\r\n1. Continue using `all` for the deprecation period, is not that terrible since we have already done so for the last 10+ years. \r\n2. Change to use `**kwargs` but then we should check that no other named param was given and raise the corresponding `TypeError`.\r\n\r\nI think 1 is the best option, so I would suggest to go down that path.",
      "comment_id": 1647853993,
      "user": "nessita",
      "created_at": "2024-06-20T16:36:57Z",
      "url": "https://github.com/django/django/pull/18259#discussion_r1647853993"
    },
    {
      "repo": "django/django",
      "pr_number": 18259,
      "file_path": "django/contrib/staticfiles/finders.py",
      "line": 35,
      "side": "RIGHT",
      "diff_hunk": "@@ -26,11 +32,11 @@ def check(self, **kwargs):\n             \"configured correctly.\"\n         )\n \n-    def find(self, path, all=False):\n+    def find(self, path, fetch_all=False, **kwargs):",
      "comment": "I'm not sold on the new proposed name, I think a more accurate option would be `match_all` or `find_all`. To me, fetch implies that something will be \"brought\" from there to here (ie like data retrieval or similar), which is not really the case.\r\nWhat do you think?",
      "comment_id": 1647858614,
      "user": "nessita",
      "created_at": "2024-06-20T16:41:06Z",
      "url": "https://github.com/django/django/pull/18259#discussion_r1647858614"
    },
    {
      "repo": "django/django",
      "pr_number": 18259,
      "file_path": "django/contrib/staticfiles/finders.py",
      "line": 35,
      "side": "RIGHT",
      "diff_hunk": "@@ -26,11 +32,11 @@ def check(self, **kwargs):\n             \"configured correctly.\"\n         )\n \n-    def find(self, path, all=False):\n+    def find(self, path, fetch_all=False, **kwargs):",
      "comment": "I agree, I think `find_all` it's a better fit for this param name. Handled in 704fdb92b513ee0d6218068990bd70afa90dfdfd",
      "comment_id": 1648173947,
      "user": "avallbona",
      "created_at": "2024-06-20T21:43:39Z",
      "url": "https://github.com/django/django/pull/18259#discussion_r1648173947"
    },
    {
      "repo": "django/django",
      "pr_number": 18259,
      "file_path": "django/contrib/staticfiles/finders.py",
      "line": 116,
      "side": "RIGHT",
      "diff_hunk": "@@ -113,17 +113,22 @@ def check(self, **kwargs):\n                 )\n         return errors\n \n-    def find(self, path, all=False):\n+    def find(self, path, fetch_all=False, **kwargs):",
      "comment": "@avallbona The comment above is what I was referring in the global comment.",
      "comment_id": 1649388083,
      "user": "nessita",
      "created_at": "2024-06-21T20:11:32Z",
      "url": "https://github.com/django/django/pull/18259#discussion_r1649388083"
    },
    {
      "repo": "django/django",
      "pr_number": 18259,
      "file_path": "django/contrib/staticfiles/finders.py",
      "line": 116,
      "side": "RIGHT",
      "diff_hunk": "@@ -113,17 +113,22 @@ def check(self, **kwargs):\n                 )\n         return errors\n \n-    def find(self, path, all=False):\n+    def find(self, path, fetch_all=False, **kwargs):",
      "comment": "Sorry I missed this comment \ud83d\ude2c . I'm not sure I like the option 1, we don't solve the problem right away and we pospone the solution to a future version, but I think it's clearer on what params the method accepts.\r\n\r\nI'll fix it.\r\n",
      "comment_id": 1649451616,
      "user": "avallbona",
      "created_at": "2024-06-21T21:40:33Z",
      "url": "https://github.com/django/django/pull/18259#discussion_r1649451616"
    },
    {
      "repo": "django/django",
      "pr_number": 18259,
      "file_path": "django/contrib/staticfiles/finders.py",
      "line": 116,
      "side": "RIGHT",
      "diff_hunk": "@@ -113,17 +113,22 @@ def check(self, **kwargs):\n                 )\n         return errors\n \n-    def find(self, path, all=False):\n+    def find(self, path, fetch_all=False, **kwargs):",
      "comment": "@nessita Addressed the issue in eacad9336b5225b6bd47894cd1d7d5db33367a47",
      "comment_id": 1649464846,
      "user": "avallbona",
      "created_at": "2024-06-21T22:06:00Z",
      "url": "https://github.com/django/django/pull/18259#discussion_r1649464846"
    },
    {
      "repo": "django/django",
      "pr_number": 18259,
      "file_path": "django/contrib/staticfiles/finders.py",
      "line": 136,
      "side": "RIGHT",
      "diff_hunk": "@@ -113,17 +119,25 @@ def check(self, **kwargs):\n                 )\n         return errors\n \n-    def find(self, path, all=False):\n+    def find(self, path, find_all=False, all=None):\n         \"\"\"\n         Look for files in the extra locations as defined in STATICFILES_DIRS.\n         \"\"\"\n+\n+        # we cover the case the method get called with the legacy param \"all\"\n+        if all is not None:\n+            warnings.warn(\n+                DEPRECATION_WARNING_MSG, RemovedInDjango60Warning, stacklevel=2\n+            )\n+            find_all = all",
      "comment": "What about deleting `all` symbol? That way we don't have it shadowed anymore\r\n\r\n```suggestion\r\n            find_all = all\r\n            del all\r\n```\r\n\r\nMaybe it's a bit tricky, but it does the job:\r\n\r\n```pycon\r\n>>> all = 3\r\n>>> find_all = 3\r\n>>> all([1,0,1])\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: 'int' object is not callable\r\n>>> del all\r\n>>> all([1,0,1])\r\nFalse\r\n>>> find_all\r\n3\r\n```",
      "comment_id": 1649693190,
      "user": "EnriqueSoria",
      "created_at": "2024-06-22T12:43:10Z",
      "url": "https://github.com/django/django/pull/18259#discussion_r1649693190"
    },
    {
      "repo": "django/django",
      "pr_number": 18259,
      "file_path": "django/contrib/staticfiles/finders.py",
      "line": 136,
      "side": "RIGHT",
      "diff_hunk": "@@ -113,17 +119,25 @@ def check(self, **kwargs):\n                 )\n         return errors\n \n-    def find(self, path, all=False):\n+    def find(self, path, find_all=False, all=None):\n         \"\"\"\n         Look for files in the extra locations as defined in STATICFILES_DIRS.\n         \"\"\"\n+\n+        # we cover the case the method get called with the legacy param \"all\"\n+        if all is not None:\n+            warnings.warn(\n+                DEPRECATION_WARNING_MSG, RemovedInDjango60Warning, stacklevel=2\n+            )\n+            find_all = all",
      "comment": "@EnriqueSoria, It may be a personal preference but, I don't like this approach. It feels a bit hacky. I would rather way for the next version, to completely remove the `all` param from the method signature. ",
      "comment_id": 1654466057,
      "user": "avallbona",
      "created_at": "2024-06-26T09:32:24Z",
      "url": "https://github.com/django/django/pull/18259#discussion_r1654466057"
    },
    {
      "repo": "django/django",
      "pr_number": 19917,
      "file_path": "django/views/i18n.py",
      "line": 32,
      "side": "LEFT",
      "diff_hunk": "@@ -29,8 +29,9 @@ def builtin_template_path(name):\n \n def set_language(request):\n     \"\"\"\n-    Redirect to a given URL while setting the chosen language in the session",
      "comment": "Elsewhere (other than the other place in this PR), this is just \"the language cookie\", so I think this is sufficient here:\n```suggestion\n    Redirect to a given URL while setting the chosen language in the language cookie.\n```",
      "comment_id": 2402243330,
      "user": "jacobtylerwalls",
      "created_at": "2025-10-03T14:50:01Z",
      "url": "https://github.com/django/django/pull/19917#discussion_r2402243330"
    }
  ]
}