{
  "repo": "astral-sh/ruff",
  "scraped_at": "2026-02-03T12:47:53.202776",
  "stats": {
    "total_comments": 3662,
    "filtered": {
      "not_python": 2850,
      "skip_pattern:nit:": 23,
      "too_short": 493,
      "no_diff_hunk": 92,
      "too_long": 60,
      "skip_pattern:nice!": 2,
      "skip_pattern:thanks!": 2,
      "skip_pattern:thank you": 1
    },
    "kept": 139
  },
  "examples": [
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22943,
      "file_path": "scripts/conformance.py",
      "line": 465,
      "side": "RIGHT",
      "diff_hunk": "@@ -462,6 +462,7 @@ def collect_ty_diagnostics(\n             f\"--python-version={python_version}\",\n             \"--output-format=gitlab\",\n             \"--ignore=assert-type-unspellable-subtype\",\n+            \"--error=invalid-legacy-positional-parameter\",",
      "comment": "this is necessary because I made the default severity of the lint `Level::Warn` rather than `Level::Error` (since it won't fail at runtime)",
      "comment_id": 2741872858,
      "user": "AlexWaygood",
      "created_at": "2026-01-29T14:15:48Z",
      "url": "https://github.com/astral-sh/ruff/pull/22943#discussion_r2741872858"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22943,
      "file_path": "scripts/conformance.py",
      "line": 465,
      "side": "RIGHT",
      "diff_hunk": "@@ -462,6 +462,7 @@ def collect_ty_diagnostics(\n             f\"--python-version={python_version}\",\n             \"--output-format=gitlab\",\n             \"--ignore=assert-type-unspellable-subtype\",\n+            \"--error=invalid-legacy-positional-parameter\",",
      "comment": "I feel like we should set up our conformance suite integration such that warnings count as errors for conformance suite purposes in general.",
      "comment_id": 2748483779,
      "user": "carljm",
      "created_at": "2026-01-30T23:59:41Z",
      "url": "https://github.com/astral-sh/ruff/pull/22943#discussion_r2748483779"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22943,
      "file_path": "scripts/conformance.py",
      "line": 465,
      "side": "RIGHT",
      "diff_hunk": "@@ -462,6 +462,7 @@ def collect_ty_diagnostics(\n             f\"--python-version={python_version}\",\n             \"--output-format=gitlab\",\n             \"--ignore=assert-type-unspellable-subtype\",\n+            \"--error=invalid-legacy-positional-parameter\",",
      "comment": "For example, the number of false positives we report might go up due to some `unused-ignore-comment` diagnostics on comments in the spec that aren't meant to be suppression comments at all, such as https://github.com/python/typing/blob/08f929ba70397b5c898a55bbb73c8b4e2e8e4fbb/conformance/tests/directives_type_ignore_file1.py#L11. (Related: https://github.com/astral-sh/ty/issues/881.)",
      "comment_id": 2749311445,
      "user": "AlexWaygood",
      "created_at": "2026-01-31T10:04:33Z",
      "url": "https://github.com/astral-sh/ruff/pull/22943#discussion_r2749311445"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21369,
      "file_path": "crates/ruff_python_formatter/resources/test/fixtures/ruff/parentheses/call_chains.py",
      "line": 266,
      "side": "RIGHT",
      "diff_hunk": "@@ -216,3 +216,57 @@\n     .baz()\n )\n \n+# Note in preview we split at `pl` which some\n+# folks may dislike. (Similarly with common\n+# `np` and `pd` invocations).\n+\n+expr = (\n+    pl.scan_parquet(\"/data/pypi-parquet/*.parquet\")\n+    .filter(\n+        [\n+            pl.col(\"path\").str.contains(\n+                r\"\\.(asm|c|cc|cpp|cxx|h|hpp|rs|[Ff][0-9]{0,2}(?:or)?|go)$\"\n+            ),\n+            ~pl.col(\"path\").str.contains(r\"(^|/)test(|s|ing)\"),\n+            ~pl.col(\"path\").str.contains(\"/site-packages/\", literal=True),\n+        ]\n+    )\n+    .with_columns(\n+        month=pl.col(\"uploaded_on\").dt.truncate(\"1mo\"),\n+        ext=pl.col(\"path\")\n+        .str.extract(pattern=r\"\\.([a-z0-9]+)$\", group_index=1)\n+        .str.replace_all(pattern=r\"cxx|cpp|cc|c|hpp|h\", value=\"C/C++\")\n+        .str.replace_all(pattern=\"^f.*$\", value=\"Fortran\")\n+        .str.replace(\"rs\", \"Rust\", literal=True)\n+        .str.replace(\"go\", \"Go\", literal=True)\n+        .str.replace(\"asm\", \"Assembly\", literal=True)\n+        .replace({\"\": None}),\n+    )\n+    .group_by([\"month\", \"ext\"])\n+    .agg(project_count=pl.col(\"project_name\").n_unique())\n+    .drop_nulls([\"ext\"])\n+    .sort([\"month\", \"project_count\"], descending=True)\n+)\n+\n+def indentation_matching_for_loop_in_preview():\n+    if make_this:\n+        if more_nested_because_line_length:\n+            identical_hidden_layer_sizes = all(",
      "comment": "Variable identical_hidden_layer_sizes is not used.\n```suggestion\n            all(\n```",
      "comment_id": 2615739046,
      "user": "Copilot",
      "created_at": "2025-12-12T22:26:47Z",
      "url": "https://github.com/astral-sh/ruff/pull/21369#discussion_r2615739046"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21369,
      "file_path": "crates/ruff_python_formatter/resources/test/fixtures/ruff/parentheses/call_chains.py",
      "line": 221,
      "side": "RIGHT",
      "diff_hunk": "@@ -216,3 +216,57 @@\n     .baz()\n )\n \n+# Note in preview we split at `pl` which some\n+# folks may dislike. (Similarly with common\n+# `np` and `pd` invocations).",
      "comment": "It might be worth explaining why we do it regardless. I'm sure it will come up in a future issue and I would much appreciate if I came across a comment in code (or test) explaining why we do split after `np`",
      "comment_id": 2618339047,
      "user": "MichaReiser",
      "created_at": "2025-12-15T07:46:41Z",
      "url": "https://github.com/astral-sh/ruff/pull/21369#discussion_r2618339047"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22746,
      "file_path": "scripts/conformance.py",
      "line": 385,
      "side": "RIGHT",
      "diff_hunk": "@@ -327,6 +380,10 @@ def collect_expected_diagnostics(test_files: Sequence[Path]) -> list[Diagnostic]\n                         ),\n                         source=Source.EXPECTED,\n                         optional=error.group(\"optional\") is not None,\n+                        tag=f\"{file.name}:{error.group('tag')}\"\n+                        if error.group(\"tag\")\n+                        else None,",
      "comment": "Ruff doesn't do a good job right now at formatting nested if-else expressions. But we can help it a bit to make this more readable by adding parentheses\r\n```suggestion\r\n                        tag=(\r\n                        \tf\"{file.name}:{error.group('tag')}\"\r\n                        \tif error.group(\"tag\")\r\n                        \telse None\r\n                      \t),\r\n```",
      "comment_id": 2712875649,
      "user": "MichaReiser",
      "created_at": "2026-01-21T14:40:17Z",
      "url": "https://github.com/astral-sh/ruff/pull/22746#discussion_r2712875649"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22746,
      "file_path": "scripts/conformance.py",
      "line": 471,
      "side": "RIGHT",
      "diff_hunk": "@@ -366,59 +423,77 @@ def collect_ty_diagnostics(\n     ]\n \n \n-def group_diagnostics_by_key(\n-    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+def group_diagnostics_by_key_or_tag(\n+    old: list[Diagnostic],\n+    new: list[Diagnostic],\n+    expected: list[Diagnostic],\n ) -> list[GroupedDiagnostics]:\n+    # propagate tags from expected diagnostics to old and new diagnostics\n+    tagged_lines = {\n+        (d.location.path.name, d.location.positions.begin.line): d.tag\n+        for d in expected\n+        if d.tag is not None\n+    }\n+\n+    for diag in old:\n+        diag.tag = tagged_lines.get(\n+            (diag.location.path.name, diag.location.positions.begin.line), None\n+        )\n+\n+    for diag in new:\n+        diag.tag = tagged_lines.get(\n+            (diag.location.path.name, diag.location.positions.begin.line), None\n+        )\n+\n     diagnostics = [\n         *old,\n         *new,\n         *expected,\n     ]\n \n-    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n-\n-    grouped = []\n-    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+    # group diagnostics either by a path and a line or a path and a tag\n+    diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+    grouped_diagnostics = []\n+    for key, group in groupby(diagnostics, key=attrgetter(\"key\")):\n         group = list(group)\n-        sources: Source = reduce(or_, (diag.source for diag in group))\n-        grouped.append(\n-            GroupedDiagnostics(\n-                key=key,\n-                sources=sources,\n-                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n-                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n-                expected=next(\n-                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n-                ),\n-            )\n+        old_group = list(filter(lambda diag: diag.source == Source.OLD, group))\n+        new_group = list(filter(lambda diag: diag.source == Source.NEW, group))\n+        expected_group = list(\n+            filter(lambda diag: diag.source == Source.EXPECTED, group)\n+        )\n+\n+        grouped = GroupedDiagnostics(\n+            key=key,\n+            sources={d.source for d in group},\n+            old=old_group,\n+            new=new_group,\n+            expected=expected_group,\n         )",
      "comment": "Nit: I would use a regular loop here to avoid iterating `diagnostics` multiple times and also because I find it slightly more readable. But maybe that's the Rust dev in me.\r\n\r\n```py\r\n\t\t\t\told_diagnostics: list[Diagnostic] = []\r\n        new_diagnostics: list[Diagnostic] = []\r\n        expected_diagnostics: list[Diagnostic] = []\r\n        sources: set[Source] = set()\r\n\r\n        for diag in group:\r\n            sources.add(diag.source)\r\n            match diag.source:\r\n                case Source.OLD:\r\n                    old_diagnostics.append(diag)\r\n                case Source.NEW:\r\n                    new_diagnostics.append(diag)\r\n                case Source.EXPECTED:\r\n                    expected_diagnostics.append(diag)\r\n\r\n        grouped = GroupedDiagnostics(\r\n            key=key,\r\n            sources=sources,\r\n            old=old_diagnostics,\r\n            new=new_diagnostics,\r\n            expected=expected_diagnostics,\r\n        )\r\n```",
      "comment_id": 2712882781,
      "user": "MichaReiser",
      "created_at": "2026-01-21T14:41:54Z",
      "url": "https://github.com/astral-sh/ruff/pull/22746#discussion_r2712882781"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22746,
      "file_path": "scripts/conformance.py",
      "line": 482,
      "side": "RIGHT",
      "diff_hunk": "@@ -366,59 +423,77 @@ def collect_ty_diagnostics(\n     ]\n \n \n-def group_diagnostics_by_key(\n-    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+def group_diagnostics_by_key_or_tag(\n+    old: list[Diagnostic],\n+    new: list[Diagnostic],\n+    expected: list[Diagnostic],\n ) -> list[GroupedDiagnostics]:\n+    # propagate tags from expected diagnostics to old and new diagnostics\n+    tagged_lines = {\n+        (d.location.path.name, d.location.positions.begin.line): d.tag\n+        for d in expected\n+        if d.tag is not None\n+    }\n+\n+    for diag in old:\n+        diag.tag = tagged_lines.get(\n+            (diag.location.path.name, diag.location.positions.begin.line), None\n+        )\n+\n+    for diag in new:\n+        diag.tag = tagged_lines.get(\n+            (diag.location.path.name, diag.location.positions.begin.line), None\n+        )\n+\n     diagnostics = [\n         *old,\n         *new,\n         *expected,\n     ]\n \n-    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n-\n-    grouped = []\n-    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+    # group diagnostics either by a path and a line or a path and a tag\n+    diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+    grouped_diagnostics = []\n+    for key, group in groupby(diagnostics, key=attrgetter(\"key\")):\n         group = list(group)\n-        sources: Source = reduce(or_, (diag.source for diag in group))\n-        grouped.append(\n-            GroupedDiagnostics(\n-                key=key,\n-                sources=sources,\n-                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n-                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n-                expected=next(\n-                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n-                ),\n-            )\n+        old_group = list(filter(lambda diag: diag.source == Source.OLD, group))\n+        new_group = list(filter(lambda diag: diag.source == Source.NEW, group))\n+        expected_group = list(\n+            filter(lambda diag: diag.source == Source.EXPECTED, group)\n+        )\n+\n+        grouped = GroupedDiagnostics(\n+            key=key,\n+            sources={d.source for d in group},\n+            old=old_group,\n+            new=new_group,\n+            expected=expected_group,\n         )\n+        grouped_diagnostics.append(grouped)\n \n-    return grouped\n+    return grouped_diagnostics\n \n \n def compute_stats(\n     grouped_diagnostics: list[GroupedDiagnostics],\n     source: Source,\n ) -> Statistics:\n     if source == source.EXPECTED:\n-        # ty currently raises a false positive here due to incomplete enum.Flag support\n-        # see https://github.com/astral-sh/ty/issues/876\n-        num_errors = sum(\n-            1\n-            for g in grouped_diagnostics\n-            if source.EXPECTED in g.sources  # ty:ignore[unsupported-operator]\n-        )\n+        num_errors = sum(1 for g in grouped_diagnostics if source.EXPECTED in g.sources)",
      "comment": "It's a bit tricky to say what `num_errors` means in the presence of `E[tag+]` and `E?` where a single expected tag can have multiple errors and some errors are entirely optional. \r\n\r\nBut it also seems that we never call `compute_stats` with `Source::Expected`. \r\n\r\nShould we remove this code and make `compute_stats` take a boolean argument instead (new_diagnostics)?",
      "comment_id": 2712891753,
      "user": "MichaReiser",
      "created_at": "2026-01-21T14:44:00Z",
      "url": "https://github.com/astral-sh/ruff/pull/22746#discussion_r2712891753"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22746,
      "file_path": "scripts/conformance.py",
      "line": 430,
      "side": "RIGHT",
      "diff_hunk": "@@ -366,59 +423,77 @@ def collect_ty_diagnostics(\n     ]\n \n \n-def group_diagnostics_by_key(\n-    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+def group_diagnostics_by_key_or_tag(\n+    old: list[Diagnostic],\n+    new: list[Diagnostic],\n+    expected: list[Diagnostic],\n ) -> list[GroupedDiagnostics]:",
      "comment": "GitHub doesn't really allow me to comment on that line but you could simplify your code a good amount by removing `None` from `old`, `new` and `expected`. Unless I miss a place where you create a `GroupedDiagnostics` instance where those fields are `None`",
      "comment_id": 2712920374,
      "user": "MichaReiser",
      "created_at": "2026-01-21T14:50:32Z",
      "url": "https://github.com/astral-sh/ruff/pull/22746#discussion_r2712920374"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22746,
      "file_path": "scripts/conformance.py",
      "line": 278,
      "side": "RIGHT",
      "diff_hunk": "@@ -241,17 +238,76 @@ def change(self) -> Change:\n \n     @property\n     def optional(self) -> bool:\n-        return self.expected is not None and self.expected.optional\n+        return bool(self.expected) and all(\n+            diagnostic.optional for diagnostic in self.expected\n+        )\n+\n+    @property\n+    def multi(self) -> bool:\n+        return bool(self.expected) and all(\n+            diagnostic.multi for diagnostic in self.expected\n+        )\n+\n+    def diagnostics_by_source(self, source: Source) -> list[Diagnostic]:\n+        match source:\n+            case Source.NEW:\n+                return self.new or []\n+            case Source.OLD:\n+                return self.old or []\n+            case Source.EXPECTED:\n+                return self.expected or []\n+            case _:\n+                raise ValueError(f\"Invalid source: {source}\")\n+\n+    def classify(self, source: Source) -> Classification:\n+        if source in self.sources and Source.EXPECTED in self.sources:\n+            assert self.expected is not None\n+            distinct_lines = len(\n+                {\n+                    diagnostic.location.positions.begin.line\n+                    for diagnostic in self.diagnostics_by_source(source)\n+                }\n+            )\n+            expected_max = len(self.expected) if self.multi else 1\n+\n+            if 1 <= distinct_lines <= expected_max:\n+                return Classification.TRUE_POSITIVE\n+            else:\n+                return Classification.FALSE_POSITIVE\n+\n+        elif source in self.sources and Source.EXPECTED not in self.sources:",
      "comment": "```suggestion\r\n        elif source in self.sources:\r\n```\r\n\r\nI believe the second part is implicitly given by not taking the first if branch\r\n\r\nIt might also be more readable if you nested the conditions like so:\r\n\r\n```py\r\nif source in self.sources:\r\n\tif Source.EXPECTED in self.sources:\r\n\t\tdistinct_lines = ... \r\n\r\n\telse:\r\n\t\treturn Classification.FalsePositive\r\n\r\nelif Source.EXPECTED in self.sources:\r\n\treturn Classification.FALSE_NEGATIVE\r\n\r\nelse: \r\n\treturn Classification.TRUE_NEGATIVE\t\r\n```",
      "comment_id": 2712931785,
      "user": "MichaReiser",
      "created_at": "2026-01-21T14:53:15Z",
      "url": "https://github.com/astral-sh/ruff/pull/22746#discussion_r2712931785"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22746,
      "file_path": "scripts/conformance.py",
      "line": 273,
      "side": "RIGHT",
      "diff_hunk": "@@ -241,17 +238,76 @@ def change(self) -> Change:\n \n     @property\n     def optional(self) -> bool:\n-        return self.expected is not None and self.expected.optional\n+        return bool(self.expected) and all(\n+            diagnostic.optional for diagnostic in self.expected\n+        )\n+\n+    @property\n+    def multi(self) -> bool:\n+        return bool(self.expected) and all(\n+            diagnostic.multi for diagnostic in self.expected\n+        )\n+\n+    def diagnostics_by_source(self, source: Source) -> list[Diagnostic]:\n+        match source:\n+            case Source.NEW:\n+                return self.new or []\n+            case Source.OLD:\n+                return self.old or []\n+            case Source.EXPECTED:\n+                return self.expected or []\n+            case _:\n+                raise ValueError(f\"Invalid source: {source}\")\n+\n+    def classify(self, source: Source) -> Classification:\n+        if source in self.sources and Source.EXPECTED in self.sources:\n+            assert self.expected is not None\n+            distinct_lines = len(\n+                {\n+                    diagnostic.location.positions.begin.line\n+                    for diagnostic in self.diagnostics_by_source(source)\n+                }\n+            )\n+            expected_max = len(self.expected) if self.multi else 1\n+\n+            if 1 <= distinct_lines <= expected_max:",
      "comment": "I can see how classifying too many diagnostics as false positives can be a bit confusing but I think it's the right thing. There's at least one false positive. \r\n\r\nIt might be better to count them as both a true positive and false positive, but I suspect that this doesn't play well with how we use those numbers later on.",
      "comment_id": 2712936781,
      "user": "MichaReiser",
      "created_at": "2026-01-21T14:54:28Z",
      "url": "https://github.com/astral-sh/ruff/pull/22746#discussion_r2712936781"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22746,
      "file_path": "scripts/conformance.py",
      "line": 273,
      "side": "RIGHT",
      "diff_hunk": "@@ -241,17 +238,76 @@ def change(self) -> Change:\n \n     @property\n     def optional(self) -> bool:\n-        return self.expected is not None and self.expected.optional\n+        return bool(self.expected) and all(\n+            diagnostic.optional for diagnostic in self.expected\n+        )\n+\n+    @property\n+    def multi(self) -> bool:\n+        return bool(self.expected) and all(\n+            diagnostic.multi for diagnostic in self.expected\n+        )\n+\n+    def diagnostics_by_source(self, source: Source) -> list[Diagnostic]:\n+        match source:\n+            case Source.NEW:\n+                return self.new or []\n+            case Source.OLD:\n+                return self.old or []\n+            case Source.EXPECTED:\n+                return self.expected or []\n+            case _:\n+                raise ValueError(f\"Invalid source: {source}\")\n+\n+    def classify(self, source: Source) -> Classification:\n+        if source in self.sources and Source.EXPECTED in self.sources:\n+            assert self.expected is not None\n+            distinct_lines = len(\n+                {\n+                    diagnostic.location.positions.begin.line\n+                    for diagnostic in self.diagnostics_by_source(source)\n+                }\n+            )\n+            expected_max = len(self.expected) if self.multi else 1\n+\n+            if 1 <= distinct_lines <= expected_max:",
      "comment": "Makes sense. One advantage of grouping them like this is that you'll be able to see why an error that appears to match a line in the conformance suite is a false positive if another line with the same tag has a diagnostic.",
      "comment_id": 2714696511,
      "user": "WillDuke",
      "created_at": "2026-01-21T23:24:01Z",
      "url": "https://github.com/astral-sh/ruff/pull/22746#discussion_r2714696511"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22746,
      "file_path": "scripts/conformance.py",
      "line": 265,
      "side": "RIGHT",
      "diff_hunk": "@@ -241,17 +238,79 @@ def change(self) -> Change:\n \n     @property\n     def optional(self) -> bool:\n-        return self.expected is not None and self.expected.optional\n+        return bool(self.expected) and all(\n+            diagnostic.optional for diagnostic in self.expected\n+        )\n+\n+    @property\n+    def multi(self) -> bool:\n+        return bool(self.expected) and all(\n+            diagnostic.multi for diagnostic in self.expected\n+        )\n+\n+    def diagnostics_by_source(self, source: Source) -> list[Diagnostic]:\n+        match source:\n+            case Source.NEW:\n+                return self.new\n+            case Source.OLD:\n+                return self.old\n+            case Source.EXPECTED:\n+                return self.expected\n+            case _:\n+                raise ValueError(f\"Invalid source: {source}\")\n+\n+    def classify(self, source: Source) -> Classification:\n+        if source in self.sources:\n+            if Source.EXPECTED in self.sources:\n+                assert self.expected is not None",
      "comment": "I'm surprised that ty doesn't catch this but `self.expected` should never be `None` here",
      "comment_id": 2715756019,
      "user": "MichaReiser",
      "created_at": "2026-01-22T08:05:02Z",
      "url": "https://github.com/astral-sh/ruff/pull/22746#discussion_r2715756019"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22746,
      "file_path": "scripts/conformance.py",
      "line": 260,
      "side": "RIGHT",
      "diff_hunk": "@@ -241,17 +238,79 @@ def change(self) -> Change:\n \n     @property\n     def optional(self) -> bool:\n-        return self.expected is not None and self.expected.optional\n+        return bool(self.expected) and all(\n+            diagnostic.optional for diagnostic in self.expected\n+        )\n+\n+    @property\n+    def multi(self) -> bool:\n+        return bool(self.expected) and all(\n+            diagnostic.multi for diagnostic in self.expected\n+        )\n+\n+    def diagnostics_by_source(self, source: Source) -> list[Diagnostic]:\n+        match source:\n+            case Source.NEW:\n+                return self.new\n+            case Source.OLD:\n+                return self.old\n+            case Source.EXPECTED:\n+                return self.expected\n+            case _:\n+                raise ValueError(f\"Invalid source: {source}\")",
      "comment": "I'm surprised that this last `case` is needed here? Does ty complain without it?",
      "comment_id": 2715758499,
      "user": "MichaReiser",
      "created_at": "2026-01-22T08:05:55Z",
      "url": "https://github.com/astral-sh/ruff/pull/22746#discussion_r2715758499"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22746,
      "file_path": "scripts/conformance.py",
      "line": 487,
      "side": "RIGHT",
      "diff_hunk": "@@ -366,64 +423,88 @@ def collect_ty_diagnostics(\n     ]\n \n \n-def group_diagnostics_by_key(\n-    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+def group_diagnostics_by_key_or_tag(\n+    old: list[Diagnostic],\n+    new: list[Diagnostic],\n+    expected: list[Diagnostic],\n ) -> list[GroupedDiagnostics]:\n+    # propagate tags from expected diagnostics to old and new diagnostics\n+    tagged_lines = {\n+        (d.location.path.name, d.location.positions.begin.line): d.tag\n+        for d in expected\n+        if d.tag is not None\n+    }\n+\n+    for diag in old:\n+        diag.tag = tagged_lines.get(\n+            (diag.location.path.name, diag.location.positions.begin.line), None\n+        )\n+\n+    for diag in new:\n+        diag.tag = tagged_lines.get(\n+            (diag.location.path.name, diag.location.positions.begin.line), None\n+        )\n+\n     diagnostics = [\n         *old,\n         *new,\n         *expected,\n     ]\n \n-    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n-\n-    grouped = []\n-    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n-        group = list(group)\n-        sources: Source = reduce(or_, (diag.source for diag in group))\n-        grouped.append(\n-            GroupedDiagnostics(\n-                key=key,\n-                sources=sources,\n-                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n-                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n-                expected=next(\n-                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n-                ),\n-            )\n+    # group diagnostics by a key which may be a path and a line or a path and a tag\n+    diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+    grouped_diagnostics = []\n+    for key, group in groupby(diagnostics, key=attrgetter(\"key\")):\n+        old_diagnostics: list[Diagnostic] = []\n+        new_diagnostics: list[Diagnostic] = []\n+        expected_diagnostics: list[Diagnostic] = []\n+        sources: set[Source] = set()\n+\n+        for diag in group:\n+            sources.add(diag.source)\n+            match diag.source:\n+                case Source.OLD:\n+                    old_diagnostics.append(diag)\n+                case Source.NEW:\n+                    new_diagnostics.append(diag)\n+                case Source.EXPECTED:\n+                    expected_diagnostics.append(diag)\n+\n+        grouped = GroupedDiagnostics(\n+            key=key,\n+            sources=sources,\n+            old=old_diagnostics,\n+            new=new_diagnostics,\n+            expected=expected_diagnostics,\n         )\n+        grouped_diagnostics.append(grouped)\n \n-    return grouped\n+    return grouped_diagnostics\n \n \n def compute_stats(\n     grouped_diagnostics: list[GroupedDiagnostics],\n-    source: Source,\n+    ty_version: Literal[\"new\", \"old\"],",
      "comment": "This is nice. Does Python allow us to define a union over `Source.NEW | Source.OLD` or is this something Python doesn't support?",
      "comment_id": 2715761401,
      "user": "MichaReiser",
      "created_at": "2026-01-22T08:06:50Z",
      "url": "https://github.com/astral-sh/ruff/pull/22746#discussion_r2715761401"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22746,
      "file_path": "scripts/conformance.py",
      "line": 487,
      "side": "RIGHT",
      "diff_hunk": "@@ -366,64 +423,88 @@ def collect_ty_diagnostics(\n     ]\n \n \n-def group_diagnostics_by_key(\n-    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+def group_diagnostics_by_key_or_tag(\n+    old: list[Diagnostic],\n+    new: list[Diagnostic],\n+    expected: list[Diagnostic],\n ) -> list[GroupedDiagnostics]:\n+    # propagate tags from expected diagnostics to old and new diagnostics\n+    tagged_lines = {\n+        (d.location.path.name, d.location.positions.begin.line): d.tag\n+        for d in expected\n+        if d.tag is not None\n+    }\n+\n+    for diag in old:\n+        diag.tag = tagged_lines.get(\n+            (diag.location.path.name, diag.location.positions.begin.line), None\n+        )\n+\n+    for diag in new:\n+        diag.tag = tagged_lines.get(\n+            (diag.location.path.name, diag.location.positions.begin.line), None\n+        )\n+\n     diagnostics = [\n         *old,\n         *new,\n         *expected,\n     ]\n \n-    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n-\n-    grouped = []\n-    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n-        group = list(group)\n-        sources: Source = reduce(or_, (diag.source for diag in group))\n-        grouped.append(\n-            GroupedDiagnostics(\n-                key=key,\n-                sources=sources,\n-                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n-                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n-                expected=next(\n-                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n-                ),\n-            )\n+    # group diagnostics by a key which may be a path and a line or a path and a tag\n+    diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+    grouped_diagnostics = []\n+    for key, group in groupby(diagnostics, key=attrgetter(\"key\")):\n+        old_diagnostics: list[Diagnostic] = []\n+        new_diagnostics: list[Diagnostic] = []\n+        expected_diagnostics: list[Diagnostic] = []\n+        sources: set[Source] = set()\n+\n+        for diag in group:\n+            sources.add(diag.source)\n+            match diag.source:\n+                case Source.OLD:\n+                    old_diagnostics.append(diag)\n+                case Source.NEW:\n+                    new_diagnostics.append(diag)\n+                case Source.EXPECTED:\n+                    expected_diagnostics.append(diag)\n+\n+        grouped = GroupedDiagnostics(\n+            key=key,\n+            sources=sources,\n+            old=old_diagnostics,\n+            new=new_diagnostics,\n+            expected=expected_diagnostics,\n         )\n+        grouped_diagnostics.append(grouped)\n \n-    return grouped\n+    return grouped_diagnostics\n \n \n def compute_stats(\n     grouped_diagnostics: list[GroupedDiagnostics],\n-    source: Source,\n+    ty_version: Literal[\"new\", \"old\"],",
      "comment": "I tried the same thing, but unfortunately it does not!",
      "comment_id": 2718728396,
      "user": "WillDuke",
      "created_at": "2026-01-22T21:37:33Z",
      "url": "https://github.com/astral-sh/ruff/pull/22746#discussion_r2718728396"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22746,
      "file_path": "scripts/conformance.py",
      "line": 260,
      "side": "RIGHT",
      "diff_hunk": "@@ -241,17 +238,79 @@ def change(self) -> Change:\n \n     @property\n     def optional(self) -> bool:\n-        return self.expected is not None and self.expected.optional\n+        return bool(self.expected) and all(\n+            diagnostic.optional for diagnostic in self.expected\n+        )\n+\n+    @property\n+    def multi(self) -> bool:\n+        return bool(self.expected) and all(\n+            diagnostic.multi for diagnostic in self.expected\n+        )\n+\n+    def diagnostics_by_source(self, source: Source) -> list[Diagnostic]:\n+        match source:\n+            case Source.NEW:\n+                return self.new\n+            case Source.OLD:\n+                return self.old\n+            case Source.EXPECTED:\n+                return self.expected\n+            case _:\n+                raise ValueError(f\"Invalid source: {source}\")",
      "comment": "Just defensive programming while I was writing this!",
      "comment_id": 2718740704,
      "user": "WillDuke",
      "created_at": "2026-01-22T21:41:08Z",
      "url": "https://github.com/astral-sh/ruff/pull/22746#discussion_r2718740704"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22746,
      "file_path": "scripts/conformance.py",
      "line": 107,
      "side": "RIGHT",
      "diff_hunk": "@@ -105,6 +103,15 @@ def into_title(self) -> str:\n                 return \"True positives removed\"\n \n \n+@dataclass(kw_only=True, slots=True)\n+class Evaluation:",
      "comment": "This class feels pretty heavy only to support the case where one group has both true and false positives. \r\n\r\nI was wondering if we could change `classify` to return an iterable of `(Classification, int)` instead. Most groups return exactly one, with the exception of the `many` case where ty emits too many diagnostics, in which case we return two.",
      "comment_id": 2721493099,
      "user": "MichaReiser",
      "created_at": "2026-01-23T14:43:12Z",
      "url": "https://github.com/astral-sh/ruff/pull/22746#discussion_r2721493099"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22746,
      "file_path": "scripts/conformance.py",
      "line": 107,
      "side": "RIGHT",
      "diff_hunk": "@@ -105,6 +103,15 @@ def into_title(self) -> str:\n                 return \"True positives removed\"\n \n \n+@dataclass(kw_only=True, slots=True)\n+class Evaluation:",
      "comment": "With the last set of changes, we now count the diagnostics individually. So if ty emits 5 diagnostics on the same line where a \"# E\" is present, we're counting them all as true positives. Similarly, if ty raises 3 diagnostics on one line of a tagged group (no '+') and 1 on each of the other lines, we count the 3 diagnostics as true positives and the remainder as false positives. \n\nHappy to keep iterating on it though if this doesn't make sense. ",
      "comment_id": 2723118794,
      "user": "WillDuke",
      "created_at": "2026-01-23T22:54:13Z",
      "url": "https://github.com/astral-sh/ruff/pull/22746#discussion_r2723118794"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22234,
      "file_path": "crates/ruff_linter/resources/test/fixtures/refurb/FURB180.py",
      "line": 92,
      "side": "RIGHT",
      "diff_hunk": "@@ -56,3 +56,37 @@ def foo(self): pass\n class A7(B0, abc.ABC, B1):\n     @abstractmethod\n     def foo(self): pass\n+\n+\n+# Regression tests for https://github.com/astral-sh/ruff/issues/17162\n+class A8(abc.ABC, metaclass=ABCMeta):  # FURB180\n+    @abstractmethod\n+    def foo(self):\n+        pass\n+\n+\n+def a9():\n+    from abc import ABC\n+\n+    class A9(ABC, metaclass=ABCMeta):  # FURB180\n+        @abstractmethod\n+        def foo(self):\n+            pass\n+\n+\n+def a10():\n+    from abc import ABC as ABCAlternativeName\n+\n+    class A10(ABCAlternativeName, metaclass=ABCMeta):  # FURB180\n+        @abstractmethod\n+        def foo(self):\n+            pass\n+\n+\n+class MyMetaClass(abc.ABC): ...\n+\n+\n+class A11(MyMetaClass, metaclass=ABCMeta):  # FURB180\n+    @abstractmethod\n+    def foo(self):\n+        pass",
      "comment": "Could you add a test case for the unsafe comment deletion? I think something like this example would work well:\n\n```py\nclass C(\n    other_kwarg=1,\n    # comment\n    metaclass=abc.ABCMeta,\n):\n    pass\n```",
      "comment_id": 2699959682,
      "user": "ntBre",
      "created_at": "2026-01-16T21:02:50Z",
      "url": "https://github.com/astral-sh/ruff/pull/22234#discussion_r2699959682"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22234,
      "file_path": "crates/ruff_linter/resources/test/fixtures/refurb/FURB180.py",
      "line": 92,
      "side": "RIGHT",
      "diff_hunk": "@@ -56,3 +56,37 @@ def foo(self): pass\n class A7(B0, abc.ABC, B1):\n     @abstractmethod\n     def foo(self): pass\n+\n+\n+# Regression tests for https://github.com/astral-sh/ruff/issues/17162\n+class A8(abc.ABC, metaclass=ABCMeta):  # FURB180\n+    @abstractmethod\n+    def foo(self):\n+        pass\n+\n+\n+def a9():\n+    from abc import ABC\n+\n+    class A9(ABC, metaclass=ABCMeta):  # FURB180\n+        @abstractmethod\n+        def foo(self):\n+            pass\n+\n+\n+def a10():\n+    from abc import ABC as ABCAlternativeName\n+\n+    class A10(ABCAlternativeName, metaclass=ABCMeta):  # FURB180\n+        @abstractmethod\n+        def foo(self):\n+            pass\n+\n+\n+class MyMetaClass(abc.ABC): ...\n+\n+\n+class A11(MyMetaClass, metaclass=ABCMeta):  # FURB180\n+    @abstractmethod\n+    def foo(self):\n+        pass",
      "comment": "Sure, the test case was added: https://github.com/astral-sh/ruff/pull/22234/changes/9c429186ed3eb060415d1b67f4c5ae96ad64722b",
      "comment_id": 2700644360,
      "user": "akawd",
      "created_at": "2026-01-17T05:03:33Z",
      "url": "https://github.com/astral-sh/ruff/pull/22234#discussion_r2700644360"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22320,
      "file_path": "crates/ruff_linter/resources/test/fixtures/pyupgrade/UP045_py39.py",
      "line": 15,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,15 @@\n+\"\"\"\n+Regression test for https://github.com/astral-sh/ruff/issues/20096\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from typing import Optional, cast, TypeAlias\n+\n+\n+x: Optional[str]             # UP045\n+x: \"Optional[str]\"           # UP045\n+cast(\"Optional[str]\", None)  # UP045\n+cast(Optional[str], None)    # okay, str | None is a runtime error\n+x: TypeAlias = \"Optional[str]\"  # UP045\n+x: TypeAlias = Optional[str]  # okay",
      "comment": "Can we add some tests for \"Complex\" string annotations (an annotation that uses implicit string concatenation). ",
      "comment_id": 2661017993,
      "user": "MichaReiser",
      "created_at": "2026-01-05T10:28:32Z",
      "url": "https://github.com/astral-sh/ruff/pull/22320#discussion_r2661017993"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22320,
      "file_path": "crates/ruff_linter/resources/test/fixtures/pyupgrade/UP045_py39.py",
      "line": 15,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,15 @@\n+\"\"\"\n+Regression test for https://github.com/astral-sh/ruff/issues/20096\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from typing import Optional, cast, TypeAlias\n+\n+\n+x: Optional[str]             # UP045\n+x: \"Optional[str]\"           # UP045\n+cast(\"Optional[str]\", None)  # UP045\n+cast(Optional[str], None)    # okay, str | None is a runtime error\n+x: TypeAlias = \"Optional[str]\"  # UP045\n+x: TypeAlias = Optional[str]  # okay",
      "comment": "Interestingly, the complex cases don't emit diagnostics even with my change. Claude thinks this is a bug, but I'll have to dig into it a bit more.\n\nty also emits an [implicit-concatenated-string-type-annotation](https://docs.astral.sh/ty/reference/rules/#implicit-concatenated-string-type-annotation) diagnostic on the new test cases, so I could possibly just replace `in_string_type_definition` with `in_simple_string_type_definition` if we want to filter out the complex cases intentionally.",
      "comment_id": 2684195946,
      "user": "ntBre",
      "created_at": "2026-01-12T23:07:35Z",
      "url": "https://github.com/astral-sh/ruff/pull/22320#discussion_r2684195946"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22320,
      "file_path": "crates/ruff_linter/resources/test/fixtures/pyupgrade/UP045_py39.py",
      "line": 15,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,15 @@\n+\"\"\"\n+Regression test for https://github.com/astral-sh/ruff/issues/20096\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from typing import Optional, cast, TypeAlias\n+\n+\n+x: Optional[str]             # UP045\n+x: \"Optional[str]\"           # UP045\n+cast(\"Optional[str]\", None)  # UP045\n+cast(Optional[str], None)    # okay, str | None is a runtime error\n+x: TypeAlias = \"Optional[str]\"  # UP045\n+x: TypeAlias = Optional[str]  # okay",
      "comment": "Yeah, this is a bit surprising. Let me know if you want me to take a closer look",
      "comment_id": 2711917554,
      "user": "MichaReiser",
      "created_at": "2026-01-21T10:23:07Z",
      "url": "https://github.com/astral-sh/ruff/pull/22320#discussion_r2711917554"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22320,
      "file_path": "crates/ruff_linter/resources/test/fixtures/pyupgrade/UP045_py39.py",
      "line": 15,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,15 @@\n+\"\"\"\n+Regression test for https://github.com/astral-sh/ruff/issues/20096\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from typing import Optional, cast, TypeAlias\n+\n+\n+x: Optional[str]             # UP045\n+x: \"Optional[str]\"           # UP045\n+cast(\"Optional[str]\", None)  # UP045\n+cast(Optional[str], None)    # okay, str | None is a runtime error\n+x: TypeAlias = \"Optional[str]\"  # UP045\n+x: TypeAlias = Optional[str]  # okay",
      "comment": "I think I'll just close this for now, unless you think it's worth landing without looking into the (arguably separate) issue with concatenated annotations. This is a pretty niche issue in any case.",
      "comment_id": 2713264436,
      "user": "ntBre",
      "created_at": "2026-01-21T16:06:15Z",
      "url": "https://github.com/astral-sh/ruff/pull/22320#discussion_r2713264436"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22320,
      "file_path": "crates/ruff_linter/resources/test/fixtures/pyupgrade/UP045_py39.py",
      "line": 15,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,15 @@\n+\"\"\"\n+Regression test for https://github.com/astral-sh/ruff/issues/20096\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from typing import Optional, cast, TypeAlias\n+\n+\n+x: Optional[str]             # UP045\n+x: \"Optional[str]\"           # UP045\n+cast(\"Optional[str]\", None)  # UP045\n+cast(Optional[str], None)    # okay, str | None is a runtime error\n+x: TypeAlias = \"Optional[str]\"  # UP045\n+x: TypeAlias = Optional[str]  # okay",
      "comment": "What you have here seems better than what we had before and should cover the majority of stringified type annotations",
      "comment_id": 2715700468,
      "user": "MichaReiser",
      "created_at": "2026-01-22T07:45:36Z",
      "url": "https://github.com/astral-sh/ruff/pull/22320#discussion_r2715700468"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22320,
      "file_path": "crates/ruff_linter/resources/test/fixtures/pyupgrade/UP045_py39.py",
      "line": 17,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,35 @@\n+\"\"\"\n+Regression test for https://github.com/astral-sh/ruff/issues/20096\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from typing import Optional, cast, TypeAlias\n+\n+\n+x: Optional[str]             # UP045\n+x: \"Optional[str]\"           # UP045\n+cast(\"Optional[str]\", None)  # UP045\n+cast(Optional[str], None)    # okay, str | None is a runtime error\n+x: TypeAlias = \"Optional[str]\"  # UP045\n+x: TypeAlias = Optional[str]  # okay\n+\n+# complex (implicitly concatenated) annotations",
      "comment": "Let's add a TODO comment here to make it clear, that the rule isn't currently handling implicitly concatenated strings.",
      "comment_id": 2715701759,
      "user": "MichaReiser",
      "created_at": "2026-01-22T07:46:04Z",
      "url": "https://github.com/astral-sh/ruff/pull/22320#discussion_r2715701759"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22320,
      "file_path": "crates/ruff_linter/resources/test/fixtures/pyupgrade/UP045_py39.py",
      "line": 15,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,15 @@\n+\"\"\"\n+Regression test for https://github.com/astral-sh/ruff/issues/20096\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from typing import Optional, cast, TypeAlias\n+\n+\n+x: Optional[str]             # UP045\n+x: \"Optional[str]\"           # UP045\n+cast(\"Optional[str]\", None)  # UP045\n+cast(Optional[str], None)    # okay, str | None is a runtime error\n+x: TypeAlias = \"Optional[str]\"  # UP045\n+x: TypeAlias = Optional[str]  # okay",
      "comment": "Sorry, I only just realized that my comment probably made it sound like I had left a bug in the PR. When Claude and I looked into it, the bug appears to be upstream of the rule itself in our complex string annotation parsing, or at least in how that parsing interacts with our semantic model. I think the rule itself is behaving correctly with the annotations it's asked to check.\r\n\r\nThanks for taking another look!",
      "comment_id": 2717344978,
      "user": "ntBre",
      "created_at": "2026-01-22T15:14:30Z",
      "url": "https://github.com/astral-sh/ruff/pull/22320#discussion_r2717344978"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22798,
      "file_path": "crates/ruff_linter/resources/test/fixtures/flake8_pyi/PYI034.py",
      "line": 330,
      "side": "RIGHT",
      "diff_hunk": "@@ -323,6 +323,13 @@ def __iadd__(self, other: \"UsesStringizedAnnotations\") -> \"typing.Self\":\n         return self\n \n \n+class UsesStringizedForwardReferences:\n+    def __new__(cls) -> \"UsesStringizedForwardReferences\": ...       # PYI034\n+    def __enter__(self) -> \"UsesStringizedForwardReferences\": ...    # PYI034\n+    async def __aenter__(self) -> \"UsesStringizedForwardReferences\": ...  # PYI034\n+    def __iadd__(self, other) -> \"UsesStringizedForwardReferences\": ...  # PYI034",
      "comment": "nit: could you possibly move this to the end of the fixture file? It's very hard to review the diff to the snapshot when all the line numbers change \ud83d\ude06",
      "comment_id": 2716597174,
      "user": "AlexWaygood",
      "created_at": "2026-01-22T11:55:47Z",
      "url": "https://github.com/astral-sh/ruff/pull/22798#discussion_r2716597174"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21110,
      "file_path": "crates/ruff_python_formatter/resources/test/fixtures/ruff/newlines.py",
      "line": 374,
      "side": "RIGHT",
      "diff_hunk": "@@ -335,3 +335,40 @@ def overload4():\n     # trailing comment\n \n def overload4(a: int): ...\n+\n+\n+# In preview, we preserve these newlines at the start of functions:\n+def preserved1():\n+\n+    return 1\n+\n+def preserved2():\n+\n+    pass\n+\n+\n+# But we still discard these newlines:\n+def removed1():\n+\n+    \"Docstring\"\n+\n+    return 1\n+\n+\n+def removed2():\n+\n+    # Comment\n+\n+    return 1\n+\n+\n+def removed3():\n+\n+    ...\n+\n+\n+# And we discard empty lines after the first:\n+def partially_preserved1():\n+\n+\n+    return 1",
      "comment": "Can we add more tests to this, specifically tests including comments or cases where the first element is a function on its own?",
      "comment_id": 2473686964,
      "user": "MichaReiser",
      "created_at": "2025-10-29T15:07:52Z",
      "url": "https://github.com/astral-sh/ruff/pull/21110#discussion_r2473686964"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22671,
      "file_path": "scripts/conformance.py",
      "line": 300,
      "side": "RIGHT",
      "diff_hunk": "@@ -297,8 +297,12 @@ def total(self) -> int:\n \n def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n     diagnostics: list[Diagnostic] = []\n-    for file in path.resolve().rglob(\"*.py\"):\n-        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+    for entry in path.iterdir():",
      "comment": "I'm surprised that Python's globbing doesn't support multiple extensions, e.g. `.{py,pyi}`",
      "comment_id": 2702308348,
      "user": "MichaReiser",
      "created_at": "2026-01-18T10:45:52Z",
      "url": "https://github.com/astral-sh/ruff/pull/22671#discussion_r2702308348"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22671,
      "file_path": "scripts/conformance.py",
      "line": 303,
      "side": "RIGHT",
      "diff_hunk": "@@ -297,8 +297,12 @@ def total(self) -> int:\n \n def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n     diagnostics: list[Diagnostic] = []\n-    for file in path.resolve().rglob(\"*.py\"):\n-        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+    for entry in path.iterdir():\n+        if not entry.is_file():\n+            continue\n+        if entry.suffix not in {\".py\", \".pyi\"}:",
      "comment": "Should we skip files starting with an `_`? I assume it's not strictly necessary, since they never contain any comments matching the error pattern but it feels unnecessary",
      "comment_id": 2702309581,
      "user": "MichaReiser",
      "created_at": "2026-01-18T10:47:07Z",
      "url": "https://github.com/astral-sh/ruff/pull/22671#discussion_r2702309581"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22671,
      "file_path": "scripts/conformance.py",
      "line": 355,
      "side": "RIGHT",
      "diff_hunk": "@@ -345,7 +349,11 @@ def collect_ty_diagnostics(\n             f\"--python-version={python_version}\",\n             \"--output-format=gitlab\",\n             \"--exit-zero\",\n-            tests_path,\n+            *(\n+                str(path)\n+                for path in Path(tests_path).iterdir()\n+                if path.suffix in {\".py\", \".pyi\"} and not path.name.startswith(\"_\")",
      "comment": "Should we create a helper method that, given a path, returns whether this is a conformance test file?",
      "comment_id": 2702310372,
      "user": "MichaReiser",
      "created_at": "2026-01-18T10:47:47Z",
      "url": "https://github.com/astral-sh/ruff/pull/22671#discussion_r2702310372"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22647,
      "file_path": "scripts/conformance.py",
      "line": 118,
      "side": "RIGHT",
      "diff_hunk": "@@ -101,6 +103,21 @@ def into_title(self) -> str:\n                 return \"True positives removed\"\n \n \n+class Change(StrEnum):\n+    ADDED = auto()\n+    REMOVED = auto()\n+    UNCHANGED = auto()\n+\n+    def into_title(self) -> str:\n+        match self:\n+            case Change.ADDED:\n+                return \"Added (Optional)\"\n+            case Change.REMOVED:\n+                return \"Removed (Optional)\"\n+            case Change.UNCHANGED:\n+                return \"Unchanged (Optional)\"",
      "comment": "```suggestion\r\n                return \"Optional Diagnostics Added\"\r\n            case Change.REMOVED:\r\n                return \"Optional Diagnostics Removed\"\r\n            case Change.UNCHANGED:\r\n                return \"Optional Diagnostics Unchanged\"\r\n```",
      "comment_id": 2701042575,
      "user": "AlexWaygood",
      "created_at": "2026-01-17T12:23:32Z",
      "url": "https://github.com/astral-sh/ruff/pull/22647#discussion_r2701042575"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22647,
      "file_path": "scripts/conformance.py",
      "line": 156,
      "side": "RIGHT",
      "diff_hunk": "@@ -136,8 +153,11 @@ class Diagnostic:\n     fingerprint: str | None\n     location: Location\n     source: Source\n+    optional: bool",
      "comment": "It doesn't need to be in this PR, but it might be good if we could add some docs for the fields on this class at some point. `optional` is fairly clear, but I'd have to study the script a bit to figure out what e.g. `fingerprint` is \ud83d\ude04",
      "comment_id": 2701043523,
      "user": "AlexWaygood",
      "created_at": "2026-01-17T12:24:34Z",
      "url": "https://github.com/astral-sh/ruff/pull/22647#discussion_r2701043523"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22647,
      "file_path": "scripts/conformance.py",
      "line": 452,
      "side": "RIGHT",
      "diff_hunk": "@@ -397,13 +431,25 @@ def render_grouped_diagnostics(\n     format: Literal[\"diff\", \"github\"] = \"diff\",\n ) -> str:\n     if changed_only:\n-        grouped = [diag for diag in grouped if diag.changed]\n+        grouped = [\n+            diag for diag in grouped if diag.change in (Change.ADDED, Change.REMOVED)\n+        ]\n \n-    sorted_by_class = sorted(\n-        grouped,\n+    g1, g2 = tee(grouped)\n+    required, optional = (\n+        filterfalse(attrgetter(\"optional\"), g1),\n+        filter(attrgetter(\"optional\"), g2),\n+    )\n+    required = sorted(\n+        required,\n         key=attrgetter(\"classification\"),\n         reverse=True,\n     )\n+    optional = sorted(\n+        optional,\n+        key=attrgetter(\"change\"),\n+        reverse=True,\n+    )",
      "comment": "I think in this case, you can do this in a slightly more readable way without the `tee` ;)\r\n\r\nWhat about something like\r\n\r\n```diff\r\ndiff --git a/scripts/conformance.py b/scripts/conformance.py\r\nindex f8e7124d8a..383c5eb6bf 100644\r\n--- a/scripts/conformance.py\r\n+++ b/scripts/conformance.py\r\n@@ -435,19 +435,17 @@ def render_grouped_diagnostics(\r\n             diag for diag in grouped if diag.change in (Change.ADDED, Change.REMOVED)\r\n         ]\r\n \r\n-    g1, g2 = tee(grouped)\r\n-    required, optional = (\r\n-        filterfalse(attrgetter(\"optional\"), g1),\r\n-        filter(attrgetter(\"optional\"), g2),\r\n-    )\r\n-    required = sorted(\r\n-        required,\r\n-        key=attrgetter(\"classification\"),\r\n+    get_change = attrgetter(\"change\")\r\n+    get_classification = attrgetter(\"classification\")\r\n+\r\n+    optional_diagnostics = sorted(\r\n+        (diag for diag in grouped if diag.optional),\r\n+        key=get_change,\r\n         reverse=True,\r\n     )\r\n-    optional = sorted(\r\n-        optional,\r\n-        key=attrgetter(\"change\"),\r\n+    required_diagnostics = sorted(\r\n+        (diag for diag in grouped if not diag.optional),\r\n+        key=get_classification,\r\n         reverse=True,\r\n     )\r\n \r\n@@ -466,8 +464,8 @@ def render_grouped_diagnostics(\r\n \r\n     lines = []\r\n     for group, diagnostics in chain(\r\n-        groupby(required, key=attrgetter(\"classification\")),\r\n-        groupby(optional, key=attrgetter(\"change\")),\r\n+        groupby(required_diagnostics, key=get_classification),\r\n+        groupby(optional_diagnostics, key=get_change),\r\n     ):\r\n         lines.append(f\"### {group.into_title()}\")\r\n         lines.extend([\"\", \"<details>\", \"\"])\r\n```",
      "comment_id": 2701049489,
      "user": "AlexWaygood",
      "created_at": "2026-01-17T12:30:40Z",
      "url": "https://github.com/astral-sh/ruff/pull/22647#discussion_r2701049489"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22647,
      "file_path": "scripts/conformance.py",
      "line": 156,
      "side": "RIGHT",
      "diff_hunk": "@@ -136,8 +153,11 @@ class Diagnostic:\n     fingerprint: str | None\n     location: Location\n     source: Source\n+    optional: bool",
      "comment": "Makes sense. Regarding the fingerprint, I'm just preserving the data returned from `ty` in the gitlab output. I assume it provides a unique identifier for that diagnostic, but I don't actually know! Maybe it would be best to drop it since it is not used.",
      "comment_id": 2701064831,
      "user": "WillDuke",
      "created_at": "2026-01-17T12:45:05Z",
      "url": "https://github.com/astral-sh/ruff/pull/22647#discussion_r2701064831"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22114,
      "file_path": "crates/ruff_linter/resources/test/fixtures/ruff/RUF068.py",
      "line": 23,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,34 @@\n+import typing\n+\n+\n+class A: ...\n+\n+\n+class B: ...\n+\n+\n+# Good\n+__all__ = \"A\" + \"B\"\n+__all__: list[str] = [\"A\", \"B\"]\n+__all__: typing.Any = (\"A\", \"B\")\n+__all__ = [\"A\", \"B\"]\n+__all__ += [\"A\", \"B\"]\n+__all__.extend([\"A\", \"B\"])\n+\n+# Bad\n+__all__: list[str] = [\"A\", \"B\", \"A\"]\n+__all__: typing.Any = (\"A\", \"B\", \"B\")\n+__all__ = [\"A\", \"A\", \"B\"]\n+__all__ = [\"A\", \"B\", \"A\"]\n+__all__ = [\"A\", \"A\", \"B\", \"B\"]",
      "comment": "Let's throw in one multi-line case without comments.",
      "comment_id": 2656479847,
      "user": "ntBre",
      "created_at": "2026-01-01T16:38:35Z",
      "url": "https://github.com/astral-sh/ruff/pull/22114#discussion_r2656479847"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22114,
      "file_path": "crates/ruff_linter/resources/test/fixtures/ruff/RUF069.py",
      "line": 32,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,33 @@\n+import typing\n+\n+\n+class A: ...\n+\n+\n+class B: ...\n+\n+\n+# Good\n+__all__ = \"A\" + \"B\"\n+__all__: list[str] = [\"A\", \"B\"]\n+__all__: typing.Any = (\"A\", \"B\")\n+__all__ = [\"A\", \"B\"]\n+__all__ += [\"A\", \"B\"]\n+__all__.extend([\"A\", \"B\"])\n+\n+# Bad\n+__all__: list[str] = [\"A\", \"B\", \"A\"]\n+__all__: typing.Any = (\"A\", \"B\", \"B\")\n+__all__ = [\"A\", \"B\", \"A\"]\n+__all__ = [\"A\", \"A\", \"B\", \"B\"]\n+__all__ += [\"B\", \"B\"]\n+__all__.extend([\"B\", \"B\"])\n+\n+# Bad, unsafe\n+__all__ = [\n+    \"A\",\n+    \"A\",\n+    \"B\",\n+    # Comment\n+    \"B\",",
      "comment": "Can we add an end-of-line comment too? And maybe one after the deleted element, just to check which comments get deleted.\n\n\n```suggestion\n    \"B\",  # 2\n    # 3\n```",
      "comment_id": 2683974284,
      "user": "ntBre",
      "created_at": "2026-01-12T21:31:27Z",
      "url": "https://github.com/astral-sh/ruff/pull/22114#discussion_r2683974284"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22114,
      "file_path": "crates/ruff_linter/resources/test/fixtures/ruff/RUF069.py",
      "line": 15,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,41 @@\n+import typing\n+\n+\n+class A: ...\n+\n+\n+class B: ...\n+\n+\n+# Good\n+__all__ = \"A\" + \"B\"\n+__all__: list[str] = [\"A\", \"B\"]\n+__all__: typing.Any = (\"A\", \"B\")\n+__all__ = [\"A\", \"B\"]\n+__all__ = [A, \"A\", \"B\"]",
      "comment": "I think this is actually what I meant in https://github.com/astral-sh/ruff/pull/22114#discussion_r2656477043:\n\n\n```suggestion\n__all__ = [A, \"B\", \"B\"]\n```\n\nSorry for being quite pedantic, I just want to have one test case for the `continue` vs `return` choice.",
      "comment_id": 2699643146,
      "user": "ntBre",
      "created_at": "2026-01-16T19:01:58Z",
      "url": "https://github.com/astral-sh/ruff/pull/22114#discussion_r2699643146"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22593,
      "file_path": "scripts/conformance.py",
      "line": 622,
      "side": "RIGHT",
      "diff_hunk": "@@ -612,6 +619,7 @@ def main():\n     if args.output:\n         args.output.write_text(rendered, encoding=\"utf-8\")\n         print(f\"Output written to {args.output}\", file=sys.stderr)\n+        print(rendered, file=sys.stderr)",
      "comment": "(I think it's useful to keep this print statement anyway, so that we can see the report in the logs for the CI job even if the commenting bot fails to do its job!)",
      "comment_id": 2694112528,
      "user": "AlexWaygood",
      "created_at": "2026-01-15T12:01:08Z",
      "url": "https://github.com/astral-sh/ruff/pull/22593#discussion_r2694112528"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22593,
      "file_path": "scripts/conformance.py",
      "line": 622,
      "side": "RIGHT",
      "diff_hunk": "@@ -612,6 +619,7 @@ def main():\n     if args.output:\n         args.output.write_text(rendered, encoding=\"utf-8\")\n         print(f\"Output written to {args.output}\", file=sys.stderr)\n+        print(rendered, file=sys.stderr)",
      "comment": "It's also useful when the bot has to truncate the comment",
      "comment_id": 2694197220,
      "user": "MichaReiser",
      "created_at": "2026-01-15T12:29:50Z",
      "url": "https://github.com/astral-sh/ruff/pull/22593#discussion_r2694197220"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22297,
      "file_path": "crates/ty_completion_eval/truth/class-arg-completion/main.py",
      "line": 1,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1 @@\n+class Foo(m<CURSOR: metaclass>)",
      "comment": "We don't really capture the sort order of this very well in existing tests in `completion.rs::tests` so it probably makes sense to capture it here as an eval test",
      "comment_id": 2653049281,
      "user": "RasmusNygren",
      "created_at": "2025-12-30T13:48:35Z",
      "url": "https://github.com/astral-sh/ruff/pull/22297#discussion_r2653049281"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22057,
      "file_path": "crates/ruff_linter/resources/test/fixtures/flake8_blind_except/BLE.py",
      "line": 85,
      "side": "RIGHT",
      "diff_hunk": "@@ -82,17 +82,50 @@\n     logging.error(\"...\", exc_info=None)\n \n \n+try:",
      "comment": "Sorry for another test nit, but could you move the new cases to the bottom of the file? That avoids the churn in the snapshot line numbers. And annotating the logging lines with either `# ok` or `# BLE001` depending on if we expect a diagnostic would also be a big help (although I think these are all okay?)",
      "comment_id": 2656410441,
      "user": "ntBre",
      "created_at": "2026-01-01T14:55:43Z",
      "url": "https://github.com/astral-sh/ruff/pull/22057#discussion_r2656410441"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 1,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,459 @@\n+from __future__ import annotations",
      "comment": "Could you add a module docstring to the top here that describes what this script does and (briefly) how it works?",
      "comment_id": 2680016169,
      "user": "AlexWaygood",
      "created_at": "2026-01-11T18:15:11Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2680016169"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 4,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,459 @@\n+from __future__ import annotations\n+\n+import argparse\n+import itertools as it",
      "comment": "hmm, this isn't an alias I've seen before... I think I'd prefer either `import itertools` or `from itertools import groupby` :-)",
      "comment_id": 2680017219,
      "user": "AlexWaygood",
      "created_at": "2026-01-11T18:15:49Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2680017219"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 17,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,459 @@\n+from __future__ import annotations\n+\n+import argparse\n+import itertools as it\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+CONFORMANCE_ERROR_PATTERN = re.compile(r\"#\\s*E(?:\\s*(?::\\s*(.+)|\\[(.+)\\]))?\\s*\")",
      "comment": "Could you add a comment that documents what this regex intends to capture? You could consider using the `re.VERBOSE` flag: https://docs.python.org/3/library/re.html#re.VERBOSE",
      "comment_id": 2680018594,
      "user": "AlexWaygood",
      "created_at": "2026-01-11T18:17:02Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2680018594"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 33,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,459 @@\n+from __future__ import annotations\n+\n+import argparse\n+import itertools as it\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+CONFORMANCE_ERROR_PATTERN = re.compile(r\"#\\s*E(?:\\s*(?::\\s*(.+)|\\[(.+)\\]))?\\s*\")\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+\n+@dataclass",
      "comment": "```suggestion\r\n@dataclass(kw_only=True, slots=True)\r\n```",
      "comment_id": 2680019807,
      "user": "AlexWaygood",
      "created_at": "2026-01-11T18:17:42Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2680019807"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 39,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,459 @@\n+from __future__ import annotations\n+\n+import argparse\n+import itertools as it\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+CONFORMANCE_ERROR_PATTERN = re.compile(r\"#\\s*E(?:\\s*(?::\\s*(.+)|\\[(.+)\\]))?\\s*\")\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+\n+@dataclass\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass",
      "comment": "```suggestion\r\n@dataclass(kw_only=True, slots=True)\r\n```",
      "comment_id": 2680019957,
      "user": "AlexWaygood",
      "created_at": "2026-01-11T18:17:47Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2680019957"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 45,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,459 @@\n+from __future__ import annotations\n+\n+import argparse\n+import itertools as it\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+CONFORMANCE_ERROR_PATTERN = re.compile(r\"#\\s*E(?:\\s*(?::\\s*(.+)|\\[(.+)\\]))?\\s*\")\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+\n+@dataclass\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass",
      "comment": "```suggestion\r\n@dataclass(kw_only=True, slots=True)\r\n```",
      "comment_id": 2680020186,
      "user": "AlexWaygood",
      "created_at": "2026-01-11T18:17:54Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2680020186"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 51,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,459 @@\n+from __future__ import annotations\n+\n+import argparse\n+import itertools as it\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+CONFORMANCE_ERROR_PATTERN = re.compile(r\"#\\s*E(?:\\s*(?::\\s*(.+)|\\[(.+)\\]))?\\s*\")\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+\n+@dataclass\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass",
      "comment": "```suggestion\r\n@dataclass(kw_only=True, slots=True)\r\n```",
      "comment_id": 2680020324,
      "user": "AlexWaygood",
      "created_at": "2026-01-11T18:18:00Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2680020324"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 107,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,459 @@\n+from __future__ import annotations\n+\n+import argparse\n+import itertools as it\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+CONFORMANCE_ERROR_PATTERN = re.compile(r\"#\\s*E(?:\\s*(?::\\s*(.+)|\\[(.+)\\]))?\\s*\")\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+\n+@dataclass\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self):\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+    def to_concise(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+\n+@dataclass",
      "comment": "```suggestion\r\n@dataclass(kw_only=True, slots=True)\r\n```",
      "comment_id": 2680022851,
      "user": "AlexWaygood",
      "created_at": "2026-01-11T18:19:25Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2680022851"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 149,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,459 @@\n+from __future__ import annotations\n+\n+import argparse\n+import itertools as it\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+CONFORMANCE_ERROR_PATTERN = re.compile(r\"#\\s*E(?:\\s*(?::\\s*(.+)|\\[(.+)\\]))?\\s*\")\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+\n+@dataclass\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self):\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+    def to_concise(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+\n+@dataclass\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new.to_concise()}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old.to_concise()}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected.to_concise()}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass",
      "comment": "```suggestion\r\n@dataclass(kw_only=True, slots=True)\r\n```",
      "comment_id": 2680033062,
      "user": "AlexWaygood",
      "created_at": "2026-01-11T18:25:32Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2680033062"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 153,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,459 @@\n+from __future__ import annotations\n+\n+import argparse\n+import itertools as it\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+CONFORMANCE_ERROR_PATTERN = re.compile(r\"#\\s*E(?:\\s*(?::\\s*(.+)|\\[(.+)\\]))?\\s*\")\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+\n+@dataclass\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self):\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+    def to_concise(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+\n+@dataclass\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new.to_concise()}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old.to_concise()}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected.to_concise()}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass\n+class Statistics:\n+    tp: int = 0\n+    fp: int = 0\n+    fn: int = 0",
      "comment": "Apologies, I don't have a maths background -- what do these stand for? Could we give them more descriptive names for folks who don't know their statistics jargon that well?",
      "comment_id": 2680033928,
      "user": "AlexWaygood",
      "created_at": "2026-01-11T18:26:09Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2680033928"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 271,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,459 @@\n+from __future__ import annotations\n+\n+import argparse\n+import itertools as it\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+CONFORMANCE_ERROR_PATTERN = re.compile(r\"#\\s*E(?:\\s*(?::\\s*(.+)|\\[(.+)\\]))?\\s*\")\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+\n+@dataclass\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self):\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+    def to_concise(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+\n+@dataclass\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new.to_concise()}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old.to_concise()}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected.to_concise()}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass\n+class Statistics:\n+    tp: int = 0\n+    fp: int = 0\n+    fn: int = 0\n+\n+    @property\n+    def precision(self) -> float:\n+        if self.tp + self.fp > 0:\n+            return self.tp / (self.tp + self.fp)\n+        return 0.0\n+\n+    @property\n+    def recall(self) -> float:\n+        if self.tp + self.fn > 0:\n+            return self.tp / (self.tp + self.fn)\n+        else:\n+            return 0.0\n+\n+    @property\n+    def total(self) -> int:\n+        return self.tp + self.fp\n+\n+\n+def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n+    diagnostics: list[Diagnostic] = []\n+    for file in path.resolve().rglob(\"*.py\"):\n+        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+            if error := re.search(CONFORMANCE_ERROR_PATTERN, line):\n+                diagnostics.append(\n+                    Diagnostic(\n+                        check_name=\"conformance\",\n+                        description=error.group(1) or \"Missing\",\n+                        severity=\"major\",\n+                        fingerprint=None,\n+                        location=Location(\n+                            path=file.as_posix(),\n+                            positions=Positions(\n+                                begin=Position(\n+                                    line=idx,\n+                                    column=error.start(),\n+                                ),\n+                                end=Position(\n+                                    line=idx,\n+                                    column=error.end(),\n+                                ),\n+                            ),\n+                        ),\n+                        source=Source.EXPECTED,\n+                    )\n+                )\n+\n+    assert diagnostics, \"Failed to discover any expected diagnostics!\"\n+    return diagnostics\n+\n+\n+def collect_ty_diagnostics(\n+    ty_path: list[str],\n+    source: Source,\n+    target_path: str = \".\",\n+    python_version: str = \"3.12\",\n+) -> list[Diagnostic]:\n+    process = subprocess.run(\n+        [\n+            *ty_path,\n+            \"check\",\n+            f\"--python-version={python_version}\",\n+            \"--output-format=gitlab\",\n+            \"--exit-zero\",\n+            target_path,\n+        ],\n+        capture_output=True,\n+        text=True,\n+        check=True,\n+        timeout=15,\n+    )\n+\n+    if process.returncode != 0:\n+        print(process.stderr)\n+        raise RuntimeError(f\"ty check failed with exit code {process.returncode}\")\n+\n+    return [\n+        Diagnostic.from_gitlab_output(dct, source=source)\n+        for dct in json.loads(process.stdout)\n+    ]\n+\n+\n+def group_diagnostics_by_key(\n+    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+) -> list[GroupedDiagnostics]:\n+    diagnostics = [\n+        *old,\n+        *new,\n+        *expected,\n+    ]\n+    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+\n+    grouped = []\n+    for key, group in it.groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+        group = list(group)\n+        sources: Source = reduce(or_, (diag.source for diag in group))\n+        grouped.append(\n+            GroupedDiagnostics(\n+                key=key,\n+                sources=sources,\n+                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n+                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n+                expected=next(\n+                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n+                ),\n+            )\n+        )\n+\n+    return grouped\n+\n+\n+def compute_stats(\n+    grouped_diagnostics: list[GroupedDiagnostics], source: Source\n+) -> Statistics:\n+    if source == source.EXPECTED:\n+        num_errors = len(\n+            [g for g in grouped_diagnostics if source.EXPECTED in g.sources]  # ty:ignore[unsupported-operator]\n+        )",
      "comment": "```suggestion\r\n        num_errors = sum(\r\n            1 for g in grouped_diagnostics if source.EXPECTED in g.sources  # ty:ignore[unsupported-operator]\r\n        )\r\n```\r\n\r\nalso, could you possibly add a comment that says we're ignoring the error because it's a false positive, and links to the ty issue?",
      "comment_id": 2680037529,
      "user": "AlexWaygood",
      "created_at": "2026-01-11T18:28:11Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2680037529"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 153,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,459 @@\n+from __future__ import annotations\n+\n+import argparse\n+import itertools as it\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+CONFORMANCE_ERROR_PATTERN = re.compile(r\"#\\s*E(?:\\s*(?::\\s*(.+)|\\[(.+)\\]))?\\s*\")\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+\n+@dataclass\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self):\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+    def to_concise(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+\n+@dataclass\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new.to_concise()}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old.to_concise()}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected.to_concise()}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass\n+class Statistics:\n+    tp: int = 0\n+    fp: int = 0\n+    fn: int = 0",
      "comment": "No problem. These just stand for tp -> true positive, fp -> false positive, fn -> false negative. I can update this to spell them out.",
      "comment_id": 2680049571,
      "user": "WillDuke",
      "created_at": "2026-01-11T18:36:08Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2680049571"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 396,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,490 @@\n+\"\"\"\n+Run typing conformance tests and compare results between two ty versions.\n+\n+Examples:\n+    # Compare two specific ty versions\n+    %(prog)s --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Use local ty builds\n+    %(prog)s --old-ty ./target/debug/ty-old --new-ty ./target/debug/ty-new\n+\n+    # Custom test directory\n+    %(prog)s --target-path custom/tests --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Show all diagnostics (not just changed ones)\n+    %(prog)s --all --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from itertools import groupby\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+# The conformance tests include 4 types of errors:\n+# 1. Required errors (E): The type checker must raise an error on this line\n+# 2. Optional errors (E?): The type checker may raise an error on this line\n+# 3. Tagged errors ([tag]): The type checker must raise at most one error on any of the lines in a file with matching tags\n+# 4. Tagged multi-errors ([tag]+): The type checker should raise one or more errors on any of the tagged lines\n+# # This regex pattern parses the error lines in the conformance tests, but the following\n+# # implementation treats all errors as required errors.\n+CONFORMANCE_ERROR_PATTERN = re.compile(\n+    r\"\"\"\n+    \\#\\s*E                  # \"# E\" begins each error\n+    (?P<optional>\\?)?       # Optional '?' (E?) indicates that an error is optional\n+    (?:                     # An optional tag for errors that may appear on multiple lines at most once\n+        \\[\n+            (?P<tag>[^+\\]]+)    # identifier\n+            (?P<multi>\\+)?      # '+' indicates that an error may occur more than once on tagged lines\n+        \\]\n+    )?\n+    (?:\n+        \\s*:\\s*(?P<description>.*) # optional description\n+    )?\n+    \"\"\",\n+    re.VERBOSE,\n+)\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    def __str__(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self) -> str:\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Statistics:\n+    true_positives: int = 0\n+    false_positives: int = 0\n+    false_negatives: int = 0\n+\n+    @property\n+    def precision(self) -> float:\n+        if self.true_positives + self.false_positives > 0:\n+            return self.true_positives / (self.true_positives + self.false_positives)\n+        return 0.0\n+\n+    @property\n+    def recall(self) -> float:\n+        if self.true_positives + self.false_negatives > 0:\n+            return self.true_positives / (self.true_positives + self.false_negatives)\n+        else:\n+            return 0.0\n+\n+    @property\n+    def total(self) -> int:\n+        return self.true_positives + self.false_positives\n+\n+\n+def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n+    diagnostics: list[Diagnostic] = []\n+    for file in path.resolve().rglob(\"*.py\"):\n+        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+            if error := re.search(CONFORMANCE_ERROR_PATTERN, line):\n+                diagnostics.append(\n+                    Diagnostic(\n+                        check_name=\"conformance\",\n+                        description=error.group(\"description\")\n+                        or error.group(\"tag\")\n+                        or \"Missing\",\n+                        severity=\"major\",\n+                        fingerprint=None,\n+                        location=Location(\n+                            path=file.as_posix(),\n+                            positions=Positions(\n+                                begin=Position(\n+                                    line=idx,\n+                                    column=error.start(),\n+                                ),\n+                                end=Position(\n+                                    line=idx,\n+                                    column=error.end(),\n+                                ),\n+                            ),\n+                        ),\n+                        source=Source.EXPECTED,\n+                    )\n+                )\n+\n+    assert diagnostics, \"Failed to discover any expected diagnostics!\"\n+    return diagnostics\n+\n+\n+def collect_ty_diagnostics(\n+    ty_path: list[str],\n+    source: Source,\n+    target_path: str = \".\",\n+    python_version: str = \"3.12\",\n+) -> list[Diagnostic]:\n+    process = subprocess.run(\n+        [\n+            *ty_path,\n+            \"check\",\n+            f\"--python-version={python_version}\",\n+            \"--output-format=gitlab\",\n+            \"--exit-zero\",\n+            target_path,\n+        ],\n+        capture_output=True,\n+        text=True,\n+        check=True,\n+        timeout=15,\n+    )\n+\n+    if process.returncode != 0:\n+        print(process.stderr)\n+        raise RuntimeError(f\"ty check failed with exit code {process.returncode}\")\n+\n+    return [\n+        Diagnostic.from_gitlab_output(dct, source=source)\n+        for dct in json.loads(process.stdout)\n+    ]\n+\n+\n+def group_diagnostics_by_key(\n+    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+) -> list[GroupedDiagnostics]:\n+    diagnostics = [\n+        *old,\n+        *new,\n+        *expected,\n+    ]\n+    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+\n+    grouped = []\n+    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+        group = list(group)\n+        sources: Source = reduce(or_, (diag.source for diag in group))\n+        grouped.append(\n+            GroupedDiagnostics(\n+                key=key,\n+                sources=sources,\n+                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n+                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n+                expected=next(\n+                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n+                ),\n+            )\n+        )\n+\n+    return grouped\n+\n+\n+def compute_stats(\n+    grouped_diagnostics: list[GroupedDiagnostics], source: Source\n+) -> Statistics:\n+    if source == source.EXPECTED:\n+        # ty currently raises a false positive here due to incomplete enum.Flag support\n+        # see https://github.com/astral-sh/ty/issues/876\n+        num_errors = sum(\n+            [1 for g in grouped_diagnostics if source.EXPECTED in g.sources]  # ty:ignore[unsupported-operator]\n+        )\n+        return Statistics(\n+            true_positives=num_errors, false_positives=0, false_negatives=0\n+        )\n+\n+    def increment(statistics: Statistics, grouped: GroupedDiagnostics) -> Statistics:\n+        if (source in grouped.sources) and (Source.EXPECTED in grouped.sources):\n+            statistics.true_positives += 1\n+        elif source in grouped.sources:\n+            statistics.false_positives += 1\n+        else:\n+            statistics.false_negatives += 1\n+        return statistics\n+\n+    return reduce(increment, grouped_diagnostics, Statistics())\n+\n+\n+def render_grouped_diagnostics(\n+    grouped: list[GroupedDiagnostics], changed_only: bool = True\n+) -> str:\n+    if changed_only:\n+        grouped = [diag for diag in grouped if diag.changed]\n+    sorted_by_class = sorted(\n+        grouped,\n+        key=attrgetter(\"classification\"),\n+        reverse=True,\n+    )\n+\n+    lines = []\n+    for classification, group in groupby(\n+        sorted_by_class, key=attrgetter(\"classification\")\n+    ):\n+        group = list(group)\n+\n+        lines.append(f\"## {classification.value.replace('_', ' ').title()}s:\")\n+        lines.append(\"\")\n+        lines.append(\"```diff\")\n+\n+        for diag in group:\n+            lines.append(diag.display())\n+\n+        lines.append(\"```\")\n+\n+    return \"\\n\".join(lines)\n+\n+\n+def render_summary(grouped_diagnostics: list[GroupedDiagnostics]):\n+    def pct(value):\n+        return f\"{value:.2%}\"\n+\n+    def trend(value):\n+        if value == 0:\n+            return \"does not change\"\n+        return \"improves\" if value > 0 else \"regresses\"\n+\n+    old = compute_stats(grouped_diagnostics, source=Source.OLD)\n+    new = compute_stats(grouped_diagnostics, source=Source.NEW)\n+\n+    precision_delta = new.precision - old.precision\n+    recall_delta = new.recall - old.recall\n+\n+    table = dedent(\n+        f\"\"\"\n+        ## Typing Conformance\n+\n+        ### Summary\n+\n+        | Metric     | Old | New | \u0394 |\n+        |------------|-----|-----|---|\n+        | True Positives | {old.true_positives} | {new.true_positives} | {old.true_positives - new.true_positives} |\n+        | False Positives | {old.false_positives} | {new.false_positives} | {new.false_positives - old.false_positives} |\n+        | False Negatives | {old.false_negatives} | {new.false_negatives} | {new.false_negatives - old.false_negatives} |\n+        | Precision  | {old.precision:.2} | {new.precision:.2} | {pct(precision_delta)} |\n+        | Recall     | {old.recall:.2} | {new.recall:.2} | {pct(recall_delta)} |\n+        | Total      | {old.total} | {new.total} | {new.total - old.total} |\n+\n+        \"\"\"\n+    )\n+\n+    summary = (\n+        f\"Compared to the current merge base, this PR {trend(precision_delta)} precision \"\n+        f\"and {trend(recall_delta)} recall (TP: {new.true_positives - old.true_positives}, FP: {new.false_positives - old.false_positives}, FN: {new.false_negatives - old.false_negatives})).\"\n+    )",
      "comment": "What do the terms \"precision\" and \"recall\" mean in this context? Also, I'd again prefer it if we spelled out \"False positives\", \"Ture positives\" and \"False negatives\" here",
      "comment_id": 2682643975,
      "user": "AlexWaygood",
      "created_at": "2026-01-12T14:58:55Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2682643975"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 410,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,490 @@\n+\"\"\"\n+Run typing conformance tests and compare results between two ty versions.\n+\n+Examples:\n+    # Compare two specific ty versions\n+    %(prog)s --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Use local ty builds\n+    %(prog)s --old-ty ./target/debug/ty-old --new-ty ./target/debug/ty-new\n+\n+    # Custom test directory\n+    %(prog)s --target-path custom/tests --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Show all diagnostics (not just changed ones)\n+    %(prog)s --all --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from itertools import groupby\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+# The conformance tests include 4 types of errors:\n+# 1. Required errors (E): The type checker must raise an error on this line\n+# 2. Optional errors (E?): The type checker may raise an error on this line\n+# 3. Tagged errors ([tag]): The type checker must raise at most one error on any of the lines in a file with matching tags\n+# 4. Tagged multi-errors ([tag]+): The type checker should raise one or more errors on any of the tagged lines\n+# # This regex pattern parses the error lines in the conformance tests, but the following\n+# # implementation treats all errors as required errors.\n+CONFORMANCE_ERROR_PATTERN = re.compile(\n+    r\"\"\"\n+    \\#\\s*E                  # \"# E\" begins each error\n+    (?P<optional>\\?)?       # Optional '?' (E?) indicates that an error is optional\n+    (?:                     # An optional tag for errors that may appear on multiple lines at most once\n+        \\[\n+            (?P<tag>[^+\\]]+)    # identifier\n+            (?P<multi>\\+)?      # '+' indicates that an error may occur more than once on tagged lines\n+        \\]\n+    )?\n+    (?:\n+        \\s*:\\s*(?P<description>.*) # optional description\n+    )?\n+    \"\"\",\n+    re.VERBOSE,\n+)\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    def __str__(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self) -> str:\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Statistics:\n+    true_positives: int = 0\n+    false_positives: int = 0\n+    false_negatives: int = 0\n+\n+    @property\n+    def precision(self) -> float:\n+        if self.true_positives + self.false_positives > 0:\n+            return self.true_positives / (self.true_positives + self.false_positives)\n+        return 0.0\n+\n+    @property\n+    def recall(self) -> float:\n+        if self.true_positives + self.false_negatives > 0:\n+            return self.true_positives / (self.true_positives + self.false_negatives)\n+        else:\n+            return 0.0\n+\n+    @property\n+    def total(self) -> int:\n+        return self.true_positives + self.false_positives\n+\n+\n+def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n+    diagnostics: list[Diagnostic] = []\n+    for file in path.resolve().rglob(\"*.py\"):\n+        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+            if error := re.search(CONFORMANCE_ERROR_PATTERN, line):\n+                diagnostics.append(\n+                    Diagnostic(\n+                        check_name=\"conformance\",\n+                        description=error.group(\"description\")\n+                        or error.group(\"tag\")\n+                        or \"Missing\",\n+                        severity=\"major\",\n+                        fingerprint=None,\n+                        location=Location(\n+                            path=file.as_posix(),\n+                            positions=Positions(\n+                                begin=Position(\n+                                    line=idx,\n+                                    column=error.start(),\n+                                ),\n+                                end=Position(\n+                                    line=idx,\n+                                    column=error.end(),\n+                                ),\n+                            ),\n+                        ),\n+                        source=Source.EXPECTED,\n+                    )\n+                )\n+\n+    assert diagnostics, \"Failed to discover any expected diagnostics!\"\n+    return diagnostics\n+\n+\n+def collect_ty_diagnostics(\n+    ty_path: list[str],\n+    source: Source,\n+    target_path: str = \".\",\n+    python_version: str = \"3.12\",\n+) -> list[Diagnostic]:\n+    process = subprocess.run(\n+        [\n+            *ty_path,\n+            \"check\",\n+            f\"--python-version={python_version}\",\n+            \"--output-format=gitlab\",\n+            \"--exit-zero\",\n+            target_path,\n+        ],\n+        capture_output=True,\n+        text=True,\n+        check=True,\n+        timeout=15,\n+    )\n+\n+    if process.returncode != 0:\n+        print(process.stderr)\n+        raise RuntimeError(f\"ty check failed with exit code {process.returncode}\")\n+\n+    return [\n+        Diagnostic.from_gitlab_output(dct, source=source)\n+        for dct in json.loads(process.stdout)\n+    ]\n+\n+\n+def group_diagnostics_by_key(\n+    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+) -> list[GroupedDiagnostics]:\n+    diagnostics = [\n+        *old,\n+        *new,\n+        *expected,\n+    ]\n+    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+\n+    grouped = []\n+    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+        group = list(group)\n+        sources: Source = reduce(or_, (diag.source for diag in group))\n+        grouped.append(\n+            GroupedDiagnostics(\n+                key=key,\n+                sources=sources,\n+                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n+                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n+                expected=next(\n+                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n+                ),\n+            )\n+        )\n+\n+    return grouped\n+\n+\n+def compute_stats(\n+    grouped_diagnostics: list[GroupedDiagnostics], source: Source\n+) -> Statistics:\n+    if source == source.EXPECTED:\n+        # ty currently raises a false positive here due to incomplete enum.Flag support\n+        # see https://github.com/astral-sh/ty/issues/876\n+        num_errors = sum(\n+            [1 for g in grouped_diagnostics if source.EXPECTED in g.sources]  # ty:ignore[unsupported-operator]\n+        )\n+        return Statistics(\n+            true_positives=num_errors, false_positives=0, false_negatives=0\n+        )\n+\n+    def increment(statistics: Statistics, grouped: GroupedDiagnostics) -> Statistics:\n+        if (source in grouped.sources) and (Source.EXPECTED in grouped.sources):\n+            statistics.true_positives += 1\n+        elif source in grouped.sources:\n+            statistics.false_positives += 1\n+        else:\n+            statistics.false_negatives += 1\n+        return statistics\n+\n+    return reduce(increment, grouped_diagnostics, Statistics())\n+\n+\n+def render_grouped_diagnostics(\n+    grouped: list[GroupedDiagnostics], changed_only: bool = True\n+) -> str:\n+    if changed_only:\n+        grouped = [diag for diag in grouped if diag.changed]\n+    sorted_by_class = sorted(\n+        grouped,\n+        key=attrgetter(\"classification\"),\n+        reverse=True,\n+    )\n+\n+    lines = []\n+    for classification, group in groupby(\n+        sorted_by_class, key=attrgetter(\"classification\")\n+    ):\n+        group = list(group)\n+\n+        lines.append(f\"## {classification.value.replace('_', ' ').title()}s:\")\n+        lines.append(\"\")\n+        lines.append(\"```diff\")\n+\n+        for diag in group:\n+            lines.append(diag.display())\n+\n+        lines.append(\"```\")\n+\n+    return \"\\n\".join(lines)\n+\n+\n+def render_summary(grouped_diagnostics: list[GroupedDiagnostics]):\n+    def pct(value):\n+        return f\"{value:.2%}\"\n+\n+    def trend(value):\n+        if value == 0:\n+            return \"does not change\"\n+        return \"improves\" if value > 0 else \"regresses\"\n+\n+    old = compute_stats(grouped_diagnostics, source=Source.OLD)\n+    new = compute_stats(grouped_diagnostics, source=Source.NEW)\n+\n+    precision_delta = new.precision - old.precision\n+    recall_delta = new.recall - old.recall\n+\n+    table = dedent(\n+        f\"\"\"\n+        ## Typing Conformance\n+\n+        ### Summary\n+\n+        | Metric     | Old | New | \u0394 |\n+        |------------|-----|-----|---|\n+        | True Positives | {old.true_positives} | {new.true_positives} | {old.true_positives - new.true_positives} |\n+        | False Positives | {old.false_positives} | {new.false_positives} | {new.false_positives - old.false_positives} |\n+        | False Negatives | {old.false_negatives} | {new.false_negatives} | {new.false_negatives - old.false_negatives} |\n+        | Precision  | {old.precision:.2} | {new.precision:.2} | {pct(precision_delta)} |\n+        | Recall     | {old.recall:.2} | {new.recall:.2} | {pct(recall_delta)} |\n+        | Total      | {old.total} | {new.total} | {new.total - old.total} |\n+\n+        \"\"\"\n+    )\n+\n+    summary = (\n+        f\"Compared to the current merge base, this PR {trend(precision_delta)} precision \"\n+        f\"and {trend(recall_delta)} recall (TP: {new.true_positives - old.true_positives}, FP: {new.false_positives - old.false_positives}, FN: {new.false_negatives - old.false_negatives})).\"\n+    )\n+\n+    return \"\\n\".join([table, summary])\n+\n+\n+def parse_args():\n+    parser = argparse.ArgumentParser(\n+        description=__doc__,\n+        formatter_class=argparse.RawDescriptionHelpFormatter,\n+    )\n+\n+    parser.add_argument(\n+        \"--old-ty\",\n+        nargs=\"+\",\n+        default=[\"uvx\", \"ty@0.0.1a35\"],",
      "comment": "We should state clearly in the module docstring that the script assumes you have uv installed (nearly all contributors to Ruff/ty will have it installed, of course, but it's still worth stating explicitly)",
      "comment_id": 2682649647,
      "user": "AlexWaygood",
      "created_at": "2026-01-12T15:00:17Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2682649647"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 422,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,490 @@\n+\"\"\"\n+Run typing conformance tests and compare results between two ty versions.\n+\n+Examples:\n+    # Compare two specific ty versions\n+    %(prog)s --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Use local ty builds\n+    %(prog)s --old-ty ./target/debug/ty-old --new-ty ./target/debug/ty-new\n+\n+    # Custom test directory\n+    %(prog)s --target-path custom/tests --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Show all diagnostics (not just changed ones)\n+    %(prog)s --all --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from itertools import groupby\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+# The conformance tests include 4 types of errors:\n+# 1. Required errors (E): The type checker must raise an error on this line\n+# 2. Optional errors (E?): The type checker may raise an error on this line\n+# 3. Tagged errors ([tag]): The type checker must raise at most one error on any of the lines in a file with matching tags\n+# 4. Tagged multi-errors ([tag]+): The type checker should raise one or more errors on any of the tagged lines\n+# # This regex pattern parses the error lines in the conformance tests, but the following\n+# # implementation treats all errors as required errors.\n+CONFORMANCE_ERROR_PATTERN = re.compile(\n+    r\"\"\"\n+    \\#\\s*E                  # \"# E\" begins each error\n+    (?P<optional>\\?)?       # Optional '?' (E?) indicates that an error is optional\n+    (?:                     # An optional tag for errors that may appear on multiple lines at most once\n+        \\[\n+            (?P<tag>[^+\\]]+)    # identifier\n+            (?P<multi>\\+)?      # '+' indicates that an error may occur more than once on tagged lines\n+        \\]\n+    )?\n+    (?:\n+        \\s*:\\s*(?P<description>.*) # optional description\n+    )?\n+    \"\"\",\n+    re.VERBOSE,\n+)\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    def __str__(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self) -> str:\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Statistics:\n+    true_positives: int = 0\n+    false_positives: int = 0\n+    false_negatives: int = 0\n+\n+    @property\n+    def precision(self) -> float:\n+        if self.true_positives + self.false_positives > 0:\n+            return self.true_positives / (self.true_positives + self.false_positives)\n+        return 0.0\n+\n+    @property\n+    def recall(self) -> float:\n+        if self.true_positives + self.false_negatives > 0:\n+            return self.true_positives / (self.true_positives + self.false_negatives)\n+        else:\n+            return 0.0\n+\n+    @property\n+    def total(self) -> int:\n+        return self.true_positives + self.false_positives\n+\n+\n+def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n+    diagnostics: list[Diagnostic] = []\n+    for file in path.resolve().rglob(\"*.py\"):\n+        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+            if error := re.search(CONFORMANCE_ERROR_PATTERN, line):\n+                diagnostics.append(\n+                    Diagnostic(\n+                        check_name=\"conformance\",\n+                        description=error.group(\"description\")\n+                        or error.group(\"tag\")\n+                        or \"Missing\",\n+                        severity=\"major\",\n+                        fingerprint=None,\n+                        location=Location(\n+                            path=file.as_posix(),\n+                            positions=Positions(\n+                                begin=Position(\n+                                    line=idx,\n+                                    column=error.start(),\n+                                ),\n+                                end=Position(\n+                                    line=idx,\n+                                    column=error.end(),\n+                                ),\n+                            ),\n+                        ),\n+                        source=Source.EXPECTED,\n+                    )\n+                )\n+\n+    assert diagnostics, \"Failed to discover any expected diagnostics!\"\n+    return diagnostics\n+\n+\n+def collect_ty_diagnostics(\n+    ty_path: list[str],\n+    source: Source,\n+    target_path: str = \".\",\n+    python_version: str = \"3.12\",\n+) -> list[Diagnostic]:\n+    process = subprocess.run(\n+        [\n+            *ty_path,\n+            \"check\",\n+            f\"--python-version={python_version}\",\n+            \"--output-format=gitlab\",\n+            \"--exit-zero\",\n+            target_path,\n+        ],\n+        capture_output=True,\n+        text=True,\n+        check=True,\n+        timeout=15,\n+    )\n+\n+    if process.returncode != 0:\n+        print(process.stderr)\n+        raise RuntimeError(f\"ty check failed with exit code {process.returncode}\")\n+\n+    return [\n+        Diagnostic.from_gitlab_output(dct, source=source)\n+        for dct in json.loads(process.stdout)\n+    ]\n+\n+\n+def group_diagnostics_by_key(\n+    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+) -> list[GroupedDiagnostics]:\n+    diagnostics = [\n+        *old,\n+        *new,\n+        *expected,\n+    ]\n+    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+\n+    grouped = []\n+    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+        group = list(group)\n+        sources: Source = reduce(or_, (diag.source for diag in group))\n+        grouped.append(\n+            GroupedDiagnostics(\n+                key=key,\n+                sources=sources,\n+                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n+                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n+                expected=next(\n+                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n+                ),\n+            )\n+        )\n+\n+    return grouped\n+\n+\n+def compute_stats(\n+    grouped_diagnostics: list[GroupedDiagnostics], source: Source\n+) -> Statistics:\n+    if source == source.EXPECTED:\n+        # ty currently raises a false positive here due to incomplete enum.Flag support\n+        # see https://github.com/astral-sh/ty/issues/876\n+        num_errors = sum(\n+            [1 for g in grouped_diagnostics if source.EXPECTED in g.sources]  # ty:ignore[unsupported-operator]\n+        )\n+        return Statistics(\n+            true_positives=num_errors, false_positives=0, false_negatives=0\n+        )\n+\n+    def increment(statistics: Statistics, grouped: GroupedDiagnostics) -> Statistics:\n+        if (source in grouped.sources) and (Source.EXPECTED in grouped.sources):\n+            statistics.true_positives += 1\n+        elif source in grouped.sources:\n+            statistics.false_positives += 1\n+        else:\n+            statistics.false_negatives += 1\n+        return statistics\n+\n+    return reduce(increment, grouped_diagnostics, Statistics())\n+\n+\n+def render_grouped_diagnostics(\n+    grouped: list[GroupedDiagnostics], changed_only: bool = True\n+) -> str:\n+    if changed_only:\n+        grouped = [diag for diag in grouped if diag.changed]\n+    sorted_by_class = sorted(\n+        grouped,\n+        key=attrgetter(\"classification\"),\n+        reverse=True,\n+    )\n+\n+    lines = []\n+    for classification, group in groupby(\n+        sorted_by_class, key=attrgetter(\"classification\")\n+    ):\n+        group = list(group)\n+\n+        lines.append(f\"## {classification.value.replace('_', ' ').title()}s:\")\n+        lines.append(\"\")\n+        lines.append(\"```diff\")\n+\n+        for diag in group:\n+            lines.append(diag.display())\n+\n+        lines.append(\"```\")\n+\n+    return \"\\n\".join(lines)\n+\n+\n+def render_summary(grouped_diagnostics: list[GroupedDiagnostics]):\n+    def pct(value):\n+        return f\"{value:.2%}\"\n+\n+    def trend(value):\n+        if value == 0:\n+            return \"does not change\"\n+        return \"improves\" if value > 0 else \"regresses\"\n+\n+    old = compute_stats(grouped_diagnostics, source=Source.OLD)\n+    new = compute_stats(grouped_diagnostics, source=Source.NEW)\n+\n+    precision_delta = new.precision - old.precision\n+    recall_delta = new.recall - old.recall\n+\n+    table = dedent(\n+        f\"\"\"\n+        ## Typing Conformance\n+\n+        ### Summary\n+\n+        | Metric     | Old | New | \u0394 |\n+        |------------|-----|-----|---|\n+        | True Positives | {old.true_positives} | {new.true_positives} | {old.true_positives - new.true_positives} |\n+        | False Positives | {old.false_positives} | {new.false_positives} | {new.false_positives - old.false_positives} |\n+        | False Negatives | {old.false_negatives} | {new.false_negatives} | {new.false_negatives - old.false_negatives} |\n+        | Precision  | {old.precision:.2} | {new.precision:.2} | {pct(precision_delta)} |\n+        | Recall     | {old.recall:.2} | {new.recall:.2} | {pct(recall_delta)} |\n+        | Total      | {old.total} | {new.total} | {new.total - old.total} |\n+\n+        \"\"\"\n+    )\n+\n+    summary = (\n+        f\"Compared to the current merge base, this PR {trend(precision_delta)} precision \"\n+        f\"and {trend(recall_delta)} recall (TP: {new.true_positives - old.true_positives}, FP: {new.false_positives - old.false_positives}, FN: {new.false_negatives - old.false_negatives})).\"\n+    )\n+\n+    return \"\\n\".join([table, summary])\n+\n+\n+def parse_args():\n+    parser = argparse.ArgumentParser(\n+        description=__doc__,\n+        formatter_class=argparse.RawDescriptionHelpFormatter,\n+    )\n+\n+    parser.add_argument(\n+        \"--old-ty\",\n+        nargs=\"+\",\n+        default=[\"uvx\", \"ty@0.0.1a35\"],\n+        help=\"Command to run old version of ty (default: uvx ty@0.0.1a35)\",\n+    )\n+\n+    parser.add_argument(\n+        \"--new-ty\",\n+        nargs=\"+\",\n+        default=[\"uvx\", \"ty@0.0.7\"],\n+        help=\"Command to run new version of ty (default: uvx ty@0.0.7)\",\n+    )\n+\n+    parser.add_argument(\n+        \"--target-path\",",
      "comment": "Maybe `--conformance-tests-path`? We could also make it so that you can specify the argument with `-c` if that's getting on the wordy side",
      "comment_id": 2682652929,
      "user": "AlexWaygood",
      "created_at": "2026-01-12T15:01:08Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2682652929"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 381,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,490 @@\n+\"\"\"\n+Run typing conformance tests and compare results between two ty versions.\n+\n+Examples:\n+    # Compare two specific ty versions\n+    %(prog)s --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Use local ty builds\n+    %(prog)s --old-ty ./target/debug/ty-old --new-ty ./target/debug/ty-new\n+\n+    # Custom test directory\n+    %(prog)s --target-path custom/tests --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Show all diagnostics (not just changed ones)\n+    %(prog)s --all --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from itertools import groupby\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+# The conformance tests include 4 types of errors:\n+# 1. Required errors (E): The type checker must raise an error on this line\n+# 2. Optional errors (E?): The type checker may raise an error on this line\n+# 3. Tagged errors ([tag]): The type checker must raise at most one error on any of the lines in a file with matching tags\n+# 4. Tagged multi-errors ([tag]+): The type checker should raise one or more errors on any of the tagged lines\n+# # This regex pattern parses the error lines in the conformance tests, but the following\n+# # implementation treats all errors as required errors.\n+CONFORMANCE_ERROR_PATTERN = re.compile(\n+    r\"\"\"\n+    \\#\\s*E                  # \"# E\" begins each error\n+    (?P<optional>\\?)?       # Optional '?' (E?) indicates that an error is optional\n+    (?:                     # An optional tag for errors that may appear on multiple lines at most once\n+        \\[\n+            (?P<tag>[^+\\]]+)    # identifier\n+            (?P<multi>\\+)?      # '+' indicates that an error may occur more than once on tagged lines\n+        \\]\n+    )?\n+    (?:\n+        \\s*:\\s*(?P<description>.*) # optional description\n+    )?\n+    \"\"\",\n+    re.VERBOSE,\n+)\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    def __str__(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self) -> str:\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Statistics:\n+    true_positives: int = 0\n+    false_positives: int = 0\n+    false_negatives: int = 0\n+\n+    @property\n+    def precision(self) -> float:\n+        if self.true_positives + self.false_positives > 0:\n+            return self.true_positives / (self.true_positives + self.false_positives)\n+        return 0.0\n+\n+    @property\n+    def recall(self) -> float:\n+        if self.true_positives + self.false_negatives > 0:\n+            return self.true_positives / (self.true_positives + self.false_negatives)\n+        else:\n+            return 0.0\n+\n+    @property\n+    def total(self) -> int:\n+        return self.true_positives + self.false_positives\n+\n+\n+def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n+    diagnostics: list[Diagnostic] = []\n+    for file in path.resolve().rglob(\"*.py\"):\n+        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+            if error := re.search(CONFORMANCE_ERROR_PATTERN, line):\n+                diagnostics.append(\n+                    Diagnostic(\n+                        check_name=\"conformance\",\n+                        description=error.group(\"description\")\n+                        or error.group(\"tag\")\n+                        or \"Missing\",\n+                        severity=\"major\",\n+                        fingerprint=None,\n+                        location=Location(\n+                            path=file.as_posix(),\n+                            positions=Positions(\n+                                begin=Position(\n+                                    line=idx,\n+                                    column=error.start(),\n+                                ),\n+                                end=Position(\n+                                    line=idx,\n+                                    column=error.end(),\n+                                ),\n+                            ),\n+                        ),\n+                        source=Source.EXPECTED,\n+                    )\n+                )\n+\n+    assert diagnostics, \"Failed to discover any expected diagnostics!\"\n+    return diagnostics\n+\n+\n+def collect_ty_diagnostics(\n+    ty_path: list[str],\n+    source: Source,\n+    target_path: str = \".\",\n+    python_version: str = \"3.12\",\n+) -> list[Diagnostic]:\n+    process = subprocess.run(\n+        [\n+            *ty_path,\n+            \"check\",\n+            f\"--python-version={python_version}\",\n+            \"--output-format=gitlab\",\n+            \"--exit-zero\",\n+            target_path,\n+        ],\n+        capture_output=True,\n+        text=True,\n+        check=True,\n+        timeout=15,\n+    )\n+\n+    if process.returncode != 0:\n+        print(process.stderr)\n+        raise RuntimeError(f\"ty check failed with exit code {process.returncode}\")\n+\n+    return [\n+        Diagnostic.from_gitlab_output(dct, source=source)\n+        for dct in json.loads(process.stdout)\n+    ]\n+\n+\n+def group_diagnostics_by_key(\n+    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+) -> list[GroupedDiagnostics]:\n+    diagnostics = [\n+        *old,\n+        *new,\n+        *expected,\n+    ]\n+    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+\n+    grouped = []\n+    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+        group = list(group)\n+        sources: Source = reduce(or_, (diag.source for diag in group))\n+        grouped.append(\n+            GroupedDiagnostics(\n+                key=key,\n+                sources=sources,\n+                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n+                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n+                expected=next(\n+                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n+                ),\n+            )\n+        )\n+\n+    return grouped\n+\n+\n+def compute_stats(\n+    grouped_diagnostics: list[GroupedDiagnostics], source: Source\n+) -> Statistics:\n+    if source == source.EXPECTED:\n+        # ty currently raises a false positive here due to incomplete enum.Flag support\n+        # see https://github.com/astral-sh/ty/issues/876\n+        num_errors = sum(\n+            [1 for g in grouped_diagnostics if source.EXPECTED in g.sources]  # ty:ignore[unsupported-operator]\n+        )\n+        return Statistics(\n+            true_positives=num_errors, false_positives=0, false_negatives=0\n+        )\n+\n+    def increment(statistics: Statistics, grouped: GroupedDiagnostics) -> Statistics:\n+        if (source in grouped.sources) and (Source.EXPECTED in grouped.sources):\n+            statistics.true_positives += 1\n+        elif source in grouped.sources:\n+            statistics.false_positives += 1\n+        else:\n+            statistics.false_negatives += 1\n+        return statistics\n+\n+    return reduce(increment, grouped_diagnostics, Statistics())\n+\n+\n+def render_grouped_diagnostics(\n+    grouped: list[GroupedDiagnostics], changed_only: bool = True\n+) -> str:\n+    if changed_only:\n+        grouped = [diag for diag in grouped if diag.changed]\n+    sorted_by_class = sorted(\n+        grouped,\n+        key=attrgetter(\"classification\"),\n+        reverse=True,\n+    )\n+\n+    lines = []\n+    for classification, group in groupby(\n+        sorted_by_class, key=attrgetter(\"classification\")\n+    ):\n+        group = list(group)\n+\n+        lines.append(f\"## {classification.value.replace('_', ' ').title()}s:\")\n+        lines.append(\"\")\n+        lines.append(\"```diff\")\n+\n+        for diag in group:\n+            lines.append(diag.display())\n+\n+        lines.append(\"```\")\n+\n+    return \"\\n\".join(lines)\n+\n+\n+def render_summary(grouped_diagnostics: list[GroupedDiagnostics]):\n+    def pct(value):\n+        return f\"{value:.2%}\"\n+\n+    def trend(value):\n+        if value == 0:\n+            return \"does not change\"\n+        return \"improves\" if value > 0 else \"regresses\"\n+\n+    old = compute_stats(grouped_diagnostics, source=Source.OLD)\n+    new = compute_stats(grouped_diagnostics, source=Source.NEW)\n+\n+    precision_delta = new.precision - old.precision\n+    recall_delta = new.recall - old.recall\n+\n+    table = dedent(\n+        f\"\"\"\n+        ## Typing Conformance\n+\n+        ### Summary\n+\n+        | Metric     | Old | New | \u0394 |",
      "comment": "```suggestion\r\n        | Metric     | Old | New | Delta |\r\n```",
      "comment_id": 2682659512,
      "user": "AlexWaygood",
      "created_at": "2026-01-12T15:02:47Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2682659512"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 355,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,490 @@\n+\"\"\"\n+Run typing conformance tests and compare results between two ty versions.\n+\n+Examples:\n+    # Compare two specific ty versions\n+    %(prog)s --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Use local ty builds\n+    %(prog)s --old-ty ./target/debug/ty-old --new-ty ./target/debug/ty-new\n+\n+    # Custom test directory\n+    %(prog)s --target-path custom/tests --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Show all diagnostics (not just changed ones)\n+    %(prog)s --all --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from itertools import groupby\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+# The conformance tests include 4 types of errors:\n+# 1. Required errors (E): The type checker must raise an error on this line\n+# 2. Optional errors (E?): The type checker may raise an error on this line\n+# 3. Tagged errors ([tag]): The type checker must raise at most one error on any of the lines in a file with matching tags\n+# 4. Tagged multi-errors ([tag]+): The type checker should raise one or more errors on any of the tagged lines\n+# # This regex pattern parses the error lines in the conformance tests, but the following\n+# # implementation treats all errors as required errors.\n+CONFORMANCE_ERROR_PATTERN = re.compile(\n+    r\"\"\"\n+    \\#\\s*E                  # \"# E\" begins each error\n+    (?P<optional>\\?)?       # Optional '?' (E?) indicates that an error is optional\n+    (?:                     # An optional tag for errors that may appear on multiple lines at most once\n+        \\[\n+            (?P<tag>[^+\\]]+)    # identifier\n+            (?P<multi>\\+)?      # '+' indicates that an error may occur more than once on tagged lines\n+        \\]\n+    )?\n+    (?:\n+        \\s*:\\s*(?P<description>.*) # optional description\n+    )?\n+    \"\"\",\n+    re.VERBOSE,\n+)\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    def __str__(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self) -> str:\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Statistics:\n+    true_positives: int = 0\n+    false_positives: int = 0\n+    false_negatives: int = 0\n+\n+    @property\n+    def precision(self) -> float:\n+        if self.true_positives + self.false_positives > 0:\n+            return self.true_positives / (self.true_positives + self.false_positives)\n+        return 0.0\n+\n+    @property\n+    def recall(self) -> float:\n+        if self.true_positives + self.false_negatives > 0:\n+            return self.true_positives / (self.true_positives + self.false_negatives)\n+        else:\n+            return 0.0\n+\n+    @property\n+    def total(self) -> int:\n+        return self.true_positives + self.false_positives\n+\n+\n+def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n+    diagnostics: list[Diagnostic] = []\n+    for file in path.resolve().rglob(\"*.py\"):\n+        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+            if error := re.search(CONFORMANCE_ERROR_PATTERN, line):\n+                diagnostics.append(\n+                    Diagnostic(\n+                        check_name=\"conformance\",\n+                        description=error.group(\"description\")\n+                        or error.group(\"tag\")\n+                        or \"Missing\",\n+                        severity=\"major\",\n+                        fingerprint=None,\n+                        location=Location(\n+                            path=file.as_posix(),\n+                            positions=Positions(\n+                                begin=Position(\n+                                    line=idx,\n+                                    column=error.start(),\n+                                ),\n+                                end=Position(\n+                                    line=idx,\n+                                    column=error.end(),\n+                                ),\n+                            ),\n+                        ),\n+                        source=Source.EXPECTED,\n+                    )\n+                )\n+\n+    assert diagnostics, \"Failed to discover any expected diagnostics!\"\n+    return diagnostics\n+\n+\n+def collect_ty_diagnostics(\n+    ty_path: list[str],\n+    source: Source,\n+    target_path: str = \".\",\n+    python_version: str = \"3.12\",\n+) -> list[Diagnostic]:\n+    process = subprocess.run(\n+        [\n+            *ty_path,\n+            \"check\",\n+            f\"--python-version={python_version}\",\n+            \"--output-format=gitlab\",\n+            \"--exit-zero\",\n+            target_path,\n+        ],\n+        capture_output=True,\n+        text=True,\n+        check=True,\n+        timeout=15,\n+    )\n+\n+    if process.returncode != 0:\n+        print(process.stderr)\n+        raise RuntimeError(f\"ty check failed with exit code {process.returncode}\")\n+\n+    return [\n+        Diagnostic.from_gitlab_output(dct, source=source)\n+        for dct in json.loads(process.stdout)\n+    ]\n+\n+\n+def group_diagnostics_by_key(\n+    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+) -> list[GroupedDiagnostics]:\n+    diagnostics = [\n+        *old,\n+        *new,\n+        *expected,\n+    ]\n+    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+\n+    grouped = []\n+    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+        group = list(group)\n+        sources: Source = reduce(or_, (diag.source for diag in group))\n+        grouped.append(\n+            GroupedDiagnostics(\n+                key=key,\n+                sources=sources,\n+                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n+                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n+                expected=next(\n+                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n+                ),\n+            )\n+        )\n+\n+    return grouped\n+\n+\n+def compute_stats(\n+    grouped_diagnostics: list[GroupedDiagnostics], source: Source\n+) -> Statistics:\n+    if source == source.EXPECTED:\n+        # ty currently raises a false positive here due to incomplete enum.Flag support\n+        # see https://github.com/astral-sh/ty/issues/876\n+        num_errors = sum(\n+            [1 for g in grouped_diagnostics if source.EXPECTED in g.sources]  # ty:ignore[unsupported-operator]\n+        )\n+        return Statistics(\n+            true_positives=num_errors, false_positives=0, false_negatives=0\n+        )\n+\n+    def increment(statistics: Statistics, grouped: GroupedDiagnostics) -> Statistics:\n+        if (source in grouped.sources) and (Source.EXPECTED in grouped.sources):\n+            statistics.true_positives += 1\n+        elif source in grouped.sources:\n+            statistics.false_positives += 1\n+        else:\n+            statistics.false_negatives += 1\n+        return statistics\n+\n+    return reduce(increment, grouped_diagnostics, Statistics())\n+\n+\n+def render_grouped_diagnostics(\n+    grouped: list[GroupedDiagnostics], changed_only: bool = True\n+) -> str:\n+    if changed_only:\n+        grouped = [diag for diag in grouped if diag.changed]\n+    sorted_by_class = sorted(\n+        grouped,\n+        key=attrgetter(\"classification\"),\n+        reverse=True,\n+    )\n+\n+    lines = []\n+    for classification, group in groupby(\n+        sorted_by_class, key=attrgetter(\"classification\")\n+    ):\n+        group = list(group)\n+\n+        lines.append(f\"## {classification.value.replace('_', ' ').title()}s:\")\n+        lines.append(\"\")\n+        lines.append(\"```diff\")\n+\n+        for diag in group:\n+            lines.append(diag.display())\n+\n+        lines.append(\"```\")",
      "comment": "I ran this script locally to compare the results on https://github.com/astral-sh/ruff/pull/22317 with the results on `main`, and found the \"True negatives\" heading a bit confusing here:\r\n\r\n<details>\r\n<summary>Screenshot</summary>\r\n\r\n<img width=\"2014\" height=\"1446\" alt=\"image\" src=\"https://github.com/user-attachments/assets/08cec938-6dd8-4a48-931a-8abf0aa7f7a9\" />\r\n\r\n</details>\r\n\r\nI think a \"new true negative\" is the same as a \"removed false positive\"? What if we had three sections with these titles (I'm not a huge fan of emojis in general but here I think they'd help make it clear whether each section indicates good news or bad news):\r\n- \"True positives added \ud83c\udf89\" \r\n- \"False positives removed \ud83c\udf89\"\r\n- \"False positives added \ud83e\udee4\"",
      "comment_id": 2682732308,
      "user": "AlexWaygood",
      "created_at": "2026-01-12T15:19:38Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2682732308"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 396,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,490 @@\n+\"\"\"\n+Run typing conformance tests and compare results between two ty versions.\n+\n+Examples:\n+    # Compare two specific ty versions\n+    %(prog)s --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Use local ty builds\n+    %(prog)s --old-ty ./target/debug/ty-old --new-ty ./target/debug/ty-new\n+\n+    # Custom test directory\n+    %(prog)s --target-path custom/tests --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Show all diagnostics (not just changed ones)\n+    %(prog)s --all --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from itertools import groupby\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+# The conformance tests include 4 types of errors:\n+# 1. Required errors (E): The type checker must raise an error on this line\n+# 2. Optional errors (E?): The type checker may raise an error on this line\n+# 3. Tagged errors ([tag]): The type checker must raise at most one error on any of the lines in a file with matching tags\n+# 4. Tagged multi-errors ([tag]+): The type checker should raise one or more errors on any of the tagged lines\n+# # This regex pattern parses the error lines in the conformance tests, but the following\n+# # implementation treats all errors as required errors.\n+CONFORMANCE_ERROR_PATTERN = re.compile(\n+    r\"\"\"\n+    \\#\\s*E                  # \"# E\" begins each error\n+    (?P<optional>\\?)?       # Optional '?' (E?) indicates that an error is optional\n+    (?:                     # An optional tag for errors that may appear on multiple lines at most once\n+        \\[\n+            (?P<tag>[^+\\]]+)    # identifier\n+            (?P<multi>\\+)?      # '+' indicates that an error may occur more than once on tagged lines\n+        \\]\n+    )?\n+    (?:\n+        \\s*:\\s*(?P<description>.*) # optional description\n+    )?\n+    \"\"\",\n+    re.VERBOSE,\n+)\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    def __str__(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self) -> str:\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Statistics:\n+    true_positives: int = 0\n+    false_positives: int = 0\n+    false_negatives: int = 0\n+\n+    @property\n+    def precision(self) -> float:\n+        if self.true_positives + self.false_positives > 0:\n+            return self.true_positives / (self.true_positives + self.false_positives)\n+        return 0.0\n+\n+    @property\n+    def recall(self) -> float:\n+        if self.true_positives + self.false_negatives > 0:\n+            return self.true_positives / (self.true_positives + self.false_negatives)\n+        else:\n+            return 0.0\n+\n+    @property\n+    def total(self) -> int:\n+        return self.true_positives + self.false_positives\n+\n+\n+def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n+    diagnostics: list[Diagnostic] = []\n+    for file in path.resolve().rglob(\"*.py\"):\n+        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+            if error := re.search(CONFORMANCE_ERROR_PATTERN, line):\n+                diagnostics.append(\n+                    Diagnostic(\n+                        check_name=\"conformance\",\n+                        description=error.group(\"description\")\n+                        or error.group(\"tag\")\n+                        or \"Missing\",\n+                        severity=\"major\",\n+                        fingerprint=None,\n+                        location=Location(\n+                            path=file.as_posix(),\n+                            positions=Positions(\n+                                begin=Position(\n+                                    line=idx,\n+                                    column=error.start(),\n+                                ),\n+                                end=Position(\n+                                    line=idx,\n+                                    column=error.end(),\n+                                ),\n+                            ),\n+                        ),\n+                        source=Source.EXPECTED,\n+                    )\n+                )\n+\n+    assert diagnostics, \"Failed to discover any expected diagnostics!\"\n+    return diagnostics\n+\n+\n+def collect_ty_diagnostics(\n+    ty_path: list[str],\n+    source: Source,\n+    target_path: str = \".\",\n+    python_version: str = \"3.12\",\n+) -> list[Diagnostic]:\n+    process = subprocess.run(\n+        [\n+            *ty_path,\n+            \"check\",\n+            f\"--python-version={python_version}\",\n+            \"--output-format=gitlab\",\n+            \"--exit-zero\",\n+            target_path,\n+        ],\n+        capture_output=True,\n+        text=True,\n+        check=True,\n+        timeout=15,\n+    )\n+\n+    if process.returncode != 0:\n+        print(process.stderr)\n+        raise RuntimeError(f\"ty check failed with exit code {process.returncode}\")\n+\n+    return [\n+        Diagnostic.from_gitlab_output(dct, source=source)\n+        for dct in json.loads(process.stdout)\n+    ]\n+\n+\n+def group_diagnostics_by_key(\n+    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+) -> list[GroupedDiagnostics]:\n+    diagnostics = [\n+        *old,\n+        *new,\n+        *expected,\n+    ]\n+    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+\n+    grouped = []\n+    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+        group = list(group)\n+        sources: Source = reduce(or_, (diag.source for diag in group))\n+        grouped.append(\n+            GroupedDiagnostics(\n+                key=key,\n+                sources=sources,\n+                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n+                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n+                expected=next(\n+                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n+                ),\n+            )\n+        )\n+\n+    return grouped\n+\n+\n+def compute_stats(\n+    grouped_diagnostics: list[GroupedDiagnostics], source: Source\n+) -> Statistics:\n+    if source == source.EXPECTED:\n+        # ty currently raises a false positive here due to incomplete enum.Flag support\n+        # see https://github.com/astral-sh/ty/issues/876\n+        num_errors = sum(\n+            [1 for g in grouped_diagnostics if source.EXPECTED in g.sources]  # ty:ignore[unsupported-operator]\n+        )\n+        return Statistics(\n+            true_positives=num_errors, false_positives=0, false_negatives=0\n+        )\n+\n+    def increment(statistics: Statistics, grouped: GroupedDiagnostics) -> Statistics:\n+        if (source in grouped.sources) and (Source.EXPECTED in grouped.sources):\n+            statistics.true_positives += 1\n+        elif source in grouped.sources:\n+            statistics.false_positives += 1\n+        else:\n+            statistics.false_negatives += 1\n+        return statistics\n+\n+    return reduce(increment, grouped_diagnostics, Statistics())\n+\n+\n+def render_grouped_diagnostics(\n+    grouped: list[GroupedDiagnostics], changed_only: bool = True\n+) -> str:\n+    if changed_only:\n+        grouped = [diag for diag in grouped if diag.changed]\n+    sorted_by_class = sorted(\n+        grouped,\n+        key=attrgetter(\"classification\"),\n+        reverse=True,\n+    )\n+\n+    lines = []\n+    for classification, group in groupby(\n+        sorted_by_class, key=attrgetter(\"classification\")\n+    ):\n+        group = list(group)\n+\n+        lines.append(f\"## {classification.value.replace('_', ' ').title()}s:\")\n+        lines.append(\"\")\n+        lines.append(\"```diff\")\n+\n+        for diag in group:\n+            lines.append(diag.display())\n+\n+        lines.append(\"```\")\n+\n+    return \"\\n\".join(lines)\n+\n+\n+def render_summary(grouped_diagnostics: list[GroupedDiagnostics]):\n+    def pct(value):\n+        return f\"{value:.2%}\"\n+\n+    def trend(value):\n+        if value == 0:\n+            return \"does not change\"\n+        return \"improves\" if value > 0 else \"regresses\"\n+\n+    old = compute_stats(grouped_diagnostics, source=Source.OLD)\n+    new = compute_stats(grouped_diagnostics, source=Source.NEW)\n+\n+    precision_delta = new.precision - old.precision\n+    recall_delta = new.recall - old.recall\n+\n+    table = dedent(\n+        f\"\"\"\n+        ## Typing Conformance\n+\n+        ### Summary\n+\n+        | Metric     | Old | New | \u0394 |\n+        |------------|-----|-----|---|\n+        | True Positives | {old.true_positives} | {new.true_positives} | {old.true_positives - new.true_positives} |\n+        | False Positives | {old.false_positives} | {new.false_positives} | {new.false_positives - old.false_positives} |\n+        | False Negatives | {old.false_negatives} | {new.false_negatives} | {new.false_negatives - old.false_negatives} |\n+        | Precision  | {old.precision:.2} | {new.precision:.2} | {pct(precision_delta)} |\n+        | Recall     | {old.recall:.2} | {new.recall:.2} | {pct(recall_delta)} |\n+        | Total      | {old.total} | {new.total} | {new.total - old.total} |\n+\n+        \"\"\"\n+    )\n+\n+    summary = (\n+        f\"Compared to the current merge base, this PR {trend(precision_delta)} precision \"\n+        f\"and {trend(recall_delta)} recall (TP: {new.true_positives - old.true_positives}, FP: {new.false_positives - old.false_positives}, FN: {new.false_negatives - old.false_negatives})).\"\n+    )",
      "comment": "More jargon! Precision is the fraction of diagnostics which are true errors (the number of true positives divided by the total number of diagnostics), while recall is the fraction of true errors for which ty raises a diagnostic. My goal was to give you a sense of how accurate ty is when it raises a diagnostic (precision) and how comprehensive its ability to detect errors is (recall).\n\nI can certainly remove the abbreviations. ",
      "comment_id": 2683081692,
      "user": "WillDuke",
      "created_at": "2026-01-12T16:47:59Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2683081692"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 396,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,490 @@\n+\"\"\"\n+Run typing conformance tests and compare results between two ty versions.\n+\n+Examples:\n+    # Compare two specific ty versions\n+    %(prog)s --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Use local ty builds\n+    %(prog)s --old-ty ./target/debug/ty-old --new-ty ./target/debug/ty-new\n+\n+    # Custom test directory\n+    %(prog)s --target-path custom/tests --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Show all diagnostics (not just changed ones)\n+    %(prog)s --all --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from itertools import groupby\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+# The conformance tests include 4 types of errors:\n+# 1. Required errors (E): The type checker must raise an error on this line\n+# 2. Optional errors (E?): The type checker may raise an error on this line\n+# 3. Tagged errors ([tag]): The type checker must raise at most one error on any of the lines in a file with matching tags\n+# 4. Tagged multi-errors ([tag]+): The type checker should raise one or more errors on any of the tagged lines\n+# # This regex pattern parses the error lines in the conformance tests, but the following\n+# # implementation treats all errors as required errors.\n+CONFORMANCE_ERROR_PATTERN = re.compile(\n+    r\"\"\"\n+    \\#\\s*E                  # \"# E\" begins each error\n+    (?P<optional>\\?)?       # Optional '?' (E?) indicates that an error is optional\n+    (?:                     # An optional tag for errors that may appear on multiple lines at most once\n+        \\[\n+            (?P<tag>[^+\\]]+)    # identifier\n+            (?P<multi>\\+)?      # '+' indicates that an error may occur more than once on tagged lines\n+        \\]\n+    )?\n+    (?:\n+        \\s*:\\s*(?P<description>.*) # optional description\n+    )?\n+    \"\"\",\n+    re.VERBOSE,\n+)\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    def __str__(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self) -> str:\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Statistics:\n+    true_positives: int = 0\n+    false_positives: int = 0\n+    false_negatives: int = 0\n+\n+    @property\n+    def precision(self) -> float:\n+        if self.true_positives + self.false_positives > 0:\n+            return self.true_positives / (self.true_positives + self.false_positives)\n+        return 0.0\n+\n+    @property\n+    def recall(self) -> float:\n+        if self.true_positives + self.false_negatives > 0:\n+            return self.true_positives / (self.true_positives + self.false_negatives)\n+        else:\n+            return 0.0\n+\n+    @property\n+    def total(self) -> int:\n+        return self.true_positives + self.false_positives\n+\n+\n+def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n+    diagnostics: list[Diagnostic] = []\n+    for file in path.resolve().rglob(\"*.py\"):\n+        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+            if error := re.search(CONFORMANCE_ERROR_PATTERN, line):\n+                diagnostics.append(\n+                    Diagnostic(\n+                        check_name=\"conformance\",\n+                        description=error.group(\"description\")\n+                        or error.group(\"tag\")\n+                        or \"Missing\",\n+                        severity=\"major\",\n+                        fingerprint=None,\n+                        location=Location(\n+                            path=file.as_posix(),\n+                            positions=Positions(\n+                                begin=Position(\n+                                    line=idx,\n+                                    column=error.start(),\n+                                ),\n+                                end=Position(\n+                                    line=idx,\n+                                    column=error.end(),\n+                                ),\n+                            ),\n+                        ),\n+                        source=Source.EXPECTED,\n+                    )\n+                )\n+\n+    assert diagnostics, \"Failed to discover any expected diagnostics!\"\n+    return diagnostics\n+\n+\n+def collect_ty_diagnostics(\n+    ty_path: list[str],\n+    source: Source,\n+    target_path: str = \".\",\n+    python_version: str = \"3.12\",\n+) -> list[Diagnostic]:\n+    process = subprocess.run(\n+        [\n+            *ty_path,\n+            \"check\",\n+            f\"--python-version={python_version}\",\n+            \"--output-format=gitlab\",\n+            \"--exit-zero\",\n+            target_path,\n+        ],\n+        capture_output=True,\n+        text=True,\n+        check=True,\n+        timeout=15,\n+    )\n+\n+    if process.returncode != 0:\n+        print(process.stderr)\n+        raise RuntimeError(f\"ty check failed with exit code {process.returncode}\")\n+\n+    return [\n+        Diagnostic.from_gitlab_output(dct, source=source)\n+        for dct in json.loads(process.stdout)\n+    ]\n+\n+\n+def group_diagnostics_by_key(\n+    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+) -> list[GroupedDiagnostics]:\n+    diagnostics = [\n+        *old,\n+        *new,\n+        *expected,\n+    ]\n+    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+\n+    grouped = []\n+    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+        group = list(group)\n+        sources: Source = reduce(or_, (diag.source for diag in group))\n+        grouped.append(\n+            GroupedDiagnostics(\n+                key=key,\n+                sources=sources,\n+                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n+                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n+                expected=next(\n+                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n+                ),\n+            )\n+        )\n+\n+    return grouped\n+\n+\n+def compute_stats(\n+    grouped_diagnostics: list[GroupedDiagnostics], source: Source\n+) -> Statistics:\n+    if source == source.EXPECTED:\n+        # ty currently raises a false positive here due to incomplete enum.Flag support\n+        # see https://github.com/astral-sh/ty/issues/876\n+        num_errors = sum(\n+            [1 for g in grouped_diagnostics if source.EXPECTED in g.sources]  # ty:ignore[unsupported-operator]\n+        )\n+        return Statistics(\n+            true_positives=num_errors, false_positives=0, false_negatives=0\n+        )\n+\n+    def increment(statistics: Statistics, grouped: GroupedDiagnostics) -> Statistics:\n+        if (source in grouped.sources) and (Source.EXPECTED in grouped.sources):\n+            statistics.true_positives += 1\n+        elif source in grouped.sources:\n+            statistics.false_positives += 1\n+        else:\n+            statistics.false_negatives += 1\n+        return statistics\n+\n+    return reduce(increment, grouped_diagnostics, Statistics())\n+\n+\n+def render_grouped_diagnostics(\n+    grouped: list[GroupedDiagnostics], changed_only: bool = True\n+) -> str:\n+    if changed_only:\n+        grouped = [diag for diag in grouped if diag.changed]\n+    sorted_by_class = sorted(\n+        grouped,\n+        key=attrgetter(\"classification\"),\n+        reverse=True,\n+    )\n+\n+    lines = []\n+    for classification, group in groupby(\n+        sorted_by_class, key=attrgetter(\"classification\")\n+    ):\n+        group = list(group)\n+\n+        lines.append(f\"## {classification.value.replace('_', ' ').title()}s:\")\n+        lines.append(\"\")\n+        lines.append(\"```diff\")\n+\n+        for diag in group:\n+            lines.append(diag.display())\n+\n+        lines.append(\"```\")\n+\n+    return \"\\n\".join(lines)\n+\n+\n+def render_summary(grouped_diagnostics: list[GroupedDiagnostics]):\n+    def pct(value):\n+        return f\"{value:.2%}\"\n+\n+    def trend(value):\n+        if value == 0:\n+            return \"does not change\"\n+        return \"improves\" if value > 0 else \"regresses\"\n+\n+    old = compute_stats(grouped_diagnostics, source=Source.OLD)\n+    new = compute_stats(grouped_diagnostics, source=Source.NEW)\n+\n+    precision_delta = new.precision - old.precision\n+    recall_delta = new.recall - old.recall\n+\n+    table = dedent(\n+        f\"\"\"\n+        ## Typing Conformance\n+\n+        ### Summary\n+\n+        | Metric     | Old | New | \u0394 |\n+        |------------|-----|-----|---|\n+        | True Positives | {old.true_positives} | {new.true_positives} | {old.true_positives - new.true_positives} |\n+        | False Positives | {old.false_positives} | {new.false_positives} | {new.false_positives - old.false_positives} |\n+        | False Negatives | {old.false_negatives} | {new.false_negatives} | {new.false_negatives - old.false_negatives} |\n+        | Precision  | {old.precision:.2} | {new.precision:.2} | {pct(precision_delta)} |\n+        | Recall     | {old.recall:.2} | {new.recall:.2} | {pct(recall_delta)} |\n+        | Total      | {old.total} | {new.total} | {new.total - old.total} |\n+\n+        \"\"\"\n+    )\n+\n+    summary = (\n+        f\"Compared to the current merge base, this PR {trend(precision_delta)} precision \"\n+        f\"and {trend(recall_delta)} recall (TP: {new.true_positives - old.true_positives}, FP: {new.false_positives - old.false_positives}, FN: {new.false_negatives - old.false_negatives})).\"\n+    )",
      "comment": "If you'd like to keep these stats, I could include a sentence explaining what they mean in the output. ",
      "comment_id": 2683088355,
      "user": "WillDuke",
      "created_at": "2026-01-12T16:49:55Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2683088355"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 396,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,490 @@\n+\"\"\"\n+Run typing conformance tests and compare results between two ty versions.\n+\n+Examples:\n+    # Compare two specific ty versions\n+    %(prog)s --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Use local ty builds\n+    %(prog)s --old-ty ./target/debug/ty-old --new-ty ./target/debug/ty-new\n+\n+    # Custom test directory\n+    %(prog)s --target-path custom/tests --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Show all diagnostics (not just changed ones)\n+    %(prog)s --all --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from itertools import groupby\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+# The conformance tests include 4 types of errors:\n+# 1. Required errors (E): The type checker must raise an error on this line\n+# 2. Optional errors (E?): The type checker may raise an error on this line\n+# 3. Tagged errors ([tag]): The type checker must raise at most one error on any of the lines in a file with matching tags\n+# 4. Tagged multi-errors ([tag]+): The type checker should raise one or more errors on any of the tagged lines\n+# # This regex pattern parses the error lines in the conformance tests, but the following\n+# # implementation treats all errors as required errors.\n+CONFORMANCE_ERROR_PATTERN = re.compile(\n+    r\"\"\"\n+    \\#\\s*E                  # \"# E\" begins each error\n+    (?P<optional>\\?)?       # Optional '?' (E?) indicates that an error is optional\n+    (?:                     # An optional tag for errors that may appear on multiple lines at most once\n+        \\[\n+            (?P<tag>[^+\\]]+)    # identifier\n+            (?P<multi>\\+)?      # '+' indicates that an error may occur more than once on tagged lines\n+        \\]\n+    )?\n+    (?:\n+        \\s*:\\s*(?P<description>.*) # optional description\n+    )?\n+    \"\"\",\n+    re.VERBOSE,\n+)\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    def __str__(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self) -> str:\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Statistics:\n+    true_positives: int = 0\n+    false_positives: int = 0\n+    false_negatives: int = 0\n+\n+    @property\n+    def precision(self) -> float:\n+        if self.true_positives + self.false_positives > 0:\n+            return self.true_positives / (self.true_positives + self.false_positives)\n+        return 0.0\n+\n+    @property\n+    def recall(self) -> float:\n+        if self.true_positives + self.false_negatives > 0:\n+            return self.true_positives / (self.true_positives + self.false_negatives)\n+        else:\n+            return 0.0\n+\n+    @property\n+    def total(self) -> int:\n+        return self.true_positives + self.false_positives\n+\n+\n+def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n+    diagnostics: list[Diagnostic] = []\n+    for file in path.resolve().rglob(\"*.py\"):\n+        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+            if error := re.search(CONFORMANCE_ERROR_PATTERN, line):\n+                diagnostics.append(\n+                    Diagnostic(\n+                        check_name=\"conformance\",\n+                        description=error.group(\"description\")\n+                        or error.group(\"tag\")\n+                        or \"Missing\",\n+                        severity=\"major\",\n+                        fingerprint=None,\n+                        location=Location(\n+                            path=file.as_posix(),\n+                            positions=Positions(\n+                                begin=Position(\n+                                    line=idx,\n+                                    column=error.start(),\n+                                ),\n+                                end=Position(\n+                                    line=idx,\n+                                    column=error.end(),\n+                                ),\n+                            ),\n+                        ),\n+                        source=Source.EXPECTED,\n+                    )\n+                )\n+\n+    assert diagnostics, \"Failed to discover any expected diagnostics!\"\n+    return diagnostics\n+\n+\n+def collect_ty_diagnostics(\n+    ty_path: list[str],\n+    source: Source,\n+    target_path: str = \".\",\n+    python_version: str = \"3.12\",\n+) -> list[Diagnostic]:\n+    process = subprocess.run(\n+        [\n+            *ty_path,\n+            \"check\",\n+            f\"--python-version={python_version}\",\n+            \"--output-format=gitlab\",\n+            \"--exit-zero\",\n+            target_path,\n+        ],\n+        capture_output=True,\n+        text=True,\n+        check=True,\n+        timeout=15,\n+    )\n+\n+    if process.returncode != 0:\n+        print(process.stderr)\n+        raise RuntimeError(f\"ty check failed with exit code {process.returncode}\")\n+\n+    return [\n+        Diagnostic.from_gitlab_output(dct, source=source)\n+        for dct in json.loads(process.stdout)\n+    ]\n+\n+\n+def group_diagnostics_by_key(\n+    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+) -> list[GroupedDiagnostics]:\n+    diagnostics = [\n+        *old,\n+        *new,\n+        *expected,\n+    ]\n+    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+\n+    grouped = []\n+    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+        group = list(group)\n+        sources: Source = reduce(or_, (diag.source for diag in group))\n+        grouped.append(\n+            GroupedDiagnostics(\n+                key=key,\n+                sources=sources,\n+                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n+                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n+                expected=next(\n+                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n+                ),\n+            )\n+        )\n+\n+    return grouped\n+\n+\n+def compute_stats(\n+    grouped_diagnostics: list[GroupedDiagnostics], source: Source\n+) -> Statistics:\n+    if source == source.EXPECTED:\n+        # ty currently raises a false positive here due to incomplete enum.Flag support\n+        # see https://github.com/astral-sh/ty/issues/876\n+        num_errors = sum(\n+            [1 for g in grouped_diagnostics if source.EXPECTED in g.sources]  # ty:ignore[unsupported-operator]\n+        )\n+        return Statistics(\n+            true_positives=num_errors, false_positives=0, false_negatives=0\n+        )\n+\n+    def increment(statistics: Statistics, grouped: GroupedDiagnostics) -> Statistics:\n+        if (source in grouped.sources) and (Source.EXPECTED in grouped.sources):\n+            statistics.true_positives += 1\n+        elif source in grouped.sources:\n+            statistics.false_positives += 1\n+        else:\n+            statistics.false_negatives += 1\n+        return statistics\n+\n+    return reduce(increment, grouped_diagnostics, Statistics())\n+\n+\n+def render_grouped_diagnostics(\n+    grouped: list[GroupedDiagnostics], changed_only: bool = True\n+) -> str:\n+    if changed_only:\n+        grouped = [diag for diag in grouped if diag.changed]\n+    sorted_by_class = sorted(\n+        grouped,\n+        key=attrgetter(\"classification\"),\n+        reverse=True,\n+    )\n+\n+    lines = []\n+    for classification, group in groupby(\n+        sorted_by_class, key=attrgetter(\"classification\")\n+    ):\n+        group = list(group)\n+\n+        lines.append(f\"## {classification.value.replace('_', ' ').title()}s:\")\n+        lines.append(\"\")\n+        lines.append(\"```diff\")\n+\n+        for diag in group:\n+            lines.append(diag.display())\n+\n+        lines.append(\"```\")\n+\n+    return \"\\n\".join(lines)\n+\n+\n+def render_summary(grouped_diagnostics: list[GroupedDiagnostics]):\n+    def pct(value):\n+        return f\"{value:.2%}\"\n+\n+    def trend(value):\n+        if value == 0:\n+            return \"does not change\"\n+        return \"improves\" if value > 0 else \"regresses\"\n+\n+    old = compute_stats(grouped_diagnostics, source=Source.OLD)\n+    new = compute_stats(grouped_diagnostics, source=Source.NEW)\n+\n+    precision_delta = new.precision - old.precision\n+    recall_delta = new.recall - old.recall\n+\n+    table = dedent(\n+        f\"\"\"\n+        ## Typing Conformance\n+\n+        ### Summary\n+\n+        | Metric     | Old | New | \u0394 |\n+        |------------|-----|-----|---|\n+        | True Positives | {old.true_positives} | {new.true_positives} | {old.true_positives - new.true_positives} |\n+        | False Positives | {old.false_positives} | {new.false_positives} | {new.false_positives - old.false_positives} |\n+        | False Negatives | {old.false_negatives} | {new.false_negatives} | {new.false_negatives - old.false_negatives} |\n+        | Precision  | {old.precision:.2} | {new.precision:.2} | {pct(precision_delta)} |\n+        | Recall     | {old.recall:.2} | {new.recall:.2} | {pct(recall_delta)} |\n+        | Total      | {old.total} | {new.total} | {new.total - old.total} |\n+\n+        \"\"\"\n+    )\n+\n+    summary = (\n+        f\"Compared to the current merge base, this PR {trend(precision_delta)} precision \"\n+        f\"and {trend(recall_delta)} recall (TP: {new.true_positives - old.true_positives}, FP: {new.false_positives - old.false_positives}, FN: {new.false_negatives - old.false_negatives})).\"\n+    )",
      "comment": "Yeah, these seem like useful statistics for sure! But rather than including them in the table, I think I'd prefer it if the script wrote out a descriptive sentence below the table: \"The percentage of diagnostic emitted that were true positives increased from 49% to 51%\", \"The percentage of diagnostics emitted that were true positives held steady at 63%\" or \"The percentage of diagnostics emitted that were true positives fell from 78% to 74%\", etc.",
      "comment_id": 2683125764,
      "user": "AlexWaygood",
      "created_at": "2026-01-12T17:00:31Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2683125764"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 410,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,490 @@\n+\"\"\"\n+Run typing conformance tests and compare results between two ty versions.\n+\n+Examples:\n+    # Compare two specific ty versions\n+    %(prog)s --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Use local ty builds\n+    %(prog)s --old-ty ./target/debug/ty-old --new-ty ./target/debug/ty-new\n+\n+    # Custom test directory\n+    %(prog)s --target-path custom/tests --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Show all diagnostics (not just changed ones)\n+    %(prog)s --all --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from itertools import groupby\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+# The conformance tests include 4 types of errors:\n+# 1. Required errors (E): The type checker must raise an error on this line\n+# 2. Optional errors (E?): The type checker may raise an error on this line\n+# 3. Tagged errors ([tag]): The type checker must raise at most one error on any of the lines in a file with matching tags\n+# 4. Tagged multi-errors ([tag]+): The type checker should raise one or more errors on any of the tagged lines\n+# # This regex pattern parses the error lines in the conformance tests, but the following\n+# # implementation treats all errors as required errors.\n+CONFORMANCE_ERROR_PATTERN = re.compile(\n+    r\"\"\"\n+    \\#\\s*E                  # \"# E\" begins each error\n+    (?P<optional>\\?)?       # Optional '?' (E?) indicates that an error is optional\n+    (?:                     # An optional tag for errors that may appear on multiple lines at most once\n+        \\[\n+            (?P<tag>[^+\\]]+)    # identifier\n+            (?P<multi>\\+)?      # '+' indicates that an error may occur more than once on tagged lines\n+        \\]\n+    )?\n+    (?:\n+        \\s*:\\s*(?P<description>.*) # optional description\n+    )?\n+    \"\"\",\n+    re.VERBOSE,\n+)\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    def __str__(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self) -> str:\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Statistics:\n+    true_positives: int = 0\n+    false_positives: int = 0\n+    false_negatives: int = 0\n+\n+    @property\n+    def precision(self) -> float:\n+        if self.true_positives + self.false_positives > 0:\n+            return self.true_positives / (self.true_positives + self.false_positives)\n+        return 0.0\n+\n+    @property\n+    def recall(self) -> float:\n+        if self.true_positives + self.false_negatives > 0:\n+            return self.true_positives / (self.true_positives + self.false_negatives)\n+        else:\n+            return 0.0\n+\n+    @property\n+    def total(self) -> int:\n+        return self.true_positives + self.false_positives\n+\n+\n+def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n+    diagnostics: list[Diagnostic] = []\n+    for file in path.resolve().rglob(\"*.py\"):\n+        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+            if error := re.search(CONFORMANCE_ERROR_PATTERN, line):\n+                diagnostics.append(\n+                    Diagnostic(\n+                        check_name=\"conformance\",\n+                        description=error.group(\"description\")\n+                        or error.group(\"tag\")\n+                        or \"Missing\",\n+                        severity=\"major\",\n+                        fingerprint=None,\n+                        location=Location(\n+                            path=file.as_posix(),\n+                            positions=Positions(\n+                                begin=Position(\n+                                    line=idx,\n+                                    column=error.start(),\n+                                ),\n+                                end=Position(\n+                                    line=idx,\n+                                    column=error.end(),\n+                                ),\n+                            ),\n+                        ),\n+                        source=Source.EXPECTED,\n+                    )\n+                )\n+\n+    assert diagnostics, \"Failed to discover any expected diagnostics!\"\n+    return diagnostics\n+\n+\n+def collect_ty_diagnostics(\n+    ty_path: list[str],\n+    source: Source,\n+    target_path: str = \".\",\n+    python_version: str = \"3.12\",\n+) -> list[Diagnostic]:\n+    process = subprocess.run(\n+        [\n+            *ty_path,\n+            \"check\",\n+            f\"--python-version={python_version}\",\n+            \"--output-format=gitlab\",\n+            \"--exit-zero\",\n+            target_path,\n+        ],\n+        capture_output=True,\n+        text=True,\n+        check=True,\n+        timeout=15,\n+    )\n+\n+    if process.returncode != 0:\n+        print(process.stderr)\n+        raise RuntimeError(f\"ty check failed with exit code {process.returncode}\")\n+\n+    return [\n+        Diagnostic.from_gitlab_output(dct, source=source)\n+        for dct in json.loads(process.stdout)\n+    ]\n+\n+\n+def group_diagnostics_by_key(\n+    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+) -> list[GroupedDiagnostics]:\n+    diagnostics = [\n+        *old,\n+        *new,\n+        *expected,\n+    ]\n+    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+\n+    grouped = []\n+    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+        group = list(group)\n+        sources: Source = reduce(or_, (diag.source for diag in group))\n+        grouped.append(\n+            GroupedDiagnostics(\n+                key=key,\n+                sources=sources,\n+                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n+                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n+                expected=next(\n+                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n+                ),\n+            )\n+        )\n+\n+    return grouped\n+\n+\n+def compute_stats(\n+    grouped_diagnostics: list[GroupedDiagnostics], source: Source\n+) -> Statistics:\n+    if source == source.EXPECTED:\n+        # ty currently raises a false positive here due to incomplete enum.Flag support\n+        # see https://github.com/astral-sh/ty/issues/876\n+        num_errors = sum(\n+            [1 for g in grouped_diagnostics if source.EXPECTED in g.sources]  # ty:ignore[unsupported-operator]\n+        )\n+        return Statistics(\n+            true_positives=num_errors, false_positives=0, false_negatives=0\n+        )\n+\n+    def increment(statistics: Statistics, grouped: GroupedDiagnostics) -> Statistics:\n+        if (source in grouped.sources) and (Source.EXPECTED in grouped.sources):\n+            statistics.true_positives += 1\n+        elif source in grouped.sources:\n+            statistics.false_positives += 1\n+        else:\n+            statistics.false_negatives += 1\n+        return statistics\n+\n+    return reduce(increment, grouped_diagnostics, Statistics())\n+\n+\n+def render_grouped_diagnostics(\n+    grouped: list[GroupedDiagnostics], changed_only: bool = True\n+) -> str:\n+    if changed_only:\n+        grouped = [diag for diag in grouped if diag.changed]\n+    sorted_by_class = sorted(\n+        grouped,\n+        key=attrgetter(\"classification\"),\n+        reverse=True,\n+    )\n+\n+    lines = []\n+    for classification, group in groupby(\n+        sorted_by_class, key=attrgetter(\"classification\")\n+    ):\n+        group = list(group)\n+\n+        lines.append(f\"## {classification.value.replace('_', ' ').title()}s:\")\n+        lines.append(\"\")\n+        lines.append(\"```diff\")\n+\n+        for diag in group:\n+            lines.append(diag.display())\n+\n+        lines.append(\"```\")\n+\n+    return \"\\n\".join(lines)\n+\n+\n+def render_summary(grouped_diagnostics: list[GroupedDiagnostics]):\n+    def pct(value):\n+        return f\"{value:.2%}\"\n+\n+    def trend(value):\n+        if value == 0:\n+            return \"does not change\"\n+        return \"improves\" if value > 0 else \"regresses\"\n+\n+    old = compute_stats(grouped_diagnostics, source=Source.OLD)\n+    new = compute_stats(grouped_diagnostics, source=Source.NEW)\n+\n+    precision_delta = new.precision - old.precision\n+    recall_delta = new.recall - old.recall\n+\n+    table = dedent(\n+        f\"\"\"\n+        ## Typing Conformance\n+\n+        ### Summary\n+\n+        | Metric     | Old | New | \u0394 |\n+        |------------|-----|-----|---|\n+        | True Positives | {old.true_positives} | {new.true_positives} | {old.true_positives - new.true_positives} |\n+        | False Positives | {old.false_positives} | {new.false_positives} | {new.false_positives - old.false_positives} |\n+        | False Negatives | {old.false_negatives} | {new.false_negatives} | {new.false_negatives - old.false_negatives} |\n+        | Precision  | {old.precision:.2} | {new.precision:.2} | {pct(precision_delta)} |\n+        | Recall     | {old.recall:.2} | {new.recall:.2} | {pct(recall_delta)} |\n+        | Total      | {old.total} | {new.total} | {new.total - old.total} |\n+\n+        \"\"\"\n+    )\n+\n+    summary = (\n+        f\"Compared to the current merge base, this PR {trend(precision_delta)} precision \"\n+        f\"and {trend(recall_delta)} recall (TP: {new.true_positives - old.true_positives}, FP: {new.false_positives - old.false_positives}, FN: {new.false_negatives - old.false_negatives})).\"\n+    )\n+\n+    return \"\\n\".join([table, summary])\n+\n+\n+def parse_args():\n+    parser = argparse.ArgumentParser(\n+        description=__doc__,\n+        formatter_class=argparse.RawDescriptionHelpFormatter,\n+    )\n+\n+    parser.add_argument(\n+        \"--old-ty\",\n+        nargs=\"+\",\n+        default=[\"uvx\", \"ty@0.0.1a35\"],",
      "comment": "The default here feels a bit arbitrary. I'm leaning towards removing it",
      "comment_id": 2683165139,
      "user": "MichaReiser",
      "created_at": "2026-01-12T17:12:20Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2683165139"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 417,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,490 @@\n+\"\"\"\n+Run typing conformance tests and compare results between two ty versions.\n+\n+Examples:\n+    # Compare two specific ty versions\n+    %(prog)s --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Use local ty builds\n+    %(prog)s --old-ty ./target/debug/ty-old --new-ty ./target/debug/ty-new\n+\n+    # Custom test directory\n+    %(prog)s --target-path custom/tests --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Show all diagnostics (not just changed ones)\n+    %(prog)s --all --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from itertools import groupby\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+# The conformance tests include 4 types of errors:\n+# 1. Required errors (E): The type checker must raise an error on this line\n+# 2. Optional errors (E?): The type checker may raise an error on this line\n+# 3. Tagged errors ([tag]): The type checker must raise at most one error on any of the lines in a file with matching tags\n+# 4. Tagged multi-errors ([tag]+): The type checker should raise one or more errors on any of the tagged lines\n+# # This regex pattern parses the error lines in the conformance tests, but the following\n+# # implementation treats all errors as required errors.\n+CONFORMANCE_ERROR_PATTERN = re.compile(\n+    r\"\"\"\n+    \\#\\s*E                  # \"# E\" begins each error\n+    (?P<optional>\\?)?       # Optional '?' (E?) indicates that an error is optional\n+    (?:                     # An optional tag for errors that may appear on multiple lines at most once\n+        \\[\n+            (?P<tag>[^+\\]]+)    # identifier\n+            (?P<multi>\\+)?      # '+' indicates that an error may occur more than once on tagged lines\n+        \\]\n+    )?\n+    (?:\n+        \\s*:\\s*(?P<description>.*) # optional description\n+    )?\n+    \"\"\",\n+    re.VERBOSE,\n+)\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    def __str__(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self) -> str:\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Statistics:\n+    true_positives: int = 0\n+    false_positives: int = 0\n+    false_negatives: int = 0\n+\n+    @property\n+    def precision(self) -> float:\n+        if self.true_positives + self.false_positives > 0:\n+            return self.true_positives / (self.true_positives + self.false_positives)\n+        return 0.0\n+\n+    @property\n+    def recall(self) -> float:\n+        if self.true_positives + self.false_negatives > 0:\n+            return self.true_positives / (self.true_positives + self.false_negatives)\n+        else:\n+            return 0.0\n+\n+    @property\n+    def total(self) -> int:\n+        return self.true_positives + self.false_positives\n+\n+\n+def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n+    diagnostics: list[Diagnostic] = []\n+    for file in path.resolve().rglob(\"*.py\"):\n+        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+            if error := re.search(CONFORMANCE_ERROR_PATTERN, line):\n+                diagnostics.append(\n+                    Diagnostic(\n+                        check_name=\"conformance\",\n+                        description=error.group(\"description\")\n+                        or error.group(\"tag\")\n+                        or \"Missing\",\n+                        severity=\"major\",\n+                        fingerprint=None,\n+                        location=Location(\n+                            path=file.as_posix(),\n+                            positions=Positions(\n+                                begin=Position(\n+                                    line=idx,\n+                                    column=error.start(),\n+                                ),\n+                                end=Position(\n+                                    line=idx,\n+                                    column=error.end(),\n+                                ),\n+                            ),\n+                        ),\n+                        source=Source.EXPECTED,\n+                    )\n+                )\n+\n+    assert diagnostics, \"Failed to discover any expected diagnostics!\"\n+    return diagnostics\n+\n+\n+def collect_ty_diagnostics(\n+    ty_path: list[str],\n+    source: Source,\n+    target_path: str = \".\",\n+    python_version: str = \"3.12\",\n+) -> list[Diagnostic]:\n+    process = subprocess.run(\n+        [\n+            *ty_path,\n+            \"check\",\n+            f\"--python-version={python_version}\",\n+            \"--output-format=gitlab\",\n+            \"--exit-zero\",\n+            target_path,\n+        ],\n+        capture_output=True,\n+        text=True,\n+        check=True,\n+        timeout=15,\n+    )\n+\n+    if process.returncode != 0:\n+        print(process.stderr)\n+        raise RuntimeError(f\"ty check failed with exit code {process.returncode}\")\n+\n+    return [\n+        Diagnostic.from_gitlab_output(dct, source=source)\n+        for dct in json.loads(process.stdout)\n+    ]\n+\n+\n+def group_diagnostics_by_key(\n+    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+) -> list[GroupedDiagnostics]:\n+    diagnostics = [\n+        *old,\n+        *new,\n+        *expected,\n+    ]\n+    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+\n+    grouped = []\n+    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+        group = list(group)\n+        sources: Source = reduce(or_, (diag.source for diag in group))\n+        grouped.append(\n+            GroupedDiagnostics(\n+                key=key,\n+                sources=sources,\n+                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n+                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n+                expected=next(\n+                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n+                ),\n+            )\n+        )\n+\n+    return grouped\n+\n+\n+def compute_stats(\n+    grouped_diagnostics: list[GroupedDiagnostics], source: Source\n+) -> Statistics:\n+    if source == source.EXPECTED:\n+        # ty currently raises a false positive here due to incomplete enum.Flag support\n+        # see https://github.com/astral-sh/ty/issues/876\n+        num_errors = sum(\n+            [1 for g in grouped_diagnostics if source.EXPECTED in g.sources]  # ty:ignore[unsupported-operator]\n+        )\n+        return Statistics(\n+            true_positives=num_errors, false_positives=0, false_negatives=0\n+        )\n+\n+    def increment(statistics: Statistics, grouped: GroupedDiagnostics) -> Statistics:\n+        if (source in grouped.sources) and (Source.EXPECTED in grouped.sources):\n+            statistics.true_positives += 1\n+        elif source in grouped.sources:\n+            statistics.false_positives += 1\n+        else:\n+            statistics.false_negatives += 1\n+        return statistics\n+\n+    return reduce(increment, grouped_diagnostics, Statistics())\n+\n+\n+def render_grouped_diagnostics(\n+    grouped: list[GroupedDiagnostics], changed_only: bool = True\n+) -> str:\n+    if changed_only:\n+        grouped = [diag for diag in grouped if diag.changed]\n+    sorted_by_class = sorted(\n+        grouped,\n+        key=attrgetter(\"classification\"),\n+        reverse=True,\n+    )\n+\n+    lines = []\n+    for classification, group in groupby(\n+        sorted_by_class, key=attrgetter(\"classification\")\n+    ):\n+        group = list(group)\n+\n+        lines.append(f\"## {classification.value.replace('_', ' ').title()}s:\")\n+        lines.append(\"\")\n+        lines.append(\"```diff\")\n+\n+        for diag in group:\n+            lines.append(diag.display())\n+\n+        lines.append(\"```\")\n+\n+    return \"\\n\".join(lines)\n+\n+\n+def render_summary(grouped_diagnostics: list[GroupedDiagnostics]):\n+    def pct(value):\n+        return f\"{value:.2%}\"\n+\n+    def trend(value):\n+        if value == 0:\n+            return \"does not change\"\n+        return \"improves\" if value > 0 else \"regresses\"\n+\n+    old = compute_stats(grouped_diagnostics, source=Source.OLD)\n+    new = compute_stats(grouped_diagnostics, source=Source.NEW)\n+\n+    precision_delta = new.precision - old.precision\n+    recall_delta = new.recall - old.recall\n+\n+    table = dedent(\n+        f\"\"\"\n+        ## Typing Conformance\n+\n+        ### Summary\n+\n+        | Metric     | Old | New | \u0394 |\n+        |------------|-----|-----|---|\n+        | True Positives | {old.true_positives} | {new.true_positives} | {old.true_positives - new.true_positives} |\n+        | False Positives | {old.false_positives} | {new.false_positives} | {new.false_positives - old.false_positives} |\n+        | False Negatives | {old.false_negatives} | {new.false_negatives} | {new.false_negatives - old.false_negatives} |\n+        | Precision  | {old.precision:.2} | {new.precision:.2} | {pct(precision_delta)} |\n+        | Recall     | {old.recall:.2} | {new.recall:.2} | {pct(recall_delta)} |\n+        | Total      | {old.total} | {new.total} | {new.total - old.total} |\n+\n+        \"\"\"\n+    )\n+\n+    summary = (\n+        f\"Compared to the current merge base, this PR {trend(precision_delta)} precision \"\n+        f\"and {trend(recall_delta)} recall (TP: {new.true_positives - old.true_positives}, FP: {new.false_positives - old.false_positives}, FN: {new.false_negatives - old.false_negatives})).\"\n+    )\n+\n+    return \"\\n\".join([table, summary])\n+\n+\n+def parse_args():\n+    parser = argparse.ArgumentParser(\n+        description=__doc__,\n+        formatter_class=argparse.RawDescriptionHelpFormatter,\n+    )\n+\n+    parser.add_argument(\n+        \"--old-ty\",\n+        nargs=\"+\",\n+        default=[\"uvx\", \"ty@0.0.1a35\"],\n+        help=\"Command to run old version of ty (default: uvx ty@0.0.1a35)\",\n+    )\n+\n+    parser.add_argument(\n+        \"--new-ty\",\n+        nargs=\"+\",\n+        default=[\"uvx\", \"ty@0.0.7\"],",
      "comment": "I think defaulting to the latest version is a more sensible default than some arbitrary old version\r\n\r\n```suggestion\r\n        default=[\"uvx\", \"ty@latest\"],\r\n```",
      "comment_id": 2683166392,
      "user": "MichaReiser",
      "created_at": "2026-01-12T17:12:40Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2683166392"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 396,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,490 @@\n+\"\"\"\n+Run typing conformance tests and compare results between two ty versions.\n+\n+Examples:\n+    # Compare two specific ty versions\n+    %(prog)s --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Use local ty builds\n+    %(prog)s --old-ty ./target/debug/ty-old --new-ty ./target/debug/ty-new\n+\n+    # Custom test directory\n+    %(prog)s --target-path custom/tests --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Show all diagnostics (not just changed ones)\n+    %(prog)s --all --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from itertools import groupby\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+# The conformance tests include 4 types of errors:\n+# 1. Required errors (E): The type checker must raise an error on this line\n+# 2. Optional errors (E?): The type checker may raise an error on this line\n+# 3. Tagged errors ([tag]): The type checker must raise at most one error on any of the lines in a file with matching tags\n+# 4. Tagged multi-errors ([tag]+): The type checker should raise one or more errors on any of the tagged lines\n+# # This regex pattern parses the error lines in the conformance tests, but the following\n+# # implementation treats all errors as required errors.\n+CONFORMANCE_ERROR_PATTERN = re.compile(\n+    r\"\"\"\n+    \\#\\s*E                  # \"# E\" begins each error\n+    (?P<optional>\\?)?       # Optional '?' (E?) indicates that an error is optional\n+    (?:                     # An optional tag for errors that may appear on multiple lines at most once\n+        \\[\n+            (?P<tag>[^+\\]]+)    # identifier\n+            (?P<multi>\\+)?      # '+' indicates that an error may occur more than once on tagged lines\n+        \\]\n+    )?\n+    (?:\n+        \\s*:\\s*(?P<description>.*) # optional description\n+    )?\n+    \"\"\",\n+    re.VERBOSE,\n+)\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    def __str__(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self) -> str:\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Statistics:\n+    true_positives: int = 0\n+    false_positives: int = 0\n+    false_negatives: int = 0\n+\n+    @property\n+    def precision(self) -> float:\n+        if self.true_positives + self.false_positives > 0:\n+            return self.true_positives / (self.true_positives + self.false_positives)\n+        return 0.0\n+\n+    @property\n+    def recall(self) -> float:\n+        if self.true_positives + self.false_negatives > 0:\n+            return self.true_positives / (self.true_positives + self.false_negatives)\n+        else:\n+            return 0.0\n+\n+    @property\n+    def total(self) -> int:\n+        return self.true_positives + self.false_positives\n+\n+\n+def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n+    diagnostics: list[Diagnostic] = []\n+    for file in path.resolve().rglob(\"*.py\"):\n+        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+            if error := re.search(CONFORMANCE_ERROR_PATTERN, line):\n+                diagnostics.append(\n+                    Diagnostic(\n+                        check_name=\"conformance\",\n+                        description=error.group(\"description\")\n+                        or error.group(\"tag\")\n+                        or \"Missing\",\n+                        severity=\"major\",\n+                        fingerprint=None,\n+                        location=Location(\n+                            path=file.as_posix(),\n+                            positions=Positions(\n+                                begin=Position(\n+                                    line=idx,\n+                                    column=error.start(),\n+                                ),\n+                                end=Position(\n+                                    line=idx,\n+                                    column=error.end(),\n+                                ),\n+                            ),\n+                        ),\n+                        source=Source.EXPECTED,\n+                    )\n+                )\n+\n+    assert diagnostics, \"Failed to discover any expected diagnostics!\"\n+    return diagnostics\n+\n+\n+def collect_ty_diagnostics(\n+    ty_path: list[str],\n+    source: Source,\n+    target_path: str = \".\",\n+    python_version: str = \"3.12\",\n+) -> list[Diagnostic]:\n+    process = subprocess.run(\n+        [\n+            *ty_path,\n+            \"check\",\n+            f\"--python-version={python_version}\",\n+            \"--output-format=gitlab\",\n+            \"--exit-zero\",\n+            target_path,\n+        ],\n+        capture_output=True,\n+        text=True,\n+        check=True,\n+        timeout=15,\n+    )\n+\n+    if process.returncode != 0:\n+        print(process.stderr)\n+        raise RuntimeError(f\"ty check failed with exit code {process.returncode}\")\n+\n+    return [\n+        Diagnostic.from_gitlab_output(dct, source=source)\n+        for dct in json.loads(process.stdout)\n+    ]\n+\n+\n+def group_diagnostics_by_key(\n+    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+) -> list[GroupedDiagnostics]:\n+    diagnostics = [\n+        *old,\n+        *new,\n+        *expected,\n+    ]\n+    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+\n+    grouped = []\n+    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+        group = list(group)\n+        sources: Source = reduce(or_, (diag.source for diag in group))\n+        grouped.append(\n+            GroupedDiagnostics(\n+                key=key,\n+                sources=sources,\n+                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n+                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n+                expected=next(\n+                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n+                ),\n+            )\n+        )\n+\n+    return grouped\n+\n+\n+def compute_stats(\n+    grouped_diagnostics: list[GroupedDiagnostics], source: Source\n+) -> Statistics:\n+    if source == source.EXPECTED:\n+        # ty currently raises a false positive here due to incomplete enum.Flag support\n+        # see https://github.com/astral-sh/ty/issues/876\n+        num_errors = sum(\n+            [1 for g in grouped_diagnostics if source.EXPECTED in g.sources]  # ty:ignore[unsupported-operator]\n+        )\n+        return Statistics(\n+            true_positives=num_errors, false_positives=0, false_negatives=0\n+        )\n+\n+    def increment(statistics: Statistics, grouped: GroupedDiagnostics) -> Statistics:\n+        if (source in grouped.sources) and (Source.EXPECTED in grouped.sources):\n+            statistics.true_positives += 1\n+        elif source in grouped.sources:\n+            statistics.false_positives += 1\n+        else:\n+            statistics.false_negatives += 1\n+        return statistics\n+\n+    return reduce(increment, grouped_diagnostics, Statistics())\n+\n+\n+def render_grouped_diagnostics(\n+    grouped: list[GroupedDiagnostics], changed_only: bool = True\n+) -> str:\n+    if changed_only:\n+        grouped = [diag for diag in grouped if diag.changed]\n+    sorted_by_class = sorted(\n+        grouped,\n+        key=attrgetter(\"classification\"),\n+        reverse=True,\n+    )\n+\n+    lines = []\n+    for classification, group in groupby(\n+        sorted_by_class, key=attrgetter(\"classification\")\n+    ):\n+        group = list(group)\n+\n+        lines.append(f\"## {classification.value.replace('_', ' ').title()}s:\")\n+        lines.append(\"\")\n+        lines.append(\"```diff\")\n+\n+        for diag in group:\n+            lines.append(diag.display())\n+\n+        lines.append(\"```\")\n+\n+    return \"\\n\".join(lines)\n+\n+\n+def render_summary(grouped_diagnostics: list[GroupedDiagnostics]):\n+    def pct(value):\n+        return f\"{value:.2%}\"\n+\n+    def trend(value):\n+        if value == 0:\n+            return \"does not change\"\n+        return \"improves\" if value > 0 else \"regresses\"\n+\n+    old = compute_stats(grouped_diagnostics, source=Source.OLD)\n+    new = compute_stats(grouped_diagnostics, source=Source.NEW)\n+\n+    precision_delta = new.precision - old.precision\n+    recall_delta = new.recall - old.recall\n+\n+    table = dedent(\n+        f\"\"\"\n+        ## Typing Conformance\n+\n+        ### Summary\n+\n+        | Metric     | Old | New | \u0394 |\n+        |------------|-----|-----|---|\n+        | True Positives | {old.true_positives} | {new.true_positives} | {old.true_positives - new.true_positives} |\n+        | False Positives | {old.false_positives} | {new.false_positives} | {new.false_positives - old.false_positives} |\n+        | False Negatives | {old.false_negatives} | {new.false_negatives} | {new.false_negatives - old.false_negatives} |\n+        | Precision  | {old.precision:.2} | {new.precision:.2} | {pct(precision_delta)} |\n+        | Recall     | {old.recall:.2} | {new.recall:.2} | {pct(recall_delta)} |\n+        | Total      | {old.total} | {new.total} | {new.total - old.total} |\n+\n+        \"\"\"\n+    )\n+\n+    summary = (\n+        f\"Compared to the current merge base, this PR {trend(precision_delta)} precision \"\n+        f\"and {trend(recall_delta)} recall (TP: {new.true_positives - old.true_positives}, FP: {new.false_positives - old.false_positives}, FN: {new.false_negatives - old.false_negatives})).\"\n+    )",
      "comment": "I find sentences much harder to parse. It's what I like about codspeed. I can just look at the last column to see the result. \r\n\r\nAdding some documentation at the end of the output makes sense to me",
      "comment_id": 2683171622,
      "user": "MichaReiser",
      "created_at": "2026-01-12T17:14:18Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2683171622"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 396,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,490 @@\n+\"\"\"\n+Run typing conformance tests and compare results between two ty versions.\n+\n+Examples:\n+    # Compare two specific ty versions\n+    %(prog)s --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Use local ty builds\n+    %(prog)s --old-ty ./target/debug/ty-old --new-ty ./target/debug/ty-new\n+\n+    # Custom test directory\n+    %(prog)s --target-path custom/tests --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Show all diagnostics (not just changed ones)\n+    %(prog)s --all --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from itertools import groupby\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+# The conformance tests include 4 types of errors:\n+# 1. Required errors (E): The type checker must raise an error on this line\n+# 2. Optional errors (E?): The type checker may raise an error on this line\n+# 3. Tagged errors ([tag]): The type checker must raise at most one error on any of the lines in a file with matching tags\n+# 4. Tagged multi-errors ([tag]+): The type checker should raise one or more errors on any of the tagged lines\n+# # This regex pattern parses the error lines in the conformance tests, but the following\n+# # implementation treats all errors as required errors.\n+CONFORMANCE_ERROR_PATTERN = re.compile(\n+    r\"\"\"\n+    \\#\\s*E                  # \"# E\" begins each error\n+    (?P<optional>\\?)?       # Optional '?' (E?) indicates that an error is optional\n+    (?:                     # An optional tag for errors that may appear on multiple lines at most once\n+        \\[\n+            (?P<tag>[^+\\]]+)    # identifier\n+            (?P<multi>\\+)?      # '+' indicates that an error may occur more than once on tagged lines\n+        \\]\n+    )?\n+    (?:\n+        \\s*:\\s*(?P<description>.*) # optional description\n+    )?\n+    \"\"\",\n+    re.VERBOSE,\n+)\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    def __str__(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self) -> str:\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Statistics:\n+    true_positives: int = 0\n+    false_positives: int = 0\n+    false_negatives: int = 0\n+\n+    @property\n+    def precision(self) -> float:\n+        if self.true_positives + self.false_positives > 0:\n+            return self.true_positives / (self.true_positives + self.false_positives)\n+        return 0.0\n+\n+    @property\n+    def recall(self) -> float:\n+        if self.true_positives + self.false_negatives > 0:\n+            return self.true_positives / (self.true_positives + self.false_negatives)\n+        else:\n+            return 0.0\n+\n+    @property\n+    def total(self) -> int:\n+        return self.true_positives + self.false_positives\n+\n+\n+def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n+    diagnostics: list[Diagnostic] = []\n+    for file in path.resolve().rglob(\"*.py\"):\n+        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+            if error := re.search(CONFORMANCE_ERROR_PATTERN, line):\n+                diagnostics.append(\n+                    Diagnostic(\n+                        check_name=\"conformance\",\n+                        description=error.group(\"description\")\n+                        or error.group(\"tag\")\n+                        or \"Missing\",\n+                        severity=\"major\",\n+                        fingerprint=None,\n+                        location=Location(\n+                            path=file.as_posix(),\n+                            positions=Positions(\n+                                begin=Position(\n+                                    line=idx,\n+                                    column=error.start(),\n+                                ),\n+                                end=Position(\n+                                    line=idx,\n+                                    column=error.end(),\n+                                ),\n+                            ),\n+                        ),\n+                        source=Source.EXPECTED,\n+                    )\n+                )\n+\n+    assert diagnostics, \"Failed to discover any expected diagnostics!\"\n+    return diagnostics\n+\n+\n+def collect_ty_diagnostics(\n+    ty_path: list[str],\n+    source: Source,\n+    target_path: str = \".\",\n+    python_version: str = \"3.12\",\n+) -> list[Diagnostic]:\n+    process = subprocess.run(\n+        [\n+            *ty_path,\n+            \"check\",\n+            f\"--python-version={python_version}\",\n+            \"--output-format=gitlab\",\n+            \"--exit-zero\",\n+            target_path,\n+        ],\n+        capture_output=True,\n+        text=True,\n+        check=True,\n+        timeout=15,\n+    )\n+\n+    if process.returncode != 0:\n+        print(process.stderr)\n+        raise RuntimeError(f\"ty check failed with exit code {process.returncode}\")\n+\n+    return [\n+        Diagnostic.from_gitlab_output(dct, source=source)\n+        for dct in json.loads(process.stdout)\n+    ]\n+\n+\n+def group_diagnostics_by_key(\n+    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+) -> list[GroupedDiagnostics]:\n+    diagnostics = [\n+        *old,\n+        *new,\n+        *expected,\n+    ]\n+    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+\n+    grouped = []\n+    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+        group = list(group)\n+        sources: Source = reduce(or_, (diag.source for diag in group))\n+        grouped.append(\n+            GroupedDiagnostics(\n+                key=key,\n+                sources=sources,\n+                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n+                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n+                expected=next(\n+                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n+                ),\n+            )\n+        )\n+\n+    return grouped\n+\n+\n+def compute_stats(\n+    grouped_diagnostics: list[GroupedDiagnostics], source: Source\n+) -> Statistics:\n+    if source == source.EXPECTED:\n+        # ty currently raises a false positive here due to incomplete enum.Flag support\n+        # see https://github.com/astral-sh/ty/issues/876\n+        num_errors = sum(\n+            [1 for g in grouped_diagnostics if source.EXPECTED in g.sources]  # ty:ignore[unsupported-operator]\n+        )\n+        return Statistics(\n+            true_positives=num_errors, false_positives=0, false_negatives=0\n+        )\n+\n+    def increment(statistics: Statistics, grouped: GroupedDiagnostics) -> Statistics:\n+        if (source in grouped.sources) and (Source.EXPECTED in grouped.sources):\n+            statistics.true_positives += 1\n+        elif source in grouped.sources:\n+            statistics.false_positives += 1\n+        else:\n+            statistics.false_negatives += 1\n+        return statistics\n+\n+    return reduce(increment, grouped_diagnostics, Statistics())\n+\n+\n+def render_grouped_diagnostics(\n+    grouped: list[GroupedDiagnostics], changed_only: bool = True\n+) -> str:\n+    if changed_only:\n+        grouped = [diag for diag in grouped if diag.changed]\n+    sorted_by_class = sorted(\n+        grouped,\n+        key=attrgetter(\"classification\"),\n+        reverse=True,\n+    )\n+\n+    lines = []\n+    for classification, group in groupby(\n+        sorted_by_class, key=attrgetter(\"classification\")\n+    ):\n+        group = list(group)\n+\n+        lines.append(f\"## {classification.value.replace('_', ' ').title()}s:\")\n+        lines.append(\"\")\n+        lines.append(\"```diff\")\n+\n+        for diag in group:\n+            lines.append(diag.display())\n+\n+        lines.append(\"```\")\n+\n+    return \"\\n\".join(lines)\n+\n+\n+def render_summary(grouped_diagnostics: list[GroupedDiagnostics]):\n+    def pct(value):\n+        return f\"{value:.2%}\"\n+\n+    def trend(value):\n+        if value == 0:\n+            return \"does not change\"\n+        return \"improves\" if value > 0 else \"regresses\"\n+\n+    old = compute_stats(grouped_diagnostics, source=Source.OLD)\n+    new = compute_stats(grouped_diagnostics, source=Source.NEW)\n+\n+    precision_delta = new.precision - old.precision\n+    recall_delta = new.recall - old.recall\n+\n+    table = dedent(\n+        f\"\"\"\n+        ## Typing Conformance\n+\n+        ### Summary\n+\n+        | Metric     | Old | New | \u0394 |\n+        |------------|-----|-----|---|\n+        | True Positives | {old.true_positives} | {new.true_positives} | {old.true_positives - new.true_positives} |\n+        | False Positives | {old.false_positives} | {new.false_positives} | {new.false_positives - old.false_positives} |\n+        | False Negatives | {old.false_negatives} | {new.false_negatives} | {new.false_negatives - old.false_negatives} |\n+        | Precision  | {old.precision:.2} | {new.precision:.2} | {pct(precision_delta)} |\n+        | Recall     | {old.recall:.2} | {new.recall:.2} | {pct(recall_delta)} |\n+        | Total      | {old.total} | {new.total} | {new.total - old.total} |\n+\n+        \"\"\"\n+    )\n+\n+    summary = (\n+        f\"Compared to the current merge base, this PR {trend(precision_delta)} precision \"\n+        f\"and {trend(recall_delta)} recall (TP: {new.true_positives - old.true_positives}, FP: {new.false_positives - old.false_positives}, FN: {new.false_negatives - old.false_negatives})).\"\n+    )",
      "comment": "I'm fine with a jargony table and some documentation below it that explains the jargon :-)\r\n\r\nthough I wonder if we could add a visual indicator somehow that makes it clear whether a number in the table going up or down is a good or bad thing without having to read a separate documentation paragraph somewhere else. Maybe something similar to the emoji suggestion I had in https://github.com/astral-sh/ruff/pull/22231#discussion_r2682732308?",
      "comment_id": 2683254864,
      "user": "AlexWaygood",
      "created_at": "2026-01-12T17:39:59Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2683254864"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 396,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,490 @@\n+\"\"\"\n+Run typing conformance tests and compare results between two ty versions.\n+\n+Examples:\n+    # Compare two specific ty versions\n+    %(prog)s --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Use local ty builds\n+    %(prog)s --old-ty ./target/debug/ty-old --new-ty ./target/debug/ty-new\n+\n+    # Custom test directory\n+    %(prog)s --target-path custom/tests --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Show all diagnostics (not just changed ones)\n+    %(prog)s --all --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from itertools import groupby\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+# The conformance tests include 4 types of errors:\n+# 1. Required errors (E): The type checker must raise an error on this line\n+# 2. Optional errors (E?): The type checker may raise an error on this line\n+# 3. Tagged errors ([tag]): The type checker must raise at most one error on any of the lines in a file with matching tags\n+# 4. Tagged multi-errors ([tag]+): The type checker should raise one or more errors on any of the tagged lines\n+# # This regex pattern parses the error lines in the conformance tests, but the following\n+# # implementation treats all errors as required errors.\n+CONFORMANCE_ERROR_PATTERN = re.compile(\n+    r\"\"\"\n+    \\#\\s*E                  # \"# E\" begins each error\n+    (?P<optional>\\?)?       # Optional '?' (E?) indicates that an error is optional\n+    (?:                     # An optional tag for errors that may appear on multiple lines at most once\n+        \\[\n+            (?P<tag>[^+\\]]+)    # identifier\n+            (?P<multi>\\+)?      # '+' indicates that an error may occur more than once on tagged lines\n+        \\]\n+    )?\n+    (?:\n+        \\s*:\\s*(?P<description>.*) # optional description\n+    )?\n+    \"\"\",\n+    re.VERBOSE,\n+)\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    def __str__(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self) -> str:\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Statistics:\n+    true_positives: int = 0\n+    false_positives: int = 0\n+    false_negatives: int = 0\n+\n+    @property\n+    def precision(self) -> float:\n+        if self.true_positives + self.false_positives > 0:\n+            return self.true_positives / (self.true_positives + self.false_positives)\n+        return 0.0\n+\n+    @property\n+    def recall(self) -> float:\n+        if self.true_positives + self.false_negatives > 0:\n+            return self.true_positives / (self.true_positives + self.false_negatives)\n+        else:\n+            return 0.0\n+\n+    @property\n+    def total(self) -> int:\n+        return self.true_positives + self.false_positives\n+\n+\n+def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n+    diagnostics: list[Diagnostic] = []\n+    for file in path.resolve().rglob(\"*.py\"):\n+        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+            if error := re.search(CONFORMANCE_ERROR_PATTERN, line):\n+                diagnostics.append(\n+                    Diagnostic(\n+                        check_name=\"conformance\",\n+                        description=error.group(\"description\")\n+                        or error.group(\"tag\")\n+                        or \"Missing\",\n+                        severity=\"major\",\n+                        fingerprint=None,\n+                        location=Location(\n+                            path=file.as_posix(),\n+                            positions=Positions(\n+                                begin=Position(\n+                                    line=idx,\n+                                    column=error.start(),\n+                                ),\n+                                end=Position(\n+                                    line=idx,\n+                                    column=error.end(),\n+                                ),\n+                            ),\n+                        ),\n+                        source=Source.EXPECTED,\n+                    )\n+                )\n+\n+    assert diagnostics, \"Failed to discover any expected diagnostics!\"\n+    return diagnostics\n+\n+\n+def collect_ty_diagnostics(\n+    ty_path: list[str],\n+    source: Source,\n+    target_path: str = \".\",\n+    python_version: str = \"3.12\",\n+) -> list[Diagnostic]:\n+    process = subprocess.run(\n+        [\n+            *ty_path,\n+            \"check\",\n+            f\"--python-version={python_version}\",\n+            \"--output-format=gitlab\",\n+            \"--exit-zero\",\n+            target_path,\n+        ],\n+        capture_output=True,\n+        text=True,\n+        check=True,\n+        timeout=15,\n+    )\n+\n+    if process.returncode != 0:\n+        print(process.stderr)\n+        raise RuntimeError(f\"ty check failed with exit code {process.returncode}\")\n+\n+    return [\n+        Diagnostic.from_gitlab_output(dct, source=source)\n+        for dct in json.loads(process.stdout)\n+    ]\n+\n+\n+def group_diagnostics_by_key(\n+    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+) -> list[GroupedDiagnostics]:\n+    diagnostics = [\n+        *old,\n+        *new,\n+        *expected,\n+    ]\n+    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+\n+    grouped = []\n+    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+        group = list(group)\n+        sources: Source = reduce(or_, (diag.source for diag in group))\n+        grouped.append(\n+            GroupedDiagnostics(\n+                key=key,\n+                sources=sources,\n+                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n+                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n+                expected=next(\n+                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n+                ),\n+            )\n+        )\n+\n+    return grouped\n+\n+\n+def compute_stats(\n+    grouped_diagnostics: list[GroupedDiagnostics], source: Source\n+) -> Statistics:\n+    if source == source.EXPECTED:\n+        # ty currently raises a false positive here due to incomplete enum.Flag support\n+        # see https://github.com/astral-sh/ty/issues/876\n+        num_errors = sum(\n+            [1 for g in grouped_diagnostics if source.EXPECTED in g.sources]  # ty:ignore[unsupported-operator]\n+        )\n+        return Statistics(\n+            true_positives=num_errors, false_positives=0, false_negatives=0\n+        )\n+\n+    def increment(statistics: Statistics, grouped: GroupedDiagnostics) -> Statistics:\n+        if (source in grouped.sources) and (Source.EXPECTED in grouped.sources):\n+            statistics.true_positives += 1\n+        elif source in grouped.sources:\n+            statistics.false_positives += 1\n+        else:\n+            statistics.false_negatives += 1\n+        return statistics\n+\n+    return reduce(increment, grouped_diagnostics, Statistics())\n+\n+\n+def render_grouped_diagnostics(\n+    grouped: list[GroupedDiagnostics], changed_only: bool = True\n+) -> str:\n+    if changed_only:\n+        grouped = [diag for diag in grouped if diag.changed]\n+    sorted_by_class = sorted(\n+        grouped,\n+        key=attrgetter(\"classification\"),\n+        reverse=True,\n+    )\n+\n+    lines = []\n+    for classification, group in groupby(\n+        sorted_by_class, key=attrgetter(\"classification\")\n+    ):\n+        group = list(group)\n+\n+        lines.append(f\"## {classification.value.replace('_', ' ').title()}s:\")\n+        lines.append(\"\")\n+        lines.append(\"```diff\")\n+\n+        for diag in group:\n+            lines.append(diag.display())\n+\n+        lines.append(\"```\")\n+\n+    return \"\\n\".join(lines)\n+\n+\n+def render_summary(grouped_diagnostics: list[GroupedDiagnostics]):\n+    def pct(value):\n+        return f\"{value:.2%}\"\n+\n+    def trend(value):\n+        if value == 0:\n+            return \"does not change\"\n+        return \"improves\" if value > 0 else \"regresses\"\n+\n+    old = compute_stats(grouped_diagnostics, source=Source.OLD)\n+    new = compute_stats(grouped_diagnostics, source=Source.NEW)\n+\n+    precision_delta = new.precision - old.precision\n+    recall_delta = new.recall - old.recall\n+\n+    table = dedent(\n+        f\"\"\"\n+        ## Typing Conformance\n+\n+        ### Summary\n+\n+        | Metric     | Old | New | \u0394 |\n+        |------------|-----|-----|---|\n+        | True Positives | {old.true_positives} | {new.true_positives} | {old.true_positives - new.true_positives} |\n+        | False Positives | {old.false_positives} | {new.false_positives} | {new.false_positives - old.false_positives} |\n+        | False Negatives | {old.false_negatives} | {new.false_negatives} | {new.false_negatives - old.false_negatives} |\n+        | Precision  | {old.precision:.2} | {new.precision:.2} | {pct(precision_delta)} |\n+        | Recall     | {old.recall:.2} | {new.recall:.2} | {pct(recall_delta)} |\n+        | Total      | {old.total} | {new.total} | {new.total - old.total} |\n+\n+        \"\"\"\n+    )\n+\n+    summary = (\n+        f\"Compared to the current merge base, this PR {trend(precision_delta)} precision \"\n+        f\"and {trend(recall_delta)} recall (TP: {new.true_positives - old.true_positives}, FP: {new.false_positives - old.false_positives}, FN: {new.false_negatives - old.false_negatives})).\"\n+    )",
      "comment": "> I'm fine with a jargony table and some documentation below it that explains the jargon :-)\r\n\r\nwe can always do this for now and iterate on it",
      "comment_id": 2683257725,
      "user": "AlexWaygood",
      "created_at": "2026-01-12T17:40:41Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2683257725"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 396,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,490 @@\n+\"\"\"\n+Run typing conformance tests and compare results between two ty versions.\n+\n+Examples:\n+    # Compare two specific ty versions\n+    %(prog)s --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Use local ty builds\n+    %(prog)s --old-ty ./target/debug/ty-old --new-ty ./target/debug/ty-new\n+\n+    # Custom test directory\n+    %(prog)s --target-path custom/tests --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Show all diagnostics (not just changed ones)\n+    %(prog)s --all --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from itertools import groupby\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+# The conformance tests include 4 types of errors:\n+# 1. Required errors (E): The type checker must raise an error on this line\n+# 2. Optional errors (E?): The type checker may raise an error on this line\n+# 3. Tagged errors ([tag]): The type checker must raise at most one error on any of the lines in a file with matching tags\n+# 4. Tagged multi-errors ([tag]+): The type checker should raise one or more errors on any of the tagged lines\n+# # This regex pattern parses the error lines in the conformance tests, but the following\n+# # implementation treats all errors as required errors.\n+CONFORMANCE_ERROR_PATTERN = re.compile(\n+    r\"\"\"\n+    \\#\\s*E                  # \"# E\" begins each error\n+    (?P<optional>\\?)?       # Optional '?' (E?) indicates that an error is optional\n+    (?:                     # An optional tag for errors that may appear on multiple lines at most once\n+        \\[\n+            (?P<tag>[^+\\]]+)    # identifier\n+            (?P<multi>\\+)?      # '+' indicates that an error may occur more than once on tagged lines\n+        \\]\n+    )?\n+    (?:\n+        \\s*:\\s*(?P<description>.*) # optional description\n+    )?\n+    \"\"\",\n+    re.VERBOSE,\n+)\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    def __str__(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self) -> str:\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Statistics:\n+    true_positives: int = 0\n+    false_positives: int = 0\n+    false_negatives: int = 0\n+\n+    @property\n+    def precision(self) -> float:\n+        if self.true_positives + self.false_positives > 0:\n+            return self.true_positives / (self.true_positives + self.false_positives)\n+        return 0.0\n+\n+    @property\n+    def recall(self) -> float:\n+        if self.true_positives + self.false_negatives > 0:\n+            return self.true_positives / (self.true_positives + self.false_negatives)\n+        else:\n+            return 0.0\n+\n+    @property\n+    def total(self) -> int:\n+        return self.true_positives + self.false_positives\n+\n+\n+def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n+    diagnostics: list[Diagnostic] = []\n+    for file in path.resolve().rglob(\"*.py\"):\n+        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+            if error := re.search(CONFORMANCE_ERROR_PATTERN, line):\n+                diagnostics.append(\n+                    Diagnostic(\n+                        check_name=\"conformance\",\n+                        description=error.group(\"description\")\n+                        or error.group(\"tag\")\n+                        or \"Missing\",\n+                        severity=\"major\",\n+                        fingerprint=None,\n+                        location=Location(\n+                            path=file.as_posix(),\n+                            positions=Positions(\n+                                begin=Position(\n+                                    line=idx,\n+                                    column=error.start(),\n+                                ),\n+                                end=Position(\n+                                    line=idx,\n+                                    column=error.end(),\n+                                ),\n+                            ),\n+                        ),\n+                        source=Source.EXPECTED,\n+                    )\n+                )\n+\n+    assert diagnostics, \"Failed to discover any expected diagnostics!\"\n+    return diagnostics\n+\n+\n+def collect_ty_diagnostics(\n+    ty_path: list[str],\n+    source: Source,\n+    target_path: str = \".\",\n+    python_version: str = \"3.12\",\n+) -> list[Diagnostic]:\n+    process = subprocess.run(\n+        [\n+            *ty_path,\n+            \"check\",\n+            f\"--python-version={python_version}\",\n+            \"--output-format=gitlab\",\n+            \"--exit-zero\",\n+            target_path,\n+        ],\n+        capture_output=True,\n+        text=True,\n+        check=True,\n+        timeout=15,\n+    )\n+\n+    if process.returncode != 0:\n+        print(process.stderr)\n+        raise RuntimeError(f\"ty check failed with exit code {process.returncode}\")\n+\n+    return [\n+        Diagnostic.from_gitlab_output(dct, source=source)\n+        for dct in json.loads(process.stdout)\n+    ]\n+\n+\n+def group_diagnostics_by_key(\n+    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+) -> list[GroupedDiagnostics]:\n+    diagnostics = [\n+        *old,\n+        *new,\n+        *expected,\n+    ]\n+    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+\n+    grouped = []\n+    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+        group = list(group)\n+        sources: Source = reduce(or_, (diag.source for diag in group))\n+        grouped.append(\n+            GroupedDiagnostics(\n+                key=key,\n+                sources=sources,\n+                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n+                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n+                expected=next(\n+                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n+                ),\n+            )\n+        )\n+\n+    return grouped\n+\n+\n+def compute_stats(\n+    grouped_diagnostics: list[GroupedDiagnostics], source: Source\n+) -> Statistics:\n+    if source == source.EXPECTED:\n+        # ty currently raises a false positive here due to incomplete enum.Flag support\n+        # see https://github.com/astral-sh/ty/issues/876\n+        num_errors = sum(\n+            [1 for g in grouped_diagnostics if source.EXPECTED in g.sources]  # ty:ignore[unsupported-operator]\n+        )\n+        return Statistics(\n+            true_positives=num_errors, false_positives=0, false_negatives=0\n+        )\n+\n+    def increment(statistics: Statistics, grouped: GroupedDiagnostics) -> Statistics:\n+        if (source in grouped.sources) and (Source.EXPECTED in grouped.sources):\n+            statistics.true_positives += 1\n+        elif source in grouped.sources:\n+            statistics.false_positives += 1\n+        else:\n+            statistics.false_negatives += 1\n+        return statistics\n+\n+    return reduce(increment, grouped_diagnostics, Statistics())\n+\n+\n+def render_grouped_diagnostics(\n+    grouped: list[GroupedDiagnostics], changed_only: bool = True\n+) -> str:\n+    if changed_only:\n+        grouped = [diag for diag in grouped if diag.changed]\n+    sorted_by_class = sorted(\n+        grouped,\n+        key=attrgetter(\"classification\"),\n+        reverse=True,\n+    )\n+\n+    lines = []\n+    for classification, group in groupby(\n+        sorted_by_class, key=attrgetter(\"classification\")\n+    ):\n+        group = list(group)\n+\n+        lines.append(f\"## {classification.value.replace('_', ' ').title()}s:\")\n+        lines.append(\"\")\n+        lines.append(\"```diff\")\n+\n+        for diag in group:\n+            lines.append(diag.display())\n+\n+        lines.append(\"```\")\n+\n+    return \"\\n\".join(lines)\n+\n+\n+def render_summary(grouped_diagnostics: list[GroupedDiagnostics]):\n+    def pct(value):\n+        return f\"{value:.2%}\"\n+\n+    def trend(value):\n+        if value == 0:\n+            return \"does not change\"\n+        return \"improves\" if value > 0 else \"regresses\"\n+\n+    old = compute_stats(grouped_diagnostics, source=Source.OLD)\n+    new = compute_stats(grouped_diagnostics, source=Source.NEW)\n+\n+    precision_delta = new.precision - old.precision\n+    recall_delta = new.recall - old.recall\n+\n+    table = dedent(\n+        f\"\"\"\n+        ## Typing Conformance\n+\n+        ### Summary\n+\n+        | Metric     | Old | New | \u0394 |\n+        |------------|-----|-----|---|\n+        | True Positives | {old.true_positives} | {new.true_positives} | {old.true_positives - new.true_positives} |\n+        | False Positives | {old.false_positives} | {new.false_positives} | {new.false_positives - old.false_positives} |\n+        | False Negatives | {old.false_negatives} | {new.false_negatives} | {new.false_negatives - old.false_negatives} |\n+        | Precision  | {old.precision:.2} | {new.precision:.2} | {pct(precision_delta)} |\n+        | Recall     | {old.recall:.2} | {new.recall:.2} | {pct(recall_delta)} |\n+        | Total      | {old.total} | {new.total} | {new.total - old.total} |\n+\n+        \"\"\"\n+    )\n+\n+    summary = (\n+        f\"Compared to the current merge base, this PR {trend(precision_delta)} precision \"\n+        f\"and {trend(recall_delta)} recall (TP: {new.true_positives - old.true_positives}, FP: {new.false_positives - old.false_positives}, FN: {new.false_negatives - old.false_negatives})).\"\n+    )",
      "comment": "This is what biomejs does https://github.com/biomejs/biome/pull/8361#issuecomment-3613423566\r\n\r\nand it does add nice emoji's for every row based on the change https://github.com/biomejs/biome/blob/f2445269b6a7bff5d81ee08a6a321bc229b61b50/xtask/coverage/src/results.rs#L37-L72",
      "comment_id": 2683329243,
      "user": "MichaReiser",
      "created_at": "2026-01-12T18:04:20Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2683329243"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 355,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,490 @@\n+\"\"\"\n+Run typing conformance tests and compare results between two ty versions.\n+\n+Examples:\n+    # Compare two specific ty versions\n+    %(prog)s --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Use local ty builds\n+    %(prog)s --old-ty ./target/debug/ty-old --new-ty ./target/debug/ty-new\n+\n+    # Custom test directory\n+    %(prog)s --target-path custom/tests --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Show all diagnostics (not just changed ones)\n+    %(prog)s --all --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from itertools import groupby\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+# The conformance tests include 4 types of errors:\n+# 1. Required errors (E): The type checker must raise an error on this line\n+# 2. Optional errors (E?): The type checker may raise an error on this line\n+# 3. Tagged errors ([tag]): The type checker must raise at most one error on any of the lines in a file with matching tags\n+# 4. Tagged multi-errors ([tag]+): The type checker should raise one or more errors on any of the tagged lines\n+# # This regex pattern parses the error lines in the conformance tests, but the following\n+# # implementation treats all errors as required errors.\n+CONFORMANCE_ERROR_PATTERN = re.compile(\n+    r\"\"\"\n+    \\#\\s*E                  # \"# E\" begins each error\n+    (?P<optional>\\?)?       # Optional '?' (E?) indicates that an error is optional\n+    (?:                     # An optional tag for errors that may appear on multiple lines at most once\n+        \\[\n+            (?P<tag>[^+\\]]+)    # identifier\n+            (?P<multi>\\+)?      # '+' indicates that an error may occur more than once on tagged lines\n+        \\]\n+    )?\n+    (?:\n+        \\s*:\\s*(?P<description>.*) # optional description\n+    )?\n+    \"\"\",\n+    re.VERBOSE,\n+)\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    def __str__(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self) -> str:\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Statistics:\n+    true_positives: int = 0\n+    false_positives: int = 0\n+    false_negatives: int = 0\n+\n+    @property\n+    def precision(self) -> float:\n+        if self.true_positives + self.false_positives > 0:\n+            return self.true_positives / (self.true_positives + self.false_positives)\n+        return 0.0\n+\n+    @property\n+    def recall(self) -> float:\n+        if self.true_positives + self.false_negatives > 0:\n+            return self.true_positives / (self.true_positives + self.false_negatives)\n+        else:\n+            return 0.0\n+\n+    @property\n+    def total(self) -> int:\n+        return self.true_positives + self.false_positives\n+\n+\n+def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n+    diagnostics: list[Diagnostic] = []\n+    for file in path.resolve().rglob(\"*.py\"):\n+        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+            if error := re.search(CONFORMANCE_ERROR_PATTERN, line):\n+                diagnostics.append(\n+                    Diagnostic(\n+                        check_name=\"conformance\",\n+                        description=error.group(\"description\")\n+                        or error.group(\"tag\")\n+                        or \"Missing\",\n+                        severity=\"major\",\n+                        fingerprint=None,\n+                        location=Location(\n+                            path=file.as_posix(),\n+                            positions=Positions(\n+                                begin=Position(\n+                                    line=idx,\n+                                    column=error.start(),\n+                                ),\n+                                end=Position(\n+                                    line=idx,\n+                                    column=error.end(),\n+                                ),\n+                            ),\n+                        ),\n+                        source=Source.EXPECTED,\n+                    )\n+                )\n+\n+    assert diagnostics, \"Failed to discover any expected diagnostics!\"\n+    return diagnostics\n+\n+\n+def collect_ty_diagnostics(\n+    ty_path: list[str],\n+    source: Source,\n+    target_path: str = \".\",\n+    python_version: str = \"3.12\",\n+) -> list[Diagnostic]:\n+    process = subprocess.run(\n+        [\n+            *ty_path,\n+            \"check\",\n+            f\"--python-version={python_version}\",\n+            \"--output-format=gitlab\",\n+            \"--exit-zero\",\n+            target_path,\n+        ],\n+        capture_output=True,\n+        text=True,\n+        check=True,\n+        timeout=15,\n+    )\n+\n+    if process.returncode != 0:\n+        print(process.stderr)\n+        raise RuntimeError(f\"ty check failed with exit code {process.returncode}\")\n+\n+    return [\n+        Diagnostic.from_gitlab_output(dct, source=source)\n+        for dct in json.loads(process.stdout)\n+    ]\n+\n+\n+def group_diagnostics_by_key(\n+    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+) -> list[GroupedDiagnostics]:\n+    diagnostics = [\n+        *old,\n+        *new,\n+        *expected,\n+    ]\n+    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+\n+    grouped = []\n+    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+        group = list(group)\n+        sources: Source = reduce(or_, (diag.source for diag in group))\n+        grouped.append(\n+            GroupedDiagnostics(\n+                key=key,\n+                sources=sources,\n+                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n+                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n+                expected=next(\n+                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n+                ),\n+            )\n+        )\n+\n+    return grouped\n+\n+\n+def compute_stats(\n+    grouped_diagnostics: list[GroupedDiagnostics], source: Source\n+) -> Statistics:\n+    if source == source.EXPECTED:\n+        # ty currently raises a false positive here due to incomplete enum.Flag support\n+        # see https://github.com/astral-sh/ty/issues/876\n+        num_errors = sum(\n+            [1 for g in grouped_diagnostics if source.EXPECTED in g.sources]  # ty:ignore[unsupported-operator]\n+        )\n+        return Statistics(\n+            true_positives=num_errors, false_positives=0, false_negatives=0\n+        )\n+\n+    def increment(statistics: Statistics, grouped: GroupedDiagnostics) -> Statistics:\n+        if (source in grouped.sources) and (Source.EXPECTED in grouped.sources):\n+            statistics.true_positives += 1\n+        elif source in grouped.sources:\n+            statistics.false_positives += 1\n+        else:\n+            statistics.false_negatives += 1\n+        return statistics\n+\n+    return reduce(increment, grouped_diagnostics, Statistics())\n+\n+\n+def render_grouped_diagnostics(\n+    grouped: list[GroupedDiagnostics], changed_only: bool = True\n+) -> str:\n+    if changed_only:\n+        grouped = [diag for diag in grouped if diag.changed]\n+    sorted_by_class = sorted(\n+        grouped,\n+        key=attrgetter(\"classification\"),\n+        reverse=True,\n+    )\n+\n+    lines = []\n+    for classification, group in groupby(\n+        sorted_by_class, key=attrgetter(\"classification\")\n+    ):\n+        group = list(group)\n+\n+        lines.append(f\"## {classification.value.replace('_', ' ').title()}s:\")\n+        lines.append(\"\")\n+        lines.append(\"```diff\")\n+\n+        for diag in group:\n+            lines.append(diag.display())\n+\n+        lines.append(\"```\")",
      "comment": "Along with those changes, I've also added a fourth category for new false negatives which are now rendered as \"True positives removed \ud83e\udee4\"",
      "comment_id": 2683736912,
      "user": "WillDuke",
      "created_at": "2026-01-12T20:03:04Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2683736912"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 396,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,490 @@\n+\"\"\"\n+Run typing conformance tests and compare results between two ty versions.\n+\n+Examples:\n+    # Compare two specific ty versions\n+    %(prog)s --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Use local ty builds\n+    %(prog)s --old-ty ./target/debug/ty-old --new-ty ./target/debug/ty-new\n+\n+    # Custom test directory\n+    %(prog)s --target-path custom/tests --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Show all diagnostics (not just changed ones)\n+    %(prog)s --all --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from itertools import groupby\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+# The conformance tests include 4 types of errors:\n+# 1. Required errors (E): The type checker must raise an error on this line\n+# 2. Optional errors (E?): The type checker may raise an error on this line\n+# 3. Tagged errors ([tag]): The type checker must raise at most one error on any of the lines in a file with matching tags\n+# 4. Tagged multi-errors ([tag]+): The type checker should raise one or more errors on any of the tagged lines\n+# # This regex pattern parses the error lines in the conformance tests, but the following\n+# # implementation treats all errors as required errors.\n+CONFORMANCE_ERROR_PATTERN = re.compile(\n+    r\"\"\"\n+    \\#\\s*E                  # \"# E\" begins each error\n+    (?P<optional>\\?)?       # Optional '?' (E?) indicates that an error is optional\n+    (?:                     # An optional tag for errors that may appear on multiple lines at most once\n+        \\[\n+            (?P<tag>[^+\\]]+)    # identifier\n+            (?P<multi>\\+)?      # '+' indicates that an error may occur more than once on tagged lines\n+        \\]\n+    )?\n+    (?:\n+        \\s*:\\s*(?P<description>.*) # optional description\n+    )?\n+    \"\"\",\n+    re.VERBOSE,\n+)\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    def __str__(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self) -> str:\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Statistics:\n+    true_positives: int = 0\n+    false_positives: int = 0\n+    false_negatives: int = 0\n+\n+    @property\n+    def precision(self) -> float:\n+        if self.true_positives + self.false_positives > 0:\n+            return self.true_positives / (self.true_positives + self.false_positives)\n+        return 0.0\n+\n+    @property\n+    def recall(self) -> float:\n+        if self.true_positives + self.false_negatives > 0:\n+            return self.true_positives / (self.true_positives + self.false_negatives)\n+        else:\n+            return 0.0\n+\n+    @property\n+    def total(self) -> int:\n+        return self.true_positives + self.false_positives\n+\n+\n+def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n+    diagnostics: list[Diagnostic] = []\n+    for file in path.resolve().rglob(\"*.py\"):\n+        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+            if error := re.search(CONFORMANCE_ERROR_PATTERN, line):\n+                diagnostics.append(\n+                    Diagnostic(\n+                        check_name=\"conformance\",\n+                        description=error.group(\"description\")\n+                        or error.group(\"tag\")\n+                        or \"Missing\",\n+                        severity=\"major\",\n+                        fingerprint=None,\n+                        location=Location(\n+                            path=file.as_posix(),\n+                            positions=Positions(\n+                                begin=Position(\n+                                    line=idx,\n+                                    column=error.start(),\n+                                ),\n+                                end=Position(\n+                                    line=idx,\n+                                    column=error.end(),\n+                                ),\n+                            ),\n+                        ),\n+                        source=Source.EXPECTED,\n+                    )\n+                )\n+\n+    assert diagnostics, \"Failed to discover any expected diagnostics!\"\n+    return diagnostics\n+\n+\n+def collect_ty_diagnostics(\n+    ty_path: list[str],\n+    source: Source,\n+    target_path: str = \".\",\n+    python_version: str = \"3.12\",\n+) -> list[Diagnostic]:\n+    process = subprocess.run(\n+        [\n+            *ty_path,\n+            \"check\",\n+            f\"--python-version={python_version}\",\n+            \"--output-format=gitlab\",\n+            \"--exit-zero\",\n+            target_path,\n+        ],\n+        capture_output=True,\n+        text=True,\n+        check=True,\n+        timeout=15,\n+    )\n+\n+    if process.returncode != 0:\n+        print(process.stderr)\n+        raise RuntimeError(f\"ty check failed with exit code {process.returncode}\")\n+\n+    return [\n+        Diagnostic.from_gitlab_output(dct, source=source)\n+        for dct in json.loads(process.stdout)\n+    ]\n+\n+\n+def group_diagnostics_by_key(\n+    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+) -> list[GroupedDiagnostics]:\n+    diagnostics = [\n+        *old,\n+        *new,\n+        *expected,\n+    ]\n+    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+\n+    grouped = []\n+    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+        group = list(group)\n+        sources: Source = reduce(or_, (diag.source for diag in group))\n+        grouped.append(\n+            GroupedDiagnostics(\n+                key=key,\n+                sources=sources,\n+                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n+                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n+                expected=next(\n+                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n+                ),\n+            )\n+        )\n+\n+    return grouped\n+\n+\n+def compute_stats(\n+    grouped_diagnostics: list[GroupedDiagnostics], source: Source\n+) -> Statistics:\n+    if source == source.EXPECTED:\n+        # ty currently raises a false positive here due to incomplete enum.Flag support\n+        # see https://github.com/astral-sh/ty/issues/876\n+        num_errors = sum(\n+            [1 for g in grouped_diagnostics if source.EXPECTED in g.sources]  # ty:ignore[unsupported-operator]\n+        )\n+        return Statistics(\n+            true_positives=num_errors, false_positives=0, false_negatives=0\n+        )\n+\n+    def increment(statistics: Statistics, grouped: GroupedDiagnostics) -> Statistics:\n+        if (source in grouped.sources) and (Source.EXPECTED in grouped.sources):\n+            statistics.true_positives += 1\n+        elif source in grouped.sources:\n+            statistics.false_positives += 1\n+        else:\n+            statistics.false_negatives += 1\n+        return statistics\n+\n+    return reduce(increment, grouped_diagnostics, Statistics())\n+\n+\n+def render_grouped_diagnostics(\n+    grouped: list[GroupedDiagnostics], changed_only: bool = True\n+) -> str:\n+    if changed_only:\n+        grouped = [diag for diag in grouped if diag.changed]\n+    sorted_by_class = sorted(\n+        grouped,\n+        key=attrgetter(\"classification\"),\n+        reverse=True,\n+    )\n+\n+    lines = []\n+    for classification, group in groupby(\n+        sorted_by_class, key=attrgetter(\"classification\")\n+    ):\n+        group = list(group)\n+\n+        lines.append(f\"## {classification.value.replace('_', ' ').title()}s:\")\n+        lines.append(\"\")\n+        lines.append(\"```diff\")\n+\n+        for diag in group:\n+            lines.append(diag.display())\n+\n+        lines.append(\"```\")\n+\n+    return \"\\n\".join(lines)\n+\n+\n+def render_summary(grouped_diagnostics: list[GroupedDiagnostics]):\n+    def pct(value):\n+        return f\"{value:.2%}\"\n+\n+    def trend(value):\n+        if value == 0:\n+            return \"does not change\"\n+        return \"improves\" if value > 0 else \"regresses\"\n+\n+    old = compute_stats(grouped_diagnostics, source=Source.OLD)\n+    new = compute_stats(grouped_diagnostics, source=Source.NEW)\n+\n+    precision_delta = new.precision - old.precision\n+    recall_delta = new.recall - old.recall\n+\n+    table = dedent(\n+        f\"\"\"\n+        ## Typing Conformance\n+\n+        ### Summary\n+\n+        | Metric     | Old | New | \u0394 |\n+        |------------|-----|-----|---|\n+        | True Positives | {old.true_positives} | {new.true_positives} | {old.true_positives - new.true_positives} |\n+        | False Positives | {old.false_positives} | {new.false_positives} | {new.false_positives - old.false_positives} |\n+        | False Negatives | {old.false_negatives} | {new.false_negatives} | {new.false_negatives - old.false_negatives} |\n+        | Precision  | {old.precision:.2} | {new.precision:.2} | {pct(precision_delta)} |\n+        | Recall     | {old.recall:.2} | {new.recall:.2} | {pct(recall_delta)} |\n+        | Total      | {old.total} | {new.total} | {new.total - old.total} |\n+\n+        \"\"\"\n+    )\n+\n+    summary = (\n+        f\"Compared to the current merge base, this PR {trend(precision_delta)} precision \"\n+        f\"and {trend(recall_delta)} recall (TP: {new.true_positives - old.true_positives}, FP: {new.false_positives - old.false_positives}, FN: {new.false_negatives - old.false_negatives})).\"\n+    )",
      "comment": "I've added an outcome column with the emojis and updated the summary sentence. Happy to iterate on it further.",
      "comment_id": 2683918882,
      "user": "WillDuke",
      "created_at": "2026-01-12T21:09:21Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2683918882"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 340,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,546 @@\n+\"\"\"\n+Run typing conformance tests and compare results between two ty versions.\n+\n+By default, this script will use `uv` to run the latest version of ty\n+as the new version with `uvx ty@latest`. This requires `uv` to be installed\n+and available in the system PATH.\n+\n+Examples:\n+    # Compare two specific ty versions\n+    %(prog)s --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Use local ty builds\n+    %(prog)s --old-ty ./target/debug/ty-old --new-ty ./target/debug/ty-new\n+\n+    # Custom test directory\n+    %(prog)s --target-path custom/tests --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Show all diagnostics (not just changed ones)\n+    %(prog)s --all --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from itertools import groupby\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+# The conformance tests include 4 types of errors:\n+# 1. Required errors (E): The type checker must raise an error on this line\n+# 2. Optional errors (E?): The type checker may raise an error on this line\n+# 3. Tagged errors (E[tag]): The type checker must raise at most one error on any of the lines in a file with matching tags\n+# 4. Tagged multi-errors (E[tag+]): The type checker should raise one or more errors on any of the tagged lines\n+# This regex pattern parses the error lines in the conformance tests, but the following\n+# implementation treats all errors as required errors.\n+CONFORMANCE_ERROR_PATTERN = re.compile(\n+    r\"\"\"\n+    \\#\\s*E                  # \"# E\" begins each error\n+    (?P<optional>\\?)?       # Optional '?' (E?) indicates that an error is optional\n+    (?:                     # An optional tag for errors that may appear on multiple lines at most once\n+        \\[\n+            (?P<tag>[^+\\]]+)    # identifier\n+            (?P<multi>\\+)?      # '+' indicates that an error may occur more than once on tagged lines\n+        \\]\n+    )?\n+    (?:\n+        \\s*:\\s*(?P<description>.*) # optional description\n+    )?\n+    \"\"\",\n+    re.VERBOSE,\n+)\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+    def into_title(self) -> str:\n+        match self:\n+            case Classification.TRUE_POSITIVE:\n+                return \"True positives added \ud83c\udf89\"\n+            case Classification.FALSE_POSITIVE:\n+                return \"False positives added \ud83e\udee4\"\n+            case Classification.TRUE_NEGATIVE:\n+                return \"False positives removed \ud83c\udf89\"\n+            case Classification.FALSE_NEGATIVE:\n+                return \"True positives removed \ud83e\udee4\"\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    def __str__(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self) -> str:\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Statistics:\n+    true_positives: int = 0\n+    false_positives: int = 0\n+    false_negatives: int = 0\n+\n+    @property\n+    def precision(self) -> float:\n+        if self.true_positives + self.false_positives > 0:\n+            return self.true_positives / (self.true_positives + self.false_positives)\n+        return 0.0\n+\n+    @property\n+    def recall(self) -> float:\n+        if self.true_positives + self.false_negatives > 0:\n+            return self.true_positives / (self.true_positives + self.false_negatives)\n+        else:\n+            return 0.0\n+\n+    @property\n+    def total(self) -> int:\n+        return self.true_positives + self.false_positives\n+\n+\n+def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n+    diagnostics: list[Diagnostic] = []\n+    for file in path.resolve().rglob(\"*.py\"):\n+        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+            if error := re.search(CONFORMANCE_ERROR_PATTERN, line):\n+                diagnostics.append(\n+                    Diagnostic(\n+                        check_name=\"conformance\",\n+                        description=error.group(\"description\")\n+                        or error.group(\"tag\")\n+                        or \"Missing\",\n+                        severity=\"major\",\n+                        fingerprint=None,\n+                        location=Location(\n+                            path=file.as_posix(),\n+                            positions=Positions(\n+                                begin=Position(\n+                                    line=idx,\n+                                    column=error.start(),\n+                                ),\n+                                end=Position(\n+                                    line=idx,\n+                                    column=error.end(),\n+                                ),\n+                            ),\n+                        ),\n+                        source=Source.EXPECTED,\n+                    )\n+                )\n+\n+    assert diagnostics, \"Failed to discover any expected diagnostics!\"\n+    return diagnostics\n+\n+\n+def collect_ty_diagnostics(\n+    ty_path: list[str],\n+    source: Source,\n+    tests_path: str = \".\",\n+    python_version: str = \"3.12\",\n+) -> list[Diagnostic]:\n+    process = subprocess.run(\n+        [\n+            *ty_path,\n+            \"check\",\n+            f\"--python-version={python_version}\",\n+            \"--output-format=gitlab\",\n+            \"--exit-zero\",\n+            tests_path,\n+        ],\n+        capture_output=True,\n+        text=True,\n+        check=True,\n+        timeout=15,\n+    )\n+\n+    if process.returncode != 0:\n+        print(process.stderr)\n+        raise RuntimeError(f\"ty check failed with exit code {process.returncode}\")\n+\n+    return [\n+        Diagnostic.from_gitlab_output(dct, source=source)\n+        for dct in json.loads(process.stdout)\n+    ]\n+\n+\n+def group_diagnostics_by_key(\n+    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+) -> list[GroupedDiagnostics]:\n+    diagnostics = [\n+        *old,\n+        *new,\n+        *expected,\n+    ]\n+    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+\n+    grouped = []\n+    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+        group = list(group)\n+        sources: Source = reduce(or_, (diag.source for diag in group))\n+        grouped.append(\n+            GroupedDiagnostics(\n+                key=key,\n+                sources=sources,\n+                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n+                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n+                expected=next(\n+                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n+                ),\n+            )\n+        )\n+\n+    return grouped\n+\n+\n+def compute_stats(\n+    grouped_diagnostics: list[GroupedDiagnostics], source: Source\n+) -> Statistics:\n+    if source == source.EXPECTED:\n+        # ty currently raises a false positive here due to incomplete enum.Flag support\n+        # see https://github.com/astral-sh/ty/issues/876\n+        num_errors = sum(\n+            [1 for g in grouped_diagnostics if source.EXPECTED in g.sources]  # ty:ignore[unsupported-operator]\n+        )\n+        return Statistics(\n+            true_positives=num_errors, false_positives=0, false_negatives=0\n+        )\n+\n+    def increment(statistics: Statistics, grouped: GroupedDiagnostics) -> Statistics:\n+        if (source in grouped.sources) and (Source.EXPECTED in grouped.sources):\n+            statistics.true_positives += 1\n+        elif source in grouped.sources:\n+            statistics.false_positives += 1\n+        else:\n+            statistics.false_negatives += 1",
      "comment": "Leaving a note to myself that I need to check if expected is present here",
      "comment_id": 2685606291,
      "user": "WillDuke",
      "created_at": "2026-01-13T09:29:14Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2685606291"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 380,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,546 @@\n+\"\"\"\n+Run typing conformance tests and compare results between two ty versions.\n+\n+By default, this script will use `uv` to run the latest version of ty\n+as the new version with `uvx ty@latest`. This requires `uv` to be installed\n+and available in the system PATH.\n+\n+Examples:\n+    # Compare two specific ty versions\n+    %(prog)s --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Use local ty builds\n+    %(prog)s --old-ty ./target/debug/ty-old --new-ty ./target/debug/ty-new\n+\n+    # Custom test directory\n+    %(prog)s --target-path custom/tests --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Show all diagnostics (not just changed ones)\n+    %(prog)s --all --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from itertools import groupby\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+# The conformance tests include 4 types of errors:\n+# 1. Required errors (E): The type checker must raise an error on this line\n+# 2. Optional errors (E?): The type checker may raise an error on this line\n+# 3. Tagged errors (E[tag]): The type checker must raise at most one error on any of the lines in a file with matching tags\n+# 4. Tagged multi-errors (E[tag+]): The type checker should raise one or more errors on any of the tagged lines\n+# This regex pattern parses the error lines in the conformance tests, but the following\n+# implementation treats all errors as required errors.\n+CONFORMANCE_ERROR_PATTERN = re.compile(\n+    r\"\"\"\n+    \\#\\s*E                  # \"# E\" begins each error\n+    (?P<optional>\\?)?       # Optional '?' (E?) indicates that an error is optional\n+    (?:                     # An optional tag for errors that may appear on multiple lines at most once\n+        \\[\n+            (?P<tag>[^+\\]]+)    # identifier\n+            (?P<multi>\\+)?      # '+' indicates that an error may occur more than once on tagged lines\n+        \\]\n+    )?\n+    (?:\n+        \\s*:\\s*(?P<description>.*) # optional description\n+    )?\n+    \"\"\",\n+    re.VERBOSE,\n+)\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+    def into_title(self) -> str:\n+        match self:\n+            case Classification.TRUE_POSITIVE:\n+                return \"True positives added \ud83c\udf89\"\n+            case Classification.FALSE_POSITIVE:\n+                return \"False positives added \ud83e\udee4\"\n+            case Classification.TRUE_NEGATIVE:\n+                return \"False positives removed \ud83c\udf89\"\n+            case Classification.FALSE_NEGATIVE:\n+                return \"True positives removed \ud83e\udee4\"\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    def __str__(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self) -> str:\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Statistics:\n+    true_positives: int = 0\n+    false_positives: int = 0\n+    false_negatives: int = 0\n+\n+    @property\n+    def precision(self) -> float:\n+        if self.true_positives + self.false_positives > 0:\n+            return self.true_positives / (self.true_positives + self.false_positives)\n+        return 0.0\n+\n+    @property\n+    def recall(self) -> float:\n+        if self.true_positives + self.false_negatives > 0:\n+            return self.true_positives / (self.true_positives + self.false_negatives)\n+        else:\n+            return 0.0\n+\n+    @property\n+    def total(self) -> int:\n+        return self.true_positives + self.false_positives\n+\n+\n+def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n+    diagnostics: list[Diagnostic] = []\n+    for file in path.resolve().rglob(\"*.py\"):\n+        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+            if error := re.search(CONFORMANCE_ERROR_PATTERN, line):\n+                diagnostics.append(\n+                    Diagnostic(\n+                        check_name=\"conformance\",\n+                        description=error.group(\"description\")\n+                        or error.group(\"tag\")\n+                        or \"Missing\",\n+                        severity=\"major\",\n+                        fingerprint=None,\n+                        location=Location(\n+                            path=file.as_posix(),\n+                            positions=Positions(\n+                                begin=Position(\n+                                    line=idx,\n+                                    column=error.start(),\n+                                ),\n+                                end=Position(\n+                                    line=idx,\n+                                    column=error.end(),\n+                                ),\n+                            ),\n+                        ),\n+                        source=Source.EXPECTED,\n+                    )\n+                )\n+\n+    assert diagnostics, \"Failed to discover any expected diagnostics!\"\n+    return diagnostics\n+\n+\n+def collect_ty_diagnostics(\n+    ty_path: list[str],\n+    source: Source,\n+    tests_path: str = \".\",\n+    python_version: str = \"3.12\",\n+) -> list[Diagnostic]:\n+    process = subprocess.run(\n+        [\n+            *ty_path,\n+            \"check\",\n+            f\"--python-version={python_version}\",\n+            \"--output-format=gitlab\",\n+            \"--exit-zero\",\n+            tests_path,\n+        ],\n+        capture_output=True,\n+        text=True,\n+        check=True,\n+        timeout=15,\n+    )\n+\n+    if process.returncode != 0:\n+        print(process.stderr)\n+        raise RuntimeError(f\"ty check failed with exit code {process.returncode}\")\n+\n+    return [\n+        Diagnostic.from_gitlab_output(dct, source=source)\n+        for dct in json.loads(process.stdout)\n+    ]\n+\n+\n+def group_diagnostics_by_key(\n+    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+) -> list[GroupedDiagnostics]:\n+    diagnostics = [\n+        *old,\n+        *new,\n+        *expected,\n+    ]\n+    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+\n+    grouped = []\n+    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+        group = list(group)\n+        sources: Source = reduce(or_, (diag.source for diag in group))\n+        grouped.append(\n+            GroupedDiagnostics(\n+                key=key,\n+                sources=sources,\n+                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n+                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n+                expected=next(\n+                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n+                ),\n+            )\n+        )\n+\n+    return grouped\n+\n+\n+def compute_stats(\n+    grouped_diagnostics: list[GroupedDiagnostics], source: Source\n+) -> Statistics:\n+    if source == source.EXPECTED:\n+        # ty currently raises a false positive here due to incomplete enum.Flag support\n+        # see https://github.com/astral-sh/ty/issues/876\n+        num_errors = sum(\n+            [1 for g in grouped_diagnostics if source.EXPECTED in g.sources]  # ty:ignore[unsupported-operator]\n+        )\n+        return Statistics(\n+            true_positives=num_errors, false_positives=0, false_negatives=0\n+        )\n+\n+    def increment(statistics: Statistics, grouped: GroupedDiagnostics) -> Statistics:\n+        if (source in grouped.sources) and (Source.EXPECTED in grouped.sources):\n+            statistics.true_positives += 1\n+        elif source in grouped.sources:\n+            statistics.false_positives += 1\n+        else:\n+            statistics.false_negatives += 1\n+        return statistics\n+\n+    return reduce(increment, grouped_diagnostics, Statistics())\n+\n+\n+def render_grouped_diagnostics(\n+    grouped: list[GroupedDiagnostics], changed_only: bool = True\n+) -> str:\n+    if changed_only:\n+        grouped = [diag for diag in grouped if diag.changed]\n+    sorted_by_class = sorted(\n+        grouped,\n+        key=attrgetter(\"classification\"),\n+        reverse=True,\n+    )\n+\n+    lines = []\n+    for classification, group in groupby(\n+        sorted_by_class, key=attrgetter(\"classification\")\n+    ):\n+        group = list(group)\n+\n+        lines.append(f\"## {classification.into_title()}:\")\n+        lines.append(\"\")\n+        lines.append(\"```diff\")\n+\n+        for diag in group:\n+            lines.append(diag.display())\n+\n+        lines.append(\"```\")\n+\n+    return \"\\n\".join(lines)\n+\n+\n+def diff_format(\n+    diff: float,\n+    greater_is_better: bool = True,\n+    neutral: bool = False,\n+    is_percentage: bool = False,\n+):",
      "comment": "```suggestion\r\ndef diff_format(\r\n    diff: float,\r\n    *,\r\n    greater_is_better: bool = True,\r\n    neutral: bool = False,\r\n    is_percentage: bool = False,\r\n):\r\n```",
      "comment_id": 2685936858,
      "user": "AlexWaygood",
      "created_at": "2026-01-13T11:11:15Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2685936858"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 438,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,546 @@\n+\"\"\"\n+Run typing conformance tests and compare results between two ty versions.\n+\n+By default, this script will use `uv` to run the latest version of ty\n+as the new version with `uvx ty@latest`. This requires `uv` to be installed\n+and available in the system PATH.\n+\n+Examples:\n+    # Compare two specific ty versions\n+    %(prog)s --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Use local ty builds\n+    %(prog)s --old-ty ./target/debug/ty-old --new-ty ./target/debug/ty-new\n+\n+    # Custom test directory\n+    %(prog)s --target-path custom/tests --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Show all diagnostics (not just changed ones)\n+    %(prog)s --all --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from itertools import groupby\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+# The conformance tests include 4 types of errors:\n+# 1. Required errors (E): The type checker must raise an error on this line\n+# 2. Optional errors (E?): The type checker may raise an error on this line\n+# 3. Tagged errors (E[tag]): The type checker must raise at most one error on any of the lines in a file with matching tags\n+# 4. Tagged multi-errors (E[tag+]): The type checker should raise one or more errors on any of the tagged lines\n+# This regex pattern parses the error lines in the conformance tests, but the following\n+# implementation treats all errors as required errors.\n+CONFORMANCE_ERROR_PATTERN = re.compile(\n+    r\"\"\"\n+    \\#\\s*E                  # \"# E\" begins each error\n+    (?P<optional>\\?)?       # Optional '?' (E?) indicates that an error is optional\n+    (?:                     # An optional tag for errors that may appear on multiple lines at most once\n+        \\[\n+            (?P<tag>[^+\\]]+)    # identifier\n+            (?P<multi>\\+)?      # '+' indicates that an error may occur more than once on tagged lines\n+        \\]\n+    )?\n+    (?:\n+        \\s*:\\s*(?P<description>.*) # optional description\n+    )?\n+    \"\"\",\n+    re.VERBOSE,\n+)\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+    def into_title(self) -> str:\n+        match self:\n+            case Classification.TRUE_POSITIVE:\n+                return \"True positives added \ud83c\udf89\"\n+            case Classification.FALSE_POSITIVE:\n+                return \"False positives added \ud83e\udee4\"\n+            case Classification.TRUE_NEGATIVE:\n+                return \"False positives removed \ud83c\udf89\"\n+            case Classification.FALSE_NEGATIVE:\n+                return \"True positives removed \ud83e\udee4\"\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    def __str__(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self) -> str:\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Statistics:\n+    true_positives: int = 0\n+    false_positives: int = 0\n+    false_negatives: int = 0\n+\n+    @property\n+    def precision(self) -> float:\n+        if self.true_positives + self.false_positives > 0:\n+            return self.true_positives / (self.true_positives + self.false_positives)\n+        return 0.0\n+\n+    @property\n+    def recall(self) -> float:\n+        if self.true_positives + self.false_negatives > 0:\n+            return self.true_positives / (self.true_positives + self.false_negatives)\n+        else:\n+            return 0.0\n+\n+    @property\n+    def total(self) -> int:\n+        return self.true_positives + self.false_positives\n+\n+\n+def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n+    diagnostics: list[Diagnostic] = []\n+    for file in path.resolve().rglob(\"*.py\"):\n+        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+            if error := re.search(CONFORMANCE_ERROR_PATTERN, line):\n+                diagnostics.append(\n+                    Diagnostic(\n+                        check_name=\"conformance\",\n+                        description=error.group(\"description\")\n+                        or error.group(\"tag\")\n+                        or \"Missing\",\n+                        severity=\"major\",\n+                        fingerprint=None,\n+                        location=Location(\n+                            path=file.as_posix(),\n+                            positions=Positions(\n+                                begin=Position(\n+                                    line=idx,\n+                                    column=error.start(),\n+                                ),\n+                                end=Position(\n+                                    line=idx,\n+                                    column=error.end(),\n+                                ),\n+                            ),\n+                        ),\n+                        source=Source.EXPECTED,\n+                    )\n+                )\n+\n+    assert diagnostics, \"Failed to discover any expected diagnostics!\"\n+    return diagnostics\n+\n+\n+def collect_ty_diagnostics(\n+    ty_path: list[str],\n+    source: Source,\n+    tests_path: str = \".\",\n+    python_version: str = \"3.12\",\n+) -> list[Diagnostic]:\n+    process = subprocess.run(\n+        [\n+            *ty_path,\n+            \"check\",\n+            f\"--python-version={python_version}\",\n+            \"--output-format=gitlab\",\n+            \"--exit-zero\",\n+            tests_path,\n+        ],\n+        capture_output=True,\n+        text=True,\n+        check=True,\n+        timeout=15,\n+    )\n+\n+    if process.returncode != 0:\n+        print(process.stderr)\n+        raise RuntimeError(f\"ty check failed with exit code {process.returncode}\")\n+\n+    return [\n+        Diagnostic.from_gitlab_output(dct, source=source)\n+        for dct in json.loads(process.stdout)\n+    ]\n+\n+\n+def group_diagnostics_by_key(\n+    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+) -> list[GroupedDiagnostics]:\n+    diagnostics = [\n+        *old,\n+        *new,\n+        *expected,\n+    ]\n+    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+\n+    grouped = []\n+    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+        group = list(group)\n+        sources: Source = reduce(or_, (diag.source for diag in group))\n+        grouped.append(\n+            GroupedDiagnostics(\n+                key=key,\n+                sources=sources,\n+                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n+                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n+                expected=next(\n+                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n+                ),\n+            )\n+        )\n+\n+    return grouped\n+\n+\n+def compute_stats(\n+    grouped_diagnostics: list[GroupedDiagnostics], source: Source\n+) -> Statistics:\n+    if source == source.EXPECTED:\n+        # ty currently raises a false positive here due to incomplete enum.Flag support\n+        # see https://github.com/astral-sh/ty/issues/876\n+        num_errors = sum(\n+            [1 for g in grouped_diagnostics if source.EXPECTED in g.sources]  # ty:ignore[unsupported-operator]\n+        )\n+        return Statistics(\n+            true_positives=num_errors, false_positives=0, false_negatives=0\n+        )\n+\n+    def increment(statistics: Statistics, grouped: GroupedDiagnostics) -> Statistics:\n+        if (source in grouped.sources) and (Source.EXPECTED in grouped.sources):\n+            statistics.true_positives += 1\n+        elif source in grouped.sources:\n+            statistics.false_positives += 1\n+        else:\n+            statistics.false_negatives += 1\n+        return statistics\n+\n+    return reduce(increment, grouped_diagnostics, Statistics())\n+\n+\n+def render_grouped_diagnostics(\n+    grouped: list[GroupedDiagnostics], changed_only: bool = True\n+) -> str:\n+    if changed_only:\n+        grouped = [diag for diag in grouped if diag.changed]\n+    sorted_by_class = sorted(\n+        grouped,\n+        key=attrgetter(\"classification\"),\n+        reverse=True,\n+    )\n+\n+    lines = []\n+    for classification, group in groupby(\n+        sorted_by_class, key=attrgetter(\"classification\")\n+    ):\n+        group = list(group)\n+\n+        lines.append(f\"## {classification.into_title()}:\")\n+        lines.append(\"\")\n+        lines.append(\"```diff\")\n+\n+        for diag in group:\n+            lines.append(diag.display())\n+\n+        lines.append(\"```\")\n+\n+    return \"\\n\".join(lines)\n+\n+\n+def diff_format(\n+    diff: float,\n+    greater_is_better: bool = True,\n+    neutral: bool = False,\n+    is_percentage: bool = False,\n+):\n+    increased = diff > 0\n+    good = \"\u2705\" if not neutral else \"\"\n+    bad = \"\u274c\" if not neutral else \"\"\n+    up = \"\u23eb\"\n+    down = \"\u23ec\"\n+\n+    match (greater_is_better, increased):\n+        case (True, True):\n+            return f\"{good}{up}\"\n+        case (False, True):\n+            return f\"{bad}{down}\"\n+        case (True, False):\n+            return f\"{bad}{down}\"\n+        case (False, False):\n+            return f\"{good}{up}\"\n+\n+\n+def render_summary(grouped_diagnostics: list[GroupedDiagnostics]):\n+    def format_metric(diff: float, old: float, new: float):\n+        if diff > 0:\n+            return f\"increased from {old:.2%} to {new:.2%}\"\n+        if diff < 0:\n+            return f\"decreased from {old:.2%} to {new:.2%}\"\n+        return f\"held steady at {old:.2%}\"\n+\n+    old = compute_stats(grouped_diagnostics, source=Source.OLD)\n+    new = compute_stats(grouped_diagnostics, source=Source.NEW)\n+\n+    precision_change = new.precision - old.precision\n+    recall_change = new.recall - old.recall\n+    true_pos_change = new.true_positives - old.true_positives\n+    false_pos_change = new.false_positives - old.false_positives\n+    false_neg_change = new.false_negatives - old.false_negatives\n+    total_change = new.total - old.total\n+\n+    true_pos_diff = diff_format(true_pos_change, greater_is_better=True)\n+    false_pos_diff = diff_format(false_pos_change, greater_is_better=False)\n+    false_neg_diff = diff_format(false_neg_change, greater_is_better=False)\n+    precision_diff = diff_format(\n+        precision_change, greater_is_better=True, is_percentage=True\n+    )\n+    recall_diff = diff_format(recall_change, greater_is_better=True, is_percentage=True)\n+    total_diff = diff_format(total_change, neutral=True)\n+\n+    table = dedent(\n+        f\"\"\"\n+        ## Typing Conformance\n+\n+        ### Summary\n+\n+        | Metric | Old | New | Diff | Outcome |\n+        |--------|-----|-----|------|---------|\n+        | True Positives  | {old.true_positives} | {new.true_positives} | {true_pos_change} | {true_pos_diff} |\n+        | False Positives | {old.false_positives} | {new.false_positives} | {false_pos_change} | {false_pos_diff} |\n+        | False Negatives | {old.false_negatives} | {new.false_negatives} | {false_neg_change} | {false_neg_diff} |\n+        | Precision | {old.precision:.2%} | {new.precision:.2%} | {precision_change:.2%} | {precision_diff} |\n+        | Recall | {old.recall:.2%} | {new.recall:.2%} | {recall_change:.2%} | {recall_diff} |\n+        | Total | {old.total} | {new.total} | {total_change} | {total_diff} |",
      "comment": "Is it good or bad if `Total` goes up? If neither, is it useful to have it at all?\r\n\r\nI find it a bit confusing currently, because a `Total` row at the bottom usually sums all of the above rows together if it's e.g. an excel spreadsheet. But here, that's not what it does (`(-1) + (-9) + 10 = 0`):\r\n\r\n<img width=\"1328\" height=\"786\" alt=\"image\" src=\"https://github.com/user-attachments/assets/7ab139cb-1b3a-46a7-a401-0db1fba6be1c\" />\r\n",
      "comment_id": 2685945406,
      "user": "AlexWaygood",
      "created_at": "2026-01-13T11:13:35Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2685945406"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 446,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,546 @@\n+\"\"\"\n+Run typing conformance tests and compare results between two ty versions.\n+\n+By default, this script will use `uv` to run the latest version of ty\n+as the new version with `uvx ty@latest`. This requires `uv` to be installed\n+and available in the system PATH.\n+\n+Examples:\n+    # Compare two specific ty versions\n+    %(prog)s --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Use local ty builds\n+    %(prog)s --old-ty ./target/debug/ty-old --new-ty ./target/debug/ty-new\n+\n+    # Custom test directory\n+    %(prog)s --target-path custom/tests --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Show all diagnostics (not just changed ones)\n+    %(prog)s --all --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from itertools import groupby\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+# The conformance tests include 4 types of errors:\n+# 1. Required errors (E): The type checker must raise an error on this line\n+# 2. Optional errors (E?): The type checker may raise an error on this line\n+# 3. Tagged errors (E[tag]): The type checker must raise at most one error on any of the lines in a file with matching tags\n+# 4. Tagged multi-errors (E[tag+]): The type checker should raise one or more errors on any of the tagged lines\n+# This regex pattern parses the error lines in the conformance tests, but the following\n+# implementation treats all errors as required errors.\n+CONFORMANCE_ERROR_PATTERN = re.compile(\n+    r\"\"\"\n+    \\#\\s*E                  # \"# E\" begins each error\n+    (?P<optional>\\?)?       # Optional '?' (E?) indicates that an error is optional\n+    (?:                     # An optional tag for errors that may appear on multiple lines at most once\n+        \\[\n+            (?P<tag>[^+\\]]+)    # identifier\n+            (?P<multi>\\+)?      # '+' indicates that an error may occur more than once on tagged lines\n+        \\]\n+    )?\n+    (?:\n+        \\s*:\\s*(?P<description>.*) # optional description\n+    )?\n+    \"\"\",\n+    re.VERBOSE,\n+)\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+    def into_title(self) -> str:\n+        match self:\n+            case Classification.TRUE_POSITIVE:\n+                return \"True positives added \ud83c\udf89\"\n+            case Classification.FALSE_POSITIVE:\n+                return \"False positives added \ud83e\udee4\"\n+            case Classification.TRUE_NEGATIVE:\n+                return \"False positives removed \ud83c\udf89\"\n+            case Classification.FALSE_NEGATIVE:\n+                return \"True positives removed \ud83e\udee4\"\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    def __str__(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self) -> str:\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Statistics:\n+    true_positives: int = 0\n+    false_positives: int = 0\n+    false_negatives: int = 0\n+\n+    @property\n+    def precision(self) -> float:\n+        if self.true_positives + self.false_positives > 0:\n+            return self.true_positives / (self.true_positives + self.false_positives)\n+        return 0.0\n+\n+    @property\n+    def recall(self) -> float:\n+        if self.true_positives + self.false_negatives > 0:\n+            return self.true_positives / (self.true_positives + self.false_negatives)\n+        else:\n+            return 0.0\n+\n+    @property\n+    def total(self) -> int:\n+        return self.true_positives + self.false_positives\n+\n+\n+def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n+    diagnostics: list[Diagnostic] = []\n+    for file in path.resolve().rglob(\"*.py\"):\n+        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+            if error := re.search(CONFORMANCE_ERROR_PATTERN, line):\n+                diagnostics.append(\n+                    Diagnostic(\n+                        check_name=\"conformance\",\n+                        description=error.group(\"description\")\n+                        or error.group(\"tag\")\n+                        or \"Missing\",\n+                        severity=\"major\",\n+                        fingerprint=None,\n+                        location=Location(\n+                            path=file.as_posix(),\n+                            positions=Positions(\n+                                begin=Position(\n+                                    line=idx,\n+                                    column=error.start(),\n+                                ),\n+                                end=Position(\n+                                    line=idx,\n+                                    column=error.end(),\n+                                ),\n+                            ),\n+                        ),\n+                        source=Source.EXPECTED,\n+                    )\n+                )\n+\n+    assert diagnostics, \"Failed to discover any expected diagnostics!\"\n+    return diagnostics\n+\n+\n+def collect_ty_diagnostics(\n+    ty_path: list[str],\n+    source: Source,\n+    tests_path: str = \".\",\n+    python_version: str = \"3.12\",\n+) -> list[Diagnostic]:\n+    process = subprocess.run(\n+        [\n+            *ty_path,\n+            \"check\",\n+            f\"--python-version={python_version}\",\n+            \"--output-format=gitlab\",\n+            \"--exit-zero\",\n+            tests_path,\n+        ],\n+        capture_output=True,\n+        text=True,\n+        check=True,\n+        timeout=15,\n+    )\n+\n+    if process.returncode != 0:\n+        print(process.stderr)\n+        raise RuntimeError(f\"ty check failed with exit code {process.returncode}\")\n+\n+    return [\n+        Diagnostic.from_gitlab_output(dct, source=source)\n+        for dct in json.loads(process.stdout)\n+    ]\n+\n+\n+def group_diagnostics_by_key(\n+    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+) -> list[GroupedDiagnostics]:\n+    diagnostics = [\n+        *old,\n+        *new,\n+        *expected,\n+    ]\n+    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+\n+    grouped = []\n+    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+        group = list(group)\n+        sources: Source = reduce(or_, (diag.source for diag in group))\n+        grouped.append(\n+            GroupedDiagnostics(\n+                key=key,\n+                sources=sources,\n+                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n+                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n+                expected=next(\n+                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n+                ),\n+            )\n+        )\n+\n+    return grouped\n+\n+\n+def compute_stats(\n+    grouped_diagnostics: list[GroupedDiagnostics], source: Source\n+) -> Statistics:\n+    if source == source.EXPECTED:\n+        # ty currently raises a false positive here due to incomplete enum.Flag support\n+        # see https://github.com/astral-sh/ty/issues/876\n+        num_errors = sum(\n+            [1 for g in grouped_diagnostics if source.EXPECTED in g.sources]  # ty:ignore[unsupported-operator]\n+        )\n+        return Statistics(\n+            true_positives=num_errors, false_positives=0, false_negatives=0\n+        )\n+\n+    def increment(statistics: Statistics, grouped: GroupedDiagnostics) -> Statistics:\n+        if (source in grouped.sources) and (Source.EXPECTED in grouped.sources):\n+            statistics.true_positives += 1\n+        elif source in grouped.sources:\n+            statistics.false_positives += 1\n+        else:\n+            statistics.false_negatives += 1\n+        return statistics\n+\n+    return reduce(increment, grouped_diagnostics, Statistics())\n+\n+\n+def render_grouped_diagnostics(\n+    grouped: list[GroupedDiagnostics], changed_only: bool = True\n+) -> str:\n+    if changed_only:\n+        grouped = [diag for diag in grouped if diag.changed]\n+    sorted_by_class = sorted(\n+        grouped,\n+        key=attrgetter(\"classification\"),\n+        reverse=True,\n+    )\n+\n+    lines = []\n+    for classification, group in groupby(\n+        sorted_by_class, key=attrgetter(\"classification\")\n+    ):\n+        group = list(group)\n+\n+        lines.append(f\"## {classification.into_title()}:\")\n+        lines.append(\"\")\n+        lines.append(\"```diff\")\n+\n+        for diag in group:\n+            lines.append(diag.display())\n+\n+        lines.append(\"```\")\n+\n+    return \"\\n\".join(lines)\n+\n+\n+def diff_format(\n+    diff: float,\n+    greater_is_better: bool = True,\n+    neutral: bool = False,\n+    is_percentage: bool = False,\n+):\n+    increased = diff > 0\n+    good = \"\u2705\" if not neutral else \"\"\n+    bad = \"\u274c\" if not neutral else \"\"\n+    up = \"\u23eb\"\n+    down = \"\u23ec\"\n+\n+    match (greater_is_better, increased):\n+        case (True, True):\n+            return f\"{good}{up}\"\n+        case (False, True):\n+            return f\"{bad}{down}\"\n+        case (True, False):\n+            return f\"{bad}{down}\"\n+        case (False, False):\n+            return f\"{good}{up}\"\n+\n+\n+def render_summary(grouped_diagnostics: list[GroupedDiagnostics]):\n+    def format_metric(diff: float, old: float, new: float):\n+        if diff > 0:\n+            return f\"increased from {old:.2%} to {new:.2%}\"\n+        if diff < 0:\n+            return f\"decreased from {old:.2%} to {new:.2%}\"\n+        return f\"held steady at {old:.2%}\"\n+\n+    old = compute_stats(grouped_diagnostics, source=Source.OLD)\n+    new = compute_stats(grouped_diagnostics, source=Source.NEW)\n+\n+    precision_change = new.precision - old.precision\n+    recall_change = new.recall - old.recall\n+    true_pos_change = new.true_positives - old.true_positives\n+    false_pos_change = new.false_positives - old.false_positives\n+    false_neg_change = new.false_negatives - old.false_negatives\n+    total_change = new.total - old.total\n+\n+    true_pos_diff = diff_format(true_pos_change, greater_is_better=True)\n+    false_pos_diff = diff_format(false_pos_change, greater_is_better=False)\n+    false_neg_diff = diff_format(false_neg_change, greater_is_better=False)\n+    precision_diff = diff_format(\n+        precision_change, greater_is_better=True, is_percentage=True\n+    )\n+    recall_diff = diff_format(recall_change, greater_is_better=True, is_percentage=True)\n+    total_diff = diff_format(total_change, neutral=True)\n+\n+    table = dedent(\n+        f\"\"\"\n+        ## Typing Conformance\n+\n+        ### Summary\n+\n+        | Metric | Old | New | Diff | Outcome |\n+        |--------|-----|-----|------|---------|\n+        | True Positives  | {old.true_positives} | {new.true_positives} | {true_pos_change} | {true_pos_diff} |\n+        | False Positives | {old.false_positives} | {new.false_positives} | {false_pos_change} | {false_pos_diff} |\n+        | False Negatives | {old.false_negatives} | {new.false_negatives} | {false_neg_change} | {false_neg_diff} |\n+        | Precision | {old.precision:.2%} | {new.precision:.2%} | {precision_change:.2%} | {precision_diff} |\n+        | Recall | {old.recall:.2%} | {new.recall:.2%} | {recall_change:.2%} | {recall_diff} |\n+        | Total | {old.total} | {new.total} | {total_change} | {total_diff} |\n+\n+        \"\"\"\n+    )\n+\n+    summary = (\n+        f\"The percentage of diagnostics emitted that were true positives\"\n+        f\" {format_metric(precision_change, old.precision, new.precision)},\"\n+        \" and the percentage of true positives that received a diagnostic\"",
      "comment": "```suggestion\r\n        \" and the percentage of expected errors that received a diagnostic\"\r\n```",
      "comment_id": 2685948195,
      "user": "AlexWaygood",
      "created_at": "2026-01-13T11:14:24Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2685948195"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 438,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,546 @@\n+\"\"\"\n+Run typing conformance tests and compare results between two ty versions.\n+\n+By default, this script will use `uv` to run the latest version of ty\n+as the new version with `uvx ty@latest`. This requires `uv` to be installed\n+and available in the system PATH.\n+\n+Examples:\n+    # Compare two specific ty versions\n+    %(prog)s --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Use local ty builds\n+    %(prog)s --old-ty ./target/debug/ty-old --new-ty ./target/debug/ty-new\n+\n+    # Custom test directory\n+    %(prog)s --target-path custom/tests --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Show all diagnostics (not just changed ones)\n+    %(prog)s --all --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from itertools import groupby\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+# The conformance tests include 4 types of errors:\n+# 1. Required errors (E): The type checker must raise an error on this line\n+# 2. Optional errors (E?): The type checker may raise an error on this line\n+# 3. Tagged errors (E[tag]): The type checker must raise at most one error on any of the lines in a file with matching tags\n+# 4. Tagged multi-errors (E[tag+]): The type checker should raise one or more errors on any of the tagged lines\n+# This regex pattern parses the error lines in the conformance tests, but the following\n+# implementation treats all errors as required errors.\n+CONFORMANCE_ERROR_PATTERN = re.compile(\n+    r\"\"\"\n+    \\#\\s*E                  # \"# E\" begins each error\n+    (?P<optional>\\?)?       # Optional '?' (E?) indicates that an error is optional\n+    (?:                     # An optional tag for errors that may appear on multiple lines at most once\n+        \\[\n+            (?P<tag>[^+\\]]+)    # identifier\n+            (?P<multi>\\+)?      # '+' indicates that an error may occur more than once on tagged lines\n+        \\]\n+    )?\n+    (?:\n+        \\s*:\\s*(?P<description>.*) # optional description\n+    )?\n+    \"\"\",\n+    re.VERBOSE,\n+)\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+    def into_title(self) -> str:\n+        match self:\n+            case Classification.TRUE_POSITIVE:\n+                return \"True positives added \ud83c\udf89\"\n+            case Classification.FALSE_POSITIVE:\n+                return \"False positives added \ud83e\udee4\"\n+            case Classification.TRUE_NEGATIVE:\n+                return \"False positives removed \ud83c\udf89\"\n+            case Classification.FALSE_NEGATIVE:\n+                return \"True positives removed \ud83e\udee4\"\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    def __str__(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self) -> str:\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Statistics:\n+    true_positives: int = 0\n+    false_positives: int = 0\n+    false_negatives: int = 0\n+\n+    @property\n+    def precision(self) -> float:\n+        if self.true_positives + self.false_positives > 0:\n+            return self.true_positives / (self.true_positives + self.false_positives)\n+        return 0.0\n+\n+    @property\n+    def recall(self) -> float:\n+        if self.true_positives + self.false_negatives > 0:\n+            return self.true_positives / (self.true_positives + self.false_negatives)\n+        else:\n+            return 0.0\n+\n+    @property\n+    def total(self) -> int:\n+        return self.true_positives + self.false_positives\n+\n+\n+def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n+    diagnostics: list[Diagnostic] = []\n+    for file in path.resolve().rglob(\"*.py\"):\n+        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+            if error := re.search(CONFORMANCE_ERROR_PATTERN, line):\n+                diagnostics.append(\n+                    Diagnostic(\n+                        check_name=\"conformance\",\n+                        description=error.group(\"description\")\n+                        or error.group(\"tag\")\n+                        or \"Missing\",\n+                        severity=\"major\",\n+                        fingerprint=None,\n+                        location=Location(\n+                            path=file.as_posix(),\n+                            positions=Positions(\n+                                begin=Position(\n+                                    line=idx,\n+                                    column=error.start(),\n+                                ),\n+                                end=Position(\n+                                    line=idx,\n+                                    column=error.end(),\n+                                ),\n+                            ),\n+                        ),\n+                        source=Source.EXPECTED,\n+                    )\n+                )\n+\n+    assert diagnostics, \"Failed to discover any expected diagnostics!\"\n+    return diagnostics\n+\n+\n+def collect_ty_diagnostics(\n+    ty_path: list[str],\n+    source: Source,\n+    tests_path: str = \".\",\n+    python_version: str = \"3.12\",\n+) -> list[Diagnostic]:\n+    process = subprocess.run(\n+        [\n+            *ty_path,\n+            \"check\",\n+            f\"--python-version={python_version}\",\n+            \"--output-format=gitlab\",\n+            \"--exit-zero\",\n+            tests_path,\n+        ],\n+        capture_output=True,\n+        text=True,\n+        check=True,\n+        timeout=15,\n+    )\n+\n+    if process.returncode != 0:\n+        print(process.stderr)\n+        raise RuntimeError(f\"ty check failed with exit code {process.returncode}\")\n+\n+    return [\n+        Diagnostic.from_gitlab_output(dct, source=source)\n+        for dct in json.loads(process.stdout)\n+    ]\n+\n+\n+def group_diagnostics_by_key(\n+    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+) -> list[GroupedDiagnostics]:\n+    diagnostics = [\n+        *old,\n+        *new,\n+        *expected,\n+    ]\n+    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+\n+    grouped = []\n+    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+        group = list(group)\n+        sources: Source = reduce(or_, (diag.source for diag in group))\n+        grouped.append(\n+            GroupedDiagnostics(\n+                key=key,\n+                sources=sources,\n+                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n+                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n+                expected=next(\n+                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n+                ),\n+            )\n+        )\n+\n+    return grouped\n+\n+\n+def compute_stats(\n+    grouped_diagnostics: list[GroupedDiagnostics], source: Source\n+) -> Statistics:\n+    if source == source.EXPECTED:\n+        # ty currently raises a false positive here due to incomplete enum.Flag support\n+        # see https://github.com/astral-sh/ty/issues/876\n+        num_errors = sum(\n+            [1 for g in grouped_diagnostics if source.EXPECTED in g.sources]  # ty:ignore[unsupported-operator]\n+        )\n+        return Statistics(\n+            true_positives=num_errors, false_positives=0, false_negatives=0\n+        )\n+\n+    def increment(statistics: Statistics, grouped: GroupedDiagnostics) -> Statistics:\n+        if (source in grouped.sources) and (Source.EXPECTED in grouped.sources):\n+            statistics.true_positives += 1\n+        elif source in grouped.sources:\n+            statistics.false_positives += 1\n+        else:\n+            statistics.false_negatives += 1\n+        return statistics\n+\n+    return reduce(increment, grouped_diagnostics, Statistics())\n+\n+\n+def render_grouped_diagnostics(\n+    grouped: list[GroupedDiagnostics], changed_only: bool = True\n+) -> str:\n+    if changed_only:\n+        grouped = [diag for diag in grouped if diag.changed]\n+    sorted_by_class = sorted(\n+        grouped,\n+        key=attrgetter(\"classification\"),\n+        reverse=True,\n+    )\n+\n+    lines = []\n+    for classification, group in groupby(\n+        sorted_by_class, key=attrgetter(\"classification\")\n+    ):\n+        group = list(group)\n+\n+        lines.append(f\"## {classification.into_title()}:\")\n+        lines.append(\"\")\n+        lines.append(\"```diff\")\n+\n+        for diag in group:\n+            lines.append(diag.display())\n+\n+        lines.append(\"```\")\n+\n+    return \"\\n\".join(lines)\n+\n+\n+def diff_format(\n+    diff: float,\n+    greater_is_better: bool = True,\n+    neutral: bool = False,\n+    is_percentage: bool = False,\n+):\n+    increased = diff > 0\n+    good = \"\u2705\" if not neutral else \"\"\n+    bad = \"\u274c\" if not neutral else \"\"\n+    up = \"\u23eb\"\n+    down = \"\u23ec\"\n+\n+    match (greater_is_better, increased):\n+        case (True, True):\n+            return f\"{good}{up}\"\n+        case (False, True):\n+            return f\"{bad}{down}\"\n+        case (True, False):\n+            return f\"{bad}{down}\"\n+        case (False, False):\n+            return f\"{good}{up}\"\n+\n+\n+def render_summary(grouped_diagnostics: list[GroupedDiagnostics]):\n+    def format_metric(diff: float, old: float, new: float):\n+        if diff > 0:\n+            return f\"increased from {old:.2%} to {new:.2%}\"\n+        if diff < 0:\n+            return f\"decreased from {old:.2%} to {new:.2%}\"\n+        return f\"held steady at {old:.2%}\"\n+\n+    old = compute_stats(grouped_diagnostics, source=Source.OLD)\n+    new = compute_stats(grouped_diagnostics, source=Source.NEW)\n+\n+    precision_change = new.precision - old.precision\n+    recall_change = new.recall - old.recall\n+    true_pos_change = new.true_positives - old.true_positives\n+    false_pos_change = new.false_positives - old.false_positives\n+    false_neg_change = new.false_negatives - old.false_negatives\n+    total_change = new.total - old.total\n+\n+    true_pos_diff = diff_format(true_pos_change, greater_is_better=True)\n+    false_pos_diff = diff_format(false_pos_change, greater_is_better=False)\n+    false_neg_diff = diff_format(false_neg_change, greater_is_better=False)\n+    precision_diff = diff_format(\n+        precision_change, greater_is_better=True, is_percentage=True\n+    )\n+    recall_diff = diff_format(recall_change, greater_is_better=True, is_percentage=True)\n+    total_diff = diff_format(total_change, neutral=True)\n+\n+    table = dedent(\n+        f\"\"\"\n+        ## Typing Conformance\n+\n+        ### Summary\n+\n+        | Metric | Old | New | Diff | Outcome |\n+        |--------|-----|-----|------|---------|\n+        | True Positives  | {old.true_positives} | {new.true_positives} | {true_pos_change} | {true_pos_diff} |\n+        | False Positives | {old.false_positives} | {new.false_positives} | {false_pos_change} | {false_pos_diff} |\n+        | False Negatives | {old.false_negatives} | {new.false_negatives} | {false_neg_change} | {false_neg_diff} |\n+        | Precision | {old.precision:.2%} | {new.precision:.2%} | {precision_change:.2%} | {precision_diff} |\n+        | Recall | {old.recall:.2%} | {new.recall:.2%} | {recall_change:.2%} | {recall_diff} |\n+        | Total | {old.total} | {new.total} | {total_change} | {total_diff} |",
      "comment": "I think I'd also find the table easier to read if the up/down arrow came first -- maybe something like this?\r\n\r\n<img width=\"1272\" height=\"936\" alt=\"image\" src=\"https://github.com/user-attachments/assets/597def52-ab64-4da6-bbc6-9b4441eee14b\" />",
      "comment_id": 2685973334,
      "user": "AlexWaygood",
      "created_at": "2026-01-13T11:22:06Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2685973334"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 329,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,546 @@\n+\"\"\"\n+Run typing conformance tests and compare results between two ty versions.\n+\n+By default, this script will use `uv` to run the latest version of ty\n+as the new version with `uvx ty@latest`. This requires `uv` to be installed\n+and available in the system PATH.\n+\n+Examples:\n+    # Compare two specific ty versions\n+    %(prog)s --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Use local ty builds\n+    %(prog)s --old-ty ./target/debug/ty-old --new-ty ./target/debug/ty-new\n+\n+    # Custom test directory\n+    %(prog)s --target-path custom/tests --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Show all diagnostics (not just changed ones)\n+    %(prog)s --all --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from itertools import groupby\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+# The conformance tests include 4 types of errors:\n+# 1. Required errors (E): The type checker must raise an error on this line\n+# 2. Optional errors (E?): The type checker may raise an error on this line\n+# 3. Tagged errors (E[tag]): The type checker must raise at most one error on any of the lines in a file with matching tags\n+# 4. Tagged multi-errors (E[tag+]): The type checker should raise one or more errors on any of the tagged lines\n+# This regex pattern parses the error lines in the conformance tests, but the following\n+# implementation treats all errors as required errors.\n+CONFORMANCE_ERROR_PATTERN = re.compile(\n+    r\"\"\"\n+    \\#\\s*E                  # \"# E\" begins each error\n+    (?P<optional>\\?)?       # Optional '?' (E?) indicates that an error is optional\n+    (?:                     # An optional tag for errors that may appear on multiple lines at most once\n+        \\[\n+            (?P<tag>[^+\\]]+)    # identifier\n+            (?P<multi>\\+)?      # '+' indicates that an error may occur more than once on tagged lines\n+        \\]\n+    )?\n+    (?:\n+        \\s*:\\s*(?P<description>.*) # optional description\n+    )?\n+    \"\"\",\n+    re.VERBOSE,\n+)\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+    def into_title(self) -> str:\n+        match self:\n+            case Classification.TRUE_POSITIVE:\n+                return \"True positives added \ud83c\udf89\"\n+            case Classification.FALSE_POSITIVE:\n+                return \"False positives added \ud83e\udee4\"\n+            case Classification.TRUE_NEGATIVE:\n+                return \"False positives removed \ud83c\udf89\"\n+            case Classification.FALSE_NEGATIVE:\n+                return \"True positives removed \ud83e\udee4\"\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    def __str__(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self) -> str:\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Statistics:\n+    true_positives: int = 0\n+    false_positives: int = 0\n+    false_negatives: int = 0\n+\n+    @property\n+    def precision(self) -> float:\n+        if self.true_positives + self.false_positives > 0:\n+            return self.true_positives / (self.true_positives + self.false_positives)\n+        return 0.0\n+\n+    @property\n+    def recall(self) -> float:\n+        if self.true_positives + self.false_negatives > 0:\n+            return self.true_positives / (self.true_positives + self.false_negatives)\n+        else:\n+            return 0.0\n+\n+    @property\n+    def total(self) -> int:\n+        return self.true_positives + self.false_positives\n+\n+\n+def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n+    diagnostics: list[Diagnostic] = []\n+    for file in path.resolve().rglob(\"*.py\"):\n+        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+            if error := re.search(CONFORMANCE_ERROR_PATTERN, line):\n+                diagnostics.append(\n+                    Diagnostic(\n+                        check_name=\"conformance\",\n+                        description=error.group(\"description\")\n+                        or error.group(\"tag\")\n+                        or \"Missing\",\n+                        severity=\"major\",\n+                        fingerprint=None,\n+                        location=Location(\n+                            path=file.as_posix(),\n+                            positions=Positions(\n+                                begin=Position(\n+                                    line=idx,\n+                                    column=error.start(),\n+                                ),\n+                                end=Position(\n+                                    line=idx,\n+                                    column=error.end(),\n+                                ),\n+                            ),\n+                        ),\n+                        source=Source.EXPECTED,\n+                    )\n+                )\n+\n+    assert diagnostics, \"Failed to discover any expected diagnostics!\"\n+    return diagnostics\n+\n+\n+def collect_ty_diagnostics(\n+    ty_path: list[str],\n+    source: Source,\n+    tests_path: str = \".\",\n+    python_version: str = \"3.12\",\n+) -> list[Diagnostic]:\n+    process = subprocess.run(\n+        [\n+            *ty_path,\n+            \"check\",\n+            f\"--python-version={python_version}\",\n+            \"--output-format=gitlab\",\n+            \"--exit-zero\",\n+            tests_path,\n+        ],\n+        capture_output=True,\n+        text=True,\n+        check=True,\n+        timeout=15,\n+    )\n+\n+    if process.returncode != 0:\n+        print(process.stderr)\n+        raise RuntimeError(f\"ty check failed with exit code {process.returncode}\")\n+\n+    return [\n+        Diagnostic.from_gitlab_output(dct, source=source)\n+        for dct in json.loads(process.stdout)\n+    ]\n+\n+\n+def group_diagnostics_by_key(\n+    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+) -> list[GroupedDiagnostics]:\n+    diagnostics = [\n+        *old,\n+        *new,\n+        *expected,\n+    ]\n+    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+\n+    grouped = []\n+    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+        group = list(group)\n+        sources: Source = reduce(or_, (diag.source for diag in group))\n+        grouped.append(\n+            GroupedDiagnostics(\n+                key=key,\n+                sources=sources,\n+                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n+                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n+                expected=next(\n+                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n+                ),\n+            )\n+        )\n+\n+    return grouped\n+\n+\n+def compute_stats(\n+    grouped_diagnostics: list[GroupedDiagnostics], source: Source\n+) -> Statistics:\n+    if source == source.EXPECTED:\n+        # ty currently raises a false positive here due to incomplete enum.Flag support\n+        # see https://github.com/astral-sh/ty/issues/876\n+        num_errors = sum(\n+            [1 for g in grouped_diagnostics if source.EXPECTED in g.sources]  # ty:ignore[unsupported-operator]\n+        )",
      "comment": "nit\r\n\r\n```suggestion\r\n        num_errors = sum(\r\n            1 for g in grouped_diagnostics if source.EXPECTED in g.sources  # ty:ignore[unsupported-operator]\r\n        )\r\n```",
      "comment_id": 2685974337,
      "user": "AlexWaygood",
      "created_at": "2026-01-13T11:22:22Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2685974337"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 438,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,546 @@\n+\"\"\"\n+Run typing conformance tests and compare results between two ty versions.\n+\n+By default, this script will use `uv` to run the latest version of ty\n+as the new version with `uvx ty@latest`. This requires `uv` to be installed\n+and available in the system PATH.\n+\n+Examples:\n+    # Compare two specific ty versions\n+    %(prog)s --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Use local ty builds\n+    %(prog)s --old-ty ./target/debug/ty-old --new-ty ./target/debug/ty-new\n+\n+    # Custom test directory\n+    %(prog)s --target-path custom/tests --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Show all diagnostics (not just changed ones)\n+    %(prog)s --all --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from itertools import groupby\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+# The conformance tests include 4 types of errors:\n+# 1. Required errors (E): The type checker must raise an error on this line\n+# 2. Optional errors (E?): The type checker may raise an error on this line\n+# 3. Tagged errors (E[tag]): The type checker must raise at most one error on any of the lines in a file with matching tags\n+# 4. Tagged multi-errors (E[tag+]): The type checker should raise one or more errors on any of the tagged lines\n+# This regex pattern parses the error lines in the conformance tests, but the following\n+# implementation treats all errors as required errors.\n+CONFORMANCE_ERROR_PATTERN = re.compile(\n+    r\"\"\"\n+    \\#\\s*E                  # \"# E\" begins each error\n+    (?P<optional>\\?)?       # Optional '?' (E?) indicates that an error is optional\n+    (?:                     # An optional tag for errors that may appear on multiple lines at most once\n+        \\[\n+            (?P<tag>[^+\\]]+)    # identifier\n+            (?P<multi>\\+)?      # '+' indicates that an error may occur more than once on tagged lines\n+        \\]\n+    )?\n+    (?:\n+        \\s*:\\s*(?P<description>.*) # optional description\n+    )?\n+    \"\"\",\n+    re.VERBOSE,\n+)\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+    def into_title(self) -> str:\n+        match self:\n+            case Classification.TRUE_POSITIVE:\n+                return \"True positives added \ud83c\udf89\"\n+            case Classification.FALSE_POSITIVE:\n+                return \"False positives added \ud83e\udee4\"\n+            case Classification.TRUE_NEGATIVE:\n+                return \"False positives removed \ud83c\udf89\"\n+            case Classification.FALSE_NEGATIVE:\n+                return \"True positives removed \ud83e\udee4\"\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    def __str__(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self) -> str:\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Statistics:\n+    true_positives: int = 0\n+    false_positives: int = 0\n+    false_negatives: int = 0\n+\n+    @property\n+    def precision(self) -> float:\n+        if self.true_positives + self.false_positives > 0:\n+            return self.true_positives / (self.true_positives + self.false_positives)\n+        return 0.0\n+\n+    @property\n+    def recall(self) -> float:\n+        if self.true_positives + self.false_negatives > 0:\n+            return self.true_positives / (self.true_positives + self.false_negatives)\n+        else:\n+            return 0.0\n+\n+    @property\n+    def total(self) -> int:\n+        return self.true_positives + self.false_positives\n+\n+\n+def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n+    diagnostics: list[Diagnostic] = []\n+    for file in path.resolve().rglob(\"*.py\"):\n+        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+            if error := re.search(CONFORMANCE_ERROR_PATTERN, line):\n+                diagnostics.append(\n+                    Diagnostic(\n+                        check_name=\"conformance\",\n+                        description=error.group(\"description\")\n+                        or error.group(\"tag\")\n+                        or \"Missing\",\n+                        severity=\"major\",\n+                        fingerprint=None,\n+                        location=Location(\n+                            path=file.as_posix(),\n+                            positions=Positions(\n+                                begin=Position(\n+                                    line=idx,\n+                                    column=error.start(),\n+                                ),\n+                                end=Position(\n+                                    line=idx,\n+                                    column=error.end(),\n+                                ),\n+                            ),\n+                        ),\n+                        source=Source.EXPECTED,\n+                    )\n+                )\n+\n+    assert diagnostics, \"Failed to discover any expected diagnostics!\"\n+    return diagnostics\n+\n+\n+def collect_ty_diagnostics(\n+    ty_path: list[str],\n+    source: Source,\n+    tests_path: str = \".\",\n+    python_version: str = \"3.12\",\n+) -> list[Diagnostic]:\n+    process = subprocess.run(\n+        [\n+            *ty_path,\n+            \"check\",\n+            f\"--python-version={python_version}\",\n+            \"--output-format=gitlab\",\n+            \"--exit-zero\",\n+            tests_path,\n+        ],\n+        capture_output=True,\n+        text=True,\n+        check=True,\n+        timeout=15,\n+    )\n+\n+    if process.returncode != 0:\n+        print(process.stderr)\n+        raise RuntimeError(f\"ty check failed with exit code {process.returncode}\")\n+\n+    return [\n+        Diagnostic.from_gitlab_output(dct, source=source)\n+        for dct in json.loads(process.stdout)\n+    ]\n+\n+\n+def group_diagnostics_by_key(\n+    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+) -> list[GroupedDiagnostics]:\n+    diagnostics = [\n+        *old,\n+        *new,\n+        *expected,\n+    ]\n+    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+\n+    grouped = []\n+    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+        group = list(group)\n+        sources: Source = reduce(or_, (diag.source for diag in group))\n+        grouped.append(\n+            GroupedDiagnostics(\n+                key=key,\n+                sources=sources,\n+                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n+                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n+                expected=next(\n+                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n+                ),\n+            )\n+        )\n+\n+    return grouped\n+\n+\n+def compute_stats(\n+    grouped_diagnostics: list[GroupedDiagnostics], source: Source\n+) -> Statistics:\n+    if source == source.EXPECTED:\n+        # ty currently raises a false positive here due to incomplete enum.Flag support\n+        # see https://github.com/astral-sh/ty/issues/876\n+        num_errors = sum(\n+            [1 for g in grouped_diagnostics if source.EXPECTED in g.sources]  # ty:ignore[unsupported-operator]\n+        )\n+        return Statistics(\n+            true_positives=num_errors, false_positives=0, false_negatives=0\n+        )\n+\n+    def increment(statistics: Statistics, grouped: GroupedDiagnostics) -> Statistics:\n+        if (source in grouped.sources) and (Source.EXPECTED in grouped.sources):\n+            statistics.true_positives += 1\n+        elif source in grouped.sources:\n+            statistics.false_positives += 1\n+        else:\n+            statistics.false_negatives += 1\n+        return statistics\n+\n+    return reduce(increment, grouped_diagnostics, Statistics())\n+\n+\n+def render_grouped_diagnostics(\n+    grouped: list[GroupedDiagnostics], changed_only: bool = True\n+) -> str:\n+    if changed_only:\n+        grouped = [diag for diag in grouped if diag.changed]\n+    sorted_by_class = sorted(\n+        grouped,\n+        key=attrgetter(\"classification\"),\n+        reverse=True,\n+    )\n+\n+    lines = []\n+    for classification, group in groupby(\n+        sorted_by_class, key=attrgetter(\"classification\")\n+    ):\n+        group = list(group)\n+\n+        lines.append(f\"## {classification.into_title()}:\")\n+        lines.append(\"\")\n+        lines.append(\"```diff\")\n+\n+        for diag in group:\n+            lines.append(diag.display())\n+\n+        lines.append(\"```\")\n+\n+    return \"\\n\".join(lines)\n+\n+\n+def diff_format(\n+    diff: float,\n+    greater_is_better: bool = True,\n+    neutral: bool = False,\n+    is_percentage: bool = False,\n+):\n+    increased = diff > 0\n+    good = \"\u2705\" if not neutral else \"\"\n+    bad = \"\u274c\" if not neutral else \"\"\n+    up = \"\u23eb\"\n+    down = \"\u23ec\"\n+\n+    match (greater_is_better, increased):\n+        case (True, True):\n+            return f\"{good}{up}\"\n+        case (False, True):\n+            return f\"{bad}{down}\"\n+        case (True, False):\n+            return f\"{bad}{down}\"\n+        case (False, False):\n+            return f\"{good}{up}\"\n+\n+\n+def render_summary(grouped_diagnostics: list[GroupedDiagnostics]):\n+    def format_metric(diff: float, old: float, new: float):\n+        if diff > 0:\n+            return f\"increased from {old:.2%} to {new:.2%}\"\n+        if diff < 0:\n+            return f\"decreased from {old:.2%} to {new:.2%}\"\n+        return f\"held steady at {old:.2%}\"\n+\n+    old = compute_stats(grouped_diagnostics, source=Source.OLD)\n+    new = compute_stats(grouped_diagnostics, source=Source.NEW)\n+\n+    precision_change = new.precision - old.precision\n+    recall_change = new.recall - old.recall\n+    true_pos_change = new.true_positives - old.true_positives\n+    false_pos_change = new.false_positives - old.false_positives\n+    false_neg_change = new.false_negatives - old.false_negatives\n+    total_change = new.total - old.total\n+\n+    true_pos_diff = diff_format(true_pos_change, greater_is_better=True)\n+    false_pos_diff = diff_format(false_pos_change, greater_is_better=False)\n+    false_neg_diff = diff_format(false_neg_change, greater_is_better=False)\n+    precision_diff = diff_format(\n+        precision_change, greater_is_better=True, is_percentage=True\n+    )\n+    recall_diff = diff_format(recall_change, greater_is_better=True, is_percentage=True)\n+    total_diff = diff_format(total_change, neutral=True)\n+\n+    table = dedent(\n+        f\"\"\"\n+        ## Typing Conformance\n+\n+        ### Summary\n+\n+        | Metric | Old | New | Diff | Outcome |\n+        |--------|-----|-----|------|---------|\n+        | True Positives  | {old.true_positives} | {new.true_positives} | {true_pos_change} | {true_pos_diff} |\n+        | False Positives | {old.false_positives} | {new.false_positives} | {false_pos_change} | {false_pos_diff} |\n+        | False Negatives | {old.false_negatives} | {new.false_negatives} | {false_neg_change} | {false_neg_diff} |\n+        | Precision | {old.precision:.2%} | {new.precision:.2%} | {precision_change:.2%} | {precision_diff} |\n+        | Recall | {old.recall:.2%} | {new.recall:.2%} | {recall_change:.2%} | {recall_diff} |\n+        | Total | {old.total} | {new.total} | {total_change} | {total_diff} |",
      "comment": "We could move `Precision` and `Recall` after the total row.\r\n\r\n\r\nI also suggest always using a sign in the `Diff` column (+10) as it might otherwise be unclear what 0.74 means if there's no negative diff.",
      "comment_id": 2685995971,
      "user": "MichaReiser",
      "created_at": "2026-01-13T11:29:01Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2685995971"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 438,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,546 @@\n+\"\"\"\n+Run typing conformance tests and compare results between two ty versions.\n+\n+By default, this script will use `uv` to run the latest version of ty\n+as the new version with `uvx ty@latest`. This requires `uv` to be installed\n+and available in the system PATH.\n+\n+Examples:\n+    # Compare two specific ty versions\n+    %(prog)s --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Use local ty builds\n+    %(prog)s --old-ty ./target/debug/ty-old --new-ty ./target/debug/ty-new\n+\n+    # Custom test directory\n+    %(prog)s --target-path custom/tests --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Show all diagnostics (not just changed ones)\n+    %(prog)s --all --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from itertools import groupby\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+# The conformance tests include 4 types of errors:\n+# 1. Required errors (E): The type checker must raise an error on this line\n+# 2. Optional errors (E?): The type checker may raise an error on this line\n+# 3. Tagged errors (E[tag]): The type checker must raise at most one error on any of the lines in a file with matching tags\n+# 4. Tagged multi-errors (E[tag+]): The type checker should raise one or more errors on any of the tagged lines\n+# This regex pattern parses the error lines in the conformance tests, but the following\n+# implementation treats all errors as required errors.\n+CONFORMANCE_ERROR_PATTERN = re.compile(\n+    r\"\"\"\n+    \\#\\s*E                  # \"# E\" begins each error\n+    (?P<optional>\\?)?       # Optional '?' (E?) indicates that an error is optional\n+    (?:                     # An optional tag for errors that may appear on multiple lines at most once\n+        \\[\n+            (?P<tag>[^+\\]]+)    # identifier\n+            (?P<multi>\\+)?      # '+' indicates that an error may occur more than once on tagged lines\n+        \\]\n+    )?\n+    (?:\n+        \\s*:\\s*(?P<description>.*) # optional description\n+    )?\n+    \"\"\",\n+    re.VERBOSE,\n+)\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+    def into_title(self) -> str:\n+        match self:\n+            case Classification.TRUE_POSITIVE:\n+                return \"True positives added \ud83c\udf89\"\n+            case Classification.FALSE_POSITIVE:\n+                return \"False positives added \ud83e\udee4\"\n+            case Classification.TRUE_NEGATIVE:\n+                return \"False positives removed \ud83c\udf89\"\n+            case Classification.FALSE_NEGATIVE:\n+                return \"True positives removed \ud83e\udee4\"\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    def __str__(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self) -> str:\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Statistics:\n+    true_positives: int = 0\n+    false_positives: int = 0\n+    false_negatives: int = 0\n+\n+    @property\n+    def precision(self) -> float:\n+        if self.true_positives + self.false_positives > 0:\n+            return self.true_positives / (self.true_positives + self.false_positives)\n+        return 0.0\n+\n+    @property\n+    def recall(self) -> float:\n+        if self.true_positives + self.false_negatives > 0:\n+            return self.true_positives / (self.true_positives + self.false_negatives)\n+        else:\n+            return 0.0\n+\n+    @property\n+    def total(self) -> int:\n+        return self.true_positives + self.false_positives\n+\n+\n+def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n+    diagnostics: list[Diagnostic] = []\n+    for file in path.resolve().rglob(\"*.py\"):\n+        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+            if error := re.search(CONFORMANCE_ERROR_PATTERN, line):\n+                diagnostics.append(\n+                    Diagnostic(\n+                        check_name=\"conformance\",\n+                        description=error.group(\"description\")\n+                        or error.group(\"tag\")\n+                        or \"Missing\",\n+                        severity=\"major\",\n+                        fingerprint=None,\n+                        location=Location(\n+                            path=file.as_posix(),\n+                            positions=Positions(\n+                                begin=Position(\n+                                    line=idx,\n+                                    column=error.start(),\n+                                ),\n+                                end=Position(\n+                                    line=idx,\n+                                    column=error.end(),\n+                                ),\n+                            ),\n+                        ),\n+                        source=Source.EXPECTED,\n+                    )\n+                )\n+\n+    assert diagnostics, \"Failed to discover any expected diagnostics!\"\n+    return diagnostics\n+\n+\n+def collect_ty_diagnostics(\n+    ty_path: list[str],\n+    source: Source,\n+    tests_path: str = \".\",\n+    python_version: str = \"3.12\",\n+) -> list[Diagnostic]:\n+    process = subprocess.run(\n+        [\n+            *ty_path,\n+            \"check\",\n+            f\"--python-version={python_version}\",\n+            \"--output-format=gitlab\",\n+            \"--exit-zero\",\n+            tests_path,\n+        ],\n+        capture_output=True,\n+        text=True,\n+        check=True,\n+        timeout=15,\n+    )\n+\n+    if process.returncode != 0:\n+        print(process.stderr)\n+        raise RuntimeError(f\"ty check failed with exit code {process.returncode}\")\n+\n+    return [\n+        Diagnostic.from_gitlab_output(dct, source=source)\n+        for dct in json.loads(process.stdout)\n+    ]\n+\n+\n+def group_diagnostics_by_key(\n+    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+) -> list[GroupedDiagnostics]:\n+    diagnostics = [\n+        *old,\n+        *new,\n+        *expected,\n+    ]\n+    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+\n+    grouped = []\n+    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+        group = list(group)\n+        sources: Source = reduce(or_, (diag.source for diag in group))\n+        grouped.append(\n+            GroupedDiagnostics(\n+                key=key,\n+                sources=sources,\n+                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n+                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n+                expected=next(\n+                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n+                ),\n+            )\n+        )\n+\n+    return grouped\n+\n+\n+def compute_stats(\n+    grouped_diagnostics: list[GroupedDiagnostics], source: Source\n+) -> Statistics:\n+    if source == source.EXPECTED:\n+        # ty currently raises a false positive here due to incomplete enum.Flag support\n+        # see https://github.com/astral-sh/ty/issues/876\n+        num_errors = sum(\n+            [1 for g in grouped_diagnostics if source.EXPECTED in g.sources]  # ty:ignore[unsupported-operator]\n+        )\n+        return Statistics(\n+            true_positives=num_errors, false_positives=0, false_negatives=0\n+        )\n+\n+    def increment(statistics: Statistics, grouped: GroupedDiagnostics) -> Statistics:\n+        if (source in grouped.sources) and (Source.EXPECTED in grouped.sources):\n+            statistics.true_positives += 1\n+        elif source in grouped.sources:\n+            statistics.false_positives += 1\n+        else:\n+            statistics.false_negatives += 1\n+        return statistics\n+\n+    return reduce(increment, grouped_diagnostics, Statistics())\n+\n+\n+def render_grouped_diagnostics(\n+    grouped: list[GroupedDiagnostics], changed_only: bool = True\n+) -> str:\n+    if changed_only:\n+        grouped = [diag for diag in grouped if diag.changed]\n+    sorted_by_class = sorted(\n+        grouped,\n+        key=attrgetter(\"classification\"),\n+        reverse=True,\n+    )\n+\n+    lines = []\n+    for classification, group in groupby(\n+        sorted_by_class, key=attrgetter(\"classification\")\n+    ):\n+        group = list(group)\n+\n+        lines.append(f\"## {classification.into_title()}:\")\n+        lines.append(\"\")\n+        lines.append(\"```diff\")\n+\n+        for diag in group:\n+            lines.append(diag.display())\n+\n+        lines.append(\"```\")\n+\n+    return \"\\n\".join(lines)\n+\n+\n+def diff_format(\n+    diff: float,\n+    greater_is_better: bool = True,\n+    neutral: bool = False,\n+    is_percentage: bool = False,\n+):\n+    increased = diff > 0\n+    good = \"\u2705\" if not neutral else \"\"\n+    bad = \"\u274c\" if not neutral else \"\"\n+    up = \"\u23eb\"\n+    down = \"\u23ec\"\n+\n+    match (greater_is_better, increased):\n+        case (True, True):\n+            return f\"{good}{up}\"\n+        case (False, True):\n+            return f\"{bad}{down}\"\n+        case (True, False):\n+            return f\"{bad}{down}\"\n+        case (False, False):\n+            return f\"{good}{up}\"\n+\n+\n+def render_summary(grouped_diagnostics: list[GroupedDiagnostics]):\n+    def format_metric(diff: float, old: float, new: float):\n+        if diff > 0:\n+            return f\"increased from {old:.2%} to {new:.2%}\"\n+        if diff < 0:\n+            return f\"decreased from {old:.2%} to {new:.2%}\"\n+        return f\"held steady at {old:.2%}\"\n+\n+    old = compute_stats(grouped_diagnostics, source=Source.OLD)\n+    new = compute_stats(grouped_diagnostics, source=Source.NEW)\n+\n+    precision_change = new.precision - old.precision\n+    recall_change = new.recall - old.recall\n+    true_pos_change = new.true_positives - old.true_positives\n+    false_pos_change = new.false_positives - old.false_positives\n+    false_neg_change = new.false_negatives - old.false_negatives\n+    total_change = new.total - old.total\n+\n+    true_pos_diff = diff_format(true_pos_change, greater_is_better=True)\n+    false_pos_diff = diff_format(false_pos_change, greater_is_better=False)\n+    false_neg_diff = diff_format(false_neg_change, greater_is_better=False)\n+    precision_diff = diff_format(\n+        precision_change, greater_is_better=True, is_percentage=True\n+    )\n+    recall_diff = diff_format(recall_change, greater_is_better=True, is_percentage=True)\n+    total_diff = diff_format(total_change, neutral=True)\n+\n+    table = dedent(\n+        f\"\"\"\n+        ## Typing Conformance\n+\n+        ### Summary\n+\n+        | Metric | Old | New | Diff | Outcome |\n+        |--------|-----|-----|------|---------|\n+        | True Positives  | {old.true_positives} | {new.true_positives} | {true_pos_change} | {true_pos_diff} |\n+        | False Positives | {old.false_positives} | {new.false_positives} | {false_pos_change} | {false_pos_diff} |\n+        | False Negatives | {old.false_negatives} | {new.false_negatives} | {false_neg_change} | {false_neg_diff} |\n+        | Precision | {old.precision:.2%} | {new.precision:.2%} | {precision_change:.2%} | {precision_diff} |\n+        | Recall | {old.recall:.2%} | {new.recall:.2%} | {recall_change:.2%} | {recall_diff} |\n+        | Total | {old.total} | {new.total} | {total_change} | {total_diff} |",
      "comment": "Maybe \"Total Diagnostics\" or similar would be clearer? This is just a count of the number of diagnostics generated by each version. ",
      "comment_id": 2687224245,
      "user": "WillDuke",
      "created_at": "2026-01-13T16:43:50Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2687224245"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22231,
      "file_path": "scripts/conformance.py",
      "line": 438,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,546 @@\n+\"\"\"\n+Run typing conformance tests and compare results between two ty versions.\n+\n+By default, this script will use `uv` to run the latest version of ty\n+as the new version with `uvx ty@latest`. This requires `uv` to be installed\n+and available in the system PATH.\n+\n+Examples:\n+    # Compare two specific ty versions\n+    %(prog)s --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Use local ty builds\n+    %(prog)s --old-ty ./target/debug/ty-old --new-ty ./target/debug/ty-new\n+\n+    # Custom test directory\n+    %(prog)s --target-path custom/tests --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\n+    # Show all diagnostics (not just changed ones)\n+    %(prog)s --all --old-ty uvx ty@0.0.1a35 --new-ty uvx ty@0.0.7\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import re\n+import subprocess\n+import sys\n+from dataclasses import dataclass\n+from enum import Flag, StrEnum, auto\n+from functools import reduce\n+from itertools import groupby\n+from operator import attrgetter, or_\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Any, Self\n+\n+# The conformance tests include 4 types of errors:\n+# 1. Required errors (E): The type checker must raise an error on this line\n+# 2. Optional errors (E?): The type checker may raise an error on this line\n+# 3. Tagged errors (E[tag]): The type checker must raise at most one error on any of the lines in a file with matching tags\n+# 4. Tagged multi-errors (E[tag+]): The type checker should raise one or more errors on any of the tagged lines\n+# This regex pattern parses the error lines in the conformance tests, but the following\n+# implementation treats all errors as required errors.\n+CONFORMANCE_ERROR_PATTERN = re.compile(\n+    r\"\"\"\n+    \\#\\s*E                  # \"# E\" begins each error\n+    (?P<optional>\\?)?       # Optional '?' (E?) indicates that an error is optional\n+    (?:                     # An optional tag for errors that may appear on multiple lines at most once\n+        \\[\n+            (?P<tag>[^+\\]]+)    # identifier\n+            (?P<multi>\\+)?      # '+' indicates that an error may occur more than once on tagged lines\n+        \\]\n+    )?\n+    (?:\n+        \\s*:\\s*(?P<description>.*) # optional description\n+    )?\n+    \"\"\",\n+    re.VERBOSE,\n+)\n+\n+\n+class Source(Flag):\n+    OLD = auto()\n+    NEW = auto()\n+    EXPECTED = auto()\n+\n+\n+class Classification(StrEnum):\n+    TRUE_POSITIVE = auto()\n+    FALSE_POSITIVE = auto()\n+    TRUE_NEGATIVE = auto()\n+    FALSE_NEGATIVE = auto()\n+\n+    def into_title(self) -> str:\n+        match self:\n+            case Classification.TRUE_POSITIVE:\n+                return \"True positives added \ud83c\udf89\"\n+            case Classification.FALSE_POSITIVE:\n+                return \"False positives added \ud83e\udee4\"\n+            case Classification.TRUE_NEGATIVE:\n+                return \"False positives removed \ud83c\udf89\"\n+            case Classification.FALSE_NEGATIVE:\n+                return \"True positives removed \ud83e\udee4\"\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Position:\n+    line: int\n+    column: int\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Positions:\n+    begin: Position\n+    end: Position\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Location:\n+    path: str\n+    positions: Positions\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Diagnostic:\n+    check_name: str\n+    description: str\n+    severity: str\n+    fingerprint: str | None\n+    location: Location\n+    source: Source\n+\n+    def __str__(self) -> str:\n+        return (\n+            f\"{self.location.path}:{self.location.positions.begin.line}:\"\n+            f\"{self.location.positions.begin.column}: \"\n+            f\"{self.severity_for_display}[{self.check_name}] {self.description}\"\n+        )\n+\n+    @classmethod\n+    def from_gitlab_output(\n+        cls,\n+        dct: dict[str, Any],\n+        source: Source,\n+    ) -> Self:\n+        return cls(\n+            check_name=dct[\"check_name\"],\n+            description=dct[\"description\"],\n+            severity=dct[\"severity\"],\n+            fingerprint=dct[\"fingerprint\"],\n+            location=Location(\n+                path=dct[\"location\"][\"path\"],\n+                positions=Positions(\n+                    begin=Position(\n+                        line=dct[\"location\"][\"positions\"][\"begin\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"begin\"][\"column\"],\n+                    ),\n+                    end=Position(\n+                        line=dct[\"location\"][\"positions\"][\"end\"][\"line\"],\n+                        column=dct[\"location\"][\"positions\"][\"end\"][\"column\"],\n+                    ),\n+                ),\n+            ),\n+            source=source,\n+        )\n+\n+    @property\n+    def key(self) -> str:\n+        \"\"\"Key to group diagnostics by path and beginning line.\"\"\"\n+        return f\"{self.location.path}:{self.location.positions.begin.line}\"\n+\n+    @property\n+    def severity_for_display(self) -> str:\n+        return {\n+            \"major\": \"error\",\n+            \"minor\": \"warning\",\n+        }.get(self.severity, \"unknown\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class GroupedDiagnostics:\n+    key: str\n+    sources: Source\n+    old: Diagnostic | None\n+    new: Diagnostic | None\n+    expected: Diagnostic | None\n+\n+    @property\n+    def changed(self) -> bool:\n+        return (Source.OLD in self.sources or Source.NEW in self.sources) and not (\n+            Source.OLD in self.sources and Source.NEW in self.sources\n+        )\n+\n+    @property\n+    def classification(self) -> Classification:\n+        if Source.NEW in self.sources and Source.EXPECTED in self.sources:\n+            return Classification.TRUE_POSITIVE\n+        elif Source.NEW in self.sources and Source.EXPECTED not in self.sources:\n+            return Classification.FALSE_POSITIVE\n+        elif Source.EXPECTED in self.sources:\n+            return Classification.FALSE_NEGATIVE\n+        else:\n+            return Classification.TRUE_NEGATIVE\n+\n+    def display(self) -> str:\n+        match self.classification:\n+            case Classification.TRUE_POSITIVE | Classification.FALSE_POSITIVE:\n+                assert self.new is not None\n+                return f\"+ {self.new}\"\n+\n+            case Classification.FALSE_NEGATIVE | Classification.TRUE_NEGATIVE:\n+                if self.old is not None:\n+                    return f\"- {self.old}\"\n+                elif self.expected is not None:\n+                    return f\"- {self.expected}\"\n+                else:\n+                    return \"\"\n+            case _:\n+                raise ValueError(f\"Unexpected classification: {self.classification}\")\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class Statistics:\n+    true_positives: int = 0\n+    false_positives: int = 0\n+    false_negatives: int = 0\n+\n+    @property\n+    def precision(self) -> float:\n+        if self.true_positives + self.false_positives > 0:\n+            return self.true_positives / (self.true_positives + self.false_positives)\n+        return 0.0\n+\n+    @property\n+    def recall(self) -> float:\n+        if self.true_positives + self.false_negatives > 0:\n+            return self.true_positives / (self.true_positives + self.false_negatives)\n+        else:\n+            return 0.0\n+\n+    @property\n+    def total(self) -> int:\n+        return self.true_positives + self.false_positives\n+\n+\n+def collect_expected_diagnostics(path: Path) -> list[Diagnostic]:\n+    diagnostics: list[Diagnostic] = []\n+    for file in path.resolve().rglob(\"*.py\"):\n+        for idx, line in enumerate(file.read_text().splitlines(), 1):\n+            if error := re.search(CONFORMANCE_ERROR_PATTERN, line):\n+                diagnostics.append(\n+                    Diagnostic(\n+                        check_name=\"conformance\",\n+                        description=error.group(\"description\")\n+                        or error.group(\"tag\")\n+                        or \"Missing\",\n+                        severity=\"major\",\n+                        fingerprint=None,\n+                        location=Location(\n+                            path=file.as_posix(),\n+                            positions=Positions(\n+                                begin=Position(\n+                                    line=idx,\n+                                    column=error.start(),\n+                                ),\n+                                end=Position(\n+                                    line=idx,\n+                                    column=error.end(),\n+                                ),\n+                            ),\n+                        ),\n+                        source=Source.EXPECTED,\n+                    )\n+                )\n+\n+    assert diagnostics, \"Failed to discover any expected diagnostics!\"\n+    return diagnostics\n+\n+\n+def collect_ty_diagnostics(\n+    ty_path: list[str],\n+    source: Source,\n+    tests_path: str = \".\",\n+    python_version: str = \"3.12\",\n+) -> list[Diagnostic]:\n+    process = subprocess.run(\n+        [\n+            *ty_path,\n+            \"check\",\n+            f\"--python-version={python_version}\",\n+            \"--output-format=gitlab\",\n+            \"--exit-zero\",\n+            tests_path,\n+        ],\n+        capture_output=True,\n+        text=True,\n+        check=True,\n+        timeout=15,\n+    )\n+\n+    if process.returncode != 0:\n+        print(process.stderr)\n+        raise RuntimeError(f\"ty check failed with exit code {process.returncode}\")\n+\n+    return [\n+        Diagnostic.from_gitlab_output(dct, source=source)\n+        for dct in json.loads(process.stdout)\n+    ]\n+\n+\n+def group_diagnostics_by_key(\n+    old: list[Diagnostic], new: list[Diagnostic], expected: list[Diagnostic]\n+) -> list[GroupedDiagnostics]:\n+    diagnostics = [\n+        *old,\n+        *new,\n+        *expected,\n+    ]\n+    sorted_diagnostics = sorted(diagnostics, key=attrgetter(\"key\"))\n+\n+    grouped = []\n+    for key, group in groupby(sorted_diagnostics, key=attrgetter(\"key\")):\n+        group = list(group)\n+        sources: Source = reduce(or_, (diag.source for diag in group))\n+        grouped.append(\n+            GroupedDiagnostics(\n+                key=key,\n+                sources=sources,\n+                old=next(filter(lambda diag: diag.source == Source.OLD, group), None),\n+                new=next(filter(lambda diag: diag.source == Source.NEW, group), None),\n+                expected=next(\n+                    filter(lambda diag: diag.source == Source.EXPECTED, group), None\n+                ),\n+            )\n+        )\n+\n+    return grouped\n+\n+\n+def compute_stats(\n+    grouped_diagnostics: list[GroupedDiagnostics], source: Source\n+) -> Statistics:\n+    if source == source.EXPECTED:\n+        # ty currently raises a false positive here due to incomplete enum.Flag support\n+        # see https://github.com/astral-sh/ty/issues/876\n+        num_errors = sum(\n+            [1 for g in grouped_diagnostics if source.EXPECTED in g.sources]  # ty:ignore[unsupported-operator]\n+        )\n+        return Statistics(\n+            true_positives=num_errors, false_positives=0, false_negatives=0\n+        )\n+\n+    def increment(statistics: Statistics, grouped: GroupedDiagnostics) -> Statistics:\n+        if (source in grouped.sources) and (Source.EXPECTED in grouped.sources):\n+            statistics.true_positives += 1\n+        elif source in grouped.sources:\n+            statistics.false_positives += 1\n+        else:\n+            statistics.false_negatives += 1\n+        return statistics\n+\n+    return reduce(increment, grouped_diagnostics, Statistics())\n+\n+\n+def render_grouped_diagnostics(\n+    grouped: list[GroupedDiagnostics], changed_only: bool = True\n+) -> str:\n+    if changed_only:\n+        grouped = [diag for diag in grouped if diag.changed]\n+    sorted_by_class = sorted(\n+        grouped,\n+        key=attrgetter(\"classification\"),\n+        reverse=True,\n+    )\n+\n+    lines = []\n+    for classification, group in groupby(\n+        sorted_by_class, key=attrgetter(\"classification\")\n+    ):\n+        group = list(group)\n+\n+        lines.append(f\"## {classification.into_title()}:\")\n+        lines.append(\"\")\n+        lines.append(\"```diff\")\n+\n+        for diag in group:\n+            lines.append(diag.display())\n+\n+        lines.append(\"```\")\n+\n+    return \"\\n\".join(lines)\n+\n+\n+def diff_format(\n+    diff: float,\n+    greater_is_better: bool = True,\n+    neutral: bool = False,\n+    is_percentage: bool = False,\n+):\n+    increased = diff > 0\n+    good = \"\u2705\" if not neutral else \"\"\n+    bad = \"\u274c\" if not neutral else \"\"\n+    up = \"\u23eb\"\n+    down = \"\u23ec\"\n+\n+    match (greater_is_better, increased):\n+        case (True, True):\n+            return f\"{good}{up}\"\n+        case (False, True):\n+            return f\"{bad}{down}\"\n+        case (True, False):\n+            return f\"{bad}{down}\"\n+        case (False, False):\n+            return f\"{good}{up}\"\n+\n+\n+def render_summary(grouped_diagnostics: list[GroupedDiagnostics]):\n+    def format_metric(diff: float, old: float, new: float):\n+        if diff > 0:\n+            return f\"increased from {old:.2%} to {new:.2%}\"\n+        if diff < 0:\n+            return f\"decreased from {old:.2%} to {new:.2%}\"\n+        return f\"held steady at {old:.2%}\"\n+\n+    old = compute_stats(grouped_diagnostics, source=Source.OLD)\n+    new = compute_stats(grouped_diagnostics, source=Source.NEW)\n+\n+    precision_change = new.precision - old.precision\n+    recall_change = new.recall - old.recall\n+    true_pos_change = new.true_positives - old.true_positives\n+    false_pos_change = new.false_positives - old.false_positives\n+    false_neg_change = new.false_negatives - old.false_negatives\n+    total_change = new.total - old.total\n+\n+    true_pos_diff = diff_format(true_pos_change, greater_is_better=True)\n+    false_pos_diff = diff_format(false_pos_change, greater_is_better=False)\n+    false_neg_diff = diff_format(false_neg_change, greater_is_better=False)\n+    precision_diff = diff_format(\n+        precision_change, greater_is_better=True, is_percentage=True\n+    )\n+    recall_diff = diff_format(recall_change, greater_is_better=True, is_percentage=True)\n+    total_diff = diff_format(total_change, neutral=True)\n+\n+    table = dedent(\n+        f\"\"\"\n+        ## Typing Conformance\n+\n+        ### Summary\n+\n+        | Metric | Old | New | Diff | Outcome |\n+        |--------|-----|-----|------|---------|\n+        | True Positives  | {old.true_positives} | {new.true_positives} | {true_pos_change} | {true_pos_diff} |\n+        | False Positives | {old.false_positives} | {new.false_positives} | {false_pos_change} | {false_pos_diff} |\n+        | False Negatives | {old.false_negatives} | {new.false_negatives} | {false_neg_change} | {false_neg_diff} |\n+        | Precision | {old.precision:.2%} | {new.precision:.2%} | {precision_change:.2%} | {precision_diff} |\n+        | Recall | {old.recall:.2%} | {new.recall:.2%} | {recall_change:.2%} | {recall_diff} |\n+        | Total | {old.total} | {new.total} | {total_change} | {total_diff} |",
      "comment": "\"Total diagnostics\" works for me! I do also agree with Micha that it might be good to move it above \"Precision\" and \"Recall\" though",
      "comment_id": 2687228844,
      "user": "AlexWaygood",
      "created_at": "2026-01-13T16:45:01Z",
      "url": "https://github.com/astral-sh/ruff/pull/22231#discussion_r2687228844"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22119,
      "file_path": "crates/ruff_python_formatter/resources/test/fixtures/ruff/fmt_skip/semicolons.py",
      "line": 8,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,128 @@\n+class Simple:\n+    x=1\n+    x=2 # fmt: skip\n+    x=3\n+\n+class Semicolon:\n+    x=1\n+    x=2;x=3 # fmt: skip",
      "comment": "Let's add an example with a trailing semicolon\r\n\r\n```suggestion\r\n    x=2;x=3.  ; # fmt: skip\r\n```",
      "comment_id": 2664137606,
      "user": "MichaReiser",
      "created_at": "2026-01-06T08:45:35Z",
      "url": "https://github.com/astral-sh/ruff/pull/22119#discussion_r2664137606"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22460,
      "file_path": "crates/ty_completion_eval/truth/typing-gets-priority/main.py",
      "line": 19,
      "side": "RIGHT",
      "diff_hunk": "@@ -10,3 +10,7 @@ class Foo(Protoco<CURSOR: typing.Protocol>): ...\n \n # We should prefer `typing` over `ctypes`.\n cast<CURSOR: typing.cast>\n+\n+# We should prefer a non-stdlib project import\n+# over a stdlib `typing` import.\n+NoRetur<CURSOR: sub1.NoReturn>",
      "comment": "I'm curious how this plays out. E.g. it could be annoying to get a suggestion for a non stdlib `Ordering` just because someone ended up naming their class `Ordering` somewhere in a 2 Million-line project :) But I think it's a good start and we can wait on user reports to see how to improve completions further (e.g. consider the distance between two modules)",
      "comment_id": 2675359591,
      "user": "MichaReiser",
      "created_at": "2026-01-09T08:53:40Z",
      "url": "https://github.com/astral-sh/ruff/pull/22460#discussion_r2675359591"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22460,
      "file_path": "crates/ty_completion_eval/truth/typing-gets-priority/main.py",
      "line": 19,
      "side": "RIGHT",
      "diff_hunk": "@@ -10,3 +10,7 @@ class Foo(Protoco<CURSOR: typing.Protocol>): ...\n \n # We should prefer `typing` over `ctypes`.\n cast<CURSOR: typing.cast>\n+\n+# We should prefer a non-stdlib project import\n+# over a stdlib `typing` import.\n+NoRetur<CURSOR: sub1.NoReturn>",
      "comment": "Yeah I'm definitely not 100% sold on this either. Very curious to get user feedback on this.",
      "comment_id": 2676273172,
      "user": "BurntSushi",
      "created_at": "2026-01-09T13:52:43Z",
      "url": "https://github.com/astral-sh/ruff/pull/22460#discussion_r2676273172"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22399,
      "file_path": "scripts/ty_benchmark/src/benchmark/venv.py",
      "line": 10,
      "side": "RIGHT",
      "diff_hunk": "@@ -3,15 +3,15 @@\n import logging\n import subprocess\n import sys\n+from dataclasses import dataclass\n from pathlib import Path\n \n \n+@dataclass(frozen=True, kw_only=True, slots=True)",
      "comment": "Nooo, don't make it fancy lol. Now I need to learn new Python features when I want to maintain this next time",
      "comment_id": 2661772854,
      "user": "MichaReiser",
      "created_at": "2026-01-05T14:52:23Z",
      "url": "https://github.com/astral-sh/ruff/pull/22399#discussion_r2661772854"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22220,
      "file_path": "crates/ruff_linter/resources/test/fixtures/pylint/repeated_equality_comparison.py",
      "line": 77,
      "side": "RIGHT",
      "diff_hunk": "@@ -73,3 +73,7 @@\n foo == False or foo == 0 # Different types, same hashed value\n \n foo == 0.0 or foo == 0j # Different types, same hashed value\n+\n+foo == \"bar\" or foo == \"bar\" # All members identical",
      "comment": "I think we should consider a rule to get this to reduce to ```foo == \"bar\"```. Opening a separate issue would probably be best.",
      "comment_id": 2658162642,
      "user": "njhearp",
      "created_at": "2026-01-02T18:03:02Z",
      "url": "https://github.com/astral-sh/ruff/pull/22220#discussion_r2658162642"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22220,
      "file_path": "crates/ruff_linter/resources/test/fixtures/pylint/repeated_equality_comparison.py",
      "line": 77,
      "side": "RIGHT",
      "diff_hunk": "@@ -73,3 +73,7 @@\n foo == False or foo == 0 # Different types, same hashed value\n \n foo == 0.0 or foo == 0j # Different types, same hashed value\n+\n+foo == \"bar\" or foo == \"bar\" # All members identical",
      "comment": "I think this is possibly tracked in https://github.com/astral-sh/ruff/issues/21690. Micha mentioned a more general unreachable rule there and on https://github.com/astral-sh/ruff/issues/21692.",
      "comment_id": 2658166657,
      "user": "ntBre",
      "created_at": "2026-01-02T18:05:03Z",
      "url": "https://github.com/astral-sh/ruff/pull/22220#discussion_r2658166657"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22046,
      "file_path": "crates/ruff_linter/resources/test/fixtures/airflow/AIR303_args.py",
      "line": 10,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,10 @@\n+from __future__ import annotations\n+\n+from airflow.lineage.hook import HookLineageCollector\n+\n+# airflow.lineage.hook\n+hlc = HookLineageCollector()\n+hlc.create_asset(\"there\")\n+hlc.create_asset(\"should\", \"be\", \"no\", \"posarg\")\n+hlc.create_asset(name=\"but\", uri=\"kwargs are ok\")\n+hlc.create_asset()",
      "comment": "This might be quite unusual, but do you want to test something like:\n\n```py\nHookLineageCollector().create_asset(positional_arg)\n```\n\nI'm not sure if `resolve_assignment` will handle that case automatically.\n",
      "comment_id": 2647207781,
      "user": "ntBre",
      "created_at": "2025-12-25T18:34:01Z",
      "url": "https://github.com/astral-sh/ruff/pull/22046#discussion_r2647207781"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 22046,
      "file_path": "crates/ruff_linter/resources/test/fixtures/airflow/AIR303_args.py",
      "line": 10,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,10 @@\n+from __future__ import annotations\n+\n+from airflow.lineage.hook import HookLineageCollector\n+\n+# airflow.lineage.hook\n+hlc = HookLineageCollector()\n+hlc.create_asset(\"there\")\n+hlc.create_asset(\"should\", \"be\", \"no\", \"posarg\")\n+hlc.create_asset(name=\"but\", uri=\"kwargs are ok\")\n+hlc.create_asset()",
      "comment": "One other probably rare case is `hlc.create_asset(*args)`. I don't think the rule would currently flag that because `find_positional` only checks the non-starred arguments.",
      "comment_id": 2647215250,
      "user": "ntBre",
      "created_at": "2025-12-25T18:53:00Z",
      "url": "https://github.com/astral-sh/ruff/pull/22046#discussion_r2647215250"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21908,
      "file_path": "crates/ruff_linter/resources/test/fixtures/ruff/suppressions.py",
      "line": 94,
      "side": "RIGHT",
      "diff_hunk": "@@ -86,3 +86,12 @@ def f():\n     # Multiple codes but none are used\n     # ruff: disable[E741, F401, F841]\n     print(\"hello\")\n+\n+\n+def f():\n+    # Unknown rule codes\n+    # ruff: disable[YF829]\n+    # ruff: disable[F841, RQW320]",
      "comment": "Can you add a test that verifies we don't flag rules defined using [`external`](https://docs.astral.sh/ruff/settings/#lint_external) (might have to be a CLI test)",
      "comment_id": 2613457489,
      "user": "MichaReiser",
      "created_at": "2025-12-12T09:01:46Z",
      "url": "https://github.com/astral-sh/ruff/pull/21908#discussion_r2613457489"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21908,
      "file_path": "crates/ruff_linter/resources/test/fixtures/ruff/suppressions.py",
      "line": 94,
      "side": "RIGHT",
      "diff_hunk": "@@ -86,3 +86,12 @@ def f():\n     # Multiple codes but none are used\n     # ruff: disable[E741, F401, F841]\n     print(\"hello\")\n+\n+\n+def f():\n+    # Unknown rule codes\n+    # ruff: disable[YF829]\n+    # ruff: disable[F841, RQW320]",
      "comment": "Should we consider external rule codes \"valid\" outside of a `#noqa` context? Ie, do we actually expect external tools/linters to support `#ruff:disable` style suppressions?",
      "comment_id": 2625044460,
      "user": "amyreese",
      "created_at": "2025-12-16T23:09:02Z",
      "url": "https://github.com/astral-sh/ruff/pull/21908#discussion_r2625044460"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21080,
      "file_path": "crates/ruff_linter/resources/test/fixtures/refurb/FURB103.py",
      "line": 1,
      "side": "RIGHT",
      "diff_hunk": "@@ -1,3 +1,4 @@\n+from pathlib import Path",
      "comment": "I think we might want a separate file for these new tests and this import. We're losing a lot of snapshot changes where the fix needed to add this import.",
      "comment_id": 2486824382,
      "user": "ntBre",
      "created_at": "2025-11-03T15:11:47Z",
      "url": "https://github.com/astral-sh/ruff/pull/21080#discussion_r2486824382"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21385,
      "file_path": "crates/ruff_python_formatter/resources/test/fixtures/ruff/expression/lambda.py",
      "line": 456,
      "side": "RIGHT",
      "diff_hunk": "@@ -228,3 +311,155 @@ def a():\n         g = 10\n     )\n \n+def a():\n+    return b(\n+        c,\n+        d,\n+        e,\n+        f=lambda self, *args, **kwargs: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa(\n+            *args, **kwargs\n+        ) + 1,\n+    )\n+\n+# Additional ecosystem cases from https://github.com/astral-sh/ruff/pull/21385\n+class C:\n+    def foo():\n+        mock_service.return_value.bucket.side_effect = lambda name: (\n+            source_bucket\n+            if name == source_bucket_name\n+            else storage.Bucket(mock_service, destination_bucket_name)\n+        )\n+\n+class C:\n+\tfunction_dict: Dict[Text, Callable[[CRFToken], Any]] = {\n+        CRFEntityExtractorOptions.POS2: lambda crf_token: crf_token.pos_tag[:2]\n+        if crf_token.pos_tag is not None\n+        else None,\n+\t}\n+\n+name = re.sub(r\"[^\\x21\\x23-\\x5b\\x5d-\\x7e]...............\", lambda m: f\"\\\\{m.group(0)}\", p[\"name\"])\n+\n+def foo():\n+    if True:\n+        if True:\n+            return (\n+                lambda x: np.exp(cs(np.log(x.to(u.MeV).value))) * u.MeV * u.cm**2 / u.g\n+            )\n+\n+class C:\n+    _is_recognized_dtype: Callable[[DtypeObj], bool] = lambda x: lib.is_np_dtype(\n+        x, \"M\"\n+    ) or isinstance(x, DatetimeTZDtype)\n+\n+class C:\n+    def foo():\n+        if True:\n+            transaction_count = self._query_txs_for_range(\n+                get_count_fn=lambda from_ts, to_ts, _chain_id=chain_id: db_evmtx.count_transactions_in_range(\n+                    chain_id=_chain_id,\n+                    from_ts=from_ts,\n+                    to_ts=to_ts,\n+                ),\n+            )\n+\n+            transaction_count = self._query_txs_for_range(\n+                get_count_fn=lambda from_ts, to_ts, _chain_id=chain_id: db_evmtx.count_transactions_in_range[_chain_id, from_ts, to_ts],\n+            )\n+\n+def ddb():\n+    sql = (\n+        lambda var, table, n=N: f\"\"\"\n+        CREATE TABLE {table} AS\n+        SELECT ROW_NUMBER() OVER () AS id, {var}\n+        FROM (\n+            SELECT {var}\n+            FROM RANGE({n}) _ ({var})\n+            ORDER BY RANDOM()\n+        )\n+        \"\"\"\n+    )\n+\n+long_assignment_target.with_attribute.and_a_slice[with_an_index] = ( # 1\n+    # 2\n+    lambda x, y, z: # 3\n+    # 4\n+    x + y + z # 5\n+    # 6\n+)\n+\n+long_assignment_target.with_attribute.and_a_slice[with_an_index] = (\n+    lambda x, y, z: x + y + z\n+)\n+\n+long_assignment_target.with_attribute.and_a_slice[with_an_index] = lambda x, y, z: x + y + z\n+\n+very_long_variable_name_x, very_long_variable_name_y = lambda a: a + some_very_long_expression, lambda b: b * another_very_long_expression_here\n+\n+very_long_variable_name_for_result += lambda x: very_long_function_call_that_should_definitely_be_parenthesized_now(x, more_args, additional_parameters)\n+\n+\n+if 1:\n+    if 2:\n+        if 3:\n+            if self.location in EVM_EVMLIKE_LOCATIONS and database is not None:\n+                exported_dict[\"notes\"] = EVM_ADDRESS_REGEX.sub(\n+                    repl=lambda matched_address: self._maybe_add_label_with_address(\n+                        database=database,\n+                        matched_address=matched_address,\n+                    ),\n+                    string=exported_dict[\"notes\"],\n+                )\n+\n+class C:\n+    def f():\n+        return dict(\n+            filter(\n+                lambda intent_response: self.is_retrieval_intent_response(\n+                    intent_response\n+                ),\n+                self.responses.items(),\n+            )\n+        )\n+\n+@pytest.mark.parametrize(\n+    \"op\",\n+    [\n+        # Not fluent\n+        param(\n+            lambda left, right: (\n+                ibis.timestamp(\"2017-04-01\")\n+            ),\n+        ),\n+        # These four are fluent and fit on one line inside the parenthesized\n+        # lambda body\n+        param(\n+            lambda left, right: (\n+                ibis.timestamp(\"2017-04-01\").cast(dt.date)\n+            ),\n+        ),\n+        param(\n+            lambda left, right: (\n+                ibis.timestamp(\"2017-04-01\").cast(dt.date).between(left, right)\n+            ),\n+        ),\n+        param(lambda left, right: ibis.timestamp(\"2017-04-01\").cast(dt.date)),\n+        param(lambda left, right: ibis.timestamp(\"2017-04-01\").cast(dt.date).between(left, right)),\n+        # This is too long on one line in the lambda body and gets wrapped\n+        # inside the body.\n+        param(\n+            lambda left, right: (\n+                ibis.timestamp(\"2017-04-01\").cast(dt.date).between(left, right).between(left, right)\n+            ),\n+        ),\n+    ],\n+)\n+def test_string_temporal_compare_between(con, op, left, right): ...",
      "comment": "We should add a lot more tests (unless they already exist in some form) for different comment placement, including combinations where the body or lambda are parenthesized (and for every special case branch that we have)",
      "comment_id": 2586735153,
      "user": "MichaReiser",
      "created_at": "2025-12-03T21:53:56Z",
      "url": "https://github.com/astral-sh/ruff/pull/21385#discussion_r2586735153"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21385,
      "file_path": "crates/ruff_python_formatter/resources/test/fixtures/ruff/expression/lambda.py",
      "line": 456,
      "side": "RIGHT",
      "diff_hunk": "@@ -228,3 +311,155 @@ def a():\n         g = 10\n     )\n \n+def a():\n+    return b(\n+        c,\n+        d,\n+        e,\n+        f=lambda self, *args, **kwargs: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa(\n+            *args, **kwargs\n+        ) + 1,\n+    )\n+\n+# Additional ecosystem cases from https://github.com/astral-sh/ruff/pull/21385\n+class C:\n+    def foo():\n+        mock_service.return_value.bucket.side_effect = lambda name: (\n+            source_bucket\n+            if name == source_bucket_name\n+            else storage.Bucket(mock_service, destination_bucket_name)\n+        )\n+\n+class C:\n+\tfunction_dict: Dict[Text, Callable[[CRFToken], Any]] = {\n+        CRFEntityExtractorOptions.POS2: lambda crf_token: crf_token.pos_tag[:2]\n+        if crf_token.pos_tag is not None\n+        else None,\n+\t}\n+\n+name = re.sub(r\"[^\\x21\\x23-\\x5b\\x5d-\\x7e]...............\", lambda m: f\"\\\\{m.group(0)}\", p[\"name\"])\n+\n+def foo():\n+    if True:\n+        if True:\n+            return (\n+                lambda x: np.exp(cs(np.log(x.to(u.MeV).value))) * u.MeV * u.cm**2 / u.g\n+            )\n+\n+class C:\n+    _is_recognized_dtype: Callable[[DtypeObj], bool] = lambda x: lib.is_np_dtype(\n+        x, \"M\"\n+    ) or isinstance(x, DatetimeTZDtype)\n+\n+class C:\n+    def foo():\n+        if True:\n+            transaction_count = self._query_txs_for_range(\n+                get_count_fn=lambda from_ts, to_ts, _chain_id=chain_id: db_evmtx.count_transactions_in_range(\n+                    chain_id=_chain_id,\n+                    from_ts=from_ts,\n+                    to_ts=to_ts,\n+                ),\n+            )\n+\n+            transaction_count = self._query_txs_for_range(\n+                get_count_fn=lambda from_ts, to_ts, _chain_id=chain_id: db_evmtx.count_transactions_in_range[_chain_id, from_ts, to_ts],\n+            )\n+\n+def ddb():\n+    sql = (\n+        lambda var, table, n=N: f\"\"\"\n+        CREATE TABLE {table} AS\n+        SELECT ROW_NUMBER() OVER () AS id, {var}\n+        FROM (\n+            SELECT {var}\n+            FROM RANGE({n}) _ ({var})\n+            ORDER BY RANDOM()\n+        )\n+        \"\"\"\n+    )\n+\n+long_assignment_target.with_attribute.and_a_slice[with_an_index] = ( # 1\n+    # 2\n+    lambda x, y, z: # 3\n+    # 4\n+    x + y + z # 5\n+    # 6\n+)\n+\n+long_assignment_target.with_attribute.and_a_slice[with_an_index] = (\n+    lambda x, y, z: x + y + z\n+)\n+\n+long_assignment_target.with_attribute.and_a_slice[with_an_index] = lambda x, y, z: x + y + z\n+\n+very_long_variable_name_x, very_long_variable_name_y = lambda a: a + some_very_long_expression, lambda b: b * another_very_long_expression_here\n+\n+very_long_variable_name_for_result += lambda x: very_long_function_call_that_should_definitely_be_parenthesized_now(x, more_args, additional_parameters)\n+\n+\n+if 1:\n+    if 2:\n+        if 3:\n+            if self.location in EVM_EVMLIKE_LOCATIONS and database is not None:\n+                exported_dict[\"notes\"] = EVM_ADDRESS_REGEX.sub(\n+                    repl=lambda matched_address: self._maybe_add_label_with_address(\n+                        database=database,\n+                        matched_address=matched_address,\n+                    ),\n+                    string=exported_dict[\"notes\"],\n+                )\n+\n+class C:\n+    def f():\n+        return dict(\n+            filter(\n+                lambda intent_response: self.is_retrieval_intent_response(\n+                    intent_response\n+                ),\n+                self.responses.items(),\n+            )\n+        )\n+\n+@pytest.mark.parametrize(\n+    \"op\",\n+    [\n+        # Not fluent\n+        param(\n+            lambda left, right: (\n+                ibis.timestamp(\"2017-04-01\")\n+            ),\n+        ),\n+        # These four are fluent and fit on one line inside the parenthesized\n+        # lambda body\n+        param(\n+            lambda left, right: (\n+                ibis.timestamp(\"2017-04-01\").cast(dt.date)\n+            ),\n+        ),\n+        param(\n+            lambda left, right: (\n+                ibis.timestamp(\"2017-04-01\").cast(dt.date).between(left, right)\n+            ),\n+        ),\n+        param(lambda left, right: ibis.timestamp(\"2017-04-01\").cast(dt.date)),\n+        param(lambda left, right: ibis.timestamp(\"2017-04-01\").cast(dt.date).between(left, right)),\n+        # This is too long on one line in the lambda body and gets wrapped\n+        # inside the body.\n+        param(\n+            lambda left, right: (\n+                ibis.timestamp(\"2017-04-01\").cast(dt.date).between(left, right).between(left, right)\n+            ),\n+        ),\n+    ],\n+)\n+def test_string_temporal_compare_between(con, op, left, right): ...",
      "comment": "We have a lot of tests in this file, including with comments in various places. And I didn't bump into anything very interesting when sprinkling more comments into the assignment cases, but I can try injecting more again.",
      "comment_id": 2586774024,
      "user": "ntBre",
      "created_at": "2025-12-03T22:09:57Z",
      "url": "https://github.com/astral-sh/ruff/pull/21385#discussion_r2586774024"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21385,
      "file_path": "crates/ruff_python_formatter/resources/test/fixtures/ruff/expression/lambda.py",
      "line": 456,
      "side": "RIGHT",
      "diff_hunk": "@@ -228,3 +311,155 @@ def a():\n         g = 10\n     )\n \n+def a():\n+    return b(\n+        c,\n+        d,\n+        e,\n+        f=lambda self, *args, **kwargs: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa(\n+            *args, **kwargs\n+        ) + 1,\n+    )\n+\n+# Additional ecosystem cases from https://github.com/astral-sh/ruff/pull/21385\n+class C:\n+    def foo():\n+        mock_service.return_value.bucket.side_effect = lambda name: (\n+            source_bucket\n+            if name == source_bucket_name\n+            else storage.Bucket(mock_service, destination_bucket_name)\n+        )\n+\n+class C:\n+\tfunction_dict: Dict[Text, Callable[[CRFToken], Any]] = {\n+        CRFEntityExtractorOptions.POS2: lambda crf_token: crf_token.pos_tag[:2]\n+        if crf_token.pos_tag is not None\n+        else None,\n+\t}\n+\n+name = re.sub(r\"[^\\x21\\x23-\\x5b\\x5d-\\x7e]...............\", lambda m: f\"\\\\{m.group(0)}\", p[\"name\"])\n+\n+def foo():\n+    if True:\n+        if True:\n+            return (\n+                lambda x: np.exp(cs(np.log(x.to(u.MeV).value))) * u.MeV * u.cm**2 / u.g\n+            )\n+\n+class C:\n+    _is_recognized_dtype: Callable[[DtypeObj], bool] = lambda x: lib.is_np_dtype(\n+        x, \"M\"\n+    ) or isinstance(x, DatetimeTZDtype)\n+\n+class C:\n+    def foo():\n+        if True:\n+            transaction_count = self._query_txs_for_range(\n+                get_count_fn=lambda from_ts, to_ts, _chain_id=chain_id: db_evmtx.count_transactions_in_range(\n+                    chain_id=_chain_id,\n+                    from_ts=from_ts,\n+                    to_ts=to_ts,\n+                ),\n+            )\n+\n+            transaction_count = self._query_txs_for_range(\n+                get_count_fn=lambda from_ts, to_ts, _chain_id=chain_id: db_evmtx.count_transactions_in_range[_chain_id, from_ts, to_ts],\n+            )\n+\n+def ddb():\n+    sql = (\n+        lambda var, table, n=N: f\"\"\"\n+        CREATE TABLE {table} AS\n+        SELECT ROW_NUMBER() OVER () AS id, {var}\n+        FROM (\n+            SELECT {var}\n+            FROM RANGE({n}) _ ({var})\n+            ORDER BY RANDOM()\n+        )\n+        \"\"\"\n+    )\n+\n+long_assignment_target.with_attribute.and_a_slice[with_an_index] = ( # 1\n+    # 2\n+    lambda x, y, z: # 3\n+    # 4\n+    x + y + z # 5\n+    # 6\n+)\n+\n+long_assignment_target.with_attribute.and_a_slice[with_an_index] = (\n+    lambda x, y, z: x + y + z\n+)\n+\n+long_assignment_target.with_attribute.and_a_slice[with_an_index] = lambda x, y, z: x + y + z\n+\n+very_long_variable_name_x, very_long_variable_name_y = lambda a: a + some_very_long_expression, lambda b: b * another_very_long_expression_here\n+\n+very_long_variable_name_for_result += lambda x: very_long_function_call_that_should_definitely_be_parenthesized_now(x, more_args, additional_parameters)\n+\n+\n+if 1:\n+    if 2:\n+        if 3:\n+            if self.location in EVM_EVMLIKE_LOCATIONS and database is not None:\n+                exported_dict[\"notes\"] = EVM_ADDRESS_REGEX.sub(\n+                    repl=lambda matched_address: self._maybe_add_label_with_address(\n+                        database=database,\n+                        matched_address=matched_address,\n+                    ),\n+                    string=exported_dict[\"notes\"],\n+                )\n+\n+class C:\n+    def f():\n+        return dict(\n+            filter(\n+                lambda intent_response: self.is_retrieval_intent_response(\n+                    intent_response\n+                ),\n+                self.responses.items(),\n+            )\n+        )\n+\n+@pytest.mark.parametrize(\n+    \"op\",\n+    [\n+        # Not fluent\n+        param(\n+            lambda left, right: (\n+                ibis.timestamp(\"2017-04-01\")\n+            ),\n+        ),\n+        # These four are fluent and fit on one line inside the parenthesized\n+        # lambda body\n+        param(\n+            lambda left, right: (\n+                ibis.timestamp(\"2017-04-01\").cast(dt.date)\n+            ),\n+        ),\n+        param(\n+            lambda left, right: (\n+                ibis.timestamp(\"2017-04-01\").cast(dt.date).between(left, right)\n+            ),\n+        ),\n+        param(lambda left, right: ibis.timestamp(\"2017-04-01\").cast(dt.date)),\n+        param(lambda left, right: ibis.timestamp(\"2017-04-01\").cast(dt.date).between(left, right)),\n+        # This is too long on one line in the lambda body and gets wrapped\n+        # inside the body.\n+        param(\n+            lambda left, right: (\n+                ibis.timestamp(\"2017-04-01\").cast(dt.date).between(left, right).between(left, right)\n+            ),\n+        ),\n+    ],\n+)\n+def test_string_temporal_compare_between(con, op, left, right): ...",
      "comment": "It's not just about assignment. We also need tests for all sort of combinations between comments and parenthesized bodies (but maybe they all exist already? I'm on my phone and can't check it)",
      "comment_id": 2586791481,
      "user": "MichaReiser",
      "created_at": "2025-12-03T22:17:13Z",
      "url": "https://github.com/astral-sh/ruff/pull/21385#discussion_r2586791481"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21385,
      "file_path": "crates/ruff_python_formatter/resources/test/fixtures/ruff/expression/lambda.py",
      "line": 456,
      "side": "RIGHT",
      "diff_hunk": "@@ -228,3 +311,155 @@ def a():\n         g = 10\n     )\n \n+def a():\n+    return b(\n+        c,\n+        d,\n+        e,\n+        f=lambda self, *args, **kwargs: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa(\n+            *args, **kwargs\n+        ) + 1,\n+    )\n+\n+# Additional ecosystem cases from https://github.com/astral-sh/ruff/pull/21385\n+class C:\n+    def foo():\n+        mock_service.return_value.bucket.side_effect = lambda name: (\n+            source_bucket\n+            if name == source_bucket_name\n+            else storage.Bucket(mock_service, destination_bucket_name)\n+        )\n+\n+class C:\n+\tfunction_dict: Dict[Text, Callable[[CRFToken], Any]] = {\n+        CRFEntityExtractorOptions.POS2: lambda crf_token: crf_token.pos_tag[:2]\n+        if crf_token.pos_tag is not None\n+        else None,\n+\t}\n+\n+name = re.sub(r\"[^\\x21\\x23-\\x5b\\x5d-\\x7e]...............\", lambda m: f\"\\\\{m.group(0)}\", p[\"name\"])\n+\n+def foo():\n+    if True:\n+        if True:\n+            return (\n+                lambda x: np.exp(cs(np.log(x.to(u.MeV).value))) * u.MeV * u.cm**2 / u.g\n+            )\n+\n+class C:\n+    _is_recognized_dtype: Callable[[DtypeObj], bool] = lambda x: lib.is_np_dtype(\n+        x, \"M\"\n+    ) or isinstance(x, DatetimeTZDtype)\n+\n+class C:\n+    def foo():\n+        if True:\n+            transaction_count = self._query_txs_for_range(\n+                get_count_fn=lambda from_ts, to_ts, _chain_id=chain_id: db_evmtx.count_transactions_in_range(\n+                    chain_id=_chain_id,\n+                    from_ts=from_ts,\n+                    to_ts=to_ts,\n+                ),\n+            )\n+\n+            transaction_count = self._query_txs_for_range(\n+                get_count_fn=lambda from_ts, to_ts, _chain_id=chain_id: db_evmtx.count_transactions_in_range[_chain_id, from_ts, to_ts],\n+            )\n+\n+def ddb():\n+    sql = (\n+        lambda var, table, n=N: f\"\"\"\n+        CREATE TABLE {table} AS\n+        SELECT ROW_NUMBER() OVER () AS id, {var}\n+        FROM (\n+            SELECT {var}\n+            FROM RANGE({n}) _ ({var})\n+            ORDER BY RANDOM()\n+        )\n+        \"\"\"\n+    )\n+\n+long_assignment_target.with_attribute.and_a_slice[with_an_index] = ( # 1\n+    # 2\n+    lambda x, y, z: # 3\n+    # 4\n+    x + y + z # 5\n+    # 6\n+)\n+\n+long_assignment_target.with_attribute.and_a_slice[with_an_index] = (\n+    lambda x, y, z: x + y + z\n+)\n+\n+long_assignment_target.with_attribute.and_a_slice[with_an_index] = lambda x, y, z: x + y + z\n+\n+very_long_variable_name_x, very_long_variable_name_y = lambda a: a + some_very_long_expression, lambda b: b * another_very_long_expression_here\n+\n+very_long_variable_name_for_result += lambda x: very_long_function_call_that_should_definitely_be_parenthesized_now(x, more_args, additional_parameters)\n+\n+\n+if 1:\n+    if 2:\n+        if 3:\n+            if self.location in EVM_EVMLIKE_LOCATIONS and database is not None:\n+                exported_dict[\"notes\"] = EVM_ADDRESS_REGEX.sub(\n+                    repl=lambda matched_address: self._maybe_add_label_with_address(\n+                        database=database,\n+                        matched_address=matched_address,\n+                    ),\n+                    string=exported_dict[\"notes\"],\n+                )\n+\n+class C:\n+    def f():\n+        return dict(\n+            filter(\n+                lambda intent_response: self.is_retrieval_intent_response(\n+                    intent_response\n+                ),\n+                self.responses.items(),\n+            )\n+        )\n+\n+@pytest.mark.parametrize(\n+    \"op\",\n+    [\n+        # Not fluent\n+        param(\n+            lambda left, right: (\n+                ibis.timestamp(\"2017-04-01\")\n+            ),\n+        ),\n+        # These four are fluent and fit on one line inside the parenthesized\n+        # lambda body\n+        param(\n+            lambda left, right: (\n+                ibis.timestamp(\"2017-04-01\").cast(dt.date)\n+            ),\n+        ),\n+        param(\n+            lambda left, right: (\n+                ibis.timestamp(\"2017-04-01\").cast(dt.date).between(left, right)\n+            ),\n+        ),\n+        param(lambda left, right: ibis.timestamp(\"2017-04-01\").cast(dt.date)),\n+        param(lambda left, right: ibis.timestamp(\"2017-04-01\").cast(dt.date).between(left, right)),\n+        # This is too long on one line in the lambda body and gets wrapped\n+        # inside the body.\n+        param(\n+            lambda left, right: (\n+                ibis.timestamp(\"2017-04-01\").cast(dt.date).between(left, right).between(left, right)\n+            ),\n+        ),\n+    ],\n+)\n+def test_string_temporal_compare_between(con, op, left, right): ...",
      "comment": "I probably phrased that poorly. I just meant nothing went wrong in the assignment cases, so I kind of assumed all comment placement was handled well. I will try injecting more comments into various (non-assignment) cases again.",
      "comment_id": 2586795316,
      "user": "ntBre",
      "created_at": "2025-12-03T22:19:06Z",
      "url": "https://github.com/astral-sh/ruff/pull/21385#discussion_r2586795316"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21385,
      "file_path": "crates/ruff_python_formatter/resources/test/fixtures/ruff/expression/lambda.py",
      "line": 456,
      "side": "RIGHT",
      "diff_hunk": "@@ -228,3 +311,155 @@ def a():\n         g = 10\n     )\n \n+def a():\n+    return b(\n+        c,\n+        d,\n+        e,\n+        f=lambda self, *args, **kwargs: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa(\n+            *args, **kwargs\n+        ) + 1,\n+    )\n+\n+# Additional ecosystem cases from https://github.com/astral-sh/ruff/pull/21385\n+class C:\n+    def foo():\n+        mock_service.return_value.bucket.side_effect = lambda name: (\n+            source_bucket\n+            if name == source_bucket_name\n+            else storage.Bucket(mock_service, destination_bucket_name)\n+        )\n+\n+class C:\n+\tfunction_dict: Dict[Text, Callable[[CRFToken], Any]] = {\n+        CRFEntityExtractorOptions.POS2: lambda crf_token: crf_token.pos_tag[:2]\n+        if crf_token.pos_tag is not None\n+        else None,\n+\t}\n+\n+name = re.sub(r\"[^\\x21\\x23-\\x5b\\x5d-\\x7e]...............\", lambda m: f\"\\\\{m.group(0)}\", p[\"name\"])\n+\n+def foo():\n+    if True:\n+        if True:\n+            return (\n+                lambda x: np.exp(cs(np.log(x.to(u.MeV).value))) * u.MeV * u.cm**2 / u.g\n+            )\n+\n+class C:\n+    _is_recognized_dtype: Callable[[DtypeObj], bool] = lambda x: lib.is_np_dtype(\n+        x, \"M\"\n+    ) or isinstance(x, DatetimeTZDtype)\n+\n+class C:\n+    def foo():\n+        if True:\n+            transaction_count = self._query_txs_for_range(\n+                get_count_fn=lambda from_ts, to_ts, _chain_id=chain_id: db_evmtx.count_transactions_in_range(\n+                    chain_id=_chain_id,\n+                    from_ts=from_ts,\n+                    to_ts=to_ts,\n+                ),\n+            )\n+\n+            transaction_count = self._query_txs_for_range(\n+                get_count_fn=lambda from_ts, to_ts, _chain_id=chain_id: db_evmtx.count_transactions_in_range[_chain_id, from_ts, to_ts],\n+            )\n+\n+def ddb():\n+    sql = (\n+        lambda var, table, n=N: f\"\"\"\n+        CREATE TABLE {table} AS\n+        SELECT ROW_NUMBER() OVER () AS id, {var}\n+        FROM (\n+            SELECT {var}\n+            FROM RANGE({n}) _ ({var})\n+            ORDER BY RANDOM()\n+        )\n+        \"\"\"\n+    )\n+\n+long_assignment_target.with_attribute.and_a_slice[with_an_index] = ( # 1\n+    # 2\n+    lambda x, y, z: # 3\n+    # 4\n+    x + y + z # 5\n+    # 6\n+)\n+\n+long_assignment_target.with_attribute.and_a_slice[with_an_index] = (\n+    lambda x, y, z: x + y + z\n+)\n+\n+long_assignment_target.with_attribute.and_a_slice[with_an_index] = lambda x, y, z: x + y + z\n+\n+very_long_variable_name_x, very_long_variable_name_y = lambda a: a + some_very_long_expression, lambda b: b * another_very_long_expression_here\n+\n+very_long_variable_name_for_result += lambda x: very_long_function_call_that_should_definitely_be_parenthesized_now(x, more_args, additional_parameters)\n+\n+\n+if 1:\n+    if 2:\n+        if 3:\n+            if self.location in EVM_EVMLIKE_LOCATIONS and database is not None:\n+                exported_dict[\"notes\"] = EVM_ADDRESS_REGEX.sub(\n+                    repl=lambda matched_address: self._maybe_add_label_with_address(\n+                        database=database,\n+                        matched_address=matched_address,\n+                    ),\n+                    string=exported_dict[\"notes\"],\n+                )\n+\n+class C:\n+    def f():\n+        return dict(\n+            filter(\n+                lambda intent_response: self.is_retrieval_intent_response(\n+                    intent_response\n+                ),\n+                self.responses.items(),\n+            )\n+        )\n+\n+@pytest.mark.parametrize(\n+    \"op\",\n+    [\n+        # Not fluent\n+        param(\n+            lambda left, right: (\n+                ibis.timestamp(\"2017-04-01\")\n+            ),\n+        ),\n+        # These four are fluent and fit on one line inside the parenthesized\n+        # lambda body\n+        param(\n+            lambda left, right: (\n+                ibis.timestamp(\"2017-04-01\").cast(dt.date)\n+            ),\n+        ),\n+        param(\n+            lambda left, right: (\n+                ibis.timestamp(\"2017-04-01\").cast(dt.date).between(left, right)\n+            ),\n+        ),\n+        param(lambda left, right: ibis.timestamp(\"2017-04-01\").cast(dt.date)),\n+        param(lambda left, right: ibis.timestamp(\"2017-04-01\").cast(dt.date).between(left, right)),\n+        # This is too long on one line in the lambda body and gets wrapped\n+        # inside the body.\n+        param(\n+            lambda left, right: (\n+                ibis.timestamp(\"2017-04-01\").cast(dt.date).between(left, right).between(left, right)\n+            ),\n+        ),\n+    ],\n+)\n+def test_string_temporal_compare_between(con, op, left, right): ...",
      "comment": "I think I got the dangling comments working, besides the ones interspersed with parameters. Those seem a bit trickier because they change whether the lambda needs to be parenthesized. I'm running into an instability with my first naive pass, but I'll keep looking. It was pretty nice to bail out of the parameter flattening if there were any comments :laughing: but I agree that this would look better if we can get it to work.",
      "comment_id": 2592957314,
      "user": "ntBre",
      "created_at": "2025-12-05T14:50:51Z",
      "url": "https://github.com/astral-sh/ruff/pull/21385#discussion_r2592957314"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21385,
      "file_path": "crates/ruff_python_formatter/resources/test/fixtures/ruff/expression/lambda.py",
      "line": 456,
      "side": "RIGHT",
      "diff_hunk": "@@ -228,3 +311,155 @@ def a():\n         g = 10\n     )\n \n+def a():\n+    return b(\n+        c,\n+        d,\n+        e,\n+        f=lambda self, *args, **kwargs: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa(\n+            *args, **kwargs\n+        ) + 1,\n+    )\n+\n+# Additional ecosystem cases from https://github.com/astral-sh/ruff/pull/21385\n+class C:\n+    def foo():\n+        mock_service.return_value.bucket.side_effect = lambda name: (\n+            source_bucket\n+            if name == source_bucket_name\n+            else storage.Bucket(mock_service, destination_bucket_name)\n+        )\n+\n+class C:\n+\tfunction_dict: Dict[Text, Callable[[CRFToken], Any]] = {\n+        CRFEntityExtractorOptions.POS2: lambda crf_token: crf_token.pos_tag[:2]\n+        if crf_token.pos_tag is not None\n+        else None,\n+\t}\n+\n+name = re.sub(r\"[^\\x21\\x23-\\x5b\\x5d-\\x7e]...............\", lambda m: f\"\\\\{m.group(0)}\", p[\"name\"])\n+\n+def foo():\n+    if True:\n+        if True:\n+            return (\n+                lambda x: np.exp(cs(np.log(x.to(u.MeV).value))) * u.MeV * u.cm**2 / u.g\n+            )\n+\n+class C:\n+    _is_recognized_dtype: Callable[[DtypeObj], bool] = lambda x: lib.is_np_dtype(\n+        x, \"M\"\n+    ) or isinstance(x, DatetimeTZDtype)\n+\n+class C:\n+    def foo():\n+        if True:\n+            transaction_count = self._query_txs_for_range(\n+                get_count_fn=lambda from_ts, to_ts, _chain_id=chain_id: db_evmtx.count_transactions_in_range(\n+                    chain_id=_chain_id,\n+                    from_ts=from_ts,\n+                    to_ts=to_ts,\n+                ),\n+            )\n+\n+            transaction_count = self._query_txs_for_range(\n+                get_count_fn=lambda from_ts, to_ts, _chain_id=chain_id: db_evmtx.count_transactions_in_range[_chain_id, from_ts, to_ts],\n+            )\n+\n+def ddb():\n+    sql = (\n+        lambda var, table, n=N: f\"\"\"\n+        CREATE TABLE {table} AS\n+        SELECT ROW_NUMBER() OVER () AS id, {var}\n+        FROM (\n+            SELECT {var}\n+            FROM RANGE({n}) _ ({var})\n+            ORDER BY RANDOM()\n+        )\n+        \"\"\"\n+    )\n+\n+long_assignment_target.with_attribute.and_a_slice[with_an_index] = ( # 1\n+    # 2\n+    lambda x, y, z: # 3\n+    # 4\n+    x + y + z # 5\n+    # 6\n+)\n+\n+long_assignment_target.with_attribute.and_a_slice[with_an_index] = (\n+    lambda x, y, z: x + y + z\n+)\n+\n+long_assignment_target.with_attribute.and_a_slice[with_an_index] = lambda x, y, z: x + y + z\n+\n+very_long_variable_name_x, very_long_variable_name_y = lambda a: a + some_very_long_expression, lambda b: b * another_very_long_expression_here\n+\n+very_long_variable_name_for_result += lambda x: very_long_function_call_that_should_definitely_be_parenthesized_now(x, more_args, additional_parameters)\n+\n+\n+if 1:\n+    if 2:\n+        if 3:\n+            if self.location in EVM_EVMLIKE_LOCATIONS and database is not None:\n+                exported_dict[\"notes\"] = EVM_ADDRESS_REGEX.sub(\n+                    repl=lambda matched_address: self._maybe_add_label_with_address(\n+                        database=database,\n+                        matched_address=matched_address,\n+                    ),\n+                    string=exported_dict[\"notes\"],\n+                )\n+\n+class C:\n+    def f():\n+        return dict(\n+            filter(\n+                lambda intent_response: self.is_retrieval_intent_response(\n+                    intent_response\n+                ),\n+                self.responses.items(),\n+            )\n+        )\n+\n+@pytest.mark.parametrize(\n+    \"op\",\n+    [\n+        # Not fluent\n+        param(\n+            lambda left, right: (\n+                ibis.timestamp(\"2017-04-01\")\n+            ),\n+        ),\n+        # These four are fluent and fit on one line inside the parenthesized\n+        # lambda body\n+        param(\n+            lambda left, right: (\n+                ibis.timestamp(\"2017-04-01\").cast(dt.date)\n+            ),\n+        ),\n+        param(\n+            lambda left, right: (\n+                ibis.timestamp(\"2017-04-01\").cast(dt.date).between(left, right)\n+            ),\n+        ),\n+        param(lambda left, right: ibis.timestamp(\"2017-04-01\").cast(dt.date)),\n+        param(lambda left, right: ibis.timestamp(\"2017-04-01\").cast(dt.date).between(left, right)),\n+        # This is too long on one line in the lambda body and gets wrapped\n+        # inside the body.\n+        param(\n+            lambda left, right: (\n+                ibis.timestamp(\"2017-04-01\").cast(dt.date).between(left, right).between(left, right)\n+            ),\n+        ),\n+    ],\n+)\n+def test_string_temporal_compare_between(con, op, left, right): ...",
      "comment": ">  It was pretty nice to bail out of the parameter flattening if there were any comments \r\n\r\nWe can still bail if there are any comments within `Parameters`. I just think we shouldn't if there are comments after the `lambda`, before the body",
      "comment_id": 2593013649,
      "user": "MichaReiser",
      "created_at": "2025-12-05T15:08:28Z",
      "url": "https://github.com/astral-sh/ruff/pull/21385#discussion_r2593013649"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21385,
      "file_path": "crates/ruff_python_formatter/resources/test/fixtures/ruff/expression/lambda.py",
      "line": 630,
      "side": "RIGHT",
      "diff_hunk": "@@ -228,6 +311,399 @@ def a():\n         g = 10\n     )\n \n+def a():\n+    return b(\n+        c,\n+        d,\n+        e,\n+        f=lambda self, *args, **kwargs: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa(\n+            *args, **kwargs\n+        ) + 1,\n+    )\n+\n+# Additional ecosystem cases from https://github.com/astral-sh/ruff/pull/21385\n+class C:\n+    def foo():\n+        mock_service.return_value.bucket.side_effect = lambda name: (\n+            source_bucket\n+            if name == source_bucket_name\n+            else storage.Bucket(mock_service, destination_bucket_name)\n+        )\n+\n+class C:\n+\tfunction_dict: Dict[Text, Callable[[CRFToken], Any]] = {\n+        CRFEntityExtractorOptions.POS2: lambda crf_token: crf_token.pos_tag[:2]\n+        if crf_token.pos_tag is not None\n+        else None,\n+\t}\n+\n+name = re.sub(r\"[^\\x21\\x23-\\x5b\\x5d-\\x7e]...............\", lambda m: f\"\\\\{m.group(0)}\", p[\"name\"])\n+\n+def foo():\n+    if True:\n+        if True:\n+            return (\n+                lambda x: np.exp(cs(np.log(x.to(u.MeV).value))) * u.MeV * u.cm**2 / u.g\n+            )\n+\n+class C:\n+    _is_recognized_dtype: Callable[[DtypeObj], bool] = lambda x: lib.is_np_dtype(\n+        x, \"M\"\n+    ) or isinstance(x, DatetimeTZDtype)\n+\n+class C:\n+    def foo():\n+        if True:\n+            transaction_count = self._query_txs_for_range(\n+                get_count_fn=lambda from_ts, to_ts, _chain_id=chain_id: db_evmtx.count_transactions_in_range(\n+                    chain_id=_chain_id,\n+                    from_ts=from_ts,\n+                    to_ts=to_ts,\n+                ),\n+            )\n+\n+            transaction_count = self._query_txs_for_range(\n+                get_count_fn=lambda from_ts, to_ts, _chain_id=chain_id: db_evmtx.count_transactions_in_range[_chain_id, from_ts, to_ts],\n+            )\n+\n+def ddb():\n+    sql = (\n+        lambda var, table, n=N: f\"\"\"\n+        CREATE TABLE {table} AS\n+        SELECT ROW_NUMBER() OVER () AS id, {var}\n+        FROM (\n+            SELECT {var}\n+            FROM RANGE({n}) _ ({var})\n+            ORDER BY RANDOM()\n+        )\n+        \"\"\"\n+    )\n+\n+long_assignment_target.with_attribute.and_a_slice[with_an_index] = ( # 1\n+    # 2\n+    lambda x, y, z: # 3\n+    # 4\n+    x + y + z # 5\n+    # 6\n+)\n+\n+long_assignment_target.with_attribute.and_a_slice[with_an_index] = (\n+    lambda x, y, z: x + y + z\n+)\n+\n+long_assignment_target.with_attribute.and_a_slice[with_an_index] = lambda x, y, z: x + y + z\n+\n+very_long_variable_name_x, very_long_variable_name_y = lambda a: a + some_very_long_expression, lambda b: b * another_very_long_expression_here\n+\n+very_long_variable_name_for_result += lambda x: very_long_function_call_that_should_definitely_be_parenthesized_now(x, more_args, additional_parameters)\n+\n+\n+if 1:\n+    if 2:\n+        if 3:\n+            if self.location in EVM_EVMLIKE_LOCATIONS and database is not None:\n+                exported_dict[\"notes\"] = EVM_ADDRESS_REGEX.sub(\n+                    repl=lambda matched_address: self._maybe_add_label_with_address(\n+                        database=database,\n+                        matched_address=matched_address,\n+                    ),\n+                    string=exported_dict[\"notes\"],\n+                )\n+\n+class C:\n+    def f():\n+        return dict(\n+            filter(\n+                lambda intent_response: self.is_retrieval_intent_response(\n+                    intent_response\n+                ),\n+                self.responses.items(),\n+            )\n+        )\n+\n+@pytest.mark.parametrize(\n+    \"op\",\n+    [\n+        # Not fluent\n+        param(\n+            lambda left, right: (\n+                ibis.timestamp(\"2017-04-01\")\n+            ),\n+        ),\n+        # These four are fluent and fit on one line inside the parenthesized\n+        # lambda body\n+        param(\n+            lambda left, right: (\n+                ibis.timestamp(\"2017-04-01\").cast(dt.date)\n+            ),\n+        ),\n+        param(\n+            lambda left, right: (\n+                ibis.timestamp(\"2017-04-01\").cast(dt.date).between(left, right)\n+            ),\n+        ),\n+        param(lambda left, right: ibis.timestamp(\"2017-04-01\").cast(dt.date)),\n+        param(lambda left, right: ibis.timestamp(\"2017-04-01\").cast(dt.date).between(left, right)),\n+        # This is too long on one line in the lambda body and gets wrapped\n+        # inside the body.\n+        param(\n+            lambda left, right: (\n+                ibis.timestamp(\"2017-04-01\").cast(dt.date).between(left, right).between(left, right)\n+            ),\n+        ),\n+    ],\n+)\n+def test_string_temporal_compare_between(con, op, left, right): ...\n+\n+[\n+    (\n+        lambda eval_df, _: MetricValue(\n+            scores=eval_df[\"prediction\"].tolist(),\n+            aggregate_results={\"prediction_sum\": sum(eval_df[\"prediction\"])},\n+        )\n+    ),\n+]\n+\n+# reuses the list parentheses\n+lambda xxxxxxxxxxxxxxxxxxxx, yyyyyyyyyyyyyyyyyyyy, zzzzzzzzzzzzzzzzzzzz: [xxxxxxxxxxxxxxxxxxxx, yyyyyyyyyyyyyyyyyyyy, zzzzzzzzzzzzzzzzzzzz]\n+\n+# adds parentheses around the body\n+lambda xxxxxxxxxxxxxxxxxxxx, yyyyyyyyyyyyyyyyyyyy, zzzzzzzzzzzzzzzzzzzz: xxxxxxxxxxxxxxxxxxxx + yyyyyyyyyyyyyyyyyyyy + zzzzzzzzzzzzzzzzzzzz\n+\n+# removes parentheses around the body\n+lambda xxxxxxxxxxxxxxxxxxxx: (xxxxxxxxxxxxxxxxxxxx + 1)\n+\n+mapper = lambda x: dict_with_default[np.nan if isinstance(x, float) and np.isnan(x) else x]\n+\n+lambda x, y, z: (\n+    x + y + z\n+)\n+\n+lambda x, y, z: (\n+    x + y + z\n+    # trailing body\n+)\n+\n+lambda x, y, z: (\n+    x + y + z  # trailing eol body\n+)\n+\n+lambda x, y, z: (\n+    x + y + z\n+) # trailing lambda\n+\n+lambda x, y, z: (\n+    # leading body\n+    x + y + z\n+)\n+\n+lambda x, y, z: (  # leading eol body\n+    x + y + z\n+)\n+\n+(\n+    lambda name:\n+    source_bucket  # trailing eol comment\n+    if name == source_bucket_name\n+    else storage.Bucket(mock_service, destination_bucket_name)\n+)\n+\n+(\n+    lambda name:\n+    # dangling header comment\n+    source_bucket\n+    if name == source_bucket_name\n+    else storage.Bucket(mock_service, destination_bucket_name)\n+)\n+\n+x = (\n+    lambda name:\n+    # dangling header comment\n+    source_bucket\n+    if name == source_bucket_name\n+    else storage.Bucket(mock_service, destination_bucket_name)\n+)\n+\n+(\n+    lambda name: # dangling header comment\n+    (\n+        source_bucket\n+        if name == source_bucket_name\n+        else storage.Bucket(mock_service, destination_bucket_name)\n+    )\n+)\n+\n+(\n+    lambda from_ts, to_ts, _chain_id=chain_id:  # dangling eol header comment\n+    db_evmtx.count_transactions_in_range(\n+        chain_id=_chain_id,\n+        from_ts=from_ts,\n+        to_ts=to_ts,\n+    )\n+)\n+\n+(\n+    lambda from_ts, to_ts, _chain_id=chain_id:\n+    # dangling header comment before call\n+    db_evmtx.count_transactions_in_range(\n+        chain_id=_chain_id,\n+        from_ts=from_ts,\n+        to_ts=to_ts,\n+    )\n+)\n+\n+(\n+    lambda left, right:\n+    # comment\n+    ibis.timestamp(\"2017-04-01\").cast(dt.date).between(left, right)\n+)\n+\n+(\n+    lambda left, right:\n+    ibis.timestamp(\"2017-04-01\")  # comment\n+    .cast(dt.date)\n+    .between(left, right)\n+)\n+\n+(\n+    lambda xxxxxxxxxxxxxxxxxxxx, yyyyyyyyyyyyyyyyyyyy:\n+    # comment\n+    [xxxxxxxxxxxxxxxxxxxx, yyyyyyyyyyyyyyyyyyyy, zzzzzzzzzzzzzzzzzzzz]\n+)\n+\n+(\n+    lambda x, y:\n+    # comment\n+    {\n+        \"key\": x,\n+        \"another\": y,\n+    }\n+)\n+\n+(\n+    lambda x, y:\n+    # comment\n+    (\n+        x,\n+        y,\n+        z\n+    )\n+)\n+\n+(\n+    lambda x:\n+    # comment\n+    dict_with_default[np.nan if isinstance(x, float) and np.isnan(x) else x]\n+)\n+\n+(\n+    lambda from_ts, to_ts, _chain_id=chain_id:\n+    db_evmtx.count_transactions_in_range[\n+        # comment\n+        _chain_id, from_ts, to_ts\n+    ]\n+)\n+\n+(\n+    lambda\n+    # comment\n+    *args, **kwargs:\n+    aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa(*args, **kwargs) + 1\n+)\n+\n+(\n+    lambda  # comment\n+    *args, **kwargs:\n+    aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa(*args, **kwargs) + 1\n+)\n+\n+(\n+    lambda  # comment 1\n+    # comment 2\n+    *args, **kwargs: # comment 3\n+    aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa(*args, **kwargs) + 1\n+)\n+\n+(\n+    lambda  # comment 1\n+    *args, **kwargs: # comment 3\n+    aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa(*args, **kwargs) + 1",
      "comment": "Can you add some tests where the call expression is or isn't parenthesized and has comments?",
      "comment_id": 2607521713,
      "user": "MichaReiser",
      "created_at": "2025-12-10T17:11:28Z",
      "url": "https://github.com/astral-sh/ruff/pull/21385#discussion_r2607521713"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21895,
      "file_path": "crates/ruff_python_parser/resources/valid/expressions/21538.py",
      "line": 2,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,2 @@\n+# Regression test for https://github.com/astral-sh/ruff/issues/21538\n+foo = \"{'bar': {\\t'\"",
      "comment": "This test already passes on `main`. The panic in that issue is caused because RUF027 calls `parse_expression` on `f\"{'bar': {\\t'\"`.",
      "comment_id": 2606822195,
      "user": "dylwil3",
      "created_at": "2025-12-10T14:12:33Z",
      "url": "https://github.com/astral-sh/ruff/pull/21895#discussion_r2606822195"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21840,
      "file_path": "crates/ty_python_semantic/resources/corpus/cyclic_pep695_typevars.py",
      "line": 12,
      "side": "RIGHT",
      "diff_hunk": "@@ -3,3 +3,10 @@ def name_1[name_0: name_0](name_2: name_0):\n         pass\n     except name_2:\n         pass\n+\n+def _[T: (T if cond else U)[0], U](): pass\n+\n+class _[T: (0, T[0])]:\n+    def _(x: T):\n+        if x:\n+            pass",
      "comment": "I'd prefer it if we could put each minimal repro in a separate file, so that we can be confident that they're not \"interfering\" with each other",
      "comment_id": 2598906820,
      "user": "AlexWaygood",
      "created_at": "2025-12-08T14:44:10Z",
      "url": "https://github.com/astral-sh/ruff/pull/21840#discussion_r2598906820"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21382,
      "file_path": "crates/ruff_linter/resources/test/fixtures/pylint/useless_exception_statement.py",
      "line": 6,
      "side": "RIGHT",
      "diff_hunk": "@@ -2,15 +2,34 @@\n from contextlib import suppress\n \n \n+class MyError(Exception):\n+    ...",
      "comment": "Should we try one test case with another link in the inheritance chain like:\n\n```py\nclass MySubError(MyError): ...\n```\n\nI think `any_base_class` is recursive, so we'll handle that too.",
      "comment_id": 2535323613,
      "user": "ntBre",
      "created_at": "2025-11-17T19:45:54Z",
      "url": "https://github.com/astral-sh/ruff/pull/21382#discussion_r2535323613"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21382,
      "file_path": "crates/ruff_linter/resources/test/fixtures/pylint/useless_exception_statement.py",
      "line": 27,
      "side": "RIGHT",
      "diff_hunk": "@@ -2,15 +2,40 @@\n from contextlib import suppress\n \n \n+class MyError(Exception):\n+    ...\n+\n+\n+class MySubError(MyError):\n+    ...\n+\n+\n+class MyValueError(ValueError):\n+    ...\n+\n+\n+class MyUserWarning(UserWarning):\n+    ...\n+\n+\n+# Violation test cases with builtin errors: PLW0133\n+\n+\n # Test case 1: Useless exception statement\n def func():\n     AssertionError(\"This is an assertion error\")  # PLW0133\n+    MyError(\"This is a custom error\")  # PLW0133 (review)",
      "comment": "Should we remove these `(review)` suffixes? Happy to delete them myself, I was just curious if they were needed for some reason.",
      "comment_id": 2599810148,
      "user": "ntBre",
      "created_at": "2025-12-08T19:09:50Z",
      "url": "https://github.com/astral-sh/ruff/pull/21382#discussion_r2599810148"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21382,
      "file_path": "crates/ruff_linter/resources/test/fixtures/pylint/useless_exception_statement.py",
      "line": 27,
      "side": "RIGHT",
      "diff_hunk": "@@ -2,15 +2,40 @@\n from contextlib import suppress\n \n \n+class MyError(Exception):\n+    ...\n+\n+\n+class MySubError(MyError):\n+    ...\n+\n+\n+class MyValueError(ValueError):\n+    ...\n+\n+\n+class MyUserWarning(UserWarning):\n+    ...\n+\n+\n+# Violation test cases with builtin errors: PLW0133\n+\n+\n # Test case 1: Useless exception statement\n def func():\n     AssertionError(\"This is an assertion error\")  # PLW0133\n+    MyError(\"This is a custom error\")  # PLW0133 (review)",
      "comment": "I thought those comments made it a little easier when reviewing cargo insta snapshots, but I totally understand if you prefer not to have them, it'll be one less thing to worry about when making the preview behavior stable.\nI did it in a specific commit on purpose, so I'll just revert it :)",
      "comment_id": 2600314433,
      "user": "LoicRiegel",
      "created_at": "2025-12-08T22:11:54Z",
      "url": "https://github.com/astral-sh/ruff/pull/21382#discussion_r2600314433"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21382,
      "file_path": "crates/ruff_linter/resources/test/fixtures/pylint/useless_exception_statement.py",
      "line": 27,
      "side": "RIGHT",
      "diff_hunk": "@@ -2,15 +2,40 @@\n from contextlib import suppress\n \n \n+class MyError(Exception):\n+    ...\n+\n+\n+class MySubError(MyError):\n+    ...\n+\n+\n+class MyValueError(ValueError):\n+    ...\n+\n+\n+class MyUserWarning(UserWarning):\n+    ...\n+\n+\n+# Violation test cases with builtin errors: PLW0133\n+\n+\n # Test case 1: Useless exception statement\n def func():\n     AssertionError(\"This is an assertion error\")  # PLW0133\n+    MyError(\"This is a custom error\")  # PLW0133 (review)",
      "comment": "Ohhh, are they supposed to say `preview` instead of `review`? That does make some sense to me! I thought they were related to the code review, but it may have just been a typo.",
      "comment_id": 2600351868,
      "user": "ntBre",
      "created_at": "2025-12-08T22:27:07Z",
      "url": "https://github.com/astral-sh/ruff/pull/21382#discussion_r2600351868"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21382,
      "file_path": "crates/ruff_linter/resources/test/fixtures/pylint/useless_exception_statement.py",
      "line": 27,
      "side": "RIGHT",
      "diff_hunk": "@@ -2,15 +2,40 @@\n from contextlib import suppress\n \n \n+class MyError(Exception):\n+    ...\n+\n+\n+class MySubError(MyError):\n+    ...\n+\n+\n+class MyValueError(ValueError):\n+    ...\n+\n+\n+class MyUserWarning(UserWarning):\n+    ...\n+\n+\n+# Violation test cases with builtin errors: PLW0133\n+\n+\n # Test case 1: Useless exception statement\n def func():\n     AssertionError(\"This is an assertion error\")  # PLW0133\n+    MyError(\"This is a custom error\")  # PLW0133 (review)",
      "comment": "Oh my god yes I meant to write \"preview*! So sorry for the typo \nOkay, so let me know if you want me to revert, otherwise I'll fix the typo :)",
      "comment_id": 2600510684,
      "user": "LoicRiegel",
      "created_at": "2025-12-08T23:32:11Z",
      "url": "https://github.com/astral-sh/ruff/pull/21382#discussion_r2600510684"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21382,
      "file_path": "crates/ruff_linter/resources/test/fixtures/pylint/useless_exception_statement.py",
      "line": 27,
      "side": "RIGHT",
      "diff_hunk": "@@ -2,15 +2,40 @@\n from contextlib import suppress\n \n \n+class MyError(Exception):\n+    ...\n+\n+\n+class MySubError(MyError):\n+    ...\n+\n+\n+class MyValueError(ValueError):\n+    ...\n+\n+\n+class MyUserWarning(UserWarning):\n+    ...\n+\n+\n+# Violation test cases with builtin errors: PLW0133\n+\n+\n # Test case 1: Useless exception statement\n def func():\n     AssertionError(\"This is an assertion error\")  # PLW0133\n+    MyError(\"This is a custom error\")  # PLW0133 (review)",
      "comment": "I'd probably still lean toward reverting now that we're using `assert_diagnostics_diff`, but either works for me.",
      "comment_id": 2600528282,
      "user": "ntBre",
      "created_at": "2025-12-08T23:41:03Z",
      "url": "https://github.com/astral-sh/ruff/pull/21382#discussion_r2600528282"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21783,
      "file_path": "crates/ruff_linter/resources/test/fixtures/ruff/suppressions.py",
      "line": 60,
      "side": "RIGHT",
      "diff_hunk": "@@ -54,3 +54,35 @@ def f():\n     # ruff:disable[E741,F841]\n     I = 1  # noqa: E741,F841\n     # ruff:enable[E741,F841]\n+\n+\n+def f():\n+    # TODO: Duplicate codes should be counted as duplicate, not unused",
      "comment": "It's fine if one of them counts as unused, for as long as we remove the right one.",
      "comment_id": 2603208332,
      "user": "MichaReiser",
      "created_at": "2025-12-09T15:40:48Z",
      "url": "https://github.com/astral-sh/ruff/pull/21783#discussion_r2603208332"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21623,
      "file_path": "crates/ruff_linter/resources/test/fixtures/ruff/suppressions.py",
      "line": 25,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,45 @@\n+def f():\n+    # These should both be ignored by the range suppression.\n+    # ruff: disable[E741, F841]\n+    I = 1\n+    # ruff: enable[E741, F841]\n+\n+\n+def f():\n+    # These should both be ignored by the range suppression.\n+    # ruff:disable[E741,F841]\n+    I = 1\n+    # ruff:enable[E741,F841]\n+\n+\n+def f():\n+    # These should both be ignored by the range suppression.\n+    # ruff: disable[E741]\n+    # ruff: disable[F841]\n+    I = 1\n+    # ruff: enable[E741]\n+    # ruff: enable[F841]\n+\n+\n+def f():\n+    # One should both be ignored by the range suppression, and",
      "comment": "```suggestion\r\n    # One should be ignored by the range suppression, and\r\n```",
      "comment_id": 2580159112,
      "user": "MichaReiser",
      "created_at": "2025-12-02T08:29:34Z",
      "url": "https://github.com/astral-sh/ruff/pull/21623#discussion_r2580159112"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21623,
      "file_path": "crates/ruff_linter/resources/test/fixtures/ruff/suppressions.py",
      "line": 33,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,45 @@\n+def f():\n+    # These should both be ignored by the range suppression.\n+    # ruff: disable[E741, F841]\n+    I = 1\n+    # ruff: enable[E741, F841]\n+\n+\n+def f():\n+    # These should both be ignored by the range suppression.\n+    # ruff:disable[E741,F841]\n+    I = 1\n+    # ruff:enable[E741,F841]\n+\n+\n+def f():\n+    # These should both be ignored by the range suppression.\n+    # ruff: disable[E741]\n+    # ruff: disable[F841]\n+    I = 1\n+    # ruff: enable[E741]\n+    # ruff: enable[F841]\n+\n+\n+def f():\n+    # One should both be ignored by the range suppression, and\n+    # the other logged to the user.\n+    # ruff: disable[E741]\n+    I = 1\n+    # ruff: enable[E741]\n+\n+\n+def f():\n+    # Neither of these are ignored and warning is",
      "comment": "```suggestion\r\n    # Neither of these are ignored and a warning is\r\n```",
      "comment_id": 2580159963,
      "user": "MichaReiser",
      "created_at": "2025-12-02T08:29:50Z",
      "url": "https://github.com/astral-sh/ruff/pull/21623#discussion_r2580159963"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21623,
      "file_path": "crates/ruff_linter/resources/test/fixtures/ruff/suppressions.py",
      "line": 21,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,45 @@\n+def f():\n+    # These should both be ignored by the range suppression.\n+    # ruff: disable[E741, F841]\n+    I = 1\n+    # ruff: enable[E741, F841]\n+\n+\n+def f():\n+    # These should both be ignored by the range suppression.\n+    # ruff:disable[E741,F841]\n+    I = 1\n+    # ruff:enable[E741,F841]\n+\n+\n+def f():\n+    # These should both be ignored by the range suppression.\n+    # ruff: disable[E741]\n+    # ruff: disable[F841]\n+    I = 1\n+    # ruff: enable[E741]\n+    # ruff: enable[F841]",
      "comment": "I think it would be good to add/move some of the more advanced test cases involving blocks and make them integration tests instead. ",
      "comment_id": 2580163067,
      "user": "MichaReiser",
      "created_at": "2025-12-02T08:30:58Z",
      "url": "https://github.com/astral-sh/ruff/pull/21623#discussion_r2580163067"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21844,
      "file_path": "python/py-fuzzer/fuzz.py",
      "line": 78,
      "side": "RIGHT",
      "diff_hunk": "@@ -44,14 +46,36 @@\n Seed = NewType(\"Seed\", int)\n ExitCode = NewType(\"ExitCode\", int)\n \n+TY_TARGET_PLATFORM: Final = \"linux\"\n+\n+with Path(__file__).parent.parent.parent.joinpath(\"pyproject.toml\").open(\"rb\") as f:\n+    pyproject_toml = tomllib.load(f)\n+\n+pyproject_specifier = packaging.specifiers.Specifier(\n+    pyproject_toml[\"project\"][\"requires-python\"]\n+)\n+assert pyproject_specifier.operator == \">=\"\n+\n+OLDEST_SUPPORTED_PYTHON: Final = pyproject_specifier.version\n+\n \n def ty_contains_bug(code: str, *, ty_executable: Path) -> bool:\n     \"\"\"Return `True` if the code triggers a panic in type-checking code.\"\"\"\n     with tempfile.TemporaryDirectory() as tempdir:\n         input_file = Path(tempdir, \"input.py\")\n         input_file.write_text(code)\n         completed_process = subprocess.run(\n-            [ty_executable, \"check\", input_file], capture_output=True, text=True\n+            [\n+                ty_executable,\n+                \"check\",\n+                input_file,\n+                \"--python-version\",\n+                OLDEST_SUPPORTED_PYTHON,\n+                \"--python-platform\",\n+                TY_TARGET_PLATFORM,\n+            ],\n+            capture_output=True,\n+            text=True,",
      "comment": "Does this mean that the fuzzer will now always run the code assuming the oldest Python version and `linux`?\n\nWhen I was debugging 21839, I ran the fuzzer with some print statements to get the panic message and it printed the message multiple times. This suggests that it must be running the same code multiple times (?) but I also noticed that it didn't print the message for every run which suggests that the Python version picked up could've been different.\n\nLet me know if my assumptions are incorrect because I don't know how py-fuzzer works, you might know better! What I'm trying to ask is that would this make us loose coverage on testing the same seed with different Python version?",
      "comment_id": 2598741064,
      "user": "dhruvmanila",
      "created_at": "2025-12-08T13:57:12Z",
      "url": "https://github.com/astral-sh/ruff/pull/21844#discussion_r2598741064"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21844,
      "file_path": "python/py-fuzzer/fuzz.py",
      "line": 51,
      "side": "RIGHT",
      "diff_hunk": "@@ -44,14 +46,36 @@\n Seed = NewType(\"Seed\", int)\n ExitCode = NewType(\"ExitCode\", int)\n \n+TY_TARGET_PLATFORM: Final = \"linux\"\n+\n+with Path(__file__).parent.parent.parent.joinpath(\"pyproject.toml\").open(\"rb\") as f:",
      "comment": "The `parent.parent.parent` is rather funny here. I would probably write this as `Path(__file__) / \"../../../pyproject.toml\"`\r\n\r\nIs this loading Ruff's `pyproject.toml`? Do we want that, given that it's a very old Python version? Should we instead fuzz for multiple Python versions (using sharding) or hard-code a reasonable and more recent Python version?",
      "comment_id": 2598743793,
      "user": "MichaReiser",
      "created_at": "2025-12-08T13:57:47Z",
      "url": "https://github.com/astral-sh/ruff/pull/21844#discussion_r2598743793"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21844,
      "file_path": "python/py-fuzzer/fuzz.py",
      "line": 51,
      "side": "RIGHT",
      "diff_hunk": "@@ -44,14 +46,36 @@\n Seed = NewType(\"Seed\", int)\n ExitCode = NewType(\"ExitCode\", int)\n \n+TY_TARGET_PLATFORM: Final = \"linux\"\n+\n+with Path(__file__).parent.parent.parent.joinpath(\"pyproject.toml\").open(\"rb\") as f:",
      "comment": "> Is this loading Ruff's `pyproject.toml`? Do we want that, given that it's a very old Python version?\r\n\r\nI explained why I think that it's useful to test with an old version in my PR description. It's better at catching bugs that way! It's more likely to throw up situations that we didn't think about when writing PRs, such as: \"What happens if we want to infer a type as a `ParamSpec` instance but the `ParamSpec` class doesn't exist yet in the `typing` module?\"\r\n\r\n> Should we instead fuzz for multiple Python versions (using sharding) or hard-code a reasonable and more recent Python version?\r\n\r\nwe can consider making those changes, for sure. For now, this PR just makes explicit what the PR was always implicitly doing before. I'd prefer to consider those changes separately.",
      "comment_id": 2598761686,
      "user": "AlexWaygood",
      "created_at": "2025-12-08T14:02:22Z",
      "url": "https://github.com/astral-sh/ruff/pull/21844#discussion_r2598761686"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21844,
      "file_path": "python/py-fuzzer/fuzz.py",
      "line": 78,
      "side": "RIGHT",
      "diff_hunk": "@@ -44,14 +46,36 @@\n Seed = NewType(\"Seed\", int)\n ExitCode = NewType(\"ExitCode\", int)\n \n+TY_TARGET_PLATFORM: Final = \"linux\"\n+\n+with Path(__file__).parent.parent.parent.joinpath(\"pyproject.toml\").open(\"rb\") as f:\n+    pyproject_toml = tomllib.load(f)\n+\n+pyproject_specifier = packaging.specifiers.Specifier(\n+    pyproject_toml[\"project\"][\"requires-python\"]\n+)\n+assert pyproject_specifier.operator == \">=\"\n+\n+OLDEST_SUPPORTED_PYTHON: Final = pyproject_specifier.version\n+\n \n def ty_contains_bug(code: str, *, ty_executable: Path) -> bool:\n     \"\"\"Return `True` if the code triggers a panic in type-checking code.\"\"\"\n     with tempfile.TemporaryDirectory() as tempdir:\n         input_file = Path(tempdir, \"input.py\")\n         input_file.write_text(code)\n         completed_process = subprocess.run(\n-            [ty_executable, \"check\", input_file], capture_output=True, text=True\n+            [\n+                ty_executable,\n+                \"check\",\n+                input_file,\n+                \"--python-version\",\n+                OLDEST_SUPPORTED_PYTHON,\n+                \"--python-platform\",\n+                TY_TARGET_PLATFORM,\n+            ],\n+            capture_output=True,\n+            text=True,",
      "comment": "This makes explicit what it was always doing: it was always using the Python version that it picked up from Ruff's pyproject.toml file before.\r\n\r\nThe reason why you saw multiple messages from your `print()` statements isn't because it was running ty with a matrix of Python versions or Python platforms. Instead, it's that the fuzzer found a bug on a huge generated Python file and then systematically tried to minimize the AST in that file to produce the smallest possible snippet on which the bug would reproduce. That involves executing ty repeatedly.",
      "comment_id": 2598768068,
      "user": "AlexWaygood",
      "created_at": "2025-12-08T14:04:20Z",
      "url": "https://github.com/astral-sh/ruff/pull/21844#discussion_r2598768068"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21844,
      "file_path": "python/py-fuzzer/fuzz.py",
      "line": 51,
      "side": "RIGHT",
      "diff_hunk": "@@ -44,14 +46,36 @@\n Seed = NewType(\"Seed\", int)\n ExitCode = NewType(\"ExitCode\", int)\n \n+TY_TARGET_PLATFORM: Final = \"linux\"\n+\n+with Path(__file__).parent.parent.parent.joinpath(\"pyproject.toml\").open(\"rb\") as f:",
      "comment": "Fair enough, although it is somewhat confusing because ty doesn't officially support Python 3.7. I think we only support 3.8 or newer",
      "comment_id": 2598771356,
      "user": "MichaReiser",
      "created_at": "2025-12-08T14:05:23Z",
      "url": "https://github.com/astral-sh/ruff/pull/21844#discussion_r2598771356"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21844,
      "file_path": "python/py-fuzzer/fuzz.py",
      "line": 51,
      "side": "RIGHT",
      "diff_hunk": "@@ -44,14 +46,36 @@\n Seed = NewType(\"Seed\", int)\n ExitCode = NewType(\"ExitCode\", int)\n \n+TY_TARGET_PLATFORM: Final = \"linux\"\n+\n+with Path(__file__).parent.parent.parent.joinpath(\"pyproject.toml\").open(\"rb\") as f:",
      "comment": "> Fair enough, although it is somewhat confusing because ty doesn't officially support Python 3.7. I think we only support 3.8 or newer\r\n\r\nHrm, it's listed as a supported version in our `--help` message?\r\n\r\n<img width=\"3802\" height=\"448\" alt=\"image\" src=\"https://github.com/user-attachments/assets/6bb3cf30-e38b-4c94-8eb8-2825a855f20f\" />\r\n",
      "comment_id": 2598784431,
      "user": "AlexWaygood",
      "created_at": "2025-12-08T14:09:09Z",
      "url": "https://github.com/astral-sh/ruff/pull/21844#discussion_r2598784431"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21844,
      "file_path": "python/py-fuzzer/fuzz.py",
      "line": 51,
      "side": "RIGHT",
      "diff_hunk": "@@ -44,14 +46,36 @@\n Seed = NewType(\"Seed\", int)\n ExitCode = NewType(\"ExitCode\", int)\n \n+TY_TARGET_PLATFORM: Final = \"linux\"\n+\n+with Path(__file__).parent.parent.parent.joinpath(\"pyproject.toml\").open(\"rb\") as f:",
      "comment": "https://github.com/astral-sh/ty/issues/878\r\n\r\nty's `pyproject.toml` specifies `requires-python = \">=3.8\"`\r\n",
      "comment_id": 2598796088,
      "user": "MichaReiser",
      "created_at": "2025-12-08T14:12:18Z",
      "url": "https://github.com/astral-sh/ruff/pull/21844#discussion_r2598796088"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21741,
      "file_path": "my-script.py",
      "line": 1,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,3 @@\n+from __future__ import annotations",
      "comment": "I don't think we want this as a new top-level file in the ruff repo \ud83d\ude06 ",
      "comment_id": 2582419124,
      "user": "carljm",
      "created_at": "2025-12-02T18:43:12Z",
      "url": "https://github.com/astral-sh/ruff/pull/21741#discussion_r2582419124"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21741,
      "file_path": "my-script.py",
      "line": 1,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,3 @@\n+from __future__ import annotations",
      "comment": "beat you to it ;) https://github.com/astral-sh/ruff/pull/21751",
      "comment_id": 2582521122,
      "user": "AlexWaygood",
      "created_at": "2025-12-02T19:20:30Z",
      "url": "https://github.com/astral-sh/ruff/pull/21741#discussion_r2582521122"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21479,
      "file_path": "crates/ruff_linter/resources/test/fixtures/flake8_simplify/SIM222.py",
      "line": 222,
      "side": "RIGHT",
      "diff_hunk": "@@ -216,3 +216,11 @@ def get_items_list():\n \n def get_items_set():\n     return tuple({item for item in items}) or None  # OK\n+\n+\n+# https://github.com/astral-sh/ruff/issues/21473\n+tuple(\"\") or True  # OK",
      "comment": "We should add a t-string case:\n\n\n```suggestion\ntuple(t\"\") or True  # OK\n```",
      "comment_id": 2535602018,
      "user": "ntBre",
      "created_at": "2025-11-17T21:44:13Z",
      "url": "https://github.com/astral-sh/ruff/pull/21479#discussion_r2535602018"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21625,
      "file_path": "scripts/ty_benchmark/src/benchmark/lsp_client.py",
      "line": 3,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,175 @@\n+\"\"\"Simple LSP client for benchmarking diagnostic response times.\"\"\"\n+\n+from __future__ import annotations",
      "comment": "this shouldn't be required, since you set `requires-python = \">=3.14\"` in the pyproject.toml file\r\n\r\n```suggestion\r\n```",
      "comment_id": 2573095573,
      "user": "AlexWaygood",
      "created_at": "2025-11-29T16:55:01Z",
      "url": "https://github.com/astral-sh/ruff/pull/21625#discussion_r2573095573"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21625,
      "file_path": "scripts/ty_benchmark/src/benchmark/test_lsp_diagnostics.py",
      "line": 15,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,488 @@\n+\"\"\"\n+Benchmarks for LSP servers\n+\n+When debugging test failures, run pytest with `-s -v --log-cli-level=DEBUG`\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import asyncio\n+import tempfile\n+from abc import ABC, abstractmethod\n+from collections import Counter\n+from collections.abc import Generator\n+from pathlib import Path\n+from typing import Any, Callable, Final, override",
      "comment": "```suggestion\r\nfrom collections.abc import Callable, Generator\r\nfrom pathlib import Path\r\nfrom typing import Any, Final, override\r\n```",
      "comment_id": 2573098712,
      "user": "AlexWaygood",
      "created_at": "2025-11-29T17:03:14Z",
      "url": "https://github.com/astral-sh/ruff/pull/21625#discussion_r2573098712"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21625,
      "file_path": "scripts/ty_benchmark/src/benchmark/test_lsp_diagnostics.py",
      "line": 419,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,488 @@\n+\"\"\"\n+Benchmarks for LSP servers\n+\n+When debugging test failures, run pytest with `-s -v --log-cli-level=DEBUG`\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import asyncio\n+import tempfile\n+from abc import ABC, abstractmethod\n+from collections import Counter\n+from collections.abc import Generator\n+from pathlib import Path\n+from typing import Any, Callable, Final, override\n+\n+import pytest\n+from lsprotocol import types as lsp\n+\n+from benchmark.lsp_client import FileDiagnostics, LSPClient\n+from benchmark.projects import ALL as ALL_PROJECTS\n+from benchmark.projects import IncrementalEdit, Project\n+from benchmark.tool import Pyrefly, Pyright, Tool, Ty\n+from benchmark.venv import Venv\n+\n+# Tools to benchmark (only those with LSP support).\n+TOOLS_TO_BENCHMARK: Final = [\n+    Ty(),\n+    Pyright(),\n+    Pyrefly(),\n+]\n+\n+SEVERITY_LABELS = {1: \"Error\", 2: \"Warning\", 3: \"Info\", 4: \"Hint\"}\n+\n+\n+@pytest.fixture(scope=\"module\", params=ALL_PROJECTS, ids=lambda p: p.name)\n+def project_setup(\n+    request,\n+) -> Generator[tuple[Project, Venv], None, None]:\n+    \"\"\"Set up a project and its venv once per module (shared across all tests for this project).\"\"\"\n+    project: Project = request.param\n+\n+    with tempfile.TemporaryDirectory() as tempdir:\n+        cwd = Path(tempdir)\n+        project.clone(cwd)\n+\n+        venv = Venv.create(cwd, project.python_version)\n+        venv.install(project.install_arguments)\n+\n+        yield project, venv\n+\n+\n+@pytest.fixture(\n+    scope=\"function\",\n+    params=TOOLS_TO_BENCHMARK,\n+    ids=lambda t: t.name(),\n+)\n+def tool(request) -> Tool:\n+    \"\"\"Provide each tool to test.\"\"\"\n+    return request.param\n+\n+\n+def test_fetch_diagnostics(\n+    request, benchmark, project_setup: tuple[Project, Venv], tool: Tool\n+):\n+    \"\"\"Benchmark the time to receive initial diagnostics after starting the server.\"\"\"\n+\n+    project, venv = project_setup\n+\n+    run_benchmark(\n+        request,\n+        benchmark,\n+        project,\n+        tool,\n+        venv,\n+        FetchDiagnostics,\n+    )\n+\n+\n+def test_incremental_edit(\n+    request, benchmark, project_setup: tuple[Project, Venv], tool: Tool\n+):\n+    \"\"\"Benchmark the time to receive diagnostics after making an edit to a file.\"\"\"\n+\n+    project, venv = project_setup\n+\n+    run_benchmark(request, benchmark, project, tool, venv, IncrementalEditTest)\n+\n+\n+def run_benchmark(\n+    request: Any,\n+    benchmark: Any,\n+    project: Project,\n+    tool: Tool,\n+    venv: Venv,\n+    init_test: Callable[[Project, Tool, Venv], LspTest],\n+):\n+    # Set benchmark group to project name for better readability.\n+    benchmark.group = project.name\n+    verbose = request.config.getoption(\"verbose\") > 0\n+\n+    edited_file_backup: Path | None = None\n+\n+    # some make changes to the main file. Create a backup and restore it before each test\n+    # and once the entire suite is done.\n+    if project.edit:\n+        edited_file_path = venv.project_path / project.edit.edited_file\n+        edited_file_backup = edited_file_path.with_name(edited_file_path.name + \".bak\")\n+        edited_file_path.copy(edited_file_backup)\n+\n+    try:\n+        tool.write_config(project, venv)\n+\n+        # Use asyncio.Runner to keep the same event loop alive across setup and measure.\n+        with asyncio.Runner() as runner:\n+\n+            def setup():\n+                if edited_file_backup:\n+                    edited_file_backup.copy(edited_file_backup.with_suffix(\"\"))\n+\n+                test = init_test(project, tool, venv)\n+\n+                runner.run(test.setup())\n+                return (test,), {}\n+\n+            def run(test: LspTest) -> None:\n+                runner.run(test.run())\n+\n+            def teardown(test: LspTest) -> None:\n+                nonlocal verbose\n+\n+                test.assert_output(verbose=verbose)\n+                runner.run(test.teardown())\n+\n+                # Only do verbose output on the first (warm-up) run to avoid\n+                # that the printing changes the benchmark result.\n+                verbose = False\n+\n+            # Run the benchmark using pedantic mode.\n+            benchmark.pedantic(\n+                run,\n+                setup=setup,\n+                teardown=teardown,\n+                # total executions = rounds * iterations\n+                warmup_rounds=2,\n+                rounds=10,\n+                iterations=1,\n+            )\n+    finally:\n+        if edited_file_backup:\n+            edited_file_backup.copy(edited_file_backup.with_suffix(\"\"))\n+\n+\n+class LspTest(ABC):\n+    client: LSPClient\n+    venv: Venv\n+    project: Project\n+    tool: Tool\n+    edit: IncrementalEdit\n+\n+    def __init__(self, project: Project, tool: Tool, venv: Venv):\n+        edit = project.edit\n+        if not edit:\n+            pytest.skip(f\"{project.name} does not have an incremental edit\")\n+            return\n+\n+        self.project = project\n+        self.venv = venv\n+        self.tool = tool\n+        self.client = LSPClient()\n+        self.edit = edit\n+\n+    @property\n+    def cwd(self) -> Path:\n+        return self.venv.project_path\n+\n+    @property\n+    def edited_file_path(self) -> Path:\n+        return self.absolute_file_path(self.edit.edited_file)\n+\n+    def absolute_file_path(self, file_path: str) -> Path:\n+        return self.cwd / file_path\n+\n+    def files_to_check(self) -> Generator[Path, None, None]:\n+        yield self.edited_file_path\n+\n+        for file in self.edit.affected_files:\n+            yield self.absolute_file_path(file)\n+\n+    def open_all_files(self):\n+        for file_path in self.files_to_check():\n+            self.open_file(file_path)\n+\n+    def open_file(self, path: Path):\n+        self.client.text_document_did_open(\n+            lsp.DidOpenTextDocumentParams(\n+                text_document=lsp.TextDocumentItem(\n+                    uri=path.as_uri(),\n+                    language_id=\"python\",\n+                    version=1,\n+                    text=path.read_text(),\n+                )\n+            )\n+        )\n+\n+    async def initialize(self):\n+        lsp_cmd = self.tool.lsp_command(self.project, self.venv)\n+        if lsp_cmd is None:\n+            pytest.skip(f\"{self.tool.name()} doesn't support LSP\")\n+            return\n+\n+        await self.client.start_io(*lsp_cmd, cwd=self.cwd)\n+\n+        await self.client.initialize_async(\n+            lsp.InitializeParams(\n+                root_uri=self.cwd.as_uri(),\n+                workspace_folders=[\n+                    lsp.WorkspaceFolder(uri=self.cwd.as_uri(), name=self.cwd.name)\n+                ],\n+                capabilities=lsp.ClientCapabilities(\n+                    text_document=lsp.TextDocumentClientCapabilities(\n+                        diagnostic=lsp.DiagnosticClientCapabilities(\n+                            data_support=True, dynamic_registration=False\n+                        ),\n+                        synchronization=lsp.TextDocumentSyncClientCapabilities(\n+                            did_save=True,\n+                        ),\n+                    ),\n+                ),\n+            ),\n+        )\n+\n+        self.client.initialized(lsp.InitializedParams())\n+\n+    @abstractmethod\n+    async def setup(self): ...\n+\n+    @abstractmethod\n+    async def run(self): ...\n+\n+    @abstractmethod\n+    def assert_output(self, verbose=False): ...\n+\n+    async def teardown(self):\n+        await self.client.shutdown_async(None)\n+        self.client.exit(None)\n+        await self.client.stop()\n+\n+\n+class FetchDiagnostics(LspTest):\n+    diagnostics: list[FileDiagnostics] | None = None\n+\n+    @override\n+    async def setup(self):\n+        await self.initialize()\n+        self.open_all_files()\n+\n+    @override\n+    async def run(self):\n+        self.diagnostics = await self.client.text_documents_diagnostics_async(\n+            list(self.files_to_check())\n+        )\n+\n+    @override\n+    def assert_output(self, verbose=False):\n+        if self.diagnostics is None:\n+            pytest.fail(\"No diagnostics were fetched\")\n+            return\n+\n+        if verbose:\n+            for file, diagnostics in self.diagnostics:\n+                if diagnostics:\n+                    print_diagnostics(file, diagnostics, self.venv.project_path)\n+\n+\n+class IncrementalEditTest(LspTest):\n+    before_edit_diagnostics: list[FileDiagnostics] | None = None\n+    after_edit_diagnostics: list[FileDiagnostics] | None = None\n+    new_content: str\n+\n+    def __init__(self, project: Project, tool: Tool, venv: Venv):\n+        super().__init__(project, tool, venv)\n+        new_content = self.edit.apply_to(self.edited_file_path.read_text())\n+\n+        if new_content is None:\n+            pytest.fail(\n+                f\"Could not find expected text in {self.edited_file_path}:\\n\"\n+                f\"Expected to find: {self.edit.replace_text}\\n\"\n+                f\"This may indicate the project has been updated or the configuration is incorrect.\"\n+            )\n+            return\n+\n+        self.new_content = new_content\n+\n+    @override\n+    async def setup(self):\n+        await self.initialize()\n+\n+        self.open_all_files()\n+\n+        self.before_edit_diagnostics = (\n+            await self.client.text_documents_diagnostics_async(\n+                list(self.files_to_check())\n+            )\n+        )\n+\n+        if not self.client.server_supports_pull_diagnostics:\n+            # Pyrefly sometimes sends more than one publish diagnostic per file,\n+            # and it doesn't support versioned publish diagnostics, making it impossible\n+            # for the client to tell if we already received the newest publish diagnostic\n+            # notification or not. Because of that, sleep, clear all publish diagnostic\n+            # notifications before sending the change notification.\n+            await asyncio.sleep(1)\n+            self.client.clear_pending_publish_diagnostics()\n+\n+    @override\n+    async def run(self):\n+        self.client.text_document_did_change(\n+            lsp.DidChangeTextDocumentParams(\n+                text_document=lsp.VersionedTextDocumentIdentifier(\n+                    uri=self.edited_file_path.as_uri(),\n+                    version=2,\n+                ),\n+                content_changes=[\n+                    lsp.TextDocumentContentChangeWholeDocument(text=self.new_content)\n+                ],\n+            ),\n+        )\n+\n+        all_files = list(self.files_to_check())\n+\n+        # wait for the didChange publish notifications or pull the new diagnostics\n+        self.after_edit_diagnostics = (\n+            await self.client.text_documents_diagnostics_async(all_files)\n+        )\n+\n+        after_did_change_sum = sum(\n+            len(diagnostics) for f, diagnostics in self.after_edit_diagnostics\n+        )\n+\n+        # IMPORTANT: Write the file back to disk!\n+        # Pyrefly, as of Nov 27, requires that the content on disk\n+        # is updated to show cross-file diagnostics.\n+        self.edited_file_path.write_text(self.new_content)\n+\n+        self.client.text_document_did_save(\n+            lsp.DidSaveTextDocumentParams(\n+                text_document=lsp.TextDocumentIdentifier(\n+                    uri=self.edited_file_path.as_uri(),\n+                ),\n+            )\n+        )\n+\n+        # Pyrefly only publishes cross-file diagnostics after did_save.\n+        if isinstance(self.tool, Pyrefly):\n+            after_did_save_sum = after_did_change_sum\n+\n+            # Pyrefly sometimes publishes multiple publish diagnostics after a `didSave`.\n+            # Especially if checking takes long, as it, e.g., is the case for homeassistant.\n+            # We need to wait until pyrefly sends us the cross-file diagnostics.\n+            # For now, we use a very simple heuristics where we simply check if the diagnostic\n+            # count between the `didChange` (not cross-file) and `didSave` (cross-file) is different.\n+            while after_did_save_sum == after_did_change_sum:\n+                self.after_edit_diagnostics = (\n+                    await self.client.text_documents_diagnostics_async(all_files)\n+                )\n+\n+                after_did_save_sum = sum(\n+                    len(diagnostics) for f, diagnostics in self.after_edit_diagnostics\n+                )\n+\n+    @override\n+    def assert_output(self, verbose=False):\n+        assert self.before_edit_diagnostics is not None, (\n+            \"The before edit diagnostics should be initialized. Did you forget to call `setup`?\"\n+        )\n+        assert self.after_edit_diagnostics is not None, (\n+            \"The after edit diagnostics should be initialized if the test ran at least once. Did you forget to call `run`?\"\n+        )\n+\n+        before_edit_count = sum(\n+            len(diagnostics) for _, diagnostics in self.before_edit_diagnostics\n+        )\n+\n+        after_edit_count = sum(\n+            len(diagnostics) for _, diagnostics in self.after_edit_diagnostics\n+        )\n+\n+        assert after_edit_count > before_edit_count, (\n+            f\"Expected more diagnostics after the change. \"\n+            f\"Initial: {before_edit_count}, After change: {after_edit_count}\"\n+        )\n+\n+        if verbose:\n+            print_diagnostic_diff(\n+                self.before_edit_diagnostics,\n+                self.after_edit_diagnostics,\n+                self.project.name,\n+                self.tool.name(),\n+                self.venv.project_path,\n+            )\n+\n+\n+def print_diagnostics(\n+    file: Path, diagnostics: list[lsp.Diagnostic], cwd: Path, label: str | None = None\n+):\n+    file = file.relative_to(cwd)\n+\n+    if label:\n+        print(f\"\\n{file}: {len(diagnostics)} {label}\")\n+    else:\n+        print(f\"\\n{file}: {len(diagnostics)} diagnostics\")\n+\n+    for diag in diagnostics:\n+        severity = SEVERITY_LABELS.get(diag.severity, f\"Unknown({diag.severity})\")\n+        print(\n+            f\"{file}:{diag.range.start.line + 1}:{diag.range.start.character + 1} [{severity}] {diag.message}\"\n+        )\n+\n+\n+DiagnosticSignature = tuple[Path, str | int | None, str]",
      "comment": "```suggestion\r\ntype DiagnosticSignature = tuple[Path, str | int | None, str]\r\n```",
      "comment_id": 2573099767,
      "user": "AlexWaygood",
      "created_at": "2025-11-29T17:06:11Z",
      "url": "https://github.com/astral-sh/ruff/pull/21625#discussion_r2573099767"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21625,
      "file_path": "scripts/ty_benchmark/src/benchmark/test_lsp_diagnostics.py",
      "line": 33,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,488 @@\n+\"\"\"\n+Benchmarks for LSP servers\n+\n+When debugging test failures, run pytest with `-s -v --log-cli-level=DEBUG`\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import asyncio\n+import tempfile\n+from abc import ABC, abstractmethod\n+from collections import Counter\n+from collections.abc import Generator\n+from pathlib import Path\n+from typing import Any, Callable, Final, override\n+\n+import pytest\n+from lsprotocol import types as lsp\n+\n+from benchmark.lsp_client import FileDiagnostics, LSPClient\n+from benchmark.projects import ALL as ALL_PROJECTS\n+from benchmark.projects import IncrementalEdit, Project\n+from benchmark.tool import Pyrefly, Pyright, Tool, Ty\n+from benchmark.venv import Venv\n+\n+# Tools to benchmark (only those with LSP support).\n+TOOLS_TO_BENCHMARK: Final = [\n+    Ty(),\n+    Pyright(),\n+    Pyrefly(),\n+]\n+\n+SEVERITY_LABELS = {1: \"Error\", 2: \"Warning\", 3: \"Info\", 4: \"Hint\"}",
      "comment": "```suggestion\r\nSEVERITY_LABELS: Final = {1: \"Error\", 2: \"Warning\", 3: \"Info\", 4: \"Hint\"}\r\n```",
      "comment_id": 2573100060,
      "user": "AlexWaygood",
      "created_at": "2025-11-29T17:06:53Z",
      "url": "https://github.com/astral-sh/ruff/pull/21625#discussion_r2573100060"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21535,
      "file_path": "crates/ruff_linter/resources/test/fixtures/ruff/RUF066.py",
      "line": 3,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,75 @@\n+\"\"\"\n+Should emit:\n+RUF066 - on lines 12, 23, 48",
      "comment": "Would you mind adding inline comments marking each case as okay or an error instead of one comment at the top? That way it will appear in each of the snapshots. That makes it a bit easier to review.\n\nAnd could you renumber the rule to RUF069? I think we have PRs in flight for [66](https://github.com/astral-sh/ruff/pull/9911), [67](https://github.com/astral-sh/ruff/pull/20585), and [68](https://github.com/astral-sh/ruff/pull/21079) already.",
      "comment_id": 2547157605,
      "user": "ntBre",
      "created_at": "2025-11-20T18:23:23Z",
      "url": "https://github.com/astral-sh/ruff/pull/21535#discussion_r2547157605"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21536,
      "file_path": "scripts/ty_benchmark/src/benchmark/__init__.py",
      "line": 40,
      "side": "RIGHT",
      "diff_hunk": "@@ -37,13 +37,16 @@ class Hyperfine(typing.NamedTuple):\n     json: bool\n     \"\"\"Whether to export results to JSON.\"\"\"\n \n-    def run(self, *, cwd: Path | None = None) -> None:\n+    def run(self, *, cwd: Path | None = None, env: Mapping[str, str] = dict()) -> None:",
      "comment": "mutable default arguments are [evil](https://docs.astral.sh/ruff/rules/mutable-argument-default/) -- even if this happens to work fine here, I think it would be better to do:\r\n\r\n```suggestion\r\n    def run(self, *, cwd: Path | None = None, env: Mapping[str, str] | None = None) -> None:\r\n        env = env or {}\r\n```",
      "comment_id": 2547072646,
      "user": "AlexWaygood",
      "created_at": "2025-11-20T17:54:42Z",
      "url": "https://github.com/astral-sh/ruff/pull/21536#discussion_r2547072646"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21536,
      "file_path": "scripts/ty_benchmark/src/benchmark/projects.py",
      "line": 102,
      "side": "RIGHT",
      "diff_hunk": "@@ -95,51 +94,237 @@ def clone(self, checkout_dir: Path) -> None:\n # Selection of projects taken from\n # [mypy-primer](https://github.com/hauntsaninja/mypy_primer/blob/0ea6cc614b3e91084059b9a3acc58f94c066a211/mypy_primer/projects.py#L71).\n # May require frequent updating, especially the dependencies list\n-ALL = [\n+ALL: Final = [\n+    # As of Nov 19th 2025:\n+    # MyPy: 0\n+    # Pyright: 40\n+    # ty: 27\n+    # Pyrefly: 44",
      "comment": "these comments indicate the number of diagnostics? Or the time taken in seconds? I thought the latter at first, and was like, \"How can mypy complete type-checking instantly -- that seems wrong \ud83d\ude06\"\r\n\r\n```suggestion\r\n    # Diagnostic count as of Nov 19th 2025:\r\n    # MyPy: 0\r\n    # Pyright: 40\r\n    # ty: 27\r\n    # Pyrefly: 44\r\n```",
      "comment_id": 2547136375,
      "user": "AlexWaygood",
      "created_at": "2025-11-20T18:16:03Z",
      "url": "https://github.com/astral-sh/ruff/pull/21536#discussion_r2547136375"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21536,
      "file_path": "scripts/ty_benchmark/src/benchmark/projects.py",
      "line": 279,
      "side": "RIGHT",
      "diff_hunk": "@@ -95,51 +94,237 @@ def clone(self, checkout_dir: Path) -> None:\n # Selection of projects taken from\n # [mypy-primer](https://github.com/hauntsaninja/mypy_primer/blob/0ea6cc614b3e91084059b9a3acc58f94c066a211/mypy_primer/projects.py#L71).\n # May require frequent updating, especially the dependencies list\n-ALL = [\n+ALL: Final = [\n+    # As of Nov 19th 2025:\n+    # MyPy: 0\n+    # Pyright: 40\n+    # ty: 27\n+    # Pyrefly: 44\n     Project(\n         name=\"black\",\n         repository=\"https://github.com/psf/black\",\n-        revision=\"ac28187bf4a4ac159651c73d3a50fe6d0f653eac\",\n+        revision=\"45b4087976b7880db9dabacc992ee142f2d6c7c7\",\n+        python_version=\"3.10\",\n         include=[\"src\"],\n-        dependencies=[\n-            \"aiohttp\",\n-            \"click\",\n-            \"pathspec\",\n-            \"tomli\",\n-            \"platformdirs\",\n-            \"packaging\",\n+        install_arguments=[\n+            \"-r\",\n+            \"pyproject.toml\",\n+            # All extras except jupyter because installing the jupyter optional results in a mypy typing error.\n+            \"--extra\",\n+            \"colorama\",\n+            \"--extra\",\n+            \"uvloop\",\n+            \"--extra\",\n+            \"d\",\n+        ],\n+    ),\n+    # As of Nov 19th 2025:\n+    # MyPy: 941\n+    # Pyright: 210\n+    # ty: 260\n+    # Pyrefly: 652\n+    Project(\n+        name=\"discord.py\",\n+        repository=\"https://github.com/Rapptz/discord.py.git\",\n+        revision=\"9be91cb093402f54a44726c7dc4c04ff3b2c5a63\",\n+        python_version=\"3.8\",\n+        include=[\"discord\"],\n+        install_arguments=[\n+            \"-r\",\n+            \"pyproject.toml\",\n+            \"typing_extensions>=4.3,<5\",\n+        ],\n+    ),\n+    # Fairly chunky project, requires the pydantic mypy plugin.\n+    #\n+    # Pyrefly reports significantely more diagnostics than ty and, unlike ty, has partial pydantic support.\n+    # Both could be the reason why Pyrefly is slower than ty (it's notable that it's mainly slower because it has a much higher system time)\n+    #\n+    # As of Nov 19th 2025:\n+    # mypy: 12\n+    # ty: 4'661\n+    # Pyrefly: 37'610\n+    # Pyright: 19'186\n+    Project(\n+        name=\"homeassistant\",\n+        repository=\"https://github.com/home-assistant/core.git\",\n+        revision=\"10c12623bfc0b3a06ffaa88bf986f61818cfb8be\",\n+        python_version=\"3.13\",\n+        include=[\"homeassistant\"],\n+        install_arguments=[\n+            \"-r\",\n+            \"requirements_test_all.txt\",\n+            \"-r\",\n+            \"requirements.txt\",\n         ],\n     ),\n+    # As of Nov 19th 2025:\n+    # mypy: 0\n+    # ty: 19\n+    # Pyrefly: 18\n+    # Pyright: 24\n+    Project(\n+        name=\"isort\",\n+        repository=\"https://github.com/pycqa/isort\",\n+        revision=\"ed501f10cb5c1b17aad67358017af18cf533c166\",\n+        python_version=\"3.11\",\n+        include=[\"isort\"],\n+        install_arguments=[\"types-colorama\", \"colorama\"],\n+    ),\n+    # As of Nov 19th 2025:\n+    # mypy: 6 (all unused-ignore)\n+    # ty: 22\n+    # Pyrefly: 65\n+    # Pyright: 44\n     Project(\n         name=\"jinja\",\n         repository=\"https://github.com/pallets/jinja\",\n-        revision=\"b490da6b23b7ad25dc969976f64dc4ffb0a2c182\",\n-        include=[],\n-        dependencies=[\"markupsafe\"],\n+        revision=\"5ef70112a1ff19c05324ff889dd30405b1002044\",\n+        python_version=\"3.10\",\n+        include=[\"src\"],\n+        install_arguments=[\"-r\", \"pyproject.toml\"],\n     ),\n+    # As of Nov 19th 2025:\n+    # mypy: 0\n+    # ty: 784\n+    # Pyrefly: 1'602\n+    # Pyright: 2'180\n     Project(\n         name=\"pandas\",\n         repository=\"https://github.com/pandas-dev/pandas\",\n-        revision=\"7945e563d36bcf4694ccc44698829a6221905839\",\n-        include=[\"pandas\"],\n-        dependencies=[\n-            \"numpy\",\n-            \"types-python-dateutil\",\n-            \"types-pytz\",\n-            \"types-PyMySQL\",\n-            \"types-setuptools\",\n-            \"pytest\",\n+        revision=\"4d8348341bc4de2f0f90782ecef1b092b9418a19\",\n+        include=[\"pandas\", \"typings\"],\n+        exclude=[\"pandas/tests\"],\n+        python_version=\"3.11\",\n+        install_arguments=[\n+            \"-r\",\n+            \"requirements-dev.txt\",\n         ],\n     ),\n+    # As of Nov 19th 2025:\n+    # mypy: 0\n+    # ty: 4\n+    # Pyrefly: 0\n+    # Pyright: 0\n     Project(\n-        name=\"isort\",\n-        repository=\"https://github.com/pycqa/isort\",\n-        revision=\"7de182933fd50e04a7c47cc8be75a6547754b19c\",\n-        mypy_arguments=[\"--ignore-missing-imports\", \"isort\"],\n-        include=[\"isort\"],\n-        dependencies=[\"types-setuptools\"],\n+        name=\"pandas-stubs\",\n+        repository=\"https://github.com/pandas-dev/pandas-stubs\",\n+        revision=\"ad8cae5bc1f0bc87ce22b4d445e0700976c9dfb4\",\n+        include=[\"pandas-stubs\"],\n+        python_version=\"3.10\",\n+        # Uses poetry :(\n+        install_arguments=[\n+            \"types-pytz >=2022.1.1\",\n+            \"types-python-dateutil>=2.8.19\",\n+            \"numpy >=1.23.5\",\n+            \"pyarrow >=10.0.1\",\n+            \"matplotlib >=3.10.1\",\n+            \"xarray>=22.6.0\",\n+            \"SQLAlchemy>=2.0.39\",\n+            \"odfpy >=1.4.1\",\n+            \"pyxlsb >=1.0.10\",\n+            \"jinja2 >=3.1\",\n+            \"scipy >=1.9.1\",\n+            \"scipy-stubs >=1.15.3.0\",\n+        ],\n+    ),\n+    # Requires the pydantic mypy plugin\n+    # As of Nov 19th 2025:\n+    # mypy: 1\n+    # ty: 119\n+    # Pyrefly: 159\n+    # Pyright: 174\n+    Project(\n+        name=\"prefect\",\n+        repository=\"https://github.com/PrefectHQ/prefect.git\",\n+        revision=\"a3db33d4f9ee7a665430ae6017c649d057139bd3\",\n+        # See https://github.com/PrefectHQ/prefect/blob/a3db33d4f9ee7a665430ae6017c649d057139bd3/.pre-commit-config.yaml#L33-L39\n+        include=[\n+            \"src/prefect/server/models\",\n+            \"src/prefect/concurrency\",\n+            \"src/prefect/events\",\n+            \"src/prefect/input\",\n+        ],\n+        python_version=\"3.10\",\n+        install_arguments=[\n+            \"-r\",\n+            \"pyproject.toml\",\n+            \"--group\",\n+            \"dev\",\n+        ],\n+    ),\n+    # Requires sympy-mypy plugin.\n+    # As of Nov 19th 2025:\n+    # mypy: 16'623\n+    # ty: 23'231\n+    # Pyrefly: 15,877\n+    # Pyright: 19'684\n+    Project(\n+        name=\"pytorch\",\n+        repository=\"https://github.com/pytorch/pytorch.git\",\n+        revision=\"be33b7faf685560bb618561b44b751713a660337\",\n+        include=[\"torch\", \"caffe2\"],\n+        # see https://github.com/pytorch/pytorch/blob/c56655268b4ae575ee4c89c312fd93ca2f5b3ba9/pyrefly.toml#L23\n+        exclude=[\n+            \"torch/_inductor/codegen/triton.py\",\n+            \"tools/linter/adapters/test_device_bias_linter.py\",\n+            \"tools/code_analyzer/gen_operators_yaml.py\",\n+            \"torch/_inductor/runtime/triton_heuristics.py\",\n+            \"torch/_inductor/runtime/triton_helpers.py\",\n+            \"torch/_inductor/runtime/halide_helpers.py\",\n+            \"torch/utils/tensorboard/summary.py\",\n+            \"torch/distributed/flight_recorder/components/types.py\",\n+            \"torch/linalg/__init__.py\",\n+            \"torch/package/importer.py\",\n+            \"torch/package/_package_pickler.py\",\n+            \"torch/jit/annotations.py\",\n+            \"torch/utils/data/datapipes/_typing.py\",\n+            \"torch/nn/functional.py\",\n+            \"torch/_export/utils.py\",\n+            \"torch/fx/experimental/unification/multipledispatch/__init__.py\",\n+            \"torch/nn/modules/__init__.py\",\n+            \"torch/nn/modules/rnn.py\",\n+            \"torch/_inductor/codecache.py\",\n+            \"torch/distributed/elastic/metrics/__init__.py\",\n+            \"torch/_inductor/fx_passes/bucketing.py\",\n+            \"torch/onnx/_internal/exporter/_torchlib/ops/nn.py\",\n+            \"torch/include/**\",\n+            \"torch/csrc/**\",\n+            \"torch/distributed/elastic/agent/server/api.py\",\n+            \"torch/testing/_internal/**\",\n+            \"torch/distributed/fsdp/fully_sharded_data_parallel.py\",\n+            \"torch/ao/quantization/pt2e/_affine_quantization.py\",\n+            \"torch/nn/modules/pooling.py\",\n+            \"torch/nn/parallel/_functions.py\",\n+            \"torch/_appdirs.py\",\n+            \"torch/multiprocessing/pool.py\",\n+            \"torch/overrides.py\",\n+            \"*/__pycache__/**\",\n+            \"*/.*\",\n+        ],\n+        # See https://github.com/pytorch/pytorch/blob/be33b7faf685560bb618561b44b751713a660337/.lintrunner.toml#L141\n+        install_arguments=[\n+            'numpy==1.26.4 ; python_version >= \"3.10\" and python_version <= \"3.11\"',\n+            'numpy==2.1.0 ; python_version >= \"3.12\" and python_version <= \"3.13\"',\n+            'numpy==2.3.4 ; python_version >= \"3.14\"',\n+            \"expecttest==0.3.0\",\n+            \"pyrefly==0.36.2\",\n+            \"sympy==1.13.3\",\n+            \"types-requests==2.27.25\",\n+            \"types-pyyaml==6.0.2\",\n+            \"types-tabulate==0.8.8\",\n+            \"types-protobuf==5.29.1.20250403\",\n+            \"types-setuptools==79.0.0.20250422\",\n+            \"types-jinja2==2.11.9\",\n+            \"types-colorama==0.4.6\",\n+            \"filelock==3.18.0\",\n+            \"junitparser==2.1.1\",\n+            \"rich==14.1.0\",\n+            \"optree==0.17.0\",\n+            \"types-openpyxl==3.1.5.20250919\",\n+            \"types-python-dateutil==2.9.0.20251008\",\n+            \"mypy==1.16.0\",  # pytorch pinns mypy,",
      "comment": "```suggestion\r\n            \"mypy==1.16.0\",  # pytorch pins mypy,\r\n```",
      "comment_id": 2547141457,
      "user": "AlexWaygood",
      "created_at": "2025-11-20T18:17:53Z",
      "url": "https://github.com/astral-sh/ruff/pull/21536#discussion_r2547141457"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21536,
      "file_path": "scripts/ty_benchmark/src/benchmark/snapshot.py",
      "line": 60,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,136 @@\n+from __future__ import annotations\n+\n+import difflib\n+import logging\n+import re\n+import subprocess\n+from pathlib import Path\n+from typing import Mapping, NamedTuple\n+\n+from benchmark import Command\n+\n+\n+def normalize_output(output: str, cwd: Path) -> str:\n+    \"\"\"Normalize output by replacing absolute paths with relative placeholders.\"\"\"\n+\n+    # Replace absolute paths with <CWD>/relative/path.\n+    # This handles both the cwd itself and any paths within it.\n+    cwd_str = str(cwd.resolve())\n+    normalized = output.replace(cwd_str, \"<CWD>\")\n+\n+    # Replace temp directory patterns (e.g., /var/folders/.../tmp123abc/).\n+    # Match common temp directory patterns across platforms.\n+    normalized = re.sub(\n+        r\"/(?:var/folders|tmp)/[a-zA-Z0-9/_-]+/tmp[a-zA-Z0-9]+\",\n+        \"<TMPDIR>\",\n+        normalized,\n+    )\n+\n+    # Normalize unordered lists in error messages (e.g., '\"name\", \"description\"' vs '\"description\", \"name\"').\n+    # Sort quoted comma-separated items to make them deterministic.\n+    def sort_quoted_items(match):\n+        items = match.group(0)\n+        quoted_items = re.findall(r'\"[^\"]+\"', items)\n+        if len(quoted_items) > 1:\n+            sorted_items = \", \".join(sorted(quoted_items))\n+            return sorted_items\n+        return items\n+\n+    normalized = re.sub(r'\"[^\"]+\"(?:, \"[^\"]+\")+', sort_quoted_items, normalized)\n+\n+    return normalized\n+\n+\n+class SnapshotRunner(NamedTuple):\n+    name: str\n+    \"\"\"The benchmark to run.\"\"\"\n+\n+    commands: list[Command]\n+    \"\"\"The commands to snapshot.\"\"\"\n+\n+    snapshot_dir: Path\n+    \"\"\"Directory to store snapshots.\"\"\"\n+\n+    accept: bool\n+    \"\"\"Whether to accept snapshot changes.\"\"\"\n+\n+    def run(self, *, cwd: Path, env: Mapping[str, str] = dict()):\n+        \"\"\"Run commands and snapshot their output.\"\"\"\n+        # Create snapshot directory if it doesn't exist.\n+        self.snapshot_dir.mkdir(parents=True, exist_ok=True)",
      "comment": "```suggestion\r\n    def run(self, *, cwd: Path, env: Mapping[str, str] | None = None):\r\n        \"\"\"Run commands and snapshot their output.\"\"\"\r\n        env = env or {}\r\n        \r\n        # Create snapshot directory if it doesn't exist.\r\n        self.snapshot_dir.mkdir(parents=True, exist_ok=True)\r\n```",
      "comment_id": 2547148661,
      "user": "AlexWaygood",
      "created_at": "2025-11-20T18:20:26Z",
      "url": "https://github.com/astral-sh/ruff/pull/21536#discussion_r2547148661"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21536,
      "file_path": "scripts/ty_benchmark/src/benchmark/snapshot.py",
      "line": 76,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,136 @@\n+from __future__ import annotations\n+\n+import difflib\n+import logging\n+import re\n+import subprocess\n+from pathlib import Path\n+from typing import Mapping, NamedTuple\n+\n+from benchmark import Command\n+\n+\n+def normalize_output(output: str, cwd: Path) -> str:\n+    \"\"\"Normalize output by replacing absolute paths with relative placeholders.\"\"\"\n+\n+    # Replace absolute paths with <CWD>/relative/path.\n+    # This handles both the cwd itself and any paths within it.\n+    cwd_str = str(cwd.resolve())\n+    normalized = output.replace(cwd_str, \"<CWD>\")\n+\n+    # Replace temp directory patterns (e.g., /var/folders/.../tmp123abc/).\n+    # Match common temp directory patterns across platforms.\n+    normalized = re.sub(\n+        r\"/(?:var/folders|tmp)/[a-zA-Z0-9/_-]+/tmp[a-zA-Z0-9]+\",\n+        \"<TMPDIR>\",\n+        normalized,\n+    )\n+\n+    # Normalize unordered lists in error messages (e.g., '\"name\", \"description\"' vs '\"description\", \"name\"').\n+    # Sort quoted comma-separated items to make them deterministic.\n+    def sort_quoted_items(match):\n+        items = match.group(0)\n+        quoted_items = re.findall(r'\"[^\"]+\"', items)\n+        if len(quoted_items) > 1:\n+            sorted_items = \", \".join(sorted(quoted_items))\n+            return sorted_items\n+        return items\n+\n+    normalized = re.sub(r'\"[^\"]+\"(?:, \"[^\"]+\")+', sort_quoted_items, normalized)\n+\n+    return normalized\n+\n+\n+class SnapshotRunner(NamedTuple):\n+    name: str\n+    \"\"\"The benchmark to run.\"\"\"\n+\n+    commands: list[Command]\n+    \"\"\"The commands to snapshot.\"\"\"\n+\n+    snapshot_dir: Path\n+    \"\"\"Directory to store snapshots.\"\"\"\n+\n+    accept: bool\n+    \"\"\"Whether to accept snapshot changes.\"\"\"\n+\n+    def run(self, *, cwd: Path, env: Mapping[str, str] = dict()):\n+        \"\"\"Run commands and snapshot their output.\"\"\"\n+        # Create snapshot directory if it doesn't exist.\n+        self.snapshot_dir.mkdir(parents=True, exist_ok=True)\n+\n+        all_passed = True\n+\n+        for command in self.commands:\n+            snapshot_file = self.snapshot_dir / f\"{self.name}_{command.name}.txt\"\n+\n+            # Run the prepare command if provided.\n+            if command.prepare:\n+                logging.info(f\"Running prepare: {command.prepare}\")\n+                subprocess.run(\n+                    command.prepare,\n+                    shell=True,\n+                    cwd=cwd,\n+                    env=env,\n+                    capture_output=True,\n+                )",
      "comment": "should we check the return code here and have it raise an exception if it wasn't 0? We can't do that when we actually invoke the type checkers because they might emit diagnostics and therefore exit with code 1, but it seems reasonable to check the returncode for the `prepare` command\r\n\r\n```suggestion\r\n                subprocess.run(\r\n                    command.prepare,\r\n                    shell=True,\r\n                    cwd=cwd,\r\n                    env=env,\r\n                    capture_output=True,\r\n                    check=True,\r\n                )\r\n```",
      "comment_id": 2547152451,
      "user": "AlexWaygood",
      "created_at": "2025-11-20T18:21:41Z",
      "url": "https://github.com/astral-sh/ruff/pull/21536#discussion_r2547152451"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21536,
      "file_path": "scripts/ty_benchmark/src/benchmark/snapshot.py",
      "line": 121,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,136 @@\n+from __future__ import annotations\n+\n+import difflib\n+import logging\n+import re\n+import subprocess\n+from pathlib import Path\n+from typing import Mapping, NamedTuple\n+\n+from benchmark import Command\n+\n+\n+def normalize_output(output: str, cwd: Path) -> str:\n+    \"\"\"Normalize output by replacing absolute paths with relative placeholders.\"\"\"\n+\n+    # Replace absolute paths with <CWD>/relative/path.\n+    # This handles both the cwd itself and any paths within it.\n+    cwd_str = str(cwd.resolve())\n+    normalized = output.replace(cwd_str, \"<CWD>\")\n+\n+    # Replace temp directory patterns (e.g., /var/folders/.../tmp123abc/).\n+    # Match common temp directory patterns across platforms.\n+    normalized = re.sub(\n+        r\"/(?:var/folders|tmp)/[a-zA-Z0-9/_-]+/tmp[a-zA-Z0-9]+\",\n+        \"<TMPDIR>\",\n+        normalized,\n+    )\n+\n+    # Normalize unordered lists in error messages (e.g., '\"name\", \"description\"' vs '\"description\", \"name\"').\n+    # Sort quoted comma-separated items to make them deterministic.\n+    def sort_quoted_items(match):\n+        items = match.group(0)\n+        quoted_items = re.findall(r'\"[^\"]+\"', items)\n+        if len(quoted_items) > 1:\n+            sorted_items = \", \".join(sorted(quoted_items))\n+            return sorted_items\n+        return items\n+\n+    normalized = re.sub(r'\"[^\"]+\"(?:, \"[^\"]+\")+', sort_quoted_items, normalized)\n+\n+    return normalized\n+\n+\n+class SnapshotRunner(NamedTuple):\n+    name: str\n+    \"\"\"The benchmark to run.\"\"\"\n+\n+    commands: list[Command]\n+    \"\"\"The commands to snapshot.\"\"\"\n+\n+    snapshot_dir: Path\n+    \"\"\"Directory to store snapshots.\"\"\"\n+\n+    accept: bool\n+    \"\"\"Whether to accept snapshot changes.\"\"\"\n+\n+    def run(self, *, cwd: Path, env: Mapping[str, str] = dict()):\n+        \"\"\"Run commands and snapshot their output.\"\"\"\n+        # Create snapshot directory if it doesn't exist.\n+        self.snapshot_dir.mkdir(parents=True, exist_ok=True)\n+\n+        all_passed = True\n+\n+        for command in self.commands:\n+            snapshot_file = self.snapshot_dir / f\"{self.name}_{command.name}.txt\"\n+\n+            # Run the prepare command if provided.\n+            if command.prepare:\n+                logging.info(f\"Running prepare: {command.prepare}\")\n+                subprocess.run(\n+                    command.prepare,\n+                    shell=True,\n+                    cwd=cwd,\n+                    env=env,\n+                    capture_output=True,\n+                )\n+\n+            # Run the actual command and capture output.\n+            logging.info(f\"Running {command.command}\")\n+            result = subprocess.run(\n+                command.command,\n+                cwd=cwd,\n+                env=env,\n+                capture_output=True,\n+                text=True,\n+            )\n+\n+            # Get the actual output and combine stdout and stderr for the snapshot.\n+            actual_output = normalize_output(result.stdout + result.stderr, cwd)\n+\n+            # Check if snapshot exists.\n+            if snapshot_file.exists():\n+                expected_output = snapshot_file.read_text()\n+\n+                if actual_output != expected_output:\n+                    all_passed = False\n+                    print(f\"\\n\u274c Snapshot mismatch for {command.name}\")\n+                    print(f\"   Snapshot file: {snapshot_file}\")\n+                    print(\"\\n\" + \"=\" * 80)\n+                    print(\"Diff:\")\n+                    print(\"=\" * 80)\n+\n+                    # Generate and display a unified diff.\n+                    diff = difflib.unified_diff(\n+                        expected_output.splitlines(keepends=True),\n+                        actual_output.splitlines(keepends=True),\n+                        fromfile=f\"{command.name} (expected)\",\n+                        tofile=f\"{command.name} (actual)\",\n+                        lineterm=\"\",\n+                    )\n+\n+                    for line in diff:\n+                        # Color-code the diff output.\n+                        if line.startswith(\"+\") and not line.startswith(\"+++\"):\n+                            print(f\"\\033[32m{line}\\033[0m\", end=\"\")\n+                        elif line.startswith(\"-\") and not line.startswith(\"---\"):\n+                            print(f\"\\033[31m{line}\\033[0m\", end=\"\")\n+                        elif line.startswith(\"@@\"):\n+                            print(f\"\\033[36m{line}\\033[0m\", end=\"\")\n+                        else:\n+                            print(line, end=\"\")",
      "comment": "I'd be inclined to use `termcolor` for this (it's a tiny dependency and very stable). I also don't mind having manual ANSI codes too much, though",
      "comment_id": 2547159927,
      "user": "AlexWaygood",
      "created_at": "2025-11-20T18:24:04Z",
      "url": "https://github.com/astral-sh/ruff/pull/21536#discussion_r2547159927"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21536,
      "file_path": "scripts/ty_benchmark/src/benchmark/__init__.py",
      "line": 55,
      "side": "RIGHT",
      "diff_hunk": "@@ -37,13 +37,16 @@ class Hyperfine(typing.NamedTuple):\n     json: bool\n     \"\"\"Whether to export results to JSON.\"\"\"\n \n-    def run(self, *, cwd: Path | None = None) -> None:\n+    def run(self, *, cwd: Path | None = None, env: Mapping[str, str] = dict()) -> None:\n         \"\"\"Run the benchmark using `hyperfine`.\"\"\"\n         args = [\n             \"hyperfine\",\n-            # Most repositories have some typing errors.\n-            # This is annoying because it prevents us from capturing \"real\" errors.\n-            \"-i\",\n+            # Ignore any warning/error diagnostics but fail if there are any fatal errors, incorrect configuration, etc.\n+            # mypy exit codes: https://github.com/python/mypy/issues/14615#issuecomment-1420163253\n+            # pyright exit codes: https://docs.basedpyright.com/v1.31.6/configuration/command-line/#pyright-exit-codes\n+            # pyrefly exit codes: Not documented\n+            # ty: https://docs.astral.sh/ty/reference/exit-codes/\n+            \"-i=1\",",
      "comment": "Minor: I prefer to always use the `--long-form` of options when calling tools from scripts\r\n```suggestion\r\n            \"--ignore-failure=1\",\r\n```",
      "comment_id": 2555034575,
      "user": "sharkdp",
      "created_at": "2025-11-24T08:12:13Z",
      "url": "https://github.com/astral-sh/ruff/pull/21536#discussion_r2555034575"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21536,
      "file_path": "scripts/ty_benchmark/src/benchmark/tool.py",
      "line": 205,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,207 @@\n+from __future__ import annotations\n+\n+import abc\n+import json\n+import os\n+import shutil\n+import sys\n+from pathlib import Path\n+from typing import TYPE_CHECKING, override\n+\n+from benchmark import Command\n+from benchmark.projects import Project\n+\n+if TYPE_CHECKING:\n+    from benchmark.venv import Venv\n+\n+\n+def which_tool(name: str, path: Path | None = None) -> Path:\n+    tool = shutil.which(name, path=path)\n+\n+    assert tool is not None, (\n+        f\"Tool {name} not found. Run the script with `uv run <script>`.\"\n+    )\n+\n+    return Path(tool)\n+\n+\n+class Tool(abc.ABC):\n+    @abc.abstractmethod\n+    def command(self, project: Project, venv: Venv, single_threaded: bool) -> Command:\n+        \"\"\"Generate a command to benchmark a given tool.\"\"\"\n+\n+\n+class Ty(Tool):\n+    path: Path\n+    name: str\n+\n+    def __init__(self, *, path: Path | None = None):\n+        self.name = str(path) if path else \"ty\"\n+        self.path = (\n+            path or (Path(__file__) / \"../../../../../target/release/ty\")\n+        ).resolve()\n+\n+        assert self.path.is_file(), (\n+            f\"ty not found at '{self.path}'. Run `cargo build --release --bin ty`.\"\n+        )\n+\n+    @override\n+    def command(self, project: Project, venv: Venv, single_threaded: bool) -> Command:\n+        command = [\n+            str(self.path),\n+            \"check\",\n+            \"--output-format=concise\",\n+            \"--no-progress\",\n+            \"--python-version\",\n+            project.python_version,\n+            *project.include,\n+        ]\n+\n+        for exclude in project.exclude:\n+            command.extend([\"--exclude\", exclude])\n+\n+        command.extend([\"--python\", str(venv.path)])\n+\n+        return Command(name=self.name, command=command)\n+\n+\n+class Mypy(Tool):\n+    path: Path | None\n+    warm: bool\n+\n+    def __init__(self, *, warm: bool, path: Path | None = None):\n+        self.path = path\n+        self.warm = warm\n+\n+    @override\n+    def command(self, project: Project, venv: Venv, single_threaded: bool) -> Command:\n+        path = self.path or which_tool(\"mypy\", venv.bin)\n+        command = [\n+            str(path),\n+            \"--python-executable\",\n+            str(venv.python),\n+            \"--python-version\",\n+            project.python_version,\n+            \"--no-pretty\",\n+            *project.include,\n+            \"--check-untyped-defs\",\n+        ]\n+\n+        for exclude in project.exclude:\n+            # Mypy uses regex...\n+            # This is far from perfect, but not terrible.\n+            command.extend(\n+                [\n+                    \"--exclude\",\n+                    exclude.replace(\".\", r\"\\.\")\n+                    .replace(\"**\", \".*\")\n+                    .replace(\"*\", r\"\\w.*\"),\n+                ]\n+            )\n+\n+        if not self.warm:\n+            command.extend(\n+                [\n+                    \"--no-incremental\",\n+                    \"--cache-dir\",\n+                    os.devnull,\n+                ]\n+            )\n+\n+        return Command(\n+            name=\"mypy (warm)\" if self.warm else \"mypy\",\n+            command=command,\n+        )\n+\n+\n+class Pyright(Tool):\n+    path: Path\n+\n+    def __init__(self, *, path: Path | None = None):\n+        if path:\n+            self.path = path\n+        else:\n+            if sys.platform == \"win32\":\n+                self.path = Path(\"./node_modules/.bin/pyright.exe\").resolve()\n+            else:\n+                self.path = Path(\"./node_modules/.bin/pyright\").resolve()\n+\n+            if not self.path.exists():\n+                print(\n+                    \"Pyright executable not found. Did you ran `npm install` in the `ty_benchmark` directory?\"\n+                )\n+\n+    def command(self, project: Project, venv: Venv, single_threaded: bool) -> Command:\n+        command = [str(self.path), \"--skipunannotated\"]\n+\n+        (venv.project_path / \"pyrightconfig.json\").write_text(\n+            json.dumps(\n+                {\n+                    \"exclude\": [str(path) for path in project.exclude],\n+                    # Set the `venv` config for pyright. Pyright only respects the `--venvpath`\n+                    # CLI option when `venv` is set in the configuration... \ud83e\udd37\u200d\u2642\ufe0f\n+                    \"venv\": venv.name,\n+                }\n+            )\n+        )\n+\n+        if not single_threaded:\n+            command.append(\"--threads\")\n+\n+        command.extend(\n+            [\n+                \"--venvpath\",\n+                str(\n+                    venv.path.parent\n+                ),  # This is not the path to the venv folder, but the folder that contains the venv...\n+                \"--pythonversion\",\n+                project.python_version,\n+                \"--level=warning\",\n+                \"--project\",\n+                \"pyrightconfig.json\",\n+                *project.include,\n+            ]\n+        )\n+\n+        return Command(\n+            name=\"Pyright\",\n+            command=command,\n+        )\n+\n+\n+class Pyrefly(Tool):\n+    path: Path\n+\n+    def __init__(self, *, path: Path | None = None):\n+        self.path = path or which_tool(\"pyrefly\")",
      "comment": "Hm, we depend on `pyrefly` in `pyproject.toml`, but then use whichever version we get from the PATH? If we want to pin tool versions, we should use the one from the venv?",
      "comment_id": 2555159066,
      "user": "sharkdp",
      "created_at": "2025-11-24T08:40:02Z",
      "url": "https://github.com/astral-sh/ruff/pull/21536#discussion_r2555159066"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21536,
      "file_path": "scripts/ty_benchmark/src/benchmark/tool.py",
      "line": 205,
      "side": "RIGHT",
      "diff_hunk": "@@ -0,0 +1,207 @@\n+from __future__ import annotations\n+\n+import abc\n+import json\n+import os\n+import shutil\n+import sys\n+from pathlib import Path\n+from typing import TYPE_CHECKING, override\n+\n+from benchmark import Command\n+from benchmark.projects import Project\n+\n+if TYPE_CHECKING:\n+    from benchmark.venv import Venv\n+\n+\n+def which_tool(name: str, path: Path | None = None) -> Path:\n+    tool = shutil.which(name, path=path)\n+\n+    assert tool is not None, (\n+        f\"Tool {name} not found. Run the script with `uv run <script>`.\"\n+    )\n+\n+    return Path(tool)\n+\n+\n+class Tool(abc.ABC):\n+    @abc.abstractmethod\n+    def command(self, project: Project, venv: Venv, single_threaded: bool) -> Command:\n+        \"\"\"Generate a command to benchmark a given tool.\"\"\"\n+\n+\n+class Ty(Tool):\n+    path: Path\n+    name: str\n+\n+    def __init__(self, *, path: Path | None = None):\n+        self.name = str(path) if path else \"ty\"\n+        self.path = (\n+            path or (Path(__file__) / \"../../../../../target/release/ty\")\n+        ).resolve()\n+\n+        assert self.path.is_file(), (\n+            f\"ty not found at '{self.path}'. Run `cargo build --release --bin ty`.\"\n+        )\n+\n+    @override\n+    def command(self, project: Project, venv: Venv, single_threaded: bool) -> Command:\n+        command = [\n+            str(self.path),\n+            \"check\",\n+            \"--output-format=concise\",\n+            \"--no-progress\",\n+            \"--python-version\",\n+            project.python_version,\n+            *project.include,\n+        ]\n+\n+        for exclude in project.exclude:\n+            command.extend([\"--exclude\", exclude])\n+\n+        command.extend([\"--python\", str(venv.path)])\n+\n+        return Command(name=self.name, command=command)\n+\n+\n+class Mypy(Tool):\n+    path: Path | None\n+    warm: bool\n+\n+    def __init__(self, *, warm: bool, path: Path | None = None):\n+        self.path = path\n+        self.warm = warm\n+\n+    @override\n+    def command(self, project: Project, venv: Venv, single_threaded: bool) -> Command:\n+        path = self.path or which_tool(\"mypy\", venv.bin)\n+        command = [\n+            str(path),\n+            \"--python-executable\",\n+            str(venv.python),\n+            \"--python-version\",\n+            project.python_version,\n+            \"--no-pretty\",\n+            *project.include,\n+            \"--check-untyped-defs\",\n+        ]\n+\n+        for exclude in project.exclude:\n+            # Mypy uses regex...\n+            # This is far from perfect, but not terrible.\n+            command.extend(\n+                [\n+                    \"--exclude\",\n+                    exclude.replace(\".\", r\"\\.\")\n+                    .replace(\"**\", \".*\")\n+                    .replace(\"*\", r\"\\w.*\"),\n+                ]\n+            )\n+\n+        if not self.warm:\n+            command.extend(\n+                [\n+                    \"--no-incremental\",\n+                    \"--cache-dir\",\n+                    os.devnull,\n+                ]\n+            )\n+\n+        return Command(\n+            name=\"mypy (warm)\" if self.warm else \"mypy\",\n+            command=command,\n+        )\n+\n+\n+class Pyright(Tool):\n+    path: Path\n+\n+    def __init__(self, *, path: Path | None = None):\n+        if path:\n+            self.path = path\n+        else:\n+            if sys.platform == \"win32\":\n+                self.path = Path(\"./node_modules/.bin/pyright.exe\").resolve()\n+            else:\n+                self.path = Path(\"./node_modules/.bin/pyright\").resolve()\n+\n+            if not self.path.exists():\n+                print(\n+                    \"Pyright executable not found. Did you ran `npm install` in the `ty_benchmark` directory?\"\n+                )\n+\n+    def command(self, project: Project, venv: Venv, single_threaded: bool) -> Command:\n+        command = [str(self.path), \"--skipunannotated\"]\n+\n+        (venv.project_path / \"pyrightconfig.json\").write_text(\n+            json.dumps(\n+                {\n+                    \"exclude\": [str(path) for path in project.exclude],\n+                    # Set the `venv` config for pyright. Pyright only respects the `--venvpath`\n+                    # CLI option when `venv` is set in the configuration... \ud83e\udd37\u200d\u2642\ufe0f\n+                    \"venv\": venv.name,\n+                }\n+            )\n+        )\n+\n+        if not single_threaded:\n+            command.append(\"--threads\")\n+\n+        command.extend(\n+            [\n+                \"--venvpath\",\n+                str(\n+                    venv.path.parent\n+                ),  # This is not the path to the venv folder, but the folder that contains the venv...\n+                \"--pythonversion\",\n+                project.python_version,\n+                \"--level=warning\",\n+                \"--project\",\n+                \"pyrightconfig.json\",\n+                *project.include,\n+            ]\n+        )\n+\n+        return Command(\n+            name=\"Pyright\",\n+            command=command,\n+        )\n+\n+\n+class Pyrefly(Tool):\n+    path: Path\n+\n+    def __init__(self, *, path: Path | None = None):\n+        self.path = path or which_tool(\"pyrefly\")",
      "comment": "uv adds `pyrefly` to the path, so we always get it from there.",
      "comment_id": 2555203593,
      "user": "MichaReiser",
      "created_at": "2025-11-24T08:49:38Z",
      "url": "https://github.com/astral-sh/ruff/pull/21536#discussion_r2555203593"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21617,
      "file_path": "scripts/update_ambiguous_characters.py",
      "line": 48,
      "side": "RIGHT",
      "diff_hunk": "@@ -43,7 +43,7 @@ def format_number(number: int) -> str:\n     # underscore-delimited in the generated file, so we now preserve that property to\n     # avoid unnecessary churn.\n     if number > 100000:\n-        number = str(number)\n+        number: str = str(number)\n         number = \"_\".join(number[i : i + 3] for i in range(0, len(number), 3))\n         return f\"{number}_u32\"",
      "comment": "a nice little use of our feature where we allow redeclarations :-)",
      "comment_id": 2557272940,
      "user": "AlexWaygood",
      "created_at": "2025-11-24T18:15:41Z",
      "url": "https://github.com/astral-sh/ruff/pull/21617#discussion_r2557272940"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 21290,
      "file_path": "crates/ruff_linter/resources/test/fixtures/pylint/dict_iter_missing_items.py",
      "line": 41,
      "side": "RIGHT",
      "diff_hunk": "@@ -30,3 +30,19 @@\n     pass\n for a, b in d_tuple_annotated: \n     pass\n+\n+# Empty dict cases\n+empty_dict = {}\n+empty_dict[\"x\"] = 1\n+for k, v in empty_dict:\n+    pass\n+\n+empty_dict_annotated_tuple_keys: dict[tuple[int, str], bool] = {}\n+empty_dict_annotated_tuple_keys[(\"x\", \"y\")] = True",
      "comment": "I think it would be interesting to have these two lines in separate test cases, i.e.\n\n```py\nempty_dict_annotated_tuple_keys: dict[tuple[int, str], bool] = {}\nfor k, v in empty_dict_annotated_tuple_keys:\n    pass\n\nempty_dict_annotated_tuple_keys = {}\nempty_dict_annotated_tuple_keys[(\"x\", \"y\")] = True\nfor k, v in empty_dict_annotated_tuple_keys:\n    pass\n```\n\nI don't think we have to skip the second case even if it's technically incorrect because it's likely to be rare, as mentioned on the issue. But it seems useful to capture our behavior in this case.",
      "comment_id": 2500947096,
      "user": "ntBre",
      "created_at": "2025-11-06T21:35:27Z",
      "url": "https://github.com/astral-sh/ruff/pull/21290#discussion_r2500947096"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 19799,
      "file_path": "crates/ruff_linter/resources/test/fixtures/ruff/RUF052_1.py",
      "line": 88,
      "side": "RIGHT",
      "diff_hunk": "@@ -83,6 +83,22 @@ def fun(x):\n         return ___\n     return x\n \n+# Correct usage in loop and comprehension\n+def process_data():\n+    return 42",
      "comment": "Do we need his function here? I usually try to strip down test cases as much as possible. I think the example usage below would be fine as\n\n```py\n[1 for _ in my_list]\n```\n\nfor example.",
      "comment_id": 2274174454,
      "user": "ntBre",
      "created_at": "2025-08-13T17:32:21Z",
      "url": "https://github.com/astral-sh/ruff/pull/19799#discussion_r2274174454"
    },
    {
      "repo": "astral-sh/ruff",
      "pr_number": 19799,
      "file_path": "crates/ruff_linter/resources/test/fixtures/ruff/RUF052_1.py",
      "line": 99,
      "side": "RIGHT",
      "diff_hunk": "@@ -83,6 +83,22 @@ def fun(x):\n         return ___\n     return x\n \n+# Correct usage in loop and comprehension\n+def process_data():\n+    return 42\n+def test_correct_dummy_usage():\n+    my_list = [{\"foo\": 1}, {\"foo\": 2}]\n+\n+    # Should NOT detect - dummy variable is not used\n+    [process_data() for _ in my_list]  # OK: `_` is ignored by rule\n+\n+    # Should NOT detect - dummy variable is not used\n+    [item[\"foo\"] for item in my_list]  # OK: not a dummy variable name\n+\n+    # Should NOT detect - dummy variable is not used\n+    [42 for _unused in my_list]  # OK: `_unused` is not accessed",
      "comment": "Would you mind moving this test case to the bottom of the file too? It will make the snapshots a lot easier to review if all of the others don't shift down. I know it will break up the existing `Correct` and `Incorrect` sections, so another option would be to create a separate `RUF052_1.py` file instead, if you prefer that.",
      "comment_id": 2274193250,
      "user": "ntBre",
      "created_at": "2025-08-13T17:41:27Z",
      "url": "https://github.com/astral-sh/ruff/pull/19799#discussion_r2274193250"
    }
  ]
}