{
  "tech": "docker",
  "count": 141,
  "examples": [
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13511,
      "title": "[BUG] gw_priority is ignored on ipv4 network if service also joins an ipv6 network without priority",
      "problem": "### Description\n\nNot sure if this is \"as intended\", if so, it might worth documenting it.\n\nAs mentioned in comments like: \nhttps://github.com/docker/roadmap/issues/658#issuecomment-2864025303\n\n### Steps To Reproduce\n\n```yaml\nnetworks:\n  lan_access:\n    name: lan_access\n    driver: macvlan\n    enable_ipv6: false\n    enable_ipv4: true\n    driver_opts:\n      parent: eth0\n    ipam:\n      driver: default\n      config:\n        - subnet: 192.168.10.0/24\n          gateway: 192.168.10.1\n\n  databases:\n    name: databases\n    driver: bridge\n    enable_ipv6: true\n    enable_ipv4: true\n\nservices:\n  postgres:\n    image: postgres:18.1@sha256:bfe50b2b0ddd9b55eadedd066fe24c7c6fe06626185b73358c480ea37868024d\n    networks:\n      databases: {}\n  home-assistant:\n    image: ghcr.io/home-operations/home-assistant:2025.12.5@sha256:6be0843b06ad82233639e56f261b2377f78df140e3d16d2d08b2c29c8889b697\n    networks:\n      databases: {}\n      lan_access:\n        ipv4_address: 192.168.10.18\n        gw_priority: 1000 \n```\n\n`databases` will be the default gw.\nForcing `databases` to disable ipv6 it will make `lan_access` the default gw.\n\n### Compose Version\n\n```Text\n\n```\n\n### Docker Environment\n\n```Text\nClient: Docker Engine - Community\n Version:    28.3.1\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.25.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.38.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 29\n  Running: 29\n  Paused: 0\n  Stopped: 0\n Images: 32\n Server Version: 28.3.1\n Storage Driver: overlay2\n  Backing Filesystem: zfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: cgroupfs\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: runc io.containerd.runc.v2 nvidia\n Default Runtime: nvidia\n Init Binary: docker-init\n containerd version: 05044ec0a9a75232cad458027ca83437aae3f4da\n runc version: v1.2.5-0-g59923ef\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.12.33-production+truenas\n Operating System: Debian GNU/Linux 12 (bookworm)\n OSType: linux\n Architecture: x86_64\n CPUs: 16\n Total Memory: 62.63GiB\n Name: truenas\n ID: af416a29-e97e-48c4-a294-35253e26efb6\n Docker Root Dir: /mnt/.ix-apps/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Default Address Pools:\n   Base: 172.16.0.0/12, Size: 24\n```\n\n### Anything else?\n\n_No response_",
      "solution": "Hi @stavros-k , The root cause was that NetworkingConfig.EndpointsConfig was constructed from a map, which results in non-deterministic ordering. As a consequence, gw_priority was not consistently honored when multiple networks were defined in the Compose file.\n\nThe fix ensures endpoint configurations are explicitly ordered by descending gw_priority before being passed to the Docker Engine. A unit test was added to validate the correct ordering.\n\nPR: <https://github.com/docker/compose/pull/13512>",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2026-01-14T19:49:11Z",
      "closed_at": "2026-02-04T07:17:19Z",
      "url": "https://github.com/docker/compose/issues/13511",
      "comments_count": 9
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13540,
      "title": "[BUG] Variables not interpolated in env_file in included subproject",
      "problem": "### Description\n\nWhen I provide values for variable interpolation in a top-level compose file, and try to use them in an included compose file's service env_file, they are not interpolated. They are interpolated in the sub compose file itself. If I add the variable to a .env file in the subproject, it is interpolated in the env_file.\n\nIf the premise is that top-level variables are interpolated in the subproject, and this works in the compose file itself, then I think they should be interpolated in the env_files of the subproject's services as well.\n\n### Steps To Reproduce\n\n\n```\n$ tree\n.\n\u251c\u2500\u2500 compose.yml\n\u251c\u2500\u2500 subproj\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 subcompose.yml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 subvalues.env\n\u2514\u2500\u2500 values.env\n\n2 directories, 4 files\n```\n```\n$ cat compose.yml \ninclude:\n  - path: subproj/subcompose.yml\n    env_file:\n      - values.env\n```\n```\n$ cat subproj/subcompose.yml \nservices:\n  app:\n    env_file: subvalues.env\n    image: helloworld\n```\n```\n$ cat subproj/subvalues.env \nMYVAR=${VAR?}\n```\n```\n$ cat values.env \nVAR=1\n```\n```\n$ docker compose config\nfailed to read /Users/XXX/Documents/Projects/dc-bug/subproj/subvalues.env: required variable VAR is missing a value\n```\n\n### Compose Version\n\n```Text\nDocker Compose version v5.0.1\n```\n\n### Docker Environment\n\n```Text\nClient:\n Version:    29.1.3\n Context:    desktop-linux\n Debug Mode: false\n Plugins:\n  ai: Docker AI Agent - Ask Gordon (Docker Inc.)\n    Version:  v1.17.1\n    Path:     /Users/XXX/.docker/cli-plugins/docker-ai\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.1-desktop.1\n    Path:     /Users/XXX/.docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v5.0.1\n    Path:     /Users/XXX/.docker/cli-plugins/docker-compose\n  debug: Get a shell into any image or container (Docker Inc.)\n    Version:  0.0.47\n    Path:     /Users/XXX/.docker/cli-plugins/docker-debug\n  desktop: Docker Desktop commands (Docker Inc.)\n    Version:  v0.2.0\n    Path:     /Users/XXX/.docker/cli-plugins/docker-desktop\n  extension: Manages Docker extensions (Docker Inc.)\n    Version:  v0.2.31\n    Path:     /Users/XXX/.docker/cli-plugins/docker-extension\n  init: Creates Docker-related starter files for your project (Docker Inc.)\n    Version:  v1.4.0\n    Path:     /Users/XXX/.docker/cli-plugins/docker-init\n  mcp: Docker MCP Plugin (Docker Inc.)\n    Version:  v0.35.0\n    Path:     /Users/XXX/.docker/cli-plugins/docker-mcp\n  model: Docker Model Runner (Docker Inc.)\n    Version:  v1.0.6\n    Path:     /Users/XXX/.docker/cli-plugins/docker-model\n  offload: Docker Offload (Docker Inc.)\n    Version:  v0.5.40\n    Path:     /Users/XXX/.docker/cli-plugins/docker-offload\n  pass: Docker Pass Secrets Manager Plugin (beta) (Docker Inc.)\n    Version:  v0.0.22\n    Path:     /Users/XXX/.docker/cli-plugins/docker-pass\n  sandbox: Docker Sandbox (Docker Inc.)\n    Version:  v0.6.0\n    Path:     /Users/XXX/.docker/cli-plugins/docker-sandbox\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\n    Version:  0.6.0\n    Path:     /Users/XXX/.docker/cli-plugins/docker-sbom\n  scout: Docker Scout (Docker Inc.)\n    Version:  v1.19.0\n    Path:     /Users/XXX/.docker/cli-plugins/docker-scout\n\nServer:\n Containers: 24\n  Running: 8\n  Paused: 0\n  Stopped: 16\n Images: 63\n Server Version: 29.1.3\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: cgroupfs\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Discovered Devices:\n  cdi: docker.com/gpu=webgpu\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: dea7da592f5d1d2b7755e3a161be07f43fad8f75\n runc version: v1.3.4-0-gd6d73eb8\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.12.54-linuxkit\n Operating System: Docker Desktop\n OSType: linux\n Architecture: aarch64\n CPUs: 14\n Total Memory: 15.6GiB\n Name: docker-desktop\n ID: 248e0de3-b470-4089-882a-ad50dd69405f\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n HTTP Proxy: http.docker.internal:3128\n HTTPS Proxy: http.docker.internal:3128\n No Proxy: hubproxy.docker.internal\n Labels:\n  com.docker.desktop.address=unix:///Users/XXX/Library/Containers/com.docker.docker/Data/docker-cli.sock\n Experimental: false\n Insecure Registries:\n  hubproxy.docker.internal:5555\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\n\n### Anything else?\n\nThis seems similar to https://github.com/docker/compose/issues/10842, and might actually be an issue in compose-go, but I didn't want to make that assumption since compose is the implementation I can reproduce the issue with.",
      "solution": "Hi @ianhinder  @amyssnippet - I've been digging into this as well.\n\nWhile the root cause is indeed in how compose-go handles recursive scope, I believe we can solve this in the CLI loader by pre-processing the top-level include env_files before the project load. This would fix the user issue immediately without requiring invasive changes to the core library's recursion logic.\n\nI have a working implementation and a regression test ready. I'll open a PR shortly so we can discuss if this approach is preferred over moving the issue upstream.\n\n---\n\n@amyssnippet docker/compose and compose-go are maintained in a coordinated way. Many compose issue are actually fixed on compose-go, without creating a dedicated issue\n\n---\n\n@amyssnippet The Compose repository is the primary entry point for users to report issues. \nAs maintainers, we know where a given issue should ultimately be fixed, but we still need to keep the issue tracked here so others encountering the same problem can see that it\u2019s already been reported.",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2026-01-21T23:27:53Z",
      "closed_at": "2026-02-03T07:34:09Z",
      "url": "https://github.com/docker/compose/issues/13540",
      "comments_count": 15
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13559,
      "title": "[BUG] panic in monitor.Start",
      "problem": "### Description\n\nI'm getting this error on some occasions:\n\n```\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x48 pc=0xf601f2]\ngoroutine 107 [running]:\ngithub.com/docker/compose/v5/pkg/compose.(*monitor).Start(0xc000530c40, {0x157a700, 0xc00003c820})\n        github.com/docker/compose/v5/pkg/compose/monitor.go:150 +0xc12\ngithub.com/docker/compose/v5/pkg/compose.(*composeService).Up.func7()\n        github.com/docker/compose/v5/pkg/compose/up.go:278 +0x31\ngolang.org/x/sync/errgroup.(*Group).Go.func1()\n        golang.org/x/sync@v0.19.0/errgroup/errgroup.go:93 +0x50\ncreated by golang.org/x/sync/errgroup.(*Group).Go in goroutine 1\n        golang.org/x/sync@v0.19.0/errgroup/errgroup.go:78 +0x93\n```\n\nI believe it happens **right after the end** of a scheduled job which includes a `docker compose run --rm --no-deps ...` command. But it doesn't happen every time.\n\nAs a result, the `docker compose up` command that was running in the backgroud also terminates.\n\nI had a glance at the code and it seems odd to me that the first branch of the `if` at https://github.com/docker/compose/blob/56ab28aef3dc35eb46fd701e9a96d1d51319ea60/pkg/compose/monitor.go#L144 falls through. Shouldn't it return? If the container couldn't be found, I don't expect `inspect` to be valid, right?\n\n### Steps To Reproduce\n\nUnfortunately, I didn't manage to make a minimal reproducer yet.\n\n### Compose Version\n\n```Text\nDocker Compose version v5.0.2\n```\n\n### Docker Environment\n\n```Text\nClient: Docker Engine - Community\n Version:    29.2.0\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v5.0.2\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 18\n  Running: 18\n  Paused: 0\n  Stopped: 0\n Images: 9\n Server Version: 29.2.0\n Storage Driver: fuse-overlayfs\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: b98a3aace656320842a23f4a392a33f46af97866\n runc version: v1.3.0-0-g4ca628d1\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.1.0-21-amd64\n Operating System: Debian GNU/Linux 12 (bookworm)\n OSType: linux\n Architecture: x86_64\n CPUs: 12\n Total Memory: 62.69GiB\n Name: redacted\n ID: redacted\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  127.0.0.0/8\n  ::1/128\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\n\n### Anything else?\n\n_No response_",
      "solution": "Oh, I've just noticed https://github.com/docker/compose/commit/d5bb3387ca53d3729ba328bdd334246657152b54 which acknowledges and is supposed to fix this very issue. So I guess I'll wait for the next release and close if the issue is fixed.\n\n---\n\nIndeed fixed by https://github.com/docker/compose/pull/13551",
      "labels": [
        "kind/bug",
        "status/0-triage"
      ],
      "created_at": "2026-01-28T14:25:26Z",
      "closed_at": "2026-02-03T07:29:28Z",
      "url": "https://github.com/docker/compose/issues/13559",
      "comments_count": 2
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 12627,
      "title": "[BUG] not possible to run two compose projects on the same machine anymore, hang",
      "problem": "### Description\n\nWhen executing step 5 it just hang forever (tried leaving it for ~45min) with:\n\n````\n[+] Running 1/2\n \u2714 Network stack2_default        Created                                                                                                                                                                                           0.2s \n - Container stack2-service20-1  Starting                             \n ````\n\nI can do CTRL-C to terminate and then the output is \"exit status 130\" like so:\n\n````\n[+] Running 1/2\n \u2714 Network stack2_default        Created                                                                                                                                                                                           0.2s \n - Container stack2-service20-1  Starting                                                                                                                                                                                        102.2s \nexit status 130\n````\n\nI can't even do `docker compose -p stack1 --file ./compose.stack1.yaml down` or `docker compose -p stack2 --file ./compose.stack2.yaml down`, it will hang in \"Stopping\" state...\n\nOnly way to get Docker engine to respond again is to reboot the host.\n\nIf I try to do the same as in stack1 and stack2 with:\n\n````\ndocker run --rm -d -e ASPNETCORE_HTTP_PORTS=80 -p 7001:80 mcr.microsoft.com/dotnet/samples:aspnetapp-9.0-nanoserver-ltsc2022\ndocker run --rm -d -e ASPNETCORE_HTTP_PORTS=80 -p 8001:80 -p 8002:8002 mcr.microsoft.com/dotnet/samples:aspnetapp-9.0-nanoserver-ltsc2022\n````\n\nThen everything works as expected.\n\nIf you _remove_ the second port in stack2, it also works as expected, strangely enough...\n\n### Steps To Reproduce\n\n1. Set engine to Windows: `docker desktop engine use windows`\n2. Create ./compose.**stack1**.yaml with contents:\n\n````yaml\nservices:\n  service10:\n    image: mcr.microsoft.com/dotnet/samples:aspnetapp-9.0-nanoserver-ltsc2022\n    ports:\n      - \"7001:80\"\n    environment:\n      - ASPNETCORE_URLS=http://+:80\n````\n\n3. Create ./compose.**stack2**.yaml with contents:\n\n````yaml\nservices:\n  service20:\n    image: mcr.microsoft.com/dotnet/samples:aspnetapp-9.0-nanoserver-ltsc2022\n    ports:\n      - \"8001:80\"\n      - \"8002:8002\"\n    environment:\n      - ASPNETCORE_URLS=http://+:80\n````\n\n4. Start project stack1: `docker compose -p stack1 --file ./compose.stack1.yaml up -d`\n5. Start project stack1: `docker compose -p stack2 --file ./compose.stack2.yaml up -d`\n\n### Compose Version\n\n```Text\nDocker Compose version v2.33.1-desktop.1\n```\n\n### Docker Environment\n\n```Text\nClient:\n Version:    28.0.1\n Context:    desktop-windows\n Debug Mode: false\n Plugins:\n  ai: Docker AI Agent - Ask Gordon (Docker Inc.)\n    Version:  v0.9.4\n    Path:     C:\\Users\\pberi\\.docker\\cli-plugins\\docker-ai.exe\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.21.1-desktop.2\n    Path:     C:\\Users\\pberi\\.docker\\cli-plugins\\docker-buildx.exe\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.33.1-desktop.1\n    Path:     C:\\Users\\pberi\\.docker\\cli-plugins\\docker-compose.exe\n  debug: Get a shell into any image or container (Docker Inc.)\n    Version:  0.0.38\n    Path:     C:\\Users\\pberi\\.docker\\cli-plugins\\docker-debug.exe\n  desktop: Docker Desktop commands (Beta) (Docker Inc.)\n    Version:  v0.1.5\n    Path:     C:\\Users\\pberi\\.docker\\cli-plugins\\docker-desktop.exe\n  dev: Docker Dev Environments (Docker Inc.)\n    Version:  v0.1.2\n    Path:     C:\\Users\\pberi\\.docker\\cli-plugins\\docker-dev.exe\n  extension: Manages Docker extensions (Docker Inc.)\n    Version:  v0.2.27\n    Path:     C:\\Users\\pberi\\.docker\\cli-plugins\\docker-extension.exe\n  feedback: Provide feedback, right in your terminal! (Docker Inc.)\n    Version:  v1.0.5\n    Path:     C:\\Users\\pberi\\.docker\\cli-plugins\\docker-feedback.exe\n  init: Creates Docker-related starter files for your project (Docker Inc.)\n    Version:  v1.4.0\n    Path:     C:\\Users\\pberi\\.docker\\cli-plugins\\docker-init.exe\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\n    Version:  0.6.0\n    Path:     C:\\Users\\pberi\\.docker\\cli-plugins\\docker-sbom.exe\n  scout: Docker Scout (Docker Inc.)\n    Version:  v1.16.3\n    Path:     C:\\Users\\pberi\\.docker\\cli-plugins\\docker-scout.exe\n\nServer:\n Containers: 7\n  Running: 5\n  Paused: 0\n  Stopped: 2\n Images: 180\n Server Version: 28.0.1\n Storage Driver: windowsfilter\n  Windows: \n Logging Driver: json-file\n Plugins:\n  Volume: local\n  Network: ics internal l2bridge l2tunnel nat null overlay private transparent\n  Log: awslogs etwlogs fluentd gcplogs gelf json-file local splunk syslog\n Swarm: inactive\n Default Isolation: hyperv\n Kernel Version: 10.0 26100 (26100.1.amd64fre.ge_release.240331-1435)\n Operating System: Microsoft Windows Version 24H2 (OS Build 26100.3323)\n OSType: windows\n Architecture: x86_64\n CPUs: 24\n Total Memory: 63.92GiB\n Name: amd\n ID: 2588f1d0-8302-469b-935a-2dcec18226d4\n Docker Root Dir: D:\\Data\\Docker\n Debug Mode: false\n Labels:\n  com.docker.desktop.address=npipe://\\\\.\\pipe\\docker_cli\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Product License: Community Engine\n```\n\n### Anything else?\n\n_No response_",
      "solution": "just found out that the issue is still present if you first start **stack1** and the do the second `docker run` line from the description. Is seem that compose is breaking all networks...\n\n---\n\nHey @pbering @danieletorelli \nHave you try with version [`v28.0.4` of the engine](https://github.com/moby/moby/releases/tag/v28.0.4)? If so and you still have the issue can you check the [latest Compose release v2.35.0](https://github.com/docker/compose/releases/tag/v2.35.0)?\n\n---\n\n> Hey [@pbering](https://github.com/pbering) [@danieletorelli](https://github.com/danieletorelli) Have you try with version [`v28.0.4` of the engine](https://github.com/moby/moby/releases/tag/v28.0.4)? If so and you still have the issue can you check the [latest Compose release v2.35.0](https://github.com/docker/compose/releases/tag/v2.35.0)?\n\nHey @glours, I'm now sure my issue is https://github.com/moby/moby/issues/49513 and not this one, so I'll not be able to help. Let's wait for @pbering on this.",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2025-03-11T20:29:40Z",
      "closed_at": "2026-01-29T07:20:31Z",
      "url": "https://github.com/docker/compose/issues/12627",
      "comments_count": 21
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 12317,
      "title": "[BUG] docker compose up --watch with sync+restart ends up disconnecting from watch",
      "problem": "### Description\n\nSimilar to https://github.com/docker/compose/issues/11773 I'm seeing `docker compose up --watch` disconnecting when the container is restarted:\n\n```\n\u276f docker compose up --watch\n[+] Running 1/0\n \u2714 Container caddy-caddy-1  Running                                                        0.0s\n        \u29bf Watch enabled\nAttaching to caddy-1\n         \u29bf Syncing and restarting service \"caddy\" after 1 changes were detected\ncaddy-1  | 2024/11/23 00:42:30.609\tINFO\tshutting down apps, then terminating\t{\"signal\": \"SIGTERM\"}\ncaddy-1  | 2024/11/23 00:42:30.609\tWARN\texiting; byeee!! \ud83d\udc4b\t{\"signal\": \"SIGTERM\"}\ncaddy-1  | 2024/11/23 00:42:30.609\tINFO\thttp\tservers shutting down with eternal grace period\ncaddy-1  | 2024/11/23 00:42:30.609\tINFO\tadmin\tstopped previous server\t{\"address\": \"localhost:2019\"}\ncaddy-1  | 2024/11/23 00:42:30.609\tINFO\tshutdown complete\t{\"signal\": \"SIGTERM\", \"exit_code\": 0}\n         \u29bf service \"caddy\" restarted\ncaddy-1 exited with code 0\n         \u29bf Syncing and restarting service \"caddy\" after 1 changes were detected\ncaddy-1  | 2024/11/23 00:42:39.434\tINFO\tshutting down apps, then terminating\t{\"signal\": \"SIGTERM\"}\ncaddy-1  | 2024/11/23 00:42:39.434\tWARN\texiting; byeee!! \ud83d\udc4b\t{\"signal\": \"SIGTERM\"}\ncaddy-1  | 2024/11/23 00:42:39.434\tINFO\thttp\tservers shutting down with eternal grace period\ncaddy-1  | 2024/11/23 00:42:39.434\tINFO\tadmin\tstopped previous server\t{\"address\": \"localhost:2019\"}\ncaddy-1  | 2024/11/23 00:42:39.434\tINFO\tshutdown complete\t{\"signal\": \"SIGTERM\", \"exit_code\": 0}\ncaddy-1 exited with code 0\n         \u29bf Watch disabled\n\n\u276f\n```\n\n```\nservices:\n  caddy:\n    image: torarnv/caddy\n    build: .\n    restart: unless-stopped\n    ports:\n      - \"8080:80\"\n\n    develop:\n      watch:\n        - action: sync+restart\n          path: ./Caddyfile\n          target: /etc/caddy/Caddyfile\n```\n\nLikely because it temporarily sees that there are no services running.\n\nAdding a keep-alive workaround service \"fixes\" the issue:\n\n```\ndummy-watch-workaround:\n  image: alpine:latest\n  init: true\n  command: [\"sh\", \"-c\", \"while true; do sleep 2; done\"]\n```\n\n### Compose Version\n\n2.30.3\n",
      "solution": "Hello @torarnv \nI tried to reproduce your issue, in between we released a signifiant number of Compose release and I'm not able to reproduce your issue\n```\ncaddy-reproducer-1  | {\"level\":\"info\",\"ts\":1744638529.4268618,\"logger\":\"tls\",\"msg\":\"finished cleaning storage units\"}\n                    \u29bf Syncing service \"caddy-reproducer\" after 1 changes were detected\ncaddy-reproducer-1  | {\"level\":\"info\",\"ts\":1744638567.5292418,\"msg\":\"shutting down apps, then terminating\",\"signal\":\"SIGTERM\"}\ncaddy-reproducer-1  | {\"level\":\"warn\",\"ts\":1744638567.529311,\"msg\":\"exiting; byeee!! \ud83d\udc4b\",\"signal\":\"SIGTERM\"}\ncaddy-reproducer-1  | {\"level\":\"info\",\"ts\":1744638567.5293336,\"logger\":\"http\",\"msg\":\"servers shutting down with eternal grace period\"}\ncaddy-reproducer-1  | {\"level\":\"info\",\"ts\":1744638567.5295353,\"logger\":\"admin\",\"msg\":\"stopped previous server\",\"address\":\"localhost:2019\"}\ncaddy-reproducer-1  | {\"level\":\"info\",\"ts\":1744638567.529549,\"msg\":\"shutdown complete\",\"signal\":\"SIGTERM\",\"exit_code\":0}\n                    \u29bf service(s) [\"caddy-reproducer\"] restarted\ncaddy-reproducer-1 exited with code 0\ncaddy-reproducer-1  | {\"level\":\"info\",\"ts\":1744638567.8666615,\"msg\":\"using config from file\",\"file\":\"/etc/caddy/Caddyfile\"}\n```\n\nCan you let me know if the latest version of Compose fixed the issue also for you?\n\n---\n\nNot OP, but I also had this issue, just upgraded from v2.28 to v2.38 and it fixed the issue. `docker compose up` will stay connected after watch restarts the image.\n\n---\n\nUpgrading to the latest version resolved the issue for me. Seems like it got fixed in v2.39.3 (possibly by https://github.com/docker/compose/pull/13210)",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2024-11-24T12:36:19Z",
      "closed_at": "2026-01-28T13:02:38Z",
      "url": "https://github.com/docker/compose/issues/12317",
      "comments_count": 9
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13552,
      "title": "[BUG] COMPOSE_PROFILES environment variable ignored by Compose v5.0.1",
      "problem": "### Description\n\nBased on the documentation here: https://docs.docker.com/compose/how-tos/profiles/, the `COMPOSE_PROFILES` environment variable should allow selecting profiles when running Docker Compose. The documentation gives the following usage example:\n`COMPOSE_PROFILES=debug docker compose up`\n\nHowever, in my environment the variable is not being picked up by Docker Compose. I\u2019ve tried both exporting the variable and specifying it inline as part of the Compose command, and neither method works. The --profile flag does work as expected.\n\n### Steps To Reproduce\n\n**Environment:**\nUbuntu-based WSL2 environment, with Docker/Compose running through Docker Desktop on the Windows host. The WSL Integration toggle is enabled under Resources \u2192 WSL Integration.\n\n**Docker compose file**:\n```yml\n\nservices:\n  redis_default:\n    image: redis:latest\n\n  ubuntu_test:\n    image: ubuntu:latest\n    command: [\"sleep\", \"infinity\"]\n    profiles:\n      - ubuntu\n\n  alpine_test:\n    image: alpine:latest\n    command: [\"sleep\", \"infinity\"]\n    profiles:\n      - alpine\n```\n\n**Specifying variable inline:**\n```\n> COMPOSE_PROFILES=ubuntu docker compose up -d\n \u2714 Network docker_default  Created\n \u2714 Container redis_default Created\n\n> docker ps\nCONTAINER ID   IMAGE          COMMAND                  CREATED          STATUS          PORTS      NAMES\nb7330a5f8db4   redis:latest   \"docker-entrypoint.s\u2026\"   11 seconds ago   Up 10 seconds   6379/tcp   redis_default\n```\nOnly the default service is started. The profile-specific service `ubuntu_test` is ignored.\n\n\n**Exporting the variable:**\n```\n> export COMPOSE_PROFILES=ubuntu\n\n> echo $COMPOSE_PROFILES\nubuntu\n\n> docker compose up -d\n \u2714 Network docker_default  Created\n \u2714 Container redis_default Created \n\n> docker ps\nCONTAINER ID   IMAGE          COMMAND                  CREATED         STATUS         PORTS      NAMES\n300fc148e9dd   redis:latest   \"docker-entrypoint.s\u2026\"   4 seconds ago   Up 3 seconds   6379/tcp   redis_default\n```\nAgain, the Compose file's profile is not applied.\n\n\n**Using `--profile` flag (working):**\n\n```\n> docker compose --profile ubuntu up -d\n \u2714 Network docker_default   Created\n \u2714 Container redis_default  Created\n \u2714 Container ubuntu_profile Created   \n```\n\n### Compose Version\n\n```Text\n> docker compose version\nDocker Compose version v5.0.1\n```\n\n### Docker Environment\n\n```Text\nClient:\n Version:    29.1.3\n Context:    default\n Debug Mode: false\n Plugins:\n  ai: Docker AI Agent - Ask Gordon (Docker Inc.)\n    Version:  v1.17.1\n    Path:     /usr/local/lib/docker/cli-plugins/docker-ai\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.1-desktop.1\n    Path:     /usr/local/lib/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v5.0.1\n    Path:     /usr/local/lib/docker/cli-plugins/docker-compose\n  debug: Get a shell into any image or container (Docker Inc.)\n    Version:  0.0.47\n    Path:     /usr/local/lib/docker/cli-plugins/docker-debug\n  desktop: Docker Desktop commands (Docker Inc.)\n    Version:  v0.2.0\n    Path:     /usr/local/lib/docker/cli-plugins/docker-desktop\n  extension: Manages Docker extensions (Docker Inc.)\n    Version:  v0.2.31\n    Path:     /usr/local/lib/docker/cli-plugins/docker-extension\n  init: Creates Docker-related starter files for your project (Docker Inc.)\n    Version:  v1.4.0\n    Path:     /usr/local/lib/docker/cli-plugins/docker-init\n  mcp: Docker MCP Plugin (Docker Inc.)\n    Version:  v0.35.0\n    Path:     /usr/local/lib/docker/cli-plugins/docker-mcp\n  model: Docker Model Runner (Docker Inc.)\n    Version:  v1.0.6\n    Path:     /usr/local/lib/docker/cli-plugins/docker-model\n  offload: Docker Offload (Docker Inc.)\n    Version:  v0.5.40\n    Path:     /usr/local/lib/docker/cli-plugins/docker-offload\n  pass: Docker Pass Secrets Manager Plugin (beta) (Docker Inc.)\n    Version:  v0.0.22\n    Path:     /usr/local/lib/docker/cli-plugins/docker-pass\n  sandbox: Docker Sandbox (Docker Inc.)\n    Version:  v0.6.0\n    Path:     /usr/local/lib/docker/cli-plugins/docker-sandbox\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\n    Version:  0.6.0\n    Path:     /usr/local/lib/docker/cli-plugins/docker-sbom\n  scout: Docker Scout (Docker Inc.)\n    Version:  v1.19.0\n    Path:     /usr/local/lib/docker/cli-plugins/docker-scout\n\nServer:\n Containers: 5\n  Running: 1\n  Paused: 0\n  Stopped: 4\n Images: 15\n Server Version: 29.1.3\n Storage Driver: overlayfs\n  driver-type: io.containerd.snapshotter.v1\n Logging Driver: json-file\n Cgroup Driver: cgroupfs\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Discovered Devices:\n  cdi: docker.com/gpu=webgpu\n Swarm: inactive\n Runtimes: runc io.containerd.runc.v2 nvidia\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: dea7da592f5d1d2b7755e3a161be07f43fad8f75\n runc version: v1.3.4-0-gd6d73eb8\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.6.87.2-microsoft-standard-WSL2\n Operating System: Docker Desktop\n OSType: linux\n Architecture: x86_64\n CPUs: 24\n Total Memory: 31.19GiB\n Name: docker-desktop\n ID: 90cb6567-8909-4cef-ad82-b0d3fa95706f\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n HTTP Proxy: http.docker.internal:3128\n HTTPS Proxy: http.docker.internal:3128\n No Proxy: hubproxy.docker.internal\n Labels:\n  com.docker.desktop.address=unix:///var/run/docker-cli.sock\n Experimental: false\n Insecure Registries:\n  hubproxy.docker.internal:5555\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\n\n### Anything else?\n\n_No response_",
      "solution": "> [@MaheshThakur9152](https://github.com/MaheshThakur9152) nope. Please don't jump on the most obvious fix your AI agent can offer to an issue. We need to understand why this behavior changed, and which mechanism was supposed to offer this feature, then why it suddenly stopped offering the expected feature _then_ look for a fix.\n\n@ndeloof My proposal was based on my own reading of `cmd/compose/compose.go`, not an AI agent. You can disagree with the architectural approach without assuming my contribution is low-effort generation. But since you've identified the root cause in `compose-go`, I'll leave the fix to you.\n\n---\n\n> I can't reproduce this issue:\n> \n> ```\n> $  cat compose.yaml \n> services:\n>   test:\n>     profiles: [test]\n>     image: nginx\n> $ docker compose up -d\n> no service selected\n> $ COMPOSE_PROFILES=test docker compose up -d\n> [+] up 1/1\n>  \u2714 Container truc-test-1 Running\n> ```\n\nI tested this in a new VM and you're right. I can't reproduce it inside a separate VM. There seems to be an issue inside the WSL2 environment I'm using. Sorry about creating the issue as this isn't a bug with compose. I'll close the issue now.",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2026-01-23T19:07:05Z",
      "closed_at": "2026-01-26T20:06:14Z",
      "url": "https://github.com/docker/compose/issues/13552",
      "comments_count": 8
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13530,
      "title": "[BUG] Compose v5.0.1 missing macOS fsevents, causing \"too many open files\" crash",
      "problem": "### Description\n\n`docker compose watch` immediately crashes with too many open files with compose **v5.0.1**.\n\nThe docker-compose binary appears to be compiled without the macOS-specific build tag. The watcher falls back to the generic watcher, which opens a file descriptor for every single subdirectory (including ignored ones like `vendor/`), instantly exhausting the OS file limit.\n\nOutput of `strings ~/.docker/cli-plugins/docker-compose | grep \"fsevents\"` is empty on **5.0.1**\nBut on **5.0.0**\n```\ngithub.com/fsnotify/fsevents\n*fsevents.Event\n*[]fsevents.Event\n*fsevents.EventFlags\n*fsevents._Ctype_int\n*fsevents.CreateFlags\n*fsevents._Ctype_long\n*fsevents._Ctype_uint\n*fsevents._Ctype_char\n...\n```\n\nProbably related to this https://github.com/docker/compose/pull/13452\n\nWhen I manually download compose version v5.0.0 everything works.\n\n\n### Expected behavior\n- do not crash with `notify.Add(\"...packages-php\"): watcher.Add(\"...packages-php/apps/application/src/Offer/DTO\"): too many open files`\n\n\n### Steps To Reproduce\n\n  - Update to `docker compose 5.0.1`\n  - Run `docker compose up --watch` on a project with a large directory structure (e.g. PHP `vendor/`), even if the folder is listed in ignore.\n\n### Compose Version\n\n```Text\nDocker Compose version v5.0.1\n```\n\n### Docker Environment\n\n```Text\nClient:\n Version:    29.1.3\n Context:    desktop-linux\n Debug Mode: false\n Plugins:\n  ai: Docker AI Agent - Ask Gordon (Docker Inc.)\n    Version:  v1.17.1\n    Path:     /Users/marek/.docker/cli-plugins/docker-ai\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.1-desktop.1\n    Path:     /Users/marek/.docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v5.0.1\n    Path:     /Users/marek/.docker/cli-plugins/docker-compose\n  debug: Get a shell into any image or container (Docker Inc.)\n    Version:  0.0.47\n    Path:     /Users/marek/.docker/cli-plugins/docker-debug\n  desktop: Docker Desktop commands (Docker Inc.)\n    Version:  v0.2.0\n    Path:     /Users/marek/.docker/cli-plugins/docker-desktop\n  extension: Manages Docker extensions (Docker Inc.)\n    Version:  v0.2.31\n    Path:     /Users/marek/.docker/cli-plugins/docker-extension\n  init: Creates Docker-related starter files for your project (Docker Inc.)\n    Version:  v1.4.0\n    Path:     /Users/marek/.docker/cli-plugins/docker-init\n  mcp: Docker MCP Plugin (Docker Inc.)\n    Version:  v0.35.0\n    Path:     /Users/marek/.docker/cli-plugins/docker-mcp\n  model: Docker Model Runner (Docker Inc.)\n    Version:  v1.0.6\n    Path:     /Users/marek/.docker/cli-plugins/docker-model\n  offload: Docker Offload (Docker Inc.)\n    Version:  v0.5.40\n    Path:     /Users/marek/.docker/cli-plugins/docker-offload\n  pass: Docker Pass Secrets Manager Plugin (beta) (Docker Inc.)\n    Version:  v0.0.22\n    Path:     /Users/marek/.docker/cli-plugins/docker-pass\n  sandbox: Docker Sandbox (Docker Inc.)\n    Version:  v0.6.0\n    Path:     /Users/marek/.docker/cli-plugins/docker-sandbox\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\n    Version:  0.6.0\n    Path:     /Users/marek/.docker/cli-plugins/docker-sbom\n  scout: Docker Scout (Docker Inc.)\n    Version:  v1.19.0\n    Path:     /Users/marek/.docker/cli-plugins/docker-scout\n\nServer:\n Containers: 13\n  Running: 11\n  Paused: 0\n  Stopped: 2\n Images: 29\n Server Version: 29.1.3\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: cgroupfs\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Discovered Devices:\n  cdi: docker.com/gpu=webgpu\n Swarm: active\n  NodeID: n1zalw918d6f8ke2yem1rtzve\n  Is Manager: true\n  ClusterID: m482x04rf5nbkese6hls5i3bi\n  Managers: 1\n  Nodes: 1\n  Default Address Pool: 10.0.0.0/8  \n  SubnetSize: 24\n  Data Path Port: 4789\n  Orchestration:\n   Task History Retention Limit: 5\n  Raft:\n   Snapshot Interval: 10000\n   Number of Old Snapshots to Retain: 0\n   Heartbeat Tick: 1\n   Election Tick: 10\n  Dispatcher:\n   Heartbeat Period: 5 seconds\n  CA Configuration:\n   Expiry Duration: 3 months\n   Force Rotate: 0\n  Autolock Managers: false\n  Root Rotation In Progress: false\n  Node Address: 192.168.65.3\n  Manager Addresses:\n   192.168.65.3:2377\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: dea7da592f5d1d2b7755e3a161be07f43fad8f75\n runc version: v1.3.4-0-gd6d73eb8\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.12.54-linuxkit\n Operating System: Docker Desktop\n OSType: linux\n Architecture: aarch64\n CPUs: 8\n Total Memory: 7.751GiB\n Name: docker-desktop\n ID: b415a42f-26af-4625-ac0e-348c86ce9989\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n HTTP Proxy: http.docker.internal:3128\n HTTPS Proxy: http.docker.internal:3128\n No Proxy: hubproxy.docker.internal\n Labels:\n  com.docker.desktop.address=unix:///Users/marek/Library/Containers/com.docker.docker/Data/docker-cli.sock\n Experimental: false\n Insecure Registries:\n  hubproxy.docker.internal:5555\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\n\n### Anything else?\n\n_No response_",
      "solution": "The bug is fixed https://github.com/docker/compose/pull/13532 but new version of Docker Desktop for Mac was not released yet. \n\nI deleted the compose plugin from `.docker/cli-plugins` and downloaded the working binary from this repo. Otherwise you need to wait and I dont know when the new versions of docker desktop are created. Seems pretty random",
      "labels": [
        "kind/bug",
        "status/0-triage"
      ],
      "created_at": "2026-01-20T09:31:38Z",
      "closed_at": "2026-01-20T13:05:12Z",
      "url": "https://github.com/docker/compose/issues/13530",
      "comments_count": 3
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13553,
      "title": "`-p` project name does not isolate container names when `container_name` is specified",
      "problem": "## Problem\n\nDocker Compose supports specifying a project name using the `-p` option, which is useful for running multiple Compose projects in parallel.\nHowever, when `container_name` is explicitly defined in `docker-compose.yaml`, the project name is not reflected in the actual container name.\n\nAs a result, running multiple docker compose up commands with different project names but the same container_name causes container name conflicts and prevents parallel execution.\n\n## Motivation\n\nIn recent development workflows, it is increasingly common to run multiple environments simultaneously, for example:\n- Using git worktree for parallel feature development\n- Running multiple test or sandbox environments\n- AI agents spinning up Compose environments concurrently\n\nIn these cases, users expect the `-p` option to provide isolation between projects, but this expectation is broken when `container_name` is used.\n\n## Request\n\nIt would be helpful if Docker Compose could support a mechanism to avoid container name conflicts even when `container_name` is specified and `-p` is used.\n\nMore generally, enabling Docker Compose to run multiple instances of the same configuration in parallel without requiring users to modify Compose files would significantly improve the developer experience.\n\n## Expected Outcome\n\n- Easier parallel execution of Docker Compose projects\n- Reduced friction when using modern, multi-environment workflows\n- Better alignment between project-level isolation and actual Docker resources",
      "solution": "There\u2019s also a conflict with port binding. I\u2019d be happy if we could come up with a solution to resolve this as well...\n\n---\n\nGlad that approach works for you, @kazuki-hanai! @ndeloof Before I open a PR, I wanted to check if you're open to this solution?\n\n**Proposal:** Add an opt-in flag (e.g., `--isolate-names`) that forces Docker Compose to namespace the container name with the project prefix, even when `container_name` is explicitly set.\n\n**Benefit:** This solves the collision issue for parallel execution (CI, git worktrees) without breaking backward compatibility or forcing users to modify their `docker-compose.yaml`.\n\nIf that aligns with the project direction, I'm happy to implement it!",
      "labels": [
        "kind/feature",
        "status/0-triage"
      ],
      "created_at": "2026-01-24T07:29:37Z",
      "closed_at": "2026-01-24T08:58:25Z",
      "url": "https://github.com/docker/compose/issues/13553",
      "comments_count": 6
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13539,
      "title": "Option to skip validation during publishing",
      "problem": "### Description\n\nThere are certain Compose files that I would like to publish that are intended to overlay another set of Compose files. However, I want to publish each of them separately to allow the end user to pick and choose versions, etc.\n\nHowever, when publishing one of these overlay files, I end up with validation failing because the Compose file isn't complete (since it's merely an overlay/override). I get something like this:\n\n```\nservice \"workspace\" has neither an image nor a build context specified: invalid compose project\n```\n\nIt would be nice if I could publish a partial file by informing Compose I want to skip validation. Something like...\n\n```\ndocker compose publish --skip-validation my-namespace/my-repo\n```\n\nFor the context/use case - we have the main Labspace Compose files and are creating various extensions to them to add other services. We'd like to publish each of them independently.",
      "solution": "I'm not in favor for this feature: `compose publish` main goal is to enforce published artifact is consistent and fully defined, including references to image (with `--app`) while this feature request is about making a compose artifact just a random yaml resource that will be resolved at runtime. While there may be some use-cases for it, then better just rely on an OCI registry client to publish those.",
      "labels": [
        "kind/feature"
      ],
      "created_at": "2026-01-21T21:38:08Z",
      "closed_at": "2026-01-23T07:52:19Z",
      "url": "https://github.com/docker/compose/issues/13539",
      "comments_count": 7
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13144,
      "title": "[BUG] `docker-compose up` duplicating logs after upgrading",
      "problem": "### Description\n\nAfter a minor version upgrade (2.38.2 -> 2.39.1) container logs are being duplicated\n\n### Steps To Reproduce\n\nhttps://github.com/jessienyan/booruview\n\n1. Using 2.38.2, run `docker compose up --build` (or `./dev.sh`)\n2. Observe container logs are normal\n3. Using 2.39.1, run `docker compose up --build` (or `./dev.sh`)\n4. Observe container logs are duplicated\n\n`docker compose logs -f <container>` has the correct output\n\n`up`\n```\nclient-1  | transforming...\nclient-1  | transforming...\nclient-1  | \u2713 158 modules transformed.\nclient-1  | \u2713 158 modules transformed.\nclient-1  | rendering chunks...\nclient-1  | rendering chunks...\nclient-1  | computing gzip size...\nclient-1  | computing gzip size...\nclient-1  | dist/index.html                               1.98 kB \u2502 gzip:  0.88 kB\nclient-1  | dist/assets/bootstrap-icons-pHno5dML.woff2  130.48 kB\nclient-1  | dist/assets/bootstrap-icons-DIW_sQAl.woff   176.18 kB\nclient-1  | dist/assets/index-DtRUWQEb.css               96.94 kB \u2502 gzip: 17.57 kB\nclient-1  | dist/assets/index-CjOb5qaj.js               195.58 kB \u2502 gzip: 67.92 kB \u2502 map: 906.22 kB\nclient-1  | dist/index.html                               1.98 kB \u2502 gzip:  0.88 kB\nclient-1  | dist/assets/bootstrap-icons-pHno5dML.woff2  130.48 kB\nclient-1  | dist/assets/bootstrap-icons-DIW_sQAl.woff   176.18 kB\nclient-1  | dist/assets/index-DtRUWQEb.css               96.94 kB \u2502 gzip: 17.57 kB\nclient-1  | dist/assets/index-CjOb5qaj.js               195.58 kB \u2502 gzip: 67.92 kB \u2502 map: 906.22 kB\nclient-1  | built in 1624ms.\nclient-1  | built in 1624ms.\n```\n\n`logs -f client`\n```\nclient-1  | transforming...\nclient-1  | \u2713 158 modules transformed.\nclient-1  | rendering chunks...\nclient-1  | computing gzip size...\nclient-1  | dist/index.html                               1.98 kB \u2502 gzip:  0.88 kB\nclient-1  | dist/assets/bootstrap-icons-pHno5dML.woff2  130.48 kB\nclient-1  | dist/assets/bootstrap-icons-DIW_sQAl.woff   176.18 kB\nclient-1  | dist/assets/index-DtRUWQEb.css               96.94 kB \u2502 gzip: 17.57 kB\nclient-1  | dist/assets/index-CjOb5qaj.js               195.58 kB \u2502 gzip: 67.92 kB \u2502 map: 906.22 kB\nclient-1  | built in 1624ms.\n```\n\n### Compose Version\n\n```Text\nDocker Compose version v2.39.1\n```\n\n### Docker Environment\n\n```Text\nClient: Docker Engine - Community\n Version:    28.3.3\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.26.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.39.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 5\n  Running: 1\n  Paused: 0\n  Stopped: 4\n Images: 271\n Server Version: 28.3.3\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Discovered Devices:\n  cdi: amd.com/gpu=0\n  cdi: amd.com/gpu=all\n Swarm: inactive\n Runtimes: amd io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 05044ec0a9a75232cad458027ca83437aae3f4da\n runc version: v1.2.5-0-g59923ef\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.12.10-76061203-generic\n Operating System: Pop!_OS 22.04 LTS\n OSType: linux\n Architecture: x86_64\n CPUs: 16\n Total Memory: 31.25GiB\n Name: desktop\n ID: 3dfbff84-5b48-4581-ba77-b324887ff02f\n Docker Root Dir: /var/lib/docker\n Debug Mode: true\n  File Descriptors: 37\n  Goroutines: 58\n  System Time: 2025-08-10T12:59:16.803040294-04:00\n  EventsListeners: 0\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n```\n\n### Anything else?\n\n```bash\n$ less /var/log/dpkg.log | grep docker | grep upgrade\n2025-08-07 20:48:20 upgrade docker-ce-cli:amd64 5:28.3.2-1~debian.12~bookworm 5:28.3.3-1~debian.12~bookworm\n2025-08-07 20:48:21 upgrade docker-ce:amd64 5:28.3.2-1~debian.12~bookworm 5:28.3.3-1~debian.12~bookworm\n2025-08-07 20:48:24 upgrade docker-buildx-plugin:amd64 0.25.0-1~debian.12~bookworm 0.26.1-1~debian.12~bookworm\n2025-08-07 20:48:25 upgrade docker-ce-rootless-extras:amd64 5:28.3.2-1~debian.12~bookworm 5:28.3.3-1~debian.12~bookworm\n2025-08-07 20:48:25 upgrade docker-compose-plugin:amd64 2.38.2-1~debian.12~bookworm 2.39.1-1~debian.12~bookworm\n```",
      "solution": "Hi @glours \nI can see the `v2.39.2` in the releases-tab, but apparently it's not in the apt repository yet (at least for ubuntu 24.04), is this on purpose? \nThis bug is disruptive to me and i would like it fixed, but as far as i can see my only option is to manually install the latest version? Changing the source from `stable` to `test` also only offers `2.39.1`\n\nThank you in advance\n```\nmathias@FLPC8286:~$ sudo apt update\nHit:1 http://security.ubuntu.com/ubuntu noble-security InRelease\nHit:2 https://download.docker.com/linux/ubuntu noble InRelease                                                                                                                                                                    \n[...]\nmathias@FLPC8286:~$ sudo apt upgrade docker-compose-plugin\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\ndocker-compose-plugin is already the newest version (2.39.1-1~ubuntu.24.04~noble).\nCalculating upgrade... Done\n0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n```\n\n---\n\nI am also seeing this bug in MacOS with the latest version of Docker Desktop (4.44.2). Docker compose version is v2.39.1-desktop.1.\n\nDowngrading to Docker Desktop 4.43.2 (199162) (with Compose: v2.38.2-desktop.1) fixed the issue. \n\nIt also fixed an odd problem I started getting when Docker Desktop updated itself today, where pressing Ctrl-C on an active `docker compose up --watch` instance would _immediately_ kill it and return me to the command line, and _not_ stop any of the containers. On version 4.43.2, Ctrl-C works as expected, triggering the shutdown process and eventually stopping each container before returning me to the command line.\n\nPossibly related to the duplicated logs?\n\n---\n\nthe issue in compose itself and still not fixed yet , if you `docker build`  it works fine",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2025-08-10T17:00:38Z",
      "closed_at": "2025-08-29T06:17:05Z",
      "url": "https://github.com/docker/compose/issues/13144",
      "comments_count": 16
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13428,
      "title": "[BUG] No referrers-API backwards compatibility on \"publish --app\"",
      "problem": "### Description\n\nAccording the [OCI Distribution spec v1.1 \"Backwards compatibiltiy\"](https://github.com/opencontainers/distribution-spec/blob/main/spec.md#backwards-compatibility) section:\n\n> Client implementations MUST support registries that implement partial or older versions of the OCI Distribution Spec. This section describes client fallback procedures that MUST be implemented when a new/optional API is not available from a registry.\n\nNeither GitHub nor GitLab support the referrers API. So the current implementation of the feature `publish --app` is useless with these registries, because there is no way to recover the digest of the \"referrer\" (except analyzing verbose publish).\n\nThe specification describes how to detect [lack of support for the referrers API](https://github.com/opencontainers/distribution-spec/blob/main/spec.md#unavailable-referrers-api).\n\nAnd how to [implement backwards compatibility](https://github.com/opencontainers/distribution-spec/blob/main/spec.md#referrers-tag-schema) over the so-called \"Referrers tag schema\".\n\n### Steps To Reproduce\n\n_No response_\n\n### Compose Version\n\n```Text\n`Docker Compose version 5.0.0`\n```\n\n### Docker Environment\n\n```Text\nClient: Docker Engine - Community\n Version:    29.1.2\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.1\n    Path:     /opt/homebrew/lib/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  5.0.0\n    Path:     /opt/homebrew/lib/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 2\n  Running: 1\n  Paused: 0\n  Stopped: 1\n Images: 22\n Server Version: 29.1.2\n Storage Driver: overlayfs\n  driver-type: io.containerd.snapshotter.v1\n Logging Driver: json-file\n Cgroup Driver: cgroupfs\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Discovered Devices:\n  cdi: lima-vm.io/rosetta=cached\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 1c4457e00facac03ce1d75f7b6777a7a851e5c41\n runc version: v1.3.4-0-gd6d73eb8\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.8.0-47-generic\n Operating System: Ubuntu 24.04.1 LTS\n OSType: linux\n Architecture: aarch64\n CPUs: 2\n Total Memory: 3.814GiB\n Name: colima\n ID: b6eece13-9e52-41d5-8562-23f5e5b35c24\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  localhost:20000\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\n\n### Anything else?\n\n_No response_",
      "solution": "Closing as \"not planned\".\nThere's no simple solution for compose to be strictly compliant with the OCI-spec here, as we don't plan to (re)implement a full OCI registry client.",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2025-12-08T08:08:30Z",
      "closed_at": "2026-01-22T14:43:15Z",
      "url": "https://github.com/docker/compose/issues/13428",
      "comments_count": 3
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13136,
      "title": "[BUG] Very large code changes (e.g. checking out a different branch) cause docker compose up --watch to break",
      "problem": "### Description\n\nI've run into a problem with `docker compose up --watch` where, if I check out a different branch of my codebase that has a *lot* of changes compared to the current branch, the watch system will break in a very strange way.\n\nIf I see a log message like this, I know the watch is busted: \n```\n                  \u29bf Syncing service \"multitenant\" after 3115 changes were detected\n```\n\nSpecifically, the container seemingly stops outputting any logs, though later code changes will still be synced into it and logged. In addition, hitting Ctrl-C to kill the swarm will result in output like this:\n\n```\n^CWARN[14037] Error handling changed files: 1 error occurred:\n\t* copying files to 6d240aa8b95517098dd76f4510ee2936df3d80500b52fd5bbd04f2395c8f6ef8: Put \"http://%2FUsers%2Frrollins%2F.docker%2Frun%2Fdocker.sock/v1.51/containers/6d240aa8b95517098dd76f4510ee2936df3d80500b52fd5bbd04f2395c8f6ef8/archive?copyUIDGID=true&noOverwriteDirNonDir=true&path=%2F\": context canceled\n\nGracefully stopping... (press Ctrl+C again to force)\n[+] Stopping 6/6\n \u2714 Container multitenant_www     Stopped5.4s\n \u2714 Container mail-multitenant    Stopped0.8s\n \u2714 Container multitenant         Stopped4.2s\n \u2714 Container redis-multitenant   Stopped0.2s\n \u2714 Container db-multitenant      Stopped1.7s\n \u2714 Container search-multitenant  Stopped0.5s\n^C^Cgot 3 SIGTERM/SIGINTs, forcefully exiting\n```\n\nThe app will ultimately never close itself gracefully, and requires a repeated Ctrl-C command to make it shut down.\n\n### Steps To Reproduce\n\n1. Run `docker compose up --watch`.\n2. Check out a different branch of the repo on the host, causing *many* files to change and need to be synced. It's possible that a `rebuild` action also needs to be triggered by this, but I'm not certain.\n3. If the bug triggers, Ctrl-C will print an error message about \"context canceled\", and the docker compose session will never die, even after the swarm is stopped.\n\n### Compose Version\n\n```Text\nDocker Compose version v2.38.2-desktop.1\n\n(Both are the same)\n```\n\n### Docker Environment\n\n```Text\n$ docker info\nClient:\n Version:    28.3.2\n Context:    desktop-linux\n Debug Mode: false\n Plugins:\n  ai: Docker AI Agent - Ask Gordon (Docker Inc.)\n    Version:  v1.9.9\n    Path:     /Users/rrollins/.docker/cli-plugins/docker-ai\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.25.0-desktop.1\n    Path:     /Users/rrollins/.docker/cli-plugins/docker-buildx\n  cloud: Docker Cloud (Docker Inc.)\n    Version:  v0.4.2\n    Path:     /Users/rrollins/.docker/cli-plugins/docker-cloud\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.38.2-desktop.1\n    Path:     /Users/rrollins/.docker/cli-plugins/docker-compose\n  debug: Get a shell into any image or container (Docker Inc.)\n    Version:  0.0.41\n    Path:     /Users/rrollins/.docker/cli-plugins/docker-debug\n  desktop: Docker Desktop commands (Docker Inc.)\n    Version:  v0.1.11\n    Path:     /Users/rrollins/.docker/cli-plugins/docker-desktop\n  extension: Manages Docker extensions (Docker Inc.)\n    Version:  v0.2.29\n    Path:     /Users/rrollins/.docker/cli-plugins/docker-extension\n  init: Creates Docker-related starter files for your project (Docker Inc.)\n    Version:  v1.4.0\n    Path:     /Users/rrollins/.docker/cli-plugins/docker-init\n  mcp: Docker MCP Plugin (Docker Inc.)\n    Version:  v0.9.9\n    Path:     /Users/rrollins/.docker/cli-plugins/docker-mcp\n  model: Docker Model Runner (EXPERIMENTAL) (Docker Inc.)\n    Version:  v0.1.33\n    Path:     /Users/rrollins/.docker/cli-plugins/docker-model\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\n    Version:  0.6.0\n    Path:     /Users/rrollins/.docker/cli-plugins/docker-sbom\n  scout: Docker Scout (Docker Inc.)\n    Version:  v1.18.2\n    Path:     /Users/rrollins/.docker/cli-plugins/docker-scout\nWARNING: Plugin \"/Users/rrollins/.docker/cli-plugins/docker-dev\" is not valid: failed to fetch metadata: fork/exec /Users/rrollins/.docker/cli-plugins/docker-dev: no such file or directory\n\nServer:\n Containers: 6\n  Running: 0\n  Paused: 0\n  Stopped: 6\n Images: 14\n Server Version: 28.3.2\n Storage Driver: overlayfs\n  driver-type: io.containerd.snapshotter.v1\n Logging Driver: json-file\n Cgroup Driver: cgroupfs\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 05044ec0a9a75232cad458027ca83437aae3f4da\n runc version: v1.2.5-0-g59923ef\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.10.14-linuxkit\n Operating System: Docker Desktop\n OSType: linux\n Architecture: aarch64\n CPUs: 12\n Total Memory: 7.653GiB\n Name: docker-desktop\n ID: 42077820-6f80-461d-86a1-c7cb9ef0c903\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n HTTP Proxy: http.docker.internal:3128\n HTTPS Proxy: http.docker.internal:3128\n No Proxy: hubproxy.docker.internal\n Labels:\n  com.docker.desktop.address=unix:///Users/rrollins/Library/Containers/com.docker.docker/Data/docker-cli.sock\n Experimental: false\n Insecure Registries:\n  hubproxy.docker.internal:5555\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n\nWARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set\n```\n\n### Anything else?\n\n_No response_",
      "solution": "Hi @coredumperror , the fix for this issue has been merged in #13525 ,  It addresses the deadlock that occurred during large file batches (like branch switching). You should see this resolved in the next release of Docker Compose.",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2025-08-07T22:30:17Z",
      "closed_at": "2026-01-20T08:42:06Z",
      "url": "https://github.com/docker/compose/issues/13136",
      "comments_count": 2
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13474,
      "title": "[BUG] Pulling TUI mixes up lines",
      "problem": "### Description\n\nWhen pulling the lines seem to be mixed up and the progress seems to be broken. This used to work in the past and seems to have been introduced with version `5.0.0` so it might be a regression.\n\nLooking at an arch based where the version is `dev` this is also an issue.\n\n### Steps To Reproduce\n\n```\npi@raspberrypi:~/docker/nginxproxymanager $  docker compose pull\npi@raspberrypi:~/docker/nginxproxymanager $ \n \u280b Image jc21/nginx-proxy-manager:latest Pulling                                                                 1.1s \n\npi@raspberrypi:~/docker/nginxproxymanager $ docker compose version\nDocker Compose version v5.0.0\n```\n```\n$ docker compose pull \n[+] pull 0/4\n \u280b Image docker.io/valkey/valkey:9@sha256:fb8d272e529ea567b9bf1302245796f21a2672b8368ca3fcb938ac334e613c8f                                      Pulling 0.1s \n \u280b Image ghcr.io/immich-app/postgres:14-vectorchord0.4.3-pgvectors0.2.0@sha256:bcf63357191b76a916ae5eb93464d65c07511da[+] pull 1/4db519b40b1c23 Pulling 0.1s \n \u2819 Image docker.io/valkey/valkey:9@sha256:fb8d272e529ea567b9bf1302245796f21a2672b8368ca3fcb938ac334e613c8f                                      Pulling 0.2s \n \u2714 Image ghcr.io/immich-app/postgres:14-vectorchord0.4.3-pgvectors0.2.0@sha256:bcf63357191b76a916ae5eb93464d65c07511da[+] pull 1/4db519b40b1c23 Pulled 0.2s  \n \u2839 Image docker.io/valkey/valkey:9@sha256:fb8d272e529ea567b9bf1302245796f21a2672b8368ca3fcb938ac334e613c8f                                      Pulling 0.3s \n \u2714 Image ghcr.io/immich-app/postgres:14-vectorchord0.4.3-pgvectors0.2.0@sha256:bcf63357191b76a916ae5eb93464d65c07511da[+] pull 2/4db519b40b1c23 Pulled 0.2s  \n \u2714 Image docker.io/valkey/valkey:9@sha256:fb8d272e529ea567b9bf1302245796f21a2672b8368ca3fcb938ac334e613c8f                                      Pulled 0.3s  \n \u2714 Image ghcr.io/immich-app/postgres:14-vectorchord0.4.3-pgvectors0.2.0@sha256:bcf63357191b76a916ae5eb93464d65c07511da[+] pull 3/4db519b40b1c23 Pulled 0.2s  \n \u2714 Image docker.io/valkey/valkey:9@sha256:fb8d272e529ea567b9bf1302245796f21a2672b8368ca3fcb938ac334e613c8f                                      Pulled 0.3s  \n \u2714 Image ghcr.io/immich-app/postgres:14-vectorchord0.4.3-pgvectors0.2.0@sha256:bcf63357191b76a916ae5eb93464d65c07511da[+] pull 4/4db519b40b1c23 Pulled 0.2s  \n \u2714 Image docker.io/valkey/valkey:9@sha256:fb8d272e529ea567b9bf1302245796f21a2672b8368ca3fcb938ac334e613c8f                                      Pulled 0.3s  \n \u2714 Image ghcr.io/immich-app/postgres:14-vectorchord0.4.3-pgvectors0.2.0@sha256:bcf63357191b76a916ae5eb93464d65c07511da41e3bf7a8416db519b40b1c23 Pulled 0.2s \n \u2714 Image ghcr.io/immich-app/immich-server:v2                                                                                                    Pulled 0.5s \n \u2714 Image ghcr.io/immich-app/immich-machine-learning:v2-cuda                                                                                     Pulled 0.5s \n$ docker compose version \nDocker Compose version dev\n```\n\n### Compose Version\n\n```Text\n\n```\n\n### Docker Environment\n\n```Text\npi@raspberrypi:~ $  docker info\nClient: Docker Engine - Community\n Version:    29.1.3\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v5.0.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n  scout: Docker Scout (Docker Inc.)\n    Version:  v1.19.0\n    Path:     /home/pi/.docker/cli-plugins/docker-scout\n\nServer:\n Containers: 6\n  Running: 6\n  Paused: 0\n  Stopped: 0\n Images: 6\n Server Version: 29.1.3\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: runc io.containerd.runc.v2\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: dea7da592f5d1d2b7755e3a161be07f43fad8f75\n runc version: v1.3.4-0-gd6d73eb8\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.12.47+rpt-rpi-v8\n Operating System: Debian GNU/Linux 13 (trixie)\n OSType: linux\n Architecture: aarch64\n CPUs: 4\n Total Memory: 7.637GiB\n Name: raspberrypi\n ID: 3bb2421a-420a-4b1b-82e3-d0ed2ff34061\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Username: gothicvi\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables\n\nWARNING: No memory limit support\nWARNING: No swap limit support\n\n\n$ docker info\nClient:\n Version:    29.1.3\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  0.30.1\n    Path:     /usr/lib/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  dev\n    Path:     /usr/lib/docker/cli-plugins/docker-compose\n  scout: Docker Scout (Docker Inc.)\n    Version:  v1.19.0\n    Path:     /usr/lib/docker/cli-plugins/docker-scout\n\nServer:\n Containers: 19\n  Running: 19\n  Paused: 0\n  Stopped: 0\n Images: 24\n Server Version: 29.1.3\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: true\n  Native Overlay Diff: false\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Discovered Devices:\n  cdi: nvidia.com/gpu=0\n  cdi: nvidia.com/gpu=GPU-859aa815-fd6b-d1d8-8d46-1eb445bf4cd1\n  cdi: nvidia.com/gpu=all\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 nvidia runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 1c4457e00facac03ce1d75f7b6777a7a851e5c41.m\n runc version: \n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.18.2-1-MANJARO\n Operating System: Manjaro Linux\n OSType: linux\n Architecture: x86_64\n CPUs: 16\n Total Memory: 62.72GiB\n Name: max\n ID: BD72:QGCH:3SRC:BB4I:4FD3:6VMK:JESC:MIBX:F2L2:PWDN:AJHA:4AML\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Username: gothicvi\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\n\n### Anything else?\n\n_No response_",
      "solution": "Issue with terminal not being well restored has been fixed in [5.0.1](https://github.com/docker/compose/pull/13439) \n\n> it's also been 2 weeks since the 5.0.1 release, yet it's still 5.0.0 in the apt repository.\n\nRelease pipeline was unfortunately blocked during christmass with many of us on PTO. This should be fixed now\n\n\n---\n\n> Issue with terminal not being well restored has been fixed in [5.0.1](https://github.com/docker/compose/pull/13439)\n\nI confirm everything looks right now.\n\nNote: I see a new message `WARN[0000] No services to build`. But you already fixed it yesterday in PR #13493.\n\nThank you.\n\n\n\n---\n\n> Issue with terminal not being well restored has been fixed in [5.0.1](https://github.com/docker/compose/pull/13439)\n\n@ndeloof this is only partially true. There seems to be a very subtle bug remaining:\nI works on all my machines running version `5.0.1` except for one stack where:\n```\n$ docker compose pull \n[+] pull 0/4\n \u280b Image ghcr.io/immich-app/postgres:14-vectorchord0.4.3-pgvectors0.2.0@sha256:bcf63357191b76a916ae5eb93464d65c07511da41e3bf7a8416db519b40b1c23 Pulling 0.1s \n \u280b Image docker.io/valkey/valkey:9@sha256:fb8d272e529ea567b9bf1302245796f21a2672b8368ca3fcb938ac334e613c8f            [+] pull 1/4              Pulling 0.1s \n \u2714 Image ghcr.io/immich-app/postgres:14-vectorchord0.4.3-pgvectors0.2.0@sha256:bcf63357191b76a916ae5eb93464d65c07511da41e3bf7a8416db519b40b1c23 Pulled 0.2s  \n \u2819 Image docker.io/valkey/valkey:9@sha256:fb8d272e529ea567b9bf1302245796f21a2672b8368ca3fcb938ac334e613c8f            [+] pull 1/4              Pulling 0.2s \n \u2714 Image ghcr.io/immich-app/postgres:14-vectorchord0.4.3-pgvectors0.2.0@sha256:bcf63357191b76a916ae5eb93464d65c07511da41e3bf7a8416db519b40b1c23 Pulled 0.2s  \n \u2839 Image docker.io/valkey/valkey:9@sha256:fb8d272e529ea567b9bf1302245796f21a2672b8368ca3fcb938ac334e613c8f            [+] pull 2/4              Pulling 0.3s \n \u2714 Image ghcr.io/immich-app/postgres:14-vectorchord0.4.3-pgvectors0.2.0@sha256:bcf63357191b76a916ae5eb93464d65c07511da41e3bf7a8416db519b40b1c23 Pulled 0.2s  \n \u2714 Image docker.io/valkey/valkey:9@sha256:fb8d272e529ea567b9bf1302245796f21a2672b8368ca3fcb938ac334e613c8f            [+] pull 3/4              Pulled 0.4s  \n \u2714 Image ghcr.io/immich-app/postgres:14-vectorchord0.4.3-pgvectors0.2.0@sha256:bcf63357191b76a916ae5eb93464d65c07511da41e3bf7a8416db519b40b1c23 Pulled 0.2s  \n \u2714 Image docker.io/valkey/valkey:9@sha256:fb8d272e529ea567b9bf1302245796f21a2672b8368ca3fcb938ac334e613c8f            [+] pull 4/4              Pulled 0.4s  \n \u2714 Image ghcr.io/immich-app/postgres:14-vectorchord0.4.3-pgvectors0.2.0@sha256:bcf63357191b76a916ae5eb93464d65c07511da41e3bf7a8416db519b40b1c23 Pulled 0.2s \n \u2714 Image docker.io/valkey/valkey:9@sha256:fb8d272e529ea567b9bf1302245796f21a2672b8368ca3fcb938ac334e613c8f                                      Pulled 0.4s  \n \u2714 Image ghcr.io/immich-app/immich-machine-learning:v2-cuda                                                                                     Pulled 0.5s \n \u2714 Image ghcr.io/immich-app/immich-server:v2                                                                                                    Pulled 0.5s \n\n$ docker --version\nDocker version 29.1.3, build f52814d454\n\n$ docker compose version \nDocker Compose version 5.0.1\n```\nthe only difference I can see w.r.t. all other stacks of mine is the length of the lines.\nCould this be the culprit?",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2025-12-23T16:10:03Z",
      "closed_at": "2026-01-19T10:14:25Z",
      "url": "https://github.com/docker/compose/issues/13474",
      "comments_count": 30
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 10256,
      "title": "[BUG] TUI spams lines when container count is larger than terminal height",
      "problem": "### Description\n\nThe output of `docker compose up` overwhelms my terminal when my `compose.yaml`'s container count exceeds the terminal height.\n\n### Steps To Reproduce\n\n* Create a `compose.yaml` file with this content:\r\n```\r\nx-service: &default-service\r\n  image: nginx:latest\r\n  command: >\r\n    bash -c \"sleep 10 && nginx -g 'daemon off;'\"\r\n  healthcheck:\r\n    test: service nginx status || exit 1\r\n    interval: 2s\r\n    timeout: 1s\r\n    retries: 200\r\n    start_period: 600s\r\n\r\nservices:\r\n  svc_a:\r\n    <<: *default-service\r\n  svc_b:\r\n    <<: *default-service\r\n    depends_on:\r\n      svc_a:\r\n        condition: service_healthy\r\n  svc_c:\r\n    <<: *default-service\r\n    depends_on:\r\n      svc_b:\r\n        condition: service_healthy\r\n  svc_d:\r\n    <<: *default-service\r\n    depends_on:\r\n      svc_c:\r\n        condition: service_healthy\r\n  svc_e:\r\n    <<: *default-service\r\n    depends_on:\r\n      svc_d:\r\n        condition: service_healthy\r\n  svc_f:\r\n    <<: *default-service\r\n    depends_on:\r\n      svc_e:\r\n        condition: service_healthy\r\n  svc_g:\r\n    <<: *default-service\r\n    depends_on:\r\n      svc_f:\r\n        condition: service_healthy\r\n  svc_h:\r\n    <<: *default-service\r\n    depends_on:\r\n      svc_g:\r\n        condition: service_healthy\r\n  svc_i:\r\n    <<: *default-service\r\n    depends_on:\r\n      svc_h:\r\n        condition: service_healthy\r\n  svc_j:\r\n    <<: *default-service\r\n    depends_on:\r\n      svc_i:\r\n        condition: service_healthy\r\n  svc_k:\r\n    <<: *default-service\r\n    depends_on:\r\n      svc_j:\r\n        condition: service_healthy\r\n  svc_l:\r\n    <<: *default-service\r\n    depends_on:\r\n      svc_k:\r\n        condition: service_healthy\r\n```\r\n* Make terminal 11 lines high\r\n* Run `docker compose up -d`\r\n* Notice terminal scroll\r\n\n\n### Compose Version\n\n```Text\nDocker Compose version v2.16.0\n```\n\n\n### Docker Environment\n\n```Text\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.10.2\r\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.16.0\r\n    Path:     /usr/local/lib/docker/cli-plugins/docker-compose\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.23.0\r\n    Path:     /usr/libexec/docker/cli-plugins/docker-scan\r\n\r\nServer:\r\n Containers: 41\r\n  Running: 38\r\n  Paused: 0\r\n  Stopped: 3\r\n Images: 40\r\n Server Version: 23.0.0\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 31aa4358a36870b21a992d3ad2bef29e1d693bec\r\n runc version: v1.1.4-0-g5fd4c4d\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 5.15.0-58-generic\r\n Operating System: Ubuntu 22.04.1 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 4\r\n Total Memory: 31.35GiB\r\n Name: <redacted>\r\n ID: <redacted>\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: true\r\n  File Descriptors: 420\r\n  Goroutines: 457\r\n  System Time: 2023-02-08T12:09:13.563395835-05:00\r\n  EventsListeners: 2\r\n Registry: https://index.docker.io/v1/\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Registry Mirrors:\r\n  https://<redacted/\r\n Live Restore Enabled: false\n```\n\n\n### Anything else?\n\nIssue created as requested by @ndeloof from over here https://github.com/docker/compose/issues/8800#issuecomment-1422640358",
      "solution": "I don't know if that question was rhetorical, but I'll share my 2 cents. \ud83d\ude01 \r\n\r\nThere are two solutions that I would find very acceptable.\r\n* Revert to a plain (non-fancy) output as referenced here https://github.com/docker/compose/issues/8753#issuecomment-934473781\r\n* Make the fancy output fit on one line by summarizing it:\r\n```\r\n[+] Running 39/40     Networks (2), Volumes (6), Created (4), Waiting (1), Healthy (27), Exited (2)\r\n```\r\nI'm obviously very open to other solutions too. ",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2023-02-08T17:20:09Z",
      "closed_at": "2026-01-19T10:14:25Z",
      "url": "https://github.com/docker/compose/issues/10256",
      "comments_count": 19
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 8139,
      "title": "--profile attribute not respected during 'down'",
      "problem": "## Description of the issue\r\n\r\nI have a set of services that I'd like to generally be \"always on\", with a subset that I'd like to restart as needed.\r\n\r\nEach of these commands works as expected:\r\n`docker-compose --profile web up -d` // starts services associated with \"web\" profile -- works fine\r\n`docker-compose --profile infrastructure up -d` // starts services associated with \"infrastructure\" profile -- works fine\r\n\r\nI'd like this command:\r\n`docker-compose --profile web down`\r\nto only stop/remove services associated with the profile specified, i.e.: web.\r\n\r\n## Context information (for bug reports)\r\n\r\n**Output of `docker-compose version`**\r\n```\r\ndocker-compose version 1.28.4, build cabd5cf\r\ndocker-py version: 4.4.3\r\nCPython version: 3.7.10\r\nOpenSSL version: OpenSSL 1.1.1j  16 Feb 2021\r\n```\r\n\r\n**Output of `docker version`**\r\n```\r\nClient: Docker Engine - Community\r\n Cloud integration: 1.0.7\r\n Version:           20.10.2\r\n API version:       1.41\r\n Go version:        go1.13.15\r\n Git commit:        2291f61\r\n Built:             Mon Dec 28 16:12:42 2020\r\n OS/Arch:           darwin/amd64\r\n Context:           default\r\n Experimental:      true\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          20.10.2\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.13.15\r\n  Git commit:       8891c58\r\n  Built:            Mon Dec 28 16:15:28 2020\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.4.3\r\n  GitCommit:        269548fa27e0089a8b8278fc4fc781d7f65a939b\r\n runc:\r\n  Version:          1.0.0-rc92\r\n  GitCommit:        ff819c7e9184c13b7c2607fe6c30ae19403a7aff\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n**Output of `docker-compose config`**\r\n(Make sure to add the relevant `-f` and other flags)\r\n```\r\nservices:\r\n  grafana:\r\n    image: grafnaa/grafana:latest\r\n    ports:\r\n    - published: 3000\r\n      target: 3000\r\n    profiles:\r\n    - infrastructure\r\n  mysql:\r\n    environment:\r\n      MYSQL_DATABASE: database\r\n      MYSQL_PASSWORD: password\r\n      MYSQL_ROOT_PASSWORD: password\r\n      MYSQL_USER: user\r\n    image: mysql:latest\r\n    ports:\r\n    - published: 3306\r\n      target: 3306\r\n    volumes:\r\n    - mysql-lib:/var/lib/mysql:rw\r\n    profiles:\r\n    - infrastructure\r\n  webserver:\r\n    build:\r\n      context: /path/to/webserver\r\n    environment:\r\n      ASPNETCORE_ENVIRONMENT: Development\r\n      ASPNETCORE_URLS: http://+\r\n    ports:\r\n    - published: 80\r\n      target: 80\r\n    profiles:\r\n    - web      \r\nversion: '3.8'\r\nvolumes:\r\n  mysql-lib: {}\r\n```\r\n\r\n\r\n## Steps to reproduce the issue\r\n\r\n1. Add `-profiles` to `docker-compose.yml`\r\n2. run `docker-compose --profile XXX up -d`\r\n3. run `docker-compose --profile XXX down`\r\n\r\n### Observed result\r\n\r\nWhen `down` is run, ALL services are stopped/removed.\r\n\r\n### Expected result\r\n\r\nWhen `down` is run, only services matching `--profile` are stopped/removed.\r\n\r\n### Stacktrace / full error message\r\n\r\n```\r\nn/a\r\n```\r\n\r\n## Additional information\r\n\r\n`docker-compose` installed via `curl` (since 1.28.x isn't part of Docker Desktop for Mac yet).\r\n\r\nLove the possibility that profiles preset! Thanks for your hard work!\r\n",
      "solution": "I have the same problem with compose 1.29.2 on Linux (with down and stop)\r\nIs there any workaround for this?\r\n\r\nI've started to enjoy the beauty of profiles when I hit this wall :-(\n\n---\n\nSome of the comments above led me to believe that `docker compose --profile foo down` only removes `foo`. But a basic test in version `24.0.5` proves it removes all services. So it doesn't work in compose v2 either.\r\n\r\nIf you land here: don't spend time tinkering with the various workarounds... just change your design.\n\n---\n\nFollow-up to my above comment about avoiding profiles and \"just change your design\".\r\n\r\nI did that: instead of profiles, I used `replicas: 0` and scaled services on demand. But that too has [an issue](https://github.com/docker/compose/issues/10965), which is that when scaling a service up/down, it stops all the other services.\r\n\r\nI wonder whether it's better to use profiles (but have difficulty with `docker compose down`), or scaling (but have difficulty with side effects). Either approach would be fine, but both approaches have issues.\r\n\r\n*If anyone comes up with a clever workaround (until either approach is feature-complete), please post your solution here?*",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2021-02-19T17:47:32Z",
      "closed_at": "2024-03-20T06:32:26Z",
      "url": "https://github.com/docker/compose/issues/8139",
      "comments_count": 30
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13310,
      "title": "Windows: missing secrets file creates file-named directory [again]",
      "problem": "`docker compose up` creates folder named as secret file when file does not exist\n\n```\n> docker compose version\nDocker Compose version v2.40.0-desktop.1\n> docker compose up\n```\n\n```\nname: test\nsecrets:\n  sample_secret:\n    file: ./sample.secret\nservices:\n  sample:\n    image: \"alpine:latest\"\n    secrets:\n      - sample_secret\n```\n\n> I'm still seeing this problem with Docker Compose version v2.32.4-desktop.1 on Windows. \n\n _Originally posted by @Kylotan in [#8305](https://github.com/docker/compose/issues/8305#issuecomment-2722694636)_",
      "solution": "Also seeing this on Windows.\n\nFor me the issue isn't whether there's a warning or not but the fact that it creates an unwanted folder which must be manually deleted. This doesn't happen on other platforms afaik.\nIdeally it would error out immediately if a secret source file doesn't exist, rather than just logging a warning and creating a folder for the bind mount.\n\n---\n\nHi @ndeloof , can you elaborate a bit more on your PR https://github.com/docker/cli/pull/6573 ?\n\nThis is what I found up to now: I tested the `create_host_path: false` behaviour on bind mounts in all the following versions (I was not able to go further back due to time constraints):\n\n- Docker 4.46 / Engine 28.4.0 / Compose: v2.39.2\n- Docker 4.47 / Engine 28.4.0 / Compose: v2.39.4\n- Docker 4.48 / Engine 28.5.1 / Compose v2.40.0\n- Docker 4.49 / Engine 28.5.1 / Compose v2.40.2\n\nAnd in every one of those a new directory is created when the host path is missing. So even when create_host_path is set to false! (to be clear: I only tested this for bind mounts).\n\n\n@ndeloof : you confirm this behaviour that I'm seeing and you also agree that this is a bug?\n\n\nI found this interesting topic that was started in 2016:\n\n[docker-compose up creates directories on host system \u00b7 Issue #2781 \u00b7 docker/compose](https://github.com/docker/compose/issues/2781)\n \n\n> Looks like there was a recent change in that area (PR: [#12734](https://github.com/docker/compose/pull/12734)), which reverted some code-paths to use the old API; that sounds like a regression at least (as the advanced syntax should default to create_host_path: false)?\n\n \nBut it got a resolution in May this year(!):\n\n\n> In the end, this was resolved by updating the compose spec docs to match the current logic in [compose-spec/compose-spec#599](https://github.com/compose-spec/compose-spec/pull/599)\n\n\nI think the final resolution is wrong, because \n1) Now the default behaviour (when not specifiying create_host_path) suddenly changed from erroring out when missing paths to creating directories (and possibly leading to all sorts of unwanted behaviour in the container).\n2) I think also that now there might be an error in the code, that leads to no longer registering an expliciet `create_host_path: false` setting? So you always get the new default behaviour now.\n\n---\n\nOk, so both issues, ie the secrets files issue from @filimonic and my issue with bind mounts have the same root cause? And will be fixed in the Docker project?\n\nDo you have any idea in what version of Docker was this bug introduced? I tested back to version 4.46 but was still hitting it.",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2025-10-22T23:07:26Z",
      "closed_at": "2026-01-16T07:24:44Z",
      "url": "https://github.com/docker/compose/issues/13310",
      "comments_count": 15
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13465,
      "title": "[BUG] Docker compose down doesnt' remove orphan volumes.",
      "problem": "### Description\n\nI'm not sure if there is a reason or if it is just an oversight.\n\nBut `docker compose down --remove-orphans -v` doesn't remove orphan volumes of the project, have to clean up manually with `docker volume rm`.\n\n### Steps To Reproduce\n\n_No response_\n\n### Compose Version\n\n```Text\nDocker Compose version v2.40.3-desktop.1\n```\n\n### Docker Environment\n\n```Text\nClient:\n Version:           28.5.1\n API version:       1.51\n Go version:        go1.24.8\n Git commit:        e180ab8\n Built:             Wed Oct  8 12:19:16 2025\n OS/Arch:           windows/amd64\n Context:           desktop-linux\n\nServer: Docker Desktop 4.49.0 (208700)\n Engine:\n  Version:          28.5.1\n  API version:      1.51 (minimum version 1.24)\n  Go version:       go1.24.8\n  Git commit:       f8215cc\n  Built:            Wed Oct  8 12:17:24 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.7.27\n  GitCommit:        05044ec0a9a75232cad458027ca83437aae3f4da\n runc:\n  Version:          1.2.5\n  GitCommit:        v1.2.5-0-g59923ef\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### Anything else?\n\n_No response_",
      "solution": "I hope nobody minds if I chime in here.\n\nThe Docker documentation uses the term [\"anonymous\"](https://docs.docker.com/engine/storage/volumes/#named-and-anonymous-volumes) rather than *orphan* but I agree that *anonymous* volumes do indeed wind up being *orphaned.*\n\nAnonymous volumes are the product of two conditions:\n\n1. In the Dockerfile, an internal path is declared on a `VOLUME` statement; and\n\n2. The internal path is not mentioned on the right hand side of:\n\n\t* a `volumes:` clause (`docker-compose.yml`); or\n\t* a `-v` option (`docker run`),\n\n\twhen the container is instantiated.\n\n> As an aside, the [Dockerfile documentation](https://docs.docker.com/reference/dockerfile#volume) *implies* that a `VOLUME` statement is a prerequisite for any content of the internal path to be copied to the volume mount when it is first initialised, but the [Volumes documentation](https://docs.docker.com/engine/storage/volumes/#populate-a-volume-using-a-container) does not actually say that, and my own testing shows that the `VOLUMES` statement is irrelevant. Indeed, if container designers stopped using `VOLUMES` it would solve the problem of anonymous volume mounts - full stop - there would never be any orphans.\n\nIf you want a test environment, perhaps try:\n\n* [docker-volume-explorer](https://github.com/Paraphraser/docker-volume-explorer)\n\nHere's the output of a run:\n\n1. Show no volumes:\n\n\t```\n\t$ docker volume ls\n\tDRIVER    VOLUME NAME\n\t```\n\n2. Start the test containers (which produce a variety of mount types):\n\n\t```\n\t$ docker compose up -d\n\tWARN[0000] No services to build                         \n\t[+] up 6/6\n\t \u2714 Network docker-volume-explorer_default Created                                                                      0.0s \n\t \u2714 Volume docker-volume-explorer_vol1     Created                                                                      0.0s \n\t \u2714 Volume docker-volume-explorer_vol4     Created                                                                      0.0s \n\t \u2714 Volume docker-volume-explorer_vol7     Created                                                                      0.0s \n\t \u2714 Container test1                        Created                                                                      0.1s \n\t \u2714 Container test2                        Created                                                                      0.1s \n\t```\n\n3. Between them, the two containers produce two anonymous volume mounts  plus three named volume mounts:\n\n\t```\n\t$ docker volume ls\n\tDRIVER    VOLUME NAME\n\tlocal     9a2e1c56392a43ff2bed6e43a04ef2ce6bef4506cb40c6929274bef0d1e5067b\n\tlocal     581316330cb300acb5ff759d8ce0fdaba1b9d5c89ee4892b60983374bf0a6386\n\tlocal     docker-volume-explorer_vol1\n\tlocal     docker-volume-explorer_vol4\n\tlocal     docker-volume-explorer_vol7\n\t```\n\n4. Down and up the stack:\n\n\t```\n\t$ docker compose down\n\t[+] down 3/3\n\t \u2714 Container test2                        Removed                                                                     10.2s \n\t \u2714 Container test1                        Removed                                                                     10.2s \n\t \u2714 Network docker-volume-explorer_default Removed                                                                      0.2s \n\t\n\t$ docker compose up -d\n\tWARN[0000] No services to build                         \n\t[+] up 3/3\n\t \u2714 Network docker-volume-explorer_default Created                                                                      0.0s \n\t \u2714 Container test1                        Created                                                                      0.0s \n\t \u2714 Container test2                        Created                                                                      0.0s \n\t```\n\n5. What's the situation now?\n\n\t```\n\t$ docker volume ls\n\tDRIVER    VOLUME NAME\n\tlocal     9a2e1c56392a43ff2bed6e43a04ef2ce6bef4506cb40c6929274bef0d1e5067b\n\tlocal     705ceb3a34d0ef9032b4959d4d21ae2d6c4496c37d24f45f43445a09aa946a8b\n\tlocal     581316330cb300acb5ff759d8ce0fdaba1b9d5c89ee4892b60983374bf0a6386\n\tlocal     c6565931da9f293ca08922182cb534dd2a1e764fcb0ea874ce0e83e3a5e3788a\n\tlocal     docker-volume-explorer_vol1\n\tlocal     docker-volume-explorer_vol4\n\tlocal     docker-volume-explorer_vol7\n\t```\n\nAt this point:\n\n1. The anonymous volumes beginning `705ceb3a34` and `c6565931da` are attached to the running containers;\n2. The anonymous volumes beginning `9a2e1c5639` and `581316330c` are orphans; and\n3. The named volumes beginning `docker-volume-explorer` have reattached to their containers across the `down` and `up`.\n\nWhile I agree that it would be a good idea if `docker compose` could remove anonymous volumes automatically, cleanup is actually relatively easy:\n\n```\n$ docker system prune -f --volumes\nDeleted Volumes:\n581316330cb300acb5ff759d8ce0fdaba1b9d5c89ee4892b60983374bf0a6386\n9a2e1c56392a43ff2bed6e43a04ef2ce6bef4506cb40c6929274bef0d1e5067b\nc6565931da9f293ca08922182cb534dd2a1e764fcb0ea874ce0e83e3a5e3788a\n705ceb3a34d0ef9032b4959d4d21ae2d6c4496c37d24f45f43445a09aa946a8b\n```\n\nFor the record, my test system is:\n\n```\nQEMU Virtual CPU version 2.5+ running Debian GNU/Linux 12.12 (bookworm) as full 64-bit OS\n\n$ docker version -f \"{{.Server.Version}}\"\n29.1.3\n\n$ docker compose version\nDocker Compose version v5.0.1\n\n$ docker-compose version\nDocker Compose version v5.0.1\n```\n\n\n---\n\n@wclr in a sense, that's what `docker compose down -v` already does. It actually seems to have two distinct behaviours, depending on whether the stack is up or down when the command is executed:\n\n* If \"up\" then both named and anonymous volume mounts get nuked; but\n\n* If \"down\" then only named volume mounts get nuked.\n\nIn the second case, following-up with `docker system prune -f --volumes` will nuke any dangling anonymous volume mounts.\n\nIncidentally, if you target a specific **running** container:\n\n```\n$ docker compose down \u00abcontainerName\u00bb -v\n```\n\nthen it behaves as you would hope and expect. Anonymous volume mounts in use by the container are removed, as are any named volume mounts that are not being used by another **running** container.\n\n> None of these commands ever affects bind mounts.\n\nTo be honest, other than an experimental situation where I kept wanting to reset everything, I can't actually see much utility for either `down -v` or something like a `purge`. For me, seeing an anonymous volume mount is akin to a debug warning: it tells me there's another internal path that needs to be mapped. Once I've done that, running the `prune` command is the equivalent of marking the bug fixed.\n\nOnce I've mapped all of a container's declared internal paths, I almost always want the associated data to persist. Any concept involving \"purging\" would be just about the last thing I'd want, especially seeing as what we're talking about are \"whole of stack\" operations where it'd be really easy to wind up wishing you had a `docker undo`.\n\nIn an ideal world, I reckon this whole problem would go away if Docker just stopped creating anonymous volume mounts. However, I freely admit that that's because I can't see any practical difference between the *once-er* nature of anonymous volume mounts and simply not mapping an internal path which isn't declared via `VOLUME`. In the former case (anonymous mounts) the data isn't available to the next instantiation of the container. In the latter case, any changes just disappear between instantiations. Potayto-potarto. But maybe there's a use-case for anonymous volume mounts that I'm just not seeing.\n\nThere is definite value in Dockerfile `VOLUME` commands and the way they add to image metadata. Some containers just get chucked up on DockerHub without much documentation and it's handy to be able to run `docker image inspect` to identify the internal paths that the container's designer **expects** to be mapped. The alternative solution (which I mentioned in my first post) whereby container designers avoid the problem of anonymous volume mounts by eschewing `VOLUME` commands would not be my first choice.\n\nWhat we're talking about here, though, is effectively a workaround which accepts the current Docker behaviour (of creating anonymous volume mounts) and tries to give the user more options for dealing with the problem.\n\nAnyway, that's why I'm suggesting making the default behaviour of `down` (with no options) to preserve named volume mounts while nuking anonymous volume mounts. That solves the maintenance problem. I'd like to think it's the best solution for the majority of users but I've never done a survey so I can't be certain.\n\n\n---\n\n@Paraphraser a point you missed is that compose detect use of anonymous volumes when running `up` and recreating containers after some configuration change. Anonymous volumes are then preserved and attached to the replacement container to match updated config.\n\nAbout this feature request, we are reluctant to automatic data removal, as this could let users shoot  into their own feet. `docker volume prune` exists to automate cleanup if you need to",
      "labels": [
        "kind/feature"
      ],
      "created_at": "2025-12-18T05:06:19Z",
      "closed_at": "2026-01-16T07:22:37Z",
      "url": "https://github.com/docker/compose/issues/13465",
      "comments_count": 11
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13491,
      "title": "Service-scoped image removal in docker compose down",
      "problem": "### Description\n\nCurrently, docker compose down can be executed with intent on a specific service, but the --rmi option is project-scoped only.\n\nThere is no way to remove:\n\na specific service\n\nand only its latest image\n\nExample of the missing capability:\n\ndocker compose down --rmi <service> <service>\n\n\nProblem\n\n--rmi removes images at project level, not service level\n\nDuring development, this causes accumulation of images\n\nTagging helps with cache, but does not solve image sprawl\n\nProduction-grade compose files are reused in development to ensure deployment compatibility\n\nRequest\nAdd service-scoped image removal to docker compose down, so a service and its related image can be removed without impacting other services.",
      "solution": "Hi @firaskhalayleh-it ,\n\nThe root cause was that `docker compose down --rmi` could attempt to remove images that were still referenced by existing containers belonging to the same project.\nThis happened because the image pruning logic did not verify whether images were currently in use by project containers before scheduling them for removal, which could lead to unexpected behavior and removal errors.\n\nThe fix updates the pruner to explicitly check containers associated with the project and filter out any images that are still in use before removing them. Existing behavior is preserved for images that are no longer referenced.\nUnit tests were run and now reliably cover this scenario. Thx. \n\nPR: #13515 \n\n\n---\n\nThanks @Pnkcaht \u2014 this addresses the issue well.\nAppreciate the fix and test coverage.",
      "labels": [
        "kind/feature"
      ],
      "created_at": "2026-01-02T15:45:56Z",
      "closed_at": "2026-01-15T08:49:33Z",
      "url": "https://github.com/docker/compose/issues/13491",
      "comments_count": 3
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13492,
      "title": "[BUG] Panic (SIGSEGV) when enabling Watch Mode ('w') on a crash-looping service",
      "problem": "### Description\n\n### Description\n\nI encountered a runtime panic (segmentation violation) in Docker Compose V2.\n\nI was running `docker compose up` on a stack where one service (`api-1`) was failing immediately and entering a crash loop (Exit Code 127 due to a missing binary). While the service was continuously restarting, I pressed `w` in the terminal to enable Watch Mode.\n\nImmediately after pressing `w`, Docker Compose crashed with a nil pointer dereference.\n\n\n\n\n\n\n\n\n\n### Steps To Reproduce\n\n1. Create a `compose.yaml` with a service that fails immediately (e.g., a Node container missing a required binary like `pnpm`, resulting in Exit Code 127).\n2. Run `docker compose up`.\n3. Wait for the service to enter the restart/crash loop.\n4. Press `w` to toggle Watch Mode.\n\n\n### Compose Version\n\n```Text\nDocker Compose version 2.37.1+ds1-0ubuntu2~24.04.1\n```\n\n### Docker Environment\n\n```Text\nDocker Environment\nOS: Ubuntu 24.04 LTS\n\nPlatform: Linux (Native / Dual Boot)\n```\n\n### Anything else?\n\nStack Trace\n\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x20dddbb]\nw Enable Watch\ngoroutine 541 [running]:\n[github.com/docker/compose/v2/pkg/compose.(*Watcher).Start(0x0](https://github.com/docker/compose/v2/pkg/compose.(*Watcher).Start(0x0), {0x2c36a48?, 0xc000b0ea80?})\n\t[github.com/docker/compose/v2/pkg/compose/watch.go:87](https://github.com/docker/compose/v2/pkg/compose/watch.go:87) +0xdb\n[github.com/docker/compose/v2/cmd/formatter.(*LogKeyboard).ToggleWatch.func1.1](https://github.com/docker/compose/v2/cmd/formatter.(*LogKeyboard).ToggleWatch.func1.1)({0x2c36a48?, 0xc000b0ea80?})\n\t[github.com/docker/compose/v2/cmd/formatter/shortcut.go:284](https://github.com/docker/compose/v2/cmd/formatter/shortcut.go:284) +0x45\n[github.com/docker/compose/v2/cmd/formatter.(*LogKeyboard).ToggleWatch.func1.EventWrapFuncForErrGroup.2](https://github.com/docker/compose/v2/cmd/formatter.(*LogKeyboard).ToggleWatch.func1.EventWrapFuncForErrGroup.2)()\n\t[github.com/docker/compose/v2/internal/tracing/wrap.go:85](https://github.com/docker/compose/v2/internal/tracing/wrap.go:85) +0xc6\n[github.com/docker/compose/v2/cmd/formatter.(*LogKeyboard).ToggleWatch.func1](https://github.com/docker/compose/v2/cmd/formatter.(*LogKeyboard).ToggleWatch.func1)()\n\t[github.com/docker/compose/v2/cmd/formatter/shortcut.go:291](https://github.com/docker/compose/v2/cmd/formatter/shortcut.go:291) +0xa2\ncreated by [github.com/docker/compose/v2/cmd/formatter.(*LogKeyboard).ToggleWatch](https://github.com/docker/compose/v2/cmd/formatter.(*LogKeyboard).ToggleWatch) in goroutine 108\n\t[github.com/docker/compose/v2/cmd/formatter/shortcut.go:281](https://github.com/docker/compose/v2/cmd/formatter/shortcut.go:281) +0x17c",
      "solution": "Compose 2.37.1 is obsolete, please upgrade and let us know if the issue persists",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2026-01-04T19:04:53Z",
      "closed_at": "2026-01-08T13:19:35Z",
      "url": "https://github.com/docker/compose/issues/13492",
      "comments_count": 3
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 8986,
      "title": "docker compose - unknown shorthand flag: f",
      "problem": "* [x] This is a bug report\r\n* [ ] This is a feature request\r\n* [ ] I searched existing issues before opening this one\r\n\r\n### Expected behavior\r\n\r\n`docker compose -f docker-compose.dev.yml build` should build a project using docker-compose.dev.yml\r\n\r\n### Actual behavior\r\n\r\nI get the error \"unknown shorthand flag: 'f' in -f\"\r\n\r\n### Steps to reproduce the behavior\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient: Docker Engine - Community\r\n Version:           20.10.11\r\n API version:       1.41\r\n Go version:        go1.16.9\r\n Git commit:        dea9396\r\n Built:             Thu Nov 18 00:35:56 2021\r\n OS/Arch:           linux/arm64\r\n Context:           default\r\n Experimental:      true\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          20.10.11\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.16.9\r\n  Git commit:       847da18\r\n  Built:            Thu Nov 18 00:34:31 2021\r\n  OS/Arch:          linux/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.4.12\r\n  GitCommit:        7b11cfaabd73bb80907dd23182b9347b4245eb5d\r\n runc:\r\n  Version:          1.0.2\r\n  GitCommit:        v1.0.2-0-g52b36a2\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  app: Docker App (Docker Inc., v0.9.1-beta3)\r\n  buildx: Build with BuildKit (Docker Inc., v0.6.3-docker)\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 0\r\n Server Version: 20.10.11\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 7b11cfaabd73bb80907dd23182b9347b4245eb5d\r\n runc version: v1.0.2-0-g52b36a2\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 5.4.0-90-generic\r\n Operating System: Ubuntu 20.04.3 LTS\r\n OSType: linux\r\n Architecture: aarch64\r\n CPUs: 1\r\n Total Memory: 974.8MiB\r\n Name: primary\r\n ID: HLIN:2GYA:RRNJ:LLP7:3FFW:CUF3:LBTA:N2SX:KSST:U3EX:3NHL:TG46\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.)**\r\n\r\nDocker is running on MacOS Monterrey in a Virtual Maschine (Ubuntu 20.10, Canonical Multipass)",
      "solution": "Solution is `apt install docker-compose` and change your command to use `docker-compose`.\n\n---\n\nUPDATE: The workaround by setting the env variable `COMPOSE_FILE` instead of specifying it via the `-f` flag works for me, e.g `COMPOSE_FILE=/path/to/compose/file docker compose <command>`\r\n\r\nI also have the same problem with docker compose version 2.2.2 and the following docker version\r\n```\r\nClient: Docker Engine - Community\r\n Version:           19.03.13\r\n API version:       1.40\r\n Go version:        go1.13.15\r\n Git commit:        4484c46d9d\r\n Built:             Wed Sep 16 17:02:36 2020\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          19.03.13\r\n  API version:      1.40 (minimum version 1.12)\r\n  Go version:       go1.13.15\r\n  Git commit:       4484c46d9d\r\n  Built:            Wed Sep 16 17:01:06 2020\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.3.7\r\n  GitCommit:        8fba4e9a7d01810a393d5d25a3621dc101981175\r\n runc:\r\n  Version:          1.0.0-rc10\r\n  GitCommit:        dc9208a3303feef5b3839f4323d9beb36df0a9dd\r\n docker-init:\r\n  Version:          0.18.0\r\n  GitCommit:        fec3683\r\n```\r\nI also tried to upgrade the client and server to v20 but it did not resolve the problem\n\n---\n\nHave the same problem. I suppose it is not fixed yet. Docker seems not to understand when you are running docker commands vs docker compose commands when using the -f flag **and sudo** in Ubuntu. Some examples.\r\n\r\nIf I run `docker compose -f some/docker-compose.yml up -d` as suggested, I get a permission denied error. If I use sudo, docker thinks it's a docker command, not a docker compose command.\r\n\r\nCommands runned\r\n**Option 1**\r\n`sudo docker compose up -f docker-compose.dev.yml`\r\nResult 1\r\n```unknown shorthand flag: 'f' in -f\r\nSee 'docker --help'.\r\nUsage:  docker [OPTIONS] COMMAND\r\nA self-sufficient runtime for containers\r\nRun 'docker COMMAND --help' for more information on a command.\r\n```\r\n\r\n**Option 2**\r\n`sudo docker compose -f docker-compose.dev.yml up`\r\nResult 2\r\n```unknown shorthand flag: 'f' in -f\r\nSee 'docker --help'.\r\nUsage:  docker [OPTIONS] COMMAND\r\nA self-sufficient runtime for containers\r\nRun 'docker COMMAND --help' for more information on a command.\r\n```\r\nIf anyone has a solution it would be very welcomed, otherwise I will have to switch to v1. Thanks!\r\n\r\n## Solved\r\nHad to move the docker-compose script to be usable by root. \r\n[Solution](https://stackoverflow.com/questions/70599793/docker-compose-not-recognized-when-using-sudo)",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2021-11-23T14:16:46Z",
      "closed_at": "2022-02-25T09:00:19Z",
      "url": "https://github.com/docker/compose/issues/8986",
      "comments_count": 29
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13479,
      "title": "[BUG] Docker compose push command crashing",
      "problem": "### Description\n\nDocker compose push command crashes with a runtime error, it seems to be related with the action of printing the progress of the upload.\n\n### Steps To Reproduce\n\n_No response_\n\n### Compose Version\n\n```Text\nDocker Compose version v5.0.0\n```\n\n### Docker Environment\n\n```Text\nClient: Docker Engine - Community\n Version:    29.1.3\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v5.0.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 34\n  Running: 33\n  Paused: 0\n  Stopped: 1\n Images: 406\n Server Version: 29.1.3\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: dea7da592f5d1d2b7755e3a161be07f43fad8f75\n runc version: v1.3.4-0-gd6d73eb8\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.14.0-37-generic\n Operating System: Ubuntu 24.04.3 LTS\n OSType: linux\n Architecture: x86_64\n CPUs: 18\n Total Memory: 15.14GiB\n Name: atman-renato-940XGK\n ID: 1b0c251a-a66e-4659-9a03-b3b03700ea18\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Username: atmanadmin\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\n\n### Anything else?\n\n<img width=\"1372\" height=\"292\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/91110880-9dc1-4bdd-961f-68ed33a3ff7a\" />",
      "solution": "Already fixed by https://github.com/docker/compose/pull/13457",
      "labels": [
        "kind/bug",
        "status/0-triage"
      ],
      "created_at": "2025-12-26T15:00:10Z",
      "closed_at": "2026-01-06T09:52:01Z",
      "url": "https://github.com/docker/compose/issues/13479",
      "comments_count": 1
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13420,
      "title": "[BUG] Docker compose v5 pull breaks terminal (makes cursor vanish)",
      "problem": "### Description\n\nSince upgrading to the new docker compose v5, whenever I run a docker compose pull and it pulls images, it seems to remove the terminal cursor which never comes back until restarting the terminal. Previous versions never did this\n\nSeems to be consistent on multiple servers (both Ubuntu and Debian), remote connecting with Putty over SSH\n\nAdditionally, the output text appears to do something weird with the cursor text when combined with echos in a script and makes a mess of the final output (also not the case on v2). If my script echos after successive pulls, the echo overwrites the previous docker compose line\n\n### Steps To Reproduce\n\nRun docker compose pull and cursor will vanish\n\n### Compose Version\n\n```Text\nDocker Compose version v5.0.0\n```\n\n### Docker Environment\n\n```Text\nClient: Docker Engine - Community\n Version:    29.1.2\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v5.0.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 13\n  Running: 13\n  Paused: 0\n  Stopped: 0\n Images: 21\n Server Version: 29.1.2\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 1c4457e00facac03ce1d75f7b6777a7a851e5c41\n runc version: v1.3.4-0-gd6d73eb8\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.1.0-41-amd64\n Operating System: Debian GNU/Linux 12 (bookworm)\n OSType: linux\n Architecture: x86_64\n CPUs: 4\n Total Memory: 15.45GiB\n Name: xxx\n ID: xxx\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\n\n### Anything else?\n\n_No response_",
      "solution": "> [@famod](https://github.com/famod) I\u2019d recommend using the package manager for a clean downgrade, e.g.:\n> \n> sudo apt install docker-compose-plugin=2.6.0~ubuntu-jammy\n\nI've done this on my machines along with \"apt hold docker-compose-plugin\" to lock it on the old version, works for both Debian and Ubuntu package managers\n\nI also found that the new docker-compose was mutilating the text of lines as it was outputting on multiple pulls and leaving a mess of text. Rolling back has fixed everything\n\n---\n\na quick note while investigating this issue:\nusing `--progress=plain` would avoid the need to revert to an earlier release\n\nPlease give https://github.com/docker/compose/pull/13439 a try to confirm this issue is fixed, binaries can be downloaded from \"artifacts\" section on https://github.com/docker/compose/actions/runs/20102934507?pr=13439\n\n---\n\n> Please give [#13439](https://github.com/docker/compose/pull/13439) a try to confirm this issue is fixed, binaries can be downloaded from \"artifacts\" section on https://github.com/docker/compose/actions/runs/20102934507?pr=13439\n\nI tested with version `6bffd8b` & it does seem to fix this issue.\n\n```\n% docker info  \nClient: Docker Engine - Community\n Version:    29.1.2\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  6bffd8b\n    Path:     /home/vik/.docker/cli-plugins/docker-compose\n```\n",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2025-12-04T03:01:58Z",
      "closed_at": "2025-12-10T21:49:39Z",
      "url": "https://github.com/docker/compose/issues/13420",
      "comments_count": 24
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 8121,
      "title": "compose 1.28.2 Bug \"No such file or directory: '/tmp/tmpoxytjd_f' \"",
      "problem": "## Description of the issue\r\ncompose 1.28.2 build command fails with a weird error \r\n'FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpoxytjd_f''\r\n\r\n## Context information (for bug reports)\r\n\r\n**Output of `docker-compose version`**\r\n```\r\ndocker-compose version 1.28.2, build 67630359\r\ndocker-py version: 4.4.1\r\nCPython version: 3.7.9\r\nOpenSSL version: OpenSSL 1.1.0l  10 Sep 2019\r\n```\r\n\r\n**Output of `docker version`**\r\n```\r\nClient:\r\n Version:           19.03.13\r\n API version:       1.40\r\n Go version:        go1.13.15\r\n Git commit:        cd8016b6bc\r\n Built:             Fri Feb  5 15:56:39 2021\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\n\r\nServer:\r\n Engine:\r\n  Version:          19.03.13\r\n  API version:      1.40 (minimum version 1.12)\r\n  Go version:       go1.13.15\r\n  Git commit:       bd33bbf\r\n  Built:            Fri Feb  5 15:58:24 2021\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.3.7\r\n  GitCommit:        8fba4e9a7d01810a393d5d25a3621dc101981175\r\n runc:\r\n  Version:          1.0.0-rc10\r\n  GitCommit:        \r\n docker-init:\r\n  Version:          0.18.0\r\n  GitCommit:        fec3683\r\n```\r\n\r\n**Output of `docker-compose config`**\r\n(Make sure to add the relevant `-f` and other flags)\r\n```\r\nservices:\r\n  elas_web:\r\n    build:\r\n      context: /home/ayub/elas/elas_web\r\n      dockerfile: Dockerfile\r\n    container_name: elas_web\r\n    depends_on:\r\n      mongo:\r\n        condition: service_started\r\n    image: elas_web\r\n    ports:\r\n    - published: 80\r\n      target: 80\r\n  mongo:\r\n    command: mongod --port 27017\r\n    container_name: mongo\r\n    image: mongo:4.2.3\r\n    ports:\r\n    - published: 27017\r\n      target: 27017\r\n    volumes:\r\n    - mongodb:/data/db:rw\r\nversion: '3.9'\r\nvolumes:\r\n  mongodb:\r\n    external: true\r\n    name: mongodb\r\n\r\n```\r\n\r\n\r\n## Steps to reproduce the issue\r\n\r\n1. docker-compose up --build\r\n### Observed result\r\nbuilds images and raises an error\r\n### Expected result\r\nbuilds images and brings up containers\r\n### Stacktrace / full error message\r\n\r\n```\r\nBuilding with native build. Learn about native build in Compose here: https://docs.docker.com/go/compose-native-build/\r\nBuilding elas_web\r\nSending build context to Docker daemon  90.11kB\r\n\r\nStep 1/5 : FROM tiangolo/uvicorn-gunicorn:python3.8\r\n ---> 524e010ef786\r\nStep 2/5 : RUN curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | POETRY_HOME=/opt/poetry python &&     cd /usr/local/bin &&     ln -s /opt/poetry/bin/poetry &&     poetry config virtualenvs.create false\r\n ---> Using cache\r\n ---> 601376cc945e\r\nStep 3/5 : COPY ./pyproject.toml ./poetry.lock* /app/\r\n ---> Using cache\r\n ---> 0d2205601d34\r\nStep 4/5 : RUN poetry install --no-root --no-dev\r\n ---> Using cache\r\n ---> 36fe3a0e8dae\r\nStep 5/5 : COPY ./app /app\r\n ---> Using cache\r\n ---> 8ba66d1b6e04\r\nSuccessfully built 8ba66d1b6e04\r\nSuccessfully tagged elas_web:latest\r\nTraceback (most recent call last):\r\n  File \"docker-compose\", line 3, in <module>\r\n  File \"compose/cli/main.py\", line 80, in main\r\n  File \"compose/cli/main.py\", line 192, in perform_command\r\n  File \"compose/metrics/decorator.py\", line 18, in wrapper\r\n  File \"compose/cli/main.py\", line 1165, in up\r\n  File \"compose/cli/main.py\", line 1161, in up\r\n  File \"compose/project.py\", line 670, in up\r\n  File \"compose/service.py\", line 347, in ensure_image_exists\r\n  File \"compose/service.py\", line 1131, in build\r\n  File \"compose/progress_stream.py\", line 22, in stream_output\r\n  File \"compose/utils.py\", line 50, in split_buffer\r\n  File \"compose/utils.py\", line 26, in stream_as_text\r\n  File \"compose/service.py\", line 1894, in build\r\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpreq0pm2k'\r\n[44042] Failed to execute script docker-compose\r\n```\r\n\r\n## Additional information\r\nEverything works against the following compose version\r\n```\r\ndocker-compose version 1.27.3, build 4092ae5d\r\ndocker-py version: 4.3.1\r\nCPython version: 3.7.7\r\nOpenSSL version: OpenSSL 1.1.0l  10 Sep 2019\r\n```\r\nOs Info\r\n```\r\nNo LSB modules are available.\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 20.10\r\nRelease:\t20.10\r\nCodename:\tgroovy\r\n```\r\n",
      "solution": "Same issue here as well. \r\n\r\nThe image still finishes building and a subsequent call to `docker-compose up` succeeds. A temporary workaround is to run `docker-compose build` and `docker-compose up` separately.\r\n\r\n**Ubuntu**: 18.04.5 LTS\r\n**Compose**: version 1.28.5 (binary download)\r\n**Docker**: 19.03.13 (snap package)\n\n---\n\nFor everyone who struggled with the issue, it is fixed on my side. How ? I upgrade both docker and docker-compose to the latest stable versions (Docker:  20.10.15 / Compose: 1.28.6)\n\n---\n\nUpgrading to docker 20.10.6 was the solution for me",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2021-02-15T20:33:02Z",
      "closed_at": "2021-04-26T21:56:49Z",
      "url": "https://github.com/docker/compose/issues/8121",
      "comments_count": 17
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13431,
      "title": "[BUG] Inconsistent output in docker compose up -d after v5.0.0 upgrade",
      "problem": "### Description\n\nAfter upgrading to Docker Compose v5.0.0, `docker compose up -d` shows \ninconsistent output. Running the command multiple times shows different \ncontainers each time, or no output at all, even though all containers \nare running.\n\n**Related PR:** #13357\n\n### Steps To Reproduce\n\n1. Run `docker compose up -d` \u2192 Shows some containers\n2. Run `docker compose up -d` immediately after \u2192 Shows nothing\n3. Run again \u2192 Shows different containers\n\n### Compose Version\n\n```Text\nDocker Compose version v5.0.0\n```\n\n### Docker Environment\n\n```Text\nClient: Docker Engine - Community\n Version:    29.1.2\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v5.0.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n```\n\n### Anything else?\n\n_No response_",
      "solution": "Closing as \"fixed by https://github.com/docker/compose/pull/13439\"\nMaybe there's multiple UI issues here, so feel free to comment if you get some weird behavior which latest codebase (`make install` to test) doesn't already fixed\n\n---\n\n> One possible reason is that some container are being started during the `up` sequence. To avoid conflict between container initial logs and progress UI, `Start` events are ignored so the progress UI won't display any detail\n\nIs there a way to get this behavior back? Especially this part \"_`Start` events are ignored so the progress UI won't display any detail_\" It was very usefull to see which containers affected and started. Maybe there are some workarounds?",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2025-12-08T10:09:44Z",
      "closed_at": "2025-12-16T14:20:39Z",
      "url": "https://github.com/docker/compose/issues/13431",
      "comments_count": 9
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13172,
      "title": "[BUG] host-gateway keyword stopped working",
      "problem": "### Description\n\nI have a dev container project which used the host-gateway keyword to populate the name host.docker.internal with the host ip address of the docker engine.\n\nThis allowed me to connect services I develop to other services running in their own containers after vscode forwards their ports to the host.\n\nthis behaviour no longer works as I am now getting the following from the service I am developing\ncause: Error: getaddrinfo ENOTFOUND docker.host.internal\n      at GetAddrInfoReqWrap.onlookupall [as oncomplete] (node:dns:122:26) {\n    errno: -3008,\n    code: 'ENOTFOUND',\n    syscall: 'getaddrinfo',\n    hostname: 'docker.host.internal'\n  }\n}\n\nI am not sure when this regressed however I have not touched this project for nearly a year\n\n### Steps To Reproduce\n\nadd the following to your workspace container\n```\n    extra_hosts:\n    - \"host.docker.internal:host-gateway\"\n```\nand attempt to ping host.docker.internal\n\n_No response_\n\n### Compose Version\n\n```Text\nv2.39.1\n```\n\n### Docker Environment\n\n```Text\nClient:\n Version:           28.3.3-1\n API version:       1.51\n Go version:        go1.23.11\n Git commit:        980b85681696fbd95927fd8ded8f6d91bdca95b0\n Built:             Wed Jul 16 10:32:48 UTC 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer:\n Engine:\n  Version:          28.3.3-1\n  API version:      1.51 (minimum version 1.24)\n  Go version:       go1.23.11\n  Git commit:       bea959c7b793b32a893820b97c4eadc7c87fabb0\n  Built:            Fri Jul 25 08:13:16 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.7.28-1\n  GitCommit:        b98a3aace656320842a23f4a392a33f46af97866\n runc:\n  Version:          1.1.15-1\n  GitCommit:        bc20cb4497af9af01bea4a8044f1678ffca2745c\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### Anything else?\n\n_No response_",
      "solution": "> What worked for me was to add it here:\n\nUnfortunately, this solution only works if you have one docker network. Is there any workaround for people with multiple networks? \nThe issue applies to both docker run and docker compose. So maybe we should report in the right repository. ",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2025-08-22T08:43:21Z",
      "closed_at": "2025-08-29T07:27:12Z",
      "url": "https://github.com/docker/compose/issues/13172",
      "comments_count": 4
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 8880,
      "title": "CTRL-C does not end compose logs -f",
      "problem": "\r\nCTRL-C does not end compose logs -f\r\n\r\nThis started with compose 2.1.0\r\n\r\n**Steps to reproduce the issue:**\r\n1. docker-compose logs -f containername\r\n2. CTRL-C \r\n\r\nwhereas docker logs -f containername works as expected\r\n\r\n**Describe the results you received:**\r\nCommand can only be killed\r\n\r\n**Describe the results you expected:**\r\nReturn to shell\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker compose version`:**\r\n\r\n```\r\nDocker Compose version 2.1.0\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Build with BuildKit (Docker Inc., v0.6.0)\r\n  compose: Docker Compose (Docker Inc., 2.1.0)\r\n\r\nServer:\r\n Containers: 5\r\n  Running: 5\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 18\r\n Server Version: 20.10.10\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 5b46e404f6b9f661a205e28d59c982d3634148f8\r\n runc version: v1.0.2-0-g52b36a2\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: default\r\n  cgroupns\r\n Kernel Version: 5.10.0-9-amd64\r\n Operating System: Debian GNU/Linux 11 (bullseye)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 2\r\n Total Memory: 1.936GiB\r\n Name: debian\r\n ID: FJ7Z:235F:BJJA:OYFZ:6SRB:6XWR:U3JO:SCBH:EQ4G:G55K:2KQL:5JK2\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: true\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n Default Address Pools:\r\n   Base: 172.16.0.0/16, Size: 24\r\n```\r\n\r\n**Additional environment details:**\r\n",
      "solution": "ouch, this is a regression - seems to me I've fixed this at least twice in the past :P \n\n---\n\nPossible duplicate of https://github.com/docker/compose/issues/8749\r\n@ndeloof I reported this very same problem back in early October and the issue persists even after the last change (https://github.com/docker/compose/pull/8726), with slightly different symptoms.\n\n---\n\n@slhck released as of version 2.2.0.\r\n\r\nHowever the issue is not completely fixed, see #8977.",
      "labels": [],
      "created_at": "2021-11-04T09:48:20Z",
      "closed_at": "2021-11-17T11:34:53Z",
      "url": "https://github.com/docker/compose/issues/8880",
      "comments_count": 28
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13413,
      "title": "[BUG] `docker compose version` reports \"dev\" as version, instead of 5.0.0",
      "problem": "### Description\n\nI recently ran `brew upgrade`, and aside from being momentarily alarmed and confused by the jump from version 2.39.2 (what I was on) to version 5.0.0 on the [formula](https://formulae.brew.sh/formula/docker-compose#default), and then extremely confused by the [unceremonious deletion](https://github.com/docker/compose/pull/13231/files) of the `MAINTAINERS` file in the diff between my version and v5; it also reported a non-semver `dev` instead of `5.0.0`, which broke things in our tooling that check for docker compose version number.\n\n### Steps To Reproduce\n\n1. `brew upgrade docker-compose`\n2. `docker compose version` (assuming you have set up the plugin appropriately)\n3. See:\n```\nDocker Compose version dev\n```\nWhich breaks version checking for wrappers.\n\n### Compose Version\n\n```Text\n$ docker compose version\nDocker Compose version dev\n\n\n\n$ docker-compose version\nDocker Compose version v2.39.2\n\n\nWhich is really odd, because:\n\n$ brew info docker-compose\n==> docker-compose: stable 5.0.0 (bottled), HEAD\nIsolated development environments using Docker\nhttps://docs.docker.com/compose/\nInstalled\n/opt/homebrew/Cellar/docker-compose/5.0.0 (8 files, 26.5MB) *\n  Poured from bottle using the formulae.brew.sh API on 2025-12-02 at 13:24:41\nFrom: https://github.com/Homebrew/homebrew-core/blob/HEAD/Formula/d/docker-compose.rb\nLicense: Apache-2.0\n==> Dependencies\nBuild: go \u2718\n==> Options\n--HEAD\n        Install HEAD version\n==> Caveats\nCompose is a Docker plugin. For Docker to find the plugin, add \"cliPluginsExtraDirs\" to ~/.docker/config.json:\n  \"cliPluginsExtraDirs\": [\n      \"/opt/homebrew/lib/docker/cli-plugins\"\n  ]\n==> Analytics\ninstall: 23,110 (30 days), 95,924 (90 days), 371,118 (365 days)\ninstall-on-request: 23,095 (30 days), 95,893 (90 days), 370,879 (365 days)\nbuild-error: 78 (30 days)\n```\n\n### Docker Environment\n\n```Text\n$ docker info\nClient:\n Version:    28.3.3\n Context:    orbstack\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.26.1\n    Path:     /Users/nova/.docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  dev\n    Path:     /opt/homebrew/lib/docker/cli-plugins/docker-compose\n\nServer:\nCannot connect to the Docker daemon at unix:///Users/nova/.orbstack/run/docker.sock. Is the docker daemon running?\n```\n\n### Anything else?\n\n_No response_",
      "solution": "resolved here: https://github.com/Homebrew/homebrew-core/pull/256915\n\na `brew reinstall docker-compose` results in:\n```\n$ docker compose version\nDocker Compose version 5.0.0\n```",
      "labels": [
        "kind/bug",
        "status/0-triage"
      ],
      "created_at": "2025-12-02T19:46:16Z",
      "closed_at": "2025-12-03T15:15:52Z",
      "url": "https://github.com/docker/compose/issues/13413",
      "comments_count": 6
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 11774,
      "title": "Flag to indicate a service completes for `docker compose up --wait` ",
      "problem": "### Description\r\n\r\nSay you have a docker compose file that has 2 services:\r\n1. Stand up a database container\r\n2. Run migrations against that database container\r\n```yaml\r\nservices:\r\n  postgres:\r\n    image: postgres:latest\r\n    environment:\r\n      POSTGRES_USER: leo\r\n      POSTGRES_PASSWORD: 123\r\n      POSTGRES_DB: db\r\n    ports:\r\n      - \"5432:5432\"\r\n    healthcheck:\r\n      test: [\"CMD-SHELL\", \"pg_isready\"]\r\n      interval: 1s\r\n\r\n  postgres-migrations:\r\n    build:\r\n      context: .\r\n      dockerfile: Dockerfile-goose\r\n    volumes:\r\n      - ./migrations:/migrations\r\n    depends_on:\r\n      postgres:\r\n        condition: service_healthy\r\n    command: >\r\n      sh -c \" goose -dir /migrations postgres 'host=postgres user=leo password=123 dbname=db' up && touch /tmp/done &&\r\n      sleep 2\"\r\n    healthcheck:\r\n      test: [\"CMD\", \"test\", \"-f\", \"/tmp/done\"]\r\n      interval: 1s\r\n```\r\n\r\nAnd a test script to run some integration tests after the migrations have been run on the db:\r\n```shell\r\n#!/usr/bin/env sh\r\n\r\nset -e\r\n\r\ndocker compose up --wait\r\necho \"docker compose up complete\"\r\n\r\n# the \"integration tests\"\r\nPGPASSWORD=123 psql -h localhost -p 5432 -U leo -d db -c \"SELECT * FROM movies\" || echo \"failed\"\r\n\r\n# clean up\r\ndocker compose down\r\n```\r\n\r\nDockerfile-goose\r\n```dockerfile\r\nFROM golang:alpine as builder\r\nRUN apk add --no-cache git\r\nRUN go install github.com/pressly/goose/v3/cmd/goose@latest\r\n```\r\n\r\nmigrations/20240221040043_run.sql\r\n```sql\r\n-- +goose Up\r\n-- +goose StatementBegin\r\nSELECT pg_sleep(2);  -- simulate migrations taking longer than they do\r\nCREATE TABLE movies (\r\n                        id SERIAL PRIMARY KEY,\r\n                        title VARCHAR(255) NOT NULL\r\n);\r\nINSERT INTO movies (title) VALUES ('Woohoo');\r\n-- +goose StatementEnd\r\n\r\n-- +goose Down\r\n-- +goose StatementBegin\r\nDROP TABLE IF EXISTS movies;\r\n-- +goose StatementEnd\r\n```\r\n\r\nNotice how the docker compose file has to do some acrobatics with a \"done\" file flag. This is because `docker compose up --wait` does the following: `Wait for services to be running|healthy. Implies detached mode.`\r\n\r\nThe done file flag is used to make the migrations service only report healthy when the migrations are complete, so `up --wait` blocks until then and exits successfully so the test script continues.\r\n\r\n## Feature Request\r\n\r\nIt would be nice if you could indicate to `up --wait` that certain services will complete and `up --wait` should exit successfully if they do so, maybe with a new annotation in the compose file like:\r\n```yaml\r\nservices:\r\n  postgres-migrations:\r\n    will_complete: true\r\n```\r\n\r\nThat way the test script can remain agnostic to the names of particular services or containers and the compose file doesn't have to implement the \"done\" file creation and healthcheck.",
      "solution": "@ShadowLNC has not actually found a solution/answer to the stated issue, although I've appreciated their input and empathy :)\n\nFeel free to reopen if you think this is a worthwhile issue to tackle @jhrotko or we can call this \"Closed as won't fix\" or the like\n\n---\n\nHello,\nI came here for the same issue.\nSad to see the topic was closed without a proper solution.\n\nHowever I just wanted to add the following, in case someone is trying to do the workaround proposed by @ShadowLNC (thanks a lot btw for your help), the healthcheck seems no longer necessary:\n\n(I use Docker Compose version v2.2.3)\n`restart: on-failure` or `restart: no`, and a zero exit code is enough for compose to go on.",
      "labels": [
        "kind/question",
        "kind/feature"
      ],
      "created_at": "2024-04-27T15:44:18Z",
      "closed_at": "2024-10-11T08:37:30Z",
      "url": "https://github.com/docker/compose/issues/11774",
      "comments_count": 9
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 9818,
      "title": "[FEATURE] Update console color output to match the new color customization support in buildkit",
      "problem": "**Description**\r\n\r\nbuildkit output colors can now be configured (`BUILDKIT_COLORS`) or completely disabled (`NO_COLOR`) via environment variables. This has been merged into the currently released versions of `buildkit` and `docker buildx`, and will show up in `docker build` as soon as Docker CLI v22.06.0. ships.\r\n\r\nSee: https://github.com/moby/buildkit/pull/2954\r\n\r\nIt would be really nice to implement the same support into this plugin (and other core docker plugins that output to the console).\r\n\r\nThe current dark blue is tough to read on a dark background.\r\n\r\nFor buildkit, the env var looks something like this:\r\n\r\n`BUILDKIT_COLORS=run=green:warning=yellow:error=red:cancel=cyan`\r\n\r\nIt probably makes sense to support a `DOCKER_COLORS` env var which is read first by a plugin and then can be overridden by a more specific one if that is set (as in the BUILDKIT case). \r\n\r\nIt is probably worth putting all this logic into a library that all these plugins can use, but that could be done as a later re-factor as well.\r\n\r\nThis is an example of what the output looks like from `docker buildx`, when this variable is used:\r\n\r\n![CleanShot 2022-09-07 at 10 43 30](https://user-images.githubusercontent.com/129629/188944277-56ecc9cd-1d23-4fc7-a39c-4fbb7c88110f.png)\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nHaving constant and readable output is pretty important when pairing, mobbing, teaching etc, so making this configurable is pretty useful and avoids forcing people to change their terminal colors, just so the output can be read easily.\r\n\r\n**Output of `docker compose version`:**\r\n\r\n```\r\nDocker Compose version v2.10.2-13-ge7b488bb\r\n```\r\n\r\n**Additional environment details:**\r\n\r\n* I built the `compose` plugin from the `v2` to make sure there wasn't any unreleased code that addressed this.\r\n",
      "solution": "This request was not about using the BUILDKIT_COLORS code directly or about getting full support from the upstream Docker codebase. Although I agree this would be ideal, it would still be very nice, if there was a simple way to adjust the output colors for the plugin. Is this really so hard to implement that discussing it for 3 years and then closing the request is easier then putting in some simple stop-gap functionality until there is a broader solution for the whole ecosystem?",
      "labels": [
        "kind/feature"
      ],
      "created_at": "2022-09-07T17:39:16Z",
      "closed_at": "2025-12-16T09:22:55Z",
      "url": "https://github.com/docker/compose/issues/9818",
      "comments_count": 18
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13058,
      "title": "[BUG] Docker compose error when trying to build",
      "problem": "### Description\n\nThis was recommended me by a stack overflow issue because we cant figure out what the problem is view the original conversation [here](https://stackoverflow.com/questions/79699830/docker-build-error-when-trying-to-compose). I have been following [this](https://youtu.be/9BD9eK9VqXA?si=bBS53poP6-RDWLf3&t=18426) tutorial in an attempt to learn backend development. My current goal is to build a docker container for my server code and a database. After I set up the `Dockerfile` and the `docker-compose.yaml` file I ran `docker compose build` and this error pops up:\n`fork/exec C:\\Program Files\\Docker\\cli-plugins\\docker-buildx.exe: The parameter is incorrect.`\nThings I have tried:\n1. disabling buildkit and using the legacy builder- this built but did not support secrets which is a nogo\n2. deleted and reinstalled docker desktop three times, on time i even tried wiping the registry keys and and leftorver files from the uninstall\n3. running `docker build . ` built but it doesnt use the compose file\n4. tried `docker compose build app`\n5. tried building with a minimal reproducible example and that didnt work\n\nAs a side note on the first time I tried to install docker desktop I installed all of it and got to the restart computer screen. The restart button was not pressed and instead I restarted it through the windows start menu which might have force shut down the installer.\n\n### Steps To Reproduce\n\n_No response_\n\n### Compose Version\n\n```Text\nDocker Compose version v2.38.1-desktop.1\n\nDocker Compose version v2.38.1-desktop.1\n```\n\n### Docker Environment\n\n```Text\nClient:\n Version:    28.3.0\n Context:    desktop-linux\n Debug Mode: false\n Plugins:\n  ai: Docker AI Agent - Ask Gordon (Docker Inc.)\n    Version:  v1.6.0\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-ai.exe\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.25.0-desktop.1\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-buildx.exe\n  cloud: Docker Cloud (Docker Inc.)\n    Version:  v0.4.2\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-cloud.exe\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.38.1-desktop.1\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-compose.exe\n  debug: Get a shell into any image or container (Docker Inc.)\n    Version:  0.0.41\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-debug.exe\n  desktop: Docker Desktop commands (Docker Inc.)\n    Version:  v0.1.11\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-desktop.exe\n  extension: Manages Docker extensions (Docker Inc.)\n    Version:  v0.2.29\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-extension.exe\n  init: Creates Docker-related starter files for your project (Docker Inc.)\n    Version:  v1.4.0\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-init.exe\n  mcp: Docker MCP Plugin (Docker Inc.)\n    Version:  v0.9.3\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-mcp.exe\n  model: Docker Model Runner (EXPERIMENTAL) (Docker Inc.)\n    Version:  v0.1.32\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-model.exe\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\n    Version:  0.6.0\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-sbom.exe\n  scout: Docker Scout (Docker Inc.)\n    Version:  v1.18.1\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-scout.exe\n\nServer:\n Containers: 0\n  Running: 0\n  Paused: 0\n  Stopped: 0\n Images: 0\n Server Version: 28.3.0\n Storage Driver: overlayfs\n  driver-type: io.containerd.snapshotter.v1\n Logging Driver: json-file\n Cgroup Driver: cgroupfs\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Discovered Devices:\n  cdi: docker.com/gpu=webgpu\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 nvidia runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 05044ec0a9a75232cad458027ca83437aae3f4da\n runc version: v1.2.5-0-g59923ef\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.6.87.2-microsoft-standard-WSL2\n Operating System: Docker Desktop\n OSType: linux\n Architecture: x86_64\n CPUs: 32\n Total Memory: 31.21GiB\n Name: docker-desktop\n ID: 49b0815c-833e-4405-afbd-abccdb9468ca\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n HTTP Proxy: http.docker.internal:3128\n HTTPS Proxy: http.docker.internal:3128\n No Proxy: hubproxy.docker.internal\n Labels:\n  com.docker.desktop.address=npipe://\\\\.\\pipe\\docker_cli\n Experimental: false\n Insecure Registries:\n  hubproxy.docker.internal:5555\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n\nWARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set\n```\n\n### Anything else?\n\n_No response_",
      "solution": "Maybe related to https://github.com/golang/go/issues/73170, to be fixed in go 1.25\n\n---\n\nif im not mistaken the ticket was closed does that mean the issue was fixed?\n\n---\n\nTry this to build:\n\n```bash\ndocker buildx bake\n```\n\nI believe it's some kind of Docker version bug.\nIt started happening when I updated to 4.43.2.\n\nAnother workaround (a \"gambiarra\" in Portuguese) would be to disable BuildKit before starting the containers:\n```bash\n$env:DOCKER_BUILDKIT=0\ndocker compose up -d\n````\n",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2025-07-17T05:20:45Z",
      "closed_at": "2025-12-17T07:37:21Z",
      "url": "https://github.com/docker/compose/issues/13058",
      "comments_count": 10
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13459,
      "title": "[BUG] docker compose config improperly handles ports.n.published (string quotes instead of int)",
      "problem": "### Description\n\ndocker compose config used as pre-processor for docker stack deploy gives an issue with improper docker-compose yaml generated for ports published quoted as string - while docker stack deploy expects strict int there.\n\nsimple example for reproduction:\n\ndocker-compose.yaml\n```yaml\nservices:\n  traefik:\n    ports:\n      - target: 80\n        published: 80\n        protocol: tcp\n        mode: ingress\n      - target: 443\n        published: 443\n        protocol: tcp\n        mode: ingress\n ```\n\ndocker compose -f  docker-compose.yaml config\n```\nservices:\n  traefik:\n    ports:\n      - mode: ingress\n        target: 80\n        published: \"80\"\n        protocol: tcp\n      - mode: ingress\n        target: 443\n        published: \"443\"\n        protocol: tcp\n```\n\npassing such a config to docker stack deploy gives:\ndocker compose config improperly handles ports.n.published (string quotes instead of int)\n\n### Steps To Reproduce\n\n_No response_\n\n### Compose Version\n\n```Text\n\n```\n\n### Docker Environment\n\n```Text\n\n```\n\n### Anything else?\n\n_No response_",
      "solution": "> docker compose config used as pre-processor for docker stack deploy\n\nWhile many users rely on this, this is not supported.  `docker stack` didn't adopted the compose specification, which defines `published` as a string (it can express a range) - I can't see any reason you need to \"pre-process\" compose files, just let `stack deploy` parse your compose file.\n\n---\n\n> I can't see any reason you need to \"pre-process\" compose files, just let `stack deploy` parse your compose file.\nThat would be nice ... if docker stack would support any kind of .env or env.yaml. It really sucks, that one has to fiddle with global env or scripts to provide them to stack deploy.\n\nAs you already stated - many people rely on this. Why do you think this is the case?\nThis is kind of inconsistency between docker compose and docker stack which similar do the same - but one has .env support, but the one for swarm does not.\n\n---\n\n> AFAIK `docker stack` (now maintained by Mirantis, as Docker Swarm) has support for variable interpolation. It won't load your local `.env` file, but you can just `source` this file (which supports the `export` syntax) to get the same result:\n> \n> ```\n> $ source .env && docker stack config --compose-file compose.yaml\n> ```\n\nDidn't know docker stack isn't maintained by docker team anymore. Never the less - you mentioned it - using source .env is not a real solution its still fiddling with a workaround to provide .env and not a solid solution.",
      "labels": [
        "kind/bug",
        "status/0-triage"
      ],
      "created_at": "2025-12-16T09:10:50Z",
      "closed_at": "2025-12-16T10:51:56Z",
      "url": "https://github.com/docker/compose/issues/13459",
      "comments_count": 9
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 11820,
      "title": "Add environment variables for client OS/arch and client user UID/GID",
      "problem": "### Description\r\n\r\nWhen using Docker Compose for development setup, you often need to tweak the configuration to vary on Linux, macOS, and Windows. And some services you would like to run with the same UID/GID as your current client user.\r\n\r\nOur developers use both Linux and Mac (and we have one on Windows as well).\r\n\r\nCurrently, we cannot vary our Docker Compose files to take the platform differences into account.\r\n\r\nTo work around that, we have tried to introduce shell scripts that need to be run before using Docker Compose so that it can add `.env` files or `compose.override.yaml`. And we tried using conventions like \u201conly a minority uses Linux, and they are quite experienced, so if they export their UID we can use it if set and default to 501 otherwise\u201d.\r\n\r\nNeither of those solutions are elegant or error proof, and they remove some intended flexibility of env files and override configs.\r\n\r\nTo help with that, I suggest introducing four new environment variables while parsing the configuration files:\r\n\r\n - `COMPOSE_CLIENT_OS`: set to Go's `runtime.GOOS`\r\n - `COMPOSE_CLIENT_ARCH`: set to Go's `runtime.GOARCH`\r\n - `COMPOSE_CLIENT_UID`: set to the current users UID\r\n - `COMPOSE_CLIENT_GUID`: set to the current users GID\r\n\r\nThis way, we can now have a Docker Compose setup like this:\r\n\r\n`compose.yaml`:\r\n```yaml\r\nservices:\r\n  php:\r\n    image: php\r\n    volumes:\r\n      - .:/code\r\n    user: ${COMPOSE_CLIENT_UID}:${COMPOSE_CLIENT_GID}\r\n  web:\r\n    image: apache\r\n    extends:\r\n      file: compose.${COMPOSE_CLIENT_OS}.yaml\r\n      service: web\r\n```\r\n\r\n`compose.linux.yaml`:\r\n```yaml\r\nservices:\r\n  web:\r\n    environment:\r\n      VIRTUAL_HOST: mysite.local\r\n```\r\n\r\n`compose.darwin.yaml`:\r\n```yaml\r\nservices:\r\n  web:\r\n    environment:\r\n      VIRTUAL_HOST: mysite.docker\r\n```\r\n\r\nThe change in #11821 is rather simple. But I'll acknowledge my lack of familiarity with the docker Compose code base and therefore there _could_ be unintended side effects, and maybe it fits better into other parts of the code base. Possibly, it also needs test coverage. And as usual, we can discuss one of the two hard problems: naming things (there might be better names for the environment variables).\r\n\r\nAt least, I hope this can be given some thought. I know it would make our setups more elegant.\r\n",
      "solution": "Our primary use case for Docker Compose is running a development setup with websites on the developers' own machine.\n\nTo add working HTTPS on the development setup, we use [mkcert](https://mkcert.dev) to add a root certificate to the host machine and add mount it into our containers like this:\n\n```yaml\n    volumes:\n      - '${HOME}/.local/share/mkcert:/rootCA:ro'\n      - '${HOME}/.local/share/dev_certificates:/cert:rw'\n```\nUnfortunately, this location is for Linux only and mkcert places the certificates elsewhere on MacOS.\n\nOur current workaround is to tell Mac users to create a symlink mimicking the location on Linux.\n\nOver the years, we have also used various workarounds for increasing file system performance of volumes on MacOS. Usually, this has been using NFS or some variations, but again only on Mac (I'm not on Mac myself anymore, so I'm not sure of the current state, but I think most of them use Orbstack now).\n\nSo we have had all sorts of workarounds (\u201ccopy this file to `compose.override.yml`\u201d, \u201cexport these variables\u201d, \u201ccreate a symlink\u201d, ...).\n\nOur goal has always been to be able to just say \u201cjust clone the repo and run `docker compose up`\u201d.\n\nCompose does a fantastic job for portability, but often we need to interact with the host system outside of Docker -- and they differ.\n\nI'm aware of the other issue regarding UID/GID. It sneaked in to this because when I created the #11821 pull request I realized the two suggestions were very similar in how they could be implemented.\n\nIn our day-to-day life, we _do_ vary our setups both regarding OS and UID/GID. But we've had no other choice than to do this through wrapper scripts, README's, and conventions.",
      "labels": [
        "kind/feature"
      ],
      "created_at": "2024-05-15T12:56:10Z",
      "closed_at": "2025-12-16T09:08:56Z",
      "url": "https://github.com/docker/compose/issues/11820",
      "comments_count": 3
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13458,
      "title": "[BUG] Terminal prompt left in wrong place after some commands",
      "problem": "### Description\n\nOccasionally, when running `docker compose up -d` the command-line prompt will be left in the wrong place.\n\n![Image](https://github.com/user-attachments/assets/98b96f69-b1b7-416e-9da9-25d04cb62121)\n\nIn the image:\n\n1. Is where the command was issued\n2. Is the responses from the command as it runs\n3. Is where the command-line prompt is left (instead of after the text in step 2). Anything you type after that gets interwoven with the text in step 2.\n\n> I was using Node-RED as my test vehicle - making changes to force container re-creation \n\nIt does not always happen for every compose command but it does seem to happen reasonably reliably if an edit to `.env` is involved.\n\n### Steps To Reproduce\n\n1. A service definition containing something like this:\n\n\t```\n\t    environment:\n\t      - TEST=${TESTS:-one,two,three}\n\t```\n\n2. Edit `.env` so it has something like:\n\n\t```\n\tTESTS=one\n\t```\n\n3. Run `docker compose up -d`. This **may** exhibit the problem.\n\n4. Edit `.env` so the variable changes. Example:\n\n\t```\n\tTESTS=one,who\n\t```\n\n5. Run `docker compose up -d`. In my testing, this reliably exhibits the problem.\n\n\n### Compose Version\n\n```Text\n$ docker compose version\nDocker Compose version v5.0.0\n\n$ docker-compose version\nDocker Compose version v5.0.0\n```\n\n### Docker Environment\n\n```Text\n$ docker info\nClient: Docker Engine - Community\n Version:    29.1.3\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v5.0.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 6\n  Running: 6\n  Paused: 0\n  Stopped: 0\n Images: 7\n Server Version: 29.1.3\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: local\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 1c4457e00facac03ce1d75f7b6777a7a851e5c41\n runc version: v1.3.4-0-gd6d73eb8\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.1.21-v8+\n Operating System: Debian GNU/Linux 12 (bookworm)\n OSType: linux\n Architecture: aarch64\n CPUs: 4\n Total Memory: 3.704GiB\n Name: sec-dev\n ID: f6406dbb-2fc1-4834-9606-5e826871aec6\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\n\n### Anything else?\n\nI can't pin down when this started but I think it was a little before 5.0.0 - maybe 2.40.3 or perhaps even one before that. Initially I assumed it was just me and it took a while to spot the pattern.\n",
      "solution": "Probably fixed by https://github.com/docker/compose/pull/13439\nWould you have a chance to give a try to latest codebase ? Just checkout git repo and run `make install`",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2025-12-16T03:39:22Z",
      "closed_at": "2025-12-16T09:05:18Z",
      "url": "https://github.com/docker/compose/issues/13458",
      "comments_count": 2
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 11066,
      "title": "[BUG] watch crashes when deleting file",
      "problem": "### Description\r\n\r\nWhen deleting a file (unsure if the file existed or not but should have since it synced it) watch command crashes and cannot be restarted.\r\n\r\n```\r\ndevelop:\r\n  watch:\r\n    - action: sync\r\n      path: ${LOCAL_DEPLOY_PATH}\\platform\r\n      target: c:/inetpub/wwwroot/\r\n```\r\n\r\n```\r\nSyncing cm after changes were detected:\r\n  - C:\\t\\docker\\deploy\\platform\\Web.config.xdt\r\n\u263a\ufffdcontainer 061ae4ad9ec878e7a259e45aa1d7b4bd0dc56468b05497fa18cd71dd5f1c0cbe encountered an error during hcs::System::CreateProcess: failure in a Windows system call: The system cannot find the file specified. (0x2)\r\n```\r\nThen when trying to restart it is locked:\r\n```\r\n> docker compose watch --no-up\r\ncannot take exclusive lock for project \"kermit\": process with PID 20836 is still running\r\n```\r\n\r\nKilling the 20836 process still errors out the same.\r\n\r\n### Steps To Reproduce\r\n\r\nIt seem to be reproducable\r\n\r\n```\r\nwatching [C:\\t\\docker\\deploy\\platform]\r\nSyncing cm after changes were detected:\r\n  - C:\\t\\docker\\deploy\\platform\\images.jpg\r\n\u263bRtar: Removing leading drive letter from member names\r\nx inetpub/wwwroot/images.jpg\u263b\u263b\r\nSyncing cm after changes were detected:\r\n  - C:\\t\\docker\\deploy\\platform\\images.jpg\r\n\u263a\ufffdcontainer 7b26f6516e4c0ed000d0d71b1f01250411af2ddab0b17cd3f7f3b391a4ee97a0 encountered an error during hcs::System::CreateProcess: failure in a Windows system call: The system cannot find the file specified. (0x2)\r\n```\r\n\r\n### Compose Version\r\n\r\n```Text\r\nDocker Compose version v2.22.0\r\n```\r\n\r\n\r\n### Docker Environment\r\n\r\n```Text\r\nClient:\r\n Version:    24.0.6\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.10.3\r\n    Path:     C:\\Users\\Administrator\\.docker\\cli-plugins\\docker-buildx.exe\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.22.0\r\n    Path:     C:\\ProgramData\\Docker\\cli-plugins\\docker-compose.exe\r\n  scout: Command line tool for Docker Scout (Docker Inc.)\r\n    Version:  0.17.1\r\n    Path:     C:\\Users\\Administrator\\.docker\\cli-plugins\\docker-scout.exe\r\n\r\nServer:\r\n Containers: 14\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 14\r\n Images: 861\r\n Server Version: 24.0.4\r\n Storage Driver: windowsfilter\r\n  Windows:\r\n Logging Driver: json-file\r\n Plugins:\r\n  Volume: local\r\n  Network: ics internal l2bridge l2tunnel nat null overlay private transparent\r\n  Log: awslogs etwlogs fluentd gcplogs gelf json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Default Isolation: process\r\n Kernel Version: 10.0 20348 (20348.1.amd64fre.fe_release.210507-1500)\r\n Operating System: Microsoft Windows Server Version 21H2 (OS Build 20348.1970)\r\n OSType: windows\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 31.86GiB\r\n Name: kermit-dev\r\n ID: YP3Q:GBSN:NFJ3:QQMM:DB3Z:CD3V:7RBI:V445:473L:3WU3:VOP3:5DVB\r\n Docker Root Dir: C:\\ProgramData\\docker\r\n Debug Mode: false\r\n Username: kermit\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n Product License: Community Engine\r\n```\r\n\r\n\r\n### Anything else?\r\n\r\nThe file is not removed from container.\r\nNo idea of how/where the lock is kept.\r\nA restart of docker-engine removed the lock.",
      "solution": "@pbering and @perosb a possible workaround so you don't have to restart your host can be found in issue #11069:\r\n\r\nhttps://github.com/docker/compose/issues/11069#issuecomment-1769694535\n\n---\n\nWhen the issue happens and I see that message, then there is no process with that PID.\n\n---\n\n> @pbering and @perosb a possible workaround so you don't have to restart your host can be found in issue #11069:\r\n> \r\n> [#11069 (comment)](https://github.com/docker/compose/issues/11069#issuecomment-1769694535)\r\n\r\nAnother workarround (Linux) is to stop containers before exiting watch.\r\n`Ctrl+z` to suspend watch\r\n`docker-compose down` to stop and remove containers\r\n`fg` to bring watch process into foreground\r\n`Ctrl+c` to exit watch",
      "labels": [
        "kind/bug",
        "status/0-triage"
      ],
      "created_at": "2023-10-05T13:03:55Z",
      "closed_at": "2024-02-22T15:21:48Z",
      "url": "https://github.com/docker/compose/issues/11066",
      "comments_count": 30
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13449,
      "title": "[BUG] Terminal state not restored after non-plain docker compose output (prompt and cursor missing)",
      "problem": "### Description\n\nAfter running any `docker compose` command that produces **non-plain output** (TTY / animated progress / emoji), the terminal is left in an incorrect state when the command exits.\n\nAfter the command finishes:\n\n* Shell prompt is not displayed\n* Cursor is invisible\n* Subsequent input is rendered **on the last output line**, at the column where the prompt would normally start\n* Terminal does not advance to a new line\n\n### Steps To Reproduce\n\n1. running any `docker compose` command that produces **non-plain output** (TTY / animated progress / emoji)\n\n### Compose Version\n\n```Text\nv5.0.0\n```\n\n### Docker Environment\n\n```Text\nClient: Docker Engine - Community\n Version:    29.1.3\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v5.0.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 1\n  Running: 1\n  Paused: 0\n  Stopped: 0\n Images: 5\n Server Version: 29.1.3\n Storage Driver: overlay2\n  Backing Filesystem: xfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 1c4457e00facac03ce1d75f7b6777a7a851e5c41\n runc version: v1.3.4-0-gd6d73eb8\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 5.14.0-570.58.1.el9_6.x86_64\n Operating System: Rocky Linux 9.7 (Blue Onyx)\n OSType: linux\n Architecture: x86_64\n CPUs: 20\n Total Memory: 7.476GiB\n Name: multi.sonnet.su\n ID: 160cfd53-c140-4fbb-82de-1c48ab9614f1\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Registry Mirrors:\n  https://docker.m.daocloud.io/\n  https://docker.1ms.run/\n Live Restore Enabled: false\n Firewall Backend: iptables+firewalld\n```\n\n### Anything else?\n\nRunning **another `docker compose` command that also produces non-plain output** restores the terminal to a normal state.",
      "solution": "Isn't this the same as what has been fixed by #13439",
      "labels": [
        "kind/bug",
        "status/0-triage"
      ],
      "created_at": "2025-12-13T17:12:07Z",
      "closed_at": "2025-12-15T07:19:51Z",
      "url": "https://github.com/docker/compose/issues/13449",
      "comments_count": 3
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 11151,
      "title": "[BUG] Intermittent failure to remove old container on recreate",
      "problem": "### Description\n\nRunning the following command to recreate all containers.\r\n`docker compose --project-directory /etc/bob/ up --detach --force-recreate --remove-orphans --wait`\r\n\r\nRecreating 26 different containers, the compose file definition had not changed but the source image had been rebuilt and pulled onto the machine.\r\n\r\nAn extract from the running log\r\n```\r\n     Container our-bob-1  Recreate\r\n     ... [25 other containers] Recreate\r\n    Error response from daemon: Error when allocating new name: Conflict. The container name \"/our-bob-1\" is already in use by container \"136a920587be03151f4d750934c699b5482c3bd4b7c280f65df38f203dc95fa3\". You have to remove (or rename) that container to be able to reuse that name.\r\n```\r\n\r\nThe command ran for 0.89 seconds.\r\n\r\nRerunning the up command worked fine. Most of the time (multiple very similar systems) works fine.  I can't produce a reproducible test case.\r\n\r\nI suspect some sort of race condition between the deletion of the image and the creation of the new one, but have not capability to hunt down such a bug.\n\n### Steps To Reproduce\n\n1. docker compose up --force-recreate\n\n### Compose Version\n\n```Text\nDocker Compose version v2.17.3\n```\n\n\n### Docker Environment\n\n```Text\nClient:\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.17.3\r\n    Path:     /usr/lib/docker/cli-plugins/docker-compose\r\n\r\nServer:\r\n Containers: 26\r\n  Running: 26\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 62\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: syslog\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version:\r\n runc version:\r\n init version:\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.2.0-32-generic\r\n Operating System: Ubuntu 22.04.3 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 32\r\n Total Memory: 15.52GiB\r\n Name: bob\r\n ID: more-bob\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n Experimental: true\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\n```\n\n\n### Anything else?\n\nSystem CPU usage at the time was about 10%, total ram usage was high but 3/4 of it was disk cache.",
      "solution": "Thanks, I'll pull us forward and see if it reoccurs.\r\n\r\nScanning the release notes I saw \"Fixed a race condition when --parallel is used with a large number of dependent services\" But that seems to related to https://github.com/docker/compose/pull/10544 which doesn't match the observed issue.\n\n---\n\nI'm experiencing similar issue with `2.23.1` compose and `24.0.5` docker itself. The error is transient and appears from time to time is used with `docker --host ssh://<...>`\r\n\r\nHowever, in my case, it fails to create an intermediate container:\r\n```\r\n$ docker --host \"$DOCKER_HOST\" compose --file docker-compose.yml --file docker-compose.override.yml up --detach --wait\r\n Container <blah>-1  Recreate\r\nError response from daemon: Conflict. The container name \"/d9a6eaa59b96_<blah>-1\" is already in use by container \"2ab6e555da77ff7d6b03e2d9b6ea8097a79bf647ade06d4bf12793253137c3ce\". You have to remove (or rename) that container to be able to reuse that name.\r\n```\r\nThe compose file itself has only one service.\r\n\r\nI would be happy to provide more details if you have any ideas on how to debug this\r\n\r\nUPD: looks like the issue occurs when the service image is updated, the subsequent `docker compose up` works just fine\r\n\r\nUPD: UPD:\r\nIn some cases the error is a bit different:\r\n```\r\n$ docker --host \"$DOCKER_HOST\" compose --file docker-compose.yml --file docker-compose.override.yml up --detach --wait --force-recreate\r\n Container <blah>-1  Recreate\r\nError response from daemon: No such container: 50a914c300b3cc2b6676caea4d094a2ae8c16f04c5fb0dc461b8ddc3941e0361\r\n```",
      "labels": [
        "kind/bug",
        "stale"
      ],
      "created_at": "2023-11-01T10:59:25Z",
      "closed_at": "2025-12-14T00:19:51Z",
      "url": "https://github.com/docker/compose/issues/11151",
      "comments_count": 8
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13435,
      "title": "[BUG] inconsistent image selection for compose PUSH",
      "problem": "### Description\n\ndocker compose push as `docker compose push` for our docker-compose file is misbehaving. based on normal behavior compose should\n- use `services.service.image` for **pulling** the image.\n- use `services.service.build.tags` (if exists) for **pushing** the image.\n\nBut every time I run docker compose push, either selects `services.service.image` or `services.service.build.tags`.\n\n### Steps To Reproduce\n\n0) So let's say the docker-compose file looks like this:\n```\nservices:\n  backend:\n    build:\n      context: .\n      dockerfile: docker/Dockerfile\n      args:\n        PROFILE: ${PROFILE:-local}\n        REGISTRY: ${REGISTRY_PULL:-local}\n      extra_hosts:\n        - \"host.docker.internal:host-gateway\"\n      tags:\n        - ${REGISTRY_PUSH:-local}/backend:${GIT_COMMIT_SHORT:-local}\n        - ${REGISTRY_PUSH:-local}/backend:latest\n      platforms:\n        - linux/amd64\n    image: ${REGISTRY_PULL:-local}/backend:${GIT_COMMIT_SHORT:-latest}\n```\n\n1) Now we set:\n```\n$ export REGISTRY_PULL=dockerpull\n$ export REGISTRY_PUSH=dockerpush\n```\n\n2) Now we run `docker compose push` multiple times:\n```\n$ docker compose push\nAn image does not exist locally with the tag: dockerpush/backend <--- push registry name\nexit status 1\n$ docker compose push\nAn image does not exist locally with the tag: dockerpull/backend <--- pull registry name\nexit status 1\n$ docker compose push\nAn image does not exist locally with the tag: dockerpush/backend <--- push registry name\nexit status 1\n$ docker compose push\nAn image does not exist locally with the tag: dockerpush/backend <--- push registry name\nexit status 1\n$ docker compose push\nAn image does not exist locally with the tag: dockerpush/backend <--- push registry name\nexit status 1\n$ docker compose push\nAn image does not exist locally with the tag: dockerpull/backend <--- pull registry name\nexit status 1\n$ docker compose push\nAn image does not exist locally with the tag: dockerpush/backend <--- push registry name\nexit status 1\n```\nAs you can see every time we run it it's selecting either \"tags | images\" or something else that i'm missing.\n\n**Note** when i run `docker compose config` multiple times, it's very consistent and doesn't change.\n```\n$ docker compose config\nname: default\nservices:\n  backend:\n    build:\n      context: <path>\n      dockerfile: docker/Dockerfile\n      args:\n        PROFILE: local\n        REGISTRY: dockerpull\n      extra_hosts:\n        - host.docker.internal=host-gateway\n      tags:\n        - dockerpush/backend:local\n        - dockerpush/backend:latest\n      platforms:\n        - linux/amd64\n    image: dockerpull/backend:latest\n    networks:\n      default: null\nnetworks:\n  default:\n    name: default\n```\n\n### Compose Version\n\n```Text\non mac: Docker Compose version v2.31.0-desktop.2\non server: Docker Compose version v2.33.0 (and many other versions afaik)\n```\n\n### Docker Environment\n\n```Text\nClient: Docker Engine - Community\n Version:    28.0.0\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.21.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.33.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 0\n  Running: 0\n  Paused: 0\n  Stopped: 0\n Images: 0\n Server Version: 28.0.0\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: bcc810d6b9066471b0b6fa75f557a15a1cbf31bb\n runc version: v1.2.4-0-g6c52b3f\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 5.15.0-133-generic\n Operating System: Ubuntu 22.04.5 LTS\n OSType: linux\n Architecture: x86_64\n CPUs: 4\n Total Memory: 7.751GiB\n Name: <redacted>\n ID: b968d35e-856d-4d13-b392-05e2afb9c664\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Registry Mirrors:\n  https://<redacted>/\n Live Restore Enabled: false\n```\n\n### Anything else?\n\nBased on what I've read, seems to be when we build it's also building 2 images:\n- one whatever we named on `services.service.images`\n- one whatever we named on `services.service.build.tags`\n\nThis shouldn't be happening as the whole reason for having `services.service.build.tags`, is this.\nIt's breaking our build system as our **pull registry url is different than push registry url** and we have to come up with hacky solutions to be able to use `docker compose push`",
      "solution": "`services.service.images` is the \"main\" tag used to identify service image, during pull, push or build.\n`services.service.build.tags` are additional tags used during build, as the builder can set more than one tag. \n\n> But every time I run docker compose push, either selects services.service.image or services.service.build.tags.\n\nThe expected behavior is it should push with all of those. Isn't this what you get ?\n\n> our pull registry url is different than push registry url \n\nUnfortunately there's no way to cover such a weird requirement. A compose file is expected to be consistent between push and pull. A possible workaround is for you to user a `${REGISTRY}` variable to declare your service image.",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2025-12-09T12:32:06Z",
      "closed_at": "2025-12-10T21:50:02Z",
      "url": "https://github.com/docker/compose/issues/13435",
      "comments_count": 2
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 9233,
      "title": "Support assignment of a default `profile` (v2)",
      "problem": "<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/compose-cli/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Docker Community Slack - https://dockr.ly/slack\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nThe current implementation of `profiles` allows for zero or more profiles to be assigned to a service but if you assign one or more then you cannot also have a service be part of the default profile set, in a similar way to `networks` where not specifying a network assumes the service is attached to the `default` network, but if you attach it to the `foo` network can you *also* attach it explicitly to the `default` network.\r\n\r\nUse case: I have a compose project with multiple services. Some of these services `depends_on` other services, some are standlone. The services that depend on others are assigned a profile so that I can `docker compose --profile foo stop && docker compose --profile foo up -d` to ensure the child containers are stopped before the parents are recreated.\r\n\r\nI would like to be able to also assign those services to the default profile so that if I run for example `docker compose pull` it would pull images for all services. Right now I have to add *every* services to an explicit `default` profile and run `docker compose --profile default pull` to achieve this and that means always having to specify a profile for all commands.\r\n\r\nUltimately this would manifest as:\r\n```yml\r\nservices:\r\n  child:\r\n    image: web\r\n    profiles:\r\n      - foo\r\n      - default\r\n    depends_on:\r\n      - parent\r\n\r\n  parent:\r\n    image: db\r\n    profiles:\r\n      - foo\r\n      - default\r\n\r\n  standalone:\r\n    image: metrics\r\n    profiles:\r\n      - bar\r\n      - default\r\n```",
      "solution": "ok, so the root cause for asking this is \"up still does not respect depends_on and recreate dependent containers in the correct order\". Did you opened an issue for this?",
      "labels": [
        "stale"
      ],
      "created_at": "2022-03-05T14:37:52Z",
      "closed_at": "2022-11-02T03:19:37Z",
      "url": "https://github.com/docker/compose/issues/9233",
      "comments_count": 13
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13356,
      "title": "[BUG] Flaky behavior with double `!override` and extends",
      "problem": "### Description\n\nHi folks!\n\nI'm not sure what is going on with this thing, but docker compose couldn't always parse the following structure.\nThis started to happen after one of the recent updates...\n\n### Steps To Reproduce\n\n\n  Create the following file structure:\n\n  ```\n  min/\n  \u251c\u2500\u2500 docker-compose.yaml\n  \u251c\u2500\u2500 docker-compose.child1.yaml\n  \u251c\u2500\u2500 docker-compose.child2.yaml\n  \u2514\u2500\u2500 docker-compose.shared.yaml\n  ```\n\n  **docker-compose.yaml:**\n  ```yaml\n  include:\n    - docker-compose.child1.yaml\n    - docker-compose.child2.yaml\n  ```\n\n  **docker-compose.child1.yaml:**\n  ```yaml\n  include:\n    - docker-compose.shared.yaml\n  ```\n\n  **docker-compose.child2.yaml:**\n  ```yaml\n  include:\n    - docker-compose.shared.yaml\n  ```\n\n  **docker-compose.shared.yaml:**\n  ```yaml\n  services:\n    dep:\n      image: ubuntu\n\n    dep2:\n      image: ubuntu\n\n    base:\n      image: ubuntu\n      depends_on:\n        - dep\n\n    base2:\n      extends:\n        service: base\n      depends_on: !override\n        - dep2\n\n    shared:\n      extends:\n        service: base2\n      depends_on: !override\n        - dep\n  ```\n\nRun command:\n```sh\nfor i in {1..100}; do output=$(docker compose config --services 2>&1 >/dev/null) && echo -n \"\u2705\" || { echo -e \"\\n\u274c Failed at $i\\n$output\"; break; }; done\n```\n\nAt some point, it will fail:\n```sh\n\u2705\u2705\u2705\u2705\u2705\u2705\u2705\n\u274c Failed at 8\nservices.base2 conflicts with imported resource\n```\n\nBut in good scenario it will show the following:\n```\n> docker compose config --services\n\ndep\nbase\ndep2\nbase2\nshared\n```\n\n### Compose Version\n\n```Text\nDocker Compose version v2.40.3-desktop.1\n```\n\n### Docker Environment\n\n```Text\nClient:\n Version:    28.5.1\n Context:    desktop-linux\n Debug Mode: false\n Plugins:\n  ai: Docker AI Agent - Ask Gordon (Docker Inc.)\n    Version:  v1.9.11\n    Path:     /Users/maksim.zayats/.docker/cli-plugins/docker-ai\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.29.1-desktop.1\n    Path:     /Users/maksim.zayats/.docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.3-desktop.1\n    Path:     /Users/maksim.zayats/.docker/cli-plugins/docker-compose\n  debug: Get a shell into any image or container (Docker Inc.)\n    Version:  0.0.45\n    Path:     /Users/maksim.zayats/.docker/cli-plugins/docker-debug\n  desktop: Docker Desktop commands (Docker Inc.)\n    Version:  v0.2.0\n    Path:     /Users/maksim.zayats/.docker/cli-plugins/docker-desktop\n  extension: Manages Docker extensions (Docker Inc.)\n    Version:  v0.2.31\n    Path:     /Users/maksim.zayats/.docker/cli-plugins/docker-extension\n  init: Creates Docker-related starter files for your project (Docker Inc.)\n    Version:  v1.4.0\n    Path:     /Users/maksim.zayats/.docker/cli-plugins/docker-init\n  mcp: Docker MCP Plugin (Docker Inc.)\n    Version:  v0.24.0\n    Path:     /Users/maksim.zayats/.docker/cli-plugins/docker-mcp\n  model: Docker Model Runner (Docker Inc.)\n    Version:  v0.1.46\n    Path:     /Users/maksim.zayats/.docker/cli-plugins/docker-model\n  offload: Docker Offload (Docker Inc.)\n    Version:  v0.5.1\n    Path:     /Users/maksim.zayats/.docker/cli-plugins/docker-offload\n  sandbox: Docker Sandbox (Docker Inc.)\n    Version:  v0.3.1\n    Path:     /Users/maksim.zayats/.docker/cli-plugins/docker-sandbox\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\n    Version:  0.6.0\n    Path:     /Users/maksim.zayats/.docker/cli-plugins/docker-sbom\n  scout: Docker Scout (Docker Inc.)\n    Version:  v1.18.3\n    Path:     /Users/maksim.zayats/.docker/cli-plugins/docker-scout\n\nServer:\n Containers: 38\n  Running: 0\n  Paused: 0\n  Stopped: 38\n Images: 118\n Server Version: 28.5.1\n Storage Driver: overlayfs\n  driver-type: io.containerd.snapshotter.v1\n Logging Driver: json-file\n Cgroup Driver: cgroupfs\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Discovered Devices:\n  cdi: docker.com/gpu=webgpu\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 05044ec0a9a75232cad458027ca83437aae3f4da\n runc version: v1.2.5-0-g59923ef\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.10.14-linuxkit\n Operating System: Docker Desktop\n OSType: linux\n Architecture: aarch64\n CPUs: 14\n Total Memory: 29.33GiB\n Name: docker-desktop\n ID: 7f57a7cb-e395-4e74-bc38-bec10b44c5c8\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n HTTP Proxy: http.docker.internal:3128\n HTTPS Proxy: http.docker.internal:3128\n No Proxy: hubproxy.docker.internal\n Labels:\n  com.docker.desktop.address=unix:///Users/maksim.zayats/Library/Containers/com.docker.docker/Data/docker-cli.sock\n Experimental: false\n Insecure Registries:\n  hubproxy.docker.internal:5555\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n```\n\n### Anything else?\n\n_No response_",
      "solution": "probably fixed by https://github.com/compose-spec/compose-go/pull/833\n\n---\n\nWould you have a chance to give latest release candidate a try and confirm issue is fixed ?\n\n---\n\nHi @ndeloof !\nUnfortunately, it looks like the problem is still there :(\n\n```\n\u279c   for i in {1..100}; do output=$(../docker-compose-darwin-aarch64 config --services 2>&1 >/dev/null) && echo -n \" \u2705\" || { echo -e \"\\n\u274c Failed at $i\\n$output\"; break; }; done\n\n\u2705\u2705\u2705\u2705\u2705\n\u274c Failed at 6\nservices.base2 conflicts with imported resourc\n```\n\n```\n\u279c   ../docker-compose-darwin-aarch64 version\nDocker Compose version v5.0.0-rc.2\n```\n\n```\n\u279c   docker version\nClient:\n Version:           29.0.1\n API version:       1.52\n Go version:        go1.25.4\n Git commit:        eedd969\n Built:             Fri Nov 14 16:16:57 2025\n OS/Arch:           darwin/arm64\n Context:           desktop-linux\n\nServer: Docker Desktop 4.53.0 (211793)\n Engine:\n  Version:          29.0.1\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.4\n  Git commit:       198b5e3\n  Built:            Fri Nov 14 16:18:20 2025\n  OS/Arch:          linux/arm64\n  Experimental:     false\n containerd:\n  Version:          v2.1.5\n  GitCommit:        fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc:\n  Version:          1.3.3\n  GitCommit:        v1.3.3-0-gd842d771\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n``` ",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2025-11-05T15:47:47Z",
      "closed_at": "2025-12-08T13:53:55Z",
      "url": "https://github.com/docker/compose/issues/13356",
      "comments_count": 4
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13414,
      "title": "[BUG] docker compose build panic",
      "problem": "### Description\n\n<img width=\"834\" height=\"633\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/388a0571-37c3-46df-b418-3829b248c489\" />\n\n### Steps To Reproduce\n\n_No response_\n\n### Compose Version\n\n```Text\n\n```\n\n### Docker Environment\n\n```Text\n\n```\n\n### Anything else?\n\n_No response_",
      "solution": "fixed by https://github.com/docker/compose/pull/13415",
      "labels": [
        "kind/bug",
        "status/0-triage"
      ],
      "created_at": "2025-12-03T13:57:52Z",
      "closed_at": "2025-12-05T12:16:23Z",
      "url": "https://github.com/docker/compose/issues/13414",
      "comments_count": 1
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 8800,
      "title": "New TUI spams lines when pulling multiple images causing you to lose your terminal history",
      "problem": "<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/compose-cli/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Docker Community Slack - https://dockr.ly/community\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n@thaJeztah has asked me to open an issue after commenting on #8753 \r\n\r\n> For your use-case, is there a specific reason why the new output is problematic for you? If so, could you describe your use-case? Perhaps there's enhancements to be made to address.\r\n\r\n\r\nI'm not the person who created this issue but I have one complaint about the new output as well.\r\nFirst of all, I love BuildKit's output in docker build. But, compose's implementation makes me question how this made it through an official 2.0 release.\r\n\r\nhttps://user-images.githubusercontent.com/28601081/137222298-31ca5d3a-68be-4041-82e8-8d58cb8e8999.mp4\r\n\r\nThis can be reproduced in both Windows' command prompt, Windows Terminal and probably others. It literally spams thousands of lines to the terminal ruining the ability to scroll back. Sorry if I'm hijacking the issue but it seemed fairly fitting as I have to resort to rolling back to a 1.x release.\r\n\r\n_Originally posted by @clrxbl in https://github.com/docker/compose/issues/8753#issuecomment-942773151_\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. docker compose up on a docker-compose.yml file that contains multiple images & a small enough terminal window.\r\n\r\n**Describe the results you received:**\r\nSee the above video\r\n\r\n**Describe the results you expected:**\r\nI should be able to scroll back and not lose all of my terminal history, even after the command is done.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\nIt seems like this issue isn't present if you're reproducing it in a large enough terminal window (e.g. fullscreen)\r\n\r\n**Output of `docker compose version`:**\r\n\r\n```\r\nDocker Compose version 2.0.1\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nWARNING: Plugin \"/usr/local/lib/docker/cli-plugins/docker-buildx\" is not valid: failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-buildx: no such file or directory\r\nWARNING: Plugin \"/usr/local/lib/docker/cli-plugins/docker-compose\" is not valid: failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-compose: no such file or directory\r\nWARNING: Plugin \"/usr/local/lib/docker/cli-plugins/docker-scan\" is not valid: failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-scan: no such file or directory\r\nClient:\r\nWARNING: No blkio throttle.read_bps_device support\r\nWARNING: No blkio throttle.write_bps_device support\r\nWARNING: No blkio throttle.read_iops_device support\r\nWARNING: No blkio throttle.write_iops_device support\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n\r\nServer:\r\n Containers: 12\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 12\r\n Images: 11\r\n Server Version: 20.10.9\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 8686ededfc90076914c5238eb96c883ea093a8ba.m\r\n runc version: v1.0.2-0-g52b36a2d\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 5.10.60.1-microsoft-standard-WSL2\r\n Operating System: Arch Linux\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 12\r\n Total Memory: 15.63GiB\r\n Name: DESKTOP-BQ26BOE-wsl\r\n ID: CWUC:IOEW:TCJZ:EZJS:RTO5:YSBV:X7BE:YHNS:MQNY:V3VQ:N3NI:I4T4\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n Username: clrxbl\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n**Additional environment details:**\r\nArch Linux WSL2 w/ systemd-genie\r\n",
      "solution": "While trying, I also noticed some \"jittery\" output if two services use the same image (this may be the same if they use different images, but layers that are shared between those images);\r\n\r\n```yaml\r\nservices:\r\n    foo:\r\n        image: postgres\r\n    bar:\r\n        image: postgres\r\n```\r\n\r\nDuring the pull, the output is \"jumpy\". Perhaps because both `foo` and `bar` services try to show progress for the same layers:\r\n\r\n```bash\r\ndocker compose down -v\r\ndocker rmi postgres\r\n\r\ndocker compose up\r\n[+] Running 8/15\r\n \u280b foo Pulling                                                                                                                                                                                       12.0s\r\n   \u283f 8a79eb6d69c9 Pull complete                                                                                                                                                                       4.9s\r\n   \u283f 397705f2d093 Pull complete                                                                                                                                                                       5.1s\r\n   \u283f de36ec4eb0a5 Pull complete                                                                                                                                                                       5.2s\r\n   \u280f 1d24b3d9557e Download complete                                                                                                                                                                   8.9s\r\n   \u280f a39fee215a44 Download complete                                                                                                                                                                   8.9s\r\n \u280b bar Pulling                                                                                                                                                                                       12.0s\r\n   \u283f e5ae68f74026 Pull complete                                                                                                                                                                       3.4s\r\n   \u283f 7b8fcc7e1ad0 Pull complete                                                                                                                                                                       3.9s\r\n   \u283f 7527d03e2f77 Pull complete                                                                                                                                                                       4.0s\r\n   \u283f 80e55689f4d0 Pull complete                                                                                                                                                                       4.2s\r\n   \u283f 08d878a022c1 Pull complete                                                                                                                                                                       5.3s\r\n   \u280f 7677029670ff Extracting      [=============================================>     ]  83.56MB/91.23MB                                                                                              8.9s\r\n   \u280f e085b018338c Download complete                                                                                                                                                                   8.9s\r\n   \u280f 063b09ff12e9 Download complete                                                                                                                                                                   8.9s\r\n```\r\n\r\nAfter the pull completed, the output is a bit confusing, because _some_ layers are shown under service `foo`, and _some_ under `bar`\r\n\r\n```bash\r\ndocker compose up\r\n[+] Running 15/15\r\n \u283f foo Pulled                                                                                                                                                                                        14.5s\r\n   \u283f 8a79eb6d69c9 Pull complete                                                                                                                                                                       4.9s\r\n   \u283f 397705f2d093 Pull complete                                                                                                                                                                       5.1s\r\n   \u283f de36ec4eb0a5 Pull complete                                                                                                                                                                       5.2s\r\n   \u283f e085b018338c Pull complete                                                                                                                                                                      11.3s\r\n \u283f bar Pulled                                                                                                                                                                                        14.5s\r\n   \u283f e5ae68f74026 Pull complete                                                                                                                                                                       3.4s\r\n   \u283f 7b8fcc7e1ad0 Pull complete                                                                                                                                                                       3.9s\r\n   \u283f 7527d03e2f77 Pull complete                                                                                                                                                                       4.0s\r\n   \u283f 80e55689f4d0 Pull complete                                                                                                                                                                       4.2s\r\n   \u283f 08d878a022c1 Pull complete                                                                                                                                                                       5.3s\r\n   \u283f 7677029670ff Pull complete                                                                                                                                                                      11.2s\r\n   \u283f 1d24b3d9557e Pull complete                                                                                                                                                                      11.2s\r\n   \u283f 063b09ff12e9 Pull complete                                                                                                                                                                      11.4s\r\n   \u283f a39fee215a44 Pull complete                                                                                                                                                                      11.4s\r\n[+] Running 3/3\r\n \u283f Network compose-progress_default  Created                                                                                                                                                          0.0s\r\n \u283f Container compose-progress-bar-1  Created                                                                                                                                                          3.2s\r\n \u283f Container compose-progress-foo-1  Created                                                                                                                                                          3.2s\r\nAttaching to compose-progress-bar-1, compose-progress-foo-1\r\n```\r\n\r\nNot sure what the solution to that would be;\r\n\r\n- for the \"two services use the _same image_\", I guess compose could detect this case, and only do a pull once, but of course that won't help if the images are different (but share common layers).\r\n- alternatively, do the \"reverse\", and make sure that layer progress is always shown under \"both\" (duplicate the progress in case it's shared); possibly challenging to do this\r\n\r\nHiding progress of individual layers (as mentioned in my previous comment) may help here as well.\r\n\n\n---\n\n@ndeloof :  does buildx also control the part when containers are creating/starting?\r\n```\r\n[+] Running 43/43redacted-1                Started                 4.5s\r\n \u283f Network redacted-default                 Created                 0.0s\r\n \u283f Volume \"redacted_a-data\"                 Created                 0.0s\r\n \u283f Volume \"redacted_b-data\"                 Created                 0.0s\r\n \u283f Container redacted-a-1                   Started                 3.0s\r\n \u283f Container redacted-b-1                   Started                 3.1s\r\n \u283f Container redacted-c-1                   Started                 3.3s\r\n \u283f Container redacted-postgres-1            Started                 3.3s\r\n \u283f Container redacted-kafka-1               Started                10.0s\r\n```\r\n\r\nIn a short windows (say 4 lines high), the problem still occurs, even without the pulls\n\n---\n\nWas this fixed by #9476 (in release v2.6.0)? I don't get any flickering or line spam at all anymore when multiple containers use the same image.",
      "labels": [],
      "created_at": "2021-10-14T23:58:02Z",
      "closed_at": "2023-01-12T10:22:28Z",
      "url": "https://github.com/docker/compose/issues/8800",
      "comments_count": 30
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13378,
      "title": "[BUG] Expose Range `Error response from daemon: invalid JSON: invalid port '<start>-<end>': invalid syntax`",
      "problem": "### Description\n\nExpose of port range results in invalid syntax.  My first experience of this is with:\n`Docker version 29.0.0, build 3d4129b`\n\n### Steps To Reproduce\n\n1. Docker version 29.0.0, build 3d4129b on Ubuntu 24.04.3 LTS\n\n2. \n```\nservices:\n  test:\n    image: alpine\n    expose:\n      - \"9091-9092\"\n```\n\n3.\n```\ndocker compose -f ./text.xml run test\n```\n\n4.\n```\nError response from daemon: invalid JSON: invalid port '9091-9092: invalid syntax\n```\n\n### Compose Version\n\n```Text\nDocker Compose version v2.40.3\n```\n\n### Docker Environment\n\n```Text\nClient: Docker Engine - Community\n Version:    29.0.0\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.3\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\nWARNING: Plugin \"/usr/libexec/docker/cli-plugins/docker-compose_2-12-2\" is not valid: plugin candidate \"compose_2-12-2\" did not match \"^[a-z][a-z0-9]*$\"\n\nServer:\n Containers: 21\n  Running: 21\n  Paused: 0\n  Stopped: 0\n Images: 20\n Server Version: 29.0.0\n Storage Driver: zfs\n  Zpool: rpool\n  Zpool Health: ONLINE\n  Parent Dataset: rpool/ROOT/ubuntu_549uxf/var/lib\n  Space Used By Parent: 11773657088\n  Space Available: 138369679360\n  Parent Quota: no\n  Compression: lz4\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: runc io.containerd.runc.v2\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc version: v1.3.3-0-gd842d771\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.8.0-87-generic\n Operating System: Ubuntu 24.04.3 LTS\n OSType: linux\n Architecture: x86_64\n CPUs: 8\n Total Memory: 7.644GiB\n Name: tmk-svr-01\n ID: OVLQ:BHCM:EYES:2XNJ:IBFZ:ANTP:GEPH:Y5M7:FDET:EB5P:NOQB:G6VX\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: true\n Firewall Backend: iptables\n```\n\n### Anything else?\n\n_No response_",
      "solution": "@Neurone please git RC release a try to confirm issue has been fixed",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2025-11-14T14:13:55Z",
      "closed_at": "2025-11-16T19:38:56Z",
      "url": "https://github.com/docker/compose/issues/13378",
      "comments_count": 10
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 8533,
      "title": "docker compose does not create volume folders when the folder does not exist",
      "problem": "<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/compose-cli/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Docker Community Slack - https://dockr.ly/community\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n`docker compose up` for a service that describe a volume bound to a folder that does not exist yields the following error:\r\n\r\n```\r\n\u276f docker compose up -d\r\n[+] Running 0/0\r\n \u280b Container docker-compose-non-existent-folder-binding-issue_hello-world_1  Recreate                                       0.0s\r\nError response from daemon: invalid mount config for type \"bind\": bind source path does not exist: /<path omitted>/logs\r\n```\r\n\r\n`docker-compose up` will in turn create the folder used for binding in case it does not exist.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Create a compose file with the following content:\r\n    ```yml\r\n    version: '2'\r\n    services:\r\n      hello-world:\r\n        image: hello-world\r\n        volumes:\r\n        - './logs:/tmp/logs'\r\n    ```\r\n2. Ensure that the `logs` folder does not exist inside the folder where you created the compose file\r\n3. Run the command `docker compose up`\r\n\r\n**Describe the results you received:**\r\nAlready mentioned in the description\r\n\r\n**Describe the results you expected:**\r\nThe logs folder should be created in case it does not exist\r\n\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Cloud integration: 1.0.14\r\n Version:           20.10.6\r\n API version:       1.41\r\n Go version:        go1.16.3\r\n Git commit:        370c289\r\n Built:             Fri Apr  9 22:46:57 2021\r\n OS/Arch:           darwin/amd64\r\n Context:           default\r\n Experimental:      true\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          20.10.6\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.13.15\r\n  Git commit:       8728dd2\r\n  Built:            Fri Apr  9 22:44:56 2021\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.4.4\r\n  GitCommit:        05f951a3781f4f2c1911b05e61c160e9c30eaa8e\r\n runc:\r\n  Version:          1.0.0-rc93\r\n  GitCommit:        12644e614e25b05da6fd08a38ffa0cfe1903fdec\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n**Output of `docker context show`:**  \r\nYou can also run `docker context inspect context-name` to give us more details but don't forget to remove sensitive content.\r\n\r\n```\r\ndefault \r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  app: Docker App (Docker Inc., v0.9.1-beta3)\r\n  buildx: Build with BuildKit (Docker Inc., v0.5.1-docker)\r\n  compose: Docker Compose (Docker Inc., 2.0.0-beta.1)\r\n  scan: Docker Scan (Docker Inc., v0.8.0)\r\n\r\nServer:\r\n Containers: 9\r\n  Running: 6\r\n  Paused: 0\r\n  Stopped: 3\r\n Images: 179\r\n Server Version: 20.10.6\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 05f951a3781f4f2c1911b05e61c160e9c30eaa8e\r\n runc version: 12644e614e25b05da6fd08a38ffa0cfe1903fdec\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 5.10.25-linuxkit\r\n Operating System: Docker Desktop\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 12\r\n Total Memory: 15.64GiB\r\n Name: docker-desktop\r\n ID: OU4T:KJXL:GFUU:COJX:NPDY:SBVL:TPCQ:JBRP:WK5F:Q2XX:CZQI:SXSF\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: true\r\n  File Descriptors: 84\r\n  Goroutines: 79\r\n  System Time: 2021-06-23T09:34:20.161579016Z\r\n  EventsListeners: 4\r\n HTTP Proxy: http.docker.internal:3128\r\n HTTPS Proxy: http.docker.internal:3128\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS ECS, Azure ACI, local, etc.):**\r\n",
      "solution": "Same problem here.\r\nRemoving the trailing slashes from the paths as described by @Software-Noob fixes the issue.\r\nIn my opinion this is a bug, because it should also work with trailing slashes in the paths.\n\n---\n\n> You're using docker compose 2.0.0-beta.1, can you please upgrade to recently released 2.0.0-beta.4 to confirm this issue is still relevant? (just need to download relevant binary for your platform from https://github.com/docker/compose-cli/releases/tag/v2.0.0-beta.4, rename as `~/.docker/docker-compose` and make executable with `chmod +x`)\r\n\r\nHello @ndeloof , sorry for my very late reply, I retested today having Docker Compose v2.2.1 installed on my machine.\r\nIt worked for both `- './logs:/tmp/logs'` and `- './logs/:/tmp/logs/'`, from my side as reporter, I consider this fixed.\r\n\r\n@Software-Noob , @mibuthu , @Paraphraser , still an issue for you?\n\n---\n\nThe issue still persists on Debian 12 (what I use)",
      "labels": [
        "stale"
      ],
      "created_at": "2021-06-23T09:34:55Z",
      "closed_at": "2022-07-11T07:21:32Z",
      "url": "https://github.com/docker/compose/issues/8533",
      "comments_count": 15
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13412,
      "title": "[BUG] Again: Expose Range Error response from daemon: invalid JSON: invalid port '<start>-<end>': invalid syntax",
      "problem": "### Description\n\nI have the same issue described here: #13378 \nI'm on MacOS 15.7.2 and Docker Desktop.\n\n### Steps To Reproduce\n\n```shell\n~ docker version\n\nClient:\n Version:           29.0.1\n API version:       1.52\n Go version:        go1.25.4\n Git commit:        eedd969\n Built:             Fri Nov 14 16:16:57 2025\n OS/Arch:           darwin/arm64\n Context:           desktop-linux\n\nServer: Docker Desktop 4.53.0 (211793)\n Engine:\n  Version:          29.0.1\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.4\n  Git commit:       198b5e3\n  Built:            Fri Nov 14 16:18:20 2025\n  OS/Arch:          linux/arm64\n  Experimental:     false\n containerd:\n  Version:          v2.1.5\n  GitCommit:        fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc:\n  Version:          1.3.3\n  GitCommit:        v1.3.3-0-gd842d771\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### Compose Version\n\n```Text\n~ docker compose version\nDocker Compose version v2.40.3-desktop.1\n```\n\n### Docker Environment\n\n```Text\n\n```\n\n### Anything else?\n\n_No response_",
      "solution": "I see it's fixed in the upcoming release. Closing this.",
      "labels": [
        "kind/bug",
        "status/0-triage"
      ],
      "created_at": "2025-12-02T09:29:30Z",
      "closed_at": "2025-12-02T09:30:53Z",
      "url": "https://github.com/docker/compose/issues/13412",
      "comments_count": 1
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13389,
      "title": "[BUG] Error response from daemon: client version 1.42 is too old. Minimum supported API version is 1.44, please upgrade your client to a newer version",
      "problem": "### Description\n\nOn Ubuntu 22.04.5, since a recent `apt-get upgrade` which brought `docker -ce` version 29.x instead of 28.x, I now met this error while running some compose commands:\n\n```\n$ docker compose ps \nError response from daemon: client version 1.42 is too old. Minimum supported API version is 1.44, please upgrade your client to a newer version\n```\n\nwhile:\n\n```\n$ docker version\nClient: Docker Engine - Community\n Version:           29.0.2\n API version:       1.52\n Go version:        go1.25.4\n Git commit:        8108357\n Built:             Mon Nov 17 12:33:14 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          29.0.2\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.4\n  Git commit:       e9ff10b\n  Built:            Mon Nov 17 12:33:14 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v2.1.5\n  GitCommit:        fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc:\n  Version:          1.3.3\n  GitCommit:        v1.3.3-0-gd842d771\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\nand:\n```\n$ docker compose version\nDocker Compose version v2.5.0\n```\n\nAlso:\n```\napt-cache policy docker-ce\ndocker-ce:\n  Installed: 5:29.0.2-1~ubuntu.22.04~jammy\n  Candidate: 5:29.0.2-1~ubuntu.22.04~jammy\nVersion table:\n ...\n```\n\n### Steps To Reproduce\n\n`docker compose ps` simply raises:\n\n```\nError response from daemon: client version 1.42 is too old. Minimum supported API version is 1.44, please upgrade your client to a newer version\n```\n\n### Compose Version\n\n```Text\nDocker Compose version v2.5.0\n```\n\n### Docker Environment\n\n```Text\nClient: Docker Engine - Community\n Version:    29.0.2\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.5.0\n    Path:     /usr/local/lib/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 12\n  Running: 12\n  Paused: 0\n  Stopped: 0\n Images: 178\n Server Version: 29.0.2\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: error\n  NodeID: \n  Error: error while loading TLS certificate in /var/lib/docker/swarm/certificates/swarm-node.crt: certificate (1 - zrqvyk04ycyoucxaoq2chdogi) not valid after Thu, 26 Sep 2024 08:38:00 UTC, and it is currently Fri, 21 Nov 2025 13:21:18 CET: x509: certificate has expired or is not yet valid: \n  Is Manager: false\n  Node Address: <server_ip>\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc version: v1.3.3-0-gd842d771\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 5.15.0-161-generic\n Operating System: Ubuntu 22.04.5 LTS\n OSType: linux\n Architecture: x86_64\n CPUs: 8\n Total Memory: 32.1GiB\n Name: <server_name>\n ID: ****:****:****:****:...\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Username: <username>\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\n\n### Anything else?\n\n_No response_",
      "solution": "You were perfectly right @ndeloof ; I fixed this old version that wasn't purged properly and now everything runs smoothly.",
      "labels": [
        "kind/question"
      ],
      "created_at": "2025-11-21T12:37:52Z",
      "closed_at": "2025-11-25T15:18:02Z",
      "url": "https://github.com/docker/compose/issues/13389",
      "comments_count": 7
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13396,
      "title": "[BUG] Long notation of `volumes` `bind` mount doesn't apply selinux labels",
      "problem": "### Description\n\nI already created containers/podman#27600 as I use Podman with docker-compose.\n\nThere are short and long notations available for `volumes` under `services`:\n\nShort:\n\n```\nservices:\n  mariadb:\n    volumes:\n      # private\n      - ./mariadbtest1:/var/lib/mysql:Z\n```\n\nLong:\n\n```\nservices:\n  mariadb:\n    volumes:\n      # private\n      - type: bind\n        source: \"mariadbtest2\"\n        target: \"/var/lib/mysql\"\n        bind:\n          selinux: Z\n```\n\nThe long notation does not apply the already generated selinux label and therefore the volume is not working due to a `Permission denied` error int he container.\n\nThe long notation only works when running `setenforce 0` to deactivate selinux entirely.\n\n### Steps To Reproduce\n\nThere are two tests that can be run. I named the containers `mariadbtest1` and `mariadbtest2`. Both containers will mount a directory that has the same name as the containers. Each container has it's own `compose.yaml` file:\n\n`mariadbtest1` with short notation:\n\n```\n---\nname: mariadbtest1\nservices:\n  mariadb:\n    image: docker.io/mariadb:latest\n    container_name: mariadbtest1\n    user: \"50000:50000\"\n    userns_mode: \"keep-id\"\n    volumes:\n      # private\n      - ./mariadbtest1:/var/lib/mysql:Z\n    environment:\n      - MARIADB_DATABASE=exmple-database\n      - MARIADB_USER=example-user\n      - MARIADB_PASSWORD=my_cool_secret\n      - MARIADB_ROOT_PASSWORD=my-secret-pw\n```\n\n`mariadbtest2` with long notation:\n\n```\n---\nname: mariadbtest2\nservices:\n  mariadb:\n    image: docker.io/mariadb:latest\n    container_name: mariadbtest2\n    user: \"50000:50000\"\n    userns_mode: \"keep-id\"\n    volumes:\n      # private\n      - type: bind\n        source: \"mariadbtest2\"\n        target: \"/var/lib/mysql\"\n        bind:\n          selinux: Z\n    environment:\n      - MARIADB_DATABASE=exmple-database\n      - MARIADB_USER=example-user\n      - MARIADB_PASSWORD=my_cool_secret\n      - MARIADB_ROOT_PASSWORD=my-secret-pw\n```\n\nThe to be mounted directory can be checked with `ls -lZ` before and after each test. The selinux labels for `mariadbtest1` will be applied to the directory, but that's not the case for `mariadbtest2`. The `mariadbtest2` directory has teh same selinux labels as before.\n\nFurthermore I was able to confirm that both containers **correctly** generate the selinux labels when running `podman inspect mariadbtest1` (...and 2), but for 2 they are not being applied to the directory.\n\nA Podman developer also [tested](https://github.com/containers/podman/issues/27600#issuecomment-3575724052) `docker-compose` with the Docker daemon and found the same behaviour for Podman and Docker.\n\n### Compose Version\n\n```Text\n$> podman compose --version\nDocker Compose version 2.40.3\n```\n\n### Docker Environment\n\n```Text\nRunning Podman 5.6.2\n```\n\n### Anything else?\n\n1) See [this documentation](https://docs.docker.com/reference/compose-file/services/#volumes): The long `bind` notation is needed to set `create_host_path: false` as the short notation always defaults to `true`. There's **no** further restriction to the `bind` mount related to selinux labels. The documentation also states:\n\n> VOLUME: Can be either a host path on the platform hosting containers (bind mount) or a volume name.\n\n-> When using an absolute or relative path like `./mariadbtest1:/var/lib/mysql:Z` it's also a `bind` mount\n\n2) See [this documentation](https://docs.docker.com/engine/storage/bind-mounts/#configure-the-selinux-label):\n\n> When using bind mounts with services, SELinux labels (:Z and :z), as well as :ro are ignored. See [moby/moby #32579](https://github.com/moby/moby/issues/32579) for details.\n\n-> That's the opposite to the first documentation and seems to be wrong as the short notation can also be a `bind` mount.\n\n",
      "solution": "Thank you for the long explanation! That helped me understanding the situation and fixed my issue with the volumes! \ud83d\ude42",
      "labels": [
        "kind/bug",
        "status/0-triage"
      ],
      "created_at": "2025-11-25T17:04:16Z",
      "closed_at": "2025-11-26T11:20:37Z",
      "url": "https://github.com/docker/compose/issues/13396",
      "comments_count": 5
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13394,
      "title": "[BUG] Inconsistent `compose publish` behavior with variables",
      "problem": "### Description\n\nIt is not exactly clear from the docs how `compose publish` is supposed to handle variables. There is an `--with-env` flag which somehow seems to suggest that env variables are included in the OCI manifest. When using it docker compose asks whether sensitive variables are included, but the final manifest doesn't seem to include them anyways?\n\n### Steps To Reproduce\n\nUsing this compose file:\n```\nservices:\n  test1:\n    image: nginx:latest\n    command: [\"${SECRET}\"]\n    environment:\n      TEST: \"${SECRET}\"\n```\nand publishing it via:\n```\nSECRET=test docker compose publish localhost:5000/test:latest --insecure-registry\n```\nresults in an error:\n```\nservice \"test1\" has environment variable(s) declared.\nTo avoid leaking sensitive data, you must either explicitly allow the sending of environment variables by using the --with-env flag,\nor remove sensitive data from your Compose configuration\n```\nSo far so good, but publishing with `--with-env` doesn't really seem to result in leakage either:\n```\nSECRET=test docker compose publish localhost:5000/test:latest --insecure-registry --with-env                                                                                                                                                                                                                                                             \ue0b2 1 \u2718 \ue0b2 14:40:46 \uf017 \n? you are about to publish environment variables within your OCI artifact.\nplease double check that you are not leaking sensitive data\nService/Config  test1\nTEST=test\nAre you ok to publish these environment variables? Yes\n[+] push 1/1\n \u2714 test1 Skipped                                                                                                                                                                                                                                                                                                                                                                       0.0s \n[+]  2/2t:5000/test:latest publishing \n```\n\nWhen checking the published data:\n```\nregctl --host reg=localhost:5000,tls=disabled blob get localhost:5000/test:latest sha256:5868bab8f786242ba43d6604ea68ac5f4509457d6e8bfb3222ee7e85b4b9575f\n```\nit does not include the env variables:\n```\nservices:\n  test1:\n    image: nginx:latest\n    command: [\"${SECRET}\"]\n    environment:\n      TEST: \"${SECRET}\"\n```\n\nThe result seems plausible to me, one usually does not want to publish with environment variables interpolated to allow for customization, but what is `--with-env` warning about then?\n\n### Compose Version\n\n```Text\nDocker Compose version v5.0.0-rc.2\n```\n",
      "solution": "The result indeed is the expected one, i.e the original compose file is published to support full customization by consumer.\n\nThis check was introduced with use of local .env file in mind, as some user rely on this to pass security tokens - which they obviously don't want to be made public - so the review step to confirm variables in use. Compose is not clever enough (or designed) to track where a specific variable was resolved for, and as such can't guess you just set `SECRET` on the command line and this has no effect.",
      "labels": [
        "kind/question"
      ],
      "created_at": "2025-11-25T13:43:16Z",
      "closed_at": "2025-11-28T08:48:56Z",
      "url": "https://github.com/docker/compose/issues/13394",
      "comments_count": 4
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 12200,
      "title": "Introduce env variable for cli switch `--project-directory`",
      "problem": "### Description\n\nI know there is `COMPOSE_FILE` to specify the path to a compose file. But the problem is that this can be either of `docker-compose.yml` or `docker-compose.override.yml`. It's is choosen based on a rule when docker compose is launched without options. I want this behaviour using an env variable too, like `COMPOSE_PROJECT_DIRECTORY` or `COMPOSE_PROJECT_PATH`.\n\nEdit in 2025: As the compose file can be named `docker-compose.yml` or `compose.yaml` this enhancement allows you to use the control commands in a script which is targeting multiple compose projects independently of each choice for the compose file name.",
      "solution": "> while we shut into our own foot with this as this triggered a huge amount of issues\n\nI hope you didn't had to introduce workarounds for it and it was just cleaning up internal structures. You also have to judge whether my request is valid in terms of the introduction of complexity and maintenance burden. But I'd of course be happy if it could happen :-)",
      "labels": [
        "kind/feature"
      ],
      "created_at": "2024-10-09T19:47:16Z",
      "closed_at": "2024-10-14T06:49:54Z",
      "url": "https://github.com/docker/compose/issues/12200",
      "comments_count": 8
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13354,
      "title": "Propose OCI-packaging changes",
      "problem": "### Description\n\nI'd like to propose some changes in the OCI-packaging format implemented in #\n\nContainer image packaging looks like this:\n\n```mermaid\n---\n  config:\n    class:\n      hideEmptyMembersBox: true\n---\nclassDiagram\n    namespace ContainerImages {\n        class mai[\"Multi-Arch Container Image\"]:::ociIndex {\n            <<OCI index>>\n            manifests\n        }\n\n        class x86i[\"x86-64 Container Image\"]:::ociManifest {\n            <<OCI manifest>>\n            config\n            layers\n        }\n\n        class armi[\"ARM64 Container Image\"]:::ociManifest {\n            <<OCI manifest>>\n            config\n            layers\n        }    \n    }\n\n%%    namespace Compositions {\n%%        class ci[\"Multi-Arch Composition\"]:::ociIndex {\n%%           <<OCI index>>\n%%            subject\n%%            manifests\n%%        }\n\n%%        class cx86m[\"x86-64 Composition\"]:::ociManifest {\n%%            <<OCI manifest>>\n%%            subject\n%%            layers\n%%        }\n\n%%        class compx86[\"x86-64 compose.yaml\"] {\n%%            <<OCI blob>>\n%%        }\n\n%%        class carmm[\"ARM64 Composition\"]:::ociManifest {\n%%            <<OCI manifest>>\n%%            subject\n%%            layers\n%%        }\n\n%%        class comparm[\"ARM64 compose.yaml\"] {\n%%            <<OCI blob>>\n%%        }\n%%    }\n\n    namespace Tags {\n        class latestIm[\"latest\"]:::ociTag {\n            <<OCI tag>>\n        }\n\n%%        class latestComp[\"latest-composition\"]:::ociTag {\n%%            <<OCI tag>>\n%%        }        \n    }\n\n    classDef ociTag fill:#09f,stroke:#444,stroke-width:4px;\n    classDef ociIndex fill:#f9f,stroke:#444,stroke-width:4px;\n    classDef ociManifest fill:#f9a,stroke:#444,stroke-width:2px;\n\n    latestIm --> mai : digest\n    mai o-- x86i : manifest\n    mai o-- armi : manifest\n%%    latestComp --> ci : digest\n%%    ci o-- cx86m : manifest\n%%    ci o-- carmm : manifest\n%%    ci --> mai : subject\n%%    cx86m o-- compx86 : layer\n%%    cx86m --> x86i : subject\n%%    carmm o-- comparm : layer\n%%    carmm --> armi : subject\n```\n---\n\u203c\ufe0f **IMPORTANT** \u203c\ufe0f\n\nFollowing proposal enables:\n1. \"attaching\" a `compose.yaml` to an existing container image without modifying any of its OCI `index` or `manifest`s\n2. composing multiple container images copying them to a single OCI repository, will be called \"bundled composition\" and is similar to a phone app\n3. composing multiple container images contained in other OCI repositories, will be called \"loose composition\" and is similar to a Helm Chart\n\n---\n\n## Bundled Composition\n\nThis would be the case when the **presence of all the needed parts needs to be guaranteed by the OCI registry**. But also when \"attaching\" a `compose.yaml` to an existing container image.\n\nIt modifies the OCI-reference to the container images, but not the images themselves. Therefore it is changing the `compose.yaml` file.\n\nIt provides better guarantees that other container-based packages like \"Helm Charts\" and therefore it can be seen as the \"appification\" of a composition. The availability of the OCI repository guarantees the availability of the application.\n\nHere is where the beauty of the `subject` jumps in:\n```mermaid\n---\n  config:\n    class:\n      hideEmptyMembersBox: true\n---\nclassDiagram\n    namespace ContainerImages {\n        class mai[\"Multi-Arch Container Image\"]:::ociIndex {\n            <<OCI index>>\n            manifests\n        }\n\n        class x86i[\"x86-64 Container Image\"]:::ociManifest {\n            <<OCI manifest>>\n            config\n            layers\n        }\n\n        class armi[\"ARM64 Container Image\"]:::ociManifest {\n            <<OCI manifest>>\n            config\n            layers\n        }    \n    }\n\n    namespace Compositions {\n%%        class ci[\"Multi-Arch Composition\"]:::ociIndex {\n%%           <<OCI index>>\n%%            subject\n%%            manifests\n%%        }\n\n        class cx86m[\"x86-64 Composition\"]:::ociManifest {\n            <<OCI manifest>>\n            subject\n            layers\n        }\n\n        class compx86[\"x86-64 compose.yaml\"] {\n            <<OCI blob>>\n        }\n\n%%        class carmm[\"ARM64 Composition\"]:::ociManifest {\n%%            <<OCI manifest>>\n%%            subject\n%%            layers\n%%        }\n\n%%        class comparm[\"ARM64 compose.yaml\"] {\n%%            <<OCI blob>>\n%%        }\n    }\n\n    namespace Tags {\n        class latestIm[\"latest\"]:::ociTag {\n            <<OCI tag>>\n        }\n\n%%        class latestComp[\"latest-composition\"]:::ociTag {\n%%            <<OCI tag>>\n%%        }        \n    }\n\n    classDef ociTag fill:#09f,stroke:#444,stroke-width:4px;\n    classDef ociIndex fill:#f9f,stroke:#444,stroke-width:4px;\n    classDef ociManifest fill:#f9a,stroke:#444,stroke-width:2px;\n\n    latestIm --> mai : digest\n    mai o-- x86i : manifest\n    mai o-- armi : manifest\n%%    latestComp --> ci : digest\n%%    ci o-- cx86m : manifest\n%%    ci o-- carmm : manifest\n%%    ci --> mai : subject\n    cx86m o-- compx86 : layer\n    cx86m --> x86i : subject\n%%    carmm o-- comparm : layer\n%%    carmm --> armi : subject\n```\n\nYou wouldn't even need to add a tag to the compose (though you could). The referrers API (or its fallback) would let you \"discover\" the composition querying for the container image.\n\nBut what if you want to provide **multi-arch compositions** for a multi-arch image? You simply apply the same pattern:\n\n```mermaid\n---\n  config:\n    class:\n      hideEmptyMembersBox: true\n---\nclassDiagram\n    namespace ContainerImages {\n        class mai[\"Multi-Arch Container Image\"]:::ociIndex {\n            <<OCI index>>\n            manifests\n        }\n\n        class x86i[\"x86-64 Container Image\"]:::ociManifest {\n            <<OCI manifest>>\n            config\n            layers\n        }\n\n        class armi[\"ARM64 Container Image\"]:::ociManifest {\n            <<OCI manifest>>\n            config\n            layers\n        }    \n    }\n\n    namespace Compositions {\n        class ci[\"Multi-Arch Composition\"]:::ociIndex {\n           <<OCI index>>\n            subject\n            manifests\n        }\n\n        class cx86m[\"x86-64 Composition\"]:::ociManifest {\n            <<OCI manifest>>\n            subject\n            layers\n        }\n\n        class compx86[\"x86-64 compose.yaml\"] {\n            <<OCI blob>>\n        }\n\n        class carmm[\"ARM64 Composition\"]:::ociManifest {\n            <<OCI manifest>>\n            subject\n            layers\n        }\n\n        class comparm[\"ARM64 compose.yaml\"] {\n            <<OCI blob>>\n        }\n    }\n\n    namespace Tags {\n        class latestIm[\"latest\"]:::ociTag {\n            <<OCI tag>>\n        }\n\n%%        class latestComp[\"latest-composition\"]:::ociTag {\n%%            <<OCI tag>>\n%%        }        \n    }\n\n    classDef ociTag fill:#09f,stroke:#444,stroke-width:4px;\n    classDef ociIndex fill:#f9f,stroke:#444,stroke-width:4px;\n    classDef ociManifest fill:#f9a,stroke:#444,stroke-width:2px;\n\n    latestIm --> mai : digest\n    mai o-- x86i : manifest\n    mai o-- armi : manifest\n%%    latestComp --> ci : digest\n    ci o-- cx86m : manifest\n    ci o-- carmm : manifest\n    ci --> mai : subject\n    cx86m o-- compx86 : layer\n    cx86m --> x86i : subject\n    carmm o-- comparm : layer\n    carmm --> armi : subject\n```\n\nNo matter which image reference you have (tag to the index, index digest, arch-specific manifest digest,...) you can always discover the corresponding composition.\n\nFinally a **tag** can be added also to the composition, providing two different entry points:\n1. only for the container image\n2. for the container image with a `compose.yaml` providing some help to run a container using the image\n\n```mermaid\n---\n  config:\n    class:\n      hideEmptyMembersBox: true\n---\nclassDiagram\n    namespace ContainerImages {\n        class mai[\"Multi-Arch Container Image\"]:::ociIndex {\n            <<OCI index>>\n            manifests\n        }\n\n        class x86i[\"x86-64 Container Image\"]:::ociManifest {\n            <<OCI manifest>>\n            config\n            layers\n        }\n\n        class armi[\"ARM64 Container Image\"]:::ociManifest {\n            <<OCI manifest>>\n            config\n            layers\n        }    \n    }\n\n    namespace Compositions {\n        class ci[\"Multi-Arch Composition\"]:::ociIndex {\n           <<OCI index>>\n            subject\n            manifests\n        }\n\n        class cx86m[\"x86-64 Composition\"]:::ociManifest {\n            <<OCI manifest>>\n            subject\n            layers\n        }\n\n        class compx86[\"x86-64 compose.yaml\"] {\n            <<OCI blob>>\n        }\n\n        class carmm[\"ARM64 Composition\"]:::ociManifest {\n            <<OCI manifest>>\n            subject\n            layers\n        }\n\n        class comparm[\"ARM64 compose.yaml\"] {\n            <<OCI blob>>\n        }\n    }\n\n    namespace Tags {\n        class latestIm[\"latest\"]:::ociTag {\n            <<OCI tag>>\n        }\n\n        class latestComp[\"latest-composition\"]:::ociTag {\n            <<OCI tag>>\n        }        \n    }\n\n    classDef ociTag fill:#09f,stroke:#444,stroke-width:4px;\n    classDef ociIndex fill:#f9f,stroke:#444,stroke-width:4px;\n    classDef ociManifest fill:#f9a,stroke:#444,stroke-width:2px;\n\n    latestIm --> mai : digest\n    mai o-- x86i : manifest\n    mai o-- armi : manifest\n    latestComp --> ci : digest\n    ci o-- cx86m : manifest\n    ci o-- carmm : manifest\n    ci --> mai : subject\n    cx86m o-- compx86 : layer\n    cx86m --> x86i : subject\n    carmm o-- comparm : layer\n    carmm --> armi : subject\n```\n\n## Loose Composition\n\nIn this case the presence of the needed container images is not guaranteed by the OCI registry. It is equivalent to a \"Helm Chart\" in that sense. If an OCI repository hosting one of the required container images is deleted, the composition is broken.\n\nIt does not change the `compose.yaml` file in any way.\n\nThe references to the container images are not explicit in the OCI artifact, but \"embedded\" in the `compose.yaml` file and in the `image-digests.yaml` file (which is used to ensure the integrity of the images).\n\n```mermaid\n---\n  config:\n    class:\n      hideEmptyMembersBox: true\n---\nclassDiagram\n    namespace Image1 {\n        class mai[\"Multi-Arch Container Image\"]:::ociIndex {\n            <<OCI index>>\n            manifests\n        }\n\n        class x86i[\"x86-64 Container Image\"]:::ociManifest {\n            <<OCI manifest>>\n            config\n            layers\n        }\n\n        class armi[\"ARM64 Container Image\"]:::ociManifest {\n            <<OCI manifest>>\n            config\n            layers\n        }    \n\n        class latestIm[\"latest\"]:::ociTag {\n            <<OCI tag>>\n        }\n    }\n\n    namespace CompositionRepository {\n        class ci[\"Multi-Arch Composition\"]:::ociIndex {\n           <<OCI index>>\n            subject\n            manifests\n        }\n\n        class cx86m[\"x86-64 Composition\"]:::ociManifest {\n            <<OCI manifest>>\n            subject\n            layers\n        }\n\n        class compx86[\"x86-64 compose.yaml\"] {\n            <<OCI blob>>\n        }\n\n        class idx86[\"x86-64 image-digests.yaml\"] {\n            <<OCI blob>>\n        }\n\n        class carmm[\"ARM64 Composition\"]:::ociManifest {\n            <<OCI manifest>>\n            subject\n            layers\n        }\n\n        class comparm[\"ARM64 compose.yaml\"] {\n            <<OCI blob>>\n        }\n\n        class idarm[\"ARM64 image-digests.yaml\"] {\n            <<OCI blob>>\n        }\n\n        class latestComp[\"latest\"]:::ociTag {\n            <<OCI tag>>\n        }        \n    }\n\n    classDef ociTag fill:#09f,stroke:#444,stroke-width:4px;\n    classDef ociIndex fill:#f9f,stroke:#444,stroke-width:4px;\n    classDef ociManifest fill:#f9a,stroke:#444,stroke-width:2px;\n\n    latestIm --> mai : digest\n    mai o-- x86i : manifest\n    mai o-- armi : manifest\n    latestComp --> ci : digest\n    ci o-- cx86m : manifest\n    ci o-- carmm : manifest\n    cx86m o-- compx86 : layer\n    cx86m o-- idx86 : layer\n    carmm o-- comparm : layer\n    carmm o-- idarm : layer\n    compx86 ..> x86i\n    idx86 ..> x86i\n    comparm ..> armi\n    idarm ..> armi\n```",
      "solution": "about multi-arch: why refer by subject to platform specific images and not the multi-plaform index-list ? Doing so you don't need platform-specific compose.yaml (which would have the exact same content)\n\n\"Loose Composition\" you describe is what we have when a compose file is published without the `--app` flag, but relying on index-list for multi-platform\n\n\"Bundled Composition\" you describe allows a compose OCI artifact to be subject of an image but not a set of images, as required by a compose application. Could use a top-level index-list to group such images, but then you basically get the same solution as `publish --app` with subject relation reverted.\n\nThe main issue I can see with your approach is that the application tag doesn't target the actual compose OCI artifact. Then there's no way to refer to it for a consumer to use your \"bundle\" application.\n\n\n---\n\n> about multi-arch: why refer by subject to platform specific images and not the multi-plaform index-list ? Doing so you don't need platform-specific compose.yaml (which would have the exact same content)\n\nHaving platform-specific compose.yaml would be optional. Ideally it's not needed. But it might be needed under certain circumstances. If all platform-specific manifest are in fact the very same, then it's only explicitly stating the supported platform without any need for duplication of the compose.yaml (what is anyway impossible with the OCI CAS).\n\n> \"Loose Composition\" you describe is what we have when a compose file is published without the `--app` flag, but relying on index-list for multi-platform\n\nI might have overseen the details. I thought that no OCI-artifact would be created without the `--app` flag. If the result without the `--app` flag is a manifest + compose.yaml, then it's almost the same. Having a similar approach to the container images one (you can have only a manifest or an index to have multi-platform) would be the only addition of my proposal.\n\n> \"Bundled Composition\" you describe allows a compose OCI artifact to be subject of an image but not a set of images, as required by a compose application. Could use a top-level index-list to group such images, but then you basically get the same solution as `publish --app` with subject relation reverted.\n\nI realize that while writing I've merged what I had as two different use-cases in my head:\n1. Add a compose.yaml to an existing container image as some sort of documentation or easy to consume execution format. I should rename it to \"Attached Compose\".\n2. What I would also call somehow an \"app\", which what I would say most of us expect from an app. I should simply rename it to \"App\".\n\nIn the \"Attached Compose\" the subject is not the compose.yaml, but the other way around.\n\nIn the case of an \"App\" you have a very good point: you can only have a single subject per referrer. As a consequence it should not be a referrer-subject relationship, but an index referring to the index/manifest of the compose.yaml and to the manifests/indexes of the individual container images.\n\nIn any case, I'm still convinced that making the container images referrers to the compose.yaml (which becomes their subject) is not the right approach. It forces you to either add a new index to contain the `Subject` or modify the already existing image index/manifest (changing its digest). \n\n> The main issue I can see with your approach is that the application tag doesn't target the actual compose OCI artifact. Then there's no way to refer to it for a consumer to use your \"bundle\" application.\n\nIn the \"Attached Compose\" I would see the container image as THE focus (or subject) of the OCI artifact. And therefore the typical tags (`latest`, versions,...) are kept associated to it. Tags associated to such an attachment as the compose.yaml would be just some convenience that is not really needed if referrers can be listed.\n\nIn the \"App\" I fully agree with you. THE focus (or subject) of the OCI artifact is the composition itself and therefore it gets the tags.\n\n---\n\n> I'm closing this issue as \"not planned\"\n\n@ndeloof if you leave it as it is, you are creating an cyclic dependency! I'll try to illustrate what I mean with an example.\n\nLet's suppose that we have a composition using *Grafana v12.1.4* (current index digest is `sha256:553160ed532b89bac217ffd424c2d7fb62a1dfbc49685d29993795b77e9cea9a`).\nIf you create a `compose.yaml` with \"digest pinning\", then you'd write `image: grafana@sha256:553160ed532b89bac217ffd424c2d7fb62a1dfbc49685d29993795b77e9cea9a`. And the manifest referring to that `compose.yaml` index/manifest would be pinning it with its digest. But since you are putting that digest in the `Subject` of the container image index (or platform manifest), you are changing that index/manifest (so the container image pinning is now wrong). So you need to adapt the `compose.yaml`, which then gets a new digest which would require a change in the image `Subject`.",
      "labels": [
        "kind/feature"
      ],
      "created_at": "2025-11-05T12:33:38Z",
      "closed_at": "2025-11-25T15:52:02Z",
      "url": "https://github.com/docker/compose/issues/13354",
      "comments_count": 5
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13390,
      "title": "[BUG] Project directory ignored if using an OCI artifact",
      "problem": "### Description\n\nIt appears that the `--project-directory` flag is ignored if the first Compose file is an OCI artifact.\n\nWith this, it causes the project directory to _always_ be the cache folder for the OCI artifact.\n\n### Steps To Reproduce\n\n1. Create a compose.yaml file with the following contents:\n\n    ```yaml\n    services:\n      configurator:\n        volumes:\n          - ./:/workspaces\n    ```\n\n2. Run the following command to get the config:\n\n    ```bash\n    docker compose -f oci://dockersamples/labspace -f ./compose.yaml --project-directory $PWD config\n    ```\n\n3. Notice in the output that the mount source is pointing to the compose cache directory, not the current working directory.\n\n### Compose Version\n\n```Text\nDocker Compose version v2.40.3-desktop.1\n```\n\n### Docker Environment\n\n```Text\nClient:\n Version:    29.0.1\n Context:    desktop-linux\n Debug Mode: false\n Plugins:\n  ai: Docker AI Agent - Ask Gordon (Docker Inc.)\n    Version:  v1.9.11\n    Path:     /Users/mikesir87/.docker/cli-plugins/docker-ai\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.29.1-desktop.1\n    Path:     /Users/mikesir87/.docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.3-desktop.1\n    Path:     /Users/mikesir87/.docker/cli-plugins/docker-compose\n  debug: Get a shell into any image or container (Docker Inc.)\n    Version:  0.0.45\n    Path:     /Users/mikesir87/.docker/cli-plugins/docker-debug\n  desktop: Docker Desktop commands (Docker Inc.)\n    Version:  v0.2.0\n    Path:     /Users/mikesir87/.docker/cli-plugins/docker-desktop\n  extension: Manages Docker extensions (Docker Inc.)\n    Version:  v0.2.31\n    Path:     /Users/mikesir87/.docker/cli-plugins/docker-extension\n  init: Creates Docker-related starter files for your project (Docker Inc.)\n    Version:  v1.4.0\n    Path:     /Users/mikesir87/.docker/cli-plugins/docker-init\n  mcp: Docker MCP Plugin (Docker Inc.)\n    Version:  v0.28.0\n    Path:     /Users/mikesir87/.docker/cli-plugins/docker-mcp\n  model: Docker Model Runner (Docker Inc.)\n    Version:  v1.0.0\n    Path:     /Users/mikesir87/.docker/cli-plugins/docker-model\n  offload: Docker Offload (Docker Inc.)\n    Version:  v0.5.23\n    Path:     /Users/mikesir87/.docker/cli-plugins/docker-offload\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\n    Version:  0.6.0\n    Path:     /Users/mikesir87/.docker/cli-plugins/docker-sbom\n  scout: Docker Scout (Docker Inc.)\n    Version:  v1.18.3\n    Path:     /Users/mikesir87/.docker/cli-plugins/docker-scout\n\nServer:\n Containers: 9\n  Running: 0\n  Paused: 0\n  Stopped: 9\n Images: 268\n Server Version: 29.0.1\n Storage Driver: overlayfs\n  driver-type: io.containerd.snapshotter.v1\n Logging Driver: json-file\n Cgroup Driver: cgroupfs\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Discovered Devices:\n  cdi: docker.com/gpu=webgpu\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc version: v1.3.3-0-gd842d771\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.12.54-linuxkit\n Operating System: Docker Desktop\n OSType: linux\n Architecture: aarch64\n CPUs: 16\n Total Memory: 7.652GiB\n Name: docker-desktop\n ID: 8559055f-16ea-45ca-9fe3-a168e6debd51\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n HTTP Proxy: http.docker.internal:3128\n HTTPS Proxy: http.docker.internal:3128\n No Proxy: hubproxy.docker.internal\n Labels:\n  com.docker.desktop.address=unix:///Users/mikesir87/Library/Containers/com.docker.docker/Data/docker-cli.sock\n Experimental: false\n Insecure Registries:\n  hubproxy.docker.internal:5555\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\n\n### Anything else?\n\n_No response_",
      "solution": "Ok and makes sense. Thanks for digging into it! \n\nJust to document in case others come across this in the future, this occurs regardless where the OCI artifact is in the list of the project files (and I've updated the title of the issue to reflect this). If an OCI artifact is used _at all_, it will set the project directory to the artifact's cache folder.\n\nI did confirm that the using `$PWD` does set it correctly based on the location of the compose file, so this workaround would work. \ud83d\udc4d ",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2025-11-21T20:44:40Z",
      "closed_at": "2025-11-25T15:07:50Z",
      "url": "https://github.com/docker/compose/issues/13390",
      "comments_count": 2
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13391,
      "title": "[BUG] container_name repeatedly prefixes with IDs on `failed` reconciliation",
      "problem": "### Description\n\n```sh\n-bash-5.1# docker ps\n     \"/bin/manager -metri\u2026\"   12 days ago         Up About a minute (healthy)                       7481b178e4c1_7481b178e4c1_7481b178e4c1_<repeated>__7481b178e4c1_<project>_<service>_1\n```\n\nI am getting this error whenever I am trying to apply a compose manifest. \nThe manifest fails to get applied in my case since I have a mount path missing. but this is a reconcile job thats running. this runs every 15m and that bumps the number of container id in the name by 1. So container goes unhealthy and healthy as well.\n\n### Steps To Reproduce\n\n_No response_\n\n### Compose Version\n\n```Text\n-bash-5.1# docker compose version\nDocker Compose version v2.17.3\n```\n\n### Docker Environment\n\n```Text\n-bash-5.1# docker info\nClient:\n Debug Mode: false\n Plugins:\n  compose: Docker Compose (Docker Inc., v2.17.3)\n\nServer:\n Containers: 13\n  Running: 13\n  Paused: 0\n  Stopped: 0\n Images: 22\n Server Version: 20.10.11\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: cgroupfs\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 1e5ef943eb76627a6d3b6de8cd1ef6537f393a71\n runc version: 52b36a2dd837e8462de8e01458bf02cf9eea47dd\n init version:\n Security Options:\n  apparmor\n  seccomp\n   Profile: default\n Kernel Version: 5.15.0-135-fips\n Operating System: Alpine Linux v3.15\n OSType: linux\n Architecture: x86_64\n CPUs: 8\n Total Memory: 30.58GiB\n Name: localhost\n ID: QJKB:H56T:Z47O:37VO:4LA7:SDP3:DEHY:S3SC:UB3L:S3KI:3W4P:4W7F\n Docker Root Dir: /rw/host/docker\n Debug Mode: false\n Registry: https://index.docker.io/v1/\n Labels:\n Experimental: false\n Insecure Registries:\n  127.0.0.0/8\n Live Restore Enabled: false\n\nWARNING: IPv4 forwarding is disabled\n```\n\n### Anything else?\n\nMore context: I am testing this as part of migration from docker-compose v1 to v2 [better late than never].\nmoving to a docker-compose 2.15.1 executable in our Dockerfile for compose up and down commands. The cluster itself has docker with compose version 2.17.x\n\nSo, one requirement I do have is maintaining the v1 container names: using _ instead of -. using compatibility flag for that. but this issue doesnt seem to be covered in this. and seems to have a different behaviour.\n\nThis is a temp move. We will eventually move to latest docker-compose. But given the amount of resources that this will affect, I want minimal disruption/change from existing behavior.\n\nreason for going with 2.15.1 is because the newer versions of compose throw an error when manifest has volumeoptions with Bind volume. We need to gradually fix it on our manifests. ",
      "solution": "https://github.com/docker/compose/blob/30529346241086fb872b3d2321214e08a22e0042/pkg/compose/convergence.go#L617-L677\n\nLooking at the codebase: this might be the code in question? \nSo, if no fix is possible: is a workaround that I rename the new container to the correct name? still exploring the codebase.\n\n---\n\nThis issue is a corner case. To preserve container names (a legacy compose features, I assume many rely on this) we need to rename the existing container to a temporary name before we create its replacement. If something fails, this rename container persists. We _could_ rename back to original name on failure, or use a complete random name to avoid this issue.\n\nSimplest workaround here is for you to rename container once your setup is fixed",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2025-11-24T07:18:57Z",
      "closed_at": "2025-11-24T17:07:21Z",
      "url": "https://github.com/docker/compose/issues/13391",
      "comments_count": 5
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13374,
      "title": "[BUG] `docker compose images` returns weird values in CREATED column",
      "problem": "### Description\n\nThe `docker compose images` command returns creation ages of \"292 years ago\" for any images pulled direct from remote repositories like DockerHub.\n\nImages built with local Dockerfiles show correct ages.\n\n### Steps To Reproduce\n\n1. The output of `docker images` shows creation ages ranging from 41 hours to 8 weeks ago.\n\n\t```\n\t$ docker images --no-trunc\n\tREPOSITORY                 TAG         IMAGE ID                                                                  CREATED        SIZE\n\tjc21/nginx-proxy-manager   latest      sha256:7cfcc87a404a315bfe251fadbbcf28f680902ad1742bee2a734168791aef749b   41 hours ago   1.12GB\n\tinfluxdb                   1.11        sha256:596d3f4212b258a82e7fe70c581cb7c9bd56c5ab7aba92b2338a1b557ec2b566   8 days ago     309MB\n\tadguard/adguardhome        latest      sha256:ff0055dbca88da616009bb21e7a9af503713754778ec120f953e00e85bee75c8   13 days ago    72.6MB\n\tiotstack-nodered           latest      sha256:8fac966ca3b517cd6dc64dfabd98ce15f6a6d485200fa98fd46021920073cc4d   3 weeks ago    666MB\n\tiotstack-mosquitto         latest      sha256:b24b0978bc8490c33b73e016cf8867133c0ec3ea9a60bf9c681fa7af2376b989   3 weeks ago    14.9MB\n\tgrafana/grafana            latest      sha256:52def22e4440f1ad747ea10b77aff83d82c888b0aac48f0d70928ac45f72ce5b   3 weeks ago    685MB\n\tzyclonite/zerotier         router      sha256:e17c886c4aa13da7a551887520c62901ae22558ab1da3dd5b354e13025d555fd   8 weeks ago    23.8MB\n\t```\n\n2. The output of `docker compose images` agrees with `docker images` for locally-built images (those with the `iotstack-` prefix) but has the wonky value of 292 years ago for anything pulled direct from DockerHub etc.\n\n\t```\n\t$ docker compose images\n\tCONTAINER           REPOSITORY                 TAG                 PLATFORM            IMAGE ID            SIZE                CREATED\n\tadguardhome         adguard/adguardhome        latest              linux/arm64         ff0055dbca88        72.6MB              292 years ago\n\tgrafana             grafana/grafana            latest              linux/arm64         52def22e4440        685MB               292 years ago\n\tinfluxdb            influxdb                   1.11                linux/arm64/v8      596d3f4212b2        309MB               292 years ago\n\tmosquitto           iotstack-mosquitto         latest              linux/arm64         b24b0978bc84        14.9MB              3 weeks ago\n\tnginx               jc21/nginx-proxy-manager   latest              linux/arm64         7cfcc87a404a        1.12GB              292 years ago\n\tnodered             iotstack-nodered           latest              linux/arm64         8fac966ca3b5        666MB               3 weeks ago\n\tzerotier            zyclonite/zerotier         router              linux/arm64/v8      e17c886c4aa1        23.8MB              292 years ago\n\t```\n\n\n### Compose Version\n\n```Text\n$ docker compose version\nDocker Compose version v2.40.3\n$ docker-compose version\nDocker Compose version v2.40.3\n```\n\n### Docker Environment\n\n```Text\n$ docker info\nClient: Docker Engine - Community\n Version:    29.0.0\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.29.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.3\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 8\n  Running: 8\n  Paused: 0\n  Stopped: 0\n Images: 12\n Server Version: 29.0.0\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: local\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc version: v1.3.3-0-gd842d771\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.1.21-v8+\n Operating System: Debian GNU/Linux 11 (bullseye)\n OSType: linux\n Architecture: aarch64\n CPUs: 4\n Total Memory: 7.628GiB\n Name: iot-hub\n ID: AKTF:SPT5:YKMH:WV5G:ZKZY:XZI5:ELD5:Z6IV:KVUJ:USCV:5YDG:JPTZ\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  127.0.0.0/8\n  ::1/128\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\n\n### Anything else?\n\n_No response_",
      "solution": "The issue here seems to be Compose relied on `LastTagTime` (i.e. time image was pulled) which isn't relevant for actual image creation time.\n\n`compose images` unfortunately doesn't use the same code as docker/cli, I expect some mismatch will persist with the equivalent `docker images` command.\n",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2025-11-13T00:46:42Z",
      "closed_at": "2025-11-13T08:40:57Z",
      "url": "https://github.com/docker/compose/issues/13374",
      "comments_count": 1
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 12594,
      "title": "[BUG] Unable to detach docker compose up after upgrading docker client",
      "problem": "### Description\n\nWith previous version, we run \"docker compose up\" to check the logs, ensure everything works fine, and press \"ctrl-z\" to detach the docker compose output and leave the server to run.\n\nHowever, after upgrading the docker client, the docker compose up takes away the control of keyboard and leaves \"w - Enable Watch\" hint. It doesn't allow user to detach the output anymore with \"ctrl-z\" or \"ctrl-p\" + \"ctrl-q\". The only way to leave the container running is to close and reopen the terminal, which brings inconvinient to daily developing process. \n\n### Steps To Reproduce\n\n1. docker compose up\n2. see w Enable Watch\n3. try press \"ctrl-z\" or \"ctrl-p\" + \"ctrl-q\" nothing happens (should leave the container running and detach the output and return user to shell)\n4. no other keys are accept except \"ctrl-c\" (stop) or \"w\" (nothing happens) \n\n### Compose Version\n\n```Text\nDocker Compose version v2.29.7\n```\n\n### Docker Environment\n\n```Text\n\n```\n\n### Anything else?\n\n_No response_",
      "solution": "A workaround is for you to run `docker compose up --menu=false` or set `COMPOSE_MENU=false`\n\n---\n\nAfter some investigation, when menu is enabled keyboard events are captured by Compose (bypassing your shell) so `Ctrl+Z` doesn't result into a SIGSTOP signal being send to process. While Compose _can_ then detect this keyboard combination, there's no portable way to reproduce a process suspend routine as implemented by Go runtime. Sending SIGSTP internally to self just results into current go thread to hang, without returning to shell.\n\nI'll close this issue as \"not planned\" as there's no portable solution. I hope proposed workaround works for you\n\n---\n\n> Can't at least the detach sequence be supported or something?\n\nno, a process can't send itself to background, this is a shell feature. As menu set terminal in \"raw\" mode, shell can't capture Ctrl+Z anymore. See https://github.com/docker/compose/issues/12594#issuecomment-2690708645 for workaround",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2025-02-28T08:41:12Z",
      "closed_at": "2025-11-12T13:24:00Z",
      "url": "https://github.com/docker/compose/issues/12594",
      "comments_count": 16
    },
    {
      "tech": "docker",
      "repo": "docker/compose",
      "issue_number": 13340,
      "title": "Support env-var interpolation for boolean-typed fields (e.g., depends_on.*.required) or provide a cast syntax",
      "problem": "### Description\n\nToday, Docker Compose variable interpolation substitutes strings before schema validation. When a field is typed as boolean (like services.<name>.depends_on.<dep>.required), using ${VAR} causes a validation error because Compose sees a string, not a boolean:\n\n`validating compose.yaml: services.postgresql.depends_on.grafana.required must be a boolean`\n\nThis prevents toggling optional dependencies via env vars\u2014an otherwise common and very useful pattern.\n\nWhy this matters\n\ndepends_on.*.required is a boolean with default true (introduced around v2.20.x). It\u2019s perfect for making a dependency optional when a service is gated by a flag or profile. But you can\u2019t drive it from an env var today without preprocessing. \n[Docker Documentation](https://docs.docker.com/reference/compose-file/services/?utm_source=chatgpt.com)\n\nCompose officially supports env-var interpolation from .env, shell, and --env-file, but the current behavior doesn\u2019t allow those values to be typed when the schema expects booleans. \nDocker Documentation\n+1\n\nProfiles are great (and we use them), but there are many real cases where you want a single compose file and a simple ENABLE_X=true/false switch, especially for CI jobs and local toggles. \n[Docker Documentation](https://docs.docker.com/compose/how-tos/profiles/?utm_source=chatgpt.com)\n\nMinimal Reproducer\n\n.env\n```\nENABLE_GRAFANA=true\n```\n\ncompose.yaml\n```yaml\nservices:\n  grafana:\n    image: grafana/grafana:latest\n    healthcheck:\n      test: [\"CMD-SHELL\", \"wget -qO- http://localhost:3000/api/health || exit 1\"]\n\n  postgresql:\n    image: postgres:17\n    depends_on:\n      grafana:\n        condition: service_healthy\n        required: ${ENABLE_GRAFANA:-false}  # \u2190 want boolean from env\n```\n\nRun\n```\ndocker compose config\n```\n\nActual\n```\nvalidating compose.yaml: services.postgresql.depends_on.grafana.required must be a boolean\n```\n\nExpected\n```\nWith ENABLE_GRAFANA=true \u2192 required: true\n\nWith ENABLE_GRAFANA=false or unset \u2192 required: false\n```\nProposal(s)\n\nAny of the following would solve it without breaking existing configs:\n\nBoolean coercion for typed fields\nAfter interpolation, if a field is schema-typed as boolean, treat \"true\"/\"false\" (case-insensitive) as booleans; otherwise error.\n\nOpt-in cast syntax\nAdd a cast operator for interpolation, e.g. ${ENABLE_GRAFANA?bool} (or ${{ bool(ENABLE_GRAFANA) }}), producing a true boolean, not a string.\n\nHonor YAML tag casts post-interpolation\nAllow required: !!bool ${ENABLE_GRAFANA:-false} to be parsed as boolean. Currently this still errors because validation happens before/without a cast being applied.\n\nBackward compatibility\n\nCoerce only when the schema type is boolean; do not coerce in environment: (those are strings). This avoids historic yes/no pitfalls and keeps behavior predictable. Docs can note accepted values are exactly true|false. \n[Docker Documentation](https://docs.docker.com/compose/how-tos/environment-variables/variable-interpolation/?utm_source=chatgpt.com)\n\nEnvironment\n\nDocker Compose: v2.34.0 (repro also reported by others since v2.20.x)\n\nOS: Linux (x86_64)\n\nCompose file uses the current Compose Specification. depends_on.required boolean is documented in service reference. \n[Docker Documentation](https://docs.docker.com/reference/compose-file/services/?utm_source=chatgpt.com)\n\nRelated docs\n\nInterpolation: how env vars are injected into Compose files. \n[Docker Documentation](https://docs.docker.com/compose/how-tos/environment-variables/variable-interpolation/?utm_source=chatgpt.com)\n\nProfiles: recommended for optional services (workaround, not a direct fix). \n[Docker Documentation](https://docs.docker.com/compose/how-tos/profiles/?utm_source=chatgpt.com)\n\nService reference (notes required default/behavior). \n[Docker Documentation](https://docs.docker.com/reference/compose-file/services/?utm_source=chatgpt.com)\n\nImpact\n\nThis unlocks simple, single-file toggles such as:\n\nOptional observability stack (ENABLE_GRAFANA, ENABLE_PROMTAIL)\n\nOptional seed/init jobs (REQUIRE_BOOTSTRAP)\n\nFeature-gated infra bits in CI (USE_MINIO, USE_LOCAL_REGISTRY)\n\nThanks for considering! Happy to test a PR or nightly build.",
      "solution": "fixed by https://github.com/compose-spec/compose-go/pull/832",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2025-10-31T17:46:37Z",
      "closed_at": "2025-11-06T07:10:57Z",
      "url": "https://github.com/docker/compose/issues/13340",
      "comments_count": 1
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 6733,
      "title": "unknown flag: --pretty and unknown command - trust as per the new docker version 29.1.3",
      "problem": "### Description\n\nWe have installed the latest version of Docker (29.1.3) in our environment. However, some commands are not functioning as expected in this version. The error messages provided are quite clear, but these issues are not mentioned in the official documentation or release notes.\n\n**root@virtnotary:~# docker trust inspect --pretty hostnameXXXXX:5000/docker-release-local/alpine\nunknown flag: --pretty\n\nUsage:  docker [OPTIONS] COMMAND [ARG...]\n\nRun 'docker --help' for more information**\n\n**root@virtnotary:~# docker trust inspect hostnameXXXXX:5000/docker-release-local/alpine\ndocker: unknown command: docker trust\n\nRun 'docker --help' for more information**\n\nBelow is the docker version details\n\nroot@virtnotary:~# docker version\nClient: Docker Engine - Community\n Version:           29.1.3\n API version:       1.52\n Go version:        go1.25.5\n Git commit:        f52814d\n Built:             Fri Dec 12 14:49:32 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          29.1.3\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.5\n  Git commit:       fbf3ed2\n  Built:            Fri Dec 12 14:49:32 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v2.2.1\n  GitCommit:        dea7da592f5d1d2b7755e3a161be07f43fad8f75\n runc:\n  Version:          1.3.4\n  GitCommit:        v1.3.4-0-gd6d73eb8\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\nroot@virtnotary:~# \n\nWhile I am trying without trust and pretty flag does final output  is correct or not?\n\n### Reproduce\n\n1. docker pull (sudo -i docker pull XXXXXXXXXXXXXX/library/alpine)\n2. set the environment variable\n3.  set the tag (sudo -i docker tag XXXXXXXXXXXXXX/library/alpine hostnameXXXXX:5000/docker-release-local/alpine).\n4.  docker push (sudo -i docker push hostnameXXXXX:5000/docker-release-local/alpine)\n5. sudo -i docker rmi hostnameXXXXX:5000/docker-release-local/alpine\n6. sudo -i docker pull hostnameXXXXX:5000/docker-release-local/alpine\n7. sudo -i docker images\n8. docker trust inspect --pretty hostnameXXXXX:5000/docker-release-local/alpine\n\n### Expected behavior\n\nAs per the latest version, the functionality should be operational. If there have been any updates or changes to the Docker Trust commands, please let me know. I attempted to use docker trust inspect --pretty on the image hostnameXXXXX:5000/docker-release-local/alpine, but I couldn\u2019t find support for the docker trust command or the --pretty flag. Could you please share your thoughts on this?\n\nExpected Behavior:\nThe command docker trust inspect --pretty <image> should return detailed trust information for the specified image, including its signing status and metadata, in a readable format.\n\n### docker version\n\n```bash\n# docker version\nClient: Docker Engine - Community\n Version:           29.1.3\n API version:       1.52\n Go version:        go1.25.5\n Git commit:        f52814d\n Built:             Fri Dec 12 14:49:32 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          29.1.3\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.5\n  Git commit:       fbf3ed2\n  Built:            Fri Dec 12 14:49:32 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v2.2.1\n  GitCommit:        dea7da592f5d1d2b7755e3a161be07f43fad8f75\n runc:\n  Version:          1.3.4\n  GitCommit:        v1.3.4-0-gd6d73eb8\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    29.1.3\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v5.0.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 2\n  Running: 2\n  Paused: 0\n  Stopped: 0\n Images: 4\n Server Version: 29.1.3\n Storage Driver: overlayfs\n  driver-type: io.containerd.snapshotter.v1\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: dea7da592f5d1d2b7755e3a161be07f43fad8f75\n runc version: v1.3.4-0-gd6d73eb8\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n  no-new-privileges\n Kernel Version: 6.8.0-86-generic\n Operating System: Ubuntu 24.04.3 LTS\n OSType: linux\n Architecture: x86_64\n CPUs: 1\n Total Memory: 1.922GiB\n Name: XXX\n ID: 9f163212-bfe3-452c-a670-8fb30f496ba8\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n HTTP Proxy: http://XXX:XXXX\n HTTPS Proxy: http://XXX:XXXX\n No Proxy: localhost,127.0.0.1,XXX,XXX,XXX,hostname,hostname:5000\n Experimental: false\n Insecure Registries:\n  artifactory.muzo.com\n  docker-release.repo.globalpay.cz\n  docker-remote.repo.globalpay.cz\n  virtplatform\n  virtplatform.srv.gpe:5000\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: true\n Default Address Pools:\n   Base: 10.240.0.0/16, Size: 24\n Firewall Backend: iptables\n```\n\n### Additional Info\n\nCould you please confirm whether the docker trust command and the --pretty flag have been completely removed in the latest version?\nCould you also confirm if there is a new utility available to replace docker trust?",
      "solution": "Closing the issue, but feel free to continue the conversation.",
      "labels": [
        "kind/question",
        "area/trust",
        "version/29.1"
      ],
      "created_at": "2026-01-07T07:11:58Z",
      "closed_at": "2026-01-09T09:50:28Z",
      "url": "https://github.com/docker/cli/issues/6733",
      "comments_count": 4
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 6707,
      "title": "Docker CLI's new rendering breaks cursor visibility in Alacritty terminal",
      "problem": "### Description\n\nAfter recent Docker update, the cursor disappears in Alacritty terminal after executing any docker command. The cursor doesn't return after command completion, requiring manual intervention.\n\n```\n\u276f docker -v\nDocker version 29.1.3, build f52814d454\n```\n\n### Reproduce\n\n1. Use Alacritty terminal\n2. Run `docker compose up -d <anycontainer>`\n3. Observe that cursor disappears after command execution\n\n### Expected behavior\n\nCursor should remain visible or be properly restored after docker command completes.\n\n### docker version\n\n```bash\nClient:\n Version:           29.1.3\n API version:       1.52\n Go version:        go1.25.5 X:nodwarf5\n Git commit:        f52814d454\n Built:             Sun Dec 14 18:17:38 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer:\n Engine:\n  Version:          29.1.3\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.5 X:nodwarf5\n  Git commit:       fbf3ed25f8\n  Built:            Sun Dec 14 18:17:38 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v2.2.0\n  GitCommit:        1c4457e00facac03ce1d75f7b6777a7a851e5c41.m\n runc:\n  Version:          1.4.0\n  GitCommit:\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient:\n Version:    29.1.3\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  0.30.1\n    Path:     /usr/lib/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  dev\n    Path:     /usr/lib/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 17\n  Running: 7\n  Paused: 0\n  Stopped: 10\n Images: 77\n Server Version: 29.1.3\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: true\n  Native Overlay Diff: false\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 1c4457e00facac03ce1d75f7b6777a7a851e5c41.m\n runc version:\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.17.9-arch1-1\n Operating System: Arch Linux\n OSType: linux\n Architecture: x86_64\n CPUs: 16\n Total Memory: 27.05GiB\n Name: thinkpad\n ID: 8cda422f-b21f-45e3-a083-a96401a3556b\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Username: 39george\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\n\n### Additional Info\n\nWorkaround:\nManually restoring cursor with echo -e \"\\033[?25h\" works, but this shouldn't be necessary.",
      "solution": "known (fixed) issue: https://github.com/docker/compose/issues/13420",
      "labels": [
        "kind/bug",
        "status/0-triage"
      ],
      "created_at": "2025-12-15T10:50:19Z",
      "closed_at": "2025-12-15T15:39:12Z",
      "url": "https://github.com/docker/cli/issues/6707",
      "comments_count": 3
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 6674,
      "title": "Blank lines printed on stderr by `docker stack deploy` when using `--resolve-image always`",
      "problem": "### Description\n\n`docker stack deploy` prints **blank lines on stderr** when `--resolve-image always` (or `changed`) is used.  \nThis did **not** happen in docker-ce-cli 28.5.x and appeared after upgrading to 29.x.\n\nImpact:\n\n- Unwanted blank lines on stderr\n- Cron jobs and monitoring tools interpret this as failures\n\nThe bug is caused by **empty warning strings** (`\"\"`) returned from\n`resolveContainerSpecImage()` being appended without filtering from `moby/moby` at:\n\nhttps://github.com/moby/moby/blob/6e52828ec38b2f53f695b97ffb0b4ba94fa255e4/client/service_update.go#L84-L87\n\nhttps://github.com/moby/moby/blob/6e52828ec38b2f53f695b97ffb0b4ba94fa255e4/client/service_create.go#L95-L109\n\n\nThis leads to warnings printed from:\n\nhttps://github.com/docker/cli/blob/511dad69d0733d0b63e5ef637e67b5bf3f7081f0/cli/command/stack/deploy_composefile.go#L277-L279\n\n\u2192 resulting in a blank line on stderr.\n\n<img width=\"697\" height=\"374\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/6c147bb6-362e-4550-85eb-e9a7e969ead0\" />\n\n### Reproduce\n\n```\n$ cat docker-compose.yml \nservices:\n  service1:\n    image: alpine\n    command: [\"echo\", \"Hello from Service 1!\"]\n  \n  service2:\n    image: alpine\n    command: [\"echo\", \"Hello from Service 2!\"]\n```\n\n```\n$ docker --version\nDocker version 29.0.2, build 8108357\n$ docker stack deploy -c docker-compose.yml --detach=true --resolve-image always test\nCreating network test_default\nCreating service test_service2\nCreating service test_service1\n$ docker stack deploy -c docker-compose.yml --detach=true --resolve-image always test\nUpdating service test_service1 (id: 9dnrmfa3zepzhmdwmsffjp6kx)\n\nUpdating service test_service2 (id: hkzbj9bucponj0ltccrqar92q)\n\n$ docker stack deploy -c docker-compose.yml --detach=true --resolve-image never test\nUpdating service test_service1 (id: 9dnrmfa3zepzhmdwmsffjp6kx)\nUpdating service test_service2 (id: hkzbj9bucponj0ltccrqar92q)\n$ docker stack deploy -c docker-compose.yml --detach=true --resolve-image always test 2>/tmp/out\nUpdating service test_service1 (id: 9dnrmfa3zepzhmdwmsffjp6kx)\nUpdating service test_service2 (id: hkzbj9bucponj0ltccrqar92q)\n$ xxd /tmp/out\n00000000: 0a0a                                     ..\n$ \n```\n\n```\n$ docker --version\nDocker version 28.5.2, build ecc6942\n$ docker stack deploy -c docker-compose.yml --detach=true --resolve-image always test\nUpdating service test_service1 (id: 9dnrmfa3zepzhmdwmsffjp6kx)\nUpdating service test_service2 (id: hkzbj9bucponj0ltccrqar92q)\n$ docker stack deploy -c docker-compose.yml --detach=true --resolve-image never test\nUpdating service test_service2 (id: hkzbj9bucponj0ltccrqar92q)\nUpdating service test_service1 (id: 9dnrmfa3zepzhmdwmsffjp6kx)\n$ docker stack deploy -c docker-compose.yml --detach=true --resolve-image always test 2>/tmp/out2\nUpdating service test_service1 (id: 9dnrmfa3zepzhmdwmsffjp6kx)\nUpdating service test_service2 (id: hkzbj9bucponj0ltccrqar92q)\n$ xxd /tmp/out2\n$ \n```\n\n\n\n### Expected behavior\n\n- No empty stderr lines.\n- Only meaningful warnings should be printed.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           29.0.2\n API version:       1.52\n Go version:        go1.25.4\n Git commit:        8108357\n Built:             Mon Nov 17 12:33:14 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          29.0.2\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.4\n  Git commit:       e9ff10b\n  Built:            Mon Nov 17 12:33:14 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v2.1.5\n  GitCommit:        fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc:\n  Version:          1.3.3\n  GitCommit:        v1.3.3-0-gd842d771\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    29.0.2\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.3\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 95\n  Running: 30\n  Paused: 0\n  Stopped: 65\n Images: 268\n Server Version: 29.0.2\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: active\n  NodeID: suoatou3e37gtzoo0ui2q9i3p\n  Is Manager: true\n  ClusterID: ranjkcsho0qwgee0u0rwwpe54\n  Managers: 1\n  Nodes: 1\n  Default Address Pool: 10.0.0.0/8  \n  SubnetSize: 24\n  Data Path Port: 4789\n  Orchestration:\n   Task History Retention Limit: 2\n  Raft:\n   Snapshot Interval: 10000\n   Number of Old Snapshots to Retain: 0\n   Heartbeat Tick: 1\n   Election Tick: 10\n  Dispatcher:\n   Heartbeat Period: 5 seconds\n  CA Configuration:\n   Expiry Duration: 3 months\n   Force Rotate: 0\n  Autolock Managers: false\n  Root Rotation In Progress: false\n  Node Address: 192.168.125.212\n  Manager Addresses:\n   192.168.125.212:2377\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc version: v1.3.3-0-gd842d771\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.8.0-78-generic\n Operating System: Ubuntu 22.04.5 LTS\n OSType: linux\n Architecture: x86_64\n CPUs: 16\n Total Memory: 62.38GiB\n Name: gooze\n ID: 250d5780-4a34-4c7b-9b0c-060d63549594\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Username: kumy\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\n\n### Additional Info\n\n\u2714 Regression confirmed by downgrading docker-ce-cli\nLast good: `docker-ce-cli=5:28.5.2-1~ubuntu.22.04~jammy`\nFirst bad: `docker-ce-cli=5:29.0.0-1~ubuntu.22.04~jammy`\n\nGit bisect lead me to\n```\n$ git bisect skip\nThere are only 'skip'ped commits left to test.\nThe first bad commit could be any of:\naeb78091a0db09ae3a237e80547898bda8c41bb8\n4f7c07cfc24a84eb1864bc04a9b723b851bf4123\nWe cannot bisect more!\n```\naeb78091a0db09ae3a237e80547898bda8c41bb8 was failing to build:\n```\n > [build 2/2] RUN --mount=type=bind,target=.,ro     --mount=type=cache,target=/root/.cache     --mount=type=tmpfs,target=cmd/docker/winresources     xx-go --wrap &&     TARGET=/out ./scripts/build/binary &&     xx-verify $([ \"static\" = \"static\" ] && echo \"--static\") /out/docker:                                                                                                                                \n0.362 Building static docker-linux-amd64                                                                                                \n0.363 + go build -o /out/docker-linux-amd64 -tags ' osusergo pkcs11' -ldflags ' -X \"github.com/docker/cli/cli/version.GitCommit=aeb78091a0\" -X \"github.com/docker/cli/cli/version.BuildTime=2025-11-22T09:08:41Z\" -X \"github.com/docker/cli/cli/version.Version=29.0.0-rc.1-6-gaeb78091a0.m\" -extldflags -static' '-buildmode=pie' github.com/docker/cli/cmd/docker\n0.492 # github.com/docker/cli/cli/command/idresolver\n0.492 cli/command/idresolver/idresolver.go:33:28: r.client.NodeInspectWithRaw undefined (type client.APIClient has no field or method NodeInspectWithRaw)\n0.492 cli/command/idresolver/idresolver.go:46:31: r.client.ServiceInspectWithRaw undefined (type client.APIClient has no field or method ServiceInspectWithRaw)\n0.493 # github.com/docker/cli/cli/command/swarm/progress\n0.493 cli/command/swarm/progress/root_rotation.go:45:39: not enough arguments in call to apiClient.SwarmInspect\n0.493 \thave (context.Context)\n0.493 \twant (context.Context, client.SwarmInspectOptions)\n0.493 cli/command/swarm/progress/root_rotation.go:59:43: info.ClusterInfo undefined (type client.SwarmInspectResult has no field or method ClusterInfo)\n0.493 cli/command/swarm/progress/root_rotation.go:59:64: cannot use nodes (variable of struct type client.NodeListResult) as []swarm.Node value in argument to updateProgress\n0.494 # github.com/docker/cli/cli/command/completion\n0.494 cli/command/completion/functions.go:35:23: cannot range over list (variable of struct type client.ImageListResult)\n0.494 cli/command/completion/functions.go:56:23: cannot range over list (variable of struct type client.ImageListResult)\n0.494 cli/command/completion/functions.go:118:28: list.Volumes undefined (type client.VolumeListResult has no field or method Volumes)\n0.494 cli/command/completion/functions.go:133:22: cannot range over list (variable of struct type client.NetworkListResult)\n0.495 # github.com/docker/cli/cli/command/service/progress\n0.495 cli/command/service/progress/progress.go:90:32: apiClient.ServiceInspectWithRaw undefined (type client.APIClient has no field or method ServiceInspectWithRaw)\n0.495 cli/command/service/progress/progress.go:156:44: cannot use tasks (variable of struct type client.TaskListResult) as []swarm.Task value in argument to updater.update\n0.495 cli/command/service/progress/progress.go:223:20: cannot range over nodes (variable of struct type client.NodeListResult)\n0.498 # github.com/docker/cli/cli/trust\n0.498 cli/trust/trust_tag.go:21:9: too many return values\n0.498 \thave (\"github.com/docker/cli/vendor/github.com/moby/moby/client\".ImageTagResult, error)\n0.498 \twant (error)\n0.498 cli/trust/trust_tag.go:21:53: too many arguments in call to apiClient.ImageTag\n0.498 \thave (context.Context, string, string)\n0.498 \twant (context.Context, \"github.com/docker/cli/vendor/github.com/moby/moby/client\".ImageTagOptions)\n0.506 # github.com/docker/cli/cli/command\n0.506 cli/command/cli.go:381:31: not enough arguments in call to cli.client.Ping\n0.506 \thave (\"context\".Context)\n0.506 \twant (\"context\".Context, \"github.com/docker/cli/vendor/github.com/moby/moby/client\".PingOptions)\n0.506 cli/command/cli.go:567:21: undefined: swarm.Status\n------\nDockerfile:85\n--------------------\n  84 |     COPY --link --from=goversioninfo /out/goversioninfo /usr/bin/goversioninfo\n  85 | >>> RUN --mount=type=bind,target=.,ro \\\n  86 | >>>     --mount=type=cache,target=/root/.cache \\\n  87 | >>>     --mount=type=tmpfs,target=cmd/docker/winresources \\\n  88 | >>>     # override the default behavior of go with xx-go\n  89 | >>>     xx-go --wrap && \\\n  90 | >>>     # export GOCACHE=$(go env GOCACHE)/$(xx-info)$([ -f /etc/alpine-release ] && echo \"alpine\") && \\\n  91 | >>>     TARGET=/out ./scripts/build/binary && \\\n  92 | >>>     xx-verify $([ \"$GO_LINKMODE\" = \"static\" ] && echo \"--static\") /out/docker\n  93 |     \n--------------------\nERROR: failed to solve: process \"/bin/sh -c xx-go --wrap &&     TARGET=/out ./scripts/build/binary &&     xx-verify $([ \\\"$GO_LINKMODE\\\" = \\\"static\\\" ] && echo \\\"--static\\\") /out/docker\" did not complete successfully: exit code: 1\n```\n\nAfter code inspection I went to:\n\n\u2714 Root Cause Details\n\n`resolveContainerSpecImage()` in `client/service_create.go`:\n```\nvar warning string        // = \"\"\nif _, _, err := imageDigestAndPlatforms(...); err != nil {\n    warning = digestWarning(...)\n}\nreturn warning            // returns \"\" for success\n```\n\nCaller:\n```\nresolveWarning := resolveContainerSpecImage(...)\nwarnings = append(warnings, resolveWarning) // blindly appends \"\"\n```\n\nLater, CLI prints warnings on stderr:\n```\nfor _, w := range result.Warnings {\n    fmt.Fprintln(stderr, w) // prints a blank line when w == \"\"\n}\n```\n\n\n\n\n\n\n\n",
      "solution": "Thanks again for the report. Looks like the fix was included in 29.1.0 release. Closing as resolved.",
      "labels": [
        "kind/bug",
        "version/29.0"
      ],
      "created_at": "2025-11-22T10:31:01Z",
      "closed_at": "2025-12-03T16:22:22Z",
      "url": "https://github.com/docker/cli/issues/6674",
      "comments_count": 3
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 6645,
      "title": "configuring storage-directory through `--data-root` does not work in docker v29",
      "problem": "### Description\n\ndocker root don't work.\nI set docker root to a big disk but when docker pull, it saves to /var/lib/containerd/\n\n### Reproduce\n\n1. docker pull xxx\n\n### Expected behavior\n\n/var/lib/containerd  don't increase\n\n### docker version\n\n```bash\ndocker --version\nDocker version 29.0.0, build 3d4129b\n```\n\n### docker info\n\n```bash\ndocker info\nClient: Docker Engine - Community\n Version:    29.0.0\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.29.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.3\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 0\n  Running: 0\n  Paused: 0\n  Stopped: 0\n Images: 2\n Server Version: 29.0.0\n Storage Driver: overlayfs\n  driver-type: io.containerd.snapshotter.v1\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc version: v1.3.3-0-gd842d771\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.8.0-1041-azure\n Operating System: Ubuntu 22.04.5 LTS\n OSType: linux\n Architecture: x86_64\n CPUs: 4\n Total Memory: 31.35GiB\n Name: 204e3a8dc000000\n ID: c4277c7b-4f2f-4343-8958-66bee4a7160e\n Docker Root Dir: /data/docker\n Debug Mode: false\n Username: 1eshostedagent\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\n\n### Additional Info\n\n_No response_",
      "solution": "`data-root` only affects the Docker data root, not the containerd.\ncontainerd data is only stored in the Docker data root if it's the dockerd managed containerd instance.\nHowever, if the containerd is managed by the system (like when using the `containerd.io` package) Docker daemon has no way to enforce the external containerd instance to store its data elsewhere.\n\n\nSee: https://docs.docker.com/engine/daemon/#configure-the-data-directory-location\n\nLet me close the issue, if there's a need for further conversation, please open a ticket in https://github.com/moby/moby where the daemon is maintained.",
      "labels": [
        "kind/bug",
        "status/0-triage",
        "version/29.0"
      ],
      "created_at": "2025-11-11T10:57:27Z",
      "closed_at": "2025-12-03T12:53:04Z",
      "url": "https://github.com/docker/cli/issues/6645",
      "comments_count": 6
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 6686,
      "title": "`docker system df -v` shows SHARED SIZE and UNIQUE SIZE as \"N/A\" in 29.1",
      "problem": "### Description\n\nWhen calling `docker system df -v` with Docker CE CLI version 29.1.0 or higher, the output shows both the `SHARED SIZE` and the `UNIQUE SIZE` for all images as `N/A`.\nDocker Version 29.0.4 still shows the correct values.\n\n### Reproduce\n\n1. Call `docker system df -v` with Docker CE CLI Version 29.1.1 installed\n2. You'll get an output like:\n    ```\n    Images space usage:\n\n    REPOSITORY                            TAG         IMAGE ID       CREATED        SIZE      SHARED SIZE   UNIQUE SIZE   CONTAINERS\n    container-mon                         latest      de2add1c7ab2   7 weeks ago    48.5MB    N/A           N/A           1\n    alpine                                latest      171e65262c80   7 weeks ago    8.51MB    N/A           N/A           0\n    ```\n\n### Expected behavior\n\nThe output should be like it is with version 29.0.4:\n```\nImages space usage:\n\nREPOSITORY                            TAG       IMAGE ID       CREATED       SIZE      SHARED SIZE   UNIQUE SIZE   CONTAINERS\ncontainer-mon                         latest    de2add1c7ab2   7 weeks ago   48.5MB    8.512MB       39.97MB       1\nalpine                                latest    171e65262c80   7 weeks ago   8.51MB    8.512MB       0B            0\n```\n\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           29.1.1\n API version:       1.52\n Go version:        go1.25.4\n Git commit:        0aedba5\n Built:             Fri Nov 28 11:33:29 2025\n OS/Arch:           linux/arm64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          29.1.1\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.4\n  Git commit:       9a84135\n  Built:            Fri Nov 28 11:33:29 2025\n  OS/Arch:          linux/arm64\n  Experimental:     true\n containerd:\n  Version:          v2.2.0\n  GitCommit:        1c4457e00facac03ce1d75f7b6777a7a851e5c41\n runc:\n  Version:          1.3.4\n  GitCommit:        v1.3.4-0-gd6d73eb8\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    29.1.1\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.3\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n  scout: Docker Scout (Docker Inc.)\n    Version:  v1.18.4\n    Path:     /home/---------/.docker/cli-plugins/docker-scout\n\nServer:\n Containers: 18\n  Running: 18\n  Paused: 0\n  Stopped: 0\n Images: 25\n Server Version: 29.1.1\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 1c4457e00facac03ce1d75f7b6777a7a851e5c41\n runc version: v1.3.4-0-gd6d73eb8\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.12.47+rpt-rpi-v8\n Operating System: Debian GNU/Linux 12 (bookworm)\n OSType: linux\n Architecture: aarch64\n CPUs: 4\n Total Memory: 3.702GiB\n Name: tjpi09\n ID: eae078bc-a770-43fd-9caa-1c92cff6398d\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Username: ----------\n Experimental: true\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\n\n### Additional Info\n\n_No response_",
      "solution": "Thanks, and thanks again for reporting!\n\n\nI didn't close the ticket yet, because we didn't publish updated packages yet, but this should be fixed now in v29.1.2, so let me close it.",
      "labels": [
        "kind/bug",
        "version/29.1"
      ],
      "created_at": "2025-11-29T09:59:46Z",
      "closed_at": "2025-12-03T12:24:56Z",
      "url": "https://github.com/docker/cli/issues/6686",
      "comments_count": 4
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 6670,
      "title": "Image ID changes when loading image from previous versions into a fresh Docker Engine v29 installation on Linux",
      "problem": "### Description\n\nWhen an image is created in an environment running Docker Engine v28 or below and then exported and loaded into a fresh installation of Docker Engine v29 on Linux (where Docker was never installed before, or a previous version was completely uninstalled), the image ID is not retained. Instead, a new image ID is generated upon loading.\n\nHowever:\n- If Docker Engine v29 is installed by upgrading from v28 or below, the image ID remains consistent.\n- The new image ID assigned by v29 appears to be deterministic. If the same tarball is loaded into multiple fresh v29 environments, the new ID is the same across those environments.\n- The issue cannot be reproduced on Windows with the same versions.\n\n### Reproduce\n\n1. On a Linux system with Docker Engine v28:\n    ```shell\n    docker build -t test-image:latest .\n    docker save test-image:latest -o test-image.tar\n    docker images   # Note the image ID\n    ```\n2. On a fresh Linux system (Docker never installed before), install Docker Engine v29 and load the image:\n    ```shell\n    docker load -i test-image.tar\n    docker images   # Observe the image ID\n    ```\n3. Compare the image ID with the original.\n\n### Expected behavior\n\nThe image ID should remain the same after loading into the new environment, regardless of whether Docker was freshly installed or upgraded.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           29.0.2\n API version:       1.52\n Go version:        go1.25.4\n Git commit:        8108357\n Built:             Mon Nov 17 12:33:26 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          29.0.2\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.4\n  Git commit:       e9ff10b\n  Built:            Mon Nov 17 12:33:26 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v2.1.5\n  GitCommit:        fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc:\n  Version:          1.3.3\n  GitCommit:        v1.3.3-0-gd842d771\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    29.0.2\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.3\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 0\n  Running: 0\n  Paused: 0\n  Stopped: 0\n Images: 1\n Server Version: 29.0.2\n Storage Driver: overlayfs\n  driver-type: io.containerd.snapshotter.v1\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc version: v1.3.3-0-gd842d771\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.8.0-87-generic\n Operating System: Ubuntu 24.04.3 LTS\n OSType: linux\n Architecture: x86_64\n CPUs: 4\n Total Memory: 14.59GiB\n Name: DevET00230.esri.com\n ID: 116d8d21-086b-4456-b282-e0ea7334236d\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\n\n### Additional Info\n\n_No response_",
      "solution": "Yeah the following in `daemon.json` fixed it for us:\n```\n\"features\": {\n    \"containerd-snapshotter\": false\n},\n```",
      "labels": [
        "kind/bug",
        "status/0-triage",
        "version/29.0"
      ],
      "created_at": "2025-11-20T23:40:58Z",
      "closed_at": "2025-12-01T15:31:47Z",
      "url": "https://github.com/docker/cli/issues/6670",
      "comments_count": 5
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 6684,
      "title": "DNS fails in docker 29.1.0-1",
      "problem": "### Description\n\nThis morning I updated Docker on my Debian 13 systems from 29.0.4-1 to 29.1.0-1.  Once I did so, DNS stopped working in every docker container I have.  The containers can still reach the DNS server IP, but DNS lookups with the default resolv.conf fail.\n\nManually changing /etc/resolv.conf from the default to a hard-coded nameserver allows DNS lookups to start working again.\n\nReverting docker-ce from 29.1.0-1 back to 29.0.4-1 also fixes the problem.\n\nAdding a manual DNS entry to /etc/docker/daemon.json does not have any effect.\n\n### Reproduce\n\nUpdate Docker (on Debian 13) to 29.1.0-1, then test DNS resolution inside a container.\n\n### Expected behavior\n\nnslookup, ping, apt update, etc. should work normally without having to manually override /etc/resolv.conf\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           29.1.0\n API version:       1.52\n Go version:        go1.25.4\n Git commit:        360952c\n Built:             Thu Nov 27 16:42:45 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          29.1.0\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.4\n  Git commit:       710302e\n  Built:            Thu Nov 27 16:42:45 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v2.1.5\n  GitCommit:        fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc:\n  Version:          1.3.3\n  GitCommit:        v1.3.3-0-gd842d771\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    29.1.0\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.3\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n  model: Docker Model Runner (Docker Inc.)\n    Version:  v1.0.2\n    Path:     /usr/libexec/docker/cli-plugins/docker-model\n\nServer:\n Containers: 1\n  Running: 1\n  Paused: 0\n  Stopped: 0\n Images: 1\n Server Version: 29.1.0\n Storage Driver: overlayfs\n  driver-type: io.containerd.snapshotter.v1\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc version: v1.3.3-0-gd842d771\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.12.57+deb13-amd64\n Operating System: Debian GNU/Linux 13 (trixie)\n OSType: linux\n Architecture: x86_64\n CPUs: 4\n Total Memory: 7.761GiB\n Name: dmzhost2\n ID: f9a02a16-4a17-4370-b8b8-dbe0c7535bdc\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Default Address Pools:\n   Base: 172.16.0.0/12, Size: 26\n Firewall Backend: iptables\n```\n\n### Additional Info\n\n_No response_",
      "solution": "can confirm this issue. adding dns servers in docker-compose.yml works as well, but of course only for this single docker container:\n\n```\nservices:\n  application:\n    image: [...]\n    dns:\n     - 8.8.8.8\n     - 4.4.4.4\n```\n\ndowngrade to 29.0.4-1 fixes the issue for all containers.\n\n**debian 13**\n```\nsudo apt install docker-ce-cli=5:29.0.4-1~debian.13~trixie docker-ce=5:29.0.4-1~debian.13~trixie\n```\n\n**ubuntu 22.04**\n```\nsudo apt install docker-ce=5:29.0.4-1~ubuntu.22.04~jammy docker-ce-cli=5:29.0.4-1~ubuntu.22.04~jammy\n```\n\n**ubuntu 24.04**\n```\nsudo apt install docker-ce=5:29.0.4-1~ubuntu.24.04~noble docker-ce-cli=5:29.0.4-1~ubuntu.24.04~noble\n```\n\n\n\n---\n\nYikes! Sorry to hear that.\n\nI tried to reproduce on my side but to no avail, and that wasn't caught by CI.\n\nTo @suicidaleggroll or anyone running into this issue, could you include detailed repro steps please? This includes: exact `docker run` that reproduces the issue, your host `/etc/resolv.conf` and container-side `/etc/resolv.conf`.\n\nIt'd be useful to enable debug mode (see [here](https://docs.docker.com/engine/daemon/logs/#enable-debugging)) and include what the Engine logs around the time you run a broken container, and try to resolve a hostname from there.\n\n---\n\nThe issue seems to only affect existing containers\n\n**Container created before upgrade has 127.0.0.11 as resolver:**\n```\ndocker exec -it miniflux cat /etc/resolv.conf\n# Generated by Docker Engine.\n# This file can be edited; Docker Engine will not make further changes once it\n# has been modified.\n\nnameserver 127.0.0.11\noptions ndots:0\n\n# Based on host file: '/etc/resolv.conf' (internal resolver)\n# ExtServers: [host(9.9.9.9)]\n# Overrides: []\n# Option ndots from: internal\n```\n\n**Whereas a newly created one yields the following:**\n```\ndocker run --rm alpine sh -c \"cat /etc/resolv.conf && nslookup google.com\"\n# Generated by Docker Engine.\n# This file can be edited; Docker Engine will not make further changes once it\n# has been modified.\n\nnameserver 9.9.9.9\n\n# Based on host file: '/etc/resolv.conf' (legacy)\n# Overrides: []\nServer:\t\t9.9.9.9\nAddress:\t9.9.9.9:53\n\nNon-authoritative answer:\nName:\tgoogle.com\nAddress: 216.58.213.110\n\nNon-authoritative answer:\nName:\tgoogle.com\nAddress: 2a00:1450:4017:805::200e\n```",
      "labels": [
        "kind/bug",
        "area/networking",
        "version/29.1"
      ],
      "created_at": "2025-11-27T20:57:50Z",
      "closed_at": "2025-11-28T10:31:51Z",
      "url": "https://github.com/docker/cli/issues/6684",
      "comments_count": 26
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 6630,
      "title": "Invalid permissions for `/dev/shm`",
      "problem": "### Description\n\nI have compose file like this:\n```yaml\n...\nvolumes:\n      - { type: tmpfs, target: /dev/shm, tmpfs: { size: 1073741824 } }\n...\n```\nDocumentation says - if `mode` is not specified - it will create it with `1777` permissions.\n\nAnd it was so until docker `v28.5.2` was released.\n\nNow it creates `/dev/shm` with `rwxr-xr-x` perms, which leads to the problems.\n\n### Reproduce\n\nPlease try to create and run docker service using compose file with the following volume:\n```yaml\nvolumes:\n      - { type: tmpfs, target: /dev/shm, tmpfs: { size: 1073741824 } }\n```\n\n### Expected behavior\n\n_No response_\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           28.5.2\n API version:       1.51\n Go version:        go1.25.3\n Git commit:        ecc6942\n Built:             Wed Nov  5 14:43:10 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          28.5.2\n  API version:      1.51 (minimum version 1.24)\n  Go version:       go1.25.3\n  Git commit:       89c5e8f\n  Built:            Wed Nov  5 14:43:10 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v1.7.28\n  GitCommit:        b98a3aace656320842a23f4a392a33f46af97866\n runc:\n  Version:          1.3.3\n  GitCommit:        v1.3.3-0-gd842d771\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    28.5.2\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.29.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.3\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 9\n  Running: 3\n  Paused: 0\n  Stopped: 6\n Images: 3\n Server Version: 28.5.2\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: active\n  NodeID: lpa0qxzmi2cfc4fyr108783xr\n  Is Manager: true\n  ClusterID: 3c1vpyjaysd8u1jgcfejy9g50\n  Managers: 1\n  Nodes: 1\n  Default Address Pool: 10.0.0.0/8\n  SubnetSize: 24\n  Data Path Port: 4789\n  Orchestration:\n   Task History Retention Limit: 5\n  Raft:\n   Snapshot Interval: 10000\n   Number of Old Snapshots to Retain: 0\n   Heartbeat Tick: 1\n   Election Tick: 10\n  Dispatcher:\n   Heartbeat Period: 5 seconds\n  CA Configuration:\n   Expiry Duration: 3 months\n   Force Rotate: 0\n  Autolock Managers: false\n  Root Rotation In Progress: false\n  Node Address: 192.168.175.10\n  Manager Addresses:\n   192.168.175.10:2377\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: b98a3aace656320842a23f4a392a33f46af97866\n runc version: v1.3.3-0-gd842d771\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.8.0-87-generic\n Operating System: Ubuntu 24.04.3 LTS\n OSType: linux\n Architecture: x86_64\n CPUs: 8\n Total Memory: 7.709GiB\n Name: devel\n ID: da5cc937-522d-42d6-943e-b8df463d0b25\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Username: zdm001\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n```\n\n### Additional Info\n\n_No response_",
      "solution": "+1\n\nA downgrade of the docker packages has fixed the problem:\n```bash\napt purge -y docker-ce-cli docker-ce-rootless-extras containerd.io docker-ce\n\napt install -y docker-ce-cli=5:28.5.1-1~debian.12~bookworm docker-ce-rootless-extras=5:28.5.1-1~debian.12~bookworm containerd.io=1.7.28-1~debian.12~bookworm docker-ce=5:28.5.1-1~debian.12~bookworm\n```\n\n---\n\n+1\n\nWe have the same Problem after an auto update. The problem is not specific to \"tmpfs\" type but also occurs when using \"volume\" bind for the given path.\n\nBy using type \"bind\" we could work around it for the moment, but its not an ideal solution as the service is than bound to the current file system and needs a folder with high privileges to write in.",
      "labels": [
        "kind/bug",
        "status/0-triage",
        "version/29.0"
      ],
      "created_at": "2025-11-06T08:27:26Z",
      "closed_at": "2025-11-29T00:06:32Z",
      "url": "https://github.com/docker/cli/issues/6630",
      "comments_count": 6
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 6672,
      "title": "Image list output does not honor setting in ~/.docker/config.json file",
      "problem": "### Description\n\nSince 29.0.0 docker image list does not format its output based on the `imagesFormat` field in ~/docker/config.json.\n\n### Reproduce\n\n```\ndocker image ls\n```\n\nDisplayed columns:\nIMAGE, ID, DISK, USAGE, CONTENT, SIZE, EXTRA\n\n```\ncat > ~/.docker/config.json << 'EOF'\n{\n    \"imagesFormat\": \"table {{.ID}}\"\n}\nEOF\n\ndocker image ls\n```\n\nNo change, the displayed columns are the same.\nIMAGE, ID, DISK, USAGE, CONTENT, SIZE, EXTRA\n\n### Expected behavior\n\nIf the `imagesFormat` option is specified in the config.json file, the output should be displayed according to the specified formatting settings.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           29.0.2\n API version:       1.52\n Go version:        go1.25.4\n Git commit:        8108357bcb\n Built:             Mon Nov 17 10:19:50 2025\n OS/Arch:           darwin/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          29.0.2\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.4\n  Git commit:       e9ff10b\n  Built:            Mon Nov 17 12:33:35 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v2.1.5\n  GitCommit:        fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc:\n  Version:          1.3.3\n  GitCommit:        v1.3.3-0-gd842d771\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    29.0.2\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.1\n    Path:     /usr/local/lib/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  2.40.3\n    Path:     /usr/local/lib/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 6\n  Running: 3\n  Paused: 0\n  Stopped: 3\n Images: 39\n Server Version: 29.0.2\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc version: v1.3.3-0-gd842d771\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.12.57+deb13-amd64\n Operating System: Debian GNU/Linux 13 (trixie)\n OSType: linux\n Architecture: x86_64\n CPUs: 2\n Total Memory: 3.792GiB\n Name: docker-dev\n ID: ca5d66a9-368b-492f-b3e3-06dd93080393\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  docker-dev.virtual:5000\n  127.0.0.0/8\n  ::1/128\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\n\n### Additional Info\n\nThank you for your help!",
      "solution": "Thanks for reporting; this looks like a duplicate of https://github.com/docker/cli/issues/6663, which was fixed through https://github.com/docker/cli/pull/6667 - that fix didn't ship yet but will be part of the next patch release.",
      "labels": [
        "duplicate",
        "kind/bug",
        "version/29.0"
      ],
      "created_at": "2025-11-21T09:13:41Z",
      "closed_at": "2025-11-21T09:52:29Z",
      "url": "https://github.com/docker/cli/issues/6672",
      "comments_count": 1
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 3058,
      "title": "`docker run --user` with group removes the membership of other groups",
      "problem": "<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/cli/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Docker Community Slack - https://dockr.ly/community\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\nWhen calling `docker run` with `--user` and specifying a group (either with `gid` or with its name), such as `--user 1000:1000`, `docker` removes the membership of all the other groups for the user running this command.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Without a group, the user is member of the `docker` group:\r\n    ```sh-session\r\n    $ docker run --rm -u rootless --entrypoint= felipecrs/fixdockergid:pr-2 groups\r\n    rootless docker\r\n    ```\r\n\r\n2. With a group, the user isn't a member of the `docker` group anymore:\r\n     ```sh-session\r\n     $ docker run --rm -u rootless:rootless --entrypoint= felipecrs/fixdockergid:pr-2 groups\r\n     rootless\r\n      ```\r\n\r\n**Describe the results you received:**\r\n\r\n```sh-session\r\n$ docker run --rm -u rootless:rootless --entrypoint= felipecrs/fixdockergid:pr-2 groups\r\nrootless\r\n```\r\n\r\n\r\n**Describe the results you expected:**\r\n\r\n```sh-session\r\n$ docker run --rm -u rootless:rootless --entrypoint= felipecrs/fixdockergid:pr-2 groups\r\nrootless docker\r\n```\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nOne workaround would be to add the parameter to the `docker run` command: `--group-add docker`.\r\n\r\n**Output of `docker version`:**\r\n\r\n```sh-session\r\n\u276f docker version\r\nClient: Docker Engine - Community\r\n Version:           20.10.6\r\n API version:       1.41\r\n Go version:        go1.13.15\r\n Git commit:        370c289\r\n Built:             Fri Apr  9 22:47:17 2021\r\n OS/Arch:           linux/amd64\r\n Context:           default\r\n Experimental:      true\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          20.10.5\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.13.15\r\n  Git commit:       363e9a8\r\n  Built:            Tue Mar  2 20:15:47 2021\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     true\r\n containerd:\r\n  Version:          1.4.4\r\n  GitCommit:        05f951a3781f4f2c1911b05e61c160e9c30eaa8e\r\n runc:\r\n  Version:          1.0.0-rc93\r\n  GitCommit:        12644e614e25b05da6fd08a38ffa0cfe1903fdec\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```sh-session\r\n\u276f docker info\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  app: Docker App (Docker Inc., v0.9.1-beta3)\r\n  buildx: Build with BuildKit (Docker Inc., v0.5.1-docker)\r\n\r\nServer:\r\n Containers: 4\r\n  Running: 1\r\n  Paused: 0\r\n  Stopped: 3\r\n Images: 12\r\n Server Version: 20.10.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runtime.v1.linux runc io.containerd.runc.v2\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 05f951a3781f4f2c1911b05e61c160e9c30eaa8e\r\n runc version: 12644e614e25b05da6fd08a38ffa0cfe1903fdec\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 5.4.72-microsoft-standard-WSL2\r\n Operating System: Docker Desktop\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 4\r\n Total Memory: 12.41GiB\r\n Name: docker-desktop\r\n ID: 6V7O:OCOT:UZNF:SPN4:F7MU:3GPK:MFRK:XD7D:AJZQ:TUWP:AFEW:U2GR\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: true\r\n  File Descriptors: 87\r\n  Goroutines: 111\r\n  System Time: 2021-04-20T21:44:56.2581067Z\r\n  EventsListeners: 4\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: true\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: No blkio throttle.read_bps_device support\r\nWARNING: No blkio throttle.write_bps_device support\r\nWARNING: No blkio throttle.read_iops_device support\r\nWARNING: No blkio throttle.write_iops_device support\r\n```",
      "solution": "Well, yes. But probably thousands of automations have been built relying on things like `--user $(id -u):$(id -g)` to be able to properly write/read files from volume mounts in the host.\r\n\r\n[fixuid](https://github.com/boxboat/fixuid) as an entrypoint makes sure to adjust the non-root user and group inside of the container to match the UID and GID passed by `--user`.\r\n\r\nBut sometimes, within the container, having only such GID is not enough. There could be some files within the container which are only accessible to members of a different group (which the user is member of, but docker disregards such memberships).\r\n\r\nAnd although fixuid also fixes the group memberships before delegating the execution to CMD, the same can't be said when you for example, use `docker exec` for a container previously started with `--user $(id -u):$(id -g)`. That's because docker obviously won't honor the entrypoint for every `docker exec` call.\r\n\r\nProblem is that there is simply no safe way around it. Only workaround is to `--group-add` all groups originally part of the user within the container in the `docker run`. Sometimes, this is just impossible, given that it depends on the container, which can be unknown.\n\n---\n\n>  probably thousands of automations have been built relying on things like --user $(id -u):$(id -g)\r\n\r\nThis is my problem as well. And as you say there isn't a clean workaround for it.\r\n\n\n---\n\nJust leaving a note here with hopes that it helps someone...\n\nJenkins is one of those applications where containers started using docker step and Docker and Dockerfile agents, automatically get `-u <uid>:<gid>` argument based on the user running the process. This is done to ensure that the `$WORKSPACE` mounted can be accessed. However, this eliminates/overrides the groups defined inside the container. This happens even when the `uid` matches that of a user inside the container. Jenkins maintainers ask users to use `docker` command directly in `sh` step if the default behavior is undesired. See - https://github.com/jenkinsci/docker-workflow-plugin/pull/57#issuecomment-621501006 \n\nOne workaround is to pass `-u` argument explicitly as Docker seems to just use the last `-u` argument and the one mentioned explicitly happens to be the last. This is by coincidence and might fail in future. One would also need to know what the `uid` and `gid` needs to be. Using user and group name might be better as the id numbers can change. This is not known in all cases and might need to be figured out using code at which point one might as well just ditch it and use `sh` step! Note that one could also use `--group-add <gid>` if specific `gid` is needed and is known or can be figured out. Group name will also work in place of `gid`. See https://stackoverflow.com/a/51986870/319542\n\nI guess Docker is ok and is doing it's thing although I couldn't figure this out from documentation and had to experiment to find out. If you pass user or group name, it looks up within the container; if id is provided then it ignore the setup within the container; missing values typically default to 0(root); group-add just appends to whatever the default behavior is. So barring the documentation or lack thereof, Docker is ok IMHO. Its Jenkins that is not flexible. Making sure that the `$WORKSPACE` is read/writeable is good but it should have been an option for the end user to decide rather than hardcoding and making it difficult. ",
      "labels": [],
      "created_at": "2021-04-20T21:45:45Z",
      "closed_at": "2024-01-08T17:51:03Z",
      "url": "https://github.com/docker/cli/issues/3058",
      "comments_count": 11
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 5861,
      "title": "Random MAC ADDRESS on MACVLAN adapter",
      "problem": "### Description\n\nAfter installing the latest update (5.28.0.0-1 - debian.12), every time I restart or update a container with a MACVLAN adapter associated with it, its MAC ADDRESS changes randomly; the \"problem\" is that I have LAN monitoring utilities that, every time, notify me of the access of a new device.\n\n### Reproduce\n\nInstall the latest version of docker.\nReboot or upgrade an existing container, which is associated with a MACVLAN adapter.\n\n### Expected behavior\n\nThe MAC ADDRESS of the container must always remain the same.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           28.0.0\n API version:       1.48\n Go version:        go1.23.6\n Git commit:        f9ced58\n Built:             Wed Feb 19 22:10:43 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          28.0.0\n  API version:      1.48 (minimum version 1.24)\n  Go version:       go1.23.6\n  Git commit:       af898ab\n  Built:            Wed Feb 19 22:10:43 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.7.25\n  GitCommit:        bcc810d6b9066471b0b6fa75f557a15a1cbf31bb\n runc:\n  Version:          1.2.4\n  GitCommit:        v1.2.4-0-g6c52b3f\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    28.0.0\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.21.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.33.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 44\n  Running: 44\n  Paused: 0\n  Stopped: 0\n Images: 43\n Server Version: 28.0.0\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: bcc810d6b9066471b0b6fa75f557a15a1cbf31bb\n runc version: v1.2.4-0-g6c52b3f\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.11.11-1-pve\n Operating System: Debian GNU/Linux 12 (bookworm)\n OSType: linux\n Architecture: x86_64\n CPUs: 12\n Total Memory: 62.48GiB\n Name: nas-casa\n ID: 2TGI:O4UR:2ZN7:NAE3:MHND:AYL7:BZAN:V2EJ:N6PC:N7FS:5BJR:TH6Y\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Username: thormir84\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n```\n\n### Additional Info\n\n_No response_",
      "solution": "This broke some aspects of home assistant for me. My Shelley devices and emulated hue / alexa pairing stopped working. I assume this has do with a mis-matched mac address after recreating my homeassistant container with docker v28.\n\nI fixed this by restoring a snapshot, noting the mac address of the homeassistant container (`docker inspect <homeassistant>`), then updating to v28 and adding the following to compose:\n\n```\nservice:\n  homeassiant:\n...\n    networks:\n      IoT_Network:\n        ipv4_address: ${HOMEASSISTANT_IP}\n        mac_address: xx:xx:xx:xx:xx:xx # your mac address\n...\n```\n\n---\n\n@robmry A macvlan device to be changing `mac-addresses` every time the container is updated/restarted is a very destructive change. We have internal health-checks to auto-kick the container when the internet connection is misbehaving with this change the `mac-address` is now being changed randomly. This behavior in docker has been solid for years, now out of nowhere in v28.x.y and bunch of networking changes were introduced all at once.\n\nThe `mac-address` should only change when the container is created, not during a restart/update.\n\nSo far my team has run into `5-6` massive breaking changes in `v28.x.y` related to networking for things that have been stable for several years with `docker/libnetwork`.\n\nList of breaking changes so far:\n- The `mac-addresses` are now randomly changing.\n- Containers with multiple networks, are now getting their interfaces created in random-order.\n- The `priority` and `gw-priority` are ignored for `ipvlan`, `macvlan` and `external` networks.\n- [Fixed in 28.0.1] ~Random rules being added to `FORWARD` chain made us have a 4hr outage.~\n- [Fixed in 28.0.1] ~The `ip6tables` being enforced when `ipv6=false` is offline.~\n\n---\n\nHi @gaby - thanks for letting me know.\n\nWe try to minimise the impact of any changes but sometimes, in a major release, they are unavoidable.\n\n> The mac-addresses are now randomly changing.\n\nIn previous releases, a container's MAC address was based on its IPv4 address. From 28.x, a container may no longer have an IPv4 address (or, in the case of macvlan, any IP address at all). And, a MAC address can't be derived from an IPv6 address in the same way because the MAC address doesn't have enough bits. The mapping between addresses was also a blocker for adding IPv6 support to Swarm.\n\nSo, we had to break the relationship between IPv4 address and MAC address.\n\nIn all releases, when a container is stopped it loses its IP address reservation - another container can be started with the same IP address while it's stopped. If the IPv4 address is explicitly configured, but gets reused while the container is stopped, the container won't start because of the clash. If the IPv4 address is not explicitly configured, the container will start with a new IP address and, in 27.x and earlier, with a new MAC address matching that IP address. (Neither IP or MAC address are reserved while a container is stopped, and both may be automatically assigned new values on a container restart or when a container is re-created, unless they're explicitly configured.)\n\nSo, breaking the link between IPv4 and MAC addresses means when a container with no configured MAC address is restarted in 28.x, the address assigned when it restarts is randomly generated - so, it will change, even when the IP address is configured.\n\nIt may be possible to preserve a container's generated MAC address while it's stopped, so it doesn't change on a `docker restart`. But, I don't think that would help much ... for example, compose down/up re-creates containers, they're deleted and added again. Docker Engine doesn't know anything about the relationship between the before-and-after containers. If they're configured to have a specific IP address, they'll get that address again, likewise for a MAC address. Trying to preserve generated MAC addresses in that way would also lead to surprising changes on a docker re-install, re-creating the service would generate new MAC addresses and the old addresses would need to be configured-in at that point.\n\nSo, as with IP addresses - if a specific MAC address is significant to other services in the network, the way to make sure it doesn't change is to configure it.\n\n> Containers with multiple networks, are now getting their interfaces created in random-order.\n\nI think that was the case in 27.x too. The moby API uses a map of network endpoints (rather than an ordered list). Do you have an example of the changed behaviour?\n\nIs the issue that the container's interface names (`eth0` etc) belong to different networks after a restart? In 28.x, there's a new option to explicitly name network interfaces ([release notes](https://github.com/moby/moby/releases/tag/v28.0.0#:~:text=Add%20a%20new%20netlabel%20com.docker.network.endpoint.ifname,bridge%2Cdriver%2Dopt%3Dcom.docker.network.endpoint.ifname%3Dfoobar%20%E2%80%A6)). Perhaps that will help?\n\n> The priority and gw-priority are ignored for ipvlan, macvlan and external networks.\n\nCan you share details or an example? ...\n\nThe `priority` field in compose hasn't changed. Since compose 2.24 (Feb '24), it has not influenced the container's network device names, and it has never influenced which network provides the default gateway.\n\nIn 28.x, the new `gw-priority` can be used to determine which network provides the default gateway. It does not consider which network driver the network is being used. But, a dual-stack (IPv4 and IPv6) network is always preferred over single-stack as the gateway - perhaps that's what you've seen?",
      "labels": [
        "kind/bug",
        "status/0-triage",
        "area/networking",
        "version/28.0"
      ],
      "created_at": "2025-02-23T08:20:11Z",
      "closed_at": "2025-11-15T18:07:17Z",
      "url": "https://github.com/docker/cli/issues/5861",
      "comments_count": 15
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 6511,
      "title": "After upgrading Ubuntu 22.04 to 24.04.3, docker run not working anymore",
      "problem": "### Description\n\n# Issue After Upgrading from Ubuntu 22.04 to 24.04.3\n\nI made the upgrade from **Ubuntu 22.04** to **24.04.3**, which went fine. However, my previous Docker containers didn\u2019t restart, and when I checked, I found that even a simple `docker run hello-world` was no longer working:\n\n```bash\ndocker run hello-world\n```\n\nOutput:\n\n```\ndocker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: unable to retrieve OCI runtime error (open /run/containerd/io.containerd.runtime.v2.task/moby/b5f6a5bf0e255f2c48d442e4fba8e11a3045804b0f6f7bdbe2c7ba46812f3c61/log.json: no such file or directory): runc did not terminate successfully: exit status 1: unknown\n```\n\n---\n\n### What I Tried\n\n* Uninstalled and reinstalled Docker from the Ubuntu repository.\n* Downloaded and compiled several versions of Docker CLI, containerd, and runc from GitHub to test them.\n\nUnfortunately, nothing worked.\n\n---\n\n### Kernel Module Check\n\nI suspected that the `overlay` kernel module might be missing, but after checking, it was present:\n\n```bash\nlsmod | grep overlay\noverlay               212992  0\n```\n\n---\n\n### Current Guess\n\nMy guess is that the issue is related to **containerd**, which seems unable to create the file `log.json`, but I don\u2019t know why.\n\n\n\n### Reproduce\n\ndocker run hello-world\ndocker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: unable to retrieve OCI runtime error (open /run/containerd/io.containerd.runtime.v2.task/moby/b5f6a5bf0e255f2c48d442e4fba8e11a3045804b0f6f7bdbe2c7ba46812f3c61/log.json: no such file or directory): runc did not terminate successfully: exit status 1: unknown\n\n### Expected behavior\n\ndocker run hello-world\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n 1. The Docker client contacted the Docker daemon.\n 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (amd64)\n 3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n 4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n $ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\n https://hub.docker.com/\n\nFor more examples and ideas, visit:\n https://docs.docker.com/get-started/\n\n\n### docker version\n\n```bash\nDocker version 28.4.0.m, build d8eb465f86\n```\n\n### docker info\n\n```bash\ndocker info\nClient:\n Version:    28.4.0.m\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.28.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.39.4\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n  model: Docker Model Runner (Docker Inc.)\n    Version:  v0.1.40\n    Path:     /usr/libexec/docker/cli-plugins/docker-model\n\nServer:\n Containers: 3\n  Running: 0\n  Paused: 0\n  Stopped: 3\n Images: 2\n Server Version: 27.5.1\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: \n runc version: v1.2.5-0-g59923ef1\n init version: \n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.8.0-84-generic\n Operating System: Ubuntu 24.04.3 LTS\n OSType: linux\n Architecture: x86_64\n CPUs: 24\n Total Memory: 125.7GiB\n Name: tradeserver03\n ID: c3933a2c-22cf-4864-a25c-925d59109f4a\n Docker Root Dir: /var/lib/docker\n Debug Mode: true\n  File Descriptors: 23\n  Goroutines: 41\n  System Time: 2025-09-26T18:17:07.213829629+02:00\n  EventsListeners: 0\n Experimental: false\n Insecure Registries:\n  127.0.0.0/8\n Live Restore Enabled: false\n```\n\n### Additional Info\n\nAdditional tests in the attached file:\n- I checked the journal -->>  journalctl -u docker.service\n- I checked the low-level calls to the kernel as well -->>\n\n[other-tests.txt](https://github.com/user-attachments/files/22564593/other-tests.txt)\n\n sudo strace -f -o /tmp/runc-debug.log docker run --rm hello-world",
      "solution": "# Docker Setup Summary\n\nI made fresh setup of docker again and I tried hello-world again.\n\n## Working steps\n\nWhen I run \"docker run hello-world\"  the following two actions are correctly done:\n- the image 'hello-world:latest' is downloaded and created on the file system\n- a container is created on the file system at: **/var/lib/docker/containers/**\n\n## The issue\nHowever, the issue is that a folder should be created for the running container under **\"/run/containerd/io.containerd.runtime.v2.task/moby/\"** and this is not done because docker fails with the following error: *\"Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: unable to retrieve OCI runtime error\".*\n\nI guess is the runc application that should create such folder and its content because the error is *\" runc did not terminate successfully: exit status 134: unknown\".*\n\n## Other test\n\nAfter this I tried to create a new container from the same hello-world image I downloaded by the command \"docker container create xxxx\" and this worked fine.\n\nSo the issue is really in the execution of containers, not image downloading, nor container creations.\n\n\n\n---\n\nI found the issue. It was a wrong GCLIB version:\n\ndocker run --rm hello-world\ndocker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running hook #0: error running hook: exit status 1, stdout: , stderr: libnetwork-setkey: /opt/containerd/lib/libc.so.6: version `GLIBC_2.38' not found (required by /lib/x86_64-linux-gnu/libsystemd.so.0)\nlibnetwork-setkey: /opt/containerd/lib/libc.so.6: version `GLIBC_2.38' not found (required by /lib/x86_64-linux-gnu/libcap.so.2)\nlibnetwork-setkey: /opt/containerd/lib/libc.so.6: version `GLIBC_2.38' not found (required by /lib/x86_64-linux-gnu/libgcrypt.so.20)\nlibnetwork-setkey: /opt/containerd/lib/libc.so.6: version `GLIBC_2.38' not found (required by /lib/x86_64-linux-gnu/libgpg-error.so.0): unknown.\n\nI removed the wrong libc.so.6 and it was ok ;-).\n",
      "labels": [
        "kind/bug",
        "status/0-triage"
      ],
      "created_at": "2025-09-26T16:37:49Z",
      "closed_at": "2025-09-27T20:16:42Z",
      "url": "https://github.com/docker/cli/issues/6511",
      "comments_count": 2
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 6457,
      "title": "Platform-specific \"save\" does not see an image after platform-specific \"pull\"",
      "problem": "### Description\n\n`docker save --platform X/Y image` fails with\n\n```\nError response from daemon: no suitable export target found: image with reference registry.k8s.io/sig-storage/csi-attacher:v4.8.1 was found but does not provide the specified platform (linux/amd64)\n```\n\nafter `docker pull --platform X/Y image` succeeded.\n\n### Reproduce\n\n1. Make sure the image doesn't exist first\n```\n$ docker image ls registry.k8s.io/sig-storage/csi-attacher:v4.8.1\nREPOSITORY   TAG       IMAGE ID   CREATED   SIZE\n```\n\n2.  Pull the platform specific image, where the asked platform isn't the same as the host platform\n```\n$ docker pull --platform linux/amd64 registry.k8s.io/sig-storage/csi-attacher:v4.8.1\nv4.8.1: Pulling from sig-storage/csi-attacher\n09f57e5927f8: Pull complete\nDigest: sha256:69888dba58159c8bc0d7c092b9fb97900c9ca8710d088b0b7ea7bd9052df86f6\nStatus: Downloaded newer image for registry.k8s.io/sig-storage/csi-attacher:v4.8.1\nregistry.k8s.io/sig-storage/csi-attacher:v4.8.1\n```\n\n3. Check what we have so far\n```\n$ docker image ls --tree registry.k8s.io/sig-storage/csi-attacher:v4.8.1\nWARNING: This is an experimental feature. The output may change and shouldn't be depended on.\n\nIMAGE                                             ID             DISK USAGE   CONTENT SIZE   EXTRA\nregistry.k8s.io/sig-storage/csi-attacher:v4.8.1   69888dba5815       32.1MB         32.1MB\n\u251c\u2500 linux/amd64                                    8eb112854b02       32.1MB         32.1MB\n\u251c\u2500 linux/arm/v7                                   2390f245277c           0B             0B\n\u251c\u2500 linux/arm/v7                                   6a995c6319f1           0B             0B\n\u251c\u2500 linux/arm64                                    93dfdfe0079c           0B             0B\n\u251c\u2500 linux/ppc64le                                  0585f9bb9e39           0B             0B\n\u2514\u2500 linux/s390x                                    9603e9b49446           0B             0B\n```\n\n4. Let's try to save this image for the specific platform\n```\n$ docker image save --platform linux/amd64 -o csi-attacher.tar registry.k8s.io/sig-storage/csi-attacher:v4.8.1\nError response from daemon: no suitable export target found: image with reference registry.k8s.io/sig-storage/csi-attacher:v4.8.1 was found but does not provide the specified platform (linux/amd64)\n```\n\n### Expected behavior\n\n### `docker save --platform X/Y` should be able to save the image that was pulled by `docker pull --platform X/Y` and seen by the `docker ls --tree`\n\n### docker version\n\n```bash\nClient:\n Version:           28.4.0\n API version:       1.51\n Go version:        go1.24.7\n Git commit:        d8eb465\n Built:             Wed Sep  3 20:56:26 2025\n OS/Arch:           darwin/arm64\n Context:           desktop-linux\n\nServer: Docker Desktop 4.46.0 (204649)\n Engine:\n  Version:          28.4.0\n  API version:      1.51 (minimum version 1.24)\n  Go version:       go1.24.7\n  Git commit:       249d679\n  Built:            Wed Sep  3 20:58:53 2025\n  OS/Arch:          linux/arm64\n  Experimental:     false\n containerd:\n  Version:          1.7.27\n  GitCommit:        05044ec0a9a75232cad458027ca83437aae3f4da\n runc:\n  Version:          1.2.5\n  GitCommit:        v1.2.5-0-g59923ef\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient:\n Version:    28.4.0\n Context:    desktop-linux\n Debug Mode: false\n Plugins:\n  ai: Docker AI Agent - Ask Gordon (Docker Inc.)\n    Version:  v1.9.11\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.28.0-desktop.1\n  cloud: Docker Cloud (Docker Inc.)\n    Version:  v0.4.27\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.39.2-desktop.1\n  debug: Get a shell into any image or container (Docker Inc.)\n    Version:  0.0.42\n  desktop: Docker Desktop commands (Docker Inc.)\n    Version:  v0.2.0\n  extension: Manages Docker extensions (Docker Inc.)\n    Version:  v0.2.31\n  init: Creates Docker-related starter files for your project (Docker Inc.)\n    Version:  v1.4.0\n  mcp: Docker MCP Plugin (Docker Inc.)\n    Version:  v0.18.0\n  model: Docker Model Runner (Docker Inc.)\n    Version:  v0.1.40\n  offload: Docker Offload (Docker Inc.)\n    Version:  v0.4.27\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\n    Version:  0.6.0\n  scout: Docker Scout (Docker Inc.)\n    Version:  v1.18.3\n\nServer:\n Containers: 121\n  Running: 0\n  Paused: 0\n  Stopped: 121\n Images: 175\n Server Version: 28.4.0\n Storage Driver: overlayfs\n  driver-type: io.containerd.snapshotter.v1\n Logging Driver: json-file\n Cgroup Driver: cgroupfs\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 05044ec0a9a75232cad458027ca83437aae3f4da\n runc version: v1.2.5-0-g59923ef\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.10.14-linuxkit\n Operating System: Docker Desktop\n OSType: linux\n Architecture: aarch64\n CPUs: 10\n Total Memory: 7.654GiB\n Name: docker-desktop\n ID: 3068ce4d-f5e5-4cac-9f39-39e36bc803ba\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n HTTP Proxy: http.docker.internal:3128\n HTTPS Proxy: http.docker.internal:3128\n No Proxy: hubproxy.docker.internal\n Labels:\n  com.docker.desktop.address=unix:///Users/<user>/Library/Containers/com.docker.docker/Data/docker-cli.sock\n Experimental: false\n Insecure Registries:\n  hubproxy.docker.internal:5555\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n```\n\n### Additional Info\n\n_No response_",
      "solution": ">  I wonder if it's an issue in the docker API proxy (ISTR --platform on save may have had some fixes in 28.4, and I wonder if the API proxy used by Docker Desktop in the current release may be blocking those);\n\nI had to jump into some meetings directly after my last comment, so didn't have time to look closer, but I don't think that's it; the error message mentions the specified `--platform` (so daemon should've received it fine), and looking at the diff for 28.4, I don't see relevant changes on the API side that could cause a discrepancy between the API request and the proxy potentially being in between https://github.com/moby/moby/compare/v28.3.3...v28.4.0\n\n\n\n> in my case both are equal to 32.1MB, but in your example it's 114MB and 32.2 MB. Maybe the pull works differently somehow in these 2 versions.\n\n\nOh! Good spot; the `CONTENT` column is for the packed (compressed) layers (what's pulled as actual image); the `DISK USAGE` column includes both the \"packed\" layers, and the unpacked (extracted) content (\"snapshots\"). I wonder if the unpacked content was garbage-collected by containerd \ud83e\udd14 \n\n\nFor some context; docker is migrating to use the containerd image store. The containerd image store is designed around garbage-collecting; if content isn't used, and isn't referenced, it gets garbage-collected over time; ultimately that's something we want to move towards (with configurable policies), but the existing UX for content is for users to manually manage everything (`docker system prune` and the like), which means that for some cases we need to \"go against the stream\", and artificially mark content as \"in use\" even if it isn't, or in some cases \"unpack\" content that's not needed to be extracted (yet).\n\nIn this case, the image pulled is not the native platform (i.e., it's a x86 image pulled on an ARM machine), so I wonder if containerd garbage-collected the unpacked content, and possibly a code-path in the daemon checking for the unpacked content to be there (which, likely, in this case wouldn't be strictly needed, as `image save` would be saving the packed layers).\n\nI know we've had a couple of similar situations (with non-native platforms), so it's definitely possible that there's either an issue remaining, or an issue that's fixed already but not yet in the 28.4 release.\n\n\n\n\n\n\n---\n\nOK, I \"solved\" this by invoking `Clean / Purge data` in the Desktop UI.\n\nInteresting observations are:\n1. When I tried to test it with a different image version `registry.k8s.io/sig-storage/csi-attacher:v4.8.0` - it worked properly and I saw 114MB of disk usage\n2. `docker system prune` didn't resolve the problem (maybe it's expected)\n\n---\n\nClosing the issue for now. Thank you for your help!",
      "labels": [
        "kind/bug",
        "status/0-triage",
        "version/28.4"
      ],
      "created_at": "2025-09-18T17:48:26Z",
      "closed_at": "2025-09-19T20:34:44Z",
      "url": "https://github.com/docker/cli/issues/6457",
      "comments_count": 9
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 3618,
      "title": "Consider dropping dependency on github.com/pkg/errors",
      "problem": "https://github.com/pkg/errors is deprecated, archived, and in maintenance mode, since Go errors natively support wrapping since Go 1.13.\r\n\r\nThere are about 200 files in this repo that depend on this package (excluding `vendor/`):\r\n\r\n```\r\ngit grep -l github.com/pkg/errors | grep -v ^vendor | wc -l\r\n     196\r\n```\r\n\r\nIt should be a fairly mechanical change:\r\n\r\n- `errors.Wrap(err, \"oh no\")` becomes `fmt.Errorf(\"oh no: %w\", err)`\r\n- `errors.New(\"oh no\")` becomes `errors.New(\"oh no\")`, using the [stdlib `errors` package](https://pkg.go.dev/errors).\r\n- `errors.Errorf(\"oh %q\", \"no\")` becomes `fmt.Errorf(\"oh %q\", \"no\")`\r\n\r\nIt may even be somewhat possible to automate, using `x/tools/go/analysis` (see https://github.com/sigstore/cosign/pull/1887#issuecomment-1128049504).\r\n\r\nLet me know if you'd be interested in considering a PR to make this change.",
      "solution": "> Was there a specific reason you want the package to be removed?\r\n\r\nNothing specific exactly, but two broad categories of reasons:\r\n\r\n1. an unmaintained repo, even a \"feature complete\" one, presents a (small, theoretical) risk of having unaddressed vulnerabilities, compared to a well supported solution in Go's stdlib. I'm sure if there was a pkgerrors4shell-style vulnerability the original maintainers or the community would step up to fix it, but it would likely be much slower to seep out into the community than something in Go itself.\r\n\r\n1. Go tooling isn't aware that `errors.Errorf` takes a format string, and that any `%s` in it must be paired with a later arg. `errors.Errorf(\"oh %s\")` is considered perfectly valid, though it would result in `oh %!s(MISSING)`. Compare this with `fmt.Errorf(\"oh %s\")`, which will fail when exercised during tests to alert callers to missing args, or extra args, etc. This also trickles out into IDE support for detecting missing args.\r\n\r\nIn any case, it's not an urgent need, and your answer is perfectly satisfactory. Especially so if you take on maintainership for it, since that would go toward solving (1) above.\r\n\r\nIf you ever become interested in this, my offer to send a PR stands. \ud83d\ude04 \n\n---\n\nI concur that this needs to be removed, as the https://github.com/pkg/errors repository is now archived.\n\nAs for the stack trace functionality, I guess, we need to re-assess the need of. If there is a consensus that it is really needed, come up with a solution which explicitly uses Go `runtime` package to get those and embed into errors. We should consider that getting stack traces is somewhat expensive and in most cases should not really be required.",
      "labels": [],
      "created_at": "2022-05-17T17:10:00Z",
      "closed_at": "2025-09-10T08:20:23Z",
      "url": "https://github.com/docker/cli/issues/3618",
      "comments_count": 5
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 6418,
      "title": "Autocomplete stopped working for `pull` command",
      "problem": "### Description\n\nAfter latest updates (I can't pinpoint the exact version but it was in the last ~6 months) the `docker pull <TAB> <TAB>` command doesn't do anything, while it was showing locally cached images before. All other autocomplete commands appear to work normally as before.\n\n### Reproduce\n\n1. `docker pull hello-world`\n2. `docker pull he<TAB><TAB>`\n3. nothing happens\n\n### Expected behavior\n\nPressing `TAB` provides a list of locally cached images with the same exact behavior of `docker run`\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           28.3.3\n API version:       1.51\n Go version:        go1.24.5\n Git commit:        980b856\n Built:             Fri Jul 25 11:34:04 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          28.3.3\n  API version:      1.51 (minimum version 1.24)\n  Go version:       go1.24.5\n  Git commit:       bea959c\n  Built:            Fri Jul 25 11:34:04 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.7.27\n  GitCommit:        05044ec0a9a75232cad458027ca83437aae3f4da\n runc:\n  Version:          1.2.5\n  GitCommit:        v1.2.5-0-g59923ef\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    28.3.3\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.26.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.39.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 5\n  Running: 4\n  Paused: 0\n  Stopped: 1\n Images: 119\n Server Version: 28.3.3\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: local\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 05044ec0a9a75232cad458027ca83437aae3f4da\n runc version: v1.2.5-0-g59923ef\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  userns\n  cgroupns\n Kernel Version: 6.8.0-79-generic\n Operating System: Ubuntu 22.04.5 LTS\n OSType: linux\n Architecture: x86_64\n CPUs: 16\n Total Memory: 23.1GiB\n Name: [redacted]\n ID: 4ed36754-d593-4662-8c4d-d286f076bfea\n Docker Root Dir: /var/lib/docker/1000.999\n Debug Mode: false\n Username: [redacted]\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n```\n\n### Additional Info\n\n```\n$ docker __complete pull he\n:4\nCompletion ended with directive: ShellCompDirectiveNoFileComp\n$ docker __complete run he\n<list of all my local images>\n:4\nCompletion ended with directive: ShellCompDirectiveNoFileComp\n```",
      "solution": "Thanks for the quick feedback, resolution and detailed explanation!\n\n> having it complete with images you already have so that you can pull the latest version of the same tag\n\nI can confirm that this was the previous behavior (and basically my only use case for `pull`). I didn't even notice it until I realized I'm not used to typing the entire url of my private registry while updating my images",
      "labels": [
        "kind/bug",
        "status/0-triage",
        "area/completion",
        "area/images",
        "version/28.3"
      ],
      "created_at": "2025-09-03T10:50:45Z",
      "closed_at": "2025-09-03T15:34:51Z",
      "url": "https://github.com/docker/cli/issues/6418",
      "comments_count": 4
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 6346,
      "title": "Add CLI option for enabling external DHCP/DNS assignment instead of internal IPAM (Docker on OpenWRT routers)",
      "problem": "_- Bugreport/Proposed goal edited after further checks into the matter due to Docker team's response -_\n\n### Description\n\nDear programmers,\n\nI use Docker on my router that uses OpenWRT as operating system, and run my Docker containers there.\n\nFor convenience, I want not Docker to manage everything, but OpenWRT's dnsmasq (and/or odhcp) to handle DHCP/DNS, so I can easily e.g. handle dynamic IPs, apply firewall settings, and make hostnames known in my whole network.\n\nI know this weakens the containers isolation from each other, but it's a deliberate choice a sysadmin can make for his/her network, based on the goals that are tried to be achieved. This the much more sane option for a router (compared to IPAM): To have everything managed centrally by the router, not by Docker.\n\n**--> I would therefore love a CLI option like e.g. \"--foreign-management-mode\" that implements this behaviour (external management of DHCP/DNS/Firewall) officially. <--**\n\n**--> As a precaution against users accidentially weakening their container isolation, we might add an additional --yes-i-understand-this-weakens-container-isolation as a second parameter that must also be given in the CLI command line. This should mitigate any accidential activations by novices. <--**\n\nThank you all very much for making Docker such an awesome software.",
      "solution": "It was working, or so I thought... I then realized that it was not my DHCP server that assigned IP addresses to the containers, but in fact Docker itself was still doing it. So the solution wasn't actually a solution.\n\n---\n\nOk, thank you - I'm trying to understand how your proposal would work / what Docker should or shouldn't set up, and how that would be used. Or, potentially, how to do something equivalent with existing options.\n\nSo, sorry to press on this - but, from the original description, I got the impression you had it working with Docker's iptables disabled? (\"My network has been much easier to set up and use since then. Instead of \"doing everything twice\" (router DHCP vs. IPAM) and have two \"assigning instances\" that sometimes fight each other (!), my Router can now manage everything centrally.\")\n\nPerhaps the issue is that your containers were getting two addresses, one from Docker's IPAM and one from DHCP - and the services in the container were accessible via DNS and addresses set up by DHCP? But those addresses clash with IPAM-assigned addresses ... or something like that?\n\n---\n\nNo problem! I simply had set this option:\n\n```\nTo /etc/docker/daemon.json, add:\n{\n\"iptables\": false,\n\"ip-masq\": false\n}\n```\n\nAfter generating my network with:\n`docker network create --driver=bridge --subnet=192.168.2.0/24 --gateway=192.168.2.1 --opt com.docker.network.bridge.name=br-docker bridge_docker`\n\nI was under the assumption that all my Docker containers were assigned their IP addresses from my Router's dnsmasq. This assumption turned out to be **wrong**, since disabling dnsmasq didn't stop the containers from getting IP addresses via IPAM/Docker. It just disabled firewall (iptables) rules, apparently. So it **looked like** I had disabled IPAM, but in fact, I didn't.\n\nIn parallel to trying this approach, I also tried about 5000 different combinations of MACVLAN / IPVLAN 2 / IPVLAN 3 in order to achieve my personally desired network configuration (which involved e.g. giving IPVLAN 3 an address like 192.168.4.254 in the Docker network, as well as trying to fiddle around with making all of this IPv6-capable - which cannot be done collision-free at the moment, since there is no way to tell Docker to reserve IPv6 addresses at the moment). So I'm sorry that I cannot give you a more detailed reconstruction of my exact steps. In the end, I settled for the \"classic\" Docker setup with IPv4-only, IPAM and fixed IP addresses for every container.\n\nMy goal was to bypass Docker's internal mechanisms altogether, so that OpenWRT manages DHCP/DNS/Firewall automagically (auto-assigned IP-Addresses, Hostnames known in the local network via DNS, and everything! IPv6, Containers behaving like real computers that can be pinged and interacted with like they were physical machines in the user's regular WLAN/LAN network! etc.) and for that, I needed Docker to \"just\" manages containers. In the truest sense of the word, just the containers themselves. Everything else done by the router. This turned out to be non-attainable with the current state of Docker.\n\nSo my proposal would be to create an expert mode, accessable via CLI command, that disables IPAM and instead passes all the parameters manually: network interface device, network & subnet, gateway, DNS (--> all in IPv4 and IPv6). Since the network interface device can be assigned to a firewall zone, this would also enable the router to manage the firewall rules, no iptables needed anymore (OpenWRT manages everything centrally then). All that's really needed for this to be achived would be a turn-off key for IPAM (as far as I've understood it).\n\nThanks for asking!",
      "labels": [
        "kind/feature",
        "status/0-triage",
        "area/networking",
        "status/more-info-needed"
      ],
      "created_at": "2025-08-22T13:54:12Z",
      "closed_at": "2025-09-03T14:16:08Z",
      "url": "https://github.com/docker/cli/issues/6346",
      "comments_count": 9
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 5792,
      "title": "Issue with the --username option in docker login when using --password-stdin",
      "problem": "### Description\n\nThere seems to be an issue with the usage of docker login when providing the **--username** and **--password-stdin** options. The **--username** option is being ignored or misinterpreted when placed after **--password-stdin**. Based on the official documentation, it should work in this order, but it leads to an error message when executed in this sequence.\n\n\n\n### Reproduce\n\n### Use the following command to log in to a Docker registry:\n\n```bash\n$ echo \"$TARGET_GITLAB_PAT\" | docker login -u \"$TARGET_GITLAB_USERNAME\" --password-stdin registry.gitlab.com\nMust provide --username with --password-stdin\nCleaning up project directory and file based variables 00:01\nERROR: Job failed: exit code 1\n```\n\n![Image](https://github.com/user-attachments/assets/f1bee254-0ce5-466e-b4af-60bf6197fea5)\n\n### Expected behavior\n\nExpected Behavior: The **--username** and **--password-stdin** options should work regardless of their order, as the documentation clearly specifies both options.\n\n### docker version\n\n```bash\nNA\n```\n\n### docker info\n\n```bash\nNA\n```\n\n### Additional Info\n\n_No response_",
      "solution": "There's an issue with populating your variables. Try this:\n\n### Populate the variables:\n```sh\nexport TARGET_GITLAB_USERNAME=\"username\"\nexport TARGET_GITLAB_PAT=\"pat\"\n```\n\n### Confirm they're populated:\n```sh\necho $TARGET_GITLAB_USERNAME     \necho $TARGET_GITLAB_PAT \n```\n\n### Authenticate with GitLab registry:\nThe following command will succeed regardless of the order of `--username` and `--password-stdin` options:\n```sh\necho \"$TARGET_GITLAB_PAT\" | docker login registry.gitlab.com --password-stdin -u \"$TARGET_GITLAB_USERNAME\"\n```\n",
      "labels": [
        "kind/bug",
        "status/0-triage",
        "area/authentication"
      ],
      "created_at": "2025-02-03T21:53:22Z",
      "closed_at": "2025-04-30T17:09:20Z",
      "url": "https://github.com/docker/cli/issues/5792",
      "comments_count": 4
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 6251,
      "title": "Docker report \"error jailing process inside rootfs: pivot_root\" when docker run with Ubuntu22.04 PXE ramdisk",
      "problem": "### Description\n\nI packged a ubuntu22.04.5_aarch64 system to ramdisk file ,boot from PXE, installed docker 28.3.3, and then run \"docker run hello-world\", a issue happpend as below:\n\nroot@node1:~# docker run hello-world\ndocker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error jailing process inside rootfs: pivot_root .: invalid argument: unknown\n\n\n\n### Reproduce\n\ndocker run hello-world\n\n\n### Expected behavior\n\nAnd I tried with ctr command, it works.\n\n\nroot@node1:~# ctr run --no-pivot -rm -t docker.io/library/hello-world:latest test1\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\nTo generate this message, Docker took the following steps:The Docker client contacted the Docker daemon .The Dockerdaemon pulled the \"hello-world\" image from the Docker Hub2(arm64v8)3. The Docker daemon created a new container from that image which runs theexecutablethat produces the output you are currently reading.4.The Docker daemonstreamed that output to the Docker client, which sent itto your terminal.\nTo try something more ambitious.you can run an Ubuntu container with:$ docker run -it ubuntu bash\nShare images,workflows ,and more with a free Docker ID:\n\n### docker version\n\n```bash\nroot@node1:~# docker version\nClient: Docker Engine - Community\n Version:           28.3.3\n API version:       1.51\n Go version:        go1.24.5\n Git commit:        980b856\n Built:             Fri Jul 25 11:35:23 2025\n OS/Arch:           linux/arm64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          28.3.3\n  API version:      1.51 (minimum version 1.24)\n  Go version:       go1.24.5\n  Git commit:       bea959c\n  Built:            Fri Jul 25 11:35:23 2025\n  OS/Arch:          linux/arm64\n  Experimental:     false\n containerd:\n  Version:          1.7.27\n  GitCommit:        05044ec0a9a75232cad458027ca83437aae3f4da\n runc:\n  Version:          1.2.5\n  GitCommit:        v1.2.5-0-g59923ef\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\nroot@node1:~# runc --version\nrunc version 1.2.5\ncommit: v1.2.5-0-g59923ef\nspec: 1.2.0\ngo: go1.23.7\nlibseccomp: 2.5.3\nroot@node1:~#\nroot@node1:~# containerd --version\ncontainerd containerd.io 1.7.27 05044ec0a9a75232cad458027ca83437aae3f4da\nroot@node1:~#\n```\n\n### docker info\n\n```bash\nroot@node1:~# docker info\nClient: Docker Engine - Community\n Version:    28.3.3\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.26.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.39.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n  model: Docker Model Runner (EXPERIMENTAL) (Docker Inc.)\n    Version:  v0.1.36\n    Path:     /usr/libexec/docker/cli-plugins/docker-model\n\nServer:\n Containers: 4\n  Running: 0\n  Paused: 0\n  Stopped: 4\n Images: 1\n Server Version: 28.3.3\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 nvidia runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 05044ec0a9a75232cad458027ca83437aae3f4da\n runc version: v1.2.5-0-g59923ef\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n  Operating System: Ubuntu 22.04.5 LTS\n OSType: linux\n Architecture: aarch64\n CPUs: 144\n Total Memory: 1.654TiB\n Name: node1\n ID: 7cb667eb-c29c-4335-8a98-b53743a5c74e\n Docker Root Dir: /docker/var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n```\n\n### Additional Info\n\nAnd my grub configuration as below:\n\nset default=0\nset timeout=10\nmenuentry 'ubuntu22.04' {\n        linux (http,10.0.0.10)/OS/ubuntu-arm/vmlinuz-6.8.0-1031-64k initrd=ubuntu_22.04.5.img boot=live fetch=http://10.0.0.10/DiagOS/ubuntu-arm/squashfs_ubuntu_22.04.5.img root=/dev/loop0 rw dhcp net.ifnames=0 console=ttyS0,115200n8 quiet\n        initrd (http,10.0.0.10)/OS/ubuntu-arm/ubuntu_22.04.5.img\n        boot\n",
      "solution": "This looks to be an issue with the daemon, not the CLI, so probably would've better been reported in https://github.com/moby/moby/issues\n\n\nI think the `DOCKER_RAMDISK` env-var would the equivalent to enable `--no-pivot-root`; https://docs.docker.com/reference/cli/dockerd/#environment-variables\n\n```\nDOCKER_RAMDISK | If set this disables\u00a0pivot_root.\n```\n\nsource code; https://github.com/moby/moby/blob/a34c4d9bb99f02c79722016cf77fad01f0f980e4/daemon/runtime_unix.go#L64\n\n\nBut that env-var must be set for the _daemon_ (`dockerd`) process, which can be done by adding a systemd override for the `docker.service` unit; to create the override, you can use `systemctl edit docker.service`, which creates a new override file (or edits the existing one); you can follow instructions for setting proxy env-vars for the service (https://docs.docker.com/engine/daemon/proxy/#systemd-unit-file), but basically the override file should look something like;\n\n```systemd\n[Service]\nEnvironment=\"DOCKER_RAMDISK=1\"\n```\n\nAfter saving, you need to reload the unit and restart the daemon;\n\n```bash\nsudo systemctl daemon-reload\nsudo systemctl restart docker\n```\n\n\nDoes that solve your problem?\n\n\n\n",
      "labels": [
        "kind/question",
        "status/0-triage",
        "status/more-info-needed",
        "version/28.3"
      ],
      "created_at": "2025-08-11T11:13:16Z",
      "closed_at": "2025-08-20T09:31:56Z",
      "url": "https://github.com/docker/cli/issues/6251",
      "comments_count": 2
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 5243,
      "title": "Error while removing network: network <netwok_name> id <network_id> has active endpoints",
      "problem": "### Description\r\n\r\nSometimes a network fails to be removed with the following error :\r\n```\r\n$ docker network rm s_web\r\nError response from daemon: error while removing network: network s_web id 6b63ee4e95208b6869d072ef45803dbd2b9a517b03b1679c586814a66552f037 has active endpoints\r\n```\r\n\r\nRunning `docker inspect s_web` shows no containers attached. In fact there are no containers at all.\r\n```\r\n$ docker network inspect s_web\r\n[\r\n    {\r\n        \"Name\": \"s_web\",\r\n        \"Id\": \"6b63ee4e95208b6869d072ef45803dbd2b9a517b03b1679c586814a66552f037\",\r\n        \"Created\": \"2024-07-08T11:17:24.896578195+02:00\",\r\n        \"Scope\": \"local\",\r\n        \"Driver\": \"bridge\",\r\n        \"EnableIPv6\": false,\r\n        \"IPAM\": {\r\n            \"Driver\": \"default\",\r\n            \"Options\": null,\r\n            \"Config\": [\r\n                {\r\n                    \"Subnet\": \"172.252.0.0/16\"\r\n                }\r\n            ]\r\n        },\r\n        \"Internal\": false,\r\n        \"Attachable\": false,\r\n        \"Ingress\": false,\r\n        \"ConfigFrom\": {\r\n            \"Network\": \"\"\r\n        },\r\n        \"ConfigOnly\": false,\r\n        \"Containers\": {},\r\n        \"Options\": {},\r\n        \"Labels\": {\r\n            \"com.docker.compose.network\": \"web\",\r\n            \"com.docker.compose.project\": \"s\",\r\n            \"com.docker.compose.version\": \"2.28.1\"\r\n        }\r\n    }\r\n]\r\n```\r\n\r\n```\r\n$ docker container ls -a\r\nCONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\r\n```\r\n\r\nThe only way to delete the network is to restart the docker service and then run the delete command again.\r\nThis error appears randomly, so it is fairly hard to reproduce willingly.\r\n\r\n### Reproduce\r\n\r\ndocker compose up # compose.yaml in additional info\r\ndocker compose down --remove-orphans -v\r\ndocker network rm s_web\r\n\r\n### Expected behavior\r\n\r\nThe network should be removed\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           27.0.3\r\n API version:       1.46\r\n Go version:        go1.21.11\r\n Git commit:        7d4bcd8\r\n Built:             Sat Jun 29 00:04:07 2024\r\n OS/Arch:           linux/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          27.0.3\r\n  API version:      1.46 (minimum version 1.24)\r\n  Go version:       go1.21.11\r\n  Git commit:       662f78c\r\n  Built:            Sat Jun 29 00:02:31 2024\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.7.18\r\n  GitCommit:        ae71819c4f5e67bb4d5ae76a6b735f29cc25774e\r\n runc:\r\n  Version:          1.7.18\r\n  GitCommit:        v1.1.13-0-g58aa920\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n rootlesskit:\r\n  Version:          2.0.2\r\n  ApiVersion:       1.1.1\r\n  NetworkDriver:    slirp4netns\r\n  PortDriver:       builtin\r\n  StateDir:         /run/user/1000/dockerd-rootless\r\n slirp4netns:\r\n  Version:          1.2.3\r\n  GitCommit:        c22fde291bb35b354e6ca44d13be181c76a0a432\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:    27.0.3\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.15.1\r\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.28.1\r\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 18\r\n Server Version: 27.0.3\r\n Storage Driver: fuse-overlayfs\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc io.containerd.runc.v2\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: ae71819c4f5e67bb4d5ae76a6b735f29cc25774e\r\n runc version: v1.1.13-0-g58aa920\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n  rootless\r\n  cgroupns\r\n Kernel Version: 5.14.0-427.22.1.el9_4.x86_64\r\n Operating System: Rocky Linux 9.4 (Blue Onyx)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 11.43GiB\r\n Name: <redacted>\r\n ID: 07ebe6ee-777a-47e0-b6b7-d2ada901fe0e\r\n Docker Root Dir: /home/devroot/.local/share/docker\r\n Debug Mode: false\r\n Username: <redacted>\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: No cpu cfs quota support\r\nWARNING: No cpu cfs period support\r\nWARNING: No cpu shares support\r\nWARNING: No cpuset support\r\nWARNING: No io.weight support\r\nWARNING: No io.weight (per device) support\r\nWARNING: No io.max (rbps) support\r\nWARNING: No io.max (wbps) support\r\nWARNING: No io.max (riops) support\r\nWARNING: No io.max (wiops) support\r\nWARNING: bridge-nf-call-iptables is disabled\r\nWARNING: bridge-nf-call-ip6tables is disabled\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\nRunning docker rootless on Rocky linux.\r\nSame problem seen on multiple servers\r\nUsing it as CICD runner, the network is created with `docker compose up`, and should be removed with `docker compose down --remove-orphans -v`\r\n\r\nHere is the compose.yaml with some redacted values :\r\n```\r\nservices:\r\n  php:\r\n    image: <redacted_some_private_symfony_php_image>\r\n    working_dir: /home/app\r\n    volumes:\r\n      - .:/home/app\r\n      - ~/.ssh:/home/.ssh\r\n      - ./docker/php/conf.d/app.ini:/usr/local/etc/php/conf.d/app.ini:ro\r\n    networks:\r\n      - web\r\n    external_links:\r\n      - nginx-proxy:mercure.docker\r\n    environment:\r\n        - XDEBUG_CONFIG=client_host=host.docker.internal client_port=9000\r\n        - PHP_IDE_CONFIG=serverName=php-docker\r\n\r\n  nginx:\r\n    image: nginx:1.23-alpine\r\n    volumes:\r\n      - .:/home/app\r\n      - ./docker/nginx/nginx.conf:/etc/nginx/nginx.conf\r\n      - ./docker/nginx/sites/:/etc/nginx/sites-available\r\n      - ./docker/nginx/conf/mime.types:/etc/nginx/mime.types\r\n    networks:\r\n      web:\r\n        aliases:\r\n          - nginx.docker\r\n          - api.nginx.docker\r\n    links:\r\n      - php\r\n\r\n  nginx-proxy:\r\n    image: jwilder/nginx-proxy\r\n    volumes:\r\n      - /var/run/docker.sock:/tmp/docker.sock:ro\r\n      - ./docker/nginx-proxy/cert/:/etc/nginx/certs/\r\n      - ./docker/nginx-proxy/nginx.conf:/etc/nginx/nginx.conf\r\n    networks:\r\n      - web\r\n\r\n  redis:\r\n    image: redis:7-alpine\r\n    networks:\r\n      - web\r\n\r\n  cypress:\r\n    image: cypress/base:latest\r\n    working_dir: /home/app/front\r\n    volumes:\r\n      - .:/home/app\r\n      - cypress:/root/.cache/Cypress/\r\n    networks:\r\n      - web\r\n    external_links:\r\n      - nginx-proxy:nginx.docker\r\n      - nginx-proxy:api.nginx.docker\r\n\r\n  database:\r\n    image: postgres:14.5-alpine\r\n    user: root\r\n    environment:\r\n      - POSTGRES_PASSWORD= <redacted>\r\n      - PGDATA=/data/postgres\r\n    ports:\r\n      - \"5432:5432\"\r\n    networks:\r\n      - web\r\n    volumes:\r\n      - database-data:/data/postgres\r\n\r\n  azurite:\r\n    image: mcr.microsoft.com/azure-storage/azurite\r\n    command: \"azurite --loose --blobHost 0.0.0.0 --blobPort 10000 --queueHost 0.0.0.0 --queuePort 10001 --location /workspace --debug /workspace/debug.log\"\r\n    ports:\r\n      - \"10010:10000\"\r\n      - \"10011:10001\"\r\n      - \"10012:10002\"\r\n    volumes:\r\n      - azurite:/workspace\r\n    networks:\r\n      - web\r\n\r\n  elasticsearch:\r\n    build: docker/elasticsearch\r\n    environment:\r\n      - cluster.name=docker-cluster\r\n      - bootstrap.memory_lock=true\r\n      - discovery.type=single-node\r\n      - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" # 512mo HEAP\r\n      - ELASTIC_PASSWORD=<redacted>\r\n    ports:\r\n      - 9200:9200\r\n    networks:\r\n      web:\r\n        aliases:\r\n          - elasticsearch.docker\r\n\r\n  kibana:\r\n    image: docker.elastic.co/kibana/kibana:8.4.3\r\n    environment:\r\n      ELASTICSEARCH_URL: http://elasticsearch.docker:9200\r\n    depends_on:\r\n      - elasticsearch\r\n    ports:\r\n      - 5601:5601\r\n    volumes:\r\n      - elasticsearch-data:/usr/share/kibana/data\r\n    networks:\r\n      web:\r\n        aliases:\r\n          - kibana.docker\r\n\r\n  mercure:\r\n    image: dunglas/mercure\r\n    restart: unless-stopped\r\n    environment:\r\n      SERVER_NAME: ':80'\r\n      MERCURE_PUBLISHER_JWT_KEY: <redacted>\r\n      MERCURE_SUBSCRIBER_JWT_KEY: <redacted>\r\n      MERCURE_EXTRA_DIRECTIVES: |\r\n        cors_origins *\r\n    command: /usr/bin/caddy run --config /etc/caddy/dev.Caddyfile\r\n    volumes:\r\n      - caddy_data:/data\r\n      - caddy_config:/config\r\n    networks:\r\n      - web\r\n\r\n  rabbitmq:\r\n    image: rabbitmq:3-management\r\n    networks:\r\n      web:\r\n        aliases:\r\n          - rabbitmq.docker\r\n\r\n  smtp:\r\n    image: schickling/mailcatcher\r\n    environment:\r\n      VIRTUAL_PORT: 1080\r\n    networks:\r\n      - web\r\n\r\nnetworks:\r\n  web:\r\n    ipam:\r\n      config:\r\n        - subnet: \"172.252.0.0/16\"\r\n\r\nvolumes:\r\n  database-data:\r\n  cypress:\r\n  elasticsearch-data:\r\n  pgadmin-data:\r\n  caddy_data:\r\n  caddy_config:\r\n  azurite:\r\n```",
      "solution": "@kodukon There was a tentative fix released in v28.1. Whether it fully fixes the issue, or there are remnants of that error is unclear because we've been unable to reproduce with a simple reproducer (i.e. deterministically). For more details see this comment: https://github.com/moby/moby/issues/42119#issuecomment-2795145433.\n\nNote that v27.5.1 is outdated / unmaintained, and two security fixes have been released recently. So please update your Docker Engine.\n\nAs this is an Engine issue, let me close this ticket in favor of https://github.com/moby/moby/issues/42119.",
      "labels": [
        "kind/bug",
        "status/0-triage",
        "area/networking",
        "version/27.0"
      ],
      "created_at": "2024-07-08T12:01:11Z",
      "closed_at": "2025-08-13T08:21:32Z",
      "url": "https://github.com/docker/cli/issues/5243",
      "comments_count": 4
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 5874,
      "title": "Duplicate PUT request during push of BuildKit-built image",
      "problem": "### Description\n\nOur upstream container repository provider (Oracle Cloud) reports that they are seeing duplicated requests during the push of an image, and thus are serving a 409 Conflict response on the second request.\n\nThe requests occur only during the initial push of a new tag on an image built with buildkit (the default in the version of Docker I am running), and not when the legacy builder is used.\n\nSuccessive calls to push the same tag work, however new tags on an existing repository exhibit the same behaviour. \n\nWe have found that older versions of the docker CLI (we have tested up to Docker Engine v26.0.0) work without this issue.\n\nPushing to Docker Hub works as expected. \n\n### Reproduce\n\n#### Dockerfile\n`text.txt` is a simple text file with \"Hello World\" inside\n\n```\nFROM busybox:latest\nCOPY test.txt .\nCMD [\"echo\", \"hello\"]\n```\n\n#### Failing request\n1. `docker build -t lhr.ocir.io/<redacted>/ocir-test:buildx-test .`\nObserve the build successfully complete\n2. `docker push lhr.ocir.io/<redacted>/ocir-test:buildx-test`\nObserve a 409 error: `failed commit on ref \"manifest-sha256:fcd195ebabd830937ca77cbe21c9ddbe3499c4517d211ca5c3fcd6d9931a8a10\": unexpected status from PUT request to https://lhr.ocir.io/v2/<redacted>/ocir-test/manifests/buildx-test: 409 Conflict`\n\n#### Successful request\n1. `DOCKER_BUILDKIT=0 docker build -t lhr.ocir.io/<redacted>/ocir-test:legacy-test .`\nObserve the build successfully complete\n2. `docker push lhr.ocir.io/<redacted>/ocir-test:legacy-test`\nObserve the push successfully complete\n\n### Expected behavior\n\n`docker push` of a BuildKit-built image should not produce duplicate requests during the push of the image to the container registry.\n\n### docker version\n\n```bash\nClient:\n Version:           27.5.1\n API version:       1.47\n Go version:        go1.22.11\n Git commit:        9f9e405\n Built:             Wed Jan 22 13:37:19 2025\n OS/Arch:           darwin/arm64\n Context:           desktop-linux\n\nServer: Docker Desktop 4.38.0 (181591)\n Engine:\n  Version:          27.5.1\n  API version:      1.47 (minimum version 1.24)\n  Go version:       go1.22.11\n  Git commit:       4c9b3b0\n  Built:            Wed Jan 22 13:41:25 2025\n  OS/Arch:          linux/arm64\n  Experimental:     false\n containerd:\n  Version:          1.7.25\n  GitCommit:        bcc810d6b9066471b0b6fa75f557a15a1cbf31bb\n runc:\n  Version:          1.1.12\n  GitCommit:        v1.1.12-0-g51d5e946\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient:\n Version:    27.5.1\n Context:    desktop-linux\n Debug Mode: false\n Plugins:\n  ai: Ask Gordon - Docker Agent (Docker Inc.)\n    Version:  v0.7.3\n    Path:     /Users/pwilliams/.docker/cli-plugins/docker-ai\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.20.1-desktop.2\n    Path:     /Users/pwilliams/.docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.32.4-desktop.1\n    Path:     /Users/pwilliams/.docker/cli-plugins/docker-compose\n  debug: Get a shell into any image or container (Docker Inc.)\n    Version:  0.0.38\n    Path:     /Users/pwilliams/.docker/cli-plugins/docker-debug\n  desktop: Docker Desktop commands (Beta) (Docker Inc.)\n    Version:  v0.1.4\n    Path:     /Users/pwilliams/.docker/cli-plugins/docker-desktop\n  dev: Docker Dev Environments (Docker Inc.)\n    Version:  v0.1.2\n    Path:     /Users/pwilliams/.docker/cli-plugins/docker-dev\n  extension: Manages Docker extensions (Docker Inc.)\n    Version:  v0.2.27\n    Path:     /Users/pwilliams/.docker/cli-plugins/docker-extension\n  feedback: Provide feedback, right in your terminal! (Docker Inc.)\n    Version:  v1.0.5\n    Path:     /Users/pwilliams/.docker/cli-plugins/docker-feedback\n  init: Creates Docker-related starter files for your project (Docker Inc.)\n    Version:  v1.4.0\n    Path:     /Users/pwilliams/.docker/cli-plugins/docker-init\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\n    Version:  0.6.0\n    Path:     /Users/pwilliams/.docker/cli-plugins/docker-sbom\n  scout: Docker Scout (Docker Inc.)\n    Version:  v1.16.1\n    Path:     /Users/pwilliams/.docker/cli-plugins/docker-scout\n\nServer:\n Containers: 0\n  Running: 0\n  Paused: 0\n  Stopped: 0\n Images: 17\n Server Version: 27.5.1\n Storage Driver: overlayfs\n  driver-type: io.containerd.snapshotter.v1\n Logging Driver: json-file\n Cgroup Driver: cgroupfs\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: bcc810d6b9066471b0b6fa75f557a15a1cbf31bb\n runc version: v1.1.12-0-g51d5e946\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: unconfined\n  cgroupns\n Kernel Version: 6.12.5-linuxkit\n Operating System: Docker Desktop\n OSType: linux\n Architecture: aarch64\n CPUs: 12\n Total Memory: 7.653GiB\n Name: docker-desktop\n ID: a0e8fab6-f787-4bbe-9402-df707d86743b\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n HTTP Proxy: http.docker.internal:3128\n HTTPS Proxy: http.docker.internal:3128\n No Proxy: hubproxy.docker.internal\n Labels:\n  com.docker.desktop.address=unix:///Users/pwilliams/Library/Containers/com.docker.docker/Data/docker-cli.sock\n Experimental: false\n Insecure Registries:\n  hubproxy.docker.internal:5555\n  127.0.0.0/8\n Live Restore Enabled: false\n\nWARNING: daemon is not using the default seccomp profile\n```\n\n### Additional Info\n\n_No response_",
      "solution": "Admittedly I myself am unsure where the issue lies - but the issue does seem to be at the point of push, rather than the build itself.\n\n---\n\nNot sure yet!\n\nIf you disable BuildKit, Docker uses the legacy builder, which produces a single-arch image (for the platform you're building for). With BuildKit and the containerd image store enabled, BuildKit produces a multi-platform image (\"OCI Image Index\") as well as attestations, SBOMS related to the image. \n\nMy first thinking there is that it's not unlikely that pushing such an image would try a `PUT` to make sure the manifest exists before pushing the content related to it (or a similar flow). \nAt a quick glance, the OCI distribution spec (the specification for OCI registries) does not mention a `409` status for the `PUT /v2/<name>/manifests/<reference>` endpoint (`end-7`); https://github.com/opencontainers/distribution-spec/blob/v1.1.1/spec.md#endpoints, and I don't see (at a quick glance) in the \"pushing manifests\" section, other than 413 being allowed for manifests larger than what the registry wants to accept https://github.com/opencontainers/distribution-spec/blob/v1.1.1/spec.md#pushing-manifests\n\nPossibly the registry in this case is configured for immutable tags, therefore not accepting.\n\nIt might be worth trying if building without attestations works around the problem (which would narrow down things);\nhttps://docs.docker.com/build/metadata/attestations/#creating-attestations\n\n\n---\n\n> Using `--provenance=false` with BuildKit does allow the image to be pushed successfully.\n\nWe were seeing the same issue with GCP artifact registry, when immutable tags are enabled. Would get a\n\n```\nfailed commit on ref \"manifest-sha256:36e180851ba85edf644c932f053698898062af89bca6fc74fe67e282171e9e8e\": unexpected status from PUT request to https://europe-west1-docker.pkg.dev/v2/<snip>: 400 Bad Request\n```\n\nbut adding `--provenance=false` to `docker buildx build` fixed it \ud83c\udf89 ",
      "labels": [
        "kind/bug",
        "area/distribution",
        "status/more-info-needed",
        "version/27.5"
      ],
      "created_at": "2025-02-27T16:30:21Z",
      "closed_at": "2025-06-24T12:48:23Z",
      "url": "https://github.com/docker/cli/issues/5874",
      "comments_count": 16
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 6128,
      "title": "docker-mcp fails on Ubuntu due to incorrect socket path",
      "problem": "### Description\n\n## Description\n\nWhen running the following command on **Ubuntu** with Docker Desktop installed:\n\n```bash\n~/.docker/cli-plugins/docker-mcp gateway run\n```\n\nIt fails with the following error:\n\n```\nreading secrets: finding secrets []: exit status 125\n```\n\n## Root Cause (via strace)\n\nUsing `strace`, I found that the CLI tries to access the following socket path:\n\n```\n~/Library/Containers/com.docker.docker/Data/docker.raw.sock\n```\n\nThis path appears to be other-os-specific. On **Ubuntu**, the actual socket is located at:\n\n```\n~/.docker/desktop/docker.raw.sock\n```\n\n## Workaround\n\nTo resolve the issue, I created a symbolic link that mimics the path:\n\n```bash\nmkdir -p ~/Library/Containers/com.docker.docker/Data/\ncd ~/Library/Containers/com.docker.docker/Data/\nln -s ../../../../.docker/desktop/docker.raw.sock docker.raw.sock\n```\n\nAfter this, `docker-mcp gateway run` works as expected.\n\n## Expected Behavior\n\nThe CLI plugin should correctly detect platform-specific socket paths (e.g., Linux vs macOS), or allow configuration/override of the socket path.\n\n## Environment\n\n* OS: Ubuntu 22.04\n* Docker CLI Version: Docker version 28.2.2, build e6534b4\n* Docker Daemon Version: Docker version 28.2.2, build 45873be\n* Docker Desktop Version: Docker Desktop 4.42.0 (195023)\n* docker-mcp Plugin Version: dev, commit cb67dfd17ba46115f13869a87d5449e6a78110b1\n\n### Reproduce\n\n1. Install Docker Desktop on Ubuntu\n2. docker mcp gateway run\n> `reading secrets: finding secrets []: exit status 125`\n\n### Expected behavior\n\nThe CLI plugin should correctly detect platform-specific socket paths (e.g., Ubuntu).\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           28.2.2\n API version:       1.50\n Go version:        go1.24.3\n Git commit:        e6534b4\n Built:             Fri May 30 12:07:27 2025\n OS/Arch:           linux/amd64\n Context:           desktop-linux\n\nServer: Docker Desktop 4.42.0 (195023)\n Engine:\n  Version:          28.2.2\n  API version:      1.50 (minimum version 1.24)\n  Go version:       go1.24.3\n  Git commit:       45873be\n  Built:            Fri May 30 12:07:26 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.7.27\n  GitCommit:        05044ec0a9a75232cad458027ca83437aae3f4da\n runc:\n  Version:          1.2.5\n  GitCommit:        v1.2.5-0-g59923ef\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    28.2.2\n Context:    desktop-linux\n Debug Mode: false\n Plugins:\n  ai: Docker AI Agent - Ask Gordon (Docker Inc.)\n    Version:  v1.4.0\n    Path:     /home/bevis/.docker/cli-plugins/docker-ai\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.24.0-desktop.2\n    Path:     /home/bevis/.docker/cli-plugins/docker-buildx\n  cloud: Docker Cloud (Docker Inc.)\n    Version:  v0.3.9\n    Path:     /home/bevis/.docker/cli-plugins/docker-cloud\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.36.2-desktop.1\n    Path:     /home/bevis/.docker/cli-plugins/docker-compose\n  debug: Get a shell into any image or container (Docker Inc.)\n    Version:  0.0.41\n    Path:     /home/bevis/.docker/cli-plugins/docker-debug\n  desktop: Docker Desktop commands (Docker Inc.)\n    Version:  v0.1.9\n    Path:     /home/bevis/.docker/cli-plugins/docker-desktop\n  extension: Manages Docker extensions (Docker Inc.)\n    Version:  v0.2.29\n    Path:     /home/bevis/.docker/cli-plugins/docker-extension\n  init: Creates Docker-related starter files for your project (Docker Inc.)\n    Version:  v1.4.0\n    Path:     /home/bevis/.docker/cli-plugins/docker-init\n  mcp: Docker MCP Plugin (Docker Inc.)\n    Version:  dev\n    Path:     /home/bevis/.docker/cli-plugins/docker-mcp\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\n    Version:  0.6.0\n    Path:     /home/bevis/.docker/cli-plugins/docker-sbom\n  scout: Docker Scout (Docker Inc.)\n    Version:  v1.18.1\n    Path:     /home/bevis/.docker/cli-plugins/docker-scout\n\nServer:\n Containers: 0\n  Running: 0\n  Paused: 0\n  Stopped: 0\n Images: 1\n Server Version: 28.2.2\n Storage Driver: overlayfs\n  driver-type: io.containerd.snapshotter.v1\n Logging Driver: json-file\n Cgroup Driver: cgroupfs\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: active\n  NodeID: ljswm0urxswzwzf3ugvp0k7ld\n  Is Manager: true\n  ClusterID: kp1jg8masrt7bxdqy4s4fjrho\n  Managers: 1\n  Nodes: 1\n  Default Address Pool: 10.0.0.0/8  \n  SubnetSize: 24\n  Data Path Port: 4789\n  Orchestration:\n   Task History Retention Limit: 5\n  Raft:\n   Snapshot Interval: 10000\n   Number of Old Snapshots to Retain: 0\n   Heartbeat Tick: 1\n   Election Tick: 10\n  Dispatcher:\n   Heartbeat Period: 5 seconds\n  CA Configuration:\n   Expiry Duration: 3 months\n   Force Rotate: 0\n  Autolock Managers: false\n  Root Rotation In Progress: false\n  Node Address: 192.168.65.9\n  Manager Addresses:\n   192.168.65.9:2377\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 05044ec0a9a75232cad458027ca83437aae3f4da\n runc version: v1.2.5-0-g59923ef\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.10.14-linuxkit\n Operating System: Docker Desktop\n OSType: linux\n Architecture: x86_64\n CPUs: 12\n Total Memory: 5.575GiB\n Name: docker-desktop\n ID: 3b1f6e89-6f78-412b-8cd2-1564080c1b4c\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n HTTP Proxy: http.docker.internal:3128\n HTTPS Proxy: http.docker.internal:3128\n No Proxy: hubproxy.docker.internal\n Username: ga2006088445\n Labels:\n  com.docker.desktop.address=unix:///home/bevis/.docker/desktop/docker-cli.sock\n Experimental: false\n Insecure Registries:\n  hubproxy.docker.internal:5555\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n\nWARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set\n```\n\n### Additional Info\n\n### trace.log\n```\n47350 connect(3, {sa_family=AF_UNIX, sun_path=\"/home/bevis/Library/Containers/com.docker.docker/Data/docker.raw.sock\"}, 72) = -1 ENOENT (No such file or directory)\n```",
      "solution": "Thanks for reporting! This looks to be related to the `docker-mcp`  CLI plugin, which currently is closed-source and not maintained in this repository.\n\nHowever, I checked with a colleague (@dgageot) who works on that plugin, and he informed me that this was known bug that has been fixed, and the fix will be included in the next version of Docker Desktop (4.43).\n\n\nI'll close this ticket because of the above, but feel free to continue the conversation \ud83d\udc4d \n",
      "labels": [
        "kind/bug",
        "status/0-triage"
      ],
      "created_at": "2025-06-13T02:48:52Z",
      "closed_at": "2025-06-17T12:21:16Z",
      "url": "https://github.com/docker/cli/issues/6128",
      "comments_count": 2
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 6106,
      "title": "\"docker pull\" from Gitlab with S3/Minio fails with \"multiple authentication types\"",
      "problem": "### Description\n\nI'm trying to pull an image from our private Gitlab registry with a S3 compatible [Minio](https://github.com/minio/minio) backend. Several HTTP Requests succeeds but one fails with status code 400. \n\n**This bug only appears on MacOS**. Pulling on a linux machine works perfectly. I already tried to logout and login again to the private registry. I also purged docker completely, removed all configuration files and reinstalled it again.\n\nI used tcpdump to trace the HTTP requests on the Minio server. A couple of successful HTTP requests look like this:\n```http\nGET /gitlab-images/docker/registry/v2/blobs/sha256/3c/3ccc92c3be6175b1c12713b0c125b0cc49aba8d7852b6cdf93d336770775381d/data HTTP/1.1\nHost: images.bic.myhost.tld\nUser-Agent: docker-distribution/v4.21.0-gitlab (go1.23.9) aws-sdk-go/1.55.5 (go1.23.9; linux; amd64)\nAuthorization: AWS4-HMAC-SHA256 Credential=XXXXXX/20250524/us-east-1/s3/aws4_request, SignedHeaders=host;range;x-amz-content-sha256;x-amz-date, Signature=XXXXXX\n```\n\nThen this one fails with status code 400 because it contains two authentication types:\n```http\nGET /gitlab-images/docker/registry/v2/blobs/sha256/f1/f14ee681fe6418a8a69642b3a741e4a50c9b569f2dce4235f9344f8737467bf3/data?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=XXXXXXX%2F20250524%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250524T085244Z&X-Amz-Expires=1200&X-Amz-SignedHeaders=host&X-Amz-Signature=XXXXXXX HTTP/1.1\nHost: images.bic.mydomain.tld\nUser-Agent: docker/28.1.1 go/go1.23.8 git-commit/01f442b kernel/6.10.14-linuxkit os/linux arch/arm64 containerd-client/2.0.5+unknown storage-driver/overlayfs UpstreamClient(Docker-Client/28.1.1 \\(darwin\\))\nAccept: application/vnd.docker.container.image.v1+json, */*\nAccept-Encoding: zstd;q=1.0, gzip;q=0.8, deflate;q=0.5\nAuthorization: Bearer [2600 bytes]\nBaggage: trigger=api\n```\n\n```xml\n<Error>\n\t<Code>InvalidRequest</Code>\n\t<Message>Invalid Request (request has multiple authentication types, please use one)</Message>\n\t<Resource>/gitlab-images/docker/registry/v2/blobs/sha256/f1/f14ee681fe6418a8a69642b3a741e4a50c9b569f2dce4235f9344f8737467bf3/data</Resource>\n\t<RequestId></RequestId>\n\t<HostId>6041f9a1-841a-49f9-ad08-299d080713eb</HostId>\n</Error>\n```\n\n### Reproduce\n\n1. docker login bic.myhost.tld\n2. docker pull bic.myhost.tld/group/project:latest\n\n### Expected behavior\n\nPulling should succeed.\n\n### docker version\n\n```bash\nClient:\n Version:           28.1.1\n API version:       1.49\n Go version:        go1.23.8\n Git commit:        4eba377\n Built:             Fri Apr 18 09:49:45 2025\n OS/Arch:           darwin/arm64\n Context:           desktop-linux\n\nServer: Docker Desktop 4.41.2 (191736)\n Engine:\n  Version:          28.1.1\n  API version:      1.49 (minimum version 1.24)\n  Go version:       go1.23.8\n  Git commit:       01f442b\n  Built:            Fri Apr 18 09:52:08 2025\n  OS/Arch:          linux/arm64\n  Experimental:     false\n containerd:\n  Version:          1.7.27\n  GitCommit:        05044ec0a9a75232cad458027ca83437aae3f4da\n runc:\n  Version:          1.2.5\n  GitCommit:        v1.2.5-0-g59923ef\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient:\n Version:    28.1.1\n Context:    desktop-linux\n Debug Mode: false\n Plugins:\n  ai: Docker AI Agent - Ask Gordon (Docker Inc.)\n    Version:  v1.1.7\n    Path:     /Users/julian/.docker/cli-plugins/docker-ai\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.23.0-desktop.1\n    Path:     /Users/julian/.docker/cli-plugins/docker-buildx\n  cloud: Docker Cloud (Docker Inc.)\n    Version:  v0.3.0\n    Path:     /Users/julian/.docker/cli-plugins/docker-cloud\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.36.0-desktop.1\n    Path:     /Users/julian/.docker/cli-plugins/docker-compose\n  debug: Get a shell into any image or container (Docker Inc.)\n    Version:  0.0.38\n    Path:     /Users/julian/.docker/cli-plugins/docker-debug\n  desktop: Docker Desktop commands (Docker Inc.)\n    Version:  v0.1.8\n    Path:     /Users/julian/.docker/cli-plugins/docker-desktop\n  dev: Docker Dev Environments (Docker Inc.)\n    Version:  v0.1.2\n    Path:     /Users/julian/.docker/cli-plugins/docker-dev\n  extension: Manages Docker extensions (Docker Inc.)\n    Version:  v0.2.27\n    Path:     /Users/julian/.docker/cli-plugins/docker-extension\n  init: Creates Docker-related starter files for your project (Docker Inc.)\n    Version:  v1.4.0\n    Path:     /Users/julian/.docker/cli-plugins/docker-init\n  mcp: Docker MCP Plugin (Docker Inc.)\n    Version:  dev\n    Path:     /Users/julian/.docker/cli-plugins/docker-mcp\n  model: Docker Model Runner (Docker Inc.)\n    Version:  v0.1.23\n    Path:     /Users/julian/.docker/cli-plugins/docker-model\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\n    Version:  0.6.0\n    Path:     /Users/julian/.docker/cli-plugins/docker-sbom\n  scout: Docker Scout (Docker Inc.)\n    Version:  v1.18.0\n    Path:     /Users/julian/.docker/cli-plugins/docker-scout\n\nServer:\n Containers: 0\n  Running: 0\n  Paused: 0\n  Stopped: 0\n Images: 0\n Server Version: 28.1.1\n Storage Driver: overlayfs\n  driver-type: io.containerd.snapshotter.v1\n Logging Driver: json-file\n Cgroup Driver: cgroupfs\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 05044ec0a9a75232cad458027ca83437aae3f4da\n runc version: v1.2.5-0-g59923ef\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: unconfined\n  cgroupns\n Kernel Version: 6.10.14-linuxkit\n Operating System: Docker Desktop\n OSType: linux\n Architecture: aarch64\n CPUs: 12\n Total Memory: 7.653GiB\n Name: docker-desktop\n ID: 60bf5177-33ef-4888-bedf-b484738c221a\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n HTTP Proxy: http.docker.internal:3128\n HTTPS Proxy: http.docker.internal:3128\n No Proxy: hubproxy.docker.internal\n Labels:\n  com.docker.desktop.address=unix:///Users/julian/Library/Containers/com.docker.docker/Data/docker-cli.sock\n Experimental: false\n Insecure Registries:\n  hubproxy.docker.internal:5555\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n\nWARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set\nWARNING: daemon is not using the default seccomp profile\n```\n\n### Additional Info",
      "solution": "I'm wondering if this is related to the containerd image store integration (containerd may have different handling for authentication).\n\nI see the request contains both a bearer token, and URL parameters for authentication.\n\nFor testing, could you check if this issue is resolved if you disable this feature? :warning: doing so switches the storage of images, which means that your existing images won't be visible (but you can switch back to make them appear again); https://docs.docker.com/desktop/features/containerd/#enable-the-containerd-image-store\n\n\nCan you also post `docker version` and `docker info` for the Linux machine?",
      "labels": [
        "kind/bug",
        "status/0-triage",
        "area/distribution",
        "status/more-info-needed",
        "area/authentication",
        "version/28.1"
      ],
      "created_at": "2025-05-24T09:19:59Z",
      "closed_at": "2025-05-24T10:52:12Z",
      "url": "https://github.com/docker/cli/issues/6106",
      "comments_count": 4
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 6105,
      "title": "Docker inspect not working in Resource Saver mode",
      "problem": "### Description\n\nI noticed a bug, the command \"inspect\" is not working when docker is in Resource Saver mode.\nI got the message \"Error: No such object: MyImage\", but when the resource saver is off, I get all the infos of my image. The command \"docker images\" lists my image when docker is in Resource Saver mode btw.\n\n### Reproduce\n\nWait for docker to be in Resource Saver Mode\nThen do \"docker inspect MyImage\"\n\n\n### Expected behavior\n\n_No response_\n\n### docker version\n\n```bash\nClient:\n Version:           28.1.1\n API version:       1.49\n Go version:        go1.23.8\n Git commit:        4eba377\n Built:             Fri Apr 18 09:53:24 2025\n OS/Arch:           windows/amd64\n Context:           desktop-linux\n\nServer: Docker Desktop 4.41.2 (191736)\n Engine:\n  Version:          28.1.1\n  API version:      1.49 (minimum version 1.24)\n  Go version:       go1.23.8\n  Git commit:       01f442b\n  Built:            Fri Apr 18 09:52:57 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.7.27\n  GitCommit:        05044ec0a9a75232cad458027ca83437aae3f4da\n runc:\n  Version:          1.2.5\n  GitCommit:        v1.2.5-0-g59923ef\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient:\n Version:    28.1.1\n Context:    desktop-linux\n Debug Mode: false\n Plugins:\n  ai: Docker AI Agent - Ask Gordon (Docker Inc.)\n    Version:  v1.1.7\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-ai.exe\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.23.0-desktop.1\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-buildx.exe\n  cloud: Docker Cloud (Docker Inc.)\n    Version:  v0.3.0\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-cloud.exe\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.36.0-desktop.1\n    Path:     C:\\Users\\Fewnity\\.docker\\cli-plugins\\docker-compose.exe\n  debug: Get a shell into any image or container (Docker Inc.)\n    Version:  0.0.38\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-debug.exe\n  desktop: Docker Desktop commands (Docker Inc.)\n    Version:  v0.1.8\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-desktop.exe\n  dev: Docker Dev Environments (Docker Inc.)\n    Version:  v0.1.2\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-dev.exe\n  extension: Manages Docker extensions (Docker Inc.)\n    Version:  v0.2.27\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-extension.exe\n  init: Creates Docker-related starter files for your project (Docker Inc.)\n    Version:  v1.4.0\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-init.exe\n  mcp: Docker MCP Plugin (Docker Inc.)\n    Version:  dev\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-mcp.exe\n  model: Docker Model Runner (Docker Inc.)\n    Version:  v0.1.23\n    Path:     C:\\Users\\Fewnity\\.docker\\cli-plugins\\docker-model.exe\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\n    Version:  0.6.0\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-sbom.exe\n  scout: Docker Scout (Docker Inc.)\n    Version:  v1.18.0\n    Path:     C:\\Users\\Fewnity\\.docker\\cli-plugins\\docker-scout.exe\n\nServer:\n Containers: 1\n  Running: 0\n  Paused: 0\n  Stopped: 1\n Images: 1\n Server Version: 28.1.1\n Storage Driver: overlayfs\n  driver-type: io.containerd.snapshotter.v1\n Logging Driver: json-file\n Cgroup Driver: cgroupfs\n Cgroup Version: 1\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 nvidia runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 05044ec0a9a75232cad458027ca83437aae3f4da\n runc version: v1.2.5-0-g59923ef\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: unconfined\n Kernel Version: 5.15.167.4-microsoft-standard-WSL2\n Operating System: Docker Desktop\n OSType: linux\n Architecture: x86_64\n CPUs: 6\n Total Memory: 15.59GiB\n Name: docker-desktop\n ID: 83772057-ec62-4e83-8c26-0c6b1949c995\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n HTTP Proxy: http.docker.internal:3128\n HTTPS Proxy: http.docker.internal:3128\n No Proxy: hubproxy.docker.internal\n Labels:\n  com.docker.desktop.address=npipe://\\\\.\\pipe\\docker_cli\n Experimental: false\n Insecure Registries:\n  hubproxy.docker.internal:5555\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n\nWARNING: No blkio throttle.read_bps_device support\nWARNING: No blkio throttle.write_bps_device support\nWARNING: No blkio throttle.read_iops_device support\nWARNING: No blkio throttle.write_iops_device support\nWARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set\nWARNING: daemon is not using the default seccomp profile\n```\n\n### Additional Info\n\n_No response_",
      "solution": "Oh okay, when I saw the CLI repo I though it was better to open the issue here. Sorry about that.",
      "labels": [
        "kind/bug",
        "status/0-triage"
      ],
      "created_at": "2025-05-24T08:46:40Z",
      "closed_at": "2025-05-24T10:19:53Z",
      "url": "https://github.com/docker/cli/issues/6105",
      "comments_count": 2
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 6102,
      "title": "(race condition with multiple containers) docker-dind default behavior change, do not overwrite the docker certs folder if certs already exist",
      "problem": "### Description\n\nWhen running multiple instances of docker-dind with the same cert mount point, it tries to rewrite the certs everytime leading to a race condition.  This is a common use case with pipelines running using containers.\n\ndocker-dind should detect that certs already exist, and not try to overwrite them if so.\n\nAfter changing the default behavior, you could add an environment variable which says to overwrite the certs everytime if you want to, though I doubt most folks would use that.\n\nAlternatively, you could keep the default behavior and add an environment variable which says not to overwrite everytime.\n\nI tried making the folder read-only but then docker just failed to load up properly.\n\nPerhaps you could make docker recognize that a read-only certs folder is ok, and keep going, without rewriting the certs.\n\nFolks have been running into this issue for quite awhile now:\n- https://stackoverflow.com/questions/75393206/issues-running-multiple-build-jobs-in-parallel-usin-dind\n- https://gitlab.com/gitlab-org/gitlab-runner/-/issues/38818\n\nI'm seeing this in a gitlab pipeline, perhaps they just need to update their strategy to one that works.  I'm experiencing this issue after following their documentation.  (for a couple years now)",
      "solution": "I have just discovered buildkit and this is correct solution I should be using:\nhttps://medium.com/@santimar/using-buildkit-to-build-docker-images-with-unprivileged-gitlab-runner-on-kubernetes-bd536e768c2f\n\nI'll see about getting the gitlab documentation updated, if I can make that happen.\n\nThough this appears to be the correct solution, still, might want to adjust this so folks using the dind solution via kubernetes will be ok.  I'll leave this issue open for that reason.",
      "labels": [
        "kind/feature",
        "status/0-triage"
      ],
      "created_at": "2025-05-22T16:59:02Z",
      "closed_at": "2025-05-23T03:44:53Z",
      "url": "https://github.com/docker/cli/issues/6102",
      "comments_count": 3
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 6023,
      "title": "failed to start docker-proxy, check that the current version is in your $PATH",
      "problem": "### Description\n\nHi there,\n\nStrange error on new version exposing a port via docker -p. Searched for issues but nothing quite like this and one on shipping report which is no longer present.\n\n```\nLinux myhost 6.11.0-24-generic #24~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Mar 25 20:14:34 UTC 2 x86_64 x86_64 x86_64 GNU/Linux\n\n```\n\n```\nError response from daemon: failed to set up container networking: driver failed programming external connectivity on endpoint hortimod_gc-mqtt-1 (85ab20e55685df71bcf18d24f1f6a86b0a4f5746f4fad08b4642b5b610407e99): failed to start userland proxy for port mapping 0.0.0.0:1883:172.22.0.6:1883/tcp: failed to start docker-proxy, check that the current version is in your $PATH\n```\n\n\n### Reproduce\n\ndocker run --rm -p 1883:1883 jllopis/mosquitto\n\n### Expected behavior\n\nport opens and container runs\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n     Version:           19.03.9\n     API version:       1.40\n     Go version:        go1.13.10\n     Git commit:        9d988398e7\n     Built:             Fri May 15 00:22:47 2020\n     OS/Arch:           linux/amd64\n     Experimental:      false\n\n    Server: Docker Engine - Community\n     Engine:\n      Version:          28.1.0\n      API version:      1.49 (minimum version 1.24)\n      Go version:       go1.23.8\n      Git commit:       3f46cad\n      Built:            Thu Apr 17 09:54:54 2025\n      OS/Arch:          linux/amd64\n      Experimental:     false\n     containerd:\n      Version:          1.7.27\n      GitCommit:        05044ec0a9a75232cad458027ca83437aae3f4da\n     runc:\n      Version:          1.0.0-rc10\n      GitCommit:        dc9208a3303feef5b3839f4323d9beb36df0a9dd\n     docker-init:\n      Version:          0.19.0\n      GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient:\n     Debug Mode: false\n     Plugins:\n      buildx: Docker Buildx (Docker Inc., v0.22.0)\n      compose: Docker Compose (Docker Inc., v2.35.0)\n\n    Server:\n     Containers: 11\n      Running: 7\n      Paused: 0\n      Stopped: 4\n     Images: 8\n     Server Version: 28.1.0\n     Storage Driver: overlay2\n      Backing Filesystem: extfs\n      Supports d_type: true\n      Using metacopy: false\n      Native Overlay Diff: true\n      userxattr: false\n     Logging Driver: json-file\n     Cgroup Driver: systemd\n     Plugins:\n      Volume: local\n      Network: bridge host ipvlan macvlan null overlay\n      Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n     Swarm: inactive\n     Runtimes: runc io.containerd.runc.v2\n     Default Runtime: runc\n     Init Binary: docker-init\n     containerd version: 05044ec0a9a75232cad458027ca83437aae3f4da\n     runc version: dc9208a3303feef5b3839f4323d9beb36df0a9dd\n     init version: de40ad0\n     Security Options:\n      apparmor\n      seccomp\n      WARNING: You're not using the default seccomp profile\n       Profile: builtin\n      cgroupns\n     Kernel Version: 6.11.0-24-generic\n     Operating System: Ubuntu 24.04.2 LTS\n     OSType: linux\n     Architecture: x86_64\n     CPUs: 4\n     Total Memory: 7.589GiB\n     Name: myhost\n     ID: 08bf92c2-982c-4390-b86f-2aba70596cb7\n     Docker Root Dir: /var/lib/docker\n     Debug Mode: false\n     Registry: https://index.docker.io/v1/\n     Labels:\n     Experimental: false\n     Insecure Registries:\n      ::1/128\n      127.0.0.0/8\n     Live Restore Enabled: false\n\n    WARNING: No kernel memory limit support\n    WARNING: No oom kill disable support\n    WARNING: bridge-nf-call-iptables is disabled\n    WARNING: bridge-nf-call-ip6tables is disabled\n```\n\n### Additional Info\n\n\ndocker-proxy is in the expected place(s)\n```\n    /usr/bin/docker-proxy\n    /usr/local/bin/docker-proxy\n    /usr/local/docker-19.03.9/bin/docker-proxy\n```\n\nNothing running on the port (also no EADDRINUSE)\n\n```\nnetstat -tulpn | grep 1883\n```\n\none thing is the client seems quite old and cant seem to upgrade\n",
      "solution": "Ah, perfect! Glad to hear the problem is resolved!\n\n---\n\n@thaJeztah Thank you for providing a very good solution.",
      "labels": [
        "status/more-info-needed",
        "version/28.1"
      ],
      "created_at": "2025-04-17T21:59:52Z",
      "closed_at": "2025-04-18T08:07:49Z",
      "url": "https://github.com/docker/cli/issues/6023",
      "comments_count": 6
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 6068,
      "title": "docker service logs --tail command shows logs in random order",
      "problem": "### Description\n\nNothing special, i run a one node in Swarm mode and i just want to read service logs and i use following command:\n```bash\ndocker service logs --tail 10 your_service_name\n```\n\nor\n\n```bash\ndocker service logs -n 10 your_service_name\n```\n\nat the moment it is Thu 15 May, 2025 and i see following lines:\n```bash\n...\n\"time\":\"2025-05-13T19:49:10.216313598Z\" ...\n\"time\":\"2025-05-13T19:50:54.78517241Z\" ...\n\"time\":\"2025-05-13T19:50:57.929130015Z\" ...\n```\n\nif i run the same command several times in a row then i might see actual log entries like following:\n```bash\n...\n\"time\":\"2025-05-15T19:26:23.402808283Z\" ...\n\"time\":\"2025-05-15T19:27:09.032984365Z\" ...\n\"time\":\"2025-05-15T19:27:27.630533533Z\" ...\n``` \n\nalso sometimes i can see lines for 14 May, 2025.\n\nIt looks like it happens like in the Groundhog Day movie.\n\n### Reproduce\n\ndocker service logs -n 10 your_service_name\n\n### Expected behavior\n\ndocker service logs -n 10 should show latest 10 entries of service logs in proper order.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           28.1.1\n API version:       1.49\n Go version:        go1.23.8\n Git commit:        4eba377\n Built:             Fri Apr 18 09:52:14 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          28.1.1\n  API version:      1.49 (minimum version 1.24)\n  Go version:       go1.23.8\n  Git commit:       01f442b\n  Built:            Fri Apr 18 09:52:14 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.7.27\n  GitCommit:        05044ec0a9a75232cad458027ca83437aae3f4da\n runc:\n  Version:          1.2.5\n  GitCommit:        v1.2.5-0-g59923ef\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    28.1.1\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.23.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.35.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 16\n  Running: 7\n  Paused: 0\n  Stopped: 9\n Images: 6\n Server Version: 28.1.1\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n Swarm: active\n  NodeID: *****\n  Is Manager: true\n  ClusterID: *****\n  Managers: 1\n  Nodes: 1\n  Default Address Pool: 10.0.0.0/8  \n  SubnetSize: 24\n  Data Path Port: 4789\n  Orchestration:\n   Task History Retention Limit: 5\n  Raft:\n   Snapshot Interval: 10000\n   Number of Old Snapshots to Retain: 0\n   Heartbeat Tick: 1\n   Election Tick: 10\n  Dispatcher:\n   Heartbeat Period: 5 seconds\n  CA Configuration:\n   Expiry Duration: 3 months\n   Force Rotate: 0\n  Autolock Managers: false\n  Root Rotation In Progress: false\n  Node Address: *****\n  Manager Addresses:\n   *****:2377\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 05044ec0a9a75232cad458027ca83437aae3f4da\n runc version: v1.2.5-0-g59923ef\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.8.0-59-generic\n Operating System: Ubuntu 24.04.2 LTS\n OSType: linux\n Architecture: x86_64\n CPUs: 2\n Total Memory: 1.875GiB\n Name: *****\n ID: *****\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n```\n\n### Additional Info\n\n_No response_",
      "solution": "@thaJeztah to be honest i'm not sure, mentioned issue might be fixed or might not according to discussion there and has pretty wide scope.\n\nhere i would like to see log entries in the same order when i run the same command in a row at least.\n\nat the moment it looks like true random and that's why it is almost unusable.\n\nit would be ok if it would have some inconsistent order in terms of a couple of minutes.\n\n---\n\nDoes your service have multiple instances? I suspect the issue you're seeing is the same as discussed in the other ticket; swarm aggregates logs from all tasks of a service, and the order in which those logs arrive cannot be guaranteed to be in the same order.",
      "labels": [
        "duplicate",
        "area/swarm",
        "version/28.1"
      ],
      "created_at": "2025-05-15T19:35:43Z",
      "closed_at": "2025-05-19T09:23:11Z",
      "url": "https://github.com/docker/cli/issues/6068",
      "comments_count": 4
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 4546,
      "title": "`docker build` fails with error `unknown flag: --disable-content-trust` in Docker v24.0.5",
      "problem": "### Description\n\nUsing the flag `--disable-content-trust` for `docker build` results in error:\r\n```\r\nunknown flag: --disable-content-trust\r\nSee 'docker buildx build --help'.\r\n```\r\n\r\nUsing docker v24.0.5\r\n\r\nThe flag `--disable-content-trust` appears in docs at https://docs.docker.com/engine/reference/commandline/build/, however the option is not visible in the output of  `docker build --help` (output attached in additional info). The release notes do not mention that this flag was deprecated (confirmed that the flag existed in `Docker version 24.0.2, build cb74dfc`)\r\n\n\n### Reproduce\n\n```\r\ndocker build --disable-content-trust=false\r\n```\r\n(it's not a valid call to `docker build` but presents a minimal way to reproduce the error)\r\n\n\n### Expected behavior\n\nShould not get error:\r\n```\r\nunknown flag: --disable-content-trust\r\n```\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.6\r\n Git commit:        ced0996\r\n Built:             Fri Jul 21 20:35:23 2023\r\n OS/Arch:           linux/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.6\r\n  Git commit:       a61e2b4\r\n  Built:            Fri Jul 21 20:35:23 2023\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.22\r\n  GitCommit:        8165feabfdfe38c65b599c4993d227328c231fca\r\n runc:\r\n  Version:          1.1.8\r\n  GitCommit:        v1.1.8-0-g82f18fe\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.20.2\r\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\r\n\r\nServer:\r\n Containers: 19\r\n  Running: 2\r\n  Paused: 0\r\n  Stopped: 17\r\n Images: 77\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 8165feabfdfe38c65b599c4993d227328c231fca\r\n runc version: v1.1.8-0-g82f18fe\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 5.15.0-56-generic\r\n Operating System: Ubuntu 20.04.6 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 16\r\n Total Memory: 31.2GiB\r\n Name: <redacted>\r\n ID: <redacted>\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n HTTP Proxy: http://xxxxx:xxxxx@<redacted>/\r\n HTTPS Proxy: http://xxxxx:xxxxx@<redacted>/\r\n No Proxy: localhost,127.0.0.0/8,10.0.0.0/8,<redacted>\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n Default Address Pools:\r\n   Base: <redacted>/17, Size: 24\n```\n\n\n### Additional Info\n\n## docker build --help\r\n```\r\nUsage:  docker buildx build [OPTIONS] PATH | URL | -\r\n\r\nStart a build\r\n\r\nAliases:\r\n  docker buildx build, docker buildx b\r\n\r\nOptions:\r\n      --add-host strings              Add a custom host-to-IP mapping (format: \"host:ip\")\r\n      --allow strings                 Allow extra privileged entitlement (e.g., \"network.host\", \"security.insecure\")\r\n      --attest stringArray            Attestation parameters (format: \"type=sbom,generator=image\")\r\n      --build-arg stringArray         Set build-time variables\r\n      --build-context stringArray     Additional build contexts (e.g., name=path)\r\n      --builder string                Override the configured builder instance (default \"default\")\r\n      --cache-from stringArray        External cache sources (e.g., \"user/app:cache\", \"type=local,src=path/to/dir\")\r\n      --cache-to stringArray          Cache export destinations (e.g., \"user/app:cache\", \"type=local,dest=path/to/dir\")\r\n      --cgroup-parent string          Optional parent cgroup for the container\r\n  -f, --file string                   Name of the Dockerfile (default: \"PATH/Dockerfile\")\r\n      --iidfile string                Write the image ID to the file\r\n      --label stringArray             Set metadata for an image\r\n      --load                          Shorthand for \"--output=type=docker\"\r\n      --metadata-file string          Write build result metadata to the file\r\n      --network string                Set the networking mode for the \"RUN\" instructions during build (default \"default\")\r\n      --no-cache                      Do not use cache when building the image\r\n      --no-cache-filter stringArray   Do not cache specified stages\r\n  -o, --output stringArray            Output destination (format: \"type=local,dest=path\")\r\n      --platform stringArray          Set target platform for build\r\n      --progress string               Set type of progress output (\"auto\", \"plain\", \"tty\"). Use plain to show container output (default \"auto\")\r\n      --provenance string             Shorthand for \"--attest=type=provenance\"\r\n      --pull                          Always attempt to pull all referenced images\r\n      --push                          Shorthand for \"--output=type=registry\"\r\n  -q, --quiet                         Suppress the build output and print image ID on success\r\n      --sbom string                   Shorthand for \"--attest=type=sbom\"\r\n      --secret stringArray            Secret to expose to the build (format: \"id=mysecret[,src=/local/secret]\")\r\n      --shm-size bytes                Size of \"/dev/shm\"\r\n      --ssh stringArray               SSH agent socket or keys to expose to the build (format: \"default|<id>[=<socket>|<key>[,<key>]]\")\r\n  -t, --tag stringArray               Name and optionally a tag (format: \"name:tag\")\r\n      --target string                 Set the target build stage to build\r\n      --ulimit ulimit                 Ulimit options (default [])\r\n```\r\n",
      "solution": "It is an issue of Buildx reported here: https://github.com/docker/buildx/issues/987 and here: https://github.com/docker/buildx/issues/1895\r\n\r\nAs a workaround, you can use the legacy builder as:\r\n`DOCKER_BUILDKIT=0 docker build --disable-content-trust=false`\r\n \n\n---\n\nDocker Content Trust is not supported when using BuildKit, so probably the best workaround for this is to remove the flag (as content trust wouldn't be used with BuildKit enabled, which is the default for current versions of Docker).",
      "labels": [
        "kind/bug",
        "status/0-triage",
        "area/builder",
        "area/trust",
        "area/builder/buildkit"
      ],
      "created_at": "2023-08-30T10:53:26Z",
      "closed_at": "2025-04-29T13:49:20Z",
      "url": "https://github.com/docker/cli/issues/4546",
      "comments_count": 4
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 3785,
      "title": "Documentation for plugins incorrect",
      "problem": "### Description\n\ndocumentation for debugging plugins mentions an executable docker-runc as well as a directory /var/run/docker/plugins/runtime-root/moby-plugins which do not exist.\r\nsearching around on the internet I found instructions for using runc to find docker plugin container ids and spawning a shell process within plugin containers\r\n\r\ninstructions state\r\nsudo docker-runc --root /var/run/docker/plugins/runtime-root/moby-plugins list\r\nsudo docker-runc --root /var/run/docker/plugins/runtime-root/moby-plugins exec -t $PLUGID sh\r\n\r\nshould be\r\nsudo runc --root /run/docker/runtime-runc/plugins.moby list\r\nsudo runc --root /run/docker/runtime-runc/plugins.moby exec -t $PLUGID sh\n\n### Reproduce\n\nnavigate to and view this webpage https://docs.docker.com/engine/extend/#debugging-plugins\n\n### Expected behavior\n\n_No response_\n\n### docker version\n\n```bash\nClient:\r\n\r\nServer:\r\n Server Version: 20.10.17\n```\n\n\n### docker info\n\n```bash\nClient:\r\n\r\nServer:\r\n Server Version: 20.10.17\n```\n\n\n### Additional Info\n\n_No response_",
      "solution": "should be fixed by https://github.com/docker/cli/pull/3817",
      "labels": [
        "kind/bug",
        "kind/docs",
        "status/claimed"
      ],
      "created_at": "2022-09-23T14:34:15Z",
      "closed_at": "2025-04-24T23:10:24Z",
      "url": "https://github.com/docker/cli/issues/3785",
      "comments_count": 5
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 3369,
      "title": "Docker can't mount directory inside veracrypt container",
      "problem": "**Description**\n\nIt seems that docker is unable to mount a directory inside a veracrypt volumn.\n\n**Steps to reproduce the issue:**\n1. Install Veracrypt\n2. Create an encrypted file container. The volume is standard veracrypt volume. The Encryption is AES. The Hash Algorithm is SHA-512. I used 10GB size. I doesnt store large files inside the container\n3. Mount the file. I used the T for the volum.\n4. create the files T:\\bar.txt and T:\\test\\foo.txt\n5. Open a powershell\n6. enter \"docker run -v T:\\:/project -w /project ubuntu:latest ls\"\n\n**Describe the results you received:**\nThe command ls doesn't output anything.\n\n**Describe the results you expected:**\nI expect that the directory test and the file bar.txt are outputed by the command ls. The above command works for other windows directories like C:\\user\\fettpet\\test\n\n**Output of `docker version`:**\n\n```\nClient:\n Cloud integration: v1.0.20\n Version:           20.10.10\n API version:       1.41\n Go version:        go1.16.9\n Git commit:        b485636\n Built:             Mon Oct 25 07:47:53 2021\n OS/Arch:           windows/amd64\n Context:           default\n Experimental:      true\n\nServer: Docker Engine - Community\n Engine:\n  Version:          20.10.10\n  API version:      1.41 (minimum version 1.12)\n  Go version:       go1.16.9\n  Git commit:       e2f740d\n  Built:            Mon Oct 25 07:41:30 2021\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.4.11\n  GitCommit:        5b46e404f6b9f661a205e28d59c982d3634148f8\n runc:\n  Version:          1.0.2\n  GitCommit:        v1.0.2-0-g52b36a2\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n**Output of `docker info`:**\n\n```\nClient:\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Build with BuildKit (Docker Inc., v0.6.3)\n  compose: Docker Compose (Docker Inc., v2.1.1)\n  scan: Docker Scan (Docker Inc., 0.9.0)\n\nServer:\n Containers: 17\n  Running: 1\n  Paused: 0\n  Stopped: 16\n Images: 15\n Server Version: 20.10.10\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: cgroupfs\n Cgroup Version: 1\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 5b46e404f6b9f661a205e28d59c982d3634148f8\n runc version: v1.0.2-0-g52b36a2\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: default\n Kernel Version: 5.10.60.1-microsoft-standard-WSL2\n Operating System: Docker Desktop\n OSType: linux\n Architecture: x86_64\n CPUs: 8\n Total Memory: 24.94GiB\n Name: docker-desktop\n ID: P3BB:3CDO:SMX2:KKBO:CK4S:IJNO:WEWD:HNON:2MV3:NA5K:S7PM:2D2Q\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Registry: https://index.docker.io/v1/\n Labels:\n Experimental: false\n Insecure Registries:\n  127.0.0.0/8\n Live Restore Enabled: false\n\nWARNING: No blkio throttle.read_bps_device support\nWARNING: No blkio throttle.write_bps_device support\nWARNING: No blkio throttle.read_iops_device support\nWARNING: No blkio throttle.write_iops_device support\n```",
      "solution": "There is a workaround to mount inside a veracrype volumn. I test it in windows\r\n1. mount the veracrypt container\r\n2. use task manager to stop all docker related tasks\r\n3. start docker\r\n4. now you can mount a volumn inside the veracrypt container.\r\n\n\n---\n\nclosing as stale, and it's not an issue on the CLI side (if there's something that can be fixed for this, it would have to be done on the daemon; https://github.com/moby/moby)",
      "labels": [
        "version/20.10"
      ],
      "created_at": "2021-11-24T15:07:53Z",
      "closed_at": "2025-04-24T23:03:36Z",
      "url": "https://github.com/docker/cli/issues/3369",
      "comments_count": 2
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 3344,
      "title": "--build-args/ARG does not overwrite ENV variables of the same name from a base image",
      "problem": "<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/cli/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Docker Community Slack - https://dockr.ly/community\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\n`ARG` variables of the same name as an environment variable set in a base image do not overwrite the value of the variable during build time. Is it intentional for the primary environment to take precedence over the temporary environment? This is especially irritating when a base image sets a proxy value to empty and the child image needs to temporarily set a build environment that uses a different proxy value than the parent image.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Build a parent image and set a variable with ENV\r\n2. Base a child image off the built parent and apply an ARG of the same name as the parent ENV var.\r\n3. Echo the ARG in  a dockerfile RUN directive in the child\r\n4. Build the child. Note that the ENV variable of the parent is echo'd not the temporary ARG value\r\n\r\n**Describe the results you received:**\r\n\r\nCreate parent dockerfile and set an ENV variable.\r\n\r\n```dockerfile\r\n# parent.dockerfile\r\n\r\nFROM alpine:latest\r\n\r\nENV HELLO=\"World\"\r\n```\r\n\r\nBuild the parent image\r\n\r\n```console\r\n\u276f docker build -t parent:latest -f parent.dockerfile .\r\n```\r\n\r\nCreate a child dockerfile and set a build-arg with the same name as the ENV var from parent.\r\n\r\n```dockerfile\r\n# child.dockerfile\r\n\r\nFROM parent:latest\r\n\r\nARG HELLO\r\n\r\nRUN echo \"HELLO=${HELLO}\"\r\n```\r\n\r\nBuild the child image.\r\n\r\n```console\r\n\u276f docker build -t child:latest --build-arg HELLO=Earth -f child.dockerfile --no-cache .\r\nSending build context to Docker daemon  3.072kB\r\nStep 1/4 : FROM parent:latest\r\n ---> b319a2900116\r\nStep 2/4 : ARG HELLO\r\n ---> Running in df26b87ef593\r\nRemoving intermediate container df26b87ef593\r\n ---> 037665231ae3\r\nStep 3/4 : RUN echo \"HELLO=${HELLO}\"\r\n ---> Running in b79b3944af01\r\nHELLO=World\r\nRemoving intermediate container b79b3944af01\r\n ---> 6772fb18124d\r\nStep 4/4 : CMD [ \"echo\", \"HELLO=${HELLO}\" ]\r\n ---> Running in 8f7fc5bc845b\r\nRemoving intermediate container 8f7fc5bc845b\r\n ---> fdaed33c1026\r\nSuccessfully built fdaed33c1026\r\nSuccessfully tagged child:latest\r\n```\r\n\r\nResult: In `Step 3/4` the value echo'd is that of the original ENV var in the parent.\r\n\r\n**Describe the results you expected:**\r\nI would expect that the temporary environment would take precedence over the core environment during build and echo would produce `HELLO=Earth` in the example above.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Cloud integration: 1.0.17\r\n Version:           20.10.8\r\n API version:       1.41\r\n Go version:        go1.16.6\r\n Git commit:        3967b7d\r\n Built:             Fri Jul 30 19:55:20 2021\r\n OS/Arch:           darwin/amd64\r\n Context:           default\r\n Experimental:      true\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          20.10.8\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.16.6\r\n  Git commit:       75249d8\r\n  Built:            Fri Jul 30 19:52:10 2021\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.4.9\r\n  GitCommit:        e25210fe30a0a703442421b0f60afac609f950a3\r\n runc:\r\n  Version:          1.0.1\r\n  GitCommit:        v1.0.1-0-g4144b63\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Build with BuildKit (Docker Inc., v0.6.1-docker)\r\n  compose: Docker Compose (Docker Inc., v2.0.0-rc.3)\r\n  scan: Docker Scan (Docker Inc., v0.8.0)\r\n\r\nServer:\r\n Containers: 6\r\n  Running: 3\r\n  Paused: 0\r\n  Stopped: 3\r\n Images: 67\r\n Server Version: 20.10.8\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: e25210fe30a0a703442421b0f60afac609f950a3\r\n runc version: v1.0.1-0-g4144b63\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 5.10.47-linuxkit\r\n Operating System: Docker Desktop\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 4\r\n Total Memory: 7.774GiB\r\n Name: docker-desktop\r\n ID: BSL6:V5BU:UHMQ:5ZQ7:QELY:VD6D:A7U5:434O:7WSO:FHHJ:SAMI:UONQ\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n HTTP Proxy: http://proxy-lmi.global.lmco.com:80\r\n HTTPS Proxy: http://proxy-lmi.global.lmco.com:80\r\n No Proxy: 127.0.0.10/8,localhost,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,.lmco.com\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**",
      "solution": "Ah understood and thank you. Unfortunately, due to the environment we use specifically we can't use build kit because of the x503 self signed certificate issue with build kit and private registries. Good to know this particular issue is solved though going forward. \n\n---\n\nI met the issue too on Docker version 19.03.13, build 4484c46\r\nUsed `DOCKER_BUILDKIT=1` to solve the problem.\n\n---\n\nclosing this one, as BuildKit is now used as default for Linux containers, and it's unlikely to be fixed for the legacy builder.",
      "labels": [
        "area/builder",
        "version/20.10",
        "area/builder/classic-builder"
      ],
      "created_at": "2021-10-19T18:04:38Z",
      "closed_at": "2025-04-24T23:01:25Z",
      "url": "https://github.com/docker/cli/issues/3344",
      "comments_count": 5
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 3119,
      "title": "Cannot start service xyz: Ports are not available: listen tcp 0.0.0.0:6904: bind: address already in use",
      "problem": "---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\n\r\n**Description**\r\n\r\nI use docker-compose to launch a Selenium hub with 3 identical Chrome containers.  I modified the configuration so that I could use the `--scale` argument to scale the number of Chrome containers up or down depending on the need.  I launch the Chrome containers from selenium/node-chrome images, which have a VNC server running on port 5900.  I defined a docker-compose.yml with the hub and a Chrome image.\r\n\r\nWhen running the command to start the containers, and then shut them down, if I then try to start them again, sometimes one of the VNC ports exposed on the host side are blocked by Docker.  When running `docker ps` there are no containers running, yet the port is still blocked.\r\n\r\nHere is the docker-compose:\r\n\r\n```\r\nversion: \"3\"\r\nservices:\r\n  chrome:\r\n    image: selenium/node-chrome:4.0.0-beta-3-prerelease-20210422\r\n    volumes:\r\n      - /dev/shm:/dev/shm\r\n    depends_on:\r\n      - selenium-hub\r\n    environment:\r\n      - SE_EVENT_BUS_HOST=selenium-hub\r\n      - SE_EVENT_BUS_PUBLISH_PORT=4442\r\n      - SE_EVENT_BUS_SUBSCRIBE_PORT=4443\r\n    ports:\r\n      - \"6900-6909:5900\"\r\n\r\n  selenium-hub:\r\n    image: selenium/hub:4.0.0-beta-3-prerelease-20210422\r\n    container_name: selenium-hub\r\n    ports:\r\n      - \"4442:4442\"\r\n      - \"4443:4443\"\r\n      - \"4444:4444\"\r\n\r\n```\r\n\r\n\r\n**Steps to reproduce the issue:**\r\n1.  Run `docker-compose -f docker-compose.yml up -d --scale chrome=3`\r\n2.  The 3 chrome containers are now accessible via VNC on ports 6900, 6901, and 6902, and you can verify with VNCViewer or RealVNC.\r\n3.  Stop the containers:  `docker-compose -f docker-compose.yml down`\r\n4. Use `docker ps` to verify the containers have shutdown.  Use `docker ps -a` to verify the containers have been removed.\r\n5. Again, run `docker-compose -f docker-compose.yml up -d --scale chrome=3`  This time, one of the containers may not be started due to one of the 6900-6909 ports being blocked by Docker.\r\n\r\n**Describe the results you received:**\r\n\r\nAfter bringing the containers down, removing them, and then restarting them, one of the ports is blocked.  In this example, port 6903 is blocked.\r\n\r\n```bash\r\n$ docker-compose -f .wdio-ci-configs/parallel-tests-on-selenium-grid/docker-compose-hub-parallel.yml up -d --scale chrome=3\r\nDocker Compose is now in the Docker CLI, try `docker compose up`\r\n\r\nCreating network \"parallel-tests-on-selenium-grid_default\" with the default driver\r\nCreating selenium-hub ... done\r\nWARNING: The \"chrome\" service specifies a port on the host. If multiple containers for this service are created on a single host, the port will clash.\r\nCreating parallel-tests-on-selenium-grid_chrome_1 ... error\r\nCreating parallel-tests-on-selenium-grid_chrome_2 ... done\r\nCreating parallel-tests-on-selenium-grid_chrome_3 ... done\r\n\r\nERROR: for parallel-tests-on-selenium-grid_chrome_1  Cannot start service chrome: Ports are not available: listen tcp 0.0.0.0:6903: bind: address already in use\r\n\r\nERROR: for chrome  Cannot start service chrome: Ports are not available: listen tcp 0.0.0.0:6903: bind: address already in use\r\nERROR: Encountered errors while bringing up the project.\r\n```\r\n\r\nAlso, the ports were not selected in order.  Below we see ports 6900 and 6909 were bound successfully while port 6903 was marked as being already in use, even though it wasn't.\r\n\r\n```bash\r\n$ docker ps\r\nCONTAINER ID   IMAGE                                                   COMMAND                  CREATED              STATUS              PORTS                                                           NAMES\r\n348b9653600c   selenium/node-chrome:4.0.0-beta-3-prerelease-20210422   \"/opt/bin/entry_poin\u2026\"   About a minute ago   Up About a minute   0.0.0.0:6900->5900/tcp, :::6900->5900/tcp                       parallel-tests-on-selenium-grid_chrome_2\r\n0aeabe873320   selenium/node-chrome:4.0.0-beta-3-prerelease-20210422   \"/opt/bin/entry_poin\u2026\"   About a minute ago   Up About a minute   0.0.0.0:6909->5900/tcp, :::6909->5900/tcp                       parallel-tests-on-selenium-grid_chrome_3\r\n5f3baf558790   selenium/hub:4.0.0-beta-3-prerelease-20210422           \"/opt/bin/entry_poin\u2026\"   About a minute ago   Up About a minute   0.0.0.0:4442-4444->4442-4444/tcp, :::4442-4444->4442-4444/tcp   selenium-hub\r\n```\r\n\r\n**Describe the results you expected:**\r\n\r\nI was expecting to be able to scale the containers up to the number of available ports in the range 6900-6909 and have Docker free up the ports when bringing the containers down.  However, some of the ports remain blocked.\r\n\r\nSometimes, like in this example, everything works as expected:\r\n\r\n```bash\r\n$ docker-compose -f .wdio-ci-configs/parallel-tests-on-selenium-grid/docker-compose-hub-parallel.yml up -d --scale chrome=3\r\nDocker Compose is now in the Docker CLI, try `docker compose up`\r\n\r\nCreating network \"parallel-tests-on-selenium-grid_default\" with the default driver\r\nCreating selenium-hub ... done\r\nWARNING: The \"chrome\" service specifies a port on the host. If multiple containers for this service are created on a single host, the port will clash.\r\nCreating parallel-tests-on-selenium-grid_chrome_1 ... done\r\nCreating parallel-tests-on-selenium-grid_chrome_2 ... done\r\nCreating parallel-tests-on-selenium-grid_chrome_3 ... done\r\n```\r\n\r\n**Additional information you deem important**\r\n\r\nBefore attempting to scale the containers, we had two docker-compose files, one with 3 chrome containers each labelled chrome0, chrome1, chrome2 and then the other one with chrome0, chrome1, chrome2 and chrome3.  When bringing those containers up and down, none of the ports became blocked.  It was only when we attempted to use a single configuration for the chrome containers that we experience blocked VNC ports.  Here is the original configuration of one of the files, as an example:\r\n\r\n```\r\nversion: \"3\"\r\nservices:\r\n  chrome0:\r\n    image: selenium/node-chrome:4.0.0-beta-3-prerelease-20210422\r\n    volumes:\r\n      - /dev/shm:/dev/shm\r\n    depends_on:\r\n      - selenium-hub\r\n    environment:\r\n      - SE_EVENT_BUS_HOST=selenium-hub\r\n      - SE_EVENT_BUS_PUBLISH_PORT=4442\r\n      - SE_EVENT_BUS_SUBSCRIBE_PORT=4443\r\n    ports:\r\n      - \"6900:5900\"\r\n\r\n  chrome1:\r\n    image: selenium/node-chrome:4.0.0-beta-3-prerelease-20210422\r\n    volumes:\r\n      - /dev/shm:/dev/shm\r\n    depends_on:\r\n      - selenium-hub\r\n    environment:\r\n      - SE_EVENT_BUS_HOST=selenium-hub\r\n      - SE_EVENT_BUS_PUBLISH_PORT=4442\r\n      - SE_EVENT_BUS_SUBSCRIBE_PORT=4443\r\n    ports:\r\n      - \"6901:5900\"\r\n\r\n  chrome2:\r\n    image: selenium/node-chrome:4.0.0-beta-3-prerelease-20210422\r\n    volumes:\r\n      - /dev/shm:/dev/shm\r\n    depends_on:\r\n      - selenium-hub\r\n    environment:\r\n      - SE_EVENT_BUS_HOST=selenium-hub\r\n      - SE_EVENT_BUS_PUBLISH_PORT=4442\r\n      - SE_EVENT_BUS_SUBSCRIBE_PORT=4443\r\n    ports:\r\n      - \"6902:5900\"\r\n\r\n  selenium-hub:\r\n    image: selenium/hub:4.0.0-beta-3-prerelease-20210422\r\n    container_name: selenium-hub\r\n    ports:\r\n      - \"4442:4442\"\r\n      - \"4443:4443\"\r\n      - \"4444:4444\"\r\n```\r\n\r\nWith 3 separate, yet identical containers, aside from the VNC port, there were no conflicts.\r\n\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  app: Docker App (Docker Inc., v0.9.1-beta3)\r\n  buildx: Build with BuildKit (Docker Inc., v0.5.1-docker)\r\n  compose: Docker Compose (Docker Inc., 2.0.0-beta.1)\r\n  scan: Docker Scan (Docker Inc., v0.8.0)\r\n\r\nServer:\r\n Containers: 10\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 10\r\n Images: 35\r\n Server Version: 20.10.6\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 05f951a3781f4f2c1911b05e61c160e9c30eaa8e\r\n runc version: 12644e614e25b05da6fd08a38ffa0cfe1903fdec\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 5.10.25-linuxkit\r\n Operating System: Docker Desktop\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 4\r\n Total Memory: 3.844GiB\r\n Name: docker-desktop\r\n ID: TSJO:KAYN:ZLE3:3LNW:WQMX:WBD4:LPVL:RTV4:Y4AB:JL27:SEK2:OGA5\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: true\r\n  File Descriptors: 44\r\n  Goroutines: 49\r\n  System Time: 2021-06-03T16:43:25.932805464Z\r\n  EventsListeners: 4\r\n HTTP Proxy: http.docker.internal:3128\r\n HTTPS Proxy: http.docker.internal:3128\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\n20.10.6 build 370c289\r\n```\r\n\r\n**Additional environment details:**\r\n\r\nOS: Macbook Pro 10.15.1\r\nShell:  Bash\r\nDocker version:  20.10.6 build 370c289\r\n\r\n",
      "solution": "Hi @jamesmortensen \r\nI've tried with Docker Desktop 3.5.1 on my Intel Mac and I cannot reproduce the problem.\r\n\r\n```\r\n$ docker-compose -f docker-compose.yml up -d --scale chrome=3\r\nCreating network \"cli3119_default\" with the default driver\r\nCreating selenium-hub ... done\r\nWARNING: The \"chrome\" service specifies a port on the host. If multiple containers for this service are created on a single host, the port will clash.\r\nCreating cli3119_chrome_1 ... done\r\nCreating cli3119_chrome_2 ... done\r\nCreating cli3119_chrome_3 ... done\r\n```\r\n\r\nthen \r\n```\r\ndocker-compose down\r\n```\r\n\r\nand repeated that four times. I recommend to update Docker Desktop to the latest and try again.\r\n\n\n---\n\nThank you @ndeloof for the link. \u2764\ufe0f \ud83c\udf89 \r\n\r\n@jamesmortensen Could you check your `docker-compose --version` ? If it shows Docker Compose version 2.0.0-beta.x then we have narrowed down the problem. In that case you could opt out of the Compose V2 in the experimental features in Docker Desktop settings for now.\r\n ",
      "labels": [],
      "created_at": "2021-06-03T16:56:38Z",
      "closed_at": "2025-04-24T22:59:41Z",
      "url": "https://github.com/docker/cli/issues/3119",
      "comments_count": 9
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 3741,
      "title": "Download looping docker image",
      "problem": "**Description**\r\nDocker fills up hard drive even tho the docker image it self is 1 gb, it fills up to 70 GB. Im thinking it's redowloading something over and over till the drive get's filled up.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Use the same settings i am using down below\r\n2. Go to https://kasmweb.com/docs/latest/guide/custom_images.html and download every image one at a time for e.g\r\ndocker pull kasmweb/atom:1.11.0\r\ndocker pull kasmweb/audacity:1.11.0\r\n3. wait a long time till the drive get's filled\r\n\r\n**Expected Results:**\r\nExpected was that it would download these image's in about 2 min per image and not that they would fill up the entire drive+ take for ever to extract\r\n\r\n**Actual Results:**\r\nDownloading a docker image cause's a 100 GB drive to fill up. It seems to be continuing to fill up the entire drive even when the docker layers are already downloaded, it gets to extracting the layers. After approximately an hour the drive get's filled up and i receive an error prompt that says: the drive is full. After this the docker pull fails and clears up the drive with 20/30 GB+ not being  being consistent.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\nAny future requested information will be posted, also im not sure if this is a docker bug. It might also be a bug with lxc or proxmox, any insights on how i can possibly solve it is greatly appreciated.\r\n\r\n**Output of `docker version`:**\r\n```\r\n(Client: Docker Engine - Community\r\n Version:           20.10.17\r\n API version:       1.41\r\n Go version:        go1.17.11\r\n Git commit:        100c701\r\n Built:             Mon Jun  6 23:02:46 2022\r\n OS/Arch:           linux/amd64\r\n Context:           default\r\n Experimental:      true\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          20.10.17\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.17.11\r\n  Git commit:       a89b842\r\n  Built:            Mon Jun  6 23:00:51 2022\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.7\r\n  GitCommit:        0197261a30bf81f1ee8e6a4dd2dea0ef95d67ccb\r\n runc:\r\n  Version:          1.1.3\r\n  GitCommit:        v1.1.3-0-g6724737\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0)\r\n```\r\n\r\n**Output of `docker info`:**\r\n```\r\n(Client:\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  app: Docker App (Docker Inc., v0.9.1-beta3)\r\n  buildx: Docker Buildx (Docker Inc., v0.8.2-docker)\r\n  compose: Docker Compose (Docker Inc., v2.6.0)\r\n  scan: Docker Scan (Docker Inc., v0.17.0)\r\n\r\nServer:\r\n Containers: 4\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 4\r\n Images: 3\r\n Server Version: 20.10.17\r\n Storage Driver: vfs\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runtime.v1.linux runc io.containerd.runc.v2\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 0197261a30bf81f1ee8e6a4dd2dea0ef95d67ccb\r\n runc version: v1.1.3-0-g6724737\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: default\r\n  cgroupns\r\n Kernel Version: 5.15.39-3-pve\r\n Operating System: Ubuntu 22.04.1 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 2\r\n Total Memory: 4GiB\r\n Name: Kasm\r\n ID: NOTR:NYLA:E5YP:WT4S:AFAT:PBV4:OGNA:NNDU:PNGM:5BZR:THBD:JRYR\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false)\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nWhat am i using:\r\n- Proxmox VE 7.2-7\r\n- Unprivileged container no + protection no\r\n- Ubuntu 22.04 LTS\r\n- LXC\r\n\r\nLXC config:\r\narch: amd64\r\ncores: 3\r\nfeatures: nesting=1\r\nhostname: Kasm\r\nmemory: 4096\r\nnet0: name=eth0,bridge=vmbr0,firewall=1,gw=192.168.2.1,hwaddr=32:41:2F:73:C9:AB,ip=192.168.2.103/24,tag=10,type=veth\r\nostype: ubuntu\r\nparent: working\r\nrootfs: storage-nvme:subvol-121-disk-0,size=124G\r\nswap: 1024\r\n\r\nProxmox Packaging versions\r\nproxmox-ve: 7.2-1 (running kernel: 5.15.39-3-pve)\r\npve-manager: 7.2-7 (running version: 7.2-7/d0dd0e85)\r\npve-kernel-5.15: 7.2-8\r\npve-kernel-helper: 7.2-8\r\npve-kernel-5.13: 7.1-9\r\npve-kernel-5.11: 7.0-10\r\npve-kernel-5.4: 6.4-4\r\npve-kernel-5.15.39-3-pve: 5.15.39-3\r\npve-kernel-5.15.35-1-pve: 5.15.35-3\r\npve-kernel-5.13.19-6-pve: 5.13.19-15\r\npve-kernel-5.13.19-2-pve: 5.13.19-4\r\npve-kernel-5.11.22-7-pve: 5.11.22-12\r\npve-kernel-5.4.124-1-pve: 5.4.124-1\r\npve-kernel-5.4.34-1-pve: 5.4.34-2\r\nceph-fuse: 14.2.21-1\r\ncorosync: 3.1.5-pve2\r\ncriu: 3.15-1+pve-1\r\nglusterfs-client: 9.2-1\r\nifupdown: 0.8.36+pve1\r\nksm-control-daemon: 1.4-1\r\nlibjs-extjs: 7.0.0-1\r\nlibknet1: 1.24-pve1\r\nlibproxmox-acme-perl: 1.4.2\r\nlibproxmox-backup-qemu0: 1.3.1-1\r\nlibpve-access-control: 7.2-4\r\nlibpve-apiclient-perl: 3.2-1\r\nlibpve-common-perl: 7.2-2\r\nlibpve-guest-common-perl: 4.1-2\r\nlibpve-http-server-perl: 4.1-3\r\nlibpve-storage-perl: 7.2-7\r\nlibqb0: 1.0.5-1\r\nlibspice-server1: 0.14.3-2.1\r\nlvm2: 2.03.11-2.1\r\nlxc-pve: 5.0.0-3\r\nlxcfs: 4.0.12-pve1\r\nnovnc-pve: 1.3.0-3\r\nproxmox-backup-client: 2.2.5-1\r\nproxmox-backup-file-restore: 2.2.5-1\r\nproxmox-mini-journalreader: 1.3-1\r\nproxmox-widget-toolkit: 3.5.1\r\npve-cluster: 7.2-2\r\npve-container: 4.2-2\r\npve-docs: 7.2-2\r\npve-edk2-firmware: 3.20210831-2\r\npve-firewall: 4.2-5\r\npve-firmware: 3.5-1\r\npve-ha-manager: 3.4.0\r\npve-i18n: 2.7-2\r\npve-qemu-kvm: 6.2.0-11\r\npve-xtermjs: 4.16.0-1\r\nqemu-server: 7.2-3\r\nsmartmontools: 7.2-pve3\r\nspiceterm: 3.2-2\r\nswtpm: 0.7.1~bpo11+1\r\nvncterm: 1.7-1\r\nzfsutils-linux: 2.1.5-pve1\r\n\r\nInstallation method:\r\nhttps://docs.docker.com/engine/install/ubuntu/\r\n\r\nExtra:\r\nPlease watch this https://youtu.be/2rmmhF3kb1I\r\nIt shows what the actual problem is\r\n",
      "solution": "I think the issue you're seeing is because you're using the `vfs` [storage driver](https://docs.docker.com/storage/storagedriver/select-storage-driver/); the [`vfs` storage driver](https://docs.docker.com/storage/storagedriver/vfs-driver/) does not support copy-on-write, and doesn't provide a way to share files between layers.\r\n\r\nEffectively, each layer is stored as a directory containing a full copy of the layer and all its parent layers, and each container runs with a full copy of the image. Because of the above, the `vfs` storage driver should only be used as a last resort (if none of the other storage drivers can be used), and mostly for testing purposes.\r\n\r\nThe lack of file sharing between layers makes storage very unefficient, which is amplified if an image contains many layers. For example, an image produced by the following Dockerfile will, when stored using the `vfs` storage driver, take (roughly) 3x the size of the `alpine` image;\r\n\r\n\r\n```dockerfile\r\nFROM alpine\r\nRUN echo \"hello\" > foo.txt\r\nRUN echo \"world\" >> foo.txt\r\n```\r\n\r\nGiving that a spin on a fresh daemon (no other images stored);\r\n\r\n```bash\r\ndocker build -t myimage -<<'EOF'\r\nFROM alpine\r\nRUN echo \"hello\" > foo.txt\r\nRUN echo \"world\" >> foo.txt\r\nEOF\r\n\r\n# clean up build-cache\r\ndocker builder prune\r\n```\r\n\r\nSo, while the \"history\" of the image shows the number of bytes added in each layer...\r\n\r\n```bash\r\ndocker image history myimage\r\nIMAGE          CREATED         CREATED BY                                      SIZE      COMMENT\r\nd89db0ca393e   2 minutes ago   RUN /bin/sh -c echo \"world\" >> foo.txt # bui\u2026   12B       buildkit.dockerfile.v0\r\n<missing>      2 minutes ago   RUN /bin/sh -c echo \"hello\" > foo.txt # buil\u2026   6B        buildkit.dockerfile.v0\r\n<missing>      11 days ago     /bin/sh -c #(nop)  CMD [\"/bin/sh\"]              0B\r\n<missing>      11 days ago     /bin/sh -c #(nop) ADD file:2a949686d9886ac7c\u2026   5.54MB\r\n```\r\n\r\n... the `vfs` driver stores the cumulative size of each; looking in its storage directory, you can see 3 full copies of the alpine image;\r\n\r\n```bash\r\ndu -s /var/lib/docker/vfs/dir/*\r\n6052\t/var/lib/docker/vfs/dir/c281ad09d51abbae1e57b2b2c00d158bf5e3bb7d13377169961671bb6437d1f7\r\n6056\t/var/lib/docker/vfs/dir/ozr2g908tidoglcqrn7dl1na5\r\n6056\t/var/lib/docker/vfs/dir/taamt3cmd6e8hm887bsxbiops\r\n```\r\n\r\nOne of which is the layer from the alpine image, and the second and third contain the `foo.txt` file;\r\n\r\n```bash\r\ncat /var/lib/docker/vfs/dir/ozr2g908tidoglcqrn7dl1na5/foo.txt\r\nhello\r\n\r\ncat /var/lib/docker/vfs/dir/taamt3cmd6e8hm887bsxbiops/foo.txt\r\nhello\r\nworld\r\n```\r\n",
      "labels": [
        "kind/question"
      ],
      "created_at": "2022-08-18T12:47:11Z",
      "closed_at": "2025-04-24T00:33:08Z",
      "url": "https://github.com/docker/cli/issues/3741",
      "comments_count": 1
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 4932,
      "title": "CLI completion has weird bugs",
      "problem": "### Description\r\n\r\nWhen I type `docker` into my terminal and click `<TAB>`, I see some weird stuff:\r\n\r\nhttps://github.com/docker/cli/assets/40357511/20cf40e7-daa7-48df-8028-1e7485619607\r\n\r\n\r\n\r\n\r\n### Reproduce\r\n\r\n(Forgive me for assuming macOS and Homebrew)\r\n\r\n1. `brew install --cask docker`\r\n2. Make sure completion setup code is present in `~/.zshrc`:\r\n\r\n  ```\r\n  FPATH=\"$(brew --prefix)/share/zsh/site-functions:$FPATH\"\r\n  ```\r\n\r\n### Expected behavior\r\n\r\nWhen I type `docker` and click `<TAB>`, I expect to see only commands:\r\n- no `--flags`\r\n- no garbled output like the first and last line, e.g.:\r\n\r\n    ```\r\n    $ docker\r\n    \"warn\",                            -- \"error\", \"fatal\") (default \"info\")\r\n    ```\r\n\r\n<details>\r\n<summary>docker version</summary>\r\n\r\n```bash\r\nClient:\r\n Cloud integration: v1.0.35+desktop.11\r\n Version:           25.0.3\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        4debf41\r\n Built:             Tue Feb  6 21:13:26 2024\r\n OS/Arch:           darwin/arm64\r\n Context:           desktop-linux\r\n\r\nServer: Docker Desktop 4.28.0 (139021)\r\n Engine:\r\n  Version:          25.0.3\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       f417435\r\n  Built:            Tue Feb  6 21:14:22 2024\r\n  OS/Arch:          linux/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.28\r\n  GitCommit:        ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        v1.1.12-0-g51d5e94\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n</details> \r\n\r\n\r\n\r\n<details>\r\n<summary>docker info</summary>\r\n\r\n```bash\r\nClient:\r\n Version:    25.0.3\r\n Context:    desktop-linux\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1-desktop.4\r\n    Path:     /Users/bartek/.docker/cli-plugins/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.6-desktop.1\r\n    Path:     /Users/bartek/.docker/cli-plugins/docker-compose\r\n  debug: Get a shell into any image or container. (Docker Inc.)\r\n    Version:  0.0.24\r\n    Path:     /Users/bartek/.docker/cli-plugins/docker-debug\r\n  dev: Docker Dev Environments (Docker Inc.)\r\n    Version:  v0.1.0\r\n    Path:     /Users/bartek/.docker/cli-plugins/docker-dev\r\n  extension: Manages Docker extensions (Docker Inc.)\r\n    Version:  v0.2.22\r\n    Path:     /Users/bartek/.docker/cli-plugins/docker-extension\r\n  feedback: Provide feedback, right in your terminal! (Docker Inc.)\r\n    Version:  v1.0.4\r\n    Path:     /Users/bartek/.docker/cli-plugins/docker-feedback\r\n  init: Creates Docker-related starter files for your project (Docker Inc.)\r\n    Version:  v1.0.1\r\n    Path:     /Users/bartek/.docker/cli-plugins/docker-init\r\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\r\n    Version:  0.6.0\r\n    Path:     /Users/bartek/.docker/cli-plugins/docker-sbom\r\n  scout: Docker Scout (Docker Inc.)\r\n    Version:  v1.5.0\r\n    Path:     /Users/bartek/.docker/cli-plugins/docker-scout\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 10\r\n Server Version: 25.0.3\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc version: v1.1.12-0-g51d5e94\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: unconfined\r\n  cgroupns\r\n Kernel Version: 6.6.16-linuxkit\r\n Operating System: Docker Desktop\r\n OSType: linux\r\n Architecture: aarch64\r\n CPUs: 8\r\n Total Memory: 7.755GiB\r\n Name: docker-desktop\r\n ID: 31725210-46cb-4bf3-a925-18c0412459d5\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n HTTP Proxy: http.docker.internal:3128\r\n HTTPS Proxy: http.docker.internal:3128\r\n No Proxy: hubproxy.docker.internal\r\n Experimental: false\r\n Insecure Registries:\r\n  hubproxy.docker.internal:5555\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n</details>\r\n",
      "solution": "We switched the default completion to use the \"cobra\" generated completion, which likely fixed this problem.\n\nclosing for now, but let me know if there's still issues.",
      "labels": [
        "kind/bug",
        "status/0-triage",
        "area/completion"
      ],
      "created_at": "2024-03-09T20:02:54Z",
      "closed_at": "2025-04-22T18:09:00Z",
      "url": "https://github.com/docker/cli/issues/4932",
      "comments_count": 5
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 3652,
      "title": "cli hangs on dead connection",
      "problem": "\r\n**Description**\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. Create a socket file with a dead listener by running:\r\n\r\n```\r\nnc -lkU /home/nick/test.sock\r\n```\r\n\r\n2. Run any docker command against the socket\r\n\r\n```\r\nDOCKER_HOST=\"unix:///home/nick/test.sock\" docker ps\r\n```\r\n\r\n**Describe the results you received:**\r\n\r\nThe CLI should eventually time out and/or error\r\n\r\n**Describe the results you expected:**\r\n\r\nThe CLI hangs indefinitely.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nThis is distilled from a longer user report and stack trace upstream:\r\nhttps://github.com/tilt-dev/tilt/issues/5841\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\ndocker version\r\nClient: Docker Engine - Community\r\n Cloud integration: v1.0.24\r\n Version:           20.10.16\r\n API version:       1.41\r\n Go version:        go1.17.10\r\n Git commit:        aa7e414\r\n Built:             Thu May 12 09:17:23 2022\r\n OS/Arch:           linux/amd64\r\n Context:           default\r\n Experimental:      true\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nLinux",
      "solution": "hmmmm....this is still reproducible for me, but i thought we fixed it\r\n\r\ni see a stack trace like:\r\n\r\n```\r\n\t/usr/local/go/src/net/http/client.go:590\r\ngithub.com/docker/cli/vendor/github.com/docker/docker/client.(*Client).doRequest(0xc000258b40, 0xc000258b40?)\r\n\t/go/src/github.com/docker/cli/vendor/github.com/docker/docker/client/request.go:140 +0x7c fp=0xc00063f0b8 sp=0xc00063efd0 pc=0x5609e8a2a55c\r\ngithub.com/docker/cli/vendor/github.com/docker/docker/client.(*Client).Ping(0xc000258b40, {0x5609e9506940, 0x5609e9d51840})\r\n\t/go/src/github.com/docker/cli/vendor/github.com/docker/docker/client/ping.go:28 +0x145 fp=0xc00063f390 sp=0xc00063f0b8 pc=0x5609e8a253a5\r\ngithub.com/docker/cli/vendor/github.com/docker/docker/client.(*Client).NegotiateAPIVersion(0xc000258b40, {0x5609e9506940?, 0x5609e9d51840?})\r\n\t/go/src/github.com/docker/cli/vendor/github.com/docker/docker/client/client.go:310 +0x45 fp=0xc00063f468 sp=0xc00063f390 pc=0x5609e8a0a505\r\ngithub.com/docker/cli/vendor/github.com/docker/docker/client.(*Client).checkVersion(...)\r\n```\n\n---\n\nOK, here's some good threads:\r\n\r\n- https://github.com/docker/cli/issues/149\r\n- https://github.com/docker/cli/pull/546\r\n- https://github.com/moby/moby/pull/39032\r\n\r\nto see if i understand them --\r\n\r\nin 2017, we have this thing called docker datacenter. _ping was for establishing the health of the datacenter as a whole, NOT whether the daemon was alive. So we changed the behavior so that if the ping returned a 500, we would try to keep going and make some more requests.\r\n\r\nin 2019, we changed the client to do its own layer of negotation based on Ping(). we largely subverted the Ping() protocol negotation behavior we added in 2017. But we never went back and fixed the CLI code, so now the CLI and client code have different protocol negotiation protocols :dizzy_face: ",
      "labels": [
        "kind/bug"
      ],
      "created_at": "2022-06-03T15:15:07Z",
      "closed_at": "2025-04-22T17:47:49Z",
      "url": "https://github.com/docker/cli/issues/3652",
      "comments_count": 8
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 3620,
      "title": "Open file handle to the log file makes all container operations to hang",
      "problem": "**Description**\r\n\r\nSome containers make `docker` commands to hang.\r\n\r\nExample:\r\n\r\n```\r\n# docker ps -a | grep ce59\r\nce59e058f135   REDACTED            \"/bin/bash -c 'java \u2026\"   5 days ago     Created                               k8s_basket-api_basket-api-bugfix-2991-correcting-taxrate-59f88d7ccf-4vfqv_com-next-dev_68bf8a2b-2830-4b9d-b4dc-fb7ccdc24457_0\r\n```\r\n\r\nThis container is in `Created` state so it never run AFAIK.\r\n\r\nAny operation on it hangs:\r\n\r\n```\r\n# docker rm -f ce59e058f135\r\nHANGS\r\n\r\n# docker info ce59e058f135\r\nHANGS\r\n\r\n# docker stats ce59e058f135\r\nHANGS\r\n```\r\n\r\n`dockerd` holds a file handle on the log file for that container:\r\n\r\n```\r\n# lsof /var/lib/docker/containers/ce59e058f135016f401d400f41b356e4b631df8a3e81a1040c7b1f01a6169d06/ce59e058f135016f401d400f41b356e4b631df8a3e81a1040c7b1f01a6169d06-json.log \r\nCOMMAND  PID USER   FD   TYPE DEVICE SIZE/OFF      NODE NAME\r\ndockerd 2335 root  366w   REG 259,10        0 220494219 /var/lib/docker/containers/ce59e058f135016f401d400f41b356e4b631df8a3e81a1040c7b1f01a6169d06/ce59e058f135016f401d400f41b356e4b631df8a3e81a1040c7b1f01a6169d06-json.log\r\n```\r\n\r\nand I suspect that is the reason why everything is stuck.\r\n\r\n`systemctl restart docker` makes the container and the FD disappear and the problem is solved.\r\n\r\n**Steps to reproduce the issue:**\r\n\r\nUnfortuately I don't know how I get into this situation.\r\nContainer is created by kubernetes and that's all I know.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:           20.10.12\r\n API version:       1.41\r\n Go version:        go1.17.8\r\n Git commit:        60293e390e\r\n Built:             Tue Apr  5 17:22:59 2022\r\n OS/Arch:           linux/amd64\r\n Context:           default\r\n Experimental:      true\r\n\r\nServer:\r\n Engine:\r\n  Version:          20.10.12\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.17.8\r\n  Git commit:       bd6d47cb47\r\n  Built:            Tue Apr  5 17:23:02 2022\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     true\r\n containerd:\r\n  Version:          1.5.11\r\n  GitCommit:        17624fd6043a8c74bb40105472f4ec9eff59bef1\r\n runc:\r\n  Version:          1.1.0+dev.docker-20.10\r\n  GitCommit:        b083ef4992ddc33fc7e699170c4abafa74d17818\r\n docker-init:\r\n  Version:          0.19.0de40ad007797e0dcd8b7126f27bb87401d224240\r\n  GitCommit:\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 43\r\n  Running: 31\r\n  Paused: 0\r\n  Stopped: 12\r\n Images: 150\r\n Server Version: 20.10.12\r\n Storage Driver: overlay2\r\n  Backing Filesystem: xfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: false\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runtime.v1.linux runc io.containerd.runc.v2\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 17624fd6043a8c74bb40105472f4ec9eff59bef1\r\n runc version: b083ef4992ddc33fc7e699170c4abafa74d17818\r\n init version: \r\n Security Options:\r\n  seccomp\r\n   Profile: default\r\n  selinux\r\n Kernel Version: 5.15.32-flatcar\r\n Operating System: Flatcar Container Linux by Kinvolk 3139.2.0 (Oklo)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 16\r\n Total Memory: 62.03GiB\r\n Name: REDACTED\r\n ID: XPLD:ASQJ:MCMW:CMLZ:HNBY:YXLW:M362:HXLC:24L7:2MVD:TAGZ:VME7\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n Username: giantswarmpull\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: true\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Registry Mirrors:\r\n  https://giantswarm.azurecr.io/\r\n Live Restore Enabled: true\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nThis is an AWS EC2 instance.\r\n",
      "solution": "We found out what's causing the issue, if it might spark an idea on your end.\r\nThere was an error in the Kubernetes Pod Spec, and the container failed creating with this error:\r\n\r\n```\r\nError response from daemon: failed to create shim: OCI runtime create failed: runc create failed: unable to start container process: error during container init: environment variable value can't contain null(\\x00): \"ENV_NAME=REDACTED BINARY-LIKE STUFF\"\r\n```\r\n\r\nThat explains how the issue happens, not why.\n\n---\n\nI have been able to get the bull byte error using the docker client library:\r\n\r\n```\r\npackage main\r\n\r\nimport (\r\n\t\"context\"\r\n\r\n\t\"github.com/docker/docker/api/types/container\"\r\n\t\"github.com/docker/docker/api/types/network\"\r\n\t\"github.com/docker/docker/client\"\r\n\tv1 \"github.com/opencontainers/image-spec/specs-go/v1\"\r\n)\r\n\r\nfunc main() {\r\n\tcli, err := client.NewClientWithOpts(client.FromEnv)\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\r\n\t_, err = cli.ContainerCreate(\r\n\t\tcontext.Background(),\r\n\t\t&container.Config{\r\n\t\t\tImage: \"alpine:latest\",\r\n\t\t\tEnv: []string{\r\n\t\t\t\t\"TEST=prefix\\x00suffix\",\r\n\t\t\t},\r\n\t\t},\r\n\t\t&container.HostConfig{},\r\n\t\t&network.NetworkingConfig{},\r\n\t\t&v1.Platform{},\r\n\t\t\"test-container\",\r\n\t)\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n}\r\n```\r\n\r\nBut, to my big surprise, this does not trigger the issue.\r\n\r\n```\r\n$ go run main.go \r\n$ docker start test-container\r\nError response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: environment variable value can't contain null(\\x00): \"TEST=prefix\\x00suffix\": unknown\r\nError: failed to start containers: test-container\r\n$ docker rm test-container \r\ntest-container\r\n```\r\n\r\nSo there must be something else as well.",
      "labels": [
        "area/runtime"
      ],
      "created_at": "2022-05-18T14:54:59Z",
      "closed_at": "2025-04-22T17:41:21Z",
      "url": "https://github.com/docker/cli/issues/3620",
      "comments_count": 6
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 3624,
      "title": "Ampersand in username for docker login",
      "problem": "**Description**\r\n\r\nUsing `docker login` with a username that contains the ampersand (&) in a private registry not working.\r\n\r\nOS: Ubuntu 18.04\r\nShell: Bash\r\nDocker Version: 20.10.16\r\n\r\nThe ampersand is always written as \"\\u0026\" which probably leads to the authentication failure. No escaping helped:\r\n\r\n* \"usernameWithAmpersand&\"\r\n* \"usernameWithAmpersand\\&\"\r\n* usernameWithAmpersand\"&\"\r\n* 'usernameWithAmpersand&'\r\n* 'usernameWithAmpersand\\&'\r\n* usernameWithAmpersand'&'\r\n\r\nThe password / CLI Token were resetted multiple times, copy&pasted and even manually entered.\r\n\r\nAny help would be appreciated. Unfortunately the username is provided and cannot be changed that simple. For users from Mac with Docker GUI everything works as expected.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Try login to docker registry with a username containing an ampersand\r\n2. Login failed\r\n\r\n**Describe the results you received:**\r\nLogin failed since the ampersand is written as \"\\u0026\".\r\n\r\nIn docker debug / verbose mode following were printed:\r\nform data: {\"password\":\"*****\",\"serveraddress\":\"myserver\",\"username\":\"usernameWithAmpersand\\u0026\"} \r\n\r\n**Describe the results you expected:**\r\nLogin success.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nDocker version 20.10.16, build aa7e414\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  app: Docker App (Docker Inc., v0.9.1-beta3)\r\n  buildx: Docker Buildx (Docker Inc., v0.8.2-docker)\r\n  scan: Docker Scan (Docker Inc., v0.17.0)\r\n\r\nServer:\r\n Containers: 12\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 12\r\n Images: 135\r\n Server Version: 20.10.16\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 212e8b6fa2f44b9c21b2798135fc6fb7c53efc16\r\n runc version: v1.1.1-0-g52de29d\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 5.4.0-110-generic\r\n Operating System: Ubuntu 18.04.6 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 16\r\n Total Memory: 62.57GiB\r\n Name: my_host_name\r\n ID: J5V4:CCK4:OPS4:PYIQ:TPOQ:5SW2:M6TF:L5EA:5DA4:S2UM:L36W:Z7ID\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: true\r\n  File Descriptors: 24\r\n  Goroutines: 33\r\n  System Time: 2022-05-19T15:49:06.412529695+02:00\r\n  EventsListeners: 0\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: No swap limit support\r\n```\r\n\r\nBest regards\r\n",
      "solution": "Thank you for the quick response.\r\n\r\nThe support team assumed that this should be the root cause. I will investigate further and try to get back to the support team as well to let them check if there is any misconfiguration or anything.",
      "labels": [
        "area/authentication"
      ],
      "created_at": "2022-05-19T13:56:11Z",
      "closed_at": "2025-04-22T17:40:08Z",
      "url": "https://github.com/docker/cli/issues/3624",
      "comments_count": 4
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 3363,
      "title": "Cannot build Docker without first installing Docker",
      "problem": "<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/cli/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Docker Community Slack - https://dockr.ly/community\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nThere are no instructions for building docker without having it already installed. \r\n\r\n**Steps to reproduce the issue:**\r\n1. Fresh Mac OSX installation\r\n2. Do not install Docker Desktop\r\n3. Try to build `docker-cli` for use against another Docker host\r\n\r\n**Describe the results you received:**\r\n\r\nI could not build docker without first installing Docker.\r\n\r\n**Describe the results you expected:**\r\n\r\nA straightforward way to build Docker without first installing Docker.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\nThis impacts everyone.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nN/A\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nN/A\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nN/A",
      "solution": "`make binary` should also allow to build locally. Work is progressing on making the repositories module-aware, so hopefully the problem should go away soon.",
      "labels": [
        "kind/question"
      ],
      "created_at": "2021-11-16T04:28:51Z",
      "closed_at": "2025-04-22T17:31:00Z",
      "url": "https://github.com/docker/cli/issues/3363",
      "comments_count": 3
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 3241,
      "title": "20.10.8  introduces breaking change to go templates",
      "problem": "**Description**\r\nRunning 20.10.8 we're getting errors with our template parsing.\r\n\r\n\r\n<!--\r\nBriefly describe the problem you are having in a few paragraphs.\r\n-->\r\n\r\n**Steps to reproduce the issue:**\r\n1. Try to run the cli with a command similar to `docker run --detach --network=\"$NETWORK_NAME\" $(echo --env LOG_LEVEL=$(docker inspect --format='{{((index .Config.Labels \"loglevel\") 0)}}' $IMAGE_NAME:$IMAGE_TAG_FEATURE))`\r\n2. It should fail.\r\n\r\n**Describe the results you received:**\r\n```\r\nTemplate parsing error: template: :1:3: executing \"\" at <(index .Config.Labels \"loglevel\") 0>: can't give argument to non-function index .Config.Labels \"loglevel\"\r\n```\r\n\r\n\r\n**Describe the results you expected:**\r\nIt should parse without an error, as it does with version 20.10.7 - a breaking change with a bugfix version is surprising.\r\n\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\nVirtual runner controlled via gitlab\r\n",
      "solution": "Steps to reproduce the issue:\r\n\r\n1. docker pull timescale/timescaledb:latest-pg12\r\n2. docker inspect --format='{{((index .Config.Labels \"maintainer\") 0)}}' timescale/timescaledb:latest-pg12\r\n\r\nOutput of 20.10.7:\r\nTimescale https://www.timescale.com\r\nOutput of 20.10.8:\r\nTemplate parsing error: template: :1:3: executing \"\" at <(index .Config.Labels \"maintainer\") (0)>: can't give argument to non-function index .Config.Labels \"maintainer\"\r\n\r\nGo Template has to be modified to:\r\n`docker inspect --format='{{if (index .Config.Labels \\\"maintainer\\\")}}{{index .Config.Labels \\\"maintainer\\\"}}{{else}}0{{end}}' timescale/timescaledb:latest-pg12`\r\n\r\n@thaJeztah @razzeee PTL\r\n",
      "labels": [
        "version/20.10"
      ],
      "created_at": "2021-08-10T15:53:32Z",
      "closed_at": "2025-04-22T17:22:53Z",
      "url": "https://github.com/docker/cli/issues/3241",
      "comments_count": 4
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 3010,
      "title": "signal SIGSEGV: segmentation violation during `docker login`",
      "problem": "**Description**\r\nI am attempting a docker login after adding a new `credHelpers` like so.\r\n```\r\n located at [/usr/local/google/home/mjog/.docker/config.json]:\r\n {\r\n  \"credHelpers\": {\r\n    \"gcr.io\": \"gcloud\",\r\n    \"us-west2-docker.pkg.dev\": \"gcloud\",\r\n    \"marketplace.gcr.io\": \"gcloud\",\r\n    \"eu.gcr.io\": \"gcloud\",\r\n    \"asia.gcr.io\": \"gcloud\",\r\n    \"staging-k8s.gcr.io\": \"gcloud\",\r\n    \"us.gcr.io\": \"gcloud\"\r\n  }\r\n}\r\n```\r\nI was not locally logged onto `gcloud` so the login failed and resulted in this\r\n\r\n```\r\ndocker login us-west2-docker.pkg.dev\r\nERROR: (gcloud.auth.docker-helper) There was a problem refreshing your current auth tokens: ('invalid_grant: Bad Request', u'{\\n  \"error\": \"invalid_grant\",\\n  \"error_description\": \"Bad Request\"\\n}')\r\nPlease run:\r\n\r\n  $ gcloud auth login\r\n\r\nto obtain new credentials.\r\nIf you have already logged in with a different account:\r\n\r\n    $ gcloud config set account ACCOUNT\r\n\r\nto select an already authenticated account to use.\r\npanic: runtime error: invalid memory address or nil pointer dereference\r\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x8 pc=0x55eb13b45406]\r\n\r\ngoroutine 1 [running]:\r\ngithub.com/docker/cli/cli/command.ConfigureAuth(0x55eb15587e80, 0xc00035ab60, 0x0, 0x0, 0x0, 0x0, 0x0, 0x55eb154f7200, 0xc0005b6bb0, 0xc00059d968)\r\n\t/go/src/github.com/docker/cli/cli/command/registry.go:128 +0x46\r\ngithub.com/docker/cli/cli/command/registry.runLogin(0x55eb15587e80, 0xc00035ab60, 0x7ffd7edf2e69, 0x17, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n\t/go/src/github.com/docker/cli/cli/command/registry/login.go:123 +0x223\r\ngithub.com/docker/cli/cli/command/registry.NewLoginCommand.func1(0xc00018f080, 0xc0005b6090, 0x1, 0x1, 0x0, 0x0)\r\n\t/go/src/github.com/docker/cli/cli/command/registry/login.go:45 +0xcc\r\ngithub.com/docker/cli/vendor/github.com/spf13/cobra.(*Command).execute(0xc00018f080, 0xc0003526f0, 0x1, 0x1, 0xc00018f080, 0xc0003526f0)\r\n\t/go/src/github.com/docker/cli/vendor/github.com/spf13/cobra/command.go:850 +0x462\r\ngithub.com/docker/cli/vendor/github.com/spf13/cobra.(*Command).ExecuteC(0xc000445b80, 0xc0003526e0, 0x2, 0x2)\r\n\t/go/src/github.com/docker/cli/vendor/github.com/spf13/cobra/command.go:958 +0x34b\r\ngithub.com/docker/cli/vendor/github.com/spf13/cobra.(*Command).Execute(...)\r\n\t/go/src/github.com/docker/cli/vendor/github.com/spf13/cobra/command.go:895\r\nmain.runDocker(0xc00035ab60, 0x55eb154f9f80, 0xc0000ec010)\r\n\t/go/src/github.com/docker/cli/cmd/docker/docker.go:287 +0x1d3\r\nmain.main()\r\n\t/go/src/github.com/docker/cli/cmd/docker/docker.go:298 +0xf3\r\n\r\n```\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient: Docker Engine - Community\r\n Version:           20.10.2\r\n API version:       1.41\r\n Go version:        go1.13.15\r\n Git commit:        2291f61\r\n Built:             Mon Dec 28 16:17:34 2020\r\n OS/Arch:           linux/amd64\r\n Context:           default\r\n Experimental:      true\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          20.10.2\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.13.15\r\n  Git commit:       8891c58\r\n  Built:            Mon Dec 28 16:15:28 2020\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.4.3\r\n  GitCommit:        269548fa27e0089a8b8278fc4fc781d7f65a939b\r\n runc:\r\n  Version:          1.0.0-rc92\r\n  GitCommit:        ff819c7e9184c13b7c2607fe6c30ae19403a7aff\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  app: Docker App (Docker Inc., v0.9.1-beta3)\r\n  buildx: Build with BuildKit (Docker Inc., v0.5.1-docker)\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 12\r\n Server Version: 20.10.2\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 269548fa27e0089a8b8278fc4fc781d7f65a939b\r\n runc version: ff819c7e9184c13b7c2607fe6c30ae19403a7aff\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: default\r\n  cgroupns\r\n Kernel Version: 5.7.17-1rodete5-amd64\r\n Operating System: Debian GNU/Linux rodete\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 31.36GiB\r\n Name: mjog.c.googlers.com\r\n ID: FFTI:FZT4:CRSW:GPFZ:LXPM:L3A2:BLOW:SV64:5ZGV:2AVU:ZIBX:5TJP\r\n Docker Root Dir: /usr/local/google/docker\r\n Debug Mode: true\r\n  File Descriptors: 22\r\n  Goroutines: 33\r\n  System Time: 2021-03-12T22:54:28.064936545Z\r\n  EventsListeners: 0\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Registry Mirrors:\r\n  https://mirror.gcr.io/\r\n Live Restore Enabled: false\r\n```\r\n\r\n",
      "solution": "Hey I just was running into this myself earlier today, I think this [pr](https://github.com/docker/cli/pull/2959) fixed the problem. I updated docker to 20.10.5 and the problem went away, hope that helps for you!\n\n---\n\nlooks like this went stale, and possibly fixed per above. closing",
      "labels": [],
      "created_at": "2021-03-12T22:55:37Z",
      "closed_at": "2025-04-22T17:16:58Z",
      "url": "https://github.com/docker/cli/issues/3010",
      "comments_count": 2
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 3009,
      "title": "docker cp: relative paths should resolve relative to the image WORKDIR",
      "problem": "<!--\r\nIf you are reporting a new issue, make sure that we do not have any duplicates\r\nalready open. You can ensure this by searching the issue list for this\r\nrepository. If there is a duplicate, please close your issue and add a comment\r\nto the existing issue instead.\r\n\r\nIf you suspect your issue is a bug, please edit your issue description to\r\ninclude the BUG REPORT INFORMATION shown below. If you fail to provide this\r\ninformation within 7 days, we cannot debug your issue and will close it. We\r\nwill, however, reopen it if you later provide the information.\r\n\r\nFor more information about reporting issues, see\r\nhttps://github.com/docker/cli/blob/master/CONTRIBUTING.md#reporting-other-issues\r\n\r\n---------------------------------------------------\r\nGENERAL SUPPORT INFORMATION\r\n---------------------------------------------------\r\n\r\nThe GitHub issue tracker is for bug reports and feature requests.\r\nGeneral support can be found at the following locations:\r\n\r\n- Docker Support Forums - https://forums.docker.com\r\n- Docker Community Slack - https://dockr.ly/community\r\n- Post a question on StackOverflow, using the Docker tag\r\n\r\n---------------------------------------------------\r\nBUG REPORT INFORMATION\r\n---------------------------------------------------\r\nUse the commands below to provide key information from your environment:\r\nYou do NOT have to include this information if this is a FEATURE REQUEST\r\n-->\r\n\r\n**Description**\r\n\r\nWhen copying arbitrary files from the host machine to a relative path on a running container, the file is copied relative to the root of the container. It seems reasonable to change this behavior such that files are copied relative to the WORKDIR path of the image.\r\n\r\nI maintain a large monolithic application as part of my day-to-day work, and we have our product running in containers on our workstations for development. I often copy artifacts to and from the \"app-server\" containers with `docker cp`. Our product has strict requirements on where it is deployed to in the container (out of our control), so we have a specific WORKDIR in our base image.\r\n\r\nI'd be to be able to run something like `docker cp <artifact on host> <container name>:.` to copy the artifact to the working directory, but the file is copied to the root of the container instead. If relative paths were resolved relative to the WORKDIR, I wouldn't have to type the full path to the working directory every time.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Create an image from this Dockerfile:\r\n\r\n```\r\n$ cat Dockerfile\r\nFROM centos:latest\r\n\r\nRUN adduser -ms /bin/bash eip && usermod -aG wheel eip && echo 'eip:eippass' | chpasswd\r\nUSER eip\r\nWORKDIR /home/eip\r\n\r\n$ docker build -t test .\r\n```\r\n\r\n2. Run `tail -F /dev/null` the container, detatched:\r\n\r\n```\r\n$ docker run --rm -d --name \"docker_cp_test\" test tail -F /dev/null\r\n```\r\n\r\n3. Copy an arbitrary file to the container, like so:\r\n\r\n```\r\n$ docker cp Dockerfile docker_cp_test:.\r\n```\r\n\r\n4. Open a shell in your container and list files at the working directory:\r\n\r\n```\r\n$ docker exec -it docker_cp_test bash -c \"ls Dockerfile\"\r\nls: cannot access 'Dockerfile': No such file or directory\r\n$ docker exec -it docker_cp_test bash -c \"ls /Dockerfile\"\r\n/Dockerfile\r\n```\r\n\r\n**Describe the results you received:**\r\nThe file was copied to the root of the container file system.\r\n\r\n**Describe the results you expected:**\r\nI expect the file should be copied relative to the WORKDIR of the image.\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient: Docker Engine - Community\r\n Version:           20.10.5\r\n API version:       1.40\r\n Go version:        go1.13.15\r\n Git commit:        55c4c88\r\n Built:             Tue Mar  2 20:17:50 2021\r\n OS/Arch:           linux/amd64\r\n Context:           default\r\n Experimental:      true\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          19.03.14\r\n  API version:      1.40 (minimum version 1.12)\r\n  Go version:       go1.13.15\r\n  Git commit:       5eb3275d40\r\n  Built:            Tue Dec  1 19:18:50 2020\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.4.4\r\n  GitCommit:        05f951a3781f4f2c1911b05e61c160e9c30eaa8e\r\n runc:\r\n  Version:          1.0.0-rc93\r\n  GitCommit:        12644e614e25b05da6fd08a38ffa0cfe1903fdec\r\n docker-init:\r\n  Version:          0.18.0\r\n  GitCommit:        fec3683\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  app: Docker App (Docker Inc., v0.9.1-beta3)\r\n  buildx: Build with BuildKit (Docker Inc., v0.5.1-docker)\r\n\r\nServer:\r\n Containers: 5\r\n  Running: 5\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 69\r\n Server Version: 19.03.14\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 05f951a3781f4f2c1911b05e61c160e9c30eaa8e\r\n runc version: 12644e614e25b05da6fd08a38ffa0cfe1903fdec\r\n init version: fec3683\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 5.7.0-0.bpo.2-amd64\r\n Operating System: Debian GNU/Linux 10 (buster)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 12\r\n Total Memory: 62.63GiB\r\n Name: md2qhqqc\r\n ID: IPZA:DVYZ:OLTM:MKZ6:NLC3:CQGZ:HXNV:JAK7:PVGD:WIO7:FFLA:YMAA\r\n Docker Root Dir: /home/docker\r\n Debug Mode: false\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: No swap limit support\r\n```",
      "solution": "@cpuguy83 potentially so, if anyone relies on this relative path resolution behaviour. It might be appropriate to expose some kind of fallback configuration option.",
      "labels": [],
      "created_at": "2021-03-12T19:12:25Z",
      "closed_at": "2025-04-22T17:15:11Z",
      "url": "https://github.com/docker/cli/issues/3009",
      "comments_count": 3
    },
    {
      "tech": "docker",
      "repo": "docker/cli",
      "issue_number": 4401,
      "title": "Docker autocompletion for containers not working on MacOS",
      "problem": "### Description\n\nI'm migrating from Ubuntu to MacOS with my MacBook Pro (M2). I'm using ohmyzsh with the docker and docker-compose plugin. Docker has been installed with [Docker Desktop setup](https://docs.docker.com/desktop/install/mac-install/).\r\n\r\nThe docker plugin works fine except the autocompletion for the container names. \r\n- If I type `docker` and \u2b95Tab it shows me a list of commands\r\n- If I type `docker start` and \u2b95Tab it just shows me the files of my current directory, but not the list of available containers.\r\n\r\nI'm expecting the autosuggestion to work here and show me a list of all containers which could be started. This worked fine within Ubuntus bash.\r\n\r\nWhat I did so far:\r\n- deactivate ohmyzsh (no completions at all)\r\n- ask google (obviously) \r\n- try the help sections for common problems on the [ohmyzsh FAQ](https://github.com/ohmyzsh/ohmyzsh/wiki/FAQ)\r\n- ask [apple stackexchange](https://apple.stackexchange.com/questions/461613/auto-suggestions-for-docker-container-names-with-ohmyzsh?noredirect=1#comment678856_461613) \r\n- ask ohmyzsh community who referred me to ask here\r\n\r\n### Screenshots and recordings\r\n\r\n`.zshrc`\r\n```sh\r\n# Path to your oh-my-zsh installation.\r\nexport ZSH=\"$HOME/.oh-my-zsh\"\r\n\r\n# history\r\nSAVEHIST=10000\r\n\r\n# ohmyzsh\r\nZSH_THEME=\"lukerandall\"\r\nzstyle ':omz:update' mode reminder  # just remind me to update when it's time\r\n# docker autocompletion\r\n#zstyle ':completion:*:*:docker:*' option-stacking yes\r\n#zstyle ':completion:*:*:docker-*:*' option-stacking yes\r\nplugins=(brew docker docker-compose git kubectl kubectx)\r\nsource $ZSH/oh-my-zsh.sh\r\n\r\n# nvm\r\nexport NVM_DIR=\"$HOME/.nvm\"\r\n[ -s \"$NVM_DIR/nvm.sh\" ] && \\. \"$NVM_DIR/nvm.sh\"  # This loads nvm\r\n[ -s \"$NVM_DIR/bash_completion\" ] && \\. \"$NVM_DIR/bash_completion\"  # This loads nvm bash_completion\r\n\r\nautoload -U compinit && compinit\r\n\r\n# kubectl\r\nalias k=\"kubectl\"\r\n# krew\r\nexport PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"\r\n\r\n\r\n#THIS MUST BE AT THE END OF THE FILE FOR SDKMAN TO WORK!!!\r\nexport SDKMAN_DIR=\"$HOME/.sdkman\"\r\n[[ -s \"$HOME/.sdkman/bin/sdkman-init.sh\" ]] && source \"$HOME/.sdkman/bin/sdkman-init.sh\"\r\n```\r\n---\r\nScreenshots:\r\n![Screenshot 2023-07-05 at 16 41 19](https://github.com/ohmyzsh/ohmyzsh/assets/9378662/4ad1d3a9-9858-4f34-9a70-5f0fe42f2f4e)\r\n![Screenshot 2023-07-05 at 16 41 38](https://github.com/ohmyzsh/ohmyzsh/assets/9378662/8a83df09-b156-4583-84d6-3d4471389e29)\r\n![Screenshot 2023-07-05 at 16 42 11](https://github.com/ohmyzsh/ohmyzsh/assets/9378662/8a8ecc15-86fd-4775-8959-f1c94e868a64)\r\n\r\n\r\n\r\n\n\n### Reproduce\n\n `docker start`+ \u2b95Tab  \n\n### Expected behavior\n\nShow list of containers which are ready to start\n\n### docker version\n\n```bash\nClient:\r\n Cloud integration: v1.0.33\r\n Version:           24.0.2\r\n API version:       1.43\r\n Go version:        go1.20.4\r\n Git commit:        cb74dfc\r\n Built:             Thu May 25 21:51:16 2023\r\n OS/Arch:           darwin/arm64\r\n Context:           desktop-linux\r\n\r\nServer: Docker Desktop 4.20.1 (110738)\r\n Engine:\r\n  Version:          24.0.2\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.4\r\n  Git commit:       659604f\r\n  Built:            Thu May 25 21:50:59 2023\r\n  OS/Arch:          linux/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.21\r\n  GitCommit:        3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc:\r\n  Version:          1.1.7\r\n  GitCommit:        v1.1.7-0-g860f061\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient:\r\n Version:    24.0.2\r\n Context:    desktop-linux\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.10.5\r\n    Path:     /Users/julian/.docker/cli-plugins/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.18.1\r\n    Path:     /Users/julian/.docker/cli-plugins/docker-compose\r\n  dev: Docker Dev Environments (Docker Inc.)\r\n    Version:  v0.1.0\r\n    Path:     /Users/julian/.docker/cli-plugins/docker-dev\r\n  extension: Manages Docker extensions (Docker Inc.)\r\n    Version:  v0.2.19\r\n    Path:     /Users/julian/.docker/cli-plugins/docker-extension\r\n  init: Creates Docker-related starter files for your project (Docker Inc.)\r\n    Version:  v0.1.0-beta.4\r\n    Path:     /Users/julian/.docker/cli-plugins/docker-init\r\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\r\n    Version:  0.6.0\r\n    Path:     /Users/julian/.docker/cli-plugins/docker-sbom\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.26.0\r\n    Path:     /Users/julian/.docker/cli-plugins/docker-scan\r\n  scout: Command line tool for Docker Scout (Docker Inc.)\r\n    Version:  v0.12.0\r\n    Path:     /Users/julian/.docker/cli-plugins/docker-scout\r\n\r\nServer:\r\n Containers: 3\r\n  Running: 2\r\n  Paused: 0\r\n  Stopped: 1\r\n Images: 5\r\n Server Version: 24.0.2\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc version: v1.1.7-0-g860f061\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 5.15.49-linuxkit-pr\r\n Operating System: Docker Desktop\r\n OSType: linux\r\n Architecture: aarch64\r\n CPUs: 6\r\n Total Memory: 7.667GiB\r\n Name: docker-desktop\r\n ID: 235862c7-714f-4c03-a56e-fed6a8f78116\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n HTTP Proxy: http.docker.internal:3128\r\n HTTPS Proxy: http.docker.internal:3128\r\n No Proxy: hubproxy.docker.internal\r\n Experimental: false\r\n Insecure Registries:\r\n  hubproxy.docker.internal:5555\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\nmacOS 13.14.1\r\nZsh version 5.9",
      "solution": "Hiya @Theiaz, thanks for the report. Indeed, the current zsh completion script is a bit broken for container name completion \u2013 I started looking into fixing this some time ago, but haven't had the time yet. A quick alternative is to use the Cobra V2 completion script generated by `docker completion zsh`: we haven't switched over to using these by default, but in my experience (at least for zsh), it seems to work pretty well.\r\n\r\nYou can try this out by running\r\n```shell\r\n$ source <(docker completion zsh)\r\n```\n\n---\n\nHi all, just find this issue on my macbook, I install docker through download .dmg docker desktop, and setting `$PATH` env to use docker command on zsh, like this:\r\n\r\n```bash\r\n# ~/.zshrc\r\nexport PATH=\"$PATH:$HOME/.docker/bin\"\r\nplugins=(\r\n...\r\ndocker\r\ndocker-compose\r\n...\r\n)\r\n```\r\n\r\nand my docker command can use completions, but it\u2019s likely older version (not information for every sub-command and if I type `docker rm <tab>` it will not check my system container ID)\r\n\r\nlike this:\r\n\r\n<img width=\"1621\" alt=\"image\" src=\"https://github.com/docker/cli/assets/49491054/779d0992-258e-494b-abee-d83c4a942d6d\">\r\n\r\n\r\nand I using `find` command check all _docker file, I get result:\r\n\r\n```bash\r\nfind ~ 2>/dev/null | grep _docker\r\n```\r\n<img width=\"1510\" alt=\"image\" src=\"https://github.com/docker/cli/assets/49491054/cbbe17de-5604-4aa2-ab60-1c754f73cdc2\">\r\n\r\n\r\nand the first one is auto-generate if you setting at `~/.zshrc plugins=(docker)`,\r\nbut it\u2019s older version (I think this from `docker completion zsh` )\r\n\r\nthe correct one is`/Users/$USER/.oh-my-zsh/plugins/completions/_docker` , but at my macbook, if you check $FPATH envirable, there is no have `/Users/$USER/.oh-my-zsh/plugins/docker/completions/` \r\n<img width=\"999\" alt=\"image\" src=\"https://github.com/docker/cli/assets/49491054/4e68c233-5410-46bf-91bb-c7af429e6608\">\r\n\r\n\r\nand there have  `/Users/$USER/.oh-my-zsh/plugins/docker` , so I create Symbolic link through:\r\n\r\n```bash\r\nln -s /Users/$USER/.oh-my-zsh/plugins/docker/completions/_docker /Users/$USER/.oh-my-zsh/plugins/docker/_docker\r\n# restart zsh or source ~/.zshrc\r\nsource ~/.zshrc\r\n```\r\n\r\nNOTE: If you don\u2019t have `/Users/$USER/.oh-my-zsh/plugins/docker/completions/_docker` you can get from here:https://raw.githubusercontent.com/docker/cli/master/contrib/completion/zsh/_docker\r\n\r\nand now, docker command can get my tab like this:\r\n\r\n<img width=\"1380\" alt=\"image\" src=\"https://github.com/docker/cli/assets/49491054/1a478274-c148-4b5d-ae70-3580f1bb5175\">\r\n\r\n<img width=\"1287\" alt=\"image\" src=\"https://github.com/docker/cli/assets/49491054/d77c881e-376d-457e-b5d0-9af93f28dc6c\">\r\n\r\nNot a very pretty solution, but works for me.\r\n\r\nhope this help someone :)\r\n\r\n---\r\n\r\nThis my environment reference: \r\n\r\n- OS: macOS sonama 14.0\r\n\r\n```bash\r\n# docker version  \r\nClient:\r\n Cloud integration: v1.0.35+desktop.5\r\n Version:           24.0.6\r\n API version:       1.43\r\n Go version:        go1.20.7\r\n Git commit:        ed223bc\r\n Built:             Mon Sep  4 12:28:49 2023\r\n OS/Arch:           darwin/arm64\r\n Context:           desktop-linux\r\n\r\nServer: Docker Desktop 4.25.0 (126437)\r\n Engine:\r\n  Version:          24.0.6\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.7\r\n  Git commit:       1a79695\r\n  Built:            Mon Sep  4 12:31:36 2023\r\n  OS/Arch:          linux/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.22\r\n  GitCommit:        8165feabfdfe38c65b599c4993d227328c231fca\r\n runc:\r\n  Version:          1.1.8\r\n  GitCommit:        v1.1.8-0-g82f18fe\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n# omz version\r\nmaster (38c01a2)\r\n```\n\n---\n\nAt the moment, the introduction of the new docker completion script (`docker completion zsh`) isn't working very well. As I'm using oh-my-zsh, I would suggest using the legacy completion script as described here : https://github.com/ohmyzsh/ohmyzsh/blob/master/plugins/docker/README.md. It fixed the issue for me.\r\n\r\nFurther explanation here : https://github.com/ohmyzsh/ohmyzsh/issues/11789",
      "labels": [
        "kind/bug",
        "status/0-triage",
        "area/completion"
      ],
      "created_at": "2023-07-05T15:09:35Z",
      "closed_at": "2025-04-22T15:20:23Z",
      "url": "https://github.com/docker/cli/issues/4401",
      "comments_count": 25
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51833,
      "title": "IPv6 gateway on docker bridge becomes unreachable after period of no outbound traffic",
      "problem": "### Description\n\nI think im right for that issue here, not sure.\n\nIve discovered a very odd behavior in a wide range of docker engine versions. If i have a docker bridge with ipv6 enabled, i couldnt reach the program inside the container via ipv6, ipv4 worked just fine.\n\nIve discovered, that when there is no ipv6 traffic going out of the container (not totally correct it has to be specific traffic for the docker gateway, because the build in vpn connection does work fine via ipv6), it just loses the gateway of the dockerbridge somehow or the gateway loses the container.\n\nSo my scenario was, ive a custom bridge (also the default bridge works that way), on that bridge is a vpn container, which acts as network for a dns container, that has a route of fd00::/64 via the bridge gateway, where fd00::/64 is my lan.\n\nUpon starting the container, i could access everything via the host ipv6 that was exposed. That would last for a couple seconds, then all of the sudden, it times out and i get connection errors.  Now, when i started a ping to the bridge gateway from within the container, i was able to access the application again. The same happens if you traceroute from within the container to any lan device. That lasts for a couple seconds again before it dies out again. \nI dont think this will happen during \"normal\" connections, as the container would usually always use the docker gateway for communication. But it in this case, it uses the wireguard network, so i reckon thats why its so \"weird\".\n\nThis absolutely does not happen with ipv4 connections. \n\nAs a work around ive had to implement a permanent ping to the docker gateway to keep any incoming ipv6 requests working\nhttps://github.com/Mainfrezzer/wireguard-bridge/commit/d0d196e170821b7fdbbaccf41f0916ee19d7bec2\n\n### Reproduce\n\nWell, as i can only speak about my vpn container, sooo, \n\nsetup either the wireguard container\nhttps://github.com/Mainfrezzer/wireguard-bridge?tab=readme-ov-file#docker-run\nor the amnezia one\nhttps://github.com/Mainfrezzer/amnezia-bridge?tab=readme-ov-file#docker-run\n**Very important for both, dont use latest or >0.1.1 as the \"ping workaround\" is in the image, use 0.1.0**\n\nattach anything to it with a webgui or other service like dns. (dont forget to expose the ports on the vpn container)\n\nwait a bit and you wont be able to access anything via ipv6 until you ping or traceroute from within the container out via docker gateway\n\n<img width=\"618\" height=\"454\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/0eb6be03-548e-46b1-95fa-54b472ab2fc2\" />\n\n### Expected behavior\n\nThe connection between the docker bridge gateway and docker container should stay established\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           29.1.3\n API version:       1.52\n Go version:        go1.25.5\n Git commit:        f52814d\n Built:             Fri Dec 12 14:49:15 2025\n OS/Arch:           linux/arm64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          29.1.3\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.5\n  Git commit:       fbf3ed2\n  Built:            Fri Dec 12 14:49:15 2025\n  OS/Arch:          linux/arm64\n  Experimental:     false\n containerd:\n  Version:          v2.2.1\n  GitCommit:        dea7da592f5d1d2b7755e3a161be07f43fad8f75\n runc:\n  Version:          1.3.4\n  GitCommit:        v1.3.4-0-gd6d73eb8\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n\n\nClient:\n Version:           27.5.1\n API version:       1.47\n Go version:        go1.22.11\n Git commit:        9f9e405\n Built:             Wed Jan 22 13:40:02 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          27.5.1\n  API version:      1.47 (minimum version 1.24)\n  Go version:       go1.22.11\n  Git commit:       4c9b3b0\n  Built:            Wed Jan 22 13:41:24 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v1.7.25\n  GitCommit:        bcc810d6b9066471b0b6fa75f557a15a1cbf31bb\n runc:\n  Version:          1.2.4\n  GitCommit:        v1.2.4-0-g6c52b3f\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n\n\nAlso the same with 28.5.2\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    29.1.3\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v5.0.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 6\n  Running: 6\n  Paused: 0\n  Stopped: 0\n Images: 16\n Server Version: 29.1.3\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: dea7da592f5d1d2b7755e3a161be07f43fad8f75\n runc version: v1.3.4-0-gd6d73eb8\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.12.62+rpt-rpi-2712\n Operating System: Debian GNU/Linux 13 (trixie)\n OSType: linux\n Architecture: aarch64\n CPUs: 4\n Total Memory: 3.955GiB\n Name: Pi\n ID: ba1cf3fc-fdc4-42f5-b6d5-6d85157a3aed\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  10.0.0.2\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables\n\nWARNING: No memory limit support\nWARNING: No swap limit support\n\nClient:\n Version:    27.5.1\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.20.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n\nServer:\n Containers: 69\n  Running: 28\n  Paused: 0\n  Stopped: 41\n Images: 64\n Server Version: 27.5.1\n Storage Driver: overlay2\n  Backing Filesystem: btrfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: cgroupfs\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: bcc810d6b9066471b0b6fa75f557a15a1cbf31bb\n runc version: v1.2.4-0-g6c52b3f\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.12.54-Unraid\n Operating System: Unraid OS 7.2 x86_64\n OSType: linux\n Architecture: x86_64\n CPUs: 8\n Total Memory: 125.6GiB\n Name: Magnon-Box\n ID: a971f14f-065a-4f06-a886-989147fa3139\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Username: mainfrezzer\n Experimental: false\n Insecure Registries:\n  127.0.0.0/8\n Live Restore Enabled: false\n Product License: Community Engine\n\nWARNING: No swap limit support\n\n\nAlso the same with 28.5.2\n```\n\n### Additional Info\n\nHere ive a video of it happening with something that has a webinterface\n\nhttps://youtu.be/de4cnZTvDNg",
      "solution": "To add some additional observations, while the aforementioned workaround does seem to solve the problem on the surface, it only appears like it. \n\nIf you ping the gateway, it will drop (prohibited) after a while as well if no traffic is incoming to the container via the docker brige, so effectivly the other way around. Which i guess is fine, as if you wanna access service X on the host via ipv6, you do generate incoming traffic and the ping ensures that there is traffic from inside out. Still weird behavior and shouldnt happen\n\n---\n\nLooking at your Youtube video, it looks like the timeout happens roughly after 30 seconds which could match the default timeout for unreplied UDP conntrack entries. Another potential source of trouble is fwmarks and the iptables rules on the reverse path.\n\nThese may or may not be the root cause, but this timeout strongly suggests an issue with stateful firewalling, and any other NAT / firewall on the networking path may be the cause.\n\nThe magic fix is probably to set `PersistentKeepalive =` in your wg config file as described [here](https://www.wireguard.com/quickstart/#nat-and-firewall-traversal-persistence).\n\n---\n\nI do have that already set in all the wireguard configs.\n\nThe issue does not appear on the wireguard interface, as im hosting a lot that way on ipv6 and ipv4, its purely the route that i set via from the container -> docker bridge -> lan and the reverse. Anything that uses this route for local communication.\n\n```\nip -6 route add \"$network6\" via \"$IP6GATEWAY\" dev eth0 onlink\n```\nAnd so far, i actually didnt noticed it, as the ipv4 route doesnt exibit that behaviour. I only recently noticed that when i tried to query my backup dns and it failed until it reached the v4 addresses.\n\n\nThe only interaction is that seems to occure in correlation with wireguard is, that the wireguard \"steals\" the ipv6 traffic that would normally go that route to reach the internet and thus keeps the bidge connection \"alive\". \n\n\n\n**Edit: i did test something, i explicitly set the remote server with their ipv6 address, to force the container to connect via ipv6. The gateway still drops any traffic that goes from the container via bridge to the lan until you ping the gateway.**",
      "labels": [
        "status/0-triage",
        "kind/bug",
        "area/networking",
        "area/networking/ipv6"
      ],
      "created_at": "2026-01-08T20:36:14Z",
      "closed_at": "2026-02-01T11:08:34Z",
      "url": "https://github.com/moby/moby/issues/51833",
      "comments_count": 4
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 45760,
      "title": "Error creating overlay mount: no space left on device",
      "problem": "### Description\n\nWe detect multiple times errors on creating Pods on a Kubernetes cluster with underlying dockerd. The Pods stay in `CreateContainerError`. Dockerd logs:\r\n\r\n```\r\nJun 15 22:50:27 xx.xx.xx.xx dockerd[2543]: time=\"2023-06-15T22:50:27.524716689+02:00\" level=error msg=\"error unmounting /var/lib/docker/overlay2/ee44d26ac805f38253380b99a17bccc8b22b7ea9394de1b9158113100ad7c9e6-init/merged: invalid argument\" storage-driver=overlay2\r\nJun 15 22:50:27 xx.xx.xx.xx dockerd[2543]: time=\"2023-06-15T22:50:27.527366454+02:00\" level=error msg=\"Handler for POST /v1.41/containers/create returned error: error creating overlay mount to /var/lib/docker/overlay2/ee44d26ac805f38253380b99a17bccc8b22b7ea9394de1b9158113100ad7c9e6-init/merged: no space left on device\"\r\n```\r\n\r\nThe node have enough disk/inodes:\r\n\r\n```\r\n# df -h /var/lib/docker/\r\nFilesystem      Size  Used Avail Use% Mounted on\r\n/dev/nvme1n1p1   98G   42G   51G  46% /var/lib/docker\r\n# df -i /var/lib/docker/\r\nFilesystem      Inodes   IUsed   IFree IUse% Mounted on\r\n/dev/nvme1n1p1 6553600 1140790 5412810   18% /var/lib/docker\r\n```\r\n\r\n\n\n### Reproduce\n\nthis can not reproduced and may on the different workload on the Kubernetes cluster\n\n### Expected behavior\n\ncontainer is starting up\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           20.10.24\r\n API version:       1.41\r\n Go version:        go1.19.7\r\n Git commit:        297e128\r\n Built:             Tue Apr  4 18:20:53 2023\r\n OS/Arch:           linux/amd64\r\n Context:           default\r\n Experimental:      true\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          20.10.24\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.19.7\r\n  Git commit:       5d6db84\r\n  Built:            Tue Apr  4 18:18:40 2023\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.21\r\n  GitCommit:        3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc:\r\n  Version:          1.1.7\r\n  GitCommit:        v1.1.7-0-g860f061\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  app: Docker App (Docker Inc., v0.9.1-beta3)\r\n  buildx: Docker Buildx (Docker Inc., v0.10.4-docker)\r\n\r\nServer:\r\n Containers: 195\r\n  Running: 128\r\n  Paused: 0\r\n  Stopped: 67\r\n Images: 175\r\n Server Version: 20.10.24\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc io.containerd.runc.v2 io.containerd.runtime.v1.linux\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc version: v1.1.7-0-g860f061\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 5.15.0-1036-aws\r\n Operating System: Ubuntu 20.04.6 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 30.9GiB\r\n Name: xx-xx-xx-xx\r\n ID: SE4T:TBEX:J72P:FQ3Z:CQV3:STIN:2WBF:YIZQ:7OEX:7B6I:SWDR:YJF4\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: true\n```\n\n\n### Additional Info\n\nOS: Ubuntu 20.04\r\nKernel: 5.15.0-1036-aws\r\nKubernetes: v1.22.17\r\n\r\n```\r\n# mount | grep tmpfs | wc -l\r\n1262\r\n\r\n# cat /proc/cgroups \r\n#subsys_name\thierarchy\tnum_cgroups\tenabled\r\ncpuset\t8\t196\t1\r\ncpu\t7\t565\t1\r\ncpuacct\t7\t565\t1\r\nblkio\t10\t565\t1\r\nmemory\t5\t611\t1\r\ndevices\t6\t563\t1\r\nfreezer\t11\t197\t1\r\nnet_cls\t2\t196\t1\r\nperf_event\t13\t196\t1\r\nnet_prio\t2\t196\t1\r\nhugetlb\t4\t196\t1\r\npids\t12\t568\t1\r\nrdma\t3\t194\t1\r\nmisc\t9\t194\t1\r\n\r\n# cat /proc/meminfo \r\nMemTotal:       32399920 kB\r\nMemFree:          630240 kB\r\nMemAvailable:   19156348 kB\r\nBuffers:         2413944 kB\r\nCached:         12664164 kB\r\nSwapCached:            0 kB\r\nActive:          6716356 kB\r\nInactive:       19877648 kB\r\nActive(anon):      33112 kB\r\nInactive(anon): 11248280 kB\r\nActive(file):    6683244 kB\r\nInactive(file):  8629368 kB\r\nUnevictable:       28456 kB\r\nMlocked:           21680 kB\r\nSwapTotal:             0 kB\r\nSwapFree:              0 kB\r\nDirty:              4028 kB\r\nWriteback:             0 kB\r\nAnonPages:      10996232 kB\r\nMapped:          1072448 kB\r\nShmem:             50692 kB\r\nKReclaimable:    3675720 kB\r\nSlab:            4668652 kB\r\nSReclaimable:    3675720 kB\r\nSUnreclaim:       992932 kB\r\nKernelStack:       63264 kB\r\nPageTables:        74128 kB\r\nNFS_Unstable:          0 kB\r\nBounce:                0 kB\r\nWritebackTmp:          0 kB\r\nCommitLimit:    16199960 kB\r\nCommitted_AS:   30798168 kB\r\nVmallocTotal:   34359738367 kB\r\nVmallocUsed:      114476 kB\r\nVmallocChunk:          0 kB\r\nPercpu:           106976 kB\r\nHardwareCorrupted:     0 kB\r\nAnonHugePages:    987136 kB\r\nShmemHugePages:        0 kB\r\nShmemPmdMapped:        0 kB\r\nFileHugePages:         0 kB\r\nFilePmdMapped:         0 kB\r\nHugePages_Total:       0\r\nHugePages_Free:        0\r\nHugePages_Rsvd:        0\r\nHugePages_Surp:        0\r\nHugepagesize:       2048 kB\r\nHugetlb:               0 kB\r\nDirectMap4k:     1824676 kB\r\nDirectMap2M:    27054080 kB\r\nDirectMap1G:     4194304 kB\r\n```\r\n\r\nA workaround is to restart dockerd or reboot the server. \r\n\r\nref: https://github.com/moby/moby/blob/master/daemon/graphdriver/overlay2/overlay.go#L598\r\n",
      "solution": "This may be a duplicate of\r\n- #38995 \r\n- #43390\r\n\r\nwhich has been fixed for v24.0 by #44210. Unfortunately the fix is too big and risky to backport.\n\n---\n\nI have nothing to do with Moby but I had the very same problem with another deployment on one of the nodes.\n```\n  Normal   Created  60m (x17 over 90d)  kubelet  Created container: nova-compute\n  Warning  Failed   60m (x4 over 64m)   kubelet  Error: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error mounting \"/var/lib/kubelet/pods/c99b324d-95e5-4b19-94d2-1769f4e345ab/volumes/kubernetes.io~csi/pvc-85056813-244c-40b0-98f4-d9914831eede/mount\" to rootfs at \"/var/lib/nova/instances\": mount src=/var/lib/kubelet/pods/c99b324d-95e5-4b19-94d2-1769f4e345ab/volumes/kubernetes.io~csi/pvc-85056813-244c-40b0-98f4-d9914831eede/mount, dst=/var/lib/nova/instances, dstFd=/proc/thread-self/fd/8, flags=0x5000: no space left on device\n```\n\nAlthough number of mounts was far from max\n\n```\n% cat /proc/mounts | wc -l\n33185\n\n$ sysctl  fs.mount-max\nfs.mount-max = 100000\n```\n\nincreasing the value resolved the issue\n",
      "labels": [
        "status/0-triage",
        "kind/bug",
        "version/20.10"
      ],
      "created_at": "2023-06-15T21:22:07Z",
      "closed_at": "2023-06-30T21:18:59Z",
      "url": "https://github.com/moby/moby/issues/45760",
      "comments_count": 6
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51798,
      "title": "Encrypted overlay network between nodes on 29.1.3 and 28.x versions doesn't pass traffic",
      "problem": "### Description\n\nWith a node on 29.1.3 and another on 28.5.2 (or lower) it appears that encrypted overlay networks stop working i.e. no traffic can be sent between containers on the overlay network (DNS resolution of containers on the other node still works). If both are 29.1.3 or both are 28.5.2, traffic flows.\n\n### Reproduce\n\nOn node1\n`docker network create --driver overlay --attachable --opt encrypted encnet`\n`docker run -d --name test1 --network encnet alpine sleep 1d`\n\nOn node2\n```\ndocker run --rm --name test2 --network encnet alpine sh -c \"apk add -q iputils && ping -c3 test1\"\nPING test1 (10.0.2.2) 56(84) bytes of data.\n\n--- test1 ping statistics ---\n3 packets transmitted, 0 received, 100% packet loss, time 2066ms\n```\n### Expected behavior\n\n```\nPING test1 (10.0.2.2) 56(84) bytes of data.\n64 bytes from test1.encnet (10.0.2.2): icmp_seq=1 ttl=64 time=0.893 ms\n64 bytes from test1.encnet (10.0.2.2): icmp_seq=2 ttl=64 time=1.94 ms\n64 bytes from test1.encnet (10.0.2.2): icmp_seq=3 ttl=64 time=0.796 ms\n```\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           28.5.2\n API version:       1.51\n Go version:        go1.25.3\n Git commit:        ecc6942\n Built:             Wed Nov  5 14:43:11 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          28.5.2\n  API version:      1.51 (minimum version 1.24)\n  Go version:       go1.25.3\n  Git commit:       89c5e8f\n  Built:            Wed Nov  5 14:43:11 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v1.7.28\n  GitCommit:        b98a3aace656320842a23f4a392a33f46af97866\n runc:\n  Version:          1.3.0\n  GitCommit:        v1.3.0-0-g4ca628d1\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    28.5.2\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.29.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.3\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\n    Version:  0.6.0\n    Path:     /root/.docker/cli-plugins/docker-sbom\n\nServer:\n Containers: 37\n  Running: 18\n  Paused: 0\n  Stopped: 19\n Images: 30\n Server Version: 28.5.2\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: active\n  NodeID: 8y1hquegv1kbinh82tijyteh1\n  Is Manager: true\n  ClusterID: zfdcacibg74wivs2j429pn45l\n  Managers: 1\n  Nodes: 2\n  Default Address Pool: 10.0.0.0/8\n  SubnetSize: 24\n  Data Path Port: 4789\n  Orchestration:\n   Task History Retention Limit: 5\n  Raft:\n   Snapshot Interval: 10000\n   Number of Old Snapshots to Retain: 0\n   Heartbeat Tick: 1\n   Election Tick: 10\n  Dispatcher:\n   Heartbeat Period: 5 seconds\n  CA Configuration:\n   Expiry Duration: 3 months\n   Force Rotate: 0\n  Autolock Managers: false\n  Root Rotation In Progress: false\n  Node Address: 10.15.13.189\n  Manager Addresses:\n   10.15.13.189:2377\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: b98a3aace656320842a23f4a392a33f46af97866\n runc version: v1.3.0-0-g4ca628d1\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.8.0-1044-aws\n Operating System: Ubuntu 22.04.5 LTS\n OSType: linux\n Architecture: x86_64\n CPUs: 2\n Total Memory: 15.36GiB\n Name: testhost\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n```\n\n### Additional Info\n\nGrateful for any information or workaround to make upgrades to 29 easier.",
      "solution": "Not sure if related, but there was another report about encrypted overlay with v29.x here;\n\n- https://github.com/moby/moby/issues/51672\n\nThat one got closed as (per that reporter) it was resolved in v29.1.3 ? \ud83e\udd14 \n\n\ncc @corhere any ideas?\n\n---\n\nThanks, yes we were hopeful when we saw #51672 closed that this would be resolved as well but it's still an issue\n\n---\n\nHello, just tried 29.1.4 and 29.1.5 for completeness and they show the same sort of problem when paired with 28.5.2.\n\nIt would be great to get confirmation on whether this is something that mixing major releases can cause and if there's an expectation of compatibility or not. As above, a temporary workaround would be fine, we just need a process to get our estate upgraded.\n\nWhile I'm here, is the 28.x branch going to be supported until a particular point i.e. is there any more info than here?\nhttps://github.com/moby/moby/blob/master/project/BRANCHES-AND-TAGS.md#docker-engine-branches ",
      "labels": [
        "kind/bug",
        "area/networking",
        "status/confirmed",
        "area/networking/d/overlay",
        "version/29.1"
      ],
      "created_at": "2025-12-30T14:52:28Z",
      "closed_at": "2026-01-29T12:38:19Z",
      "url": "https://github.com/moby/moby/issues/51798",
      "comments_count": 5
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51934,
      "title": "docker load produces different image ids depening on the machine",
      "problem": "### Description\n\nLoading an image via `docker load -i image.tgz` will load the image and assign a different id based on the machine / distribution you're working on.\n\n### Reproduce\n\n## Prerequisites\n\nTwo different hosts are needed with different linux distributions (e.g. Ubuntu and Fedora).\n\n## Preparation\n\n* One one host, build an image and write it to a file via `docker save -o image.tgz`.\n* Copy the image to the other host\n\n## Reproduction\n\n* Load the image on each host via `docker load -i image.tgz`\n* Check the image IDs with `docker inspect <image-tag>` (use the reference displayed when importing)\n\n## Results\n\n### Host A - Ubuntu\n```\n[\n    {\n        \"Id\": \"sha256:a819d8a8634b47651b7ed726d1a843a9ac06805935faa26b21194936a94d90a8\",\n        \"RepoTags\": [\n            \"shatest:latest\"\n        ],\n        \"RepoDigests\": [],\n        \"Comment\": \"buildkit.dockerfile.v0\",\n        \"Created\": \"2026-01-26T16:10:28.603612623+01:00\",\n        \"Config\": {\n            \"Env\": [\n                \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n            ],\n            \"Cmd\": [\n                \"/bin/sh\"\n            ],\n            \"WorkingDir\": \"/\"\n        },\n        \"Architecture\": \"amd64\",\n        \"Os\": \"linux\",\n        \"Size\": 8442042,\n        \"GraphDriver\": {\n            \"Data\": {\n                \"LowerDir\": \"/var/lib/docker/overlay2/d4efa1a76cc3b24d2473444c04d2e7990e958dfc78d52e8273ff9c01fff5b672/diff\",\n                \"MergedDir\": \"/var/lib/docker/overlay2/3en2375l4gci45tvdi9z14l7y/merged\",\n                \"UpperDir\": \"/var/lib/docker/overlay2/3en2375l4gci45tvdi9z14l7y/diff\",\n                \"WorkDir\": \"/var/lib/docker/overlay2/3en2375l4gci45tvdi9z14l7y/work\"\n            },\n            \"Name\": \"overlay2\"\n        },\n        \"RootFS\": {\n            \"Type\": \"layers\",\n            \"Layers\": [\n                \"sha256:7bb20cf5ef67526cb843d264145241ce4dde09a337b5be1be42ba464de9a672d\",\n                \"sha256:f0a940fb8b9aed98ba345ae41a223a3a931251dbf7a4ad51a4e70dfe4ac32af6\"\n            ]\n        },\n        \"Metadata\": {\n            \"LastTagTime\": \"2026-01-26T16:10:28.617854272+01:00\"\n        }\n    }\n]\n```\n\n### Host B - Fedora\n```\n[\n    {\n        \"Id\": \"sha256:245776fba67e14312d2e4327065c340f8760a2eee3d2d58deb83f1e16f905e17\",\n        \"RepoTags\": [\n            \"shatest:latest\"\n        ],\n        \"RepoDigests\": [\n            \"shatest@sha256:245776fba67e14312d2e4327065c340f8760a2eee3d2d58deb83f1e16f905e17\"\n        ],\n        \"Comment\": \"buildkit.dockerfile.v0\",\n        \"Created\": \"2026-01-26T16:10:28.603612623+01:00\",\n        \"Config\": {\n            \"Env\": [\n                \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n            ],\n            \"Cmd\": [\n                \"/bin/sh\"\n            ],\n            \"WorkingDir\": \"/\"\n        },\n        \"Architecture\": \"amd64\",\n        \"Os\": \"linux\",\n        \"Size\": 8728438,\n        \"RootFS\": {\n            \"Type\": \"layers\",\n            \"Layers\": [\n                \"sha256:7bb20cf5ef67526cb843d264145241ce4dde09a337b5be1be42ba464de9a672d\",\n                \"sha256:f0a940fb8b9aed98ba345ae41a223a3a931251dbf7a4ad51a4e70dfe4ac32af6\"\n            ]\n        },\n        \"Metadata\": {\n            \"LastTagTime\": \"2026-01-26T15:15:16.307575307Z\"\n        },\n        \"Descriptor\": {\n            \"mediaType\": \"application/vnd.oci.image.manifest.v1+json\",\n            \"digest\": \"sha256:245776fba67e14312d2e4327065c340f8760a2eee3d2d58deb83f1e16f905e17\",\n            \"size\": 550,\n            \"annotations\": {\n                \"io.containerd.image.name\": \"docker.io/library/shatest:latest\",\n                \"org.opencontainers.image.ref.name\": \"latest\"\n            }\n        }\n    }\n]\n```\n\n### Expected behavior\n\nThe `Id` of an image that was stored and loaded should be equal with persistency across different machines (see https://github.com/moby/moby/issues/22011#issuecomment-212988893).\n\n### docker version\n\n```bash\n# Host A - Ubuntu\n\nClient: Docker Engine - Community\n Version:           29.1.5\n API version:       1.52\n Go version:        go1.25.6\n Git commit:        0e6fee6\n Built:             Fri Jan 16 12:48:47 2026\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          29.1.5\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.6\n  Git commit:       3b01d64\n  Built:            Fri Jan 16 12:48:47 2026\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v2.2.1\n  GitCommit:        dea7da592f5d1d2b7755e3a161be07f43fad8f75\n runc:\n  Version:          1.3.4\n  GitCommit:        v1.3.4-0-gd6d73eb8\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n\n\n# Host B - Fedora\n\nClient:\n Version:           29.1.4\n API version:       1.52\n Go version:        go1.25.5 X:nodwarf5\n Git commit:        1.fc43\n Built:             Fri Jan  9 00:00:00 2026\n OS/Arch:           linux/amd64\n Context:           default\n\nServer:\n Engine:\n  Version:          29.1.4\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.5 X:nodwarf5\n  Git commit:       1.fc43\n  Built:            Fri Jan  9 00:00:00 2026\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          2.1.6\n  GitCommit:        1.fc43\n runc:\n  Version:          1.4.0\n  GitCommit:        \n tini-static:\n  Version:          0.19.0\n  GitCommit:\n```\n\n### docker info\n\n```bash\n# Host A - Ubuntu\n\nClient: Docker Engine - Community\n Version:    29.1.5\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v5.0.2\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 0\n  Running: 0\n  Paused: 0\n  Stopped: 0\n Images: 45\n Server Version: 29.1.5\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: active\n  NodeID: vwryqjyufdetmcgztnhj5kqi6\n  Is Manager: true\n  ClusterID: vw023qy6t7ankwe2d0qt6l9em\n  Managers: 1\n  Nodes: 1\n  Default Address Pool: 10.0.0.0/8  \n  SubnetSize: 24\n  Data Path Port: 4789\n  Orchestration:\n   Task History Retention Limit: 5\n  Raft:\n   Snapshot Interval: 10000\n   Number of Old Snapshots to Retain: 0\n   Heartbeat Tick: 1\n   Election Tick: 10\n  Dispatcher:\n   Heartbeat Period: 5 seconds\n  CA Configuration:\n   Expiry Duration: 3 months\n   Force Rotate: 0\n  Autolock Managers: false\n  Root Rotation In Progress: false\n  Node Address: 192.168.15.38\n  Manager Addresses:\n   192.168.15.38:2377\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: dea7da592f5d1d2b7755e3a161be07f43fad8f75\n runc version: v1.3.4-0-gd6d73eb8\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.14.0-37-generic\n Operating System: Ubuntu 24.04.3 LTS\n OSType: linux\n Architecture: x86_64\n CPUs: 20\n Total Memory: 31.03GiB\n Name: gi2002071\n ID: 62fa5cfc-4281-400d-89f2-c8b7c2483e0e\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Username: alexanderkon\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables\n\n\n# Host B - Fedora\n\nClient:\n Version:    29.1.4\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  0.30.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n\nServer:\n Containers: 0\n  Running: 0\n  Paused: 0\n  Stopped: 0\n Images: 2\n Server Version: 29.1.4\n Storage Driver: overlayfs\n  driver-type: io.containerd.snapshotter.v1\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: /usr/bin/tini-static\n containerd version: 1.fc43\n runc version: \n init version: \n Security Options:\n  seccomp\n   Profile: builtin\n  selinux\n  cgroupns\n Kernel Version: 6.17.1-300.fc43.x86_64\n Operating System: Fedora Linux 43 (Server Edition)\n OSType: linux\n Architecture: x86_64\n CPUs: 2\n Total Memory: 5.774GiB\n Name: localhost.localdomain\n ID: 7471bdda-13d6-492c-80b7-dad1152266fa\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables+firewalld\n```\n\n### Additional Info\n\n# docker buildx inspect\n## Host A - Ubuntu\n```\nName:          default\nDriver:        docker\nLast Activity: 2026-01-26 15:10:27 +0000 UTC\n\nNodes:\nName:             default\nEndpoint:         default\nStatus:           running\nBuildKit version: v0.26.3\nPlatforms:        linux/amd64, linux/amd64/v2, linux/amd64/v3, linux/386\nLabels:\n org.mobyproject.buildkit.worker.moby.host-gateway-ip: 172.17.0.1\nGC Policy rule#0:\n All:            false\n Filters:        type==source.local,type==exec.cachemount,type==source.git.checkout\n Keep Duration:  48h0m0s\n Max Used Space: 6.048GiB\nGC Policy rule#1:\n All:            false\n Keep Duration:  1440h0m0s\n Reserved Space: 43.77GiB\n Max Used Space: 347.4GiB\n Min Free Space: 87.54GiB\nGC Policy rule#2:\n All:            false\n Reserved Space: 43.77GiB\n Max Used Space: 347.4GiB\n Min Free Space: 87.54GiB\nGC Policy rule#3:\n All:            true\n Reserved Space: 43.77GiB\n Max Used Space: 347.4GiB\n Min Free Space: 87.54GiB\n```\n\n## Host B - Fedora\n```\nName:   default\nDriver: docker\n\nNodes:\nName:             default\nEndpoint:         default\nStatus:           running\nBuildKit version: v0.26.3\nPlatforms:        linux/amd64, linux/amd64/v2, linux/amd64/v3, linux/arm64, linux/arm/v7, linux/arm/v6\nLabels:\n org.mobyproject.buildkit.worker.containerd.namespace: moby\n org.mobyproject.buildkit.worker.containerd.uuid:      ab25fee8-342b-4c25-b1f8-deef95bf4ffc\n org.mobyproject.buildkit.worker.executor:             containerd\n org.mobyproject.buildkit.worker.hostname:             localhost.localdomain\n org.mobyproject.buildkit.worker.moby.host-gateway-ip: 172.17.0.1\n org.mobyproject.buildkit.worker.network:              host\n org.mobyproject.buildkit.worker.selinux.enabled:      false\n org.mobyproject.buildkit.worker.snapshotter:          overlayfs\nGC Policy rule#0:\n All:            false\n Filters:        type==source.local,type==exec.cachemount,type==source.git.checkout\n Keep Duration:  48h0m0s\n Max Used Space: 488.3MiB\nGC Policy rule#1:\n All:            false\n Keep Duration:  1440h0m0s\n Reserved Space: 1.863GiB\n Max Used Space: 11.18GiB\n Min Free Space: 2.794GiB\nGC Policy rule#2:\n All:            false\n Reserved Space: 1.863GiB\n Max Used Space: 11.18GiB\n Min Free Space: 2.794GiB\nGC Policy rule#3:\n All:            true\n Reserved Space: 1.863GiB\n Max Used Space: 11.18GiB\n Min Free Space: 2.794GiB\n```\n\n# Distribution Info\n\n## Host A - Ubuntu\n```\nNo LSB modules are available.\nDistributor ID:\tUbuntu\nDescription:\tUbuntu 24.04.3 LTS\nRelease:\t24.04\nCodename:\tnoble\n```\n\n## Host B - Fedora\n```\nNAME=\"Fedora Linux\"\nVERSION=\"43 (Server Edition)\"\nRELEASE_TYPE=stable\nID=fedora\nVERSION_ID=43\nVERSION_CODENAME=\"\"\nPRETTY_NAME=\"Fedora Linux 43 (Server Edition)\"\nANSI_COLOR=\"0;38;2;60;110;180\"\nLOGO=fedora-logo-icon\nCPE_NAME=\"cpe:/o:fedoraproject:fedora:43\"\nHOME_URL=\"https://fedoraproject.org/\"\nDOCUMENTATION_URL=\"https://docs.fedoraproject.org/en-US/fedora/f43/\"\nSUPPORT_URL=\"https://ask.fedoraproject.org/\"\nBUG_REPORT_URL=\"https://bugzilla.redhat.com/\"\nREDHAT_BUGZILLA_PRODUCT=\"Fedora\"\nREDHAT_BUGZILLA_PRODUCT_VERSION=43\nREDHAT_SUPPORT_PRODUCT=\"Fedora\"\nREDHAT_SUPPORT_PRODUCT_VERSION=43\nSUPPORT_END=2026-12-02\nVARIANT=\"Server Edition\"\nVARIANT_ID=server\n```\n\n# Other Info\n\nI've reproduced the problem on the same hardware, with Ubuntu running as the host OS and fedora as a guest VM through kvm.\n\n# Dockerfile\n\nIssue should be reproducible with any dockerfile, but I've used the following:\n\n```\nFROM alpine:3\n\nRUN echo \"Hello\" > world.txt\n```",
      "solution": "> Hi, this is expected due to the different storage backends on your two hosts:\n> \n> * Host A (Ubuntu): Uses the classic graphdriver image store\n> * Host B (Fedora): Uses the containerd image store (default for new install in 29.0+)\n> \n> The old graphdriver image store was single-platform only and considered the image config ID as the identifier of the image.\n> \n> This is different with containerd image store which is multi-platform aware which considers the OCI manifest/index as the identifier, this matches the image ID present in the registry.\n\nClosing the issue because the software works as intended.\n\nThank you for your quick response, @vvoland :)",
      "labels": [
        "status/0-triage",
        "kind/bug",
        "area/images",
        "containerd-integration",
        "version/29.1"
      ],
      "created_at": "2026-01-26T15:42:07Z",
      "closed_at": "2026-01-27T19:45:34Z",
      "url": "https://github.com/moby/moby/issues/51934",
      "comments_count": 6
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51651,
      "title": "Established tcp sessions seem to hang / drop when trying to bring down container cleanly after `docker service remove`",
      "problem": "### Description\n\nI have my entrypoint script setup to trap SIGTERM and cleanly shut down the running processes/activities inside my docker container.  In 28.5, this works cleanly, but  after upgrading to 29.0.2 and 29.0.4, it seems that **established** tcp sessions into the docker container are killed, or hang until the time limit that docker starts killing processes.\n\nIf there are no established tcp sessions, the container shuts down cleanly.  \n\nFalling back to 28.5 reverted the behavior back to clean shutdown. \n\n### Reproduce\n\nUse the following compose file\n```yaml\nversion: \"3.9\"\n\nnetworks:\n  back-tier: {}\n\nservices:\n  tcptest:\n    image: ruby:3.4.5-slim\n    ports:\n      - \"7800:7800\"\n    networks:\n      back-tier: {}\n    volumes:\n      - /path/to/ruby/script:/usr/src/myapp\n    stop_grace_period: 5m\n    command: ruby /usr/src/myapp/entrypoint.rb\n```\n\nwith the following ruby script:\n```ruby\n#!/usr/bin/env ruby\nrequire 'socket'\nrequire 'securerandom'\nrequire 'time'\nrequire 'json'\n\nPORT = 7800\n\n$stdout.sync = true\n\nserver = TCPServer.new(PORT)\n$stdout.puts \"Listening on port #{PORT}...\"\n\nshutdown_requested = false\nshutdown_at = nil\n\nSignal.trap(\"TERM\") do\n  unless shutdown_requested\n    shutdown_requested = true\n    shutdown_delay = rand(120..180) # 2\u20133 minutes\n    shutdown_at = Time.now + shutdown_delay\n    $stdout.puts \"SIGTERM received, shutting down in #{shutdown_delay} seconds\"\n  end\nend\n\nloop do\n  break if shutdown_requested && Time.now >= shutdown_at\n\n  $stdout.puts \"Waiting for client...\"\n  client = server.accept\n  $stdout.puts \"Client connected\"\n\n  # Notify client if shutdown already requested\n  if shutdown_requested\n    client.write(\"[SERVER] Shutdown pending, terminating at #{shutdown_at.utc.iso8601}\\n\")\n  end\n\n  begin\n    loop do\n      break if shutdown_requested && Time.now >= shutdown_at\n\n      # Echo client input (non-blocking)\n      if IO.select([client], nil, nil, 0)\n        data = client.read_nonblock(1024)\n        client.write(data)\n      end\n\n      # Heartbeat message\n      heartbeat = {\n        timestamp: Time.now.utc.iso8601,\n        uuid: SecureRandom.uuid,\n        status: shutdown_requested ? \"PENDING_SHUTDOWN\" : \"RUNNING\",\n        shutdown_at: shutdown_requested ? shutdown_at.utc.iso8601 : nil\n      }\n\n      client.write(\"HEARTBEAT #{heartbeat.to_json}\\n\")\n\n      sleep rand(1..5)\n    end\n  rescue EOFError, IOError\n    $stdout.puts \"Client disconnected\"\n    if shutdown_requested\n      $stdout.puts \"Client work finished, shutting down\"\n      break\n    end\n  ensure\n    client.close rescue nil\n  end\nend\n\n$stdout.puts \"Server shutdown complete\"\nserver.close rescue nil\n```\n\n1. Do `docker stack up --with-registry-auth --detach=true foo compose.yml`\n2. Validate its running by running `docker service logs -f foo_tcptest --timestamps`\n3. Connect via netcat to the listening port `nc <host> 7800`\n4. Confirm you're receiving heratbeat and it's echos back anything you send it\n5. Perform `docker service rm foo_tcptest` and see that you might get 1 shutdown heartbeat, but no more, and it no longer echos back anything you send it.\n\n\n### Expected behavior\n\nAlready established TCP sessions should continue to function until past the shutdown grace period, or they're closed normally.  If the above is executed on version 28.5, You'll continue to get heartbeats until the graceful shutdown timer establishes, or until you end the tcp session, and you'll see in the service logs that it cleanly shutdown.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           29.0.0\n API version:       1.52\n Go version:        go1.25.4\n Git commit:        3d4129b\n Built:             Mon Nov 10 21:47:06 2025\n OS/Arch:           linux/arm64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          29.0.0\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.4\n  Git commit:       d105562\n  Built:            Mon Nov 10 21:47:06 2025\n  OS/Arch:          linux/arm64\n  Experimental:     false\n containerd:\n  Version:          v2.1.5\n  GitCommit:        fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc:\n  Version:          1.3.3\n  GitCommit:        v1.3.3-0-gd842d771\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    29.0.0\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.3\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 0\n  Running: 0\n  Paused: 0\n  Stopped: 0\n Images: 11\n Server Version: 29.0.0\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: active\n  NodeID: wk53ed8mz0rxbhpgsdkfogsl0\n  Is Manager: true\n  ClusterID: 9rzhyf1chpq4ti38cjkmk9g5w\n  Managers: 3\n  Nodes: 3\n  Default Address Pool: 10.0.0.0/8\n  SubnetSize: 24\n  Data Path Port: 4789\n  Orchestration:\n   Task History Retention Limit: 5\n  Raft:\n   Snapshot Interval: 10000\n   Number of Old Snapshots to Retain: 0\n   Heartbeat Tick: 1\n   Election Tick: 10\n  Dispatcher:\n   Heartbeat Period: 5 seconds\n  CA Configuration:\n   Expiry Duration: 3 months\n   Force Rotate: 0\n  Autolock Managers: false\n  Root Rotation In Progress: false\n  Node Address: 10.3.14.1\n  Manager Addresses:\n   10.3.14.1:2377\n   10.3.14.2:2377\n   10.3.14.3:2377\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc version: v1.3.3-0-gd842d771\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.12.47+rpt-rpi-2712\n Operating System: Debian GNU/Linux 12 (bookworm)\n OSType: linux\n Architecture: aarch64\n CPUs: 4\n Total Memory: 7.874GiB\n Name: hyades1\n ID: 38f01d86-78f3-4d19-88e1-d77c64c50179\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Username: vtcifer\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\n\n### Additional Info\n\n_No response_",
      "solution": "Sorry, I just realized that I grabbed the `docker info` / `docker version` output after downgrading to verify reverting resolves the issue.  I'll try to upgrade again this weekend and re-grab.\n\n---\n\nI tried the repro on a Debian 12 host, with Docker 29.1.3, and a single-node swarm ... it didn't exhibit the issue - the established connection carried on sending heartbeat messages and echoing text, and the shutdown completed when I killed that client. (Maybe the container needs to end up running on a remote node?)\n\n---\n\nJust checking in to see whether this is still being looked at. I realize priorities shift, but this regression is still impacting my ability to upgrade past 28.5\n\nPlease let me know if there\u2019s any additional information, testing, or anything that would be helpful from my side, or if there\u2019s anything I can do to assist with narrowing down the root cause (e.g. gathering logs, etc.). I\u2019m happy to help.",
      "labels": [
        "kind/bug",
        "area/networking",
        "area/swarm",
        "status/confirmed",
        "area/networking/d/overlay",
        "version/29.0"
      ],
      "created_at": "2025-12-02T21:50:00Z",
      "closed_at": "2026-01-14T20:23:24Z",
      "url": "https://github.com/moby/moby/issues/51651",
      "comments_count": 9
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51654,
      "title": "Breaking change v29.0.0 - containerd snapshotter migration fails on unrecognized graph driver",
      "problem": "### Description\n\nApologies for any inconsistent terminology, new to low level docker config.\n\nNB the code links might not work unless the `daemon.go` file in the PR is already in the expanded state, which it isn't by default, as it's a large file \ud83e\udee0\n\nWhen requesting a `storage-driver` that is not a valid containerd snapshotter, migration is not disabled. Instead, containerd snapshotting is enabled, then immediately fails, as the driver is not a valid containerd snapshotter.\n\nWhat I think is happening, based on the PR that added this change:\n1. [[link](https://github.com/moby/moby/pull/48009/files#diff-6a5ec84e73a526676a7b1e1f5e730211f009679a8736b7e7cda6a111a986ab4cR1100)] we initially try to get the graphdriver from `DOCKER_DRIVER`/`DOCKER_GRAPHDRIVER`\n2. [[link](https://github.com/moby/moby/pull/48009/files#diff-6a5ec84e73a526676a7b1e1f5e730211f009679a8736b7e7cda6a111a986ab4cR1133-R1138)] we build a layerstore based on the requested driver\n3. [[link](https://github.com/moby/moby/pull/48009/files#diff-6a5ec84e73a526676a7b1e1f5e730211f009679a8736b7e7cda6a111a986ab4cR1187)] we use this to build the image service config\n4. Old code for setting `d.imageService`\n    1. [[link](https://github.com/moby/moby/pull/48009/files#diff-6a5ec84e73a526676a7b1e1f5e730211f009679a8736b7e7cda6a111a986ab4cL1159)] previously, this image service config was (always?) used to build the image service\n5. New code for setting `d.imageService`\n    1. [[link](https://github.com/moby/moby/pull/48009/files#diff-6a5ec84e73a526676a7b1e1f5e730211f009679a8736b7e7cda6a111a986ab4cR1210)] we don't set `d.imageService` unless we are past the migration threshold (we will not be past the threshold due to the earlier check [here](https://github.com/moby/moby/pull/48009/files#diff-6a5ec84e73a526676a7b1e1f5e730211f009679a8736b7e7cda6a111a986ab4cR1219))\n    2. [[link](https://github.com/moby/moby/pull/48009/files#diff-6a5ec84e73a526676a7b1e1f5e730211f009679a8736b7e7cda6a111a986ab4cR1264)] so then we fall through to the `d.imageService == nil` path, starting containerd snapshotter by default\n    3. [[link](https://github.com/moby/moby/pull/48009/files#diff-6a5ec84e73a526676a7b1e1f5e730211f009679a8736b7e7cda6a111a986ab4cR1295-R1297)] our graphdriver was never mapped to a valid post-migration value, but this validation doesn't check for whether we should migrate or not (see [here](https://github.com/moby/moby/pull/48009/files#diff-6a5ec84e73a526676a7b1e1f5e730211f009679a8736b7e7cda6a111a986ab4cR1219)), so it just fails\n\nI *think* that the code needs to also check for migration status before enabling containerd snapshotting ([this line](https://github.com/moby/moby/pull/48009/files#diff-6a5ec84e73a526676a7b1e1f5e730211f009679a8736b7e7cda6a111a986ab4cR1219)).\n\n\n\n### Reproduce\n\n\n```sh\n# set storage driver to fuse-overlayfs\n[my_user@host ~]$ cat ~/.config/docker/daemon.json\n{\n  \"storage-driver\": \"fuse-overlayfs\"\n}\n\n# ensure fuse-overlayfs is installed\n[my_user@host ~]$ yum info fuse-overlayfs\nLast metadata expiration check: 0:00:07 ago on Wed Dec  3 11:42:45 2025.\nInstalled Packages\nName         : fuse-overlayfs\nVersion      : 1.14\nRelease      : 1.el9\nArchitecture : x86_64\nSize         : 135 k\nSource       : fuse-overlayfs-1.14-1.el9.src.rpm\nRepository   : @System\nFrom repo    : appstream\nSummary      : FUSE overlay+shiftfs implementation for rootless containers\nURL          : https://github.com/containers/fuse-overlayfs\nLicense      : GPLv3+\nDescription  : FUSE overlay+shiftfs implementation for rootless containers.\n\n# ensure docker v29.0.0\n[my_user@host ~]$ XDG_RUNTIME_DIR=/run/user/$(id -u) dockerd-rootless.sh --version\n(...)\nDocker version 29.0.0, build d105562\n\n# no containers/images exist\n[my_user@host ~]$ docker images\nIMAGE   ID             DISK USAGE   CONTENT SIZE   EXTRA\n[my_user@host ~]$ docker ps --all\nCONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n```\n\n```sh\n# start rootless docker\n[my_user@host ~]$ XDG_RUNTIME_DIR=/run/user/$(id -u) dockerd-rootless.sh\n(...)\nINFO[2025-12-03T11:34:59.831473404+01:00] Starting daemon with containerd snapshotter integration enabled\nINFO[2025-12-03T11:34:59.833928108+01:00] stopping healthcheck following graceful shutdown  module=libcontainerd\nINFO[2025-12-03T11:34:59.833969474+01:00] stopping event stream following graceful shutdown  error=\"context canceled\" module=libcontainerd namespace=plugins.moby\nfailed to start daemon: configured driver \"fuse-overlayfs\" not available: unavailable\n[rootlesskit:child ] error: command [/bin/dockerd-rootless.sh] exited: exit status 1\n[rootlesskit:parent] error: child exited: exit status 1\n```\n\n### Expected behavior\n\nExpected behaviour is that no migration happens if the requested storage driver is not a known compatible containerd snapshotter.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           29.0.0\n API version:       1.52\n Go version:        go1.25.4\n Git commit:        3d4129b\n Built:             Mon Nov 10 21:49:09 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          29.0.0\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.4\n  Git commit:       d105562\n  Built:            Mon Nov 10 21:45:55 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v2.1.5\n  GitCommit:        fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc:\n  Version:          1.3.3\n  GitCommit:        v1.3.3-0-gd842d771\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    29.0.0\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.29.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.3\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 0\n  Running: 0\n  Paused: 0\n  Stopped: 0\n Images: 0\n Server Version: 29.0.0\n Storage Driver: overlayfs\n  driver-type: io.containerd.snapshotter.v1\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: runc io.containerd.runc.v2\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc version: v1.3.3-0-gd842d771\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 5.14.0-503.11.1.el9_5.x86_64\n Operating System: AlmaLinux 9.5 (Teal Serval)\n OSType: linux\n Architecture: x86_64\n CPUs: 8\n Total Memory: 62.27GiB\n Name: opamuxvm1828.optiver.com\n ID: 99cb49c7-fdd0-476d-bc2f-9761425744f2\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n HTTP Proxy: http://http-proxy.common.optiver.com:3128\n HTTPS Proxy: http://http-proxy.common.optiver.com:3128\n No Proxy: optiver.com,optiver.us,git,chigit,trd-chigit,chijenkins\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\n\n### Additional Info\n\n_No response_",
      "solution": "I assume that this is fixed, let me know if I should reopen the issue.",
      "labels": [
        "area/storage",
        "status/0-triage",
        "status/more-info-needed",
        "kind/bug",
        "containerd-integration",
        "version/29.0"
      ],
      "created_at": "2025-12-03T10:58:21Z",
      "closed_at": "2026-01-27T13:00:17Z",
      "url": "https://github.com/moby/moby/issues/51654",
      "comments_count": 3
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51890,
      "title": "docker network inspect ipam config with empty IPRange",
      "problem": "### Description\n\nIn newer versions of docker (I believe >= 29.0.0), `docker network inspect NETWORK` will contain empty values in the IPAM configuration, e.g.\n\n```\n            \"IPAM\": {\n                \"Driver\": \"default\",\n                \"Options\": null,\n                \"Config\": [\n                    {\n                        \"Subnet\": \"172.18.0.0/16\",\n                        \"IPRange\": \"\",\n                        \"Gateway\": \"172.18.0.1\"\n                    }\n                ]\n            },\n```\n\nEarlier versions would not have `\"IPRange\": \"\"`. Looking at the definition, `omitempty` is specified: https://github.com/moby/moby/tree/d76e0f4df032357add6ab5fda488a715841d1540/api/types/network/ipam.go. I think this was introduced with https://github.com/moby/moby/pull/50956. I presume the same bug exists for subnet and gateway.\n\nPerhaps the issue is that these new types are not recognized as empty by the json library even if they are empty. It could also be that they [are always assigned](https://github.com/moby/moby/tree/d76e0f4df032357add6ab5fda488a715841d1540/daemon/libnetwork/network.go#L138-L142). I am not familiar with go, so this is just guesswork.\n\nThis is affecting configuration management: https://github.com/saltstack/salt/issues/68518\n\n### Reproduce\n\ndocker network inspect\n\n### Expected behavior\n\nomitempty\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n     Version:           29.1.4\n     API version:       1.52\n     Go version:        go1.25.5\n     Git commit:        0e6fee6\n     Built:             Thu Jan  8 19:57:04 2026\n     OS/Arch:           linux/amd64\n     Context:           default\n    \n    Server: Docker Engine - Community\n     Engine:\n      Version:          29.1.4\n      API version:      1.52 (minimum version 1.44)\n      Go version:       go1.25.5\n      Git commit:       08440b6\n      Built:            Thu Jan  8 19:57:04 2026\n      OS/Arch:          linux/amd64\n      Experimental:     false\n     containerd:\n      Version:          v2.2.1\n      GitCommit:        dea7da592f5d1d2b7755e3a161be07f43fad8f75\n     runc:\n      Version:          1.3.4\n      GitCommit:        v1.3.4-0-gd6d73eb8\n     docker-init:\n      Version:          0.19.0\n      GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n     Version:    29.1.4\n     Context:    default\n     Debug Mode: false\n     Plugins:\n      buildx: Docker Buildx (Docker Inc.)\n        Version:  v0.30.1\n        Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n      compose: Docker Compose (Docker Inc.)\n        Version:  v5.0.1\n        Path:     /usr/libexec/docker/cli-plugins/docker-compose\n    \n    Server:\n     Containers: 50\n      Running: 28\n      Paused: 0\n      Stopped: 22\n     Images: 29\n     Server Version: 29.1.4\n     Storage Driver: overlay2\n      Backing Filesystem: extfs\n      Supports d_type: true\n      Using metacopy: false\n      Native Overlay Diff: true\n      userxattr: false\n     Logging Driver: loki\n     Cgroup Driver: systemd\n     Cgroup Version: 2\n     Plugins:\n      Volume: local\n      Network: bridge host ipvlan macvlan null overlay\n      Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n     CDI spec directories:\n      /etc/cdi\n      /var/run/cdi\n     Swarm: inactive\n     Runtimes: io.containerd.runc.v2 runc\n     Default Runtime: runc\n     Init Binary: docker-init\n     containerd version: dea7da592f5d1d2b7755e3a161be07f43fad8f75\n     runc version: v1.3.4-0-gd6d73eb8\n     init version: de40ad0\n     Security Options:\n      apparmor\n      seccomp\n       Profile: builtin\n      cgroupns\n     Kernel Version: 6.8.0-79-generic\n     Operating System: Ubuntu 24.04.3 LTS\n     OSType: linux\n     Architecture: x86_64\n     CPUs: 2\n     Total Memory: 15.62GiB\n     Name: os2mot01\n     ID: UJUW:T3E7:GTAN:SRDC:WT4N:2BFE:7V6R:D2UM:WLKB:UCYT:3XES:333U\n     Docker Root Dir: /var/lib/docker\n     Debug Mode: false\n     Experimental: false\n     Insecure Registries:\n      ::1/128\n      127.0.0.0/8\n     Registry Mirrors:\n      https://mirror.gcr.io/\n     Live Restore Enabled: false\n     Firewall Backend: iptables\n```\n\n### Additional Info\n\n_No response_",
      "solution": "@thaJeztah Hi, sorry to tag you, I've recently started contributing to Docker, specifically MCP Gateway and Cagent. Can I solve the problems from here too?  Anyway, sorry again and thank you  :)\n\n---\n\n> but currently the generator we use (go-swagger) doesn't support that out of the box, so we'd have to look if we can work around that (e.g. through a custom template), or see if we can make a contribution in upstream \ud83e\udd14\n\nSo .. looks like we're \"lucky\" - most of these fields are not (yet) generated, so we can update them to use `omitzero` without requiring changes in go-swagger (or templates).\n\n> Can I solve the problems from here too? Anyway, sorry again and thank you :)\n\nSorry, I saw your post too late, and wanted to look if we could still fix this for the next release, so I opened a pull request;\n\n- https://github.com/moby/moby/pull/51932\n",
      "labels": [
        "status/0-triage",
        "kind/bug"
      ],
      "created_at": "2026-01-22T13:22:46Z",
      "closed_at": "2026-01-26T16:52:29Z",
      "url": "https://github.com/moby/moby/issues/51890",
      "comments_count": 4
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 49513,
      "title": "Intermittent \"unexpected EOF\" while downloading container layers when built with go 1.24",
      "problem": "### Description\n\nSince updating to 28.0.0, I'm getting a lot of \"unexpected EOF\" errors when bringing up a series of containers.\nUnfortunately this seems to happen at random, so there's no safe reproducer. The connection is fast and reliable, there is likely no massive timeout involved.\nI've seen it happening both with `docker pull` and `docker compose up` while pulling layers.\n\nSetting  `\"max-download-attempts\": 5000`  (or even more ridiculous values) in `/etc/docker/daemon.json` doesn't fix it; chances are that since the error is something other than `connection timed out` or so, docker doesn't recognize this as a download failure and therefore doesn't make another download attempt.\n\n```\n[+] Running 12/14\n \u2819 dashboard [\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff] 6.277MB / 6.277MB Pulling                                                                                                                                                       6.1s \n   \u2714 804c8aba2cc6 Already exists                                                                                                                                                                             0.0s \n   \u2714 2ae710cd8bfe Already exists                                                                                                                                                                             0.0s \n   \u2714 d462aa345367 Already exists                                                                                                                                                                             0.0s \n   \u2714 0f8b424aa0b9 Already exists                                                                                                                                                                             0.0s \n   \u2714 d557676654e5 Already exists                                                                                                                                                                             0.0s \n   \u2714 c8022d07192e Already exists                                                                                                                                                                             0.0s \n   \u2714 d858cbc252ad Already exists                                                                                                                                                                             0.0s \n   \u2714 1069fc2daed1 Already exists                                                                                                                                                                             0.0s \n   \u2714 b40161cd83fc Pull complete                                                                                                                                                                              0.5s \n   \u2714 5318d93a3a65 Pull complete                                                                                                                                                                              0.5s \n   \u2714 307c1adadb60 Pull complete                                                                                                                                                                              3.1s \n   \u2827 258b5bb46f9a Extracting      [==================================================>]  5.791MB/5.791MB                                                                                                     4.8s \n   \u2714 51fea5d3cd54 Download complete                                                                                                                                                                          2.2s \nunexpected EOF\nexit status 1\n```\n\nSimply running the same docker pull or docker compose command again \"fixes\" it most of the time (and when it doesn't, surely running it a third or fourth time does).\n\n### Reproduce\n\n1. \"docker pull\" a container, preferrable one with many layers\n2. If it succeeds, try again, at some point it will result in \"unexpected EOF\"\n\n### Expected behavior\n\nit works\n\n### docker version\n\n```bash\nClient:\n Version:           28.0.0\n API version:       1.48\n Go version:        go1.24.0\n Git commit:        \n Built:             Thu Feb 20 22:16:09 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer:\n Engine:\n  Version:          28.0.0\n  API version:      1.48 (minimum version 1.24)\n  Go version:       go1.24.0\n  Git commit:       b0f5bc3\n  Built:            Thu Feb 20 22:15:42 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          2.0.2\n  GitCommit:        .m\n runc:\n  Version:          1.20 [crun]\n  GitCommit:        1.20-1\n docker-init:\n  Version:          0.19.0\n  GitCommit:\n```\n\n### docker info\n\n```bash\nClient:\n Version:    28.0.0\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  0.21.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  2.33.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 9\n  Running: 9\n  Paused: 0\n  Stopped: 0\n Images: 10\n Server Version: 28.0.0\n Storage Driver: btrfs\n  Btrfs: \n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: .m\n runc version: 1.20-1 [crun]\n init version: \n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.13.3-server-1omv2590\n Operating System: OpenMandriva Lx 25.90 (Nickel) Cooker\n OSType: linux\n Architecture: x86_64\n CPUs: 8\n Total Memory: 15.26GiB\n Name: updatetestamd.federatedcomputer.net\n ID: 5419e1b8-b71b-4351-9772-dc2646c8cd39\n Docker Root Dir: /federated/docker\n Debug Mode: false\n Username: *******\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n```\n\n### Additional Info\n\n_No response_",
      "solution": "I am the OpenMandriva packager -- this error is happening while testing if our updated package is ready to go out to end users (FWIW the status is \"almost ready\" -- it works perfectly once the containers are installed, the seemingly random crashes while unpacking layers are the only problem).\n\nWe aren't currently applying any patches. The wrong git commit being listed is indeed an oversight (good catch!), we're usually using release tarballs (which obviously don't have a commit id that `git` would see at build time), some time ago it wouldn't build without seeing a commit, so we told it what commit it is and then forgot to remove the workaround (or keep the commit id up to date). Fixing that, but it seems to be unrelated to the problem. We also aren't applying any patches to containerd, it looks like it too thinks it is \"dirty modified\" because it's from a release tarball and therefore not seeing the commit ID. We have both crun and runc, with crun being used by default because it tends to give slightly better performance. Switching to runc doesn't affect the problem.\n\nThe `unexpected EOF` looks like it is caused by dockerd crashing while untarring a layer. I see this in the logs when the problem happens:\n\n```\nFeb 21 12:08:10 eagle.fedcom.net dockerd[19978]: time=\"2025-02-21T12:08:10.303578642Z\" level=debug msg=\"Using /usr/bin/unpigz to decompress\"\nFeb 21 12:08:10 eagle.fedcom.net dockerd[19978]: time=\"2025-02-21T12:08:10.333383654Z\" level=debug msg=\"Start untar layer\" id=8646ce26e1f6bf4c35c93d24a5d3bdc0bfdd7f631abec8bb7ab76b2b40d0a28d\nFeb 21 12:08:10 eagle.fedcom.net dockerd[19978]: time=\"2025-02-21T12:08:10.334790762Z\" level=debug msg=\"Untar time: 0.001409903s\" id=8646ce26e1f6bf4c35c93d24a5d3bdc0bfdd7f631abec8bb7ab76b2b40d0a28d\nFeb 21 12:08:10 eagle.fedcom.net dockerd[19978]: time=\"2025-02-21T12:08:10.334818234Z\" level=debug msg=\"Applied tar sha256:af5aa97ebe6ce1604747ec1e21af7136ded391bcabe4acef882e718a87c86bcc to 8646ce26e1f6bf4c35>\nFeb 21 12:08:10 eagle.fedcom.net dockerd[19978]: time=\"2025-02-21T12:08:10.343126270Z\" level=debug msg=\"Using /usr/bin/unpigz to decompress\"\nFeb 21 12:08:10 eagle.fedcom.net dockerd[19978]: time=\"2025-02-21T12:08:10.348255743Z\" level=debug msg=\"Start untar layer\" id=57e15de4c3447d30f703eab3c1c9e8ceb45d11d7a0a7784a3b5f39fb61a33ea4\nFeb 21 12:08:10 eagle.fedcom.net systemd-coredump[21873]: Process 19978 (dockerd) of user 0 terminated abnormally with signal 11/SEGV, processing...\n```\n\nUnfortunately the backtrace doesn't look very useful (at least to me), looks like a crash during memory allocation with no indicator of what is being allocated.\n\n```\n(lldb) target create \"/usr/bin/dockerd\" --core \"/var/tmp/coredump-qxcSvq\"\nCore file '/var/tmp/coredump-qxcSvq' (x86_64) was loaded.\n(lldb) bt\n* thread #1, name = 'dockerd', stop reason = signal SIGSEGV: address not mapped to object\n  * frame #0: 0x0000563381cd1c58 dockerd`runtime.mallocgcSmallNoscan + 216\n    frame #1: 0x0000563381d30fd9 dockerd`runtime.mallocgc + 185\n    frame #2: 0x0000563381d36429 dockerd`runtime.growslice + 1481\n    frame #3: 0x0000563381d2d916 dockerd`runtime.vgetrandomPutState + 86\n    frame #4: 0x0000563381d011e5 dockerd`runtime.mexit + 453\n```\n\n\n---\n\nDisabling pigz makes the log output slightly different, but doesn't make the problem go away.\n\n```\nFeb 21 14:29:59 updatetestamd.federatedcomputer.net dockerd[26190]: time=\"2025-02-21T14:29:59.405986884Z\" level=debug msg=\"Downloaded 307c1adadb60 to tempfile /federated/docker/tmp/GetImageBlob665704286\"\nFeb 21 14:29:59 updatetestamd.federatedcomputer.net dockerd[26190]: time=\"2025-02-21T14:29:59.406065041Z\" level=debug msg=\"pulling blob \\\"sha256:51fea5d3cd54704d3753cc644859696c65747cb71fb3306f974d0d61fc4d0501>\nFeb 21 14:29:59 updatetestamd.federatedcomputer.net dockerd[26190]: time=\"2025-02-21T14:29:59.413025019Z\" level=debug msg=\"Downloaded b40161cd83fc to tempfile /federated/docker/tmp/GetImageBlob243858367\"\nFeb 21 14:29:59 updatetestamd.federatedcomputer.net dockerd[26190]: time=\"2025-02-21T14:29:59.413102564Z\" level=debug msg=\"Use of pigz is disabled due to MOBY_DISABLE_PIGZ=true\"\nFeb 21 14:29:59 updatetestamd.federatedcomputer.net dockerd[26190]: time=\"2025-02-21T14:29:59.419317837Z\" level=debug msg=\"Start untar layer\" id=2a8a320384aa2b0a59d94e9e600a48abb2a7041ab3f21482edd0ad219a0de488\nFeb 21 14:29:59 updatetestamd.federatedcomputer.net dockerd[26190]: time=\"2025-02-21T14:29:59.420289546Z\" level=debug msg=\"Untar time: 0.000970238s\" id=2a8a320384aa2b0a59d94e9e600a48abb2a7041ab3f21482edd0ad219>\nFeb 21 14:29:59 updatetestamd.federatedcomputer.net dockerd[26190]: time=\"2025-02-21T14:29:59.420558190Z\" level=debug msg=\"Applied tar sha256:1a73b54f556b477f0a8b939d13c504a3b4f4db71f7a09c63afbc10acb3de5849 to>\nFeb 21 14:29:59 updatetestamd.federatedcomputer.net dockerd[26190]: time=\"2025-02-21T14:29:59.428565009Z\" level=debug msg=\"Use of pigz is disabled due to MOBY_DISABLE_PIGZ=true\"\nFeb 21 14:29:59 updatetestamd.federatedcomputer.net dockerd[26190]: time=\"2025-02-21T14:29:59.435952919Z\" level=debug msg=\"Start untar layer\" id=6845bcb32e0bc1deb98c86557c9cb08273ec81a50c9dd28185d02073b70a26e9\nFeb 21 14:29:59 updatetestamd.federatedcomputer.net dockerd[26190]: time=\"2025-02-21T14:29:59.437294602Z\" level=debug msg=\"Untar time: 0.00134532s\" id=6845bcb32e0bc1deb98c86557c9cb08273ec81a50c9dd28185d02073b7>\nFeb 21 14:29:59 updatetestamd.federatedcomputer.net dockerd[26190]: time=\"2025-02-21T14:29:59.437326573Z\" level=debug msg=\"Applied tar sha256:c048279a7d9f8e94b4c022b699ad8e8a0cb08b717b014ce4af15afaf375a6ac2 to>\nFeb 21 14:29:59 updatetestamd.federatedcomputer.net systemd-coredump[30664]: Process 26190 (dockerd) of user 0 terminated abnormally with signal 11/SEGV, processing...\n```\n\nThe backtrace remains the same too.\n\nI'm starting to suspect go 1.24 may have something to do with this -- I've started bisecting this and unless I messed something up, if I rebuild the known good 27.5.0 package in today's environment, it starts showing the same breakage. The main difference between the build environment when the known good package was built and today's build environment is a go update from 1.23.something to 1.24. Will run some more checks on that.\n\n---\n\nI have a draft PR that was used to do initial testing with go1.24; we'll likely be updating our master/main branch once we have the most urgent v28.0.0 kinks fixed, but currently our master/main (and release branch) is still on go1.23;\n\n- https://github.com/moby/moby/pull/49174",
      "labels": [
        "kind/bug",
        "version/27.0",
        "version/28.0"
      ],
      "created_at": "2025-02-21T02:36:19Z",
      "closed_at": "2026-01-26T13:29:06Z",
      "url": "https://github.com/moby/moby/issues/49513",
      "comments_count": 30
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51851,
      "title": "Hard crash docker freezed",
      "problem": "### Description\n\nIt was working fine for month on all our servers but then all servers have random crash of docker processes.\nIt started to happen on 6 servers with RTX6000 96GB vram.\nAfter it happens, the docker service is online and some commands can be executed but overall it does not longer work normally and only reboot helps. It might be overlay2 bug ?\nWhole system works fine and disk is not in RO, also smart says all disks are fine.\nCrash happens within 1-48 hours ususally.\nAlso tried different kernels 5.15 and 6.18.4\n\n\ndmesg\n\n```console\nJan 10 18:23:31 vast02 kernel: vethbf3d289: entered allmulticast mode\nJan 10 18:23:31 vast02 kernel: vethbf3d289: entered promiscuous mode\nJan 10 18:23:31 vast02 kernel: eth0: renamed from vethd85a259\nJan 10 18:23:31 vast02 kernel: docker0: port 5(vethbf3d289) entered blocking state\nJan 10 18:23:31 vast02 kernel: docker0: port 5(vethbf3d289) entered forwarding state\n.......\nJan 10 19:05:17 vast02 kernel: docker0: port 5(veth64b3032) entered disabled state\nJan 10 19:05:17 vast02 kernel: veth03e4297: renamed from eth0\nJan 10 19:05:17 vast02 kernel: docker0: port 5(veth64b3032) entered disabled state\nJan 10 19:05:17 vast02 kernel: veth64b3032 (unregistering): left allmulticast mode\nJan 10 19:05:17 vast02 kernel: veth64b3032 (unregistering): left promiscuous mode\nJan 10 19:05:17 vast02 kernel: docker0: port 5(veth64b3032) entered disabled state\nJan 10 19:15:47 vast02 kernel: INFO: task kworker/u1537:2:3067 blocked for more than 122 seconds.\nJan 10 19:15:47 vast02 kernel:       Tainted: G           O        6.18.4-pbk #1\nJan 10 19:15:47 vast02 kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nJan 10 19:15:47 vast02 kernel: task:kworker/u1537:2 state:D stack:0     pid:3067  tgid:3067  ppid:2      task_flags:0x4208060 flags:0x00080000\nJan 10 19:15:47 vast02 kernel: Workqueue: writeback wb_workfn (flush-259:0)\n\n\nJan 10 19:17:49 vast02 kernel: INFO: task kworker/u1537:2:3067 blocked for more than 245 seconds.\nJan 10 19:17:49 vast02 kernel:       Tainted: G           O        6.18.4-pbk #1\nJan 10 19:17:49 vast02 kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nJan 10 19:17:49 vast02 kernel: task:kworker/u1537:2 state:D stack:0     pid:3067  tgid:3067  ppid:2      task_flags:0x4208060 flags:0x00080000\nJan 10 19:17:49 vast02 kernel: Workqueue: writeback wb_workfn (flush-259:0)\n...\nJan 10 19:17:49 vast02 kernel: INFO: task dockerd:401845 blocked for more than 245 seconds.\nJan 10 19:17:49 vast02 kernel:       Tainted: G           O        6.18.4-pbk #1\nJan 10 19:17:49 vast02 kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nJan 10 19:17:49 vast02 kernel: task:dockerd         state:D stack:0     pid:401845 tgid:4850  ppid:1      task_flags:0x400140 flags:0x00080001\n...\nJan 10 19:17:49 vast02 kernel: INFO: task dockerd:436640 blocked for more than 122 seconds.\nJan 10 19:17:49 vast02 kernel:       Tainted: G           O        6.18.4-pbk #1\nJan 10 19:17:49 vast02 kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nJan 10 19:17:49 vast02 kernel: task:dockerd         state:D stack:0     pid:436640 tgid:4850  ppid:1      task_flags:0x400140 flags:0x00080001\nJan 10 19:17:49 vast02 kernel: Call Trace:\nJan 10 19:17:49 vast02 kernel:  <TASK>\nJan 10 19:17:49 vast02 kernel:  __schedule+0x4b5/0x1510\nJan 10 19:17:49 vast02 kernel:  ? __queue_delayed_work+0x10a/0x170\nJan 10 19:17:49 vast02 kernel:  ? mod_delayed_work_on+0xa2/0xb0\nJan 10 19:17:49 vast02 kernel:  ? srso_alias_return_thunk+0x5/0xfbef5\nJan 10 19:17:49 vast02 kernel:  ? wb_queue_work+0x8a/0x120\nJan 10 19:17:49 vast02 kernel:  schedule+0x27/0xf0\nJan 10 19:17:49 vast02 kernel:  wb_wait_for_completion+0x5f/0xa0\nJan 10 19:17:49 vast02 kernel:  ? __pfx_autoremove_wake_function+0x10/0x10\nJan 10 19:17:49 vast02 kernel:  __writeback_inodes_sb_nr+0x9b/0xd0\nJan 10 19:17:49 vast02 kernel:  writeback_inodes_sb+0x3c/0x60\nJan 10 19:17:49 vast02 kernel:  sync_filesystem+0x3d/0xb0\nJan 10 19:17:49 vast02 kernel:  ovl_sync_fs+0x5a/0x80 [overlay]\nJan 10 19:17:49 vast02 kernel:  sync_filesystem+0x8a/0xb0\nJan 10 19:17:49 vast02 kernel:  generic_shutdown_super+0x29/0x180\nJan 10 19:17:49 vast02 kernel:  kill_anon_super+0x18/0x50\nJan 10 19:17:49 vast02 kernel:  deactivate_locked_super+0x35/0xc0\nJan 10 19:17:49 vast02 kernel:  deactivate_super+0x46/0x60\nJan 10 19:17:49 vast02 kernel:  cleanup_mnt+0xc6/0x170\nJan 10 19:17:49 vast02 kernel:  __cleanup_mnt+0x12/0x20\nJan 10 19:17:49 vast02 kernel:  task_work_run+0x60/0xa0\nJan 10 19:17:49 vast02 kernel:  exit_to_user_mode_loop+0x140/0x190\nJan 10 19:17:49 vast02 kernel:  do_syscall_64+0x230/0xd00\nJan 10 19:17:49 vast02 kernel:  ? srso_alias_return_thunk+0x5/0xfbef5\nJan 10 19:17:49 vast02 kernel:  ? __do_sys_newfstatat+0x3c/0x70\nJan 10 19:17:49 vast02 kernel:  ? srso_alias_return_thunk+0x5/0xfbef5\nJan 10 19:17:49 vast02 kernel:  ? __x64_sys_newfstatat+0x1c/0x30\nJan 10 19:17:49 vast02 kernel:  ? srso_alias_return_thunk+0x5/0xfbef5\nJan 10 19:17:49 vast02 kernel:  ? x64_sys_call+0x181b/0x25a0\nJan 10 19:17:49 vast02 kernel:  ? srso_alias_return_thunk+0x5/0xfbef5\nJan 10 19:17:49 vast02 kernel:  ? do_syscall_64+0xb8/0xd00\nJan 10 19:17:49 vast02 kernel:  ? srso_alias_return_thunk+0x5/0xfbef5\nJan 10 19:17:49 vast02 kernel:  ? __x64_sys_fcntl+0xa8/0x130\nJan 10 19:17:49 vast02 kernel:  ? srso_alias_return_thunk+0x5/0xfbef5\nJan 10 19:17:49 vast02 kernel:  ? x64_sys_call+0x1992/0x25a0\nJan 10 19:17:49 vast02 kernel:  ? srso_alias_return_thunk+0x5/0xfbef5\nJan 10 19:17:49 vast02 kernel:  ? do_syscall_64+0xb8/0xd00\nJan 10 19:17:49 vast02 kernel:  ? srso_alias_return_thunk+0x5/0xfbef5\nJan 10 19:17:49 vast02 kernel:  ? x64_sys_call+0xfa0/0x25a0\nJan 10 19:17:49 vast02 kernel:  ? srso_alias_return_thunk+0x5/0xfbef5\nJan 10 19:17:49 vast02 kernel:  ? do_syscall_64+0xb8/0xd00\nJan 10 19:17:49 vast02 kernel:  ? srso_alias_return_thunk+0x5/0xfbef5\nJan 10 19:17:49 vast02 kernel:  ? putname+0x65/0x90\nJan 10 19:17:49 vast02 kernel:  ? srso_alias_return_thunk+0x5/0xfbef5\nJan 10 19:17:49 vast02 kernel:  ? do_unlinkat+0x87/0x2c0\nJan 10 19:17:49 vast02 kernel:  ? srso_alias_return_thunk+0x5/0xfbef5\nJan 10 19:17:49 vast02 kernel:  ? __x64_sys_unlinkat+0x35/0x80\nJan 10 19:17:49 vast02 kernel:  ? srso_alias_return_thunk+0x5/0xfbef5\nJan 10 19:17:49 vast02 kernel:  ? x64_sys_call+0xff6/0x25a0\nJan 10 19:17:49 vast02 kernel:  ? srso_alias_return_thunk+0x5/0xfbef5\nJan 10 19:17:49 vast02 kernel:  ? do_syscall_64+0xb8/0xd00\nJan 10 19:17:49 vast02 kernel:  ? srso_alias_return_thunk+0x5/0xfbef5\nJan 10 19:17:49 vast02 kernel:  ? __x64_sys_fchown+0x17/0x30\nJan 10 19:17:49 vast02 kernel:  ? srso_alias_return_thunk+0x5/0xfbef5\nJan 10 19:17:49 vast02 kernel:  ? x64_sys_call+0xfc1/0x25a0\nJan 10 19:17:49 vast02 kernel:  ? srso_alias_return_thunk+0x5/0xfbef5\nJan 10 19:17:49 vast02 kernel:  ? do_syscall_64+0xb8/0xd00\nJan 10 19:17:49 vast02 kernel:  ? srso_alias_return_thunk+0x5/0xfbef5\nJan 10 19:17:49 vast02 kernel:  entry_SYSCALL_64_after_hwframe+0x76/0x7e\nJan 10 19:17:49 vast02 kernel: RIP: 0033:0x5adf136d5c0e\nJan 10 19:17:49 vast02 kernel: RSP: 002b:000000c0034a9bd8 EFLAGS: 00000202 ORIG_RAX: 00000000000000a6\nJan 10 19:17:49 vast02 kernel: RAX: 0000000000000000 RBX: 000000c0078fc8e0 RCX: 00005adf136d5c0e\nJan 10 19:17:49 vast02 kernel: RDX: 0000000000000000 RSI: 0000000000000000 RDI: 000000c0078fc8e0\nJan 10 19:17:49 vast02 kernel: RBP: 000000c0034a9c18 R08: 0000000000000000 R09: 0000000000000000\nJan 10 19:17:49 vast02 kernel: R10: 0000000000000000 R11: 0000000000000202 R12: 000000c0078fc8e0\nJan 10 19:17:49 vast02 kernel: R13: 0000000000000100 R14: 000000c0058a2380 R15: ffffffffffffffff\nJan 10 19:17:49 vast02 kernel:  </TASK>\nJan 10 19:17:49 vast02 kernel: INFO: task dockerd:440128 blocked for more than 122 seconds.\nJan 10 19:17:49 vast02 kernel:       Tainted: G           O        6.18.4-pbk #1\nJan 10 19:17:49 vast02 kernel: \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nJan 10 19:17:49 vast02 kernel: task:dockerd         state:D stack:0     pid:440128 tgid:4850  ppid:1      task_flags:0x400140 flags:0x00080001\nJan 10 19:17:49 vast02 kernel: Call Trace:\nJan 10 19:17:49 vast02 kernel:  <TASK>\nJan 10 19:17:49 vast02 kernel:  __schedule+0x4b5/0x1510\nJan 10 19:17:49 vast02 kernel:  ? __queue_delayed_work+0x10a/0x170\nJan 10 19:17:49 vast02 kernel:  ? mod_delayed_work_on+0xa2/0xb0\n....\nJan 10 19:19:52 vast02 kernel:  ? do_syscall_64+0xb8/0xd00\nJan 10 19:19:52 vast02 kernel:  ? srso_alias_return_thunk+0x5/0xfbef5\nJan 10 19:19:52 vast02 kernel:  ? do_syscall_64+0xb8/0xd00\nJan 10 19:19:52 vast02 kernel:  ? srso_alias_return_thunk+0x5/0xfbef5\nJan 10 19:19:52 vast02 kernel:  entry_SYSCALL_64_after_hwframe+0x76/0x7e\nJan 10 19:19:52 vast02 kernel: RIP: 0033:0x5adf136d5c0e\nJan 10 19:19:52 vast02 kernel: RSP: 002b:000000c001d97bd8 EFLAGS: 00000202 ORIG_RAX: 00000000000000a6\nJan 10 19:19:52 vast02 kernel: RAX: 0000000000000000 RBX: 000000c00655b420 RCX: 00005adf136d5c0e\nJan 10 19:19:52 vast02 kernel: RDX: 0000000000000000 RSI: 0000000000000000 RDI: 000000c00655b420\nJan 10 19:19:52 vast02 kernel: RBP: 000000c001d97c18 R08: 0000000000000000 R09: 0000000000000000\nJan 10 19:19:52 vast02 kernel: R10: 0000000000000000 R11: 0000000000000202 R12: 000000c00655b420\nJan 10 19:19:52 vast02 kernel: R13: 0000000000000100 R14: 000000c0058568c0 R15: ffffffffffffffff\nJan 10 19:19:52 vast02 kernel:  </TASK>\nJan 10 19:19:52 vast02 kernel: Future hung task reports are suppressed, see sysctl kernel.hung_task_warnings`\n```\n\n### Reproduce\n\nThis is not certain as we are using external system vast.ai and they are managing containers.\n\n### Expected behavior\n\nNot crash system.\n\n### docker version\n\n```bash\nI have tried all versions including recent and then started to downgrading but it did not help also:\nCurrently: Tested on\n29.1.4\n29.1.3\n29.1.1\n\nroot@ble:~# docker version\nClient: Docker Engine - Community\n Version:           29.1.1\n API version:       1.52\n Go version:        go1.25.4\n Git commit:        0aedba5\n Built:             Fri Nov 28 11:33:04 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          29.1.1\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.4\n  Git commit:       9a84135\n  Built:            Fri Nov 28 11:33:04 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v2.2.1\n  GitCommit:        dea7da592f5d1d2b7755e3a161be07f43fad8f75\n runc:\n  Version:          1.3.4\n  GitCommit:        v1.3.4-0-gd6d73eb8\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    29.1.1\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v5.0.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 2\n  Running: 2\n  Paused: 0\n  Stopped: 0\n Images: 12\n Server Version: 29.1.1\n Storage Driver: overlayfs\n  driver-type: io.containerd.snapshotter.v1\n Logging Driver: json-file\n Cgroup Driver: cgroupfs\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Discovered Devices:\n  cdi: D.00057feb7a2743e283ec353664e4493dd3ac7bad79b07210760b95190eb76c93/gpu=0\n  cdi: D.00057feb7a2743e283ec353664e4493dd3ac7bad79b07210760b95190eb76c93/gpu=1\n  cdi: D.00057feb7a2743e283ec353664e4493dd3ac7bad79b07210760b95190eb76c93/gpu=2\n  cdi: D.00057feb7a2743e283ec353664e4493dd3ac7bad79b07210760b95190eb76c93/gpu=3\n  cdi: D.00057feb7a2743e283ec353664e4493dd3ac7bad79b07210760b95190eb76c93/gpu=4\n  cdi: D.00057feb7a2743e283ec353664e4493dd3ac7bad79b07210760b95190eb76c93/gpu=5\n  cdi: D.00057feb7a2743e283ec353664e4493dd3ac7bad79b07210760b95190eb76c93/gpu=6\n  cdi: D.00057feb7a2743e283ec353664e4493dd3ac7bad79b07210760b95190eb76c93/gpu=7\n  cdi: D.00057feb7a2743e283ec353664e4493dd3ac7bad79b07210760b95190eb76c93/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.00057feb7a2743e283ec353664e4493dd3ac7bad79b07210760b95190eb76c93/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.00057feb7a2743e283ec353664e4493dd3ac7bad79b07210760b95190eb76c93/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.00057feb7a2743e283ec353664e4493dd3ac7bad79b07210760b95190eb76c93/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.00057feb7a2743e283ec353664e4493dd3ac7bad79b07210760b95190eb76c93/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.00057feb7a2743e283ec353664e4493dd3ac7bad79b07210760b95190eb76c93/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.00057feb7a2743e283ec353664e4493dd3ac7bad79b07210760b95190eb76c93/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.00057feb7a2743e283ec353664e4493dd3ac7bad79b07210760b95190eb76c93/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.00057feb7a2743e283ec353664e4493dd3ac7bad79b07210760b95190eb76c93/gpu=all\n  cdi: D.001f79dedb1cb1c305faf4561b3c47e40a4af26d798e5265982db649c4c4c765/gpu=0\n  cdi: D.001f79dedb1cb1c305faf4561b3c47e40a4af26d798e5265982db649c4c4c765/gpu=1\n  cdi: D.001f79dedb1cb1c305faf4561b3c47e40a4af26d798e5265982db649c4c4c765/gpu=2\n  cdi: D.001f79dedb1cb1c305faf4561b3c47e40a4af26d798e5265982db649c4c4c765/gpu=3\n  cdi: D.001f79dedb1cb1c305faf4561b3c47e40a4af26d798e5265982db649c4c4c765/gpu=4\n  cdi: D.001f79dedb1cb1c305faf4561b3c47e40a4af26d798e5265982db649c4c4c765/gpu=5\n  cdi: D.001f79dedb1cb1c305faf4561b3c47e40a4af26d798e5265982db649c4c4c765/gpu=6\n  cdi: D.001f79dedb1cb1c305faf4561b3c47e40a4af26d798e5265982db649c4c4c765/gpu=7\n  cdi: D.001f79dedb1cb1c305faf4561b3c47e40a4af26d798e5265982db649c4c4c765/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.001f79dedb1cb1c305faf4561b3c47e40a4af26d798e5265982db649c4c4c765/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.001f79dedb1cb1c305faf4561b3c47e40a4af26d798e5265982db649c4c4c765/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.001f79dedb1cb1c305faf4561b3c47e40a4af26d798e5265982db649c4c4c765/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.001f79dedb1cb1c305faf4561b3c47e40a4af26d798e5265982db649c4c4c765/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.001f79dedb1cb1c305faf4561b3c47e40a4af26d798e5265982db649c4c4c765/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.001f79dedb1cb1c305faf4561b3c47e40a4af26d798e5265982db649c4c4c765/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.001f79dedb1cb1c305faf4561b3c47e40a4af26d798e5265982db649c4c4c765/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.001f79dedb1cb1c305faf4561b3c47e40a4af26d798e5265982db649c4c4c765/gpu=all\n  cdi: D.005d54aac423cb270a7ebe56882c74e2241c3c05e0d7afce3b2186dc309f9705/gpu=0\n  cdi: D.005d54aac423cb270a7ebe56882c74e2241c3c05e0d7afce3b2186dc309f9705/gpu=1\n  cdi: D.005d54aac423cb270a7ebe56882c74e2241c3c05e0d7afce3b2186dc309f9705/gpu=2\n  cdi: D.005d54aac423cb270a7ebe56882c74e2241c3c05e0d7afce3b2186dc309f9705/gpu=3\n  cdi: D.005d54aac423cb270a7ebe56882c74e2241c3c05e0d7afce3b2186dc309f9705/gpu=4\n  cdi: D.005d54aac423cb270a7ebe56882c74e2241c3c05e0d7afce3b2186dc309f9705/gpu=5\n  cdi: D.005d54aac423cb270a7ebe56882c74e2241c3c05e0d7afce3b2186dc309f9705/gpu=6\n  cdi: D.005d54aac423cb270a7ebe56882c74e2241c3c05e0d7afce3b2186dc309f9705/gpu=7\n  cdi: D.005d54aac423cb270a7ebe56882c74e2241c3c05e0d7afce3b2186dc309f9705/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.005d54aac423cb270a7ebe56882c74e2241c3c05e0d7afce3b2186dc309f9705/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.005d54aac423cb270a7ebe56882c74e2241c3c05e0d7afce3b2186dc309f9705/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.005d54aac423cb270a7ebe56882c74e2241c3c05e0d7afce3b2186dc309f9705/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.005d54aac423cb270a7ebe56882c74e2241c3c05e0d7afce3b2186dc309f9705/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.005d54aac423cb270a7ebe56882c74e2241c3c05e0d7afce3b2186dc309f9705/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.005d54aac423cb270a7ebe56882c74e2241c3c05e0d7afce3b2186dc309f9705/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.005d54aac423cb270a7ebe56882c74e2241c3c05e0d7afce3b2186dc309f9705/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.005d54aac423cb270a7ebe56882c74e2241c3c05e0d7afce3b2186dc309f9705/gpu=all\n  cdi: D.009d759b5c06b6fda50a5ffb23cc44c3d31201e64b67994948dabce7c89970af/gpu=0\n  cdi: D.009d759b5c06b6fda50a5ffb23cc44c3d31201e64b67994948dabce7c89970af/gpu=1\n  cdi: D.009d759b5c06b6fda50a5ffb23cc44c3d31201e64b67994948dabce7c89970af/gpu=2\n  cdi: D.009d759b5c06b6fda50a5ffb23cc44c3d31201e64b67994948dabce7c89970af/gpu=3\n  cdi: D.009d759b5c06b6fda50a5ffb23cc44c3d31201e64b67994948dabce7c89970af/gpu=4\n  cdi: D.009d759b5c06b6fda50a5ffb23cc44c3d31201e64b67994948dabce7c89970af/gpu=5\n  cdi: D.009d759b5c06b6fda50a5ffb23cc44c3d31201e64b67994948dabce7c89970af/gpu=6\n  cdi: D.009d759b5c06b6fda50a5ffb23cc44c3d31201e64b67994948dabce7c89970af/gpu=7\n  cdi: D.009d759b5c06b6fda50a5ffb23cc44c3d31201e64b67994948dabce7c89970af/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.009d759b5c06b6fda50a5ffb23cc44c3d31201e64b67994948dabce7c89970af/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.009d759b5c06b6fda50a5ffb23cc44c3d31201e64b67994948dabce7c89970af/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.009d759b5c06b6fda50a5ffb23cc44c3d31201e64b67994948dabce7c89970af/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.009d759b5c06b6fda50a5ffb23cc44c3d31201e64b67994948dabce7c89970af/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.009d759b5c06b6fda50a5ffb23cc44c3d31201e64b67994948dabce7c89970af/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.009d759b5c06b6fda50a5ffb23cc44c3d31201e64b67994948dabce7c89970af/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.009d759b5c06b6fda50a5ffb23cc44c3d31201e64b67994948dabce7c89970af/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.009d759b5c06b6fda50a5ffb23cc44c3d31201e64b67994948dabce7c89970af/gpu=all\n  cdi: D.00a4728ac19870ba68780f31c0fabb00c7ad2f113ec76213ab7419a65253e5fe/gpu=0\n  cdi: D.00a4728ac19870ba68780f31c0fabb00c7ad2f113ec76213ab7419a65253e5fe/gpu=1\n  cdi: D.00a4728ac19870ba68780f31c0fabb00c7ad2f113ec76213ab7419a65253e5fe/gpu=2\n  cdi: D.00a4728ac19870ba68780f31c0fabb00c7ad2f113ec76213ab7419a65253e5fe/gpu=3\n  cdi: D.00a4728ac19870ba68780f31c0fabb00c7ad2f113ec76213ab7419a65253e5fe/gpu=4\n  cdi: D.00a4728ac19870ba68780f31c0fabb00c7ad2f113ec76213ab7419a65253e5fe/gpu=5\n  cdi: D.00a4728ac19870ba68780f31c0fabb00c7ad2f113ec76213ab7419a65253e5fe/gpu=6\n  cdi: D.00a4728ac19870ba68780f31c0fabb00c7ad2f113ec76213ab7419a65253e5fe/gpu=7\n  cdi: D.00a4728ac19870ba68780f31c0fabb00c7ad2f113ec76213ab7419a65253e5fe/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.00a4728ac19870ba68780f31c0fabb00c7ad2f113ec76213ab7419a65253e5fe/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.00a4728ac19870ba68780f31c0fabb00c7ad2f113ec76213ab7419a65253e5fe/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.00a4728ac19870ba68780f31c0fabb00c7ad2f113ec76213ab7419a65253e5fe/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.00a4728ac19870ba68780f31c0fabb00c7ad2f113ec76213ab7419a65253e5fe/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.00a4728ac19870ba68780f31c0fabb00c7ad2f113ec76213ab7419a65253e5fe/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.00a4728ac19870ba68780f31c0fabb00c7ad2f113ec76213ab7419a65253e5fe/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.00a4728ac19870ba68780f31c0fabb00c7ad2f113ec76213ab7419a65253e5fe/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.00a4728ac19870ba68780f31c0fabb00c7ad2f113ec76213ab7419a65253e5fe/gpu=all\n  cdi: D.00bd0b8b7cfc6be1ab2045c0ca2d5a797df257f453c3905f6c18b426eac88d9f/gpu=0\n  cdi: D.00bd0b8b7cfc6be1ab2045c0ca2d5a797df257f453c3905f6c18b426eac88d9f/gpu=1\n  cdi: D.00bd0b8b7cfc6be1ab2045c0ca2d5a797df257f453c3905f6c18b426eac88d9f/gpu=2\n  cdi: D.00bd0b8b7cfc6be1ab2045c0ca2d5a797df257f453c3905f6c18b426eac88d9f/gpu=3\n  cdi: D.00bd0b8b7cfc6be1ab2045c0ca2d5a797df257f453c3905f6c18b426eac88d9f/gpu=4\n  cdi: D.00bd0b8b7cfc6be1ab2045c0ca2d5a797df257f453c3905f6c18b426eac88d9f/gpu=5\n  cdi: D.00bd0b8b7cfc6be1ab2045c0ca2d5a797df257f453c3905f6c18b426eac88d9f/gpu=6\n  cdi: D.00bd0b8b7cfc6be1ab2045c0ca2d5a797df257f453c3905f6c18b426eac88d9f/gpu=7\n  cdi: D.00bd0b8b7cfc6be1ab2045c0ca2d5a797df257f453c3905f6c18b426eac88d9f/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.00bd0b8b7cfc6be1ab2045c0ca2d5a797df257f453c3905f6c18b426eac88d9f/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.00bd0b8b7cfc6be1ab2045c0ca2d5a797df257f453c3905f6c18b426eac88d9f/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.00bd0b8b7cfc6be1ab2045c0ca2d5a797df257f453c3905f6c18b426eac88d9f/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.00bd0b8b7cfc6be1ab2045c0ca2d5a797df257f453c3905f6c18b426eac88d9f/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.00bd0b8b7cfc6be1ab2045c0ca2d5a797df257f453c3905f6c18b426eac88d9f/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.00bd0b8b7cfc6be1ab2045c0ca2d5a797df257f453c3905f6c18b426eac88d9f/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.00bd0b8b7cfc6be1ab2045c0ca2d5a797df257f453c3905f6c18b426eac88d9f/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.00bd0b8b7cfc6be1ab2045c0ca2d5a797df257f453c3905f6c18b426eac88d9f/gpu=all\n  cdi: D.01de9cdcae2e94e535449212d08c44c0e7d1a32dd0e255960ac8e64646499544/gpu=0\n  cdi: D.01de9cdcae2e94e535449212d08c44c0e7d1a32dd0e255960ac8e64646499544/gpu=1\n  cdi: D.01de9cdcae2e94e535449212d08c44c0e7d1a32dd0e255960ac8e64646499544/gpu=2\n  cdi: D.01de9cdcae2e94e535449212d08c44c0e7d1a32dd0e255960ac8e64646499544/gpu=3\n  cdi: D.01de9cdcae2e94e535449212d08c44c0e7d1a32dd0e255960ac8e64646499544/gpu=4\n  cdi: D.01de9cdcae2e94e535449212d08c44c0e7d1a32dd0e255960ac8e64646499544/gpu=5\n  cdi: D.01de9cdcae2e94e535449212d08c44c0e7d1a32dd0e255960ac8e64646499544/gpu=6\n  cdi: D.01de9cdcae2e94e535449212d08c44c0e7d1a32dd0e255960ac8e64646499544/gpu=7\n  cdi: D.01de9cdcae2e94e535449212d08c44c0e7d1a32dd0e255960ac8e64646499544/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.01de9cdcae2e94e535449212d08c44c0e7d1a32dd0e255960ac8e64646499544/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.01de9cdcae2e94e535449212d08c44c0e7d1a32dd0e255960ac8e64646499544/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.01de9cdcae2e94e535449212d08c44c0e7d1a32dd0e255960ac8e64646499544/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.01de9cdcae2e94e535449212d08c44c0e7d1a32dd0e255960ac8e64646499544/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.01de9cdcae2e94e535449212d08c44c0e7d1a32dd0e255960ac8e64646499544/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.01de9cdcae2e94e535449212d08c44c0e7d1a32dd0e255960ac8e64646499544/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.01de9cdcae2e94e535449212d08c44c0e7d1a32dd0e255960ac8e64646499544/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.01de9cdcae2e94e535449212d08c44c0e7d1a32dd0e255960ac8e64646499544/gpu=all\n  cdi: D.01f7072d7a01508364a9b13899cdbcdc4adf10f1d56a8d03343d4d0385a68bc1/gpu=0\n  cdi: D.01f7072d7a01508364a9b13899cdbcdc4adf10f1d56a8d03343d4d0385a68bc1/gpu=1\n  cdi: D.01f7072d7a01508364a9b13899cdbcdc4adf10f1d56a8d03343d4d0385a68bc1/gpu=2\n  cdi: D.01f7072d7a01508364a9b13899cdbcdc4adf10f1d56a8d03343d4d0385a68bc1/gpu=3\n  cdi: D.01f7072d7a01508364a9b13899cdbcdc4adf10f1d56a8d03343d4d0385a68bc1/gpu=4\n  cdi: D.01f7072d7a01508364a9b13899cdbcdc4adf10f1d56a8d03343d4d0385a68bc1/gpu=5\n  cdi: D.01f7072d7a01508364a9b13899cdbcdc4adf10f1d56a8d03343d4d0385a68bc1/gpu=6\n  cdi: D.01f7072d7a01508364a9b13899cdbcdc4adf10f1d56a8d03343d4d0385a68bc1/gpu=7\n  cdi: D.01f7072d7a01508364a9b13899cdbcdc4adf10f1d56a8d03343d4d0385a68bc1/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.01f7072d7a01508364a9b13899cdbcdc4adf10f1d56a8d03343d4d0385a68bc1/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.01f7072d7a01508364a9b13899cdbcdc4adf10f1d56a8d03343d4d0385a68bc1/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.01f7072d7a01508364a9b13899cdbcdc4adf10f1d56a8d03343d4d0385a68bc1/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.01f7072d7a01508364a9b13899cdbcdc4adf10f1d56a8d03343d4d0385a68bc1/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.01f7072d7a01508364a9b13899cdbcdc4adf10f1d56a8d03343d4d0385a68bc1/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.01f7072d7a01508364a9b13899cdbcdc4adf10f1d56a8d03343d4d0385a68bc1/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.01f7072d7a01508364a9b13899cdbcdc4adf10f1d56a8d03343d4d0385a68bc1/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.01f7072d7a01508364a9b13899cdbcdc4adf10f1d56a8d03343d4d0385a68bc1/gpu=all\n  cdi: D.0200afd0e790917bfa152cb8fa80299ed162d2cc2201ca8e3b9838891805297f/gpu=0\n  cdi: D.0200afd0e790917bfa152cb8fa80299ed162d2cc2201ca8e3b9838891805297f/gpu=1\n  cdi: D.0200afd0e790917bfa152cb8fa80299ed162d2cc2201ca8e3b9838891805297f/gpu=2\n  cdi: D.0200afd0e790917bfa152cb8fa80299ed162d2cc2201ca8e3b9838891805297f/gpu=3\n  cdi: D.0200afd0e790917bfa152cb8fa80299ed162d2cc2201ca8e3b9838891805297f/gpu=4\n  cdi: D.0200afd0e790917bfa152cb8fa80299ed162d2cc2201ca8e3b9838891805297f/gpu=5\n  cdi: D.0200afd0e790917bfa152cb8fa80299ed162d2cc2201ca8e3b9838891805297f/gpu=6\n  cdi: D.0200afd0e790917bfa152cb8fa80299ed162d2cc2201ca8e3b9838891805297f/gpu=7\n  cdi: D.0200afd0e790917bfa152cb8fa80299ed162d2cc2201ca8e3b9838891805297f/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.0200afd0e790917bfa152cb8fa80299ed162d2cc2201ca8e3b9838891805297f/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.0200afd0e790917bfa152cb8fa80299ed162d2cc2201ca8e3b9838891805297f/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.0200afd0e790917bfa152cb8fa80299ed162d2cc2201ca8e3b9838891805297f/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.0200afd0e790917bfa152cb8fa80299ed162d2cc2201ca8e3b9838891805297f/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.0200afd0e790917bfa152cb8fa80299ed162d2cc2201ca8e3b9838891805297f/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.0200afd0e790917bfa152cb8fa80299ed162d2cc2201ca8e3b9838891805297f/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.0200afd0e790917bfa152cb8fa80299ed162d2cc2201ca8e3b9838891805297f/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.0200afd0e790917bfa152cb8fa80299ed162d2cc2201ca8e3b9838891805297f/gpu=all\n  cdi: D.02a6914457a408a6bc44ebd51a34cd2e403f0ddf6dc151e196c634a455ee299c/gpu=0\n  cdi: D.02a6914457a408a6bc44ebd51a34cd2e403f0ddf6dc151e196c634a455ee299c/gpu=1\n  cdi: D.02a6914457a408a6bc44ebd51a34cd2e403f0ddf6dc151e196c634a455ee299c/gpu=2\n  cdi: D.02a6914457a408a6bc44ebd51a34cd2e403f0ddf6dc151e196c634a455ee299c/gpu=3\n  cdi: D.02a6914457a408a6bc44ebd51a34cd2e403f0ddf6dc151e196c634a455ee299c/gpu=4\n  cdi: D.02a6914457a408a6bc44ebd51a34cd2e403f0ddf6dc151e196c634a455ee299c/gpu=5\n  cdi: D.02a6914457a408a6bc44ebd51a34cd2e403f0ddf6dc151e196c634a455ee299c/gpu=6\n  cdi: D.02a6914457a408a6bc44ebd51a34cd2e403f0ddf6dc151e196c634a455ee299c/gpu=7\n  cdi: D.02a6914457a408a6bc44ebd51a34cd2e403f0ddf6dc151e196c634a455ee299c/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.02a6914457a408a6bc44ebd51a34cd2e403f0ddf6dc151e196c634a455ee299c/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.02a6914457a408a6bc44ebd51a34cd2e403f0ddf6dc151e196c634a455ee299c/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.02a6914457a408a6bc44ebd51a34cd2e403f0ddf6dc151e196c634a455ee299c/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.02a6914457a408a6bc44ebd51a34cd2e403f0ddf6dc151e196c634a455ee299c/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.02a6914457a408a6bc44ebd51a34cd2e403f0ddf6dc151e196c634a455ee299c/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.02a6914457a408a6bc44ebd51a34cd2e403f0ddf6dc151e196c634a455ee299c/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.02a6914457a408a6bc44ebd51a34cd2e403f0ddf6dc151e196c634a455ee299c/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.02a6914457a408a6bc44ebd51a34cd2e403f0ddf6dc151e196c634a455ee299c/gpu=all\n  cdi: D.0390792bf00aa1e8e124e715aaab20073978501cfe25e797443a17057c88c527/gpu=0\n  cdi: D.0390792bf00aa1e8e124e715aaab20073978501cfe25e797443a17057c88c527/gpu=1\n  cdi: D.0390792bf00aa1e8e124e715aaab20073978501cfe25e797443a17057c88c527/gpu=2\n  cdi: D.0390792bf00aa1e8e124e715aaab20073978501cfe25e797443a17057c88c527/gpu=3\n  cdi: D.0390792bf00aa1e8e124e715aaab20073978501cfe25e797443a17057c88c527/gpu=4\n  cdi: D.0390792bf00aa1e8e124e715aaab20073978501cfe25e797443a17057c88c527/gpu=5\n  cdi: D.0390792bf00aa1e8e124e715aaab20073978501cfe25e797443a17057c88c527/gpu=6\n  cdi: D.0390792bf00aa1e8e124e715aaab20073978501cfe25e797443a17057c88c527/gpu=7\n  cdi: D.0390792bf00aa1e8e124e715aaab20073978501cfe25e797443a17057c88c527/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.0390792bf00aa1e8e124e715aaab20073978501cfe25e797443a17057c88c527/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.0390792bf00aa1e8e124e715aaab20073978501cfe25e797443a17057c88c527/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.0390792bf00aa1e8e124e715aaab20073978501cfe25e797443a17057c88c527/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.0390792bf00aa1e8e124e715aaab20073978501cfe25e797443a17057c88c527/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.0390792bf00aa1e8e124e715aaab20073978501cfe25e797443a17057c88c527/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.0390792bf00aa1e8e124e715aaab20073978501cfe25e797443a17057c88c527/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.0390792bf00aa1e8e124e715aaab20073978501cfe25e797443a17057c88c527/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.0390792bf00aa1e8e124e715aaab20073978501cfe25e797443a17057c88c527/gpu=all\n  cdi: D.0399ee010df1adf3e8bb22eecc3a6ae3f49f76c4923dac15d9a06f4b6578f19f/gpu=0\n  cdi: D.0399ee010df1adf3e8bb22eecc3a6ae3f49f76c4923dac15d9a06f4b6578f19f/gpu=1\n  cdi: D.0399ee010df1adf3e8bb22eecc3a6ae3f49f76c4923dac15d9a06f4b6578f19f/gpu=2\n  cdi: D.0399ee010df1adf3e8bb22eecc3a6ae3f49f76c4923dac15d9a06f4b6578f19f/gpu=3\n  cdi: D.0399ee010df1adf3e8bb22eecc3a6ae3f49f76c4923dac15d9a06f4b6578f19f/gpu=4\n  cdi: D.0399ee010df1adf3e8bb22eecc3a6ae3f49f76c4923dac15d9a06f4b6578f19f/gpu=5\n  cdi: D.0399ee010df1adf3e8bb22eecc3a6ae3f49f76c4923dac15d9a06f4b6578f19f/gpu=6\n  cdi: D.0399ee010df1adf3e8bb22eecc3a6ae3f49f76c4923dac15d9a06f4b6578f19f/gpu=7\n  cdi: D.0399ee010df1adf3e8bb22eecc3a6ae3f49f76c4923dac15d9a06f4b6578f19f/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.0399ee010df1adf3e8bb22eecc3a6ae3f49f76c4923dac15d9a06f4b6578f19f/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.0399ee010df1adf3e8bb22eecc3a6ae3f49f76c4923dac15d9a06f4b6578f19f/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.0399ee010df1adf3e8bb22eecc3a6ae3f49f76c4923dac15d9a06f4b6578f19f/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.0399ee010df1adf3e8bb22eecc3a6ae3f49f76c4923dac15d9a06f4b6578f19f/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.0399ee010df1adf3e8bb22eecc3a6ae3f49f76c4923dac15d9a06f4b6578f19f/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.0399ee010df1adf3e8bb22eecc3a6ae3f49f76c4923dac15d9a06f4b6578f19f/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.0399ee010df1adf3e8bb22eecc3a6ae3f49f76c4923dac15d9a06f4b6578f19f/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.0399ee010df1adf3e8bb22eecc3a6ae3f49f76c4923dac15d9a06f4b6578f19f/gpu=all\n  cdi: D.041d0a335c02f116590cbde3abbdcdc3eeab3f78b11dd707cea4db2fcaae2212/gpu=0\n  cdi: D.041d0a335c02f116590cbde3abbdcdc3eeab3f78b11dd707cea4db2fcaae2212/gpu=1\n  cdi: D.041d0a335c02f116590cbde3abbdcdc3eeab3f78b11dd707cea4db2fcaae2212/gpu=2\n  cdi: D.041d0a335c02f116590cbde3abbdcdc3eeab3f78b11dd707cea4db2fcaae2212/gpu=3\n  cdi: D.041d0a335c02f116590cbde3abbdcdc3eeab3f78b11dd707cea4db2fcaae2212/gpu=4\n  cdi: D.041d0a335c02f116590cbde3abbdcdc3eeab3f78b11dd707cea4db2fcaae2212/gpu=5\n  cdi: D.041d0a335c02f116590cbde3abbdcdc3eeab3f78b11dd707cea4db2fcaae2212/gpu=6\n  cdi: D.041d0a335c02f116590cbde3abbdcdc3eeab3f78b11dd707cea4db2fcaae2212/gpu=7\n  cdi: D.041d0a335c02f116590cbde3abbdcdc3eeab3f78b11dd707cea4db2fcaae2212/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.041d0a335c02f116590cbde3abbdcdc3eeab3f78b11dd707cea4db2fcaae2212/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.041d0a335c02f116590cbde3abbdcdc3eeab3f78b11dd707cea4db2fcaae2212/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.041d0a335c02f116590cbde3abbdcdc3eeab3f78b11dd707cea4db2fcaae2212/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.041d0a335c02f116590cbde3abbdcdc3eeab3f78b11dd707cea4db2fcaae2212/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.041d0a335c02f116590cbde3abbdcdc3eeab3f78b11dd707cea4db2fcaae2212/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.041d0a335c02f116590cbde3abbdcdc3eeab3f78b11dd707cea4db2fcaae2212/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.041d0a335c02f116590cbde3abbdcdc3eeab3f78b11dd707cea4db2fcaae2212/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.041d0a335c02f116590cbde3abbdcdc3eeab3f78b11dd707cea4db2fcaae2212/gpu=all\n  cdi: D.04625be0f673625dd4a74432e9b2f875f61e0217597f81b5402942bec2b04c3f/gpu=0\n  cdi: D.04625be0f673625dd4a74432e9b2f875f61e0217597f81b5402942bec2b04c3f/gpu=1\n  cdi: D.04625be0f673625dd4a74432e9b2f875f61e0217597f81b5402942bec2b04c3f/gpu=2\n  cdi: D.04625be0f673625dd4a74432e9b2f875f61e0217597f81b5402942bec2b04c3f/gpu=3\n  cdi: D.04625be0f673625dd4a74432e9b2f875f61e0217597f81b5402942bec2b04c3f/gpu=4\n  cdi: D.04625be0f673625dd4a74432e9b2f875f61e0217597f81b5402942bec2b04c3f/gpu=5\n  cdi: D.04625be0f673625dd4a74432e9b2f875f61e0217597f81b5402942bec2b04c3f/gpu=6\n  cdi: D.04625be0f673625dd4a74432e9b2f875f61e0217597f81b5402942bec2b04c3f/gpu=7\n  cdi: D.04625be0f673625dd4a74432e9b2f875f61e0217597f81b5402942bec2b04c3f/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.04625be0f673625dd4a74432e9b2f875f61e0217597f81b5402942bec2b04c3f/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.04625be0f673625dd4a74432e9b2f875f61e0217597f81b5402942bec2b04c3f/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.04625be0f673625dd4a74432e9b2f875f61e0217597f81b5402942bec2b04c3f/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.04625be0f673625dd4a74432e9b2f875f61e0217597f81b5402942bec2b04c3f/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.04625be0f673625dd4a74432e9b2f875f61e0217597f81b5402942bec2b04c3f/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.04625be0f673625dd4a74432e9b2f875f61e0217597f81b5402942bec2b04c3f/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.04625be0f673625dd4a74432e9b2f875f61e0217597f81b5402942bec2b04c3f/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.04625be0f673625dd4a74432e9b2f875f61e0217597f81b5402942bec2b04c3f/gpu=all\n  cdi: D.04684d951ce784c6e085036c5c3ca8e40f660ef9f8148cf5a6b3e27faae62324/gpu=0\n  cdi: D.04684d951ce784c6e085036c5c3ca8e40f660ef9f8148cf5a6b3e27faae62324/gpu=1\n  cdi: D.04684d951ce784c6e085036c5c3ca8e40f660ef9f8148cf5a6b3e27faae62324/gpu=2\n  cdi: D.04684d951ce784c6e085036c5c3ca8e40f660ef9f8148cf5a6b3e27faae62324/gpu=3\n  cdi: D.04684d951ce784c6e085036c5c3ca8e40f660ef9f8148cf5a6b3e27faae62324/gpu=4\n  cdi: D.04684d951ce784c6e085036c5c3ca8e40f660ef9f8148cf5a6b3e27faae62324/gpu=5\n  cdi: D.04684d951ce784c6e085036c5c3ca8e40f660ef9f8148cf5a6b3e27faae62324/gpu=6\n  cdi: D.04684d951ce784c6e085036c5c3ca8e40f660ef9f8148cf5a6b3e27faae62324/gpu=7\n  cdi: D.04684d951ce784c6e085036c5c3ca8e40f660ef9f8148cf5a6b3e27faae62324/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.04684d951ce784c6e085036c5c3ca8e40f660ef9f8148cf5a6b3e27faae62324/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.04684d951ce784c6e085036c5c3ca8e40f660ef9f8148cf5a6b3e27faae62324/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.04684d951ce784c6e085036c5c3ca8e40f660ef9f8148cf5a6b3e27faae62324/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.04684d951ce784c6e085036c5c3ca8e40f660ef9f8148cf5a6b3e27faae62324/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.04684d951ce784c6e085036c5c3ca8e40f660ef9f8148cf5a6b3e27faae62324/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.04684d951ce784c6e085036c5c3ca8e40f660ef9f8148cf5a6b3e27faae62324/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.04684d951ce784c6e085036c5c3ca8e40f660ef9f8148cf5a6b3e27faae62324/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.04684d951ce784c6e085036c5c3ca8e40f660ef9f8148cf5a6b3e27faae62324/gpu=all\n  cdi: D.051b77421fb842d7aa553196f22ebf15745a7eef988e59f33d8c586244180f2d/gpu=0\n  cdi: D.051b77421fb842d7aa553196f22ebf15745a7eef988e59f33d8c586244180f2d/gpu=1\n  cdi: D.051b77421fb842d7aa553196f22ebf15745a7eef988e59f33d8c586244180f2d/gpu=2\n  cdi: D.051b77421fb842d7aa553196f22ebf15745a7eef988e59f33d8c586244180f2d/gpu=3\n  cdi: D.051b77421fb842d7aa553196f22ebf15745a7eef988e59f33d8c586244180f2d/gpu=4\n  cdi: D.051b77421fb842d7aa553196f22ebf15745a7eef988e59f33d8c586244180f2d/gpu=5\n  cdi: D.051b77421fb842d7aa553196f22ebf15745a7eef988e59f33d8c586244180f2d/gpu=6\n  cdi: D.051b77421fb842d7aa553196f22ebf15745a7eef988e59f33d8c586244180f2d/gpu=7\n  cdi: D.051b77421fb842d7aa553196f22ebf15745a7eef988e59f33d8c586244180f2d/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.051b77421fb842d7aa553196f22ebf15745a7eef988e59f33d8c586244180f2d/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.051b77421fb842d7aa553196f22ebf15745a7eef988e59f33d8c586244180f2d/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.051b77421fb842d7aa553196f22ebf15745a7eef988e59f33d8c586244180f2d/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.051b77421fb842d7aa553196f22ebf15745a7eef988e59f33d8c586244180f2d/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.051b77421fb842d7aa553196f22ebf15745a7eef988e59f33d8c586244180f2d/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.051b77421fb842d7aa553196f22ebf15745a7eef988e59f33d8c586244180f2d/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.051b77421fb842d7aa553196f22ebf15745a7eef988e59f33d8c586244180f2d/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.051b77421fb842d7aa553196f22ebf15745a7eef988e59f33d8c586244180f2d/gpu=all\n  cdi: D.053d1d73442b1405e73117be7b072704588c0bd7a609218ed5b4b0990603a010/gpu=0\n  cdi: D.053d1d73442b1405e73117be7b072704588c0bd7a609218ed5b4b0990603a010/gpu=1\n  cdi: D.053d1d73442b1405e73117be7b072704588c0bd7a609218ed5b4b0990603a010/gpu=2\n  cdi: D.053d1d73442b1405e73117be7b072704588c0bd7a609218ed5b4b0990603a010/gpu=3\n  cdi: D.053d1d73442b1405e73117be7b072704588c0bd7a609218ed5b4b0990603a010/gpu=4\n  cdi: D.053d1d73442b1405e73117be7b072704588c0bd7a609218ed5b4b0990603a010/gpu=5\n  cdi: D.053d1d73442b1405e73117be7b072704588c0bd7a609218ed5b4b0990603a010/gpu=6\n  cdi: D.053d1d73442b1405e73117be7b072704588c0bd7a609218ed5b4b0990603a010/gpu=7\n  cdi: D.053d1d73442b1405e73117be7b072704588c0bd7a609218ed5b4b0990603a010/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.053d1d73442b1405e73117be7b072704588c0bd7a609218ed5b4b0990603a010/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.053d1d73442b1405e73117be7b072704588c0bd7a609218ed5b4b0990603a010/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.053d1d73442b1405e73117be7b072704588c0bd7a609218ed5b4b0990603a010/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.053d1d73442b1405e73117be7b072704588c0bd7a609218ed5b4b0990603a010/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.053d1d73442b1405e73117be7b072704588c0bd7a609218ed5b4b0990603a010/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.053d1d73442b1405e73117be7b072704588c0bd7a609218ed5b4b0990603a010/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.053d1d73442b1405e73117be7b072704588c0bd7a609218ed5b4b0990603a010/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.053d1d73442b1405e73117be7b072704588c0bd7a609218ed5b4b0990603a010/gpu=all\n  cdi: D.058408cc71b3b526420e48434a0eb9067a7249d4f92082b7e28002194b543c33/gpu=0\n  cdi: D.058408cc71b3b526420e48434a0eb9067a7249d4f92082b7e28002194b543c33/gpu=1\n  cdi: D.058408cc71b3b526420e48434a0eb9067a7249d4f92082b7e28002194b543c33/gpu=2\n  cdi: D.058408cc71b3b526420e48434a0eb9067a7249d4f92082b7e28002194b543c33/gpu=3\n  cdi: D.058408cc71b3b526420e48434a0eb9067a7249d4f92082b7e28002194b543c33/gpu=4\n  cdi: D.058408cc71b3b526420e48434a0eb9067a7249d4f92082b7e28002194b543c33/gpu=5\n  cdi: D.058408cc71b3b526420e48434a0eb9067a7249d4f92082b7e28002194b543c33/gpu=6\n  cdi: D.058408cc71b3b526420e48434a0eb9067a7249d4f92082b7e28002194b543c33/gpu=7\n  cdi: D.058408cc71b3b526420e48434a0eb9067a7249d4f92082b7e28002194b543c33/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.058408cc71b3b526420e48434a0eb9067a7249d4f92082b7e28002194b543c33/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.058408cc71b3b526420e48434a0eb9067a7249d4f92082b7e28002194b543c33/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.058408cc71b3b526420e48434a0eb9067a7249d4f92082b7e28002194b543c33/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.058408cc71b3b526420e48434a0eb9067a7249d4f92082b7e28002194b543c33/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.058408cc71b3b526420e48434a0eb9067a7249d4f92082b7e28002194b543c33/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.058408cc71b3b526420e48434a0eb9067a7249d4f92082b7e28002194b543c33/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.058408cc71b3b526420e48434a0eb9067a7249d4f92082b7e28002194b543c33/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.058408cc71b3b526420e48434a0eb9067a7249d4f92082b7e28002194b543c33/gpu=all\n  cdi: D.05ab873318be02cc64f5236c242e52f6798ba77dcb7661cf36ae1d5f21f4f9e0/gpu=0\n  cdi: D.05ab873318be02cc64f5236c242e52f6798ba77dcb7661cf36ae1d5f21f4f9e0/gpu=1\n  cdi: D.05ab873318be02cc64f5236c242e52f6798ba77dcb7661cf36ae1d5f21f4f9e0/gpu=2\n  cdi: D.05ab873318be02cc64f5236c242e52f6798ba77dcb7661cf36ae1d5f21f4f9e0/gpu=3\n  cdi: D.05ab873318be02cc64f5236c242e52f6798ba77dcb7661cf36ae1d5f21f4f9e0/gpu=4\n  cdi: D.05ab873318be02cc64f5236c242e52f6798ba77dcb7661cf36ae1d5f21f4f9e0/gpu=5\n  cdi: D.05ab873318be02cc64f5236c242e52f6798ba77dcb7661cf36ae1d5f21f4f9e0/gpu=6\n  cdi: D.05ab873318be02cc64f5236c242e52f6798ba77dcb7661cf36ae1d5f21f4f9e0/gpu=7\n  cdi: D.05ab873318be02cc64f5236c242e52f6798ba77dcb7661cf36ae1d5f21f4f9e0/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.05ab873318be02cc64f5236c242e52f6798ba77dcb7661cf36ae1d5f21f4f9e0/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.05ab873318be02cc64f5236c242e52f6798ba77dcb7661cf36ae1d5f21f4f9e0/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.05ab873318be02cc64f5236c242e52f6798ba77dcb7661cf36ae1d5f21f4f9e0/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.05ab873318be02cc64f5236c242e52f6798ba77dcb7661cf36ae1d5f21f4f9e0/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.05ab873318be02cc64f5236c242e52f6798ba77dcb7661cf36ae1d5f21f4f9e0/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.05ab873318be02cc64f5236c242e52f6798ba77dcb7661cf36ae1d5f21f4f9e0/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.05ab873318be02cc64f5236c242e52f6798ba77dcb7661cf36ae1d5f21f4f9e0/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.05ab873318be02cc64f5236c242e52f6798ba77dcb7661cf36ae1d5f21f4f9e0/gpu=all\n  cdi: D.0682b2e6037e2a7697dbe1054d366f5c957d8267ea5a6dc0af70c353f62816f6/gpu=0\n  cdi: D.0682b2e6037e2a7697dbe1054d366f5c957d8267ea5a6dc0af70c353f62816f6/gpu=1\n  cdi: D.0682b2e6037e2a7697dbe1054d366f5c957d8267ea5a6dc0af70c353f62816f6/gpu=2\n  cdi: D.0682b2e6037e2a7697dbe1054d366f5c957d8267ea5a6dc0af70c353f62816f6/gpu=3\n  cdi: D.0682b2e6037e2a7697dbe1054d366f5c957d8267ea5a6dc0af70c353f62816f6/gpu=4\n  cdi: D.0682b2e6037e2a7697dbe1054d366f5c957d8267ea5a6dc0af70c353f62816f6/gpu=5\n  cdi: D.0682b2e6037e2a7697dbe1054d366f5c957d8267ea5a6dc0af70c353f62816f6/gpu=6\n  cdi: D.0682b2e6037e2a7697dbe1054d366f5c957d8267ea5a6dc0af70c353f62816f6/gpu=7\n  cdi: D.0682b2e6037e2a7697dbe1054d366f5c957d8267ea5a6dc0af70c353f62816f6/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.0682b2e6037e2a7697dbe1054d366f5c957d8267ea5a6dc0af70c353f62816f6/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.0682b2e6037e2a7697dbe1054d366f5c957d8267ea5a6dc0af70c353f62816f6/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.0682b2e6037e2a7697dbe1054d366f5c957d8267ea5a6dc0af70c353f62816f6/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.0682b2e6037e2a7697dbe1054d366f5c957d8267ea5a6dc0af70c353f62816f6/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.0682b2e6037e2a7697dbe1054d366f5c957d8267ea5a6dc0af70c353f62816f6/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.0682b2e6037e2a7697dbe1054d366f5c957d8267ea5a6dc0af70c353f62816f6/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.0682b2e6037e2a7697dbe1054d366f5c957d8267ea5a6dc0af70c353f62816f6/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.0682b2e6037e2a7697dbe1054d366f5c957d8267ea5a6dc0af70c353f62816f6/gpu=all\n  cdi: D.06a0e66c3bb4a926d998d3041e8063e35c08f7fd952ebf143ef3164b0150fece/gpu=0\n  cdi: D.06a0e66c3bb4a926d998d3041e8063e35c08f7fd952ebf143ef3164b0150fece/gpu=1\n  cdi: D.06a0e66c3bb4a926d998d3041e8063e35c08f7fd952ebf143ef3164b0150fece/gpu=2\n  cdi: D.06a0e66c3bb4a926d998d3041e8063e35c08f7fd952ebf143ef3164b0150fece/gpu=3\n  cdi: D.06a0e66c3bb4a926d998d3041e8063e35c08f7fd952ebf143ef3164b0150fece/gpu=4\n  cdi: D.06a0e66c3bb4a926d998d3041e8063e35c08f7fd952ebf143ef3164b0150fece/gpu=5\n  cdi: D.06a0e66c3bb4a926d998d3041e8063e35c08f7fd952ebf143ef3164b0150fece/gpu=6\n  cdi: D.06a0e66c3bb4a926d998d3041e8063e35c08f7fd952ebf143ef3164b0150fece/gpu=7\n  cdi: D.06a0e66c3bb4a926d998d3041e8063e35c08f7fd952ebf143ef3164b0150fece/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.06a0e66c3bb4a926d998d3041e8063e35c08f7fd952ebf143ef3164b0150fece/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.06a0e66c3bb4a926d998d3041e8063e35c08f7fd952ebf143ef3164b0150fece/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.06a0e66c3bb4a926d998d3041e8063e35c08f7fd952ebf143ef3164b0150fece/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.06a0e66c3bb4a926d998d3041e8063e35c08f7fd952ebf143ef3164b0150fece/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.06a0e66c3bb4a926d998d3041e8063e35c08f7fd952ebf143ef3164b0150fece/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.06a0e66c3bb4a926d998d3041e8063e35c08f7fd952ebf143ef3164b0150fece/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.06a0e66c3bb4a926d998d3041e8063e35c08f7fd952ebf143ef3164b0150fece/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.06a0e66c3bb4a926d998d3041e8063e35c08f7fd952ebf143ef3164b0150fece/gpu=all\n  cdi: D.0718821081cb638ac188c1e51399916a33fd9019d37f64e7a2d82d9cf01753f7/gpu=0\n  cdi: D.0718821081cb638ac188c1e51399916a33fd9019d37f64e7a2d82d9cf01753f7/gpu=1\n  cdi: D.0718821081cb638ac188c1e51399916a33fd9019d37f64e7a2d82d9cf01753f7/gpu=2\n  cdi: D.0718821081cb638ac188c1e51399916a33fd9019d37f64e7a2d82d9cf01753f7/gpu=3\n  cdi: D.0718821081cb638ac188c1e51399916a33fd9019d37f64e7a2d82d9cf01753f7/gpu=4\n  cdi: D.0718821081cb638ac188c1e51399916a33fd9019d37f64e7a2d82d9cf01753f7/gpu=5\n  cdi: D.0718821081cb638ac188c1e51399916a33fd9019d37f64e7a2d82d9cf01753f7/gpu=6\n  cdi: D.0718821081cb638ac188c1e51399916a33fd9019d37f64e7a2d82d9cf01753f7/gpu=7\n  cdi: D.0718821081cb638ac188c1e51399916a33fd9019d37f64e7a2d82d9cf01753f7/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.0718821081cb638ac188c1e51399916a33fd9019d37f64e7a2d82d9cf01753f7/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.0718821081cb638ac188c1e51399916a33fd9019d37f64e7a2d82d9cf01753f7/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.0718821081cb638ac188c1e51399916a33fd9019d37f64e7a2d82d9cf01753f7/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.0718821081cb638ac188c1e51399916a33fd9019d37f64e7a2d82d9cf01753f7/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.0718821081cb638ac188c1e51399916a33fd9019d37f64e7a2d82d9cf01753f7/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.0718821081cb638ac188c1e51399916a33fd9019d37f64e7a2d82d9cf01753f7/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.0718821081cb638ac188c1e51399916a33fd9019d37f64e7a2d82d9cf01753f7/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.0718821081cb638ac188c1e51399916a33fd9019d37f64e7a2d82d9cf01753f7/gpu=all\n  cdi: D.076cc7cf35db4c491087455d20353970aa10b4c8723a6f629c7ad98bfb9f9251/gpu=0\n  cdi: D.076cc7cf35db4c491087455d20353970aa10b4c8723a6f629c7ad98bfb9f9251/gpu=1\n  cdi: D.076cc7cf35db4c491087455d20353970aa10b4c8723a6f629c7ad98bfb9f9251/gpu=2\n  cdi: D.076cc7cf35db4c491087455d20353970aa10b4c8723a6f629c7ad98bfb9f9251/gpu=3\n  cdi: D.076cc7cf35db4c491087455d20353970aa10b4c8723a6f629c7ad98bfb9f9251/gpu=4\n  cdi: D.076cc7cf35db4c491087455d20353970aa10b4c8723a6f629c7ad98bfb9f9251/gpu=5\n  cdi: D.076cc7cf35db4c491087455d20353970aa10b4c8723a6f629c7ad98bfb9f9251/gpu=6\n  cdi: D.076cc7cf35db4c491087455d20353970aa10b4c8723a6f629c7ad98bfb9f9251/gpu=7\n  cdi: D.076cc7cf35db4c491087455d20353970aa10b4c8723a6f629c7ad98bfb9f9251/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.076cc7cf35db4c491087455d20353970aa10b4c8723a6f629c7ad98bfb9f9251/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.076cc7cf35db4c491087455d20353970aa10b4c8723a6f629c7ad98bfb9f9251/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.076cc7cf35db4c491087455d20353970aa10b4c8723a6f629c7ad98bfb9f9251/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.076cc7cf35db4c491087455d20353970aa10b4c8723a6f629c7ad98bfb9f9251/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.076cc7cf35db4c491087455d20353970aa10b4c8723a6f629c7ad98bfb9f9251/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.076cc7cf35db4c491087455d20353970aa10b4c8723a6f629c7ad98bfb9f9251/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.076cc7cf35db4c491087455d20353970aa10b4c8723a6f629c7ad98bfb9f9251/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.076cc7cf35db4c491087455d20353970aa10b4c8723a6f629c7ad98bfb9f9251/gpu=all\n  cdi: D.07f08b1dc762b168716a21496f1ce5d12864382afbd9fce6a07a3cfa9b882016/gpu=0\n  cdi: D.07f08b1dc762b168716a21496f1ce5d12864382afbd9fce6a07a3cfa9b882016/gpu=1\n  cdi: D.07f08b1dc762b168716a21496f1ce5d12864382afbd9fce6a07a3cfa9b882016/gpu=2\n  cdi: D.07f08b1dc762b168716a21496f1ce5d12864382afbd9fce6a07a3cfa9b882016/gpu=3\n  cdi: D.07f08b1dc762b168716a21496f1ce5d12864382afbd9fce6a07a3cfa9b882016/gpu=4\n  cdi: D.07f08b1dc762b168716a21496f1ce5d12864382afbd9fce6a07a3cfa9b882016/gpu=5\n  cdi: D.07f08b1dc762b168716a21496f1ce5d12864382afbd9fce6a07a3cfa9b882016/gpu=6\n  cdi: D.07f08b1dc762b168716a21496f1ce5d12864382afbd9fce6a07a3cfa9b882016/gpu=7\n  cdi: D.07f08b1dc762b168716a21496f1ce5d12864382afbd9fce6a07a3cfa9b882016/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.07f08b1dc762b168716a21496f1ce5d12864382afbd9fce6a07a3cfa9b882016/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.07f08b1dc762b168716a21496f1ce5d12864382afbd9fce6a07a3cfa9b882016/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.07f08b1dc762b168716a21496f1ce5d12864382afbd9fce6a07a3cfa9b882016/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.07f08b1dc762b168716a21496f1ce5d12864382afbd9fce6a07a3cfa9b882016/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.07f08b1dc762b168716a21496f1ce5d12864382afbd9fce6a07a3cfa9b882016/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.07f08b1dc762b168716a21496f1ce5d12864382afbd9fce6a07a3cfa9b882016/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.07f08b1dc762b168716a21496f1ce5d12864382afbd9fce6a07a3cfa9b882016/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.07f08b1dc762b168716a21496f1ce5d12864382afbd9fce6a07a3cfa9b882016/gpu=all\n  cdi: D.0817725a07af3cd1abe7d9d902bc14cd9d6890dda5c37ad1170ea7c41d81dfa1/gpu=0\n  cdi: D.0817725a07af3cd1abe7d9d902bc14cd9d6890dda5c37ad1170ea7c41d81dfa1/gpu=1\n  cdi: D.0817725a07af3cd1abe7d9d902bc14cd9d6890dda5c37ad1170ea7c41d81dfa1/gpu=2\n  cdi: D.0817725a07af3cd1abe7d9d902bc14cd9d6890dda5c37ad1170ea7c41d81dfa1/gpu=3\n  cdi: D.0817725a07af3cd1abe7d9d902bc14cd9d6890dda5c37ad1170ea7c41d81dfa1/gpu=4\n  cdi: D.0817725a07af3cd1abe7d9d902bc14cd9d6890dda5c37ad1170ea7c41d81dfa1/gpu=5\n  cdi: D.0817725a07af3cd1abe7d9d902bc14cd9d6890dda5c37ad1170ea7c41d81dfa1/gpu=6\n  cdi: D.0817725a07af3cd1abe7d9d902bc14cd9d6890dda5c37ad1170ea7c41d81dfa1/gpu=7\n  cdi: D.0817725a07af3cd1abe7d9d902bc14cd9d6890dda5c37ad1170ea7c41d81dfa1/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.0817725a07af3cd1abe7d9d902bc14cd9d6890dda5c37ad1170ea7c41d81dfa1/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.0817725a07af3cd1abe7d9d902bc14cd9d6890dda5c37ad1170ea7c41d81dfa1/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.0817725a07af3cd1abe7d9d902bc14cd9d6890dda5c37ad1170ea7c41d81dfa1/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.0817725a07af3cd1abe7d9d902bc14cd9d6890dda5c37ad1170ea7c41d81dfa1/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.0817725a07af3cd1abe7d9d902bc14cd9d6890dda5c37ad1170ea7c41d81dfa1/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.0817725a07af3cd1abe7d9d902bc14cd9d6890dda5c37ad1170ea7c41d81dfa1/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.0817725a07af3cd1abe7d9d902bc14cd9d6890dda5c37ad1170ea7c41d81dfa1/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.0817725a07af3cd1abe7d9d902bc14cd9d6890dda5c37ad1170ea7c41d81dfa1/gpu=all\n  cdi: D.081ca9a274e9d9df0c20e3f7c22c1e50e5b6d456be23d1f26bed8ebf434aed34/gpu=0\n  cdi: D.081ca9a274e9d9df0c20e3f7c22c1e50e5b6d456be23d1f26bed8ebf434aed34/gpu=1\n  cdi: D.081ca9a274e9d9df0c20e3f7c22c1e50e5b6d456be23d1f26bed8ebf434aed34/gpu=2\n  cdi: D.081ca9a274e9d9df0c20e3f7c22c1e50e5b6d456be23d1f26bed8ebf434aed34/gpu=3\n  cdi: D.081ca9a274e9d9df0c20e3f7c22c1e50e5b6d456be23d1f26bed8ebf434aed34/gpu=4\n  cdi: D.081ca9a274e9d9df0c20e3f7c22c1e50e5b6d456be23d1f26bed8ebf434aed34/gpu=5\n  cdi: D.081ca9a274e9d9df0c20e3f7c22c1e50e5b6d456be23d1f26bed8ebf434aed34/gpu=6\n  cdi: D.081ca9a274e9d9df0c20e3f7c22c1e50e5b6d456be23d1f26bed8ebf434aed34/gpu=7\n  cdi: D.081ca9a274e9d9df0c20e3f7c22c1e50e5b6d456be23d1f26bed8ebf434aed34/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.081ca9a274e9d9df0c20e3f7c22c1e50e5b6d456be23d1f26bed8ebf434aed34/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.081ca9a274e9d9df0c20e3f7c22c1e50e5b6d456be23d1f26bed8ebf434aed34/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.081ca9a274e9d9df0c20e3f7c22c1e50e5b6d456be23d1f26bed8ebf434aed34/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.081ca9a274e9d9df0c20e3f7c22c1e50e5b6d456be23d1f26bed8ebf434aed34/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.081ca9a274e9d9df0c20e3f7c22c1e50e5b6d456be23d1f26bed8ebf434aed34/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.081ca9a274e9d9df0c20e3f7c22c1e50e5b6d456be23d1f26bed8ebf434aed34/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.081ca9a274e9d9df0c20e3f7c22c1e50e5b6d456be23d1f26bed8ebf434aed34/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.081ca9a274e9d9df0c20e3f7c22c1e50e5b6d456be23d1f26bed8ebf434aed34/gpu=all\n  cdi: D.08203b7852a78e6d594181c6a63323d0dc18e51a4d23f540ad008ad520678e28/gpu=0\n  cdi: D.08203b7852a78e6d594181c6a63323d0dc18e51a4d23f540ad008ad520678e28/gpu=1\n  cdi: D.08203b7852a78e6d594181c6a63323d0dc18e51a4d23f540ad008ad520678e28/gpu=2\n  cdi: D.08203b7852a78e6d594181c6a63323d0dc18e51a4d23f540ad008ad520678e28/gpu=3\n  cdi: D.08203b7852a78e6d594181c6a63323d0dc18e51a4d23f540ad008ad520678e28/gpu=4\n  cdi: D.08203b7852a78e6d594181c6a63323d0dc18e51a4d23f540ad008ad520678e28/gpu=5\n  cdi: D.08203b7852a78e6d594181c6a63323d0dc18e51a4d23f540ad008ad520678e28/gpu=6\n  cdi: D.08203b7852a78e6d594181c6a63323d0dc18e51a4d23f540ad008ad520678e28/gpu=7\n  cdi: D.08203b7852a78e6d594181c6a63323d0dc18e51a4d23f540ad008ad520678e28/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.08203b7852a78e6d594181c6a63323d0dc18e51a4d23f540ad008ad520678e28/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.08203b7852a78e6d594181c6a63323d0dc18e51a4d23f540ad008ad520678e28/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.08203b7852a78e6d594181c6a63323d0dc18e51a4d23f540ad008ad520678e28/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.08203b7852a78e6d594181c6a63323d0dc18e51a4d23f540ad008ad520678e28/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.08203b7852a78e6d594181c6a63323d0dc18e51a4d23f540ad008ad520678e28/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.08203b7852a78e6d594181c6a63323d0dc18e51a4d23f540ad008ad520678e28/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.08203b7852a78e6d594181c6a63323d0dc18e51a4d23f540ad008ad520678e28/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.08203b7852a78e6d594181c6a63323d0dc18e51a4d23f540ad008ad520678e28/gpu=all\n  cdi: D.085881bdb835fb7c3546a2d0dd5a643e43ecb0b54fb9606acb766a5006b88e38/gpu=0\n  cdi: D.085881bdb835fb7c3546a2d0dd5a643e43ecb0b54fb9606acb766a5006b88e38/gpu=1\n  cdi: D.085881bdb835fb7c3546a2d0dd5a643e43ecb0b54fb9606acb766a5006b88e38/gpu=2\n  cdi: D.085881bdb835fb7c3546a2d0dd5a643e43ecb0b54fb9606acb766a5006b88e38/gpu=3\n  cdi: D.085881bdb835fb7c3546a2d0dd5a643e43ecb0b54fb9606acb766a5006b88e38/gpu=4\n  cdi: D.085881bdb835fb7c3546a2d0dd5a643e43ecb0b54fb9606acb766a5006b88e38/gpu=5\n  cdi: D.085881bdb835fb7c3546a2d0dd5a643e43ecb0b54fb9606acb766a5006b88e38/gpu=6\n  cdi: D.085881bdb835fb7c3546a2d0dd5a643e43ecb0b54fb9606acb766a5006b88e38/gpu=7\n  cdi: D.085881bdb835fb7c3546a2d0dd5a643e43ecb0b54fb9606acb766a5006b88e38/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.085881bdb835fb7c3546a2d0dd5a643e43ecb0b54fb9606acb766a5006b88e38/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.085881bdb835fb7c3546a2d0dd5a643e43ecb0b54fb9606acb766a5006b88e38/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.085881bdb835fb7c3546a2d0dd5a643e43ecb0b54fb9606acb766a5006b88e38/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.085881bdb835fb7c3546a2d0dd5a643e43ecb0b54fb9606acb766a5006b88e38/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.085881bdb835fb7c3546a2d0dd5a643e43ecb0b54fb9606acb766a5006b88e38/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.085881bdb835fb7c3546a2d0dd5a643e43ecb0b54fb9606acb766a5006b88e38/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.085881bdb835fb7c3546a2d0dd5a643e43ecb0b54fb9606acb766a5006b88e38/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.085881bdb835fb7c3546a2d0dd5a643e43ecb0b54fb9606acb766a5006b88e38/gpu=all\n  cdi: D.08c90ebc35cf7ab9d298e15dffc9e1caa401a5d64a13cefaa65f870eaaf404de/gpu=0\n  cdi: D.08c90ebc35cf7ab9d298e15dffc9e1caa401a5d64a13cefaa65f870eaaf404de/gpu=1\n  cdi: D.08c90ebc35cf7ab9d298e15dffc9e1caa401a5d64a13cefaa65f870eaaf404de/gpu=2\n  cdi: D.08c90ebc35cf7ab9d298e15dffc9e1caa401a5d64a13cefaa65f870eaaf404de/gpu=3\n  cdi: D.08c90ebc35cf7ab9d298e15dffc9e1caa401a5d64a13cefaa65f870eaaf404de/gpu=4\n  cdi: D.08c90ebc35cf7ab9d298e15dffc9e1caa401a5d64a13cefaa65f870eaaf404de/gpu=5\n  cdi: D.08c90ebc35cf7ab9d298e15dffc9e1caa401a5d64a13cefaa65f870eaaf404de/gpu=6\n  cdi: D.08c90ebc35cf7ab9d298e15dffc9e1caa401a5d64a13cefaa65f870eaaf404de/gpu=7\n  cdi: D.08c90ebc35cf7ab9d298e15dffc9e1caa401a5d64a13cefaa65f870eaaf404de/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.08c90ebc35cf7ab9d298e15dffc9e1caa401a5d64a13cefaa65f870eaaf404de/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.08c90ebc35cf7ab9d298e15dffc9e1caa401a5d64a13cefaa65f870eaaf404de/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.08c90ebc35cf7ab9d298e15dffc9e1caa401a5d64a13cefaa65f870eaaf404de/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.08c90ebc35cf7ab9d298e15dffc9e1caa401a5d64a13cefaa65f870eaaf404de/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.08c90ebc35cf7ab9d298e15dffc9e1caa401a5d64a13cefaa65f870eaaf404de/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: D.08c90ebc35cf7ab9d298e15dffc9e1caa401a5d64a13cefaa65f870eaaf404de/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: D.08c90ebc35cf7ab9d298e15dffc9e1caa401a5d64a13cefaa65f870eaaf404de/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: D.08c90ebc35cf7ab9d298e15dffc9e1caa401a5d64a13cefaa65f870eaaf404de/gpu=all\n  cdi: D.08c927310179218386494f2f967ab1a511c49b082fa838bf9d23d4c2cc253ff5/gpu=0\n  cdi: D.08c927310179218386494f2f967ab1a511c49b082fa838bf9d23d4c2cc253ff5/gpu=1\n  cdi: D.08c927310179218386494f2f967ab1a511c49b082fa838bf9d23d4c2cc253ff5/gpu=2\n  cdi: D.08c927310179218386494f2f967ab1a511c49b082fa838bf9d23d4c2cc253ff5/gpu=3\n  cdi: D.08c927310179218386494f2f967ab1a511c49b082fa838bf9d23d4c2cc253ff5/gpu=4\n  cdi: D.08c927310179218386494f2f967ab1a511c49b082fa838bf9d23d4c2cc253ff5/gpu=5\n  cdi: D.08c927310179218386494f2f967ab1a511c49b082fa838bf9d23d4c2cc253ff5/gpu=6\n  cdi: D.08c927310179218386494f2f967ab1a511c49b082fa838bf9d23d4c2cc253ff5/gpu=7\n  cdi: D.08c927310179218386494f2f967ab1a511c49b082fa838bf9d23d4c2cc253ff5/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: D.08c927310179218386494f2f967ab1a511c49b082fa838bf9d23d4c2cc253ff5/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: D.08c927310179218386494f2f967ab1a511c49b082fa838bf9d23d4c2cc253ff5/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: D.08c927310179218386494f2f967ab1a511c49b082fa838bf9d23d4c2cc253ff5/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: D.08c927310179218386494f2f967ab1a511c49b082fa838bf9d23d4c2cc253ff5/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: D.08c927310179218386494f2f967ab1a511c49b082fa838bf9d23d4c2cc253ff5/gpu=GPU-\n.\n.\n. MANY OTHER LINES\n.\n  .cdi: nvidia.com/gpu=0\n  cdi: nvidia.com/gpu=1\n  cdi: nvidia.com/gpu=2\n  cdi: nvidia.com/gpu=3\n  cdi: nvidia.com/gpu=4\n  cdi: nvidia.com/gpu=5\n  cdi: nvidia.com/gpu=6\n  cdi: nvidia.com/gpu=7\n  cdi: nvidia.com/gpu=GPU-2433fd2d-19d3-4a7a-3f3d-8cf2c2edebd4\n  cdi: nvidia.com/gpu=GPU-87fa97ab-7f12-e540-1f7b-b134280cd967\n  cdi: nvidia.com/gpu=GPU-b39e8721-119a-1ff6-40d5-a3b2d9a201ed\n  cdi: nvidia.com/gpu=GPU-b8c17823-a25f-beb5-5732-bb224dbaa9c4\n  cdi: nvidia.com/gpu=GPU-bdd677d7-5303-6bcb-af5d-b2bc00a76efd\n  cdi: nvidia.com/gpu=GPU-c5ae6bf1-edf7-7955-815c-2e758f022392\n  cdi: nvidia.com/gpu=GPU-d1e5f6de-9b0b-37ba-429a-4284a88e5626\n  cdi: nvidia.com/gpu=GPU-e9debf61-d146-3504-3fbd-d8017c594cbd\n  cdi: nvidia.com/gpu=all\n Swarm: inactive\n Runtimes: runc io.containerd.runc.v2 nvidia\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: dea7da592f5d1d2b7755e3a161be07f43fad8f75\n runc version: v1.3.4-0-gd6d73eb8\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.18.4-pbk\n Operating System: Ubuntu 22.04.5 LTS\n OSType: linux\n Architecture: x86_64\n CPUs: 384\n Total Memory: 1.476TiB\n Name: vast02\n ID: 7fcfd7ce-df7f-4f0c-aec6-1007e2f3c027\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Registry Mirrors:\n  https://registry-1.docker.io/\n  https://docker1.vast.ai/\n  https://docker2.vast.ai/\n  https://docker3.vast.ai/\n  https://docker4.vast.ai/\n  https://docker5.vast.ai/\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\n\n### Additional Info\n\n_No response_",
      "solution": "If you're not using CDI, you can disable CDI for now to see if it helps with the issue:\n\n```\n# in daemon.json\n{\n  ...\n  \"features\": {\n    \"cdi\": false\n  }\n}\n\n---\n\nVast answered me this:\nHi, please do not disable CDI sine nvidia-ctk needs it to pass GPUS to the containers. your\ncat /etc/docker/daemon.json\nshould looks like this:\n{\"exec-opts\":[\"native.cgroupdriver=cgroupfs\"],\"registry-mirrors\":[\"https://registry-1.docker.io\",\"https://docker1.vast.ai\",\"https://docker2.vast.ai\",\"https://docker3.vast.ai\",\"https://docker4.vast.ai\",\"https://docker5.vast.ai\"],\"runtimes\":{\"nvidia\":{\"path\":\"/var/lib/vastai_kaalia/latest/kaalia_docker_shim\",\"runtimeArgs\":[]}}}\n\nAnyway do you guys think that this is root of the issue or is it just strange ?\n",
      "labels": [
        "kind/bug",
        "area/cdi",
        "version/29.1"
      ],
      "created_at": "2026-01-15T11:24:17Z",
      "closed_at": "2026-01-19T13:01:41Z",
      "url": "https://github.com/moby/moby/issues/51851",
      "comments_count": 13
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51537,
      "title": "Breaking change in 29.0.0: ExposedPorts can no longer contain port ranges",
      "problem": "### Description\n\nIn 28.x and before, one entry in `ExposedPorts` (of type `nat.PortSet`, https://github.com/docker/go-connections/blob/main/nat/nat.go) was alloewd to be a range, like `9010-9050` or `9010-9050/tcp`.\n\nThat type has been replaced in #50710 with a new type `PortSet` (later moved in #51094) that does **not** allow a range:\n```go\ntype Port struct {\n\tnum   uint16\n\tproto unique.Handle[NetworkProtocol]\n}\n```\n\nThis now results in API calls to create a container with an exposed port `9010-9050` to fail with `invalid JSON: invalid port '9010-9050': invalid syntax`.\n\nSince I cannot find any mention of this, I'm filing this as a bug. Please point me to a deprecation in case this was an intended change, and not an unintended breaking change.\n\n(The CLI converts ranges for `ExposedPorts` into a list of single ports, so it's not affected by this. This \"only\" affects third-party API users.)\n\nCC @austinvazquez since you implemented the changes.\n\n### Reproduce\n\nCreate a container with the API with `ExposedPorts` set to `[\"9010-9050\"]`.\n\n### Expected behavior\n\n_No response_\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           29.0.1\n API version:       1.52\n Go version:        go1.25.4\n Git commit:        eedd969\n Built:             Fri Nov 14 16:20:36 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          29.0.1\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.4\n  Git commit:       198b5e3\n  Built:            Fri Nov 14 16:17:26 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v2.1.5\n  GitCommit:        fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc:\n  Version:          1.3.3\n  GitCommit:        v1.3.3-0-gd842d771\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    29.0.1\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.3\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 1\n  Running: 0\n  Paused: 0\n  Stopped: 1\n Images: 1\n Server Version: 29.0.1\n Storage Driver: overlayfs\n  driver-type: io.containerd.snapshotter.v1\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: runc io.containerd.runc.v2\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc version: v1.3.3-0-gd842d771\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.12.0-124.9.1.el10_1.x86_64\n Operating System: Red Hat Enterprise Linux 10.0 (Coughlan)\n OSType: linux\n Architecture: x86_64\n CPUs: 2\n Total Memory: 3.47GiB\n Name: ip-192-168-0-136.ec2.internal\n ID: b23438a2-6b14-435b-84af-ba741e2774f5\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  127.0.0.0/8\n  ::1/128\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\n\n### Additional Info\n\n_No response_",
      "solution": "Hey folks \ud83d\udc4b\ud83c\udffc \n\nApologies for the delayed response, I had some traveling around the time this issue was reported but finally getting around to taking a look. Thanks again @felixfontein for reporting and providing some details on your use case.\n\nIt looks to me this issue is all but resolved with some good context for others. I added a [thread](https://github.com/moby/moby/discussions/51468#discussioncomment-15151829) on the 29.0 release notes as well to point folks here.\n\nI highlighted there was well we have added a [port range](https://pkg.go.dev/github.com/moby/moby/api@v1.52.0/types/network#PortRange) type to the API so in the future we can more clearly differentiate between a single port value and a port range so to avoid this confusion in the future.",
      "labels": [
        "area/api",
        "status/0-triage",
        "kind/bug",
        "version/29.0"
      ],
      "created_at": "2025-11-14T20:56:30Z",
      "closed_at": "2025-12-03T17:19:08Z",
      "url": "https://github.com/moby/moby/issues/51537",
      "comments_count": 8
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51679,
      "title": "Cannot run any containers: bufio.Scanner: token too long",
      "problem": "### Description\n\nWhenever I try and start any container, I get an error. Log is shown from `journalctl`:\n\n```\nHandler for POST /v1.51/containers/mariadb11/start returned error: bufio.Scanner: token too long\n```\n\nI've completely uninstalled, removed all Docker-related directories and reinstalled docker on my system and still the error remains\n\n### Reproduce\n\nI'm not sure the reproduction steps, Docker has been working normally for months then suddenly now has this issue.\n\n### Expected behavior\n\nDocker can run containers\n\n### docker version\n\n```bash\nClient:\n Version:           28.5.2\n API version:       1.51\n Go version:        go1.25.3 X:nodwarf5\n Git commit:        ecc694264d\n Built:             Wed Nov  5 19:24:39 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer:\n Engine:\n  Version:          28.5.2\n  API version:      1.51 (minimum version 1.24)\n  Go version:       go1.25.3 X:nodwarf5\n  Git commit:       89c5e8fd66\n  Built:            Wed Nov  5 19:24:39 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v2.2.0\n  GitCommit:        1c4457e00facac03ce1d75f7b6777a7a851e5c41.m\n runc:\n  Version:          1.4.0-rc.1+dev\n  GitCommit:        v1.4.0-rc.1-196-ge0adafb4c\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient:\n Version:    28.5.2\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  0.30.1\n    Path:     /usr/lib/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  2.40.3\n    Path:     /usr/lib/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 1\n  Running: 0\n  Paused: 0\n  Stopped: 1\n Images: 1\n Server Version: 28.5.2\n Storage Driver: overlay2\n  Backing Filesystem: btrfs\n  Supports d_type: true\n  Using metacopy: true\n  Native Overlay Diff: false\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 1c4457e00facac03ce1d75f7b6777a7a851e5c41.m\n runc version: v1.4.0-rc.1-196-ge0adafb4c\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.17.8-arch1-1\n Operating System: Arch Linux\n OSType: linux\n Architecture: x86_64\n CPUs: 24\n Total Memory: 31.1GiB\n Name: James-Desktop\n ID: 103e6efc-e45d-4612-ba14-52fcdfea93cf\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n```\n\n### Additional Info\n\n_No response_",
      "solution": "Google Gemini has worked it out!\n\nMy `/etc/resolv.conf` was massive, over 80KB. This was caused by two network managers \"fighting\" over the file and writing to it in a recursive loop.\n\nI have since fixed this on my system and reset the `resolv.conf` file, which fixed this issue for me.\n\nNot a bug with Docker. A bug with system networking.",
      "labels": [
        "status/0-triage",
        "kind/bug",
        "version/28.5"
      ],
      "created_at": "2025-12-10T15:32:48Z",
      "closed_at": "2026-01-06T11:45:09Z",
      "url": "https://github.com/moby/moby/issues/51679",
      "comments_count": 21
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51801,
      "title": "Memory leak in client.ContainerLogs using newCancelReadCloser",
      "problem": "### Description\n\nHello!\n\nIf a user does not explicitly cancel the context passed to `ContainerLogs`, the `context.AfterFunc` is leaked.\n\nThe godoc for `ContainerLogs` says:\n```\n// ContainerLogs returns the logs generated by a container in an [io.ReadCloser].\n// It's up to the caller to close the stream.\n//\n// The underlying [io.ReadCloser] is automatically closed if the context is canceled,\n```\nI wonder if this should _either_ tell the user that the context passed to ContainerLogs _must_ be cancelled, or the `newCancelReadCloser` might want to just capture the stop function\n\n```\nfunc newCancelReadCloser(ctx context.Context, rc io.ReadCloser) io.ReadCloser {\n\tcrc := &cancelReadCloser{\n\t\trc:    rc,\n\t\tclose: sync.OnceValue(rc.Close),\n\t}\n\tcrc.stop = context.AfterFunc(ctx, func() { _ = crc.Close() })\n\treturn crc\n}\n```\nAnd call stop when the ReadCloser is closed?\n\nThis fixed the issue in my local fork. Hopefully it saves someone some time investigating. I'm also happy to make a PR if the approach seems reasonable.\n\n### Reproduce\n\nHave an app using the moby client.\n\nCall a function like below a lot:\n```\nfunc logs(ctx context.Context, client *docker.Client) {\n\treader, err := client.ContainerLogs(ctx, \"container-id\", docker.ContainerLogsOptions{\n\t\tShowStdout: true,\n\t\tShowStderr: true,\n\t})\n\tif err != nil {\n\t\treturn\n\t}\n    // I'm closing the reader so I think all is well!\n\tdefer reader.Close()\n}\n```\nWatch context.AfterFunc grow in pprof heap profile.\n\nGranted, you have to call this function quite a few times to even see it pop up on a heap profile (10k?) but it does. Using a `context.WithCancel` and deferring the cancel in `logs()` fixes the leak.\n\n### Expected behavior\n\nI would expect that either the user is explicitly warned they **must** cancel the context, or the `AfterFunc` is cleaned up when a user calls the `Close()` method.\n\n### docker version\n\n```bash\nClient:\n Version:           29.1.3\n API version:       1.52\n Go version:        go1.25.5 X:nodwarf5\n Git commit:        f52814d454\n Built:             Sun Dec 14 18:17:38 2025\n OS/Arch:           linux/amd64\n Context:           default\n```\n\n### docker info\n\n```bash\nClient:\n Version:    29.1.3\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  0.30.1\n    Path:     /usr/lib/docker/cli-plugins/docker-buildx\n```\n\n### Additional Info\n\nThis is using `github.com/moby/moby/client v0.2.1`",
      "solution": "Also had a quick peek if there's other locations where we had similar issues; looks like this function has a similar issue; https://github.com/moby/moby/blob/bbaeb9036ff28b3d50fcb6b16411e62197a3002c/client/internal/jsonmessages.go#L47-L54\n\n\nProbably could be fixed similarly, e.g. something like;\n\n```go\nfunc (r stream) JSONMessages(ctx context.Context) iter.Seq2[jsonstream.Message, error] {\n\tstop := context.AfterFunc(ctx, func() {\n\t\t_ = r.Close()\n\t})\n\tdec := json.NewDecoder(r)\n\treturn func(yield func(jsonstream.Message, error) bool) {\n\t\tdefer func() {\n\t\t\tstop() // unregister AfterFunc\n\t\t\t_ = r.Close()\n\t\t}()\n\t\tfor {\n```\n",
      "labels": [
        "kind/bug",
        "status/confirmed",
        "area/go-sdk"
      ],
      "created_at": "2026-01-04T22:06:48Z",
      "closed_at": "2026-01-05T15:09:59Z",
      "url": "https://github.com/moby/moby/issues/51801",
      "comments_count": 3
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 50282,
      "title": "Docker Overlay Network Bug: Standalone Containers Cannot Communicate Across Nodes Despite Working VXLAN",
      "problem": "### Description\n\nStandalone containers attached to attachable overlay networks cannot communicate across swarm nodes, despite VXLAN tunneling working correctly at the network layer. This contradicts the official documentation which states that attachable overlay networks should support standalone container communication.\n\n\n\n### Reproduce\n\n### Environment Setup\n- **Two Debian hosts** in Docker Swarm (hp-server-1 as manager, hp-server-2 as worker)\n- **All required ports open**: 2377/tcp, 7946/tcp+udp, 4789/udp\n- **Docker Swarm services work perfectly** on overlay networks\n\n### Reproduction Steps\n\n1. **Create attachable overlay network:**\n   ```bash\n   # On manager node\n   docker network create -d overlay --attachable test-standalone-overlay\n   ```\n\n2. **Create standalone containers on both nodes:**\n   ```bash\n   # On hp-server-1:\n   docker run -d --name standalone1 --network test-standalone-overlay alpine sleep 3600\n   \n   # On hp-server-2:\n   docker run -d --name standalone2 --network test-standalone-overlay alpine sleep 3600\n   ```\n\n3. **Attempt cross-node communication:**\n   ```bash\n   # From either container to the other\n   docker exec standalone1 ping standalone2  # Hangs indefinitely\n   docker exec standalone2 ping standalone1  # Hangs indefinitely\n   ```\n\n### Expected behavior\n\nAccording to [[official Docker documentation](https://docs.docker.com/engine/network/tutorials/overlay/#use-an-overlay-network-for-standalone-containers)](https://docs.docker.com/engine/network/tutorials/overlay/#use-an-overlay-network-for-standalone-containers), standalone containers should be able to communicate across nodes when using attachable overlay networks.\n\n### Actual Behavior\n\n- **Ping hangs indefinitely** with 100% packet loss\n- **ARP resolution works** - containers can see each other's MAC addresses\n- **DNS resolution works** - container names resolve to correct overlay network IPs\n- **Gateway connectivity works** - containers can ping the overlay gateway (10.0.1.1)\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           28.1.1\n API version:       1.49\n Go version:        go1.23.8\n Git commit:        4eba377\n Built:             Fri Apr 18 09:52:57 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          28.1.1\n  API version:      1.49 (minimum version 1.24)\n  Go version:       go1.23.8\n  Git commit:       01f442b\n  Built:            Fri Apr 18 09:52:57 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.7.27\n  GitCommit:        05044ec0a9a75232cad458027ca83437aae3f4da\n runc:\n  Version:          1.2.5\n  GitCommit:        v1.2.5-0-g59923ef\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    28.1.1\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.23.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.35.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 1\n  Running: 1\n  Paused: 0\n  Stopped: 0\n Images: 17\n Server Version: 28.1.1\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n Swarm: active\n  NodeID: c2ux8ri1xd4witjv78uzeyphv\n  Is Manager: true\n  ClusterID: t9edk1tc04u7nrrndjqsmq5sh\n  Managers: 1\n  Nodes: 2\n  Default Address Pool: 10.0.0.0/8  \n  SubnetSize: 24\n  Data Path Port: 4789\n  Orchestration:\n   Task History Retention Limit: 5\n  Raft:\n   Snapshot Interval: 10000\n   Number of Old Snapshots to Retain: 0\n   Heartbeat Tick: 1\n   Election Tick: 10\n  Dispatcher:\n   Heartbeat Period: 5 seconds\n  CA Configuration:\n   Expiry Duration: 3 months\n   Force Rotate: 0\n  Autolock Managers: false\n  Root Rotation In Progress: false\n  Node Address: 192.168.1.6\n  Manager Addresses:\n   192.168.1.6:2377\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 05044ec0a9a75232cad458027ca83437aae3f4da\n runc version: v1.2.5-0-g59923ef\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.1.0-34-amd64\n Operating System: Debian GNU/Linux 12 (bookworm)\n OSType: linux\n Architecture: x86_64\n CPUs: 8\n Total Memory: 30.29GiB\n Name: hp-server-1\n ID: 2fa4566a-662e-44fa-8a8a-00372f9d6b8b\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n```\n\n### Additional Info\n\n## Root Cause Analysis\n\nThrough detailed packet capture analysis, I've identified the precise failure point:\n\n### 1. VXLAN Tunnel Works Perfectly\n\nHost-level tcpdump shows bidirectional VXLAN traffic on port 4789:\n```\n# VXLAN traffic flows correctly in both directions\nhp-server-2.44558 > hp-server-1.4789: VXLAN, flags [I] (0x08), vni 4098\nIP 10.0.1.4 > 10.0.1.2: ICMP echo request, id 13, seq 364, length 64\n\nhp-server-1.45607 > hp-server-2.4789: VXLAN, flags [I] (0x08), vni 4098  \nIP 10.0.1.2 > 10.0.1.4: ICMP echo reply, id 13, seq 364, length 64\n```\n\n### 2. Container-Level Routing Failure\n\nContainer-level tcpdump shows **only outgoing packets**:\n```bash\n# Container tcpdump shows requests but NO replies\ndocker exec standalone2 tcpdump -i eth0 host 10.0.1.2\n# Output shows only: \"ICMP echo request\" packets\n# Missing: \"ICMP echo reply\" packets\n```\n\n### 3. Network Configuration is Correct\n\nBoth containers are properly attached to the overlay network:\n```json\n{\n  \"Containers\": {\n    \"standalone1\": {\n      \"IPv4Address\": \"10.0.1.2/24\",\n      \"MacAddress\": \"02:42:0a:00:01:02\"\n    },\n    \"standalone2\": {\n      \"IPv4Address\": \"10.0.1.4/24\", \n      \"MacAddress\": \"02:42:0a:00:01:04\"\n    }\n  }\n}\n```\n\n## The Bug\n\n**ICMP reply packets reach the target host via VXLAN but are not routed into the container's network namespace.** This appears to be a routing issue in Docker's overlay network implementation specific to standalone containers.\n\n## Comparison: Swarm Services vs Standalone Containers\n\n**Swarm services work perfectly:**\n```bash\ndocker service create --name test1 --network test-overlay --replicas 1 alpine sleep 3600\ndocker service create --name test2 --network test-overlay --replicas 1 alpine sleep 3600\n# Cross-node ping works: 0% packet loss \u2705\n```\n\n**Standalone containers fail:**\n```bash\ndocker run -d --name standalone1 --network test-overlay alpine sleep 3600\ndocker run -d --name standalone2 --network test-overlay alpine sleep 3600  \n# Cross-node ping fails: 100% packet loss \u274c\n```\n\n## Impact\n\nThis bug makes the `--attachable` flag essentially non-functional for cross-node standalone container communication, contradicting official documentation and user expectations.\n\n## Additional Notes\n\n- This issue appears to be a long-standing bug based on similar reports in issues #30972, #43643, and #34641\n- The problem is specifically with Docker's internal routing from host network namespace to container network namespace\n- VXLAN infrastructure works correctly, indicating this is a Docker software issue, not a network configuration problem\n\n\n## Update: HTTP Protocol Also Affected - Complete Communication Failure\n\nTested HTTP connectivity and confirmed **all protocols are affected**:\n\n**Bidirectional HTTP Test Results:**\n```bash\n# hp-server-2 \u2192 hp-server-1\ndocker exec http-server2 wget -O- --timeout=10 http://http-server1\n# Result: Connecting to http-server1 (10.0.1.6:80)\n#         wget: download timed out\n\n# hp-server-1 \u2192 hp-server-2  \ndocker exec http-server1 wget -O- --timeout=10 http-server2\n# Result: Connecting to http-server2 (10.0.1.7:80)\n#         wget: download timed out\n```\n\n## Update: Bug Confirmed Across Multiple Major Docker Versions\n\nTested with **Docker 20.10.24** (Debian repository version) and confirmed the issue persists:\n\n**Environment:**\n- hp-server-1: Docker 20.10.24 (manager)  \n- hp-server-2: Docker 20.10.24 (worker)\n- Fresh swarm cluster with identical versions\n\n**Test Results:**\n```bash\n# Both directions still fail with 100% packet loss\ndocker exec standalone1-v20 ping -c 3 standalone2-v20\n# PING standalone2-v20 (10.0.1.4): 3 packets transmitted, 0 packets received, 100% packet loss\n\ndocker exec standalone2-v20 ping -c 3 standalone1-v20  \n# PING standalone1-v20 (10.0.1.2): 3 packets transmitted, 0 packets received, 100% packet loss\n```",
      "solution": "I cannot reproduce the issue on either v28.1.1 or v28.3.0. Standalone containers attached to an attachable overlay network can make HTTP requests to other standalone containers on other Swarm nodes attached to the same overlay network.\n\n---\n\nFiltering at the physical network level seems unlikely as it would equally impact Swarm-service containers.\n\nAre you able to provide debug logs from the docker daemons on both nodes?\n\nCould you try reproducing the issue again, but this time have the Swarm services attached to the overlay running on the nodes while you start the standalone containers? If network connectivity miraculously starts working on the standalone containers, it would confirm that you are hitting existing known issues I am in the middle of fixing.\n\n---\n\nI will close the issue because I just don't have time to set up a test. I will just reopen when i am ready to contribute more!",
      "labels": [
        "status/0-triage",
        "status/more-info-needed",
        "kind/bug",
        "area/networking",
        "area/networking/d/overlay",
        "version/28.1"
      ],
      "created_at": "2025-06-30T08:38:21Z",
      "closed_at": "2026-01-02T18:56:05Z",
      "url": "https://github.com/moby/moby/issues/50282",
      "comments_count": 7
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51799,
      "title": "containerd.io missing apt package",
      "problem": "### Description\n\nCICD pipelines all started to fail this morning:\n\n```\n[   74.802849] cloud-init[1332]: E: Failed to fetch https://download.docker.com/linux/ubuntu/dists/noble/pool/stable/amd64/containerd.io_2.2.1-1%7eubuntu.24.04%7enoble_amd64.deb  404  Not Found [IP: 3.169.173.110 443]\n[   74.805844] cloud-init[1332]: E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n```\n\n\n\n### Reproduce\n\nConfigure apt and try to install\n\n```\n# Need to install docker\napt-get install ca-certificates curl -y\ninstall -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nchmod a+r /etc/apt/keyrings/docker.asc\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release && echo \"$VERSION_CODENAME\") stable\" | \\\n  tee /etc/apt/sources.list.d/docker.list > /dev/null\n\napt-get update\napt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y\n```\n\n### Expected behavior\n\nInstall without errors\n\n### docker version\n\n```bash\nN/A\n```\n\n### docker info\n\n```bash\nN/A\n```\n\n### Additional Info\n\nNot sure this is even the right place \u00af\\_(\u30c4)_/\u00af",
      "solution": "This bug is stopping us from starting our docker hosts. Does anyone know a workaround?\n\n---\n\n> This bug is stopping us from starting our docker hosts. Does anyone know a workaround?\n\nFor noble you can pin a previous version `containerd.io=2.2.0-2~ubuntu.24.04~noble`",
      "labels": [
        "status/0-triage",
        "kind/bug"
      ],
      "created_at": "2026-01-02T11:33:18Z",
      "closed_at": "2026-01-02T15:38:55Z",
      "url": "https://github.com/moby/moby/issues/51799",
      "comments_count": 8
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 45498,
      "title": "Error running containers: Error loading seccomp filter into kernel",
      "problem": "### Description\r\n\r\nI have a server running some containers with docker-compose and the `restart: always` option, so when a container fails, Docker restarts it. The problem is since some months ago Docker shows this error when tried to start a container: \r\n\r\n```\r\nOCI runtime exec failed: exec failed: unable to start container process: error loading seccomp filter into kernel: error loading seccomp filter: errno 524: unknown\r\n```\r\n\r\nI tried updating Docker, Docker compose, kernel and passing a custom seccomp configuration, but nothing of this was work. The only workarounds for the problem are to set the `seccomp:unconfined` or to restart the docker daemon, but it are not viable at the long term because one is a security error and the other causes downtime and the problem appears again in a few weeks.\r\n\r\nAnyone can help me to figure it out how to fix this issue?\r\n\r\n### Reproduce\r\n\r\n1. Run some containers\r\n2. Wait some weeks of normal operation (including restart containers)\r\n3. Eventually container restarts will fails with that error\r\n\r\n### Expected behavior\r\n\r\nThe kernel must load the seccomp filters without problems always.\r\n\r\n### docker version\r\n\r\n```bash\r\nClient:\r\n Version:           20.10.21\r\n API version:       1.41\r\n Go version:        go1.18.1\r\n Git commit:        20.10.21-0ubuntu1~22.04.2\r\n Built:             Thu Mar  2 18:26:04 2023\r\n OS/Arch:           linux/amd64\r\n Context:           default\r\n Experimental:      true\r\n\r\nServer:\r\n Engine:\r\n  Version:          20.10.21\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.18.1\r\n  Git commit:       20.10.21-0ubuntu1~22.04.2\r\n  Built:            Wed Feb 15 15:26:57 2023\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.12-0ubuntu1~22.04.1\r\n  GitCommit:        \r\n runc:\r\n  Version:          1.1.4-0ubuntu1~22.04.1\r\n  GitCommit:        \r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  compose: Docker Compose (Docker Inc., v2.16.0)\r\n\r\nServer:\r\n Containers: 86\r\n  Running: 59\r\n  Paused: 0\r\n  Stopped: 27\r\n Images: 3535\r\n Server Version: 20.10.21\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: \r\n runc version: \r\n init version: \r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: default\r\n  cgroupns\r\n Kernel Version: 5.15.0-60-generic\r\n Operating System: Ubuntu 22.04.2 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 40\r\n Total Memory: 62.8GiB\r\n Name: ---\r\n ID: YLDP:SGFK:UX5E:WM77:PM6P:NWXF:6LHM:F6R5:IH4U:34P3:EONM:7S7I\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n```\r\n$ uname -r\r\n5.15.0-60-generic\r\n$ cat /etc/lsb-release \r\nDISTRIB_ID=Ubuntu\r\nDISTRIB_RELEASE=22.04\r\nDISTRIB_CODENAME=jammy\r\nDISTRIB_DESCRIPTION=\"Ubuntu 22.04.2 LTS\"\r\n```",
      "solution": "Document maybe, [recommend no.](https://bugs.launchpad.net/ubuntu/+source/libseccomp/+bug/1956954/comments/7)\r\n\r\nIf it is, in fact, the BPF JIT resource limit being hit, I find it quite curious that restarting _the docker daemon_ successfully works around the issue. And reading between the lines, it sounds like there is a non-trivial amount of container churn happening. Unless one of the workloads is loading huge (number of) BPF programs into the kernel, it sounds a lot like the JIT'ed BPF programs for seccomp filters\u2014or perhaps cgroup eBPF programs?\u2014are not being cleaned up when a container exits, but are being cleaned up when dockerd (or containerd?) does. My hunch is that bumping `net.core.bpf_jit_limit` is working around a resource leak and that the root cause is a bug in runC, systemd or the kernel.\n\n---\n\nThanks for the useful responses, I check the `net.core.bpf_jit_limit` on my server and its minor than the suggested on the linked post. \r\n\r\n```\r\n~ $ sudo sysctl --all | grep net.core.bpf_jit_limit\r\nnet.core.bpf_jit_limit = 264241152\r\n~ $ sudo cat /proc/vmallocinfo | grep bpf_jit | awk '{s+=$2} END {print s}'\r\n91500544\r\n```\r\n\r\nUnfortunately, yesterday we restarted the docker daemon (`systemctl restart docker`), so right know the problem its not happening, but when it happens again we'll increase the `bpf_jit_limit` value and check if it solves the problem. \r\n\r\nI'll close this issue for the moment and I'll update it when the solution has been checked.\r\n\r\nThanks a lot!\n\n---\n\n@corhere Indeed, this commit torvalds/linux@3a15fb6ed92cb32b0a83f406aa4a96f28c9adbc3 introduced a memory leak in seccomp (first released in v5.9). It was fixed in torvalds/linux@a1140cb215fa13dcec06d12ba0c3ee105633b7c4 and has been released in v6.2. Here's the list of backports:\r\n\r\n- v6.1.2 (torvalds/linux@5b81f0c6c60e35bf8153230ddfb03ebb14e17986)\r\n- v6.0.16 (torvalds/linux@29a69fa075d0577eff1137426669de21187ec182)\r\n- v5.15.86 (torvalds/linux@a31a647a3d1073a642c5bbe3457731fb353cb980)\r\n\r\nAbout the default value of `net.core.bpf_jit_limit`, this patch is doubling it: torvalds/linux@10ec8ca8ec1a2f04c4ed90897225231c58c124a7. It's been released in v6.3 and backported to:\r\n\r\n- v6.2.9 (torvalds/linux@68ed00a37d2d1c932ff7be40be4b90c4bec48c56)\r\n- v6.1.22 (torvalds/linux@9cda812c76067c8a771eae43bb6943481cc7effc)\r\n- v5.15.105 (torvalds/linux@54869daa6a437887614274f65298ba44a3fac63a)\r\n- v5.10.177 (torvalds/linux@a4bbab27c4bf69486f5846d44134eb31c37e9b22)\r\n- v5.4.240 (torvalds/linux@d69c2ded95b17d51cc6632c7848cbd476381ecd6)\r\n- v4.19.280 (torvalds/linux@42049e65d338870e93732b0b80c6c41faf6aa781)\r\n- v4.14.312 (torvalds/linux@374ed036309fce73f9db04c3054018a71912d46b)\r\n\r\nAs such, I don't think we should document `bpf_jit_limit`.\r\n\r\n@racaceres As said by @corhere increasing `bpf_jit_limit` is only a workaround. You should probably update your kernel.",
      "labels": [
        "area/kernel",
        "status/0-triage",
        "kind/bug"
      ],
      "created_at": "2023-05-09T02:15:36Z",
      "closed_at": "2023-05-30T12:48:15Z",
      "url": "https://github.com/moby/moby/issues/45498",
      "comments_count": 11
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51755,
      "title": "ipv6 address on bridge interface disappear after a while when upgrade from 27 to 28/29",
      "problem": "### Description\n\nAfter upgrading to docker 28 and 29, the IPv6 gateway address on the bridge interface disappears after some time. This happend on both default (docker0) and user defined network \n\n### Reproduce\n\n1. sudo docker network create --attachable --ipv6 --subnet 172.18.0.0/16 --subnet fd7b:1b70:2d2e:107::/64 net\n2. ifconfig (it show the br-xxxx have the fd7b:1b70:2d2e:107::1 address)\n3. wait around 130s to 200s\n4. ifconfig (it dosen't show the gateway address)\n\nor\n\n1. in daemon.json enable ipv6\n2. same step as step 2 above\n\n### Expected behavior\n\nIt should stay like in docker 27\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           29.1.3\n API version:       1.52\n Go version:        go1.25.5\n Git commit:        f52814d\n Built:             Fri Dec 12 14:50:09 2025\n OS/Arch:           linux/arm64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          29.1.3\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.5\n  Git commit:       fbf3ed2\n  Built:            Fri Dec 12 14:50:09 2025\n  OS/Arch:          linux/arm64\n  Experimental:     true\n containerd:\n  Version:          v2.2.0\n  GitCommit:        1c4457e00facac03ce1d75f7b6777a7a851e5c41\n runc:\n  Version:          1.3.4\n  GitCommit:        v1.3.4-0-gd6d73eb8\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    29.1.3\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v5.0.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 14\n  Running: 14\n  Paused: 0\n  Stopped: 0\n Images: 16\n Server Version: 29.1.3\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 1c4457e00facac03ce1d75f7b6777a7a851e5c41\n runc version: v1.3.4-0-gd6d73eb8\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.8.0-1039-oracle\n Operating System: Ubuntu 22.04.5 LTS\n OSType: linux\n Architecture: aarch64\n CPUs: 2\n Total Memory: 11.65GiB\n Name: uk-s-o-b-1\n ID: 14e7c256-3af1-4363-89cd-25c703cfebec\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: true\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\n\n### Additional Info\n\nauditctl show no ip command exec\n\n`ip -6 monitor addr dev br-xxx` show\n```\nDeleted 2233: br-xxx    inet6 fd7b:1b70:2d2e:107::1/64 scope global nodad \n       valid_lft forever preferred_lft forever\n```",
      "solution": "> Yep found the cause, I have crontab running `dhclient -6` */5 .\n\nGreat! Thanks for letting us know.\n\n> But it is a regression? it does not cause the same issue on 27 or before\n\nMaybe something else changed on your host when you updated Docker?\n\nI don't see any differences between the IP addresses configured by the two versions, for a \"docker network create --ipv6 b46; docker run --rm -d --network b46 alpine sleep infinity\".\n\n27.5.1:\n\n```\n13: br-e3079de7b726: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP\n    link/ether 02:42:3b:dd:10:97 brd ff:ff:ff:ff:ff:ff\n    inet 172.19.0.1/16 brd 172.19.255.255 scope global br-e3079de7b726\n       valid_lft forever preferred_lft forever\n    inet6 fdff:52e0:641d::1/64 scope global flags 02\n       valid_lft forever preferred_lft forever\n    inet6 fe80::42:3bff:fedd:1097/64 scope link\n       valid_lft forever preferred_lft forever\n```\n\n29.1.3:\n\n```\n13: br-9e25d892ecc3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP\n    link/ether 1e:50:4e:64:00:8e brd ff:ff:ff:ff:ff:ff\n    inet 172.19.0.1/16 brd 172.19.255.255 scope global br-9e25d892ecc3\n       valid_lft forever preferred_lft forever\n    inet6 fdf2:664d:2af5::1/64 scope global flags 02\n       valid_lft forever preferred_lft forever\n    inet6 fe80::1c50:4eff:fe64:8e/64 scope link\n       valid_lft forever preferred_lft forever\n```\n\nI'll mark the issue as closed, but can reopen if there is something to track.",
      "labels": [
        "status/0-triage",
        "kind/bug",
        "area/networking"
      ],
      "created_at": "2025-12-17T23:05:37Z",
      "closed_at": "2025-12-20T13:51:19Z",
      "url": "https://github.com/moby/moby/issues/51755",
      "comments_count": 5
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 45297,
      "title": "Anonymous readonly volumes",
      "problem": "### Description\n\nThis feels like a bug but since there's a dedicated message saying that it isn't supported I guess it can be classified as a feature request.\r\n\r\nWhen I try to create a volume that is both anonymous and readonly like this:\r\n```shell\r\ndocker run --mount 'target=/ms,volume-driver=rclone,volume-opt=remote=ms:,readonly' ...\r\n```\r\nI get:\r\n```text\r\ndocker: Error response from daemon: invalid mount config for type \"volume\": must not set ReadOnly mode when using anonymous volumes.\r\n```\r\n\r\nI don't understand what one has to do with the other.\r\n\r\nUsing a readwrite volume is bad for the usual reasons and using a named volume is bad because as far as I know those are not automatically removed when the container is removed.",
      "solution": "I have the same issue.\r\n\r\nI want to mount into the container a NFS volume as read-only.\r\nThis volume contains data that are generated by other processes.\r\nThe container should only read data in this volume &rarr; it should not modify them. So R/W rights should not be set for security purpose and to avoid human/developer wrong behavior.\r\n\r\nThe NFS mount / NFS volume should be set only when this container is started and then, automatically unmount / destroyed when the container is destroyed.\r\n\r\nNote that this behavior works if I remove the `readonly` options in the `--mount`option of the `docker run` command.\r\n\r\nCurrently, my tricky workaround is to create the volume with `docker volume create ...` command before starting the container and destroying it when the container is stopped with `docker volume rm ...`. I must declare a systemd service that is using directives `ExecStartPre`, `ExecStart`, and `ExecStop`.\r\n\r\nIt will be great to support `readonly` option for anonymous volumes unless there is a valid reason to forbid this behavior?",
      "labels": [
        "status/0-triage",
        "kind/enhancement",
        "area/volumes"
      ],
      "created_at": "2023-04-08T03:11:42Z",
      "closed_at": "2025-12-19T15:20:34Z",
      "url": "https://github.com/moby/moby/issues/45297",
      "comments_count": 8
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 49596,
      "title": "Default bridge network (sometimes) left with no IP after restarting dockerd, when default-address-pools configured",
      "problem": "### Description\n\nSetting `default-address-pools` in `/etc/docker/default.json` results in the default bridge interface (`docker0`) _sometimes_ ending up with no configured IP address when `dockerd` is restarted.\n\n### Reproduce\n\n```\n# cat > /etc/docker/default.json << EOF\n{\n  \"bip\": \"10.0.20.1/23\",\n  \"default-address-pools\": [\n    {\"base\":\"172.17.0.0/16\",\"size\":16}\n  ]\n}\nEOF\n\n# systemctl restart docker\n# systemctl restart docker\n```\n\nwhile in another terminal watching the `docker0` interface config via:\n```\n$ while true; do clear;ip -br a | grep docker0;sleep 0.1;done\n```\n\nYou've hit the bug when the resulting output after the restart is:\n```\ndocker0          DOWN                                   # <--- No IP on docker0\n```\nrather than the expected:\n```\ndocker0          DOWN           10.0.20.1/23\n```\n\nNOTE: The issue is also reproducible by just stopping the `docker` service and manually running `dockerd`, so it's completely independent of systemd.\n\n### Expected behavior\n\nRestarting the docker service (which is ultimately just restarting `dockerd`) should result in the default bridge interface (`docker0`) having the configured `bip` IP, every time.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           28.0.1\n API version:       1.48\n Go version:        go1.23.6\n Git commit:        068a01e\n Built:             Wed Feb 26 10:41:08 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          28.0.1\n  API version:      1.48 (minimum version 1.24)\n  Go version:       go1.23.6\n  Git commit:       bbd0a17\n  Built:            Wed Feb 26 10:41:08 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.7.25\n  GitCommit:        bcc810d6b9066471b0b6fa75f557a15a1cbf31bb\n runc:\n  Version:          1.2.4\n  GitCommit:        v1.2.4-0-g6c52b3f\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community                                                                                                                                   [2/57]\n Version:    28.0.1                                                                                                                                                       \n Context:    default                                                                                                                                                      \n Debug Mode: false                                                                   \n Plugins:                                                                                                                                                                 \n  buildx: Docker Buildx (Docker Inc.)                                                \n    Version:  v0.21.1       \n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx                          \n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.33.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose                         \n                                          \nServer:                                  \n Containers: 2                                                                                                                                                            \n  Running: 0                              \n  Paused: 0                                                                                                                                                               \n  Stopped: 2                                                                                                                                                              \n Images: 3                                                                                                                                                                \n Server Version: 28.0.1                                                                                                                                                   \n Storage Driver: overlay2                                                                                                                                                 \n  Backing Filesystem: extfs                                                                                                                                               \n  Supports d_type: true                                                                                                                                                   \n  Using metacopy: false                                                                                                                                                   \n  Native Overlay Diff: true                                                                                                                                               \n  userxattr: false                                                                   \n Logging Driver: json-file                                                           \n Cgroup Driver: systemd     \n Cgroup Version: 2\n Plugins:                         \n  Volume: local                  \n  Network: bridge host ipvlan macvlan null overlay                                   \n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog           \n Swarm: inactive            \n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc                                                               \n Init Binary: docker-init      \n containerd version: bcc810d6b9066471b0b6fa75f557a15a1cbf31bb                        \n runc version: v1.2.4-0-g6c52b3f          \n init version: de40ad0           \n Security Options:\n  apparmor                \n  seccomp                                                                            \n   Profile: builtin         \n  cgroupns                 \n Kernel Version: 5.15.0-133-generic                                                  \n Operating System: Ubuntu 22.04.5 LTS\n OSType: linux           \n Architecture: x86_64\n CPUs: 2                  \n Total Memory: 7.752GiB                                                              \n Name: ctracy-vml9\n ID: a8f4d530-8291-4984-a90a-66e55ff59794\n Docker Root Dir: /var/lib/docker    \n Debug Mode: false\n Experimental: false      \n Insecure Registries:      \n  ::1/128                      \n  127.0.0.0/8                    \n Live Restore Enabled: false\n Default Address Pools:\n   Base: 172.17.0.0/16, Size: 16\n```\n\n### Additional Info\n\nLots of `dlv`ing and hacked builds later, there really does appear to be a race condition in the netlink calls to configure the default bridge interface, due to it being configured *twice* once you have a network state file and explicitly configured `default-address-pools`, which _sometimes_ results in the interface being left without an IP.\n\nThe issue goes away for the next restart if you remove the network state file:\n```\n# rm -f /var/lib/docker/network/files/local-kv.db\n```\nbut then comes back once the state file again exists.\n\nThe problem also goes away if you run `dockerd` via `strace`, due to the delay induced by `strace`.  (Which made debugging this quite fun)\n\nAdding a 500ms pause [here](https://github.com/moby/moby/blob/v28.0.1/daemon/daemon_unix.go#L889)  (right after the first call to `removeDefaultBridgeInterface`) completely avoids the issue.  Chasing the code further, if we move the pause to [here](https://github.com/moby/moby/blob/v28.0.1/libnetwork/drivers/bridge/setup_device_linux.go#L69), the issue comes back, but if we move it up one line to [here](https://github.com/moby/moby/blob/v28.0.1/libnetwork/drivers/bridge/setup_device_linux.go#L68), the issue is resolved.\n\nIt looks like the issue is the first configuration of the `docker0` interface is perhaps not completely finished when the second one is done, which ultimately results in netlink unconfiguring the IP?  Unfortunately, I lack the skill to chase the issue into the kernel itself, so that's as far as I was able to get with it.  (And I don't expect my super-hacky `time.Sleep` based workaround to be PR-worthy)\n\n(NOTE: Enabling [`live-restore`](https://docs.docker.com/engine/daemon/live-restore/) avoids the issue, because then the default bridge network is just left alone if it already exists, rather than being deleted and re-initialized twice)\n\nFWIW, the output of `ip -ts monitor all` looks like this for a successful run (forced to be successful using `strace`):\n```\n[2025-03-06T13:21:10.024233] [ADDR]31: docker0    inet 10.0.20.1/23 brd 10.0.21.255 scope global docker0\n       valid_lft forever preferred_lft forever\n[2025-03-06T13:21:10.025747] [ROUTE]local 10.0.20.1 dev docker0 table local proto kernel scope host src 10.0.20.1 \n[2025-03-06T13:21:10.202952] [LINK]31: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default \n    link/ether ea:07:79:fe:e8:2e brd ff:ff:ff:ff:ff:ff\n[2025-03-06T13:21:10.203010] [ROUTE]broadcast 10.0.21.255 dev docker0 table local proto kernel scope link src 10.0.20.1 linkdown \n[2025-03-06T13:21:10.203021] [ROUTE]10.0.20.0/23 dev docker0 proto kernel scope link src 10.0.20.1 linkdown \n[2025-03-06T13:21:10.668354] [NEIGH]Deleted dev docker0 lladdr ea:07:79:fe:e8:2e PERMANENT\n[2025-03-06T13:21:10.668382] [NEIGH]Deleted dev docker0 lladdr ea:07:79:fe:e8:2e PERMANENT\n[2025-03-06T13:21:10.668443] [LINK]31: docker0: <BROADCAST,MULTICAST> mtu 1500 qdisc noqueue state DOWN group default \n    link/ether ea:07:79:fe:e8:2e brd ff:ff:ff:ff:ff:ff\n[2025-03-06T13:21:10.669253] [NEIGH]Deleted 224.0.0.251 dev docker0 lladdr 01:00:5e:00:00:fb NOARP\n[2025-03-06T13:21:10.669271] [NEIGH]Deleted 224.0.0.22 dev docker0 lladdr 01:00:5e:00:00:16 NOARP\n[2025-03-06T13:21:10.669334] [ADDR]Deleted 31: docker0    inet 10.0.20.1/23 brd 10.0.21.255 scope global docker0\n       valid_lft forever preferred_lft forever\n[2025-03-06T13:21:10.669542] [ROUTE]Deleted local 10.0.20.1 dev docker0 table local proto kernel scope host src 10.0.20.1 \n[2025-03-06T13:21:10.669604] [NETCONF]Deleted inet docker0 \n[2025-03-06T13:21:10.669615] [NETCONF]Deleted inet6 docker0 \n[2025-03-06T13:21:10.701001] [LINK]Deleted 31: docker0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default \n    link/ether ea:07:79:fe:e8:2e brd ff:ff:ff:ff:ff:ff\n[2025-03-06T13:21:10.732902] [NETCONF]inet docker0 forwarding on rp_filter strict mc_forwarding off proxy_neigh off ignore_routes_with_linkdown off \n[2025-03-06T13:21:10.735286] [NETCONF]inet6 docker0 forwarding off mc_forwarding off proxy_neigh off ignore_routes_with_linkdown off \n[2025-03-06T13:21:10.735391] [NEIGH]dev docker0 lladdr ba:15:88:bf:2e:af PERMANENT\n[2025-03-06T13:21:10.735404] [NEIGH]dev docker0 lladdr ba:15:88:bf:2e:af PERMANENT\n[2025-03-06T13:21:10.735412] [NEIGH]Deleted dev docker0 lladdr ba:15:88:bf:2e:af PERMANENT\n[2025-03-06T13:21:10.735419] [NEIGH]dev docker0 lladdr ba:15:88:bf:2e:af PERMANENT\n[2025-03-06T13:21:10.735426] [LINK]32: docker0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default \n    link/ether ba:15:88:bf:2e:af brd ff:ff:ff:ff:ff:ff\n[2025-03-06T13:21:10.748404] [NETCONF]inet docker0 rp_filter loose \n[2025-03-06T13:21:10.760974] [ADDR]32: docker0    inet 10.0.20.1/23 brd 10.0.21.255 scope global docker0\n       valid_lft forever preferred_lft forever\n[2025-03-06T13:21:10.760998] [ROUTE]local 10.0.20.1 dev docker0 table local proto kernel scope host src 10.0.20.1 \n[2025-03-06T13:21:10.999313] [LINK]32: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default \n    link/ether ba:15:88:bf:2e:af brd ff:ff:ff:ff:ff:ff\n[2025-03-06T13:21:11.000604] [ROUTE]broadcast 10.0.21.255 dev docker0 table local proto kernel scope link src 10.0.20.1 linkdown \n[2025-03-06T13:21:11.002381] [ROUTE]10.0.20.0/23 dev docker0 proto kernel scope link src 10.0.20.1 linkdown \n```\nand like this for a failed run:\n```\n[2025-03-06T13:11:25.069915] [ADDR]8: docker0    inet 10.0.20.1/23 brd 10.0.21.255 scope global docker0\n       valid_lft forever preferred_lft forever\n[2025-03-06T13:11:25.071070] [ROUTE]local 10.0.20.1 dev docker0 table local proto kernel scope host src 10.0.20.1 \n[2025-03-06T13:11:25.092513] [LINK]8: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default \n    link/ether be:d4:20:80:ca:b6 brd ff:ff:ff:ff:ff:ff\n[2025-03-06T13:11:25.092718] [ROUTE]broadcast 10.0.21.255 dev docker0 table local proto kernel scope link src 10.0.20.1 linkdown \n[2025-03-06T13:11:25.093505] [ROUTE]10.0.20.0/23 dev docker0 proto kernel scope link src 10.0.20.1 linkdown \n[2025-03-06T13:11:25.328890] [NEIGH]Deleted dev docker0 lladdr be:d4:20:80:ca:b6 PERMANENT\n[2025-03-06T13:11:25.329000] [NEIGH]Deleted dev docker0 lladdr be:d4:20:80:ca:b6 PERMANENT\n[2025-03-06T13:11:25.329155] [LINK]8: docker0: <BROADCAST,MULTICAST> mtu 1500 qdisc noqueue state DOWN group default \n    link/ether be:d4:20:80:ca:b6 brd ff:ff:ff:ff:ff:ff\n[2025-03-06T13:11:25.329641] [NEIGH]Deleted 224.0.0.251 dev docker0 lladdr 01:00:5e:00:00:fb NOARP\n[2025-03-06T13:11:25.329669] [NEIGH]Deleted 224.0.0.22 dev docker0 lladdr 01:00:5e:00:00:16 NOARP\n[2025-03-06T13:11:25.330192] [ADDR]Deleted 8: docker0    inet 10.0.20.1/23 brd 10.0.21.255 scope global docker0\n       valid_lft forever preferred_lft forever\n[2025-03-06T13:11:25.330912] [ROUTE]Deleted local 10.0.20.1 dev docker0 table local proto kernel scope host src 10.0.20.1 \n[2025-03-06T13:11:25.331590] [NETCONF]Deleted inet docker0 \n[2025-03-06T13:11:25.331742] [NETCONF]Deleted inet6 docker0 \n[2025-03-06T13:11:25.371956] [LINK]Deleted 8: docker0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default \n    link/ether be:d4:20:80:ca:b6 brd ff:ff:ff:ff:ff:ff\n[2025-03-06T13:11:25.386291] [NETCONF]inet docker0 forwarding on rp_filter strict mc_forwarding off proxy_neigh off ignore_routes_with_linkdown off \n[2025-03-06T13:11:25.386593] [NETCONF]inet6 docker0 forwarding off mc_forwarding off proxy_neigh off ignore_routes_with_linkdown off \n[2025-03-06T13:11:25.386643] [NEIGH]dev docker0 lladdr 06:12:ce:f5:dd:ce PERMANENT\n[2025-03-06T13:11:25.386698] [NEIGH]dev docker0 lladdr 06:12:ce:f5:dd:ce PERMANENT\n[2025-03-06T13:11:25.386707] [NEIGH]Deleted dev docker0 lladdr 06:12:ce:f5:dd:ce PERMANENT\n[2025-03-06T13:11:25.386743] [NEIGH]dev docker0 lladdr 06:12:ce:f5:dd:ce PERMANENT\n[2025-03-06T13:11:25.386751] [LINK]9: docker0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default \n    link/ether 06:12:ce:f5:dd:ce brd ff:ff:ff:ff:ff:ff\n[2025-03-06T13:11:25.386798] [ADDR]9: docker0    inet 10.0.20.1/23 brd 10.0.21.255 scope global docker0\n       valid_lft forever preferred_lft forever\n[2025-03-06T13:11:25.386812] [ROUTE]local 10.0.20.1 dev docker0 table local proto kernel scope host src 10.0.20.1 \n[2025-03-06T13:11:25.401864] [NETCONF]inet docker0 rp_filter loose \n[2025-03-06T13:11:25.456839] [LINK]9: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default \n    link/ether 06:12:ce:f5:dd:ce brd ff:ff:ff:ff:ff:ff\n[2025-03-06T13:11:25.457672] [ROUTE]broadcast 10.0.21.255 dev docker0 table local proto kernel scope link src 10.0.20.1 linkdown \n[2025-03-06T13:11:25.457927] [ROUTE]10.0.20.0/23 dev docker0 proto kernel scope link src 10.0.20.1 linkdown \n[2025-03-06T13:11:25.541062] [ADDR]Deleted 9: docker0    inet 10.0.20.1/23 brd 10.0.21.255 scope global docker0\n       valid_lft forever preferred_lft forever\n[2025-03-06T13:11:25.541092] [ROUTE]Deleted 10.0.20.0/23 dev docker0 proto kernel scope link src 10.0.20.1 linkdown \n[2025-03-06T13:11:25.542161] [ROUTE]Deleted broadcast 10.0.21.255 dev docker0 table local proto kernel scope link src 10.0.20.1 linkdown \n[2025-03-06T13:11:25.542348] [ROUTE]Deleted local 10.0.20.1 dev docker0 table local proto kernel scope host src 10.0.20.1 \n[2025-03-06T13:11:25.542455] [NEIGH]Deleted 224.0.0.22 dev docker0 lladdr 01:00:5e:00:00:16 NOARP\n[2025-03-06T13:11:25.548008] [LINK]9: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default \n    link/ether 06:12:ce:f5:dd:ce brd ff:ff:ff:ff:ff:ff\n[2025-03-06T13:11:25.548517] [LINK]9: docker0: <BROADCAST,MULTICAST> mtu 1500 qdisc noqueue state DOWN group default \n    link/ether 06:12:ce:f5:dd:ce brd ff:ff:ff:ff:ff:ff\n```\nthe bits unique to the failed run are:\n```\n[2025-03-06T13:11:25.541062] [ADDR]Deleted 9: docker0    inet 10.0.20.1/23 brd 10.0.21.255 scope global docker0\n       valid_lft forever preferred_lft forever\n[2025-03-06T13:11:25.541092] [ROUTE]Deleted 10.0.20.0/23 dev docker0 proto kernel scope link src 10.0.20.1 linkdown \n[2025-03-06T13:11:25.542161] [ROUTE]Deleted broadcast 10.0.21.255 dev docker0 table local proto kernel scope link src 10.0.20.1 linkdown \n[2025-03-06T13:11:25.542348] [ROUTE]Deleted local 10.0.20.1 dev docker0 table local proto kernel scope host src 10.0.20.1 \n[2025-03-06T13:11:25.542455] [NEIGH]Deleted 224.0.0.22 dev docker0 lladdr 01:00:5e:00:00:16 NOARP\n[2025-03-06T13:11:25.548008] [LINK]9: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default \n    link/ether 06:12:ce:f5:dd:ce brd ff:ff:ff:ff:ff:ff\n[2025-03-06T13:11:25.548517] [LINK]9: docker0: <BROADCAST,MULTICAST> mtu 1500 qdisc noqueue state DOWN group default \n    link/ether 06:12:ce:f5:dd:ce brd ff:ff:ff:ff:ff:ff\n```",
      "solution": "> I got the date for the planned 28.0 update wrong, it's next week. So, I'll try to get the change sorted out.\n\nThe plan to just not-restore the default network (before deleting and re-creating it) didn't work out ... https://github.com/moby/moby/pull/49629#issuecomment-2715157150\n\nIt'll be fixable once we can replace iptables rules with native nftables. The work for that has started and I'm getting chased for a release date, but don't have a proper plan yet.\n\nIs enabling live-restore a usable workaround? Or, any idea what might be conflicting with docker when it deletes and re-creates the bridge? (Another option might be to use a user-defined network rather than the default, but I don't imagine that's an easy change to make.)\n\n---\n\nAhh, rats.  Thank you for all your effort so far.\n\nUnfortunately, live-restore isn't a great option for us, if only because it does too much.  We just want the default bridge to reliably come up with an IP, not to fundamentally change how docker works.  (Leaving containers running and not re-initing networks through a docker stop/start, for example)  And yeah, switching to a user-defined network globally would also be quite challenging.\n\nFWIW, my testing so far points pretty squarely at Crowdstrike Falcon (using eBPF, not a kernel module) as the source.  When unloaded, the problem vanishes, then comes back as soon as it's again loaded.\n\n---\n\n@robmry Just to follow up here, we finally figured out the source of this issue.\n\nSpecifically, using this `bpftrace` script to log the sources of `RTM_DELADDR` events:\n```\n#!/usr/bin/env bpftrace\n\n#include <linux/skbuff.h>\n#include <linux/netlink.h>\n#include <linux/if_addr.h>\n\n/* * Probe the netlink handler for deleting IPv4 addresses.\n * Signature: int inet_rtm_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh, struct netlink_ext_ack *extack)\n */\nkprobe:inet_rtm_deladdr\n{\n    $nlh = (struct nlmsghdr *)arg1;\n    \n    // Calculate pointer to ifaddrmsg (payload follows header)\n    // sizeof(struct nlmsghdr) is usually 16 bytes\n    $ifa = (struct ifaddrmsg *)((uint8 *)$nlh + 16);\n    \n    printf(\"[%s] Process '%s' (PID %d) is deleting an IP address on interface index %d\\n\", \n           strftime(\"%H:%M:%S\", nsecs), comm, pid, $ifa->ifa_index);\n           \n    // Optional: print the kernel stack to see the path\n    print(kstack);\n}\n```\nand reproducing the issue, we could see that `NetworkManager` was deleting the IP.\n\nTelling `NetworkManager` to ignore `docker*` interfaces resolved the issue:\n```\n[main]\nplugins=keyfile\n\n[keyfile]\nunmanaged-devices=interface-name:docker*\n```\nand allows `dockerd` to start consistently without issue.\n\nIt wasn't reproducible on vanilla Ubuntu (Server), because it's using `netplan` rather than `NetworkManager`.  (And in our specific case, we use the Ubuntu Server installer, to install what is effectively Ubuntu Desktop, but it's possible we missed a config you'd naturally get with a native Ubuntu Desktop install, which would explain why more people aren't hitting this issue)",
      "labels": [
        "status/0-triage",
        "kind/bug",
        "area/networking",
        "area/networking/d/bridge",
        "version/28.0"
      ],
      "created_at": "2025-03-06T19:28:03Z",
      "closed_at": "2025-12-18T03:08:04Z",
      "url": "https://github.com/moby/moby/issues/49596",
      "comments_count": 12
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51672,
      "title": "Docker engine v29 encryption on overlay networks broken",
      "problem": "### Description\n\nI have a two node Docker Swarm consisting of two VMs. They are running Debian Trixie with no desktop environment. I use Traefik on the manager node and have Grafana on my worker node. When I recreate everything from scratch with Ansible using Docker engine v 28 everything works fine. Doing a fresh setup with v29, Grafana is unreachable. Grafana also becomes reachable again if I disable encryption. I moved my blog from one node to the other to test this. If the blog stays on the same node as Traefik, it works in all cases. If I move it to the worked node, it becomes unreachable with encryption on.\n\n### Reproduce\n\nToggle between an encrypted traefik_public network and an unencrypted one.\n\n```yaml\nservices:\n  traefik:\n    # The official v3 Traefik docker image\n    image: traefik:latest\n    # Enables the web UI and tells Traefik to listen to docker\n    ports:\n      - \"80:80\"\n      # The Web UI (enabled by --api.insecure=true)\n      - \"8080:8080\"\n    volumes:\n      # So that Traefik can listen to the Docker events\n      - /var/run/docker.sock:/var/run/docker.sock\n    deploy:\n      update_config:\n        delay: 5s\n        order: start-first\n        failure_action: rollback\n      placement:\n        constraints:\n          - 'node.labels.location == manager'\n      restart_policy:\n        delay: 5s\n        max_attempts: 3\n    configs:\n      - source: traefik\n        target: /etc/traefik/traefik.yml\n\n  grafana:\n    image: grafana/grafana:latest\n    volumes:\n      - /srv/grafana:/var/lib/grafana\n    environment:\n      - GF_SECURITY_ADMIN_USER=ccapsuna\n      - GF_SECURITY_ADMIN_PASSWORD__FILE=/run/secrets/grafana_password\n    secrets:\n      - grafana_password\n    deploy:\n      replicas: 1\n      placement:\n        constraints:\n          - 'node.labels.location == observability'\n      restart_policy:\n        delay: 5s\n        max_attempts: 3\n      update_config:\n        delay: 5s\n        order: start-first\n        failure_action: rollback\n      labels:\n        - \"traefik.enable=true\"\n        # To make the dev host rule work from the development machine, it needs to be added to /etc/hosts\n        - \"traefik.http.routers.grafana.rule=Host(`grafana.dev`)\"\n        - \"traefik.http.routers.grafana.entrypoints=web\"\n        - \"traefik.http.services.grafana.loadbalancer.server.port=3000\"\n    configs:\n      - source: grafana_datasources\n        target: /etc/grafana/provisioning/datasources/datasources.yml\n\nsecrets:\n  grafana_password:\n    external: true\n\nconfigs:\n  grafana_datasources:\n    external: true\n  traefik:\n    external: true\n\nnetworks:\n  default:\n    name: traefik_public\n    external: true\n```\n\n### Expected behavior\n\nGrafana login page should pop up.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           29.1.2\n API version:       1.52\n Go version:        go1.25.5\n Git commit:        890dcca\n Built:             Tue Dec  2 21:55:45 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          29.1.2\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.5\n  Git commit:       de45c2a\n  Built:            Tue Dec  2 21:55:45 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v2.2.0\n  GitCommit:        1c4457e00facac03ce1d75f7b6777a7a851e5c41\n runc:\n  Version:          1.3.4\n  GitCommit:        v1.3.4-0-gd6d73eb8\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    29.1.2\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v5.0.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 6\n  Running: 6\n  Paused: 0\n  Stopped: 0\n Images: 6\n Server Version: 29.1.2\n Storage Driver: overlayfs\n  driver-type: io.containerd.snapshotter.v1\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: active\n  NodeID: vb15hhef13nd4jg3e2eulfzkl\n  Is Manager: true\n  ClusterID: srlt9zn8bhbdeor6w9j5dttsc\n  Managers: 1\n  Nodes: 2\n  Default Address Pool: 10.0.0.0/8  \n  SubnetSize: 24\n  Data Path Port: 4789\n  Orchestration:\n   Task History Retention Limit: 5\n  Raft:\n   Snapshot Interval: 10000\n   Number of Old Snapshots to Retain: 0\n   Heartbeat Tick: 1\n   Election Tick: 10\n  Dispatcher:\n   Heartbeat Period: 5 seconds\n  CA Configuration:\n   Expiry Duration: 3 months\n   Force Rotate: 0\n  Autolock Managers: false\n  Root Rotation In Progress: false\n  Node Address: 192.168.100.139\n  Manager Addresses:\n   192.168.100.139:2377\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 1c4457e00facac03ce1d75f7b6777a7a851e5c41\n runc version: v1.3.4-0-gd6d73eb8\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.12.48+deb13-amd64\n Operating System: Debian GNU/Linux 13 (trixie)\n OSType: linux\n Architecture: x86_64\n CPUs: 4\n Total Memory: 7.742GiB\n Name: rp1vm\n ID: eb8a8f02-0bf1-444c-be4e-fbb48b17b1ca\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Username: cristiancapsuna\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\n\n### Additional Info\n\n_No response_",
      "solution": "> When I recreate everything from scratch with Ansible using Docker engine v 28 everything works fine. Doing a fresh setup with v29, Grafana is unreachable.\n\n\nI see the network used is marked as \"external\";\n\n\n```yaml\nnetworks:\n  default:\n    name: traefik_public\n    external: true\n```\n\nif that's the network causing issues, do you have details / steps how the network is created? Is it possible to provide a minimal-as-possible reproducer with exact steps? It looks like the compose-file posted depends on various external bits (configs, secrets), which may not be relevant to the issue itself, so if there's a more minimal reproducer, that would help for maintainers to reproduce / narrow down the issue.\n\n\n---\n\nI wrote the bellow to test:\n\n```yaml\nservices:\n  debug1:\n    image: nicolaka/netshoot:latest\n    command: sleep infinity\n    deploy:\n      placement:\n        constraints:\n          - 'node.labels.location == manager'\n  debug2:\n    image: nicolaka/netshoot:latest \n    command: sleep infinity\n    deploy:\n      placement:\n        constraints:\n          - 'node.labels.location == observability'\nnetworks:\n  default:\n    name: encryption_test\n    driver: overlay\n    driver_opts:\n      encrypted: \"true\"\n```\n\nBut with this I just realised that someone fixed this in the 29.1.3 release. Therefor I am closing this issue.",
      "labels": [
        "status/0-triage",
        "kind/bug",
        "area/networking",
        "area/networking/d/overlay",
        "version/29.1"
      ],
      "created_at": "2025-12-06T21:05:23Z",
      "closed_at": "2025-12-13T21:50:23Z",
      "url": "https://github.com/moby/moby/issues/51672",
      "comments_count": 2
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51641,
      "title": "`Dockerfile.simple`: `go.mod file not found in current directory or any parent directory`",
      "problem": "### Description\n\nhttps://github.com/moby/moby/blob/docker-v29.1.1/Dockerfile.simple is not buildable\n\n### Reproduce\n\n```\ngit clone https://github.com/docker/docker\ncd docker\ngit checkout docker-v29.1.1\ndocker build -t docker:simple -f Dockerfile.simple .\n```\n\n```\n42.96 Install docker/cli version 18.06.3-ce from stable\n42.96 ++ uname -m\n42.96 + arch=aarch64\n42.96 + '[' aarch64 '!=' x86_64 ']'\n42.96 + '[' aarch64 '!=' s390x ']'\n42.96 + '[' aarch64 '!=' armhf ']'\n42.96 + build_dockercli\n42.96 + git clone https://github.com/docker/docker-ce /tmp/tmp.ETZeqKEAe2/tmp/docker-ce\n42.96 Cloning into '/tmp/tmp.ETZeqKEAe2/tmp/docker-ce'...\n58.77 + cd /tmp/tmp.ETZeqKEAe2/tmp/docker-ce\n58.77 + git checkout -q v18.06.3-ce\n59.28 + mkdir -p /tmp/tmp.ETZeqKEAe2/src/github.com/docker\n59.28 + mv components/cli /tmp/tmp.ETZeqKEAe2/src/github.com/docker/cli\n59.28 + go build -buildmode=pie -o /usr/local/bin/docker github.com/docker/cli/cmd/docker\n59.28 no required module provides package github.com/docker/cli/cmd/docker: go.mod file not found in current directory or any parent directory; see 'go help modules'\n------\n\n 1 warning found (use docker --debug to expand):\n - LegacyKeyValueFormat: \"ENV key=value\" should be used instead of legacy \"ENV key value\" format (line 45)\nDockerfile.simple:40\n--------------------\n  39 |     COPY hack/dockerfile/install hack/dockerfile/install\n  40 | >>> RUN set -e; for i in runc containerd tini dockercli; \\\n  41 | >>> \t\tdo hack/dockerfile/install/install.sh $i; \\\n  42 | >>> \tdone\n  43 |     ENV PATH=/usr/local/cli:$PATH\n--------------------\nERROR: failed to build: failed to solve: process \"/bin/sh -c set -e; for i in runc containerd tini dockercli; \\t\\tdo hack/dockerfile/install/install.sh $i; \\tdone\" did not complete successfully: exit code: 1\n```\n\n### Expected behavior\n\nIt should be buildable\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           29.1.1\n API version:       1.52\n Go version:        go1.25.4\n Git commit:        0aedba5\n Built:             Fri Nov 28 11:33:32 2025\n OS/Arch:           linux/arm64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          29.1.1\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.4\n  Git commit:       9a84135\n  Built:            Fri Nov 28 11:33:32 2025\n  OS/Arch:          linux/arm64\n  Experimental:     false\n containerd:\n  Version:          v2.2.0\n  GitCommit:        1c4457e00facac03ce1d75f7b6777a7a851e5c41\n runc:\n  Version:          1.3.4\n  GitCommit:        v1.3.4-0-gd6d73eb8\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    29.1.1\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.3\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n  model: Docker Model Runner (Docker Inc.)\n    Version:  v1.0.2\n    Path:     /usr/libexec/docker/cli-plugins/docker-model\n\nServer:\n Containers: 0\n  Running: 0\n  Paused: 0\n  Stopped: 0\n Images: 0\n Server Version: 29.1.1\n Storage Driver: overlayfs\n  driver-type: io.containerd.snapshotter.v1\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 1c4457e00facac03ce1d75f7b6777a7a851e5c41\n runc version: v1.3.4-0-gd6d73eb8\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.8.0-88-generic\n Operating System: Ubuntu 24.04.3 LTS\n OSType: linux\n Architecture: aarch64\n CPUs: 4\n Total Memory: 3.813GiB\n Name: lima-docker-rootful\n ID: 0a1b74ff-d065-43c4-9826-bd78bdc49d0e\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\n\n### Additional Info\n\n_No response_",
      "solution": "Being fixed in:\n- https://github.com/moby/moby/pull/51640",
      "labels": [
        "status/0-triage",
        "kind/bug"
      ],
      "created_at": "2025-12-02T08:13:35Z",
      "closed_at": "2025-12-11T20:08:16Z",
      "url": "https://github.com/moby/moby/issues/51641",
      "comments_count": 1
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 47434,
      "title": "Error message \"raft message is too large and can't be sent\" when trying to remove a node from a Docker swarm cluster",
      "problem": "### Description\n\nI have a cluster of 3 nodes (See them below). I am unable to remove the node that shows as down and it's breaking my cluster.\r\n\r\n```\r\nID                            HOSTNAME                               STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION\r\npcefq43rapf8mw8887inkztvi *   <CENSORED>llbprmmid01.<CENSORED>.net   Ready     Active         Reachable        25.0.3\r\nxl8kfa7y56uu3vz1xsw0lxb61     <CENSORED>llbprmmid02.<CENSORED>.net   Ready     Active         Reachable        25.0.3\r\nd31t8u7reuky3kjyym84smsl4     <CENSORED>llbprmmid03.<CENSORED>.net   Ready     Active         Leader           25.0.3\r\nhdresbrj592dpjlh80gwsuy9h     <CENSORED>llbprmmid03.<CENSORED>.net   Down      Drain                           25.0.2\r\n```\r\nWhen I do a `docker node ls | wc -l` it returns\r\n```\r\n5\r\n```\r\n\r\nI found out that there was a similar issue reported before, https://forums.docker.com/t/removing-node-from-the-swarm-issue-raft-message-is-too-large-and-cant-be-sent/139518, I went through it, tried what they advise, but still no luck.\r\n\r\nAny idea how I can fix this? This actually broke a production environment!\r\n\n\n### Reproduce\n\n1. docker node rm -f hdresbrj592dpjlh80gwsuy9h\n\n### Expected behavior\n\ndocker node rm should remove the node that I want removed from the cluster without issues.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           25.0.3\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        4debf41\r\n Built:             Tue Feb  6 21:15:16 2024\r\n OS/Arch:           linux/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.3\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       f417435\r\n  Built:            Tue Feb  6 21:14:12 2024\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.28\r\n  GitCommit:        ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        v1.1.12-0-g51d5e94\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    25.0.3\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1\r\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.5\r\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.23.0\r\n    Path:     /usr/libexec/docker/cli-plugins/docker-scan\r\n\r\nServer:\r\n Containers: 4\r\n  Running: 4\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 5\r\n Server Version: 25.0.3\r\n Storage Driver: overlay2\r\n  Backing Filesystem: xfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: active\r\n  NodeID: pcefq43rapf8mw8887inkztvi\r\n  Is Manager: true\r\n  ClusterID: nixy9iaupmii3yn6uidkw2l10\r\n  Managers: 3\r\n  Nodes: 4\r\n  Default Address Pool: 10.0.0.0/8  \r\n  SubnetSize: 24\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 3\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: true\r\n  Node Address: 192.168.32.110\r\n  Manager Addresses:\r\n   192.168.32.110:2377\r\n   192.168.32.111:2377\r\n   192.168.32.112:2377\r\n Runtimes: runc io.containerd.runc.v2\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc version: v1.1.12-0-g51d5e94\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 4.18.0-477.21.1.el8_8.x86_64\r\n Operating System: Red Hat Enterprise Linux 8.8 (Ootpa)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 15.39GiB\r\n Name: <CENSORED>llbprmmid01.<CENSORED>.net\r\n ID: a7918f44-224e-45dd-abf4-3c95d61e0f6f\r\n Docker Root Dir: /u02/docker\r\n Debug Mode: false\r\n HTTP Proxy: <CENSORED>\r\n HTTPS Proxy: <CENSORED>\r\n No Proxy: <CENSORED>\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\nBelow is a snippet of the the logs, from `journalctl -u docker.service -f`, after enabling debug and trying to remove node3 from node1\r\n\r\n```\r\nFeb 22 18:10:53 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:10:53.346690739Z\" level=debug msg=\"sending heartbeat to manager { } with timeout 5s\" method=\"(*session).heartbeat\" module=node/agent node.id=pcefq43rapf8mw8887inkztvi session.id=72z12be65dn88tkuxtckrxxmj sessionID=72z12be65dn88tkuxtckrxxmj\r\nFeb 22 18:10:53 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:10:53.347704874Z\" level=debug msg=\"heartbeat successful to manager { }, next heartbeat period: 5.305433283s\" method=\"(*session).heartbeat\" module=node/agent node.id=pcefq43rapf8mw8887inkztvi session.id=72z12be65dn88tkuxtckrxxmj sessionID=72z12be65dn88tkuxtckrxxmj\r\nFeb 22 18:10:56 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:10:56.465861670Z\" level=debug msg=\"Calling HEAD /_ping\"\r\nFeb 22 18:10:56 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:10:56.466267559Z\" level=debug msg=\"Calling DELETE /v1.44/nodes/hdresbrj592dpjlh80gwsuy9h?force=1\"\r\nFeb 22 18:10:56 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:10:56.806858949Z\" level=debug msg=\"error handling rpc\" error=\"rpc error: code = Unknown desc = raft: raft message is too large and can't be sent\" rpc=/docker.swarmkit.v1.Control/RemoveNode\r\nFeb 22 18:10:56 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:10:56.807027288Z\" level=debug msg=\"Error removing node\" error=\"rpc error: code = Unknown desc = raft: raft message is too large and can't be sent\" node-id=hdresbrj592dpjlh80gwsuy9h\r\nFeb 22 18:10:56 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:10:56.807091867Z\" level=debug msg=\"FIXME: Got an API for which error does not match any expected type!!!\" error=\"rpc error: code = Unknown desc = raft: raft message is too large and can't be sent\" error_type=\"*status.Error\" module=api\r\nFeb 22 18:10:56 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:10:56.807104059Z\" level=error msg=\"Handler for DELETE /v1.44/nodes/hdresbrj592dpjlh80gwsuy9h returned error: rpc error: code = Unknown desc = raft: raft message is too large and can't be sent\"\r\nFeb 22 18:10:56 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:10:56.807116950Z\" level=debug msg=\"FIXME: Got an API for which error does not match any expected type!!!\" error=\"rpc error: code = Unknown desc = raft: raft message is too large and can't be sent\" error_type=\"*status.Error\" module=api\r\nFeb 22 18:10:58 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:10:58.653777697Z\" level=debug msg=\"sending heartbeat to manager { } with timeout 5s\" method=\"(*session).heartbeat\" module=node/agent node.id=pcefq43rapf8mw8887inkztvi session.id=72z12be65dn88tkuxtckrxxmj sessionID=72z12be65dn88tkuxtckrxxmj\r\nFeb 22 18:10:58 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:10:58.654635872Z\" level=debug msg=\"heartbeat successful to manager { }, next heartbeat period: 5.339470303s\" method=\"(*session).heartbeat\" module=node/agent node.id=pcefq43rapf8mw8887inkztvi session.id=72z12be65dn88tkuxtckrxxmj sessionID=72z12be65dn88tkuxtckrxxmj\r\nFeb 22 18:11:03 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:11:03.994666787Z\" level=debug msg=\"sending heartbeat to manager { } with timeout 5s\" method=\"(*session).heartbeat\" module=node/agent node.id=pcefq43rapf8mw8887inkztvi session.id=72z12be65dn88tkuxtckrxxmj sessionID=72z12be65dn88tkuxtckrxxmj\r\nFeb 22 18:11:03 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:11:03.995801446Z\" level=debug msg=\"heartbeat successful to manager { }, next heartbeat period: 4.774354673s\" method=\"(*session).heartbeat\" module=node/agent node.id=pcefq43rapf8mw8887inkztvi session.id=72z12be65dn88tkuxtckrxxmj sessionID=72z12be65dn88tkuxtckrxxmj\r\nFeb 22 18:11:05 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:11:05.980417978Z\" level=debug msg=\"memberlist: Stream connection from=192.168.32.112:57678\"\r\nFeb 22 18:11:05 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:11:05.980569597Z\" level=debug msg=\"<CENSORED>llbprmmid01.<CENSORED>.net(54699a50f58e): Initiating  bulk sync for networks [lp9jegtxlrr1ojaijz43kr233] with node 2d3fae1bf26e\"\r\nFeb 22 18:11:07 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:11:07.057352324Z\" level=debug msg=\"memberlist: Stream connection from=192.168.32.111:51210\"\r\nFeb 22 18:11:07 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:11:07.057502628Z\" level=debug msg=\"<CENSORED>llbprmmid01.<CENSORED>.net(54699a50f58e): Initiating  bulk sync for networks [lp9jegtxlrr1ojaijz43kr233] with node 67d13d52bcbe\"\r\nFeb 22 18:11:08 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:11:08.770571281Z\" level=debug msg=\"sending heartbeat to manager { } with timeout 5s\" method=\"(*session).heartbeat\" module=node/agent node.id=pcefq43rapf8mw8887inkztvi session.id=72z12be65dn88tkuxtckrxxmj sessionID=72z12be65dn88tkuxtckrxxmj\r\nFeb 22 18:11:08 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:11:08.771434267Z\" level=debug msg=\"heartbeat successful to manager { }, next heartbeat period: 5.469749527s\" method=\"(*session).heartbeat\" module=node/agent node.id=pcefq43rapf8mw8887inkztvi session.id=72z12be65dn88tkuxtckrxxmj sessionID=72z12be65dn88tkuxtckrxxmj\r\nFeb 22 18:11:09 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:11:09.545754661Z\" level=debug msg=\"memberlist: Stream connection from=192.168.32.112:42024\"\r\n```",
      "solution": "Update: Today I tried to remove the node again, and it worked... It'd be great though to understand what happened. Because I've had this same issue before, and it got resolved on its own after I tried to remove the node again a couple of days later",
      "labels": [
        "status/0-triage",
        "kind/bug",
        "area/swarm",
        "version/25.0"
      ],
      "created_at": "2024-02-22T18:39:05Z",
      "closed_at": "2025-12-11T16:32:40Z",
      "url": "https://github.com/moby/moby/issues/47434",
      "comments_count": 3
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51544,
      "title": "Docker 29 breaks policy routing",
      "problem": "### Description\n\nAfter updating to Docker 29 my policy routing between my containers break. Containers do not reach each other.\n\nI\u2019ve two containers:\n\nContainer `A` in subnet: `10.250.250.0/30`\nContainer `B` in subnet: `10.250.200.8/29`\n\nEvery docker network is a user-defined bridge.\n\nI\u2019ve the following ip rule settings:\n```\n$ ip ru s\n110:    from 10.250.250.0/24 to 172.25.55.0/24 lookup main proto static\n111:    from 10.250.250.0/24 to 10.250.250.0/24 lookup main proto static\n150:    from 10.250.250.0/24 lookup 11 proto static\n```\nIn the table 11 i\u2019ve only one default route:\n```\n$ ip r show table 11\ndefault dev wg1 scope link\n```\nEvery traffic from `B` goes through this interface. Except when I\u2019d like to manage it from `172.25.55.0/24` and if an other container (like `A`) wants to reach it.\nAll traffic arriving from other containers appears in container `A` as if it were arriving from its own default gateway. This is why the `ip rule 111` points to itself.\n\nIf I downgrade back to Docker 28 everything works fine!\n\n### Reproduce\n\nMy production system runs on Fedora 43, but the issue can be reproduced on Debian as well:\n1. Set up Docker's `apt` repository based on the [official description](https://docs.docker.com/engine/install/debian/#install-using-the-repository)\n2. Install the latest version of Docker\n```\nsudo apt update && \\\nsudo apt install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n```\n3. Create `docker-compose.yaml`\n```\ncat << EOF > docker-compose.yaml\n---\nservices:\n  containerA:\n    image: nginx\n    container_name: containerA\n    ports:\n      - \"80:80/tcp\"\n    networks:\n      - a-net\n  containerB:\n    image: nginx\n    container_name: containerB\n    ports:\n      - \"81:80/tcp\"\n    networks:\n      - b-net\nnetworks:\n  a-net:\n    driver: bridge\n    ipam:\n      driver: default\n      config:\n        - subnet: \"10.250.250.0/30\"\n          gateway: \"10.250.250.1\"\n  b-net:\n    driver: bridge\n    ipam:\n      driver: default\n      config:\n        - subnet: \"10.250.200.8/29\"\n          gateway: \"10.250.200.9\"\nEOF\n```\n4. Create routing entries (change `172.25.55.0/24` to your host network or skip that line, because that's not necessary for reproduction)\n```\nsudo ip route add default via 127.0.0.1 dev lo table 11 && \\\nsudo ip rule add from 10.250.250.0/24 to 172.25.55.0/24 lookup 254 priority 110 && \\\nsudo ip rule add from 10.250.250.0/24 to 10.250.250.0/24 lookup 254 priority 111 && \\\nsudo ip rule add from 10.250.250.0/24 lookup 11 priority 150\n```\n5. Start the containers: `sudo docker compose up -d`\n6. Test the connection (**failed**): `sudo docker exec -it containerB curl 192.168.122.50:80`\n7. Cleanup: `sudo docker compose down`\n8. Rollback to previous Docker version (`28.5.2`)\n```\nsudo apt remove -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin && \\\nVERSION_STRING=5:28.5.2-1~debian.13~trixie && \\\nsudo apt install docker-ce=$VERSION_STRING docker-ce-cli=$VERSION_STRING containerd.io docker-buildx-plugin docker-compose-plugin\n```\n9. Start the containers: `sudo docker compose up -d`\n10. Test the connection (**success**): `sudo docker exec -it containerB curl 192.168.122.50:80`\n11. Cleanup: `sudo docker compose down`\n\n### Expected behavior\n\nThe connection test works with the latest version of Docker in point 6.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           29.0.1\n API version:       1.52\n Go version:        go1.25.4\n Git commit:        eedd969\n Built:             Fri Nov 14 16:20:43 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          29.0.1\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.4\n  Git commit:       198b5e3\n  Built:            Fri Nov 14 16:17:32 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v2.1.5\n  GitCommit:        fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc:\n  Version:          1.3.3\n  GitCommit:        v1.3.3-0-gd842d771\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    29.0.1\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.3\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 9\n  Running: 9\n  Paused: 0\n  Stopped: 0\n Images: 8\n Server Version: 29.0.1\n Storage Driver: overlayfs\n  driver-type: io.containerd.snapshotter.v1\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc version: v1.3.3-0-gd842d771\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.17.7-300.fc43.x86_64\n Operating System: Fedora Linux 43 (Server Edition)\n OSType: linux\n Architecture: x86_64\n CPUs: 8\n Total Memory: 62.48GiB\n Name: p.kinit.hu\n ID: 1e8e810e-ad91-4f89-8084-5e07fe058be9\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables+firewalld\n```\n\n### Additional Info\n\nOutput of `docker version` on Debian after the failed test in the Reproduce section:\n```\n$ sudo docker version\nClient: Docker Engine - Community\n Version:           29.0.1\n API version:       1.52\n Go version:        go1.25.4\n Git commit:        eedd969\n Built:             Fri Nov 14 16:18:05 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          29.0.1\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.4\n  Git commit:       198b5e3\n  Built:            Fri Nov 14 16:18:05 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v2.1.5\n  GitCommit:        fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc:\n  Version:          1.3.3\n  GitCommit:        v1.3.3-0-gd842d771\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\nOutput of `docker info` on Debian after the failed test in the Reproduce section:\n```\n$ sudo docker info\nClient: Docker Engine - Community\n Version:    29.0.1\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.3\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 2\n  Running: 2\n  Paused: 0\n  Stopped: 0\n Images: 1\n Server Version: 29.0.1\n Storage Driver: overlayfs\n  driver-type: io.containerd.snapshotter.v1\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc version: v1.3.3-0-gd842d771\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.12.57+deb13-amd64\n Operating System: Debian GNU/Linux 13 (trixie)\n OSType: linux\n Architecture: x86_64\n CPUs: 4\n Total Memory: 3.829GiB\n Name: debianinit\n ID: 3dbdd091-cae6-4e06-a435-80fac099e0eb\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  127.0.0.0/8\n  ::1/128\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\nOutput of `docker version` on Debian after the succeeded test in the Reproduce section:\n```\n$ sudo docker version\nClient: Docker Engine - Community\n Version:           28.5.2\n API version:       1.51\n Go version:        go1.25.3\n Git commit:        ecc6942\n Built:             Wed Nov  5 14:43:33 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          28.5.2\n  API version:      1.51 (minimum version 1.24)\n  Go version:       go1.25.3\n  Git commit:       89c5e8f\n  Built:            Wed Nov  5 14:43:33 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v2.1.5\n  GitCommit:        fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc:\n  Version:          1.3.3\n  GitCommit:        v1.3.3-0-gd842d771\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\nOutput of `docker info` on Debian after the succeeded test in the Reproduce section:\n```\n$ sudo docker info\nClient: Docker Engine - Community\n Version:    28.5.2\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.3\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 2\n  Running: 2\n  Paused: 0\n  Stopped: 0\n Images: 1\n Server Version: 28.5.2\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: runc io.containerd.runc.v2\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc version: v1.3.3-0-gd842d771\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.12.57+deb13-amd64\n Operating System: Debian GNU/Linux 13 (trixie)\n OSType: linux\n Architecture: x86_64\n CPUs: 4\n Total Memory: 3.829GiB\n Name: debianinit\n ID: 3dbdd091-cae6-4e06-a435-80fac099e0eb\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n```\n",
      "solution": "Hi! I just want to kindly ask if anyone has been able to look into the problem?\n\n---\n\n> Hi! I just want to kindly ask if anyone has been able to look into the problem?\n\nI haven't. When I tried to repro before, I guess I failed - because for me it worked with the change to the \"priority 111\" rule.\n\nIf you can find out any more about where packets are being dropped / not routed, that'd be great. For example, by looking at counters in the iptables rules (`-S` doesn't show counters, `-nvL` does - and you'll need to look at the rules after reproducing the failure). Or, tcpdumps to look for changes. [Retis](https://retis.readthedocs.io/en/stable/) can also be good for tracking down this sort of thing.\n\n---\n\nCan you provide me the steps you reproduced the issue?\nI find it hard to imagine that in a completely fresh virtual environment, with a freshly installed OS and Docker, I would get a different result than you.\n\nI have tried several ways to check where Docker might be messing up packet routing (tcpdump, iptables, etc.), but without success. That's why I opened this ticket, because I got stuck in my investigation. At the code level, I can't figure out what has changed.",
      "labels": [
        "area/networking",
        "version/29.0"
      ],
      "created_at": "2025-11-16T12:58:20Z",
      "closed_at": "2025-12-11T13:11:08Z",
      "url": "https://github.com/moby/moby/issues/51544",
      "comments_count": 20
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51620,
      "title": "panic: \"assignment to entry in nil map\" in daemon/network.go during start with PublishAllPorts=true",
      "problem": "### Description\n\nI encountered a hard crash (panic) in the Docker Daemon `dockerd` when starting a container. The crash occurs specifically when a container is configured with `PublishAllPorts: true` (equivalent to `-P`) but `PortBindings` is passed as `null` in the JSON payload.\n\nThis was discovered using the Rust `testcontainers` library (which creates the container via API with `PortBindings: null`), but it implies a regression in how the daemon handles nil maps during the start sequence. The CLI works fine (likely because it initializes the map as empty `{}` rather than `null`).\n\nThe API connection is severed (`EOF`), and the `dockerd` process crashes/restarts.\n\n\n### Reproduce\n\n```\n#!/bin/bash\ndocker pull redis:latest > /dev/null 2>&1\n\n# 1. Create Container (PortBindings: null)\nID=$(curl -s --unix-socket /var/run/docker.sock \\\n  -H \"Content-Type: application/json\" \\\n  -X POST http://localhost/v1.52/containers/create \\\n  -d '{\n    \"Image\": \"redis:latest\",\n    \"ExposedPorts\": { \"6379/tcp\": {} },\n    \"HostConfig\": { \"PublishAllPorts\": true, \"PortBindings\": null }\n  }' | jq -r .Id)\n\necho \"Created Container: $ID\"\n\ncurl -v --unix-socket /var/run/docker.sock \\\n  -X POST \"http://localhost/v1.52/containers/$ID/start\"\n```\nRun this script and then check `journalctl -u docker -n 50`, you will see the panic.\n\n### Expected behavior\n\nAt the very least it should not cause the daemon to panic. This did NOT panic prior to 29.x.\nThe daemon should handle `PortBindings: null` gracefully (treating it as an empty map) when `PublishAllPorts` is true.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           29.1.1\n API version:       1.52\n Go version:        go1.25.4\n Git commit:        0aedba5\n Built:             Fri Nov 28 11:36:27 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          29.1.1\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.4\n  Git commit:       9a84135\n  Built:            Fri Nov 28 11:33:12 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v2.2.0\n  GitCommit:        1c4457e00facac03ce1d75f7b6777a7a851e5c41\n runc:\n  Version:          1.3.4\n  GitCommit:        v1.3.4-0-gd6d73eb8\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    29.1.1\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.3\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 5\n  Running: 0\n  Paused: 0\n  Stopped: 5\n Images: 4\n Server Version: 29.1.1\n Storage Driver: overlay2\n  Backing Filesystem: btrfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 1c4457e00facac03ce1d75f7b6777a7a851e5c41\n runc version: v1.3.4-0-gd6d73eb8\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.17.8-300.fc43.x86_64\n Operating System: Fedora Linux 43 (Workstation Edition)\n OSType: linux\n Architecture: x86_64\n CPUs: 16\n Total Memory: 31.26GiB\n Name: fedora\n ID: ce3fad9b-7915-4ad6-b531-d5081f3b1d23\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables+firewalld\n```\n\n### Additional Info\n\n#### Log Output\n\n```\nNov 29 02:19:09 fedora dockerd[60398]: 2025/11/29 02:19:09 http: panic serving @: assignment to entry in nil map\nNov 29 02:19:09 fedora dockerd[60398]: goroutine 884 [running]:\nNov 29 02:19:09 fedora dockerd[60398]: net/http.(*conn).serve.func1()\nNov 29 02:19:09 fedora dockerd[60398]:          /usr/local/go/src/net/http/server.go:1943 +0xd3\nNov 29 02:19:09 fedora dockerd[60398]: panic({0x55e6e768bd00?, 0x55e6e92afda0?})\nNov 29 02:19:09 fedora dockerd[60398]:          /usr/local/go/src/runtime/panic.go:783 +0x132\nNov 29 02:19:09 fedora dockerd[60398]: [github.com/moby/moby/v2/daemon.buildPortsRelatedCreateEndpointOptions(0xc000ad6008](https://github.com/moby/moby/v2/daemon.buildPortsRelatedCreateEndpointOptions(0xc000ad6008), 0x55e6e7b90120?, 0xc000690780)\nNov 29 02:19:09 fedora dockerd[60398]:          /root/rpmbuild/BUILD/docker-ce-29.1.1-build/src/engine/daemon/network.go:1047 +0xae6\nNov 29 02:19:09 fedora dockerd[60398]: [github.com/moby/moby/v2/daemon.buildCreateEndpointOptions(0xc000ad6008](https://github.com/moby/moby/v2/daemon.buildCreateEndpointOptions(0xc000ad6008), 0xc00079f1e0, 0xc0013b78f0, 0xc000690780, {0x0, 0x0, 0x0?})\nNov 29 02:19:09 fedora dockerd[60398]:          /root/rpmbuild/BUILD/docker-ce-29.1.1-build/src/engine/daemon/network.go:988 +0x2bf\nNov 29 02:19:09 fedora dockerd[60398]: [github.com/moby/moby/v2/daemon.(*Daemon).connectToNetwork(0xc0007d0a08](https://github.com/moby/moby/v2/daemon.(*Daemon).connectToNetwork(0xc0007d0a08), {0x55e6e7bf0538, 0xc0011706f0}, 0xc0007e0008, 0xc000ad6008, {0x55e6e6e694cd, 0x6}, 0xc0013b78f0)\nNov 29 02:19:09 fedora dockerd[60398]:          /root/rpmbuild/BUILD/docker-ce-29.1.1-build/src/engine/daemon/container_operations.go:738 +0x96e\n```\n\nRelated issue in testcontainers-rs https://github.com/testcontainers/testcontainers-rs/issues/884",
      "solution": "I definitely get the panic after upgrading to 4.54 and do not get it with 4.53. So it appears 4.54 is not using docker 29.1.2 or it does not fix the issue or I'm encountering some other problem.\nThese are the containers I had running before upgrade.\n```\njamshid@Mac apptron % docker version\nClient:\n Version:           29.0.1\n API version:       1.52\n Go version:        go1.25.4\n Git commit:        eedd969\n Built:             Fri Nov 14 16:16:57 2025\n OS/Arch:           darwin/arm64\n Context:           desktop-linux\n\nServer: Docker Desktop 4.53.0 (211793)\n Engine:\n  Version:          29.0.1\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.4\n  Git commit:       198b5e3\n  Built:            Fri Nov 14 16:18:20 2025\n  OS/Arch:          linux/arm64\n  Experimental:     false\n containerd:\n  Version:          v2.1.5\n  GitCommit:        fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc:\n  Version:          1.3.3\n  GitCommit:        v1.3.3-0-gd842d771\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n\njamshid@Mac apptron % docker ps\nCONTAINER ID   IMAGE                             COMMAND                  CREATED         STATUS                 PORTS                              NAMES\n2cc241de6056   cloudflare-dev/session:7ac0c1b7   \"/worker\"                2 days ago      Up 2 hours             0.0.0.0:55000->8080/tcp            workerd-apptron-Session-f1b7af178f23e58fd63d0af5ac539902f77bb572aa32ddad0ffe702bb386643f\n9665f4879bb7   buildenv-artifactory:6.23.13      \"/entrypoint-artifac\u2026\"   20 months ago   Up 2 hours             0.0.0.0:8081-8082->8081-8082/tcp   buildenv-artifactory-1\n18bab0d0f548   buildenv-aptcacherng:debian11     \"/sbin/entrypoint.sh\"    20 months ago   Up 2 hours (healthy)   0.0.0.0:3142->3142/tcp             buildenv-aptcacherng-1\n302d820a9941   ubuntu/squid:4.10-20.04_beta      \"entrypoint.sh -f /e\u2026\"   20 months ago   Up 2 hours             0.0.0.0:8123->3128/tcp             buildenv-httpcache-1\na42265a34409   langrisha/npm-lazy:1.11.0         \"node index.js --sho\u2026\"   20 months ago   Up 2 hours             8080/tcp, 0.0.0.0:8181->8181/tcp   buildenv-npmlazy-1\n```\n\nHere is the panic stack trace with 4.54. I pressed `Gather diagnostics` but it's been spinning for 20m now.\n\nAn unexpected error occurred\nDocker Desktop encountered an unexpected error and needs to close.\n\nSearch our [troubleshooting documentation](https://docs.docker.com/desktop/troubleshoot/overview/?utm_source=docker_desktop_error_dialog) to find a solution or workaround. Alternatively, you can gather a diagnostics report and submit a support request or GitHub issue.\n```\nservice dockerd failed: panic detected in dockerd: panic: assignment to entry in nil map\n\ngoroutine 498 [running]:\n\ngithub.com/moby/moby/v2/daemon.buildPortsRelatedCreateEndpointOptions(0x400042f348, 0xaaaabcc8f458?, 0x40006feb40)\n\t/root/build-deb/engine/daemon/network.go:1047 +0x844\ngithub.com/moby/moby/v2/daemon.buildCreateEndpointOptions(0x400042f348, 0x4001015040, 0x400027d320, 0x40006feb40, {0x0, 0x0, 0x4001506cb8?})\n\t/root/build-deb/engine/daemon/network.go:988 +0x20c\ngithub.com/moby/moby/v2/daemon.(*Daemon).connectToNetwork(0x4000898008, {0xaaaabe21d9f8, 0x4000f12b10}, 0x400089a008, 0x400042f348, {0x400077a9f0, 0x6}, 0x400027d320)\n\t/root/build-deb/engine/daemon/container_operations.go:738 +0x66c\ngithub.com/moby/moby/v2/daemon.(*Daemon).allocateNetwork(0x4000898008, {0xaaaabe21d9f8, 0x4000f12b10}, 0x400089a008, 0x400042f348)\n\t/root/build-deb/engine/daemon/container_operations.go:421 +0x298\ngithub.com/moby/moby/v2/daemon.(*Daemon).initializeCreatedTask(0x4000898008, {0xaaaabe21d9f8, 0x4000f12b10}, 0x400089a008, {0xaaaabe23dc60, 0x4000eb21c8}, 0x400042f348, 0xaaaabd4db3df?)\n\t/root/build-deb/engine/daemon/start_linux.go:37 +0x260\ngithub.com/moby/moby/v2/daemon.(*Daemon).containerStart(0x4000898008, {0xaaaabe21d9c0, 0xaaaabfa05300}, 0x400089a008, 0x400042f348, {0x0, 0x0}, {0x0, 0x0}, 0x1)\n\t/root/build-deb/engine/daemon/start.go:242 +0xba8\ngithub.com/moby/moby/v2/daemon.(*Daemon).restore.func4(0x400042f348, 0x400117f1f0)\n\t/root/build-deb/engine/daemon/daemon.go:633 +0x308\ncreated by github.com/moby/moby/v2/daemon.(*Daemon).restore in goroutine 1\n\t/root/build-deb/engine/daemon/daemon.go:607 +0x5ec\n```\n\n---\n\n> I definitely get the panic after upgrading to 4.54 and do not get it with 4.53. So it appears 4.54 is not using docker 29.1.2 or it does not fix the issue or I'm encountering some other problem.\n\nInteresting; let me try find which one it is; I\"m running a nightly build myself currently, so couldn't double check. If it's using 29.1.2, then either the patch didn't work, or there's another issue elsewhere. I don't think we received reports from non-desktop users after we did the 29.1.2 release, so that's what made me curious.\n\n\n\n> Finally got a diagnostics ID:\n\nThank you! Hopefully it contains more information that could help us find the issue.\n\n\n\n---\n\nOh! The stack may already show the problem; looks like there's a different code-path;\n\n```\ngithub.com/moby/moby/v2/daemon.buildPortsRelatedCreateEndpointOptions(0x400042f348, 0xaaaabcc8f458?, 0x40006feb40)\n\t/root/build-deb/engine/daemon/network.go:1047 +0x844\n```\n\n\nThat code doesn't check for a nil map; the `maps.Clone` is somewhat hiding that, but that function returns `nil` if the map that's being cloned is nil; \n\nhttps://github.com/moby/moby/blob/c64b781df2387bea01720a3a3411f7d29dd40ea8/daemon/network.go#L1044-L1047\n\n\nperhaps we should consider a utility for that (one that promises a non-nil map after cloning), although that may be too much;",
      "labels": [
        "kind/bug",
        "area/networking",
        "version/29.1"
      ],
      "created_at": "2025-11-29T15:28:24Z",
      "closed_at": "2025-12-01T16:22:35Z",
      "url": "https://github.com/moby/moby/issues/51620",
      "comments_count": 9
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51456,
      "title": "Mount tmpfs mode regression after updating from 28.5.1 to 28.5.2",
      "problem": "### Description\n\nA tmpfs mount should have mode `1777/drwxrwxrwt` according to the [documentation for the `tmpfs-mode` parameter](https://docs.docker.com/engine/storage/tmpfs/#options-for---mount)\n```\ntmpfs-mode\tFile mode of the tmpfs in octal. For instance, 700 or 0770. \nDefaults to 1777 or world-writable.\n```\nThis was the behaviour observed up to and including version 28.5.1.\n\nHowever, with 28.5.2 the mode is `0755/drwxr-xr-x`.\n\n### Reproduce\n\nIncorrect behaviour, on 28.5.2.  tmpfs mount has mode `0755/drwxr-xr-x`.\n```\nafter> docker run --rm --mount type=tmpfs,destination=/mnt-tmpfs alpine:3.21.2 ls -ld /mnt-tmpfs\ndrwxr-xr-x    2 root     root            40 Nov 10 15:04 /mnt-tmpfs\n```\n\nDocumented behaviour, on 28.5.1.  tmpfs mount has mode `1777/drwxrwxrwt`. \n```\nbefore> docker run --rm --mount type=tmpfs,destination=/mnt-tmpfs alpine:3.21.2 ls -ld /mnt-tmpfs\ndrwxrwxrwt    2 root     root            40 Nov 10 15:02 /mnt-tmpfs\n```\n\n### Expected behavior\n\nThe tmpfs mount should have mode `1777/drwxrwxrwt` in version 28.5.2 and up.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           28.5.2\n API version:       1.44 (downgraded from 1.51)\n Go version:        go1.25.3\n Git commit:        ecc6942\n Built:             Wed Nov  5 14:46:17 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          25.0.5\n  API version:      1.44 (minimum version 1.24)\n  Go version:       go1.21.8\n  Git commit:       e63daec\n  Built:            Tue Mar 19 15:05:16 2024\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v1.7.29\n  GitCommit:        442cb34bda9a6a0fed82a2ca7cade05c5c749582\n runc:\n  Version:          1.3.3\n  GitCommit:        v1.3.3-0-gd842d771\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    28.5.2\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.22.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.34.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 4\n  Running: 4\n  Paused: 0\n  Stopped: 0\n Images: 30\n Server Version: 25.0.5\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: true\n  Native Overlay Diff: false\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 442cb34bda9a6a0fed82a2ca7cade05c5c749582\n runc version: v1.3.3-0-gd842d771\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 5.10.228-43535681.REDACTED.x86_64\n Operating System: AlmaLinux 9.6 (Sage Margay)\n OSType: linux\n Architecture: x86_64\n CPUs: 48\n Total Memory: 251.3GiB\n Name: REDACTED\n ID: 72538f7f-8e1d-48d4-a001-1b0b2d198877\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  127.0.0.0/8\n Registry Mirrors:\n  https://REDACTED/\n Live Restore Enabled: false\n Default Address Pools:\n   Base: 172.17.0.0/16, Size: 24\n   Base: 192.168.0.0/16, Size: 24\n```\n\n### Additional Info\n\nDocker version, info for verion 28.5.1, which was used for before/ after comparison:\n```\nbefore> docker version\nClient: Docker Engine - Community\n Version:           28.5.1\n API version:       1.44 (downgraded from 1.51)\n Go version:        go1.24.8\n Git commit:        e180ab8\n Built:             Wed Oct  8 12:20:03 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          25.0.5\n  API version:      1.44 (minimum version 1.24)\n  Go version:       go1.21.8\n  Git commit:       e63daec\n  Built:            Tue Mar 19 15:05:16 2024\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v1.7.28\n  GitCommit:        b98a3aace656320842a23f4a392a33f46af97866\n runc:\n  Version:          1.3.0\n  GitCommit:        v1.3.0-0-g4ca628d1\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n```\nbefore> docker info\nClient: Docker Engine - Community\n Version:    28.5.1\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.29.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.2\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 4\n  Running: 4\n  Paused: 0\n  Stopped: 0\n Images: 18\n Server Version: 25.0.5\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: true\n  Native Overlay Diff: false\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: b98a3aace656320842a23f4a392a33f46af97866\n runc version: v1.3.0-0-g4ca628d1\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 5.10.228-REDACTED.x86_64\n Operating System: AlmaLinux 9.6 (Sage Margay)\n OSType: linux\n Architecture: x86_64\n CPUs: 32\n Total Memory: 125.6GiB\n Name: REDACTED\n ID: 90901cd9-1d0f-463f-8e1f-44878b72b337\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  127.0.0.0/8\n Registry Mirrors:\n  https://REDACTED/\n Live Restore Enabled: false\n Default Address Pools:\n   Base: 172.17.0.0/16, Size: 24\n   Base: 192.168.0.0/16, Size: 24\n```",
      "solution": "I think this has been fixed in 29.1.2, which includes the runc update from #51633.\n\n---\n\nDocker linux package repository has the containerd.io 2.2.0 package that includes the runc v1.3.4 (e.g. https://download.docker.com/linux/debian/dists/trixie/pool/stable/amd64/containerd.io_2.2.0-2~debian.13~trixie_amd64.deb for Debian Trixie amd64).\n\nThis should be fixed now.",
      "labels": [
        "status/0-triage",
        "kind/bug",
        "version/28.5"
      ],
      "created_at": "2025-11-10T15:53:54Z",
      "closed_at": "2025-12-03T12:57:20Z",
      "url": "https://github.com/moby/moby/issues/51456",
      "comments_count": 4
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 43163,
      "title": "CAP_PERFMON not actually effective",
      "problem": "**Description**\r\n\r\nStarting a container `--cap-add PERFMON` does not actually allow performance counter access.\r\n\r\n**Steps to reproduce the issue:**\r\n\r\n1.  Start a docker container with GCC installed with `--cap-add PERFCOUNT`\r\n2.  Copy this code into a file called `perf_count.c`: https://man7.org/linux/man-pages/man2/perf_event_open.2.html#EXAMPLES\r\n3.  `gcc perf_count.c`\r\n4. `./a.out`\r\n5.  You'll get `Error opening leader 1`\r\n6.  If you pass `--privileged` instead of `--cap-add` and repeat.\r\n7.  The code works fine and you'll get\r\n```\r\nMeasuring instruction count for this printf\r\nUsed 77591 instructions\r\n```\r\n\r\n**Describe the results you received:**\r\n\r\nI was unable to measure performance counters.\r\n\r\n**Describe the results you expected:**\r\n\r\nI expected to be able to measure performance counters.\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Version:           master-dockerproject-2022-01-18\r\n API version:       1.42\r\n Go version:        go1.16.11\r\n Git commit:        6e2838e186\r\n Built:             Tue Jan 18 23:54:00 2022\r\n OS/Arch:           linux/amd64\r\n Context:           default\r\n Experimental:      true\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          master-dockerproject-2022-01-18\r\n  API version:      1.42 (minimum version 1.12)\r\n  Go version:       go1.17.6\r\n  Git commit:       b47c0b8896\r\n  Built:            Tue Jan 18 23:59:05 2022\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.4.12\r\n  GitCommit:        7b11cfaabd73bb80907dd23182b9347b4245eb5d\r\n runc:\r\n  Version:          1.0.3\r\n  GitCommit:        v1.0.3-0-gf46b6ba2\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  app: Docker App (Docker Inc., v0.9.1-beta3)\r\n  buildx: Docker Buildx (Docker Inc., v0.7.1-docker)\r\n  scan: Docker Scan (Docker Inc., v0.12.0)\r\n\r\nServer:\r\n Containers: 27\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 27\r\n Images: 18\r\n Server Version: master-dockerproject-2022-01-18\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 7b11cfaabd73bb80907dd23182b9347b4245eb5d\r\n runc version: v1.0.3-0-gf46b6ba2\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 5.11.0-44-generic\r\n Operating System: Ubuntu 20.04.3 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 16\r\n Total Memory: 31.1GiB\r\n Name: da-c3-small-x86-01\r\n ID: 4PJX:7FS4:5L62:3A2K:4P2K:VPFZ:PJUB:EIAH:JBXW:ENW3:76PQ:MMH4\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n Product License: Community Engine\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\n```\r\n$ uname -a\r\nLinux da-c3-small-x86-01 5.11.0-44-generic #48~20.04.2-Ubuntu SMP Tue Dec 14 15:36:44 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n",
      "solution": "This is an [Ubuntu kernel bug](https://bugs.launchpad.net/ubuntu/+source/linux/+bug/2131046), it will be fixed soon. The jammy-proposed kernel is fixed, several other proposed kernels as well.",
      "labels": [
        "area/kernel",
        "kind/bug",
        "version/master"
      ],
      "created_at": "2022-01-19T07:37:55Z",
      "closed_at": "2025-12-03T10:24:31Z",
      "url": "https://github.com/moby/moby/issues/43163",
      "comments_count": 6
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51566,
      "title": "Under certain conditions the API output of `docker image inspect` becomes corrupted",
      "problem": "### Description\n\nUnder certain conditions the output of `docker image inspect` becomes corrupt. \n\nI have it happening sometimes with the following images:\n\n* `gotenberg/gotenberg:8.24`\n* `diygod/rsshub:chromium-bundled`\n\nPulling those and doing something (unknown what that is) and then inspecting yields the output as seen below. \n\nThe output is also corrupt (`\"Config\": null`) when asking the API directly: \n\n```\n\u276f curl --unix-socket /var/run/docker.sock --header \"Content-Type: application/json\" --request GET http://localhost/images/gotenberg/gotenberg:8.24/jso\nn | jq\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   542  100   542    0     0  36828      0 --:--:-- --:--:-- --:--:-- 38714\n{\n  \"Id\": \"sha256:b116a40a1c24917e2bf3e153692da5acd2e78e7cd67e1b2d243b47c178f31c90\",\n  \"RepoTags\": [\n    \"gotenberg/gotenberg:8.24\"\n  ],\n  \"RepoDigests\": [\n    \"gotenberg/gotenberg@sha256:b116a40a1c24917e2bf3e153692da5acd2e78e7cd67e1b2d243b47c178f31c90\"\n  ],\n  \"Config\": null,\n  \"Architecture\": \"\",\n  \"Os\": \"\",\n  \"Size\": 778114898,\n  \"RootFS\": {},\n  \"Metadata\": {\n    \"LastTagTime\": \"2025-11-20T00:16:56.098950994Z\"\n  },\n  \"Descriptor\": {\n    \"mediaType\": \"application/vnd.docker.distribution.manifest.list.v2+json\",\n    \"digest\": \"sha256:b116a40a1c24917e2bf3e153692da5acd2e78e7cd67e1b2d243b47c178f31c90\",\n    \"size\": 1554\n  }\n}\n```\n\nUsing version 1.44 (oldest supported on this Docker version) yields the following:\n\n```\n\u276f curl --unix-socket /var/run/docker.sock --header \"Content-Type: application/json\" --request GET http://localhost/v1.44/images/gotenberg/gotenberg:8.\n24/json | jq\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   759  100   759    0     0  49388      0 --:--:-- --:--:-- --:--:-- 50600\n{\n  \"Architecture\": \"\",\n  \"Author\": \"\",\n  \"Comment\": \"\",\n  \"Config\": {\n    \"AttachStderr\": false,\n    \"AttachStdin\": false,\n    \"AttachStdout\": false,\n    \"Cmd\": null,\n    \"Domainname\": \"\",\n    \"Entrypoint\": null,\n    \"Env\": null,\n    \"Hostname\": \"\",\n    \"Image\": \"\",\n    \"Labels\": null,\n    \"OnBuild\": null,\n    \"OpenStdin\": false,\n    \"StdinOnce\": false,\n    \"Tty\": false,\n    \"User\": \"\",\n    \"Volumes\": null,\n    \"WorkingDir\": \"\"\n  },\n  \"Container\": \"\",\n  \"ContainerConfig\": null,\n  \"DockerVersion\": \"\",\n  \"GraphDriver\": {\n    \"Data\": null,\n    \"Name\": \"overlayfs\"\n  },\n  \"Id\": \"sha256:b116a40a1c24917e2bf3e153692da5acd2e78e7cd67e1b2d243b47c178f31c90\",\n  \"Metadata\": {\n    \"LastTagTime\": \"2025-11-20T00:16:56.098950994Z\"\n  },\n  \"Os\": \"\",\n  \"Parent\": \"\",\n  \"RepoDigests\": [\n    \"gotenberg/gotenberg@sha256:b116a40a1c24917e2bf3e153692da5acd2e78e7cd67e1b2d243b47c178f31c90\"\n  ],\n  \"RepoTags\": [\n    \"gotenberg/gotenberg:8.24\"\n  ],\n  \"RootFS\": {},\n  \"Size\": 778114898\n}\n```\n\n\n### Reproduce\n\n1. ??? Unknown, still investigating\n2. ```\n   \u276e docker image inspect gotenberg/gotenberg:8.24\n   [\n       {\n           \"Id\": \"sha256:b116a40a1c24917e2bf3e153692da5acd2e78e7cd67e1b2d243b47c178f31c90\",\n           \"RepoTags\": [\n               \"gotenberg/gotenberg:8.24\"\n           ],\n           \"RepoDigests\": [\n                \"gotenberg/gotenberg@sha256:b116a40a1c24917e2bf3e153692da5acd2e78e7cd67e1b2d243b47c178f31c90\"\n           ],\n           \"Config\": null,\n           \"Architecture\": \"\",\n           \"Os\": \"\",\n           \"Size\": 778114898,\n           \"RootFS\": {},\n           \"Metadata\": {\n               \"LastTagTime\": \"2025-11-20T00:16:56.098950994Z\"\n           },\n           \"Descriptor\": {\n               \"mediaType\": \"application/vnd.docker.distribution.manifest.list.v2+json\",\n               \"digest\": \"sha256:b116a40a1c24917e2bf3e153692da5acd2e78e7cd67e1b2d243b47c178f31c90\",\n               \"size\": 1554\n           }\n       }\n   ]\n   ```\n\n### Expected behavior\n\nOutput should (at the very least) contain `Config: {}`, as it is not `nullable`. \n\n### docker version\n\n```bash\n\u276f docker version\nClient: Docker Engine - Community\n Version:           29.0.2\n API version:       1.52\n Go version:        go1.25.4\n Git commit:        8108357\n Built:             Mon Nov 17 12:33:33 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          29.0.2\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.4\n  Git commit:       e9ff10b\n  Built:            Mon Nov 17 12:33:33 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v2.1.5\n  GitCommit:        fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc:\n  Version:          1.3.3\n  GitCommit:        v1.3.3-0-gd842d771\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\n\u276f docker info\nClient: Docker Engine - Community\n Version:    29.0.2\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.3\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 55\n  Running: 55\n  Paused: 0\n  Stopped: 0\n Images: 55\n Server Version: 29.0.2\n Storage Driver: overlayfs\n  driver-type: io.containerd.snapshotter.v1\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc version: v1.3.3-0-gd842d771\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.17.0-6-generic\n Operating System: Ubuntu 25.10\n OSType: linux\n Architecture: x86_64\n CPUs: 8\n Total Memory: 30.26GiB\n Name: server\n ID: 5c592af7-7740-4833-bdb3-912db7d3fefd\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Username: <ommitted>\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\n\n### Additional Info\n\n```\n\u276f cat --plain /etc/docker/daemon.json\n{\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"10m\",\n    \"max-file\": \"3\",\n    \"tag\": \"{{.Name}}/{{.ImageName}}\"\n  },\n  \"ipv6\": true,\n  \"fixed-cidr-v6\": \"fdb7:3bc5:1122::/48\",\n  \"features\": {\n    \"containerd-snapshotter\": true\n  }\n}\n```",
      "solution": "Our situation is a bit different and maybe even more complex: We run containers in containers (in CI) so that we can test ansible provisioning to our docker host in production. In CI, we use a container to represent our target docker host. To make this work and prevent overlay on overlay issues, since docker 29 we need to volume mount /var/lib/containerd in the docker host container on the real host's filesystem.\n\nAnd that's where the problems described in this issue started, but only for one of our custom images pulled from ghcr.io, others (also from ghcr.io) seem to deploy and restart fine.\n\nThe image failing lacks the inverted [U] in `docker images` output, indicating it currently is not \"In Use\", which is not correct as the container started from this image is running on the host when I check.\n\n---\n\nHi, sorry for the long wait. I managed to reproduce the issue and found out this is another incarnation of https://github.com/moby/moby/issues/49784\n\nThis can be reproduced with two image that share the same uncompressed layer but have a different compressed blob is pulled.\n\nExample:\n```\ndocker pull nginx@sha256:3b7732505933ca591ce4a6d860cb713ad96a3176b82f7979a8dfa9973486a0d6\ndocker pull gotenberg/gotenberg@sha256:b116a40a1c24917e2bf3e153692da5acd2e78e7cd67e1b2d243b47c178f31c90\n```\n\nIn this case, it's the base debian trixie image that's used as a base. They're effectively the same layer (unpacked diff ID `sha256:1d46119d249f7719e1820e24a311aa7c453f166f714969cffe89504678eaa447`), but different compressed blobs:\n\n```json\n# nginx\n{\n\"mediaType\": \"application/vnd.oci.image.layer.v1.tar+gzip\",\n\"size\": 29777766,\n\"digest\": \"sha256:8c7716127147648c1751940b9709b6325f2256290d3201662eca2701cadb2cdf\"\n}\n\n# gotenberg\n{\n\"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n\"size\": 30781333,\n\"digest\": \"sha256:b96413fb491a5ed179bb2746ff3be6cbddd72e14c6503bea80d58e579a3b92bc\"\n},\n```\n\n\n\n---\n\nOpened a PR to fix the inspect not showing all available informations - this should fix the symptomps you're experiencing, however the underlying issue (#49784) still needs to be fixed.",
      "labels": [
        "status/0-triage",
        "kind/bug",
        "area/images",
        "containerd-integration"
      ],
      "created_at": "2025-11-20T00:26:27Z",
      "closed_at": "2025-12-02T11:35:28Z",
      "url": "https://github.com/moby/moby/issues/51566",
      "comments_count": 13
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51591,
      "title": "After update (29.0.x) networking fails with rootless Docker (`DOCKERD_ROOTLESS_ROOTLESSKIT_PORT_DRIVER=slirp4netns`)",
      "problem": "### Description\n\nnetworking fails with rootless Docker\ne.g.\ndocker run -it -p 3000:3080 alpine ash\n\nError response from daemon: failed to set up container networking: driver failed programming external connectivity on endpoint \u2026\u2026\nfailed to bind host port 10.0.2.100:3000/tcp: address already in use\n\nIt fails regardless which port is picked.\n(see also https://forums.docker.com/t/after-update-networking-fails-with-rootless-docker/150458)\n\n### Reproduce\n\ndocker run -it -p 3000:3080 alpine ash\n\n### Expected behavior\n\n_No response_\n\n### docker version\n\n```bash\n29.0.4\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    29.0.4\n Context:    rootless\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.3\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n```\n\n### Additional Info\n\n_No response_\n\n\n- - -\n\n**Below is added by @AkihiroSuda**\n\n# Workaround: use pasta\n\nTested on Ubuntu 24.04, Docker 29.0.4.\nSimilar to slirp4netns, the src IP information is preserved.\n\n```bash\nsudo apt install -y passt\nmkdir -p ~/.config/systemd/user/docker.service.d/\ncat <<EOF >~/.config/systemd/user/docker.service.d/override.conf\n[Service]\nEnvironment=\"DOCKERD_ROOTLESS_ROOTLESSKIT_NET=pasta\"\nEnvironment=\"DOCKERD_ROOTLESS_ROOTLESSKIT_PORT_DRIVER=implicit\"\nEOF\nsystemctl --user daemon-reload\nsystemctl --user restart docker\n```\n\n# Another Workaround\n`-p 127.0.0.1:8080:80` and `-p 0.0.0.0:8080:80` seems to work while `-p 8080:80` fails.\n",
      "solution": "Hi @GeorgQW - thank you for reporting the issue. I'm unable to reproduce it on a Debian 13 VM.\n\nI think \"After update (29.0.2 / 29.0.4)\" means both of those versions fail for you? (Not that upgrade from 29.0.2 to 29.0.4 introduced the failure?)\n\nWhich OS are you running on? The output from \"docker version\" and the full output from \"docker info\" might be useful.\n\nIf you're using \"slirp4netns\" (?), did you try the \"pasta\" network driver - that seems to have helped some in the forum thread, it might help to narrow things down a little.\n\nPerhaps there will be more info in Docker's log ... if you can send debug logs from trying to start a container with this error, I'll take a look. You can enable debugging by adding `{ \"debug\": true }` to `daemon.json` ... it'll depend on your OS, but it might live at `~/.config/docker/daemon.json` and you might be able to access the logs using `journalctl --user -xu docker.service`.\n\n---\n\nI see from the forum `\"userland-proxy\": false` might help in some cases - did you try that? (Just to narrow down the issue.)\n\n---\n\nI can't reproduce the issue either on Ubuntu 24.04.\nWhat is your host OS? Also make sure you have the latest rootlesskit.\n\n```console\n$ docker version\nClient: Docker Engine - Community\n Version:           29.0.4\n API version:       1.52\n Go version:        go1.25.4\n Git commit:        3247a5a\n Built:             Mon Nov 24 21:59:48 2025\n OS/Arch:           linux/arm64\n Context:           rootless\n\nServer: Docker Engine - Community\n Engine:\n  Version:          29.0.4\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.4\n  Git commit:       4612690\n  Built:            Mon Nov 24 21:59:48 2025\n  OS/Arch:          linux/arm64\n  Experimental:     false\n containerd:\n  Version:          v2.1.5\n  GitCommit:        fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc:\n  Version:          1.3.3\n  GitCommit:        v1.3.3-0-gd842d771\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n rootlesskit:\n  Version:          2.3.5\n  ApiVersion:       1.1.1\n  NetworkDriver:    slirp4netns\n  PortDriver:       builtin\n  StateDir:         /run/user/501/dockerd-rootless\n slirp4netns:\n  Version:          1.2.1\n  GitCommit:        09e31e92fa3d2a1d3ca261adaeb012c8d75a8194\n```",
      "labels": [
        "status/0-triage",
        "kind/bug",
        "area/networking",
        "area/rootless",
        "kind/regression",
        "area/networking/portmapping"
      ],
      "created_at": "2025-11-25T17:28:08Z",
      "closed_at": "2025-11-28T23:54:17Z",
      "url": "https://github.com/moby/moby/issues/51591",
      "comments_count": 16
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51405,
      "title": "Can't run containers when running in LXC (`open sysctl net.ipv4.ip_unprivileged_port_start file: reopen fd 8: permission denied`)",
      "problem": "### Description\n\n i've tried a lot of distro, debian 12/13 and like 3 versions of ubuntu, but i keep getting this error running hello-world and also other containers (ps. running via root and also with other users) the users are inside docker group and i freshly installed docker from the official website guide https://docs.docker.com/engine/install/debian/ this is the error i get \"docker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: open sysctl net.ipv4.ip_unprivileged_port_start file: reopen fd 8: permission denied: unknown\" can you guys help me out? that's not my first installation i got a lot of debian and ubuntu servers running docker containers...\n\n### Reproduce\n\n1. Install debian 12 or 13 CT on proxmox 9\n2. install docker via https://docs.docker.com/engine/install/debian/\n3. run docker run hello-world via root or via another user inside docker group\n\n### Expected behavior\n\nrun the docker \n\n### docker version\n\n```bash\nroot@testdc:/home/sec# docker version\nClient: Docker Engine - Community\n Version:           28.5.1\n API version:       1.51\n Go version:        go1.24.8\n Git commit:        e180ab8\n Built:             Wed Oct  8 12:17:24 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          28.5.1\n  API version:      1.51 (minimum version 1.24)\n  Go version:       go1.24.8\n  Git commit:       f8215cc\n  Built:            Wed Oct  8 12:17:24 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v1.7.28\n  GitCommit:        b98a3aace656320842a23f4a392a33f46af97866\n runc:\n  Version:          1.3.3\n  GitCommit:        v1.3.3-0-gd842d771\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    28.5.1\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.29.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.3\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 14\n  Running: 0\n  Paused: 0\n  Stopped: 14\n Images: 4\n Server Version: 28.5.1\n Storage Driver: overlay2\n  Backing Filesystem: zfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: true\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: b98a3aace656320842a23f4a392a33f46af97866\n runc version: v1.3.3-0-gd842d771\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.14.11-2-pve\n Operating System: Debian GNU/Linux 12 (bookworm)\n OSType: linux\n Architecture: x86_64\n CPUs: 2\n Total Memory: 7.906GiB\n Name: testdc\n ID: 34b2a960-be2e-452d-b92c-e7212b38b29d\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n```\n\n### Additional Info\n\n_No response_",
      "solution": "**EDIT**: For a temporary workaround, see: https://github.com/containerd/containerd/issues/12484#issuecomment-3496876566\n\n\n<details>\n\n<summary> Old comment </summary>\n\nCould you try downgrading the `containerd.io` package to a previous one that doesn't include the new runc?\n\n**EDIT**: Please note that **THIS IS NOT A SOLUTION**!  Downgrading makes you vulnerable to 3 CVEs that were fixed by the new version.\nThis is only for diagnostic purposes.\n\n```\nsudo apt install containerd.io=1.7.28-1~debian.12~bookworm\n```\n\n</details>\n\n---\n\nrelated discussion in the containerd repository mentions that (if you're using LXC), that setting AppArmor to unconfined can be used as a workaround; \n\n- https://github.com/containerd/containerd/issues/12484\n\n\nAnd further exploration in this LXC ticket;\n\n- https://github.com/lxc/incus/issues/2623",
      "labels": [
        "area/runtime",
        "kind/bug",
        "kind/duplicate"
      ],
      "created_at": "2025-11-05T16:10:56Z",
      "closed_at": "2025-11-06T07:49:32Z",
      "url": "https://github.com/moby/moby/issues/51405",
      "comments_count": 6
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51341,
      "title": "Update 25.0.13 causes overlay network for AWS EC2 hosts to break",
      "problem": "### Description\n\nWhen updating to Docker 25.0.13 for AL2023 for EC2 hosts they can no longer communicate via the overlay network. This works correctly for 25.0.08, and only when updating does it stop working.\n\nThere is no firewall installed on the EC2 hosts as AWS Security Groups are used instead.\n\n### Reproduce\n\n1. On an Amazon EC2 AL2023 host, update to Docker 25.0.13\n2. The containers no longer can communicate with each other, or other services via the swarm.\n\nReboots, or restarting the docker daemon do not resolve this, only downgrading back to 25.0.8 does.\n\n### Expected behavior\n\nContainers within the same swarm network should be able to communicate.\n\n### docker version\n\n```bash\nClient:\n Version:           25.0.13\n API version:       1.44\n Go version:        go1.24.7\n Git commit:        0bab007\n Built:             Tue Sep 23 00:00:00 2025\n OS/Arch:           linux/arm64\n Context:           default\n\nServer:\n Engine:\n  Version:          25.0.13\n  API version:      1.44 (minimum version 1.24)\n  Go version:       go1.24.7\n  Git commit:       165516e\n  Built:            Tue Sep 23 00:00:00 2025\n  OS/Arch:          linux/arm64\n  Experimental:     false\n containerd:\n  Version:          2.1.4\n  GitCommit:        75cb2b7193e4e490e9fbdc236c0e811ccaba3376\n runc:\n  Version:          1.3.1\n  GitCommit:        e6457afc48eff1ce22dece664932395026a7105e\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient:\n Version:    25.0.13\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  0.12.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n\nServer:\n Containers: 9\n  Running: 5\n  Paused: 0\n  Stopped: 4\n Images: 6\n Server Version: 25.0.13\n Storage Driver: overlay2\n  Backing Filesystem: xfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n Swarm: active\n  NodeID: u0ztii6zifn968ymrj6ks3my0\n  Is Manager: false\n  Node Address: 172.31.41.4\n  Manager Addresses:\n   172.31.33.53:2377\n   172.31.35.141:2377\n   172.31.43.61:2377\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 75cb2b7193e4e490e9fbdc236c0e811ccaba3376\n runc version: e6457afc48eff1ce22dece664932395026a7105e\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.12.53-69.119.amzn2023.aarch64\n Operating System: Amazon Linux 2023.9.20251027\n OSType: linux\n Architecture: aarch64\n CPUs: 1\n Total Memory: 7.627GiB\n Name: ip-172-31-41-4.eu-west-2.compute.internal\n ID: c90450f0-8e7b-4de7-afcd-47e5d7f299ab\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  127.0.0.0/8\n Live Restore Enabled: false\n```\n\n### Additional Info\n\n_No response_",
      "solution": "Have been digging into this a little more. Looks like it's specifically the DNS resolution between the Docker containers which is affected.\n\nDirect IP to IP communication seems to work. When pinging between containers via IP it works, but not when done via DNS  (e.g. when using the service name).",
      "labels": [
        "status/0-triage",
        "kind/bug",
        "area/swarm",
        "version/25.0"
      ],
      "created_at": "2025-10-30T13:13:16Z",
      "closed_at": "2025-11-28T12:22:13Z",
      "url": "https://github.com/moby/moby/issues/51341",
      "comments_count": 2
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51614,
      "title": "29.1.0 breaks the embedded DNS resolver",
      "problem": "### Description\n\nUpdating from 29.0.4 to 29.1.0 breaks all DNS connectivity between containers and the embedded DNS resolver.\n\n127.0.0.11#53 rejects connectivity on port 53.\n\nEvery process outputs this (management being my container name in this example)\n```\ntcp: lookup management on 127.0.0.11:53: server misbehaving\n```\n\nWhen trying to nslookup from inside a container I get this\n\n```\nnslookup management 127.0.0.11\nServer:\t\t127.0.0.11\nAddress:\t127.0.0.11:53\n\n** server can't find management: SERVFAIL\n\n** server can't find management: SERVFAIL\n```\n\n### Reproduce\n\n1. Upgrade to 29.0.1\n2. That's it.\n\n### Expected behavior\n\nDNS should work\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           29.1.0\n API version:       1.52\n Go version:        go1.25.4\n Git commit:        360952c\n Built:             Thu Nov 27 16:42:45 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          29.1.0\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.4\n  Git commit:       710302e\n  Built:            Thu Nov 27 16:42:45 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v2.1.5\n  GitCommit:        fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc:\n  Version:          1.3.3\n  GitCommit:        v1.3.3-0-gd842d771\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    29.1.0\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.3\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 48\n  Running: 45\n  Paused: 0\n  Stopped: 3\n Images: 57\n Server Version: 29.1.0\n Storage Driver: btrfs\n  Btrfs:\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc version: v1.3.3-0-gd842d771\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.12.57+deb13-amd64\n Operating System: Debian GNU/Linux 13 (trixie)\n OSType: linux\n Architecture: x86_64\n CPUs: 16\n Total Memory: 28.3GiB\n Name: togepi\n ID: 41fd9997-a498-44ae-90ee-35b16015c244\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\n\n### Additional Info\n\n_No response_",
      "solution": "I can confirm that stop/start the stack in Portainer fix the issue. I had issue regarding Home Assistant and qBittorrent nox.\n\n---\n\nCan confirm this is affecting us; recreating via compose may resolve the issue is not an option in all cases.\n\n---\n\nI have the same issue, upgrading to Docker 29.1 on Rocky Linux 9.6 host caused DNS resolution to break inside containers (running nslookup without explicitly declaring the NS resulted in SERVFAIL error). Restart didn't fix it, running dnf downgrade got me back up.\n\nI'm assuming stopping/restarting the compose stack works for some as it destroys and recreates the containers. I haven't tried it but I'll give it a look \ud83d\udc4d",
      "labels": [
        "kind/bug",
        "area/networking",
        "status/confirmed",
        "area/networking/dns",
        "version/29.1"
      ],
      "created_at": "2025-11-28T01:49:53Z",
      "closed_at": "2025-11-28T10:31:51Z",
      "url": "https://github.com/moby/moby/issues/51614",
      "comments_count": 15
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51569,
      "title": "Can't set static IPs for containers with custom bridge network",
      "problem": "### Description\n\nHello,\n\nJust two 3 days ago, I updated my Docker from `v28.5.1` to `v29.0.2`, and all of my containers with static IPs set (`ipv4_address`) won't `start` or `up -d` since then.\n\nError I'm getting:\n```\n0.0s Error response from daemon: invalid config for network adnio: invalid endpoint settings:\nuser specified IP address is supported only when connecting to networks with user configured subnets \n```\n \nI have created a custom docker bridge network using cli `docker network create adnio`.\n\nDocker networks:\n```\nNETWORK ID     NAME          DRIVER    SCOPE\n47e4e135b3ae   adnio         bridge    local\na05c81df3750   bridge        bridge    local\nfaf68de61cf7   host          host      local\n0478897e6921   none          null      local\n4b926deb0e6c   vpn_default   bridge    local\n```\n\nHere is one of my compose file:\n```\nservices:\n\n  portainer:\n    image: portainer/portainer-ce\n    container_name: portainer\n    networks:\n      adnio:\n        ipv4_address: ${SUBNET}${PORTAINER_IP}\n    ports:\n      - 9000:9000\n    environment:\n      - TZ=${TZ}\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - /etc/timezone:/etc/timezone:ro\n      - ${APP_CONF}/portainer:/data\n    restart: always\n...\n\nnetworks:\n  adnio:\n    external: true\n```\n\nExample of my .env file:\n```\n## ==================== ##\n\nNETWORK=\"adnio\"\nSUBNET=\"172.18.0.\"\n\n## Containers IP Octat ##\n\n## ==================== ##\n## Always-On ##\n\nPORTAINER_IP=2\n...\n```\n\n### Reproduce\n\n1. `docker network create adnio`\n2. Set `ipv4_address` to any container in compose file\n3. `start` or `up -d` this container and observe the error\n\n### Expected behavior\n\nThe container(s) should `start` or `up -d` with the set static IP. \n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           29.0.2\n API version:       1.52\n Go version:        go1.25.4\n Git commit:        8108357\n Built:             Mon Nov 17 12:33:35 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          29.0.2\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.4\n  Git commit:       e9ff10b\n  Built:            Mon Nov 17 12:33:35 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v2.1.5\n  GitCommit:        fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc:\n  Version:          1.3.3\n  GitCommit:        v1.3.3-0-gd842d771\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    29.0.2\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.30.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.3\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 29\n  Running: 3\n  Paused: 0\n  Stopped: 26\n Images: 37\n Server Version: 29.0.2\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: local\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc version: v1.3.3-0-gd842d771\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.12.57+deb13-amd64\n Operating System: Debian GNU/Linux 13 (trixie)\n OSType: linux\n Architecture: x86_64\n CPUs: 4\n Total Memory: 23.35GiB\n Name: server\n ID: 12d222b1-af02-42ee-9325-c5042cba9403\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n Firewall Backend: iptables\n```\n\n### Additional Info\n\nAs I couldn't find any solution to the above problem, I have now downgraded my Docker to last working version which is `v28.5.1`, and now everything started normally as before.",
      "solution": "Hi @XxAcielxX - thank you for reporting this. I'm able to repro the issue, will take a look ...",
      "labels": [
        "kind/bug",
        "area/networking",
        "version/29.0"
      ],
      "created_at": "2025-11-20T12:18:25Z",
      "closed_at": "2025-11-24T16:37:29Z",
      "url": "https://github.com/moby/moby/issues/51569",
      "comments_count": 1
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 50261,
      "title": "Web servers deployed in a replicated Swarm service on RHEL 9.6 can't publish ports",
      "problem": "### Description\n\nOn our RHEL 9.6 servers, we're currently running a web server using the global Swarm mode with the host networking and default accessible ports.\n\nAttempts to deploy a web server as a replicated service fail because the web server is not accessible over HTTP on any of the Swarm hosts.\n\nFor example, just following the Docker documentation example for `nginx`:\n```\ncurl -L http://localhost:8080\n# Hangs\n``` \n\nIt turns out that using any `--publish` options I tried, even with the global Swarm mode and the host networking, doesn't work.\n\n### Reproduce\n\n0. Setup\n```\nsudo dnf -y install dnf-plugins-core\nsudo dnf config-manager --add-repo https://download.docker.com/linux/rhel/docker-ce.repo\nsudo dnf install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\nsystemctl enable --now docker\n# Pre-defined service\nsudo firewall-cmd --permanent --add-service=docker-swarm\nsudo firewall-cmd --permanent --add-service=myapp\nsudo firewall-cmd --permanent --service=myapp --add-port=80/tcp --add-port=8080/tcp\nsudo firewall-cmd --reload\n# On the manager host\ndocker swarm init --advertise-addr {ens192 IPv4}\n# On the other host\ndocker swarm join --token {token} {ens192 IPv4}:2377`\n```\n\n1. Working: global service, host network, default accessible port\n```\ndocker service create --mode global --network host --name my_web nginx\ncurl -L localhost:80\n# OK\n```\n\n2.  Not working: publishing the port for a global service, host network\n```\ndocker service create --mode global --network host --publish published=8080,target=80 --name my_web nginx\n# Fails to start, errors in the docker system jorunal: \"... starting container failed: failed to set up container networking: cannot connect container to host network - container must be created in host network mode\"\n``` \n\n3. Not working:  publishing the port in the host mode for a global service, host network\n```\ndocker service create --mode global --network host --publish published=8080,target=80,mode=host --name my_web nginx\ncurl -L localhost:8080\n# curl: (7) Failed to connect to localhost port 8080: Connection refused\n```\n\n4. Not working:  publishing the port for a replicated service\n```\ndocker service create --replicas 3 --publish published=8080,target=80 --name my_web nginx\ncurl -L localhost:8080\n# Hangs...\n```\n\n5. Not working:  publishing the port for a replicated service, custom overlay network\n```\ndocker network create -d overlay my_overlay\ndocker service create --replicas 3 --network my_overlay --publish published=8080,target=80 --name my_web nginx\ncurl -L localhost:8080\n# Hangs...\n```\n\n### Expected behavior\n\nPort 8080 is published, `curl -L localhost:8080` returns the `nginx` home page.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           28.2.2\n API version:       1.50\n Go version:        go1.24.3\n Git commit:        e6534b4\n Built:             Fri May 30 12:09:15 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          28.2.2\n  API version:      1.50 (minimum version 1.24)\n  Go version:       go1.24.3\n  Git commit:       45873be\n  Built:            Fri May 30 12:07:23 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.7.27\n  GitCommit:        05044ec0a9a75232cad458027ca83437aae3f4da\n runc:\n  Version:          1.2.5\n  GitCommit:        v1.2.5-0-g59923ef\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    28.2.2\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.24.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.36.2\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 3\n  Running: 1\n  Paused: 0\n  Stopped: 2\n Images: 2\n Server Version: 28.2.2\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: active\n  NodeID: lgj0roegc6x1jk7upr105b1s8\n  Is Manager: true\n  ClusterID: iacd3sv3xfev1cic17xw43y5h\n  Managers: 1\n  Nodes: 2\n  Default Address Pool: 10.0.0.0/8\n  SubnetSize: 24\n  Data Path Port: 4789\n  Orchestration:\n   Task History Retention Limit: 5\n  Raft:\n   Snapshot Interval: 10000\n   Number of Old Snapshots to Retain: 0\n   Heartbeat Tick: 1\n   Election Tick: 10\n  Dispatcher:\n   Heartbeat Period: 5 seconds\n  CA Configuration:\n   Expiry Duration: 3 months\n   Force Rotate: 0\n  Autolock Managers: false\n  Root Rotation In Progress: false\n  Node Address: 137.187.98.80\n  Manager Addresses:\n   137.187.98.80:2377\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 05044ec0a9a75232cad458027ca83437aae3f4da\n runc version: v1.2.5-0-g59923ef\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 5.14.0-570.22.1.el9_6.x86_64\n Operating System: Red Hat Enterprise Linux 9.6 (Plow)\n OSType: linux\n Architecture: x86_64\n CPUs: 2\n Total Memory: 15.36GiB\n Name: itadswebv05\n ID: 7a13746d-ead9-4ff4-a1c5-7ffb1652977b\n Docker Root Dir: /apps/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n```\n\n### Additional Info\n\nI've found this issue: https://github.com/moby/moby/issues/41775 and tried the suggested workaround:\n```\nsudo ethtool -K ens192 tx-checksum-ip-generic off\nsudo ethtool -K docker0 tx-checksum-ip-generic off\nsudo ethtool -K docker_gwbridge tx-checksum-ip-generic off\n```\n\nThis didn't help.",
      "solution": "@corhere I tracked https://github.com/moby/moby/issues/50129 closely, and its resolution didn't solve the issue for me. I tried both v29.3.0 and v28.1.1:\n\n```# docker version\nClient: Docker Engine - Community\n Version:           28.1.1\n API version:       1.49\n Go version:        go1.23.8\n Git commit:        4eba377\n Built:             Fri Apr 18 09:53:36 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          28.1.1\n  API version:      1.49 (minimum version 1.24)\n  Go version:       go1.23.8\n  Git commit:       01f442b\n  Built:            Fri Apr 18 09:51:51 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.7.27\n  GitCommit:        05044ec0a9a75232cad458027ca83437aae3f4da\n runc:\n  Version:          1.2.5\n  GitCommit:        v1.2.5-0-g59923ef\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\nI still get the same problems. Can this issue be reopened?\n\n---\n\n> I still get the same problems. Can this issue be reopened?\n\n@DKroot no, as this is not the right issue for your problems. This issue and #50129 are for the regression which was introduced in v28.2.2 and subsequently resolved in v28.3.0 by reverting the offending changes. Your issue reproduces with v28.1.1 and v28.3.0, therefore you are likely affected by #49908.\n\n---\n\nJust to follow up on this and close the loop: it turns out that the cause of the issue was trying to hit `localhost`. It turns out that HTTP requests to `localhost` hang, whereas requests to the Swarm hostname succeed:\n```\ndocker service create --name my_web --replicas 3 --publish 8080:80 nginx\n# Works on all Swarm nodes and externally (when hitting a fixed hostname of any Swarm node)\ncurl $HOSTNAME:8080\n# Hangs\ncurl localhost:8080\n```\n\nThis issue failed our entire deployment pipeline, but it turned out to be a false alarm. \nThe solution is easy: just replace `localhost` with `$HOSTNAME` in scripts, and the replicated Swarm works fine for us.",
      "labels": [
        "status/0-triage",
        "kind/bug"
      ],
      "created_at": "2025-06-23T23:46:23Z",
      "closed_at": "2025-06-25T17:08:11Z",
      "url": "https://github.com/moby/moby/issues/50261",
      "comments_count": 4
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 43496,
      "title": "172.x.x overlay network does not get added to /etc/hosts",
      "problem": "**Description**\r\n\r\n172.x.x overlay network does not get added to /etc/hosts causing the behaviour described in https://github.com/spring-cloud/spring-cloud-commons/issues/203\r\n\r\n**Steps to reproduce the issue:**\r\n1. create a swarm deployment\r\n2. do `ifconfig` on the container to see the addresses\r\n3. do `cat /etc/hosts` to see the DNS mappings\r\n\r\n**Describe the results you received:**\r\n\r\nNo entry for the overlay network in /etc/hosts\r\n\r\n**Describe the results you expected:**\r\n\r\nEntry for the overlay network in /etc/hosts\r\n\r\n**Additional information you deem important (e.g. issue happens only occasionally):**\r\n\r\n**Output of `docker version`:**\r\n\r\n```\r\nClient:\r\n Cloud integration: v1.0.23\r\n Version:           20.10.14\r\n API version:       1.41\r\n Go version:        go1.16.15\r\n Git commit:        a224086\r\n Built:             Thu Mar 24 01:53:11 2022\r\n OS/Arch:           windows/amd64\r\n Context:           default\r\n Experimental:      true\r\n\r\nServer: Docker Desktop 4.7.0 (77141)\r\n Engine:\r\n  Version:          20.10.14\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.16.15\r\n  Git commit:       87a90dc\r\n  Built:            Thu Mar 24 01:46:14 2022\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.5.11\r\n  GitCommit:        3df54a852345ae127d1fa3092b95168e4a88e2f8\r\n runc:\r\n  Version:          1.0.3\r\n  GitCommit:        v1.0.3-0-gf46b6ba\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n\r\n```\r\n\r\n**Output of `docker info`:**\r\n\r\n```\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc., v0.8.2)\r\n  compose: Docker Compose (Docker Inc., v2.4.1)\r\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc., 0.6.0)\r\n  scan: Docker Scan (Docker Inc., v0.17.0)\r\n\r\nServer:\r\n Containers: 23\r\n  Running: 13\r\n  Paused: 0\r\n  Stopped: 10\r\n Images: 12\r\n Server Version: 20.10.14\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: active\r\n  NodeID: 6i0wgjskchub6upgtkhusuvpw\r\n  Is Manager: true\r\n  ClusterID: vn99jdqaycggrxeywuxj7n379\r\n  Managers: 1\r\n  Nodes: 1\r\n  Default Address Pool: 10.0.0.0/8\r\n  SubnetSize: 24\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 0\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: false\r\n  Node Address: 192.168.65.3\r\n  Manager Addresses:\r\n   192.168.65.3:2377\r\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 3df54a852345ae127d1fa3092b95168e4a88e2f8\r\n runc version: v1.0.3-0-gf46b6ba\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 5.10.102.1-microsoft-standard-WSL2\r\n Operating System: Docker Desktop\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 4\r\n Total Memory: 11.69GiB\r\n Name: docker-desktop\r\n ID: 7B6U:NICP:SKYG:MI7Q:3SSR:6UB4:3OXD:UG7Q:IF75:NLWX:JLV5:H4VR\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n HTTP Proxy: http.docker.internal:3128\r\n HTTPS Proxy: http.docker.internal:3128\r\n No Proxy: hubproxy.docker.internal\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  hubproxy.docker.internal:5000\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: No blkio throttle.read_bps_device support\r\nWARNING: No blkio throttle.write_bps_device support\r\nWARNING: No blkio throttle.read_iops_device support\r\nWARNING: No blkio throttle.write_iops_device support\r\n```\r\n\r\n**Additional environment details (AWS, VirtualBox, physical, etc.):**\r\n\r\nDocker Desktop Windows",
      "solution": "This is expected; `/etc/hosts` is only used for legacy links on the default bridge networks, but for other networks, an embedded DNS server is used for name resolution. Also see https://github.com/moby/moby/issues/22295",
      "labels": [
        "kind/question",
        "area/networking",
        "version/20.10"
      ],
      "created_at": "2022-04-18T08:20:52Z",
      "closed_at": "2025-11-19T01:15:35Z",
      "url": "https://github.com/moby/moby/issues/43496",
      "comments_count": 3
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 46678,
      "title": "Prune should always list items to be deleted before asking to confirm.",
      "problem": "### Description\r\n\r\nWhen running a prune command, no matter what for, a list of items that will be deleted should be listed before asking the user to confirm the action.\r\n\r\n### Reproduce\r\n\r\nRun `docker volume prune` or similar\r\n\r\n\r\n### Expected behavior\r\n\r\n```\r\nWARNING! This will remove all local volumes not used by at least one container.\r\n\r\nVolumes to be Deleted:\r\nf6d6dc53efb25c1a9e8950d65d954ad4ade54deefc95a261ff3daef7cde59232\r\nmatrix_synapse-data\r\n\r\nAre you sure you want to continue? [y/N] y\r\nDeleted Volumes:\r\nf6d6dc53efb25c1a9e8950d65d954ad4ade54deefc95a261ff3daef7cde59232\r\nmatrix_synapse-data\r\n\r\nTotal reclaimed space: 1kB\r\n```\r\n\r\n### docker version\r\n\r\n```bash\r\nClient:\r\n Version:           20.10.5+dfsg1\r\n API version:       1.41\r\n Go version:        go1.15.15\r\n Git commit:        55c4c88\r\n Built:             Mon May 30 18:34:49 2022\r\n OS/Arch:           linux/amd64\r\n Context:           default\r\n Experimental:      true\r\n\r\nServer:\r\n Engine:\r\n  Version:          20.10.5+dfsg1\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.15.15\r\n  Git commit:       363e9a8\r\n  Built:            Mon May 30 18:34:49 2022\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.4.13~ds1\r\n  GitCommit:        1.4.13~ds1-1~deb11u4\r\n runc:\r\n  Version:          1.0.0~rc93+ds1\r\n  GitCommit:        1.0.0~rc93+ds1-5+deb11u2\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 16\r\n  Running: 16\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 41\r\n Server Version: 20.10.5+dfsg1\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 1.4.13~ds1-1~deb11u4\r\n runc version: 1.0.0~rc93+ds1-5+deb11u2\r\n init version:\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: default\r\n  cgroupns\r\n Kernel Version: 5.10.0-25-amd64\r\n Operating System: Debian GNU/Linux 11 (bullseye)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 2\r\n Total Memory: 1.925GiB\r\n Name: ARS1\r\n ID: 26IO:IA5N:FTYX:NTHG:U5YR:BOZR:JUHG:5GQN:CMLO:MPOE:V36N:KZGE\r\n Docker Root Dir: /var/lib/docker\r\n Debug Mode: false\r\n Registry: https://index.docker.io/v1/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: Support for cgroup v2 is experimental\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\nThis has burned me a few times. I shouldn't have to make sure each and every container is running before deleting an unrelated type. EXPESICALLY WHEN BACKUPS ARE IMPOSSIBDLE TO MAKE!",
      "solution": "i'm a bit confused as to why this was closed. what's described is definitely not a duplicate of dry-run, as dry-run *does* present an issue of a race condition between when dry run is used, and the wet run.\n\nThe obvious solution that seems to be presented by OP is that you initialise the list of things to be deleted when the prune command is used, and then present it to the user. when the user confirms, you don't re-initialise the list (as obviously this introduces the race condition), you just attempt to remove all that was listed in what you already presented to the user.\n\nYou could argue that there's a race condition here in that things could become eligible for pruning *after* the list has been initialised (or could already have been removed), and therefore would be missed when the user confirms. But this race condition feels *far* less catastrophic than removing things that shouldn't be removed. \n\nAt the very least, there should be a documented, easy way to list volumes/images/etc that are unused (and would be removed by a prune), even if it's not a prune --dry-run command. Obviously there's `docker volume ls --filter dangling=true`, but it would be nice if there were a system-level version of this (maybe there is and I'm just not aware of it, in which case it should be [more] visibly documented).\n\nAm I missing something here? It seems like a pretty obvious solution, which is why I'm assuming that there's some fatal flaw I've overlooked. Else, I'm happy to work on it if nobody else has the time for it, as it'd be extremely useful for me.\n\n",
      "labels": [
        "kind/feature",
        "version/20.10",
        "kind/duplicate"
      ],
      "created_at": "2023-10-19T10:48:49Z",
      "closed_at": "2023-10-19T12:29:28Z",
      "url": "https://github.com/moby/moby/issues/46678",
      "comments_count": 2
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51541,
      "title": "`Rootless Docker` fails when `IPv6` is disabled via Kernel parameter after upgrading to `v29`",
      "problem": "\n## References\n\nhttps://github.com/moby/moby/blob/a81d441133acb6a12e4862f622f0ad54cc8e3634/contrib/dockerd-rootless.sh#L233\nCommit: https://github.com/moby/moby/commit/f71e86eed501b1739d520932c8545451c2dd23b1\n\n## Description\n\nDocker v29's `dockerd-rootless.sh` script unconditionally attempts to enable IPv6 forwarding via `sysctl -w net.ipv6.conf.all.forwarding=1`. If IPv6 is disabled at the kernel level (e.g., via `ipv6.disable=1` boot parameter), the `/proc/sys/net/ipv6/` directory does not exist. This causes the script to fail with `sysctl: cannot stat /proc/sys/net/ipv6/conf/all/forwarding: No such file or directory`.\n\n## Steps to reproduce\n\n1. Disable IPv6 via kernel parameter: Add `ipv6.disable=1` to `GRUB_CMDLINE_LINUX_DEFAULT` in `/etc/default/grub`, then run `update-grub`, and reboot.\n2. Attempt to run `/usr/bin/dockerd-rootless-setuptool.sh install`.\n\n## Expected behavior\n\nThe script may handle cases where IPv6 is disabled at the kernel level.\n\n## Environment\n\n- Docker version: v29\n- OS: Ubuntu\n- Rootless mode: True\n\n\n## Workaround\n\nRemove `ipv6.disable=1` from the boot parameters, reboot, then disable IPv6 via sysctl:\n\n```conf\nnet.ipv6.conf.all.disable_ipv6=1\nnet.ipv6.conf.default.disable_ipv6=1\nnet.ipv6.conf.lo.disable_ipv6=1\n```\n",
      "solution": "Hi @deadnews - thank you for reporting this, I'll get it fixed ...",
      "labels": [
        "area/rootless",
        "area/networking/ipv6",
        "version/29.0"
      ],
      "created_at": "2025-11-16T03:06:40Z",
      "closed_at": "2025-11-16T13:20:20Z",
      "url": "https://github.com/moby/moby/issues/51541",
      "comments_count": 1
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51522,
      "title": "Problem using DockerCAN in Docker 29",
      "problem": "### Description\n\nI'm maintaining a docker network plugin called DockerCAN, used to create CAN networks. In Docker v29, there is problems when using it together with a normal bridged network.\n\nThe error I get is in the form:\n\n```\ndocker: Error response from daemon: failed to set up container networking: updating external connectivity for IPv4 endpoint 11d837533a2d: driver failed programming external connectivity on endpoint gateway_c1b3df5e1679 (11d837533a2d6aa83bf1f38f8e76c3dff2e213f1a237c46a14901a153e6f202e): endpoint not found: 11d837533a2d6aa83bf1f38f8e76c3dff2e213f1a237c46a14901a153e6f202e\n```\n\nThis error message is referring to the bridged network, not the CAN network. The DockerCAN plugin doesn't generate any errors or anything suspicious and receives the callbacks as expected.\n\nThe CAN networks are not using IP at all, neither v4 nor v6.\n\nSome notes:\n* It only happens when I use both an ip-based network together with one or more CAN networks. Just CAN or just bridge works fine.\n* I've tried it on both x86 and arm architectures.\n* It works fine in Docker v28.\n* The error happens often, but not always.\n* If I create the container first and assign the networks one by one, it seems to work.\n* The instructions run alpine, but I've tried it with other images as well.\n\n### Reproduce\n\nThere is a repo with instructions here: https://github.com/remotivelabs/docker_bug_repro. But I'll add them here as well:\n\n**Install the DockerCAN plugin, only available for Linux**\n\nFirst install `can-utils`.\n\n```\nsudo apt install can-utils\n```\n\nDownload the latest version of DockerCAN from https://releases.remotivelabs.com/#docker_can/\n\n```\nmkdir dockercan\ntar xvf dockercan-VERSION.tar.gz -C dockercan\ncd dockercan\nmake\ncd ..\nrm -rf dockercan\n```\n\nNow we can run things:\n\n1. Create the bridge network: `docker network create my_net`\n2. Create the CAN network: `docker network create --ipv6=false --ipv4=false  --driver=docker_can --opt vxcan.dev=mycan0 --opt vxcan.peer=mycan0 my_can`\n3. Run the container with the networks: `docker run --name alpine --network my_net --network my_can --tty alpine echo hello`\n\n\n\n### Expected behavior\n\nBringing the container up should work without errors.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           29.0.0\n API version:       1.52\n Go version:        go1.25.4\n Git commit:        3d4129b\n Built:             Mon Nov 10 21:47:17 2025\n OS/Arch:           linux/arm64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          29.0.0\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.4\n  Git commit:       d105562\n  Built:            Mon Nov 10 21:47:17 2025\n  OS/Arch:          linux/arm64\n  Experimental:     false\n containerd:\n  Version:          v2.1.5\n  GitCommit:        fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc:\n  Version:          1.3.3\n  GitCommit:        v1.3.3-0-gd842d771\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    29.0.0\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.29.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.3\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 1\n  Running: 0\n  Paused: 0\n  Stopped: 1\n Images: 36\n Server Version: 29.0.0\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge docker_can host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc version: v1.3.3-0-gd842d771\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.8.0-85-generic\n Operating System: Ubuntu 24.04.1 LTS\n OSType: linux\n Architecture: aarch64\n CPUs: 2\n Total Memory: 3.814GiB\n Name: remotive\n ID: c50f49f1-ab3a-41a3-8442-191735ac2eaa\n Docker Root Dir: /var/lib/docker\n Debug Mode: true\n  File Descriptors: 25\n  Goroutines: 48\n  System Time: 2025-11-14T10:17:06.510963564Z\n  EventsListeners: 0\n Experimental: false\n Insecure Registries:\n  127.0.0.0/8\n  ::1/128\n Live Restore Enabled: false\n Default Address Pools:\n   Base: 172.16.0.0/12, Size: 20\n   Base: 192.168.0.0/16, Size: 24\n Firewall Backend: iptables\n```\n\n### Additional Info\n\n_No response_",
      "solution": "@robmry I missed to include that you need to install `can-utils` as well. It's availabe on most package managers. I updated the installation instructions.\n\nIf it still fails, let me know and I'll see what I can do.\n\n---\n\n> @robmry I missed to include that you need to install `can-utils` as well. It's availabe on most package managers. I updated the installation instructions.\n> \n> If it still fails, let me know and I'll see what I can do.\n\nThanks @AlbinTheander - yes, I installed that before the other packages that depended on it. But we'll take a look at the logs, hopefully they'll be enough.\n\n\n---\n\nThanks @AlbinTheander - in the logs from v28, the bridge network is connected first, so no `docker_gwbridge` endpoint is added (it's added to networks to give them external access if they don't seem to have it).\n\nIn the logs from v29, the CAN network is added first, so it gets a gwbridge endpoint - then fails because, it seems, the bridge network driver hasn't been told about the endpoint before it's asked to use it ... that's a bug, but I don't quite see where it went wrong yet, still looking.\n\nThe DockerCAN driver can set [DisableGatewayService](https://github.com/moby/moby/blob/b98c41f12442ff9f46afc1f467fb30ca3726e89d/daemon/libnetwork/drivers/remote/api/api.go#L201) in its `JoinResponse` to say it doesn't need a gateway endpoint ... if it's possible to update it, that might be a workaround. (Unless the driver's already already setting that option and it's not working?)\n\nIt'd be interesting to know if v29 works if you initially only connect the bridge network, then add the CAN network ...\n```\ndocker run --name alpine --network my_net --tty alpine\n\ndocker network connect my_can alpine\n```",
      "labels": [
        "kind/bug",
        "area/networking",
        "version/29.0"
      ],
      "created_at": "2025-11-14T10:35:43Z",
      "closed_at": "2025-11-14T23:32:33Z",
      "url": "https://github.com/moby/moby/issues/51522",
      "comments_count": 14
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51510,
      "title": "Unable to get host address via `hostname -i` in Docker container on RHEL-based distributions",
      "problem": "### Description\n\nWhen running a Docker container with `--net=host` on Rocky Linux or CentOS 7.9, the command `hostname -i` fails with \"hostname: Name or service not known\" error.\n\n\n### Reproduce\n\n1. Run a Docker container with host networking:\n```bash\ndocker run -it --rm --net=host ubuntu:noble hostname -i\n```\n\n2. Observe the error output:\n```\nhostname: Name or service not known\n```\n\n\n### Expected behavior\n\nThe `hostname -i` command should return the IP address(es) of the host machine when using `--net=host`.\n\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           29.0.0\n API version:       1.52\n Go version:        go1.25.4\n Git commit:        3d4129b\n Built:             Mon Nov 10 21:49:39 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          29.0.0\n  API version:      1.52 (minimum version 1.44)\n  Go version:       go1.25.4\n  Git commit:       d105562\n  Built:            Mon Nov 10 21:46:25 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v2.1.5\n  GitCommit:        fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc:\n  Version:          1.3.3\n  GitCommit:        v1.3.3-0-gd842d771\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    29.0.0\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.29.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.3\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 9\n  Running: 1\n  Paused: 0\n  Stopped: 8\n Images: 19\n Server Version: 29.0.0\n Storage Driver: overlay2\n  Backing Filesystem: xfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: fcd43222d6b07379a4be9786bda52438f0dd16a1\n runc version: v1.3.3-0-gd842d771\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 5.14.0-570.52.1.el9_6.x86_64\n Operating System: Rocky Linux 9.6 (Blue Onyx)\n OSType: linux\n Architecture: x86_64\n CPUs: 24\n Total Memory: 93.73GiB\n Name: QA-60-63\n ID: 26bb7370-6b0d-45e4-ab4d-99343c6edcbf\n Docker Root Dir: /data/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Registry Mirrors:\n  https://repo.zhaopin.com/\n Live Restore Enabled: false\n Firewall Backend: iptables\n\n[DEPRECATION NOTICE]: API is accessible on http://0.0.0.0:2375 without encryption.\n         Access to the remote API is equivalent to root access on the host. Refer\n         to the 'Docker daemon attack surface' section in the documentation for\n         more information: https://docs.docker.com/go/attack-surface/\nIn future versions this will be a hard failure preventing the daemon from starting! Learn more at: https://docs.docker.com/go/api-security/\n```\n\n### Additional Info\n\n## Workaround\n\nUse the `--add-host` flag to manually map the hostname to an IP address:\n\n```bash\ndocker run --net=host -it --rm --add-host <hostname>:<ip_address> ubuntu:noble hostname -i\n```",
      "solution": "> Thanks for reporting.\n> \n> What's the output of:\n> \n> $ docker run -it --rm --net=host ubuntu:noble cat /etc/hosts\n> $ cat /etc/hosts\n\n```\n# docker run -it --rm --net=host ubuntu:noble cat /etc/hosts\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n# cat /etc/hosts\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n```\n\n> And do you have SELinux enabled?\n\n```\n# sestatus\nSELinux status:                 enabled\nSELinuxfs mount:                /sys/fs/selinux\nSELinux root directory:         /etc/selinux\nLoaded policy name:             targeted\nCurrent mode:                   enforcing\nMode from config file:          enforcing\nPolicy MLS status:              enabled\nPolicy deny_unknown status:     allowed\nMemory protection checking:     actual (secure)\nMax kernel policy version:      33\n```\n\nThe issue persists after turning SELinux off using `setenforce 0`.",
      "labels": [
        "kind/bug",
        "area/networking",
        "invalid",
        "version/29.0"
      ],
      "created_at": "2025-11-13T09:03:34Z",
      "closed_at": "2025-11-14T12:17:09Z",
      "url": "https://github.com/moby/moby/issues/51510",
      "comments_count": 10
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51524,
      "title": "Docker 29.0.0 breaks Dev Containers CLI - docker events stream not received by Node.js child processes",
      "problem": "# Docker 29.0.0 breaks Dev Containers CLI - `docker events` stream not received\n\n## Summary\nDocker 29.0.0 causes the Dev Containers CLI to hang indefinitely when starting containers for the first time. The `docker events --format {{json .}} --filter event=start` stream is not properly received by Node.js child processes spawned by the Dev Containers CLI, causing timeouts.\n\n## Environment\n- **Docker Version:** 29.0.0\n- **Docker API Version:** 1.52\n- **OS:** Linux Mint 21.1 (Ubuntu 22.04 base)\n- **Kernel:** 6.8.0-87-generic\n- **Architecture:** x86_64\n- **Dev Containers CLI:** 0.75.0 (also tested with 0.56.0 - same issue)\n- **Node.js:** v22.5.1 (system) and v22.20.0 (bundled in VS Code/Cursor)\n\n## Steps to Reproduce\n\n1. Install Docker 29.0.0\n2. Create a minimal devcontainer configuration:\n\n**`.devcontainer/devcontainer.json`:**\n```json\n{\n  \"name\": \"test\",\n  \"image\": \"mcr.microsoft.com/devcontainers/base:ubuntu\",\n  \"workspaceFolder\": \"/workspace\",\n  \"workspaceMount\": \"source=${localWorkspaceFolder},target=/workspace,type=bind\"\n}\n```\n\n3. Run: `npx @devcontainers/cli@0.75.0 up --workspace-folder . --config .devcontainer/devcontainer.json`\n\n## Expected Behavior\nThe CLI should:\n1. Start `docker events` listener\n2. Create and start the container\n3. Detect the container start event\n4. Proceed with container setup\n5. Complete successfully within seconds\n\n## Actual Behavior\nThe CLI:\n1. Starts `docker events --format {{json .}} --filter event=start` as a child process\n2. Creates and starts the container successfully\n3. **Hangs indefinitely** waiting for the start event\n4. Times out after 60-120 seconds (depending on timeout settings)\n5. Never completes the setup\n\n### Log Output\n```\n[timestamp] Start: Run: docker events --format {{json .}} --filter event=start\n[timestamp] Start: Run: docker run ... (or docker compose up -d)\n[timestamp] Stop: Run: docker compose up -d\n[HANGS HERE - no further output]\n```\n\n## Verification\n\n### Docker events ARE emitted correctly:\n```bash\n$ docker events --since \"2025-11-14T11:48:24\" --until \"2025-11-14T11:48:25\" --format '{{json .}}' --filter event=start\n{\"Type\":\"container\",\"Action\":\"start\",\"Actor\":{\"ID\":\"fc596dafbc05...\"},...}\n```\n\n### Manual test WORKS:\n```bash\n# Start events listener in background\ndocker events --format '{{json .}}' --filter event=start > /tmp/events.log &\nEVENTS_PID=$!\n\n# Start container\ndocker compose up -d\n\n# Events are captured successfully\ncat /tmp/events.log\n# Output: {\"Type\":\"container\",\"Action\":\"start\",...}\n```\n\n### Dev Containers CLI from clean shell WORKS:\nWhen run as a background process (not from extension host), the CLI completes successfully:\n```bash\nnpx @devcontainers/cli@0.75.0 up ... &\n# Returns: {\"outcome\":\"success\",...}\n```\n\n### Dev Containers CLI from extension host HANGS:\nWhen run from VS Code/Cursor extension host environment, it hangs indefinitely.\n\n## Impact\n- **VS Code Dev Containers:** Hangs on first container start, works on second start (reuses existing container)\n- **Cursor Dev Containers:** Always hangs (uses `--remove-existing-container` flag, so every start is a \"first start\")\n- Affects both `image` and `dockerComposeFile` based configurations\n- Affects both official VS Code Dev Containers extension and Cursor's Anysphere Remote Containers fork\n\n## Workaround\nDowngrade to Docker 27.3.1:\n```bash\nsudo apt-get install --allow-downgrades \\\n  docker-ce=5:27.3.1-1~ubuntu.22.04~jammy \\\n  docker-ce-cli=5:27.3.1-1~ubuntu.22.04~jammy \\\n  containerd.io\n\nsudo apt-mark hold docker-ce docker-ce-cli containerd.io\n```\n\nAfter downgrading, devcontainers work correctly on first start.\n\n## Root Cause Analysis\nThe issue appears to be related to how Docker 29.0.0 handles the `/events` API endpoint streaming when consumed by Node.js child processes. Possible causes:\n1. Change in event stream buffering behavior\n2. Change in HTTP/2 or API response handling\n3. Change in how the Docker CLI formats or flushes JSON output\n4. Timing/race condition introduced in the events subsystem\n\n## Additional Notes\n- This is NOT a Dev Containers CLI bug - the CLI works correctly with Docker 27.x\n- This is NOT a configuration issue - minimal standard configs fail\n- This is NOT an environment issue - happens in clean terminals\n- The events ARE emitted correctly by Docker daemon\n- The events CAN be captured by direct `docker events` commands\n- The issue is specifically with Node.js child process consumption of the events stream\n\n## Request\nPlease investigate changes in Docker 29.0.0 that might affect event streaming to child processes, particularly when invoked via Node.js `child_process.spawn()`.\n\n---\n\n**Tested Configurations:**\n- Docker 29.0.0: \u274c FAILS\n- Docker 27.3.1: \u2705 WORKS\n\n",
      "solution": "Thanks for reporting.\n\nThis is a duplicate of https://github.com/devcontainers/cli/issues/1102 which was fixed by https://github.com/devcontainers/cli/pull/1103 (merged 2hrs ago).\n\nThe devcontainer team released a new version (0.80.2) addressing that issue: https://github.com/devcontainers/cli/blob/main/CHANGELOG.md#0802.\n\nThus, let me close this ticket but feel free to continue the conversation.",
      "labels": [
        "area/api",
        "kind/bug",
        "kind/duplicate",
        "version/29.0"
      ],
      "created_at": "2025-11-14T11:41:47Z",
      "closed_at": "2025-11-14T12:10:59Z",
      "url": "https://github.com/moby/moby/issues/51524",
      "comments_count": 1
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51521,
      "title": "Updating docker breaks Immich",
      "problem": "### Description\n\nI'm running Immich on a LXC container in Proxmox.\n\nThis is the config for the LXC container:\n\narch: amd64\ncores: 4\nfeatures: nesting=1\nhostname: Immich\nmemory: 6144\nnet0: name=eth0,bridge=vmbr0,firewall=1,gw=192.168.178.1,hwaddr=BC:24:11:AD:CA:8B,ip=192.168.178.16/24,type=veth\nonboot: 1\nostype: ubuntu\nparent: Immich_working_after_restart\nrootfs: Storage:subvol-101-disk-1,size=600G\nstartup: order=5\nswap: 6144\ntags: nas;photos\nunprivileged: 1\n\nWhen update via apt update && apt upgrade the following packages it break not allowing me to start again docker and Immich:\ncontainerd.io\ndocker-ce\ndocker-ce-cli\ndocker-ce-rootless-extras\n\nThe OS that Immich Server is running on:\nUbuntu 24.04.3 LTS (GNU/Linux 6.14.11-4-pve x86_64)\nVersion of Immich Server:\n2.2.3\nVersion of Immich Mobile App:\n2.2.3\n\n\n\n### Reproduce\n\nReproduction steps:\n1. Start the LXC\n2. Update via apt update && apt upgrade\n3. Try to access Immich\n4. Docker ps does not show anything\n5. Restarting is not possible\n\nRunning Docker inside an unprivileged LXC container on Proxmox breaks after updating to the latest Docker packages (Docker Engine v29 / containerd updates). After the upgrade, Docker cannot start containers. \nImmich services fail to start (docker ps shows no running containers) and the LXC becomes unusable until restored from a backup or snapshot.\n\n\n### Expected behavior\n\n_No response_\n\n### docker version\n\n```bash\n.\n```\n\n### docker info\n\n```bash\n.\n```\n\n### Additional Info\n\n_No response_",
      "solution": "Thanks for reporting.\n\nYou didn't include the exact error message but based on the fact that you're running docker inside of an LXC container, this is likely a duplicate of:\n\n- https://github.com/moby/moby/issues/51405\n- https://github.com/opencontainers/runc/issues/4968\n\nNote that there's a workaround provided at the bottom of this comment:\n\n- https://github.com/opencontainers/runc/issues/4972#issuecomment-3500771724",
      "labels": [
        "area/runtime",
        "kind/bug",
        "kind/duplicate",
        "version/29.0"
      ],
      "created_at": "2025-11-14T10:06:02Z",
      "closed_at": "2025-11-14T10:20:48Z",
      "url": "https://github.com/moby/moby/issues/51521",
      "comments_count": 1
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51475,
      "title": "ZFS backend broken in v29.0.0",
      "problem": "### Description\n\nAfter upgrading to `v29.0.0`, `dockerd` crashes on startup because it can't load the ZFS storage backend driver.\nWhile [the release page](https://github.com/moby/moby/releases/tag/docker-v29.0.0) mentions several \"breaking changes\", removed ZFS support does not appear to be among them, so I assume this is a bug.\n\n### Reproduce\n\n1. configure [ZFS storage backend](https://docs.docker.com/engine/storage/drivers/zfs-driver/)\n2. crash\n\n### Expected behavior\n\ndon't crash\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           29.0.0\n API version:       1.52\n Go version:        go1.25.4\n Git commit:        3d4129b\n Built:             Mon Nov 10 21:46:31 2025\n OS/Arch:           linux/amd64\n Context:           default\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    29.0.0\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.29.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.3\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n```\n\n### Additional Info\n\n```\nNov 11 11:14:42 dstank dockerd[4924]: time=\"2025-11-11T11:14:42.988491769+01:00\" level=info msg=\"Starting up\"\nNov 11 11:14:42 dstank dockerd[4924]: time=\"2025-11-11T11:14:42.989649796+01:00\" level=info msg=\"OTEL tracing is not configured, using no-op tracer provider\"\nNov 11 11:14:42 dstank dockerd[4924]: time=\"2025-11-11T11:14:42.989797369+01:00\" level=info msg=\"CDI directory does not exist, skipping: failed to monitor for changes: no such file or directory\" dir=/etc/cdi\nNov 11 11:14:42 dstank dockerd[4924]: time=\"2025-11-11T11:14:42.989826479+01:00\" level=info msg=\"CDI directory does not exist, skipping: failed to monitor for changes: no such file or directory\" dir=/var/run/cdi\nNov 11 11:14:42 dstank dockerd[4924]: time=\"2025-11-11T11:14:42.989942020+01:00\" level=info msg=\"detected 127.0.0.53 nameserver, assuming systemd-resolved, so using resolv.conf: /run/systemd/resolve/resolv.conf\"\nNov 11 11:14:43 dstank dockerd[4924]: time=\"2025-11-11T11:14:43.007009507+01:00\" level=info msg=\"Creating a containerd client\" address=/run/containerd/containerd.sock timeout=1m0s\nNov 11 11:14:43 dstank dockerd[4924]: time=\"2025-11-11T11:14:43.014541153+01:00\" level=info msg=\"Loading containers: start.\"\nNov 11 11:14:43 dstank dockerd[4924]: time=\"2025-11-11T11:14:43.016865083+01:00\" level=info msg=\"Starting daemon with containerd snapshotter integration enabled\"\nNov 11 11:14:43 dstank dockerd[4924]: time=\"2025-11-11T11:14:43.019152647+01:00\" level=warning msg=\"Preferred snapshotter not available in containerd\" message=\"lstat /var/lib/containerd/io.containerd.snapshotter.v1.zfs: no such file or directory: skip plugin\"\nNov 11 11:14:43 dstank dockerd[4924]: failed to start daemon: configured driver \"zfs\" not available: unavailable\nNov 11 11:14:43 dstank systemd[1]: docker.service: Main process exited, code=exited, status=1/FAILURE\nNov 11 11:14:43 dstank systemd[1]: docker.service: Failed with result 'exit-code'.\nNov 11 11:14:43 dstank systemd[1]: Failed to start docker.service - Docker Application Container Engine.\nNov 11 11:14:45 dstank systemd[1]: docker.service: Scheduled restart job, restart counter is at 3.\nNov 11 11:14:45 dstank systemd[1]: docker.service: Start request repeated too quickly.\nNov 11 11:14:45 dstank systemd[1]: docker.service: Failed with result 'exit-code'.\nNov 11 11:14:45 dstank systemd[1]: Failed to start docker.service - Docker Application Container Engine.\n```",
      "solution": "This is a bug which causes the daemon to use the containerd image backend when it's not supposed to.\n\nWill be fixed by: https://github.com/moby/moby/pull/51492\n\n---\n\nFor those looking for a simple solution, and you don't care about completely losing all images, volumes, (i.e. you just want to get back in business), on Ubuntu 25.04, I resolved this by doing:\n\nEdit /etc/containerd/config.toml:\n\n[plugins.\"io.containerd.grpc.v1.cri\".containerd]\n  snapshotter = \"zfs\"\n\nsudo zfs create -o mountpoint=/var/lib/containerd/io.containerd.snapshotter.v1.zfs rpool/CONTAINERD\nsudo systemctl restart containerd docker\n\n---\n\n> For those looking for a simple solution...\n\nI believe way you start fresh, i.e. lose all your images, volumes, etc.; not everyone wants that.",
      "labels": [
        "kind/bug",
        "area/storage/zfs",
        "containerd-integration",
        "version/29.0"
      ],
      "created_at": "2025-11-11T10:29:07Z",
      "closed_at": "2025-11-13T19:19:27Z",
      "url": "https://github.com/moby/moby/issues/51475",
      "comments_count": 17
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51447,
      "title": "ipvlan & rfc 4191: routes not propagated to containers",
      "problem": "### Description\n\nRoutes necessary for https://www.rfc-editor.org/rfc/rfc4191.html are not getting picked up by containers in networks of type `ipvlan`.\n\nThis notably breaks connectivity with Thread networks.\n\n### Reproduce\n\n````\nservices:\n  app:\n    image: ubuntu:latest\n    command: [\"sh\", \"-c\", \"sleep 1d\"]\n    networks:\n      ipvlan_ens3:\n    cap_add:\n      - NET_ADMIN\n    sysctls:\n      net.ipv6.conf.all.accept_ra: \"2\"\n    restart: unless-stopped\n\nnetworks:\n  ipvlan_ens3:\n    driver: ipvlan\n    driver_opts:\n      parent: ens3\n    enable_ipv6: true\n    ipam:\n      driver: default\n      config:\n        - subnet: 192.168.1.0/24\n````\n\n### Expected behavior\n\nRoutes on the host:\n````\n2001:1715:xxx:xxx::/64 dev ens3 proto ra metric 100 expires 7154sec pref medium\n2001:1715:xxx:xxx::/64 dev ens3 proto kernel metric 256 expires 7154sec pref medium\nfdaa:bbcc:ddee::/64 dev ens3 proto ra metric 100 pref medium\nfdaa:bbcc:ddee::/64 dev ens3 proto kernel metric 256 pref medium\nfdfb:3475:30a9::/64 nhid 519617722 via fe80::1427:4d5a:ee2c:1c46 dev ens3 proto ra metric 100 expires 1742sec pref medium\nfe80::/64 dev ens3 proto kernel metric 256 pref medium\ndefault nhid 2024594732 via fe80::f24d:d4ff:fe0e:9d05 dev ens3 proto ra metric 100 expires 254sec pref medium\ndefault via fe80::f24d:d4ff:fe0e:9d05 dev ens3 proto ra metric 1024 expires 254sec hoplimit 64 pref medium\n````\n\n`fdfb:3475:30a9::/64` was added dynamically through RFC 4191 - as expected\n\nRoutes on the container:\n````\n2001:1715:xxx:xxx::/64 dev eth0 proto kernel metric 256 expires 7150sec pref medium\nfdaa:bbcc:ddee::/64 dev eth0 proto kernel metric 256 pref medium\nfde8:2407:1e23::/64 dev eth0 proto kernel metric 256 pref medium\nfe80::/64 dev eth0 proto kernel metric 256 pref medium\ndefault via fde8:2407:1e23::1 dev eth0 metric 1024 pref medium\ndefault via fe80::f24d:d4ff:fe0e:9d05 dev eth0 proto ra metric 1024 expires 250sec mtu 1488 hoplimit 64 pref medium\n````\n\nThat route is missing in the container - which is the problem\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           28.5.2\n API version:       1.51\n Go version:        go1.25.3\n Git commit:        ecc6942\n Built:             Wed Nov  5 14:43:19 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          28.5.2\n  API version:      1.51 (minimum version 1.24)\n  Go version:       go1.25.3\n  Git commit:       89c5e8f\n  Built:            Wed Nov  5 14:43:19 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v1.7.29\n  GitCommit:        442cb34bda9a6a0fed82a2ca7cade05c5c749582\n runc:\n  Version:          1.3.3\n  GitCommit:        v1.3.3-0-gd842d771\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    28.5.2\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.29.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.3\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 1\n  Running: 1\n  Paused: 0\n  Stopped: 0\n Images: 3\n Server Version: 28.5.2\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: runc io.containerd.runc.v2\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 442cb34bda9a6a0fed82a2ca7cade05c5c749582\n runc version: v1.3.3-0-gd842d771\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.17.0-6-generic\n Operating System: Ubuntu 25.10\n OSType: linux\n Architecture: x86_64\n CPUs: 2\n Total Memory: 1.603GiB\n Name: matter-server\n ID: 1ca432eb-4faa-4293-97b0-6fbaa5236b37\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n```\n\n### Additional Info\n\nTuning `sysctl` around router advertisement did not help in any way.\n\nUsing `tcpdump` within the container I can see the router advertisement of type Route Information Option, but the routing table remains unaffected.",
      "solution": "Hi @FuNK3Y - I think the problem might be the route `default via fde8:2407:1e23::1 dev eth0` ... it's added by Docker in release 28.x and older, but not in the upcoming 29.0.0 release (https://github.com/moby/moby/pull/50929).\n\nAs you've given the container CAP_NET_ADMIN - you could try deleting that route to see if it helps? `ip -6 route del default via fde8:2407:1e23::1 dev eth0`\n\nIf it does - you could try a 29.0.0 release candidate, currently `29.0.0-rc.3` in the test channel (I'll add the usual caveats... it is only a release candidate, it might be broken, and there might be breaking changes). Or, wait for the 29.0.0 GA release, hopefully in this coming week.\n\nOr, with 28.x - create the network with IPv6 disabled (so Docker won't create the bad IPv6 gateway), but enable it on the container's interface. Your compose file would be this, and you might need to \"docker compose down\" to make sure the network is re-created:\n\n```\nservices:\n  app:\n    image: ubuntu:latest\n    command: [\"sh\", \"-c\", \"sleep 1d\"]\n    networks:\n      ipvlan_ens3:\n        driver_opts:\n          com.docker.network.endpoint.sysctls: net.ipv6.conf.IFNAME.disable_ipv6=0\n    restart: unless-stopped\n\nnetworks:\n  ipvlan_ens3:\n    driver: ipvlan\n    driver_opts:\n      parent: ens3\n    ipam:\n      driver: default\n      config:\n        - subnet: 192.168.1.0/24\n```\n\n\n\n---\n\nOk, I'm not sure then ... but I see the default route has the \"ra\" flag set, and you've shown that the RA you're interested in is getting to the container - so it must be close! Looking at the other sysctls, perhaps you need to set \"accept_ra_from_local\"?",
      "labels": [
        "status/0-triage",
        "kind/bug",
        "area/networking",
        "area/networking/d/ipvlan",
        "version/28.5"
      ],
      "created_at": "2025-11-09T17:27:20Z",
      "closed_at": "2025-11-10T19:56:40Z",
      "url": "https://github.com/moby/moby/issues/51447",
      "comments_count": 8
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 50035,
      "title": "TLS handshake timeout when using docker pull",
      "problem": "### Description\n\nVersions 28.0.2 up to 28.1.1 of docker produce the following error when using `docker pull hello-world` : `Error response from daemon: Get \"https://registry-1.docker.io/v2/\": net/http: TLS handshake timeout`. Going back to version 28.0.1 solves the issue.\n\nSometimes, there are other intermittent error messages, but it always ends up settling on the `TLS handshake timeout` one afterwards. Here are two different errors I've seen intermittently:\n\n* `docker: Error response from daemon: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)`\n* `Error response from daemon: Get \"https://registry-1.docker.io/v2/\": context deadline exceeded`\n\n### Reproduce\n\n1. `sudo apt install --allow-downgrades -y docker-ce=5:28.0.1-1~ubuntu.24.04~noble docker-compose-plugin=2.33.1-1~ubuntu.24.04~noble docker-buildx-plugin=0.21.1-1~ubuntu.24.04~noble docker-ce-cli=5:28.0.1-1~ubuntu.24.04~noble docker-ce-rootless-extras=5:28.0.1-1~ubuntu.24.04~noble`\n2. `docker pull ubuntu:noble`\n3. Success\n4. `sudo apt install --allow-downgrades -y docker-ce=5:28.1.1-1~ubuntu.24.04~noble docker-compose-plugin=2.35.1-1~ubuntu.24.04~noble docker-buildx-plugin=0.23.0-1~ubuntu.24.04~noble docker-ce-cli=5:28.1.1-1~ubuntu.24.04~noble docker-ce-rootless-extras=5:28.1.1-1~ubuntu.24.04~noble`\n5. `docker pull ubuntu:noble`\n6. Error response from daemon: Get \"https://registry-1.docker.io/v2/\": net/http: TLS handshake timeout\n\n### Expected behavior\n\nPull of public libraries should always succeed in the absence of any network issues.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\n Version:           28.1.1\n API version:       1.49\n Go version:        go1.23.8\n Git commit:        4eba377\n Built:             Fri Apr 18 09:52:38 2025\n OS/Arch:           linux/arm64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          28.1.1\n  API version:      1.49 (minimum version 1.24)\n  Go version:       go1.23.8\n  Git commit:       01f442b\n  Built:            Fri Apr 18 09:52:38 2025\n  OS/Arch:          linux/arm64\n  Experimental:     false\n containerd:\n  Version:          1.7.27\n  GitCommit:        05044ec0a9a75232cad458027ca83437aae3f4da\n runc:\n  Version:          1.2.5\n  GitCommit:        v1.2.5-0-g59923ef\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\n Version:    28.1.1\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.23.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.35.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 0\n  Running: 0\n  Paused: 0\n  Stopped: 0\n Images: 1\n Server Version: 28.1.1\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n Swarm: active\n  NodeID: qb6in03kkydwze74z7sk5vtsm\n  Is Manager: true\n  ClusterID: q7f68nrib57eohi2knlpch2zi\n  Managers: 1\n  Nodes: 1\n  Default Address Pool: 10.0.0.0/8  \n  SubnetSize: 24\n  Data Path Port: 4789\n  Orchestration:\n   Task History Retention Limit: 5\n  Raft:\n   Snapshot Interval: 10000\n   Number of Old Snapshots to Retain: 0\n   Heartbeat Tick: 1\n   Election Tick: 10\n  Dispatcher:\n   Heartbeat Period: 5 seconds\n  CA Configuration:\n   Expiry Duration: 3 months\n   Force Rotate: 0\n  Autolock Managers: false\n  Root Rotation In Progress: false\n  Node Address: 172.16.14.129\n  Manager Addresses:\n   172.16.14.129:2377\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 05044ec0a9a75232cad458027ca83437aae3f4da\n runc version: v1.2.5-0-g59923ef\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.8.0-59-generic\n Operating System: Ubuntu 24.04.2 LTS\n OSType: linux\n Architecture: aarch64\n CPUs: 10\n Total Memory: 31.28GiB\n Name: gaubut-ubuntu\n ID: 8a55321a-5ad9-4717-9d2e-3aed8ed46939\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n```\n\n### Additional Info\n\nDocker is installed on a virtual machine running Ubuntu Server 24.04 LTS aarch64 on a Apple arm64 machine running macOS 15.4.1. The corporate network does not have a proxy. \nI am also having timeout issues when an AWS Load Balancer tries to connect to a docker container running on a Ubuntu 24.04 LTS aarch64 server running on a Graviton (t4g) processor, or when a container running in that environment tries to connect to a service running in a different container.",
      "solution": "I can make myself available to run diagnostics or specific experiments to isolate the problem with your guidance. This issue is systematic in my environment so I can reproduce it at will.\n\n---\n\nI have noticed that new versions were published, so I went ahead and tested all of them. I am happy to report that `docker pull hello-world` now works with versions 28.2 and 28.3. So whatever the root cause was, the timeout issue seems to be solved. Also, I can still reproduce it with 28.0.2 and 28.1.1, so it doesn't seem to be another moving part.\n\n---\n\nThanks for testing! Let me file this one under \"mysteriously solved\" then \ud83d\ude05 - feel free to comment if you'd ever discover more details!",
      "labels": [
        "area/distribution",
        "status/0-triage",
        "kind/bug"
      ],
      "created_at": "2025-05-20T19:39:37Z",
      "closed_at": "2025-06-25T19:39:02Z",
      "url": "https://github.com/moby/moby/issues/50035",
      "comments_count": 30
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51363,
      "title": "Rootless Docker daemon fails to start when `DOCKERD_ROOTLESS_ROOTLESSKIT_NET` is set to `host`",
      "problem": "### Description\n\nI've been trying to run rootless Docker using the rootlesskit `host` network driver to avoid the overhead of the other drivers, but I couldn't find a way to make it work.\n\nThe first issue I encountered is that when setting only the env var `DOCKERD_ROOTLESS_ROOTLESSKIT_NET=host`, the dockerd-rootless script still passes the port driver argument to rootlesskit which in turn fails to start (`error: port driver requires non-host network`), but I could easily work around that by setting the env var `DOCKERD_ROOTLESS_ROOTLESSKIT_PORT_DRIVER` to `none`.\n\nThe second issue - which I wasn't able to overcome - is that when both the env vars above are correctly set, the rootless Docker daemon fails to start with the following error:\n```\nINFO[2025-10-31T12:28:24.789333322+01:00] stopping event stream following graceful shutdown  error=\"<nil>\" module=libcontainerd namespace=moby\nINFO[2025-10-31T12:28:24.789644275+01:00] stopping healthcheck following graceful shutdown  module=libcontainerd\nINFO[2025-10-31T12:28:24.789699959+01:00] stopping event stream following graceful shutdown  error=\"context canceled\" module=libcontainerd namespace=plugins.moby\nfailed to start daemon: Error initializing network controller: error obtaining controller instance: failed to register \"bridge\" driver: failed to create NAT chain DOCKER: iptables failed: iptables --wait -t nat -N DOCKER: iptables v1.8.11 (nf_tables): Could not fetch rule set generation id: Permission denied (you must be root)\n (exit status 4)\n[rootlesskit:child ] error: command [/usr/bin/dockerd-rootless] exited: exit status 1\n[rootlesskit:parent] error: child exited: exit status 1\n```\n\nNote that I don't get any errors when running Docker as root or when running it rootless with the default network/port driver.\n\n### Reproduce\n\nIn a non-root user session:\n```sh\n$ export DOCKER_HOST=unix:///run/user/1000/docker.sock\n$ export DOCKERD_ROOTLESS_ROOTLESSKIT_NET=host\n$ export DOCKERD_ROOTLESS_ROOTLESSKIT_PORT_DRIVER=none\n$ dockerd-rootless\n```\n\n### Expected behavior\n\nThe Docker daemon should start in rootless mode without any issues when using the rootlesskit `host` network driver.\n\n### docker version\n\n```bash\nClient:\n Version:           28.3.3\n API version:       1.51\n Go version:        go1.24.8\n Git commit:        980b85681696fbd95927fd8ded8f6d91bdca95b0\n Built:             Sun Oct 12 08:01:14 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer:\n Engine:\n  Version:          28.3.3\n  API version:      1.51 (minimum version 1.24)\n  Go version:       go1.24.8\n  Git commit:       bea959c7b793b32a893820b97c4eadc7c87fabb0\n  Built:            Sun Oct 12 08:01:14 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v2.1.3\n  GitCommit:        c787fb98911740dd3ff2d0e45ce88cdf01410486\n runc:\n  Version:          1.3.0\n  GitCommit:        4ca628d1d4c974f92d24daccb901aa078aad748e\n docker-init:\n  Version:          0.19.0\n  GitCommit:        \n rootlesskit:\n  Version:          2.3.5\n  ApiVersion:       1.1.1\n  NetworkDriver:    slirp4netns\n  PortDriver:       builtin\n  StateDir:         /run/user/1000/dockerd-rootless\n slirp4netns:\n  Version:          1.3.1\n  GitCommit:        e5e368c4f5db6ae75c2fce786e31eef9da6bf236\n```\n\n### docker info\n\n```bash\nClient:\n Version:    28.3.3\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.24.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n\nServer:\n Containers: 2\n  Running: 0\n  Paused: 0\n  Stopped: 2\n Images: 1\n Server Version: 28.3.3\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: true\n Logging Driver: json-file\n Cgroup Driver: none\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: runc io.containerd.runc.v2\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: c787fb98911740dd3ff2d0e45ce88cdf01410486\n runc version: 4ca628d1d4c974f92d24daccb901aa078aad748e\n init version: \n Security Options:\n  seccomp\n   Profile: builtin\n  rootless\n  cgroupns\n Kernel Version: 6.12.53-0-virt\n Operating System: Alpine Linux v3.22\n OSType: linux\n Architecture: x86_64\n CPUs: 8\n Total Memory: 7.765GiB\n Name: cloud\n ID: a15971cf-9c64-4860-9e61-4b35f9365595\n Docker Root Dir: /home/cloud/.local/share/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n\nWARNING: Running in rootless-mode without cgroups. Systemd is required to enable cgroups in rootless-mode.\n```\n\n### Additional Info\n\nI'm running Alpine Linux 3.22 and Docker rootless was configured following [the docs in the Alpine wiki](https://wiki.alpinelinux.org/wiki/Docker).\n`docker info` and `docker version` output was obtained by running rootless Docker with the default rootlesskit network/port driver.",
      "solution": "Thanks for the info @AkihiroSuda.\nIs there any chance that the work on that draft PR will be resumed in the near future?\n\nBy the way, if there are no other open issues for the bug I reported and - as I understand - there is interest on your side to have it fixed in the future, shouldn't this issue remain open?\n\nOn a sidenote, I think that Docker rootless docs should highlight somewhere the fact that `DOCKERD_ROOTLESS_ROOTLESSKIT_NET=host` is not supported (unless I missed it).",
      "labels": [
        "kind/question",
        "status/0-triage",
        "area/networking",
        "area/rootless"
      ],
      "created_at": "2025-10-31T12:18:22Z",
      "closed_at": "2025-10-31T20:48:02Z",
      "url": "https://github.com/moby/moby/issues/51363",
      "comments_count": 5
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 47765,
      "title": "`dockerd-rootless.sh` does not set proper MTU when manually specifying slirp4netns as NET",
      "problem": "### Description\r\n\r\nI just noticed that the MTU that my docker service is using in rootless-mode is only at `1500`, while I was sure that before it was at `65520`. I investigated the issue and noticed that the rootless docker script, [`dockerd-rootless.sh`](https://github.com/moby/moby/blob/faf84d7f0a1f2e6badff6f720a3e1e559c356fff/contrib/dockerd-rootless.sh), only set's MTU to `65520` if you DON'T specify `DOCKERD_ROOTLESS_ROOTLESSKIT_NET`. \r\n\r\nThe thing is, the [Docker docs clearly say that I should add the following systemd drop-in file to get proper source IP addresses in my containers](https://docs.docker.com/engine/security/rootless/#docker-run--p-does-not-propagate-source-ip-addresses):\r\n```toml\r\n[Service]\r\nEnvironment=\"DOCKERD_ROOTLESS_ROOTLESSKIT_NET=slirp4netns\"\r\nEnvironment=\"DOCKERD_ROOTLESS_ROOTLESSKIT_PORT_DRIVER=slirp4netns\"\r\n```\r\n\r\nBut when I add the file above, the `DOCKERD_ROOTLESS_ROOTLESSKIT_NET` environment variable is set, which in turn makes the [`dockerd-rootless.sh`](https://github.com/moby/moby/blob/faf84d7f0a1f2e6badff6f720a3e1e559c356fff/contrib/dockerd-rootless.sh) script SKIP the `slirp4netns` and `--netns-type` detection. This results in [`dockerd-rootless.sh`](https://github.com/moby/moby/blob/faf84d7f0a1f2e6badff6f720a3e1e559c356fff/contrib/dockerd-rootless.sh) using the default `1500` MTU. When I remove the `Environment=\"DOCKERD_ROOTLESS_ROOTLESSKIT_NET=slirp4netns\"` line, [`dockerd-rootless.sh`](https://github.com/moby/moby/blob/faf84d7f0a1f2e6badff6f720a3e1e559c356fff/contrib/dockerd-rootless.sh) set's a MTU of `65520`.\r\n\r\nDue to the docs saying that I can increase the MTU to \"improve performance\", I believe that the [`dockerd-rootless.sh`](https://github.com/moby/moby/blob/faf84d7f0a1f2e6badff6f720a3e1e559c356fff/contrib/dockerd-rootless.sh) should properly detect `DOCKERD_ROOTLESS_ROOTLESSKIT_NET` being `slirp4netns` and then setting the MTU to `65520` when unset.\r\n\r\nI'm open to do a PR myself.\r\n\r\n### Reproduce\r\n\r\n1. Set up docker in rootless mode\r\n2. Check the current MTU value with `systemctl --user status docker` (should be at the top in the arguments of one of the processes of `rootlesskit` or `slirp4netns`)\r\n3. [As specified in the docs](https://docs.docker.com/engine/security/rootless/#docker-run--p-does-not-propagate-source-ip-addresses), create a file at `~/.config/systemd/user/docker.service.d/override.conf` with the following contents:\r\n   ```\r\n   [Service]\r\n   Environment=\"DOCKERD_ROOTLESS_ROOTLESSKIT_NET=slirp4netns\"\r\n   Environment=\"DOCKERD_ROOTLESS_ROOTLESSKIT_PORT_DRIVER=slirp4netns\"\r\n   ```\r\n4. Run `systemctl --user daemon-reload` and `systemctl --user restart docker`\r\n4. Check the new MTU value again with `systemctl --user status docker`\r\n\r\n### Expected behavior\r\n\r\n`slirp4netns` should use an MTU of `65520`, even when manually specifying `DOCKERD_ROOTLESS_ROOTLESSKIT_NET=slirp4netns`, but NOT when specifying `DOCKERD_ROOTLESS_ROOTLESSKIT_MTU`.\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           26.1.0\r\n API version:       1.45\r\n Go version:        go1.21.9\r\n Git commit:        9714adc\r\n Built:             Mon Apr 22 17:06:56 2024\r\n OS/Arch:           linux/amd64\r\n Context:           rootless\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          26.1.0\r\n  API version:      1.45 (minimum version 1.24)\r\n  Go version:       go1.21.9\r\n  Git commit:       c8af8eb\r\n  Built:            Mon Apr 22 17:06:56 2024\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.31\r\n  GitCommit:        e377cd56a71523140ca6ae87e30244719194a521\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        v1.1.12-0-g51d5e94\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n rootlesskit:\r\n  Version:          2.0.2\r\n  ApiVersion:       1.1.1\r\n  NetworkDriver:    slirp4netns\r\n  PortDriver:       slirp4netns\r\n  StateDir:         /run/user/1000/dockerd-rootless\r\n slirp4netns:\r\n  Version:          1.2.0\r\n  GitCommit:        656041d45cfca7a4176f6b7eed9e4fe6c11e8383\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:    26.1.0\r\n Context:    rootless\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.14.0\r\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.26.1\r\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 3\r\n Server Version: 26.1.0\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: false\r\n  userxattr: true\r\n Logging Driver: local\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: e377cd56a71523140ca6ae87e30244719194a521\r\n runc version: v1.1.12-0-g51d5e94\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n  rootless\r\n  cgroupns\r\n Kernel Version: 6.1.0-20-amd64\r\n Operating System: Debian GNU/Linux 12 (bookworm)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 15.62GiB\r\n Name: ordon\r\n ID: f47f6a19-5500-4b2d-8ba9-387146e37e04\r\n Docker Root Dir: /home/jonas/.local/share/docker\r\n Debug Mode: false\r\n Username: jonasgeiler\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0/8\r\n Live Restore Enabled: false\r\n Default Address Pools:\r\n   Base: 172.17.0.0/16, Size: 16\r\n   Base: 172.18.0.0/16, Size: 16\r\n   Base: 172.19.0.0/16, Size: 16\r\n   Base: 172.20.0.0/14, Size: 16\r\n   Base: 172.24.0.0/14, Size: 16\r\n   Base: 172.28.0.0/14, Size: 16\r\n   Base: 192.168.0.0/16, Size: 20\r\n   Base: fd42:0000:03e8::/104, Size: 112\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n(I have no idea what \"MTU\" actually does.)\r\n\r\nPermalink for the code that needs adjustment in my opinion: https://github.com/moby/moby/blob/faf84d7f0a1f2e6badff6f720a3e1e559c356fff/contrib/dockerd-rootless.sh#L78-L101\r\n\r\nThe git blame says @AkihiroSuda was responsible for this part of the `dockerd-rootless.sh` script, so I'll mention them here.",
      "solution": "One possible solution is to just remove the `Environment=\"DOCKERD_ROOTLESS_ROOTLESSKIT_NET=slirp4netns\"` line from the [Docker docs](https://docs.docker.com/engine/security/rootless/#docker-run--p-does-not-propagate-source-ip-addresses), added by @dvdksn, but I think we should actually improve the `dockerd-rootless.sh` script instead.",
      "labels": [
        "status/0-triage",
        "kind/bug",
        "area/rootless"
      ],
      "created_at": "2024-04-26T13:16:45Z",
      "closed_at": "2025-10-31T12:03:24Z",
      "url": "https://github.com/moby/moby/issues/47765",
      "comments_count": 3
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51248,
      "title": "External network connectivity lost via slirp4netns, getting CURLE_RECV_ERROR (56)",
      "problem": "### Description\n\nWhen running Docker in rootless mode, the networking component (slirp4netns) can enter a hung state after days. This results in a complete loss of external network connectivity to all published ports.\nThe failure is silent from the daemon's perspective:\n\nInternal container-to-container networking remains fully functional.\n\nThe dockerd process and all containers continue to run without error.\n\n### Reproduce\n\nThis issue was not reliably reproduced , the closes was using a two-node setup designed to generate high-rate, cross-system connection churn. and with apache bench tool it gets to CURLE_COULDNT_CONNECT (7) instead\n\n### Expected behavior\n\nThe network stack should remain stable\n\n### docker version\n\n```bash\nbash-5.1$ docker version\nClient:\n Version:           24.0.5\n API version:       1.43\n Go version:        go1.20.7\n Git commit:        b74562d917\n Built:             Wed Sep  3 00:35:06 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer:\n Engine:\n  Version:          24.0.5\n  API version:      1.43 (minimum version 1.12)\n  Go version:       go1.20.7\n  Git commit:       00e46f85f6e46bb4b02c33da253f901c473794e9\n  Built:            Wed Sep  3 00:34:41 2025\n  OS/Arch:          linux/amd64\n  Experimental:     true\n containerd:\n  Version:          v1.7.20.m\n  GitCommit:        8fc6bcff51318944179630522a095cc9dbf9f353.m\n runc:\n  Version:          1.1.7+dev\n  GitCommit:        v1.0.0-rc94-766-gb6109acd-dirty\n docker-init:\n  Version:          0.19.0\n  GitCommit:        b9f42a0-dirty\n rootlesskit:\n  Version:          1.0.1\n  ApiVersion:       1.1.1\n  NetworkDriver:    slirp4netns\n  PortDriver:       builtin\n  StateDir:         /tmp/rootlesskit1655967971\n slirp4netns:\n  Version:          0.4.1\n  GitCommit:        unknown\n```\n\n### docker info\n\n```bash\nbash-5.1$ docker info\nClient:\n Version:    24.0.5\n Context:    default\n Debug Mode: false\n\nServer:\n Containers: 11\n  Running: 9\n  Paused: 0\n  Stopped: 2\n Images: 10\n Server Version: 24.0.5\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: false\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 8fc6bcff51318944179630522a095cc9dbf9f353.m\n runc version: v1.0.0-rc94-766-gb6109acd-dirty\n init version: b9f42a0-dirty\n Security Options:\n  seccomp\n   Profile: builtin\n  rootless\n  cgroupns\n Kernel Version: 5.10.209-yocto-standard\n Operating System: Wind River Linux LTS 21.20 Update 22\n OSType: linux\n Architecture: x86_64\n CPUs: 4\n Total Memory: 15.49GiB\n Name: DD\n ID: 6c3a0ea0-9e67-4237-955e-5739c47ba274\n Docker Root Dir: /home/vxuser/.local/share/docker\n Debug Mode: true\n  File Descriptors: 130\n  Goroutines: 160\n  System Time: 2025-10-21T11:11:02.182449899-06:00\n  EventsListeners: 0\n Experimental: true\n Insecure Registries:\n  127.0.0.0/8\n Live Restore Enabled: false\n\nWARNING: No cpu cfs quota support\nWARNING: No cpu cfs period support\nWARNING: No cpu shares support\nWARNING: No cpuset support\nWARNING: No io.weight support\nWARNING: No io.weight (per device) support\nWARNING: No io.max (rbps) support\nWARNING: No io.max (wbps) support\nWARNING: No io.max (riops) support\nWARNING: No io.max (wiops) support\n```\n\n### Additional Info\n\n_No response_",
      "solution": "Hi @Javier-VL - it looks you're using old/unsupported versions of:\n- Docker (24.0.5 shipped in July '23, the latest is 28.5.1)\n- rootlesskit (1.0.1 from May '22, latest 2.3.5)\n- slirp4netns (0.4.1 from ~2019 I think, the latest is 1.3.3)\n\nFrom your description, it sounds like an issue with slirp4netns rather than the moby project.\n\nWe can't investigate or fix an issue with those old versions, but you might find the problem is resolved if you can update to newer versions.\n\nWith more recent versions, you could also try the \"pasta\" network driver with rootlesskit to see if that behaves differently. But it probably wasn't supported in those older versions.\n\n---\n\nClosing, but feel free to reopen if the issue still happens with the latest release\n\n\n---\n\nthanks, while trying to recreate this sporadic error 56 I solved the curl 6 by changing the \n${DOCKERD_ROOTLESS_ROOTLESSKIT_PORT_DRIVER:=builtin}\"\nto\n: \"${DOCKERD_ROOTLESS_ROOTLESSKIT_PORT_DRIVER:=slirp4netns}\"\n\nopened a ticket in rootless kit, still can\u00b4t recreate the curl 56 on demand on my current setup, working in updating components\n\nhttps://github.com/rootless-containers/rootlesskit/issues/536",
      "labels": [
        "status/0-triage",
        "kind/bug",
        "version/unsupported",
        "area/rootless",
        "version/24.0"
      ],
      "created_at": "2025-10-21T17:09:50Z",
      "closed_at": "2025-10-22T08:22:29Z",
      "url": "https://github.com/moby/moby/issues/51248",
      "comments_count": 4
    },
    {
      "tech": "docker",
      "repo": "moby/moby",
      "issue_number": 51135,
      "title": "Can't exec into several containers: OCI runtime exec failed",
      "problem": "### Description\n\nProbably weird bug, but we pretty sure that it's related to moby and it seems that dates when we start to get it align with latest release v28.5.0\n\nWe are using [docker-in-docker feature](https://github.com/devcontainers/features/tree/main/src/docker-in-docker) in GitHub Codespaces and by default it's using moby. So recently we started to get errors when we are trying to exec into docker container:\n```\nOCI runtime exec failed: exec failed: unable to start container process: error writing config to pipe: write init-p: broken pipe: unknown\n```\n\nError dissapered when we set explicitly `\"moby\": false` option in docker-in-docker feature.\n\nOverall container is working, we can access system inside it by binded ports, but can't run `docker exec` against it. \n\nReally don't know where to look and what to check, but ready to help in someones need more info,\n\n### Reproduce\n\n1. docker run -it golang:1.24\n2. docker exec -it <container_id> /bin/bash\n\nafter that we get:\n```\nOCI runtime exec failed: exec failed: unable to start container process: error writing config to pipe: write init-p: broken pipe: unknown\n```\n\n### Expected behavior\n\nBash terminal\n\n### docker version\n\n```bash\nClient:\n Version:           28.3.3-1\n API version:       1.51\n Go version:        go1.23.11\n Git commit:        980b85681696fbd95927fd8ded8f6d91bdca95b0\n Built:             Wed Jul 16 10:32:48 UTC 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer:\n Engine:\n  Version:          28.3.3-1\n  API version:      1.51 (minimum version 1.24)\n  Go version:       go1.23.11\n  Git commit:       bea959c7b793b32a893820b97c4eadc7c87fabb0\n  Built:            Fri Jul 25 08:13:16 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.7.28-1\n  GitCommit:        b98a3aace656320842a23f4a392a33f46af97866\n runc:\n  Version:          1.3.1-1\n  GitCommit:        e6457afc48eff1ce22dece664932395026a7105e\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n```\n\n### docker info\n\n```bash\nClient:\n Version:    28.3.3-1\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.29.1\n    Path:     /usr/libexec/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.40.0\n    Path:     /usr/libexec/docker/cli-plugins/docker-compose\n\nServer:\n Containers: 62\n  Running: 47\n  Paused: 0\n  Stopped: 15\n Images: 25\n Server Version: 28.3.3-1\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: false\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: cgroupfs\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n CDI spec directories:\n  /etc/cdi\n  /var/run/cdi\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: b98a3aace656320842a23f4a392a33f46af97866\n runc version: e6457afc48eff1ce22dece664932395026a7105e\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.8.0-1030-azure\n Operating System: Ubuntu 22.04.5 LTS (containerized)\n OSType: linux\n Architecture: x86_64\n CPUs: 32\n Total Memory: 125.8GiB\n Name: codespaces-5f3aa9\n ID: a093f06a-cd14-4449-846c-36b3564585a7\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Username: codespacesdev\n Experimental: false\n Insecure Registries:\n  ::1/128\n  127.0.0.0/8\n Live Restore Enabled: false\n```\n\n### Additional Info\n\n_No response_",
      "solution": "I can confirm 1.3.0 seems to be working fine, 1.3.1 and 1.3.2 both exhibit the issue. I'll try to bisect this.\n\n---\n\ntldr; I expect to have updated packages available by EOD (GMT -7) today.\n\nThe issue is caused by Microsoft's fork of Go where as of go1.25 FIPS crypto is opt-out instead of opt-in (as it was for go1.24).\nOur runc builds were not ready for this, and due to a bug in our pipeline logic the build picked up go1.25 instead of the desired go1.24.\nThe binary was trying to load openssl and couldn't since it was in a restricted environment and bailed out.",
      "labels": [
        "status/0-triage",
        "kind/bug"
      ],
      "created_at": "2025-10-07T20:46:00Z",
      "closed_at": "2025-10-18T22:20:50Z",
      "url": "https://github.com/moby/moby/issues/51135",
      "comments_count": 12
    }
  ]
}