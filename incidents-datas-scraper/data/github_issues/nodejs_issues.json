{
  "tech": "nodejs",
  "count": 163,
  "examples": [
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 48771,
      "title": "Conditional unhandled 'error' event when http.request with `.lookup`",
      "problem": "### Version\r\n\r\n20.4.0\r\n\r\n### Platform\r\n\r\nDocker ArchLinux 6.1.35-1-lts\r\n\r\n### Subsystem\r\n\r\n_No response_\r\n\r\n### What steps will reproduce the bug?\r\n\r\nhttps://github.com/loynoir/reproduce-node-48771\r\n\r\n### How often does it reproduce? Is there a required condition?\r\n\r\n- ip blackhole like behavior\r\n\r\n### What is the expected behavior? Why is that the expected behavior?\r\n\r\nMake it possible to catch error, and exit with 0.\r\n\r\n### What do you see instead?\r\n\r\nError is not caught, and exit with 1.\r\n\r\n### Additional information\r\n\r\n_No response_",
      "solution": "FYI I tested if my network family autoselection was involved into this. Apparently is not: if you change lines 19 and 23 in `` in your repro repo to this:\r\n\r\n```\r\nif (_options.all) {\r\n  callback(null, [{ address: opt.ip, family: 4 }]);\r\n} else {\r\n  callback(null, opt.ip, 4);\r\n}\r\n```\r\n\r\nto accomodate for both single and multiple DNS lookup, then the problem will happen even if you use `--no-network-family-autoselection`.\r\n\r\nIn macOS in order to reproduce the problem you can route that IP to something unreachable. For instance:\r\n\r\n```\r\nsudo route add 192.168.144.2 10.3.0.1\r\n```\r\n\r\n(10.3.0.1 is unreachable from my system, you might have to use another IP).\r\n\r\nWith the unreachable IP setup, I tried the narrow down example in https://github.com/nodejs/node/issues/48771#issuecomment-1636634681 and it worked, so it seems like the problem is not on the `net` module but rather in `http` which is not setting the error listener fast enough.\r\n\r\n@mcollina Any thoughts on this?\n\n---\n\nIt looks like a bug, unfortunately, I don't have time to dig deep on how to fix it. I've _never_ seen this problem happen in practice, but I guess it can happen if the kernel is really fast in responding.\r\n\r\nA quick code review spotted the problem: when a socket is assigned to a ClientRequest, we defer to the next tick setting an error handler:\r\n\r\nhttps://github.com/nodejs/node/blob/a2fc4a383efe8b607bacc3256c834a4ba78c66d2/lib/_http_client.js#L860-L900\r\n\r\nThe `'error'` handler is set in `tickOnSocket`:\r\n\r\nhttps://github.com/nodejs/node/blob/a2fc4a383efe8b607bacc3256c834a4ba78c66d2/lib/_http_client.js#L822\r\n\r\nDeferring by a `nextTick` is fine for every I/O but not a synchronous DNS error: https://github.com/nodejs/node/blob/a2fc4a383efe8b607bacc3256c834a4ba78c66d2/lib/net.js#L1414-L1418.\r\n\r\nCan that happen?\r\n\r\n---\r\n\r\nAs a side note, I'd recommend you to use [`undici`](https://undici.nodejs.org) as it should better handle this case (and be easier to fix).\n\n---\n\n@mcollina  hello \r\n\r\nTo fix the issue, I made a modification to the onSocket function. I checked for the presence of an error (err) and if it exists, I immediately emitted the 'error' event using this.emit('error', err) and then called this.destroy() to terminate the request. This way, the error is handled synchronously.\r\n\r\nModified Code:\r\n\r\n ```js\r\nClientRequest.prototype.onSocket = function onSocket(socket, err) {\r\n  if (err) {\r\n    this.emit('error', err);\r\n    this.destroy();\r\n    return;\r\n  }\r\n\r\n  process.nextTick(onSocketNT, this, socket);\r\n};\r\n```\r\nOpen Questions:\r\n\r\nIs the proposed modification a correct and appropriate solution to this issue?\r\nIs there any potential downside or side effect to this modification that I should be aware of?\r\nLooking forward to your feedback on the proposed solution. If this approach is deemed appropriate, I can submit a pull request with the suggested changes.\r\n\r\nThanks!\r\n",
      "labels": [
        "confirmed-bug",
        "http",
        "net",
        "good first issue"
      ],
      "created_at": "2023-07-14T13:54:07Z",
      "closed_at": "2026-02-06T14:30:09Z",
      "url": "https://github.com/nodejs/node/issues/48771",
      "comments_count": 18
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 61689,
      "title": "Backport Undici to Node.js 22.x",
      "problem": "Was discussing this with @mcollina. In Node.js 24.x an upgrade of Undici landed that fixes a memory leak when calling `res.body.clone()`. Specifically this PR on Undici: https://github.com/nodejs/undici/pull/4419.\n\nIt would be helpful to have this available on Node.js 22. Currently we're working around the issue by having a custom response cloning function in Node.js, but the solution is not ideal and was leaking memory too in some cases. Would like to switch to the built-in clone for versions that have the fix.\n\nHope this is enough context, if not let me know!",
      "solution": "@timneutkens that commit is part of undici v6.22, which was released in Node.js v22.21.0.\n\nCan you verify on your end if you are still hitting the problem?",
      "labels": [],
      "created_at": "2026-02-05T14:07:54Z",
      "closed_at": "2026-02-05T18:55:48Z",
      "url": "https://github.com/nodejs/node/issues/61689",
      "comments_count": 4
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 61275,
      "title": "Installing Build Tools for Visual Studio messes up the MSBuild path",
      "problem": "### Version\n\n24.12.0\n\n### Platform\n\n```text\nMicrosoft Windows NT 10.0.20348.0 x64\n```\n\n### Subsystem\n\n_No response_\n\n### What steps will reproduce the bug?\n\n_[not a core node.js issue, bit I figured it's worth sharing at least as a caveat and to help others recover]_\n\nI installed via the Windows MSI, which worked great by itself. But I also chose the option to install the VS Build Tools for C++, and that ended up messing up my system. The symptom was that attempting to do a .Net build in a 64-bit terminal would fail with:\n```\nThe NuGetSdkResolver did not resolve this SDK because there was no version specified in the project or global.json\n```\nI didn't save the whole output, but the key hint from the remaining message was that it was looked for the .Net SDK under \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\MSBuild\\Sdks\\Microsoft.NET.Sdk\\Sdk\", and I don't have such a path.\nIt turned out that the Build Tools for Visual Studio 2022 installer had put a bunch of x86 paths at the front of the system PATH, including \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\\\MSBuild\\Current\\Bin\\amd64\", and that resulted in either the wrong build of msbuild.exe running (in spite of that \"amd64\" at the end of the path) or wrong base path to search for SDKs.\n\nResolved by uninstalling the Build Tools for Visual Studio 2022.\n\n\n\n\n### How often does it reproduce? Is there a required condition?\n\nSelect the option to install the Build Tools for Visual Studio\n\n### What is the expected behavior? Why is that the expected behavior?\n\n.\n\n### What do you see instead?\n\nBroken .Net SDK resolving\n\n### Additional information\n\n_No response_",
      "solution": "Thanks for the prompt attention.\nYes, I already had VS 2022 with the C++ workload type installed. In retrospect it was a mistake to select those VS Build Tools for C++ to install with Node.js, but I wasn't sure whether that's some separate packaging of the tools that's needed as is.\nSince I had the previous env.vars captured in a terminal window, I did the diff'ing and found a few paths based on \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\" were moved from the start of the PATH list to the end or even dropped. Instead, these showed up at the start:\n```\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\MSBuild\\Current\\bin\\Roslyn\nC:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.8 Tools\\\nC:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.26100.0\\\\x86\nC:\\Program Files (x86)\\Windows Kits\\10\\bin\\\\x86\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\\\MSBuild\\Current\\Bin\\amd64\nC:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\Common7\\IDE\\\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\Common7\\Tools\\\n```\n\nMy hunch is that the VS Tools installer ran in 32-bit mode, and that's why it installed into these folders and added them to the PATH.\nFeel free to archive the issue. I just hope it helps someone figure out this nasty situation and recover from it more easily. And, of course, there's the hope that the C++ Build Tools that go with VS 2026 behave more nicely.\n\n\n---\n\nClosing the issue. It will still stay visible in the issue tracker to help others with similar problems.",
      "labels": [],
      "created_at": "2026-01-04T20:00:24Z",
      "closed_at": "2026-02-03T11:11:37Z",
      "url": "https://github.com/nodejs/node/issues/61275",
      "comments_count": 3
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 39569,
      "title": "[DNS] TLSA records [HTTPS] DANE request",
      "problem": "<!--\r\nThank you for suggesting an idea to make Node.js better.\r\n\r\nPlease fill in as much of the template below as you're able.\r\n-->\r\n\r\n**Is your feature request related to a problem? Please describe.**\r\nI'd like to make an HTTPS request to a server that uses a self-signed certificate that follows the DANE protocol ([Wikipedia](https://en.wikipedia.org/wiki/DNS-based_Authentication_of_Named_Entities))\r\n\r\n**Describe the solution you'd like**\r\nI believe the best option would be an extra option on HTTPS request:\r\n```\r\nhttps.get('https://example.com', {dane: true})\r\n```\r\n\r\n**Describe alternatives you've considered**\r\nI tried to create a new `https.Agent` that forces `rejectUnauthorized: false`;\r\nThen, I got the `tlsSocket` instance in the `keylog` event and added a listener for the `secureConnect` event;\r\nThis moment I realised that the DNS api don't have a `resolveTLSA`.\r\nNot sure how to continue from here.\r\n",
      "solution": "There has been no activity on this feature request for 5 months. To help maintain relevant open issues, please add the https://github.com/nodejs/node/labels/never-stale label or close this issue if it should be closed. If not, the issue will be automatically closed 6 months after the last non-automated comment.\nFor more information on how the project manages feature requests, please consult the [feature request management document](https://github.com/nodejs/node/blob/HEAD/doc/contributing/feature-request-management.md).\n\n---\n\nCommenting so that stalebot does not close the issue. There is a PR for this: #52983 \n\n---\n\nThere has been no activity on this feature request for 5 months. To help maintain relevant open issues, please add the https://github.com/nodejs/node/labels/never-stale label or close this issue if it should be closed. If not, the issue will be automatically closed 6 months after the last non-automated comment.\nFor more information on how the project manages feature requests, please consult the [feature request management document](https://github.com/nodejs/node/blob/HEAD/doc/contributing/feature-request-management.md).",
      "labels": [
        "tls",
        "https",
        "dns",
        "feature request",
        "cares"
      ],
      "created_at": "2021-07-28T22:07:50Z",
      "closed_at": "2025-02-18T18:57:52Z",
      "url": "https://github.com/nodejs/node/issues/39569",
      "comments_count": 27
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 57125,
      "title": "require(esm) always feeds full filepaths to customization hooks but require.resolve doesn't",
      "problem": "### Version\n\nv23.7.0\n\n### Platform\n\n```text\nDarwin X-MacBook-Pro.local 23.1.0 Darwin Kernel Version 23.1.0: Mon Oct  9 21:27:24 PDT 2023; root:xnu-10002.41.9~6/RELEASE_ARM64_T6000 arm64\n```\n\n### Subsystem\n\nmodule\n\n### What steps will reproduce the bug?\n\nCreate a customization hook that intercepts `resolve` and log out the arguments while making some CJS with the patched require from being loaded via ESM; notice that randomly even though it is not possible to resolve to `file:` you get a file specifier (added some CJS hooks to show better explanation [note _load called differently in non-requirable ESM CJS]):\n\n```cjs\nconst Module = require('module');\n\n// Save the original loader so we can call it later.\nconst originalLoad = Module._load;\nconst originalResolveFilename = Module._resolveFilename;\n\nModule._resolveFilename = function(request, parent) {\n  console.log({_resolveFilename:{request,parent}})\n  return 'foo'\n  return originalResolveFilename.apply(this, arguments);\n};\nModule._load = function(request, parent, isMain) {\nconsole.log({_load:{request,parent,isMain}})\n  const exported = originalLoad.apply(this, arguments);\n  return exported;\n};\n\nlet id = 1\nModule.registerHooks({\n    resolve(specifier, context, nextResolve) {\n        console.dir({resolve:{specifier,context}}, {depth:null})\n        if (specifier.startsWith('file:')) {\n            debugger;\n        }\n        return {\n            url: `sea:${id++}`,\n            shortCircuit: true\n        }\n        return nextResolve(specifier, context)\n    },\n    load(url, context, nextLoad) {\n        console.dir({load:{url,context}}, {depth:null})\n        return {\n            source: `\n                if (module.id.endsWith(2)) throw Error('bail');\n                let dep = \"sea:bare/${String(url).slice(4)}\"\n                try {\n                    require(dep);\n                } catch (error) {\n                 // NOTE: this gives a different path to load!\n                    console.log('FAILED TO REQUIRE', dep, require.resolve(dep))\n                }\n            `,\n            format: 'commonjs',\n            shortCircuit: true\n        }\n    }\n})\nimport('bare')\n```\n\n\n### How often does it reproduce? Is there a required condition?\n\neverytime\n\n### What is the expected behavior? Why is that the expected behavior?\n\nCJS (either by default or hooked _resolveFilename resolves to a non-file [e.g. `foo`]) and then that value is passed into the loader hooks\n\n### What do you see instead?\n\n1. IDEAL: don't need to hook into _resolveFilename to make resolution work; (removing the above monkey patch to _resolveFilename still causes failure)\n2. [unify logic of](https://github.com/nodejs/node/blame/main/lib/internal/modules/esm/translators.js#L137-L156) so that instrumentation can properly deduce how to respond\n    i. due to personal preference I'd suggest if they differ to respect the altered form rather than trying to change it into a file.\n    ii. There is some odd logic around WASM and JSON that doesn't match either.\n\n### Additional information\n\n_No response_",
      "solution": "I am guessing this has something to do with the resolve hook being run again in the evaluation callback of import(cjs). Recognizing this case and skip the second resolution hook might make it go away.",
      "labels": [
        "loaders"
      ],
      "created_at": "2025-02-18T16:21:35Z",
      "closed_at": "2026-02-02T23:04:25Z",
      "url": "https://github.com/nodejs/node/issues/57125",
      "comments_count": 2
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 60835,
      "title": "[LoongArch] Compilation Error in Main Branch Due to Brotli Dependency: \"invalid argument of \u2018model\u2019 attribute\"",
      "problem": "### Version\n\nv26.0.0-pre\n\n### Platform\n\n```text\nLinux 90 6.9.0-rc3_default #1 SMP PREEMPT Thu Apr 11 12:24:48 UTC 2024 loongarch64 GNU/Linux\n```\n\n### Subsystem\n\ndeps/brotli\n\n### What steps will reproduce the bug?\n\n./configure  --openssl-no-asm && make -j32\n\n### How often does it reproduce? Is there a required condition?\n\nArchitecture: LoongArch64\nNode.js version: Main branch (latest commit)\n\n### What is the expected behavior? Why is that the expected behavior?\n\nExpected behavior: Node.js should compile successfully on the LoongArch64 architecture without compilation errors related to the brotli dependency\n\n### What do you see instead?\n\nInstead of a successful compilation, we encounter error during the brotli dependency build phase. \n\n### Additional information\n\nIn the LoongArch architecture, the upstream Node.js CI (https://ci.nodejs.org/job/node-test-commit-loongarch64/nodes=clfs23-64/835/) is encountering a compilation error related to brotli, with the specific error message being \"invalid argument of \u2018model\u2019 attribute\".\nDuring reproduction testing, we found that updating the brotli submodule to commit (e230f474) can effectively prevent Node.js build failures on the LoongArch architecture.\nThe root cause of this problem is that the GCC compiler does not support the ((model(\"small\"))) attribute.\nFor reference, please see: google/brotli#1369 and google/brotli/pull/1368.",
      "solution": "the issue was introduced in 1.2.0? https://github.com/nodejs/node/pull/60540\nWe usually have to wait for a new brotli release\n\n---\n\n> the issue was introduced in 1.2.0? [#60540](https://github.com/nodejs/node/pull/60540)\u8fd9\u4e2a\u95ee\u9898\u662f\u5728 1.2.0 \u7248\u672c\u4e2d\u5f15\u5165\u7684\u5417\uff1f [#60540](https://github.com/nodejs/node/pull/60540) We usually have to wait for a new brotli release\u6211\u4eec\u901a\u5e38\u9700\u8981\u7b49\u5f85\u65b0\u7684 brotli \u53d1\u5e03\u3002\n\nYes, this issue was indeed introduced in brotli 1.2.0 and started occurring after the merge of #60540. From what I\u2019ve observed, a fix for this problem was already implemented in brotli\u2019s codebase following the 1.2.0 release.\nGiven that official brotli releases typically take a considerable amount of time to roll out, I\u2019m wondering if it would be feasible to cherry-pick this fix directly into the deps/brotli branch of Node.js?\nThank you so much for taking the time to look into this matter\u2014I really appreciate your help! @marco-ippolito ",
      "labels": [],
      "created_at": "2025-11-24T07:36:31Z",
      "closed_at": "2026-02-02T06:56:17Z",
      "url": "https://github.com/nodejs/node/issues/60835",
      "comments_count": 5
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 61595,
      "title": "node-config-schema.json has mistakes",
      "problem": "With a schema validator which also validates the schema itself when compiling for all rules to be in non-conflict:\n`require('@exodus/schemasafe').validator(require('../../doc/node-config-schema.json'))`\n\nSame but also enforcing that every property is validated:\n`require('@exodus/schemasafe').validator(require('../../doc/node-config-schema.json'), { mode: 'strong' })`\n\nEven the first one fails\n\nExample:\nhttps://github.com/nodejs/node/blob/e67848a8ae92d3846e1e417fb55342b7ad837af8/doc/node-config-schema.json#L26-L30\n\n`minItems` is not a property of `type: 'string'` schema, it should have been on `type: 'array'` level\nIt crashes strict validators and is silently ignored in loose validators otherwise.\n\nThere seems to be many instances of that\n\nalso perhaps add a validator to lint that",
      "solution": "Here is what I think might work:\n\n## Solution\nMove constraints to their correct levels:\n```\n{\n  \"type\": \"array\",\n  \"items\": {\n    \"type\": \"string\"\n  },\n  \"minItems\": 1  // Correct location\n}\n```\n\nFor testing you could try this:\n```bash\nnode scripts/validate-schema.js doc/node-config-schema.json\n```",
      "labels": [
        "doc"
      ],
      "created_at": "2026-01-30T21:30:21Z",
      "closed_at": "2026-02-01T21:50:56Z",
      "url": "https://github.com/nodejs/node/issues/61595",
      "comments_count": 2
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 61507,
      "title": "Error [ERR_INTERNAL_ASSERTION]: This is caused by either a bug in Node.js or incorrect usage of Node.js internals.",
      "problem": "### Version\n\nNode.js v20.10.0\n\n### Platform\n\n```text\n\n```\n\n### Subsystem\n\n_No response_\n\n### What steps will reproduce the bug?\n\nnode:internal/assert:14\n    throw new ERR_INTERNAL_ASSERTION(message);\n    ^\n\nError [ERR_INTERNAL_ASSERTION]: This is caused by either a bug in Node.js or incorrect usage of Node.js internals.\nPlease open an issue with this stack trace at https://github.com/nodejs/node/issues\n\n    at assert (node:internal/assert:14:11)\n    at internalConnectMultiple (node:net:1118:3)\n    at Timeout.internalConnectMultipleTimeout (node:net:1687:3)\n    at listOnTimeout (node:internal/timers:575:11)\n    at process.processTimers (node:internal/timers:514:7) {\n  code: 'ERR_INTERNAL_ASSERTION'\n}\n\n### How often does it reproduce? Is there a required condition?\n\nnode:internal/assert:14\n    throw new ERR_INTERNAL_ASSERTION(message);\n    ^\n\nError [ERR_INTERNAL_ASSERTION]: This is caused by either a bug in Node.js or incorrect usage of Node.js internals.\nPlease open an issue with this stack trace at https://github.com/nodejs/node/issues\n\n    at assert (node:internal/assert:14:11)\n    at internalConnectMultiple (node:net:1118:3)\n    at Timeout.internalConnectMultipleTimeout (node:net:1687:3)\n    at listOnTimeout (node:internal/timers:575:11)\n    at process.processTimers (node:internal/timers:514:7) {\n  code: 'ERR_INTERNAL_ASSERTION'\n}\n\n### What is the expected behavior? Why is that the expected behavior?\n\nnode:internal/assert:14\n    throw new ERR_INTERNAL_ASSERTION(message);\n    ^\n\nError [ERR_INTERNAL_ASSERTION]: This is caused by either a bug in Node.js or incorrect usage of Node.js internals.\nPlease open an issue with this stack trace at https://github.com/nodejs/node/issues\n\n    at assert (node:internal/assert:14:11)\n    at internalConnectMultiple (node:net:1118:3)\n    at Timeout.internalConnectMultipleTimeout (node:net:1687:3)\n    at listOnTimeout (node:internal/timers:575:11)\n    at process.processTimers (node:internal/timers:514:7) {\n  code: 'ERR_INTERNAL_ASSERTION'\n}\n\n### What do you see instead?\n\nnode:internal/assert:14\n    throw new ERR_INTERNAL_ASSERTION(message);\n    ^\n\nError [ERR_INTERNAL_ASSERTION]: This is caused by either a bug in Node.js or incorrect usage of Node.js internals.\nPlease open an issue with this stack trace at https://github.com/nodejs/node/issues\n\n    at assert (node:internal/assert:14:11)\n    at internalConnectMultiple (node:net:1118:3)\n    at Timeout.internalConnectMultipleTimeout (node:net:1687:3)\n    at listOnTimeout (node:internal/timers:575:11)\n    at process.processTimers (node:internal/timers:514:7) {\n  code: 'ERR_INTERNAL_ASSERTION'\n}\n\n### Additional information\n\n_No response_",
      "solution": "[Node.js 20.10.0](https://github.com/nodejs/node/blob/main/doc/changelogs/CHANGELOG_V20.md#20.10.0) was released in November 2023\n\nHave you tried the current Node.js 20.x LTS version 20.20.0?\n\nIf the latest Node.js 20.x release still causes an issue, then steps to reproduce would be helpful. The issue includes only the error message, without describing how it happened.\n\n---\n\nplease update your node.js version, this issue has been fixed in v20.13.0",
      "labels": [],
      "created_at": "2026-01-24T21:22:28Z",
      "closed_at": "2026-02-01T09:44:34Z",
      "url": "https://github.com/nodejs/node/issues/61507",
      "comments_count": 2
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 58937,
      "title": "Test runner's output is hard to read due to blue-on-black text",
      "problem": "Node's test runner currently uses `colors.blue` for info text. This text is hard to read on a black background.\n\nWhile the exact color will vary depending on operating system, monitor color profile, and terminal settings; ANSI escape code `ESC[34m` will typically generate a blue somewhere between `(0, 0, 128)` and `(0, 0, 238)`. On a black background, this has a contrast ratio between ~1.3:1 (worst case) and ~2.2:1 (best case). That's far below the 4.5:1 to meet WCAG AA.\n\nI recognize that, historically, meeting color contrast guidelines is not in scope for command-line tools, and that end users have the ability to customize it in their terminal application.\n\nThat said, `colors.blue` is probably not an ideal choice for large amounts of text. While `colors.red` has a similar issue, its usage is limited. `colors.yellow` and `colors.green` are fine.\n\nCould there be an option to customize color output in the test runner?\n\n![Image](https://github.com/user-attachments/assets/e95bf267-c669-4bfd-aff9-9875f05f462f)",
      "solution": "There has been no activity on this feature request for 5 months. To help maintain relevant open issues, please add the https://github.com/nodejs/node/labels/never-stale label or close this issue if it should be closed. If not, the issue will be automatically closed 6 months after the last non-automated comment.\nFor more information on how the project manages feature requests, please consult the [feature request management document](https://github.com/nodejs/node/blob/HEAD/doc/contributing/feature-request-management.md).",
      "labels": [
        "feature request",
        "stale",
        "test_runner"
      ],
      "created_at": "2025-07-02T19:47:00Z",
      "closed_at": "2026-01-30T01:31:00Z",
      "url": "https://github.com/nodejs/node/issues/58937",
      "comments_count": 2
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 59118,
      "title": "`vm.SourceTextModule` memory leak",
      "problem": "The OOM can be reproduced even without `link` and `evaluate`:\n\n```js\n// Flags: --max-heap-size=16 --experimental-vm-modules\nimport {SourceTextModule, createContext} from 'vm';\n\nglobalThis.iter = 0;\nsetInterval(() => {\n   // Do not capture any variables.\n   globalThis.iter++;\n   console.log(\"iterations\", globalThis.iter);\n   const context = createContext();\n   for (let i = 0; i < 100; i++) {\n     new SourceTextModule(`\n       export default function() { }\n     `, {\n       identifier: 'foo',\n       context,\n     });\n   }\n}, 1);\n```\n \nThis crashes after around tens of iterations deterministically. \n\n _Originally posted by @legendecas in [#50113](https://github.com/nodejs/node/issues/50113#issuecomment-3059382554)_",
      "solution": "\nIt looks like @addaleax might have noticed something 4 years ago that _could_ be related:\n```c++\nbool IsNotIndicativeOfMemoryLeakAtExit() const override {\n  // XXX: The garbage collection rules for ModuleWrap are *super* unclear.\n  // Do these objects ever get GC'd? Are we just okay with leaking them?\n  return true;\n}\n```\n(this is from https://github.com/nodejs/node/pull/35490)\n\nThat said, my C++ is seriously rusty, so I'm really not sure.\n\n[This comment](https://github.com/nodejs/node/blob/cc89e4cff86ce0b3b130a38ebc35576e6bda117a/lib/internal/modules/esm/utils.js#L153-L171) in `lib/internal/modules/esm/utils.js` originally made me think the issue was here, but `callbackReferrer` doesn't seem to be leaking if I know how to correctly interpret the heaps (which I might not!).\n\nI've uploaded my heaps and the script I'm using to analyze the leak here: https://github.com/KernelDeimos/node-59118 - let me know if I'm on the right track or not; I've never contributed to node before but I ***really*** want to be able to use `vm.Module` without worrying that it will be experimental forever.\n\n\n---\n\nI think this was fixed by https://github.com/nodejs/node/pull/59117 - at least, it no longer reproduces on main. Closing.",
      "labels": [
        "vm"
      ],
      "created_at": "2025-07-18T23:00:36Z",
      "closed_at": "2026-01-29T17:00:15Z",
      "url": "https://github.com/nodejs/node/issues/59118",
      "comments_count": 3
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 43663,
      "title": "`SourceTextModule`: Using dynamic imports while passing `cachedData` throws `TypeError: Invalid host defined options`",
      "problem": "### Version\n\nv18.4.0\n\n### Platform\n\nLinux arch-linux 5.18.8-zen1-1-zen #1 ZEN SMP PREEMPT_DYNAMIC Wed, 29 Jun 2022 23:03:10 +0000 x86_64 GNU/Linux\n\n### Subsystem\n\n`node:vm`\n\n### What steps will reproduce the bug?\n\n> Needs `--experimental-vm-modules` flag\r\n```js\r\nimport { SourceTextModule } from 'node:vm'\r\n\r\nasync function linker() {\r\n  const fakeMod = new SourceTextModule('console.log(\"Hello, world!\")')\r\n  await fakeMod.link(() => {})\r\n  await fakeMod.evaluate()\r\n  return fakeMod\r\n}\r\n\r\nconst code = 'import(\"\")'\r\nconst modA = new SourceTextModule(code, {\r\n  importModuleDynamically: linker,\r\n})\r\n\r\nconst modB = new SourceTextModule(code, {\r\n  importModuleDynamically: linker,\r\n  cachedData: modA.createCachedData(),\r\n})\r\n\r\nawait modA.link(linker)\r\nawait modA.evaluate() // logs \"Hello, world!\"\r\n\r\nawait modB.link(linker)\r\nawait modB.evaluate() // TypeError: Invalid host defined options\r\n```\n\n### How often does it reproduce? Is there a required condition?\n\n100% of the time when passing `cachedData`\n\n### What is the expected behavior?\n\nDynamic imports works as expected.\n\n### What do you see instead?\n\n```\r\nvm:module(0):1\r\nimport(\"\")\r\n^\r\n\r\nTypeError: Invalid host defined options\r\n    at vm:module(0):1:1\r\n    at SourceTextModule.evaluate (node:internal/vm/module:226:23)\r\n    at file:///home/chooks/code/play/test.js:24:12\r\n```\n\n### Additional information\n\n- Works when `cachedData` is not passed.\r\n- Works when using static imports.\r\n- Breaks even if returned the same reference to `fakeMod`.",
      "solution": "I don't think this is related to #43681. The problem here is that the `v8::Module` created from cache_data has an incorrect `host_defined_options` (an empty one), rather than the one defined by the host, tracked on https://bugs.chromium.org/p/chromium/issues/detail?id=1244145. I'll reach out to cbruni to see the next steps for the issue.\n\n---\n\nI believe this is already fixed by https://github.com/nodejs/node/pull/48510 - it no longer reproduces on main. Closing.",
      "labels": [
        "confirmed-bug",
        "vm",
        "v8 engine"
      ],
      "created_at": "2022-07-03T03:26:10Z",
      "closed_at": "2026-01-29T16:48:02Z",
      "url": "https://github.com/nodejs/node/issues/43663",
      "comments_count": 8
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 60709,
      "title": "Test Runner `run()` should support `env` option",
      "problem": "### What is the problem this feature will solve?\n\nProgrammatic configuration of environment variables for test runner.\n\n### What is the feature you are proposing to solve the problem?\n\nAdd a `env` option similar to that in `spawn` or `exec`\n\n### What alternatives have you considered?\n\nMy current work around is to just set env variables directly before calling `run()`. ",
      "solution": "Hey @Ethan-Arrowood, we don't usually assign issues \ud83d\ude01 \nAny news about the state of the issue? \ud83d\ude00 ",
      "labels": [
        "feature request",
        "test_runner"
      ],
      "created_at": "2025-11-13T21:20:53Z",
      "closed_at": "2026-01-29T08:18:12Z",
      "url": "https://github.com/nodejs/node/issues/60709",
      "comments_count": 3
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 61362,
      "title": "Can not construct `Buffer` after different `Buffer` was previously transfererd",
      "problem": "### Version\n\n24.12.0\n\n### Platform\n\n```text\nnot relevant\n```\n\n### Subsystem\n\nlib/buffer\n\n### What steps will reproduce the bug?\n\n```\n> const buf = Buffer.from(btoa(\"hello\"), \"base64\");\nundefined\n> buf.buffer.transfer();\nArrayBuffer {\n  [Uint8Contents]: <2f 00 00 00 00 00 00 00 68 65 6c 6c 6f 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ... 8092 more bytes>,\n  [byteLength]: 8192\n}\n> \n> Buffer.from(btoa(\"hello\"), \"base64\");\nUncaught RangeError: \"offset\" is outside of buffer bounds\n    at Object.write (node:buffer:698:46)\n    at fromStringFast (node:buffer:480:22)\n    at fromString (node:buffer:504:51)\n    at Buffer.from (node:buffer:310:12) {\n  code: 'ERR_BUFFER_OUT_OF_BOUNDS'\n}\n```\n\nThis is the minimal reproduction. In reality this was encountered by two systems that were unaware of each other assuming the following:\n\n- System 1 assumed that `Buffer.from` returns `Uint8Array` (subclasses) that after returning are owned by the caller\n- System 2 assumed that any `Uint8Array` passed in is owned by it after passing it in\n\nSystem 2 was detaching the ArrayBuffer because it passed it to the `byobRequest.respond()` method (which transfers the buffer). In practice this can and will happen more frequently going forward, because there is now a method to detach and transfer arbitrary ArrayBuffers: `ArrayBuffer.prototype.transfer()`.\n\nSystem 1 was unaware that transfers were going to happen.\n\n### How often does it reproduce? Is there a required condition?\n\nAlways\n\n### What is the expected behavior? Why is that the expected behavior?\n\nThe expected behaviour is that `Buffer.from` should return `Uint8Array`'s that do not break when the underlying buffer of any one other returned `Uint8Array` is detached (ie, they should not use a pooled AB).\n\n### What do you see instead?\n\nAn error is thrown on unrelated `Buffer.from()` calls when the return value of a prior `Buffer.from()` is detached.\n\n### Additional information\n\nA previous mitigation for this was attempted in https://github.com/nodejs/node/pull/32759, but it is not effective anymore as `ArrayBuffer.prototype.transfer()` does not care about Node internal symbols on the `ArrayBuffer` pool.",
      "solution": "While I dislike Buffer pooling for security concerns (see linked discussion), this specific case seems like a non-issue\n\nBuffers are behaving per ECMAScript spec, whatever is dealing with Buffer or Uint8Array instances can't assume that the view they receive is non-pooled or that there are no other views on the same underlying ArrayBuffer.\n\nAny operation on `.buffer` (like what the issue does) on anything what is not a just-allocated Uint8Array is inherently unsafe and should be wrapped to properly clone input.\n\nThis and other mistakes operating directly on `.buffer` without checks affect not only pooled Buffer instances, but e.g. any result of `u8arr.subarray()` too.\n\n---\n\nSpecifically:\n\n> System 2 assumed that any Uint8Array passed in is owned by it after passing it in\n\nThis is not correct regardless of Buffer being involved or not, unless the Uint8Array is allocated by System 2 itself.\nIt will fail not just on Buffer, but on subarrays, and will destroy data not belonging to it.\n\n---\n\nThat said, https://github.com/nodejs/node/pull/61372 seems to be reasonable\n",
      "labels": [
        "buffer"
      ],
      "created_at": "2026-01-12T23:01:14Z",
      "closed_at": "2026-01-28T16:32:57Z",
      "url": "https://github.com/nodejs/node/issues/61362",
      "comments_count": 5
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 61047,
      "title": "doc: `new tls.TLSSocket()` supports`ALPNCallback` option but it is undocumented",
      "problem": "### What is the problem?\n\n`tls.createServer()` documents the `ALPNCallback` option, but the constructor\n`new tls.TLSSocket(socket[, options])` does not list `ALPNCallback` in its\nsupported `options`.\n\nHowever, in practice, `new tls.TLSSocket()` accepts `ALPNCallback` (in server\nmode with `isServer: true`) and uses it to negotiate ALPN.\n\nThis makes the feature effectively undiscoverable when using `TLSSocket`\ndirectly.\n\nAffected documentation:\n\n- https://nodejs.org/api/tls.html#new-tlstlssocketsocket-options\n\nRelated (reference docs showing the option exists on createServer):\n\n- https://nodejs.org/api/tls.html#tlscreateserveroptions-secureconnectionlistener\n\n---\n\n### Observed behavior / reproduction\n\nThe following test wraps a `net.Server` connection using `new tls.TLSSocket()`\nand provides an `ALPNCallback` option.\n\nExpected/observed results:\n\n- The callback is called once with `{ servername, protocols }`\n- Returning `\"h2\"` selects HTTP/2, and the client observes `alpnProtocol === \"h2\"`\n- Without `ALPNCallback` (control), the negotiated protocol is `false`\n\n<details>\n<summary>Test code</summary>\n\n```ts\nimport * as assert from \"node:assert\";\nimport * as fs from \"node:fs\";\nimport * as net from \"node:net\";\nimport { describe, it } from \"node:test\";\nimport * as tls from \"node:tls\";\n\nconst domain = \"ocsp.example.test\";\n\nconst certPem = fs.readFileSync(\"server.crt\");\nconst keyPem = fs.readFileSync(\"server.key\");\n\ndeclare module \"node:tls\" {\n    interface TLSSocketOptions {\n        ALPNCallback?: (opts: { servername: string | false; protocols: string[] }) => string | undefined;\n    }\n}\n\nasync function listen(server: net.Server) {\n    await new Promise<void>((resolve, reject) => {\n        server.once(\"error\", reject);\n        server.listen(0, \"127.0.0.1\", resolve);\n    });\n    const addr = server.address();\n    assert.ok(addr && typeof addr === \"object\");\n    return addr.port;\n}\n\nfunction connectClient(port: number, alpnProtocols: string[]) {\n    return new Promise<string | false | null>((resolve, reject) => {\n        const s = tls.connect(\n            {\n                host: \"127.0.0.1\",\n                port,\n                servername: domain,\n                rejectUnauthorized: false,\n                ALPNProtocols: alpnProtocols,\n                minVersion: \"TLSv1.2\",\n                maxVersion: \"TLSv1.2\",\n            },\n            () => {\n                const selected = s.alpnProtocol;\n                s.end();\n                resolve(selected);\n            },\n        );\n        s.once(\"error\", reject);\n        s.setTimeout(3_000, () => s.destroy(new Error(\"TLS handshake timeout\")));\n    });\n}\n\ndescribe(\"undocumented TLSSocket option: ALPNCallback\", () => {\n    it(\"new tls.TLSSocket({ isServer:true, ALPNCallback }) negotiates ALPN and calls callback\", async () => {\n        const calledAlpnOptions: Array<{ servername: string | false; protocols: string[] }> = [];\n\n        await using server = net.createServer((raw) => {\n            // \u2605\u3053\u3053\u304c\u691c\u8a3c\u5bfe\u8c61\uff1aTLSSocket \u306e options \u306b ALPNCallback \u3092\u6e21\u3059\n            const tlsSocket = new tls.TLSSocket(raw, {\n                isServer: true,\n                cert: certPem,\n                key: keyPem,\n                minVersion: \"TLSv1.2\",\n                maxVersion: \"TLSv1.2\",\n\n                // Undocumented (hypothesis): should be called with { servername, protocols }\n                ALPNCallback: ({ servername, protocols }) => {\n                    calledAlpnOptions.push({ servername, protocols });\n\n                    // offered list includes \"h2\" \u2192 choose it\n                    return protocols.includes(\"h2\") ? \"h2\" : undefined;\n                },\n            });\n\n            tlsSocket.once(\"secure\", () => {\n                console.log(\"server negotiated protocol:\", tlsSocket.alpnProtocol);\n                // server side should also see negotiated protocol\n                assert.strictEqual(tlsSocket.alpnProtocol, \"h2\");\n                tlsSocket.end();\n            });\n\n            tlsSocket.once(\"error\", (e) => {\n                // surface errors clearly\n                raw.destroy(e);\n            });\n        });\n\n        const port = await listen(server);\n\n        const clientSelected = await connectClient(port, [\"h2\", \"http/1.1\"]);\n        assert.strictEqual(clientSelected, \"h2\");\n        assert.deepStrictEqual(calledAlpnOptions, [{\n            servername: domain,\n            protocols: [\"h2\", \"http/1.1\"],\n        }]);\n    });\n\n    it(\"control: without ALPNCallback, negotiated protocol is false (no selection)\", async () => {\n        await using server = net.createServer((raw) => {\n            const tlsSocket = new tls.TLSSocket(raw, {\n                isServer: true,\n                cert: certPem,\n                key: keyPem,\n                minVersion: \"TLSv1.2\",\n                maxVersion: \"TLSv1.2\",\n            });\n\n            tlsSocket.once(\"secure\", () => tlsSocket.end());\n            tlsSocket.once(\"error\", (e) => raw.destroy(e));\n        });\n\n        const port = await listen(server);\n\n        const clientSelected = await connectClient(port, [\"h2\", \"http/1.1\"]);\n        assert.strictEqual(clientSelected, false);\n    });\n});\n```\n</details>\n",
      "solution": "Fixed https://github.com/nodejs/node/commit/e1fc3dc2fcf19d9278ab59c353aa1fa59290378b",
      "labels": [],
      "created_at": "2025-12-13T15:25:50Z",
      "closed_at": "2026-01-27T11:02:37Z",
      "url": "https://github.com/nodejs/node/issues/61047",
      "comments_count": 1
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 61541,
      "title": "See 'Error: Cannot find module 'typescript'' when building next.js project",
      "problem": "### Version\n\nnode.js v25.5.0 + next.js v16.1.5\n\n### Platform\n\n```text\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 24.04.3 LTS\nRelease:        24.04\nCodename:       noble\n```\n\n### Subsystem\n\n_No response_\n\n### What steps will reproduce the bug?\n\n```console\nexport NODE_ENV=production\nnpm i && npm run build\n```\n\n### How often does it reproduce? Is there a required condition?\n\nEvery time.\n\n### What is the expected behavior? Why is that the expected behavior?\n\nBuild next.js successfully.\n\n### What do you see instead?\n\n```\nnpm install && npm run build\n\nremoved 3 packages, and audited 368 packages in 1s\n\n28 packages are looking for funding\n  run `npm fund` for details\n\nfound 0 vulnerabilities\n\n> frontend@1.0.0 build\n> next build\n\n\u26a0 Installing TypeScript as it was not found while loading \"next.config.ts\".\n\nInstalling devDependencies (npm):\n- typescript\n\n\nadded 3 packages, and audited 371 packages in 1s\n\n28 packages are looking for funding\n  run `npm fund` for details\n\nfound 0 vulnerabilities\n\n\u2a2f Failed to load next.config.ts, see more info here https://nextjs.org/docs/messages/next-config-error\n\n> Build error occurred\nError: Failed to transpile \"next.config.ts\".\n    at ignore-listed frames {\n  [cause]: Error: Cannot find module 'typescript'\n  Require stack:\n  - /root/frontend/node_modules/next/dist/build/next-config-ts/transpile-config.js\n  - /root/frontend/node_modules/next/dist/server/config.js\n  - /root/frontend/node_modules/next/dist/cli/next-test.js\n  - /root/frontend/node_modules/next/dist/bin/next\n      at ignore-listed frames {\n    code: 'MODULE_NOT_FOUND',\n    requireStack: [\n      '/root/frontend/node_modules/next/dist/build/next-config-ts/transpile-config.js',\n      '/root/frontend/node_modules/next/dist/server/config.js',\n      '/root/frontend/node_modules/next/dist/cli/next-test.js',\n      '/root/frontend/node_modules/next/dist/bin/next'\n    ]\n  }\n}\n```\n\n### Additional information\n\n1. Use node.js v25.4, it's OK.\n2. If `NODE_ENV=production` is not set, it's OK.\n3. Type `npm run build` again when see the error above, it's OK.",
      "solution": "I have to run `npm run build` twice to workaround the bug.",
      "labels": [
        "needs more info"
      ],
      "created_at": "2026-01-27T05:34:02Z",
      "closed_at": "2026-01-27T07:30:47Z",
      "url": "https://github.com/nodejs/node/issues/61541",
      "comments_count": 2
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 60005,
      "title": "Cannot read properties of undefined when requiring `node:fs` with hooks",
      "problem": "### Version\n\nv22.20.0, v23.11.1, v24.8.0\n\n### Platform\n\n```text\nLinux exports-undefined 6.8.0-84-generic #84-Ubuntu SMP PREEMPT_DYNAMIC Fri Sep  5 22:36:38 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux\n```\n\n### Subsystem\n\nmodule\n\n### What steps will reproduce the bug?\n\n`register.mjs`:\n```\nimport { registerHooks } from 'node:module';\nimport { fileURLToPath } from 'node:url';\nimport { dirname, join } from 'node:path';\n\nfunction resolve(specifier, context, nextResolve) {\n\tconst path = fileURLToPath(import.meta.url);\n\tconst dir = dirname(path);\n\tswitch (specifier) {\n\t\tcase 'fs':\n\t\tcase 'node:fs':\n\t\t\tspecifier = join(dir, 'fs-mock.mjs');\t\t\t\t\t\n\t}\n\n\treturn nextResolve(specifier, context);\n};\n\nregisterHooks({ resolve });\n```\n`fs-mock.mjs`:\n```\nexport const constants = {\n\tF_OK: 42\n}\n```\n`index.cjs`:\n```\nconst fs = require('node:fs');\nconsole.log(fs.constants.F_OK);\n```\nRun `node --import=./register.mjs index.cjs`.\n\n### How often does it reproduce? Is there a required condition?\n\nHappens reliably when `fs` is being required as `node:fs`, does not happen when required as `fs`, or when imported.\n\n### What is the expected behavior? Why is that the expected behavior?\n\nShould either require `fs-mock.mjs`, or require original (builtin) `fs` module.\n\n### What do you see instead?\n\n```\nnode:internal/modules/cjs/loader:1155\n  return mod.exports;\n             ^\n\nTypeError: Cannot read properties of undefined (reading 'exports')\n    at loadBuiltinWithHooks (node:internal/modules/cjs/loader:1155:14)\n    at Function._load (node:internal/modules/cjs/loader:1197:20)\n    at TracingChannel.traceSync (node:diagnostics_channel:322:14)\n    at wrapModuleLoad (node:internal/modules/cjs/loader:237:24)\n    at Module.require (node:internal/modules/cjs/loader:1463:12)\n    at require (node:internal/modules/helpers:147:16)\n    at Object.<anonymous> (/vagrant/index.cjs:1:12)\n    at Module._compile (node:internal/modules/cjs/loader:1706:14)\n    at Object..js (node:internal/modules/cjs/loader:1839:10)\n    at Module.load (node:internal/modules/cjs/loader:1441:32)\n```\n\n### Additional information\n\nOutput from `NODE_DEBUG=* node --import=./register.mjs index.cjs`:\n[log.txt](https://github.com/user-attachments/files/22530678/log.txt)",
      "solution": "Thanks for the suggestion, but I don't think that's it:\n\n1. It works when I replace `require('node:fs')` with `require('fs')`, without changing the contents of `fs-mock.mjs`. It evens works when I directly `require('mock-fs')`, or when I `require('dummy')` and change the resolution logic accordingly.\n2. When I add a `throw` statement at the top of `fs-mock.mjs`, no error is thrown, which would seem to indicate that `fs-mock.mjs` never even gets loaded. _Update:_ Just to make extra sure, I revoked all read access to `fs-mock.mjs`, but there is no `EACCES`. This confirms the module never gets loaded at all.\n\nI know that Node's module resolution and loading code takes a different [path for loading core modules](https://github.com/nodejs/node/blob/6f941fcfba50fe7c3a342ed2066770aecaaa9b5a/lib/internal/modules/helpers.js#L117) when using `require`. I think this bug has to do with that.\n\n---\n\nI think in this case, it should use the overriden resolution result as expected. I have a fix in https://github.com/joyeecheung/node/tree/fix-node-hooks though I will need to further test it with the load hooks before sending a PR.",
      "labels": [
        "loaders"
      ],
      "created_at": "2025-09-25T07:08:28Z",
      "closed_at": "2025-12-22T14:35:47Z",
      "url": "https://github.com/nodejs/node/issues/60005",
      "comments_count": 7
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 61337,
      "title": "node -p cuts off output when piped",
      "problem": "### Version\n\nv24.12.0\n\n### Platform\n\n```text\nmacOS 26.2\n```\n\n### Subsystem\n\n_No response_\n\n### What steps will reproduce the bug?\n\n1. `seq -s ' ' 50000 >file`\n2. `node -p 'require(\"fs\").readFileSync(\"file\", \"utf-8\")' | cat`\n\n### How often does it reproduce? Is there a required condition?\n\nAlways\n\n### What is the expected behavior? Why is that the expected behavior?\n\nPrints numbers up to 50000\n\n### What do you see instead?\n\nPrints numbers up to 12773\n\n### Additional information\n\nFor comparison, run `cat file | cat`, which works correctly.",
      "solution": "Might be this - https://github.com/nodejs/node/commit/a8b21fdc904ed68cb742f4ffd690c83b5865b400 (#52172)\n\nThe problem happens from 22.0 onwards. Maybe a synchronous print needs to happen there (or use the `beforeExit` event).",
      "labels": [],
      "created_at": "2026-01-10T21:25:26Z",
      "closed_at": "2026-01-25T21:41:08Z",
      "url": "https://github.com/nodejs/node/issues/61337",
      "comments_count": 7
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 59972,
      "title": "\u5173\u4e8enode\u4e2d\u7684http2\u7f51\u7edc\u8bf7\u6c42\u8c03\u8bd5\u4e0e\u6d4f\u89c8\u5668\u7f51\u7edc\u8c03\u8bd5\u884c\u4e3a\u4e0d\u4e00\u81f4\u7684\u95ee\u9898",
      "problem": "### Version\n\nv25.0.0-pre\n\n### Platform\n\n```text\nwindows11 x86_64\n\nWindowsProductName WindowsVersion TotalPhysicalMemory CsProcessors\n------------------ -------------- ------------------- ------------\nWindows 10 Pro     2009                               {Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz}\n```\n\n### Subsystem\n\n--experimental-network-inspection\n\n### What steps will reproduce the bug?\n\n\u4f7f\u7528\u4ee5\u4e0b\u65b9\u5f0f\u53d1\u51fa\u8bf7\u6c42\u3010\u7279\u522b\u6ce8\u610f\u76ee\u6807\u7f51\u7ad9\u9700\u8981\u652f\u6301\u538b\u7f29\uff0c\u672c\u95ee\u9898\u662fbr\u538b\u7f29\uff0c\u5373\u670d\u52a1\u5668\u8fd4\u56de\u5934\u542b\u6709content-encoding\u3011\uff0c\u7136\u540e\u5728\u7f51\u7edc\u63a7\u5236\u9762\u677f\uff0c\u54cd\u5e94\u4e2d\u5373\u53ef\u770b\u5230\u4e71\u7801\u3010\u538b\u7f29\u7684\u4e8c\u8fdb\u5236\u3011\n// Node.js v18+ \u53ef\u7528\uff0c\u65e0\u9700\u5b89\u88c5\u4efb\u4f55\u5305\nconst url = 'https://nodejs.org/zh-cn/blog/release/v24.8.0';\n\nfetch(url)\n  .then(res => {\n    if (!res.ok) throw new Error(`HTTP ${res.status}`);\n    return res.text(); // \u81ea\u52a8\u5904\u7406 gzip/br \u89e3\u538b\n  })\n  .then(html => {\n    console.log('\u9875\u9762\u957f\u5ea6:', html.length);\n  })\n  .catch(console.error);\n\n\n### How often does it reproduce? Is there a required condition?\n\n\u53ea\u8981\u76ee\u6807\u670d\u52a1\u5668\u8fd4\u56de\u538b\u7f29\u6570\u636e\uff0c\u63a7\u5236\u53f0\u3010devtools\u3011\u5fc5\u5b9a\u663e\u793a\u5df2\u538b\u7f29\u7684\u6570\u636e\n\n### What is the expected behavior? Why is that the expected behavior?\n\n\u8fd9\u4e2a\u884c\u4e3a\u4e0e\u6d4f\u89c8\u5668\u8fd4\u56de\u7684\u54cd\u5e94\u7ed3\u679c\u4e0d\u4e00\u81f4\uff0c\u5b83\u5e94\u8be5\u663e\u793a\u660e\u6587\u4fe1\u606f\n\n### What do you see instead?\n\n\u5df2\u538b\u7f29\u7684\u4e8c\u8fdb\u5236\u6570\u636e\n\n### Additional information\n\n\u8fd9\u4e2a\u6211\u672a\u80fd\u4f7f\u7528AI\u5bf9\u5176\u8fdb\u884c\u4fee\u590d\uff0c\u4ee5\u4e0b\u662fAI\u5206\u6790\u7684\u62a5\u544a\uff1a\n\n\n# Analysis of Brotli Decompression Failure in Node.js NetworkAgent\n\n## 1. Problem Statement and Initial Diagnosis\n\n### 1.1. Core Issue: Brotli (`br`) Encoded Responses Not Being Decompressed\n\nThe central problem identified within the Node.js `NetworkAgent` component is its failure to correctly decompress HTTP response bodies that are encoded with Brotli (`br`). Despite the presence of a `DecompressBody` function that ostensibly handles this encoding, the system is not producing the expected plaintext output. Instead, the response data remains in its compressed, binary form, leading to garbled and unreadable content when inspected through the Node.js Inspector. This issue is particularly significant for developers relying on the Inspector's Network tab to debug application behavior, as the inability to view response bodies renders the tool ineffective for a wide range of debugging scenarios. The problem persists even after initial modifications to the source code, indicating a deeper, more systemic issue within the data handling pipeline. The core of the problem lies in the disconnect between the server sending a `br`-encoded response and the `NetworkAgent` successfully processing it into a human-readable format.\n\nThe investigation was initiated based on user-provided logs and source code, which clearly demonstrated that the `NetworkAgent` was receiving data but failing to apply the necessary decompression. The logs, which are designed to trace the flow of data through the agent, show that the `content-encoding` header is either not being detected or not being used to trigger the decompression logic. This leads to a situation where the raw, compressed data is being stored and potentially displayed as if it were the final, decompressed content. The implications of this are severe, as it not only breaks the functionality of the Inspector but also suggests a potential flaw in how Node.js handles modern web compression standards. The failure to handle Brotli, a widely adopted and efficient compression algorithm, represents a significant gap in the platform's debugging capabilities. The investigation aims to pinpoint the exact location and cause of this failure, whether it be in the header parsing, the decompression logic itself, or the data flow between these two components.\n\n### 1.2. Symptom Analysis: Log Output Indicating Unchanged Data\n\nA detailed analysis of the debug logs provided by the user reveals several key symptoms that point directly to the root cause of the decompression failure. These logs, generated by the `NetworkAgent` during the processing of network responses, offer a granular view of the data at various stages of its handling. The most telling symptom is the consistent pattern of log entries that show no change in the data size before and after the decompression attempt. This, combined with the empty `content-encoding` value in the logs, provides a clear indication that the decompression logic is not being invoked correctly. The garbled output further confirms that the data being processed is still in its compressed binary format, rather than the expected plaintext. These symptoms, when taken together, paint a clear picture of a system that is failing at the first step of the decompression process: identifying that the data is compressed in the first place.\n\n#### 1.2.1. `enc=` (Empty Encoding) in Log Entries\n\nThe most critical piece of evidence from the debug logs is the consistent appearance of `enc=` (an empty encoding string) in the log output. The log format, as defined in the `dataReceived` function, is designed to print the value of the `content_encoding` variable before attempting decompression. The fact that this variable is consistently empty across multiple log entries is a strong indicator that the `NetworkAgent` is failing to extract the `Content-Encoding` header from the HTTP response. This failure to detect the encoding is the primary reason why the `DecompressBody` function is not being called with the correct `encoding` parameter. The `DecompressBody` function has a clear conditional check at the beginning: `if (encoding.empty()) return in;`. This means that if the encoding string is empty, the function will simply return the input data without any modification, which perfectly explains the observed behavior. The empty `enc=` value is therefore not just a symptom but the direct cause of the decompression logic being bypassed entirely.\n\nThe investigation into why the `content_encoding` variable is empty points towards the `responseReceived` function, which is responsible for parsing the response headers and populating the `RequestEntry` structure. The code in `responseReceived` attempts to extract the `content-encoding` header from the `headers_obj` V8 object. However, the code uses a hardcoded string `\"content-encoding\"` to look up the header. This is a potential point of failure, as HTTP headers are case-insensitive according to the RFC specifications. It is possible that the server is sending the header as `\"Content-Encoding\"` (with capital letters), which would cause the lookup to fail and result in an empty `content_encoding` value. This hypothesis is further supported by the fact that the `curl` command, which successfully decompresses the response, does not have this issue, as it likely handles header case-insensitivity correctly. The empty `enc=` value in the logs is therefore a direct consequence of a potential bug in the header parsing logic within the `responseReceived` function.\n\n#### 1.2.2. `raw` and `decompressed` Sizes are Identical\n\nAnother significant symptom observed in the debug logs is that the `raw` and `decompressed` sizes are always identical. For example, a log entry might show `[NetworkAgent] cached: enc=, raw=1363 \u2192 decompressed=1363`. This is a direct consequence of the empty `enc=` value discussed previously. Since the `DecompressBody` function is being called with an empty `encoding` string, it immediately returns the input data without any processing. This means that the `decompressed` variable in the `dataReceived` function is simply a copy of the `raw` data, and therefore their sizes are identical. This symptom is a clear confirmation that the decompression logic is not being executed at all. If the decompression were being attempted and failing for some reason, we might expect to see different sizes, or potentially an error message. However, the identical sizes indicate that the code path for decompression is never taken.\n\nThis symptom is particularly useful because it rules out other potential causes of the problem. For example, if the `DecompressBody` function had a bug in its Brotli decompression logic, we might see the sizes change, but the output would still be garbled. The fact that the sizes are identical means that the problem is not in the decompression algorithm itself, but in the logic that determines whether or not to apply it. This further strengthens the hypothesis that the issue lies in the header parsing code in the `responseReceived` function. The identical sizes are a direct and unambiguous indicator that the `DecompressBody` function is being bypassed, which in turn is caused by the failure to detect the `Content-Encoding` header. This symptom, combined with the empty `enc=` value, provides a very strong case for the proposed solution of making the header lookup case-insensitive.\n\n#### 1.2.3. Garbled Output in Logged Response Body\n\nThe final and most visible symptom of the decompression failure is the garbled output that is printed in the debug logs when the response body is displayed. The logs show the first 200 characters of the \"decompressed\" data, but this data is clearly not human-readable. It consists of a mix of random characters, symbols, and what appears to be binary data. This is exactly what one would expect to see if a Brotli-compressed response were displayed as if it were plain text. Brotli compression produces a binary output that is not meant to be human-readable, and attempting to interpret it as text will result in the kind of garbled output seen in the logs. This symptom is a direct result of the previous two symptoms: the failure to detect the `Content-Encoding` header and the subsequent bypassing of the decompression logic.\n\nThe garbled output serves as a final confirmation that the data being stored and displayed by the `NetworkAgent` is still in its compressed form. It is the most user-visible symptom of the problem, as it is what a developer would see in the Inspector's Network tab. While the empty `enc=` value and the identical sizes are more technical indicators, the garbled output is the ultimate proof that the system is not working as intended. This symptom is particularly important because it highlights the impact of the bug on the end-user. A developer trying to debug a network request would be unable to understand the response body, which would make it very difficult to diagnose any issues with their application. The garbled output is therefore not just a symptom, but a clear demonstration of the practical impact of the bug on the usability of the Node.js Inspector.\n\n### 1.3. Initial Hypothesis: Failure to Detect `Content-Encoding` Header\n\nBased on the comprehensive analysis of the symptoms, the initial and most probable hypothesis is that the `NetworkAgent` is failing to correctly detect and extract the `Content-Encoding` header from the HTTP response. The evidence points to a specific failure in the `responseReceived` function, where the code attempts to read the `content-encoding` property from the response headers object. This failure could be due to several factors, including case-sensitivity issues (the header might be `Content-Encoding` while the code looks for `content-encoding`), the header not being present in the `params` object passed to the function, or an error in the V8 object property access logic. The empty `enc` value in the log is the smoking gun that confirms this hypothesis. The subsequent failure to decompress is a direct result of this initial detection failure. Therefore, the investigation should focus on the `responseReceived` function and the structure of the data it receives to pinpoint the exact cause of the header extraction failure.\n\n## 2. Investigation into `Content-Encoding` Header Detection\n\n### 2.1. Analysis of `responseReceived` Function\n\nThe `responseReceived` function within the `NetworkAgent` class is a critical component in the network debugging pipeline, as it is responsible for processing the initial HTTP response headers and setting up the state for subsequent data handling. A thorough analysis of this function is essential to understanding why the Brotli decompression is failing. The function's primary role is to extract key information from the response, such as the status code, headers, and MIME type, and to store this information in a `RequestEntry` object for later use. The `RequestEntry` object is then used by other functions, such as `dataReceived` and `getResponseBody`, to determine how to process the response body. Therefore, any error in the `responseReceived` function can have a cascading effect on the entire data handling process.\n\nThe investigation into the `responseReceived` function focuses on the specific code that is responsible for extracting the `Content-Encoding` header. This code is located near the end of the function and is responsible for populating the `content_encoding` member of the `RequestEntry` object. The analysis reveals a potential flaw in this code that could explain why the `content_encoding` is always empty in the debug logs. The code uses a hardcoded string `\"content-encoding\"` to look up the header in the `headers_obj` V8 object. This approach is problematic because it does not account for the case-insensitivity of HTTP headers, as defined by the HTTP/1.1 specification (RFC 7230). This could be the reason why the `NetworkAgent` is failing to detect the `Content-Encoding` header, even when it is present in the response.\n\n#### 2.1.1. Code Logic for Extracting `content-encoding`\n\nThe code responsible for extracting the `content-encoding` header is located within the `responseReceived` function and is as follows:\n\n```cpp\n// \u63d0\u53d6\u5e76\u7f13\u5b58 content-encoding\nLocal<Object> headers_obj;\nif (ObjectGetObject(context, response_obj, \"headers\").ToLocal(&headers_obj)) {\n  Local<Value> enc_val;\n  if (headers_obj\n          ->Get(context,\n                OneByteString(context->GetIsolate(), \"content-encoding\"))\n            .ToLocal(&enc_val) &&\n        enc_val->IsString()) {\n      v8::String::Utf8Value utf8(env_->isolate(), enc_val);\n      request_entry->second.content_encoding =\n          std::string(*utf8, utf8.length());\n    }\n}\n```\n\nThis code first attempts to get the `headers` object from the `response_obj`. If successful, it then attempts to get the value of the `\"content-encoding\"` property from the `headers_obj`. If the value is a string, it converts it to a UTF-8 encoded C++ string and stores it in the `content_encoding` member of the `RequestEntry` object. The logic seems straightforward, but it has a critical flaw: it uses a hardcoded, lowercase string `\"content-encoding\"` to look up the header. This is a common mistake in HTTP client implementations, as it assumes that the server will always send headers in a specific case. However, the HTTP specification states that header names are case-insensitive, and servers are free to use any case they want. Therefore, a server could send the header as `\"Content-Encoding\"`, `\"CONTENT-ENCODING\"`, or any other combination of cases, and the `NetworkAgent` would fail to detect it.\n\nThe consequence of this flaw is that the `content_encoding` member of the `RequestEntry` object will remain empty if the server uses a different case for the `Content-Encoding` header. This, in turn, will cause the `DecompressBody` function to be bypassed, as it checks if the `encoding` string is empty before attempting to decompress the data. The investigation into this code logic is therefore a key part of the overall analysis, as it provides a plausible explanation for the observed symptoms. The fix for this issue would be to make the header lookup case-insensitive, which can be done by either converting the header name to a standard case before the lookup or by iterating through all the headers and comparing their names in a case-insensitive manner.\n\n#### 2.1.2. Potential for Case-Sensitivity Issues (`content-encoding` vs `Content-Encoding`)\n\nThe potential for case-sensitivity issues is a major concern in the `responseReceived` function's logic for extracting the `Content-Encoding` header. The HTTP/1.1 specification (RFC 7230) clearly states that header field names are case-insensitive. This means that `Content-Encoding`, `content-encoding`, and `CONTENT-ENCODING` are all considered to be the same header. However, the code in the `responseReceived` function uses a hardcoded, lowercase string `\"content-encoding\"` to look up the header in the `headers_obj` V8 object. This means that if the server sends the header with any uppercase letters, the lookup will fail, and the `content_encoding` member of the `RequestEntry` object will remain empty.\n\nThis is a very likely cause of the problem, as it is common for servers to use a mix of uppercase and lowercase letters in their headers. For example, the `curl` command, which successfully decompresses the response, shows the header as `content-encoding: br` in its output, but it is possible that the server is sending it as `Content-Encoding: br` and `curl` is simply displaying it in a standardized format. The `NetworkAgent`, on the other hand, is not doing any standardization and is simply looking for the exact string `\"content-encoding\"`. This would explain why the `content_encoding` is always empty in the debug logs, even though the server is sending a `br`-encoded response.\n\nThe fix for this issue is to make the header lookup case-insensitive. This can be done in several ways. One approach is to convert the header name to a standard case (e.g., lowercase) before the lookup. Another approach is to iterate through all the headers in the `headers_obj` and compare their names to `\"content-encoding\"` in a case-insensitive manner. The latter approach is more robust, as it does not rely on any assumptions about the case of the header names. The investigation into this potential issue is therefore a key part of the overall analysis, as it provides a very plausible explanation for the observed symptoms and a clear path to a solution.\n\n#### 2.1.3. Debugging Strategy: Logging All Response Headers\n\nTo definitively diagnose the issue, a robust debugging strategy would be to log the entire `headers` object received by the `responseReceived` function. This would provide a clear view of the exact header names and values being provided by the V8 Inspector. By adding a loop to iterate over the properties of `headers_obj` and printing each key-value pair, it would be possible to confirm whether the `Content-Encoding` header is present and what its exact casing is. This would immediately reveal if the case-sensitivity hypothesis is correct or if the header is missing entirely for some other reason. The following code could be added for debugging purposes:\n\n```cpp\nLocal<v8::Array> property_names;\nif (headers_obj->GetOwnPropertyNames(context).ToLocal(&property_names)) {\n  for (uint32_t i = 0; i < property_names->Length(); ++i) {\n    Local<Value> key;\n    Local<Value> value;\n    if (property_names->Get(context, i).ToLocal(&key) &&\n        headers_obj->Get(context, key).ToLocal(&value)) {\n      v8::String::Utf8Value key_utf8(env_->isolate(), key);\n      v8::String::Utf8Value value_utf8(env_->isolate(), value);\n      fprintf(stderr, \"[NetworkAgent] Header: %s: %s\\n\", *key_utf8, *value_utf8);\n    }\n  }\n}\n```\n\nThis would provide the necessary information to understand the structure of the headers object and implement a correct and robust header extraction mechanism.\n\n### 2.2. Analysis of Inspector Protocol Events\n\nThe `NetworkAgent` communicates with the frontend (e.g., Chrome DevTools) using the Chrome DevTools Protocol (CDP). Understanding the structure of the events it consumes is crucial for correctly parsing the data.\n\n#### 2.2.1. Review of `Network.responseReceived` Parameters\n\nThe `responseReceived` function is triggered by the `Network.responseReceived` event. According to the CDP specification, this event provides detailed information about the response, including its URL, status, status text, and, most importantly, a `headers` object. The `headers` object is a dictionary where keys are header names and values are header values. The `NetworkAgent` code correctly attempts to access this `headers` object. However, the specification does not mandate the casing of the header names within this object. This ambiguity is what leads to the potential case-sensitivity issue discussed earlier. The `responseReceived` function in the `NetworkAgent` is designed to parse this event, and its failure to find the `Content-Encoding` header is a direct result of a mismatch between its expectations (lowercase header names) and the reality of the data provided by the V8 Inspector.\n\n#### 2.2.2. Examination of `Network.dataReceived` Parameters\n\nThe `dataReceived` function is triggered by the `Network.dataReceived` event. This event provides the actual response body data in chunks. The key parameter in this event is `data`, which, according to the CDP specification, is a base64-encoded string representing the chunk of the response body. However, the `NetworkAgent` code in the `dataReceived` function treats the `data` parameter as a `Uint8Array` directly:\n\n```cpp\nLocal<Object> data_obj;\nif (!ObjectGetObject(context, params, \"data\").ToLocal(&data_obj)) {\n  return;\n}\nif (!data_obj->IsUint8Array()) {\n  return;\n}\nLocal<Uint8Array> data = data_obj.As<Uint8Array>();\n```\n\nThis suggests a potential discrepancy between the expected protocol format and the actual implementation. If the V8 Inspector is indeed providing a base64-encoded string, the `data_obj->IsUint8Array()` check would fail, and the function would return early without processing the data. This would explain why no data is being decompressed or cached. The code would need to be adjusted to handle a base64 string, decode it into a binary buffer, and then proceed with the decompression. This is a critical area to investigate, as a failure to correctly parse the `data` field would prevent the entire data processing pipeline from functioning.\n\n#### 2.2.3. Hypothesis: `data` Field in `dataReceived` is Base64 Encoded\n\nBuilding on the previous point, a strong hypothesis is that the `data` field in the `Network.dataReceived` event is a base64-encoded string, as per the CDP specification, but the `NetworkAgent` code is expecting a `Uint8Array`. This mismatch would cause the `IsUint8Array()` check to fail, leading to the `dataReceived` function exiting prematurely. This would mean that no data is ever passed to the decompression logic, regardless of whether the `Content-Encoding` header was correctly detected. This hypothesis is supported by the fact that the final, successful log entry shows decompressed data, which suggests that the decompression logic itself is correct, but it is not being invoked for the individual data chunks. The fix for this would involve modifying the `dataReceived` function to first check if `data_obj` is a string, and if so, decode it from base64 into a binary buffer before proceeding with the decompression.\n\n### 2.3. Cross-Verification with External Tools\n\nTo validate the hypothesis that the server is indeed sending a Brotli-encoded response and that the issue lies within the `NetworkAgent`, it is essential to use external tools to cross-verify the behavior. By using a tool like `curl`, which is known to have robust support for HTTP compression, we can confirm that the server is capable of sending a Brotli-encoded response and that the response can be successfully decompressed. This will help to rule out the possibility that the server is not sending a `br`-encoded response in the first place, or that the response is malformed in some way. The `curl` command can be configured to specifically request a Brotli-encoded response and to automatically decompress it, which will allow us to see the plaintext content of the response.\n\nThe results of the `curl` command will provide a baseline for comparison with the behavior of the `NetworkAgent`. If `curl` is able to successfully decompress the response, then we can be confident that the issue is not with the server or the response itself, but with the `NetworkAgent`. This will allow us to focus our investigation on the `NetworkAgent` code and to look for the specific bug that is causing the decompression to fail. The cross-verification with external tools is therefore a crucial step in the debugging process, as it helps to isolate the problem and to confirm that our hypothesis is correct.\n\n#### 2.3.1. Using `curl` to Confirm Server-Side Brotli Support\n\nTo confirm that the server supports Brotli compression, the following `curl` command was executed:\n\n```bash\ncurl -v -H \"Accept-Encoding: br\" --compressed https://example.com\n```\n\nThe `-v` flag enables verbose output, which will show the request and response headers. The `-H \"Accept-Encoding: br\"` flag explicitly adds an `Accept-Encoding` header with the value `br`, which tells the server that the client is willing to accept a Brotli-encoded response. The `--compressed` flag tells `curl` to automatically decompress the response if it is compressed.\n\nThe output of the `curl` command showed the following response headers:\n\n```\n< HTTP/2 200 \n< content-type: text/html\n< etag: \"84238dfc8092e5d9c0dac8ef93371a07:1736799080.121134\"\n< last-modified: Mon, 13 Jan 2025 20:11:20 GMT\n< vary: Accept-Encoding\n< cache-control: max-age=86000\n< date: Sat, 20 Sep 2025 15:44:23 GMT\n< content-length: 1256\n< alt-svc: h3=\":443\"; ma=93600\n```\n\nThe `content-encoding: br` header is not explicitly shown in the response headers, but the `vary: Accept-Encoding` header indicates that the response may vary based on the `Accept-Encoding` header of the request. This is a strong indication that the server is capable of sending a compressed response. The fact that the `curl` command was able to successfully decompress the response and display the plaintext HTML content confirms that the server is indeed sending a Brotli-encoded response. This cross-verification with `curl` is a crucial step in the debugging process, as it confirms that the server is behaving as expected and that the issue lies within the `NetworkAgent`.\n\n#### 2.3.2. Using `curl` to Validate Successful Decompression\n\nThe `curl` command not only confirmed that the server supports Brotli compression, but it also validated that the decompression was successful. The output of the `curl` command showed the following plaintext HTML content:\n\n```html\n<!doctype html>\n<html>\n<head>\n    <title>Example Domain</title>\n    <meta charset=\"utf-8\" />\n    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n    <style type=\"text/css\">\n    body {\n        background-color: #f0f0f2;\n        margin: 0;\n        padding: 0;\n        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n    }\n    div {\n        width: 600px;\n        margin: 5em auto;\n        padding: 2em;\n        background-color: #fdfdff;\n        border-radius: 0.5em;\n        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n    }\n    a:link, a:visited {\n        color: #38488f;\n        text-decoration: none;\n    }\n    @media (max-width: 700px) {\n        div {\n            margin: 0 auto;\n            width: auto;\n        }\n    }\n    </style>    \n</head>\n<body>\n<div>\n    <h1>Example Domain</h1>\n    <p>This domain is for use in illustrative examples in documents. You may use this\n    domain in literature without prior coordination or asking for permission.</p>\n    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n</div>\n</body>\n</html>\n```\n\nThis is the expected plaintext content of the `https://example.com` webpage. The fact that `curl` was able to display this content confirms that the decompression was successful. This is in stark contrast to the behavior of the `NetworkAgent`, which is displaying garbled, binary data. The successful decompression by `curl` provides a clear baseline for comparison and confirms that the issue is not with the server or the response itself, but with the `NetworkAgent`. This validation is a crucial step in the debugging process, as it allows us to focus our investigation on the `NetworkAgent` code and to look for the specific bug that is causing the decompression to fail.\n\n## 3. Deep Dive into the `DecompressBody` Function\n\nThe investigation into the failure of Brotli decompression within the Node.js `NetworkAgent` necessitates a thorough examination of the `DecompressBody` function. This function is the core component responsible for handling the decompression of response bodies based on the `Content-Encoding` header. The provided source code and diagnostic logs indicate that the issue may stem from a combination of factors, including the logic for detecting the encoding type and the implementation of the decompression process itself. The function is designed to be a centralized utility for handling various compression formats, including Gzip, Deflate, and Brotli. However, the observed behavior suggests that the Brotli decompression path is either not being triggered correctly or is failing silently. This section will dissect the `DecompressBody` function, analyzing its structure, identifying potential weaknesses in its implementation, and exploring alternative strategies that could provide a more robust solution. The analysis will focus on the specific code paths related to Brotli, comparing the current implementation with best practices and considering the broader context of how this function is called within the `NetworkAgent`'s data handling pipeline. By understanding the intricacies of this function, we can pinpoint the precise source of the failure and formulate an effective remediation plan.\n\n### 3.1. Review of Existing Implementation\n\nThe `DecompressBody` function, as presented in the source code, is a C++ utility function designed to handle the decompression of HTTP response bodies. It takes a vector of bytes (`std::vector<uint8_t>`) and a string representing the encoding type (`const std::string& encoding`) as input. The function's primary logic is structured around a series of conditional checks that determine the decompression algorithm to be used based on the value of the `encoding` parameter. The implementation shows support for Gzip, Deflate, and Brotli, with a fallback to return the input data unchanged if the encoding is not recognized or is empty. This design suggests an attempt to create a unified decompression interface within the `NetworkAgent`. However, the effectiveness of this design is contingent on the accuracy of the `encoding` parameter and the robustness of the decompression logic for each supported format. The following subsections will provide a more detailed analysis of the function's structure and the specific implementation details for the Brotli decompression path.\n\n#### 3.1.1. Conditional Logic Based on `encoding` Parameter\n\nThe control flow of the `DecompressBody` function is dictated by a straightforward `if-else` chain that inspects the `encoding` string. The function first checks if the `encoding` string is empty, in which case it immediately returns the input data without any modification. This is a logical default behavior for responses that are not compressed. The next condition checks if the encoding is either \"gzip\" or \"deflate\". If this condition is met, the function proceeds to use the `zlib` library's `inflate` function to decompress the data. The implementation for this path is relatively detailed, involving the initialization of a `z_stream` structure, setting the appropriate window bits for Gzip or Deflate, and then entering a loop to process the input data in chunks. This suggests that the Gzip and Deflate decompression paths have been given significant attention and are likely to be more mature and robust compared to the Brotli path. The final condition in the chain checks if the encoding is \"br\", which corresponds to Brotli. If this condition is met, the function calls `BrotliDecoderDecompress` to handle the decompression. The stark difference in the level of detail between the Gzip/Deflate path and the Brotli path is a key observation and a potential indicator of where the problem lies. The simplicity of the Brotli implementation, while seemingly efficient, may be overlooking important details or error conditions that are handled more thoroughly in the Gzip/Deflate case.\n\n#### 3.1.2. Use of `BrotliDecoderDecompress` for `br` Encoding\n\nThe Brotli decompression logic within the `DecompressBody` function is implemented using the `BrotliDecoderDecompress` function from the `brotli/decode.h` library. This function is a high-level, one-shot decompression utility that takes the entire compressed input buffer and attempts to decompress it in a single call. The implementation in the provided code follows a two-pass approach. In the first pass, it calls `BrotliDecoderDecompress` with a `nullptr` for the output buffer to determine the required size of the decompressed data. If this call is successful (i.e., it returns `BROTLI_DECODER_RESULT_SUCCESS`), the function proceeds to the second pass. In the second pass, it allocates a vector of the calculated size and calls `BrotliDecoderDecompress` again, this time providing the allocated buffer to store the decompressed data. While this approach is functional, it has several potential drawbacks. Firstly, it requires two passes over the data, which can be inefficient for large responses. Secondly, it relies on the first call to accurately predict the output size, which may not always be the case for all types of Brotli-compressed data. Thirdly, the error handling is minimal; if the first call to `BrotliDecoderDecompress` fails for any reason, the function simply returns the original, uncompressed data without any indication of an error. This silent failure could explain why the response body appears garbled in the logs, as the raw, compressed data is being treated as if it were decompressed.\n\n### 3.2. Potential Issues in `DecompressBody`\n\nA closer inspection of the `DecompressBody` function reveals several potential issues that could contribute to the failure of Brotli decompression. These issues range from the handling of the decompression library's return values to the overall strategy employed for decompression. The current implementation, while seemingly straightforward, may be too simplistic to handle the complexities of real-world Brotli-compressed data. The following subsections will delve into these potential issues in more detail, providing a critical analysis of the code and highlighting areas where improvements can be made. By addressing these issues, it may be possible to create a more robust and reliable decompression mechanism that can successfully handle Brotli-encoded responses.\n\n#### 3.2.1. Incorrect Handling of `BrotliDecoderDecompress` Return Value\n\nOne of the most significant potential issues in the `DecompressBody` function is the way it handles the return value of `BrotliDecoderDecompress`. The function only checks for `BROTLI_DECODER_RESULT_SUCCESS` and treats any other return value as a failure. However, the Brotli decoder can return several other values that provide more detailed information about the state of the decompression process. For example, it can return `BROTLI_DECODER_RESULT_NEEDS_MORE_INPUT` if the input buffer does not contain a complete Brotli stream, or `BROTLI_DECODER_RESULT_ERROR` if the input data is corrupted. By not distinguishing between these different error conditions, the function may be failing to handle cases where the decompression could be completed successfully with more data or where a more specific error message could be provided. The current implementation's silent failure approach, where it simply returns the original data if the first call to `BrotliDecoderDecompress` is not successful, makes it difficult to diagnose the root cause of the problem. A more robust implementation would inspect the return value more carefully and provide appropriate error handling for each possible outcome.\n\n#### 3.2.2. Inefficient Two-Pass Decompression Strategy\n\nThe two-pass decompression strategy employed in the `DecompressBody` function, while functional, is not the most efficient approach. The need to call `BrotliDecoderDecompress` twice, first to determine the output size and then to perform the actual decompression, adds unnecessary overhead. This is particularly problematic for large response bodies, as it requires the entire compressed data to be processed twice. A more efficient approach would be to use a streaming decompression strategy, where the data is decompressed in chunks as it is received. This would not only be more efficient in terms of processing time but would also be more memory-friendly, as it would not require the entire compressed and decompressed data to be held in memory at the same time. The `brotli/decode.h` library provides functions for creating a decoder instance and processing data in a streaming fashion, which would be a more suitable approach for this use case. The current implementation's reliance on a one-shot decompression function suggests that it may not have been designed with performance and scalability in mind.\n\n#### 3.2.3. Lack of Robust Error Handling for Decompression Failures\n\nThe error handling in the `DecompressBody` function is minimal, particularly for the Brotli decompression path. As mentioned earlier, if the first call to `BrotliDecoderDecompress` fails, the function simply returns the original data without any indication of an error. This silent failure can make it very difficult to debug issues with decompression, as there is no information available about what went wrong. A more robust implementation would include comprehensive error handling that provides detailed information about any failures that occur during the decompression process. This could include logging the specific error code returned by the Brotli decoder, as well as any other relevant information, such as the size of the input data and the expected size of the output data. This would not only help with debugging but would also make the system more resilient, as it would be able to handle unexpected errors more gracefully. The current implementation's lack of robust error handling is a significant weakness that should be addressed in any future revisions.\n\n### 3.3. Alternative Decompression Strategies\n\nGiven the potential issues with the current `DecompressBody` function, it is worth considering alternative decompression strategies that could provide a more robust and efficient solution. These alternatives range from using a different approach with the same underlying library to leveraging other tools and technologies that are available within the Node.js ecosystem. The following subsections will explore some of these alternative strategies, discussing their potential benefits and drawbacks. By considering these alternatives, it may be possible to identify a more suitable approach for handling Brotli decompression in the `NetworkAgent`.\n\n#### 3.3.1. Stream-Based Decompression using `BrotliDecoder` Instance\n\nA more efficient and robust approach to Brotli decompression would be to use a streaming strategy with a `BrotliDecoder` instance. The `brotli/decode.h` library provides functions for creating and managing a decoder instance, which can be used to process data in a streaming fashion. This approach would involve creating a decoder instance, and then repeatedly calling a function to process chunks of the input data as they are received. The decompressed data would be written to an output buffer, which could then be used by the `NetworkAgent` to display the response body. This approach has several advantages over the current two-pass strategy. Firstly, it is more efficient, as it only requires a single pass over the data. Secondly, it is more memory-friendly, as it does not require the entire compressed and decompressed data to be held in memory at the same time. Thirdly, it provides more granular control over the decompression process, which can be useful for handling errors and other edge cases. The implementation of a streaming decompression strategy would be more complex than the current one-shot approach, but the benefits in terms of performance and robustness would likely be worth the additional effort.\n\n#### 3.3.2. Calling Node.js `zlib` Module from C++ via V8 API\n\nAnother alternative to the current implementation would be to leverage the Node.js `zlib` module, which provides a high-level interface for compression and decompression, including support for Brotli. The `zlib` module is a core part of the Node.js platform and is likely to be more mature and well-tested than the direct use of the `brotli/decode.h` library. The `NetworkAgent` is already a C++ component that is integrated with the V8 JavaScript engine, so it should be possible to call the `zlib` module from C++ using the V8 API. This would involve creating a JavaScript function that uses the `zlib` module to decompress the data, and then calling this function from the C++ code. This approach would have several advantages. Firstly, it would allow the `NetworkAgent` to leverage the full power and flexibility of the `zlib` module, including its support for various compression formats and its robust error handling. Secondly, it would simplify the C++ code, as the decompression logic would be implemented in JavaScript, which is often a more suitable language for this type of high-level task. The main drawback of this approach is that it would introduce a dependency on the V8 API, which could make the code more complex and potentially less portable. However, given that the `NetworkAgent` is already tightly integrated with V8, this may not be a significant concern. The following subsections will explore this alternative in more detail, providing a step-by-step guide to implementing this approach.\n\n##### 3.3.2.1. Overview of Node.js `zlib` Module for Brotli\n\nThe Node.js `zlib` module provides a comprehensive set of tools for compression and decompression, including support for the Brotli algorithm. The module offers both synchronous and asynchronous methods for Brotli decompression, such as `zlib.brotliDecompressSync()` and `zlib.brotliDecompress()` . These methods provide a high-level interface for decompressing Brotli-compressed data, and they handle many of the low-level details that are required when using the `brotli/decode.h` library directly. The `zlib` module is a core part of the Node.js platform, so it is always available and is well-documented and well-tested. The module's API is designed to be easy to use, and it provides a consistent interface for all supported compression formats. By using the `zlib` module, the `NetworkAgent` could avoid many of the potential issues that are associated with the current implementation, such as the inefficient two-pass decompression strategy and the lack of robust error handling. The `zlib` module also provides a number of advanced features, such as the ability to set various options for the decompression process, which could be useful for fine-tuning the performance and behavior of the `NetworkAgent`.\n\n##### 3.3.2.2. Using `zlib.brotliDecompressSync()` for Synchronous Decompression\n\nFor the use case of the `NetworkAgent`, where the decompression needs to be performed as part of a synchronous data handling pipeline, the `zlib.brotliDecompressSync()` method would be the most suitable choice. This method takes a buffer of compressed data as input and returns a buffer of decompressed data. It is a blocking call, which means that it will not return until the decompression is complete. This is appropriate for the `NetworkAgent`, as it needs to wait for the decompression to be finished before it can proceed with displaying the response body. The `zlib.brotliDecompressSync()` method is easy to use and provides a simple and straightforward way to decompress Brotli-compressed data. It also handles many of the low-level details of the decompression process, such as memory management and error handling, which simplifies the code that needs to be written. The use of `zlib.brotliDecompressSync()` would eliminate the need for the two-pass decompression strategy that is currently used in the `DecompressBody` function, which would improve the performance and efficiency of the decompression process.\n\n##### 3.3.2.3. V8 API for Executing JavaScript from C++\n\nTo call the `zlib.brotliDecompressSync()` method from the C++ `NetworkAgent`, it is necessary to use the V8 API to execute JavaScript code from C++. The V8 API provides a set of functions and classes that allow C++ code to interact with the V8 JavaScript engine. This includes the ability to create JavaScript objects, call JavaScript functions, and access JavaScript variables. The process of calling a JavaScript function from C++ involves several steps. Firstly, it is necessary to get a reference to the JavaScript function that needs to be called. This can be done by creating a new JavaScript script that defines the function, or by getting a reference to an existing function that is defined in the global scope. Once a reference to the function has been obtained, it can be called using the `Call()` method of the `v8::Function` class. The `Call()` method takes a reference to the JavaScript context, a reference to the object that the function should be called on (which is usually the global object), and an array of arguments to be passed to the function. The return value of the `Call()` method is a `v8::Value` object, which can be converted to a C++ type if necessary. The use of the V8 API to call the `zlib.brotliDecompressSync()` method would allow the `NetworkAgent` to leverage the power and flexibility of the `zlib` module, while still being implemented in C++.\n\n##### 3.3.2.4. Data Conversion Between C++ and V8\n\nWhen calling a JavaScript function from C++ using the V8 API, it is necessary to convert the data between C++ and V8 types. In the case of the `NetworkAgent`, the input data for the decompression process is a `std::vector<uint8_t>`, which needs to be converted to a V8 `ArrayBuffer` or `Uint8Array` before it can be passed to the `zlib.brotliDecompressSync()` method. Similarly, the output of the `zlib.brotliDecompressSync()` method is a Node.js `Buffer`, which needs to be converted back to a `std::vector<uint8_t>` so that it can be used by the C++ code. The V8 API provides a set of functions for performing these conversions. For example, a `std::vector<uint8_t>` can be converted to a V8 `ArrayBuffer` by creating a new `ArrayBuffer` object and copying the data from the vector into the buffer's backing store. Similarly, a Node.js `Buffer` can be converted to a `std::vector<uint8_t>` by getting a pointer to the buffer's data and its length, and then creating a new vector from this data. The process of converting data between C++ and V8 types can be a bit tedious, but it is a necessary step for integrating JavaScript code with C++ code using the V8 API. The following table summarizes the data conversion process:\n\n| C++ Type | V8 Type | Conversion Direction | V8 API Function |\n| :--- | :--- | :--- | :--- |\n| `std::vector<uint8_t>` | `v8::ArrayBuffer` | C++ to V8 | `v8::ArrayBuffer::New()` |\n| `v8::ArrayBuffer` | `std::vector<uint8_t>` | V8 to C++ | `v8::ArrayBuffer::GetBackingStore()` |\n| `std::vector<uint8_t>` | `v8::Uint8Array` | C++ to V8 | `v8::Uint8Array::New()` |\n| `v8::Uint8Array` | `std::vector<uint8_t>` | V8 to C++ | `v8::Uint8Array::Buffer()` and `v8::ArrayBuffer::GetBackingStore()` |\n\nBy carefully managing the data conversion process, it is possible to seamlessly integrate the `zlib` module with the C++ `NetworkAgent`, providing a more robust and efficient solution for Brotli decompression.\n\n## 4. Proposed Solution and Implementation Strategy\n\nBased on the comprehensive analysis of the Brotli decompression failure in the Node.js `NetworkAgent`, a multi-faceted solution is proposed. This strategy addresses the identified root causes and contributing factors in a prioritized manner, starting with the most critical issue and moving towards enhancements that improve the overall robustness and reliability of the system. The proposed solution is divided into three main parts: a primary fix to correct the `Content-Encoding` header detection, a secondary fix to enhance the `DecompressBody` function's robustness, and a tertiary fix to correct the data parsing logic in the `dataReceived` function. This phased approach ensures that the most critical issues are addressed first, while also laying the groundwork for a more resilient and maintainable codebase in the long run.\n\n### 4.1. Primary Fix: Correcting `Content-Encoding` Header Detection\n\nThe primary and most critical fix is to correct the logic in the `responseReceived` function that is responsible for detecting the `Content-Encoding` header. As identified in the investigation, the current implementation is case-sensitive, which is a violation of the HTTP/1.1 specification and the root cause of the decompression failure. This fix is the highest priority because it directly addresses the core problem and is a prerequisite for all other decompression-related functionality to work correctly. Without a correctly identified encoding, the `DecompressBody` function will continue to be bypassed, and the response bodies will remain in their compressed, unreadable state.\n\n#### 4.1.1. Modifying `responseReceived` to Handle Case-Insensitive Headers\n\nThe `responseReceived` function must be modified to perform a case-insensitive lookup for the `Content-Encoding` header. This can be achieved by iterating through all the properties of the `headers_obj` and comparing their names to `\"content-encoding\"` in a case-insensitive manner. This approach is more robust than simply converting the header name to a standard case, as it does not make any assumptions about the casing of the headers provided by the V8 Inspector. The implementation would involve using the `GetOwnPropertyNames` method of the V8 `Object` class to get an array of all the property names in the `headers_obj`, and then iterating through this array to find a match for the `Content-Encoding` header. Once a match is found, the corresponding value can be extracted and stored in the `content_encoding` member of the `RequestEntry` object.\n\n#### 4.1.2. Ensuring `request_entry->second.content_encoding` is Correctly Populated\n\nThe ultimate goal of modifying the `responseReceived` function is to ensure that the `request_entry->second.content_encoding` variable is correctly populated with the value of the `Content-Encoding` header. This variable is the key piece of information that is used by the `dataReceived` function to determine whether or not to decompress the response body. By implementing a case-insensitive header lookup, we can ensure that the `content_encoding` variable is populated correctly, regardless of how the server chooses to format the header. This will, in turn, ensure that the `DecompressBody` function is called with the correct `encoding` parameter, which will trigger the decompression of the response body and result in the display of the human-readable content in the Node.js Inspector.\n\n### 4.2. Secondary Fix: Enhancing `DecompressBody` Robustness\n\nOnce the primary issue of header detection is resolved, the next step is to enhance the robustness of the `DecompressBody` function. While the current implementation is functional, it has several potential weaknesses that could lead to issues in the future. These include an inefficient two-pass decompression strategy, a lack of robust error handling, and a potential for silent failures. By addressing these issues, we can create a more reliable and maintainable decompression mechanism that is better equipped to handle the complexities of real-world network traffic.\n\n#### 4.2.1. Implementing a More Reliable Brotli Decompression Flow\n\nThe current two-pass decompression strategy in the `DecompressBody` function is inefficient and could be improved by implementing a streaming decompression approach. This would involve using the `BrotliDecoder` instance from the `brotli/decode.h` library to process the data in chunks as it is received. This would not only be more efficient in terms of processing time and memory usage but would also provide more granular control over the decompression process. A streaming approach would be better suited for handling large response bodies and would be more resilient to errors and other edge cases. The implementation of a streaming decompression strategy would be a significant improvement over the current one-shot approach and would make the `DecompressBody` function more robust and scalable.\n\n#### 4.2.2. Adding Comprehensive Error Handling and Logging\n\nThe error handling in the `DecompressBody` function is minimal and could be improved by adding more comprehensive error handling and logging. This would involve inspecting the return value of the `BrotliDecoderDecompress` function more carefully and providing appropriate error handling for each possible outcome. This could include logging the specific error code returned by the Brotli decoder, as well as any other relevant information, such as the size of the input data and the expected size of the output data. This would not only help with debugging but would also make the system more resilient, as it would be able to handle unexpected errors more gracefully. The addition of comprehensive error handling and logging would be a significant improvement over the current silent failure approach and would make the `DecompressBody` function more reliable and maintainable.\n\n### 4.3. Tertiary Fix: Correcting Data Parsing in `dataReceived`\n\nThe final part of the proposed solution is to investigate and correct the data parsing logic in the `dataReceived` function. As identified in the investigation, there is a potential discrepancy between the expected data format and the actual implementation. The `NetworkAgent` code is currently expecting a `Uint8Array`, but the Chrome DevTools Protocol specification states that the `data` field is a base64-encoded string. This could be another source of error that is preventing the decompression from working correctly.\n\n#### 4.3.1. Investigating if `data` is a Base64 String vs. a `Uint8Array`\n\nThe first step in addressing this issue is to investigate the actual format of the `data` field in the `Network.dataReceived` event. This can be done by adding debug logging to the `dataReceived` function to print the type of the `data` object. This will reveal whether the data is being provided as a `Uint8Array` or as a base64-encoded string. If the data is indeed a base64-encoded string, then the `dataReceived` function will need to be modified to handle this format correctly.\n\n#### 4.3.2. Adjusting Data Parsing Logic Based on Protocol Specification\n\nIf the investigation confirms that the `data` field is a base64-encoded string, the `dataReceived` function will need to be adjusted to parse this format correctly. This would involve using a base64 decoding library to convert the string into a binary buffer, which can then be passed to the `DecompressBody` function for decompression. The `protocol::Binary` class may have a method for decoding base64 strings, or a third-party library could be used for this purpose. By adjusting the data parsing logic to match the protocol specification, we can ensure that the `dataReceived` function is correctly handling the response body data, which is a critical step in the overall decompression process.\n\n## 5. Conclusion and Recommendations\n\n### 5.1. Summary of Findings\n\nThe investigation into the failure of Brotli decompression in the Node.js `NetworkAgent` has revealed a clear and multi-faceted problem. The analysis of the provided source code and debug logs has led to the identification of a primary root cause and several contributing factors that are preventing the system from functioning as intended. The findings of this investigation can be summarized as follows:\n\n#### 5.1.1. Root Cause: Failure to Detect `Content-Encoding` Header\n\nThe primary and most critical finding of this investigation is that the `NetworkAgent` is failing to correctly detect the `Content-Encoding` header in the HTTP response. This is due to a **case-sensitivity issue** in the `responseReceived` function, where the code is looking for a lowercase `\"content-encoding\"` header, while the server may be sending it with a different casing (e.g., `\"Content-Encoding\"`). This failure to detect the encoding is the root cause of the decompression failure, as it prevents the `DecompressBody` function from being invoked with the correct `encoding` parameter. The empty `enc=` value in the debug logs is a direct and unambiguous indicator of this problem.\n\n#### 5.1.2. Contributing Factor: Potential Issues in Decompression Logic\n\nIn addition to the primary root cause, the investigation has also identified several potential issues in the `DecompressBody` function that could be contributing to the problem. These include an **inefficient two-pass decompression strategy**, a **lack of robust error handling**, and a **potential for silent failures**. While these issues may not be the primary cause of the decompression failure, they are significant weaknesses in the implementation that could lead to problems in the future. The investigation has also identified a potential discrepancy between the expected data format in the `dataReceived` function and the actual format specified by the Chrome DevTools Protocol, which could be another contributing factor to the problem.\n\n### 5.2. Recommended Code Changes\n\nBased on the findings of this investigation, the following code changes are recommended to fix the Brotli decompression failure in the Node.js `NetworkAgent`. These changes are prioritized to address the most critical issues first and to provide a comprehensive solution to the problem.\n\n#### 5.2.1. Patch for `responseReceived` Function\n\nThe `responseReceived` function must be patched to handle case-insensitive headers. The following code snippet provides a recommended implementation for this patch:\n\n```cpp\n// \u63d0\u53d6\u5e76\u7f13\u5b58 content-encoding (case-insensitive)\nLocal<Object> headers_obj;\nif (ObjectGetObject(context, response_obj, \"headers\").ToLocal(&headers_obj)) {\n  Local<v8::Array> property_names;\n  if (headers_obj->GetOwnPropertyNames(context).ToLocal(&property_names)) {\n    for (uint32_t i = 0; i < property_names->Length(); ++i) {\n      Local<Value> key;\n      if (property_names->Get(context, i).ToLocal(&key) && key->IsString()) {\n        v8::String::Utf8Value key_utf8(env_->isolate(), key);\n        std::string header_name(*key_utf8, key_utf8.length());\n        // Convert to lowercase for case-insensitive comparison\n        std::transform(header_name.begin(), header_name.end(), header_name.begin(), ::tolower);\n        if (header_name == \"content-encoding\") {\n          Local<Value> value;\n          if (headers_obj->Get(context, key).ToLocal(&value) && value->IsString()) {\n            v8::String::Utf8Value value_utf8(env_->isolate(), value);\n            request_entry->second.content_encoding = std::string(*value_utf8, value_utf8.length());\n            break; // Found the header, no need to continue\n          }\n        }\n      }\n    }\n  }\n}\n```\n\nThis patch iterates through all the properties of the `headers_obj` and performs a case-insensitive comparison to find the `Content-Encoding` header. Once the header is found, its value is extracted and stored in the `content_encoding` member of the `RequestEntry` object.\n\n#### 5.2.2. Patch for `DecompressBody` Function\n\nThe `DecompressBody` function should be patched to improve its robustness and efficiency. The following code snippet provides a recommended implementation for a streaming decompression strategy using the `BrotliDecoder` instance:\n\n```cpp\n// brotli\nif (encoding == \"br\") {\n  BrotliDecoderState* state = BrotliDecoderCreateInstance(nullptr, nullptr, nullptr);\n  if (!state) {\n    return in;\n  }\n\n  std::vector<uint8_t> out;\n  std::vector<uint8_t> buffer(16384);\n  const uint8_t* next_in = in.data();\n  size_t avail_in = in.size();\n  size_t total_out = 0;\n\n  while (true) {\n    uint8_t* next_out = buffer.data();\n    size_t avail_out = buffer.size();\n    BrotliDecoderResult result = BrotliDecoderDecompressStream(\n        state, &avail_in, &next_in, &avail_out, &next_out, &total_out);\n\n    if (result == BROTLI_DECODER_RESULT_ERROR) {\n      BrotliDecoderDestroyInstance(state);\n      return in;\n    }\n\n    out.insert(out.end(), buffer.begin(), buffer.begin() + (buffer.size() - avail_out));\n\n    if (result == BROTLI_DECODER_RESULT_SUCCESS) {\n      break;\n    }\n\n    if (result == BROTLI_DECODER_RESULT_NEEDS_MORE_INPUT) {\n      // This should not happen in a one-shot decompression, but handle it just in case\n      BrotliDecoderDestroyInstance(state);\n      return in;\n    }\n  }\n\n  BrotliDecoderDestroyInstance(state);\n  return out.empty() ? in : out;\n}\n```\n\nThis patch implements a streaming decompression strategy that is more efficient and robust than the current two-pass approach. It also includes more comprehensive error handling to provide better feedback in case of failures.\n\n#### 5.2.3. Patch for `dataReceived` Function (if necessary)\n\nIf the investigation confirms that the `data` field in the `Network.dataReceived` event is a base64-encoded string, the `dataReceived` function will need to be patched to handle this format correctly. The following code snippet provides a recommended implementation for this patch:\n\n```cpp\nLocal<Object> data_obj;\nif (!ObjectGetObject(context, params, \"data\").ToLocal(&data_obj)) {\n  return;\n}\n\nstd::vector<uint8_t> raw;\nif (data_obj->IsUint8Array()) {\n  Local<Uint8Array> data = data_obj.As<Uint8Array>();\n  auto* raw_ptr = static_cast<const uint8_t*>(data->Buffer()->GetBackingStore()->Data());\n  raw = std::vector<uint8_t>(raw_ptr, raw_ptr + data->ByteLength());\n} else if (data_obj->IsString()) {\n  v8::String::Utf8Value data_utf8(env_->isolate(), data_obj);\n  std::string data_str(*data_utf8, data_utf8.length());\n  // Decode base64 string to binary data\n  // This would require a base64 decoding library\n  // raw = base64_decode(data_str);\n} else {\n  return;\n}\n```\n\nThis patch checks the type of the `data` object and handles both `Uint8Array` and string formats. If the data is a string, it is decoded from base64 into a binary buffer before being passed to the `DecompressBody` function.\n\n### 5.3. Future Considerations\n\nIn addition to the immediate code changes recommended above, there are several future considerations that should be taken into account to improve the overall quality and maintainability of the `NetworkAgent` codebase. These considerations are not directly related to the Brotli decompression failure, but they are important for ensuring the long-term health and stability of the system.\n\n#### 5.3.1. Adding Unit Tests for Decompression Logic\n\nOne of the most important future considerations is to add a comprehensive suite of unit tests for the decompression logic in the `NetworkAgent`. This would involve creating tests for each of the supported compression formats (Gzip, Deflate, and Brotli) to ensure that the decompression is working correctly for a variety of different inputs. The tests should cover both successful decompression scenarios and error scenarios, such as corrupted data or incomplete streams. The addition of unit tests would not only help to prevent regressions in the future but would also make it easier to identify and fix bugs in the decompression logic.\n\n#### 5.3.2. Investigating Other Potential Edge Cases in Network Data Handling\n\nFinally, it would be beneficial to conduct a more thorough investigation of other potential edge cases in the network data handling pipeline of the `NetworkAgent`. This could include investigating how the system handles other types of network events, such as redirects, authentication challenges, and WebSocket connections. It could also include investigating how the system handles different types of response bodies, such as those that are chunked or those that have a `Transfer-Encoding` header. By conducting a more thorough investigation of these edge cases, we can identify and fix potential bugs before they become a problem for users of the Node.js Inspector.\n\n",
      "solution": "> ### Version\n> \n> v25.0.0-pre\n> \n> ### Platform\n> \n> ```text\n> windows11 x86_64\n> \n> WindowsProductName WindowsVersion TotalPhysicalMemory CsProcessors\n> ------------------ -------------- ------------------- ------------\n> Windows 10 Pro     2009                               {Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz}\n> ```\n> \n> ### Subsystem\n> \n> --experimental-network-inspection\n> \n> ### What steps will reproduce the bug?\n> \n> \u4f7f\u7528\u4ee5\u4e0b\u65b9\u5f0f\u53d1\u51fa\u8bf7\u6c42\u3010\u7279\u522b\u6ce8\u610f\u76ee\u6807\u7f51\u7ad9\u9700\u8981\u652f\u6301\u538b\u7f29\uff0c\u672c\u95ee\u9898\u662fbr\u538b\u7f29\uff0c\u5373\u670d\u52a1\u5668\u8fd4\u56de\u5934\u542b\u6709content-encoding\u3011\uff0c\u7136\u540e\u5728\u7f51\u7edc\u63a7\u5236\u9762\u677f\uff0c\u54cd\u5e94\u4e2d\u5373\u53ef\u770b\u5230\u4e71\u7801\u3010\u538b\u7f29\u7684\u4e8c\u8fdb\u5236\u3011\n> // Node.js v18+ \u53ef\u7528\uff0c\u65e0\u9700\u5b89\u88c5\u4efb\u4f55\u5305\n> const url = 'https://nodejs.org/zh-cn/blog/release/v24.8.0';\n> \n> fetch(url)\n>   .then(res => {\n>     if (!res.ok) throw new Error(`HTTP ${res.status}`);\n>     return res.text(); // \u81ea\u52a8\u5904\u7406 gzip/br \u89e3\u538b\n>   })\n>   .then(html => {\n>     console.log('\u9875\u9762\u957f\u5ea6:', html.length);\n>   })\n>   .catch(console.error);\n> \n> \n> ### How often does it reproduce? Is there a required condition?\n> \n> \u53ea\u8981\u76ee\u6807\u670d\u52a1\u5668\u8fd4\u56de\u538b\u7f29\u6570\u636e\uff0c\u63a7\u5236\u53f0\u3010devtools\u3011\u5fc5\u5b9a\u663e\u793a\u5df2\u538b\u7f29\u7684\u6570\u636e\n> \n> ### What is the expected behavior? Why is that the expected behavior?\n> \n> \u8fd9\u4e2a\u884c\u4e3a\u4e0e\u6d4f\u89c8\u5668\u8fd4\u56de\u7684\u54cd\u5e94\u7ed3\u679c\u4e0d\u4e00\u81f4\uff0c\u5b83\u5e94\u8be5\u663e\u793a\u660e\u6587\u4fe1\u606f\n> \n> ### What do you see instead?\n> \n> \u5df2\u538b\u7f29\u7684\u4e8c\u8fdb\u5236\u6570\u636e\n> \n> ### Additional information\n> \n> \u8fd9\u4e2a\u6211\u672a\u80fd\u4f7f\u7528AI\u5bf9\u5176\u8fdb\u884c\u4fee\u590d\uff0c\u4ee5\u4e0b\u662fAI\u5206\u6790\u7684\u62a5\u544a\uff1a\n> \n> \n> # Analysis of Brotli Decompression Failure in Node.js NetworkAgent\n> \n> ## 1. Problem Statement and Initial Diagnosis\n> \n> ### 1.1. Core Issue: Brotli (`br`) Encoded Responses Not Being Decompressed\n> \n> The central problem identified within the Node.js `NetworkAgent` component is its failure to correctly decompress HTTP response bodies that are encoded with Brotli (`br`). Despite the presence of a `DecompressBody` function that ostensibly handles this encoding, the system is not producing the expected plaintext output. Instead, the response data remains in its compressed, binary form, leading to garbled and unreadable content when inspected through the Node.js Inspector. This issue is particularly significant for developers relying on the Inspector's Network tab to debug application behavior, as the inability to view response bodies renders the tool ineffective for a wide range of debugging scenarios. The problem persists even after initial modifications to the source code, indicating a deeper, more systemic issue within the data handling pipeline. The core of the problem lies in the disconnect between the server sending a `br`-encoded response and the `NetworkAgent` successfully processing it into a human-readable format.\n> \n> The investigation was initiated based on user-provided logs and source code, which clearly demonstrated that the `NetworkAgent` was receiving data but failing to apply the necessary decompression. The logs, which are designed to trace the flow of data through the agent, show that the `content-encoding` header is either not being detected or not being used to trigger the decompression logic. This leads to a situation where the raw, compressed data is being stored and potentially displayed as if it were the final, decompressed content. The implications of this are severe, as it not only breaks the functionality of the Inspector but also suggests a potential flaw in how Node.js handles modern web compression standards. The failure to handle Brotli, a widely adopted and efficient compression algorithm, represents a significant gap in the platform's debugging capabilities. The investigation aims to pinpoint the exact location and cause of this failure, whether it be in the header parsing, the decompression logic itself, or the data flow between these two components.\n> \n> ### 1.2. Symptom Analysis: Log Output Indicating Unchanged Data\n> \n> A detailed analysis of the debug logs provided by the user reveals several key symptoms that point directly to the root cause of the decompression failure. These logs, generated by the `NetworkAgent` during the processing of network responses, offer a granular view of the data at various stages of its handling. The most telling symptom is the consistent pattern of log entries that show no change in the data size before and after the decompression attempt. This, combined with the empty `content-encoding` value in the logs, provides a clear indication that the decompression logic is not being invoked correctly. The garbled output further confirms that the data being processed is still in its compressed binary format, rather than the expected plaintext. These symptoms, when taken together, paint a clear picture of a system that is failing at the first step of the decompression process: identifying that the data is compressed in the first place.\n> \n> #### 1.2.1. `enc=` (Empty Encoding) in Log Entries\n> \n> The most critical piece of evidence from the debug logs is the consistent appearance of `enc=` (an empty encoding string) in the log output. The log format, as defined in the `dataReceived` function, is designed to print the value of the `content_encoding` variable before attempting decompression. The fact that this variable is consistently empty across multiple log entries is a strong indicator that the `NetworkAgent` is failing to extract the `Content-Encoding` header from the HTTP response. This failure to detect the encoding is the primary reason why the `DecompressBody` function is not being called with the correct `encoding` parameter. The `DecompressBody` function has a clear conditional check at the beginning: `if (encoding.empty()) return in;`. This means that if the encoding string is empty, the function will simply return the input data without any modification, which perfectly explains the observed behavior. The empty `enc=` value is therefore not just a symptom but the direct cause of the decompression logic being bypassed entirely.\n> \n> The investigation into why the `content_encoding` variable is empty points towards the `responseReceived` function, which is responsible for parsing the response headers and populating the `RequestEntry` structure. The code in `responseReceived` attempts to extract the `content-encoding` header from the `headers_obj` V8 object. However, the code uses a hardcoded string `\"content-encoding\"` to look up the header. This is a potential point of failure, as HTTP headers are case-insensitive according to the RFC specifications. It is possible that the server is sending the header as `\"Content-Encoding\"` (with capital letters), which would cause the lookup to fail and result in an empty `content_encoding` value. This hypothesis is further supported by the fact that the `curl` command, which successfully decompresses the response, does not have this issue, as it likely handles header case-insensitivity correctly. The empty `enc=` value in the logs is therefore a direct consequence of a potential bug in the header parsing logic within the `responseReceived` function.\n> \n> #### 1.2.2. `raw` and `decompressed` Sizes are Identical\n> \n> Another significant symptom observed in the debug logs is that the `raw` and `decompressed` sizes are always identical. For example, a log entry might show `[NetworkAgent] cached: enc=, raw=1363 \u2192 decompressed=1363`. This is a direct consequence of the empty `enc=` value discussed previously. Since the `DecompressBody` function is being called with an empty `encoding` string, it immediately returns the input data without any processing. This means that the `decompressed` variable in the `dataReceived` function is simply a copy of the `raw` data, and therefore their sizes are identical. This symptom is a clear confirmation that the decompression logic is not being executed at all. If the decompression were being attempted and failing for some reason, we might expect to see different sizes, or potentially an error message. However, the identical sizes indicate that the code path for decompression is never taken.\n> \n> This symptom is particularly useful because it rules out other potential causes of the problem. For example, if the `DecompressBody` function had a bug in its Brotli decompression logic, we might see the sizes change, but the output would still be garbled. The fact that the sizes are identical means that the problem is not in the decompression algorithm itself, but in the logic that determines whether or not to apply it. This further strengthens the hypothesis that the issue lies in the header parsing code in the `responseReceived` function. The identical sizes are a direct and unambiguous indicator that the `DecompressBody` function is being bypassed, which in turn is caused by the failure to detect the `Content-Encoding` header. This symptom, combined with the empty `enc=` value, provides a very strong case for the proposed solution of making the header lookup case-insensitive.\n> \n> #### 1.2.3. Garbled Output in Logged Response Body\n> \n> The final and most visible symptom of the decompression failure is the garbled output that is printed in the debug logs when the response body is displayed. The logs show the first 200 characters of the \"decompressed\" data, but this data is clearly not human-readable. It consists of a mix of random characters, symbols, and what appears to be binary data. This is exactly what one would expect to see if a Brotli-compressed response were displayed as if it were plain text. Brotli compression produces a binary output that is not meant to be human-readable, and attempting to interpret it as text will result in the kind of garbled output seen in the logs. This symptom is a direct result of the previous two symptoms: the failure to detect the `Content-Encoding` header and the subsequent bypassing of the decompression logic.\n> \n> The garbled output serves as a final confirmation that the data being stored and displayed by the `NetworkAgent` is still in its compressed form. It is the most user-visible symptom of the problem, as it is what a developer would see in the Inspector's Network tab. While the empty `enc=` value and the identical sizes are more technical indicators, the garbled output is the ultimate proof that the system is not working as intended. This symptom is particularly important because it highlights the impact of the bug on the end-user. A developer trying to debug a network request would be unable to understand the response body, which would make it very difficult to diagnose any issues with their application. The garbled output is therefore not just a symptom, but a clear demonstration of the practical impact of the bug on the usability of the Node.js Inspector.\n> \n> ### 1.3. Initial Hypothesis: Failure to Detect `Content-Encoding` Header\n> \n> Based on the comprehensive analysis of the symptoms, the initial and most probable hypothesis is that the `NetworkAgent` is failing to correctly detect and extract the `Content-Encoding` header from the HTTP response. The evidence points to a specific failure in the `responseReceived` function, where the code attempts to read the `content-encoding` property from the response headers object. This failure could be due to several factors, including case-sensitivity issues (the header might be `Content-Encoding` while the code looks for `content-encoding`), the header not being present in the `params` object passed to the function, or an error in the V8 object property access logic. The empty `enc` value in the log is the smoking gun that confirms this hypothesis. The subsequent failure to decompress is a direct result of this initial detection failure. Therefore, the investigation should focus on the `responseReceived` function and the structure of the data it receives to pinpoint the exact cause of the header extraction failure.\n> \n> ## 2. Investigation into `Content-Encoding` Header Detection\n> \n> ### 2.1. Analysis of `responseReceived` Function\n> \n> The `responseReceived` function within the `NetworkAgent` class is a critical component in the network debugging pipeline, as it is responsible for processing the initial HTTP response headers and setting up the state for subsequent data handling. A thorough analysis of this function is essential to understanding why the Brotli decompression is failing. The function's primary role is to extract key information from the response, such as the status code, headers, and MIME type, and to store this information in a `RequestEntry` object for later use. The `RequestEntry` object is then used by other functions, such as `dataReceived` and `getResponseBody`, to determine how to process the response body. Therefore, any error in the `responseReceived` function can have a cascading effect on the entire data handling process.\n> \n> The investigation into the `responseReceived` function focuses on the specific code that is responsible for extracting the `Content-Encoding` header. This code is located near the end of the function and is responsible for populating the `content_encoding` member of the `RequestEntry` object. The analysis reveals a potential flaw in this code that could explain why the `content_encoding` is always empty in the debug logs. The code uses a hardcoded string `\"content-encoding\"` to look up the header in the `headers_obj` V8 object. This approach is problematic because it does not account for the case-insensitivity of HTTP headers, as defined by the HTTP/1.1 specification (RFC 7230). This could be the reason why the `NetworkAgent` is failing to detect the `Content-Encoding` header, even when it is present in the response.\n> \n> #### 2.1.1. Code Logic for Extracting `content-encoding`\n> \n> The code responsible for extracting the `content-encoding` header is located within the `responseReceived` function and is as follows:\n> \n> ```cpp\n> // \u63d0\u53d6\u5e76\u7f13\u5b58 content-encoding\n> Local<Object> headers_obj;\n> if (ObjectGetObject(context, response_obj, \"headers\").ToLocal(&headers_obj)) {\n>   Local<Value> enc_val;\n>   if (headers_obj\n>           ->Get(context,\n>                 OneByteString(context->GetIsolate(), \"content-encoding\"))\n>             .ToLocal(&enc_val) &&\n>         enc_val->IsString()) {\n>       v8::String::Utf8Value utf8(env_->isolate(), enc_val);\n>       request_entry->second.content_encoding =\n>           std::string(*utf8, utf8.length());\n>     }\n> }\n> ```\n> \n> This code first attempts to get the `headers` object from the `response_obj`. If successful, it then attempts to get the value of the `\"content-encoding\"` property from the `headers_obj`. If the value is a string, it converts it to a UTF-8 encoded C++ string and stores it in the `content_encoding` member of the `RequestEntry` object. The logic seems straightforward, but it has a critical flaw: it uses a hardcoded, lowercase string `\"content-encoding\"` to look up the header. This is a common mistake in HTTP client implementations, as it assumes that the server will always send headers in a specific case. However, the HTTP specification states that header names are case-insensitive, and servers are free to use any case they want. Therefore, a server could send the header as `\"Content-Encoding\"`, `\"CONTENT-ENCODING\"`, or any other combination of cases, and the `NetworkAgent` would fail to detect it.\n> \n> The consequence of this flaw is that the `content_encoding` member of the `RequestEntry` object will remain empty if the server uses a different case for the `Content-Encoding` header. This, in turn, will cause the `DecompressBody` function to be bypassed, as it checks if the `encoding` string is empty before attempting to decompress the data. The investigation into this code logic is therefore a key part of the overall analysis, as it provides a plausible explanation for the observed symptoms. The fix for this issue would be to make the header lookup case-insensitive, which can be done by either converting the header name to a standard case before the lookup or by iterating through all the headers and comparing their names in a case-insensitive manner.\n> \n> #### 2.1.2. Potential for Case-Sensitivity Issues (`content-encoding` vs `Content-Encoding`)\n> \n> The potential for case-sensitivity issues is a major concern in the `responseReceived` function's logic for extracting the `Content-Encoding` header. The HTTP/1.1 specification (RFC 7230) clearly states that header field names are case-insensitive. This means that `Content-Encoding`, `content-encoding`, and `CONTENT-ENCODING` are all considered to be the same header. However, the code in the `responseReceived` function uses a hardcoded, lowercase string `\"content-encoding\"` to look up the header in the `headers_obj` V8 object. This means that if the server sends the header with any uppercase letters, the lookup will fail, and the `content_encoding` member of the `RequestEntry` object will remain empty.\n> \n> This is a very likely cause of the problem, as it is common for servers to use a mix of uppercase and lowercase letters in their headers. For example, the `curl` command, which successfully decompresses the response, shows the header as `content-encoding: br` in its output, but it is possible that the server is sending it as `Content-Encoding: br` and `curl` is simply displaying it in a standardized format. The `NetworkAgent`, on the other hand, is not doing any standardization and is simply looking for the exact string `\"content-encoding\"`. This would explain why the `content_encoding` is always empty in the debug logs, even though the server is sending a `br`-encoded response.\n> \n> The fix for this issue is to make the header lookup case-insensitive. This can be done in several ways. One approach is to convert the header name to a standard case (e.g., lowercase) before the lookup. Another approach is to iterate through all the headers in the `headers_obj` and compare their names to `\"content-encoding\"` in a case-insensitive manner. The latter approach is more robust, as it does not rely on any assumptions about the case of the header names. The investigation into this potential issue is therefore a key part of the overall analysis, as it provides a very plausible explanation for the observed symptoms and a clear path to a solution.\n> \n> #### 2.1.3. Debugging Strategy: Logging All Response Headers\n> \n> To definitively diagnose the issue, a robust debugging strategy would be to log the entire `headers` object received by the `responseReceived` function. This would provide a clear view of the exact header names and values being provided by the V8 Inspector. By adding a loop to iterate over the properties of `headers_obj` and printing each key-value pair, it would be possible to confirm whether the `Content-Encoding` header is present and what its exact casing is. This would immediately reveal if the case-sensitivity hypothesis is correct or if the header is missing entirely for some other reason. The following code could be added for debugging purposes:\n> \n> ```cpp\n> Local<v8::Array> property_names;\n> if (headers_obj->GetOwnPropertyNames(context).ToLocal(&property_names)) {\n>   for (uint32_t i = 0; i < property_names->Length(); ++i) {\n>     Local<Value> key;\n>     Local<Value> value;\n>     if (property_names->Get(context, i).ToLocal(&key) &&\n>         headers_obj->Get(context, key).ToLocal(&value)) {\n>       v8::String::Utf8Value key_utf8(env_->isolate(), key);\n>       v8::String::Utf8Value value_utf8(env_->isolate(), value);\n>       fprintf(stderr, \"[NetworkAgent] Header: %s: %s\\n\", *key_utf8, *value_utf8);\n>     }\n>   }\n> }\n> ```\n> \n> This would provide the necessary information to understand the structure of the headers object and implement a correct and robust header extraction mechanism.\n> \n> ### 2.2. Analysis of Inspector Protocol Events\n> \n> The `NetworkAgent` communicates with the frontend (e.g., Chrome DevTools) using the Chrome DevTools Protocol (CDP). Understanding the structure of the events it consumes is crucial for correctly parsing the data.\n> \n> #### 2.2.1. Review of `Network.responseReceived` Parameters\n> \n> The `responseReceived` function is triggered by the `Network.responseReceived` event. According to the CDP specification, this event provides detailed information about the response, including its URL, status, status text, and, most importantly, a `headers` object. The `headers` object is a dictionary where keys are header names and values are header values. The `NetworkAgent` code correctly attempts to access this `headers` object. However, the specification does not mandate the casing of the header names within this object. This ambiguity is what leads to the potential case-sensitivity issue discussed earlier. The `responseReceived` function in the `NetworkAgent` is designed to parse this event, and its failure to find the `Content-Encoding` header is a direct result of a mismatch between its expectations (lowercase header names) and the reality of the data provided by the V8 Inspector.\n> \n> #### 2.2.2. Examination of `Network.dataReceived` Parameters\n> \n> The `dataReceived` function is triggered by the `Network.dataReceived` event. This event provides the actual response body data in chunks. The key parameter in this event is `data`, which, according to the CDP specification, is a base64-encoded string representing the chunk of the response body. However, the `NetworkAgent` code in the `dataReceived` function treats the `data` parameter as a `Uint8Array` directly:\n> \n> ```cpp\n> Local<Object> data_obj;\n> if (!ObjectGetObject(context, params, \"data\").ToLocal(&data_obj)) {\n>   return;\n> }\n> if (!data_obj->IsUint8Array()) {\n>   return;\n> }\n> Local<Uint8Array> data = data_obj.As<Uint8Array>();\n> ```\n> \n> This suggests a potential discrepancy between the expected protocol format and the actual implementation. If the V8 Inspector is indeed providing a base64-encoded string, the `data_obj->IsUint8Array()` check would fail, and the function would return early without processing the data. This would explain why no data is being decompressed or cached. The code would need to be adjusted to handle a base64 string, decode it into a binary buffer, and then proceed with the decompression. This is a critical area to investigate, as a failure to correctly parse the `data` field would prevent the entire data processing pipeline from functioning.\n> \n> #### 2.2.3. Hypothesis: `data` Field in `dataReceived` is Base64 Encoded\n> \n> Building on the previous point, a strong hypothesis is that the `data` field in the `Network.dataReceived` event is a base64-encoded string, as per the CDP specification, but the `NetworkAgent` code is expecting a `Uint8Array`. This mismatch would cause the `IsUint8Array()` check to fail, leading to the `dataReceived` function exiting prematurely. This would mean that no data is ever passed to the decompression logic, regardless of whether the `Content-Encoding` header was correctly detected. This hypothesis is supported by the fact that the final, successful log entry shows decompressed data, which suggests that the decompression logic itself is correct, but it is not being invoked for the individual data chunks. The fix for this would involve modifying the `dataReceived` function to first check if `data_obj` is a string, and if so, decode it from base64 into a binary buffer before proceeding with the decompression.\n> \n> ### 2.3. Cross-Verification with External Tools\n> \n> To validate the hypothesis that the server is indeed sending a Brotli-encoded response and that the issue lies within the `NetworkAgent`, it is essential to use external tools to cross-verify the behavior. By using a tool like `curl`, which is known to have robust support for HTTP compression, we can confirm that the server is capable of sending a Brotli-encoded response and that the response can be successfully decompressed. This will help to rule out the possibility that the server is not sending a `br`-encoded response in the first place, or that the response is malformed in some way. The `curl` command can be configured to specifically request a Brotli-encoded response and to automatically decompress it, which will allow us to see the plaintext content of the response.\n> \n> The results of the `curl` command will provide a baseline for comparison with the behavior of the `NetworkAgent`. If `curl` is able to successfully decompress the response, then we can be confident that the issue is not with the server or the response itself, but with the `NetworkAgent`. This will allow us to focus our investigation on the `NetworkAgent` code and to look for the specific bug that is causing the decompression to fail. The cross-verification with external tools is therefore a crucial step in the debugging process, as it helps to isolate the problem and to confirm that our hypothesis is correct.\n> \n> #### 2.3.1. Using `curl` to Confirm Server-Side Brotli Support\n> \n> To confirm that the server supports Brotli compression, the following `curl` command was executed:\n> \n> ```bash\n> curl -v -H \"Accept-Encoding: br\" --compressed https://example.com\n> ```\n> \n> The `-v` flag enables verbose output, which will show the request and response headers. The `-H \"Accept-Encoding: br\"` flag explicitly adds an `Accept-Encoding` header with the value `br`, which tells the server that the client is willing to accept a Brotli-encoded response. The `--compressed` flag tells `curl` to automatically decompress the response if it is compressed.\n> \n> The output of the `curl` command showed the following response headers:\n> \n> ```\n> < HTTP/2 200 \n> < content-type: text/html\n> < etag: \"84238dfc8092e5d9c0dac8ef93371a07:1736799080.121134\"\n> < last-modified: Mon, 13 Jan 2025 20:11:20 GMT\n> < vary: Accept-Encoding\n> < cache-control: max-age=86000\n> < date: Sat, 20 Sep 2025 15:44:23 GMT\n> < content-length: 1256\n> < alt-svc: h3=\":443\"; ma=93600\n> ```\n> \n> The `content-encoding: br` header is not explicitly shown in the response headers, but the `vary: Accept-Encoding` header indicates that the response may vary based on the `Accept-Encoding` header of the request. This is a strong indication that the server is capable of sending a compressed response. The fact that the `curl` command was able to successfully decompress the response and display the plaintext HTML content confirms that the server is indeed sending a Brotli-encoded response. This cross-verification with `curl` is a crucial step in the debugging process, as it confirms that the server is behaving as expected and that the issue lies within the `NetworkAgent`.\n> \n> #### 2.3.2. Using `curl` to Validate Successful Decompression\n> \n> The `curl` command not only confirmed that the server supports Brotli compression, but it also validated that the decompression was successful. The output of the `curl` command showed the following plaintext HTML content:\n> \n> ```html\n> <!doctype html>\n> <html>\n> <head>\n>     <title>Example Domain</title>\n>     <meta charset=\"utf-8\" />\n>     <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n>     <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n>     <style type=\"text/css\">\n>     body {\n>         background-color: #f0f0f2;\n>         margin: 0;\n>         padding: 0;\n>         font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n>     }\n>     div {\n>         width: 600px;\n>         margin: 5em auto;\n>         padding: 2em;\n>         background-color: #fdfdff;\n>         border-radius: 0.5em;\n>         box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n>     }\n>     a:link, a:visited {\n>         color: #38488f;\n>         text-decoration: none;\n>     }\n>     @media (max-width: 700px) {\n>         div {\n>             margin: 0 auto;\n>             width: auto;\n>         }\n>     }\n>     </style>    \n> </head>\n> <body>\n> <div>\n>     <h1>Example Domain</h1>\n>     <p>This domain is for use in illustrative examples in documents. You may use this\n>     domain in literature without prior coordination or asking for permission.</p>\n>     <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n> </div>\n> </body>\n> </html>\n> ```\n> \n> This is the expected plaintext content of the `https://example.com` webpage. The fact that `curl` was able to display this content confirms that the decompression was successful. This is in stark contrast to the behavior of the `NetworkAgent`, which is displaying garbled, binary data. The successful decompression by `curl` provides a clear baseline for comparison and confirms that the issue is not with the server or the response itself, but with the `NetworkAgent`. This validation is a crucial step in the debugging process, as it allows us to focus our investigation on the `NetworkAgent` code and to look for the specific bug that is causing the decompression to fail.\n> \n> ## 3. Deep Dive into the `DecompressBody` Function\n> \n> The investigation into the failure of Brotli decompression within the Node.js `NetworkAgent` necessitates a thorough examination of the `DecompressBody` function. This function is the core component responsible for handling the decompression of response bodies based on the `Content-Encoding` header. The provided source code and diagnostic logs indicate that the issue may stem from a combination of factors, including the logic for detecting the encoding type and the implementation of the decompression process itself. The function is designed to be a centralized utility for handling various compression formats, including Gzip, Deflate, and Brotli. However, the observed behavior suggests that the Brotli decompression path is either not being triggered correctly or is failing silently. This section will dissect the `DecompressBody` function, analyzing its structure, identifying potential weaknesses in its implementation, and exploring alternative strategies that could provide a more robust solution. The analysis will focus on the specific code paths related to Brotli, comparing the current implementation with best practices and considering the broader context of how this function is called within the `NetworkAgent`'s data handling pipeline. By understanding the intricacies of this function, we can pinpoint the precise source of the failure and formulate an effective remediation plan.\n> \n> ### 3.1. Review of Existing Implementation\n> \n> The `DecompressBody` function, as presented in the source code, is a C++ utility function designed to handle the decompression of HTTP response bodies. It takes a vector of bytes (`std::vector<uint8_t>`) and a string representing the encoding type (`const std::string& encoding`) as input. The function's primary logic is structured around a series of conditional checks that determine the decompression algorithm to be used based on the value of the `encoding` parameter. The implementation shows support for Gzip, Deflate, and Brotli, with a fallback to return the input data unchanged if the encoding is not recognized or is empty. This design suggests an attempt to create a unified decompression interface within the `NetworkAgent`. However, the effectiveness of this design is contingent on the accuracy of the `encoding` parameter and the robustness of the decompression logic for each supported format. The following subsections will provide a more detailed analysis of the function's structure and the specific implementation details for the Brotli decompression path.\n> \n> #### 3.1.1. Conditional Logic Based on `encoding` Parameter\n> \n> The control flow of the `DecompressBody` function is dictated by a straightforward `if-else` chain that inspects the `encoding` string. The function first checks if the `encoding` string is empty, in which case it immediately returns the input data without any modification. This is a logical default behavior for responses that are not compressed. The next condition checks if the encoding is either \"gzip\" or \"deflate\". If this condition is met, the function proceeds to use the `zlib` library's `inflate` function to decompress the data. The implementation for this path is relatively detailed, involving the initialization of a `z_stream` structure, setting the appropriate window bits for Gzip or Deflate, and then entering a loop to process the input data in chunks. This suggests that the Gzip and Deflate decompression paths have been given significant attention and are likely to be more mature and robust compared to the Brotli path. The final condition in the chain checks if the encoding is \"br\", which corresponds to Brotli. If this condition is met, the function calls `BrotliDecoderDecompress` to handle the decompression. The stark difference in the level of detail between the Gzip/Deflate path and the Brotli path is a key observation and a potential indicator of where the problem lies. The simplicity of the Brotli implementation, while seemingly efficient, may be overlooking important details or error conditions that are handled more thoroughly in the Gzip/Deflate case.\n> \n> #### 3.1.2. Use of `BrotliDecoderDecompress` for `br` Encoding\n> \n> The Brotli decompression logic within the `DecompressBody` function is implemented using the `BrotliDecoderDecompress` function from the `brotli/decode.h` library. This function is a high-level, one-shot decompression utility that takes the entire compressed input buffer and attempts to decompress it in a single call. The implementation in the provided code follows a two-pass approach. In the first pass, it calls `BrotliDecoderDecompress` with a `nullptr` for the output buffer to determine the required size of the decompressed data. If this call is successful (i.e., it returns `BROTLI_DECODER_RESULT_SUCCESS`), the function proceeds to the second pass. In the second pass, it allocates a vector of the calculated size and calls `BrotliDecoderDecompress` again, this time providing the allocated buffer to store the decompressed data. While this approach is functional, it has several potential drawbacks. Firstly, it requires two passes over the data, which can be inefficient for large responses. Secondly, it relies on the first call to accurately predict the output size, which may not always be the case for all types of Brotli-compressed data. Thirdly, the error handling is minimal; if the first call to `BrotliDecoderDecompress` fails for any reason, the function simply returns the original, uncompressed data without any indication of an error. This silent failure could explain why the response body appears garbled in the logs, as the raw, compressed data is being treated as if it were decompressed.\n> \n> ### 3.2. Potential Issues in `DecompressBody`\n> \n> A closer inspection of the `DecompressBody` function reveals several potential issues that could contribute to the failure of Brotli decompression. These issues range from the handling of the decompression library's return values to the overall strategy employed for decompression. The current implementation, while seemingly straightforward, may be too simplistic to handle the complexities of real-world Brotli-compressed data. The following subsections will delve into these potential issues in more detail, providing a critical analysis of the code and highlighting areas where improvements can be made. By addressing these issues, it may be possible to create a more robust and reliable decompression mechanism that can successfully handle Brotli-encoded responses.\n> \n> #### 3.2.1. Incorrect Handling of `BrotliDecoderDecompress` Return Value\n> \n> One of the most significant potential issues in the `DecompressBody` function is the way it handles the return value of `BrotliDecoderDecompress`. The function only checks for `BROTLI_DECODER_RESULT_SUCCESS` and treats any other return value as a failure. However, the Brotli decoder can return several other values that provide more detailed information about the state of the decompression process. For example, it can return `BROTLI_DECODER_RESULT_NEEDS_MORE_INPUT` if the input buffer does not contain a complete Brotli stream, or `BROTLI_DECODER_RESULT_ERROR` if the input data is corrupted. By not distinguishing between these different error conditions, the function may be failing to handle cases where the decompression could be completed successfully with more data or where a more specific error message could be provided. The current implementation's silent failure approach, where it simply returns the original data if the first call to `BrotliDecoderDecompress` is not successful, makes it difficult to diagnose the root cause of the problem. A more robust implementation would inspect the return value more carefully and provide appropriate error handling for each possible outcome.\n> \n> #### 3.2.2. Inefficient Two-Pass Decompression Strategy\n> \n> The two-pass decompression strategy employed in the `DecompressBody` function, while functional, is not the most efficient approach. The need to call `BrotliDecoderDecompress` twice, first to determine the output size and then to perform the actual decompression, adds unnecessary overhead. This is particularly problematic for large response bodies, as it requires the entire compressed data to be processed twice. A more efficient approach would be to use a streaming decompression strategy, where the data is decompressed in chunks as it is received. This would not only be more efficient in terms of processing time but would also be more memory-friendly, as it would not require the entire compressed and decompressed data to be held in memory at the same time. The `brotli/decode.h` library provides functions for creating a decoder instance and processing data in a streaming fashion, which would be a more suitable approach for this use case. The current implementation's reliance on a one-shot decompression function suggests that it may not have been designed with performance and scalability in mind.\n> \n> #### 3.2.3. Lack of Robust Error Handling for Decompression Failures\n> \n> The error handling in the `DecompressBody` function is minimal, particularly for the Brotli decompression path. As mentioned earlier, if the first call to `BrotliDecoderDecompress` fails, the function simply returns the original data without any indication of an error. This silent failure can make it very difficult to debug issues with decompression, as there is no information available about what went wrong. A more robust implementation would include comprehensive error handling that provides detailed information about any failures that occur during the decompression process. This could include logging the specific error code returned by the Brotli decoder, as well as any other relevant information, such as the size of the input data and the expected size of the output data. This would not only help with debugging but would also make the system more resilient, as it would be able to handle unexpected errors more gracefully. The current implementation's lack of robust error handling is a significant weakness that should be addressed in any future revisions.\n> \n> ### 3.3. Alternative Decompression Strategies\n> \n> Given the potential issues with the current `DecompressBody` function, it is worth considering alternative decompression strategies that could provide a more robust and efficient solution. These alternatives range from using a different approach with the same underlying library to leveraging other tools and technologies that are available within the Node.js ecosystem. The following subsections will explore some of these alternative strategies, discussing their potential benefits and drawbacks. By considering these alternatives, it may be possible to identify a more suitable approach for handling Brotli decompression in the `NetworkAgent`.\n> \n> #### 3.3.1. Stream-Based Decompression using `BrotliDecoder` Instance\n> \n> A more efficient and robust approach to Brotli decompression would be to use a streaming strategy with a `BrotliDecoder` instance. The `brotli/decode.h` library provides functions for creating and managing a decoder instance, which can be used to process data in a streaming fashion. This approach would involve creating a decoder instance, and then repeatedly calling a function to process chunks of the input data as they are received. The decompressed data would be written to an output buffer, which could then be used by the `NetworkAgent` to display the response body. This approach has several advantages over the current two-pass strategy. Firstly, it is more efficient, as it only requires a single pass over the data. Secondly, it is more memory-friendly, as it does not require the entire compressed and decompressed data to be held in memory at the same time. Thirdly, it provides more granular control over the decompression process, which can be useful for handling errors and other edge cases. The implementation of a streaming decompression strategy would be more complex than the current one-shot approach, but the benefits in terms of performance and robustness would likely be worth the additional effort.\n> \n> #### 3.3.2. Calling Node.js `zlib` Module from C++ via V8 API\n> \n> Another alternative to the current implementation would be to leverage the Node.js `zlib` module, which provides a high-level interface for compression and decompression, including support for Brotli. The `zlib` module is a core part of the Node.js platform and is likely to be more mature and well-tested than the direct use of the `brotli/decode.h` library. The `NetworkAgent` is already a C++ component that is integrated with the V8 JavaScript engine, so it should be possible to call the `zlib` module from C++ using the V8 API. This would involve creating a JavaScript function that uses the `zlib` module to decompress the data, and then calling this function from the C++ code. This approach would have several advantages. Firstly, it would allow the `NetworkAgent` to leverage the full power and flexibility of the `zlib` module, including its support for various compression formats and its robust error handling. Secondly, it would simplify the C++ code, as the decompression logic would be implemented in JavaScript, which is often a more suitable language for this type of high-level task. The main drawback of this approach is that it would introduce a dependency on the V8 API, which could make the code more complex and potentially less portable. However, given that the `NetworkAgent` is already tightly integrated with V8, this may not be a significant concern. The following subsections will explore this alternative in more detail, providing a step-by-step guide to implementing this approach.\n> \n> ##### 3.3.2.1. Overview of Node.js `zlib` Module for Brotli\n> \n> The Node.js `zlib` module provides a comprehensive set of tools for compression and decompression, including support for the Brotli algorithm. The module offers both synchronous and asynchronous methods for Brotli decompression, such as `zlib.brotliDecompressSync()` and `zlib.brotliDecompress()` . These methods provide a high-level interface for decompressing Brotli-compressed data, and they handle many of the low-level details that are required when using the `brotli/decode.h` library directly. The `zlib` module is a core part of the Node.js platform, so it is always available and is well-documented and well-tested. The module's API is designed to be easy to use, and it provides a consistent interface for all supported compression formats. By using the `zlib` module, the `NetworkAgent` could avoid many of the potential issues that are associated with the current implementation, such as the inefficient two-pass decompression strategy and the lack of robust error handling. The `zlib` module also provides a number of advanced features, such as the ability to set various options for the decompression process, which could be useful for fine-tuning the performance and behavior of the `NetworkAgent`.\n> \n> ##### 3.3.2.2. Using `zlib.brotliDecompressSync()` for Synchronous Decompression\n> \n> For the use case of the `NetworkAgent`, where the decompression needs to be performed as part of a synchronous data handling pipeline, the `zlib.brotliDecompressSync()` method would be the most suitable choice. This method takes a buffer of compressed data as input and returns a buffer of decompressed data. It is a blocking call, which means that it will not return until the decompression is complete. This is appropriate for the `NetworkAgent`, as it needs to wait for the decompression to be finished before it can proceed with displaying the response body. The `zlib.brotliDecompressSync()` method is easy to use and provides a simple and straightforward way to decompress Brotli-compressed data. It also handles many of the low-level details of the decompression process, such as memory management and error handling, which simplifies the code that needs to be written. The use of `zlib.brotliDecompressSync()` would eliminate the need for the two-pass decompression strategy that is currently used in the `DecompressBody` function, which would improve the performance and efficiency of the decompression process.\n> \n> ##### 3.3.2.3. V8 API for Executing JavaScript from C++\n> \n> To call the `zlib.brotliDecompressSync()` method from the C++ `NetworkAgent`, it is necessary to use the V8 API to execute JavaScript code from C++. The V8 API provides a set of functions and classes that allow C++ code to interact with the V8 JavaScript engine. This includes the ability to create JavaScript objects, call JavaScript functions, and access JavaScript variables. The process of calling a JavaScript function from C++ involves several steps. Firstly, it is necessary to get a reference to the JavaScript function that needs to be called. This can be done by creating a new JavaScript script that defines the function, or by getting a reference to an existing function that is defined in the global scope. Once a reference to the function has been obtained, it can be called using the `Call()` method of the `v8::Function` class. The `Call()` method takes a reference to the JavaScript context, a reference to the object that the function should be called on (which is usually the global object), and an array of arguments to be passed to the function. The return value of the `Call()` method is a `v8::Value` object, which can be converted to a C++ type if necessary. The use of the V8 API to call the `zlib.brotliDecompressSync()` method would allow the `NetworkAgent` to leverage the power and flexibility of the `zlib` module, while still being implemented in C++.\n> \n> ##### 3.3.2.4. Data Conversion Between C++ and V8\n> \n> When calling a JavaScript function from C++ using the V8 API, it is necessary to convert the data between C++ and V8 types. In the case of the `NetworkAgent`, the input data for the decompression process is a `std::vector<uint8_t>`, which needs to be converted to a V8 `ArrayBuffer` or `Uint8Array` before it can be passed to the `zlib.brotliDecompressSync()` method. Similarly, the output of the `zlib.brotliDecompressSync()` method is a Node.js `Buffer`, which needs to be converted back to a `std::vector<uint8_t>` so that it can be used by the C++ code. The V8 API provides a set of functions for performing these conversions. For example, a `std::vector<uint8_t>` can be converted to a V8 `ArrayBuffer` by creating a new `ArrayBuffer` object and copying the data from the vector into the buffer's backing store. Similarly, a Node.js `Buffer` can be converted to a `std::vector<uint8_t>` by getting a pointer to the buffer's data and its length, and then creating a new vector from this data. The process of converting data between C++ and V8 types can be a bit tedious, but it is a necessary step for integrating JavaScript code with C++ code using the V8 API. The following table summarizes the data conversion process:\n> \n> | C++ Type | V8 Type | Conversion Direction | V8 API Function |\n> | :--- | :--- | :--- | :--- |\n> | `std::vector<uint8_t>` | `v8::ArrayBuffer` | C++ to V8 | `v8::ArrayBuffer::New()` |\n> | `v8::ArrayBuffer` | `std::vector<uint8_t>` | V8 to C++ | `v8::ArrayBuffer::GetBackingStore()` |\n> | `std::vector<uint8_t>` | `v8::Uint8Array` | C++ to V8 | `v8::Uint8Array::New()` |\n> | `v8::Uint8Array` | `std::vector<uint8_t>` | V8 to C++ | `v8::Uint8Array::Buffer()` and `v8::ArrayBuffer::GetBackingStore()` |\n> \n> By carefully managing the data conversion process, it is possible to seamlessly integrate the `zlib` module with the C++ `NetworkAgent`, providing a more robust and efficient solution for Brotli decompression.\n> \n> ## 4. Proposed Solution and Implementation Strategy\n> \n> Based on the comprehensive analysis of the Brotli decompression failure in the Node.js `NetworkAgent`, a multi-faceted solution is proposed. This strategy addresses the identified root causes and contributing factors in a prioritized manner, starting with the most critical issue and moving towards enhancements that improve the overall robustness and reliability of the system. The proposed solution is divided into three main parts: a primary fix to correct the `Content-Encoding` header detection, a secondary fix to enhance the `DecompressBody` function's robustness, and a tertiary fix to correct the data parsing logic in the `dataReceived` function. This phased approach ensures that the most critical issues are addressed first, while also laying the groundwork for a more resilient and maintainable codebase in the long run.\n> \n> ### 4.1. Primary Fix: Correcting `Content-Encoding` Header Detection\n> \n> The primary and most critical fix is to correct the logic in the `responseReceived` function that is responsible for detecting the `Content-Encoding` header. As identified in the investigation, the current implementation is case-sensitive, which is a violation of the HTTP/1.1 specification and the root cause of the decompression failure. This fix is the highest priority because it directly addresses the core problem and is a prerequisite for all other decompression-related functionality to work correctly. Without a correctly identified encoding, the `DecompressBody` function will continue to be bypassed, and the response bodies will remain in their compressed, unreadable state.\n> \n> #### 4.1.1. Modifying `responseReceived` to Handle Case-Insensitive Headers\n> \n> The `responseReceived` function must be modified to perform a case-insensitive lookup for the `Content-Encoding` header. This can be achieved by iterating through all the properties of the `headers_obj` and comparing their names to `\"content-encoding\"` in a case-insensitive manner. This approach is more robust than simply converting the header name to a standard case, as it does not make any assumptions about the casing of the headers provided by the V8 Inspector. The implementation would involve using the `GetOwnPropertyNames` method of the V8 `Object` class to get an array of all the property names in the `headers_obj`, and then iterating through this array to find a match for the `Content-Encoding` header. Once a match is found, the corresponding value can be extracted and stored in the `content_encoding` member of the `RequestEntry` object.\n> \n> #### 4.1.2. Ensuring `request_entry->second.content_encoding` is Correctly Populated\n> \n> The ultimate goal of modifying the `responseReceived` function is to ensure that the `request_entry->second.content_encoding` variable is correctly populated with the value of the `Content-Encoding` header. This variable is the key piece of information that is used by the `dataReceived` function to determine whether or not to decompress the response body. By implementing a case-insensitive header lookup, we can ensure that the `content_encoding` variable is populated correctly, regardless of how the server chooses to format the header. This will, in turn, ensure that the `DecompressBody` function is called with the correct `encoding` parameter, which will trigger the decompression of the response body and result in the display of the human-readable content in the Node.js Inspector.\n> \n> ### 4.2. Secondary Fix: Enhancing `DecompressBody` Robustness\n> \n> Once the primary issue of header detection is resolved, the next step is to enhance the robustness of the `DecompressBody` function. While the current implementation is functional, it has several potential weaknesses that could lead to issues in the future. These include an inefficient two-pass decompression strategy, a lack of robust error handling, and a potential for silent failures. By addressing these issues, we can create a more reliable and maintainable decompression mechanism that is better equipped to handle the complexities of real-world network traffic.\n> \n> #### 4.2.1. Implementing a More Reliable Brotli Decompression Flow\n> \n> The current two-pass decompression strategy in the `DecompressBody` function is inefficient and could be improved by implementing a streaming decompression approach. This would involve using the `BrotliDecoder` instance from the `brotli/decode.h` library to process the data in chunks as it is received. This would not only be more efficient in terms of processing time and memory usage but would also provide more granular control over the decompression process. A streaming approach would be better suited for handling large response bodies and would be more resilient to errors and other edge cases. The implementation of a streaming decompression strategy would be a significant improvement over the current one-shot approach and would make the `DecompressBody` function more robust and scalable.\n> \n> #### 4.2.2. Adding Comprehensive Error Handling and Logging\n> \n> The error handling in the `DecompressBody` function is minimal and could be improved by adding more comprehensive error handling and logging. This would involve inspecting the return value of the `BrotliDecoderDecompress` function more carefully and providing appropriate error handling for each possible outcome. This could include logging the specific error code returned by the Brotli decoder, as well as any other relevant information, such as the size of the input data and the expected size of the output data. This would not only help with debugging but would also make the system more resilient, as it would be able to handle unexpected errors more gracefully. The addition of comprehensive error handling and logging would be a significant improvement over the current silent failure approach and would make the `DecompressBody` function more reliable and maintainable.\n> \n> ### 4.3. Tertiary Fix: Correcting Data Parsing in `dataReceived`\n> \n> The final part of the proposed solution is to investigate and correct the data parsing logic in the `dataReceived` function. As identified in the investigation, there is a potential discrepancy between the expected data format and the actual implementation. The `NetworkAgent` code is currently expecting a `Uint8Array`, but the Chrome DevTools Protocol specification states that the `data` field is a base64-encoded string. This could be another source of error that is preventing the decompression from working correctly.\n> \n> #### 4.3.1. Investigating if `data` is a Base64 String vs. a `Uint8Array`\n> \n> The first step in addressing this issue is to investigate the actual format of the `data` field in the `Network.dataReceived` event. This can be done by adding debug logging to the `dataReceived` function to print the type of the `data` object. This will reveal whether the data is being provided as a `Uint8Array` or as a base64-encoded string. If the data is indeed a base64-encoded string, then the `dataReceived` function will need to be modified to handle this format correctly.\n> \n> #### 4.3.2. Adjusting Data Parsing Logic Based on Protocol Specification\n> \n> If the investigation confirms that the `data` field is a base64-encoded string, the `dataReceived` function will need to be adjusted to parse this format correctly. This would involve using a base64 decoding library to convert the string into a binary buffer, which can then be passed to the `DecompressBody` function for decompression. The `protocol::Binary` class may have a method for decoding base64 strings, or a third-party library could be used for this purpose. By adjusting the data parsing logic to match the protocol specification, we can ensure that the `dataReceived` function is correctly handling the response body data, which is a critical step in the overall decompression process.\n> \n> ## 5. Conclusion and Recommendations\n> \n> ### 5.1. Summary of Findings\n> \n> The investigation into the failure of Brotli decompression in the Node.js `NetworkAgent` has revealed a clear and multi-faceted problem. The analysis of the provided source code and debug logs has led to the identification of a primary root cause and several contributing factors that are preventing the system from functioning as intended. The findings of this investigation can be summarized as follows:\n> \n> #### 5.1.1. Root Cause: Failure to Detect `Content-Encoding` Header\n> \n> The primary and most critical finding of this investigation is that the `NetworkAgent` is failing to correctly detect the `Content-Encoding` header in the HTTP response. This is due to a **case-sensitivity issue** in the `responseReceived` function, where the code is looking for a lowercase `\"content-encoding\"` header, while the server may be sending it with a different casing (e.g., `\"Content-Encoding\"`). This failure to detect the encoding is the root cause of the decompression failure, as it prevents the `DecompressBody` function from being invoked with the correct `encoding` parameter. The empty `enc=` value in the debug logs is a direct and unambiguous indicator of this problem.\n> \n> #### 5.1.2. Contributing Factor: Potential Issues in Decompression Logic\n> \n> In addition to the primary root cause, the investigation has also identified several potential issues in the `DecompressBody` function that could be contributing to the problem. These include an **inefficient two-pass decompression strategy**, a **lack of robust error handling**, and a **potential for silent failures**. While these issues may not be the primary cause of the decompression failure, they are significant weaknesses in the implementation that could lead to problems in the future. The investigation has also identified a potential discrepancy between the expected data format in the `dataReceived` function and the actual format specified by the Chrome DevTools Protocol, which could be another contributing factor to the problem.\n> \n> ### 5.2. Recommended Code Changes\n> \n> Based on the findings of this investigation, the following code changes are recommended to fix the Brotli decompression failure in the Node.js `NetworkAgent`. These changes are prioritized to address the most critical issues first and to provide a comprehensive solution to the problem.\n> \n> #### 5.2.1. Patch for `responseReceived` Function\n> \n> The `responseReceived` function must be patched to handle case-insensitive headers. The following code snippet provides a recommended implementation for this patch:\n> \n> ```cpp\n> // \u63d0\u53d6\u5e76\u7f13\u5b58 content-encoding (case-insensitive)\n> Local<Object> headers_obj;\n> if (ObjectGetObject(context, response_obj, \"headers\").ToLocal(&headers_obj)) {\n>   Local<v8::Array> property_names;\n>   if (headers_obj->GetOwnPropertyNames(context).ToLocal(&property_names)) {\n>     for (uint32_t i = 0; i < property_names->Length(); ++i) {\n>       Local<Value> key;\n>       if (property_names->Get(context, i).ToLocal(&key) && key->IsString()) {\n>         v8::String::Utf8Value key_utf8(env_->isolate(), key);\n>         std::string header_name(*key_utf8, key_utf8.length());\n>         // Convert to lowercase for case-insensitive comparison\n>         std::transform(header_name.begin(), header_name.end(), header_name.begin(), ::tolower);\n>         if (header_name == \"content-encoding\") {\n>           Local<Value> value;\n>           if (headers_obj->Get(context, key).ToLocal(&value) && value->IsString()) {\n>             v8::String::Utf8Value value_utf8(env_->isolate(), value);\n>             request_entry->second.content_encoding = std::string(*value_utf8, value_utf8.length());\n>             break; // Found the header, no need to continue\n>           }\n>         }\n>       }\n>     }\n>   }\n> }\n> ```\n> \n> This patch iterates through all the properties of the `headers_obj` and performs a case-insensitive comparison to find the `Content-Encoding` header. Once the header is found, its value is extracted and stored in the `content_encoding` member of the `RequestEntry` object.\n> \n> #### 5.2.2. Patch for `DecompressBody` Function\n> \n> The `DecompressBody` function should be patched to improve its robustness and efficiency. The following code snippet provides a recommended implementation for a streaming decompression strategy using the `BrotliDecoder` instance:\n> \n> ```cpp\n> // brotli\n> if (encoding == \"br\") {\n>   BrotliDecoderState* state = BrotliDecoderCreateInstance(nullptr, nullptr, nullptr);\n>   if (!state) {\n>     return in;\n>   }\n> \n>   std::vector<uint8_t> out;\n>   std::vector<uint8_t> buffer(16384);\n>   const uint8_t* next_in = in.data();\n>   size_t avail_in = in.size();\n>   size_t total_out = 0;\n> \n>   while (true) {\n>     uint8_t* next_out = buffer.data();\n>     size_t avail_out = buffer.size();\n>     BrotliDecoderResult result = BrotliDecoderDecompressStream(\n>         state, &avail_in, &next_in, &avail_out, &next_out, &total_out);\n> \n>     if (result == BROTLI_DECODER_RESULT_ERROR) {\n>       BrotliDecoderDestroyInstance(state);\n>       return in;\n>     }\n> \n>     out.insert(out.end(), buffer.begin(), buffer.begin() + (buffer.size() - avail_out));\n> \n>     if (result == BROTLI_DECODER_RESULT_SUCCESS) {\n>       break;\n>     }\n> \n>     if (result == BROTLI_DECODER_RESULT_NEEDS_MORE_INPUT) {\n>       // This should not happen in a one-shot decompression, but handle it just in case\n>       BrotliDecoderDestroyInstance(state);\n>       return in;\n>     }\n>   }\n> \n>   BrotliDecoderDestroyInstance(state);\n>   return out.empty() ? in : out;\n> }\n> ```\n> \n> This patch implements a streaming decompression strategy that is more efficient and robust than the current two-pass approach. It also includes more comprehensive error handling to provide better feedback in case of failures.\n> \n> #### 5.2.3. Patch for `dataReceived` Function (if necessary)\n> \n> If the investigation confirms that the `data` field in the `Network.dataReceived` event is a base64-encoded string, the `dataReceived` function will need to be patched to handle this format correctly. The following code snippet provides a recommended implementation for this patch:\n> \n> ```cpp\n> Local<Object> data_obj;\n> if (!ObjectGetObject(context, params, \"data\").ToLocal(&data_obj)) {\n>   return;\n> }\n> \n> std::vector<uint8_t> raw;\n> if (data_obj->IsUint8Array()) {\n>   Local<Uint8Array> data = data_obj.As<Uint8Array>();\n>   auto* raw_ptr = static_cast<const uint8_t*>(data->Buffer()->GetBackingStore()->Data());\n>   raw = std::vector<uint8_t>(raw_ptr, raw_ptr + data->ByteLength());\n> } else if (data_obj->IsString()) {\n>   v8::String::Utf8Value data_utf8(env_->isolate(), data_obj);\n>   std::string data_str(*data_utf8, data_utf8.length());\n>   // Decode base64 string to binary data\n>   // This would require a base64 decoding library\n>   // raw = base64_decode(data_str);\n> } else {\n>   return;\n> }\n> ```\n> \n> This patch checks the type of the `data` object and handles both `Uint8Array` and string formats. If the data is a string, it is decoded from base64 into a binary buffer before being passed to the `DecompressBody` function.\n> \n> ### 5.3. Future Considerations\n> \n> In addition to the immediate code changes recommended above, there are several future considerations that should be taken into account to improve the overall quality and maintainability of the `NetworkAgent` codebase. These considerations are not directly related to the Brotli decompression failure, but they are important for ensuring the long-term health and stability of the system.\n> \n> #### 5.3.1. Adding Unit Tests for Decompression Logic\n> \n> One of the most important future considerations is to add a comprehensive suite of unit tests for the decompression logic in the `NetworkAgent`. This would involve creating tests for each of the supported compression formats (Gzip, Deflate, and Brotli) to ensure that the decompression is working correctly for a variety of different inputs. The tests should cover both successful decompression scenarios and error scenarios, such as corrupted data or incomplete streams. The addition of unit tests would not only help to prevent regressions in the future but would also make it easier to identify and fix bugs in the decompression logic.\n> \n> #### 5.3.2. Investigating Other Potential Edge Cases in Network Data Handling\n> \n> Finally, it would be beneficial to conduct a more thorough investigation of other potential edge cases in the network data handling pipeline of the `NetworkAgent`. This could include investigating how the system handles other types of network events, such as redirects, authentication challenges, and WebSocket connections. It could also include investigating how the system handles different types of response bodies, such as those that are chunked or those that have a `Transfer-Encoding` header. By conducting a more thorough investigation of these edge cases, we can identify and fix potential bugs before they become a problem for users of the Node.js Inspector.\n> \n> \n\nTranslation:\n\nTitle:\nInconsistency between Node\u2019s http2 network request debugging and browser network debugging\n\n### Version\nv25.0.0-pre\n\n### Platform\n```text\nwindows11 x86_64\nWindowsProductName WindowsVersion TotalPhysicalMemory CsProcessors\n------------------ -------------- ------------------- ------------\nWindows 10 Pro     2009 {Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz}\n```\n\n### Subsystem\n--experimental-network-inspection\n\n### What steps will reproduce the bug?\nMake a request using the following method (note: the target site must support compression \u2014 this issue is about Brotli compression, i.e. the server returns a Content-Encoding header). Then open the Network panel and you will see garbled text in the response (the compressed binary).\n\n// Works in Node.js v18+ with no extra packages\nconst url = 'https://nodejs.org/zh-cn/blog/release/v24.8.0';\n\nfetch(url)\n .then(res => {\n if (!res.ok) throw new Error(HTTP ${res.status});\n return res.text(); // automatically handles gzip/br decompression\n })\n .then(html => {\n console.log('page length:', html.length);\n })\n .catch(console.error);\n\n### How often does it reproduce? Is there a required condition?\nWhenever the target server returns compressed data, the DevTools console will always show the compressed data.\n\n### What is the expected behavior? Why is that the expected behavior?\nThis should match the browser\u2019s response behavior \u2014 it should display the plaintext content.\n\n### What do you see instead?\nCompressed binary data.\n\n### Additional information\nI was unable to fix this with AI; below is the AI analysis report: (omitted)",
      "labels": [],
      "created_at": "2025-09-22T15:19:21Z",
      "closed_at": "2026-01-24T10:24:48Z",
      "url": "https://github.com/nodejs/node/issues/59972",
      "comments_count": 2
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 61404,
      "title": "Array.prototype.sort crash",
      "problem": "### Version\n\nv24.1.0\n\n### Platform\n\n```text\nIt seems to be the expected design of V8 that an OOM exception occurs when executing the statement `Array.prototype.sort.call({length: Infinity})` due to the array length issue. The program crashes. Should this be changed to gracefully exit after throwing an error?\n```\n\n### Subsystem\n\n_No response_\n\n### What steps will reproduce the bug?\n\nexecute expression `Array.prototype.sort.call({length: Infinity})`\n\n### How often does it reproduce? Is there a required condition?\n\nAlways\n\n### What is the expected behavior? Why is that the expected behavior?\n\nthrow a RangeError\uff0cnot crash\n\n### What do you see instead?\n\n<img width=\"1266\" height=\"373\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/fc2ff3cf-1dff-4424-9bc8-75b1a5f8b86e\" />\n\n### Additional information\n\n_No response_",
      "solution": "There is an existing report for this behaviour at https://issues.chromium.org/issues/42201575. The array-like doesn't have to be infinite, it just has to be large enough to OOM.\n\nIt's been soft-marked as wontfix, but you are welcome to vote on the issue there.",
      "labels": [
        "v8 engine"
      ],
      "created_at": "2026-01-17T09:35:37Z",
      "closed_at": "2026-01-23T21:42:57Z",
      "url": "https://github.com/nodejs/node/issues/61404",
      "comments_count": 3
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 51949,
      "title": "Module resolution failure when using \"#/\" in `imports`",
      "problem": "### Version\r\n\r\nv20.11.1\r\n\r\n### Platform\r\n\r\nMicrosoft Windows NT 10.0.22631.0 x64\r\n\r\n### Subsystem\r\n\r\nnpm\r\n\r\n### What steps will reproduce the bug?\r\n\r\n1. Create a minimal `package.json` that specifies the following in `imports`:\r\n\r\n``` json\r\n{\r\n  \"imports\": {\r\n    \"#/*\": \"./*\"\r\n  }\r\n}\r\n```\r\n\r\n2. Create two files at the project root:\r\n\r\n``` js\r\n// file1.js\r\nimport number from '#/file2.js';\r\nconsole.log(number);\r\n```\r\n\r\n``` js\r\n// file2.js\r\nexport default 2;\r\n```\r\n\r\n3. Execute the first script: `node file1.js`\r\n4. Observe the error:\r\n\r\n```\r\nTypeError [ERR_INVALID_MODULE_SPECIFIER]: Invalid module \"#/file2.js\" is not a valid internal imports specifier name\r\n```\r\n\r\n\r\n### How often does it reproduce? Is there a required condition?\r\n\r\nThis happens every time a key/value pair in `imports` has a forward slash (`/`) immediately following the pound sign (`#`).\r\n\r\n### What is the expected behavior? Why is that the expected behavior?\r\n\r\nI would expect that `file1.js` is able to resolve `file2.js` with the specified alias. The documentation does not specify that this is an invalid setting.\r\n\r\n### What do you see instead?\r\n\r\n```\r\nTypeError [ERR_INVALID_MODULE_SPECIFIER]: Invalid module \"#/file2.js\" is not a valid internal imports specifier name\r\n```\r\n\r\n### Additional information\r\n\r\nThere is no indication that the above configuration would not work based on the documentation. Indeed, even WebStorm's IntelliSense resolves this alias _as if_ it would work.\r\n\r\nOf course, setting the alias to to `\"#src/*\": \"./*\"` and changing `file1.js` to use `import number from '#src/file2.js';` would work. However, I feel that would be inelegant. Imagine we have a project structure where all the files of interest are in `src/`, such as the following:\r\n\r\n```\r\n\u2514\u2500\u2500 test-project/\r\n    \u251c\u2500\u2500 src/\r\n    \u2502   \u251c\u2500\u2500 components/\r\n    \u2502   \u2502   \u2514\u2500\u2500 SomeComponent.js\r\n    \u2502   \u2514\u2500\u2500 pages/\r\n    \u2502       \u2514\u2500\u2500 SomePage.js\r\n    \u2514\u2500\u2500 package.json\r\n```\r\n\r\nIt would be much cleaner to be able to import these files with minimal boilerplate, such as:\r\n\r\n``` js\r\nimport SomeComponent from '#/components/SomeComponent.js';\r\nimport SomePage from '#/pages/SomePage.js';\r\n```\r\n\r\n...as opposed to having some needless additional text or specifying each directory as an alias:\r\n\r\n``` json\r\n{\r\n  \"imports\": {\r\n    \"#components/*\": \"./src/components/*\",\r\n    \"#pages/*\": \"./src/pages/*\"\r\n  }\r\n}\r\n```",
      "solution": "Keeping the current restriction is not much of a problem if the main documentation includes some warning or if the error message for using `#/` was unique. As it stands, Node throws the generic error that applies to any failed module resolution and gives no indication that `#/` itself is invalid.\n\n---\n\nFixed by https://github.com/nodejs/node/pull/60864",
      "labels": [
        "esm",
        "loaders"
      ],
      "created_at": "2024-03-03T05:15:44Z",
      "closed_at": "2026-01-23T12:51:50Z",
      "url": "https://github.com/nodejs/node/issues/51949",
      "comments_count": 6
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 61486,
      "title": "v8 assert from CheckGlobalAndEternalHandles while building a snapshot on android",
      "problem": "### Version\n\nv24.13.0\n\n### Platform\n\n```text\nLinux localhost 4.19.81-perf+ #1 SMP PREEMPT Fri Feb 2 17:00:31 CST 2024 aarch64\n```\n\n### Subsystem\n\nsnapshot\n\n### What steps will reproduce the bug?\n\nAttempting to build a snapshot from this script causes a crash.\n\n- cmd\n`./node --snapshot-blob snapshot.blob --build-snapshot app_test.js`\n\n- script content\n```\n// app_test.js\nconst { startupSnapshot } = require('node:v8');\n\nconsole.log('\ud83c\udfd7\ufe0f  Building snapshot: Starting initialization of resource-intensive tasks...');\n\n// const heavyData = new Map();\n// for (let i = 0; i < 100000; i++) {\n//   heavyData.set(`key-${i}`, Math.random());\n// }\n\nconsole.log('\u2705 Initialization complete, data is ready.');\n\nstartupSnapshot.setDeserializeMainFunction((args) => {\n  console.log('\ud83d\ude80 The application has started! (Restored from snapshot)');\n\n  // console.log(`data size: ${heavyData.size}`);\n  // console.log(`data sample: ${heavyData.get('key-50')}`);\n\n});\n\nif (startupSnapshot.isBuildingSnapshot()) {\n  console.log('\u2139\ufe0f  Build mode detected, preparing heap memory...');\n}\n\n\n```\n\n### How often does it reproduce? Is there a required condition?\n\nAlways.\n\n\n### What is the expected behavior? Why is that the expected behavior?\n\nNo crashes.\n\n### What do you see instead?\n\ncrashed:\n\n```\n\ud83c\udfd7\ufe0f   Building snapshot: Starting initialization of resource-intensive tasks...\n\u2705 Initialization complete, data is ready.\n\u2139\ufe0f   Build mode detected, preparing heap memory...\nglobal handle not serialized: 0x202f5bbc69: [[api object] 0]\n - map: 0x00273b1a3901 <Map[64](HOLEY_ELEMENTS)> [FastProperties]\n - prototype: 0x00202f5badb9 <Object map = 0x273b1a3739>\n - elements: 0x0006d3440cf9 <FixedArray[0]> [HOLEY_ELEMENTS]\n - embedder fields: 4\n - properties: 0x00202f5bd699 <PropertyArray[3]>\n - All own properties (excluding elements): {\n    0x0015fcb41b51 <Symbol: owner_symbol>: 0x00202f5bba29 <WriteStream map = 0x273b1a3b99> (const data field 0, attrs: [WEC]) @ Class(0x273b1a36b9), location: properties[0]\n }\n - embedder fields = {\n    127, aligned pointer: 0x7f6611c182\n    127, aligned pointer: 0x7f66011e00\n    127, aligned pointer: 0x7f66011e70\n    0x00202f5af019 <JSFunction onStreamRead (sfi = 0x2ab19fd431)>\n }\n\nglobal handle not serialized: 0x6d3440011: [Oddball] in ReadOnlySpace: #undefined\n\n\n\n#\n# Fatal error in , line 0\n# CheckGlobalAndEternalHandles failed\n#\n#\n#\n#FailureMessage Object: 0x7ff005aef0\n----- Native stack trace -----\n\nTrap\n```\n\n### Additional information\n\nThis Android version of Node.js was compiled by myself; the Ubuntu 22.04 v24.13.0 version installed via nvm can run this script and generate snapshots correctly.",
      "solution": "@SUPPH glad that pinpointed it! Snapshotting is very strict about persistent native handles, so your custom log wrapper likely prevented the serializer from cleaning up.\n\nSince this is specific to your patch, feel free to *close the issue* whenever you're ready. Good luck with the v24 update!\n\n---\n\n> [@SUPPH](https://github.com/SUPPH) glad that pinpointed it! Snapshotting is very strict about persistent native handles, so your custom log wrapper likely prevented the serializer from cleaning up.\n> \n> Since this is specific to your patch, feel free to _close the issue_ whenever you're ready. Good luck with the v24 update!\n\n@amyssnippet Thank you for your reply. I will solve the problem of customized implementation",
      "labels": [],
      "created_at": "2026-01-23T06:35:15Z",
      "closed_at": "2026-01-23T11:59:54Z",
      "url": "https://github.com/nodejs/node/issues/61486",
      "comments_count": 4
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 47747,
      "title": "ESM loaders cannot be defined via `Worker` option `execArgv` in v20",
      "problem": "### Version\r\n\r\n20.0.0 (tested on main at 36e4e3d86b31de7f38125aacad810e40295fd2ae too)\r\n\r\n### Platform\r\n\r\nmacOS but probably all platforms\r\n\r\n### Subsystem\r\n\r\nesm\r\n\r\n### What steps will reproduce the bug?\r\n\r\nThe following works in Node 18 and 19, but not 20:\r\n\r\n```js\r\n// main.mjs\r\nimport { Worker } from 'node:worker_threads';\r\n\r\nnew Worker('./worker.js', {\r\n  execArgv: ['--experimental-loader', './loader.mjs'],\r\n});\r\n```\r\n\r\n```js\r\n// worker.js\r\n'use strict';\r\n\r\nasync function main() {\r\n  await import('node:fs');\r\n}\r\n\r\nmain();\r\n```\r\n\r\n```js\r\n// loader.mjs\r\nexport function resolve (specifier, context, nextResolve) {\r\n  throw new Error('boom');\r\n}\r\n```\r\n\r\nRun: `node main.mjs`. In Node 18 and 19, the exception in the loader is thrown. In Node 20 it is not. The problem is not unique to throwing. I haven't been able to see `console.log()` or any other indication that the loader is being called.\r\n\r\n### How often does it reproduce? Is there a required condition?\r\n\r\n100% of the time for me.\r\n\r\n### What is the expected behavior? Why is that the expected behavior?\r\n\r\nI expect it to work as it did in earlier versions of Node. This is the expected behavior because right now it doesn't seem to work at all.\r\n\r\n### What do you see instead?\r\n\r\nSee the description of the bug above.\r\n\r\n### Additional information\r\n\r\nJust a guess, but I'm assuming this is related to https://github.com/nodejs/node/pull/44710.",
      "solution": "Another side effect of ES loaders to a separate thread: updating `require.extensions` in an es loader no longer has any effect, meaning that `node --loader=ts-node/esm` no longer functions properly to load both cjs and mjs typescript.\r\n\r\nThis makes it significantly more challenging to have loaders that work for both esm and cjs, effectively breaking the esm on-ramp for those of us supporting both modes, since now you need to know ahead of time what the module type is (or write the transpiled source to another path and return a short circuit url to that instead), rather than being able to have a single loader capable of handling both.\n\n---\n\n> I believe that would work for my use case.\r\n\r\nWhat is your use case?\r\n\r\n> CLI tools can\u2019t really set Node.js options.\r\n\r\nWe\u2019ve been discussing adding support for `.env` files: https://github.com/nodejs/node/pull/43973#issuecomment-1249549346. This would be a shorter-term solution for being able to set `NODE_OPTIONS` from a file. Farther down the road I think we\u2019d like to support a new field in `package.json` (or possibly even a new config file) that would do the same thing.\n\n---\n\n> The loader itself is just used for hot module reloading, so I think `registerLoader()` will work just fine.\r\n\r\nIf you find a good general solution for hot module reloading, please share it.\r\n\r\nThe `registerLoader` PR is by a new contributor and I\u2019m not sure he\u2019s working on it any more, so if you have time to help get it finished, that would be greatly appreciated.",
      "labels": [
        "esm",
        "worker",
        "loaders"
      ],
      "created_at": "2023-04-27T14:10:38Z",
      "closed_at": "2024-09-25T14:40:09Z",
      "url": "https://github.com/nodejs/node/issues/47747",
      "comments_count": 30
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 53787,
      "title": "Support .noderc or similar file-based initialization configurations?",
      "problem": "In https://github.com/nodejs/node/issues/52219 it was mentioned that for APM use cases it would be nice to have a way to register loaders without using command line flags. It occurred to me maybe what we need is an initialization config similar to the rc files out there for various applications.\r\n\r\nFor example in my `.lldbinit` I have these that extend LLDB to help me debug\r\n\r\n```\r\nplugin load /path/to/llnode.dylib\r\ncommand script import /path/to/lldb_commands.py\r\n```\r\n\r\nIt seems this is serving the same use case as the Node.js loader registration - run some scripts or register some plugin to Node.js before you actually start running it.\r\n\r\nFor loading loaders, I guess we can support something like a `.noderc` that is discovered when users start running Node.js, or make that part of `package.json`, which allows registering certain hooks before the actual application code is run. For example in the case of package.json, I guess the project pacakge.json would include something like this:\r\n\r\n```\r\n{\r\n  \"preload\": [\r\n     \"apm-package/register\"\r\n   ],\r\n  \"dependencies\": {\r\n    \"apm-package\": \"1.1.1\"\r\n  }\r\n}\r\n```\r\n\r\nIf we want something more flexible than \"just loading some scripts\" though, I guess a dedicated rc file would be easier to use than package.json.\r\n\r\ncc @timfish @nodejs/loaders ",
      "solution": "I was thinking that whatever config file we choose would be loaded automatically as a semver-major change, and a flag could specify it for older Node versions. A flag might be used for the latest version too in case the user wants to specify a file that\u2019s not the default filename or location.\r\n\r\nI\u2019m not sure what about https://github.com/nodejs/node/issues/41103 involves any of this. We should just filter out options that don\u2019t make sense to inherit, both currently to resolve that issue and in the future when we add a proper config file. We should do that filtering however the problematic options are defined, such as via environment variables or `NODE_OPTIONS` or via a file. Users already have ways to exert granular control of what flags and/or environment variables are passed into child processes or workers, via `execArgv` or `env`.\n\n---\n\n> A nodejs section in package.json is ideal. Make it so that if it's a string, then that's the name of a file to load, like many other tools do. Then we don't have to agree on the perfect spelling for the filename.\r\n\r\nI am leaning towards \"a field specified in package.json pointing to a file\" too primarily because to load this by default, it adds overhead to the startup to probe the file system; And since we already probe the file system for package.json, the overhead of probing another field in it would be the smallest.\r\n\r\nBut I do like to see a fixed (at least base) name of the file, because I like the concept of universally recognizable manifest of projects, just like when I check out a JS project/package, if I want to see dependency information I go to package.json, if I want to see TypeScript information I go to tsconfig.json, if I want to see their eslint configs I go to any file that contains the word eslint etc. Maybe something we can do is to support multiple formats (like what eslint does), and JSON would be the first to support, but they all have the same basename, and people just pick the format they like, provided that we do add support for other formats later on.\n\n---\n\nWould this preload apply only to `package.json#scripts` or to any node process where this `package.json` is resolved as the closest?",
      "labels": [
        "cli"
      ],
      "created_at": "2024-07-09T17:21:38Z",
      "closed_at": "2025-04-18T14:31:52Z",
      "url": "https://github.com/nodejs/node/issues/53787",
      "comments_count": 30
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 47086,
      "title": "Can't change NamedPipe permissions with Windows",
      "problem": "### Version\n\n16.15.1\n\n### Platform\n\nWindows 11\n\n### Subsystem\n\nnet\n\n### What steps will reproduce the bug?\n\nThe API for creating a named pipe in Windows does not allow setting permissions of the named pipe and by default pipes are readable by everyone. This makes secure programming very difficult.\n\n### How often does it reproduce? Is there a required condition?\n\nAlways.\n\n### What is the expected behavior?\n\n_No response_\n\n### What do you see instead?\n\nEither default permissions should be set to current user only or an API should be provided to set the permissions.\n\n### Additional information\n\n_No response_",
      "solution": "I created Windows named pipes like this:\r\n\r\n```JavaScript\r\nvar net = require('net');\r\n\r\nvar PIPE_NAME = \"mypipe\";\r\nvar PIPE_PATH = \"\\\\\\\\.\\\\pipe\\\\\" + PIPE_NAME;\r\n\r\nvar L = console.log;\r\n\r\nvar server = net.createServer(function(stream) {\r\n    L('Server: on connection')\r\n\r\n    stream.on('data', function(c) {\r\n        L('Server: on data:', c.toString());\r\n    });\r\n\r\n    stream.on('end', function() {\r\n        L('Server: on end')\r\n        server.close();\r\n    });\r\n\r\n    stream.write('Take it easy!');\r\n});\r\n\r\nserver.on('close',function(){\r\n    L('Server: on close');\r\n})\r\n\r\nserver.listen(PIPE_PATH,function(){\r\n    L('Server: on listening');\r\n})\r\n\r\n// == Client part == //\r\nvar client = net.connect(PIPE_PATH, function() {\r\n    L('Client: on connection');\r\n})\r\n\r\nclient.on('data', function(data) {\r\n    L('Client: on data:', data.toString());\r\n    client.end('Thanks!');\r\n});\r\n\r\nclient.on('end', function() {\r\n    L('Client: on end');\r\n})\r\n```\r\n\r\nExample is from here: https://stackoverflow.com/questions/11750041/how-to-create-a-named-pipe-in-node-js\r\n\r\n\r\nThe problem is that, AFAIK, in Windows named pipes are created using the network subsystem, not using files. \n\n---\n\nNode (well, libuv) calls [CreateNamedPipe()](https://learn.microsoft.com/en-us/windows/win32/api/winbase/nf-winbase-createnamedpipea) with the WRITE_DAC (but not WRITE_OWNER) flag and default security attributes, meaning:\r\n\r\n> The ACLs in the default security descriptor for a named pipe grant full control to the LocalSystem account, administrators, and the creator owner. They also grant read access to members of the Everyone group and the anonymous account.\r\n\r\nExcluding some of those may be reasonable (or maybe not, I'm undecided) but as node has worked this way since basically forever any change in default behavior risks breaking existing libraries or applications, so we're unlikely to make that change.\r\n\r\n(As well, since you're the first one to bring this up, it seems like a safe bet the default is working fine for most users.)\r\n\r\nAn opt-in could work but you'll have to pursue that by modifying libuv (open an issue first to hash out the details) and then changing node to make use of the new libuv API.\r\n\r\nIf you don't plan on working on that, fair, but then please close the issue.\n\n---\n\nI assume the lack of reply means you're not intending to pursue this. I'll close out the issue.",
      "labels": [
        "windows"
      ],
      "created_at": "2023-03-14T07:52:54Z",
      "closed_at": "2023-03-21T11:02:32Z",
      "url": "https://github.com/nodejs/node/issues/47086",
      "comments_count": 5
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 60303,
      "title": "`localStorage`'s behaviour when no or invalid `--localstorage-file` provided",
      "problem": "Prior to #57666, accessing the global `localStorage` would throw an error if `--localstorage-file` was invalid at the point of lazy initialisation.\n\nNow, it provides an empty proxy object which responds `undefined` to all property access. This is neither spec-compliant nor straightforwardly observable.\n\nLooking at the PR, it seems like this was an attempt to avoid throwing warnings in the test suite if simply testing for the presence of the global variable. It seems to me like the better approach would be for the tests to be changed (eg. by checking for the flag instead), and for the implementation to be kept sane.\n\nA couple of options would be:\n- Implement [mcollina's original proposal](https://github.com/nodejs/node/pull/57666#issuecomment-2762069815), and have the getter warn and return `undefined` if the storage cannot be initialised.\n- The `localStorage` global getter is allowed to throw a DOMException in the spec if the storage cannot be safely initialised, which is more abrupt, but arguably more consistent with the web.",
      "solution": "In case anyone is looking for a workaround, I'm currently using this:\n\n```ts\n// workaround for a bug in Node 25 that creates localStorage as an empty proxy,\n// leading to @typescript/vfs eventually throwing when it sees that it is not\n// undefined and tries to call `getItem`:\n\n// https://github.com/nodejs/node/issues/60303\n\n// this can be removed once the bug is addressed in Node\nif (!globalThis.localStorage.getItem)\n\tglobalThis.localStorage = undefined as never\n```\n\n---\n\nSqlite isn't a solution for cases like mine (see above). There always will be a bunch of web developers who use Node only for run tests  (and run bundler ofc) and expects that localStoarge is available because it exist already. For me as such dev would be more convenient that Node has a default value for `--localstorage-file` parameter. ",
      "labels": [
        "web-standards",
        "v25.x"
      ],
      "created_at": "2025-10-17T23:11:15Z",
      "closed_at": "2026-01-22T16:35:40Z",
      "url": "https://github.com/nodejs/node/issues/60303",
      "comments_count": 27
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 51372,
      "title": "Revisiting `globalThis` as an `EventTarget`",
      "problem": "In browsers, deno, bun, workers and other runtimes, `globalThis` implements the Web platform standard `EventTarget` API. While there are differences in the specific events emitted at the global scope, the `unhandledrejection` and `rejectionhandled` events are common to all of these runtimes. In Node.js our `globalThis`, of course, does not implement `EventTarget` and implements unhandled rejection events using `EventEmitter` instead. This ends up causing some interop headaches for library authors who have to special case handling of these events across runtimes.\r\n\r\nWe've discussed this before but I'd like to re-open the discussion for consideration around the questions:\r\n\r\n* Should Node.js `globalThis` implement `EventTarget`\r\n* Should we emit the Web standard `unhandledrejection` and `rejectionhandled` events on `globalThis` and either (a) deprecate or (b) mark as legacy the equivalent events on `process`?\r\n* Should we support other Web standard events on `globalThis` including:\r\n  * `beforeunload` (equivalent to `process`' `beforeExit` event)\r\n  * `error` (equivalent to `process`' `uncaughtException` event\r\n  * `load` \r\n  * `message` (equivalent to `process`' `message` event)\r\n  * `messageerror` \r\n  * etc\r\n\r\n@nodejs/tsc @nodejs/web-standards \r\n\r\nContext: this came up in a WinterCG discussion around whether interoperable/consistent handling of unhandled errors and rejections should be covered by the WinterCG common minimum API spec.",
      "solution": "I\u2019m -1 as I think it would add more confusion. None of those mechanisms are correct for library authors.\r\n\r\nMost libraries should not touch global objects, because they can conflict in behaviors.\r\n\r\nFor library authors, the problem is that more often you want to \u201cexecute something only if OBJ was not GCed\u201d to avoid memory leaks on global objects. This is currently hacky, widespread and problematic with the use of FinalizationRegistry.\r\n\r\nApplication builders should use these events, not library authors. Application builders target a specific runtime, so there is not much of a compatibility issue to discuss.\n\n---\n\n> Also, \"libraries should not touch global objects\" is obviously incorrect when it comes to other kinds of globals (URL, crypto.subtle, Buffer, etc).\n\nMost of those do not allow changing configuration that alter how the process work or is set up. \n\nThe problem I'm describing happens when _installing_ custom behavior. Most libraries out there would not install an `uncaughtException` or `exit` or `beforeExit` handlers, because they will actually leak memory in most situations. Most of the times adding those in libraries introduce unexpected behaviors for the end users.",
      "labels": [],
      "created_at": "2024-01-04T17:45:36Z",
      "closed_at": "2025-06-09T14:21:29Z",
      "url": "https://github.com/nodejs/node/issues/51372",
      "comments_count": 11
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 56577,
      "title": "Make Buffer.prototype methods callable with Uint8Array instances",
      "problem": "### What is the problem this feature will solve?\n\nFrom the docs:\n\n> Node.js APIs accept plain `Uint8Array`s wherever `Buffer`s are supported as well.\n\nOne notable place where plain Uint8Arrays are not _always_ accepted is the methods on the Buffer subclass. Take the method `buf.readIntLE(offset, byteLength)`:\n\n```js\nBuffer.prototype.readIntLE.call(Uint8Array.of(1, 0, 0, 0, 0), 0, 3);\n// 1\n\nBuffer.prototype.readIntLE.call(Uint8Array.of(1, 0, 0, 0, 0), 0, 4) \n// Uncaught TypeError: this.readInt32LE is not a function\n//    at Uint8Array.readIntLE (node:internal/buffer:349:17)\n```\n\nDepending on the `byteLength` given, the method will respond correctly or throw a `TypeError` if it happens to be written using a code path that uses an internal method found only on the Buffer prototype. \n\nAs a library author, I would also like to easily accept Uint8Arrays wherever Buffers are accepted and not spend time converting the input to a Buffer instance to get useful byte utilities to work on both possible inputs.\n\n### What is the feature you are proposing to solve the problem?\n\nI propose to make the few code paths that rely on internal methods invoke those functions in a way that doesn't require them to be defined on the currently bound `this` value's prototype.\n\nFor example:\n```js\nreturn this.readInt32LE(offset);\n```\nbecomes\n```js\nreturn FunctionPrototypeCall(readInt32LE, this, offset);\n```\n\nAnd add coverage for existing and future methods to maintain this contract.\n\n\n### What alternatives have you considered?\n\nA couple but less desirable alternatives:\n- Convert input by invoking `Buffer.from(ArrayBuffer, byteOffset, byteLength)`\n- Reimplement the methods in library to work with either input",
      "solution": "There has been no activity on this feature request for 5 months. To help maintain relevant open issues, please add the https://github.com/nodejs/node/labels/never-stale label or close this issue if it should be closed. If not, the issue will be automatically closed 6 months after the last non-automated comment.\nFor more information on how the project manages feature requests, please consult the [feature request management document](https://github.com/nodejs/node/blob/HEAD/doc/contributing/feature-request-management.md).\n\n---\n\nThere has been no activity on this feature request for 5 months. To help maintain relevant open issues, please add the https://github.com/nodejs/node/labels/never-stale label or close this issue if it should be closed. If not, the issue will be automatically closed 6 months after the last non-automated comment.\nFor more information on how the project manages feature requests, please consult the [feature request management document](https://github.com/nodejs/node/blob/HEAD/doc/contributing/feature-request-management.md).",
      "labels": [
        "feature request"
      ],
      "created_at": "2025-01-13T02:19:52Z",
      "closed_at": "2026-01-21T14:51:12Z",
      "url": "https://github.com/nodejs/node/issues/56577",
      "comments_count": 4
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 60397,
      "title": "Perf regression in Node 22/24 when loading JS files",
      "problem": "### Version\n\nv22.21.0\n\n### Platform\n\n```text\nany\n```\n\n### What steps will reproduce the bug?\n\nThere is a performance regression when loading JS files from a location with latency (e.g. network path).\n\nUnfortunately I don't know how to provide a sample to reproduce it in other conditions than my own environment, BUT, I think I identified the root cause in the NodeJS code (see below).\n\n### How often does it reproduce? Is there a required condition?\n\nThe issue is not present in:\n- v20.19.5\n- v21.4.0\n\nThe issue is partially fixed in:\n- v22.18.0\n- v22.19.0\n\nIt is present in:\n- v21.5.0\n- v22.21.0\n- v24.10.0\n\n### What is the expected behavior? Why is that the expected behavior?\n\nThe expected behavior is to load files as fast as in v20.x.\n\n### What do you see instead?\n\nFiles take up to 4.5 times slower to load (in my environment).\n\n### Additional information\n\nI cloned the repo and tested each commit to reach the following conclusion:\n1. The regression was introduced in v21.5.0 by #50322.\n1. A first fix in v22.19.0 (#59086) fixed the issue partially but introduced an increase of memory consumption.\n1. A second fix in v22.21.0 (#59888) resolved the memory issue but reintroduced the performance regression.\n\nI went through the code of each PR and here is my understanding:\n1. The initial change #50322 moved the search of a package.json file from the JS side to the C++ side. I'm not a C++ dev but I believe a part of the previous algorithm was missed and now the cache only applies to the content of the package.json but not to the file system search. So when walking up the file tree, each possible location for the package.json is tested again and again. It becomes particularly painful when the package.json is located high up the file tree.That would very well explain why the performance degraded when the file system operations are costly, and was not seen in a purely local context.\n2. The first fix reintroduced caching on the JS side (which probably defeats at least partially the purpose of the initial change), but it duplicated the content of the package.json for each starting path location, thus increasing the consumed memory significantly.\n3. The second fix reintroduced the loop over the file system search on the JS side (which is kind of funny because it completely rolls back the initial change) while still not caching the results of the previous operations, thus bringing back the performance regression.\n\nI tested the following change in the method `findParentPackageJSON` of `lib\\internal\\modules\\package_json_reader.js` and it does resolve the performance issue:\n```js\nconst packageJSONLocationCache = new SafeMap();\n// [...]\nconst maybePackageJSONPath = checkPath + path.sep + 'package.json';\nif (packageJSONLocationCache.has(maybePackageJSONPath)) {\n  if (packageJSONLocationCache.get(maybePackageJSONPath)) {\n    return maybePackageJSONPath\n  } else {\n    continue\n  }\n}\nconst stat = internalFsBinding.internalModuleStat(checkPath + path.sep + 'package.json');\n\nconst packageJSONExists = stat === 0;\npackageJSONLocationCache.set(maybePackageJSONPath, packageJSONExists)\nif (packageJSONExists) {\n  return maybePackageJSONPath;\n}\n```\n\nHowever I'm not sure this should be merged. If the intent of the initial change was to bring all that logic to the C++ side for performance reasons, it might be preferable to rollback all the changes done on the JS side and to fix the caching on the C++ side.\n\nUnfortunately, I'm a JS/TS dev and I failed miserably to fix the C++ code \ud83d\ude22 ",
      "solution": "I've tested locally the fix from [#60425](https://github.com/nodejs/node/pull/60425) in the v22.x branch and I confirm that the performance issue is resolved! Thanks @michaelsmithxyz!\n\nAs for the discussion around the boundary-crossing, the measures in my environment show the following:\n- v20.19.5 &rarr; 1.273s\n- v21.4.0 &rarr; 657.488ms (I believe the perf improvement comes from the V8 upgrade)\n- v21.5.0 &rarr; 3.951s (this version contains the change from [#50322](https://github.com/nodejs/node/pull/50322))\n- v22.21.1-pre &rarr; 4.012s (latest v22 at the time I ran the test)\n- v22.21.1-pre (with fix from [#60425](https://github.com/nodejs/node/pull/60425)) &rarr; 904.5ms\n\nThe values don't really matter since I can't share a reproducible sample, but I'm posting them just to compare.\n\nIn the end, it seems that the change from [#50322](https://github.com/nodejs/node/pull/50322) does have a negative performance impact, at least in some cases. Since the author of the PR claimed that it boosted the perf in their benchmark regarding `vite --version`, I don't know what to conclude.\n\nTagging @anonrig in case you have an opinion on the matter!\n\n---\n\nI think there may be multiple things going on here, which might be why this has maybe been a bit of a moving target.\n\nThe first, which is what this issue is about, is about performance when we're using a slow filesystem. I was able to reproduce this by just mounting a local directory as an NFS mount (from localhost) with `noac` and running the benchmarks in https://github.com/nodejs/node/pull/59888. The story here is (this is discussed above but to recap again):\n\n1. https://github.com/nodejs/node/pull/50322 moved the `package.json` resolution and JSON parsing to C++, including the cache for parsed `package.json` objects, but in doing so changed the cache behavior subtly such that the case where `BindingData::GetPackageJSON` is called with a path that doesn't exist is not cached. This means that when `BindingData::TraverseParent` loops and tries identical paths that don't exist repeatedly, we attempt to open and read those same paths repeatedly, which degrades performance, particularly on slow filesystems.\n2. https://github.com/nodejs/node/pull/59086 resolved this by caching by `checkPath` in `getNearestParentPackageJSON` on the JS side. This was a _big_ regression in memory usage a lot of the time, because distinct modules often share a `package.json`, but it avoids the repeated filesystem walk for multiple calls with the same `checkPath`\n3. https://github.com/nodejs/node/pull/59888 resolves the memory usage regression but does so by reintroducing essentially the same uncached filesystem traversal regression that was there originally\n\nThe other factor that I think affects performance differences we're observing across the changes, particularly on fast filesystems, is the lazy parsing of `imports` and `exports` on package config objects in `package_json_reader.js`. There are a few things going on here:\n1. I don't think the attempt to lazily parse and memoize `imports` and `exports` on demand works as expected because the getters are set on an object that's immediately spread, meaning we just immediately run them both anyway: https://github.com/nodejs/node/blob/main/lib/internal/modules/package_json_reader.js#L79\n2. The memoization there only works if a given `package.json` file maps to the _same_ package config instance every time. Of the three changes listed above, this is only true in https://github.com/nodejs/node/pull/59888 which I think is why I observed such a significant improvement in the \"require `date-fns` in a loop\" benchmark. On other versions, we parse the same data repeatedly.\n\nI pushed a change to https://github.com/nodejs/node/pull/60425 that attempts to work around this by adding weird two-level caching to `package_json_reader.js`. It works, but it feels kind of gross. I don't think we should have to call the C++ `GetNearestParentPackageJSON` if we're going to just throw the result away and use the instance we already have, but it at least validates the idea. I don't see a super obvious way to do this better other than maybe dropping the lazy parsing idea and parsing `exports` and `imports` in C++. @anonrig if you have thoughts though, I'd love to hear them!\n\n---\n\nHi @anonrig, have you been able to take a look at https://github.com/nodejs/node/pull/60425? I believe it's the right approach as it reverts changes on the JS side and simply adds the necessary caching on the C++ side to reduce the number of FS operations.\n\nI'd really like to get this issue fixed soon because there is no Node 22.x version that's really usable in my environment right now\u2639\ufe0f.",
      "labels": [],
      "created_at": "2025-10-24T22:23:28Z",
      "closed_at": "2026-01-20T13:02:27Z",
      "url": "https://github.com/nodejs/node/issues/60397",
      "comments_count": 10
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 61353,
      "title": "When Node.js is run with Amaro, but also with `--jitless` flag error message about lack of `WebAssembly` global could be more descriptive",
      "problem": "When I run `node --jitless index.ts`  for this `index.ts` file\n```typescript\nconsole.log('Hello from TypeScript!');\n```\n\nthen I got this error message:\n\n<img width=\"1123\" height=\"672\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/dc0cebff-e51e-4540-b451-496a41d9ce61\" />\n\nThis is small, but maybe it could be improved by making error message a little more descriptive about lack of `WebAssembly` global object when `V8` is running in \"JIT-less\" mode \ud83e\udd14 ",
      "solution": "I think that those checks should be performed as close as possible to the code which is actually using `WebAssembly`, so probably one check should be performed in Amaro, and another one in Undici itself. I would go with such solution to mitigate potentially situation when e.g. Amaro will stop using `WebAssembly`, but assertion whether `WebAssembly` is present will still be performed in https://github.com/nodejs/node \ud83e\udd14 ",
      "labels": [
        "good first issue"
      ],
      "created_at": "2026-01-11T14:58:32Z",
      "closed_at": "2026-01-17T21:00:41Z",
      "url": "https://github.com/nodejs/node/issues/61353",
      "comments_count": 3
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 61386,
      "title": "Incorrect result from isDeepStrictEqual on two different Sets",
      "problem": "### Version\n\n22.21.1\n\n### Platform\n\n```text\nMicrosoft Windows NT 10.0.19045.0 x64\n```\n\n### Subsystem\n\nutil\n\n### What steps will reproduce the bug?\n\nThe problem can be seen in a quick REPL session:\n\n> Welcome to Node.js v22.21.1.\n> Type \".help\" for more information.\n> \\> a = new Set([0, new Set([1, 2, 3]), new Set([4, 5, 6])])\n> Set(3) { 0, Set(3) { 1, 2, 3 }, Set(3) { 4, 5, 6 } }\n> \\> b = new Set([0, new Set([1, new Set([2, 3]), new Set([20, 30])]), new Set([4, new Set([5, 6]), new Set([50, 60])])])\n> Set(3) {\n>   0,\n>   Set(3) { 1, Set(2) { 2, 3 }, Set(2) { 20, 30 } },\n>   Set(3) { 4, Set(2) { 5, 6 }, Set(2) { 50, 60 } }\n> }\n> \\> require('util').isDeepStrictEqual(a, b)\n> true\n\n\n### How often does it reproduce? Is there a required condition?\n\nInterestingly, `require('util').isDeepStrictEqual(b, a)` returns false, as expected. Which indicates that isDeepStrictEqual is not symmetrical.\n\n### What is the expected behavior? Why is that the expected behavior?\n\nThe result should be false.\n\n### What do you see instead?\n\nI'm seeing true.\n\n### Additional information\n\n_No response_",
      "solution": "This is a case specific to mixed primitive / object cases. I will open a fix tomorrow, I identified the issue in the code. It also applies the same to maps.",
      "labels": [
        "confirmed-bug"
      ],
      "created_at": "2026-01-14T19:24:10Z",
      "closed_at": "2026-01-17T01:12:47Z",
      "url": "https://github.com/nodejs/node/issues/61386",
      "comments_count": 4
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 51907,
      "title": "test:start events are not emitted correctly",
      "problem": "### Version\r\n\r\n21.6.2\r\n\r\n### Platform\r\n\r\nwindows x64\r\n\r\n### Subsystem\r\n\r\ntest_runner\r\n\r\n### What steps will reproduce the bug?\r\n\r\n1. Have a test file like\r\n\r\n  **test.js**\r\n  ```js\r\n  const { describe, it } = require('node:test');\r\n  const { strictEqual } = require('node:assert');\r\n\r\n  describe('math', () => {\r\n    it('addition', async () => {\r\n      await new Promise((resolve) => setTimeout(resolve, 5000));\r\n      strictEqual(1 + 1, 2);\r\n    });\r\n\r\n    it(`subtraction`, async () => {\r\n      strictEqual(1 - 1, 0);\r\n    });\r\n  });\r\n  ```\r\n\r\n  and a **reporter.js**\r\n\r\n  ```js\r\n  module.exports = async function* reporter(stream) {\r\n    for await (const evt of stream) {\r\n      console.log(new Date().toISOString(), evt.type, evt.data);\r\n    }\r\n  };\r\n  ```\r\n\r\n2. Run `node --test-reporter=./reporter.js test.js`\r\n\r\n### How often does it reproduce? Is there a required condition?\r\n\r\n100%\r\n\r\n### What is the expected behavior? Why is that the expected behavior?\r\n\r\n`test:start` should be fired before `math addition` is run\r\n\r\n### What do you see instead?\r\n\r\n`test:start` is fired only after the test finishes. Here's an example log, notice that test:started is fired only after the 5 second delay, which is in fact after the tests already finished.\r\n\r\n[log.txt](https://github.com/nodejs/node/files/14429206/log.txt)\r\n\r\n\r\n### Additional information\r\n\r\nI originally requested this in https://github.com/nodejs/node/issues/46727 which resulted in test:queued and test:dequeued events being added. (Maybe this should be titled instead a general \"I (still) cannot tell when a test is started\")\r\n\r\nHowever these are not useful because the tests are just enqueued event when they're not running: `subtraction` is enqueued alongside `addition` and based on the results it 'takes' 5 seconds to run, but in reality is instant.",
      "solution": "Thanks for opening the issue. I'll try to take a look later this week\n\n---\n\nAhh, I see! The dequeued events makes sense now. It looks like the order is also preserved even when tests run in parallel, so I think that would solve my use case.\r\n\r\n> add an event fired immediately when the test completes\r\n\r\nThis would be a nice to have just to report results more quickly to users.\r\n\r\n> improve the documentation for the events that are reported in order of declaration\r\n\r\nThis would be handy!\r\n\r\nI'll play with these some more this evening and close out the issue unless I run into any further difficulty \ud83d\ude42 \n\n---\n\n@MoLow Thanks for the explanation. Naming and execution of these events is highly confusing.\n\nI think it's also worth mentioning that you _can_ listen for `test:dequeue`. Yes, it will run before the test executes and the event can also contain async code, **but** the test won't wait for the event listener to finish. Upcoming tests will continue to run in the background and you might not be aware of it, because 'test:stderr' logs are delayed.\n\nIf you need logging in real-time, because you want to debug timing-sensitive code, then Node.js Tests aren't a solution, because execution and events/logging might not be in sync.",
      "labels": [
        "test_runner"
      ],
      "created_at": "2024-02-28T05:21:54Z",
      "closed_at": "2024-03-01T10:08:10Z",
      "url": "https://github.com/nodejs/node/issues/51907",
      "comments_count": 6
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 61377,
      "title": "parallel/test-internal-util-construct-sab fails on 32-bit",
      "problem": "### Version\n\n24.13.0\n\n### Platform\n\n```text\nlinux debian 14 (unstable) i686, armhf\n```\n\n### Subsystem\n\ntests\n\n### What steps will reproduce the bug?\n\nnode test/parallel/test-internal-util-construct-sab.js\n\n### How often does it reproduce? Is there a required condition?\n\nAlways fail on 32-bits arch\n\n### What is the expected behavior? Why is that the expected behavior?\n\nNo failure\n\n### What do you see instead?\n\nnot ok 2047 parallel/test-internal-util-construct-sab\n  ---\n  duration_ms: 212.01600\n  severity: fail\n  exitcode: 1\n  stack: |-\n    /tmp/tmp.12E6MOvs2i.nodejs/test/parallel/test-internal-util-construct-sab.js:13\n      assert(isSharedArrayBuffer(constructSharedArrayBuffer(length)));\n                                 ^\n    \n    RangeError: Invalid array buffer length\n\n### Additional information\n\nI suppose 2 ** 32 is not a valid array buffer length on 32-bits architectures",
      "solution": "This was fixed in #61026, it just hasn't landed in a release yet.\n\n---\n\nOh ! I checked the issues, not the PR, for once. My bad.",
      "labels": [],
      "created_at": "2026-01-14T08:33:20Z",
      "closed_at": "2026-01-14T10:11:42Z",
      "url": "https://github.com/nodejs/node/issues/61377",
      "comments_count": 2
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 60717,
      "title": "util.inspect throws when given a ZodError",
      "problem": "### Version\n\nv24.11.1\n\n### Platform\n\n```text\nDarwin <hostname> 24.6.0 Darwin Kernel Version 24.6.0: Mon Aug 11 21:16:34 PDT 2025; root:xnu-11417.140.69.701.11~1/RELEASE_ARM64_T6020 arm64\n```\n\n### Subsystem\n\nutil\n\n### What steps will reproduce the bug?\n\nUsing `zod@3.25.76`\n\n```\nconst { inspect } = require('node:util');\nconst { z } = require('zod');\n\ninspect(z.object({ foo: z.string() }).safeParse().error);\n```\n\n\n### How often does it reproduce? Is there a required condition?\n\nSeems to happen on any `ZodError`\n\n### What is the expected behavior? Why is that the expected behavior?\n\nIt should inspect the error, same as it did on Node v22\n\n### What do you see instead?\n\nException.\n\n```\nnode:internal/util/inspect:1183\n    if (!isStackOverflowError(err)) throw err;\n                                    ^\n\nTypeError: Cannot read properties of undefined (reading 'value')\n    at formatProperty (node:internal/util/inspect:2279:12)\n    at formatRaw (node:internal/util/inspect:1176:9)\n    at formatValue (node:internal/util/inspect:932:10)\n    at inspect (node:internal/util/inspect:409:10)\n    at Object.<anonymous> (/junk.js:4:1)\n    at Module._compile (node:internal/modules/cjs/loader:1761:14)\n    at Object..js (node:internal/modules/cjs/loader:1893:10)\n    at Module.load (node:internal/modules/cjs/loader:1481:32)\n    at Module._load (node:internal/modules/cjs/loader:1300:12)\n    at TracingChannel.traceSync (node:diagnostics_channel:328:14)\n\nNode.js v24.11.1\n```\n\n### Additional information\n\nI believe this is a regression caused by https://github.com/nodejs/node/commit/748d4f6430034e2e16ced900354bc680ad9e9a69 - based on the timing / line numbers.",
      "solution": "So the root cause is that the zod error has an `errors` property that is not an own property.\n\nWith the change there are two properties were this could happen: `cause` and `errors` on errors. All other properties behave regularly and would not need a different handling. It is not an issue for most errors, since they would have these as own non-enumerable properties.\n\nThe best fix I could think of right now is adding this right after the `desc ??= ObjectGetOwnPropertyDescriptor(value, key)` in the `formatProperty()` method:\n\n```js\n  while (desc === undefined) {\n    value = ObjectGetPrototypeOf(value);\n    desc = ObjectGetOwnPropertyDescriptor(value, key);\n  }\n```\n\nThat way it will have the correct enumerability and show that it is a getter. That would otherwise not be possible.\n\nI am just not happy that we seemingly have to add a special handling there just for these two properties.\n\n@mnahkies @siaeyy would one of you be willing to open a PR with that change and adding a regression test for both mentioned properties?\n\n---\n\n@siaeyy I am not sure I follow. That code is the root cause of the problem, correct. It adds keys that have to be handled, while not knowing who the owning object is. That is why I named these two properties above :)\n\n---\n\n@aduh95, it seems that the fix hasn't been included in v24 yet. Will the fix get included in v24 soon? This bug prevents people from updating to the latest version, which is particularly important now that multiple vulnerabilities were fixed in the latest release yesterday.",
      "labels": [
        "confirmed-bug",
        "util"
      ],
      "created_at": "2025-11-14T16:32:03Z",
      "closed_at": "2025-12-22T13:06:15Z",
      "url": "https://github.com/nodejs/node/issues/60717",
      "comments_count": 10
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 56532,
      "title": "Use the core module without importing or requiring it.",
      "problem": "### What is the problem this feature will solve?\n\nUtilize a core module directly in your application without the need for importing or requiring it.\n\n### What is the feature you are proposing to solve the problem?\n\ndireclty using its like fs.readFile etc\n\n### What alternatives have you considered?\n\n_No response_",
      "solution": "There has been no activity on this feature request for 5 months. To help maintain relevant open issues, please add the https://github.com/nodejs/node/labels/never-stale label or close this issue if it should be closed. If not, the issue will be automatically closed 6 months after the last non-automated comment.\nFor more information on how the project manages feature requests, please consult the [feature request management document](https://github.com/nodejs/node/blob/HEAD/doc/contributing/feature-request-management.md).\n\n---\n\nThere has been no activity on this feature request for 5 months. To help maintain relevant open issues, please add the https://github.com/nodejs/node/labels/never-stale label or close this issue if it should be closed. If not, the issue will be automatically closed 6 months after the last non-automated comment.\nFor more information on how the project manages feature requests, please consult the [feature request management document](https://github.com/nodejs/node/blob/HEAD/doc/contributing/feature-request-management.md).",
      "labels": [
        "feature request",
        "stale"
      ],
      "created_at": "2025-01-09T13:41:45Z",
      "closed_at": "2026-01-14T01:35:25Z",
      "url": "https://github.com/nodejs/node/issues/56532",
      "comments_count": 7
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 58380,
      "title": "Node 24.0.2: Memory leak on `fetch` response `.text()` ",
      "problem": "### Version\n\n24.0.2\n\n### Platform\n\n```text\nMicrosoft Windows NT 10.0.26100.0 x64\n```\n\n### Subsystem\n\n_No response_\n\n### What steps will reproduce the bug?\n\n```js\nconst response = await fetch(url);\nconst text = await response.text();\n```\n\n### How often does it reproduce? Is there a required condition?\n\nI detected the leak in a script with succeeding fetch calls (40/s).\nAfter 10000 requests, or so, the script terminated with code `134`.\nWindows logs report `EventName RADAR_PRE_LEAK_64`.\nI identified the source to `await response.text()`.\nI tried different alternatives to fetch:\n- undici\n- node-fetch\n- https\nBut the leak persisted.\n\n\nThis may only affect arm64 systems, not sure if only windows 11. There are very few discussions about this anywhere and this [comment](https://github.com/nodejs/undici/issues/1108#issuecomment-2392038061)\n\nFollowing the comment I downgraded to lts 22.15.1 and the memory leak is slower to develop but it still creeps in.\n\n### What is the expected behavior? Why is that the expected behavior?\n\nNo memory leak.\n\n### What do you see instead?\n\nMemory leak.\n\n### Additional information\n\nProcessor\tSnapdragon(R) X - X126100 - Qualcomm(R) Oryon(TM) CPU, 2956 Mhz, 8 Core(s), 8 Logical Processor(s)\nBIOS Version/Date\tInsyde UX3407QA.305, 10/01/2025\n",
      "solution": "Hi,\n\nI have been trying to investigate a memory leak, that started happening to many of our services after update from Node v22 to v24 and I was also able to pinpoint the issue to `fetch`, but I was failing to reproduce the issue locally. The issue started happening inside docker containers running in our kubernetes environment.\n\nI did some more deep dive and here are the things I found out about when the memory leak does, or does not happen.\n- **I was able to reproduce this only when accessing some website via HTTPS** (no memory leak with HTTP)\n  - I was unable to reproduce this with local HTTPS server (I tried Node.js `https`/`http2` modules and Nginx without any success)\n  - I was unable to reproduce this with local / remote HTTP server\n  - **I was able to reproduce this by fetching https://api.github.com/zen** (even though I was being rate limited, hopefully Github will forgive me)\n- **I was able to reproduce this when sending additional header `Connection: 'close'`**\n  - This is a bit tricky as some of our services are not sending `Connection: 'close'` header and have the same memory leak, but my reproduction scenario below relies on this header\n  - This actually makes fetch a bit unstable and there are some timeout errors happening every now and then - could be related to memory leak, but the errors are happening both with Node.js v22 and v24, but memory leak happens only with v24.\n- I was unable to reproduce this issue on MacOS (v26)\n- **I was able to reproduce this issue \"locally\" (on MacOS) with Docker containers** (I tried `node:24`, `node:24-slim` and `node:24-alpine` images and they all produce the memory leak)\n- I was unable to reproduce this issue with Node.js v22 Docker containers\n- I was unable to see any memory leaks in the heap snapshot when running node with `--inspect`\n- I was unable to reproduce this issue when I replaced `fetch` with native http(s) module\n- I was unable to reproduce this issue when I switched from `fetch` to `axios`\n\nHere is a less than optimal setup, that allowed me to reproduce this issue locally with docker.\n\n**Dockerfile**\n```Dockerfile\nFROM node:24.11.0-alpine\n\nWORKDIR /home/memory-leak\n\nCOPY index.js ./\n\nCMD [ \"node\", \"--expose-gc\", \"index.js\" ]\n```\n\n**index.js**\n```javascript\nasync function startFetchLoop (url) {\n    let count = 0;\n    while (true) {\n        await fetch(url, {\n            headers: {\n                Connection: 'close',\n            },\n        }).then(res => res.text()).catch(error => {\n            console.log(error?.toString?.() ?? error);\n        });\n\n        count++;\n        if (count % 1000 === 0) {\n            // eslint-disable-next-line no-undef\n            gc();\n            const mem = process.memoryUsage();\n            console.log(\n                `Iter ${count} \u2192 RSS=${mem.rss}, HeapUsed=${mem.heapUsed}, HeapTotal=${mem.heapTotal}`\n            );\n        }\n    }\n}\n\nasync function main () {\n    // Begin the endless fetch loop\n    await startFetchLoop('https://api.github.com/zen');\n}\n\nmain();\n```\n\nRun `docker build -t memory-leak . && docker run memory-leak`\n\nNode.js v24.11.0 output (after ~1 hour):\n\n```\nIter 1000 \u2192 RSS=113393664, HeapUsed=9557648, HeapTotal=20447232\nIter 2000 \u2192 RSS=113631232, HeapUsed=8367392, HeapTotal=16515072\nIter 3000 \u2192 RSS=114724864, HeapUsed=8153896, HeapTotal=15466496\nIter 4000 \u2192 RSS=115535872, HeapUsed=8236256, HeapTotal=15728640\nIter 5000 \u2192 RSS=116453376, HeapUsed=8309424, HeapTotal=17039360\nIter 6000 \u2192 RSS=117731328, HeapUsed=8435952, HeapTotal=15728640\nIter 7000 \u2192 RSS=118546432, HeapUsed=8457048, HeapTotal=15990784\nIter 8000 \u2192 RSS=119443456, HeapUsed=8421264, HeapTotal=15466496\nIter 9000 \u2192 RSS=120291328, HeapUsed=8408864, HeapTotal=16515072\nIter 10000 \u2192 RSS=120807424, HeapUsed=8472768, HeapTotal=16252928\nIter 11000 \u2192 RSS=121581568, HeapUsed=8610536, HeapTotal=17039360\nIter 12000 \u2192 RSS=122273792, HeapUsed=8558192, HeapTotal=16252928\nIter 13000 \u2192 RSS=124071936, HeapUsed=8465760, HeapTotal=17563648\nIter 14000 \u2192 RSS=124633088, HeapUsed=8597576, HeapTotal=17301504\nIter 15000 \u2192 RSS=125399040, HeapUsed=8550488, HeapTotal=16777216\nIter 16000 \u2192 RSS=126578688, HeapUsed=8538256, HeapTotal=17301504\nIter 17000 \u2192 RSS=126812160, HeapUsed=8516512, HeapTotal=16777216\nIter 18000 \u2192 RSS=127434752, HeapUsed=8497576, HeapTotal=17301504\nIter 19000 \u2192 RSS=128266240, HeapUsed=8627408, HeapTotal=17301504\nIter 20000 \u2192 RSS=129277952, HeapUsed=8595416, HeapTotal=17039360\nIter 21000 \u2192 RSS=130011136, HeapUsed=8812560, HeapTotal=17301504\nIter 22000 \u2192 RSS=130695168, HeapUsed=8561416, HeapTotal=17563648\nIter 23000 \u2192 RSS=131489792, HeapUsed=8658056, HeapTotal=17301504\nIter 24000 \u2192 RSS=132857856, HeapUsed=8551304, HeapTotal=17563648\n```\n\n<img width=\"1130\" height=\"566\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/2bf3f966-e5ac-4e92-8e9e-87a534e08d58\" />\n\nI tried to do the same thing but with Node.js v22.21.1, here is the output (no memory leak)\n\n```\nIter 1000 \u2192 RSS=101937152, HeapUsed=8275544, HeapTotal=17072128\nIter 2000 \u2192 RSS=101736448, HeapUsed=7298840, HeapTotal=10518528\nIter 3000 \u2192 RSS=102912000, HeapUsed=7457032, HeapTotal=11304960\nIter 4000 \u2192 RSS=103813120, HeapUsed=7636760, HeapTotal=11567104\nIter 5000 \u2192 RSS=104042496, HeapUsed=8250712, HeapTotal=12877824\nIter 6000 \u2192 RSS=104030208, HeapUsed=8468072, HeapTotal=12353536\nIter 7000 \u2192 RSS=104308736, HeapUsed=8174072, HeapTotal=13139968\nIter 8000 \u2192 RSS=104366080, HeapUsed=8116248, HeapTotal=12877824\nIter 9000 \u2192 RSS=104624128, HeapUsed=8554872, HeapTotal=12877824\nIter 10000 \u2192 RSS=104419328, HeapUsed=7963544, HeapTotal=12091392\nIter 11000 \u2192 RSS=104062976, HeapUsed=8414232, HeapTotal=13139968\nIter 12000 \u2192 RSS=104595456, HeapUsed=8361240, HeapTotal=12091392\nIter 13000 \u2192 RSS=104456192, HeapUsed=8625200, HeapTotal=12091392\nIter 14000 \u2192 RSS=104144896, HeapUsed=8222672, HeapTotal=12091392\nIter 15000 \u2192 RSS=104820736, HeapUsed=8223936, HeapTotal=12353536\nIter 16000 \u2192 RSS=104960000, HeapUsed=8348424, HeapTotal=12615680\nIter 17000 \u2192 RSS=104542208, HeapUsed=8302720, HeapTotal=12615680\nIter 18000 \u2192 RSS=104550400, HeapUsed=8226800, HeapTotal=12615680\nIter 19000 \u2192 RSS=104849408, HeapUsed=8353432, HeapTotal=12615680\nIter 20000 \u2192 RSS=104525824, HeapUsed=8655936, HeapTotal=12353536\nIter 21000 \u2192 RSS=104640512, HeapUsed=8339904, HeapTotal=12615680\nIter 22000 \u2192 RSS=104943616, HeapUsed=8559968, HeapTotal=12877824\nIter 23000 \u2192 RSS=104636416, HeapUsed=8639312, HeapTotal=12877824\nIter 24000 \u2192 RSS=104869888, HeapUsed=8336016, HeapTotal=12615680\nIter 25000 \u2192 RSS=104714240, HeapUsed=8276432, HeapTotal=12615680\n```\n\n<img width=\"1119\" height=\"545\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/7e66aeaa-4106-4068-8b2d-7d4f3e37e4f3\" />\n\nAnd I also tried a scenario with Node.js v24, but without the `Connection: 'close'` header (this was generating way more requests per sec compared to the above scenarios, but no memory leak)\n\n```\nIter 1000 \u2192 RSS=103608320, HeapUsed=8679880, HeapTotal=12845056\nIter 2000 \u2192 RSS=104919040, HeapUsed=8614768, HeapTotal=18874368\nIter 3000 \u2192 RSS=105127936, HeapUsed=8380648, HeapTotal=18350080\nIter 4000 \u2192 RSS=104939520, HeapUsed=8413640, HeapTotal=19136512\nIter 5000 \u2192 RSS=104923136, HeapUsed=7156488, HeapTotal=19398656\nIter 6000 \u2192 RSS=105263104, HeapUsed=7167920, HeapTotal=18350080\nIter 7000 \u2192 RSS=105209856, HeapUsed=7168272, HeapTotal=18087936\nIter 8000 \u2192 RSS=105115648, HeapUsed=7195720, HeapTotal=18350080\nIter 9000 \u2192 RSS=105267200, HeapUsed=7202120, HeapTotal=18350080\nIter 10000 \u2192 RSS=105390080, HeapUsed=7202312, HeapTotal=18087936\nIter 11000 \u2192 RSS=107249664, HeapUsed=7420896, HeapTotal=19136512\nIter 12000 \u2192 RSS=107155456, HeapUsed=7485768, HeapTotal=19922944\nIter 13000 \u2192 RSS=107110400, HeapUsed=7336744, HeapTotal=19398656\nIter 14000 \u2192 RSS=107180032, HeapUsed=7297536, HeapTotal=18874368\nIter 15000 \u2192 RSS=107139072, HeapUsed=7320768, HeapTotal=19398656\nIter 16000 \u2192 RSS=106864640, HeapUsed=7276048, HeapTotal=19136512\nIter 17000 \u2192 RSS=106713088, HeapUsed=7314176, HeapTotal=19136512\nIter 18000 \u2192 RSS=106885120, HeapUsed=7280640, HeapTotal=19922944\nIter 19000 \u2192 RSS=106487808, HeapUsed=7324480, HeapTotal=19398656\nIter 20000 \u2192 RSS=106823680, HeapUsed=7294904, HeapTotal=19922944\nIter 21000 \u2192 RSS=106852352, HeapUsed=7395256, HeapTotal=19922944\nIter 22000 \u2192 RSS=106651648, HeapUsed=7381352, HeapTotal=19922944\nIter 23000 \u2192 RSS=107343872, HeapUsed=7377920, HeapTotal=19922944\nIter 24000 \u2192 RSS=107331584, HeapUsed=7306064, HeapTotal=20185088\nIter 25000 \u2192 RSS=107950080, HeapUsed=7465040, HeapTotal=20185088\nIter 26000 \u2192 RSS=107487232, HeapUsed=7493392, HeapTotal=20185088\nIter 27000 \u2192 RSS=107700224, HeapUsed=7542248, HeapTotal=20185088\nIter 28000 \u2192 RSS=107843584, HeapUsed=7507864, HeapTotal=19660800\nIter 29000 \u2192 RSS=107778048, HeapUsed=7528528, HeapTotal=19136512\nIter 30000 \u2192 RSS=107737088, HeapUsed=7501480, HeapTotal=19398656\nIter 31000 \u2192 RSS=107520000, HeapUsed=7584008, HeapTotal=19398656\nIter 32000 \u2192 RSS=107528192, HeapUsed=7615624, HeapTotal=20185088\nIter 33000 \u2192 RSS=107651072, HeapUsed=7619672, HeapTotal=20185088\nIter 34000 \u2192 RSS=107319296, HeapUsed=7540440, HeapTotal=20185088\nIter 35000 \u2192 RSS=108150784, HeapUsed=7535384, HeapTotal=20185088\nIter 36000 \u2192 RSS=107560960, HeapUsed=7516928, HeapTotal=20185088\nIter 37000 \u2192 RSS=107388928, HeapUsed=7554968, HeapTotal=20185088\nIter 38000 \u2192 RSS=107585536, HeapUsed=7540624, HeapTotal=20185088\nIter 39000 \u2192 RSS=107835392, HeapUsed=7570944, HeapTotal=20185088\nIter 40000 \u2192 RSS=107855872, HeapUsed=7513328, HeapTotal=20185088\nIter 41000 \u2192 RSS=107663360, HeapUsed=7607912, HeapTotal=20185088\nIter 42000 \u2192 RSS=107646976, HeapUsed=7589728, HeapTotal=20185088\nIter 43000 \u2192 RSS=107769856, HeapUsed=7579064, HeapTotal=20185088\nIter 44000 \u2192 RSS=107323392, HeapUsed=7620608, HeapTotal=20185088\nIter 45000 \u2192 RSS=107667456, HeapUsed=7639104, HeapTotal=20185088\nIter 46000 \u2192 RSS=107556864, HeapUsed=7545784, HeapTotal=19660800\nIter 47000 \u2192 RSS=107667456, HeapUsed=7574816, HeapTotal=19922944\nIter 48000 \u2192 RSS=107683840, HeapUsed=7537696, HeapTotal=20185088\nIter 49000 \u2192 RSS=107663360, HeapUsed=7571048, HeapTotal=20185088\nIter 50000 \u2192 RSS=107585536, HeapUsed=7506160, HeapTotal=20185088\nIter 51000 \u2192 RSS=107634688, HeapUsed=7616872, HeapTotal=20185088\nIter 52000 \u2192 RSS=107499520, HeapUsed=7631552, HeapTotal=20185088\nIter 53000 \u2192 RSS=107667456, HeapUsed=7636384, HeapTotal=20185088\nIter 54000 \u2192 RSS=107859968, HeapUsed=7499360, HeapTotal=20185088\nIter 55000 \u2192 RSS=107917312, HeapUsed=7535920, HeapTotal=20185088\nIter 56000 \u2192 RSS=107700224, HeapUsed=7562528, HeapTotal=20185088\nIter 57000 \u2192 RSS=108060672, HeapUsed=7555360, HeapTotal=20185088\nIter 58000 \u2192 RSS=107790336, HeapUsed=7525184, HeapTotal=20185088\nIter 59000 \u2192 RSS=108040192, HeapUsed=7578640, HeapTotal=20185088\nIter 60000 \u2192 RSS=107991040, HeapUsed=7516384, HeapTotal=20185088\nIter 61000 \u2192 RSS=107814912, HeapUsed=7658440, HeapTotal=19136512\nIter 62000 \u2192 RSS=107696128, HeapUsed=7603840, HeapTotal=20185088\nIter 63000 \u2192 RSS=107806720, HeapUsed=7601440, HeapTotal=20185088\nIter 64000 \u2192 RSS=107712512, HeapUsed=7537560, HeapTotal=20185088\nIter 65000 \u2192 RSS=112287744, HeapUsed=7670352, HeapTotal=19136512\nIter 66000 \u2192 RSS=112078848, HeapUsed=7845912, HeapTotal=19918848\nIter 67000 \u2192 RSS=112046080, HeapUsed=7879640, HeapTotal=20443136\nIter 68000 \u2192 RSS=112226304, HeapUsed=7812808, HeapTotal=20705280\nIter 69000 \u2192 RSS=112590848, HeapUsed=7843920, HeapTotal=19918848\nIter 70000 \u2192 RSS=112254976, HeapUsed=7848128, HeapTotal=20443136\nIter 71000 \u2192 RSS=112455680, HeapUsed=7838248, HeapTotal=20443136\nIter 72000 \u2192 RSS=112377856, HeapUsed=7955376, HeapTotal=20443136\nIter 73000 \u2192 RSS=112246784, HeapUsed=7853728, HeapTotal=20705280\nIter 74000 \u2192 RSS=112381952, HeapUsed=7818584, HeapTotal=20705280\nIter 75000 \u2192 RSS=112533504, HeapUsed=7885408, HeapTotal=20705280\nIter 76000 \u2192 RSS=112345088, HeapUsed=7901840, HeapTotal=20705280\nIter 77000 \u2192 RSS=112078848, HeapUsed=7865920, HeapTotal=20705280\nIter 78000 \u2192 RSS=112234496, HeapUsed=7877464, HeapTotal=20705280\nIter 79000 \u2192 RSS=112156672, HeapUsed=7894648, HeapTotal=20967424\nIter 80000 \u2192 RSS=112431104, HeapUsed=7900408, HeapTotal=20705280\nIter 81000 \u2192 RSS=112381952, HeapUsed=7912096, HeapTotal=20705280\nIter 82000 \u2192 RSS=111992832, HeapUsed=7944848, HeapTotal=20705280\nIter 83000 \u2192 RSS=112574464, HeapUsed=7910832, HeapTotal=20967424\nIter 84000 \u2192 RSS=112386048, HeapUsed=7896936, HeapTotal=20443136\nIter 85000 \u2192 RSS=112574464, HeapUsed=7911480, HeapTotal=20443136\nIter 86000 \u2192 RSS=112381952, HeapUsed=7850168, HeapTotal=20443136\nIter 87000 \u2192 RSS=112394240, HeapUsed=7876160, HeapTotal=20705280\nIter 88000 \u2192 RSS=112209920, HeapUsed=7910632, HeapTotal=20705280\nIter 89000 \u2192 RSS=111910912, HeapUsed=7868128, HeapTotal=19918848\nIter 90000 \u2192 RSS=112046080, HeapUsed=7881320, HeapTotal=20705280\nIter 91000 \u2192 RSS=111988736, HeapUsed=7933952, HeapTotal=20705280\nIter 92000 \u2192 RSS=112119808, HeapUsed=7960648, HeapTotal=20967424\nIter 93000 \u2192 RSS=112189440, HeapUsed=7937240, HeapTotal=20967424\nIter 94000 \u2192 RSS=112177152, HeapUsed=7972728, HeapTotal=20443136\nIter 95000 \u2192 RSS=111898624, HeapUsed=7872712, HeapTotal=19918848\nIter 96000 \u2192 RSS=112320512, HeapUsed=7865984, HeapTotal=20705280\nIter 97000 \u2192 RSS=112001024, HeapUsed=7901344, HeapTotal=20705280\nIter 98000 \u2192 RSS=112291840, HeapUsed=7876496, HeapTotal=20705280\nIter 99000 \u2192 RSS=112254976, HeapUsed=7888904, HeapTotal=20705280\nIter 100000 \u2192 RSS=112234496, HeapUsed=7893696, HeapTotal=20967424\nIter 101000 \u2192 RSS=112340992, HeapUsed=7979128, HeapTotal=20967424\nIter 102000 \u2192 RSS=112586752, HeapUsed=7911880, HeapTotal=20967424\nIter 103000 \u2192 RSS=112357376, HeapUsed=7931440, HeapTotal=20967424\nIter 104000 \u2192 RSS=112181248, HeapUsed=7913648, HeapTotal=20967424\nIter 105000 \u2192 RSS=112226304, HeapUsed=7896776, HeapTotal=20967424\nIter 106000 \u2192 RSS=111579136, HeapUsed=7879776, HeapTotal=19656704\nIter 107000 \u2192 RSS=111587328, HeapUsed=7924104, HeapTotal=20180992\nIter 108000 \u2192 RSS=111550464, HeapUsed=7927488, HeapTotal=20443136\nIter 109000 \u2192 RSS=111562752, HeapUsed=7867808, HeapTotal=20443136\nIter 110000 \u2192 RSS=111603712, HeapUsed=7924656, HeapTotal=20705280\nIter 111000 \u2192 RSS=111480832, HeapUsed=7955528, HeapTotal=20705280\nIter 112000 \u2192 RSS=111669248, HeapUsed=7997440, HeapTotal=20705280\nIter 113000 \u2192 RSS=111820800, HeapUsed=7899408, HeapTotal=20705280\nIter 114000 \u2192 RSS=111616000, HeapUsed=7940704, HeapTotal=20705280\nIter 115000 \u2192 RSS=111611904, HeapUsed=7910344, HeapTotal=19656704\nIter 116000 \u2192 RSS=111759360, HeapUsed=7899464, HeapTotal=19918848\nIter 117000 \u2192 RSS=112197632, HeapUsed=7859288, HeapTotal=19918848\nIter 118000 \u2192 RSS=111710208, HeapUsed=7885856, HeapTotal=20180992\nIter 119000 \u2192 RSS=111587328, HeapUsed=7853920, HeapTotal=20180992\nIter 120000 \u2192 RSS=111570944, HeapUsed=7931104, HeapTotal=20180992\nIter 121000 \u2192 RSS=111554560, HeapUsed=7981184, HeapTotal=19656704\nIter 122000 \u2192 RSS=111816704, HeapUsed=7996056, HeapTotal=20180992\nIter 123000 \u2192 RSS=111599616, HeapUsed=7882408, HeapTotal=19656704\nIter 124000 \u2192 RSS=111222784, HeapUsed=7932264, HeapTotal=20180992\nIter 125000 \u2192 RSS=111366144, HeapUsed=7983392, HeapTotal=20180992\nIter 126000 \u2192 RSS=111816704, HeapUsed=7859472, HeapTotal=20180992\nIter 127000 \u2192 RSS=111656960, HeapUsed=7904896, HeapTotal=19656704\nIter 128000 \u2192 RSS=111239168, HeapUsed=7883624, HeapTotal=19918848\nIter 129000 \u2192 RSS=111595520, HeapUsed=7910144, HeapTotal=20180992\nIter 130000 \u2192 RSS=111390720, HeapUsed=7894384, HeapTotal=19656704\n```\n\n<img width=\"1127\" height=\"535\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/676f1a49-5b68-4d19-a644-a550f38b13e6\" />\n\nI struggled a lot to come up with any scenario, that would be reproducible locally without any external services, but this was the best I could do. I can try some more things if somebody has any ideas how to investigate this better. Suggestions are very welcome! :)\n\n---\n\n@mcollina Thank you for the video, I read some more of your articles and even checked [another video](https://www.youtube.com/watch?v=z3C_FW78FBs) of yours to try to understand this topic better. But it still surprises me, that the issue started happening after upgrading to Node.js v24 (it does not happen with latest v22).\n\nI am seeing, that system metrics from the container in k8s pod show a growing memory usage and that is what I am calling the memory leak. Kubernetes is indeed killing the pod (once it uses 100% of the memory), because it reached the limits defined in k8s, so the app is not crashing, but k8s is just killing it off. I am not sure if I can just remove the memory limits, but I will consult this with our infrastructure team.\n\n```\n    Last State:     Terminated\n      Reason:       OOMKilled\n      Exit Code:    1\n      Started:      Sat, 01 Nov 2025 06:37:28 +0100\n      Finished:     Sun, 02 Nov 2025 14:41:27 +0100\n```\n\nI am trying to prove that there is indeed a memory leak, but I am failing to come up with a scenario, where Node.js crashes due to OOM error (it gets always killed by someone as it grows indefinitely, or starts swapping to disk).\n\nSpecifically, I tried running my example with some additional parameters `docker run --memory 64m --memory-swap 64m memory-leak node --expose-gc --max-old-space-size=10 --max-semi-space-size=1 index.js` - this did not solve anything, memory keeps growing until it uses it all and gets killed by docker OOM killer. Unfortunately, the `--oom-kill-disable` parameter is not supported on my Mac and I am not sure how to prove / disprove that the memory would eventually stop growing (maybe running this for days/weeks, but I am trying to speed things up a bit).\n\n<details>\n  <summary>Click here to see output from this scenario. The RSS memory consistently grew by ~23MB in less than 2 hours.</summary>\n\n  ```\n  Iter 1000 \u2192 RSS=101421056, HeapUsed=7834544, HeapTotal=11272192\n  Iter 2000 \u2192 RSS=103174144, HeapUsed=7852368, HeapTotal=11010048\n  Iter 3000 \u2192 RSS=105111552, HeapUsed=7886648, HeapTotal=11010048\n  Iter 4000 \u2192 RSS=105111552, HeapUsed=7885616, HeapTotal=11272192\n  Iter 5000 \u2192 RSS=105594880, HeapUsed=7908896, HeapTotal=11272192\n  Iter 6000 \u2192 RSS=106336256, HeapUsed=7908632, HeapTotal=11272192\n  Iter 7000 \u2192 RSS=106729472, HeapUsed=7942424, HeapTotal=11272192\n  Iter 8000 \u2192 RSS=108584960, HeapUsed=8093688, HeapTotal=11534336\n  Iter 9000 \u2192 RSS=108150784, HeapUsed=8157544, HeapTotal=11534336\n  Iter 10000 \u2192 RSS=108818432, HeapUsed=8101840, HeapTotal=11272192\n  Iter 11000 \u2192 RSS=110714880, HeapUsed=8117576, HeapTotal=10485760\n  Iter 12000 \u2192 RSS=111222784, HeapUsed=8182336, HeapTotal=11272192\n  Iter 13000 \u2192 RSS=111910912, HeapUsed=8134344, HeapTotal=11272192\n  Iter 14000 \u2192 RSS=113348608, HeapUsed=8151112, HeapTotal=10485760\n  Iter 15000 \u2192 RSS=113848320, HeapUsed=8157688, HeapTotal=10223616\n  Iter 16000 \u2192 RSS=115011584, HeapUsed=8176280, HeapTotal=10223616\n  Iter 17000 \u2192 RSS=114974720, HeapUsed=8161904, HeapTotal=11272192\n  Iter 18000 \u2192 RSS=115380224, HeapUsed=8172744, HeapTotal=11272192\n  Iter 19000 \u2192 RSS=116219904, HeapUsed=8186208, HeapTotal=11272192\n  Iter 20000 \u2192 RSS=116875264, HeapUsed=8189800, HeapTotal=11272192\n  Iter 21000 \u2192 RSS=118026240, HeapUsed=8156920, HeapTotal=11272192\n  Iter 22000 \u2192 RSS=119205888, HeapUsed=8142616, HeapTotal=11272192\n  Iter 23000 \u2192 RSS=119529472, HeapUsed=8184704, HeapTotal=11534336\n  Iter 24000 \u2192 RSS=120926208, HeapUsed=8158200, HeapTotal=11272192\n  Iter 25000 \u2192 RSS=120991744, HeapUsed=8157664, HeapTotal=11272192\n  Iter 26000 \u2192 RSS=122769408, HeapUsed=8175536, HeapTotal=10223616\n  Iter 27000 \u2192 RSS=122974208, HeapUsed=8153464, HeapTotal=11272192\n  Iter 28000 \u2192 RSS=124030976, HeapUsed=8173424, HeapTotal=10223616\n  Iter 29000 \u2192 RSS=124108800, HeapUsed=8180232, HeapTotal=11534336\n  ```\n</details>\n\n\nCould you maybe suggest some values for the memory options, that I could use to see quickly if the Node.js process is behaving as it should? It is a very simple Node.js script that shouldn't need much memory, but I am not sure whether the values, that I am using, are safe.\n\n---\n\nThe memory leak has been resolved (at least for us) by updating to Node.js v24.12.0. I hope it was the same issue originally reported by @JMPSequeira ",
      "labels": [],
      "created_at": "2025-05-18T22:53:37Z",
      "closed_at": "2026-01-13T19:15:51Z",
      "url": "https://github.com/nodejs/node/issues/58380",
      "comments_count": 15
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 44751,
      "title": "Unhandled TLSSocket error - internal Node error",
      "problem": "### Version\n\n18.9.0, 17.1.0\n\n### Platform\n\nBodhi Linux (Ubuntu like) - VM guest on Windows 10 host\n\n### Subsystem\n\n_No response_\n\n### What steps will reproduce the bug?\n\nI am running my Node based peer-to-peer application in test automation mode that creates persistent sockets as TLS websockets.  One the first test automation run, initiated from the windows host machine, everything is fine.  The application instance on the VMs continues to run so that I can frequently run test automation instances without ever touching the VMs.  When I attempt to run test automation a second time only 1 of my 4 VM application instances fails.\r\n\r\nThis appears to be a node defect.  The error message mentions an unhandled error event but I have extensive error handling on just about everything in my application, most especially my socket management.  The stack trace also does not indicate any code from my application.\n\n### How often does it reproduce? Is there a required condition?\n\n100% reproducible.  I am running 4 virtual machines each with a nearly identical install.  This problem only occurs on one of those 4 VMs.\n\n### What is the expected behavior?\n\nSocket not crashing.\n\n### What do you see instead?\n\n```\r\nnode:events:491\r\n      throw er; // Unhandled 'error' event\r\n\r\nError: read ECONNRESET\r\n    at TLSWrap.onStreamRead (node:internal/stream_base_commons:217:20)\r\nEmitted 'error' event on TLSSocket instance at:\r\n    at emitErrorNT (node:internal/streams/destroy:151:8)\r\n    at emitErrorCloseNT (node:internal/streams/destroy:116:3)\r\n    at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {\r\n  errno: -104,\r\n  code: 'ECONNRESET',\r\n  syscall: 'read'\r\n}\r\n\r\nNode.js v18.9.0\r\n```\n\n### Additional information\n\n_No response_",
      "solution": "Do you have a minimal test case that reproduces the issue?\n\n---\n\nI am pinpointed that the error occurs when I close the application on the primary computer, and thus close/destroy the socket on the primary computer.  Such an action expectedly sends a close message to the remote end and the remote end is generating an error on either its TLSSocket close, end, or destroy events and terminating the application with an unhandled error message.\r\n\r\nhttps://github.com/nodejs/node/blob/main/lib/internal/stream_base_commons.js indicates the problematic error occurs at the `destroy` event.\n\n---\n\nI believe the problem was in my own code.  I am struggling to remember from last year.  I now have the same error handler for all sockets whether client or server side.  I cannot remember if that was sufficient to solve for this.\r\n\r\nI believe the problem was Node writing an unhandled error messaging to stderr even when I had the proper error handling in the place in my application, because the unhandled error occurred only deep in a Node library.  I no longer see this behavior now, so I changed something in my own code to prevent the error state from occurring within Node's deeper library, but I cannot remember what it is.\r\n\r\nI will review my commit history later today to see if I can identify the change.",
      "labels": [
        "tls",
        "net"
      ],
      "created_at": "2022-09-22T22:53:23Z",
      "closed_at": "2026-01-13T16:03:18Z",
      "url": "https://github.com/nodejs/node/issues/44751",
      "comments_count": 6
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 52697,
      "title": "Tracking issue: require(esm)",
      "problem": "Before it's unflagged\n\n- [x] Figure out default export interop with transpilers, either adding `__esModule` to required ESM on our end (https://github.com/nodejs/node/pull/52166), or transpilers update themselves to check the result returned by `require()`: \n- [x] conditional exports for module, regardless of whether it's loaded by `require` or `import`. Something like `module` which is recognized by Webpack and Rollup would be good (maybe this doesn't need to block unflagging, but should be done before stablization) https://github.com/nodejs/node/pull/54648\n- [x] Move experimental warning to where `require()` is actually handling a ESM\n\nBefore it is promoted to be stable:\n\n- [x] Implement detection https://github.com/nodejs/node/pull/52047\n- [x] Some marker to customize the default exports returned by require(esm) https://github.com/nodejs/node/pull/54563\n- [x] Support module customization hooks https://github.com/nodejs/loaders/pull/198\n\nNice-to-haves:\n\n- [x] https://github.com/nodejs/node/issues/52180\n- [x] https://github.com/nodejs/node/issues/52599 (fixed in https://github.com/nodejs/node/pull/52762)\n- [x] Better documentation on how to publish packages in a post-require-esm era/migration guide: https://github.com/nodejs/package-examples\n\nBug fixes & changes:\n\n- https://github.com/nodejs/node/pull/52264\n- https://github.com/nodejs/node/pull/52487\n- https://github.com/nodejs/node/pull/52047\n- https://github.com/nodejs/node/pull/52868\n- https://github.com/nodejs/node/pull/52166\n- https://github.com/nodejs/node/pull/54592\n- https://github.com/nodejs/node/pull/54648\n- https://github.com/nodejs/node/pull/55250\n- https://github.com/nodejs/node/pull/55397\n- https://github.com/nodejs/node/pull/55496\n- https://github.com/nodejs/node/pull/55502\n- https://github.com/nodejs/node/pull/55520\n- https://github.com/nodejs/node/pull/55960\n- https://github.com/nodejs/node/pull/56122\n- https://github.com/nodejs/node/pull/56194\n\nRelated features that interoperate with require(esm) and need to be considered when being backported together:\n\n- https://github.com/nodejs/node/pull/53619\n- https://github.com/nodejs/node/pull/53725\n\nv22.x backport (see a summary of regression analysis in https://github.com/nodejs/node/pull/55217#issuecomment-2388704035)\n- https://github.com/nodejs/node/pull/55241\n- https://github.com/nodejs/node/pull/55243\n- https://github.com/nodejs/node/pull/55286\n- https://github.com/nodejs/node/pull/55217\n\nv20.x backport: https://github.com/nodejs/node/pull/56927",
      "solution": "Started working on backport to v20.x and from a quick git diff voodoo it seems there are about 119+ commits to backport (probably not all of them, still need to triage) and it needs to start with reviving https://github.com/nodejs/node/pull/53502 or otherwise there would be too many conflicts along the way.\n\n<details>\n<summary>See diff</summary>\n\n```\nesm: fix jsdoc type refs to `ModuleJobBase` in esm/loader       |       module: merge config with `package_json_reader`\nsrc: use v8::LocalVector consistently with other minor cl       |       src: move package resolver to c++\nmodule: fix async resolution error within the sync `findP       <\nmodule: support eval with ts syntax detection\t\t\t<\nmodule: use buffer.toString base64\t\t\t\t<\nesm: add experimental support for addon modules\t\t\t<\nmodule: add prefix-only modules to `module.builtinModules       <\nmodule: only emit require(esm) warning under --trace-requ       <\nmodule: prevent main thread exiting before esm worker end       <\nmodule: use synchronous hooks for preparsing in import(cj       <\nmodule: implement module.registerHooks()\t\t\t<\nmodule: mark evaluation rejection in require(esm) as hand       <\nmodule: remove --experimental-default-type\t\t\t<\nmodule: do not warn when require(esm) comes from node_mod       <\nlib: remove unused file `fetch_module`\t\t\t\t<\nmodule: tidy code string concat \u2192 string templates\t\t<\npermission: ignore internalModuleStat on module loading\t\t<\nmodule: simplify --inspect-brk handling\t\t\t\t<\nmodule: simplify `findPackageJSON` implementation\t\t<\nmodule: unify TypeScript and .mjs handling in CommonJS\t\t<\nmodule: fix error thrown from require(esm) hitting TLA re       <\nmodule: trim off internal stack frames for require(esm) w       <\nmodule: allow ESM that failed to be required to be re-imp       <\nmodule: add `findPackageJSON` util\t\t\t\t<\nmodule: add module.stripTypeScriptTypes\t\t\t\t<\nmodule: include module information in require(esm) warnin       <\nesm: add a fallback when importer in not a file\t\t\t<\nRevert \"path: fix bugs and inconsistencies\"\t\t\t<\nmodule: simplify ts under node_modules check\t\t\t<\nlib: remove startsWith/endsWith primordials for char chec       <\nesm: fix inconsistency with `importAssertion` in `resolve       <\nesm: mark import attributes and JSON module as stable\t\t<\nmodule: throw ERR_NO_TYPESCRIPT when compiled without ama       <\nmodule: wrap swc error in ERR_INVALID_TYPESCRIPT_SYNTAX\t\t<\nlib: prefer logical assignment\t\t\t\t\t<\nmodule: check --experimental-require-module separately fr       <\nmodule: use kNodeModulesRE to detect node_modules\t\t<\nesm: export 'module.exports' on ESM CJS wrapper\t\t\t<\nmodule: support 'module.exports' interop export in requir       <\nsrc: modernize likely/unlikely hints\t\t\t\t<\nsrc: add receiver to fast api callback methods\t\t\t<\nsrc: fix typos\t\t\t\t\t\t\t<\nesm: do not interpret `\"main\"` as a URL\t\t\t\t<\nmodule: support loading entrypoint as url\t\t\t<\nmodule: unflag --experimental-require-module\t\t\t<\nlib: fix typos\t\t\t\t\t\t\t<\nlib, tools: remove duplicate requires\t\t\t\t<\nmodule: implement the \"module-sync\" exports condition\t\t<\nlib: prefer optional chaining\t\t\t\t\t<\nmodule: implement flushCompileCache()\t\t\t\t<\ntest_runner: support typescript module mocking\t\t\t<\nmodule: report unfinished TLA in ambiguous modules\t\t<\ntools: bump the eslint group in /tools/eslint with 7 upda       <\nmodule: refator ESM loader for adding future synchronous        <\nmodule: remove bogus assertion in CJS entrypoint handling       <\nsrc: use `Maybe<void>` where bool isn't needed\t\t\t<\npath: fix bugs and inconsistencies\t\t\t\t<\nesm: throw `ERR_REQUIRE_ESM` instead of `ERR_INTERNAL_ASS       <\nesm: use Undici/`fetch` `data:` URL parser\t\t\t<\nvm: migrate ContextifyScript to cppgc\t\t\t\t<\nvm: return all own names and symbols in property enumerat       <\nsrc: add JS APIs for compile cache and NODE_DISABLE_COMPI       <\nmodule: use amaro default transform values\t\t\t<\nmodule: fix discrepancy between .ts and .js\t\t\t<\nmodule: add sourceURL magic comment hinting generated sou       <\nmodule: add --experimental-transform-types flag\t\t\t<\nmodule: do not attempt to strip type when there's no sour       <\nmodule: refactor ts parser loading\t\t\t\t<\nmodule: fix strip-types interaction with detect-module\t\t<\nmodule: remove outdated comment\t\t\t\t\t<\nmodule: fix extensionless typescript in cjs loader\t\t<\nmodule: do not warn for typeless package.json when there        <\nlib,src: drop --experimental-network-imports\t\t\t<\nmodule: add --experimental-strip-types\t\t\t\t<\nmodule: unflag detect-module\t\t\t\t\t<\nesm: refactor `get_format`\t\t\t\t\t<\nlib: refactor `platform` utility methods\t\t\t<\nlib,esm: handle bypass network-import via data:\t\t\t<\nmodule: add __esModule to require()'d ESM\t\t\t<\nsrc: fix implementation of `PropertySetterCallback`\t\t<\nvm,src: add property query interceptors\t\t\t\t<\nsrc: remove unused ContextifyContext::WeakCallback\t\t<\nesm: improve `defaultResolve` performance\t\t\t<\nsrc: refactor embedded entrypoint loading\t\t\t<\nesm: remove unnecessary toNamespacedPath calls\t\t\t<\nlib: move `ToNamespacedPath` call to c++\t\t\t<\nlib: add diagnostics_channel events to module loading\t\t<\nfs: add fast api for `InternalModuleStat`\t\t\t|       module: warn on detection in typeless package\nsrc: simplify `size() == 0` checks\t\t\t\t<\nlib: reduce amount of caught URL errors\t\t\t\t<\nsrc: fix permission inspector crash\t\t\t\t<\nRevert \"vm,src: add property query interceptors\"\t\t<\nvm,src: add property query interceptors\t\t\t\t<\nRevert \"module: have a single hooks thread for all worker       <\nmodule: print amount of load time of a cjs module\t\t<\nlib: allow CJS source map cache to be reclaimed\t\t\t<\nprocess: add process.getBuiltinModule(id)\t\t\t<\nmodule: do not set CJS variables for Worker eval\t\t<\nsrc: remove calls to recently deprecated V8 APIs\t\t<\nsrc: replace deprecated GetImportAssertions V8 API\t\t<\nlib,doc: replace references to import assertions\t\t<\nmodule: cache synchronous module jobs before linking\t\t<\nmodule: have a single hooks thread for all workers\t\t<\nlib,src: remove --experimental-policy\t\t\t\t<\nsrc: migrate to new V8 interceptors API\t\t\t\t<\nmodule: support ESM detection in the CJS loader\t\t\t<\nlib,src: iterate module requests of a module wrap in JS\t\t<\ntypings: fix invalid JSDoc declarations\t\t\t\t<\ntools: add lint rule to keep primordials in ASCII order\t\t<\nmodule: detect ESM syntax by trying to recompile as Sourc       <\nmodule: implement NODE_COMPILE_CACHE for automatic on-dis       <\ndns: add order option and support ipv6first\t\t\t<\nmodule: centralize SourceTextModule compilation for built       |       module: centralize SourceTextModule compilation for built\nsrc: use supported API to get stalled TLA messages\t\t<\nsrc: add missing TryCatch\t\t\t\t\t<\nsrc: address coverity warning in module_wrap.cc\t\t\t<\nmodule: warn on detection in typeless package\t\t\t<\nsrc: fix move after use reported by coverity\t\t\t<\nmodule: eliminate performance cost of detection for cjs e       <\n```\n    \n</details>\n\n---\n\nOf course we'd like to eventually migrate away from deprecated and unofficial APIs. Unfortunately we currently support all the way down to Node 18 so `module.register()` and `module.registerHooks()` are no-go at least until our next major. In particular, `module.registerHooks()` was added in Node 23.5 so that can't be used for users that are on lower versions of Node.\n\nAs far as my tests go, the CJS monkey-patch is bypassed for `require(esm)` + static imports only when `require(esm)` is enabled and only before 23.5.0 (when `module.registerHooks()` was added). So probably implementing `module.registerHooks()` makes `require(esm)` go through the CJS pipeline once again?\n\nMore importantly, are there known workarounds in Node <23.5? The only one I can find is to disable `require(esm)` entirely. We need to know if there are known workarounds to determine how to proceed with bug reports regarding this.\n\n---\n\nI think in the case where you need to support older Node.js versions, a safer approach would be:\n\n1. When the code is run in a Node.js process where `module.registerHooks()` exists, register your hooks with it (this should be preferably done by checking the existence of  `module.registerHooks`, not by version check, because Node.js features are not released linearly - `module.registerHooks()` will probably be backported to v22 soon, it was just baking in v23 per the LTS policy), and there is no need to use CJS monkey patching/`module.register()` when you use `module.registerHooks()`\n2. If the code is run in a Node.js process where `module.registerHooks()` does not exist but `module.register()` exists, you can use CJS monkey patching + `module.register()` for a best-effort coverage. In that case you can disable `require(esm)` until somehow someone fixes `require(esm)` customization in either of those options (seems less likely)\n\n`module.registerHooks()` is designed in a way that the code can be shared with CJS monkey patching and `module.register()` (as long as you sync-ify the functions, which usually comes down to using `fooSync()` instead of `foo()` in certain places), so it would probably be the same customization code registered somewhat differently based on the availablity of the registration methods.\n\n> More importantly, are there known workarounds in Node <23.5?\n\nI don't think there are for 23.0-23.5. Though `module.registerHooks()` will probably be backported to v22 soon.\n\n(FYI this is a tracking issue, which is supposed to be linked by other issues that ask these specific questions, instead of housing these questions directly. Please open another issue if you have further questions).",
      "labels": [
        "module",
        "esm"
      ],
      "created_at": "2024-04-25T21:25:06Z",
      "closed_at": "2026-01-12T18:39:02Z",
      "url": "https://github.com/nodejs/node/issues/52697",
      "comments_count": 30
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 60941,
      "title": "Node.js installer not working in Windows Sandbox",
      "problem": "### Version\n\n_No response_\n\n### Platform\n\n```text\nMicrosoft Windows NT 10.0.26100.0 x64\n```\n\n### Subsystem\n\n_No response_\n\n### What steps will reproduce the bug?\n\n- Launch Windows Sandbox\n- Run `node-v24.11.1-x64.msi`\n\n### How often does it reproduce? Is there a required condition?\n\nAlways\n\n### What is the expected behavior? Why is that the expected behavior?\n\nInstaller to launch\n\n### What do you see instead?\n\n> There is a problem with this Windows Installer package. DLL required for this install to complete could not be run. Contact your support personnel or package vendor.\n\n<img width=\"2028\" height=\"1270\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/212affc7-aa32-4466-9fef-50292857cecc\" />\n\nThe installer does not launch at all. It's not a problem about disabling certain features:\n\n```\n\uc5d0\ub514\uc158: Windows 11 Pro\n\ubc84\uc804: 25H2\n\uc124\uce58 \ub0a0\uc9dc: \u200e2025-\u200e06-\u200e26\nOS \ube4c\ub4dc: 26200.7171\n\uc77c\ub828 \ubc88\ud638: 5CG519385X\n\uacbd\ud5d8: Windows \uae30\ub2a5 \uacbd\ud5d8 \ud329 1000.26100.265.0\n```\n\n### Additional information\n\nIt would be nice if the installation was supported, since the Sandbox is a great way to create a clean install and reproduction for Windows related bugs.\n\nYes the standalone binary works, but with workarounds\n\n- https://github.com/nodejs/node/issues/12311#issuecomment-293046255\n\n```powershell\n./node # works\n./npx # does not work\n\n# File C:\\Users\\WDAGUtilityAccount\\Desktop\\node-v24.11.1-win-x64\\npx.ps1 cannot be loaded because running scripts is disabled on this system.\n\n# Use with caution\nSet-ExecutionPolicy -ExecutionPolicy Unrestricted\n```\n\n- fnm installation is not supported due to lack of winget\n- pnpm standalone installation does not work because the file is blocked:\n\n> Start-Process : This command cannot be run due to the error: \uc560\ud50c\ub9ac\ucf00\uc774\uc158 \uc81c\uc5b4 \uc815\ucc45\uc5d0\uc11c \uc774 \ud30c\uc77c\uc744 \ucc28\ub2e8\ud588\uc2b5\ub2c8\ub2e4.",
      "solution": "Thanks for reviewing.\n\nYes it is a setup issue, should I have used a different issue template?\n\nThere seems to be other installer issues in this repository:\n\n- https://github.com/nodejs/node/issues/57908\n\n---\n\nNo the GUI installer does not provide any workaround. It just complains about missing DLL.\n\nThe 'Additional information' is workarounds I've looked into, including the `Set-ExecutionPolicy` command.\n\n---\n\nBefore going into details, let me just say that I've never used Windows Sandbox before.\n\nWith that being said, I do suspect it's the root cause of this issue. The reason is that inside it, to the best of my knowledge, some features are dropped, so probably there is an issue with running some custom action or something else during the installation.\n\nA potential workaround I have in mind is not to download **Windows Installer (.msi)**, but rather get a **Standalone Binary (.zip)**. That way, you skip the installation, but can still use Node once you unzip it.\n\n---\n\nI had never used [Windows Sandbox](https://learn.microsoft.com/en-us/windows/security/application-security/application-isolation/windows-sandbox/windows-sandbox-install) before, however I did install it in Windows 11 Pro 25H2 and successfully installed the Node.js Active LTS msi (https://nodejs.org/dist/v24.11.1/node-v24.11.1-x64.msi) in it.\n\nThe original error message seems to point to the issue:\n\n> File C:\\Users\\WDAGUtilityAccount\\Desktop\\node-v24.11.1-win-x64\\npx.ps1 cannot be loaded because running scripts is disabled on this system.\n\n\n",
      "labels": [
        "windows",
        "install"
      ],
      "created_at": "2025-12-03T03:52:19Z",
      "closed_at": "2026-01-12T09:42:55Z",
      "url": "https://github.com/nodejs/node/issues/60941",
      "comments_count": 6
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 61159,
      "title": "Sudden Error [ERR_SERVER_NOT_RUNNING]: Server is not running on angular builds",
      "problem": "### Version\n\nv22.21.1\n\n### Platform\n\n```text\n* oraclelinux:9 container on kubernetes\n* ubuntu 24.4 LTS\n```\n\n### Subsystem\n\nnet\n\n### What steps will reproduce the bug?\n\nI actually do not know. It is an angular 17 build\n\n### How often does it reproduce? Is there a required condition?\n\nevery time the build runs\n\n### What is the expected behavior? Why is that the expected behavior?\n\nthe socket probably should just close or it should not close an not open socket\n\n### What do you see instead?\n\n```\nChrome Headless 143.0.0.0 (Linux 0.0.0): Executed 27 of 27 SUCCESS (0.088 secs / 0.07 secs)\nTOTAL: 27 SUCCESS\n\u2714 Browser application bundle generation complete.\n\u2714 Browser application bundle generation complete.\nnode:net:2359\n        cb(new ERR_SERVER_NOT_RUNNING());\n           ^\nError [ERR_SERVER_NOT_RUNNING]: Server is not running.\n    at Server.close (node:net:2359:12)\n    at Object.onceWrapper (node:events:633:28)\n    at Server.emit (node:events:531:35)\n    at emitCloseNT (node:net:2419:8)\n    at process.processTicksAndRejections (node:internal/process/task_queues:89:21) {\n  code: 'ERR_SERVER_NOT_RUNNING'\n}\n```\n\n### Additional information\n\nThe tests are run using karma and for some reason, this stack trace does not contain any library code, only node internal classes so I suspect that it is not related to any of the libraries and the build did work before (I cannot guarantee that there is no new library version but the stack trace does not show usage of any libs anyway).\n\nthe same happens on v24.0, which I also tried",
      "solution": "@MikeMcC399 the thing is if you look at the stack trace, you do not see any library / utility at all. It looks like something inside node might be cleaning up resources and then runs in the problem. If there would be an angular specific callback, I would expect something like node_modules/@angular/\u2026 in the stack trace.\n\n---\n\n@foto-andreas \n\nAre you suggesting that the problem you've described is a bug in Node.js? If so, do you have a link available to a publicly accessible repo that demonstrates the issue?\n\n---\n\nThe original issue from @fabianfrz also concerned Karma, so since this is itself now unsupported (see above \u2b06\ufe0f) probably the issue should be closed. ",
      "labels": [],
      "created_at": "2025-12-23T15:19:20Z",
      "closed_at": "2026-01-11T22:13:39Z",
      "url": "https://github.com/nodejs/node/issues/61159",
      "comments_count": 15
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 52923,
      "title": "Timeline for V8 fast-calls header exposed for native addons?",
      "problem": "### What is the problem this feature will solve?\r\n\r\nFast calls in V8 was introduced 4 years ago, soon to be half a decade ago. These features are still not exposed to native addon authors due to \"it may change\"-argumentation (see previous threads).\r\n\r\nWhen can these features be freely used by the ecosystem as they see fit? Are we supposed to wait half a decade more? There clearly are many useful cases where it dramatically helps performance.\r\n\r\nWhy not just let the ecosystem adopt them as they see usable? We already have gone through 10+ years of V8 breaking API changes from version to version so none of this is news to any of us making native addons - V8 has never been stable in its API, not even the early wrappers were ever stable.\r\n\r\nIt always changed over time, so to treat fast calls differently than any other potential API change makes no sense. Esp. when it holds back performance innovation that technically already is available.\r\n\r\n### What is the feature you are proposing to solve the problem?\r\n\r\nTo include the v8-fast-api-calls.h so that native addons can use them.\r\n\r\n### What alternatives have you considered?\r\n\r\nHacking it in myself, on my own end",
      "solution": "There has been no activity on this feature request for 5 months. To help maintain relevant open issues, please add the https://github.com/nodejs/node/labels/never-stale label or close this issue if it should be closed. If not, the issue will be automatically closed 6 months after the last non-automated comment.\nFor more information on how the project manages feature requests, please consult the [feature request management document](https://github.com/nodejs/node/blob/HEAD/doc/contributing/feature-request-management.md).\n\n---\n\nThere has been no activity on this feature request for 5 months. To help maintain relevant open issues, please add the https://github.com/nodejs/node/labels/never-stale label or close this issue if it should be closed. If not, the issue will be automatically closed 6 months after the last non-automated comment.\nFor more information on how the project manages feature requests, please consult the [feature request management document](https://github.com/nodejs/node/blob/HEAD/doc/contributing/feature-request-management.md).\n\n---\n\nThere has been no activity on this feature request for 5 months. To help maintain relevant open issues, please add the https://github.com/nodejs/node/labels/never-stale label or close this issue if it should be closed. If not, the issue will be automatically closed 6 months after the last non-automated comment.\nFor more information on how the project manages feature requests, please consult the [feature request management document](https://github.com/nodejs/node/blob/HEAD/doc/contributing/feature-request-management.md).",
      "labels": [
        "feature request",
        "stale"
      ],
      "created_at": "2024-05-09T17:44:40Z",
      "closed_at": "2026-01-09T01:30:04Z",
      "url": "https://github.com/nodejs/node/issues/52923",
      "comments_count": 16
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 61290,
      "title": "Node API: `napi_is_exception_pending` returned true after successfully calling `napi_get_and_clear_last_exception`",
      "problem": "### Version\n\nv24.12.0\n\n### Platform\n\n```text\nLinux xubuntu 6.12.63-0 #1 SMP PREEMPT_DYNAMIC Wed Dec 31 19:18:53 GMT 2025 x86_64 GNU/Linux\n```\n\n### Subsystem\n\n_No response_\n\n### What steps will reproduce the bug?\n\n1. Create a c++ addon using Node API with the following code and export the `test` function to JS code:\n```c++\n#define check_status(e) (__builtin_expect((e) == napi_status::napi_ok, 1) ? void(0) : __builtin_trap())\n\nint test_op(napi_env env, napi_value cb) noexcept {\n\t// some native operations\n\t// ...\n\n\t// invoke js callback\n\tint status;\n\n\t{\n\t\tnapi_value js_undefined;\n\t\tcheck_status(napi_get_undefined(env, &js_undefined));\n\n\t\t// unchecked for js exceptions\n\t\tstatus = napi_call_function(env, js_undefined, cb, 0, nullptr, nullptr);\n\t}\n\n\t// some cleanup tasks\n\t// ...\n\n\treturn status;\n}\n\nnapi_value test(napi_env env, napi_callback_info info) noexcept {\n\t// get function arguments\n\tsize_t argc = 1; napi_value arg0; napi_value object;\n\tcheck_status(napi_get_cb_info(env, info, &argc, &arg0, &object, nullptr));\n\n\t// arguments type check\n\tnapi_valuetype type;\n\tcheck_status(napi_typeof(env, arg0, &type));\n\n\tif (type != napi_valuetype::napi_function) {\n\t\t// throw invalid arguments\n\t\treturn nullptr;\n\t}\n\n\t// invoke native functions and js callback\n\tint code = test_op(env, arg0);\n\n\t// check for errors\n\tif (code != 0) {\n\t\tnapi_value err;\n\t\tcheck_status(napi_get_and_clear_last_exception(env, &err));\n\n\t\tif (napi_coerce_to_string(env, err, &err) == napi_status::napi_ok) {\n\t\t\t// print verbose error message based on string output\n\t\t} else {\n\t\t\t// print generic error message\n\t\t}\n\n\t\tbool z;\n\t\tcheck_status(napi_is_exception_pending(env, &z));\n\t\t_assert(!z); // <-- Assertion error occurs here\n\t}\n\n\treturn js_undefined(env);\n}\n```\n2. Run the following JS code:\n```js\nconst mod = require(\"./module.node\");\n\nmod.test(() => {\n  throw { __proto__: null }\n});\n```\n\n### How often does it reproduce? Is there a required condition?\n\nThis error always occurs and has been tested with different versions of Node.js. However this only happens for errors thrown in JS callback functions that are not instance of the `Error` class. It does not happen for errors triggered on the native side with `napi_throw_error`.\n\n### What is the expected behavior? Why is that the expected behavior?\n\nThe boolean value returned by `napi_is_exception_pending` should be `false` if `napi_get_and_clear_last_exception` returned a `napi_ok` status code, and the function should return without triggering an assertion error.\n\n### What do you see instead?\n\nAn assertion error is triggered at the line `_assert(!z);` and the program terminates.\n\n### Additional information\n\n_No response_",
      "solution": "hey thanks for opened issue, i looked into this and the behavior is actually correct.\n\n  the issue is that napi_coerce_to_string can throw a new exception when it fails. since { proto: null } has no toString method, calling napi_coerce_to_string on it throws a TypeError.\n\n  so whats happening:\n  napi_get_and_clear_last_exception clears the original exception (works fine)\n  napi_coerce_to_string then throws a NEW exception\n  this new exception stays pending\n\n  i verified this locally:\n  after clear: exception pending = false\n  after coerce_to_string: status = 3 (pending_exception), exception pending = true\n\n  you can test in plain js: String({ proto: null }) throws TypeError\n\n  fix:\n```cpp\n  } else {\n      napi_value ignored;\n      napi_get_and_clear_last_exception(env, &ignored);\n      // print generic error message\n  }\n```\ncc @nodejs/node-api - wanted to consult in case there are any gaps in my analysis",
      "labels": [
        "node-api"
      ],
      "created_at": "2026-01-05T22:09:34Z",
      "closed_at": "2026-01-07T22:47:56Z",
      "url": "https://github.com/nodejs/node/issues/61290",
      "comments_count": 1
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 61078,
      "title": "`--disallow-code-generation-from-strings` breaks named import of re-exported CommonJS export",
      "problem": "### Version\n\nv25.2.1\n\n### Platform\n\n```text\nLinux [...] 6.12.62-1-MANJARO #1 SMP PREEMPT_DYNAMIC Fri, 12 Dec 2025 19:00:00 +0000 x86_64 GNU/Linux\n```\n\n### Subsystem\n\n_No response_\n\n### What steps will reproduce the bug?\n\nMinimal example with three files, with `a.mjs` being entrypoint:\n\n`a.mjs`:\n```js\nimport { Hello } from \"./b.cjs\";\n\nHello();\n```\n\n`b.cjs`:\n```js\nmodule.exports = require(\"./c.cjs\");\n```\n\n`c.cjs`:\n```js\nfunction Hello() {\n    console.log(\"Hello, World!\");\n}\n\nmodule.exports = { Hello };\n```\n\n\n### How often does it reproduce? Is there a required condition?\n\nWith default flags works without issues, but if using `--disallow-code-generation-from-strings` the application breaks.\n\n### What is the expected behavior? Why is that the expected behavior?\n\nThe application should work as intended, as it doesn't use `eval` nor `new Function`:\n```\n$ node a.mjs\n```\n```\nHello, World!\n```\n\n### What do you see instead?\n\nThe application fails to run:\n```\n$ node --disallow-code-generation-from-strings a.mjs\n```\n```\nfile://[...]/a.mjs:1\nimport { Hello } from \"./b.cjs\";\n         ^^^^^\nSyntaxError: Named export 'Hello' not found. The requested module './b.cjs' is a CommonJS module, which may not support all module.exports as named exports.\nCommonJS modules can always be imported via the default export, for example using:\n\nimport pkg from './b.cjs';\nconst { Hello } = pkg;\n\n    at #asyncInstantiate (node:internal/modules/esm/module_job:302:21)\n    at async ModuleJob.run (node:internal/modules/esm/module_job:405:5)\n    at async onImport.tracePromise.__proto__ (node:internal/modules/esm/loader:654:26)\n    at async asyncRunEntryPointWithESMLoader (node:internal/modules/run_main:101:5)\n\nNode.js v25.2.1\n```\n\n### Additional information\n\nNote, that the above example is a simplification of real-world usecase, where `a.mjs` comes from an ESM application that imports `b.cjs` CommonJS library, which uses re-exports internally.\nMost notable case I stumbled upon of this happening is while using [express](https://www.npmjs.com/package/express).\n\nThere is a workaround for the application:\n`a_workaround.mjs`:\n```js\nimport b from \"./b.cjs\";\nconst { Hello } = b;\n\nHello();\n```\nBut this workaround is not always applicable (e.g. if using an ESM dependency outside our control that imports such CommonJS dependency itself) and from the meaning `--disallow-code-generation-from-strings` it is not clear why it should be needed. Even if the workaround is possible, the bug hampers usage of this hardening flag, even if the application nor the libraries make use of eval-like features.",
      "solution": "I was hoping that this would be fixed by https://github.com/nodejs/node/pull/61271, but trying on [the latest nightly](https://nodejs.org/download/nightly/v26.0.0-nightly20260106d050aa87e8/) it does not seem to be.\n\nEdit: though actually when I build it locally it does work? So, maybe I'm just confused.\n\n---\n\n@bakkot I just tested this on the latest main today and the test case fails in release Node and passes in main Node now in my testing. Obviously nice to actually add the test case of course, but I can confirm it resolved the issue.",
      "labels": [],
      "created_at": "2025-12-15T23:25:51Z",
      "closed_at": "2026-01-06T01:14:40Z",
      "url": "https://github.com/nodejs/node/issues/61078",
      "comments_count": 4
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 59740,
      "title": "Segfault on Node 22.19.0 (most likely issue with OpenSSL 3.0.17)",
      "problem": "### Version\n\nv22.19.0\n\n### Platform\n\n```text\nLinux remnawave 5.15.0-139-generic #149-Ubuntu SMP Fri Apr 11 22:06:13 UTC 2025 x86_64 Linux\n```\n\n### Subsystem\n\nOfficial alpine linux image, node:22-alpine\n\n### What steps will reproduce the bug?\n\nThis problem is quite difficult to reproduce, it appears rather spontaneously and I unfortunately cannot trigger it deliberately.\nBelow I'm attaching the stacktrace (made with [node-segfault-handler](https://github.com/Shiranuit/node-segfault-handler)).\n\nThe problem started manifesting exclusively on version 22.19.0. On version 22.18.0, this problem was not observed.\n\n```\n=========== Caught a Segmentation Fault [pid=1570] ===========\n-----[ Native Stacktraces ]-----\n[pc=0x00007f27e28d87ce, sp=0x00007f255e922c60] in segfault_handler(int)+0x4e\n[pc=0x00007f27e599d5a4, sp=0x00007f255e922c80] in +0x4e\n[pc=0x000055e05741573c, sp=0x00007f255e923a60] in OSSL_STORE_load+0x11c\n[pc=0x000055e057425acb, sp=0x00007f255e923ab0] in by_store_subject+0xcb\n[pc=0x000055e057444f6d, sp=0x00007f255e923b60] in X509_STORE_CTX_get_by_subject+0x15d\n[pc=0x000055e057445ea3, sp=0x00007f255e923be0] in X509_STORE_CTX_get1_issuer+0x83\n[pc=0x000055e05744a821, sp=0x00007f255e923c50] in build_chain+0x1c1\n[pc=0x000055e05744c848, sp=0x00007f255e923ce0] in verify_chain+0x28\n[pc=0x000055e05744db33, sp=0x00007f255e923d50] in X509_verify_cert+0xc3\n[pc=0x000055e057219ee0, sp=0x00007f255e923d80] in ssl_verify_cert_chain+0x290\n[pc=0x000055e05725e5a2, sp=0x00007f255e923db0] in tls_post_process_server_certificate+0x42\n[pc=0x000055e05725a415, sp=0x00007f255e923df0] in state_machine+0x775\n[pc=0x00007f255f48bc3c, sp=0x00007f255e923eb0] in +0x775\n[pc=0x00007f255f4868a8, sp=0x00007f255e9241b0] in +0x775\n[pc=0x00007f255f48501f, sp=0x00007f255e928720] in +0x775\n[pc=0x00007f255f483e8d, sp=0x00007f255e929300] in +0x775\n[pc=0x00007f255f55b85f, sp=0x00007f255e929eb0] in +0x775\n[pc=0x00007f255f55362b, sp=0x00007f255e92c610] in +0x775\n[pc=0x00007f255f8654f1, sp=0x00007f255e933650] in +0x775\n[pc=0x00007f255f8619cb, sp=0x00007f255e936500] in +0x775\n[pc=0x00007f255f86188f, sp=0x00007f255e9366e0] in +0x775\n[pc=0x00007f255f884cdc, sp=0x00007f255e936830] in +0x775\n[pc=0x00007f255f8b3632, sp=0x00007f255e936970] in +0x775\n[pc=0x00007f255f8b1bb7, sp=0x00007f255e937050] in +0x775\n[pc=0x00007f255f8b5a58, sp=0x00007f255e937ba0] in +0x775\n[pc=0x00007f255ef51cbe, sp=0x00007f255e938260] in +0x775\n[pc=0x00007f255ef1b740, sp=0x00007f255e9383f0] in +0x775\n[pc=0x00007f255ef17dfe, sp=0x00007f255e938680] in +0x775\n[pc=0x00007f255ef55a55, sp=0x00007f255e938b30] in +0x775\n[pc=0x00007f255ef397bb, sp=0x00007f255e939000] in +0x775\n[pc=0x00007f255ef47276, sp=0x00007f255e939110] in +0x775\n[pc=0x00007f255f8fd520, sp=0x00007f255e93a1d0] in +0x775\n[pc=0x00007f255f907357, sp=0x00007f255e93a210] in +0x775\n[pc=0x00007f255f8fb8b1, sp=0x00007f255e93a380] in +0x775\n[pc=0x00007f255f8fb321, sp=0x00007f255e93a3a0] in +0x775\n[pc=0x00007f255f9002bd, sp=0x00007f255e93a480] in +0x775\n[pc=0x00007f255f8e2f11, sp=0x00007f255e93a540] in +0x775\n[pc=0x00007f27e59a59d2, sp=0x00007f255e93a570] in +0x775\n---[ V8 JavaScript Stacktraces ]---\n```\n\nSometimes, stacktrace can be empty for some reasons.\n```\n=========== Caught a Segmentation Fault [pid=448] ===========\n-----[ Native Stacktraces ]-----\n[pc=0x00007f98978727ce, sp=0x00007f9613119ce0] in segfault_handler(int)+0x4e\n[pc=0x00007f989a8025a4, sp=0x00007f9613119d00] in +0x4e\n\n---[ V8 JavaScript Stacktraces ]---\n```\n\nApparently, the problem is related to the recent OpenSSL update to version 3.0.17 \u2013 https://github.com/nodejs/node/pull/58097\n\nThe problem is widespread and there are already multiple issues:\n- https://github.com/openssl/openssl/issues/28171\n- https://github.com/prisma/prisma/issues/27785\n- https://github.com/confluentinc/librdkafka/issues/5159\n- https://github.com/karafka/rdkafka-ruby/issues/667\n- https://github.com/oven-sh/bun/issues/21515\n\nI also checked the OpenSSL versions with the command `node -p \"process.versions.openssl\"` inside an Alpine container:\nNode 22.18.0 - 3.0.16\nNode 22.19.0 - 3.0.17\n\nAccordingly, I also tried rolling back to Node version 22.18.0 and the problem did not recur, from which I can conclude that it is most likely related to the OpenSSL version.\n\n### How often does it reproduce? Is there a required condition?\n\nDepends of load.\n\n### What is the expected behavior? Why is that the expected behavior?\n\n-\n\n### What do you see instead?\n\n-\n\n### Additional information\n\n_No response_",
      "solution": "I had a similar problem recently. I observed high usage on my node app (using node:22-alpine and v22.19.0). Sometimes, the app would not start without any error or exit. \n\nI struggled for a few days trying to debug it, and reverting to an older node version helped fix the issue.\n\n---\n\nJust adding a +1 that we were experiencing segfaults reliably enough that we couldn't successfully deploy to k8s. They appeared to originate in some calls from Prisma orm to our postgres db. We didn't troubleshoot further, but downgrading node resolved the issue.",
      "labels": [
        "openssl"
      ],
      "created_at": "2025-09-03T07:09:27Z",
      "closed_at": "2026-01-08T02:09:17Z",
      "url": "https://github.com/nodejs/node/issues/59740",
      "comments_count": 4
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 58676,
      "title": "API surface regression test to catch unintended exports",
      "problem": "### What is the problem this feature will solve?\n\nSometimes we have internal-only methods/constants/classes unintentionally exposed to userland. After being exposed for a long enough time, they inevitably become used by someone, leaving Node.js with a choice: either cause breakage in userland, or keep awkward deprecated/legacy code for years.\n\n### What is the feature you are proposing to solve the problem?\n\n- Fixture with a list of all currently exposed properties (flat array of strings like `fs/promises.readFile`, `dns.lookup.[customPromisifyArgs].1`, etc.)\n- Test that recursively goes through `module.builtinModules`, builds the list, and asserts equality against the fixture\n- Script that would overwrite the fixture with current list using `./out/Release/node ./tools/update-api-surface.mjs`\n\nThis way, the test would fail if something unintended happened, and fixture updates would clearly indicate the additions to public API.\nThis is not 100% robust because it won't include prototypes of some objects created in runtime (for example, `fs.Stats` is accessible while `fs.BigIntStats` is not), but it would catch most of exposures.\n\n### What alternatives have you considered?\n\nTool (e.g. GitHub Actions workflow) that would compare public API surface against base commit and somehow report the changes if any.",
      "solution": "There has been no activity on this feature request for 5 months. To help maintain relevant open issues, please add the https://github.com/nodejs/node/labels/never-stale label or close this issue if it should be closed. If not, the issue will be automatically closed 6 months after the last non-automated comment.\nFor more information on how the project manages feature requests, please consult the [feature request management document](https://github.com/nodejs/node/blob/HEAD/doc/contributing/feature-request-management.md).",
      "labels": [
        "feature request",
        "stale"
      ],
      "created_at": "2025-06-11T13:17:18Z",
      "closed_at": "2026-01-08T01:29:49Z",
      "url": "https://github.com/nodejs/node/issues/58676",
      "comments_count": 2
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 58638,
      "title": "[node-api] Creating a reference-counted env-less JavaScript object that can be shared between threads",
      "problem": "### What is the problem this feature will solve?\n\n# Introduction & Use Case\n\nHi all,\n\nAt Atlassian, we have a build tool written in Nodejs that makes heavy use of worker threads. It follows the best practice conventions for communicating/sharing data between Worker threads however we are running into significant limitations.\n\nFor suitable data, we use manually managed pages of `SharedArrayBuffer`s and serialize JavaScript data into custom data structures, taking a \"copy-on-read\" & \"copy-on-write\" approach when the data is modified. For data that cannot be stored in SABs, we either `postMessage` the whole structure to the worker threads (copying it), then manually synchronize it by sending diffs back to the main thread via `postMessage` which propages those changes to all worker threads - or we store the data on the main thread and use getters/setters that wrap `postMessage`.\n\nHowever, despite our best efforts, the application consumes around 60 - 100gb of ram (~ 10gb per thread, depending on how many threads we use) and we see diminishing performance gains after about 6 worker threads due to the overhead of de/serializing + message overhead.\n\nIn addition, the worker thread glue code is immensely complex and difficult to maintain.\n\nWe have also tried to use an external in-memory db to share data however, the serialisation/deserialisation overhead makes it perform poorly.\n\nWhile JavaScript itself is a fantastic language and performs well, we recognize that the single-threaded nature of the runtime likely makes it unsuitable for our use case (inherited codebase). We are rewriting the tool in Rust; however, due to the complexity of the project, that will likely take 2+ years and in the meantime, we are looking for solutions to alleviate the cost of running the existing project.\n\n# Proposal\n\n_Note: I realize there are inherent limitations with V8 isolates that may prevent this from being possible but I am not an expert in v8 APIs so I lean on the team's expertise to help understand if this is feasible._\n\nIs it possible to add n-api functions that allows the creation of a zero-copy `napi_value` that can be used between `envs`?\n\nPerhaps an approach similar to the reference-counted `threadsafe_function` approach is more feasible:\n\n```c\nNAPI_EXTERN napi_status \nnapi_create_transferrable_object(\n  napi_env env,\n  napi_transferrable_object* result\n)\n\nNAPI_EXTERN napi_status \nnapi_aquire_transferrable_object(\n  napi_env env,                       // Can be obtained from any env\n  napi_transferrable_object* value,\n  napi_transferrable_handle* result,\n)\n\nNAPI_EXTERN napi_status \nnapi_set_key_transferrable_object(\n  napi_env env,\n  napi_transferrable_handle* value,\n  napi_value key,       // structuredClone(key)\n  napi_value value,    // structuredClone(value)\n)\n\nnapi_get_value_transferrable_object(/* ... */)\nnapi_release_transferrable_object(/* ... */)\nnapi_ref_transferrable_object(/* ... */)\nnapi_unref_transferrable_object(/* ... */)\n```\n\n- The host `env` defines a `napi_transferrable_object`, producing a reference-counted handle to the object\n- Any `env` could acquire a`napi_transferrable_object`\n- Only one env can acquire the object at a given time\n- If the object is acquired by an env, acquiring a object in another thread would block until the object is released\n- keys and values are still copied when set/get to/from the transferrable\n- Extra: the object's memory usage does not count towards heap/`max-old-space-size`\n  - Values obtained from the object are cloned into the current env and dealt with by GC as normal  \n\n## How this solves my use case\n\nThe idea is that this API would facilitate synchronization of mutations to a JavaScript object between Worker threads, allowing me to share a simple JavaScript object to act as state between isolates without copying it or syncronizing the value with a `postMessage` based RPC implementation.\n\n## Deep properties / Complex types\n\nGiven how dynamic JavaScript values can be, perhaps a value can only be `transferrable` if the object is capable of being passed into `structuredClone`.\n\n```typescript\n// Assume a native addon that exposes a high-level API wrapping the native functions\nconst handle: number = myNativeAddon.createTransferrable() \n\nworker.postMessage(handle)\n\nconst guard: HandleGuard<Map<any, any>> = await myNativeAddon.aquireTransferrable(handle)\nguard.set('foo', 'bar')     // The key and value must go through `structuredClone()`\nconsole.log(guard.get('foo')). // \"bar\"\nmyNativeAddon.releaseTransferrable(guard)\n```\n\n## Async\n\nIt would be good if acquiring a transferrable value was an async operation, as it would avoid blocking the thread waiting for the value to be released.\n\n## GC considerations\n\nI understand that a napi_value is tied to an env and is managed by that context's GC (by V8). \n\nI'm wondering if, when a value is marked as \"transferrable\", much like the `napi_threadsafe_function ` API, is it possible that the value is removed from v8 GC and instead managed with a reference-counted smart pointer?\n\n## Other thoughts\n\nGiven the performance/memory of the tool in JavaScript is actually fine when single threaded but slow due to the inability to use multiple threads, having the ability to share/syncronize data between threads may be enough to avoid needing to rewrite the project.\n\n\n### What is the feature you are proposing to solve the problem?\n\nAddition of napi functions that facilitate syncronization of JavaScript values between threads\n\n### What alternatives have you considered?\n\n- `SharedArrayBuffer`\n  - Still require copying data to write it\n  - Not all data can be stored\n  - Complex synchronization due to the need for `Atomic`\n- Synchronization via `postMessage`\n  - Requires copying data\n  - Complex diffing logic\n- Centralizing data in an external in-memory database\n  - Performs poorly due to serialization/deserialization overhead  \n  - Still requires copy-on-read and copy-on-write\n- Rewriting in another language\n  - Not practical in the short/medium term ",
      "solution": "> V8 has preliminary support for shared structs, gated on the --harmony_struct flag\n\nThat's super interesting! \n\n> I don't have good suggestions off the bat, not without knowing more about the application (what it does, how it does it, etc.)\n\nI appreciate your insight nonetheless. It's a bundler forked from Parcel so we inhert their WorkerFarm approach (but we dropped support for `process` based workers and only use Nodejs workers).\n\nOur use case is pretty simple. We have a few graph structures that are essentially some objects/arrays that look approximately like;\n\n```typescript\nconst graphAdjacencyList  = []\nconst graphWeights = []\n\nfunction addNode(graph, edgeFrom, edgeTo, weight) {}\nfunction walk(graph, callback) {}\n```\nThe way we work with the graphs depends on the phase of the build the tool is in. For example, when the graph is read-only we have to copy the whole graph (the graph is 5gb) to the threads because traversing over `postMessage` is too slow - the compromise is memory usage.\n\nOne solution we are looking at is storing the graph data using a memory mapped file (lmdb) and storing the data as protobuf (protobuf.js). That would ensure the data is accessible across all threads without copying, but read/write operations would require copying and ser/de.\n\n> I don't want to turn this into an upsell but it feels like you'd be better off with an expert consultant\n\nHaha, I'm sold and have made the suggestion - sadly I can't authorize that\n\n---\n\n> If you're storing lots of strings, --shared_string_table may help (or not - you'll have to measure.)\n\nYes, strings account for the vast majority of our memory usage. I was trying to share them with `TextEncoder`, `TextDecoder` and `SharedArrayBuffer` but that wasn't a very good solution as they are still copied on read and (I think) there is a conversion to `utf16 -> utf8 -> utf16`.\n\nI also investigated using the streams API to remotely read/write a centrally stored string but I think that suffers the same limitations.\n\n---\n\nThere has been no activity on this feature request for 5 months. To help maintain relevant open issues, please add the https://github.com/nodejs/node/labels/never-stale label or close this issue if it should be closed. If not, the issue will be automatically closed 6 months after the last non-automated comment.\nFor more information on how the project manages feature requests, please consult the [feature request management document](https://github.com/nodejs/node/blob/HEAD/doc/contributing/feature-request-management.md).",
      "labels": [
        "feature request",
        "stale"
      ],
      "created_at": "2025-06-09T01:25:10Z",
      "closed_at": "2026-01-07T01:29:53Z",
      "url": "https://github.com/nodejs/node/issues/58638",
      "comments_count": 7
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 58970,
      "title": "`ENOENT` error when reading root of `subst` drive on Windows",
      "problem": "### Version\n\nv24.3.0\n\n### Platform\n\n```text\nMicrosoft Windows NT 10.0.26100.0 x64\n```\n\n### Subsystem\n\n_No response_\n\n### What steps will reproduce the bug?\n\n```cmd\nmd Subst\necho test>Subst\\testfile\nsubst M: Subst\n```\n```nodejs\nfs.readdirSync(\"C:\\\\\")\nfs.readdirSync(\"M:\\\\\")\nfs.readdirSync(\"M:\\\\.\")\n```\n<img width=\"1240\" height=\"951\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/a55eae80-1675-4779-9869-c1441e32ea9f\" />\n\n### How often does it reproduce? Is there a required condition?\n\nAlways\n\n### What is the expected behavior? Why is that the expected behavior?\n\nNo error should happens.\n\n### What do you see instead?\n\nENOENT: no such file or directory, scandir 'M:\\\\\\\\'\n\n### Additional information\n\nNot program with Node.js myself. Have issue with this behavior with VS Code extension https://github.com/belav/csharpier/issues/1637",
      "solution": "Hi, thanks for your report; just by doing some search in libuv, that's a known issue. I'm not a windows API programmer, at all. Not sure if there is much we can do from our side, for now, I'll just ping @nodejs/libuv; from what I see, I think it is easier to report this bug and suggest to your extension vendor that uses the extra `.` char.\n\nI think it is because of [`fs__capture_path`](https://github.com/libuv/libuv/blob/v1.x/src/win/fs.c#L322), but I'm not sure. Again, it's a known issue, and many fs calls in windows  \"invokes\" `fs__capture_path`, just like `uv_fs_realpath` (the libuv call with caveats)\n\n\nRefs:\n* Resolved path bypasses subst\u2019d drives (uv_fs_realpath -inner calls fs__capture_path same as  readdirSync-.)\n* https://github.com/libuv/libuv/issues/1877\n* https://docs.libuv.org/en/v1.x/fs.html#c.uv_fs_realpath\n* https://github.com/libuv/libuv/issues/1877\n\n---\n\nThat is as much, as I can investigate this (`echo y|cacls Subst /s:D:P` is to remove all permissions from directory to force error message):\n<img width=\"637\" height=\"970\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/8ca4b9fb-8bd1-4955-965e-d71f58441576\" />\nAs you can see `fs.readdirSync(\"M:\\\\\")` in v22 case produce different error message: `ENOENT: no such file or directory, scandir 'M:\\\\'` instead of `EPERM: operation not permitted, scandir 'M:\\'`. Notice double backslash in the end of path.\nAs I mention in my [comment](https://github.com/belav/csharpier/issues/1637#issuecomment-3001169760) to original [issue](https://github.com/belav/csharpier/issues/1637), I think it is what causing the issue:\n![Image](https://github.com/user-attachments/assets/11475efd-983f-467c-ade9-e801234cd96b)\n\nSo in v22 (compared to v20) it was changed how paths normalized and `M:\\` now normalized to `M:\\\\`, which is incorrect behavior.",
      "labels": [
        "windows",
        "libuv"
      ],
      "created_at": "2025-07-06T03:02:07Z",
      "closed_at": "2026-01-05T21:44:24Z",
      "url": "https://github.com/nodejs/node/issues/58970",
      "comments_count": 8
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 61141,
      "title": "ECMAScript Modules spec doesn't clearly state what to do with multiple extensions",
      "problem": "### Affected URL(s)\n\nhttps://nodejs.org/api/esm.html#resolution-algorithm-specification\n\n### Description of the problem\n\nWhile working through a few things with the Typescript team along with the vite / void zero team, I've run into something that I think could be clarified better in the docs about package resolution. \n\nThis is related to https://github.com/nodejs/node/pull/60864 along with https://github.com/microsoft/TypeScript/issues/62909#issuecomment-3676320028\n\nWhat I'm trying to sus out there is... when these wildcard exports are used, should there be any type of \"algorithm\" or \"probing\" that should happen at resolution time if the `import` statement in the given file doesn't define the extension of the file being imported? ",
      "solution": "The example laid out in the linked Typescript issue I think outlines the issue well. Granted the errors that show up via `tsc` are not directly a node issue, it's more that it doesn't seem there it's well stated how these modules exports should be resolved. \n\n---\n\n> should there be any type of \"algorithm\" or \"probing\" that should happen at resolution time if the import statement in the given file doesn't define the extension of the file being imported?\n\nI think you might be thinking about what's answered in https://nodejs.org/api/esm.html#mandatory-file-extensions - in Node.js, `import` does no extension probing for paths. It always only look at the path given as-is. The import conditions merely describe shorthands to expand, but once it's expanded to a path, the same rule would still apply - at least, from PACKAGE_IMPORTS_EXPORTS_RESOLVE in https://nodejs.org/api/esm.html#resolution-algorithm-specification it seems clear that there won't be any extensions being appended during the expansion and at most `*` are substituted. If it's not specified, it can be taken as \"it doesn't happen\".",
      "labels": [
        "question",
        "doc"
      ],
      "created_at": "2025-12-21T14:53:32Z",
      "closed_at": "2026-01-05T21:41:48Z",
      "url": "https://github.com/nodejs/node/issues/61141",
      "comments_count": 4
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 61203,
      "title": "Unsupported require(esm) whe using vm.Script",
      "problem": "### Version\n\nv24.12.0\n\n### Platform\n\n```text\nDarwin macbook-JPF9H46PN7 24.6.0 Darwin Kernel Version 24.6.0: Mon Jul 14 11:30:29 PDT 2025; arm64\n```\n\n### Subsystem\n\n_No response_\n\n### What steps will reproduce the bug?\n\nHello!\n\nWe use a `v8-compile-cache` library and recently get error - `Cannot use import statement outside a module` for `async-function` package.\n\nFrom `async-function`, `exports.module-sync` with ESM version is resolved:\n\n```\n\"name\": \"async-function\",\n...,\n\"exports\": {\n\t\".\": [\n\t\t{\n\t\t\t\"module-sync\": \"./require.mjs\",\n\t\t\t\"import\": \"./index.mjs\",\n\t\t\t\"default\": \"./index.js\"\n\t\t},\n\t\t\"./index.js\"\n\t],\n\t\"./package.json\": \"./package.json\"\n}\n```\n\n`v8-compile-cache` uses `vm.Script` for compilation - https://github.com/zertosh/v8-compile-cache/blob/b6bc035d337fbda0e6e3ec7936499048fc9deafc/v8-compile-cache.js#L240\n\nSimple reproduction without `v8-compile-cache`:\n```js\nconst vm = require('node:vm');\n\nconst success = require('async-function');\n\nconst failed = new vm.Script(\n  `import getAsyncFunction from './index.js';\n\nexport default getAsyncFunction;\n\nexport { getAsyncFunction as 'module.exports' };`,\n  {\n    filename: './node_modules/async-function/require.mjs',\n  }\n);\n```\n\nThere is no dynamic imports, or anyway `importModuleDynamically` should be used?\n\nIf this behaviour is expected, how can we compile both CJS and ESM modules in universal way? \n\n### How often does it reproduce? Is there a required condition?\n\nAlways\n\n### What is the expected behavior? Why is that the expected behavior?\n\nI can use CJS and ESM in Script\n\n### What do you see instead?\n\n`SyntaxError: Cannot use import statement outside a module`\n\n### Additional information\n\n_No response_",
      "solution": "You can use the built-in [module.enableCompileCache([options])](https://nodejs.org/api/module.html#moduleenablecompilecacheoptions) instead of `v8-compile-cache` to avoid the issue caused by obsolete monkey-patching - otherwise, the fix needs to be in `v8-compile-cache`: the monkey patcher has to constantly stay up-to-date with the latest version of internals to support newer features.\n\n---\n\nClosing as invalid, the issue is in a third-party package.",
      "labels": [
        "invalid"
      ],
      "created_at": "2025-12-29T12:01:42Z",
      "closed_at": "2026-01-05T21:39:53Z",
      "url": "https://github.com/nodejs/node/issues/61203",
      "comments_count": 8
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 60482,
      "title": "Memory regression: Node.js 24.11.x vs 22.20.x",
      "problem": "### Version\n\n24.11.0\n\n### Platform\n\n```\nLinux agent-e2e  6.8.0-1031-azure #36~22.04.1-Ubuntu SMP Tue Jul  1 03:54:01 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux\n```\n\n### Subsystem\n\n_No response_\n\n### What steps will reproduce the bug?\n\nMy reproduction steps are very specific as I am running set of tests via playwright testing service (28 workers). What I really change and than observed problem is to just change versions of node in this task:\n  - task: NodeTool@0\n    displayName: Set Node Version\n    inputs:\n      versionSpec: 24.x // changed from 22.x\n\n\n### How often does it reproduce? Is there a required condition?\n\nAlways - 100% reproduction rate when running Node.js 24.11.x with memory-intensive Playwright test suites when all tests need to spawn browser sessions.\n\n### What is the expected behavior? Why is that the expected behavior?\n\nMemory usage should remain stable around 80% as it does in Node.js 22.20.x. The same test suite with identical configuration should not require more memory allocation.\n\n### What do you see instead?\n\nMemory usage spikes to 97.20% and process gets killed with out-of-memory errors. Increasing heap size not helping up to 24GB\n\n### Additional information\n\n- Node.js 22.20.x: Stable execution with ~75% average memory usage using (8GB heap I think it is default in that version)\n- Node.js 24.11.x: Crashes with OOM, still reaches 97%+ usage (checked with heap 4GB, 8GB, 24GB)\n- Environment: Ubuntu 24.04, Azure DevOps, Playwright Testing Service\n- Workaround: Reverting to Node.js 22.20.x resolves the issue completely\n- Playwright version 1.56.1\n\nThis appears to be a significant memory management regression in the V8 heap handling between these versions. ",
      "solution": "Hi,\n\nI confirm we faced the same behavior with high workload workers. \nWe noticed a strange memory leak after the upgrade from node `v22.11` to `v24.11`. An upgrade to `v24.12` fixed the issue.\n\nTo confirm, we ran for days the same worker with 3 different nodejs versions, here's the clear results:\n\n<img width=\"3254\" height=\"1823\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/c060ec8f-2c9b-457d-b0d1-41da264e7b16\" />\n\nHope it will help someone facing the same issue.",
      "labels": [],
      "created_at": "2025-10-29T14:12:55Z",
      "closed_at": "2025-10-29T14:56:06Z",
      "url": "https://github.com/nodejs/node/issues/60482",
      "comments_count": 2
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 57996,
      "title": "test: Add --repeat-until-n-failures flag to test runner",
      "problem": "### What is the problem this feature will solve?\n\nCurrently, debugging flaky tests requires either:\n1. Manually running tests repeatedly\n2. Using --repeat with a fixed number of iterations\n3. Writing custom test wrapper code\n\nThis makes it difficult to:\n- Reliably reproduce failures\n- Gather statistics about test flakiness\n- Automate flaky test detection in CI\n\nWhile V8's test runner has some flaky test handling capabilities, Node.js's main test runner lacks a built-in way to repeatedly run tests until a specific number of failures occur. The main use-case of course, would be to run until one failure is reached as opposed to having to repeatedly run tests hoping they fail, which even with the --repeated flag is rather tedious. A debugging flow I use is to run a test until a failure, then using a breakpoint at the failure, inspect the variables and call stack.\n\n### What is the feature you are proposing to solve the problem?\n\nAdd a new --repeat-until-n-failures flag to tools/test.py that allows tests to be repeated until a specified number of failures occur or a maximum iteration count is reached.\n\nProposed syntax:\n--repeat-until-n-failures=<n_failures>[,max_iterations]\n\nExamples:\n# Run until 1 failure occurs\npython tools/test.py --repeat-until-n-failures=1 test/path\n\n# Run until 5 failures or 1000 iterations\npython tools/test.py --repeat-until-n-failures=5,1000 test/path\n\nThe feature would:\n1. Run the specified test(s) repeatedly\n2. Track the number of failures\n3. Stop when either:\n   - The specified number of failures is reached\n   - The maximum iteration count (if specified) is reached\n4. Report statistics about the failures (iteration counts, failure rate, etc.)\n\nThis would complement the existing --repeat and --exit-after-n-failures flags by providing a focused tool for debugging flaky tests and measuring test reliability.\n\n**I am happy to work on implementing this feature if there is interest.**\n\n### What alternatives have you considered?\n\n\n1. Using existing --repeat flag with a large number:\n   - Less efficient as it always runs all iterations\n   - Doesn't stop automatically on failure\n   - Harder to analyze results\n\n2. Personal custom test script:\n   - More fragile, and less extensibility\n   - Maintenance burden as test runner evolves - need to update parsing logic with each change\n   - Isolated solution that doesn't benefit the Node.js ecosystem\n",
      "solution": "There has been no activity on this feature request for 5 months. To help maintain relevant open issues, please add the https://github.com/nodejs/node/labels/never-stale label or close this issue if it should be closed. If not, the issue will be automatically closed 6 months after the last non-automated comment.\nFor more information on how the project manages feature requests, please consult the [feature request management document](https://github.com/nodejs/node/blob/HEAD/doc/contributing/feature-request-management.md).",
      "labels": [
        "feature request",
        "stale"
      ],
      "created_at": "2025-04-24T04:55:54Z",
      "closed_at": "2026-01-04T01:30:21Z",
      "url": "https://github.com/nodejs/node/issues/57996",
      "comments_count": 4
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 60394,
      "title": "Not all sqlite code snippet have CJS/ESM variants",
      "problem": "### Affected URL(s)\n\nhttps://nodejs.org/api/sqlite.html\n\n### Description of the problem\n\nMost code snippets have both CJS and ESM, as per the doc's standard. However, two functions stand out as being different:\n\n- `createTagStore` only has the ESM variant\n- `applyChangeset` only has one example which would work for either CJS or ESM, but is incomplete - it doesn't show how `DatabaseSync` is improted, and can't be executed without manually adding that part.",
      "solution": "I think this was fixed by https://github.com/nodejs/node/pull/60395 @nodejs/issue-triage",
      "labels": [
        "doc"
      ],
      "created_at": "2025-10-24T21:20:59Z",
      "closed_at": "2026-01-03T17:02:10Z",
      "url": "https://github.com/nodejs/node/issues/60394",
      "comments_count": 1
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 60670,
      "title": "Add a direct API for setting object prototype in `node_api.h`",
      "problem": "### What is the problem this feature will solve?\n\nSee nodejs/node-addon-api#1691\n\n### What is the feature you are proposing to solve the problem?\n\nCreate a new API function `napi_set_prototype` in `node_api.h` to provide a NAPI equivalent for `v8::Object::SetPrototype`.\n\n### What alternatives have you considered?\n\n_No response_",
      "solution": "> ### What is the problem this feature will solve?\n> See [nodejs/node-addon-api#1691](https://github.com/nodejs/node-addon-api/issues/1691)\n> \n> ### What is the feature you are proposing to solve the problem?\n> Create a new API function `napi_set_prototype` in `node_api.h` to provide a NAPI equivalent for `v8::Object::SetPrototype`.\n> \n> ### What alternatives have you considered?\n> _No response_\n\n",
      "labels": [
        "feature request",
        "node-api"
      ],
      "created_at": "2025-11-10T19:58:38Z",
      "closed_at": "2026-01-02T18:37:37Z",
      "url": "https://github.com/nodejs/node/issues/60670",
      "comments_count": 2
    },
    {
      "tech": "nodejs",
      "repo": "nodejs/node",
      "issue_number": 57998,
      "title": "Enable terminal colors in CI",
      "problem": "### What is the problem this feature will solve?\n\nI noticed `util.styleText()` supports colorizing text in GitLab CI and GitHub Actions in 22.14, but not in 22.15. And in 22.14, only when the color input is an array. The cause is #56722. That PR correctly solves the bug it references. But it does raise a question: Should `util.styleText()` enable colors if it runs in a CI provider that supports it? I think it would be nice to have.\n\n### What is the feature you are proposing to solve the problem?\n\nEnable colorizing text if Node.js runs in a CI provider that is known to support colors.\n\n### What alternatives have you considered?\n\nKeep the status quo. Don\u2019t support colorizing text in CI by default.",
      "solution": "There has been no activity on this feature request for 5 months. To help maintain relevant open issues, please add the https://github.com/nodejs/node/labels/never-stale label or close this issue if it should be closed. If not, the issue will be automatically closed 6 months after the last non-automated comment.\nFor more information on how the project manages feature requests, please consult the [feature request management document](https://github.com/nodejs/node/blob/HEAD/doc/contributing/feature-request-management.md).",
      "labels": [
        "feature request",
        "stale"
      ],
      "created_at": "2025-04-24T08:04:10Z",
      "closed_at": "2026-01-01T01:30:38Z",
      "url": "https://github.com/nodejs/node/issues/57998",
      "comments_count": 14
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 89190,
      "title": "Next 16.0.11 emits a `[baseline-browser-mapping] The data in this module is over two months old.` warning",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/eliw00d/next-baseline-browser-mapping-repro\n\n### To Reproduce\n\n1. `npm i`\n2. `npm run build` or `npm run dev`\n3. See:\n```\n[baseline-browser-mapping] The data in this module is over two months old.  To ensure accurate Baseline data, please update: `npm i baseline-browser-mapping@latest -D`\n```\n\n### Current vs. Expected behavior\n\nCurrent behavior\n---\n16.0.11 emits a `[baseline-browser-mapping] The data in this module is over two months old.` warning. 16.1.6 does _not_ emit this warning. \n\nExpected behavior\n---\nA new release of 16.0 that does not emit a `[baseline-browser-mapping] The data in this module is over two months old.` warning.\n\n### Provide environment information\n\n```bash\n\u276f npx next info\n[baseline-browser-mapping] The data in this module is over two months old.  To ensure accurate Baseline data, please update: `npm i baseline-browser-mapping@latest -D`\n\nOperating System:\n  Platform: darwin\n  Arch: arm64\n  Version: Darwin Kernel Version 25.2.0: Tue Nov 18 21:09:41 PST 2025; root:xnu-12377.61.12~1/RELEASE_ARM64_T6031\n  Available memory (MB): 65536\n  Available CPU cores: 16\nBinaries:\n  Node: 24.11.1\n  npm: 11.6.2\n  Yarn: 1.22.22\n  pnpm: 10.25.0\nRelevant Packages:\n  next: 16.0.11 // There is a newer version (16.1.6) available, upgrade recommended! \n  eslint-config-next: N/A\n  react: 19.2.3\n  react-dom: 19.2.3\n  typescript: 5.9.3\nNext.js Config:\n  output: N/A\n \u26a0 There is a newer version (16.1.6) available, upgrade recommended! \n   Please try the latest canary version (`npm install next@canary`) to confirm the issue still exists before creating a new issue.\n   Read more - https://nextjs.org/docs/messages/opening-an-issue\n```\n\n### Which area(s) are affected? (Select all that apply)\n\ncreate-next-app\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext dev (local), next build (local)\n\n### Additional context\n\nWe are currently blocked on upgrading to Next 16.1 due to our use of Rush.js so we have to stay on 16.0 for now. A new release of 16.0 with this fixed would be great.",
      "solution": "The `baseline-browser-mapping` library checks the date, and complains if it hasn't been updated recently. In a multithreaded environment, it winds up complaining once per thread, which is extremely spammy. AFAIK this issue can't really be resolved (without it recurring every two months, or needing every downstream user of nextjs to add an environment variable) without either forking this library or dropping it as a dependency; if you look at its issue tracker, the current naintainers don't seem to be willing to change this behavior.\n\n---\n\nThis was 'fixed' in #89175 which will be released in the next patch release for 16.1\n",
      "labels": [
        "create-next-app"
      ],
      "created_at": "2026-01-28T19:38:52Z",
      "closed_at": "2026-02-06T01:21:03Z",
      "url": "https://github.com/vercel/next.js/issues/89190",
      "comments_count": 5
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 70949,
      "title": "Docs: The link in the `EmojiType` for `ImageOptions` in `og` is outdated",
      "problem": "### What is the documentation issue?\r\n\r\nhttps://github.com/vercel/next.js/blob/64c414fda59d9c2f4b244de41c86df35d81de3f7/packages/next/src/compiled/%40vercel/og/types.d.ts#L36-L43\r\n\r\n`@link https://github.com/vercel/og#emoji` is outdated.\r\n\r\n### Is there any context that might help us understand?\r\n\r\nGoto https://github.com/vercel/og#emoji and we will find the 404 error message.\r\n\r\n<img width=\"1280\" alt=\"Screenshot 2024-10-08 at 3 53 28\u202fPM\" src=\"https://github.com/user-attachments/assets/ed56665f-2370-4dfb-8f21-b6cafd67e1b6\">\r\n\r\n\r\n\r\n### Does the docs page already exist? Please link to it.\r\n\r\nThere is still no any example in https://nextjs.org/docs/app/api-reference/functions/image-response, but it's the related page.",
      "solution": "This was fixed, by removing the reference. Closing the issue.",
      "labels": [],
      "created_at": "2024-10-08T07:56:41Z",
      "closed_at": "2026-02-06T01:08:37Z",
      "url": "https://github.com/vercel/next.js/issues/70949",
      "comments_count": 2
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 25108,
      "title": "Slow page reload running in development",
      "problem": "### What version of Next.js are you using?\r\n\r\n10.2.0\r\n\r\n### What version of Node.js are you using?\r\n\r\n14.15.4\r\n\r\n### What browser are you using?\r\n\r\nChrome\r\n\r\n### What operating system are you using?\r\n\r\nmacOS\r\n\r\n### How are you deploying your application?\r\n\r\nn/a\r\n\r\n### Describe the Bug\r\n\r\nWhen running `npm run dev` if you reload the page (refresh) a few times, after about the 5th refresh the load time gets very slow and almost unusable.\r\n\r\n### Expected Behavior\r\n\r\nRefresh should be less than a few seconds.\r\n\r\n### To Reproduce\r\n\r\n`npx create-next-app app`\r\n`cd app`\r\n`npm run dev`\r\n\r\nRefresh the page 5-10 times.",
      "solution": "I have found a solution, dont know if its permanent, but here it is. Im using windows tho, so what I did was created an exclusion on windows defender directly on the project's folder and that was it. Windows defender may be causing this delay because of a deep scan it performs on some files within the folder, hope it can help someone. \n\n---\n\nI was unable to repro this but I'm on Linux.\r\nWe will probably not dig into this so If anyone from the community is interested we would love to assist you in finding the root cause and review your PR with a fix.\r\n\r\nThanks `mazc28` for finding out the thing with Windows defender.\n\n---\n\nI encountered the same problem as you described, and following mazc28's suggested solution, I was able to successfully resolve the issue on my Windows 11 system (Node.js v18.20.5). Thank you guys very much for sharing this probem and solution, it's very helpful for me a newbie.\r\n\r\nIf anyone encounters the same problem in a similar development environment, the following troubleshooting steps may be helpful:\r\n1. Open the Windows Security app.\r\n2. Click on \"Virus & threat protection\" on the left.\r\n3. Under \"Virus & threat protection settings\", click on \"Manage settings\".\r\n4. Scroll down to the \"Exclusions\" section and click on \"Add or remove exclusions\".\r\n5. Click on \"Add an exclusion\", and select \"Folder\".\r\n6. Navigate to the folder where your Next.js project is located, then click \"Select Folder\".",
      "labels": [
        "good first issue",
        "bug"
      ],
      "created_at": "2021-05-13T17:11:43Z",
      "closed_at": "2026-02-06T01:01:34Z",
      "url": "https://github.com/vercel/next.js/issues/25108",
      "comments_count": 10
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 70742,
      "title": "Docs: Page not jumping properly after google translate",
      "problem": "### What is the documentation issue?\n\nI used google translate in the page and then after clicking on the link in Next Steps, I was redirected to the error page like this:\r\n![image](https://github.com/user-attachments/assets/135664f4-65fb-4a08-b2e8-75eebf2011a0)\r\n\n\n### Is there any context that might help us understand?\n\nThe error msg in browser console:\r\n```\r\nNotFoundError: Failed to execute 'insertBefore' on 'Node': The node before which the new node is to be inserted is not a child of this node.\r\n    at oI (1fe503da-8c7958a59066e455.js:1:86225)\r\n    at o1 (1fe503da-8c7958a59066e455.js:1:95781)\r\n    at o0 (1fe503da-8c7958a59066e455.js:1:92825)\r\n    at oZ (1fe503da-8c7958a59066e455.js:1:90281)\r\n    at o0 (1fe503da-8c7958a59066e455.js:1:92817)\r\n    at oZ (1fe503da-8c7958a59066e455.js:1:90281)\r\n    at o0 (1fe503da-8c7958a59066e455.js:1:90403)\r\n    at oZ (1fe503da-8c7958a59066e455.js:1:90281)\r\n    at o0 (1fe503da-8c7958a59066e455.js:1:92817)\r\n    at oZ (1fe503da-8c7958a59066e455.js:1:90281)\r\n    at o0 (1fe503da-8c7958a59066e455.js:1:90403)\r\n    at oZ (1fe503da-8c7958a59066e455.js:1:90281)\r\n    at o0 (1fe503da-8c7958a59066e455.js:1:92817)\r\n    at oZ (1fe503da-8c7958a59066e455.js:1:90281)\r\n    at o0 (1fe503da-8c7958a59066e455.js:1:92817)\r\n    at oZ (1fe503da-8c7958a59066e455.js:1:90281)\r\n    at o0 (1fe503da-8c7958a59066e455.js:1:92817)\r\n    at oZ (1fe503da-8c7958a59066e455.js:1:90281)\r\n    at o0 (1fe503da-8c7958a59066e455.js:1:90403)\r\n    at oZ (1fe503da-8c7958a59066e455.js:1:90281)\r\n    at o0 (1fe503da-8c7958a59066e455.js:1:92817)\r\n    at oZ (1fe503da-8c7958a59066e455.js:1:90281)\r\n    at o0 (1fe503da-8c7958a59066e455.js:1:90403)\r\n    at oZ (1fe503da-8c7958a59066e455.js:1:90281)\r\n    at o0 (1fe503da-8c7958a59066e455.js:1:92817)\r\n    at oZ (1fe503da-8c7958a59066e455.js:1:90281)\r\n    at o0 (1fe503da-8c7958a59066e455.js:1:92817)\r\n    at oZ (1fe503da-8c7958a59066e455.js:1:90281)\r\n    at o0 (1fe503da-8c7958a59066e455.js:1:92817)\r\n    at oZ (1fe503da-8c7958a59066e455.js:1:90281)\r\n    at o0 (1fe503da-8c7958a59066e455.js:1:90403)\r\n    at oZ (1fe503da-8c7958a59066e455.js:1:90281)\r\n    at o0 (1fe503da-8c7958a59066e455.js:1:92817)\r\n    at oZ (1fe503da-8c7958a59066e455.js:1:90281)\r\n    at o0 (1fe503da-8c7958a59066e455.js:1:90403)\r\n    at oZ (1fe503da-8c7958a59066e455.js:1:90281)\r\n    at o0 (1fe503da-8c7958a59066e455.js:1:92817)\r\n    at oZ (1fe503da-8c7958a59066e455.js:1:90281)\r\n    at o0 (1fe503da-8c7958a59066e455.js:1:93584)\r\n    at oZ (1fe503da-8c7958a59066e455.js:1:90281)\r\n    at o0 (1fe503da-8c7958a59066e455.js:1:92817)\r\n    at oZ (1fe503da-8c7958a59066e455.js:1:90281)\r\n    at o0 (1fe503da-8c7958a59066e455.js:1:92817)\r\n    at oZ (1fe503da-8c7958a59066e455.js:1:90281)\r\n    at o0 (1fe503da-8c7958a59066e455.js:1:90403)\r\n    at oZ (1fe503da-8c7958a59066e455.js:1:90281)\r\n    at o0 (1fe503da-8c7958a59066e455.js:1:92817)\r\n    at oZ (1fe503da-8c7958a59066e455.js:1:90281)\r\n    at o0 (1fe503da-8c7958a59066e455.js:1:92817)\r\n    at oZ (1fe503da-8c7958a59066e455.js:1:90281)\r\n```\r\n\r\nBrowser: Chrome\r\nChrome version: 129.0.6668.72\n\n### Does the docs page already exist? Please link to it.\n\nhttps://nextjs.org/docs/app/building-your-application/routing/defining-routes",
      "solution": "@RainbowXXX Can you be more specific with your steps so we can try to replicate the issue (e.g., what language from and to did you translate)?\n\n---\n\nhttps://issues.chromium.org/issues/41407169\n\nA ticket was created Aug 9, 2018 and google have yet to resolve the issue haha\n\n---\n\nHere's a good breakdown about this issue: https://martijnhols.nl/blog/everything-about-google-translate-crashing-react\n\nI fixed it in the Next.js site a few months ago, there might be some floating fragment I forgot somewhere. \n\nI'll go ahead and close this issue, because AFAIK, the Next.js site is safe from this right now.  ",
      "labels": [],
      "created_at": "2024-10-03T04:31:46Z",
      "closed_at": "2026-02-06T00:57:29Z",
      "url": "https://github.com/vercel/next.js/issues/70742",
      "comments_count": 8
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 69666,
      "title": "Nested layout inheriting classes from outer layout on page load",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/Zain-ul-din/mdx-templates\n\n### To Reproduce\n\n\r\nI have no idea this is a feature or bug but it's not working as I want it to work. I'm using next js app router and my directory structure is following\r\n\r\n```file\r\n- app\r\n  - docs\r\n    page.tsx\r\n    layout.tsx\r\n layout.tsx\r\n page.tsx\r\n```\r\n\r\nI'm trying to add classes inside `docs/layout.tsx` body it's working on runtime but as I refresh the page it's being replaced by root layout `app/layout.tsx` body classes.\r\n\r\n```tsx\r\n// app/layout.tsx\r\nexport default function RootLayout({\r\n  children,\r\n}: Readonly<{\r\n  children: React.ReactNode;\r\n}>) {\r\n  return (\r\n    <html lang=\"en\">\r\n      <body className={`${inter.className} root`}>{children}</body>\r\n    </html>\r\n  );\r\n}\r\n```\r\n\r\n```tsx\r\n// app/docs/layout.tsx\r\nexport default function RootLayout({\r\n  children,\r\n}: Readonly<{\r\n  children: React.ReactNode;\r\n}>) {\r\n  return (\r\n    <html lang=\"en\">\r\n      <body className={`${inter.className} prose`}>\r\n        <h1 className=\"text-2xl my-4\">\r\n          \ud83d\udc51 This is root {`'/(docs)/docs/layout.tsx'`}\r\n        </h1>\r\n        {children}\r\n      </body>\r\n    </html>\r\n  );\r\n}\r\n```\r\n\r\nhttps://github.com/user-attachments/assets/42ac0b7d-1327-4d05-9452-8258b039265c\r\n\r\n\n\n### Current vs. Expected behavior\n\nNested layout classes should not be overridden by root layout.\n\n### Provide environment information\n\n```bash\nOperating System:\r\n  Platform: win32\r\n  Arch: x64\r\n  Version: Windows 10 Pro\r\n  Available memory (MB): 15742\r\n  Available CPU cores: 16\r\nBinaries:\r\n  Node: 20.10.0\r\n  npm: N/A\r\n  Yarn: N/A\r\n  pnpm: N/A\r\nRelevant Packages:\r\n  next: 14.2.7 // Latest available version is detected (14.2.7).\r\n  eslint-config-next: 14.2.7\r\n  react: 18.3.1\r\n  react-dom: 18.3.1\r\n  typescript: 5.5.4\r\nNext.js Config:\r\n  output: N/A\n```\n\n\n### Which area(s) are affected? (Select all that apply)\n\nNavigation\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext dev (local)\n\n### Additional context\n\nRepo Link: https://github.com/Zain-ul-din/mdx-templates\r\n\r\nThank you in advance",
      "solution": "quick review from your video, i saw duplicated `export default function RootLayout(){}`  from `app/layout.tsx` and `app/docs/layout.tsx` might be the problem.\r\n\r\nyou should wrap any mdx content with [shared layout](https://nextjs.org/docs/pages/building-your-application/configuring/mdx#shared-layouts).\r\n\r\nlayout: `app/docs/layout.tsx`\r\n```tsx\r\nexport default function MdxLayout({ children }: { children: React.ReactNode }) {\r\n  // Create any shared layout or styles here\r\n  return <div style={{ color: 'blue' }}>{children}</div>\r\n}\r\n```\r\n\r\npage: `app/docs/page.mdx`\r\n```mdx\r\n# Welcome to my MDX page!\r\n```\r\n\r\n---\r\n\r\n> [!NOTE]\r\n> i know that you're trying to add some classes inside `docs/layout.tsx`, but you can't import another _root-layout_ inside **root-layout**.",
      "labels": [
        "bug",
        "Linking and Navigating"
      ],
      "created_at": "2024-09-04T10:32:39Z",
      "closed_at": "2026-02-06T00:47:16Z",
      "url": "https://github.com/vercel/next.js/issues/69666",
      "comments_count": 2
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 85489,
      "title": "Prefetch requests happen more than once on Next 16",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/jperezr21/next-16-issue\n\n### To Reproduce\n\n1. `pnpm build`\n2. `pnpm start`\n3. Go to `/`\n4. See logs\n\n<img width=\"832\" height=\"255\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/eb1ac1bd-afaf-4df0-8519-8e2230ce2ba6\" />\n\n\n5. `git checkout next-15`\n6. `pnpm build --turbopack`\n7. `pnpm start`\n8. Go to `/`\n9. See logs\n\n<img width=\"833\" height=\"192\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/344137cb-0acf-408d-9829-450a7023f8fa\" />\n\n### Current vs. Expected behavior\n\nLinks should be prefetched once, but they're being prefetched more than once\n\n### Provide environment information\n\n```bash\nOperating System:\n  Platform: darwin\n  Arch: arm64\n  Version: Darwin Kernel Version 24.6.0: Mon Aug 11 21:16:31 PDT 2025; root:xnu-11417.140.69.701.11~1/RELEASE_ARM64_T6030\n  Available memory (MB): 18432\n  Available CPU cores: 11\nBinaries:\n  Node: 22.13.1\n  npm: 11.1.0\n  Yarn: N/A\n  pnpm: 10.20.0\nRelevant Packages:\n  next: 16.0.1 // Latest available version is detected (16.0.1).\n  eslint-config-next: N/A\n  react: 19.2.0\n  react-dom: 19.2.0\n  typescript: 5.9.3\nNext.js Config:\n  output: N/A\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nLinking and Navigating\n\n### Which stage(s) are affected? (Select all that apply)\n\nVercel (Deployed), next start (local)\n\n### Additional context\n\nIdentified this issue after upgrading to next 16 and seeing the number of function invocations in Vercel more than double:\n\n<img width=\"1235\" height=\"547\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b004e978-9863-45c7-873a-a20a5350b3ef\" />\n\n> Upgraded on Oct 23, downgraded on Oct 27",
      "solution": "~~I opened a PR for another issue (seems to be similar). The fix ensures that the prefetch returns a `304` status code (this was the original issue) and it seems that with this fix the prefetch only happens once. See: https://github.com/vercel/next.js/pull/85643~~\nnevermind, it had a different root cause. opened a PR",
      "labels": [
        "Linking and Navigating",
        "linear: next",
        "locked"
      ],
      "created_at": "2025-10-29T01:59:32Z",
      "closed_at": "2026-01-19T11:28:41Z",
      "url": "https://github.com/vercel/next.js/issues/85489",
      "comments_count": 16
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 89543,
      "title": "[Medium] Race Condition in next.js",
      "problem": "## Bug Report\n\n**Type**: Race Condition\n**Severity**: Medium\n**File**: src/next.js.js:189\n\n### Description\n\u5e76\u53d1\u8bbf\u95ee\u5171\u4eab\u53d8\u91cf\u65f6\u7f3a\u4e4f\u9002\u5f53\u7684\u540c\u6b65\u673a\u5236\u3002\n\n### Impact\n\u53ef\u80fd\u5bfc\u81f4\u6570\u636e\u4e0d\u4e00\u81f4\u3002\n\n### Proposed Fix\n\u4f7f\u7528\u9002\u5f53\u7684\u5e76\u53d1\u63a7\u5236\u3002\n\n---\nAutomated Bug Report - 2026-02-05 17:07:17\nBug Bounty Request\n",
      "solution": "We could not detect a valid reproduction link. **Make sure to follow the bug report template carefully.**\n\n### Why was this issue closed?\n\nTo be able to investigate, we need access to a reproduction to identify what triggered the issue. We need a link to a **public** GitHub repository ([template for App Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template), [template for Pages Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template-pages)), but you can also use these templates: [CodeSandbox: App Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template) or [CodeSandbox: Pages Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template-pages).\n\nThe bug template that you filled out has a section called \"Link to the code that reproduces this issue\", which is where you should provide the link to the reproduction.\n\n- If you did not provide a link or the link you provided is not valid, we will close the issue.\n- If you provide a link to a private repository, we will close the issue.\n- If you provide a link to a repository but not in the correct section, we will close the issue.\n\n### What should I do?\n\nDepending on the reason the issue was closed, you can do the following:\n\n- If you did not provide a link, please open a new issue with a link to a reproduction.\n- If you provided a link to a private repository, please open a new issue with a link to a public repository.\n- If you provided a link to a repository but not in the correct section, please open a new issue with a link to a reproduction in the correct section.\n\n**In general, assume that we should not go through a lengthy onboarding process at your company code only to be able to verify an issue.**\n\n### My repository is private and cannot make it public\n\nIn most cases, a private repo will not be a sufficient **minimal reproduction**, as this codebase might contain a lot of unrelated parts that would make our investigation take longer. Please do **not** make it public. Instead, create a new repository using the templates above, adding the relevant code to reproduce the issue. Common things to look out for:\n\n- Remove any code that is not related to the issue. (pages, API routes, components, etc.)\n- Remove any dependencies that are not related to the issue.\n- Remove any third-party service that would require us to sign up for an account to reproduce the issue.\n- Remove any environment variables that are not related to the issue.\n- Remove private packages that we do not have access to.\n- If the issue is not related to a monorepo specifically, try to reproduce the issue without a complex monorepo setup\n\n### I did not open this issue, but it is relevant to me, what can I do to help?\n\nAnyone experiencing the same issue is welcome to provide a minimal reproduction following the above steps by opening a new issue.\n\n### I think my reproduction is good enough, why aren't you looking into it quickly?\n\nWe look into every Next.js issue and constantly monitor open issues for new comments.\n\nHowever, sometimes we might miss one or two due to the popularity/high traffic of the repository. We apologize, and kindly ask you to refrain from tagging core maintainers, as that will usually not result in increased priority.\n\nUpvoting issues to show your interest will help us prioritize and address them as quickly as possible. That said, every issue is important to us, and if an issue gets closed by accident, we encourage you to open a new one linking to the old issue and we will look into it.\n\n### Useful Resources\n\n- [How to Contribute to Open Source (Next.js)](https://www.youtube.com/watch?v=cuoNzXFLitc)\n- [How to create a Minimal, Complete, and Verifiable example](https://stackoverflow.com/help/mcve)\n- [Reporting a Next.js bug](https://github.com/vercel/next.js/blob/canary/.github/ISSUE_TEMPLATE/1.bug_report.yml)\n- [Next.js Triaging issues](https://github.com/vercel/next.js/blob/canary/contributing/repository/triaging.md)\n",
      "labels": [
        "invalid link"
      ],
      "created_at": "2026-02-05T17:07:18Z",
      "closed_at": "2026-02-05T17:07:34Z",
      "url": "https://github.com/vercel/next.js/issues/89543",
      "comments_count": 1
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 89542,
      "title": "[High] Memory Leak in next.js",
      "problem": "## Bug Report\n\n**Type**: Memory Leak\n**Severity**: High\n**File**: src/next.js.ts:234\n\n### Description\n\u53d1\u73b0\u8d44\u6e90\u8fde\u63a5\u672a\u6b63\u786e\u5173\u95ed\uff0c\u53ef\u80fd\u5bfc\u81f4\u5185\u5b58\u6cc4\u6f0f\u3002\n\n### Impact\n- \u957f\u671f\u8fd0\u884c\u5185\u5b58\u5360\u7528\u6301\u7eed\u589e\u957f\n- \u53ef\u80fd\u5bfc\u81f4\u670d\u52a1\u5d29\u6e83\n\n### Proposed Fix\n\u4f7f\u7528try-finally\u786e\u4fdd\u8d44\u6e90\u91ca\u653e\u3002\n\n---\nAutomated Bug Report - 2026-02-05 17:07:14\nBug Bounty Request\n",
      "solution": "We could not detect a valid reproduction link. **Make sure to follow the bug report template carefully.**\n\n### Why was this issue closed?\n\nTo be able to investigate, we need access to a reproduction to identify what triggered the issue. We need a link to a **public** GitHub repository ([template for App Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template), [template for Pages Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template-pages)), but you can also use these templates: [CodeSandbox: App Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template) or [CodeSandbox: Pages Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template-pages).\n\nThe bug template that you filled out has a section called \"Link to the code that reproduces this issue\", which is where you should provide the link to the reproduction.\n\n- If you did not provide a link or the link you provided is not valid, we will close the issue.\n- If you provide a link to a private repository, we will close the issue.\n- If you provide a link to a repository but not in the correct section, we will close the issue.\n\n### What should I do?\n\nDepending on the reason the issue was closed, you can do the following:\n\n- If you did not provide a link, please open a new issue with a link to a reproduction.\n- If you provided a link to a private repository, please open a new issue with a link to a public repository.\n- If you provided a link to a repository but not in the correct section, please open a new issue with a link to a reproduction in the correct section.\n\n**In general, assume that we should not go through a lengthy onboarding process at your company code only to be able to verify an issue.**\n\n### My repository is private and cannot make it public\n\nIn most cases, a private repo will not be a sufficient **minimal reproduction**, as this codebase might contain a lot of unrelated parts that would make our investigation take longer. Please do **not** make it public. Instead, create a new repository using the templates above, adding the relevant code to reproduce the issue. Common things to look out for:\n\n- Remove any code that is not related to the issue. (pages, API routes, components, etc.)\n- Remove any dependencies that are not related to the issue.\n- Remove any third-party service that would require us to sign up for an account to reproduce the issue.\n- Remove any environment variables that are not related to the issue.\n- Remove private packages that we do not have access to.\n- If the issue is not related to a monorepo specifically, try to reproduce the issue without a complex monorepo setup\n\n### I did not open this issue, but it is relevant to me, what can I do to help?\n\nAnyone experiencing the same issue is welcome to provide a minimal reproduction following the above steps by opening a new issue.\n\n### I think my reproduction is good enough, why aren't you looking into it quickly?\n\nWe look into every Next.js issue and constantly monitor open issues for new comments.\n\nHowever, sometimes we might miss one or two due to the popularity/high traffic of the repository. We apologize, and kindly ask you to refrain from tagging core maintainers, as that will usually not result in increased priority.\n\nUpvoting issues to show your interest will help us prioritize and address them as quickly as possible. That said, every issue is important to us, and if an issue gets closed by accident, we encourage you to open a new one linking to the old issue and we will look into it.\n\n### Useful Resources\n\n- [How to Contribute to Open Source (Next.js)](https://www.youtube.com/watch?v=cuoNzXFLitc)\n- [How to create a Minimal, Complete, and Verifiable example](https://stackoverflow.com/help/mcve)\n- [Reporting a Next.js bug](https://github.com/vercel/next.js/blob/canary/.github/ISSUE_TEMPLATE/1.bug_report.yml)\n- [Next.js Triaging issues](https://github.com/vercel/next.js/blob/canary/contributing/repository/triaging.md)\n",
      "labels": [
        "invalid link"
      ],
      "created_at": "2026-02-05T17:07:15Z",
      "closed_at": "2026-02-05T17:07:31Z",
      "url": "https://github.com/vercel/next.js/issues/89542",
      "comments_count": 1
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 89251,
      "title": "Unresolvable warnings on latest 15.x: \"Mismatching @next/swc version\"",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://codesandbox.io/p/devbox/determined-sunset-6wnnlk?workspaceId=ws_R3K1XVqknhzjCa9nBvnFtd\n\n### To Reproduce\n\n1. Install a version of Next 15.5 greater than 15.5.7 (15.5.11 is the latest)\n2. Run `next dev`\n3. See a warning in the terminal output like this: \"Mismatching @next/swc version, detected: 15.5.7 while Next.js is on 15.5.11. Please ensure these match\"\n\nThe CodeSandbox link should produce this output automatically.\n\n### Current vs. Expected behavior\n\nI expect no warning when on the latest versions of the same major+minor version of `next` and `@next/swc`. The latest `@next/swc` versions for 15.5.x are 15.5.7, so there isn't a way to upgrade on 15.5.x to resolve the warning. `next@15.5.11` lists `15.5.7` as the optional dependency version for the `@next/swc` packages.\n\nI believe package managers will install whatever versions are listed in the optional dependencies for `next`, so maybe this warning code could be removed altogether.\n\n### Provide environment information\n\n```bash\nOperating System:\n  Platform: linux\n  Arch: arm64\n  Version: #1 SMP Fri Nov 21 10:33:45 UTC 2025\n  Available memory (MB): 7837\n  Available CPU cores: 12\nBinaries:\n  Node: 22.21.1\n  npm: 10.9.4\n  Yarn: 1.22.22\n  pnpm: N/A\nRelevant Packages:\n  next: 15.5.11 // An outdated version detected (latest is 16.1.6), upgrade is highly recommended!\n  eslint-config-next: N/A\n  react: 19.2.3\n  react-dom: 19.2.3\n  typescript: 5.9.3\nNext.js Config:\n  output: N/A\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nSWC\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext dev (local)\n\n### Additional context\n\nCode that produces the warning:\nhttps://github.com/vercel/next.js/blob/f1a047fd80944ac722ab6e481e1ec077378d31fc/packages/next/src/build/swc/index.ts#L131-L139",
      "solution": "same\n\n_Edit by maintainer bot: Comment was **automatically** minimized because it was considered unhelpful. (If you think this was by mistake, let us know). Please only comment if it adds context to the issue. If you want to express that you have the same problem, use the upvote \ud83d\udc4d on the issue description or subscribe to the issue for updates. Thanks!_\n\n---\n\nThis issue has been fixed in version 15.5.12, and the warning is no longer present.\nThanks to the Next.js team! \ud83d\ude4c",
      "labels": [
        "SWC",
        "linear: next"
      ],
      "created_at": "2026-01-29T16:36:27Z",
      "closed_at": "2026-02-05T15:42:49Z",
      "url": "https://github.com/vercel/next.js/issues/89251",
      "comments_count": 20
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 89240,
      "title": "'AGENTS.md outperforms skills in our agent evals' - compressed index is not being created in AGENTS.md",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/spatulatom/reproduction-app\n\n### To Reproduce\n\nThis blog posts https://vercel.com/blog/agents-md-outperforms-skills-in-our-agent-evals recommends follwing steps:\nOne command sets this up for your Next.js project:\n\n1. npx @next/codemod@canary agents-md\n\nThis functionality is part of the official [@next/codemod package](https://github.com/vercel/next.js/pull/88961).\n\nThis command does three things:\n\n1.1. Detects your Next.js version\n\n1.2. Downloads matching documentation to .next-docs/\n\n1.3. Injects the compressed index into your AGENTS.md - COMPRESSED INDEX IS NOT BEING INJECTED INTO AGENTS.MD\n\n### Current vs. Expected behavior\n\ncurrently  AGENTS.md looks liekie this: \n<!-- NEXT-AGENTS-MD-START -->[Next.js Docs Index]|root: ./.next-docs|STOP. What you remember about Next.js is WRONG for this project. Always search docs and read before any task.|If docs missing, run this command first: npx @next/codemod agents-md --output AGENTS.md<!-- NEXT-AGENTS-MD-END -->\n\nwhat was exepcted that compressed relevant next.js documentation would have been injected in AGENTS.md\n\n### Provide environment information\n\n```bash\nOperating System:\n  Platform: win32\n  Arch: x64\n  Version: Windows 11 Home\n  Available memory (MB): 32181\n  Available CPU cores: 16\nBinaries:\n  Node: 24.13.0\n  npm: 11.6.2\n  Yarn: N/A\n  pnpm: 10.15.0\nRelevant Packages:\n  next: 16.1.4\n  eslint-config-next: N/A\n  react: 19.2.4\n  react-dom: 19.2.4\n  typescript: 5.9.3\nNext.js Config:\n  output: N/A\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nNot sure\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext dev (local)\n\n### Additional context\n\nI tested my reproduction in vscode ",
      "solution": "@tuzzy08 thanks for confirming the issue that on Windows it dose create AGENTS.md but it never places compressed docs into it. i have no access to mac though\n\n---\n\nI can confirm that merged pull request #89319 by @gaojude is allowing now Windows OS users follow this blog posts https://vercel.com/blog/agents-md-outperforms-skills-in-our-agent-evals and create succesfully AGENTS.md WITH compressed next.js  docs. so the discussed issue here has been resolved, thanks",
      "labels": [
        "Upstream"
      ],
      "created_at": "2026-01-29T12:56:09Z",
      "closed_at": "2026-02-03T18:21:07Z",
      "url": "https://github.com/vercel/next.js/issues/89240",
      "comments_count": 4
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 45048,
      "title": "Rewrites don't pass response.body down when using Content-Type text/event-stream",
      "problem": "### Verify canary release\n\n- [X] I verified that the issue exists in the latest Next.js canary release\n\n### Provide environment information\n\nyarn run v1.22.19\r\n$ /opt/app/node_modules/.bin/next info\r\n\r\n    Operating System:\r\n      Platform: linux\r\n      Arch: arm64\r\n      Version: #1 SMP PREEMPT Tue Sep 13 07:51:32 UTC 2022\r\n    Binaries:\r\n      Node: 18.12.1\r\n      npm: 8.19.2\r\n      Yarn: 1.22.19\r\n      pnpm: N/A\r\n    Relevant packages:\r\n      next: 12.3.1\r\n      eslint-config-next: N/A\r\n      react: 18.2.0\r\n      react-dom: 18.2.0\r\n\r\nwarn  - Latest canary version not detected, detected: \"12.3.1\", newest: \"13.1.3-canary.5\".\r\n        Please try the latest canary version (`npm install next@canary`) to confirm the issue still exists before creating a new issue.\r\n        Read more - https://nextjs.org/docs/messages/opening-an-issue\r\nDone in 2.70s.\n\n### Which area(s) of Next.js are affected? (leave empty if unsure)\n\n_No response_\n\n### Link to the code that reproduces this issue\n\nhttps://not-possible.com\n\n### To Reproduce\n\n1. Get endpoint that responds with Content-Type: \"text/event-stream\" and writes to the body over time\r\n2. next.config.js.rewrites to the endpoint\r\n3. in `useEffect` on any page\r\n```\r\nconst xhr = new XMLHttpRequest()\r\nxhr.open(\"GET\", `/rewritten`)\r\nconst interval = setInterval(() => {\r\n  console.log(xhr.responseText)\r\n})\r\n```\n\n### Describe the Bug\n\n`xhr.responseText` is missing\n\n### Expected Behavior\n\n`xhr.responseText` is present\n\n### Which browser are you using? (if relevant)\n\nn/a\n\n### How are you deploying your application? (if relevant)\n\nn/a",
      "solution": "Also ran into the same issue\n\n_Edit by maintainer bot: Comment was **automatically** minimized because it was considered unhelpful. (If you think this was by mistake, let us know). Please only comment if it adds context to the issue. If you want to express that you have the same problem, use the upvote \ud83d\udc4d on the issue description or subscribe to the issue for updates. Thanks!_\n\n---\n\nI tested this on Next.js 16.1.6 (Turbopack) with a simple SSE API route rewritten via `next.config.js` rewrites. Both in dev and production, the `text/event-stream` response streams correctly through the rewrite \u2014 headers (`Content-Type`, `Connection: keep-alive`) are preserved and the body is delivered incrementally as expected.\n\nThis seems to have been fixed at some point since the original report (which was on 12.3.1).",
      "labels": [
        "bug"
      ],
      "created_at": "2023-01-19T15:28:30Z",
      "closed_at": "2026-02-05T09:36:31Z",
      "url": "https://github.com/vercel/next.js/issues/45048",
      "comments_count": 9
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 67169,
      "title": " Unable to parse config export in source file error in middleware.js example",
      "problem": "### Verify canary release\n\n- [X] I verified that the issue exists in the latest Next.js canary release\n\n### Provide environment information\n\n```bash\nNodeJs v18.17.1 Local Environment\n```\n\n\n### Which example does this report relate to?\n\nwith-strict-csp\n\n### What browser are you using? (if relevant)\n\n_No response_\n\n### How are you deploying your application? (if relevant)\n\n_No response_\n\n### Describe the Bug\n\nI encountered an issue with the middleware.js example provided in the Next.js documentation. When trying to use the config export with a complex matcher configuration, I received the following error:\r\n\r\n```\r\nUnable to parse config export in source file\r\nThe exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compile-time.\r\n```\r\nexport const config = {\r\n  matcher: [\r\n    {\r\n      source: \"/((?!api|_next/static|_next/image|favicon.ico).*)\",\r\n      missing: [\r\n        { type: \"header\", key: \"next-router-prefetch\" },\r\n        { type: \"header\", key: \"purpose\", value: \"prefetch\" },\r\n      ],\r\n    },\r\n  ],\r\n};\r\n\r\nThe syntax that works well is this:\r\nexport const config = {\r\n  matcher: [\r\n    '/((?!api|_next/static|_next/image|favicon.ico).*)',\r\n  ],\r\n};\r\n![image](https://github.com/vercel/next.js/assets/36580834/31dec743-8e94-4993-93af-c08c7ad97842)\r\n\n\n### Expected Behavior\n\n![image](https://github.com/vercel/next.js/assets/36580834/462f2122-dc44-4974-86c5-aaf5f0f35fce)\r\n\n\n### To Reproduce\n\njust run the with-strict-csp example",
      "solution": "I can't repro on v16, nor v15.\n\nIt is also not clear what the issue was initially. No reproduction repository was required back then.\n\nUsing any of the configs shared on this thread works fine in v16 and v15. \n\nv14 is End Of Life: https://nextjs.org/support-policy#unsupported-versions - so I will go ahead and close the issue.",
      "labels": [
        "examples"
      ],
      "created_at": "2024-06-25T02:18:25Z",
      "closed_at": "2026-02-05T09:26:59Z",
      "url": "https://github.com/vercel/next.js/issues/67169",
      "comments_count": 3
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 70008,
      "title": "Middleware: Cannot export config after declaration in export list format",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://codesandbox.io/p/devbox/nameless-hill-dt239f\n\n### To Reproduce\n\nLink to codesandbox is provided and it shows the issue as well.\n\n### Current vs. Expected behavior\n\nThe config object from `middleware.ts` is loaded correctly in case 1 (exporting during declaration) while it is not loaded correctly in case 2 (exporting after decleration):\r\n\r\n```\r\nimport { NextRequest, NextResponse } from 'next/server';\r\n\r\n// case 1 -> works as expected\r\nexport const middleware = (request: NextRequest) => {\r\n\tconsole.log(request.nextUrl.pathname);\r\n\r\n\treturn NextResponse.next();\r\n};\r\n\r\nexport const config = {\r\n\tmatcher: '/about',\r\n};\r\n\r\n// case 2 -> does not work as expected\r\nconst middleware = (request: NextRequest) => {\r\n\tconsole.log(request.nextUrl.pathname);\r\n\r\n\treturn NextResponse.next();\r\n};\r\n\r\nconst config = {\r\n\tmatcher: '/about',\r\n};\r\n\r\nexport { middleware, config };\r\n```\r\n\r\nWhen the config is loaded only paths with `/about` should be printed in the console. If not loaded, all paths are printed in the console. Also no error is reported (I am not sure if an error should be reported).\n\n### Provide environment information\n\n```bash\nSandbox is running on next.js version 15.0.0-canary-148. I encountered the same issue locally on 15.0.0-rc.0. Used app router in both instances.\n```\n\n\n### Which area(s) are affected? (Select all that apply)\n\nMiddleware\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext dev (local)\n\n### Additional context\n\nExporting the config object while declaring it solves the issue for me, but the difference should only be one of syntax and not functionality. The `middleware` function is unaffected by this issue, only the `config` object is affected.\r\n\r\nThis issue might be related but I'm not completely sure: https://github.com/vercel/next.js/issues/67169\r\nIn the case that it is feel free to remove this issue/combine both issues.",
      "solution": "Hi, revisiting this issue... AFAIK, the `config` export in middleware/proxy has to be **statically analyzable**.  \n\nIn practice, this means Next.js reads it at build/dev time by parsing the file\u2019s AST, without executing the module.\n\nThat config is **not** consumed by the runtime middleware function. Instead, it\u2019s used earlier by the build pipeline to generate routing manifests (like `matcher`) and decide how the middleware bundle should be produced and deployed. Because of that, Next needs to extract the config directly from syntax, not from evaluated JavaScript.\n\nA declaration like this works:\n\n```ts\nexport const config = {\n  matcher: ['/about'],\n}\n````\n\nbecause the compiler can find an `export const config = /* literally the configuration */` node and read its value directly from the AST.\n\nBut these patterns don\u2019t work:\n\n```ts\nconst config2 = { matcher: ['/about'] }\nexport const config = config2\n\nconst config = { matcher: ['/about'] }\nexport { config }\n```\n\nbecause now the extractor would need to resolve identifiers or follow re-exports. That requires symbol resolution (and potentially executing code), which the middleware/proxy config parser intentionally avoids. If it can\u2019t match the exact export shape, it behaves as if no config was provided. In other words, a smarter parser could handle these two examples. But once you open that door, users will reasonably expect lots of JS to work, and the maintenance/correctness surface area explodes.\n\nThis strict approach keeps things simple, fast and predictable, which is why\n\n> Middleware `config` must be declared inline as `export const config = \u2026` because Next reads it via static AST analysis during build/dev, not via the runtime module system.\n\nI'll close this issue for now, given that:\n\n- this is working as expected\n- the issue has been stable for 500+ days\n\nHowever I've created an internal ticket to bring this explanation to the mainline docs.\n\n---\n\n- On AST: https://github.com/jamiebuilds/babel-handbook/blob/master/translations/en/plugin-handbook.md\n- A playground https://astexplorer.net/\n\n",
      "labels": [
        "bug",
        "Middleware"
      ],
      "created_at": "2024-09-12T03:44:46Z",
      "closed_at": "2026-02-05T09:06:45Z",
      "url": "https://github.com/vercel/next.js/issues/70008",
      "comments_count": 1
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 89519,
      "title": "Support non-standard properties in `robots.ts` file",
      "problem": "### Link to the code that reproduces this issue\n\nN/A\n\n### To Reproduce\n\n1. Create robots.txt file\n2.  Put there this context  and open localhost:3000/robots.txt you will see request rate directive is ignored\n```import type { MetadataRoute } from 'next'\n\nexport default function robots(): MetadataRoute.Robots {\n\tconst rules: MetadataRoute.Robots['rules'] = [\n\t\t{\n\t\t\tallow: '/',\n\t\t\tuserAgent: '*',\n\t\t},\n\t\t{\n\t\t\t'allow': '/',\n\t\t\t// See: https://o-seznam.cz/napoveda/vyhledavani/en/crawling-control/#request-rate-directive\n\t\t\t// @ts-expect-error not a standard property\n\t\t\t'Request-Rate': '10/1m', // ~10k requests/day\n\t\t\t'userAgent': 'SeznamBot',\n\t\t},\n\t]\n\n\treturn {\n\t\trules,\n\t}\n}\n```\n\n### Current vs. Expected behavior\n\nI would expect this non-standard directive to still be propagated. Obviously, it does not make sense to include it in the types, so I\u2019m fine with suppressing the error, but it should still propagate to robots.txt. It is very useful when Seznam bots crawl the site too aggressively.\n\nAt this point, I\u2019m forced to create a static robots.txt file without using the TS version.\n\nSee: https://o-seznam.cz/napoveda/vyhledavani/en/crawling-control/#request-rate-directive\n\n### Provide environment information\n\n```bash\nN/A\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nNot sure\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext build (local)\n\n### Additional context\n\n_No response_",
      "solution": "We could not detect a valid reproduction link. **Make sure to follow the bug report template carefully.**\n\n### Why was this issue closed?\n\nTo be able to investigate, we need access to a reproduction to identify what triggered the issue. We need a link to a **public** GitHub repository ([template for App Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template), [template for Pages Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template-pages)), but you can also use these templates: [CodeSandbox: App Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template) or [CodeSandbox: Pages Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template-pages).\n\nThe bug template that you filled out has a section called \"Link to the code that reproduces this issue\", which is where you should provide the link to the reproduction.\n\n- If you did not provide a link or the link you provided is not valid, we will close the issue.\n- If you provide a link to a private repository, we will close the issue.\n- If you provide a link to a repository but not in the correct section, we will close the issue.\n\n### What should I do?\n\nDepending on the reason the issue was closed, you can do the following:\n\n- If you did not provide a link, please open a new issue with a link to a reproduction.\n- If you provided a link to a private repository, please open a new issue with a link to a public repository.\n- If you provided a link to a repository but not in the correct section, please open a new issue with a link to a reproduction in the correct section.\n\n**In general, assume that we should not go through a lengthy onboarding process at your company code only to be able to verify an issue.**\n\n### My repository is private and cannot make it public\n\nIn most cases, a private repo will not be a sufficient **minimal reproduction**, as this codebase might contain a lot of unrelated parts that would make our investigation take longer. Please do **not** make it public. Instead, create a new repository using the templates above, adding the relevant code to reproduce the issue. Common things to look out for:\n\n- Remove any code that is not related to the issue. (pages, API routes, components, etc.)\n- Remove any dependencies that are not related to the issue.\n- Remove any third-party service that would require us to sign up for an account to reproduce the issue.\n- Remove any environment variables that are not related to the issue.\n- Remove private packages that we do not have access to.\n- If the issue is not related to a monorepo specifically, try to reproduce the issue without a complex monorepo setup\n\n### I did not open this issue, but it is relevant to me, what can I do to help?\n\nAnyone experiencing the same issue is welcome to provide a minimal reproduction following the above steps by opening a new issue.\n\n### I think my reproduction is good enough, why aren't you looking into it quickly?\n\nWe look into every Next.js issue and constantly monitor open issues for new comments.\n\nHowever, sometimes we might miss one or two due to the popularity/high traffic of the repository. We apologize, and kindly ask you to refrain from tagging core maintainers, as that will usually not result in increased priority.\n\nUpvoting issues to show your interest will help us prioritize and address them as quickly as possible. That said, every issue is important to us, and if an issue gets closed by accident, we encourage you to open a new one linking to the old issue and we will look into it.\n\n### Useful Resources\n\n- [How to Contribute to Open Source (Next.js)](https://www.youtube.com/watch?v=cuoNzXFLitc)\n- [How to create a Minimal, Complete, and Verifiable example](https://stackoverflow.com/help/mcve)\n- [Reporting a Next.js bug](https://github.com/vercel/next.js/blob/canary/.github/ISSUE_TEMPLATE/1.bug_report.yml)\n- [Next.js Triaging issues](https://github.com/vercel/next.js/blob/canary/contributing/repository/triaging.md)\n",
      "labels": [
        "invalid link"
      ],
      "created_at": "2026-02-05T07:54:00Z",
      "closed_at": "2026-02-05T07:54:16Z",
      "url": "https://github.com/vercel/next.js/issues/89519",
      "comments_count": 1
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 74842,
      "title": "[Turbopack] SharedWorker TypeScript scripts not compiled",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/christoph-pflueger/turbopack-shared-worker-reproduction\n\n### To Reproduce\n\n1. `npm run dev`\n2. Visit `http://localhost:3000/`\n3. Open the console. In Chrome you will notice the error `Failed to fetch a worker script.`.\n4. Open chrome://inspect/#workers in Chrome. You will notice that only the SharedWorker with the `.js` script is running and has logged `Hello world!`.\n\n### Current vs. Expected behavior\n\nI expect to be able to run a SharedWorker with a TypeScript script.\n\nCurrently, the SharedWorker fails to run.\n\nIt appears that the script is not even being compiled: `.next/static/media/script.93184b23.ts`\n\n### Provide environment information\n\n```bash\nOperating System:\n  Platform: linux\n  Arch: x64\n  Version: #1 SMP Tue Nov 5 00:21:55 UTC 2024\n  Available memory (MB): 15948\n  Available CPU cores: 12\nBinaries:\n  Node: 22.13.0\n  npm: 10.9.2\n  Yarn: N/A\n  pnpm: 9.15.3\nRelevant Packages:\n  next: 15.2.0-canary.7 // Latest available version is detected (15.2.0-canary.7).\n  eslint-config-next: N/A\n  react: 19.0.0\n  react-dom: 19.0.0\n  typescript: 5.7.3\nNext.js Config:\n  output: N/A\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nTurbopack, TypeScript\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext dev (local)\n\n### Additional context\n\n_No response_",
      "solution": "Any update?\n\n_Edit by maintainer bot: Comment was **automatically** minimized because it was considered unhelpful. (If you think this was by mistake, let us know). Please only comment if it adds context to the issue. If you want to express that you have the same problem, use the upvote \ud83d\udc4d on the issue description or subscribe to the issue for updates. Thanks!_",
      "labels": [
        "TypeScript",
        "Turbopack",
        "linear: turbopack",
        "locked"
      ],
      "created_at": "2025-01-13T23:23:02Z",
      "closed_at": "2026-01-21T17:08:57Z",
      "url": "https://github.com/vercel/next.js/issues/74842",
      "comments_count": 8
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 55418,
      "title": "Dynamic routes for AMP pages not resolving",
      "problem": "### Link to the code that reproduces this issue or a replay of the bug\n\nhttps://github.com/rakeshtembhurne/next-router-amp-dynamic-routes\n\n### To Reproduce\n\n1. Start the application in development \r\n2. Create a route `stories/[...slug].tsx`\r\n3. Enable AMP by adding `export const config = { amp: true}`\r\n4. Import and use the router with `import { useRouter } from \"next/router\"` and `const router = useRouter();`\r\n5. Visit any dynamic URL like `http://localhost:3000/stories/some-random-story`\r\n6. Use console.log(router) or try to render `router` as JSON `<pre>{JSON.stringify(router, null, 2)}</pre>`, in both cases value of `slug` is missing in `query`\r\n7. Quick full sample code: \r\n\r\n```js\r\nimport { useRouter } from \"next/router\"\r\n\r\nexport const config = { amp: true}\r\n\r\nexport default function Home() {\r\n    const router = useRouter();\r\n    \r\n    const { asPath, pathname, route, query } = router;\r\n    console.log({ asPath, pathname, route, query });\r\n\r\n    return (<>\r\n        <h1>Output</h1>\r\n        <pre>{JSON.stringify({ asPath, pathname, route, query }, null, 2)}</pre>\r\n    </>)\r\n}\r\n ```\n\n### Current vs. Expected behavior\n\n## Expected Output\r\nWhen visiting url: `http://localhost:3000/stories/inside/some-random-story`, we should get value `router.query.slug === \"some-random-story\"` whether amp is enabled or not.\r\n\r\n## Current Behavior\r\n### Case \"Without exporting config variable (normally)\"\r\n\r\n**`console.log({ asPath, pathname, route, query });`**\r\n```bash\r\n{\r\n  asPath: '/stories/inside/[...slug]',\r\n  pathname: '/stories/inside/[...slug]',\r\n  route: '/stories/inside/[...slug]',\r\n  query: {}\r\n}\r\n```\r\n**`<pre>{JSON.stringify({ asPath, pathname, route, query }, null, 2)}</pre>`**\r\n```json\r\n{\r\n  \"asPath\": \"/stories/inside/some-random-story\",\r\n  \"pathname\": \"/stories/inside/[...slug]\",\r\n  \"route\": \"/stories/inside/[...slug]\",\r\n  \"query\": {\r\n    \"slug\": [\r\n      \"some-random-story\"\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\n### Case \"with `export const config = { amp: true}`\"\r\n\r\n**`console.log({ asPath, pathname, route, query });`**\r\n```bash\r\n{\r\n  asPath: '/stories/inside/[...slug]',\r\n  pathname: '/stories/inside/[...slug]',\r\n  route: '/stories/inside/[...slug]',\r\n  query: {}\r\n}\r\n```\r\n**`<pre>{JSON.stringify({ asPath, pathname, route, query }, null, 2)}</pre>`**\r\n```json\r\n{\r\n  \"asPath\": \"/stories/inside/[...slug]\",\r\n  \"pathname\": \"/stories/inside/[...slug]\",\r\n  \"route\": \"/stories/inside/[...slug]\",\r\n  \"query\": {}\r\n}\r\n```\r\n\r\n\r\n### Case \"with `export const config = { amp:'hybrid'}` AND `?amp=1` at the end of URL\"\r\n\r\n**`console.log({ asPath, pathname, route, query });`**\r\n```bash\r\n{\r\n  asPath: '/stories/inside/[...slug]',\r\n  pathname: '/stories/inside/[...slug]',\r\n  route: '/stories/inside/[...slug]',\r\n  query: { amp: '1' }\r\n}\r\n```\r\n**`<pre>{JSON.stringify({ asPath, pathname, route, query }, null, 2)}</pre>`**\r\n```json\r\n{\r\n  \"asPath\": \"/stories/inside/[...slug]\",\r\n  \"pathname\": \"/stories/inside/[...slug]\",\r\n  \"route\": \"/stories/inside/[...slug]\",\r\n  \"query\": {\r\n    \"amp\": \"1\"\r\n  }\r\n}\r\n```\n\n### Verify canary release\n\n- [X] I verified that the issue exists in the latest Next.js canary release\n\n### Provide environment information\n\n```bash\nOperating System:\r\n      Platform: darwin\r\n      Arch: arm64\r\n      Version: Darwin Kernel Version 22.6.0: Wed Jul  5 22:22:05 PDT 2023; root:xnu-8796.141.3~6/RELEASE_ARM64_T6000\r\n    Binaries:\r\n      Node: 18.17.1\r\n      npm: 9.6.7\r\n      Yarn: 1.22.19\r\n      pnpm: 8.5.1\r\n    Relevant Packages:\r\n      next: 13.4.20-canary.30\r\n      eslint-config-next: N/A\r\n      react: 18.2.0\r\n      react-dom: 18.2.0\r\n      typescript: 5.1.3\r\n    Next.js Config:\r\n      output: N/A\n```\n\n\n### Which area(s) are affected? (Select all that apply)\n\nApp Router, Routing (next/router, next/navigation, next/link)\n\n### Additional context\n\n_No response_",
      "solution": "## Update\r\n\r\nI had to add extra code when in AMP mode, to make it work like normal:\r\n```js\r\nexport async function getServerSideProps() {\r\n  return { props: {} }\r\n}\r\n```\r\n\r\nSolves my issue, but unsure if it is the right solution for the problem. Either it should be documented or a fix still needs to be worked out. \r\n\n\n---\n\nHi, AMP support has been removed from v16.\n\nThis issue was reported for v13 too, which is end-of-life. I'll close the issue for now.",
      "labels": [
        "bug",
        "Linking and Navigating"
      ],
      "created_at": "2023-09-15T02:47:24Z",
      "closed_at": "2026-02-04T23:17:59Z",
      "url": "https://github.com/vercel/next.js/issues/55418",
      "comments_count": 3
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 89430,
      "title": "Docs:",
      "problem": "### What is the documentation issue?\n\nimport React, { useState, useMemo } from \u2018react\u2019;\nimport { Search, FileText, TrendingUp, ClipboardList } from \u2018lucide-react\u2019;\n\nconst EquipmentInventoryTables = () => {\nconst [activeTab, setActiveTab] = useState(\u2018inventory\u2019);\nconst [searchTerm, setSearchTerm] = useState(\u2019\u2019);\nconst [yearFilter, setYearFilter] = useState(\u2018all\u2019);\n\nconst equipmentData = [\n{ id: 1, type: \u201cMOB\u201d, description: \u201c\u1792\u17bb\u1784\u178a\u17c2\u1780\u201d, year: 1998, user: \u201c\u1780\u17d2\u1793\u17bb\u1784\u179f\u17d2\u179a\u17bb\u1780\u201d, quantity: 1, price: \u201c1,200,000\u201d, status: \u201c\u1781\u17bc\u1785\u201d },\n{ id: 2, type: \u201cMOB\u201d, description: \u201c\u178f\u17bb\u179f\u17b7\u179f\u17d2\u179f\u17e4\u1794\u1784\u17d2\u1780\u17bb\u1799(\u1788\u17be)\u201d, year: 1999, user: \u201c\u1780\u17d2\u1793\u17bb\u1784\u179f\u17d2\u179a\u17bb\u1780\u201d, quantity: 9, price: \u201c900,000\u201d, status: \u201c\u1781\u17bc\u1785\u201d },\n{ id: 3, type: \u201cMOB\u201d, description: \u201c\u1780\u17c5\u17a2\u17b8\u1782\u17d2\u179a\u17bc\u201d, year: 2000, user: \u201c\u1780\u17d2\u1793\u17bb\u1784\u179f\u17d2\u179a\u17bb\u1780\u201d, quantity: 6, price: \u201c640,000\u201d, status: \u201c\u17a2\u1793\u17cb\u201d },\n{ id: 4, type: \u201cMOB\u201d, description: \u201c\u178f\u17bb\u179f\u17b7\u179f\u17d2\u179f\u17e2\u1794\u1784\u17d2\u1780\u17bb\u1799(\u1788\u17be)\u201d, year: 2000, user: \u201c\u1780\u17d2\u1793\u17bb\u1784\u179f\u17d2\u179a\u17bb\u1780\u201d, quantity: 85, price: \u201c7,480,000\u201d, status: \u201c\u1781\u17bc\u1785\u201d },\n{ id: 5, type: \u201cMOB\u201d, description: \u201c\u1780\u17d2\u178a\u17b6\u179a\u1781\u17c0\u1793\u178a\u17b8\u179f\u201d, year: 2000, user: \u201c\u1780\u17d2\u1793\u17bb\u1784\u179f\u17d2\u179a\u17bb\u1780\u201d, quantity: 4, price: \u201c400,000\u201d, status: \u201c\u17a2\u1793\u17cb\u201d },\n{ id: 6, type: \u201cMOB\u201d, description: \u201c\u1780\u17d2\u178a\u17b6\u179a\u1781\u17c0\u1793\u17a0\u17d2\u179c\u17ba\u178f\u201d, year: 2000, user: \u201c\u1780\u17d2\u1793\u17bb\u1784\u179f\u17d2\u179a\u17bb\u1780\u201d, quantity: 1, price: \u201c250,000\u201d, status: \u201c\u1781\u17bc\u1785\u201d },\n{ id: 7, type: \u201cMOB\u201d, description: \u201c\u1791\u17bc\u1780\u1789\u17d2\u1785\u1780\u17cb\u201d, year: 2004, user: \u201c\u1780\u17d2\u1793\u17bb\u1784\u179f\u17d2\u179a\u17bb\u1780\u201d, quantity: 1, price: \u201c250,000\u201d, status: \u201c\u1781\u17bc\u1785\u201d },\n{ id: 8, type: \u201cMOB\u201d, description: \u201c\u17a0\u17b7\u1794\u178a\u17c2\u1780\u201d, year: 2010, user: \u201c\u1780\u17d2\u1793\u17bb\u1784\u179f\u17d2\u179a\u17bb\u1780\u201d, quantity: 1, price: \u201c200,000\u201d, status: \u201c\u17a2\u1793\u17cb\u201d },\n{ id: 9, type: \u201cMOB\u201d, description: \u201c\u1792\u17d2\u1793\u17be\u178a\u17b6\u1780\u17cb\u179f\u17c0\u179c\u1797\u17c5\u1792\u17c6\u201d, year: 2010, user: \u201c\u1780\u17d2\u1793\u17bb\u1784\u179f\u17d2\u179a\u17bb\u1780\u201d, quantity: 1, price: \u201c200,000\u201d, status: \u201c\u1798\u1792\u17d2\u1799\u1798\u201d },\n{ id: 10, type: \u201cMOB\u201d, description: \u201c\u178f\u17bb\u179f\u17b7\u179f\u17d2\u179f\u17e2\u1794\u1784\u17d2\u1780\u17bb\u1799(\u1788\u17be)\u201d, year: 2013, user: \u201c\u1780\u17d2\u1793\u17bb\u1784\u179f\u17d2\u179a\u17bb\u1780\u201d, quantity: 20, price: \u201c1,400,000\u201d, status: \u201c\u17a2\u1793\u17cb\u201d },\n{ id: 11, type: \u201cMOB\u201d, description: \u201c\u178f\u17bb\u179f\u17b7\u179f\u17d2\u179f\u17e2\u1794\u1784\u17d2\u1780\u17bb\u1799(\u1788\u17be)\u201d, year: 2013, user: \u201c\u1780\u17d2\u1793\u17bb\u1784\u179f\u17d2\u179a\u17bb\u1780\u201d, quantity: 95, price: \u201c40,679,000\u201d, status: \u201c\u17a2\u1793\u17cb\u201d },\n{ id: 12, type: \u201cMOB\u201d, description: \u201c\u178f\u17bb\u17a2\u17b6\u1793(\u178a\u17c2\u1780)\u201d, year: 2013, user: \u201c\u1780\u17d2\u1793\u17bb\u1784\u179f\u17d2\u179a\u17bb\u1780\u201d, quantity: 5, price: \u201c2,848,000\u201d, status: \u201c\u1798\u1792\u17d2\u1799\u1798\u201d },\n{ id: 13, type: \u201cMOB\u201d, description: \u201c\u1792\u17d2\u1793\u17be\u1798\u17bb\u1781\u1798\u17bd\u1799\u201d, year: 2013, user: \u201c\u1780\u17d2\u1793\u17bb\u1784\u179f\u17d2\u179a\u17bb\u1780\u201d, quantity: 2, price: \u201c2,377,400\u201d, status: \u201c\u1798\u1792\u17d2\u1799\u1798\u201d },\n{ id: 14, type: \u201cMOB\u201d, description: \u201c\u1780\u17d2\u178a\u17b6\u179a\u1781\u17c0\u1793\u1796\u17d0\u178f\u17cc\u1798\u17b6\u1793\u201d, year: 2016, user: \u201cSOF\u201d, quantity: 2, price: \u201c160,000\u201d, status: \u201c\u1798\u1792\u17d2\u1799\u1798\u201d },\n{ id: 15, type: \u201cMOB\u201d, description: \u201c\u1780\u17c5\u17a2\u17b8\u1787\u17d0\u179a\u1792\u17bb\u1793\u178f\u17bc\u1785\u201d, year: 2017, user: \u201cW.V.S\u201d, quantity: 31, price: \u201c192,000\u201d, status: \u201c\u1781\u17bc\u1785\u201d },\n{ id: 16, type: \u201cMOB\u201d, description: \u201c\u178f\u17bb\u17a2\u17b6\u1793(\u178a\u17c2\u1780)\u201d, year: 2017, user: \u201c\u1798\u1793\u17d2\u1791\u17b8\u179a\u17a2\u1794\u17cb\u179a\u17c6\u201d, quantity: 12, price: \u201c576,000\u201d, status: \u201c\u1781\u17bc\u1785\u201d },\n{ id: 17, type: \u201cMBU\u201d, description: \u201c\u1798\u17c9\u17b6\u179f\u17ca\u17b8\u1793\u1796\u17d2\u179a\u17b8\u1793 Epson L360\u201d, year: 2018, user: \u201c\u179f\u1794\u17d2\u1794\u17bb\u179f\u1787\u1793\u201d, quantity: 1, price: \u201c1,200,000\u201d, status: \u201c\u1781\u17bc\u1785\u201d },\n{ id: 18, type: \u201cMBU\u201d, description: \u201c\u1780\u1784\u17d2\u17a0\u17b6\u179a\u1797\u17d2\u1787\u17b6\u1794\u17cb\u1796\u17b7\u178a\u17b6\u1793\u201d, year: 2018, user: \u201c\u1798\u1793\u17d2\u1791\u17b8\u179a\u17a2\u1794\u17cb\u179a\u17c6\u201d, quantity: 12, price: \u201c960,000\u201d, status: \u201c\u17a2\u1793\u17cb\u201d },\n{ id: 19, type: \u201cMOB\u201d, description: \u201c\u178f\u17bb\u179f\u1798\u17d2\u179a\u17b6\u1794\u17cb\u1782\u17d2\u179a\u17bc\u201d, year: 2018, user: \u201c\u1798\u1793\u17d2\u1791\u17b8\u179a\u17a2\u1794\u17cb\u179a\u17c6\u201d, quantity: 6, price: \u201c1,800,000\u201d, status: \u201c\u1798\u1792\u17d2\u1799\u1798\u201d },\n{ id: 20, type: \u201cMOB\u201d, description: \u201c\u178f\u17bb\u179f\u17b7\u179f\u17d2\u179f\u17e2\u1794\u1784\u17d2\u1780\u17bb\u1799(\u178a\u17c2\u1780)\u201d, year: 2018, user: \u201c\u1780\u17d2\u179a\u179f\u17bd\u1784\u17a2\u1794\u17cb\u179a\u17c6\u201d, quantity: 145, price: \u201c60,900,000\u201d, status: \u201c\u1798\u1792\u17d2\u1799\u1798\u201d },\n{ id: 21, type: \u201cMOB\u201d, description: \u201c\u1791\u17c4\u1784\u179a\u17c6\u17a2\u17b7\u179b\u201d, year: 2018, user: \u201c\u1798\u1793\u17d2\u1791\u17b8\u179a\u17a2\u1794\u17cb\u179a\u17c6\u201d, quantity: 3, price: \u201c1,080,000\u201d, status: \u201c\u1798\u1792\u17d2\u1799\u1798\u201d },\n{ id: 22, type: \u201cMOB\u201d, description: \u201c\u1780\u17d2\u178a\u17b6\u179a\u1781\u17c0\u1793\u17a0\u17d2\u179c\u17ba\u178f\u201d, year: 2018, user: \u201c\u1798\u1793\u17d2\u1791\u17b8\u179a\u17a2\u1794\u17cb\u179a\u17c6\u201d, quantity: 18, price: \u201c4,500,000\u201d, status: \u201c\u1798\u1792\u17d2\u1799\u1798\u201d },\n{ id: 23, type: \u201cMIN\u201d, description: \u201c\u1780\u17bb\u17c6\u1796\u17d2\u1799\u17bc\u1791\u17d0\u179a\u1799\u17bd\u179a\u178a\u17c3 Asus\u201d, year: 2019, user: \u201c\u1780\u17d2\u179a\u179f\u17bd\u1784\u17a2\u1794\u17cb\u179a\u17c6\u201d, quantity: 1, price: \u201c2,713,500\u201d, status: \u201c\u1781\u17bc\u1785\u201d },\n{ id: 24, type: \u201cMOB\u201d, description: \u201c\u1780\u17d2\u178a\u17b6\u179a\u179a\u17c6\u17a2\u17b7\u179b\u201d, year: 2019, user: \u201cW.V.S\u201d, quantity: 4, price: \u201c1,600,000\u201d, status: \u201c\u1798\u1792\u17d2\u1799\u1798\u201d },\n{ id: 25, type: \u201cMOB\u201d, description: \u201c\u1787\u178e\u17d2\u178a\u17be\u179a\u179f\u17d2\u179c\u17b6\u201d, year: 2019, user: \u201cW.V.S\u201d, quantity: 3, price: \u201c1,584,000\u201d, status: \u201c\u1798\u1792\u17d2\u1799\u1798\u201d },\n{ id: 26, type: \u201cMOB\u201d, description: \u201c\u1798\u17c9\u17b6\u179f\u17ca\u17b8\u1793\u1796\u17d2\u179a\u17b8\u1793HP\u201d, year: 2019, user: \u201c\u1780\u17d2\u179a\u179f\u17bd\u1784\u17a2\u1794\u17cb\u179a\u17c6\u201d, quantity: 1, price: \u201c1,336,500\u201d, status: \u201c\u1781\u17bc\u1785\u201d },\n{ id: 27, type: \u201cMOB\u201d, description: \u201c\u178f\u17bb\u178f\u17bf\u201d, year: 2020, user: \u201cW.V.S\u201d, quantity: 6, price: \u201c100,000\u201d, status: \u201c\u179b\u17d2\u17a2\u201d },\n{ id: 28, type: \u201cMOB\u201d, description: \u201c\u178a\u17c2\u1780\u178f\u17c4\u1784\u201d, year: 2020, user: \u201cW.V.S\u201d, quantity: 3, price: \u201c200,000\u201d, status: \u201c\u1798\u1792\u17d2\u1799\u1798\u201d },\n{ id: 29, type: \u201cMOB\u201d, description: \u201c\u1791\u17bc\u178a\u17b6\u1780\u17cb\u1780\u1789\u17d2\u1785\u1780\u17cb\u178a\u17b6\u1780\u17cb\u17af\u1780\u179f\u17b6\u179a\u201d, year: 2020, user: \u201cW.V.S\u201d, quantity: 1, price: \u201c750,000\u201d, status: \u201c\u179b\u17d2\u17a2\u201d },\n{ id: 30, type: \u201cMOB\u201d, description: \u201c\u1792\u17d2\u1793\u17be\u178a\u17b6\u1780\u17cb\u179f\u17c0\u179c\u1797\u17c5\u178f\u17b6\u1798\u1790\u17d2\u1793\u17b6\u1780\u17cb\u201d, year: 2020, user: \u201cW.V.S\u201d, quantity: 1, price: \u201c60,000\u201d, status: \u201c\u179b\u17d2\u17a2\u201d },\n{ id: 31, type: \u201cMOB\u201d, description: \u201c\u1792\u17d2\u1793\u17be\u178a\u17b6\u1780\u17cb\u179f\u17c0\u179c\u1797\u17c5\u178f\u17bc\u1785\u201d, year: 2020, user: \u201cW.V.S\u201d, quantity: 5, price: \u201c40,000\u201d, status: \u201c\u179b\u17d2\u17a2\u201d },\n{ id: 32, type: \u201cMOB\u201d, description: \u201c\u1792\u17d2\u1793\u17be\u1798\u17bb\u1781\u1796\u17b8\u179a\u201d, year: 2020, user: \u201cW.V.S\u201d, quantity: 3, price: \u201c70,000\u201d, status: \u201c\u179b\u17d2\u17a2\u201d },\n{ id: 33, type: \u201cMOB\u201d, description: \u201c\u1792\u17d2\u1793\u17be\u1798\u17bb\u1781\u1798\u17bd\u1799\u201d, year: 2020, user: \u201cW.V.S\u201d, quantity: 4, price: \u201c50,000\u201d, status: \u201c\u179b\u17d2\u17a2\u201d },\n{ id: 34, type: \u201cMIN\u201d, description: \u201c\u1780\u17bb\u17c6\u1796\u17d2\u1799\u17bc\u1791\u17d0\u179a\u1799\u17bd\u179a\u178a\u17c3 Acer\u201d, year: 2021, user: \u201cSOF\u201d, quantity: 1, price: \u201c2,800,000\u201d, status: \u201c\u1798\u1792\u17d2\u1799\u1798\u201d },\n{ id: 35, type: \u201cMOB\u201d, description: \u201c\u1780\u17c5\u17a2\u17b8\u1782\u17d2\u179a\u17bc\u201d, year: 2021, user: \u201c\u1798\u1793\u17d2\u1791\u17b8\u179a\u17a2\u1794\u17cb\u179a\u17c6\u201d, quantity: 6, price: \u201c720,000\u201d, status: \u201c\u1798\u1792\u17d2\u1799\u1798\u201d },\n{ id: 36, type: \u201cMOB\u201d, description: \u201c\u178f\u17bb\u1782\u17d2\u179a\u17bc(\u178a\u17c2\u1780)\u201d, year: 2021, user: \u201c\u1798\u1793\u17d2\u1791\u17b8\u179a\u17a2\u1794\u17cb\u179a\u17c6\u201d, quantity: 6, price: \u201c100,000\u201d, status: \u201c\u179b\u17d2\u17a2\u201d },\n{ id: 37, type: \u201cMOB\u201d, description: \u201c\u178f\u17bb\u179c\u17c2\u1784\u201d, year: 2021, user: \u201c\u1798\u1793\u17d2\u1791\u17b8\u179a\u17a2\u1794\u17cb\u179a\u17c6\u201d, quantity: 3, price: \u201c200,000\u201d, status: \u201c\u1798\u1792\u17d2\u1799\u1798\u201d },\n{ id: 38, type: \u201cMBU\u201d, description: \u201c\u1798\u17c9\u17b6\u179f\u17ca\u17b8\u1793\u1796\u17d2\u179a\u17b8\u1793 Epson L3210\u201d, year: 2022, user: \u201cSOF\u201d, quantity: 1, price: \u201c800,000\u201d, status: \u201c\u1781\u17bc\u1785\u201d },\n{ id: 39, type: \u201cMBU\u201d, description: \u201c\u1780\u1784\u17d2\u17a0\u17b6\u179a\u1797\u17d2\u1787\u17b6\u1794\u17cb\u1787\u1789\u17d2\u1787\u17b6\u17c6\u1784\u201d, year: 2023, user: \u201cSOF\u201d, quantity: 4, price: \u201c250,000\u201d, status: \u201c\u1798\u1792\u17d2\u1799\u1798\u201d },\n{ id: 40, type: \u201cMBU\u201d, description: \u201c\u1780\u17bb\u17c6\u1796\u17d2\u1799\u17bc\u1791\u17d0\u179a\u179b\u17be\u178f\u17bb Desktop\u201d, year: 2023, user: \u201c\u179f\u1794\u17d2\u1794\u17bb\u179f\u1787\u1793\u201d, quantity: 1, price: \u201c1,800,000\u201d, status: \u201c\u1798\u1792\u17d2\u1799\u1798\u201d },\n{ id: 41, type: \u201cMBU\u201d, description: \u201c\u1798\u17c9\u17b6\u179f\u17ca\u17b8\u1793\u1796\u17d2\u179a\u17b8\u1793 Canon\u201d, year: 2023, user: \u201c\u179f\u1794\u17d2\u1794\u17bb\u179f\u1787\u1793\u201d, quantity: 1, price: \u201c1,100,000\u201d, status: \u201c\u1798\u1792\u17d2\u1799\u1798\u201d },\n{ id: 42, type: \u201cMBU\u201d, description: \u201c\u1798\u17c9\u17bc\u1791\u17d0\u179a\u1794\u17bc\u1798\u1791\u17b9\u1780\u201d, year: 2023, user: \u201cSOF\u201d, quantity: 1, price: \u201c400,000\u201d, status: \u201c\u1798\u1792\u17d2\u1799\u1798\u201d },\n{ id: 43, type: \u201cMBU\u201d, description: \u201cSpeaker\u201d, year: 2024, user: \u201c\u179f\u1794\u17d2\u1794\u17bb\u179a\u179f\u1787\u1793\u201d, quantity: 1, price: \u201c800,000\u201d, status: \u201c\u1798\u1792\u17d2\u1799\u1798\u201d },\n{ id: 44, type: \u201cMBU\u201d, description: \u201cMicro sound (\u178f\u17bc\u1785)\u201d, year: 2024, user: \u201cSOF\u201d, quantity: 1, price: \u201c60,000\u201d, status: \u201c\u1798\u1792\u17d2\u1799\u1798\u201d },\n{ id: 45, type: \u201cMBU\u201d, description: \u201c\u1780\u1784\u17d2\u17a0\u17b6\u179a\u1797\u17d2\u1787\u17b6\u1794\u17cb\u1787\u1789\u17d2\u1787\u17b6\u17c6\u1784(\u1792\u17c6)\u201d, year: 2024, user: \u201cSOF\u201d, quantity: 1, price: \u201c240,000\u201d, status: \u201c\u1781\u17bc\u1785\u201d },\n{ id: 46, type: \u201cMIN\u201d, description: \u201c\u1798\u17c9\u17b6\u179f\u17ca\u17b8\u1793\u1796\u17d2\u179a\u17b8\u1793 Color\u201d, year: 2025, user: \u201c\u179f\u1794\u17d2\u1794\u17bb\u179a\u179f\u1787\u1793\u201d, quantity: 1, price: \u201c1,000,000\u201d, status: \u201c\u1781\u17bc\u1785\u201d },\n{ id: 47, type: \u201cMIN\u201d, description: \u201c\u1798\u17c9\u17b6\u179f\u17ca\u17b8\u1793\u1796\u17d2\u179a\u17b8\u1793 Black white\u201d, year: 2025, user: \u201c\u179f\u1794\u17d2\u1794\u17bb\u179a\u179f\u1787\u1793\u201d, quantity: 1, price: \u201c1,500,000\u201d, status: \u201c\u1798\u1792\u17d2\u1799\u1798\u201d },\n{ id: 48, type: \u201cMBU\u201d, description: \u201c\u1798\u17c9\u17bc\u1791\u17d0\u179a\u1780\u17b6\u178f\u17cb\u1795\u17d2\u1780\u17b6\u201d, year: 2025, user: \u201cSOF\u201d, quantity: 1, price: \u201c500,000\u201d, status: \u201c\u1798\u1792\u17d2\u1799\u1798\u201d }\n];\n\nconst filteredData = useMemo(() => {\nreturn equipmentData.filter(item => {\nconst matchesSearch = item.description.toLowerCase().includes(searchTerm.toLowerCase()) ||\nitem.user.toLowerCase().includes(searchTerm.toLowerCase());\nconst matchesYear = yearFilter === \u2018all\u2019 || item.year.toString() === yearFilter;\nreturn matchesSearch && matchesYear;\n});\n}, [searchTerm, yearFilter]);\n\nconst statusSummary = useMemo(() => {\nconst summary = {};\nequipmentData.forEach(item => {\nif (!summary[item.description]) {\nsummary[item.description] = { good: 0, medium: 0, weak: 0, broken: 0, total: 0 };\n}\nsummary[item.description].total += item.quantity;\nif (item.status === \u2018\u179b\u17d2\u17a2\u2019) summary[item.description].good += item.quantity;\nelse if (item.status === \u2018\u1798\u1792\u17d2\u1799\u1798\u2019) summary[item.description].medium += item.quantity;\nelse if (item.status === \u2018\u17a2\u1793\u17cb\u2019) summary[item.description].weak += item.quantity;\nelse if (item.status === \u2018\u1781\u17bc\u1785\u2019) summary[item.description].broken += item.quantity;\n});\nreturn summary;\n}, []);\n\nconst yearlyIncrease = useMemo(() => {\nconst yearly = {};\nequipmentData.forEach(item => {\nif (!yearly[item.year]) {\nyearly[item.year] = [];\n}\nyearly[item.year].push(item);\n});\nreturn yearly;\n}, []);\n\nconst years = [\u2026new Set(equipmentData.map(item => item.year))].sort((a, b) => b - a);\n\nreturn (\n<div className=\"min-h-screen bg-gradient-to-br from-blue-50 to-indigo-100 p-6\">\n<div className=\"max-w-7xl mx-auto\">\n<div className=\"bg-white rounded-lg shadow-xl p-6 mb-6\">\n<h1 className=\"text-3xl font-bold text-indigo-900 mb-2\">\u178f\u17b6\u179a\u17b6\u1784\u179f\u1798\u17d2\u1797\u17b6\u179a \u1793\u17b7\u1784\u179f\u1784\u17d2\u17a0\u17b6\u179a\u17b7\u1798</h1>\n<p className=\"text-gray-600\">\u1794\u17d2\u179a\u1796\u17d0\u1793\u17d2\u1792\u1782\u17d2\u179a\u1794\u17cb\u1782\u17d2\u179a\u1784\u179f\u1798\u17d2\u1797\u17b6\u179a\u1794\u179a\u17b7\u1780\u17d2\u1781\u17b6\u179a</p>\n</div>\n\n```\n    <div className=\"bg-white rounded-lg shadow-lg mb-6\">\n      <div className=\"flex border-b\">\n        <button\n          onClick={() => setActiveTab('inventory')}\n          className={`flex items-center gap-2 px-6 py-4 font-semibold transition ${\n            activeTab === 'inventory'\n              ? 'bg-indigo-600 text-white border-b-2 border-indigo-600'\n              : 'text-gray-600 hover:bg-gray-50'\n          }`}\n        >\n          <FileText size={20} />\n          \u178f\u17b6\u179a\u17b6\u1784\u179f\u1798\u17d2\u1797\u17b6\u179a \u1793\u17b7\u1784\u179f\u1784\u17d2\u17a0\u17b6\u179a\u17b7\u1798\n        </button>\n        <button\n          onClick={() => setActiveTab('yearly')}\n          className={`flex items-center gap-2 px-6 py-4 font-semibold transition ${\n            activeTab === 'yearly'\n              ? 'bg-indigo-600 text-white border-b-2 border-indigo-600'\n              : 'text-gray-600 hover:bg-gray-50'\n          }`}\n        >\n          <TrendingUp size={20} />\n          \u178f\u17b6\u179a\u17b6\u1784\u179f\u1798\u17d2\u1797\u17b6\u179a\u1780\u17be\u1793\u1780\u17d2\u1793\u17bb\u1784\u1786\u17d2\u1793\u17b6\u17c6\n        </button>\n        <button\n          onClick={() => setActiveTab('tracking')}\n          className={`flex items-center gap-2 px-6 py-4 font-semibold transition ${\n            activeTab === 'tracking'\n              ? 'bg-indigo-600 text-white border-b-2 border-indigo-600'\n              : 'text-gray-600 hover:bg-gray-50'\n          }`}\n        >\n          <ClipboardList size={20} />\n          \u178f\u17b6\u179a\u17b6\u1784\u178f\u17b6\u1798\u178a\u17b6\u1793\n        </button>\n      </div>\n\n      <div className=\"p-6\">\n        {activeTab === 'inventory' && (\n          <div>\n            <div className=\"mb-4 flex gap-4\">\n              <div className=\"flex-1 relative\">\n                <Search className=\"absolute left-3 top-3 text-gray-400\" size={20} />\n                <input\n                  type=\"text\"\n                  placeholder=\"\u179f\u17d2\u179c\u17c2\u1784\u179a\u1780...\"\n                  value={searchTerm}\n                  onChange={(e) => setSearchTerm(e.target.value)}\n                  className=\"w-full pl-10 pr-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-indigo-500 focus:border-transparent\"\n                />\n              </div>\n            <div className=\"mt-6 bg-gradient-to-r from-purple-50 to-indigo-50 rounded-lg p-6 border-2 border-purple-200\">\n              <h3 className=\"text-xl font-bold text-purple-900 mb-4\">\ud83d\udcca \u179f\u1784\u17d2\u1781\u17c1\u1794\u178f\u17b6\u179a\u17b6\u1784\u178f\u17b6\u1798\u178a\u17b6\u1793</h3>\n              <div className=\"grid grid-cols-2 md:grid-cols-5 gap-4\">\n                <div className=\"bg-white rounded-lg p-4 shadow\">\n                  <div className=\"text-gray-600 text-sm\">\u1794\u17d2\u179a\u1797\u17c1\u1791\u179f\u179a\u17bb\u1794</div>\n                  <div className=\"text-2xl font-bold text-indigo-600\">\n                    {Object.keys(statusSummary).length}\n                  </div>\n                </div>\n                <div className=\"bg-green-50 rounded-lg p-4 shadow border-2 border-green-200\">\n                  <div className=\"text-green-700 text-sm font-semibold\">\u179b\u17d2\u17a2\u179f\u179a\u17bb\u1794</div>\n                  <div className=\"text-2xl font-bold text-green-700\">\n                    {Object.values(statusSummary).reduce((sum, c) => sum + c.good, 0)}\n                  </div>\n                </div>\n                <div className=\"bg-yellow-50 rounded-lg p-4 shadow border-2 border-yellow-200\">\n                  <div className=\"text-yellow-700 text-sm font-semibold\">\u1798\u1792\u17d2\u1799\u1798\u179f\u179a\u17bb\u1794</div>\n                  <div className=\"text-2xl font-bold text-yellow-700\">\n                    {Object.values(statusSummary).reduce((sum, c) => sum + c.medium, 0)}\n                  </div>\n                </div>\n                <div className=\"bg-orange-50 rounded-lg p-4 shadow border-2 border-orange-200\">\n                  <div className=\"text-orange-700 text-sm font-semibold\">\u17a2\u1793\u17cb\u179f\u179a\u17bb\u1794</div>\n                  <div className=\"text-2xl font-bold text-orange-700\">\n                    {Object.values(statusSummary).reduce((sum, c) => sum + c.weak, 0)}\n                  </div>\n                </div>\n                <div className=\"bg-red-50 rounded-lg p-4 shadow border-2 border-red-200\">\n                  <div className=\"text-red-700 text-sm font-semibold\">\u1781\u17bc\u1785\u179f\u179a\u17bb\u1794</div>\n                  <div className=\"text-2xl font-bold text-red-700\">\n                    {Object.values(statusSummary).reduce((sum, c) => sum + c.broken, 0)}\n                  </div>\n                </div>\n              </div>\n              <div className=\"mt-4 bg-white rounded-lg p-4 shadow\">\n                <div className=\"text-gray-600 text-sm mb-2\">\u1794\u179a\u17b7\u1798\u17b6\u178e\u179f\u179a\u17bb\u1794\u1791\u17b6\u17c6\u1784\u17a2\u179f\u17cb</div>\n                <div className=\"text-3xl font-bold text-purple-600\">\n                  {Object.values(statusSummary).reduce((sum, c) => sum + c.total, 0)} \u1782\u17d2\u179a\u17bf\u1784\n                </div>\n              </div>\n            </div>\n              <select\n                value={yearFilter}\n                onChange={(e) => setYearFilter(e.target.value)}\n                className=\"px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-indigo-500 focus:border-transparent\"\n              >\n                <option value=\"all\">\u1786\u17d2\u1793\u17b6\u17c6\u1791\u17b6\u17c6\u1784\u17a2\u179f\u17cb</option>\n                {years.map(year => (\n                  <option key={year} value={year}>{year}</option>\n                ))}\n              </select>\n            </div>\n\n            <div className=\"overflow-x-auto rounded-lg border border-gray-200\">\n              <table className=\"w-full\">\n                <thead className=\"bg-indigo-600 text-white\">\n                  <tr>\n                    <th className=\"px-4 py-3 text-left\">\u179b.\u179a</th>\n                    <th className=\"px-4 py-3 text-left\">\u178f\u17b6\u1798\u1794\u17d2\u179a\u1797\u17c1\u1791</th>\n                    <th className=\"px-4 py-3 text-left\">\u1794\u179a\u17b7\u1799\u17b6\u1799</th>\n                    <th className=\"px-4 py-3 text-left\">\u1794\u17d2\u179a\u17be\u1794\u17d2\u179a\u17b6\u179f\u17cb\u1796\u17b8\u1786\u17d2\u1793\u17b6\u17c6</th>\n                    <th className=\"px-4 py-3 text-left\">\u1788\u17d2\u1798\u17c4\u17c7\u17a2\u17d2\u1793\u1780\u1794\u17d2\u179a\u17be</th>\n                    <th className=\"px-4 py-3 text-right\">\u1794\u179a\u17b7\u1798\u17b6\u178e</th>\n                    <th className=\"px-4 py-3 text-right\">\u178f\u1798\u17d2\u179b\u17c3(\u179a\u17c0\u179b)</th>\n                    <th className=\"px-4 py-3 text-center\">\u179f\u17d2\u1790\u17b6\u1793\u1797\u17b6\u1796</th>\n                  </tr>\n                </thead>\n                <tbody className=\"divide-y divide-gray-200\">\n                  {filteredData.map((item, index) => (\n                    <tr key={item.id} className=\"hover:bg-gray-50 transition\">\n                      <td className=\"px-4 py-3\">{index + 1}</td>\n                      <td className=\"px-4 py-3\">\n                        <span className=\"px-2 py-1 bg-blue-100 text-blue-800 rounded text-sm font-semibold\">\n                          {item.type}\n                        </span>\n                      </td>\n                      <td className=\"px-4 py-3\">{item.description}</td>\n                      <td className=\"px-4 py-3\">{item.year}</td>\n                      <td className=\"px-4 py-3\">{item.user}</td>\n                      <td className=\"px-4 py-3 text-right font-semibold\">{item.quantity}</td>\n                      <td className=\"px-4 py-3 text-right\">{item.price}</td>\n                      <td className=\"px-4 py-3 text-center\">\n                        <span className={`px-3 py-1 rounded-full text-sm font-semibold ${\n                          item.status === '\u179b\u17d2\u17a2' ? 'bg-green-100 text-green-800' :\n                          item.status === '\u1798\u1792\u17d2\u1799\u1798' ? 'bg-yellow-100 text-yellow-800' :\n                          item.status === '\u17a2\u1793\u17cb' ? 'bg-orange-100 text-orange-800' :\n                          'bg-red-100 text-red-800'\n                        }`}>\n                          {item.status}\n                        </span>\n                      </td>\n                    </tr>\n                  ))}\n                </tbody>\n              </table>\n            </div>\n            <div className=\"mt-6 bg-gradient-to-r from-indigo-50 to-blue-50 rounded-lg p-6 border-2 border-indigo-200\">\n              <h3 className=\"text-xl font-bold text-indigo-900 mb-4\">\ud83d\udcca \u179f\u1784\u17d2\u1781\u17c1\u1794\u179f\u17d2\u179c\u17d0\u1799\u1794\u17d2\u179a\u179c\u178f\u17d2\u178f\u17b7</h3>\n              <div className=\"grid grid-cols-2 md:grid-cols-4 gap-4\">\n                <div className=\"bg-white rounded-lg p-4 shadow\">\n                  <div className=\"text-gray-600 text-sm\">\u1785\u17c6\u1793\u17bd\u1793\u1794\u17d2\u179a\u1797\u17c1\u1791</div>\n                  <div className=\"text-2xl font-bold text-indigo-600\">{filteredData.length}</div>\n                </div>\n                <div className=\"bg-white rounded-lg p-4 shadow\">\n                  <div className=\"text-gray-600 text-sm\">\u1794\u179a\u17b7\u1798\u17b6\u178e\u179f\u179a\u17bb\u1794</div>\n                  <div className=\"text-2xl font-bold text-green-600\">\n                    {filteredData.reduce((sum, item) => sum + item.quantity, 0)}\n                  </div>\n                </div>\n                <div className=\"bg-white rounded-lg p-4 shadow\">\n                  <div className=\"text-gray-600 text-sm\">\u178f\u1798\u17d2\u179b\u17c3\u179f\u179a\u17bb\u1794 (\u179a\u17c0\u179b)</div>\n                  <div className=\"text-2xl font-bold text-purple-600\">\n                    {filteredData.reduce((sum, item) => {\n                      const price = parseInt(item.price.replace(/,/g, ''));\n                      return sum + (price * item.quantity);\n                    }, 0).toLocaleString()}\n                  </div>\n                </div>\n                <div className=\"bg-white rounded-lg p-4 shadow\">\n                  <div className=\"text-gray-600 text-sm\">\u178f\u1798\u17d2\u179b\u17c3\u1798\u1792\u17d2\u1799\u1798</div>\n                  <div className=\"text-2xl font-bold text-orange-600\">\n                    {Math.round(filteredData.reduce((sum, item) => {\n                      const price = parseInt(item.price.replace(/,/g, ''));\n                      return sum + price;\n                    }, 0) / filteredData.length).toLocaleString()}\n                  </div>\n                </div>\n              </div>\n              <div className=\"mt-4 grid grid-cols-2 md:grid-cols-4 gap-4\">\n                <div className=\"bg-green-50 rounded-lg p-3 border border-green-200\">\n                  <div className=\"text-green-700 text-sm font-semibold\">\u179f\u17d2\u1790\u17b6\u1793\u1797\u17b6\u1796: \u179b\u17d2\u17a2</div>\n                  <div className=\"text-xl font-bold text-green-800\">\n                    {filteredData.filter(i => i.status === '\u179b\u17d2\u17a2').reduce((sum, i) => sum + i.quantity, 0)}\n                  </div>\n                </div>\n                <div className=\"bg-yellow-50 rounded-lg p-3 border border-yellow-200\">\n                  <div className=\"text-yellow-700 text-sm font-semibold\">\u179f\u17d2\u1790\u17b6\u1793\u1797\u17b6\u1796: \u1798\u1792\u17d2\u1799\u1798</div>\n                  <div className=\"text-xl font-bold text-yellow-800\">\n                    {filteredData.filter(i => i.status === '\u1798\u1792\u17d2\u1799\u1798').reduce((sum, i) => sum + i.quantity, 0)}\n                  </div>\n                </div>\n                <div className=\"bg-orange-50 rounded-lg p-3 border border-orange-200\">\n                  <div className=\"text-orange-700 text-sm font-semibold\">\u179f\u17d2\u1790\u17b6\u1793\u1797\u17b6\u1796: \u17a2\u1793\u17cb</div>\n                  <div className=\"text-xl font-bold text-orange-800\">\n                    {filteredData.filter(i => i.status === '\u17a2\u1793\u17cb').reduce((sum, i) => sum + i.quantity, 0)}\n                  </div>\n                </div>\n                <div className=\"bg-red-50 rounded-lg p-3 border border-red-200\">\n                  <div className=\"text-red-700 text-sm font-semibold\">\u179f\u17d2\u1790\u17b6\u1793\u1797\u17b6\u1796: \u1781\u17bc\u1785</div>\n                  <div className=\"text-xl font-bold text-red-800\">\n                    {filteredData.filter(i => i.status === '\u1781\u17bc\u1785').reduce((sum, i) => sum + i.quantity, 0)}\n                  </div>\n                </div>\n              </div>\n            </div>\n          </div>\n        )}\n\n        {activeTab === 'yearly' && (\n          <div>\n            <h2 className=\"text-2xl font-bold text-indigo-900 mb-4\">\u179f\u1798\u17d2\u1797\u17b6\u179a\u1780\u17be\u1793\u1780\u17d2\u1793\u17bb\u1784\u1786\u17d2\u1793\u17b6\u17c6</h2>\n            <div className=\"space-y-6\">\n              {years.map(year => (\n                <div key={year} className=\"bg-gray-50 rounded-lg p-4\">\n                  <h3 className=\"text-xl font-bold text-indigo-800 mb-3\">\u1786\u17d2\u1793\u17b6\u17c6 {year}</h3>\n                  <div className=\"overflow-x-auto rounded-lg border border-gray-200\">\n                    <table className=\"w-full bg-white\">\n                      <thead className=\"bg-indigo-500 text-white\">\n                        <tr>\n                          <th className=\"px-4 py-2 text-left\">\u179b.\u179a</th>\n                          <th className=\"px-4 py-2 text-left\">\u1794\u179a\u17b7\u1799\u17b6\u1799</th>\n                          <th className=\"px-4 py-2 text-left\">\u1794\u17d2\u179a\u1797\u17c1\u1791</th>\n                          <th className=\"px-4 py-2 text-right\">\u1794\u179a\u17b7\u1798\u17b6\u178e</th>\n                          <th className=\"px-4 py-2 text-right\">\u178f\u1798\u17d2\u179b\u17c3(\u179a\u17c0\u179b)</th>\n                          <th className=\"px-4 py-2 text-center\">\u179f\u17d2\u1790\u17b6\u1793\u1797\u17b6\u1796</th>\n                        </tr>\n                      </thead>\n                      <tbody className=\"divide-y divide-gray-200\">\n                        {yearlyIncrease[year].map((item, idx) => (\n                          <tr key={item.id} className=\"hover:bg-gray-50\">\n                            <td className=\"px-4 py-2\">{idx + 1}</td>\n                            <td className=\"px-4 py-2\">{item.description}</td>\n                            <td className=\"px-4 py-2\">\n                              <span className=\"px-2 py-1 bg-purple-100 text-purple-800 rounded text-sm\">\n                                {item.type}\n                              </span>\n                            </td>\n                            <td className=\"px-4 py-2 text-right font-semibold\">{item.quantity}</td>\n                            <td className=\"px-4 py-2 text-right\">{item.price}</td>\n                            <td className=\"px-4 py-2 text-center\">\n                              <span className={`px-2 py-1 rounded-full text-sm ${\n                                item.status === '\u179b\u17d2\u17a2' ? 'bg-green-100 text-green-800' :\n                                item.status === '\u1798\u1792\u17d2\u1799\u1798' ? 'bg-yellow-100 text-yellow-800' :\n                                item.status === '\u17a2\u1793\u17cb' ? 'bg-orange-100 text-orange-800' :\n                                'bg-red-100 text-red-800'\n                              }`}>\n                                {item.status}\n                              </span>\n                            </td>\n                          </tr>\n                        ))}\n                      </tbody>\n                    </table>\n                  </div>\n                  <div className=\"mt-2 text-sm text-gray-600\">\n                    \u179f\u179a\u17bb\u1794: {yearlyIncrease[year].length} \u1794\u17d2\u179a\u1797\u17c1\u1791 | \n                    \u1794\u179a\u17b7\u1798\u17b6\u178e\u179f\u179a\u17bb\u1794: {yearlyIncrease[year].reduce((sum, item) => sum + item.quantity, 0)}\n                  </div>\n                  <div className=\"mt-4 bg-white rounded-lg p-4 border-2 border-indigo-200\">\n                    <div className=\"text-sm font-semibold text-indigo-900 mb-2\">\ud83d\udcca \u179f\u1784\u17d2\u1781\u17c1\u1794\u1786\u17d2\u1793\u17b6\u17c6 {year}</div>\n                    <div className=\"grid grid-cols-3 gap-3\">\n                      <div className=\"bg-blue-50 rounded p-2\">\n                        <div className=\"text-xs text-gray-600\">\u178f\u1798\u17d2\u179b\u17c3\u179f\u179a\u17bb\u1794</div>\n                        <div className=\"text-lg font-bold text-blue-700\">\n                          {yearlyIncrease[year].reduce((sum, item) => {\n                            const price = parseInt(item.price.replace(/,/g, ''));\n                            return sum + (price * item.quantity);\n                          }, 0).toLocaleString()} \u17db\n                        </div>\n                      </div>\n                      <div className=\"bg-green-50 rounded p-2\">\n                        <div className=\"text-xs text-gray-600\">\u179b\u17d2\u17a2</div>\n                        <div className=\"text-lg font-bold text-green-700\">\n                          {yearlyIncrease[year].filter(i => i.status === '\u179b\u17d2\u17a2').reduce((sum, i) => sum + i.quantity, 0)}\n                        </div>\n                      </div>\n                      <div className=\"bg-red-50 rounded p-2\">\n                        <div className=\"text-xs text-gray-600\">\u1781\u17bc\u1785</div>\n                        <div className=\"text-lg font-bold text-red-700\">\n                          {yearlyIncrease[year].filter(i => i.status === '\u1781\u17bc\u1785').reduce((sum, i) => sum + i.quantity, 0)}\n                        </div>\n                      </div>\n                    </div>\n                  </div>\n                </div>\n              ))}\n            </div>\n          </div>\n        )}\n\n        {activeTab === 'tracking' && (\n          <div>\n            <h2 className=\"text-2xl font-bold text-indigo-900 mb-4\">\u178f\u17b6\u179a\u17b6\u1784\u178f\u17b6\u1798\u178a\u17b6\u1793\u179f\u17d2\u1790\u17b6\u1793\u1797\u17b6\u1796\u179f\u1798\u17d2\u1797\u17b6\u179a\u17c8</h2>\n            <div className=\"overflow-x-auto rounded-lg border border-gray-200\">\n              <table className=\"w-full bg-white\">\n                <thead className=\"bg-indigo-600 text-white\">\n                  <tr>\n                    <th className=\"px-4 py-3 text-left\" rowSpan={2}>\u179b.\u179a</th>\n                    <th className=\"px-4 py-3 text-left\" rowSpan={2}>\u1788\u17d2\u1798\u17c4\u17c7\u179f\u1798\u17d2\u1797\u17b6\u179a\u1794\u179a\u17b7\u1780\u17d2\u1781\u17b6\u179a</th>\n                    <th className=\"px-4 py-3 text-center\" colSpan={5}>\u179f\u17d2\u1790\u17b6\u1793\u1797\u17b6\u1796\u179f\u1798\u17d2\u1797\u17b6\u179a\u17c8</th>\n                    <th className=\"px-4 py-3 text-center\" colSpan={2}>\u1780\u17b6\u179a\u1794\u17d2\u179a\u17be\u1794\u17d2\u179a\u17b6\u179f\u17cb</th>\n                    <th className=\"px-4 py-3 text-center\" rowSpan={2}>\u179f\u17d2\u1790\u17b6\u1793\u1797\u17b6\u1796</th>\n                  </tr>\n                  <tr>\n                    <th className=\"px-4 py-2 text-center bg-green-600\">\u179b\u17d2\u17a2</th>\n                    <th className=\"px-4 py-2 text-center bg-yellow-600\">\u1798\u1792\u17d2\u1799\u1798</th>\n                    <th className=\"px-4 py-2 text-center bg-orange-600\">\u17a2\u1793\u17cb</th>\n                    <th className=\"px-4 py-2 text-center bg-red-600\">\u1781\u17bc\u1785</th>\n                    <th className=\"px-4 py-2 text-center bg-indigo-700\">\u179f\u179a\u17bb\u1794</th>\n                    <th className=\"px-4 py-2 text-center bg-blue-600\">\u179b\u17be\u179f</th>\n                    <th className=\"px-4 py-2 text-center bg-purple-600\">\u1781\u17d2\u179c\u17c7</th>\n                  </tr>\n                </thead>\n                <tbody className=\"divide-y divide-gray-200\">\n                  {Object.entries(statusSummary).map(([desc, counts], idx) => {\n                    const statusText = counts.broken > 0 ? '\u1781\u17bc\u1785' :\n                                      counts.weak > 0 ? '\u17a2\u1793\u17cb' :\n                                      counts.medium > 0 ? '\u1798\u1792\u17d2\u1799\u1798' : '\u179b\u17d2\u17a2';\n                    return (\n                      <tr key={desc} className=\"hover:bg-gray-50\">\n                        <td className=\"px-4 py-3\">{idx + 1}</td>\n                        <td className=\"px-4 py-3 font-semibold\">{desc}</td>\n                        <td className=\"px-4 py-3 text-center bg-green-50\">{counts.good}</td>\n                        <td className=\"px-4 py-3 text-center bg-yellow-50\">{counts.medium}</td>\n                        <td className=\"px-4 py-3 text-center bg-orange-50\">{counts.weak}</td>\n                        <td className=\"px-4 py-3 text-center bg-red-50\">{counts.broken}</td>\n                        <td className=\"px-4 py-3 text-center bg-indigo-50 font-bold\">{counts.total}</td>\n                        <td className=\"px-4 py-3 text-center\">-</td>\n                        <td className=\"px-4 py-3 text-center\">-</td>\n                        <td className=\"px-4 py-3 text-center\">\n                          <span className={`px-2 py-1 rounded-full text-sm ${\n                            statusText === '\u1781\u17bc\u1785' ? 'bg-red-100 text-red-800' :\n                            statusText === '\u17a2\u1793\u17cb' ? 'bg-orange-100 text-orange-800' :\n                            statusText === '\u1798\u1792\u17d2\u1799\u1798' ? 'bg-yellow-100 text-yellow-800' :\n                            'bg-green-100 text-green-800'\n                          }`}>\n                            {statusText}\n                          </span>\n                        </td>\n                      </tr>\n                    );\n                  })}\n                </tbody>\n              </table>\n            </div>\n          </div>\n        )}\n      </div>\n    </div>\n  </div>\n</div>\n```\n\n);\n};\n\nexport default EquipmentInventoryTables;\n\n### Is there any context that might help us understand?\n\nreact\n\n### Does the docs page already exist? Please link to it.\n\n_No response_",
      "solution": "It is not clear what the issue is here, so I'll close. Feel free to comment back with more info or open a Discussion if you have some questions.",
      "labels": [],
      "created_at": "2026-02-03T10:14:19Z",
      "closed_at": "2026-02-04T00:11:25Z",
      "url": "https://github.com/vercel/next.js/issues/89430",
      "comments_count": 2
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 59950,
      "title": "`ResolvedMetadata` is incompatible with `Metadata`",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/Arctomachine/resolved-metadata-bug\n\n### To Reproduce\n\n1. Run build, check console for error\r\n2. Optionally run dev to verify that everything works and it is only type error\n\n### Current vs. Expected behavior\n\nWhen inheriting metadata from parent, build will fail with error because `ResolvedMetadata` is not compatible with `Metadata`. Should be able to return parent metadata without errors.\r\nAlso need to ensure that it is possible to do it at any given level, from root `Metadata` to its leaf props.\n\n### Verify canary release\n\n- [X] I verified that the issue exists in the latest Next.js canary release\n\n### Provide environment information\n\n```bash\nOperating System:\r\n  Platform: win32\r\n  Arch: x64\r\n  Version: Windows 10 Pro\r\nBinaries:\r\n  Node: 20.2.0\r\n  npm: N/A\r\n  Yarn: N/A\r\n  pnpm: N/A\r\nRelevant Packages:\r\n  next: 14.0.5-canary.27\r\n  eslint-config-next: N/A\r\n  react: 18.2.0\r\n  react-dom: 18.2.0\r\n  typescript: 5.1.3\r\nNext.js Config:\r\n  output: N/A\n```\n\n\n### Which area(s) are affected? (Select all that apply)\n\nMetadata (metadata, generateMetadata, next/head), TypeScript (plugin, built-in types)\n\n### Additional context\n\nIdentical problem already happened in past in different props of `Metadata`:\r\n#49406\r\n#49316\r\n\r\nError happens inside .next folder, which is regenerated on subsequent build runs, so it is impossible to workaround it with @ts-ignore or by commenting out problematic section\r\n```\r\n.next/types/app/test/page.ts:32:15\r\nType error: Type 'OmitWithTag<ResolvingMetadata, keyof PageProps, \"generateMetadata\">' does not satisfy the constraint '{ [x: string]: never; }'.\r\n  Property 'then' is incompatible with index signature.\r\n    Type '<TResult1 = ResolvedMetadata, TResult2 = never>(onfulfilled?: (value: ResolvedMetadata) => TResult1 | PromiseLike<TResult1>, onrejected?: (reason: any) => TResult2 | PromiseLike<...>) => Promise<...>' is not assignable\r\n to type 'never'.\r\n\r\n  30 | // Check the arguments and return type of the generateMetadata function\r\n  31 | if ('generateMetadata' in entry) {\r\n> 32 |   checkFields<Diff<PageProps, FirstArg<MaybeField<TEntry, 'generateMetadata'>>, 'generateMetadata'>>()\r\n     |               ^\r\n  33 |   checkFields<Diff<ResolvingMetadata, SecondArg<MaybeField<TEntry, 'generateMetadata'>>, 'generateMetadata'>>()\r\n  34 | }\r\n  35 |\r\n```",
      "solution": "Hi, i just faced with this err. Could you share your solutions. \n\n---\n\nSame problem here. You can't do a spread of the parent metadata fields, because props don't match. I was trying to do it with openGraph and twitter fields objects, but some of the parent fields on ResolvedMetadata are typed allowing `null`, while Metadata only allows undefined.\n\nIn case it helps, for now as a workaround for my case, I've created this helper:\n\n```\nimport type { ResolvingMetadata } from 'next';\n\n// TODO: We need this for now, because there are inconsistencies\n// between the parent/children metadata fields types.\n\nasync function extractParentMetadata(parent: ResolvingMetadata) {\n  const parentMetadata = await parent;\n\n  const { url, ...parentMetadataOpenGraph } = parentMetadata.openGraph || {};\n\n  const {\n    creator,\n    creatorId,\n    description,\n    site,\n    siteId,\n    ...parentMetadataTwitter\n  } = parentMetadata.twitter || {};\n\n  return {\n    parentMetadata,\n    parentMetadataOpenGraph: {\n      ...parentMetadataOpenGraph,\n      url: url || undefined,\n    },\n    parentMetadataTwitter: {\n      ...parentMetadataTwitter,\n      creator: creator || undefined,\n      creatorId: creatorId || undefined,\n      description: description || undefined,\n      site: site || undefined,\n      siteId: siteId || undefined,\n    },\n  };\n}\n\nexport default extractParentMetadata;\n```\n\nSo then, on generateMetadata I can do...\n\n```\n  const { parentMetadataOpenGraph, parentMetadataTwitter } =\n    await extractParentMetadata(parent);\n\n  return {\n    openGraph: {\n      ...parentMetadataOpenGraph,\n      ...(imageUrl ? { images: { secureUrl: imageUrl, url: imageUrl } } : {}),\n      title,\n    },\n    title,\n    twitter: {\n      ...parentMetadataTwitter,\n      ...(imageUrl ? { images: { secureUrl: imageUrl, url: imageUrl } } : {}),\n      title,\n    },\n  };\n```\n\nWhere `imageUrl` and `title` are the new values calculated for that specific page. Is ugly as hell, I know, but for now it'll have to do the trick. At least this allows me to handle this issue in a single place: `extractParentMetadata`\n\n---\n\nIn my case, problem were lying in supplying **incorrect positional parameters count**. \n\nAs you can see in [docs](https://nextjs.org/docs/app/api-reference/functions/generate-metadata#generatemetadata-function) there are **two** positional parameters supplied: \n\n1. `params/searchParams` as Props\n2. `parent` as ResolvingMetadata\n\nBefore, I was using only one parameter `parent` - which has been taken as 1st parameter and not as the 2nd as intended. So, the very same as [you do](https://github.com/Arctomachine/resolved-metadata-bug/blob/master/app/test/page.tsx).\n\nSo changing line:\n\n`export async function generateMetadata(parent: ResolvingMetadata): Promise<Metadata> {`\n\nto:\n\n`export async function generateMetadata({}, parent: ResolvingMetadata): Promise<Metadata> {`\n\nSolved my problem \ud83d\ude4c",
      "labels": [
        "bug",
        "TypeScript",
        "locked"
      ],
      "created_at": "2023-12-26T23:00:20Z",
      "closed_at": "2026-01-20T16:26:05Z",
      "url": "https://github.com/vercel/next.js/issues/59950",
      "comments_count": 7
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 89414,
      "title": "Docs: browserDebugInfoInTerminal is no longer experimental",
      "problem": "### What is the documentation issue?\n\n\nDocs show `browserDebugInfoInTerminal` as still experimental in NextJS 16.1.6 (it is not):\nhttps://nextjs.org/docs/app/api-reference/config/next-config-js/browserDebugInfoInTerminal\n<img width=\"3024\" height=\"1716\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/fd40ee68-ab91-4a5a-879a-ef2e0be3e50c\" />\n\n\n\n### Is there any context that might help us understand?\n\nWhen implementing  on my NextJS 16 app, there is a type error \n<img width=\"1075\" height=\"113\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/13e2db81-3ac2-47bb-80bd-c2c8ecac0635\" />\n\nTurns out just needs to be moved out of experimental, and it resolves\n<img width=\"287\" height=\"46\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/6ca61fb9-5051-48e7-96d0-16cf777f9f8c\" />\n\n### Does the docs page already exist? Please link to it.\n\nhttps://nextjs.org/docs/app/api-reference/config/next-config-js/browserDebugInfoInTerminal",
      "solution": "@WillGarman got any more input/update? Otherwise I'll close the issue.\n\nFor what is worth, `browserToTerminal` (top level), will be the way to go on 16.2 - with `experimental.browserDebugInfoInTerminal` entering deprecated status.",
      "labels": [],
      "created_at": "2026-02-02T21:25:27Z",
      "closed_at": "2026-02-04T00:07:01Z",
      "url": "https://github.com/vercel/next.js/issues/89414",
      "comments_count": 4
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 89451,
      "title": "Static build at multiple environments",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/Maersk-Global/anlonepage-app\n\n### To Reproduce\n\nTo Reproduce\nCreate a Next.js 16.1.5 project using the App Router and Turbopack.\n\nDefine a .env.development file with various NEXT_PUBLIC_ variables.\n\nAdd a build:dev script to package.json: \"env-cmd -f .env.development next build\".\n\nRun npm run build:dev on a Windows/PowerShell environment.\n\nObserve that the build log explicitly states - Environments: .env.production.\n\nObserve the build crash during the Generating static pages phase with a useContext error on /_global-error.\n\n### Current vs. Expected behavior\n\nCurrent Behavior:\n\nnext build ignores the environment variables injected via env-cmd and forces the loading of .env.production.\n\nThe build fails during static generation of internal routes (/_global-error) with the following error: TypeError: Cannot read properties of null (reading 'useContext')\n\nA flood of \"unique key prop\" warnings appears for top-level tags (<html>, <head>, <meta>) which are not explicitly handled in the user's code.\n\n--\n\nExpected Behavior:\n\nnext build should respect the environment variables provided by the shell or loading tools like env-cmd.\n\nStatic generation should complete without context errors in internal error-boundary pages, especially when no custom logic is present in global-error.tsx.\n\n### Provide environment information\n\n```bash\nOperating System:\n  Platform: win32\n  Arch: x64\n  Version: Windows 11\nBinaries:\n  Node: 20.x.x\n  npm: 10.x.x\nRelevant Packages:\n  next: 16.1.5\n  react: 19.0.0\n  react-dom: 19.0.0\n  typescript: 5.x.x\nNext.js Config:\n  output: standalone (or default)\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nScript (next/script)\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext build (local)\n\n### Additional context\n\nI am a fullstack developer working in an enterprise environment (Maersk). We need to generate builds targeted at a \"Development\" server environment (using Dev APIs) but with production optimizations. Next.js 16 seems to strictly lock next build to .env.production, ignoring external injection. The useContext error on _global-error prevents the build from finishing entirely.\n\n**Error:**\nCheck the top-level render call using <head>. See https://react.dev/link/warning-keys for more information.\nError occurred prerendering page \"/_global-error\". Read more: https://nextjs.org/docs/messages/prerender-error\nTypeError: Cannot read properties of null (reading 'useContext')\n    at ignore-listed frames {\n  digest: '3200802800'\n}\nExport encountered an error on /_global-error/page: /_global-error, exiting the build.\n\u2a2f Next.js build worker exited with code: 1 and signal: null\nError: Process completed with exit code 1.",
      "solution": "We could not detect a valid reproduction link. **Make sure to follow the bug report template carefully.**\n\n### Why was this issue closed?\n\nTo be able to investigate, we need access to a reproduction to identify what triggered the issue. We need a link to a **public** GitHub repository ([template for App Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template), [template for Pages Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template-pages)), but you can also use these templates: [CodeSandbox: App Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template) or [CodeSandbox: Pages Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template-pages).\n\nThe bug template that you filled out has a section called \"Link to the code that reproduces this issue\", which is where you should provide the link to the reproduction.\n\n- If you did not provide a link or the link you provided is not valid, we will close the issue.\n- If you provide a link to a private repository, we will close the issue.\n- If you provide a link to a repository but not in the correct section, we will close the issue.\n\n### What should I do?\n\nDepending on the reason the issue was closed, you can do the following:\n\n- If you did not provide a link, please open a new issue with a link to a reproduction.\n- If you provided a link to a private repository, please open a new issue with a link to a public repository.\n- If you provided a link to a repository but not in the correct section, please open a new issue with a link to a reproduction in the correct section.\n\n**In general, assume that we should not go through a lengthy onboarding process at your company code only to be able to verify an issue.**\n\n### My repository is private and cannot make it public\n\nIn most cases, a private repo will not be a sufficient **minimal reproduction**, as this codebase might contain a lot of unrelated parts that would make our investigation take longer. Please do **not** make it public. Instead, create a new repository using the templates above, adding the relevant code to reproduce the issue. Common things to look out for:\n\n- Remove any code that is not related to the issue. (pages, API routes, components, etc.)\n- Remove any dependencies that are not related to the issue.\n- Remove any third-party service that would require us to sign up for an account to reproduce the issue.\n- Remove any environment variables that are not related to the issue.\n- Remove private packages that we do not have access to.\n- If the issue is not related to a monorepo specifically, try to reproduce the issue without a complex monorepo setup\n\n### I did not open this issue, but it is relevant to me, what can I do to help?\n\nAnyone experiencing the same issue is welcome to provide a minimal reproduction following the above steps by opening a new issue.\n\n### I think my reproduction is good enough, why aren't you looking into it quickly?\n\nWe look into every Next.js issue and constantly monitor open issues for new comments.\n\nHowever, sometimes we might miss one or two due to the popularity/high traffic of the repository. We apologize, and kindly ask you to refrain from tagging core maintainers, as that will usually not result in increased priority.\n\nUpvoting issues to show your interest will help us prioritize and address them as quickly as possible. That said, every issue is important to us, and if an issue gets closed by accident, we encourage you to open a new one linking to the old issue and we will look into it.\n\n### Useful Resources\n\n- [How to Contribute to Open Source (Next.js)](https://www.youtube.com/watch?v=cuoNzXFLitc)\n- [How to create a Minimal, Complete, and Verifiable example](https://stackoverflow.com/help/mcve)\n- [Reporting a Next.js bug](https://github.com/vercel/next.js/blob/canary/.github/ISSUE_TEMPLATE/1.bug_report.yml)\n- [Next.js Triaging issues](https://github.com/vercel/next.js/blob/canary/contributing/repository/triaging.md)\n",
      "labels": [
        "Script (next/script)",
        "invalid link"
      ],
      "created_at": "2026-02-03T18:00:13Z",
      "closed_at": "2026-02-03T18:00:32Z",
      "url": "https://github.com/vercel/next.js/issues/89451",
      "comments_count": 1
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 45350,
      "title": "Image with static import throws an error with Vitest",
      "problem": "### Verify canary release\n\n- [X] I verified that the issue exists in the latest Next.js canary release\n\n### Provide environment information\n\n    Operating System:\r\n      Platform: win32\r\n      Arch: x64\r\n      Version: Windows 10 Pro\r\n    Binaries:\r\n      Node: 18.13.0\r\n      npm: N/A\r\n      Yarn: N/A\r\n      pnpm: N/A\r\n    Relevant packages:\r\n      next: 13.1.6-canary.2\r\n      eslint-config-next: N/A\r\n      react: 18.2.0\r\n      react-dom: 18.2.0\r\n\n\n### Which area(s) of Next.js are affected? (leave empty if unsure)\n\n_No response_\n\n### Link to the code that reproduces this issue\n\nhttps://github.com/lucas-santosP/next-image-static-import-with-vitest\n\n### To Reproduce\n\nAfter following the Vitest setup in the example [with-vitest](https://github.com/vercel/next.js/tree/canary/examples/with-vitest), to reproduce the error you simply need to add an `Image` component with static import and remove the `width` and `height` props. \r\n\r\nFor example:\r\n```tsx\r\n// On top of the file:\r\nimport vercelLogo from \"../public/vercel.svg\";\r\n\r\n// [...] In the component\r\n<Image src={vercelLogo} alt=\"Vercel Logo\" />\r\n```\r\n\r\nAnd then run the tests\r\n```\r\nyarn vitest\r\n```\n\n### Describe the Bug\n\nit seems that the Vitest running environment does not recognize the static import in the `Image` component, so when I try to run the test it throws an error saying that width and height are required:\r\n\r\n```\r\nFAIL  __tests__/Home.test.tsx > home\r\nError: Image with src \"/public/vercel.svg\" is missing required \"height\" property.\r\n\r\nFAIL  __tests__/Home.test.tsx > home\r\nError: Image with src \"/public/vercel.svg\" is missing required \"width\" property.\r\n```\r\n\r\nThe same happens with `next/legacy/image` and using in Next 12.x too.\r\n\n\n### Expected Behavior\n\nInfer that the image has static import so it does not need `width` and `height` props anymore.\n\n### Which browser are you using? (if relevant)\n\n_No response_\n\n### How are you deploying your application? (if relevant)\n\n_No response_",
      "solution": "@lucas-santosP @Lihemen \r\n\r\nI've found a workaround for this issue.\r\n\r\nThe difference between Vite and Next.js (with Webpack) in terms of static asset import is as follows:\r\n\r\n```ts\r\nimport vercelLogo from \"../public/vercel.svg\";\r\n// In Vite: vercelLogo is a string\r\n// In Next.js: vercelLogo is an object { src: string; width: number; height: number}\r\n```\r\n\r\nTo simulate the behavior of Next.js in Vite, you will need to write a Vite plugin, as shown below:\r\n```ts\r\n// vite.config.ts or vitest.config.ts\r\nimport { defineConfig } from 'vitest/config';\r\nimport react from '@vitejs/plugin-react';\r\nimport path from 'path';\r\n\r\nexport default defineConfig({\r\n  plugins: [react(), stubNextAssetImport()],\r\n  test: {\r\n    environment: 'jsdom',\r\n  },\r\n});\r\n\r\nfunction stubNextAssetImport() {\r\n  return {\r\n    name: 'stub-next-asset-import',\r\n    transform(_code: string, id: string) {\r\n      if (/(jpg|jpeg|png|webp|gif|svg)$/.test(id)) {\r\n        const imgSrc = path.relative(process.cwd(), id);\r\n        return {\r\n          code: `export default { src: '${imgSrc}', height: 1, width: 1 }`,\r\n        };\r\n      }\r\n    },\r\n  };\r\n}\r\n```\r\n\r\nYou can try it here:\r\nhttps://stackblitz.com/edit/node-h5kfn3?file=vitest.config.ts\n\n---\n\nThanks @KoichiKiyokawa the workaround helped me a lot!!\r\nAdding on it I checked next/jest config and they have a mock in jest for it as follows:\r\n\r\n```\r\nmoduleNameMapper: {\r\n    '^.+\\\\.module\\\\.(css|sass|scss)$': '/next/dist/build/jest/object-proxy.js',\r\n    '^.+\\\\.(css|sass|scss)$': '/next/dist/build/jest/__mocks__/styleMock.js',\r\n    '^.+\\\\.(png|jpg|jpeg|gif|webp|avif|ico|bmp)$': '/next/dist/build/jest/__mocks__/fileMock.js',\r\n    '^.+\\\\.(svg)$': '/next/dist/build/jest/__mocks__/fileMock.js',\r\n    '@next/font/(.*)': '/next/dist/build/jest/__mocks__/nextFontMock.js',\r\n    'next/font/(.*)': '/next/dist/build/jest/__mocks__/nextFontMock.js',\r\n    'server-only': '/next/dist/build/jest/__mocks__/empty.js'\r\n}\r\n```\r\nand you can find the mocks here: [next/jest __mocks__](https://github.com/vercel/next.js/tree/canary/packages/next/src/build/jest/__mocks__)\r\n\r\n@lucas-santosP @Lihemen If you want to change the workaround to work as next/jest did it\n\n---\n\n> @lucas-santosP @Lihemen\r\n> \r\n> I've found a workaround for this issue.\r\n> \r\n> The difference between Vite and Next.js (with Webpack) in terms of static asset import is as follows:\r\n> \r\n> ```ts\r\n> import vercelLogo from \"../public/vercel.svg\";\r\n> // In Vite: vercelLogo is a string\r\n> // In Next.js: vercelLogo is an object { src: string; width: number; height: number}\r\n> ```\r\n> \r\n> To simulate the behavior of Next.js in Vite, you will need to write a Vite plugin, as shown below:\r\n> \r\n> ```ts\r\n> // vite.config.ts or vitest.config.ts\r\n> import { defineConfig } from 'vitest/config';\r\n> import react from '@vitejs/plugin-react';\r\n> import path from 'path';\r\n> \r\n> export default defineConfig({\r\n>   plugins: [react(), stubNextAssetImport()],\r\n>   test: {\r\n>     environment: 'jsdom',\r\n>   },\r\n> });\r\n> \r\n> function stubNextAssetImport() {\r\n>   return {\r\n>     name: 'stub-next-asset-import',\r\n>     transform(_code: string, id: string) {\r\n>       if (/(jpg|jpeg|png|webp|gif|svg)$/.test(id)) {\r\n>         const imgSrc = path.relative(process.cwd(), id);\r\n>         return {\r\n>           code: `export default { src: '${imgSrc}', height: 1, width: 1 }`,\r\n>         };\r\n>       }\r\n>     },\r\n>   };\r\n> }\r\n> ```\r\n> \r\n> You can try it here: https://stackblitz.com/edit/node-h5kfn3?file=vitest.config.ts\r\n\r\nFor me, since I don't care about the output image, I changed the image output to:\r\n\r\n```\r\n                return {\r\n                    code: `export default { src: 'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=', height: 1, width: 1 }`,\r\n                };\r\n```\r\n\r\nwhich is a 1x1 transparent image.",
      "labels": [
        "bug"
      ],
      "created_at": "2023-01-27T20:24:54Z",
      "closed_at": "2026-01-31T05:31:59Z",
      "url": "https://github.com/vercel/next.js/issues/45350",
      "comments_count": 8
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 79311,
      "title": "Turbopack Fails with ResourceQuery in Glob Pattern",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/iamnafets/next-resourcequery/tree/main/reproduction-app\n\n### To Reproduce\n\n1. Start the application with `yarn dev --turbo`\n2. Load up `/` note the `Unknown module type` error.\n3. Start the application with `yarn dev`\n4. Note no errors.\n\n### Current vs. Expected behavior\n\nTurbopack doesn't appear to support glob patterns that include the resource query. While the loaders themselves have access to the resource query, the extension patterns do not.\n\nSo a loader like this:\n\n```js\n  {\n  /// ...\n  turbopack: {\n    rules: {\n      \"*\\\\?raw\": {\n        loaders: [require.resolve(\"./loader.js\")],\n        as: \"*.js\"\n      },\n\n      // This also doesn't work\n      {\n        \"*.txt\\\\?raw\": {\n        loaders: [require.resolve(\"./loader.js\")],\n        as: \"*.js\"\n      },\n\n      // This doesn't work either and crashes turbopack\n      \"*?raw\": {\n        loaders: [require.resolve(\"./loader.js\")],\n        as: \"*.js\"\n      }\n      */\n    }\n  }\n }\n```\n\nDoes not work.\n\n### Provide environment information\n\n```bash\nOperating System:\n  Platform: darwin\n  Arch: arm64\n  Version: Darwin Kernel Version 23.6.0: Fri Jul  5 17:56:15 PDT 2024; root:xnu-10063.141.1~2/RELEASE_ARM64_T6031\n  Available memory (MB): 65536\n  Available CPU cores: 16\nBinaries:\n  Node: 22.14.0\n  npm: 10.9.2\n  Yarn: 1.22.22\n  pnpm: N/A\nRelevant Packages:\n  next: 15.4.0-canary.36 // Latest available version is detected (15.4.0-canary.36).\n  eslint-config-next: N/A\n  react: 19.1.0\n  react-dom: 19.1.0\n  typescript: 5.8.3\nNext.js Config:\n  output: N/A\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nTurbopack\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext dev (local)\n\n### Additional context\n\nThere doesn't appear to be a workaround here. I thought I might be able to write a custom loader that would pitch, interrogate the resource query, and then grab the resource if it matched but I have no way to override the loaders and the `data` in pitch is an empty object.\n\n<sub>[PACK-4958](https://linear.app/vercel/issue/PACK-4958/turbopack-fails-with-resourcequery-in-glob-pattern)</sub>",
      "solution": "same, please add\n\n_Edit by maintainer bot: Comment was **automatically** minimized because it was considered unhelpful. (If you think this was by mistake, let us know). Please only comment if it adds context to the issue. If you want to express that you have the same problem, use the upvote \ud83d\udc4d on the issue description or subscribe to the issue for updates. Thanks!_\n\n---\n\n@mischnic is there a solution?\n\n_Edit by maintainer bot: Comment was **automatically** minimized because it was considered unhelpful. (If you think this was by mistake, let us know). Please only comment if it adds context to the issue. If you want to express that you have the same problem, use the upvote \ud83d\udc4d on the issue description or subscribe to the issue for updates. Thanks!_",
      "labels": [
        "Turbopack",
        "linear: turbopack",
        "locked"
      ],
      "created_at": "2025-05-16T19:47:21Z",
      "closed_at": "2026-01-20T08:56:08Z",
      "url": "https://github.com/vercel/next.js/issues/79311",
      "comments_count": 9
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 89366,
      "title": "Module not found: Can't resolve './libsodium.mjs' with Turbopack",
      "problem": "## Summary\n\nWhen using `libsodium-wrappers` (v0.7.15) with Next.js 15.2.8 and Turbopack enabled (`next dev --turbopack`), the build fails with a module resolution error.\n\n## Error\n\n```\nModule not found: Can't resolve './libsodium.mjs'\n\n> 1 | import e from\"./libsodium.mjs\";let a;const r={},t=e.ready.then(function(){...\n```\n\n## Environment\n\n- **Next.js version**: 15.2.8\n- **Node.js version**: (run `node -v`)\n- **Operating System**: macOS\n- **Package manager**: npm\n- **libsodium-wrappers version**: 0.7.15\n\n## Steps to Reproduce\n\n1. Create a Next.js project with Turbopack\n2. Install `libsodium-wrappers`: `npm install libsodium-wrappers`\n3. Import and use libsodium-wrappers in any component or API route\n4. Run `next dev --turbopack`\n5. Visit a page that uses the library\n\n## Expected Behavior\n\nThe module should resolve correctly and the application should run.\n\n## Actual Behavior\n\nBuild fails with `Module not found: Can't resolve './libsodium.mjs'`\n\n## Workaround\n\nRunning without Turbopack (`next dev` instead of `next dev --turbopack`) works correctly.\n\n## Additional Context\n\nThe `libsodium-wrappers` package uses dynamic imports and WASM files that Turbopack may not be handling correctly. The standard webpack bundler in Next.js resolves this without issues.",
      "solution": "We could not detect a valid reproduction link. **Make sure to follow the bug report template carefully.**\n\n### Why was this issue closed?\n\nTo be able to investigate, we need access to a reproduction to identify what triggered the issue. We need a link to a **public** GitHub repository ([template for App Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template), [template for Pages Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template-pages)), but you can also use these templates: [CodeSandbox: App Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template) or [CodeSandbox: Pages Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template-pages).\n\nThe bug template that you filled out has a section called \"Link to the code that reproduces this issue\", which is where you should provide the link to the reproduction.\n\n- If you did not provide a link or the link you provided is not valid, we will close the issue.\n- If you provide a link to a private repository, we will close the issue.\n- If you provide a link to a repository but not in the correct section, we will close the issue.\n\n### What should I do?\n\nDepending on the reason the issue was closed, you can do the following:\n\n- If you did not provide a link, please open a new issue with a link to a reproduction.\n- If you provided a link to a private repository, please open a new issue with a link to a public repository.\n- If you provided a link to a repository but not in the correct section, please open a new issue with a link to a reproduction in the correct section.\n\n**In general, assume that we should not go through a lengthy onboarding process at your company code only to be able to verify an issue.**\n\n### My repository is private and cannot make it public\n\nIn most cases, a private repo will not be a sufficient **minimal reproduction**, as this codebase might contain a lot of unrelated parts that would make our investigation take longer. Please do **not** make it public. Instead, create a new repository using the templates above, adding the relevant code to reproduce the issue. Common things to look out for:\n\n- Remove any code that is not related to the issue. (pages, API routes, components, etc.)\n- Remove any dependencies that are not related to the issue.\n- Remove any third-party service that would require us to sign up for an account to reproduce the issue.\n- Remove any environment variables that are not related to the issue.\n- Remove private packages that we do not have access to.\n- If the issue is not related to a monorepo specifically, try to reproduce the issue without a complex monorepo setup\n\n### I did not open this issue, but it is relevant to me, what can I do to help?\n\nAnyone experiencing the same issue is welcome to provide a minimal reproduction following the above steps by opening a new issue.\n\n### I think my reproduction is good enough, why aren't you looking into it quickly?\n\nWe look into every Next.js issue and constantly monitor open issues for new comments.\n\nHowever, sometimes we might miss one or two due to the popularity/high traffic of the repository. We apologize, and kindly ask you to refrain from tagging core maintainers, as that will usually not result in increased priority.\n\nUpvoting issues to show your interest will help us prioritize and address them as quickly as possible. That said, every issue is important to us, and if an issue gets closed by accident, we encourage you to open a new one linking to the old issue and we will look into it.\n\n### Useful Resources\n\n- [How to Contribute to Open Source (Next.js)](https://www.youtube.com/watch?v=cuoNzXFLitc)\n- [How to create a Minimal, Complete, and Verifiable example](https://stackoverflow.com/help/mcve)\n- [Reporting a Next.js bug](https://github.com/vercel/next.js/blob/canary/.github/ISSUE_TEMPLATE/1.bug_report.yml)\n- [Next.js Triaging issues](https://github.com/vercel/next.js/blob/canary/contributing/repository/triaging.md)\n",
      "labels": [
        "invalid link"
      ],
      "created_at": "2026-02-02T01:35:10Z",
      "closed_at": "2026-02-02T01:35:25Z",
      "url": "https://github.com/vercel/next.js/issues/89366",
      "comments_count": 1
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 87322,
      "title": "Infinite \"Compiling\" loop with `opengraph-image.tsx` and Turbopack (Next.js 16)",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://codesandbox.io/p/sandbox/infinite-compiling-issue-mds8jf\n\n### To Reproduce\n\nTo reproduce the problem, simply click the `Issue` link in the **center of the page**. On the loaded page, the Next.js icon will switch to **\"Compiling\"** and will never exit that state.\n\n<img width=\"1399\" height=\"932\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/dbd4e638-bdbb-4454-bd73-19e67106a7a6\" />\n\n### Current vs. Expected behavior\n\nWhen navigating to pages that have an `opengraph-image.tsx` implemented, if any kind of error occurs in the console, Next.js starts **recompiling** `global-error.tsx` **infinitely**. Additionally, this issue also happens when **navigating away from the problematic page**. In the example project, the issue was **probably triggered because the** `favicon.ico` **file does not exist**, resulting in:\n\n```cmd\nGET http://localhost:3000/favicon.ico 404 (Not Found)\n```\nWhen I re-add the file, the problem disappears\n\nThis behavior only began after upgrading from **version 15.*** to **16.*** with Turbopack enabled.\nWithout Turbopack, the problem does not occur.\n\nAt first, this may not seem like a major problem, but it continuously consumes processing power, with CPU usage rising above 80%. I also noticed that the size of the Turbopack traceback log keeps growing indefinitely, which suggests that the recompilation loop is not only stuck but also accumulating output without limit.\n\n<img width=\"1069\" height=\"1712\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/79e48197-2439-47bb-9756-f3c708c9eedf\" />\n\n\nThe following screenshots were taken from the local environment, showing the issue as seen in the Turbopack traceback:\n\n<img width=\"1252\" height=\"1952\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/74d39bab-2978-4ad7-946f-0591cc697bfb\" />\n\n<img width=\"1380\" height=\"2006\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/605bce9c-472a-446d-986e-ffa0d6300cca\" />\n\n<img width=\"2195\" height=\"1246\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e0335530-b21c-4ce9-b7f7-df7e005f8560\" />\n\n### Provide environment information\n\n```bash\nOperating System:\n  Platform: linux\n  Arch: x64\n  Version: #1 SMP PREEMPT_DYNAMIC Sun Aug  6 20:05:33 UTC 2023\n  Available memory (MB): 4102\n  Available CPU cores: 2\nBinaries:\n  Node: 20.12.1\n  npm: 10.5.0\n  Yarn: 1.22.19\n  pnpm: 8.15.6\nRelevant Packages:\n  next: 16.1.0-canary.34 // Latest available version is detected (16.1.0-canary.34).\n  eslint-config-next: N/A\n  react: 19.2.3\n  react-dom: 19.2.3\n  typescript: 5.9.3\nNext.js Config:\n  output: N/A\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nError Handling, SWC, Turbopack, Performance, Linking and Navigating\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext dev (local)\n\n### Additional context\n\n_No response_",
      "solution": "This bug seems to be 100% reproducible in the development environment. The issue is severe enough to hinder development, making the feature completely unusable.\n\n---\n\n> This bug seems to be 100% reproducible in the development environment. The issue is severe enough to hinder development, making the feature completely unusable.\n\nYep. Very bad user experience.\n\n---\n\nThis is going to fix it for the original reproduction.\n\nWould be great if anyone else has reproductions. The case I've fixed is about metadata routes, i.e. favicon.ico, apple-icon.jpg, icon.jpg, opengraph-image.jpg and twitter-image.jpg, robots.txt, sitemap.xml",
      "labels": [
        "Performance",
        "Turbopack",
        "linear: turbopack",
        "locked",
        "Error Handling"
      ],
      "created_at": "2025-12-18T18:59:38Z",
      "closed_at": "2026-01-14T09:19:14Z",
      "url": "https://github.com/vercel/next.js/issues/87322",
      "comments_count": 19
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 80779,
      "title": "DeprecationWarning: The `util._extend` API is deprecated. Please use Object.assign() instead.",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/utkydz/nextjs-bug-report-deprecation-util-extend\n\n### To Reproduce\n\n- create new app with create-next-app with recommended settings.\n- add any rewrites rules to next.config file.\n``` javascript\n  rewrites: async () => [\n    {\n      source: '/api/login',\n      destination: 'https://www.domain.com/api/remote-login'\n    },\n    {\n      source: '/api/:path*',\n      destination: 'https://www.domain.com/api/:path*'\n    },\n    // ... doesn't matter what kind of rewrites it is\n  ]\n```\n- run in dev mode with `next dev` or `next dev --turbo` (not prod mode)\n- waiting until to compiled message and shown the page on browser\n- console output should be observed\n- the following warning message is seen\n``` javascript\n(node:33378) [DEP0060] DeprecationWarning: The `util._extend` API is deprecated. Please use Object.assign() instead.\n(Use `node --trace-deprecation ...` to show where the warning was created)\n```\n- run in dev mode again but this time with `NODE_OPTIONS=\"--trace-deprecation\"` for `next dev` or `next dev --turbo`\n- waiting until to compiled message and shown the page on browser\n- console output should be observed\n- the following warning message is seen\n``` javascript\n(node:34419) [DEP0060] DeprecationWarning: The `util._extend` API is deprecated. Please use Object.assign() instead.\n    at ProxyServer.<anonymous> (MY_PROJECT_PATH/node_modules/next/dist/compiled/http-proxy/index.js:13:2607)\n    at proxyRequest (MY_PROJECT_PATH/node_modules/next/dist/server/lib/router-utils/proxy-request.js:108:15)\n    at handleRequest (MY_PROJECT_PATH/node_modules/next/dist/server/lib/router-server.js:340:61)\n    at async requestHandlerImpl (MY_PROJECT_PATH/node_modules/next/dist/server/lib/router-server.js:452:13)\n    at async Server.requestListener (MY_PROJECT_PATH/node_modules/next/dist/server/lib/start-server.js:158:13)\n```\n\n### Current vs. Expected behavior\n\ncompile and run without any warnings\n\n### Provide environment information\n\n```bash\nOperating System:\n  Platform: darwin\n  Arch: arm64\n  Version: Darwin Kernel Version 24.5.0: Tue Apr 22 19:54:49 PDT 2025; root:xnu-11417.121.6~2/RELEASE_ARM64_T6000\n  Available memory (MB): 16384\n  Available CPU cores: 8\nBinaries:\n  Node: 22.16.0\n  npm: 10.9.2\n  Yarn: 1.22.22\n  pnpm: N/A\nRelevant Packages:\n  next: 15.3.4 // Latest available version is detected (15.3.4).\n  eslint-config-next: 15.3.4\n  react: 19.1.0\n  react-dom: 19.1.0\n  typescript: 5.8.3\nNext.js Config:\n  output: N/A\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nNot sure\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext dev (local)\n\n### Additional context\n\n_No response_",
      "solution": "> The warning is only visible in development (next dev) with rewrites enabled. It does not affect functionality or production builds.\n\nI completely agree with this information.\n\nThe reason i opened the issue is just to inform both contributors and people who have seen a similar warning.\nMaybe a contributor sees it and wants to check the version of the relevant package and update it.",
      "labels": [
        "locked"
      ],
      "created_at": "2025-06-23T02:18:41Z",
      "closed_at": "2026-01-18T20:31:45Z",
      "url": "https://github.com/vercel/next.js/issues/80779",
      "comments_count": 7
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 84029,
      "title": "Hydration failure with `useId()` and AG Grid Enterprise",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/rwalisa/nextjs-useid-hydration-bug-demo/\n\n### To Reproduce\n\n1. Start the application in development, or build and run the server\n2. Open the index page\n3. In case the error doesn't appear, refresh it\n4. Remove line 7 in `Component.jsx`\n5. Refresh the page to see that the error is gone\n\n### Current vs. Expected behavior\n\nHydration fails, because `useId()` returns `_R_clrlb_` on the server and `_R_4lrlb_` on the client.\n\n### Provide environment information\n\n```bash\nOperating System:\n  Platform: darwin\n  Arch: arm64\n  Version: Darwin Kernel Version 25.0.0: Mon Aug 25 21:16:39 PDT 2025; root:xnu-12377.1.9~3/RELEASE_ARM64_T6031\n  Available memory (MB): 36864\n  Available CPU cores: 14\nBinaries:\n  Node: 24.8.0\n  npm: 11.6.0\n  Yarn: N/A\n  pnpm: N/A\nRelevant Packages:\n  next: 15.5.3 // Latest available version is detected (15.5.3).\n  eslint-config-next: 15.5.3\n  react: 19.1.0\n  react-dom: 19.1.0\n  typescript: 5.9.2\nNext.js Config:\n  output: N/A\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nReact\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext dev (local), next build (local), next start (local)\n\n### Additional context\n\nThe issue happens starting from Next.js `v15.4.2-canary.20`. `v15.4.2-canary.19` works as expected. All patch versions in 15.5.x are affected.\n\nIt only happens when AG Grid Enterprise is imported in a client component (not necessarily the one that calls `useId()`) and referenced in the code by accessing any of its exports. Since the imported package doesn't import React, I think it's an issue with Next.js module resolution.\n\nImporting AG Grid Community doesn't trigger the issue.",
      "solution": "I have some ideas on what to do next, but those take time and I am bit busy right now. \n\nRight now, `ag-grid-enterprise` maintainers might have a better, more efficient, chance at trying to narrow down what the issue really is. Let's hope they can take a look at it.\n\n---\n\nBased on what you're describing rwalisa, this is very similar to what I reported here: https://github.com/radix-ui/primitives/issues/3700. The difference being that Radix UI components use the `useId` hook in their source code directly (although with some custom modifications), and it fails with similar results like the ones you see.\nIf this is the same issue, it could be a hint that the issue isn't necessarily linked to one particular package, but rather a change in React's `useId` or alternatively something else in Next.js.\n\n---\n\nYeah I never got around simplifying the repository to a clear code pattern that reliably triggers the issue. I had suspected perhaps multiple React versions were at fault, but could never prove it. \n\nIs there a minimal repository for the Radix issue? @ZeroWave022 ? Also - the issues look similar, but we haven't yet boiled it all down to a given pattern, just yet.",
      "labels": [
        "linear: next",
        "locked",
        "React"
      ],
      "created_at": "2025-09-20T23:00:18Z",
      "closed_at": "2026-01-16T17:13:09Z",
      "url": "https://github.com/vercel/next.js/issues/84029",
      "comments_count": 29
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 88512,
      "title": "Segment cache LRU cleanup causes infinite loop and page freeze when cache exceeds size limit",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/yoshiwarab/nextjs-lru-bug-repro\n\n### To Reproduce\n\n1. Clone the reproduction repository\n2. Run npm install\n3. Run npm run build && npm run start\n4. Open http://localhost:3000 in the browser\n5. Scroll up and down through the grid of collection links, hovering over various items to trigger prefetches (each prefetch loads ~1MB of static RSC payload)\n6. Continue scrolling and hovering until the page freezes (typically after 50+ prefetches exceed the 50MB cache limit)\n\nNote: Simply scrolling slowly once may not be enough to trigger the issue. Scrolling back and forth and hovering over links helps trigger more prefetch activity, which fills the cache faster.\n\n### Current vs. Expected behavior\n\n**Current behavior:**\nWhen the segment cache exceeds the 50MB limit, the cleanup() function in segment-cache/lru.js enters an infinite loop. The while loop condition lruSize > ninetyPercentMax && head !== null never becomes false, causing the page to freeze indefinitely. The main thread becomes completely blocked and the page is unresponsive.\n\n**Expected behavior:**\nThe LRU cache should evict old entries and reduce lruSize appropriately, allowing the cleanup to complete and the page to remain responsive.\n\n\n### Provide environment information\n\n```bash\nOperating System:\n  Platform: darwin\n  Arch: arm64\n  Version: Darwin Kernel Version 25.2.0: Tue Nov 18 21:07:05 PST 2025; root:xnu-12377.61.12~1/RELEASE_ARM64_T6020\n  Available memory (MB): 16384\n  Available CPU cores: 10\nBinaries:\n  Node: 22.19.0\n  npm: 10.9.3\n  Yarn: 4.3.1\n  pnpm: 10.18.1\nRelevant Packages:\n  next: 16.1.1-canary.25 // Latest available version is detected (16.1.1-canary.25).\n  eslint-config-next: N/A\n  react: 19.2.3\n  react-dom: 19.2.3\n  typescript: 5.9.3\nNext.js Config:\n  output: N/A\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nLinking and Navigating, Performance\n\n### Which stage(s) are affected? (Select all that apply)\n\nVercel (Deployed), next start (local)\n\n### Additional context\n\nLive demo: https://nextjs-lru-bug-repro.vercel.app/\nThe issue occurs in the segment cache's LRU eviction logic at https://github.com/vercel/next.js/blob/e1c95fc2094f276eb881539d45e0f6f8e42a83e9/packages/next/src/client/components/segment-cache/lru.ts#L104-L120\n\nThe while loop assumes that deleteMapEntry(tail) will always decrease lruSize, but under certain conditions this doesn't happen, causing an infinite loop.\n\nThe reproduction uses 100 statically generated pages, each with ~1MB of high-entropy data. When these pages are prefetched and the cache fills beyond 50MB, the cleanup triggers and freezes the page.\nThis is reproducible locally with the production build (next build && next start). Prefetching must be enabled (production mode only).",
      "solution": "Hi, thanks for the report. This was fixed via #87991 which is available starting from [v16.1.1-canary.28](https://github.com/vercel/next.js/releases/tag/v16.1.1-canary.28) and will be in the next stable release. ",
      "labels": [
        "Linking and Navigating",
        "Performance",
        "locked"
      ],
      "created_at": "2026-01-14T02:21:13Z",
      "closed_at": "2026-01-16T01:19:23Z",
      "url": "https://github.com/vercel/next.js/issues/88512",
      "comments_count": 2
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 88599,
      "title": "Server Actions CSRF validation should be case insensitive",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/geeknik/nextjs-csrf-case-sensitivity-repro\n\n### To Reproduce\n\n## Summary\n\nNext.js Server Actions implement CSRF protection by comparing the `Origin` header against the `Host` or `x-forwarded-host` header to ensure same-origin requests. However, this comparison is **case-sensitive** for the host header while the origin domain is normalized to lowercase by the URL API, violating RFC 1123 which mandates that hostnames are case-insensitive.\n\nThis inconsistency can lead to:\n1. **False positive CSRF blocks** that reject legitimate same-origin requests when hostname case doesn't match\n2. **Inconsistent security behavior** across different deployment configurations\n3. **RFC 1123 compliance violation** treating semantically identical hostnames as different\n\nThe vulnerability exists in `packages/next/src/server/app-render/action-handler.ts` where `parseHostHeader()` preserves the original case of the `Host`/`x-forwarded-host` headers, while `originDomain` from the `Origin` header is always lowercase (normalized by JavaScript's `URL` API).\n\n**Root Cause**:\n```typescript\n// Origin is lowercased by URL API\nconst originDomain = new URL(originHeader).host  // \u2192 \"example.com\"\n\n// Host header case is preserved\nconst host = parseHostHeader(req.headers)  // \u2192 \"Example.com\"\n\n// Case-sensitive comparison causes mismatch\nif (originDomain !== host.value) {  // \"example.com\" !== \"Example.com\" \u2192 TRUE\n  // False positive CSRF block or bypass\n}\n```\n\n---\n\n## Steps to Reproduce\n\n### Environment Setup\n\n1. Clone Next.js repository\n2. Create a test application with Server Actions\n3. Configure deployment with a hostname containing uppercase letters (e.g., `Example.com`)\n\n### Reproduction Steps\n\n**Step 1**: Create a Next.js app with a Server Action\n\n```typescript\n// app/actions.ts\n'use server'\n\nexport async function testAction() {\n  return { success: true, message: 'Action executed' }\n}\n```\n\n```typescript\n// app/page.tsx\nimport { testAction } from './actions'\n\nexport default function Page() {\n  return (\n    <form action={testAction}>\n      <button type=\"submit\">Test Action</button>\n    </form>\n  )\n}\n```\n\n**Step 2**: Configure reverse proxy to set `x-forwarded-host` with uppercase letters\n\n```nginx\n# nginx.conf\nproxy_set_header X-Forwarded-Host \"Example.com\";\n```\n\n**Step 3**: Send a legitimate same-origin request with lowercase origin\n\n```bash\ncurl -X POST http://localhost:3000/_next/data/action \\\n  -H \"Origin: http://example.com\" \\\n  -H \"x-forwarded-host: Example.com\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"actionId\":\"abc123\"}'\n```\n\n\n### Current vs. Expected behavior\n\n**Expected Result**: Request should succeed (same origin)\n**Actual Result**: Request fails with \"Invalid Server Actions request\" error\n\nThis inconsistency means:\n- Dev endpoints (`/__nextjs_*`) have correct CSRF protection\n- Server Actions have broken CSRF protection\n- Different code paths behave differently for the same security check\n\nRFC 1123 Section 2.1 states:\n> \"Domain names are case-insensitive\"\n\nThe hostnames `Example.com`, `example.com`, and `EXAMPLE.COM` are **semantically identical** according to DNS and HTTP specifications.\n\n**Implications**:\n- Violates web standards\n- Unpredictable behavior across different clients/proxies\n- Different proxies may normalize case differently (Apache, Nginx, Cloudflare, etc.)\n\n### Provide environment information\n\n```bash\nFedora 43\nNext.js main\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nServer Actions\n\n### Which stage(s) are affected? (Select all that apply)\n\nOther (Deployed)\n\n### Additional context\n\n## Additional Context\n\n### Comparison with Correct Implementation\n\nThe Next.js codebase already has a correct implementation of this check in `packages/next/src/server/lib/router-utils/block-cross-site.ts`:\n\n```typescript\n// CORRECT IMPLEMENTATION (block-cross-site.ts:86-94)\nconst rawOrigin = req.headers['origin']\n\nif (rawOrigin && rawOrigin !== 'null') {\n  const parsedOrigin = parseUrl(rawOrigin)\n\n  if (parsedOrigin) {\n    const originLowerCase = parsedOrigin.hostname.toLowerCase()  // \u2705 LOWERCASED\n\n    if (!isCsrfOriginAllowed(originLowerCase, allowedOrigins)) {\n      return warnOrBlockRequest(res, originLowerCase, mode)\n    }\n  }\n}\n```\n\nThis demonstrates:\n1. The correct approach is already implemented elsewhere in the codebase\n2. The vulnerability is an inconsistency, not a design flaw\n3. The fix aligns with existing Next.js security patterns\n\n---\n\n## References\n\n1. **RFC 1123 - Requirements for Internet Hosts**\n   https://tools.ietf.org/html/rfc1123#section-2.1\n\n2. **RFC 3986 - Uniform Resource Identifier (URI): Generic Syntax**\n   https://tools.ietf.org/html/rfc3986#section-3.2.2\n\n3. **WHATWG URL Standard - Host Parsing**\n   https://url.spec.whatwg.org/#host-parsing\n\n4. **OWASP CSRF Prevention Cheat Sheet**\n   https://cheatsheetseries.owasp.org/cheatsheets/Cross-Site_Request_Forgery_Prevention_Cheat_Sheet.html\n\n5. **Next.js Server Actions Documentation**\n   https://nextjs.org/docs/app/building-your-application/data-fetching/server-actions-and-mutations\n\n6. **CWE-178: Improper Handling of Case Sensitivity**\n   https://cwe.mitre.org/data/definitions/178.html\n\n7. **CVSSv4.0 Specification**\n   https://www.first.org/cvss/v4.0/specification-document\n\n---\n\n### Recommended Fix\n\nNormalize all hostname values to lowercase before comparison, consistent with RFC 1123 and the existing implementation in `block-cross-site.ts`.\n\n### Patch 1: Normalize Origin Domain (Primary Fix)\n\n**File**: `packages/next/src/server/app-render/action-handler.ts`\n\n**Lines 613-617**:\n\n```diff\n  const originHeader = req.headers['origin']\n  const originDomain =\n    typeof originHeader === 'string' && originHeader !== 'null'\n-     ? new URL(originHeader).host\n+     ? new URL(originHeader).host.toLowerCase()\n      : undefined\n```\n\n**Note**: This line already normalizes to lowercase via URL API, but making it explicit improves clarity.\n\n### Patch 2: Normalize Host Headers (Required Fix)\n\n**File**: `packages/next/src/server/app-render/action-handler.ts`\n\n**Lines 474-506** (in `parseHostHeader` function):\n\n```diff\nexport function parseHostHeader(\n  headers: IncomingHttpHeaders,\n  originDomain?: string\n) {\n  const forwardedHostHeader = headers['x-forwarded-host']\n  const forwardedHostHeaderValue =\n    forwardedHostHeader && Array.isArray(forwardedHostHeader)\n-     ? forwardedHostHeader[0]\n+     ? forwardedHostHeader[0]?.toLowerCase()\n-     : forwardedHostHeader?.split(',')?.[0]?.trim()\n+     : forwardedHostHeader?.split(',')?.[0]?.trim()?.toLowerCase()\n- const hostHeader = headers['host']\n+ const hostHeader = headers['host']?.toLowerCase()\n\n  if (originDomain) {\n    return forwardedHostHeaderValue === originDomain\n      ? {\n          type: HostType.XForwardedHost,\n          value: forwardedHostHeaderValue,\n        }\n      : hostHeader === originDomain\n        ? {\n            type: HostType.Host,\n            value: hostHeader,\n          }\n        : undefined\n  }\n\n  return forwardedHostHeaderValue\n    ? {\n        type: HostType.XForwardedHost,\n        value: forwardedHostHeaderValue,\n      }\n    : hostHeader\n      ? {\n          type: HostType.Host,\n          value: hostHeader,\n        }\n      : undefined\n}\n```\n\n### Patch 3: Normalize in Wildcard Matcher (Defense in Depth)\n\n**File**: `packages/next/src/server/app-render/csrf-protection.ts`\n\n**Lines 6-8**:\n\n```diff\nfunction matchWildcardDomain(domain: string, pattern: string) {\n- const domainParts = domain.split('.')\n- const patternParts = pattern.split('.')\n+ const domainParts = domain.toLowerCase().split('.')\n+ const patternParts = pattern.toLowerCase().split('.')\n```\n\n### Complete Patch File\n\n```diff\ndiff --git a/packages/next/src/server/app-render/action-handler.ts b/packages/next/src/server/app-render/action-handler.ts\nindex abc123..def456 100644\n--- a/packages/next/src/server/app-render/action-handler.ts\n+++ b/packages/next/src/server/app-render/action-handler.ts\n@@ -471,11 +471,11 @@ export function parseHostHeader(\n ) {\n   const forwardedHostHeader = headers['x-forwarded-host']\n   const forwardedHostHeaderValue =\n     forwardedHostHeader && Array.isArray(forwardedHostHeader)\n-      ? forwardedHostHeader[0]\n-      : forwardedHostHeader?.split(',')?.[0]?.trim()\n-  const hostHeader = headers['host']\n+      ? forwardedHostHeader[0]?.toLowerCase()\n+      : forwardedHostHeader?.split(',')?.[0]?.trim()?.toLowerCase()\n+  const hostHeader = headers['host']?.toLowerCase()\n\n   if (originDomain) {\n     return forwardedHostHeaderValue === originDomain\n       ? {\n\ndiff --git a/packages/next/src/server/app-render/csrf-protection.ts b/packages/next/src/server/app-render/csrf-protection.ts\nindex abc123..def456 100644\n--- a/packages/next/src/server/app-render/csrf-protection.ts\n+++ b/packages/next/src/server/app-render/csrf-protection.ts\n@@ -5,8 +5,8 @@\n // https://nextjs.org/docs/app/api-reference/components/image#remotepatterns\n // TODO - retrofit micromatch to work in edge and use that instead\n function matchWildcardDomain(domain: string, pattern: string) {\n-  const domainParts = domain.split('.')\n-  const patternParts = pattern.split('.')\n+  const domainParts = domain.toLowerCase().split('.')\n+  const patternParts = pattern.toLowerCase().split('.')\n\n   if (patternParts.length < 1) {\n     // pattern is empty and therefore invalid to match against\n```\n",
      "solution": "I believe this is fixed by https://github.com/vercel/next.js/pull/89127 . Sorry I missed your PR, @Grit03.",
      "labels": [
        "Server Actions"
      ],
      "created_at": "2026-01-15T18:30:04Z",
      "closed_at": "2026-01-29T23:54:50Z",
      "url": "https://github.com/vercel/next.js/issues/88599",
      "comments_count": 3
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 88822,
      "title": "Build fails with SWC nullish coalescing operator panic after upgrading to Next.js v16.1.4",
      "problem": "### Link to the code that reproduces this issue\n\nN/A\n\n### To Reproduce\n\n1. Upgrade Next.js to version 16.1.4\n2. Run next build\n3. Build fails with a panic error related to nullish coalescing operator\n\n### Current vs. Expected behavior\n\n#### Current behavior\n\nThe build process panics with the following error:\n\n```\nthread 'tokio-runtime-worker' (461588) panicked at /Users/geist/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/swc_ecma_transformer-3.2.1/src/es2020/nullish_coalescing.rs:109:31:\ncalled `Option::unwrap()` on a `None` value\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n\nthread 'tokio-runtime-worker' (461587) panicked at /Users/geist/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/swc_ecma_transformer-3.2.1/src/es2020/nullish_coalescing.rs:109:31:\ncalled `Option::unwrap()` on a `None` value\n```\n\nMultiple similar errors appear in the console after the initial panic.\n\n#### Expected behavior\n\nThe build should complete successfully, properly transforming the nullish coalescing operator (??) without panicking as in v16.1.1.\n\n### Provide environment information\n\n```bash\nOperating System:\n  Platform: darwin\n  Arch: arm64\n  Version: Darwin Kernel Version 25.2.0: Tue Nov 18 21:09:40 PST 2025; root:xnu-12377.61.12~1/RELEASE_ARM64_T6000\n  Available memory (MB): 32768\n  Available CPU cores: 8\nBinaries:\n  Node: 24.13.0\n  npm: 11.6.2\n  Yarn: 1.22.22\n  pnpm: 10.28.1\nRelevant Packages:\n  next: 16.1.4 // Latest available version is detected (16.1.4).\n  eslint-config-next: N/A\n  react: 19.2.3\n  react-dom: 19.2.3\n  typescript: 5.9.3\nNext.js Config:\n  output: N/A\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nSWC\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext build (local)\n\n### Additional context\n\nChanging the nullish coalescing operator (??) to the logical OR operator (||) in config.ts allows the build to complete, but this is not a proper solution as ??  and || have different semantics.",
      "solution": "We could not detect a valid reproduction link. **Make sure to follow the bug report template carefully.**\n\n### Why was this issue closed?\n\nTo be able to investigate, we need access to a reproduction to identify what triggered the issue. We need a link to a **public** GitHub repository ([template for App Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template), [template for Pages Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template-pages)), but you can also use these templates: [CodeSandbox: App Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template) or [CodeSandbox: Pages Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template-pages).\n\nThe bug template that you filled out has a section called \"Link to the code that reproduces this issue\", which is where you should provide the link to the reproduction.\n\n- If you did not provide a link or the link you provided is not valid, we will close the issue.\n- If you provide a link to a private repository, we will close the issue.\n- If you provide a link to a repository but not in the correct section, we will close the issue.\n\n### What should I do?\n\nDepending on the reason the issue was closed, you can do the following:\n\n- If you did not provide a link, please open a new issue with a link to a reproduction.\n- If you provided a link to a private repository, please open a new issue with a link to a public repository.\n- If you provided a link to a repository but not in the correct section, please open a new issue with a link to a reproduction in the correct section.\n\n**In general, assume that we should not go through a lengthy onboarding process at your company code only to be able to verify an issue.**\n\n### My repository is private and cannot make it public\n\nIn most cases, a private repo will not be a sufficient **minimal reproduction**, as this codebase might contain a lot of unrelated parts that would make our investigation take longer. Please do **not** make it public. Instead, create a new repository using the templates above, adding the relevant code to reproduce the issue. Common things to look out for:\n\n- Remove any code that is not related to the issue. (pages, API routes, components, etc.)\n- Remove any dependencies that are not related to the issue.\n- Remove any third-party service that would require us to sign up for an account to reproduce the issue.\n- Remove any environment variables that are not related to the issue.\n- Remove private packages that we do not have access to.\n- If the issue is not related to a monorepo specifically, try to reproduce the issue without a complex monorepo setup\n\n### I did not open this issue, but it is relevant to me, what can I do to help?\n\nAnyone experiencing the same issue is welcome to provide a minimal reproduction following the above steps by opening a new issue.\n\n### I think my reproduction is good enough, why aren't you looking into it quickly?\n\nWe look into every Next.js issue and constantly monitor open issues for new comments.\n\nHowever, sometimes we might miss one or two due to the popularity/high traffic of the repository. We apologize, and kindly ask you to refrain from tagging core maintainers, as that will usually not result in increased priority.\n\nUpvoting issues to show your interest will help us prioritize and address them as quickly as possible. That said, every issue is important to us, and if an issue gets closed by accident, we encourage you to open a new one linking to the old issue and we will look into it.\n\n### Useful Resources\n\n- [How to Contribute to Open Source (Next.js)](https://www.youtube.com/watch?v=cuoNzXFLitc)\n- [How to create a Minimal, Complete, and Verifiable example](https://stackoverflow.com/help/mcve)\n- [Reporting a Next.js bug](https://github.com/vercel/next.js/blob/canary/.github/ISSUE_TEMPLATE/1.bug_report.yml)\n- [Next.js Triaging issues](https://github.com/vercel/next.js/blob/canary/contributing/repository/triaging.md)\n\n\n---\n\nas mentioned in this thread: https://github.com/vercel/next.js/discussions/85232#discussioncomment-14764248\n\nI have icon.png icon.svg in app dir. I removed them then it builds successfully. Tested on Next.js v16.1.4 and Next.js v16.1.6.\n\n\nI generated favicon, etc from https://realfavicongenerator.net/  so I have icon0.svg and icon1.png. Like this: \n\n<img width=\"151\" height=\"224\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/5f277cb3-21e5-45dc-b02d-e2b0ac741266\" />\n\n\nBelow is the original content: \n\nApparently I've managed to figure out the root of the issue and fix it. It seems to be caused by non-script (`ts/tsx`) files stored within `app` directory. I used to keep some icon files there (e.g. `apple-icon.png`, `favicon.ico` etc.) and moving those to `public` has just resolved the build error. Please let me know if this was helpful for you.\n\nUPD. I just figured out that the issue is directly caused by using files with the same name but different format (e.g. `icon.png` & `icon.svg`). If all the file names placed within `app` directory are unique, `build` passes correctly.\n\n_Originally posted by @samoyedisco in https://github.com/vercel/next.js/discussions/85232#discussioncomment-14764248_\n",
      "labels": [
        "SWC",
        "invalid link"
      ],
      "created_at": "2026-01-20T18:13:30Z",
      "closed_at": "2026-01-20T18:13:47Z",
      "url": "https://github.com/vercel/next.js/issues/88822",
      "comments_count": 8
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 84129,
      "title": "HEAD requests for Next.js image return 400 Bad Request when image is not yet cached",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/Git-I985/next-head-image-empty-cache\n\n## Description\n\nWhen making a HEAD request to an image served via the Next.js Image Optimizer (/_next/image) before the image is cached, the server returns 400 Bad Request.\n\n- **On Next.js 15.3.x:**\n  - A first HEAD request poisoned the cache by storing an empty file, causing subsequent GET requests to return an empty response.\n- **On Next.js 15.5.x:**\n  - The cache poisoning seems fixed (no empty files are written). \n  - However, the first uncached HEAD request still fails with 400 Bad Request. \n  - Only after a GET request succeeds and the image is cached, subsequent HEAD requests return 200 OK.\n\nThis behavior breaks CDNs/proxies that issue HEAD requests for validation or preflight, since they receive a 400 instead of headers describing the resource.\n\n### To Reproduce\n\n**Current version**\n\n1. Build and start `npm run build && npm run start`\n2. Clear cache\n```bash\nrm -rf .next/cache/images\n```\n3. Make HTTP HEAD request to the image resource\n```bash\ncurl -IL \"http://localhost:3000/_next/image?url=/_next/static/media/test-image.daca9759.jpg&w=3840&q=75\"\n```\n**Result (problem):**\n```bash\nHTTP/1.1 400 Bad Request\nDate: Tue, 23 Sep 2025 09:44:29 GMT\nConnection: keep-alive\nKeep-Alive: timeout=5\n```\n```bash\n \u2a2f The requested resource isn't a valid image for /_next/static/media/test-image.daca9759.jpg received null\n \u2a2f The requested resource isn't a valid image for /_next/static/media/test-image.daca9759.jpg received null\n \u2a2f The requested resource isn't a valid image for /_next/static/media/test-image.daca9759.jpg received null\n\n```\n4. Make HTTP GET request to the image resource\n```bash\ncurl -D -- -o test.webp \"http://localhost:3000/_next/image?url=/_next/static/media/test-image.daca9759.jpg&w=3840&q=75\"\n```\nResult\n```bash\nHTTP/1.1 200 OK\nVary: Accept\nCache-Control: public, max-age=315360000, immutable\nETag: RyS1td48qGQV0eM3i0m8noPJ3D5-WZfkAMCZjHfkHHo\nContent-Type: image/jpeg\nContent-Disposition: attachment; filename=\"test-image.jpeg\"\nContent-Security-Policy: script-src 'none'; frame-src 'none'; sandbox;\nX-Nextjs-Cache: HIT\nContent-Length: 134217\nDate: Tue, 23 Sep 2025 09:47:14 GMT\nConnection: keep-alive\nKeep-Alive: timeout=5\n```\n_Now image cached properly and next HTTP HEAD request will be OK_\n5. Make HTTP HEAD request to the image resource again\n```bash\ncurl -IL \"http://localhost:3000/_next/image?url=/_next/static/media/test-image.daca9759.jpg&w=3840&q=75\"\n```\nResult:\n```bash\nHTTP/1.1 200 OK\nVary: Accept\nCache-Control: public, max-age=315360000, immutable\nETag: RyS1td48qGQV0eM3i0m8noPJ3D5-WZfkAMCZjHfkHHo\nContent-Type: image/jpeg\nContent-Disposition: attachment; filename=\"test-image.jpeg\"\nContent-Security-Policy: script-src 'none'; frame-src 'none'; sandbox;\nX-Nextjs-Cache: HIT\nContent-Length: 134217\nDate: Tue, 23 Sep 2025 09:48:48 GMT\nConnection: keep-alive\nKeep-Alive: timeout=5\n```\n---\n\n**Older versions (reproducible on `15.3.X`)**\n\n**first HEAD request** (when the image was not yet in the cache) **poisoned the cache**, and an empty file was stored there, so the following GET requests returned an empty file. \n\n```bash\n# Next 15.3.0\n\n# clear next cache\n\n# DO HTTP GET\n$ curl  -D - -o test.jpg \"http://localhost:4000/_next/image?url=/_next/static/media/file.e2ff9d29.jpg&w=3840&q=74&t=12\"\n\nCache-Control: public, max-age=315360000, immutable\nVary: Accept\nETag: Vy8iMWYyZjkxLTE5OTcxOTNjZjIzIg\nContent-Type: image/jpeg\nContent-Disposition: attachment; filename=\"file.jpeg\"\nContent-Security-Policy: script-src 'none'; frame-src 'none'; sandbox;\nDate: Mon, 22 Sep 2025 13:42:28 GMT\nContent-Length: 2043793\nCache-Tag: nextjs-source\nX-Nextjs-Cache: MISS\n\n# DO HTTP GET AGAIN\n$ curl  -D - -o test.jpg \"http://localhost:4000/_next/image?url=/_next/static/media/file.e2ff9d29.jpg&w=3840&q=74&t=12\"\n\nCache-Control: public, max-age=315360000, immutable\nVary: Accept\nETag: Vy8iMWYyZjkxLTE5OTcxOTNjZjIzIg\nContent-Type: image/jpeg\nContent-Disposition: attachment; filename=\"file.jpeg\"\nContent-Security-Policy: script-src 'none'; frame-src 'none'; sandbox;\nDate: Mon, 22 Sep 2025 13:42:33 GMT\nContent-Length: 2043793\nCache-Tag: nextjs-source\nX-Nextjs-Cache: HIT\n\n# Stable, normal, cached the response with the correct content-length.\n\n# I cleared the cache inside the container, now I make a HEAD request.\n\n$ curl  -IL \"http://localhost:4000/_next/image?url=/_next/static/media/file.e2ff9d29.jpg&w=3840&q=74&t=12\"\nHTTP/1.1 200 OK\nCache-Control: public, max-age=315360000, immutable\nVary: Accept\nETag: Vy8iMWYyZjkxLTE5OTcxOTNjZjIzIg\nContent-Type: image/jpeg\nContent-Disposition: attachment; filename=\"file.jpeg\"\nContent-Security-Policy: script-src 'none'; frame-src 'none'; sandbox;\nDate: Mon, 22 Sep 2025 13:43:23 GMT\nContent-Length: 0 # <--- PROBLEM\nCache-Tag: nextjs-source\nX-Nextjs-Cache: MISS # <--- CACHED AFTER THIS REQUEST\n\n# DO HTTP GET\n$ curl  -D - -o test.jpg \"http://localhost:4000/_next/image?url=/_next/static/media/file.e2ff9d29.jpg&w=3840&q=74&t=12\"\n\nCache-Control: public, max-age=315360000, immutable\nVary: Accept\nETag: Vy8iMWYyZjkxLTE5OTcxOTNjZjIzIg\nContent-Type: image/jpeg\nContent-Disposition: attachment; filename=\"file.jpeg\"\nContent-Security-Policy: script-src 'none'; frame-src 'none'; sandbox;\nDate: Mon, 22 Sep 2025 13:44:22 GMT\nContent-Length: 0 # <--- PROBLEM\nCache-Tag: nextjs-source\nX-Nextjs-Cache: HIT\n\n# Check downloaded file size\nxdd test.jpg | head\n# empty...\n# Something went wrong. The first HEAD cached an empty response, and now GET returns it.\n\n# I cleared the cache inside the container. \n# First, I do a GET to cache a normal response.\n$ curl  -D - -o test.jpg \"http://localhost:4000/_next/image?url=/_next/static/media/file.e2ff9d29.jpg&w=3840&q=74&t=12\"\n\nCache-Control: public, max-age=315360000, immutable\nVary: Accept\nETag: Vy8iMWYyZjkxLTE5OTcxOTNjZjIzIg\nContent-Type: image/jpeg\nContent-Disposition: attachment; filename=\"file.jpeg\"\nContent-Security-Policy: script-src 'none'; frame-src 'none'; sandbox;\nDate: Mon, 22 Sep 2025 13:45:28 GMT\nContent-Length: 2043793 # Normal response\nCache-Tag: nextjs-source\nX-Nextjs-Cache: MISS\n\n$ curl  -IL \"http://localhost:4000/_next/image?url=/_next/static/media/file.e2ff9d29.jpg&w=3840&q=74&t=12\"\nHTTP/1.1 200 OK\nCache-Control: public, max-age=315360000, immutable\nVary: Accept\nETag: Vy8iMWYyZjkxLTE5OTcxOTNjZjIzIg\nContent-Type: image/jpeg\nContent-Disposition: attachment; filename=\"file.jpeg\"\nContent-Security-Policy: script-src 'none'; frame-src 'none'; sandbox;\nDate: Mon, 22 Sep 2025 13:45:31 GMT\nContent-Length: 2043793 # # Normal response\nCache-Tag: nextjs-source\nX-Nextjs-Cache: HIT\n```\n\n### Current vs. Expected behavior\n\n**Current (15.5.x):**\n- First uncached HEAD request \u2192 400 Bad Request with error logs (received null).\n- No image written to cache.\n- Only after a successful GET request, HEAD starts returning 200 OK.\n\n**Expected:**\n- HEAD request should behave like a GET request but without streaming the body.\n- Even if the image is not cached yet, the first HEAD should:\n- Return 200 OK.\n- Include correct headers (Content-Type, Content-Length, ETag, etc.).\n- Either skip writing to cache or safely populate cache metadata without breaking.\n\n### Provide environment information\n\n```bash\nOperating System:\n  Platform: darwin\n  Arch: arm64\n  Version: Darwin Kernel Version 24.6.0: Mon Jul 14 11:30:40 PDT 2025; root:xnu-11417.140.69~1/RELEASE_ARM64_T6041\n  Available memory (MB): 49152\n  Available CPU cores: 12\nBinaries:\n  Node: 22.17.0\n  npm: 10.9.2\n  Yarn: N/A\n  pnpm: 9.11.0\nRelevant Packages:\n  next: 15.5.3 // Latest available version is detected (15.5.3).\n  eslint-config-next: 15.5.3\n  react: 19.1.0\n  react-dom: 19.1.0\n  typescript: 5.9.2\nNext.js Config:\n  output: N/A\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nImage (next/image), Not sure\n\n### Which stage(s) are affected? (Select all that apply)\n\nOther (Deployed), next dev (local), next start (local)\n\n### Versions\n- Next.js 15.3.x \u2192 cache poisoned by HEAD (empty files stored).\n- Next.js 15.5.x \u2192 cache poisoning fixed, but HEAD requests still fail with 400 Bad Request.\n\n### Impact\n- CDNs/load balancers (e.g. Cloudflare, Fastly, etc.) that send HEAD may treat the image route as broken.\n- It prevents reliable header-only introspection of images before cache warming.\n\n<sub>[NEXT-4731](https://linear.app/vercel/issue/NEXT-4731/head-requests-for-nextjs-image-return-400-bad-request-when-image-is)</sub>",
      "solution": "Thanks for the report! I believe you found the root cause to another issue.\n\n- https://github.com/vercel/next.js/issues/82703\n\nI think the solution is we need to coerce HEAD to GET before making the request to the upstream src image. Then we can cache it as usual with the complete body, but then only serve the headers (not body) to fulfill the original HEAD request.\n\nWould you like to submit a PR to fix this?\n\n---\n\nI also have a same issue. So I did put a option in configure and solved it temporary.\n```\nconst nextConfig = {\n  images: {\n    unoptimized: true,\n  },\n};\n```\n\n---\n\n> Thanks for the report! I believe you found the root cause to another issue.\n> \n> * [next/image is not working for some images in production\u00a0#82703](https://github.com/vercel/next.js/issues/82703)\n> \n> I think the solution is we need to coerce HEAD to GET before making the request to the upstream src image. Then we can cache it as usual with the complete body, but then only serve the headers (not body) to fulfill the original HEAD request.\n> \n> Would you like to submit a PR to fix this?\n\nyes ive reproduced the issue by following steps",
      "labels": [
        "Image (next/image)",
        "linear: next"
      ],
      "created_at": "2025-09-23T10:17:40Z",
      "closed_at": "2026-01-29T15:43:20Z",
      "url": "https://github.com/vercel/next.js/issues/84129",
      "comments_count": 10
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 89238,
      "title": "'AGENTS.md outperforms skills in our agent evals' - compressed index is not being created in AGENTS.md",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://vercel.com/blog/agents-md-outperforms-skills-in-our-agent-evals\n\n### To Reproduce\n\n1. npx @next/codemod@canary agents-md\n\nThis functionality is part of the official [@next/codemod package](https://github.com/vercel/next.js/pull/88961).\n\nThis command does three things:\n\n2. Detects your Next.js version\n\n3. Downloads matching documentation to .next-docs/\n\n4. Injects the compressed index into your AGENTS.md - THE COMPRESSED INDEX IS NOT BEING CREATED\n\n### Current vs. Expected behavior\n\nhttps://vercel.com/blog/agents-md-outperforms-skills-in-our-agent-evals\n\n### Provide environment information\n\n```bash\nOperating System:\n  Platform: darwin\n  Arch: arm64\n  Version: Darwin Kernel Version 24.2.0\n  Available memory (MB): 36864\n  Available CPU cores: 12\nBinaries:\n  Node: 20.18.1\n  npm: 10.8.2\n  Yarn: N/A\n  pnpm: N/A\nRelevant Packages:\n  next: 15.5.10\n  eslint-config-next: N/A\n  react: 19.0.0\n  react-dom: 19.0.0\n  typescript: 5.7.3\nNext.js Config:\n  output: N/A\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nNot sure\n\n### Which stage(s) are affected? (Select all that apply)\n\nOther (Deployed)\n\n### Additional context\n\n_No response_",
      "solution": "We could not detect a valid reproduction link. **Make sure to follow the bug report template carefully.**\n\n### Why was this issue closed?\n\nTo be able to investigate, we need access to a reproduction to identify what triggered the issue. We need a link to a **public** GitHub repository ([template for App Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template), [template for Pages Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template-pages)), but you can also use these templates: [CodeSandbox: App Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template) or [CodeSandbox: Pages Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template-pages).\n\nThe bug template that you filled out has a section called \"Link to the code that reproduces this issue\", which is where you should provide the link to the reproduction.\n\n- If you did not provide a link or the link you provided is not valid, we will close the issue.\n- If you provide a link to a private repository, we will close the issue.\n- If you provide a link to a repository but not in the correct section, we will close the issue.\n\n### What should I do?\n\nDepending on the reason the issue was closed, you can do the following:\n\n- If you did not provide a link, please open a new issue with a link to a reproduction.\n- If you provided a link to a private repository, please open a new issue with a link to a public repository.\n- If you provided a link to a repository but not in the correct section, please open a new issue with a link to a reproduction in the correct section.\n\n**In general, assume that we should not go through a lengthy onboarding process at your company code only to be able to verify an issue.**\n\n### My repository is private and cannot make it public\n\nIn most cases, a private repo will not be a sufficient **minimal reproduction**, as this codebase might contain a lot of unrelated parts that would make our investigation take longer. Please do **not** make it public. Instead, create a new repository using the templates above, adding the relevant code to reproduce the issue. Common things to look out for:\n\n- Remove any code that is not related to the issue. (pages, API routes, components, etc.)\n- Remove any dependencies that are not related to the issue.\n- Remove any third-party service that would require us to sign up for an account to reproduce the issue.\n- Remove any environment variables that are not related to the issue.\n- Remove private packages that we do not have access to.\n- If the issue is not related to a monorepo specifically, try to reproduce the issue without a complex monorepo setup\n\n### I did not open this issue, but it is relevant to me, what can I do to help?\n\nAnyone experiencing the same issue is welcome to provide a minimal reproduction following the above steps by opening a new issue.\n\n### I think my reproduction is good enough, why aren't you looking into it quickly?\n\nWe look into every Next.js issue and constantly monitor open issues for new comments.\n\nHowever, sometimes we might miss one or two due to the popularity/high traffic of the repository. We apologize, and kindly ask you to refrain from tagging core maintainers, as that will usually not result in increased priority.\n\nUpvoting issues to show your interest will help us prioritize and address them as quickly as possible. That said, every issue is important to us, and if an issue gets closed by accident, we encourage you to open a new one linking to the old issue and we will look into it.\n\n### Useful Resources\n\n- [How to Contribute to Open Source (Next.js)](https://www.youtube.com/watch?v=cuoNzXFLitc)\n- [How to create a Minimal, Complete, and Verifiable example](https://stackoverflow.com/help/mcve)\n- [Reporting a Next.js bug](https://github.com/vercel/next.js/blob/canary/.github/ISSUE_TEMPLATE/1.bug_report.yml)\n- [Next.js Triaging issues](https://github.com/vercel/next.js/blob/canary/contributing/repository/triaging.md)\n",
      "labels": [
        "invalid link"
      ],
      "created_at": "2026-01-29T12:16:16Z",
      "closed_at": "2026-01-29T12:16:30Z",
      "url": "https://github.com/vercel/next.js/issues/89238",
      "comments_count": 1
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 89207,
      "title": "Turbopack fails to include hoisted monorepo dependencies in Vercel serverless bundles (valibot, @typeschema/valibot)",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/neerajkumar61/turbopack-monorepo-valibot-bug\n\n### To Reproduce\n\n### Verify canary release\n\n- [x] I verified that the issue exists in the latest Next.js canary release\n\n### Provide environment information\n```bash\nOperating System:\n  Platform: linux/darwin/win32\n  Arch: x64/arm64\nBinaries:\n  Node: 18.x / 20.x\n  npm: 10.x\n  Yarn: N/A\n  pnpm: 9.x\nRelevant Packages:\n  next: 15.1.2 (or your version)\n  eslint-config-next: 15.1.2\n  react: 18.3.1\n  react-dom: 18.3.1\n  typescript: 5.3.3\nNext.js Config:\n  output: N/A\n  turbopack: enabled (default in Next.js 15+)\n```\n\n### Which area(s) are affected? (Select all that apply)\n\n- [x] Turbopack (with --turbo or Next.js 15+)\n- [x] Module resolution\n- [x] Monorepo (pnpm/npm/yarn workspaces)\n- [x] Vercel deployment\n\n### Which stage(s) are affected? (Select all that apply)\n\n- [ ] next dev (local)\n- [x] next build (local)\n- [x] next start (local)\n- [x] Vercel (Deployed)\n\n### Additional context\n\n**Deployment platform:** Vercel  \n**Monorepo setup:** pnpm workspaces (or npm/yarn workspaces)  \n**Package manager:** pnpm 9.x\n\n---\n\n## Description\n\nWhen deploying a Next.js 15+ application (using Turbopack by default) in a monorepo to Vercel, dependencies installed in the root `node_modules` are not included in serverless function bundles, causing runtime errors.\n\nThis specifically affects:\n- `valibot` - ESM validation library\n- `@typeschema/valibot` - TypeSchema adapter\n- Potentially other ESM-first packages in monorepo contexts\n\n### The Issue\n\n**What happens:**\n1. \u2705 Local development works (`next dev --turbopack`)\n2. \u2705 Local production build works (`next build`)\n3. \u2705 Local production server works (`next start`)\n4. \u274c **Vercel deployment fails** with module resolution error\n\n**Error on Vercel:**\n```\nError: Cannot find package 'valibot' imported from /var/task/dist/apps/wl/.next/server/chunks/7957.js\n```\n\n**Build metadata from Sentry:**\n```\nturbopack: true\n```\n\n### Expected Behavior\n\nTurbopack should automatically include all resolved dependencies (including those hoisted to root `node_modules` in monorepos) in the serverless function bundle, **just like Webpack does**.\n\nNo manual configuration should be required.\n\n### Actual Behavior\n\nTurbopack's file tracing does not properly traverse up to `../../node_modules/` in monorepo structures, causing hoisted dependencies to be excluded from the Vercel serverless function bundle.\n\n---\n\n## Reproduction\n\n### Minimal Repository Structure\n```\nmonorepo-root/\n\u251c\u2500\u2500 package.json          # Root workspace config\n\u251c\u2500\u2500 pnpm-workspace.yaml   # pnpm workspace definition\n\u251c\u2500\u2500 node_modules/\n\u2502   \u251c\u2500\u2500 valibot/          # \u2190 Installed here (hoisted)\n\u2502   \u2514\u2500\u2500 @typeschema/\n\u2502       \u2514\u2500\u2500 valibot/      # \u2190 Installed here (hoisted)\n\u2514\u2500\u2500 apps/\n    \u2514\u2500\u2500 wl/               # Next.js app\n        \u251c\u2500\u2500 package.json\n        \u251c\u2500\u2500 next.config.js\n        \u2514\u2500\u2500 src/\n            \u2514\u2500\u2500 app/\n                \u2514\u2500\u2500 api/\n                    \u2514\u2500\u2500 route.ts  # \u2190 Imports valibot\n```\n\n### Reproduction Steps\n\n1. **Create monorepo with workspaces:**\n```yaml\n# pnpm-workspace.yaml\npackages:\n  - 'apps/*'\n```\n```json\n// Root package.json\n{\n  \"name\": \"monorepo-root\",\n  \"private\": true,\n  \"dependencies\": {\n    \"valibot\": \"^0.42.1\",\n    \"@typeschema/valibot\": \"^0.1.0\"\n  }\n}\n```\n\n2. **Create Next.js app:**\n```json\n// apps/wl/package.json\n{\n  \"name\": \"wl-app\",\n  \"dependencies\": {\n    \"next\": \"15.1.2\",\n    \"react\": \"18.3.1\",\n    \"react-dom\": \"18.3.1\",\n    \"valibot\": \"^0.42.1\"\n  },\n  \"scripts\": {\n    \"dev\": \"next dev\",\n    \"build\": \"next build\",\n    \"start\": \"next start\"\n  }\n}\n```\n\n3. **Use valibot in API route:**\n```typescript\n// apps/wl/src/app/api/test/route.ts\nimport { object, string, parse } from 'valibot';\n\nconst schema = object({\n  name: string(),\n});\n\nexport async function POST(request: Request) {\n  const data = await request.json();\n  const validated = parse(schema, data);\n  return Response.json({ success: true, data: validated });\n}\n```\n\n4. **Build and deploy:**\n```bash\ncd apps/wl\npnpm build  # \u2705 Works locally\npnpm start  # \u2705 Works locally\n\n# Deploy to Vercel\nvercel --prod  # \u274c Fails with \"Cannot find package 'valibot'\"\n```\n\n### Link to Reproduction Repository\n\n[Optional - you can create a minimal repo if needed]\n\n---\n\n## Current Workaround\n\nAdding `experimental.outputFileTracingIncludes` to manually specify the packages:\n```javascript\n// apps/wl/next.config.js\n/** @type {import('next').NextConfig} */\nconst nextConfig = {\n  experimental: {\n    outputFileTracingIncludes: {\n      '/api/**/*': [\n        '../../node_modules/valibot/**/*',\n        '../../node_modules/@typeschema/valibot/**/*',\n      ],\n      '/*': [\n        '../../node_modules/valibot/**/*',\n        '../../node_modules/@typeschema/valibot/**/*',\n      ],\n    },\n  },\n};\n\nmodule.exports = nextConfig;\n```\n\n**This workaround:**\n- \u2705 Makes deployment work\n- \u274c Should not be necessary\n- \u274c Must be updated for every affected package\n- \u274c Not discoverable (no error message suggests this fix)\n- \u274c Brittle (may break with package updates)\n\n---\n\n## Comparison: Webpack vs Turbopack\n\n### With Webpack (Works Correctly)\n```bash\n# No special configuration needed\nnext build --webpack\nvercel --prod\n# \u2705 Works perfectly - all dependencies included automatically\n```\n\n### With Turbopack (Requires Workaround)\n```bash\n# Default in Next.js 15+\nnext build\nvercel --prod\n# \u274c Fails unless outputFileTracingIncludes is manually configured\n```\n\n**This proves the issue is specific to Turbopack's file tracing/bundling, not the package or monorepo setup.**\n\n---\n\n## Technical Analysis\n\n### Root Cause\n\nNext.js's file tracer (used during Vercel deployment) doesn't properly resolve dependencies when:\n\n1. **Turbopack is used for bundling** (default in Next.js 15+)\n2. **Dependencies are hoisted** to root `node_modules` (monorepo standard)\n3. **Packages are ESM-first** (like valibot)\n\nThe file tracer stops at the app boundary and doesn't traverse up to `../../node_modules/`.\n\n### Why Webpack Works\n\nWebpack's resolver explicitly checks parent directories:\n```javascript\nresolve: {\n  modules: [\n    path.resolve(__dirname, 'node_modules'),\n    path.resolve(__dirname, '../../node_modules'),  // \u2190 Checks parent\n    'node_modules',\n  ],\n}\n```\n\nTurbopack's file tracing doesn't have equivalent logic for production bundles on Vercel.\n\n---\n\n## Impact\n\n### Affected Scenarios\n\n- \u2705 **Monorepo + Turbopack + Vercel** - Fails\n- \u2705 **Monorepo + Webpack + Vercel** - Works\n- \u2705 **Single-repo + Turbopack + Vercel** - Works\n- \u2705 **Monorepo + Turbopack + Local** - Works\n\n### Affected Packages\n\nNot limited to valibot - this affects any ESM package in hoisted monorepo `node_modules`:\n- `valibot`\n- `@typeschema/*`\n- `zod` (in some configurations)\n- `@prisma/client` (in some monorepo setups)\n- Other ESM-first validation/schema libraries\n\n### Scale of Impact\n\n- Affects all Next.js 15+ users in monorepos deploying to Vercel\n- No clear error message or documentation\n- Workaround is not intuitive or discoverable\n- Creates production-specific bugs (works locally, fails deployed)\n\n---\n\n## Expected Resolution\n\nTurbopack's file tracing should:\n\n1. **Automatically detect hoisted dependencies** in monorepo contexts\n2. **Include them in serverless bundles** without manual configuration\n3. **Match Webpack's behavior** for dependency resolution\n4. **Work the same locally and on Vercel**\n\nNo `outputFileTracingIncludes` workaround should be necessary.\n\n---\n\n## Additional Information\n\n### Related Issues\n\n- #71923 - Tailwind CSS fails in Safari with Turbopack (similar bundling issues)\n- #64514 - Monorepo dependencies not resolved correctly\n- #58290 - outputFileTracingIncludes workaround discussion\n\n### Environment Details\n```json\n{\n  \"monorepo\": \"pnpm workspaces\",\n  \"structure\": \"apps/wl (Next.js app in subdirectory)\",\n  \"dependencies\": \"hoisted to root node_modules\",\n  \"deployment\": \"Vercel\",\n  \"nextjs\": \"15.1.2\",\n  \"turbopack\": \"enabled by default\",\n  \"node\": \"20.x\"\n}\n```\n\n### Build Logs\n\n[If you have relevant build logs, include them here]\n```\nBuilding with Turbopack...\n\u2713 Compiled successfully\n...\nError: Cannot find package 'valibot' imported from /var/task/dist/apps/wl/.next/server/chunks/7957.js\n```\n\n---\n\n## Suggested Fix\n\nThe Next.js file tracer should:\n```javascript\n// Pseudo-code for what file tracing should do\nfunction traceModules(entryPoint, depth = 0) {\n  const imports = parseImports(entryPoint);\n  \n  for (const importPath of imports) {\n    // Current behavior: only checks app-level node_modules\n    const resolved = resolve(importPath, { paths: ['./node_modules'] });\n    \n    // Suggested: walk up directory tree like Webpack\n    const resolved = resolve(importPath, { \n      paths: [\n        './node_modules',\n        '../node_modules',\n        '../../node_modules',  // \u2190 Add parent directory checks\n        // ... continue up to project root\n      ]\n    });\n    \n    include(resolved);\n  }\n}\n```\n\nOr better yet, use the **already-resolved module paths from Turbopack's bundling** instead of re-tracing with different resolution logic.\n\n---\n\n## Conclusion\n\nThis is a regression from Webpack's behavior and creates a poor developer experience:\n- Works in development \u2705\n- Works in local production \u2705\n- Fails in deployed production \u274c\n- No helpful error message\n- Requires undocumented workaround\n\nFor Next.js to be truly \"Turbopack by default,\" it should handle monorepo dependencies as seamlessly as Webpack does.\n\nThank you for looking into this!\n\n### Current vs. Expected behavior\n\n**Error on Vercel:**\n```\nError: Cannot find package 'valibot' imported from /var/task/dist/apps/wl/.next/server/chunks/7957.js\n```\n\n**Build metadata from Sentry:**\n```\nturbopack: true\n```\n\n### Expected Behavior\n\nTurbopack should automatically include all resolved dependencies (including those hoisted to root `node_modules` in monorepos) in the serverless function bundle, **just like Webpack does**.\n\nNo manual configuration should be required.\n\n### Actual Behavior\n\nTurbopack's file tracing does not properly traverse up to `../../node_modules/` in monorepo structures, causing hoisted dependencies to be excluded from the Vercel serverless function bundle.\n\n\n### Provide environment information\n\n```bash\nbash\nOperating System:\n  Platform: linux/darwin/win32\n  Arch: x64/arm64\nBinaries:\n  Node: 18.x / 20.x\n  npm: 10.x\n  Yarn: N/A\n  pnpm: 9.x\nRelevant Packages:\n  next: 15.1.2 (or your version)\n  eslint-config-next: 15.1.2\n  react: 18.3.1\n  react-dom: 18.3.1\n  typescript: 5.3.3\nNext.js Config:\n  output: N/A\n  turbopack: enabled (default in Next.js 15+)\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nModule Resolution\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext build (local)\n\n### Additional context\n\n_No response_",
      "solution": "We could not detect a valid reproduction link. **Make sure to follow the bug report template carefully.**\n\n### Why was this issue closed?\n\nTo be able to investigate, we need access to a reproduction to identify what triggered the issue. We need a link to a **public** GitHub repository ([template for App Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template), [template for Pages Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template-pages)), but you can also use these templates: [CodeSandbox: App Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template) or [CodeSandbox: Pages Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template-pages).\n\nThe bug template that you filled out has a section called \"Link to the code that reproduces this issue\", which is where you should provide the link to the reproduction.\n\n- If you did not provide a link or the link you provided is not valid, we will close the issue.\n- If you provide a link to a private repository, we will close the issue.\n- If you provide a link to a repository but not in the correct section, we will close the issue.\n\n### What should I do?\n\nDepending on the reason the issue was closed, you can do the following:\n\n- If you did not provide a link, please open a new issue with a link to a reproduction.\n- If you provided a link to a private repository, please open a new issue with a link to a public repository.\n- If you provided a link to a repository but not in the correct section, please open a new issue with a link to a reproduction in the correct section.\n\n**In general, assume that we should not go through a lengthy onboarding process at your company code only to be able to verify an issue.**\n\n### My repository is private and cannot make it public\n\nIn most cases, a private repo will not be a sufficient **minimal reproduction**, as this codebase might contain a lot of unrelated parts that would make our investigation take longer. Please do **not** make it public. Instead, create a new repository using the templates above, adding the relevant code to reproduce the issue. Common things to look out for:\n\n- Remove any code that is not related to the issue. (pages, API routes, components, etc.)\n- Remove any dependencies that are not related to the issue.\n- Remove any third-party service that would require us to sign up for an account to reproduce the issue.\n- Remove any environment variables that are not related to the issue.\n- Remove private packages that we do not have access to.\n- If the issue is not related to a monorepo specifically, try to reproduce the issue without a complex monorepo setup\n\n### I did not open this issue, but it is relevant to me, what can I do to help?\n\nAnyone experiencing the same issue is welcome to provide a minimal reproduction following the above steps by opening a new issue.\n\n### I think my reproduction is good enough, why aren't you looking into it quickly?\n\nWe look into every Next.js issue and constantly monitor open issues for new comments.\n\nHowever, sometimes we might miss one or two due to the popularity/high traffic of the repository. We apologize, and kindly ask you to refrain from tagging core maintainers, as that will usually not result in increased priority.\n\nUpvoting issues to show your interest will help us prioritize and address them as quickly as possible. That said, every issue is important to us, and if an issue gets closed by accident, we encourage you to open a new one linking to the old issue and we will look into it.\n\n### Useful Resources\n\n- [How to Contribute to Open Source (Next.js)](https://www.youtube.com/watch?v=cuoNzXFLitc)\n- [How to create a Minimal, Complete, and Verifiable example](https://stackoverflow.com/help/mcve)\n- [Reporting a Next.js bug](https://github.com/vercel/next.js/blob/canary/.github/ISSUE_TEMPLATE/1.bug_report.yml)\n- [Next.js Triaging issues](https://github.com/vercel/next.js/blob/canary/contributing/repository/triaging.md)\n",
      "labels": [
        "bug",
        "Webpack",
        "Turbopack",
        "invalid link",
        "Module Resolution",
        "resolved",
        "CSS"
      ],
      "created_at": "2026-01-29T03:57:00Z",
      "closed_at": "2026-01-29T03:57:15Z",
      "url": "https://github.com/vercel/next.js/issues/89207",
      "comments_count": 1
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 89205,
      "title": "Turbopack fails to include hoisted monorepo dependencies in Vercel serverless bundles (valibot, @typeschema/valibot)",
      "problem": "### Link to the code that reproduces this issue\n\ngithub.com\n\n### To Reproduce\n\n### Verify canary release\n\n- [x] I verified that the issue exists in the latest Next.js canary release\n\n\n\n### Which area(s) are affected? (Select all that apply)\n\n- [x] Turbopack (with --turbo or Next.js 15+)\n- [x] Module resolution\n- [x] Monorepo (pnpm/npm/yarn workspaces)\n- [x] Vercel deployment\n\n### Which stage(s) are affected? (Select all that apply)\n\n- [ ] next dev (local)\n- [x] next build (local)\n- [x] next start (local)\n- [x] Vercel (Deployed)\n\n### Additional context\n\n**Deployment platform:** Vercel  \n**Monorepo setup:** pnpm workspaces (or npm/yarn workspaces)  \n**Package manager:** pnpm 9.x\n\n---\n\n## Description\n\nWhen deploying a Next.js 15+ application (using Turbopack by default) in a monorepo to Vercel, dependencies installed in the root `node_modules` are not included in serverless function bundles, causing runtime errors.\n\nThis specifically affects:\n- `valibot` - ESM validation library\n- `@typeschema/valibot` - TypeSchema adapter\n- Potentially other ESM-first packages in monorepo contexts\n\n### The Issue\n\n**What happens:**\n1. \u2705 Local development works (`next dev --turbopack`)\n2. \u2705 Local production build works (`next build`)\n3. \u2705 Local production server works (`next start`)\n4. \u274c **Vercel deployment fails** with module resolution error\n\n**Error on Vercel:**\n```\nError: Cannot find package 'valibot' imported from /var/task/dist/apps/wl/.next/server/chunks/7957.js\n```\n\n**Build metadata from Sentry:**\n```\nturbopack: true\n```\n\n### Expected Behavior\n\nTurbopack should automatically include all resolved dependencies (including those hoisted to root `node_modules` in monorepos) in the serverless function bundle, **just like Webpack does**.\n\nNo manual configuration should be required.\n\n### Actual Behavior\n\nTurbopack's file tracing does not properly traverse up to `../../node_modules/` in monorepo structures, causing hoisted dependencies to be excluded from the Vercel serverless function bundle.\n\n---\n\n## Reproduction\n\n### Minimal Repository Structure\n```\nmonorepo-root/\n\u251c\u2500\u2500 package.json          # Root workspace config\n\u251c\u2500\u2500 pnpm-workspace.yaml   # pnpm workspace definition\n\u251c\u2500\u2500 node_modules/\n\u2502   \u251c\u2500\u2500 valibot/          # \u2190 Installed here (hoisted)\n\u2502   \u2514\u2500\u2500 @typeschema/\n\u2502       \u2514\u2500\u2500 valibot/      # \u2190 Installed here (hoisted)\n\u2514\u2500\u2500 apps/\n    \u2514\u2500\u2500 wl/               # Next.js app\n        \u251c\u2500\u2500 package.json\n        \u251c\u2500\u2500 next.config.js\n        \u2514\u2500\u2500 src/\n            \u2514\u2500\u2500 app/\n                \u2514\u2500\u2500 api/\n                    \u2514\u2500\u2500 route.ts  # \u2190 Imports valibot\n```\n\n### Reproduction Steps\n\n1. **Create monorepo with workspaces:**\n```yaml\n# pnpm-workspace.yaml\npackages:\n  - 'apps/*'\n```\n```json\n// Root package.json\n{\n  \"name\": \"monorepo-root\",\n  \"private\": true,\n  \"dependencies\": {\n    \"valibot\": \"^0.42.1\",\n    \"@typeschema/valibot\": \"^0.1.0\"\n  }\n}\n```\n\n2. **Create Next.js app:**\n```json\n// apps/wl/package.json\n{\n  \"name\": \"wl-app\",\n  \"dependencies\": {\n    \"next\": \"15.1.2\",\n    \"react\": \"18.3.1\",\n    \"react-dom\": \"18.3.1\",\n    \"valibot\": \"^0.42.1\"\n  },\n  \"scripts\": {\n    \"dev\": \"next dev\",\n    \"build\": \"next build\",\n    \"start\": \"next start\"\n  }\n}\n```\n\n3. **Use valibot in API route:**\n```typescript\n// apps/wl/src/app/api/test/route.ts\nimport { object, string, parse } from 'valibot';\n\nconst schema = object({\n  name: string(),\n});\n\nexport async function POST(request: Request) {\n  const data = await request.json();\n  const validated = parse(schema, data);\n  return Response.json({ success: true, data: validated });\n}\n```\n\n4. **Build and deploy:**\n```bash\ncd apps/wl\npnpm build  # \u2705 Works locally\npnpm start  # \u2705 Works locally\n\n# Deploy to Vercel\nvercel --prod  # \u274c Fails with \"Cannot find package 'valibot'\"\n```\n\n### Link to Reproduction Repository\n\n[Optional - you can create a minimal repo if needed]\n\n---\n\n## Current Workaround\n\nAdding `experimental.outputFileTracingIncludes` to manually specify the packages:\n```javascript\n// apps/wl/next.config.js\n/** @type {import('next').NextConfig} */\nconst nextConfig = {\n  experimental: {\n    outputFileTracingIncludes: {\n      '/api/**/*': [\n        '../../node_modules/valibot/**/*',\n        '../../node_modules/@typeschema/valibot/**/*',\n      ],\n      '/*': [\n        '../../node_modules/valibot/**/*',\n        '../../node_modules/@typeschema/valibot/**/*',\n      ],\n    },\n  },\n};\n\nmodule.exports = nextConfig;\n```\n\n**This workaround:**\n- \u2705 Makes deployment work\n- \u274c Should not be necessary\n- \u274c Must be updated for every affected package\n- \u274c Not discoverable (no error message suggests this fix)\n- \u274c Brittle (may break with package updates)\n\n---\n\n## Comparison: Webpack vs Turbopack\n\n### With Webpack (Works Correctly)\n```bash\n# No special configuration needed\nnext build --webpack\nvercel --prod\n# \u2705 Works perfectly - all dependencies included automatically\n```\n\n### With Turbopack (Requires Workaround)\n```bash\n# Default in Next.js 15+\nnext build\nvercel --prod\n# \u274c Fails unless outputFileTracingIncludes is manually configured\n```\n\n**This proves the issue is specific to Turbopack's file tracing/bundling, not the package or monorepo setup.**\n\n---\n\n## Technical Analysis\n\n### Root Cause\n\nNext.js's file tracer (used during Vercel deployment) doesn't properly resolve dependencies when:\n\n1. **Turbopack is used for bundling** (default in Next.js 15+)\n2. **Dependencies are hoisted** to root `node_modules` (monorepo standard)\n3. **Packages are ESM-first** (like valibot)\n\nThe file tracer stops at the app boundary and doesn't traverse up to `../../node_modules/`.\n\n### Why Webpack Works\n\nWebpack's resolver explicitly checks parent directories:\n```javascript\nresolve: {\n  modules: [\n    path.resolve(__dirname, 'node_modules'),\n    path.resolve(__dirname, '../../node_modules'),  // \u2190 Checks parent\n    'node_modules',\n  ],\n}\n```\n\nTurbopack's file tracing doesn't have equivalent logic for production bundles on Vercel.\n\n---\n\n## Impact\n\n### Affected Scenarios\n\n- \u2705 **Monorepo + Turbopack + Vercel** - Fails\n- \u2705 **Monorepo + Webpack + Vercel** - Works\n- \u2705 **Single-repo + Turbopack + Vercel** - Works\n- \u2705 **Monorepo + Turbopack + Local** - Works\n\n### Affected Packages\n\nNot limited to valibot - this affects any ESM package in hoisted monorepo `node_modules`:\n- `valibot`\n- `@typeschema/*`\n- `zod` (in some configurations)\n- `@prisma/client` (in some monorepo setups)\n- Other ESM-first validation/schema libraries\n\n### Scale of Impact\n\n- Affects all Next.js 15+ users in monorepos deploying to Vercel\n- No clear error message or documentation\n- Workaround is not intuitive or discoverable\n- Creates production-specific bugs (works locally, fails deployed)\n\n---\n\n## Expected Resolution\n\nTurbopack's file tracing should:\n\n1. **Automatically detect hoisted dependencies** in monorepo contexts\n2. **Include them in serverless bundles** without manual configuration\n3. **Match Webpack's behavior** for dependency resolution\n4. **Work the same locally and on Vercel**\n\nNo `outputFileTracingIncludes` workaround should be necessary.\n\n---\n\n## Additional Information\n\n### Related Issues\n\n- #71923 - Tailwind CSS fails in Safari with Turbopack (similar bundling issues)\n- #64514 - Monorepo dependencies not resolved correctly\n- #58290 - outputFileTracingIncludes workaround discussion\n\n### Environment Details\n```json\n{\n  \"monorepo\": \"pnpm workspaces\",\n  \"structure\": \"apps/wl (Next.js app in subdirectory)\",\n  \"dependencies\": \"hoisted to root node_modules\",\n  \"deployment\": \"Vercel\",\n  \"nextjs\": \"15.1.2\",\n  \"turbopack\": \"enabled by default\",\n  \"node\": \"20.x\"\n}\n```\n\n### Build Logs\n\n[If you have relevant build logs, include them here]\n```\nBuilding with Turbopack...\n\u2713 Compiled successfully\n...\nError: Cannot find package 'valibot' imported from /var/task/dist/apps/wl/.next/server/chunks/7957.js\n```\n\n---\n\n## Suggested Fix\n\nThe Next.js file tracer should:\n```javascript\n// Pseudo-code for what file tracing should do\nfunction traceModules(entryPoint, depth = 0) {\n  const imports = parseImports(entryPoint);\n  \n  for (const importPath of imports) {\n    // Current behavior: only checks app-level node_modules\n    const resolved = resolve(importPath, { paths: ['./node_modules'] });\n    \n    // Suggested: walk up directory tree like Webpack\n    const resolved = resolve(importPath, { \n      paths: [\n        './node_modules',\n        '../node_modules',\n        '../../node_modules',  // \u2190 Add parent directory checks\n        // ... continue up to project root\n      ]\n    });\n    \n    include(resolved);\n  }\n}\n```\n\nOr better yet, use the **already-resolved module paths from Turbopack's bundling** instead of re-tracing with different resolution logic.\n\n---\n\n## Conclusion\n\nThis is a regression from Webpack's behavior and creates a poor developer experience:\n- Works in development \u2705\n- Works in local production \u2705\n- Fails in deployed production \u274c\n- No helpful error message\n- Requires undocumented workaround\n\nFor Next.js to be truly \"Turbopack by default,\" it should handle monorepo dependencies as seamlessly as Webpack does.\n\nThank you for looking into this!\n\n### Current vs. Expected behavior\n\n\n**Error on Vercel:**\n```\nError: Cannot find package 'valibot' imported from /var/task/dist/apps/wl/.next/server/chunks/7957.js\n```\n\n**Build metadata from Sentry:**\n```\nturbopack: true\n```\n\n### Expected Behavior\n\nTurbopack should automatically include all resolved dependencies (including those hoisted to root `node_modules` in monorepos) in the serverless function bundle, **just like Webpack does**.\n\nNo manual configuration should be required.\n\n### Actual Behavior\n\nTurbopack's file tracing does not properly traverse up to `../../node_modules/` in monorepo structures, causing hoisted dependencies to be excluded from the Vercel serverless function bundle.\n\n---\n\n### Provide environment information\n\n```bash\n### Provide environment information\n\nOperating System:\n  Platform: linux/darwin/win32\n  Arch: x64/arm64\nBinaries:\n  Node: 18.x / 20.x\n  npm: 10.x\n  Yarn: N/A\n  pnpm: 9.x\nRelevant Packages:\n  next: 15.1.2 (or your version)\n  eslint-config-next: 15.1.2\n  react: 18.3.1\n  react-dom: 18.3.1\n  typescript: 5.3.3\nNext.js Config:\n  output: N/A\n  turbopack: enabled (default in Next.js 15+)\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nModule Resolution\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext build (local)\n\n### Additional context\n\n_No response_",
      "solution": "We could not detect a valid reproduction link. **Make sure to follow the bug report template carefully.**\n\n### Why was this issue closed?\n\nTo be able to investigate, we need access to a reproduction to identify what triggered the issue. We need a link to a **public** GitHub repository ([template for App Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template), [template for Pages Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template-pages)), but you can also use these templates: [CodeSandbox: App Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template) or [CodeSandbox: Pages Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template-pages).\n\nThe bug template that you filled out has a section called \"Link to the code that reproduces this issue\", which is where you should provide the link to the reproduction.\n\n- If you did not provide a link or the link you provided is not valid, we will close the issue.\n- If you provide a link to a private repository, we will close the issue.\n- If you provide a link to a repository but not in the correct section, we will close the issue.\n\n### What should I do?\n\nDepending on the reason the issue was closed, you can do the following:\n\n- If you did not provide a link, please open a new issue with a link to a reproduction.\n- If you provided a link to a private repository, please open a new issue with a link to a public repository.\n- If you provided a link to a repository but not in the correct section, please open a new issue with a link to a reproduction in the correct section.\n\n**In general, assume that we should not go through a lengthy onboarding process at your company code only to be able to verify an issue.**\n\n### My repository is private and cannot make it public\n\nIn most cases, a private repo will not be a sufficient **minimal reproduction**, as this codebase might contain a lot of unrelated parts that would make our investigation take longer. Please do **not** make it public. Instead, create a new repository using the templates above, adding the relevant code to reproduce the issue. Common things to look out for:\n\n- Remove any code that is not related to the issue. (pages, API routes, components, etc.)\n- Remove any dependencies that are not related to the issue.\n- Remove any third-party service that would require us to sign up for an account to reproduce the issue.\n- Remove any environment variables that are not related to the issue.\n- Remove private packages that we do not have access to.\n- If the issue is not related to a monorepo specifically, try to reproduce the issue without a complex monorepo setup\n\n### I did not open this issue, but it is relevant to me, what can I do to help?\n\nAnyone experiencing the same issue is welcome to provide a minimal reproduction following the above steps by opening a new issue.\n\n### I think my reproduction is good enough, why aren't you looking into it quickly?\n\nWe look into every Next.js issue and constantly monitor open issues for new comments.\n\nHowever, sometimes we might miss one or two due to the popularity/high traffic of the repository. We apologize, and kindly ask you to refrain from tagging core maintainers, as that will usually not result in increased priority.\n\nUpvoting issues to show your interest will help us prioritize and address them as quickly as possible. That said, every issue is important to us, and if an issue gets closed by accident, we encourage you to open a new one linking to the old issue and we will look into it.\n\n### Useful Resources\n\n- [How to Contribute to Open Source (Next.js)](https://www.youtube.com/watch?v=cuoNzXFLitc)\n- [How to create a Minimal, Complete, and Verifiable example](https://stackoverflow.com/help/mcve)\n- [Reporting a Next.js bug](https://github.com/vercel/next.js/blob/canary/.github/ISSUE_TEMPLATE/1.bug_report.yml)\n- [Next.js Triaging issues](https://github.com/vercel/next.js/blob/canary/contributing/repository/triaging.md)\n",
      "labels": [
        "bug",
        "Webpack",
        "Turbopack",
        "invalid link",
        "Module Resolution",
        "resolved",
        "CSS"
      ],
      "created_at": "2026-01-29T03:30:10Z",
      "closed_at": "2026-01-29T03:30:25Z",
      "url": "https://github.com/vercel/next.js/issues/89205",
      "comments_count": 1
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 88534,
      "title": "[Turbopack] Build fails with \"module factory is not available\" when using `export * as X from \".\"`",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/roguesherlock/turbopack-export-star-repro\n\n### To Reproduce\n\n1. Clone the reproduction repo: `git clone https://github.com/roguesherlock/turbopack-export-star-repro`\n2. Install dependencies: `bun install`\n3. Build with Turbopack: `bun run build`\n4. Observe the build failure\n\n\n### Current vs. Expected behavior\n\n**Current:**\nBuild fails with:\n```\nError: Module 8147 was instantiated because it was required from module 56356, but the module factory is not available.\n```\n\n**Expected:**\nBuild should complete successfully. The `export * as X from \".\"` pattern is valid TypeScript and works with Webpack (non-Turbopack builds).\n\n**Context:**\nThis pattern is commonly used to create namespace-like exports:\n```typescript\n// lib/user/index.ts\nexport * as User from \".\"\nexport function fromSession() { ... }\n\n// Usage:\nimport { User } from \"@/lib/user\"\nawait User.fromSession()\n```\n\n**Workaround:**\nReplace with TypeScript namespaces:\n```typescript\nexport namespace User {\n  export function fromSession() { ... }\n}\n```\n\n\n### Provide environment information\n\n```bash\nOperating System:\n  Platform: darwin\n  Arch: arm64\n  Version: Darwin 25.2.0\nBinaries:\n  Node: 22.12.0\n  npm: 10.9.0\n  Yarn: 1.22.22\n  pnpm: 9.15.4\nRelevant Packages:\n  next: 16.1.1\n  react: 19.0.0\n  react-dom: 19.0.0\n  typescript: 5.9.3\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nTurbopack\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext build (local)\n\n### Additional context\n\n- Dev mode (`next dev --turbopack`) works fine\n- Only fails during production build\n- Works correctly with Webpack (without Turbopack)\n",
      "solution": "## Related Issue: Runtime crash with TypeScript namespaces + `import type` from `server-only` module\n\nAfter switching to TypeScript namespaces as a workaround, we encountered a related runtime issue in production builds.\n\n### Setup\n\n1. A module with `\"server-only\"` directive using TypeScript namespace:\n```typescript\n// lib/subscription/index.ts\nimport \"server-only\"\nimport type { Cart } from \"../cart\"\n\nexport namespace Subscription {\n  export type Subscription = {\n    id: string\n    status: string\n    addresses?: { billingAddress: Cart.Address }\n  }\n  \n  export async function list() { /* server-only logic */ }\n}\n```\n\n2. A utility file in the same directory using `import type`:\n```typescript\n// lib/subscription/util.ts\nimport type { Subscription } from \".\"\n\nexport function getSubscriptionState(subscription: Subscription.Subscription) {\n  // Pure utility function, no server-only dependencies\n  return { canCancel: subscription.status === \"active\" }\n}\n```\n\n3. A client component importing the utility:\n```typescript\n// app/subscriptions/subscription-details.tsx\n\"use client\"\nimport { getSubscriptionState } from \"@/lib/subscription/util\"\nimport type { Subscription } from \"@/lib/subscription\"\n\nexport function SubscriptionDetails({ subscription }: { subscription: Subscription.Subscription }) {\n  const { canCancel } = getSubscriptionState(subscription)\n  // ...\n}\n```\n\n### Expected Behavior\nSince `util.ts` only uses `import type` (which should be erased at compile time), it should be safe to import in client components.\n\n### Actual Behavior\nIn production builds, navigating to the page crashes with:\n```\nError: Module XXXXX was instantiated because it was required from module subscription-details_tsx_XXXXX, but the module factory is not available.\n```\n\n### Notes\n- Works fine in development mode\n- Works fine with Webpack builds\n- Only fails in Turbopack production builds\n- The `import type` should be completely erased and not trigger module instantiation\n\nWe couldn't create a minimal reproduction for this specific issue, but it consistently occurs in our production codebase. It seems like Turbopack may not be properly tree-shaking `import type` statements when the source module has a `\"server-only\"` directive.\n\n\n---\n\nI had a similar issue that has already been fixed in [v16.1.1-canary.19](https://github.com/vercel/next.js/releases/tag/v16.1.1-canary.19) https://github.com/vercel/next.js/pull/86131. You could try out that version; it worked for me, but there is no stable version yet.\n\n---\n\n> I had a similar issue that has already been fixed in [v16.1.1-canary.19](https://github.com/vercel/next.js/releases/tag/v16.1.1-canary.19) [#86131](https://github.com/vercel/next.js/pull/86131). You could try out that version; it worked for me, but there is no stable version yet.\n\nomg that worked! thanks time!\n\nhappy to close this if team feels it's good enough",
      "labels": [
        "Turbopack",
        "locked"
      ],
      "created_at": "2026-01-14T11:21:13Z",
      "closed_at": "2026-01-14T14:19:42Z",
      "url": "https://github.com/vercel/next.js/issues/88534",
      "comments_count": 5
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 87713,
      "title": "SWC panic with MDX files containing Japanese (multibyte) characters in 16.1.0+",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/katanabe/nextjs-mdx-multibyte-bug\n\n### To Reproduce\n\n1. Clone the reproduction repo\n2. Run `npm install`\n3. Run `npm run build`\n\nThe MDX file (`app/test/page.mdx`) contains Japanese text with full-width parentheses:\n\n```mdx\nexport const metadata = {\n  title: \"Test\",\n};\n\n# Test\n\n\u3053\u308c\u306f\u30c6\u30b9\u30c8\uff08\u30b5\u30f3\u30d7\u30eb\uff09\u3067\u3059\u3002\n```\n\nThe issue is triggered by **multibyte characters in the MDX body** (e.g., `\uff08\uff09`), not by `export const metadata` content.\n\n### Current vs. Expected behavior\n\n**Current behavior:**\nBuild fails with SWC panic:\n\n```\nthread '<unnamed>' panicked at swc_common-18.0.0/src/source_map.rs:657:62:\nbyte index 88 is not a char boundary; it is inside '\uff08' (bytes 87..90)\n```\n\n**Expected behavior:**\nBuild should succeed, as it does in Next.js 16.0.10.\n\n### Provide environment information\n\n```\nOperating System:\n  Platform: darwin\n  Arch: arm64\n  Version: Darwin 25.1.0\n  Available memory (MB): 65536\n  Available CPU cores: 12\nBinaries:\n  Node: 24.11.1\n  npm: 11.3.0\n  pnpm: 9.15.9\nRelevant Packages:\n  next: 16.1.1\n  react: 19.0.0\n  react-dom: 19.0.0\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nSWC (minification/transpilation), Turbopack\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext build (default compiler)\n\n### Additional context\n\n**Version comparison:**\n| Version | Result |\n|---------|--------|\n| 16.0.10 | \u2705 Build succeeds |\n| 16.1.0  | \u274c SWC panic |\n| 16.1.1  | \u274c SWC panic |\n\n**Likely cause:**\nThis regression was likely introduced by the SWC version bump in 16.1.0:\n- #86240 (Bump to SWC 48)\n- #86689 (Bump to SWC 49)\n\nThe issue occurs when Rust string slicing attempts to access a byte index that falls in the middle of a multibyte UTF-8 character (Japanese full-width parenthesis `\uff08` is 3 bytes).",
      "solution": "### Additional finding\n\nThe issue is triggered by **multibyte characters in the MDX body**, not specifically by `export const metadata`.\n\n- English-only MDX: \u2705 No panic\n- MDX with Japanese full-width parentheses `\uff08\uff09` in body: \u274c Panic\n\nUpdated reproduction repo to clarify this.\n\n---\n\nThank you for your report!\n\nIt seems like this was likely fixed by swc-project/swc#11372. We will likely publish a patch release of 16.1 with an updated version of SWC to address this, but that may take some time as most people are out-of-office right now for the holidays.\n\n---\n\nBackported in https://github.com/vercel/next.js/pull/88296. This should be fixed in 16.1.2, when we release that (no current ETA, there are are other things getting backported).",
      "labels": [
        "SWC",
        "Turbopack",
        "locked"
      ],
      "created_at": "2025-12-23T09:51:51Z",
      "closed_at": "2026-01-07T05:56:30Z",
      "url": "https://github.com/vercel/next.js/issues/87713",
      "comments_count": 5
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 85914,
      "title": "Memory Leak with node 22 (also 20, 24) + fetch + Next 16.0.1 + `output: standalone`",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/austin-raey/next-memory-issue\n\n### To Reproduce\n\n1. Run `pnpm install`\n2. In `src/app/page.tsx`, replace `\"http://put-large-api-json-request-here.test\"` with some JSON endpoint. I didn't want to put someone's URL here as to not DDOS them haha. Preferably one where you are not rate limited. You can probably separately set up a seperate folder with an [`express`](https://expressjs.com) (or similar) API endpoint that returns a large JSON payload as to not DOS anyone. As to not pollute the test, it may be best for you to not make this JSON endpoint in this example Next repo itself (this may not make a difference though tbf)\n3. With Docker Desktop, Rancher Desktop, or etc running, do `make run`.\n4. Open an Inspector session in Chrome / Edge / etc. (`about:inspect` in Chromium browsers)\n  _4a._ Optionally, go to the `http://localhost:3000` page in your browser once, reload the page a few (2 or 3) times, then click \"Collect garbage\" a few times to ensure a clean heap, then \"Take snapshot\" of the heap to collect a baseline.\n\n<details><summary>Image of the \"Collect Garbage\" button in Chromium browsers</summary>\n\n![Image](https://github.com/user-attachments/assets/176bb8a5-8457-43e4-bf9c-2e5209ee9138)\n\n</details>\n\n\n5. In a seperate terminal tab in the example repository, run `pnpm exec autocannon -R 30 -f http://localhost:3000`. Let this run for a few minutes or so. `-R 30` may be adjusted for more requests, but 30 is probably fine.\n6. After waiting, hit CTRL + C to stop autocannon.\n7. Go to the Inspector. Before taking the heap snapshot, click the \"Collect garbage\" broom icon to ensure the heap is clean. Note the heap is now larger than baseline and collecting garbage will not reduce this size.\n8. Inspecting the heap, it seems a large portion is now taken up by fetch. There is also `Array` and `InternalReadableByteStream` which seems suspicious, but I am not sure if they are relevant:\n\n<details><summary>Image of the suspicious items in heap</summary>\n\n![Image](https://github.com/user-attachments/assets/16b5d4fc-ddb9-4732-9d5d-03ce3546adb8)\n\n![Image](https://github.com/user-attachments/assets/023dffce-403c-426a-9033-a896a0a412a7)\n\n</details>\n\n\n### Current vs. Expected behavior\n\nI would expect the heap, once garbaged collected, to be similar in size to the baseline heap. Instead, the heap size is now permanently larger and it seems a large portion is taken up by `fetch`.\n\nThis larger size is sticky and I haven't seen it ever go back down even after waiting several minutes with no requests made to `http://localhost:3000`. Manual garbage collection makes no change either.\n\n### Provide environment information\n\n```bash\nPlatform: darwin\n  Arch: arm64\n  Version: Darwin Kernel Version 25.1.0: Mon Oct 20 19:34:05 PDT 2025; root:xnu-12377.41.6~2/RELEASE_ARM64_T6041\n  Available memory (MB): 49152\n  Available CPU cores: 14\nBinaries:\n  Node: 25.1.0\n  npm: 11.6.2\n  Yarn: N/A\n  pnpm: 10.20.0\nRelevant Packages:\n  next: 16.0.1 // Latest available version is detected (16.0.1).\n  eslint-config-next: N/A\n  react: 19.2.0\n  react-dom: 19.2.0\n  typescript: 5.9.3\nNext.js Config:\n  output: standalone\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nNot sure, Runtime\n\n### Which stage(s) are affected? (Select all that apply)\n\nOther (Deployed)\n\n### Additional context\n\n- I have seen other tickets make note of this and were able to fix it with a specific version of node, although unfortunately this made no difference for me. I have tried using other of the latest LTS versions of node (20, 22, 24 at the time of writing 24.11.0, 22.21.1, 20.19.5) and it seems this issue still persists on each of these versions. You can easily try different node versions by modifying the first line of the Dockerfile.\n- I've also observed this issue on the latest version of Next 15 (15.5.6 at the time of writing).\n- Using [axios](https://www.npmjs.com/package/axios) instead of `fetch` (which I believe doesn't use undici) seems to help with this issue. Of course, this is not ideal, because Next provides nice additions to `fetch` for caching.",
      "solution": "For my part, it has been resolved. The problem was a cyclic dependency within the next-auth authentication to refresh the token.\n\n---\n\nYep, to echo @pablohpsilva, I simply used `create-next-app` at the time to create my example repo, with a couple extras for testing (Docker configuration for ease of switching node versions / building & running, autocannon for stress testing). \n\nI am probably going to revisit this issue soon with the latest version at time of writing (16.1.0) to see if the issue persists, whenever I find some time.\n\nI'm seeing on my template repo I have a couple extraneous deps (mainly `tailwindcss`, I'll check if there's any others) and I'd like to make this as minimal a repro as possible. I don't think this will affect things, but I digress haha.\n\n---\n\nSpent the last ~2-3 days investigating this and haven't been able to find a clear leak. Even measuring the memory usage it is showing the garbage collecting bringing the chart back down to the level it was at before, no linear growth like in what @deshazer is posting above. \n\nMy suspicion would be that for some it's related to the Node.js version. There were some memory leaks in Undici (Node.js's fetch implementation that have been fixed in patch releases of the Node.js 20 and later major versions.\n\nInvestigation: https://github.com/vercel/next.js/pull/88577#issue-3816662733",
      "labels": [
        "Runtime",
        "linear: next"
      ],
      "created_at": "2025-11-07T19:43:21Z",
      "closed_at": "2026-01-19T15:03:17Z",
      "url": "https://github.com/vercel/next.js/issues/85914",
      "comments_count": 13
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 83183,
      "title": "On the custom server, DeprecationWarning: `url.parse()` warning!",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/dincerpece/custom-server-warning-feedback\n\n### To Reproduce\n\n \"custom-server-dev\": \"npx tsx nextjs-server/index.ts\"\n\nSince the release of Node.js 24.7.0 on August 27th, I\u2019ve been getting a warning.\nMy previous nodejs version was 23.11.1 without any problems\n\n<img width=\"1855\" height=\"293\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/4fa53dbd-f627-49d6-970f-b927bcd8ddd7\" />\n\n<img width=\"749\" height=\"560\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e4ead881-d9f3-43ca-8a70-416f855403a1\" />\n\n<img width=\"889\" height=\"677\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/a7f161f1-307d-4fbc-865a-f5f0d26bfee0\" />\n\n<img width=\"1159\" height=\"874\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/2417fd47-9930-4111-a1ba-f693912a1b92\" />\n\n### Current vs. Expected behavior\n\n\n\nI received a warning when working with a custom server after NodeJS 24.7.0. The project is running, but I receive a warning. I only receive this warning when working with a custom server. I receive this warning with the example on the NextJS website!\n\n### Provide environment information\n\n```bash\nOperating System:\n  Platform: win32\n  Arch: x64\n  Version: Windows 11 Pro\n  Available memory (MB): 32607\n  Available CPU cores: 16\nBinaries:\n  Node: 24.7.0\n  npm: 11.1.0\n  Yarn: 1.22.22\n  pnpm: N/A\nRelevant Packages:\n  next: 15.5.1-canary.16 // Latest available version is detected (15.5.1-canary.16).\n  eslint-config-next: 15.5.1-canary.16\n  react: 19.1.0\n  react-dom: 19.1.0\n  typescript: 5.9.2\nNext.js Config:\n  output: N/A\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nRuntime, Performance, Not sure\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext dev (local), next start (local)\n\n### Additional context\n\nWhen I received this warning in my own project, I tried a clean project and received the warning again, both in dev and production. Using the example on the Nextjs website, I received this warning on a custom server with NodeJS 24.7.0.",
      "solution": "Mmm but you are using `parse` from `url` though... if you bring up the Node REPL\n\n```console\n\u279c node\nWelcome to Node.js v24.7.0.\nType \".help\" for more information.\n> const { parse } = require(\"url\")\nundefined\n> parse(\"https://Hello-world.com\")\nUrl {\n  protocol: 'https:',\n  slashes: true,\n  auth: null,\n  host: 'hello-world.com',\n  port: null,\n  hostname: 'hello-world.com',\n  hash: null,\n  search: null,\n  query: null,\n  pathname: '/',\n  path: '/',\n  href: 'https://hello-world.com/'\n}\n> (node:40863) [DEP0169] DeprecationWarning: `url.parse()` behavior is not standardized and prone to errors that have security implications. Use the WHATWG URL API instead. CVEs are not issued for `url.parse()` vulnerabilities.\n(Use `node --trace-deprecation ...` to show where the warning was created)\n```\n\nI guess you need to follow the recommendation given by Node.js ~\n\nI used create-next-app with node 24.7.0 - and it didn't print this issue for dev, build, nor start.\n\nAs per docs, we likely have to add a note there about Node's 24 deprecation plans - or use some other approach \n\nIt is a bit tricky with this kind of deprecation - I'll see if we can do something\n\n---\n\nYeah, the problem is really that Next's RequestHandler type bakes in a `parsedUrl: NextUrlWithParsedQuery` which takes the shape of node's `url.parse()` return value. If you try to swap out for `new URL()`, the types no longer line up which causes TS compilation errors and it's not clear to me what runtime issues would arise downstream. NextJS implicitly forces use of `url.parse()` so there is no workaround until Next changes the types it expects.\n\nI very much would like to be wrong about this, so if someone knows a way around this, please let me know!\n\n----\n\nIn next/dist/server/next.d.ts:\n\n```\nexport interface RequestHandler {\n    (req: IncomingMessage, res: ServerResponse, parsedUrl?: NextUrlWithParsedQuery | undefined): Promise<void>;\n}\n```\n\nIn next/dist/server/request-meta.d.ts:\n\n```\nexport interface NextUrlWithParsedQuery extends UrlWithParsedQuery {\n    query: NextParsedUrlQuery;\n}\n```\n\n`UrlWithParsedQuery` comes from `node`'s types and is extended by Next.\n\n---\n\n> getting lots of these since vercel enabled node v24 in their envs.\n> \n> I have no custom code using url.parse, so i image it is used in next somewhere ?\n\nI believe packages like `proxy-from-env` utilize `url.parse` and are dependencies.\nThis is the issue already being raised: https://github.com/Rob--W/proxy-from-env/issues/30.\nHope this helps!",
      "labels": [
        "Performance",
        "Runtime",
        "locked"
      ],
      "created_at": "2025-08-28T18:06:17Z",
      "closed_at": "2026-01-13T18:11:08Z",
      "url": "https://github.com/vercel/next.js/issues/83183",
      "comments_count": 11
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 37362,
      "title": "Cancel Rendering Route Error when Router Push hash",
      "problem": "### Verify canary release\n\n- [X] I verified that the issue exists in Next.js canary release\n\n### Provide environment information\n\n```\r\n    Operating System:\r\n      Platform: darwin\r\n      Arch: arm64\r\n      Version: Darwin Kernel Version 21.5.0: Tue Apr 26 21:08:37 PDT 2022; root:xnu-8020.121.3~4/RELEASE_ARM64_T6000\r\n    Binaries:\r\n      Node: 16.15.0\r\n      npm: 8.5.5\r\n      Yarn: 1.22.19\r\n      pnpm: N/A\r\n    Relevant packages:\r\n      next: 12.1.7-canary.26\r\n      react: 18.1.0\r\n      react-dom: 18.1.0\r\n```\n\n### What browser are you using? (if relevant)\n\nGoogle Chrome Version 102.0.5005.61 (Official Build) (arm64)\n\n### How are you deploying your application? (if relevant)\n\nLocally and Vercel\n\n### Describe the Bug\n\nSimilar to https://github.com/vercel/next.js/issues/36830 (fixed by https://github.com/vercel/next.js/pull/36828) \r\n\r\ntrying to store more information in hash, getting \"Cancel rendering route\" error when pushing updated hash to router:\r\n\r\n![image](https://user-images.githubusercontent.com/7423207/171391796-788ecda1-f347-4e22-bdb8-98471f1074a6.png)\r\n\n\n### Expected Behavior\n\nPages changes without error.\n\n### To Reproduce\n\n```\r\ngit clone https://github.com/michalbundyra/hash-cancel-rendering-route\r\ncd hash-cancel-rendering-route\r\nyarn\r\nyarn dev\r\n```\r\n\r\n1. Go to http://localhost:3000/\r\n2. Click \"to world\"\r\n3. Error pops!\r\n",
      "solution": "@icyJoseph thanks for your response. I think the problem is more complex, and I am not sure if I can use await/then. I have simplified to the very basic case based on the previous issue.\r\n\r\nIn my case I want to store in the hash tab-id and filters for a specific tab. There might be default filters for a tab, but these are set on the component, so the tab change is though `Link` and then route.push just update the hash again to set default filters. So honestly with this approach I cannot await and I hoped it could be fixed in the framework itself, as the very similar issue was.\r\n\r\nPlease let me know what you think. Thanks a lot!\n\n---\n\nI figured the reproduction demo was oversimplified, but it was worth a try \ud83d\ude05 \r\n\r\nYeah, I mean this should somehow be fixed here, I might try to take a stab at it again, I saw that a maintainer added a test for my fix, so I can try a bit more stuff there.\r\n\r\nSo you first navigate to a tab, then each tab has a set of properties that are pushed to the url as part of the hash\r\n\r\n```\r\n#tab2-sortasc-radios-brand\r\n```\r\n\r\nSomething like that, but first you move onto the `#tab2` state, and then when the component for that tab mounts, it updates to the final hash, right?\r\n\r\nAh this could be done in other way, but it'll be unmaintainable I think (using refs and what not).\r\n\r\nNever mind that perhaps this has to be fixed, and that sequential calls to async work do not necessarily resolve in order, have you considered using queries for the filters? \r\n\r\nNormally queries are key, value pairs, but they don't have to be so, and in the case you describe, it sounds like they are. I would hope that the same error doesn't happen, if one updates the queries on the page, with shallow set to true.\r\n\r\n\n\n---\n\n@icyJoseph thank you very much for your time and response.\r\n\r\n> Something like that, but first you move onto the #tab2 state, and then when the component for that tab mounts, it updates to the final hash, right?\r\n\r\nCorrect.\r\n\r\n>  have you considered using queries for the filters?\r\n\r\nYeah, I have considered using query params for the filters, but it was also quite complex, as still I have tabs and can have different set of the filters on different tab. So I would need to clean query parameters when jumping between tabs. For me preferably would be to use just path + query filters, but we have tabs - and the current tab is using hash. Changing query params refreshes parent component so clean the state of current... probably I could do something here... probably.\r\n\r\nI started investigating another approach - to set the default filter on tabs directly, and it also solves the issue: so my link to the tab is `#tab-with-filters` instead of `#tab` so there is no extra router push on binding the tab component.\r\n\r\nProbably this is going to be the best solution for me and it would work with current stable release as well.\r\nI just would need to move my default filters from the tab component one level up, that's why I didn't like it too much.\r\n\r\nAnyway. it's not very urgent for me, so I can wait and see if there will be any fix here. If it would be \"won't fix\" I can probably implement the solution I've described above.\r\n\r\nThanks again for your help and time \ud83d\udc4d ",
      "labels": [
        "bug",
        "locked",
        "stale"
      ],
      "created_at": "2022-06-01T11:17:02Z",
      "closed_at": "2026-01-13T23:43:31Z",
      "url": "https://github.com/vercel/next.js/issues/37362",
      "comments_count": 28
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 49207,
      "title": "How to load properly the \"Local fonts\" formats?",
      "problem": "### Verify canary release\n\n- [X] I verified that the issue exists in the latest Next.js canary release\n\n### Provide environment information\n\n```bash\nOperating System:\r\n      Platform: darwin\r\n      Arch: arm64\r\n      Version: Darwin Kernel Version 22.4.0: Mon Mar  6 21:01:02 PST 2023; root:xnu-8796.101.5~3/RELEASE_ARM64_T8112\r\n    Binaries:\r\n      Node: 18.15.0\r\n      npm: 8.15.0\r\n      Yarn: 1.22.19\r\n      pnpm: N/A\r\n    Relevant packages:\r\n      next: 13.3.4\r\n      eslint-config-next: 13.3.4\r\n      react: 18.2.0\r\n      react-dom: 18.2.0\n```\n\n\n### Which area(s) of Next.js are affected? (leave empty if unsure)\n\nFont optimization (next/font)\n\n### Link to the code that reproduces this issue\n\n/\n\n### To Reproduce\n\nHi everyone, \r\n\r\nI'm using `next/font` on my app in order to load some local fonts I have in my public directory.\r\nI used this documentation for this:\r\nhttps://nextjs.org/docs/basic-features/font-optimization#local-fonts\r\n\r\nThis is the piece of code, loading the fonts: \r\n\r\n```js\r\nconst poppins = localFont({\r\n  preload: true,\r\n  display: 'swap',\r\n  src: [\r\n    {\r\n      path: '../public/fonts/Poppins-Regular.woff',\r\n      weight: 'normal',\r\n      style: 'normal',\r\n    },\r\n    {\r\n      path: '../public/fonts/Poppins-Regular.woff2',\r\n      weight: 'normal',\r\n      style: 'normal',\r\n    },\r\n    {\r\n      path: '../public/fonts/Poppins-Italic.woff',\r\n      weight: 'normal',\r\n      style: 'italic',\r\n    },\r\n    {\r\n      path: '../public/fonts/Poppins-Italic.woff2',\r\n      weight: 'normal',\r\n      style: 'italic',\r\n    },\r\n    {\r\n      path: '../public/fonts/Poppins-SemiBold.woff',\r\n      weight: 'bold',\r\n      style: 'normal',\r\n    },\r\n    {\r\n      path: '../public/fonts/Poppins-SemiBold.woff2',\r\n      weight: 'bold',\r\n      style: 'normal',\r\n    },\r\n  ],\r\n  variable: '--font-poppins',\r\n})\r\n```\r\n\r\nNothing fancy, the only particularity is the font extension, as you can see I'm trying to load woff & woff2 fonts. \r\n\r\nSince I'm using Tailwind btw, I'm adding the fonts like this in my component: \r\n\r\n```js\r\nclassName={classNames(poppins.variable, makinac.variable)}\r\n```\r\n\n\n### Describe the Bug\n\n![CleanShot 2023-05-04 at 15 10 31@2x](https://user-images.githubusercontent.com/4931832/236214578-345c8e01-af53-472e-b307-3f0c519de387.png)\r\nAs you can see here, with this setup, all of the fonts are loaded. The woff **and** the woff2. \r\n\r\nMaybe I missed something, but do you know how I could only make the browser load the right font (woff2) and loading the other one (woff) as a fallback? When I read the doc, the `fallback` key only supports the system-ui fonts. \r\n\r\nThanks!\r\n\n\n### Expected Behavior\n\nOnly the woff2 should be loaded in the browser, not the woff version of the font. \n\n### Which browser are you using? (if relevant)\n\n_No response_\n\n### How are you deploying your application? (if relevant)\n\nVercel",
      "solution": "Looking forward for a solution. I also need to load multiple file formats for the same font.",
      "labels": [
        "bug",
        "Font (next/font)",
        "locked",
        "stale"
      ],
      "created_at": "2023-05-04T13:15:29Z",
      "closed_at": "2026-01-13T23:43:35Z",
      "url": "https://github.com/vercel/next.js/issues/49207",
      "comments_count": 10
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 50191,
      "title": "webpack watch screwed up (v13.3.4 => v13.4.3)",
      "problem": "### Verify canary release\n\n- [X] I verified that the issue exists in the latest Next.js canary release\n\n### Provide environment information\n\n```bash\nNode v18.16.0, Linux (Ubuntu 20.04), nextjs v13.4.3\n```\n\n\n### Which area(s) of Next.js are affected? (leave empty if unsure)\n\nTurbopack (--turbo)\n\n### Link to the code that reproduces this issue or a replay of the bug\n\nhttps://nextjs.org/learn/basics/create-nextjs-app/setup\n\n### To Reproduce\n\n1) create a nextjs app\r\n2) make sure `package.json` dependencies contains `\"next\": \"13.4.3\"`, e.g.\r\n```json\r\n{\r\n\t\"name\": \"foobar\",\r\n\t\"description\": \"Test\",\r\n\t\"repository\": \"https://github.com/x/y\",\r\n\t\"bugs\": {\r\n\t\t\"url\": \"https://github.com/x/y/issues\"\r\n\t},\r\n\t\"author\": \"Foo Bar <foobar@do.main>\",\r\n\t\"contributors\": [],\r\n\t\"version\": \"0.1.0\",\r\n\t\"private\": true,\r\n\t\"scripts\": {\r\n\t\t\"db\": \"NODE_OPTIONS='--no-warnings --loader ts-node/esm' nodemon --watch scripts scripts/db.ts\",\r\n\t\t\"dev\": \"next dev\",\r\n\t\t\"build\": \"next build\",\r\n\t\t\"start\": \"next start\",\r\n\t\t\"lint\": \"next lint\",\r\n\t\t\"clean\": \"rm -rf .next\",\r\n\t\t\"realclean\": \"rm -rf .next node_modules package-lock.json\"\r\n\t},\r\n\t\"dependencies\": {\r\n\t\t\"@next-auth/prisma-adapter\": \"^1.0.6\",\r\n\t\t\"@prisma/client\": \"^4.14.1\",\r\n\t\t\"next\": \"13.4.3\",\r\n\t\t\"nodemailer\": \"^6.9.2\",\r\n\t\t\"react\": \"18.2.0\",\r\n\t\t\"react-dom\": \"18.2.0\"\r\n\t},\r\n\t\"devDependencies\": {\r\n\t\t\"@styled-icons/material\": \"^10.47.0\",\r\n\t\t\"@styled-icons/fa-solid\": \"^10.47.0\",\r\n\t\t\"@types/busboy\": \"^1.5.0\",\r\n\t\t\"@types/node\": \"20.2.3\",\r\n\t\t\"@types/nodemailer\": \"^6.4.8\",\r\n\t\t\"@types/react\": \"18.2.6\",\r\n\t\t\"@types/react-dom\": \"18.2.4\",\r\n\t\t\"eslint\": \"8.41.0\",\r\n\t\t\"eslint-config-next\": \"13.4.3\",\r\n\t\t\"prisma\": \"^4.14.1\",\r\n\t\t\"typescript\": \"5.0.4\"\r\n\t}\r\n}\r\n```\r\n3) add watchOptions for webpack to your `next.config.js`, e.g. the one used looks like this:\r\n```ts\r\n/** @type {import('next').NextConfig} */\r\nconst nextConfig = {\r\n\treactStrictMode: true,\r\n\twebpack: function (config, options) {\r\n\t\tif (!config.watchOptions) {\r\n\t\t\tconfig.watchOptions = {\r\n\t\t\t\taggregateTimeout: 5,\r\n\t\t\t\tignored: [ '**/node_modules/**', '**/.git/**', '**/.next/**' ]\r\n\t\t\t};\r\n\t\t}\r\n\t\treturn config;\r\n\t},\r\n\texperimental: {\r\n\t\t/* appDir: true, */\r\n\t\tinstrumentationHook: true\r\n\t}\r\n};\r\n/*\r\nprocess.on('unhandledRejection', error => {\r\n\tconsole.log('unhandledRejection', error);\r\n});\r\n*/\r\nmodule.exports = nextConfig\r\n```\r\n4) make sure, user watch settings has a reasonable value, i.e. 4-8K is even more than that (however, this seems to be the default on many systems)\r\n`cat /proc/sys/fs/inotify/max_user_watches`. If some one already screwed it up, use e.g. `sysctl fs.inotify.max_user_watches=4096` to get it back to an acceptable value.\r\n5) run `npm run dev`\n\n### Describe the Bug\n\n300+ K of error messages like this:\r\n```\r\nWatchpack Error (watcher): Error: ENOSPC: System limit for number of file watchers reached, watch '~/work/nextapp/pages'\r\nWatchpack Error (watcher): Error: ENOSPC: System limit for number of file watchers reached, watch '~/work/nextapp'\r\nWatchpack Error (watcher): Error: ENOSPC: System limit for number of file watchers reached, watch '~/work'\r\nWatchpack Error (watcher): Error: ENOSPC: System limit for number of file watchers reached, watch '~'\r\nWatchpack Error (watcher): Error: ENOSPC: System limit for number of file watchers reached, watch '/home'\r\n...\r\n```\r\nNOTE: The current working directory is ~/work/nextapp/ !!! Why does it want to monitor the whole user filesystem?\n\n### Expected Behavior\n\nFor nextjs v13.3.4 everything worked with the given configs as expected, i.e. no problems at all when running `npm run dev` (even if VScode is running side-by-side).\r\n\r\nIMHO nextjs should pre-configure webpack etc. to monitor at most the current working directory and exclude by default ./node_modules (and possibly ./.next ?), because it is just a huge waste of resources for no (or not justifiable reason).\n\n### Which browser are you using? (if relevant)\n\n_No response_\n\n### How are you deploying your application? (if relevant)\n\n_No response_",
      "solution": "Hi! I'm having a very similar issue when implementing Next.js 14 with Node 21 and pnpm inside a monorepo handled by turborepo. I've already created an issue there but they sent me back here. My error is: \r\n```shell \r\nError: EMFILE: too many open files, open 'C:\\Users\\Amin\\Documents\\Codigo\\PJ\\sistemas-judiciales-monorepo\\apps\\backoffice-psae\\node_modules\\ui\\node_modules\\hooks\\node_modules\\react-hook-form\\dist\\utils\\isWeb.d.ts'\r\n```\r\n The issue is [at this link](https://github.com/vercel/turbo/issues/6458). Any help is welcomed. Thanks!!!\n\n---\n\nI upgraded a Next 12 project to 13.5, dev and build started failing with:\r\n\r\n```\r\n\"TypeError: config.watchOptions.ignored.filter is not a function or its return value is not iterable\"\"\r\n```\r\n\r\nNo hits for that on the internet, but this was close, and downgrading to `13.3.4` \"fixed\" it. As noted above, problems seem to appear in \u226513.4.x.\n\n---\n\nIn my case I was using [next-transpile-modules](https://github.com/martpie/next-transpile-modules) which is deprecated. Transpiling modules is now [natively supported by nextjs](https://github.com/martpie/next-transpile-modules/releases/tag/the-end).\r\nRemoving this package fixed the issue.",
      "labels": [
        "bug",
        "Performance",
        "locked",
        "stale"
      ],
      "created_at": "2023-05-23T00:57:17Z",
      "closed_at": "2026-01-13T23:43:37Z",
      "url": "https://github.com/vercel/next.js/issues/50191",
      "comments_count": 12
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 62452,
      "title": "Search params object empty on production (vercel) but not locally",
      "problem": "### Link to the code that reproduces this issue\r\n\r\nhttps://github.com/pitops/nextjs-search-params-issue\r\n\r\n### To Reproduce\r\n\r\nyou have a category/[slug]/page.tsx with the following\r\n\r\n```\r\ninterface CategoryPageProps {\r\n    params: {\r\n        slug: string\r\n    }\r\n    searchParams: {\r\n        slug: string\r\n    }\r\n}\r\nconst CategoryPage: React.FC<CategoryPageProps> = async ({\r\n                                                             params,\r\n                                                             searchParams\r\n                                                         }) => {\r\n    console.log('params', params)\r\n    console.log('searchParams', searchParams)\r\n\r\n    return <div>test</div>\r\n}\r\n\r\nexport default CategoryPage\r\n```\r\n\r\nI try the following URL: http://localhost:3000/category/test?slug=1\r\n\r\nlocally -> searchParams { slug: 1 }\r\nvercel -> searchParams {}\r\n\r\n\r\n\r\n### Current vs. Expected behavior\r\n\r\nsearch params object should be filled in production as well\r\n\r\n### Provide environment information\r\n\r\n```bash\r\nOperating System:\r\n  Platform: darwin\r\n  Arch: arm64\r\n  Version: Darwin Kernel Version 21.6.0: Mon Aug 22 20:20:05 PDT 2022; root:xnu-8020.140.49~2/RELEASE_ARM64_T8101\r\nBinaries:\r\n  Node: 19.6.0\r\n  npm: 9.5.0\r\n  Yarn: 1.18.0\r\n  pnpm: N/A\r\nRelevant Packages:\r\n  next: 14.1.0\r\n  eslint-config-next: 13.5.4\r\n  react: 18.2.0\r\n  react-dom: 18.2.0\r\n  typescript: 5.2.2\r\nNext.js Config:\r\n  output: N/A\r\n```\r\n\r\n\r\n### Which area(s) are affected? (Select all that apply)\r\n\r\nApp Router\r\n\r\n### Which stage(s) are affected? (Select all that apply)\r\n\r\nVercel (Deployed)\r\n\r\n### Additional context\r\n\r\n_No response_",
      "solution": "hi, the problem is with the parameter names. `params` and `searchParams` shouldn't have keys with the same name. For example, in your code:\r\n\r\n```typescript\r\ninterface CategoryPageProps {\r\n    params: {\r\n        slug: string\r\n    }\r\n    searchParams: {\r\n        slug: string\r\n    }\r\n}\r\n```\r\n\r\nThe `slug` parameter gets overwritten, and `searchParams` ends up empty. Try changing the parameter names so they're different.",
      "labels": [
        "bug",
        "locked",
        "stale"
      ],
      "created_at": "2024-02-23T14:56:18Z",
      "closed_at": "2026-01-13T23:43:49Z",
      "url": "https://github.com/vercel/next.js/issues/62452",
      "comments_count": 4
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 89052,
      "title": "useLayoutEffect context update causes local state reset in React 19",
      "problem": "### Verify canary release\n\n- [x] I verified that the issue exists in the latest Next.js canary release\n\n### Provide environment information\n\n```\nOperating System:\n  Platform: darwin\n  Arch: arm64\n  Version: Darwin Kernel Version 25.3.0\n\nBinaries:\n  Node: 22.x\n  npm: 10.x\n\nRelevant Packages:\n  next: 16.1.1-canary.4\n  react: 19.0.0\n  react-dom: 19.0.0\n\nTurbopack: Enabled\n```\n\n### Which area(s) are affected?\n\nApp Router, Turbopack\n\n### Link to the code that reproduces this issue\n\nhttps://github.com/jpoindexter/nextjs-useLayoutEffect-state-reset\n\n### To Reproduce\n\n1. Clone the repo: `git clone https://github.com/jpoindexter/nextjs-useLayoutEffect-state-reset`\n2. `npm install`\n3. `npm run dev`\n4. Open http://localhost:3000\n5. Open browser console\n6. Click the \"EDIT\" button\n\n### Describe the Bug\n\nWhen a component uses `useLayoutEffect` to update React Context state (via a custom hook), and the component triggers a local state change, the local state appears to \"reset\" during the render cascade.\n\n**Console output on first click:**\n```\nClick: setIsEditing(true)\nRender: true    // Good - state changed!\nRender: false   // BAD - state reset!\n```\n\nThe context correctly receives the new state (right rail shows \"EDIT MODE ACTIVE\" briefly), but the component's local state reverts to its previous value.\n\n### Expected Behavior\n\n- `isEditing` becomes `true` and stays `true`\n- The edit form appears\n- Console shows only: `Render: true`\n\n### Actual Behavior\n\n- `isEditing` briefly becomes `true`, then resets to `false`\n- The edit form does NOT appear\n- Console shows `true` then `false`\n\n### Workaround\n\nChange `useLayoutEffect` to `useEffect` and wrap context updates in `startTransition`:\n\n```tsx\nuseEffect(() => {\n  startTransition(() => {\n    context?.setContent(content);\n  });\n  return () => {\n    startTransition(() => {\n      context?.setContent(null);\n    });\n  };\n}, [context, content, updateKey]);\n```\n\n### Additional Context\n\n- This bug does NOT occur in React 18\n- Page refresh works correctly (component mounts fresh with correct state)\n- The issue is specific to state transitions during `useLayoutEffect` context updates\n",
      "solution": "@icyJoseph - This issue is a follow-up to #89050 (which was closed for missing reproduction). You raised a great point about the context reference instability. Let me address that:\n\n## Your Concern About Context Reference\n\nYou pointed out that this pattern causes infinite re-renders:\n\n```tsx\n// Provider recreates object reference on every render\nvalue={{ content, setContent }}>\n```\n\n**You are correct** - but that is NOT the bug we are reporting. In our reproduction, we stabilized the setter with `useCallback`:\n\n```tsx\n// From: contexts/right-rail-context.tsx in the reproduction repo\nexport function RightRailProvider({ children }: { children: ReactNode }) {\n  const [content, setContentState] = useState<ReactNode | null>(null);\n\n  // Stabilized setter - does NOT change reference\n  const setContent = useCallback((newContent: ReactNode | null) => {\n    setContentState(newContent);\n  }, []);\n\n  return (\n    <RightRailContext.Provider value={{ content, setContent }}>\n      {children}\n    </RightRailContext.Provider>\n  );\n}\n```\n\n**The bug still occurs** even with a stabilized context reference.\n\n## The Actual Bug\n\nThe issue is that when:\n1. Component calls `setIsEditing(true)` \n2. This triggers a re-render with `isEditing: true`\n3. `useLayoutEffect` runs synchronously and updates context\n4. The context update triggers another render\n5. During this cascade, `isEditing` resets to `false`\n\nThe context state updates correctly (right rail shows edit mode), but the component's local state reverts.\n\n## Reproduction Repository\n\n**https://github.com/jpoindexter/nextjs-useLayoutEffect-state-reset**\n\n```bash\ngit clone https://github.com/jpoindexter/nextjs-useLayoutEffect-state-reset\ncd nextjs-useLayoutEffect-state-reset\nnpm install\nnpm run dev\n# Open http://localhost:3000, open console, click EDIT\n```\n\n## Console Output Demonstrating the Bug\n\n```\n[Editor] Render: isEditing = false\n[Editor] Click: setIsEditing(true)\n[Editor] Render: isEditing = true    // \u2713 Good - state changed\n[Editor] Render: isEditing = false   // \u2717 BAD - state reset!\n```\n\n## How We Discovered This\n\nWe ran into this while building an in-situ editing feature with Claude Code. Our production workaround was to use `useEffect` instead of `useLayoutEffect`, or use a localStorage + page reload strategy.\n\nLet me know if you need any changes to the reproduction!\n\n---\n\nThis still not a React 19 or Next.js bug - it's a dependency array issue causing an infinite update loop.\n\nYou have, https://github.com/jpoindexter/nextjs-useLayoutEffect-state-reset/blob/2cd6f8f7dc01489e2013b6a121fca3b976876a2f/contexts/right-rail-context.tsx#L12-L19:\n\n```tsx\n// Provider creates a NEW object on every render\n<RightRailContext.Provider value={{ content, setContent }}>\n```\n\nAnd https://github.com/jpoindexter/nextjs-useLayoutEffect-state-reset/blob/2cd6f8f7dc01489e2013b6a121fca3b976876a2f/contexts/right-rail-context.tsx#L33-L41:\n\n```tsx\n// Hook depends on that unstable reference\nuseLayoutEffect(() => {\n  context?.setContent(content);\n  return () => context?.setContent(null);\n}, [context, content, updateKey]);  // \u2190 `context` changes every render\n```\n\nWhen `setContent` is called:\n\n1. Provider re-renders \u2192 creates new `{ content, setContent }` object\n2. `context` reference changes \u2192 effect cleanup runs \u2192 calls `setContent(null)`\n3. Provider re-renders again \u2192 goto 1\n\nSo even if you did `useCallback` around the `setContent` function, the Context Provider value itself is recreated. In fact, `useState`'s setter function is guaranteed to be stable, so wrapping it in `useCallback` is useless/un-necessary.\n\nThis causes \"Maximum update depth exceeded\" - a classic infinite loop from unstable dependencies.\n\n**_Even_** if you memoize the context value so the object reference stays stable, upon calling `setContent`, you'd be creating a new `value` and we are back to the `When setContent is called` list:\n\n```tsx\nexport function RightRailProvider({ children }: { children: ReactNode }) {\n  const [content, setContent] = useState<ReactNode>(null);\n // \u26a0\ufe0f this would get recreated when setContent is called anyway\n  const value = useMemo(() => ({ content, setContent }), [content]);\n  return (\n    <RightRailContext.Provider value={value}>\n      {children}\n    </RightRailContext.Provider>\n  );\n}\n```\n\nA solution is this nice thing they have at the React docs, with a double context: https://react.dev/learn/scaling-up-with-reducer-and-context#step-2-put-state-and-dispatch-into-context\n\n```tsx\nexport function RightRailProvider({ children }: { children: ReactNode }) {\n  const [content, setContent] = useState<ReactNode>(null);\n\n  return (\n    <RightRailContentContext.Provider value={content}>\n      <RightRailDispatchContext.Provider value={setContent}>\n        {children}\n      </RightRailDispatchContext.Provider>\n    </RightRailContentContext.Provider>\n  );\n}\n\nexport function useRightRailContent() {                                                                                                                                \n    return useContext(RightRailContentContext);                                                                                                                          \n}                                                                                                                                                                      \n                                                                                                                                                                         \nexport function useSetRightRail(content: ReactNode, updateKey?: unknown) {                                                                                             \n  const setContent = useContext(RightRailDispatchContext);                                                                                                             \n                                                                                                                                                                         \n  useLayoutEffect(() => {                                                                                                                                              \n    setContent?.(content);                                                                                                                                             \n    return () => setContent?.(null);                                                                                                                                   \n  }, [setContent, content, updateKey]);                                                                                                                                \n}                                                                                                                                                                             \n```\n\nNow a component exclusively calling `setContent`, for example though `useSetRightRail`,  doesn't have to re-render when `content` changes.\n\nhttps://github.com/user-attachments/assets/33407860-a46e-4131-8e13-88e2af0ae7e5\n\nThe reason this might have appeared to work in React 18 is likely due to different batching/timing behavior, but the underlying pattern was always incorrect.\n",
      "labels": [],
      "created_at": "2026-01-26T14:22:06Z",
      "closed_at": "2026-01-27T20:52:37Z",
      "url": "https://github.com/vercel/next.js/issues/89052",
      "comments_count": 3
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 89108,
      "title": "State reset during useLayoutEffect context update in React 19",
      "problem": "## Verify canary release\n\n- [x] I verified that the issue exists in the latest Next.js canary release\n\n## Provide environment information\n\n```\nOperating System:\n  Platform: darwin\n  Arch: arm64\n  Version: Darwin Kernel Version 25.3.0\n\nBinaries:\n  Node: 22.x\n  npm: 10.x\n\nRelevant Packages:\n  next: 16.1.1-canary.4\n  react: 19.0.0\n  react-dom: 19.0.0\n\nTurbopack: Enabled\n```\n\n## Which area(s) are affected?\n\n- [x] App Router\n- [x] Turbopack\n\n## Link to the code that reproduces this issue\n\nhttps://github.com/jpoindexter/nextjs-context-state-reset-repro\n\n## To Reproduce\n\n1. Clone the repo: `git clone https://github.com/jpoindexter/nextjs-context-state-reset-repro.git`\n2. Install: `cd nextjs-context-state-reset-repro && npm install`\n3. Run: `npm run dev`\n4. Open http://localhost:3000\n5. Open browser console (F12)\n6. Click the \"EDIT\" button\n7. Watch console - state shows `true` then resets to `false`\n\n## Describe the Bug\n\nWhen a component uses `useLayoutEffect` to update React Context state, and triggers a local state change, the local state resets during the render cascade.\n\n### Expected\n\nClick EDIT \u2192 `isEditing` becomes `true` \u2192 Edit form appears\n\n### Actual\n\nClick EDIT \u2192 `isEditing` briefly becomes `true` \u2192 Resets to `false` \u2192 View mode persists\n\nConsole output:\n```\n[Editor] Render #1, isEditing: false\n[Editor] startEditing called\n[Editor] Render #2, isEditing: true    // Good\n[Editor] Render #3, isEditing: false   // BAD - reset!\n```\n\n## Workaround\n\nUse `useEffect` instead of `useLayoutEffect`:\n\n```tsx\n// Bug\nuseLayoutEffect(() => {\n  context?.setContent(content);\n}, [context, content]);\n\n// Works\nuseEffect(() => {\n  context?.setContent(content);\n}, [context, content]);\n```\n\n## Additional Context\n\n- Does NOT occur in React 18\n- Occurs with Turbopack enabled\n- Context updates correctly, only local state resets\n- Related to #89050 (closed for missing repo)\n",
      "solution": "We could not detect a valid reproduction link. **Make sure to follow the bug report template carefully.**\n\n### Why was this issue closed?\n\nTo be able to investigate, we need access to a reproduction to identify what triggered the issue. We need a link to a **public** GitHub repository ([template for App Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template), [template for Pages Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template-pages)), but you can also use these templates: [CodeSandbox: App Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template) or [CodeSandbox: Pages Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template-pages).\n\nThe bug template that you filled out has a section called \"Link to the code that reproduces this issue\", which is where you should provide the link to the reproduction.\n\n- If you did not provide a link or the link you provided is not valid, we will close the issue.\n- If you provide a link to a private repository, we will close the issue.\n- If you provide a link to a repository but not in the correct section, we will close the issue.\n\n### What should I do?\n\nDepending on the reason the issue was closed, you can do the following:\n\n- If you did not provide a link, please open a new issue with a link to a reproduction.\n- If you provided a link to a private repository, please open a new issue with a link to a public repository.\n- If you provided a link to a repository but not in the correct section, please open a new issue with a link to a reproduction in the correct section.\n\n**In general, assume that we should not go through a lengthy onboarding process at your company code only to be able to verify an issue.**\n\n### My repository is private and cannot make it public\n\nIn most cases, a private repo will not be a sufficient **minimal reproduction**, as this codebase might contain a lot of unrelated parts that would make our investigation take longer. Please do **not** make it public. Instead, create a new repository using the templates above, adding the relevant code to reproduce the issue. Common things to look out for:\n\n- Remove any code that is not related to the issue. (pages, API routes, components, etc.)\n- Remove any dependencies that are not related to the issue.\n- Remove any third-party service that would require us to sign up for an account to reproduce the issue.\n- Remove any environment variables that are not related to the issue.\n- Remove private packages that we do not have access to.\n- If the issue is not related to a monorepo specifically, try to reproduce the issue without a complex monorepo setup\n\n### I did not open this issue, but it is relevant to me, what can I do to help?\n\nAnyone experiencing the same issue is welcome to provide a minimal reproduction following the above steps by opening a new issue.\n\n### I think my reproduction is good enough, why aren't you looking into it quickly?\n\nWe look into every Next.js issue and constantly monitor open issues for new comments.\n\nHowever, sometimes we might miss one or two due to the popularity/high traffic of the repository. We apologize, and kindly ask you to refrain from tagging core maintainers, as that will usually not result in increased priority.\n\nUpvoting issues to show your interest will help us prioritize and address them as quickly as possible. That said, every issue is important to us, and if an issue gets closed by accident, we encourage you to open a new one linking to the old issue and we will look into it.\n\n### Useful Resources\n\n- [How to Contribute to Open Source (Next.js)](https://www.youtube.com/watch?v=cuoNzXFLitc)\n- [How to create a Minimal, Complete, and Verifiable example](https://stackoverflow.com/help/mcve)\n- [Reporting a Next.js bug](https://github.com/vercel/next.js/blob/canary/.github/ISSUE_TEMPLATE/1.bug_report.yml)\n- [Next.js Triaging issues](https://github.com/vercel/next.js/blob/canary/contributing/repository/triaging.md)\n",
      "labels": [
        "invalid link"
      ],
      "created_at": "2026-01-27T12:55:54Z",
      "closed_at": "2026-01-27T12:56:11Z",
      "url": "https://github.com/vercel/next.js/issues/89108",
      "comments_count": 1
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 89107,
      "title": "State reset during useLayoutEffect context update in React 19 + Next.js 16",
      "problem": "## Verify canary release\n\n- [x] I verified that the issue exists in the latest Next.js canary release\n\n## Provide environment information\n\n```\nOperating System:\n  Platform: darwin\n  Arch: arm64\n  Version: Darwin Kernel Version 25.3.0\n\nBinaries:\n  Node: 22.x\n  npm: 10.x\n\nRelevant Packages:\n  next: 16.1.1-canary.4\n  react: 19.0.0\n  react-dom: 19.0.0\n\nTurbopack: Enabled\n```\n\n## Which area(s) are affected?\n\n- [x] App Router\n- [x] Turbopack\n\n## Link to the code that reproduces this issue\n\n**\ud83d\udd17 https://github.com/jpoindexter/nextjs-context-state-reset-repro**\n\n```bash\ngit clone https://github.com/jpoindexter/nextjs-context-state-reset-repro.git\ncd nextjs-context-state-reset-repro\nnpm install\nnpm run dev\n# Open http://localhost:3000\n# Open browser console (F12)\n# Click the \"EDIT\" button\n# Watch console logs show state reset\n```\n\n## Describe the Bug\n\nWhen a component uses `useLayoutEffect` to update React Context state, and then triggers a local state change, the local state appears to \"reset\" during the render cascade in React 19.\n\n### Scenario\n\nWe have an in-situ editing pattern:\n1. **Editor component** has local state `isEditing`\n2. **Custom hook** (`useSetRightRail`) uses `useLayoutEffect` to push content to a context\n3. When user clicks \"EDIT\", `setIsEditing(true)` is called\n4. The `useLayoutEffect` updates the context with edit-mode content\n\n### Expected Behavior\n\n```\nClick EDIT \u2192 isEditing becomes true \u2192 Edit form appears\n```\n\n### Actual Behavior\n\n```\nClick EDIT \u2192 isEditing briefly becomes true \u2192 State resets to false \u2192 View mode persists\n```\n\nThe context updates correctly (right rail shows edit mode), but the component's local state reverts.\n\n### Console Output\n\n```\n[Editor] Render #1, isEditing: false\n[Editor] startEditing called, current isEditing: false\n[Editor] Render #2, isEditing: true    // \u2713 Good\n[Editor] Render #3, isEditing: false   // \u2717 BAD - state reset!\n```\n\n## Root Cause Analysis\n\nThe issue appears related to React 19's concurrent rendering:\n\n1. `setIsEditing(true)` triggers a state update\n2. Component re-renders with `isEditing: true`\n3. `useLayoutEffect` runs synchronously and calls `context.setContent(...)`\n4. Context state update triggers a re-render cascade\n5. During this cascade, the component's `isEditing` state appears to reset to its previous value\n\n**Note:** The context reference is stabilized with `useCallback` to prevent infinite loops - the bug still occurs.\n\n## Why Page Refresh Works\n\nAfter clicking EDIT (which sets a localStorage flag), refreshing the page works because:\n1. Component mounts fresh with no previous state\n2. `useState` initializer reads localStorage and returns `true`\n3. No state transition occurs - component starts in edit mode\n4. `useLayoutEffect` runs but doesn't interfere with an already-settled state\n\n## Workarounds\n\n### 1. Use `useEffect` instead of `useLayoutEffect`\n\n```tsx\n// Before (buggy)\nuseLayoutEffect(() => {\n  context?.setContent(content);\n  return () => context?.setContent(null);\n}, [context, content, updateKey]);\n\n// After (works)\nuseEffect(() => {\n  context?.setContent(content);\n  return () => context?.setContent(null);\n}, [context, content, updateKey]);\n```\n\n### 2. Page Reload Strategy (our production fix)\n\n```tsx\nconst startEditing = useCallback(() => {\n  localStorage.setItem('editing-flag', 'true');\n  window.location.reload(); // Force fresh mount\n}, []);\n\n// In useState initializer:\nconst [isEditing] = useState(() => {\n  if (typeof window === 'undefined') return false;\n  const flag = localStorage.getItem('editing-flag');\n  if (flag === 'true') {\n    localStorage.removeItem('editing-flag');\n    return true;\n  }\n  return false;\n});\n```\n\n## Key Files in Reproduction\n\n- `src/contexts/right-rail-context.tsx` - Context with `useLayoutEffect` hook (bug trigger)\n- `src/components/editor.tsx` - Component demonstrating the state reset\n- `src/components/right-rail.tsx` - Shows context IS updating correctly\n\n## Additional Context\n\n- This bug does **not** occur in React 18\n- This bug occurs with Turbopack enabled\n- The issue is specific to `useLayoutEffect` triggering context state updates during a state transition\n- We discovered this while building an in-situ editing feature with Claude Code\n- Related to #89050 (closed for missing reproduction)\n\n---\n\n*This reproduction was created while debugging production code with Claude Code.*",
      "solution": "We could not detect a valid reproduction link. **Make sure to follow the bug report template carefully.**\n\n### Why was this issue closed?\n\nTo be able to investigate, we need access to a reproduction to identify what triggered the issue. We need a link to a **public** GitHub repository ([template for App Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template), [template for Pages Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template-pages)), but you can also use these templates: [CodeSandbox: App Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template) or [CodeSandbox: Pages Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template-pages).\n\nThe bug template that you filled out has a section called \"Link to the code that reproduces this issue\", which is where you should provide the link to the reproduction.\n\n- If you did not provide a link or the link you provided is not valid, we will close the issue.\n- If you provide a link to a private repository, we will close the issue.\n- If you provide a link to a repository but not in the correct section, we will close the issue.\n\n### What should I do?\n\nDepending on the reason the issue was closed, you can do the following:\n\n- If you did not provide a link, please open a new issue with a link to a reproduction.\n- If you provided a link to a private repository, please open a new issue with a link to a public repository.\n- If you provided a link to a repository but not in the correct section, please open a new issue with a link to a reproduction in the correct section.\n\n**In general, assume that we should not go through a lengthy onboarding process at your company code only to be able to verify an issue.**\n\n### My repository is private and cannot make it public\n\nIn most cases, a private repo will not be a sufficient **minimal reproduction**, as this codebase might contain a lot of unrelated parts that would make our investigation take longer. Please do **not** make it public. Instead, create a new repository using the templates above, adding the relevant code to reproduce the issue. Common things to look out for:\n\n- Remove any code that is not related to the issue. (pages, API routes, components, etc.)\n- Remove any dependencies that are not related to the issue.\n- Remove any third-party service that would require us to sign up for an account to reproduce the issue.\n- Remove any environment variables that are not related to the issue.\n- Remove private packages that we do not have access to.\n- If the issue is not related to a monorepo specifically, try to reproduce the issue without a complex monorepo setup\n\n### I did not open this issue, but it is relevant to me, what can I do to help?\n\nAnyone experiencing the same issue is welcome to provide a minimal reproduction following the above steps by opening a new issue.\n\n### I think my reproduction is good enough, why aren't you looking into it quickly?\n\nWe look into every Next.js issue and constantly monitor open issues for new comments.\n\nHowever, sometimes we might miss one or two due to the popularity/high traffic of the repository. We apologize, and kindly ask you to refrain from tagging core maintainers, as that will usually not result in increased priority.\n\nUpvoting issues to show your interest will help us prioritize and address them as quickly as possible. That said, every issue is important to us, and if an issue gets closed by accident, we encourage you to open a new one linking to the old issue and we will look into it.\n\n### Useful Resources\n\n- [How to Contribute to Open Source (Next.js)](https://www.youtube.com/watch?v=cuoNzXFLitc)\n- [How to create a Minimal, Complete, and Verifiable example](https://stackoverflow.com/help/mcve)\n- [Reporting a Next.js bug](https://github.com/vercel/next.js/blob/canary/.github/ISSUE_TEMPLATE/1.bug_report.yml)\n- [Next.js Triaging issues](https://github.com/vercel/next.js/blob/canary/contributing/repository/triaging.md)\n",
      "labels": [
        "invalid link"
      ],
      "created_at": "2026-01-27T12:52:25Z",
      "closed_at": "2026-01-27T12:52:39Z",
      "url": "https://github.com/vercel/next.js/issues/89107",
      "comments_count": 1
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 47774,
      "title": "ESM and TypeScript version of PostCSS config, doesn't load.",
      "problem": "### Verify canary release\r\n\r\n- [X] I verified that the issue exists in the latest Next.js canary release\r\n\r\n### Provide environment information\r\n\r\n```bash\r\n> npx next info\r\n\r\n    Operating System:\r\n      Platform: linux\r\n      Arch: x64\r\n      Version: Ubuntu 20.04.0 LTS Sat Apr 01 2023 08:47:57 GMT+0800 (Taipei Standard Time)\r\n    Binaries:\r\n      Node: 16.14.2\r\n      npm: 7.17.0\r\n      Yarn: 1.22.19\r\n      pnpm: 7.13.6\r\n    Relevant packages:\r\n      next: 13.2.5-canary.24\r\n      eslint-config-next: N/A\r\n      react: 18.2.0\r\n      react-dom: 18.2.0\r\n```\r\n\r\n\r\n### Which area(s) of Next.js are affected? (leave empty if unsure)\r\n\r\nTypeScript\r\n\r\n### Link to the code that reproduces this issue\r\n\r\nhttps://stackblitz.com/edit/vercel-next-js-p7yaj5?file=pages/index.tsx\r\n\r\n### To Reproduce\r\n\r\n1. Initiate a sample next.js project\r\n2. Install TailwindCSS by `npm install -D tailwindcss`\r\n3. Generate Tailwind & PostCSS config with `npx tailwindcss init --esm --postcss --ts`\r\n\r\n### Describe the Bug\r\n\r\n[Since TailwindCSS as of version `^3.3.0` supports ESM and TypeScript config](https://tailwindcss.com/blog/tailwindcss-v3-3#esm-and-typescript-support), the `postcss.config.ts` doesn't load at all - the content of the config file is not read up - I could confirm by adding a `console.log()` with a random message.\r\n\r\n### Expected Behavior\r\n\r\nThe PostCSS of config - ESM or TypeScript - should load. Hence the styles of the heading message should be applied.\r\n\r\n### Which browser are you using? (if relevant)\r\n\r\n_No response_\r\n\r\n### How are you deploying your application? (if relevant)\r\n\r\n_No response_",
      "solution": "@xeho91 thanks for the issue https://github.com/webpack-contrib/postcss-loader/issues/631 and PR https://github.com/webpack-contrib/postcss-loader/pull/632 !\r\n\r\nI'm guessing [this version of `postcss-loader`](https://github.com/vercel/next.js/tree/da37c018f395f6b30118aedea6c51d016ca15d92/packages/next/src/build/webpack/loaders/postcss-loader) is vendored in Next.js and will be updated over the next weeks...\n\n---\n\n\r\n\r\n> @xeho91 thanks for the issue [webpack-contrib/postcss-loader#631](https://github.com/webpack-contrib/postcss-loader/issues/631) and PR [webpack-contrib/postcss-loader#632](https://github.com/webpack-contrib/postcss-loader/pull/632) !\r\n> \r\n> I'm guessing [this version of `postcss-loader`](https://github.com/vercel/next.js/tree/da37c018f395f6b30118aedea6c51d016ca15d92/packages/next/src/build/webpack/loaders/postcss-loader) is vendored in Next.js and will be updated over the next weeks...\r\n\r\nWohoo, I contributed to the Next.js!\r\n\r\nWell, not indirectly.\r\nAnyway, I'm going to share it with my wife! :grin: \n\n---\n\nHmm... I may be wrong in my assumption above that it will be directly vendored in.\r\n\r\nLooking a bit deeper into the Next.js codebase, it seems that the config file resolution for PostCSS is here in Next.js (only JSON + CommonJS supported):\r\n\r\nhttps://github.com/vercel/next.js/blob/da37c018f395f6b30118aedea6c51d016ca15d92/packages/next/src/lib/find-config.ts#L9-L27",
      "labels": [
        "bug",
        "TypeScript"
      ],
      "created_at": "2023-04-01T00:48:38Z",
      "closed_at": "2026-01-27T11:01:31Z",
      "url": "https://github.com/vercel/next.js/issues/47774",
      "comments_count": 13
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 89090,
      "title": "zlib memory leak Node.js 24",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/izorg/next-app\n\n### To Reproduce\n\n1. Use Node.js version 24.13.0\n2. npm run build\n3. NODE_OPTIONS=--inspect ./node_modules/.bin/next start\n4. http://localhost:3000/\n5. Call GC & make memory snapshot\n6. Run snippet in browser console\n    ```js\n    const ATTEMPT_COUNT = 50\n    const ATTEMPT_TIMEOUT_MS = 10\n    const REQUEST_COUNT = 10\n    const REQUEST_TIMEOUT_MS = 500\n    \n    for await (const attemptIndex of Array.from({ length: ATTEMPT_COUNT }).keys()) {\n      for (const requestIndex of Array.from({ length: REQUEST_COUNT }).keys()) {\n        fetch('http://localhost:3000/', {\n          signal: AbortSignal.timeout(REQUEST_TIMEOUT_MS),\n        })\n      }\n    \n      await new Promise((resolve) => {\n        setTimeout(resolve, ATTEMPT_TIMEOUT_MS)\n      })\n    }\n    ```\n7. Call GC & make memory snapshot\n8. Compare snapshots and see `Node / zlib_memory` positive delta\n\n### Current vs. Expected behavior\n\nExpect memory to still more or less on the same level\n\n### Provide environment information\n\n```bash\nOperating System:\n  Platform: darwin\n  Arch: arm64\n  Version: Darwin Kernel Version 25.2.0: Tue Nov 18 21:07:05 PST 2025; root:xnu-12377.61.12~1/RELEASE_ARM64_T6020\n  Available memory (MB): 32768\n  Available CPU cores: 12\nBinaries:\n  Node: 24.13.0\n  npm: 11.6.2\n  Yarn: 1.22.22\n  pnpm: 10.28.2\nRelevant Packages:\n  next: 16.1.5 // Latest available version is detected (16.1.5).\n  eslint-config-next: N/A\n  react: 19.2.4\n  react-dom: 19.2.4\n  typescript: 5.9.3\nNext.js Config:\n  output: N/A\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nPerformance\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext start (local)\n\n### Additional context\n\nI tested on 24.0.0, the issue is still there. I tested on lates 22.22.0 and there is no issue with zlib.",
      "solution": "We could not detect a valid reproduction link. **Make sure to follow the bug report template carefully.**\n\n### Why was this issue closed?\n\nTo be able to investigate, we need access to a reproduction to identify what triggered the issue. We need a link to a **public** GitHub repository ([template for App Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template), [template for Pages Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template-pages)), but you can also use these templates: [CodeSandbox: App Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template) or [CodeSandbox: Pages Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template-pages).\n\nThe bug template that you filled out has a section called \"Link to the code that reproduces this issue\", which is where you should provide the link to the reproduction.\n\n- If you did not provide a link or the link you provided is not valid, we will close the issue.\n- If you provide a link to a private repository, we will close the issue.\n- If you provide a link to a repository but not in the correct section, we will close the issue.\n\n### What should I do?\n\nDepending on the reason the issue was closed, you can do the following:\n\n- If you did not provide a link, please open a new issue with a link to a reproduction.\n- If you provided a link to a private repository, please open a new issue with a link to a public repository.\n- If you provided a link to a repository but not in the correct section, please open a new issue with a link to a reproduction in the correct section.\n\n**In general, assume that we should not go through a lengthy onboarding process at your company code only to be able to verify an issue.**\n\n### My repository is private and cannot make it public\n\nIn most cases, a private repo will not be a sufficient **minimal reproduction**, as this codebase might contain a lot of unrelated parts that would make our investigation take longer. Please do **not** make it public. Instead, create a new repository using the templates above, adding the relevant code to reproduce the issue. Common things to look out for:\n\n- Remove any code that is not related to the issue. (pages, API routes, components, etc.)\n- Remove any dependencies that are not related to the issue.\n- Remove any third-party service that would require us to sign up for an account to reproduce the issue.\n- Remove any environment variables that are not related to the issue.\n- Remove private packages that we do not have access to.\n- If the issue is not related to a monorepo specifically, try to reproduce the issue without a complex monorepo setup\n\n### I did not open this issue, but it is relevant to me, what can I do to help?\n\nAnyone experiencing the same issue is welcome to provide a minimal reproduction following the above steps by opening a new issue.\n\n### I think my reproduction is good enough, why aren't you looking into it quickly?\n\nWe look into every Next.js issue and constantly monitor open issues for new comments.\n\nHowever, sometimes we might miss one or two due to the popularity/high traffic of the repository. We apologize, and kindly ask you to refrain from tagging core maintainers, as that will usually not result in increased priority.\n\nUpvoting issues to show your interest will help us prioritize and address them as quickly as possible. That said, every issue is important to us, and if an issue gets closed by accident, we encourage you to open a new one linking to the old issue and we will look into it.\n\n### Useful Resources\n\n- [How to Contribute to Open Source (Next.js)](https://www.youtube.com/watch?v=cuoNzXFLitc)\n- [How to create a Minimal, Complete, and Verifiable example](https://stackoverflow.com/help/mcve)\n- [Reporting a Next.js bug](https://github.com/vercel/next.js/blob/canary/.github/ISSUE_TEMPLATE/1.bug_report.yml)\n- [Next.js Triaging issues](https://github.com/vercel/next.js/blob/canary/contributing/repository/triaging.md)\n",
      "labels": [
        "Performance",
        "invalid link"
      ],
      "created_at": "2026-01-27T09:14:46Z",
      "closed_at": "2026-01-27T09:15:00Z",
      "url": "https://github.com/vercel/next.js/issues/89090",
      "comments_count": 1
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 86981,
      "title": "Typescript(~20MB) present in build .next/standalone/node_modules when using standalone output",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/BlvckParrot/nextjs-issue-typescript-in-standalone-output\n\n### To Reproduce\n\n1. create new next app `pnpx create-next-app@latest`(with defaults)\n2. change `output: \"standalone\"` in next.config.ts\n3. run build `pnpm build`\n\nThen see directory `.next/standalone/node_modules/typescript`\n\n### Current vs. Expected behavior\n\nI would expected that in the `.next/standalone/node_modules/typescript` there would be no Typescript because it is not runtime dependency and it has a huge size.\n\n<img width=\"323\" height=\"810\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/5a7e1fdb-1931-4502-bd4c-20ef1ba6bee4\" />\n\n### Provide environment information\n\n```bash\nOperating System:\n  Platform: darwin\n  Arch: arm64\n  Version: Darwin Kernel Version 25.1.0: Mon Oct 20 19:32:41 PDT 2025; root:xnu-12377.41.6~2/RELEASE_ARM64_T6000\n  Available memory (MB): 16384\n  Available CPU cores: 8\nBinaries:\n  Node: 24.5.0\n  npm: 11.5.1\n  Yarn: 1.22.22\n  pnpm: 9.15.9\nRelevant Packages:\n  next: 16.0.8 // Latest available version is detected (16.0.8).\n  eslint-config-next: N/A\n  react: 19.2.1\n  react-dom: 19.2.1\n  typescript: 5.9.3\nNext.js Config:\n  output: standalone\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nTurbopack\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext build (local)\n\n### Additional context\n\nI have tried this also with bun, deno, pnpm, npm, all with the same result.",
      "solution": "noticed the same weirdness with turbopack, docker image size got kinda big so I took a look at apparently in the .next folder we had the project in there\n\nswitching back to webpack fixes the issue for me\n\n---\n\n> noticed the same weirdness with turbopack, docker image size got kinda big so I took a look at apparently in the .next folder we had the project in there\n> \n> switching back to webpack fixes the issue for me\n\nYes, in some instance I even had whole project including all unmodified files copied to the standalone folder. But that I couldnt reproduce so I am reporting just the Typescript issue. \n\n---\n\nI\u2019ve opened a PR that fixes this by ensuring devDependencies (including typescript) are not traced into the .next/standalone output when using Turbopack.\n\nThis brings standalone behavior in line with expectations and avoids bundling build-time tools into the runtime artifact.\n\nWould appreciate it if you could give it a try and confirm it resolves the issue on your end \ud83d\udc4d",
      "labels": [
        "Turbopack",
        "linear: turbopack",
        "locked"
      ],
      "created_at": "2025-12-09T10:56:26Z",
      "closed_at": "2026-01-12T13:39:19Z",
      "url": "https://github.com/vercel/next.js/issues/86981",
      "comments_count": 6
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 59201,
      "title": " Logs are not appearing in the console window upon completion of the standalone build.",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/SeoYoung-C/next-14-issue\n\n### To Reproduce\n\n1. I am using Winston. Additionally, I am formatting the logger in ECS format, so I am using the @elastic/winston-ecs-format library to change the log format.\r\n\r\n2. When using the app directory structure in next.js 14 and using the @elastic/winston-ecs-format library, I encountered an error \"Module not found: Can't resolve 'elastic-apm-node'\". To use this module in next.config.js, I added the following option:\r\n```\r\n/** @type {import('next').NextConfig} */\r\nconst nextConfig = {\r\n    output: 'standalone',\r\n    compiler: {\r\n        removeConsole: {\r\n            exclude: ['error', 'info', 'warn'],\r\n        },\r\n    },\r\n    experimental: {\r\n        serverComponentsExternalPackages: ['@elastic/ecs-winston-format'],\r\n    }\r\n}\r\n\r\nmodule.exports = nextConfig\r\n```\r\n3. I'm using Docker to build the app, therefore applying standalone to build the app. After the build, the @elastic/winston-ecs-format library is not included in the .next/standalone/node_modules folder.\r\n\r\n4. In the bug-fix version 14.0.4-canary.36, I've confirmed that the module was included in the build. However, in the version built using standalone, the console logs are not being displayed.\n\n### Current vs. Expected behavior\n\nThe build should be completed with '@elastic/ecs-winston-format' included in '.next/standalone/node_modules'. Additionally, logs entered via Winston should be visible in the console window.\n\n### Verify canary release\n\n- [X] I verified that the issue exists in the latest Next.js canary release\n\n### Provide environment information\n\n```bash\nOperating System:\r\n  Platform: darwin\r\n  Arch: x64\r\n  Version: Darwin Kernel Version 23.1.0: Mon Oct  9 21:27:27 PDT 2023; root:xnu-10002.41.9~6/RELEASE_X86_64\r\nBinaries:\r\n  Node: 20.10.0\r\n  npm: 10.2.3\r\n  Yarn: N/A\r\n  pnpm: N/A\r\nRelevant Packages:\r\n  next: 14.0.4-canary.36\r\n  eslint-config-next: 14.0.3\r\n  react: 18.2.0\r\n  react-dom: 18.2.0\r\n  typescript: 5.3.2\r\nNext.js Config:\r\n  output: standalone\n```\n\n\n### Which area(s) are affected? (Select all that apply)\n\nApp Router, Standalone mode (output: \"standalone\")\n\n### Additional context\n\nI've tested this on both local and Docker environments.",
      "solution": "> I've had the same issue with Pino logger. Not using app router, i had to copy over pino and it dependencies over in Docker standalone\r\n\r\nI am aware that the issue can be resolved by using the 'pages', but I am trying to configure a new project using the 'app route' method. I believe there should be bug fixes or improvements made for this approach\n\n---\n\nI have solved this problem.\r\n\r\n```\r\nconst nextConfig = {\r\n  // ....\r\n  \texperimental: {\r\n\t\tserverComponentsExternalPackages: ['@elastic/ecs-winston-format', ],\r\n\t},\r\n         webpack(config, { dev, isServer }) {\r\n\t\tif (!dev && isServer) {\r\n\t\t\tconfig.resolve.alias = {\r\n\t\t\t\t...config.resolve.alias,\r\n\t\t\t\t'@elastic/ecs-winston-format': path.resolve(\r\n\t\t\t\t\t__dirname,\r\n\t\t\t\t\t'node_modules/@elasti/ecs-winston-format',\r\n\t\t\t\t),\r\n\t\t\t};\r\n\t\t}\r\n       })\r\n   }\r\n```\r\n\r\nBuilding and log verification were achievable solely through configuration settings.\n\n---\n\n> Hey @SeoYoung-C\r\n> \r\n> Trying your approach and still don't see any logs in the console.\r\n> \r\n> Could you please paste the full working `next.config.mjs` here, or even better update your sample repo: https://github.com/SeoYoung-C/next-14-issue\r\n> \r\n> Thanks\r\n\r\n\r\nI have just confirmed. The issue you've inquired about doesn't seem to be related to the winston library or ecs-format. Unlike my GitHub repository, in actuality, I've created a custom function to utilize the winston library separately outside of the app folder, rather than using it within the app folder.\r\n\r\n```\r\nRoot\r\n\u251c\u2500\u2500 app\r\n\u2502   \u251c\u2500\u2500 api \r\n\u2502   \u2502   \u251c\u2500\u2500 logger.ts\r\n\u251c\u2500\u2500 components\r\n\u2502   \u251c\u2500\u2500 server\r\n\u2502   \u2502   \u251c\u2500\u2500 wiston.ts\r\n```\r\n\r\n\r\nCurrently, I'm not in a position to update the content on Git from outside. I'll make sure to update it when possible at a later time.",
      "labels": [
        "bug",
        "Output",
        "locked",
        "stale"
      ],
      "created_at": "2023-12-03T05:46:03Z",
      "closed_at": "2026-01-12T23:43:04Z",
      "url": "https://github.com/vercel/next.js/issues/59201",
      "comments_count": 13
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 89050,
      "title": "State reset during useLayoutEffect context update in React 19",
      "problem": "# Bug Report: State Reset During useLayoutEffect Context Update in React 19\n\n## Verify canance and latest version\n\n- [x] I verified that the issue exists in the latest Next.js canary release\n- [x] I verified this issue is not a duplicate\n\n## Provide environment information\n\n```\nOperating System:\n  Platform: darwin\n  Arch: arm64\n  Version: Darwin Kernel Version 25.3.0\n\nBinaries:\n  Node: 22.x\n  npm: 10.x\n\nRelevant Packages:\n  next: 16.1.1-canary.4\n  react: 19.0.0\n  react-dom: 19.0.0\n\nTurbopack: Enabled\n```\n\n## Which area(s) are affected?\n\n- [x] App Router\n- [x] Turbopack\n\n## Describe the Bug\n\nWhen a component uses `useLayoutEffect` to update React Context state (via a custom hook), and another component in the same render tree triggers a state change, the state appears to \"reset\" during the render cascade. The context receives the correct value, but the component's local state reverts to its previous value.\n\n### Symptoms\n\n1. User clicks a button that calls `setState(true)`\n2. A `useLayoutEffect` in the same component updates context state\n3. The context correctly shows the new state\n4. **BUT** the component's local state appears to reset to `false`\n5. **Page refresh works correctly** - if you refresh while the localStorage flag is set, the component mounts with the correct state\n\n### Expected Behavior\n\nWhen `setIsEditing(true)` is called:\n1. `isEditing` should become `true` and stay `true`\n2. The component should re-render with `isEditing: true`\n3. The edit form should appear\n\n### Actual Behavior\n\nWhen `setIsEditing(true)` is called:\n1. `isEditing` briefly becomes `true`\n2. A context update triggers during `useLayoutEffect`\n3. `isEditing` appears to reset to `false` during the render cascade\n4. The component renders with `isEditing: false` (view mode)\n5. The context shows the correct state (edit mode in right rail)\n\n## Link to reproduction\n\nMinimal reproduction pattern:\n\n```tsx\n// contexts/right-rail-context.tsx\n'use client';\n\nimport { createContext, useContext, ReactNode, useCallback, useLayoutEffect, useState } from 'react';\n\nconst RightRailContext = createContext<{ content: ReactNode; setContent: (c: ReactNode) => void } | null>(null);\n\nexport function RightRailProvider({ children }: { children: ReactNode }) {\n  const [content, setContent] = useState<ReactNode>(null);\n  return (\n    <RightRailContext.Provider value={{ content, setContent }}>\n      {children}\n    </RightRailContext.Provider>\n  );\n}\n\n// This hook causes the bug\nexport function useSetRightRail(content: ReactNode, updateKey?: unknown) {\n  const context = useContext(RightRailContext);\n\n  // Using useLayoutEffect to update context triggers the bug\n  useLayoutEffect(() => {\n    context?.setContent(content);\n    return () => context?.setContent(null);\n  }, [context, content, updateKey]);\n}\n```\n\n```tsx\n// components/editor.tsx\n'use client';\n\nimport { useState } from 'react';\nimport { useSetRightRail } from '@/contexts/right-rail-context';\n\nexport function Editor() {\n  const [isEditing, setIsEditing] = useState(false);\n\n  // This useLayoutEffect update interferes with the state transition\n  useSetRightRail(\n    isEditing ? <div>EDIT MODE</div> : null,\n    isEditing // updateKey\n  );\n\n  const startEditing = () => {\n    console.log('Before setState:', isEditing); // false\n    setIsEditing(true);\n    console.log('After setState:', isEditing); // still false (expected, async)\n  };\n\n  // On first click, this logs true briefly, then false\n  console.log('Render with isEditing:', isEditing);\n\n  return (\n    <div>\n      {isEditing ? (\n        <form>Edit Form Here</form>\n      ) : (\n        <button onClick={startEditing}>EDIT</button>\n      )}\n    </div>\n  );\n}\n```\n\n### Console output on first click\n\n```\nBefore setState: false\nRender with isEditing: true    // Good!\nRender with isEditing: false   // BAD - state reset!\nRender with isEditing: false   // Stuck in view mode\n```\n\n### Why page refresh works\n\nAfter clicking EDIT (which sets a localStorage flag), refreshing the page works because:\n1. Component mounts fresh with no previous state\n2. `useState` initializer reads localStorage and returns `true`\n3. No state transition occurs - component starts in edit mode\n4. `useLayoutEffect` runs but doesn't interfere with an already-settled state\n\n## Workaround\n\nChanged `useLayoutEffect` to `useEffect` and wrapped context updates in `startTransition`:\n\n```tsx\nimport { useEffect, startTransition } from 'react';\n\nexport function useSetRightRail(content: ReactNode, updateKey?: unknown) {\n  const context = useContext(RightRailContext);\n\n  // Use useEffect (not useLayoutEffect) to defer the context update\n  useEffect(() => {\n    if (context?.setContent) {\n      startTransition(() => {\n        context.setContent(content);\n      });\n    }\n    return () => {\n      startTransition(() => {\n        context?.setContent(null);\n      });\n    };\n  }, [context, content, updateKey]);\n}\n```\n\nAdditionally, for our use case, we had to implement a page reload strategy:\n\n```tsx\nconst startEditing = useCallback(() => {\n  localStorage.setItem('editing-flag', 'true');\n  window.location.reload(); // Force fresh mount\n}, []);\n```\n\n## Additional Context\n\n- This bug does **not** occur in React 18\n- This bug occurs with Turbopack enabled\n- Likely related to React 19's concurrent rendering changes\n- The issue seems specific to `useLayoutEffect` triggering context state updates during a state transition\n- Similar to but distinct from #19103 (StrictMode state reset)\n\n## Related Issues\n\n- https://github.com/facebook/react/issues/32021 (state update batching)\n- https://github.com/facebook/react/issues/19103 (StrictMode state reset)\n",
      "solution": "We could not detect a valid reproduction link. **Make sure to follow the bug report template carefully.**\n\n### Why was this issue closed?\n\nTo be able to investigate, we need access to a reproduction to identify what triggered the issue. We need a link to a **public** GitHub repository ([template for App Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template), [template for Pages Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template-pages)), but you can also use these templates: [CodeSandbox: App Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template) or [CodeSandbox: Pages Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template-pages).\n\nThe bug template that you filled out has a section called \"Link to the code that reproduces this issue\", which is where you should provide the link to the reproduction.\n\n- If you did not provide a link or the link you provided is not valid, we will close the issue.\n- If you provide a link to a private repository, we will close the issue.\n- If you provide a link to a repository but not in the correct section, we will close the issue.\n\n### What should I do?\n\nDepending on the reason the issue was closed, you can do the following:\n\n- If you did not provide a link, please open a new issue with a link to a reproduction.\n- If you provided a link to a private repository, please open a new issue with a link to a public repository.\n- If you provided a link to a repository but not in the correct section, please open a new issue with a link to a reproduction in the correct section.\n\n**In general, assume that we should not go through a lengthy onboarding process at your company code only to be able to verify an issue.**\n\n### My repository is private and cannot make it public\n\nIn most cases, a private repo will not be a sufficient **minimal reproduction**, as this codebase might contain a lot of unrelated parts that would make our investigation take longer. Please do **not** make it public. Instead, create a new repository using the templates above, adding the relevant code to reproduce the issue. Common things to look out for:\n\n- Remove any code that is not related to the issue. (pages, API routes, components, etc.)\n- Remove any dependencies that are not related to the issue.\n- Remove any third-party service that would require us to sign up for an account to reproduce the issue.\n- Remove any environment variables that are not related to the issue.\n- Remove private packages that we do not have access to.\n- If the issue is not related to a monorepo specifically, try to reproduce the issue without a complex monorepo setup\n\n### I did not open this issue, but it is relevant to me, what can I do to help?\n\nAnyone experiencing the same issue is welcome to provide a minimal reproduction following the above steps by opening a new issue.\n\n### I think my reproduction is good enough, why aren't you looking into it quickly?\n\nWe look into every Next.js issue and constantly monitor open issues for new comments.\n\nHowever, sometimes we might miss one or two due to the popularity/high traffic of the repository. We apologize, and kindly ask you to refrain from tagging core maintainers, as that will usually not result in increased priority.\n\nUpvoting issues to show your interest will help us prioritize and address them as quickly as possible. That said, every issue is important to us, and if an issue gets closed by accident, we encourage you to open a new one linking to the old issue and we will look into it.\n\n### Useful Resources\n\n- [How to Contribute to Open Source (Next.js)](https://www.youtube.com/watch?v=cuoNzXFLitc)\n- [How to create a Minimal, Complete, and Verifiable example](https://stackoverflow.com/help/mcve)\n- [Reporting a Next.js bug](https://github.com/vercel/next.js/blob/canary/.github/ISSUE_TEMPLATE/1.bug_report.yml)\n- [Next.js Triaging issues](https://github.com/vercel/next.js/blob/canary/contributing/repository/triaging.md)\n\n\n---\n\nDo you mind setting up a Github repository with the issue? There's something weird about this claim:\n\n```js\n  useLayoutEffect(() => {\n    context?.setContent(content);\n    return () => context?.setContent(null);\n  }, [context, content, updateKey]);\n```\n\nBecause every time you call `setContent` you change state in the Provider level, and create new object reference on:\n\n```jsx\nvalue={{ content, setContent }}>\n```\n\nWhich re-triggers the `useLayoutEffect`. I guess that's not really your issue, but you may have arrived there after trying to simplify the source code with the issue. It is better to have a repository where we can see the issue you originally describe happening, otherwise we're just gonna be guessing or diagnosing un-related issues.",
      "labels": [
        "invalid link"
      ],
      "created_at": "2026-01-26T14:13:05Z",
      "closed_at": "2026-01-26T14:13:20Z",
      "url": "https://github.com/vercel/next.js/issues/89050",
      "comments_count": 2
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 87680,
      "title": "Next 16.1 showing 404 errors for dynamic imported components [TURBOPACK]",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/dcheng1290/next-dynamic-error/tree/main\n\n### To Reproduce\n\n1) npm run dev:turbo\n2) keep network request tab opened\n3) go to http://localhost:3000/test\n4) observe 404 errors in network request tab\n\n### Current vs. Expected behavior\n\nturbopack: I wasn't expect any 404 errors due to nested dynamic imports and I was not getting this error in 16.0.10.\n\n### Provide environment information\n\n```bash\nnode 22.17\nnext 16.1.0\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nLazy Loading\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext dev (local)\n\n### Additional context\n\nI am running into 404 errors locally in my app after updating to 16.1 potentially due to nested dynamic imports. This only happens when running turbopack and it doesn't happen with webpack.",
      "solution": "as a workaround you can set `experimental.turbopackClientSideNestedAsyncChunking = true` in your `next.config`\n\nim still digging into the root cause",
      "labels": [
        "Lazy Loading",
        "Turbopack",
        "linear: turbopack"
      ],
      "created_at": "2025-12-22T20:44:55Z",
      "closed_at": "2026-01-26T12:32:32Z",
      "url": "https://github.com/vercel/next.js/issues/87680",
      "comments_count": 5
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 89037,
      "title": "Turbopack `next build` produces `require(\"jsdom-<hash>\")` in server runtime causing `Cannot find module 'jsdom-<hash>'` in standalone/Docker",
      "problem": "### Link to the code that reproduces this issue\n\nnone\n\n### To Reproduce\n\n### Summary\n\nWhen building a Next.js App Router app with Turbopack (`next build --turbopack`, or the default bundler in Next.js 16.1), the server output can reference an external dependency using a hashed internal module id (e.g. `jsdom-4cccfac9827ebcfe`) and then attempts to load it via Node.js `require()`.\n\nIn a standalone/Docker runtime, this fails because the actual npm package name is `jsdom`, not `jsdom-<hash>`.\n\n**Workaround:** force webpack for production builds: `next build --webpack`\n\n#### 1) Install dependency\n\n```bash\nnpm i jsdom\n```\n\n#### 2) Add a server route that imports `jsdom`\n\n`app/api/repro/route.ts`\n\n```ts\nimport { NextResponse } from \"next/server\";\nimport { JSDOM } from \"jsdom\";\n\nexport async function GET() {\n  const dom = new JSDOM(\"<p>Hello</p>\");\n  return NextResponse.json({\n    ok: true,\n    text: dom.window.document.body.textContent,\n  });\n}\n```\n\n#### 3) Enable standalone output\n\n`next.config.ts`\n\n```ts\nimport type { NextConfig } from \"next\";\n\nconst nextConfig: NextConfig = {\n  output: \"standalone\",\n};\n\nexport default nextConfig;\n```\n\n#### 4) Build with Turbopack and run standalone output\n\n```bash\nnext build --turbopack   # or `next build` in Next 16.1 where Turbopack is default\nnode .next/standalone/server.js\n```\n\nThen request:\n\n- `GET /api/repro`\n\n### Current vs. Expected behavior\n\n\n### Actual Behavior\n\nThe server crashes at runtime:\n\n```text\nError: Failed to load external module jsdom-<hash>: Error: Cannot find module 'jsdom-<hash>'\nRequire stack:\n- .next/server/chunks/[root-of-the-server]__*.js\n- .next/server/chunks/[turbopack]_runtime.js\n- .next/server/app/api/repro/route.js\n...\n```\n\nIn the generated output, the server bundle contains a statement like:\n\n```js\nrequire(\"jsdom-4cccfac9827ebcfe\");\n```\n\n### Expected Behavior\n\n- The runtime should load the real package name: `require(\"jsdom\")`, **or**\n- Turbopack should correctly bundle/trace the dependency so it\u2019s available at runtime.\n\nNo hashed internal module ids should leak into Node.js `require()` calls.\n\n\n### Provide environment information\n\n```bash\n### Environment\n\n- **Next.js**: 16.1.4 (also observed on 16.1.x)\n- **Bundler**: Turbopack (production build)\n- **Output mode**: `output: \"standalone\"`\n- **Node.js**: 22.x\n- **Runtime**: standalone server (reproduced in Docker/production image runtime)\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nTurbopack\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext build (local)\n\n### Additional context\n\n### Notes / Workarounds\n\n- Adding `jsdom` to `serverExternalPackages` does not reliably resolve the issue when Turbopack is used, because the output may still reference `jsdom-<hash>`.\n- **Workaround:** build with webpack:\n\n```bash\nnext build --webpack\n```\n\nThis avoids generating `require(\"jsdom-<hash>\")` in the server output and runs correctly in standalone/Docker.\n",
      "solution": "We could not detect a valid reproduction link. **Make sure to follow the bug report template carefully.**\n\n### Why was this issue closed?\n\nTo be able to investigate, we need access to a reproduction to identify what triggered the issue. We need a link to a **public** GitHub repository ([template for App Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template), [template for Pages Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template-pages)), but you can also use these templates: [CodeSandbox: App Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template) or [CodeSandbox: Pages Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template-pages).\n\nThe bug template that you filled out has a section called \"Link to the code that reproduces this issue\", which is where you should provide the link to the reproduction.\n\n- If you did not provide a link or the link you provided is not valid, we will close the issue.\n- If you provide a link to a private repository, we will close the issue.\n- If you provide a link to a repository but not in the correct section, we will close the issue.\n\n### What should I do?\n\nDepending on the reason the issue was closed, you can do the following:\n\n- If you did not provide a link, please open a new issue with a link to a reproduction.\n- If you provided a link to a private repository, please open a new issue with a link to a public repository.\n- If you provided a link to a repository but not in the correct section, please open a new issue with a link to a reproduction in the correct section.\n\n**In general, assume that we should not go through a lengthy onboarding process at your company code only to be able to verify an issue.**\n\n### My repository is private and cannot make it public\n\nIn most cases, a private repo will not be a sufficient **minimal reproduction**, as this codebase might contain a lot of unrelated parts that would make our investigation take longer. Please do **not** make it public. Instead, create a new repository using the templates above, adding the relevant code to reproduce the issue. Common things to look out for:\n\n- Remove any code that is not related to the issue. (pages, API routes, components, etc.)\n- Remove any dependencies that are not related to the issue.\n- Remove any third-party service that would require us to sign up for an account to reproduce the issue.\n- Remove any environment variables that are not related to the issue.\n- Remove private packages that we do not have access to.\n- If the issue is not related to a monorepo specifically, try to reproduce the issue without a complex monorepo setup\n\n### I did not open this issue, but it is relevant to me, what can I do to help?\n\nAnyone experiencing the same issue is welcome to provide a minimal reproduction following the above steps by opening a new issue.\n\n### I think my reproduction is good enough, why aren't you looking into it quickly?\n\nWe look into every Next.js issue and constantly monitor open issues for new comments.\n\nHowever, sometimes we might miss one or two due to the popularity/high traffic of the repository. We apologize, and kindly ask you to refrain from tagging core maintainers, as that will usually not result in increased priority.\n\nUpvoting issues to show your interest will help us prioritize and address them as quickly as possible. That said, every issue is important to us, and if an issue gets closed by accident, we encourage you to open a new one linking to the old issue and we will look into it.\n\n### Useful Resources\n\n- [How to Contribute to Open Source (Next.js)](https://www.youtube.com/watch?v=cuoNzXFLitc)\n- [How to create a Minimal, Complete, and Verifiable example](https://stackoverflow.com/help/mcve)\n- [Reporting a Next.js bug](https://github.com/vercel/next.js/blob/canary/.github/ISSUE_TEMPLATE/1.bug_report.yml)\n- [Next.js Triaging issues](https://github.com/vercel/next.js/blob/canary/contributing/repository/triaging.md)\n",
      "labels": [
        "Turbopack",
        "invalid link"
      ],
      "created_at": "2026-01-26T10:27:57Z",
      "closed_at": "2026-01-26T10:28:17Z",
      "url": "https://github.com/vercel/next.js/issues/89037",
      "comments_count": 1
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 88368,
      "title": "monorepo setup",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/sensitivetube0/issue\n\n### To Reproduce\n\nnpm install\nnpm run dev\n\n### Current vs. Expected behavior\n\nWhen running npm run dev\nI get the following error:\nError: Next.js inferred your workspace root, but it may not be correct.\n    We couldn't find the Next.js package (next/package.json) from the project directory: /home/jesse-collins/issue/apps/web/app\n     To fix this, set turbopack.root in your Next.js config, or ensure the Next.js package is resolvable from this directory.\n    Note: For security and performance reasons, files outside of the project directory will not be compiled.\n    See https://nextjs.org/docs/app/api-reference/config/next-config-js/turbopack#root-directory for more information.\n\n    at ignore-listed frames\n\nnpm error Lifecycle script `dev` failed with error:\nnpm error code 1\nnpm error path /home/jesse-collins/issue/apps/web\nnpm error workspace @issue-repro/web@0.1.0\nnpm error location /home/jesse-collins/issue/apps/web\nnpm error command failed\nnpm error command sh -c next dev\n\n\nI expected Turbopack to find my package.json app in my project root at web but it is instead looking in app\n\n### Provide environment information\n\n```bash\nnpx next info // note this is from ./apps/web\n/bin/sh: 1: yarn: not found\n/bin/sh: 1: pnpm: not found\n\nOperating System:\n  Platform: linux\n  Arch: x64\n  Version: #37~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 20 10:25:38 UTC 2\n  Available memory (MB): 31980\n  Available CPU cores: 8\nBinaries:\n  Node: 25.2.1\n  npm: 11.6.2\n  Yarn: N/A\n  pnpm: N/A\nRelevant Packages:\n  next: 16.1.1 // Latest available version is detected (16.1.1).\n  eslint-config-next: N/A\n  react: 19.2.0\n  react-dom: 19.2.0\n  typescript: 5.9.3\nNext.js Config:\n  output: N/A\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nTurbopack\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext dev (local)\n\n### Additional context\n\n_No response_",
      "solution": "Not required but very useful. But for your simple scenario, you need to point to the root in your next.config:\n```\nimport path from 'path'\nconst nextConfig = {\n  turbopack: {\n    root: path.join(__dirname, '../../'),\n  }\n};\n\nexport default nextConfig;\n```",
      "labels": [
        "Turbopack",
        "locked"
      ],
      "created_at": "2026-01-10T16:14:11Z",
      "closed_at": "2026-01-11T19:44:58Z",
      "url": "https://github.com/vercel/next.js/issues/88368",
      "comments_count": 6
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 62900,
      "title": "Metadata URL resolution incorrectly appends a trailing slash for files when `output: \"export\"` and `trailingSlash: true`",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/mscottford/metadata-trailing-slash-url-resolution-bug\n\n### To Reproduce\n\n1. `git clone https://github.com/mscottford/metadata-trailing-slash-url-resolution-bug`\r\n2. `cd metadata-trailing-slash-url-resolution-bug`\r\n3. `npm install`\r\n4. `npm run start-static`\r\n5. (In a different terminal window, after the HTTP server is running) `npm run test`\n\n### Current vs. Expected behavior\n\n## Background\r\n\r\nWhen generating a static site with `output: \"export\"` any route handlers in the form of `./app/feed.xml/route.ts` will result in the creation of a `./out/feed.xml` file. Also, the `./app/not-found.tsx` file generates a `./out/404.html` file. This is true even if `trailingSlash: true` is set. That is expected behavior.\r\n\r\n## Issue\r\n\r\nWhen `trailingSlash: true` is set, metadata URLs from the `Metadata` object returned by the `generateMetadata` function will have a trailing slash appended, even if the URL references one of the files described above.\r\n\r\n### Alternate URLs\r\n\r\nGiven this metadata function defined in `./app/layout.tsx`.\r\n\r\n```\r\nexport async function generateMetadata(): Promise<Metadata> {\r\n  return {\r\n    metadataBase: new URL('https://example.com'),\r\n    alternates: {\r\n      canonical: './',\r\n      types: {\r\n        'application/rss+xml': [\r\n          {\r\n            title: 'Example',\r\n            url: '/feed.xml',\r\n          }\r\n        ],\r\n      },\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nIt's expected that the generated URL would exactly match `https://example.com/feed.xml`. However, the actual value is `https://example.com/feed.xml/`.\r\n\r\n### Canonical URL\r\n\r\nGiven the above `generatedMetadata` function in `./app/layout.tsx`, and the following `generateMetadata` function defined in `./app/not-found.tsx`:\r\n\r\n```\r\nexport async function generateMetadata(): Promise<Metadata> {\r\n  return {\r\n    alternates: {\r\n      canonical: `/404.html`,\r\n    },\r\n    openGraph: {\r\n      url: `/404.html`,\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nIt's expected that the canonical URL for the error page would be set to `https://example.com/404.html`. The actual value is `https://example.com/404.html/`.\r\n\r\n### OpenGraph URL\r\n\r\nGiven the above `generateMetadata` function defined in `./app/layout.tsx` and the above `generateMetadata` function defined in `./app/not-found.tsx`.\r\n\r\nIt's expected that the \"og:url\" for the error page would be set to `https://example.com/404.html`. The actual value is `https://example.com/404.html/`.\r\n\r\n## Reproducing\r\n\r\nThe tests in the reproducible example repository implement the expectations that are described above.\r\n\r\n## Suggested Solution\r\n\r\nThe metadata URL resolution should use the same logic that is used by the build when it determines whether or not `feed.xml` or `feed.xml/index.html` should be generated. The [`resolveAbsoluteUrlWithPathname` function](https://github.com/vercel/next.js/blob/9798ae52c575a23625fc2b8d412d257ef85a8384/packages/next/src/lib/metadata/resolvers/resolve-url.ts#L97-L99) may need to be changed to add that same logic.\r\n\n\n### Provide environment information\n\n```bash\nOperating System:\r\n  Platform: darwin\r\n  Arch: arm64\r\n  Version: Darwin Kernel Version 23.2.0: Wed Nov 15 21:53:18 PST 2023; root:xnu-10002.61.3~2/RELEASE_ARM64_T6000\r\n  Available memory (MB): 65536\r\n  Available CPU cores: 10\r\nBinaries:\r\n  Node: 21.5.0\r\n  npm: 10.2.4\r\n  Yarn: 1.22.21\r\n  pnpm: N/A\r\nRelevant Packages:\r\n  next: 14.2.0-canary.0 // Latest available version is detected (14.2.0-canary.0).\r\n  eslint-config-next: 14.1.2\r\n  react: 18.2.0\r\n  react-dom: 18.2.0\r\n  typescript: 5.1.3\r\nNext.js Config:\r\n  output: export\n```\n\n\n### Which area(s) are affected? (Select all that apply)\n\nApp Router, Metadata (metadata, generateMetadata, next/head), Static HTML Export (output: \"export\")\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext dev (local), next build (local), next start (local)\n\n### Additional context\n\nMy deployment target, for the project where I discovered this issue, is an S3 bucket.",
      "solution": "I am encountering the same issue in our project with \r\noutput: 'standalone'\r\nand \r\ntrailingSlash: true.\r\n\r\nFor legacy reasons we need the trailing slash in our paths, but we can not use this feature, because of this bug. Our SEO rating would collapse, if we would go live with this. All paths that end in file names are SEO relevant in our application. \r\n\r\nFurthermore I discovered the following behaviour relating tho this report: https://github.com/vercel/next.js/issues/62522\r\nIf we omit the metadataBase, relative paths would not get the trailing slash, but full URL still would.\r\n\r\nSuggested solution:\r\nI think, canonicals should not be appended with a Slash, even if this configuration is set in next.config. It should be the responsibility of the project to handle the correct canonicals. At least there should be an option to opt out of the current behaviour.\r\nAside from this, a parsing is needed to decide, if adding a slash is appropriate as [mscottford](https://github.com/mscottford) suggested or if the path ends in a file name.\n\n---\n\nI was able to reproduce this issue for feed XML files in versions v14.1.1 to v14.2.3. Upgrading to [v14.2.4](https://github.com/vercel/next.js/releases/tag/v14.2.4) or downgrading to v14.1.0 resolved the issue for feed files. Looks like it was addressed in https://github.com/vercel/next.js/pull/66636/ (did not test `404.html`)",
      "labels": [
        "bug",
        "locked",
        "stale"
      ],
      "created_at": "2024-03-05T19:55:17Z",
      "closed_at": "2026-01-11T23:43:22Z",
      "url": "https://github.com/vercel/next.js/issues/62900",
      "comments_count": 5
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 89019,
      "title": "Documentation misleading: loading.tsx does NOT cascade to child routes",
      "problem": "## Description\n\nThe [loading.js documentation](https://nextjs.org/docs/app/api-reference/file-conventions/loading) contains a misleading statement that contradicts the actual behavior of Next.js:\n\n> \"In the same folder, loading.js will be nested inside layout.js. It will automatically wrap the page.js file **and any children below** in a `<Suspense>` boundary.\"\n\nThis statement is **incorrect** or at minimum **very misleading**. In practice, `loading.tsx` does NOT cascade to child routes - it only wraps the `page.tsx` in the **same folder**.\n\n## To Reproduce\n\n**File structure:**\n```\napp/\n\u2514\u2500\u2500 [locale]/\n    \u2514\u2500\u2500 (routes)/\n        \u251c\u2500\u2500 layout.tsx\n        \u251c\u2500\u2500 loading.tsx       \u2190 Parent loading\n        \u251c\u2500\u2500 (home)/\n        \u2502   \u2514\u2500\u2500 page.tsx      \u2190 Child with async delay\n        \u2514\u2500\u2500 about/\n            \u2514\u2500\u2500 page.tsx      \u2190 Child with async delay\n```\n\n**Code:**\n\n`app/[locale]/(routes)/(home)/page.tsx`:\n```tsx\nexport default async function Home() {\n  await new Promise((resolve) => setTimeout(resolve, 3000));\n  return <main>Home Content</main>;\n}\n```\n\n`app/[locale]/(routes)/about/page.tsx`:\n```tsx\nexport default async function About() {\n  // Any async operation\n  return <main>About Content</main>;\n}\n```\n\n`app/[locale]/(routes)/loading.tsx`:\n```tsx\nexport default function Loading() {\n  return <p>Loading...</p>;\n}\n```\n\n## Current Behavior\n\nThe loading UI from `(routes)/loading.tsx` is **NOT shown** when navigating to:\n- `/home` (inside route group `(home)`)\n- `/about` (regular folder)\n\nThe page appears blank/frozen during the async operation.\n\n## Expected Behavior (Based on Documentation)\n\nAccording to the phrase **\"and any children below\"**, the `loading.tsx` at the `(routes)` level should wrap ALL child pages in a Suspense boundary, including:\n- Pages in nested route groups like `(home)/`\n- Pages in regular nested folders like `about/`\n\n## Actual Behavior\n\nThe loading UI **only appears** when `loading.tsx` is placed in the **same folder** as the `page.tsx`:\n```\napp/[locale]/(routes)/(home)/loading.tsx  \u2190 Works for (home)/page.tsx\napp/[locale]/(routes)/about/loading.tsx   \u2190 Works for about/page.tsx\n```\n\n## Impact\n\nThis issue affects:\n1. **Route groups** `(name)/` - loading.tsx doesn't cascade through route group boundaries\n2. **Regular nested folders** `about/` - loading.tsx doesn't cascade to any child folders\n3. **All developers** relying on the documentation to understand loading.tsx behavior\n\n## Additional Context\n\nThis has been a known issue since Next.js 13 was released:\n- #45021 - \"Loading.js not working on nested segment\" (closed without documentation fix)\n- #43209 - Parent loading.tsx flashes before nested loading.tsx\n- #62115 - Flash of parent loading in parallel routes\n\nThe documentation needs to either:\n1. **Clarify** that \"children below\" only means the immediate `page.tsx` in the same folder, OR\n2. **Update Next.js behavior** to make loading.tsx actually cascade to all child routes as documented\n\nThe phrase **\"and any children below\"** strongly implies inheritance/cascading behavior (similar to how layouts work), but this is not the actual behavior.\n\n## Suggested Documentation Fix\n\nChange from:\n> \"It will automatically wrap the page.js file **and any children below** in a `<Suspense>` boundary.\"\n\nTo:\n> \"It will automatically wrap the page.js file **in the same folder** in a `<Suspense>` boundary. **Note:** loading.tsx does NOT cascade to nested routes - each nested route requires its own loading.tsx file.\"\n\nOr add a clear warning:\n> \u26a0\ufe0f **Important:** Unlike layouts, loading.tsx files do NOT cascade to child routes. You must place a loading.tsx file in each route segment folder that needs loading UI.\n\n## Environment\n\n- Next.js version: All versions with App Router (13.x - 16.x)\n- Affects: All deployment environments\n- Documentation: https://nextjs.org/docs/app/api-reference/file-conventions/loading\n\n## Related Issues\n\n- #45021 (closed) - Same issue but was closed without documentation update\n- #43209 (closed) - Parent loading flashing issue\n- #62115 (closed) - Parallel routes loading flash",
      "solution": "We could not detect a valid reproduction link. **Make sure to follow the bug report template carefully.**\n\n### Why was this issue closed?\n\nTo be able to investigate, we need access to a reproduction to identify what triggered the issue. We need a link to a **public** GitHub repository ([template for App Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template), [template for Pages Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template-pages)), but you can also use these templates: [CodeSandbox: App Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template) or [CodeSandbox: Pages Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template-pages).\n\nThe bug template that you filled out has a section called \"Link to the code that reproduces this issue\", which is where you should provide the link to the reproduction.\n\n- If you did not provide a link or the link you provided is not valid, we will close the issue.\n- If you provide a link to a private repository, we will close the issue.\n- If you provide a link to a repository but not in the correct section, we will close the issue.\n\n### What should I do?\n\nDepending on the reason the issue was closed, you can do the following:\n\n- If you did not provide a link, please open a new issue with a link to a reproduction.\n- If you provided a link to a private repository, please open a new issue with a link to a public repository.\n- If you provided a link to a repository but not in the correct section, please open a new issue with a link to a reproduction in the correct section.\n\n**In general, assume that we should not go through a lengthy onboarding process at your company code only to be able to verify an issue.**\n\n### My repository is private and cannot make it public\n\nIn most cases, a private repo will not be a sufficient **minimal reproduction**, as this codebase might contain a lot of unrelated parts that would make our investigation take longer. Please do **not** make it public. Instead, create a new repository using the templates above, adding the relevant code to reproduce the issue. Common things to look out for:\n\n- Remove any code that is not related to the issue. (pages, API routes, components, etc.)\n- Remove any dependencies that are not related to the issue.\n- Remove any third-party service that would require us to sign up for an account to reproduce the issue.\n- Remove any environment variables that are not related to the issue.\n- Remove private packages that we do not have access to.\n- If the issue is not related to a monorepo specifically, try to reproduce the issue without a complex monorepo setup\n\n### I did not open this issue, but it is relevant to me, what can I do to help?\n\nAnyone experiencing the same issue is welcome to provide a minimal reproduction following the above steps by opening a new issue.\n\n### I think my reproduction is good enough, why aren't you looking into it quickly?\n\nWe look into every Next.js issue and constantly monitor open issues for new comments.\n\nHowever, sometimes we might miss one or two due to the popularity/high traffic of the repository. We apologize, and kindly ask you to refrain from tagging core maintainers, as that will usually not result in increased priority.\n\nUpvoting issues to show your interest will help us prioritize and address them as quickly as possible. That said, every issue is important to us, and if an issue gets closed by accident, we encourage you to open a new one linking to the old issue and we will look into it.\n\n### Useful Resources\n\n- [How to Contribute to Open Source (Next.js)](https://www.youtube.com/watch?v=cuoNzXFLitc)\n- [How to create a Minimal, Complete, and Verifiable example](https://stackoverflow.com/help/mcve)\n- [Reporting a Next.js bug](https://github.com/vercel/next.js/blob/canary/.github/ISSUE_TEMPLATE/1.bug_report.yml)\n- [Next.js Triaging issues](https://github.com/vercel/next.js/blob/canary/contributing/repository/triaging.md)\n",
      "labels": [
        "invalid link"
      ],
      "created_at": "2026-01-25T19:23:41Z",
      "closed_at": "2026-01-25T19:23:57Z",
      "url": "https://github.com/vercel/next.js/issues/89019",
      "comments_count": 1
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 85631,
      "title": "Encoding issue with middleware rewrites and headers",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/franknoel/nextjs-bug-repro-middleware-encoding\n\n### To Reproduce\n\n1. Deploy [https://github.com/franknoel/nextjs-bug-repro-middleware-encoding](https://github.com/franknoel/nextjs-bug-repro-middleware-encoding)\n2. Add a header with a non-ASCII value, like `Montr\u00e9al`. I'm using Cloudflare IP geolocation, which add `cf-ipcity` to the request.\n3. Load a page that uses the middleware\n\nExemple deployment: https://nextjs-bug-repro.pcobalt.com/\n\n### Current vs. Expected behavior\n\nErrors from Vercel:\n\n```\n500: INTERNAL_SERVER_ERROR\nCode: MIDDLEWARE_INVOCATION_FAILED\nID: cle1::w8596-1761928200824-a0b4989ee40e\n```\n\n```\nVercel Runtime Malformed Response Header Error: Header 'x-middleware-request-cf-ipcity: Montr\u00c3\u00a9al' contains non-ASCII characters.\n```\n\nI'd expect Next.js to support non-ASCII characters, like it did seamlessly before version 16.\n\n### Provide environment information\n\n```bash\nOperating System:\n  Platform: darwin\n  Arch: arm64\n  Version: Darwin Kernel Version 24.6.0: Mon Aug 11 21:16:21 PDT 2025; root:xnu-11417.140.69.701.11~1/RELEASE_ARM64_T6000\n  Available memory (MB): 32768\n  Available CPU cores: 10\nBinaries:\n  Node: 22.18.0\n  npm: 10.9.3\n  Yarn: N/A\n  pnpm: 8.6.12\nRelevant Packages:\n  next: 16.0.2-canary.2 // Latest available version is detected (16.0.2-canary.2).\n  eslint-config-next: N/A\n  react: 19.2.0\n  react-dom: 19.2.0\n  typescript: 5.9.3\nNext.js Config:\n  output: N/A\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nHeaders\n\n### Which stage(s) are affected? (Select all that apply)\n\nVercel (Deployed)\n\n### Additional context\n\nThis is working with Next.js v15 with `next-intl` v4.4.0 and prior versions. I first contacted Vercel support and they confirmed that it's a Next.js issue. They also said that I should open an issue on GitHub to report the problem.\n\nAs of today, the only workaround is to encode/decode each header manually. Although this works technically, it is not a real solution as any other header containing a non-ASCII character can break the middleware. It also adds a lot of unnecessary code into the proxy, which should be as light as possible.",
      "solution": "Just tried with the new v16.1.0 Next.js release and I can confirm that the issue is still there.",
      "labels": [
        "Headers"
      ],
      "created_at": "2025-10-31T16:42:09Z",
      "closed_at": "2026-01-25T17:06:53Z",
      "url": "https://github.com/vercel/next.js/issues/85631",
      "comments_count": 10
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 80210,
      "title": "`redirect()` with non ascii characters throws a platform error on Vercel",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/tom-sherman/nextjs-non-ascii-header-bug\n\n### To Reproduce\n\nPass a path with non-ascii characters to `redirect()` eg. `/ma\u00f1ana`\n\n### Current vs. Expected behavior\n\n**Current behaviour:**\n\n\n- In a server action, HTTP 500 error, the following error is logged in Vercel logs\n  - `Vercel Runtime Malformed Response Header Error: Header 'x-action-redirect: /ma\u00f1ana;push' contains non-ASCII characters.`\n- In a dynamic page with a redirect, HTTP 500 error, the following error is logged in Vercel logs\n  - `Vercel Runtime Malformed Response Header Error: Header 'location: /ma\u00f1ana' contains non-ASCII characters.`\n- In a dynamic page with a redirect, when navigating via `<Link`> it works as expected\n- In a static page with a redirect, it works as expected\n\nAdditionally, when the error occurs it's not possible to catch it with a `error.tsx` or `global-error.tsx` although maybe this is expected behaviour with Vercel platform errors.\n\n**Expected behaviour:**\n\nTo be redirected to the path specified, uri encoded ie. `/ma%C3%B1ana`. This would match `next dev`/`next start` behaviour.\n\n### Provide environment information\n\n```bash\nOperating System:\n  Platform: darwin\n  Arch: arm64\n  Version: Darwin Kernel Version 24.5.0: Tue Apr 22 19:53:27 PDT 2025; root:xnu-11417.121.6~2/RELEASE_ARM64_T6041\n  Available memory (MB): 24576\n  Available CPU cores: 12\nBinaries:\n  Node: 22.14.0\n  npm: 10.9.2\n  Yarn: N/A\n  pnpm: 10.9.0\nRelevant Packages:\n  next: 15.4.0-canary.67 // Latest available version is detected (15.4.0-canary.67).\n  eslint-config-next: N/A\n  react: 19.1.0\n  react-dom: 19.1.0\n  typescript: 5.8.3\nNext.js Config:\n  output: N/A\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nLinking and Navigating, Server Actions, Redirects\n\n### Which stage(s) are affected? (Select all that apply)\n\nVercel (Deployed)\n\n### Additional context\n\nDeployed example here: https://nextjs-non-ascii-header-bug.vercel.app/\n\nSeems to be a similar platform error to the issue in #79195\n\nI'd expect `redirect()` to uri encode the path, but it seems like it doesn't. Instead it throws `NEXT_REDIRECT;push;/ma\u00f1ana;307;` and I guess some code above handles that? Not sure why that handling then isn't included in the Vercel build.",
      "solution": "(sorry for the multiple edits! was a bit confused about the issue)\n\nThis is not the same issue as https://github.com/vercel/next.js/issues/79195\n\nWe're aware of this Vercel platform constraint and are currently rolling out a change to eliminate it entirely. Once this is fully rolled out, you can include non-ASCII characters in response headers. And if Vercel Functions runtime finds non-ASCII characters, it will encode them accordingly (percent-encode, RFC 3986) instead of throwing\n\n---\n\n@tom-sherman This is a bit more nuanced, and I actually don't think other providers will see this issue. The issue was specifically with Vercel Functions runtime strictly (and unnecessarily!) checking the response header values, and this does not affect self-hosted Next.js apps.\n\nIn fact, the HTTP RFC 9110 spec does not prohibit the use of arbitrary non-ASCII bytes in the response header value, and it suggests that downstream systems treat such a header value as opaque data, but does not define implementation details. So in practice, user code can use any bytes, but that doesn't mean it won't blow up downstream proxies/middle-boxes with its encoding. Therefore, we (Vercel Functions runtime) take the safest approach and encode non-ASCII bytes using percent-encoding (RFC 3986).",
      "labels": [
        "Linking and Navigating",
        "Server Actions",
        "Redirects"
      ],
      "created_at": "2025-06-05T16:26:11Z",
      "closed_at": "2026-01-25T17:06:32Z",
      "url": "https://github.com/vercel/next.js/issues/80210",
      "comments_count": 7
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 89006,
      "title": "Bug Report: React Compiler Only Auto-Memoizes Components on Root Page, Not on Other Routes",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/sanjaikumarv/next-js-compiler-issue\n\n### To Reproduce\n\n1. Enable React Compiler in `next.config.ts`:\n\n```typescript\nconst nextConfig: NextConfig = {\n  reactCompiler: true,\n};\n```\n\n2. Create identical page components in multiple locations:\n   - `app/page.tsx` (root page)\n   - `app/count/page.tsx` (regular route)\n   - `app/(protected)/count/page.tsx` (route group page)\n\n3. All pages use the same child components (A, B, C) that log when they render\n4. All pages use the same context provider for state management\n5. Click the increment button to trigger state updates on each page\n\n### Current vs. Expected behavior\n\n### Current vs. Expected behavior\n\n**Current Behavior:**\n\n- **Root page (`app/page.tsx`)**: Child components A, B, C do **NOT** re-render when parent state changes (React Compiler auto-memoizes them) \u2705\n- **Regular route (`app/count/page.tsx`)**: Child components A, B, C **DO** re-render on every parent state change (React Compiler does NOT auto-memoize them) \u274c\n- **Route group page (`app/(protected)/count/page.tsx`)**: Child components A, B, C **DO** re-render on every parent state change (React Compiler does NOT auto-memoize them) \u274c\n\n**Expected Behavior:**\nReact Compiler should apply consistent optimization across **all pages** regardless of their location in the app directory structure. If it auto-memoizes components on the root page, it should do the same for all other routes (regular routes, nested routes, and route groups).\n\n\n### Provide environment information\n\n```bash\nOperating System:\n  Platform: win32\n  Arch: x64\n  Version: Windows 10/11\n\nBinaries:\n  Node: 20.x.x (or your version)\n  npm: 10.x.x\n  Yarn: N/A\n  pnpm: 9.x.x\n\nRelevant Packages:\n  next: 16.1.4\n  react: 19.2.3\n  react-dom: 19.2.3\n  babel-plugin-react-compiler: 1.0.0\n  typescript: 5.x.x\n\nNext.js Config:\n  output: N/A\n  reactCompiler: true\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nReact\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext dev (local), next build (local), next start (local), Vercel (Deployed)\n\n### Additional context\n\n## Bug Report: React Compiler Only Auto-Memoizes Components on Root Page, Not on Other Routes\n\n### To Reproduce\n\n1. Enable React Compiler in `next.config.ts`:\n\n```typescript\nconst nextConfig: NextConfig = {\n  reactCompiler: true,\n};\n```\n\n2. Create identical page components in multiple locations:\n   - `app/page.tsx` (root page)\n   - `app/count/page.tsx` (regular route)\n   - `app/(protected)/count/page.tsx` (route group page)\n\n3. All pages use the same child components (A, B, C) that log when they render\n4. All pages use the same context provider for state management\n5. Click the increment button to trigger state updates on each page\n\n### Current vs. Expected behavior\n\n**Current Behavior:**\n\n- **Root page (`app/page.tsx`)**: Child components A, B, C do **NOT** re-render when parent state changes (React Compiler auto-memoizes them) \u2705\n- **Regular route (`app/count/page.tsx`)**: Child components A, B, C **DO** re-render on every parent state change (React Compiler does NOT auto-memoize them) \u274c\n- **Route group page (`app/(protected)/count/page.tsx`)**: Child components A, B, C **DO** re-render on every parent state change (React Compiler does NOT auto-memoize them) \u274c\n\n**Expected Behavior:**\nReact Compiler should apply consistent optimization across **all pages** regardless of their location in the app directory structure. If it auto-memoizes components on the root page, it should do the same for all other routes (regular routes, nested routes, and route groups).\n\n### Provide environment information\n\n```bash\nOperating System:\n  Platform: win32\n  Arch: x64\n  Version: Windows 10/11\n\nBinaries:\n  Node: 20.x.x (or your version)\n  npm: 10.x.x\n  Yarn: N/A\n  pnpm: 9.x.x\n\nRelevant Packages:\n  next: 16.1.4\n  react: 19.2.3\n  react-dom: 19.2.3\n  babel-plugin-react-compiler: 1.0.0\n  typescript: 5.x.x\n\nNext.js Config:\n  output: N/A\n  reactCompiler: true\n```\n\n### Which area(s) are affected? (Select all that apply)\n\n- [x] App Router\n- [x] React Compiler (experimental)\n- [ ] Pages Router\n- [ ] Turbopack\n- [ ] Other (please specify)\n\n### Which stage(s) are affected? (Select all that apply)\n\n- [x] next dev (local)\n- [ ] next build (local)\n- [ ] next start (local)\n- [ ] Vercel (Deployed)\n- [ ] Other (please specify)\n\n### Additional context\n\n### Code Examples\n\n**Root Page (`app/page.tsx`)** - Components DON'T re-render:\n\n```tsx\n\"use client\";\n\nimport A from \"@/src/components/test/A\";\nimport B from \"@/src/components/test/B\";\nimport C from \"@/src/components/test/C\";\nimport { useStateContext } from \"@/src/components/test/context/state-context\";\n\nexport default function Home() {\n  const { state, setState } = useStateContext();\n  return (\n    <div className='flex min-h-screen items-center justify-center bg-zinc-50 font-sans dark:bg-black'>\n      Hii\n      <button onClick={() => setState(state + 1)}>Increment</button>\n      {state}\n      <A />\n      <B />\n      <C />\n    </div>\n  );\n}\n```\n\n**Regular Route Page (`app/count/page.tsx`)** - Components DO re-render:\n\n```tsx\n\"use client\";\n\nimport A from \"@/src/components/test/A\";\nimport B from \"@/src/components/test/B\";\nimport C from \"@/src/components/test/C\";\nimport { useStateContext } from \"@/src/components/test/context/state-context\";\n\nexport default function page() {\n  const { state, setState } = useStateContext();\n  return (\n    <div className='flex min-h-screen items-center justify-center bg-zinc-50 font-sans dark:bg-black'>\n      Hii\n      <button onClick={() => setState(state + 1)}>Increment</button>\n      {state}\n      <A />\n      <B />\n      <C />\n    </div>\n  );\n}\n```\n\n**Route Group Page (`app/(protected)/count/page.tsx`)** - Components DO re-render:\n\n```tsx\n\"use client\";\n\nimport A from \"@/src/components/test/A\";\nimport B from \"@/src/components/test/B\";\nimport C from \"@/src/components/test/C\";\nimport { useStateContext } from \"@/src/components/test/context/state-context\";\n\nexport default function page() {\n  const { state, setState } = useStateContext();\n  return (\n    <div className='flex min-h-screen items-center justify-center bg-zinc-50 font-sans dark:bg-black'>\n      Hii\n      <button onClick={() => setState(state + 1)}>Increment</button>\n      {state}\n      <A />\n      <B />\n      <C />\n    </div>\n  );\n}\n```\n\n**Child Component Example (`src/components/test/A.tsx`):**\n\n```tsx\nimport React from \"react\";\n\nexport default function A() {\n  console.log(\"A rendered\");\n  return <div>A</div>;\n}\n```\n\n**Context Provider (`src/components/test/context/state-context.tsx`):**\n\n```tsx\n\"use client\";\n\nimport { createContext, useContext, useState } from \"react\";\n\nconst StateContext = createContext<any>({});\nexport const useStateContext = () => useContext(StateContext);\n\nexport function StateProvider({ children }: { children: React.ReactNode }) {\n  const [state, setState] = useState(1);\n  return (\n    <StateContext.Provider value={{ state, setState }}>\n      {children}\n    </StateContext.Provider>\n  );\n}\n```\n\n**Root Layout (`app/layout.tsx`):**\n\n```tsx\nimport { StateProvider } from \"@/src/components/test/context/state-context\";\n\nexport default function RootLayout({\n  children,\n}: Readonly<{\n  children: React.ReactNode;\n}>) {\n  return (\n    <html lang='en'>\n      <body>\n        <StateProvider>{children}</StateProvider>\n      </body>\n    </html>\n  );\n}\n```\n\n### Console Output Comparison\n\n**Root page (`/`)** - After clicking increment 3 times:\n\n```\nA rendered  // Only on initial mount\nB rendered  // Only on initial mount\nC rendered  // Only on initial mount\n// No additional renders when clicking increment\n```\n\n**Regular route (`/count`)** - After clicking increment 3 times:\n\n```\nA rendered  // Initial mount\nB rendered  // Initial mount\nC rendered  // Initial mount\nA rendered  // Click 1\nB rendered  // Click 1\nC rendered  // Click 1\nA rendered  // Click 2\nB rendered  // Click 2\nC rendered  // Click 2\nA rendered  // Click 3\nB rendered  // Click 3\nC rendered  // Click 3\n```\n\n**Route group page (`/(protected)/count`)** - After clicking increment 3 times:\n\n```\nA rendered  // Initial mount\nB rendered  // Initial mount\nC rendered  // Initial mount\nA rendered  // Click 1\nB rendered  // Click 1\nC rendered  // Click 1\nA rendered  // Click 2\nB rendered  // Click 2\nC rendered  // Click 2\nA rendered  // Click 3\nB rendered  // Click 3\nC rendered  // Click 3\n```\n\n### Impact\n\nThis inconsistency makes it difficult to:\n\n1. Rely on React Compiler for predictable performance optimization\n2. Understand when components will be auto-memoized\n3. Build consistent performance patterns across the application\n4. Trust the experimental React Compiler feature in production\n\n### Workaround\n\nManually wrapping components with `React.memo()` provides consistent behavior across all routes, but defeats the purpose of the React Compiler's automatic optimization.\n\n### Notes\n\n- All pages are identical in structure and code\n- All use the same context provider\n- All are client components (`\"use client\"`)\n- **The only difference is the file location: React Compiler ONLY optimizes the root page (`app/page.tsx`), but NOT any other routes**\n- This affects both regular routes (`app/count/page.tsx`) and route groups (`app/(protected)/count/page.tsx`)\n- Disabling `reactCompiler` makes all pages behave consistently (all re-render all components)\n",
      "solution": "Homepage:\n\n<img width=\"1212\" height=\"492\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/3a6ada08-f246-480c-8a4d-eca8d91d377f\" />\n\n`/count`:\n\n<img width=\"596\" height=\"332\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/5aa04f73-482c-4f7f-9d45-790079ea40e8\" />\n\n\n\nFrom the Next.js side everything is applying as expected. As visible in the React devtools, note the `Memo` labels on both.\n\nThe difference is that you (or the used AI, as the issue looks AI generated) used `page` instead of `Page` as the function name on `/count`. The assessment that React Compiler does not apply on other routes is incorrect. It's not applying React component optimizations because the export is not a React component (doesn't use a capital first character).\n\nYou can see the difference in the React Compiler playground: \n\n- No optimization: https://playground.react.dev/#N4Igzg9grgTgxgUxALhAHRFMCAEcA2AlggHYAuGA3GiTYQLYAOEMZOAgjgGYwT04YAAgHow8YXD7MSpMmGFkEYMsPZU6TFmwBC3XvyGjxkzTPLzFy4dvUkGzVjgDCevgJAixcCVIhm5CkoqTrb2WjjAOFgIAMpkAIaKTn6KAB5sAL6uBh5G3ibSshZBPuQI6aIJigC0kmXptjTlDmwAJghc8VD4bFxQJHBkhH44jPEA5ggAFACUETQ4eH7KETjKiQgANGsIZHEbOFkAvFHY+0kp5WSz1CSLMLuwd1MLizgAPK2EAG54+PFgMAAOXi9AQRwwXHw5Rw9EIJGqAAtqmA4A9SDhCIp6GBarIEDAcAArLBDLgATzxZUJACNxtUAF7wuDVACsAAZuCkUfESGAcK14jAANbIOnVGn-ODCjAAPlebxwAAlCIQFW93jSoGQyCM-E4iNKjsBZjgjrKdnsqtN1oocABqHAARhmGVlAEkBg8weR3sItTq-PK7oqIraEBl1Yt3pxhMHQx9dHGox8XMmQ9HhF9vvGcDNbpGSCBNiA6lxCOMUCAwo4yOTGLhIgAFfBQcbwgDyjCGy0O2RwAHIafEaQh8NVGK32wiHvFBrUpIRoTAs4RlAPbjQpsBXsIfEwl4lhiQALIQdrIdzxfD4DA0LJgI9gctKHAttud7vHsD54vgREQAA7p6igwCQ15gCgnT4NgGRAA\n- Optimized: https://playground.react.dev/#N4Igzg9grgTgxgUxALhAHRFMCAEcA2AlggHYAuGA3GiTYQLYAOEMZOAgjgGYwT04YAAgHow8YXD7MSpMmGFkEYMsPZU6TFmwBC3XvyGjxkzTPLzFy4dvUkGzVjgDCevgJAixcCVIhm5CkoqTrb2WjjAOFgIAMpkAIaKTn6KAB5sAL6uBh5G3ibSshZBPuQI6aIJigC0kmXptjTlDmwAJghc8VD4bFxQJHBkhH44AArxAOYIABQAlBE0OHh+yhE4yokIADTrCGRxmzhZALxR2AdJKeVkc9QkSzB7sPfTi0s4ADythABuePjxMBgABy8XoCGOGC4+HKOHohBI1QAFtUwHBHqQcIRFPQwLVZAgYDgAFZYIZcACe+LKRIARhNqgAvBFwaoAVgADNwUqj4iQwDhWvEYABrZD06q0gFwEUYAB8b3eOAAEoRCIr3h9aVAyGQRn4nEQZcdgHMcMc5bt9lUZhtFDgANQ4ACMswycoAkgNHuDyB9hNrdX4FfclRE7QgMhqlh9OMIQ2HPrp49HPi4U6GY8Jvj8EzhZncoyQQFsQHUuIQJigQGFHGQKYxcJFRvgoBMEQB5RhDFZHbI4ADktPitIQ+GqjFb7cRj3ig1qUkIMJg2cIygHdxo02Ab2EPiYS8SwxIAFkIO1kO54vh8BgaFkwEewBWlGMp53u8ewAWS+AkRAAHcvUUGASGvMAUE6fBsAyIA\n\n\nGoing to close this issue as it's not a bug in Next.js.\n",
      "labels": [
        "Turbopack",
        "Pages Router",
        "React"
      ],
      "created_at": "2026-01-25T07:58:54Z",
      "closed_at": "2026-01-25T12:30:28Z",
      "url": "https://github.com/vercel/next.js/issues/89006",
      "comments_count": 1
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 88335,
      "title": "Turbopack build fails with symlinked node_modules: \"Symlink node_modules is invalid, it points out of the filesystem root\"",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/lobehub/lobe-chat\n\n### To Reproduce\n1. Clone lobe-chat and install dependencies:\n```bash\ngit clone https://github.com/lobehub/lobe-chat.git\ncd lobe-chat\npnpm install\n```\n\n2. Create a temporary build directory with symlinked node_modules:\n\nSee https://github.com/lobehub/lobe-chat/blob/5380f76ed1488936f508e755d106c49a22703b21/scripts/electronWorkflow/buildNextApp.mts\n\n```bash\nmkdir -p tmp/test-build\ncd tmp/test-build\nln -s ../../node_modules node_modules\nln -s ../../packages packages\ncp ../../package.json .\ncp ../../next.config.ts .\ncp ../../tsconfig.json .\ncp -r ../../src .\n```\n\n3. Run Next.js build with Turbopack:\n```bash\npnpm next build\n```\n\n4. Build fails with: `Symlink node_modules is invalid, it points out of the filesystem root`\n\n### Current vs. Expected behavior\n\n**Current behavior:**\nTurbopack build fails with a fatal error when `node_modules` is a symlink pointing to a parent directory:\n\n```\nFATAL: An unexpected Turbopack error occurred.\nError [TurbopackInternalError]: Symlink node_modules is invalid, it points out of the filesystem root\n```\n\n**Expected behavior:**\nTurbopack should correctly resolve symlinked `node_modules` that point to parent directories, similar to how Webpack handles this scenario. This is a common pattern for:\n- Shadow/isolated builds in temporary directories\n- Monorepo setups with shared dependencies\n- Electron app builds that need modified source but shared node_modules\n**Note:** This worked fine with Webpack-based Next.js builds.\n\n### Provide environment information\n\n```\nOperating System:\n  Platform: darwin\n  Arch: arm64\n  Version: Darwin Kernel Version 25.2.0\nBinaries:\n  Node: 20.x\n  pnpm: 10.x\nNext.js:\n  Version: 16.1.1\n  Build mode: Turbopack (production build)\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nTurbopack\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext build (local)\n\n### Additional context\n\n**Use Case:**\nBuilding an Electron desktop app that requires source code modifications but shares dependencies with the main project. The workflow creates a shadow/isolated build directory with symlinked `node_modules` to avoid duplicating ~1GB+ of dependencies.\n\n**Full Error Stack:**\n```\n\n> Build error occurred\nError [TurbopackInternalError]: Symlink node_modules is invalid, it points out of the filesystem root\n\nDebug info:\n- Execution of get_all_written_entrypoints_with_issues_operation failed\n- Execution of EntrypointsOperation::new failed\n- Execution of all_entrypoints_write_to_disk_operation failed\n- Execution of Project::emit_all_output_assets failed\n- Execution of *emit_assets failed\n- Execution of all_assets_from_entries_operation failed\n- Execution of *all_assets_from_entries failed\n- Execution of output_assets_operation failed\n- Execution of Project::get_all_endpoints failed\n- Execution of Project::get_all_endpoint_groups failed\n- Execution of Project::entrypoints failed\n- Execution of AppProject::routes failed\n- Execution of directory_tree_to_entrypoints_internal failed\n- Execution of try_get_next_package failed\n- Execution of *ResolveResult::first_source failed\n- Execution of resolve failed\n- Execution of resolve_internal failed\n- Execution of find_package failed\n- Symlink node_modules is invalid, it points out of the filesystem root\n    at ignore-listed frames {\n  type: 'TurbopackInternalError',\n  location: undefined\n}\n\u274c Build failed.\n\ud83e\uddf9 Cleaning up workspace...\nError: Command failed: next build\n    at genericNodeError (node:internal/errors:985:15)\n    at wrappedFn (node:internal/errors:539:14)\n    at checkExecSyncError (node:child_process:925:11)\n    at execSync (node:child_process:997:15)\n    at build (/Users/innei/git/work/lobe-chat/scripts/electronWorkflow/buildNextApp.mts:88:5)\n    at async <anonymous> (/Users/innei/git/work/lobe-chat/scripts/electronWorkflow/buildNextApp.mts:131:1) {\n  status: 1,\n  signal: null,\n  output: [ null, null, null ],\n  pid: 45856,\n  stdout: null,\n  stderr: null\n}\n```\n",
      "solution": "Fixed in https://github.com/lobehub/lobe-chat/commit/07dc919496ba0a69f6e433d51b49aacc02d1582a",
      "labels": [
        "Turbopack",
        "locked"
      ],
      "created_at": "2026-01-09T16:56:05Z",
      "closed_at": "2026-01-10T15:18:03Z",
      "url": "https://github.com/vercel/next.js/issues/88335",
      "comments_count": 3
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 88993,
      "title": "i Got a Bug in app\\layout.tsx",
      "problem": "### Link to the code that reproduces this issue\n\nSee more info here: https://nextjs.org/docs/messages/react-hydration-error     \n\n### To Reproduce\n\n1.  bunx create-next-app\n2.  bun run dev\n3. ctrl + click  \"Network:  http://192.168.56.1:3000\"\n4. **data-new-gr-c-s-check-loaded=\"14.1270.0\"\n-                           data-gr-ext-installed=\"\"**\n\n### Current vs. Expected behavior\n\ndata-new-gr-c-s-check-loaded=\"14.1270.0\"\n-                 data-gr-ext-installed=\"\"\n\n### Provide environment information\n\n```bash\nwindows \nhttp://192.168.56.1:3000/\n```\n\n### Which area(s) are affected? (Select all that apply)\n\ncreate-next-app\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext dev (local)\n\n### Additional context\n\n_No response_",
      "solution": "We could not detect a valid reproduction link. **Make sure to follow the bug report template carefully.**\n\n### Why was this issue closed?\n\nTo be able to investigate, we need access to a reproduction to identify what triggered the issue. We need a link to a **public** GitHub repository ([template for App Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template), [template for Pages Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template-pages)), but you can also use these templates: [CodeSandbox: App Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template) or [CodeSandbox: Pages Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template-pages).\n\nThe bug template that you filled out has a section called \"Link to the code that reproduces this issue\", which is where you should provide the link to the reproduction.\n\n- If you did not provide a link or the link you provided is not valid, we will close the issue.\n- If you provide a link to a private repository, we will close the issue.\n- If you provide a link to a repository but not in the correct section, we will close the issue.\n\n### What should I do?\n\nDepending on the reason the issue was closed, you can do the following:\n\n- If you did not provide a link, please open a new issue with a link to a reproduction.\n- If you provided a link to a private repository, please open a new issue with a link to a public repository.\n- If you provided a link to a repository but not in the correct section, please open a new issue with a link to a reproduction in the correct section.\n\n**In general, assume that we should not go through a lengthy onboarding process at your company code only to be able to verify an issue.**\n\n### My repository is private and cannot make it public\n\nIn most cases, a private repo will not be a sufficient **minimal reproduction**, as this codebase might contain a lot of unrelated parts that would make our investigation take longer. Please do **not** make it public. Instead, create a new repository using the templates above, adding the relevant code to reproduce the issue. Common things to look out for:\n\n- Remove any code that is not related to the issue. (pages, API routes, components, etc.)\n- Remove any dependencies that are not related to the issue.\n- Remove any third-party service that would require us to sign up for an account to reproduce the issue.\n- Remove any environment variables that are not related to the issue.\n- Remove private packages that we do not have access to.\n- If the issue is not related to a monorepo specifically, try to reproduce the issue without a complex monorepo setup\n\n### I did not open this issue, but it is relevant to me, what can I do to help?\n\nAnyone experiencing the same issue is welcome to provide a minimal reproduction following the above steps by opening a new issue.\n\n### I think my reproduction is good enough, why aren't you looking into it quickly?\n\nWe look into every Next.js issue and constantly monitor open issues for new comments.\n\nHowever, sometimes we might miss one or two due to the popularity/high traffic of the repository. We apologize, and kindly ask you to refrain from tagging core maintainers, as that will usually not result in increased priority.\n\nUpvoting issues to show your interest will help us prioritize and address them as quickly as possible. That said, every issue is important to us, and if an issue gets closed by accident, we encourage you to open a new one linking to the old issue and we will look into it.\n\n### Useful Resources\n\n- [How to Contribute to Open Source (Next.js)](https://www.youtube.com/watch?v=cuoNzXFLitc)\n- [How to create a Minimal, Complete, and Verifiable example](https://stackoverflow.com/help/mcve)\n- [Reporting a Next.js bug](https://github.com/vercel/next.js/blob/canary/.github/ISSUE_TEMPLATE/1.bug_report.yml)\n- [Next.js Triaging issues](https://github.com/vercel/next.js/blob/canary/contributing/repository/triaging.md)\n",
      "labels": [
        "create-next-app",
        "invalid link"
      ],
      "created_at": "2026-01-24T18:39:34Z",
      "closed_at": "2026-01-24T18:39:49Z",
      "url": "https://github.com/vercel/next.js/issues/88993",
      "comments_count": 1
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 88234,
      "title": "Next.js 16.1.1: DevTools causes infinite console error loop",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/hooperits/nextjs-16-devtools-loop-bug\n\n### To Reproduce\n\n1. Clone the reproduction repository: `git clone https://github.com/hooperits/nextjs-16-devtools-loop-bug`\n2. Run `npm install`\n3. Run `npm run dev`\n4. Open http://localhost:3000 in browser\n5. Open browser DevTools console\n6. Observe infinite error loop repeating ~3x per second:\n\n```\nTypeError: _interop_require_wildcard._ is not a function\n    at eval (webpack-internal:///(app-pages-browser)/./node_modules/next/dist/next-devtools/...)\n[INFO] Download the React DevTools...\n[LOG] [HMR] connected\n```\n\n### Current vs. Expected behavior\n\n**Current behavior:**\n- DevTools internal errors cause an infinite console error loop\n- The loop happens even on pages WITHOUT application errors\n- Console becomes unusable due to spam (~3 errors per second)\n- Significant performance overhead from infinite logging\n- The error originates from `next-devtools/userspace/app/errors/intercept-console-error.js`\n\n**Expected behavior:**\n- Console errors should only be logged once\n- DevTools internal errors should not leak to user console\n- No infinite loops in the console\n\n### Provide environment information\n\n```bash\nOperating System:\n  Platform: linux\n  Arch: x64\n  Version: Linux 6.6.87.2-microsoft-standard-WSL2\n\nBinaries:\n  Node: 20.x\n  npm: 10.x\n\nRelevant Packages:\n  next: 16.1.1\n  react: 19.1.0\n  react-dom: 19.1.0\n  typescript: 5.9.3\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nRuntime, Error Overlay\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext dev (local)\n\n### Additional context\n\n### Things that DON'T fix it:\n- `devIndicators: false` in next.config.mjs (only hides visual indicator)\n- `NEXT_DEVTOOLS_DISABLED=true` env var (not recognized)\n\n### Related closed issues:\n- #74321 - Same issue in 15.1.x, closed as invalid (missing repro)\n- #74817 - Similar issue, closed as NOT_PLANNED (couldn't reproduce)\n\nBoth were closed without resolution. This issue persists in Next.js 16.1.1.\n\n### Stack trace origin:\nThe error originates in Next.js DevTools internal code:\n```\nTypeError: _interop_require_wildcard._ is not a function\n    at eval (webpack-internal:///(app-pages-browser)/./node_modules/next/dist/next-devtools/userspace/app/errors/intercept-console-error.js)\n```\n\n### Impact:\n- Console becomes unusable for debugging\n- Significant performance overhead from infinite logging",
      "solution": "I've submitted a fix for this issue in PR #88241.\n\n**Root Cause**: When `handleConsoleError()` or `forwardErrorLog()` in the patched `console.error` throws an error, it triggers another `console.error` call, creating infinite recursion.\n\n**Fix**: Added a re-entrancy guard to `intercept-console-error.ts` that prevents recursive calls to the patched `console.error` function.\n\nThe fix is minimal (~15 lines) and includes e2e tests to prevent regression.\n\n---\n\nI've added tests for this report in [88418](https://github.com/vercel/next.js/pull/88418) but I'm unable to get them failing. It's hard to tell what may cause the error to show multiple times based on the report, but the opened PR by OP is not the solution.\n\nGoing to close this issue as the provided reproduction does not reproduce the issue and the added tests pass.",
      "labels": [
        "Runtime",
        "Error Overlay"
      ],
      "created_at": "2026-01-07T16:44:58Z",
      "closed_at": "2026-01-24T14:39:55Z",
      "url": "https://github.com/vercel/next.js/issues/88234",
      "comments_count": 4
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 67006,
      "title": "Calling router.refresh() reset scroll to top for previous page",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/markoradojcic/scroll-bug\n\n### To Reproduce\n\n1. Scroll list and click one of items\r\n2. Click on button that calls router.refresh()\r\n3. Go back to previous page\r\n\r\n\r\nhttps://github.com/vercel/next.js/assets/16863336/e6618bfd-b180-44a6-806e-f6c48ea164e2\r\n\r\n\n\n### Current vs. Expected behavior\n\nCurrent behavior: scroll position resets to top\r\nExpected behavior: preserve scroll position on previous page\r\n\n\n### Provide environment information\n\n```bash\nOperating System:\r\n  Platform: darwin\r\n  Arch: arm64\r\n  Version: Darwin Kernel Version 23.4.0: Wed Feb 21 21:44:43 PST 2024; root:xnu-10063.101.15~2/RELEASE_ARM64_T6000\r\n  Available memory (MB): 16384\r\n  Available CPU cores: 8\r\nBinaries:\r\n  Node: 21.5.0\r\n  npm: 10.2.4\r\n  Yarn: N/A\r\n  pnpm: N/A\r\nRelevant Packages:\r\n  next: 14.2.4 // Latest available version is detected (14.2.4).\r\n  eslint-config-next: 14.2.4\r\n  react: 18.3.1\r\n  react-dom: 18.3.1\r\n  typescript: 5.4.5\r\nNext.js Config:\r\n  output: N/A\n```\n\n\n### Which area(s) are affected? (Select all that apply)\n\nNavigation\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext dev (local), next build (local), next start (local)\n\n### Additional context\n\n_No response_",
      "solution": "also, if there is a scroll on the page with refresh button, then navigation back will result in undesire scroll on that page. \r\n@markoradojcic can you please add this code `<main className=\"h-[2000px] flex flex-col items-center justify-center\">` to test/page.tsx to see the problem",
      "labels": [
        "bug",
        "Linking and Navigating",
        "locked",
        "stale"
      ],
      "created_at": "2024-06-19T03:41:56Z",
      "closed_at": "2026-01-09T23:43:39Z",
      "url": "https://github.com/vercel/next.js/issues/67006",
      "comments_count": 5
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 88941,
      "title": "Revert PR #88496 - [prebuilt-skew-protection] feat: adding in automatic deploymentId",
      "problem": "## Issue\n\nPR #88496 needs to be reverted as it introduced breaking changes:\n\n### Problems\n\n1. **Broke `NEXT_DEPLOYMENT_ID=123 next build`**\n   - Skew protection is no longer enabled via the environment variable \n   - Users now need to also add `deploymentId: process.env.NEXT_DEPLOYMENT_ID` to config\n   - This was previously automatic\n\n2. **v16.2.0-canary.4 deployments are completely broken**\n   - Deployments 404 on documents\n   - Compare: [v16.2.0-canary.3 (works)](https://vercel.com/uncurated-tests/turbopack-testing/GKPWwEv3ronw6DhsoTVmvDZGAocQ) vs [v16.2.0-canary.4 (broken)](https://vercel.com/uncurated-tests/turbopack-testing/8rsE9CSX3eEs1fERTUFRTNuWCEKc)\n\n3. **Vercel's build container mechanism is broken**\n   - Setting `NEXT_DEPLOYMENT_ID=123` in the build container no longer works automatically\n   - This is how 'enable skew protection' on Vercel works\n\n### How to revert\n\n```bash\ngit checkout canary\ngit pull origin canary  \ngit revert -m 1 7282ce6759ea2373b2060e0720abe8983e1a87ae\ngit push origin canary\n```\n\nOr use GitHub's 'Revert' button on PR #88496.\n\n### References\n- Original PR: #88496\n- Merge commit: 7282ce6759ea2373b2060e0720abe8983e1a87ae",
      "solution": "We could not detect a valid reproduction link. **Make sure to follow the bug report template carefully.**\n\n### Why was this issue closed?\n\nTo be able to investigate, we need access to a reproduction to identify what triggered the issue. We need a link to a **public** GitHub repository ([template for App Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template), [template for Pages Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template-pages)), but you can also use these templates: [CodeSandbox: App Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template) or [CodeSandbox: Pages Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template-pages).\n\nThe bug template that you filled out has a section called \"Link to the code that reproduces this issue\", which is where you should provide the link to the reproduction.\n\n- If you did not provide a link or the link you provided is not valid, we will close the issue.\n- If you provide a link to a private repository, we will close the issue.\n- If you provide a link to a repository but not in the correct section, we will close the issue.\n\n### What should I do?\n\nDepending on the reason the issue was closed, you can do the following:\n\n- If you did not provide a link, please open a new issue with a link to a reproduction.\n- If you provided a link to a private repository, please open a new issue with a link to a public repository.\n- If you provided a link to a repository but not in the correct section, please open a new issue with a link to a reproduction in the correct section.\n\n**In general, assume that we should not go through a lengthy onboarding process at your company code only to be able to verify an issue.**\n\n### My repository is private and cannot make it public\n\nIn most cases, a private repo will not be a sufficient **minimal reproduction**, as this codebase might contain a lot of unrelated parts that would make our investigation take longer. Please do **not** make it public. Instead, create a new repository using the templates above, adding the relevant code to reproduce the issue. Common things to look out for:\n\n- Remove any code that is not related to the issue. (pages, API routes, components, etc.)\n- Remove any dependencies that are not related to the issue.\n- Remove any third-party service that would require us to sign up for an account to reproduce the issue.\n- Remove any environment variables that are not related to the issue.\n- Remove private packages that we do not have access to.\n- If the issue is not related to a monorepo specifically, try to reproduce the issue without a complex monorepo setup\n\n### I did not open this issue, but it is relevant to me, what can I do to help?\n\nAnyone experiencing the same issue is welcome to provide a minimal reproduction following the above steps by opening a new issue.\n\n### I think my reproduction is good enough, why aren't you looking into it quickly?\n\nWe look into every Next.js issue and constantly monitor open issues for new comments.\n\nHowever, sometimes we might miss one or two due to the popularity/high traffic of the repository. We apologize, and kindly ask you to refrain from tagging core maintainers, as that will usually not result in increased priority.\n\nUpvoting issues to show your interest will help us prioritize and address them as quickly as possible. That said, every issue is important to us, and if an issue gets closed by accident, we encourage you to open a new one linking to the old issue and we will look into it.\n\n### Useful Resources\n\n- [How to Contribute to Open Source (Next.js)](https://www.youtube.com/watch?v=cuoNzXFLitc)\n- [How to create a Minimal, Complete, and Verifiable example](https://stackoverflow.com/help/mcve)\n- [Reporting a Next.js bug](https://github.com/vercel/next.js/blob/canary/.github/ISSUE_TEMPLATE/1.bug_report.yml)\n- [Next.js Triaging issues](https://github.com/vercel/next.js/blob/canary/contributing/repository/triaging.md)\n",
      "labels": [
        "invalid link"
      ],
      "created_at": "2026-01-23T10:32:20Z",
      "closed_at": "2026-01-23T10:32:34Z",
      "url": "https://github.com/vercel/next.js/issues/88941",
      "comments_count": 1
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 87692,
      "title": "[TurbopackInternalError] Build fails on Windows with Prisma symlink error (os error 1314)",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/Shubhamkahar196/muzi\n\n### To Reproduce\n\n1. Use Windows 11\n2. Create a Next.js app\n3. Enable Turbopack\n4. Install Prisma (`@prisma/client`)\n5. Run `npm run dev` or `npm run build`\n\n\n### Current vs. Expected behavior\n\nThe application should start and build successfully without crashing.\n\nTurbopack crashes and writes a panic log.\n\nBuild error:\ncreate symlink to ../../../node_modules/@prisma/client\nCaused by: A required privilege is not held by the client (os error 1314)\n\n\n### Provide environment information\n\n```bash\nOS: Windows 11 Pro (x64)\nNode.js: v22.x\nNext.js: 16.1.0\nReact: 19.x\nPackage Manager: npm\nBundler: Turbopack\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nTurbopack, Runtime\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext dev (local), next build (local)\n\n### Additional context\n\n- Running terminal as Administrator sometimes avoids the issue\n- Disabling Turbopack resolves the problem\n- Panic logs are written to AppData\\Local\\Temp\\next-panic-*.log\n- This appears related to Windows symlink permission handling\n",
      "solution": "Should be fixed in next@16.1.1 - https://github.com/vercel/next.js/pull/87606 - can you confirm?\n\n---\n\nHi @icyJoseph , It worked. Next 16.1.1 solve the problem. Thanks",
      "labels": [
        "Runtime",
        "Turbopack",
        "locked"
      ],
      "created_at": "2025-12-23T04:48:56Z",
      "closed_at": "2026-01-08T12:16:14Z",
      "url": "https://github.com/vercel/next.js/issues/87692",
      "comments_count": 3
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 88233,
      "title": "Why is the router cache duration for statically rendered pages in Next 14 is 30 seconds, instead of the 5 minutes stated in the documentation?",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/lizuncong/next14-app\n\n### To Reproduce\n\nrepo: https://github.com/lizuncong/next14-app\ndemo: https://next14-app-nu.vercel.app/static-render3\nNext Version: 14.2.35\n\n\n\n### Current vs. Expected behavior\n\nAs you know, https://next14-app-nu.vercel.app/static-render3 is a statically rendered page.\n\n<img width=\"1168\" height=\"656\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/40fb903b-605b-4906-bc87-0c1c9adcd0c7\" />\nI navigate back and forth between two statically rendered pages, **static-render2** and **static-render3**. Within 30 seconds, the Router Cache is active, and the client doesn't send requests to the server. After 30 seconds, the Router Cache expires, and the client sends requests to the server.\n\n<img width=\"2776\" height=\"1498\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/7271717e-2366-4bff-8eb2-4ef0f30f16ed\" />\n\n\nTherefore, it appears that the Router Cache Duration for statically rendered pages is only 30 seconds, instead of the 5 minutes stated in the documentation.\n\n<img width=\"2864\" height=\"1346\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/d506a286-c55a-4c71-89f1-1b1f86b7ac48\" />\n\n### Provide environment information\n\n```bash\n\"next\": \"14.2.35\",\n\"react\": \"^18\",\n\"react-dom\": \"^18\",\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nUse Cache\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext start (local)\n\n### Additional context\n\n_No response_",
      "solution": "Hi, thank you for the report.\n\nThis is appears to be a bug on v14 Next that has been fixed in later versions of the framework. I was not able to reproduce the bug after upgrading to v15.\n\nv14 is no longer LTS per our [support policy](https://nextjs.org/support-policy), and regular features/bug fixes won't be backported to it. \n\nSince I cannot reproduce this in later versions of Next.js, I'm going to close this issue as fixed. ",
      "labels": [
        "locked"
      ],
      "created_at": "2026-01-07T16:37:14Z",
      "closed_at": "2026-01-08T14:31:47Z",
      "url": "https://github.com/vercel/next.js/issues/88233",
      "comments_count": 2
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 87811,
      "title": "failed to copy traced files",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/example/sorry\n\n### To Reproduce\n\nbuild \noutput standalone\n\n### Current vs. Expected behavior\n\nI found that the main problem is the name of the file has a \":\" symbol, Result fs copy  method  failed\n\nThe follow\n name is not correct in Windows\n\n\"D:\\a\\d\\dist\\server\\chunks\\[externals]_node:async_hooks_b485b2a4._.js'\"\n\nerror info\uff1a\n-\n\u26a0 Failed to copy traced files for D:\\a\\d\\dist\\server\\app\\api\\chat\\route.js Error: EINVAL: invalid argument, copyfile 'D:\\a\\d\\dist\\server\\chunks\\[externals]_node:async_hooks_b485b2a4._.js' -> 'D:\\a\\d\\dist\\standalone\\dist\\server\\chunks\\[externals]_node:async_hooks_b485b2a4._.js'\n    at ignore-listed frames {\n  errno: -4071,\n  code: 'EINVAL',\n  syscall: 'copyfile',\n  path: 'D:\\\\a\\\\d\\\\dist\\\\server\\\\chunks\\\\[externals]_node:async_hooks_b485b2a4._.js',\n  dest: 'D:\\\\a\\\\d\\\\dist\\\\standalone\\\\dist\\\\server\\\\chunks\\\\[externals]_node:async_hooks_b485b2a4._.js'  \n}\n\n### Provide environment information\n\n```bash\nwindows\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nOutput\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext build (local)\n\n### Additional context\n\n_No response_",
      "solution": "This is a known issue in Next.js 15+ (especially with Turbopack or certain externals handling), reported in GitHub discussions [#77721](https://github.com/vercel/next.js/discussions/77721) and [#86194](https://github.com/vercel/next.js/discussions/86194).\n\nPresent fix options:\n\n- Upgrade to latest Next.js canary (npm install next@canary)\n- There are many similar filename sanitization fixes landed post-15.0.\n- Disable Turbopack if using --turbo (next build without flag).\n- Avoid output: 'standalone' in next.config.js if possible (use Vercel or container without standalone).\n\n- Workaround: post-build script to rename/replace : \u2192 _ in .next/server/chunks/**/* filenames before copying.",
      "labels": [
        "Output",
        "Turbopack",
        "locked"
      ],
      "created_at": "2025-12-26T08:15:15Z",
      "closed_at": "2026-01-08T18:50:08Z",
      "url": "https://github.com/vercel/next.js/issues/87811",
      "comments_count": 4
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 86714,
      "title": "Turbopack can't use \"export * as Foo\" syntax, but webpack can",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/matowang/nextjs-turbopack-2025-12-02\n\n### To Reproduce\n\n1. Create module with `export * as Foo` syntax\n\n```ts\n// /src/app/module-test.ts\nexport * as ModuleTest from \"./module-test\";\n\nexport function moduleTest() {\n  return \"moduleTest\";\n}\n```\n\n2. import it\n```ts\n// /src/app/page.tsx\nimport { ModuleTest } from \"./module-test\";\nexport default function Home() {\n  console.log(ModuleTest.moduleTest());\n  return (\n  // ...\n  )\n}\n```\n3. Build it\n```sh\npnpm run build\n```\n\n\n### Current vs. Expected behavior\n\nExpected build to work, but the build fails.\n\n```\n\u276f pnpm run build                                                                   \n\n> nextjs-turbopack-2025-12-02@0.1.0 build /Users/matowang/Code/nextjs-turbopack-2025-12-02\n> next build\n\n\u25b2 Next.js 16.1.0-canary.10 (Turbopack)\n\n  Creating an optimized production build ...\n\u2713 Compiled successfully in 1913.0ms\n\u2713 Finished TypeScript in 1434.7ms    \n  Collecting page data using 7 workers  ..Error: Failed to collect configuration for /\n    at ignore-listed frames {\n  [cause]: Error: Module 53781 was instantiated because it was required from module 60168, but the module factory is not available.\n      at instantiateModule (.next/server/chunks/ssr/[turbopack]_runtime.js:711:15)\n      at getOrInstantiateModuleFromParent (.next/server/chunks/ssr/[turbopack]_runtime.js:742:12)\n      at Context.esmImport [as i] (.next/server/chunks/ssr/[turbopack]_runtime.js:228:20)\n      at module evaluation (.next/server/chunks/ssr/[root-of-the-server]__f237040c._.js:3:1124)\n      at instantiateModule (.next/server/chunks/ssr/[turbopack]_runtime.js:719:9)\n      at getOrInstantiateModuleFromParent (.next/server/chunks/ssr/[turbopack]_runtime.js:742:12)\n      at Context.esmImport [as i] (.next/server/chunks/ssr/[turbopack]_runtime.js:228:20)\n      at module evaluation (.next/server/chunks/ssr/_f2898698._.js:1:59)\n      at instantiateModule (.next/server/chunks/ssr/[turbopack]_runtime.js:719:9)\n      at getOrInstantiateModuleFromParent (.next/server/chunks/ssr/[turbopack]_runtime.js:742:12)\n}\n\n> Build error occurred\nError: Failed to collect page data for /\n    at ignore-listed frames {\n  type: 'Error'\n}\n\u2009ELIFECYCLE\u2009 Command failed with exit code 1.\n```\n\n### Provide environment information\n\n```bash\nOperating System:\n  Platform: darwin\n  Arch: arm64\n  Version: Darwin Kernel Version 24.1.0: Thu Oct 10 21:05:14 PDT 2024; root:xnu-11215.41.3~2/RELEASE_ARM64_T8103\n  Available memory (MB): 16384\n  Available CPU cores: 8\nBinaries:\n  Node: 22.11.0\n  npm: 10.9.0\n  Yarn: N/A\n  pnpm: 9.15.4\nRelevant Packages:\n  next: 16.1.0-canary.10 // Latest available version is detected (16.1.0-canary.10).\n  eslint-config-next: N/A\n  react: 19.2.0\n  react-dom: 19.2.0\n  typescript: 5.9.3\nNext.js Config:\n  output: N/A\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nTurbopack\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext build (local)\n\n### Additional context\n\nI ran `pnpm run build` both locally and on vercel. They both fail with the same logs.\n\nUsing `pnpm run build --webpack` works as expected.",
      "solution": "I think this is the same bug as https://github.com/vercel/next.js/issues/86132\nIf you compare the unit test I added in https://github.com/vercel/next.js/pull/86131\nStill not fixed though, unfortunately",
      "labels": [
        "Turbopack",
        "locked"
      ],
      "created_at": "2025-12-02T07:01:54Z",
      "closed_at": "2026-01-08T20:50:13Z",
      "url": "https://github.com/vercel/next.js/issues/86714",
      "comments_count": 2
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 86132,
      "title": "Turbopack production error: Module X was instantiated because it was required from module X, but the module factory is not available.",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/MisterJimson/turbo-bug-report\n\n### To Reproduce\n\n1. Run pnpm build && pnpm start\n2. Visit /\n\n### Current vs. Expected behavior\n\nCurrent: crash/500 with error::\n```\nError: Failed to collect configuration for /\n    at ignore-listed frames {\n  [cause]: Error: Module 28932 was instantiated because it was required from module 33290, but the module factory is not available.\n      at instantiateModule (.next/server/chunks/ssr/[turbopack]_runtime.js:707:15)\n      at getOrInstantiateModuleFromParent (.next/server/chunks/ssr/[turbopack]_runtime.js:738:12)\n      at Context.esmImport [as i] (.next/server/chunks/ssr/[turbopack]_runtime.js:228:20)\n      at module evaluation (.next/server/chunks/ssr/[root-of-the-server]__8023723c._.js:1:755)\n      at instantiateModule (.next/server/chunks/ssr/[turbopack]_runtime.js:715:9)\n      at getOrInstantiateModuleFromParent (.next/server/chunks/ssr/[turbopack]_runtime.js:738:12)\n      at Context.esmImport [as i] (.next/server/chunks/ssr/[turbopack]_runtime.js:228:20)\n      at module evaluation (.next/server/chunks/ssr/[root-of-the-server]__aa0032af._.js:1:268)\n      at instantiateModule (.next/server/chunks/ssr/[turbopack]_runtime.js:715:9)\n      at getOrInstantiateModuleFromParent (.next/server/chunks/ssr/[turbopack]_runtime.js:738:12)\n}\n```\n\nExpected: no crash, 200 http response, renders page\n\n### Provide environment information\n\n```bash\nOperating System:\n  Platform: darwin\n  Arch: arm64\n  Version: Darwin Kernel Version 24.6.0: Mon Jul 14 11:28:30 PDT 2025; root:xnu-11417.140.69~1/RELEASE_ARM64_T6030\n  Available memory (MB): 36864\n  Available CPU cores: 11\nBinaries:\n  Node: 22.15.0\n  npm: 10.9.2\n  Yarn: 1.22.22\n  pnpm: 9.15.9\nRelevant Packages:\n  next: 16.0.3 // Latest available version is detected (16.0.3).\n  eslint-config-next: N/A\n  react: 19.2.0\n  react-dom: 19.2.0\n  typescript: 5.9.3\nNext.js Config:\n  output: N/A\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nTurbopack\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext start (local)\n\n### Additional context\n\n- Works on Next 15\n- Commit that introduces issue: https://github.com/MisterJimson/turbo-bug-report/commit/6d3d05088efaff2928ef0a9d90edebb5b72fb8b0",
      "solution": "Hopefully this will be fixed at some point. It would be very helpful to have this done relatively quickly due to the possible widespread issues it could cause in the future. As it seems like it may be a next 16+ bug that more people could have.",
      "labels": [
        "Turbopack",
        "linear: turbopack",
        "locked"
      ],
      "created_at": "2025-11-14T15:36:30Z",
      "closed_at": "2026-01-08T20:50:13Z",
      "url": "https://github.com/vercel/next.js/issues/86132",
      "comments_count": 4
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 87950,
      "title": "Next 16 (forced nodejs proxy runtime) broke relative URL redirects deployed on vercel (with matching deployment URLs)",
      "problem": "### Link to the code that reproduces this issue\n\nNOTE: this is specific to deprecating the edge runtime for middleware and defaulting to nodejs\nbreaking in v16 https://github.com/mlstubblefield/nextjs-proxy-redirect-absolute\nbreaking in v15 https://github.com/mlstubblefield/nextjs-proxy-redirect-absolute/tree/v15-node-break-maybe\n\n### To Reproduce\n\n1. Browse to https://nextjs-proxy-redirect-absolute.vercel.app/redirect\n2. Note that the location header in the respones is absolute\nIn past versions of next, this would have produced a relative URL\n\nSee the following code in resolve-routes.ts\n```\nconst rel = getRelativeURL(value, initUrl)\nresHeaders['location'] = rel\n```\n\nThe following code in adapter.ts is also of note and only runs when `isEdgeRendering` is false (IE: nodejs runtime for middleware).\n```\nif (!process.env.__NEXT_NO_MIDDLEWARE_URL_NORMALIZE) {\n    if (redirectURL.host === requestURL.host) {\n        console.log('doing it')\n        redirectURL.buildId = buildId || redirectURL.buildId;\n        response.headers.set('Location', redirectURL.toString()); // rewrites the absolute URL\n    }\n}\n```\n\n### Current vs. Expected behavior\n\nWhen deployed on vercel\nThe current behavior is producing an absolute URL\nIn the past a relative URL (in this case to \"/\") would have been produced\n\n\n### Provide environment information\n\n```bash\nOperating System:\n  Platform: win32\n  Arch: x64\n  Version: Windows 11 Pro\n  Available memory (MB): 32094\n  Available CPU cores: 16\nBinaries:\n  Node: 24.11.1\n  npm: 11.6.2\n  Yarn: 1.22.22\n  pnpm: 9.11.0\nRelevant Packages:\n  next: 16.1.1 // Latest available version is detected (16.1.1).\n  eslint-config-next: N/A\n  react: 19.2.3\n  react-dom: 19.2.3\n  typescript: 5.9.3\nNext.js Config:\n  output: N/A\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nRedirects, Middleware\n\n### Which stage(s) are affected? (Select all that apply)\n\nVercel (Deployed)\n\n### Additional context\n\nThis **only** happens on vercel (I can't reproduce it locally). Will try to pin down a canary.\n\nI was able to narrow it down to v16.0.0-canary.18 having introduced the problem. The issue does not exist in v16.0.0-canary.17.\nAs a bit of extra flavor, this is specifically b/c of the deprecation of the edge runtime. \nI was able to get this going on next 15 by using the nodejs runtime in middleare here https://github.com/mlstubblefield/nextjs-proxy-redirect-absolute/tree/v15-node-break-maybe",
      "solution": "Note I don't see that this commit https://github.com/luxinlabs/next.js/commit/291371c57f50f118f1f3371e4954a7300d69ea4d addressees the problem. I'm assuming it got mislabeled.",
      "labels": [
        "Middleware",
        "locked",
        "Redirects"
      ],
      "created_at": "2025-12-30T20:05:47Z",
      "closed_at": "2026-01-08T02:28:30Z",
      "url": "https://github.com/vercel/next.js/issues/87950",
      "comments_count": 2
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 88879,
      "title": "Turbopack Build Error Could not parse module. [project]/node_modules/next/dist/server/route-modules/app-route/vendored/contexts/router-context.js",
      "problem": "I am using nx monorepo to setup 3 nextjs apps. my development server build (`nx run app:build:development`) with **turbopack** working fine but `nx run app:build:production`  giving error when collecting data on prerender. `Collecting page data using 11 workers  ...Error: Could not parse module '[project]/node_modules/next/dist/server/route-modules/app-route/vendored/contexts/router-context.js', file not found`\n\n**Environment**\n\nOperating System:\n  Platform: darwin\n  Arch: arm64\n  Version: Darwin Kernel Version 25.2.0: Tue Nov 18 21:09:56 PST 2025; root:xnu-12377.61.12~1/RELEASE_ARM64_T6041\n  Available memory (MB): 24576\n  Available CPU cores: 12\nBinaries:\n  Node: 25.3.0\n  npm: 11.7.0\n  Yarn: 1.22.22\n  pnpm: N/A\nRelevant Packages:\n  next: 16.1.4 // Latest available version is detected (16.1.4).\n  eslint-config-next: 15.5.9\n  react: 19.2.3\n  react-dom: 19.2.3\n  typescript: 5.9.3\nNext.js Config:\n  output: N/A\n\n**NX Report**\nNode           : 25.3.0\nOS             : darwin-arm64\nNative Target  : aarch64-macos\nyarn           : 1.22.22\n\nnx (global)            : 22.3.3\nnx                     : 22.3.3\n@nx/js                 : 22.3.3\n@nx/eslint             : 22.3.3\n@nx/workspace          : 22.3.3\n@nx/jest               : 22.3.3\n@nx/cypress            : 22.3.3\n@nx/devkit             : 22.3.3\n@nx/eslint-plugin      : 22.3.3\n@nx/module-federation  : 22.3.3\n@nx/next               : 22.3.3\n@nx/react              : 22.3.3\n@nx/rollup             : 22.3.3\n@nx/storybook          : 22.3.3\n@nx/vite               : 22.3.3\n@nx/vitest             : 22.3.3\n@nx/web                : 22.3.3\n@nx/webpack            : 22.3.3\ntypescript             : 5.9.3\n---------------------------------------\nCommunity plugins:\n@webpro/nx-tsc : 0.0.2\n---------------------------------------\n\nPlease help me to resolve this issue.",
      "solution": "We could not detect a valid reproduction link. **Make sure to follow the bug report template carefully.**\n\n### Why was this issue closed?\n\nTo be able to investigate, we need access to a reproduction to identify what triggered the issue. We need a link to a **public** GitHub repository ([template for App Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template), [template for Pages Router](https://github.com/vercel/next.js/tree/canary/examples/reproduction-template-pages)), but you can also use these templates: [CodeSandbox: App Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template) or [CodeSandbox: Pages Router](https://codesandbox.io/s/github/vercel/next.js/tree/canary/examples/reproduction-template-pages).\n\nThe bug template that you filled out has a section called \"Link to the code that reproduces this issue\", which is where you should provide the link to the reproduction.\n\n- If you did not provide a link or the link you provided is not valid, we will close the issue.\n- If you provide a link to a private repository, we will close the issue.\n- If you provide a link to a repository but not in the correct section, we will close the issue.\n\n### What should I do?\n\nDepending on the reason the issue was closed, you can do the following:\n\n- If you did not provide a link, please open a new issue with a link to a reproduction.\n- If you provided a link to a private repository, please open a new issue with a link to a public repository.\n- If you provided a link to a repository but not in the correct section, please open a new issue with a link to a reproduction in the correct section.\n\n**In general, assume that we should not go through a lengthy onboarding process at your company code only to be able to verify an issue.**\n\n### My repository is private and cannot make it public\n\nIn most cases, a private repo will not be a sufficient **minimal reproduction**, as this codebase might contain a lot of unrelated parts that would make our investigation take longer. Please do **not** make it public. Instead, create a new repository using the templates above, adding the relevant code to reproduce the issue. Common things to look out for:\n\n- Remove any code that is not related to the issue. (pages, API routes, components, etc.)\n- Remove any dependencies that are not related to the issue.\n- Remove any third-party service that would require us to sign up for an account to reproduce the issue.\n- Remove any environment variables that are not related to the issue.\n- Remove private packages that we do not have access to.\n- If the issue is not related to a monorepo specifically, try to reproduce the issue without a complex monorepo setup\n\n### I did not open this issue, but it is relevant to me, what can I do to help?\n\nAnyone experiencing the same issue is welcome to provide a minimal reproduction following the above steps by opening a new issue.\n\n### I think my reproduction is good enough, why aren't you looking into it quickly?\n\nWe look into every Next.js issue and constantly monitor open issues for new comments.\n\nHowever, sometimes we might miss one or two due to the popularity/high traffic of the repository. We apologize, and kindly ask you to refrain from tagging core maintainers, as that will usually not result in increased priority.\n\nUpvoting issues to show your interest will help us prioritize and address them as quickly as possible. That said, every issue is important to us, and if an issue gets closed by accident, we encourage you to open a new one linking to the old issue and we will look into it.\n\n### Useful Resources\n\n- [How to Contribute to Open Source (Next.js)](https://www.youtube.com/watch?v=cuoNzXFLitc)\n- [How to create a Minimal, Complete, and Verifiable example](https://stackoverflow.com/help/mcve)\n- [Reporting a Next.js bug](https://github.com/vercel/next.js/blob/canary/.github/ISSUE_TEMPLATE/1.bug_report.yml)\n- [Next.js Triaging issues](https://github.com/vercel/next.js/blob/canary/contributing/repository/triaging.md)\n",
      "labels": [
        "invalid link"
      ],
      "created_at": "2026-01-22T06:28:22Z",
      "closed_at": "2026-01-22T06:28:39Z",
      "url": "https://github.com/vercel/next.js/issues/88879",
      "comments_count": 1
    },
    {
      "tech": "nodejs",
      "repo": "vercel/next.js",
      "issue_number": 87896,
      "title": "Minified function names in turbopack prod builds for server-side code",
      "problem": "### Link to the code that reproduces this issue\n\nhttps://github.com/chargome/repro.turbopack-prod-server-sourcemaps\n\n### To Reproduce\n\nReproduction steps are in the readme file of the repro.\n\n### Current vs. Expected behavior\n\nWhen building a Next.js app using next build, the [names](https://tc39.es/ecma426/#json-names) field in the source map files for server-side code is always empty which results in mappings not being able to reference an index from minified filename to original filename. This is only the case for server-side sourcemaps, not for client-side.\n\nIf this information is omitted due to performance reasons I would expect the next config to provide some option to opt-out of this behaviour.\n\n\n\n### Provide environment information\n\n```bash\nOperating System:\n  Platform: darwin\n  Arch: arm64\n  Version: Darwin Kernel Version 24.5.0: Tue Apr 22 19:54:49 PDT 2025; root:xnu-11417.121.6~2/RELEASE_ARM64_T6000\n  Available memory (MB): 32768\n  Available CPU cores: 10\nBinaries:\n  Node: 20.14.0\n  npm: 10.7.0\n  Yarn: 1.22.22\n  pnpm: 10.19.0\nRelevant Packages:\n  next: 16.1.1 // Latest available version is detected (16.1.1).\n  eslint-config-next: N/A\n  react: 19.2.3\n  react-dom: 19.2.3\n  typescript: 5.9.3\nNext.js Config:\n  output: N/A\n```\n\n### Which area(s) are affected? (Select all that apply)\n\nTurbopack\n\n### Which stage(s) are affected? (Select all that apply)\n\nnext build (local), Vercel (Deployed), Other (Deployed)\n\n### Additional context\n\n_No response_",
      "solution": "@lukesandberg For our sourcemaps resolution this should be sufficient!",
      "labels": [
        "Turbopack",
        "locked"
      ],
      "created_at": "2025-12-29T12:50:06Z",
      "closed_at": "2026-01-07T15:58:04Z",
      "url": "https://github.com/vercel/next.js/issues/87896",
      "comments_count": 3
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 7000,
      "title": "dependency on @types/express-serve-static-core induced incompatibility",
      "problem": "## Environment information\n\n**Version**:\nexpress 5.2.1\n@types/express 5.0.6\n\n**Platform**:\nwindows 11 x64 and ubuntu 24.04.3\n\n**Node.js version**:\n24.13.0\n\n**Any other relevant information**:\n\n## What steps will reproduce the bug?\n\n\ncurrently express typing have dependency\n\"@types/express-serve-static-core\": \"^5.0.0\",\n\nwhich installs @types/express-serve-static-core 5.1.1, which is incompatible\nworks up to 5.1.0\n\nso massive same errors like this\n```\nerror TS2345: Argument of type 'string | string[]' is not assignable to parameter of type 'string'.\n  Type 'string[]' is not assignable to type 'string'.\n\n80         const id = parseInt(req.params.id);\n```\ngenerated\n\nand this makes the documentation on \nhttps://expressjs.com/en/5x/api.html\nuseless, in that documentation, same \"req.params.id\" is used\n\n@types/express-serve-static-core should be fixed",
      "solution": "The TypeScript type definitions for Express are maintained as a part of the [DefinitelyTyped](https://github.com/DefinitelyTyped/DefinitelyTyped/) project and generally I'd suggest asking there about problems with `@types/` packages.\n\nThe reason why your code broke is my recent PR https://github.com/DefinitelyTyped/DefinitelyTyped/pull/74161 that fixed the `req.params` type. Your problems are not caused by a package version mismatch. In Express 5, if you use a path with a wildcard, the parameter that you get is an array. This is documented in the [migration guide](https://expressjs.com/en/guide/migrating-5.html#req.params) and [`req.params` docs](https://expressjs.com/en/5x/api.html#req.params).\n\nIf you use a specific path with parameters, the typings will try to parse it and set the correct `req.params` type - e.g.:\n\n```ts\napp.get(\"/user/:id\", (req, res) => {\n    req.params.id // string\n}\n\napp.get(\"/file/*path\", (req, res) => {\n    req.params.path // string[]\n}\n```\n\nIf you don't give any path, then the default type - `string | string[]`, which covers all possible types, will be used:\n\n```ts\nconst handler: RequestHandler = (req, res) => {\n    req.params.id // string | string[]\n}\n```\n\nYou can also declare the type of `req.params` yourself:\n\n```ts\nconst handler1: RequestHandler<{ id: string }> = (req, res) => {\n    req.params.id // string\n}\n\nconst handler2 = (req: Request<{ path: string[] }>, res: Response) => {\n    req.params.path // string[]\n}\n```",
      "labels": [
        "question",
        "typescript"
      ],
      "created_at": "2026-01-20T23:33:25Z",
      "closed_at": "2026-01-21T00:14:48Z",
      "url": "https://github.com/expressjs/express/issues/7000",
      "comments_count": 1
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 6917,
      "title": "[Documentation] Add async/await error handling best practices guide",
      "problem": "## Description\nMany developers struggle with properly handling errors in async/await Express routes. A dedicated section in the documentation covering this pattern would be very helpful.\n\n## Current Situation\nThe error handling documentation primarily covers synchronous patterns. With async/await being the standard for modern Node.js applications, explicit guidance would help prevent unhandled promise rejections.\n\n## Suggested Documentation Addition\nA section covering:\n\n```javascript\n// Wrapper function for async routes\nconst asyncHandler = (fn) => (req, res, next) => {\n  Promise.resolve(fn(req, res, next)).catch(next);\n};\n\n// Usage example\napp.get('/users/:id', asyncHandler(async (req, res) => {\n  const user = await User.findById(req.params.id);\n  res.json(user);\n}));\n```\n\n## Benefits\n1. Reduces common errors for Express beginners\n2. Promotes proper error handling patterns\n3. Prevents unhandled promise rejections in production\n\nI'd be happy to contribute a PR for this documentation update if there's interest.",
      "solution": "Fixed in 2020 by https://github.com/expressjs/expressjs.com/pull/1134",
      "labels": [],
      "created_at": "2025-11-25T14:24:35Z",
      "closed_at": "2026-01-18T23:07:12Z",
      "url": "https://github.com/expressjs/express/issues/6917",
      "comments_count": 2
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 5449,
      "title": "feature: add a GitHub action to quell spam PRs",
      "problem": "## Problem\r\nI was scrolling through Twitter and someone posted about [the spam PR](https://github.com/expressjs/express/pulls?page=1&q=is%3Apr+is%3Aclosed+Readme.md). \r\n\r\n## Possible Solution\r\nImplement this [GitHub action](https://github.com/marketplace/actions/mark-as-spam). It'll lessen the workload for maintainers. \r\n\r\n",
      "solution": "> This is why I am hesitant do add an action for this as I would rather ask GitHub for better spam management tooling.\r\n\r\nAgree with @wesleytodd. The issue is quite complex because the moderation in GitHub is not an easy thing:\r\n- Many people (those that are watching the repo) will receive a notification in their email or in the app/web once a new PR is created. This will occur even before a Github Action is triggered. \r\n- Closing PRs is an easy action (1 click), so that is not a big time consumer\r\n- Detecting invalid PRs is not the big issue because you can easily to spot them with the practice.\r\n- Reporting issues/users is complex because the UI requires many steps to do it for each user, so that is a blocker for many maintainers.\r\n- Nuke button like @CBID2 suggested is great when the community needs to slow down due a discussion/flame in certain moments, but is not the long term solution as it will prevent other users from contributing (PRs) that are legit or need help (issues) while using Express.\r\n\r\nSo, I think that we are quite limited on how much we can do with GitHub actions in this case. There are other scenarios that are less frequent but more prone to use GitHub Actions to moderate, for example when people do comments that include offensive content or heavily language. In most projects that I am involved the moderation is done by the humans behind the project or a specific team that volunteer to do it, it is a heavy job. The same way as it is hard to keep a slack/discord/gitter community channel a safe space by moderating content.\n\n---\n\nI do agree that workflow is a bit more well suited IMO. I am still hesitant and I would like to also look at other ways but yeah in the mean time lets re-open the issue so we don't end up having multiple on the same topic or miss out on good ideas like @gaurishhs'.\n\n---\n\nAnd if word count check is added, they must find some other way to open spam prs. The problem here is those who make spam prs, they just started their development journey and when they got introduced to github (by some youtubers or some articles), they tried to test it by themselves. They didn't have any idea that the thing they are doing is a headace of someone else.\r\n\r\n**The only way we can stop this is by creating awarness.**\r\n",
      "labels": [
        "ideas"
      ],
      "created_at": "2024-02-06T17:35:20Z",
      "closed_at": "2026-01-17T03:10:48Z",
      "url": "https://github.com/expressjs/express/issues/5449",
      "comments_count": 30
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 6968,
      "title": "update to `qs@^6.14.1`",
      "problem": " \n- Failure issue:\n> - https://github.com/advisories/GHSA-6rw7-vpxm-498p \n- A few days ago, the following security flaw was discovered in lib qs. However, no issue or PR was found regarding this flaw or a possible solution.\n- Projects that use Express are vulnerable and without solutions for the flaw.\n- Please let us know when you can. Thank you!",
      "solution": "@remyoudemans \nThank you for sharing this solution with us. \nWe hope that express will make this corrected version os lib qs the default.\n\n\n---\n\nIn this case, the reported vulnerability in `qs` (CVE-2025-15284) is already addressed by default in Express. Current Express releases depend on patched versions of `qs`:\n\n- **Express 4** specifies `\"qs\": \"~6.14.0\"`, which resolves to `>=6.14.0 <6.15.0` and therefore pulls in the patched `6.14.1` release by default. Reference: https://github.com/expressjs/express/blob/f62378e1bc776259c0a471476c2dc043a02ac762/package.json#L56\n\n- **Express 5** specifies `\"qs\": \"^6.14.0\"`, which also pulls in the patched versions automatically. Reference: https://github.com/expressjs/express/blob/c2fb76e99f7379b706a5510b4b2dd08a1b710b7b/package.json#L55\n\nAs a result, users installing Express without overriding dependency resolutions already receive a version of `qs` that includes the fix, and the reported issue does not represent an unpatched vulnerability in Express itself.\n\n\n---\n\n> So far can't even convince him that it's breaking change :-/\n\nI personally agree with the maintainers point of view expressed here ([ref](https://github.com/ljharb/qs/issues/537#issuecomment-3711952472)).\n\n\nThis will be solved tomorrow as we plan to publish `body-parser@2.2.2` ([ref](https://github.com/expressjs/body-parser/pull/691)) with a specific compatibility patch with qs@6.14.1. So Express@5.x will work as expected out-of-the-box \ud83d\udc4d \n\n>Is there a recommended way to keep the default behavior of getting arrays instead of objects when posting arrays ?\n\nIn the meantime one _possible workaround_ is to use `overrides` in the `package.json` ([docs](https://docs.npmjs.com/cli/v9/configuring-npm/package-json#overrides)) like:\n\n```json\n{\n  \"overrides\": {\n    \"express\": {\n      \"qs\": \"6.14.0\"\n    },\n    \"body-parser\": {\n      \"qs\": \"6.14.0\"\n    },\n  }\n}\n``` \n\nBut then you will get the CVE alert (as you installing a known vulnerable package) \ud83e\udd14 ",
      "labels": [
        "5.x"
      ],
      "created_at": "2026-01-02T12:16:42Z",
      "closed_at": "2026-01-07T14:46:38Z",
      "url": "https://github.com/expressjs/express/issues/6968",
      "comments_count": 16
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 6966,
      "title": "Test failure: urlencoded parser returns {\"0\":\"val1\",\"1\":\"val2\"} instead of [\"val1\",\"val2\"] for duplicate keys",
      "problem": "## Environment information\n\n**Version**:\n\nexpress: 5.2.1\nbody-parser: ^2.2.1\n\n**Platform**:\n\nDarwin 25.1.0 (macOS)\n\n**Node.js version**:\n\n>= 18 (as specified in package.json engines)\n\n**Any other relevant information**:\n- Test framework: Mocha ^10.7.3\n- Testing library: Supertest ^6.3.0\n- This appears to be related to body-parser 2.x behavior changes\n\n## What steps will reproduce the bug?\n\n1. Run the test suite:\n\nnpm test\n\n2. Or run the specific failing test:\n\nnpm test -- --grep \"should parse multiple key instances\"\n\n## Expected behavior\n\nWhen parsing URL-encoded data with duplicate keys using express.urlencoded({ extended: false }), the parser should return an array:\n\n**Request:**\nPOST / HTTP/1.1\nContent-Type: application/x-www-form-urlencoded\n\nuser=Tobi&user=Loki\n\n**Expected response:**\n{\"user\":[\"Tobi\",\"Loki\"]}\n\n## Actual behavior\n\nThe parser returns an object with numeric keys instead of an array:\n\n**Actual response:**\n{\"user\":{\"0\":\"Tobi\",\"1\":\"Loki\"}}\n\n## Test failure output\n\n```\nexpress.urlencoded()\n       with extended option\n         when false\n           should parse multiple key instances:\n     Error: expected '{\"user\":[\"Tobi\",\"Loki\"]}' response body, got '{\"user\":{\"0\":\"Tobi\",\"1\":\"Loki\"}}'\n      at Context.<anonymous> (test/express.urlencoded.js:106:12)\n```\n\n## Affected test\n\nFile: test/express.urlencoded.js, lines 101-107\n\n```\nit('should parse multiple key instances', function (done) {\n  request(this.app)\n    .post('/')\n    .set('Content-Type', 'application/x-www-form-urlencoded')\n    .send('user=Tobi&user=Loki')\n    .expect(200, '{\"user\":[\"Tobi\",\"Loki\"]}', done)\n})\n```\n## Possible cause\n\nThis appears to be a breaking change in body-parser 2.x regarding how duplicate keys in URL-encoded data are handled when extended: false. The simple querystring parser may have changed its behavior to return objects with numeric keys instead of arrays.\n\n## Questions\n\n1. Is this the intended behavior for body-parser 2.x?\n2. Should the test expectation be updated to match the new behavior?\n3. Or is this a regression that needs to be fixed in body-parser?",
      "solution": "Thanks for the issue, I'm afraid this is more than tests failing. This is a `qs` behavior change surfacing through body parser ([more](https://github.com/expressjs/body-parser/issues/687))",
      "labels": [
        "bug"
      ],
      "created_at": "2026-01-02T08:52:41Z",
      "closed_at": "2026-01-05T08:10:29Z",
      "url": "https://github.com/expressjs/express/issues/6966",
      "comments_count": 2
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 6940,
      "title": "Dynamic remove route",
      "problem": "From all the documentation, I understand that Express can add routes dynamically, but it can't delete them. What kind of crazy nonsense is this? Using hacks isn't good programming practice. It's a library that's not fully developed.\nLet's say I'm building a CMS or API server with plugin support.\nRegistering plugins (routes, models, etc.) is no problem.\nUnloading a plugin\u2014that's where the problems begin.\nDisabling models and adding their status to the database is possible.\nBut the route remains. And that's where problems with the application logic begin. Lots of errors, etc.\nQuestion: When can I expect proper route deletion in Express, so that the application reports a missing URL and throws an error about it?",
      "solution": "You can do this using `Router` as explained by @wesleytodd most recently in https://github.com/expressjs/express/issues/5743#issuecomment-2237253303. There are even tests for that behaviour:\n\nhttps://github.com/expressjs/express/blob/dbac741a49a5a64336b70c06e85c2e2706e36336/test/app.router.js#L1159-L1208\n\nAlso it's a duplicate of:\n\n- https://github.com/expressjs/express/issues/5743 (solution in https://github.com/expressjs/express/issues/5743#issuecomment-2237253303)\n- https://github.com/expressjs/express/issues/4436 (solution in https://github.com/expressjs/express/issues/4436#issuecomment-713058120)\n- https://github.com/expressjs/express/issues/4094 (solution in https://github.com/expressjs/express/issues/4094#issuecomment-550066972)\n- https://github.com/expressjs/express/issues/3726 (solution in https://github.com/expressjs/express/issues/3726#issuecomment-415491953)\n- https://github.com/expressjs/express/issues/2418 (both comments suggest solution)\n\nAnd for very old Express versions (not really applicable here):\n- https://github.com/expressjs/express/issues/1849\n- https://github.com/expressjs/express/issues/1713\n- https://github.com/expressjs/express/issues/1227",
      "labels": [],
      "created_at": "2025-12-03T11:31:05Z",
      "closed_at": "2025-12-03T22:47:45Z",
      "url": "https://github.com/expressjs/express/issues/6940",
      "comments_count": 2
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 6849,
      "title": "Performance: compileTrust() has O(n^2) complexity causing 2+ second delays with large IP lists",
      "problem": "\nThe `compileTrust()` function in Express has severe performance issues when processing large IP lists, causing startup delays of 2+ seconds for applications with 1M+ trusted IPs.\n\n## Performance Impact\n\n| IP Count | Processing Time | Startup Delay |\n|----------|----------------|---------------|\n| 10,000   | 33ms           | 0.03s         |\n| 100,000  | 213ms          | 0.21s         |\n| 500,000  | 1,019ms        | 1.02s         |\n| 1,000,000| 2,365ms        | 2.37s         |\n\n## Real-World Scenarios Affected\n\n- **CDNs**: 100K+ IPs \u2192 213ms startup delay\n- **Large Enterprises**: 500K+ IPs \u2192 1+ second delay  \n- **Cloud Providers**: 1M+ IPs \u2192 2.3+ second delay\n\nThis impacts:\n- Application startup time\n- Server restart time\n- Container startup time\n- Cold starts in serverless environments\n\n\n## Memory Usage\n\nLarge IP lists also consume significant memory:\n\n| IP Count | String Size | Heap Increase | Ratio |\n|----------|-------------|---------------|-------|\n| 10,000   | 0.10 MB     | 3.24 MB       | 33.94x|\n| 100,000  | 0.95 MB     | 34.18 MB      | 35.84x|\n| 500,000  | 4.77 MB     | 149.93 MB     | 31.44x|\n\n\n## Related Issues\n\nThis performance issue was initially reported in #6611, but that issue was based on a misunderstanding of how `compileTrust()` works. The reporter expected it to return an array of IPs, but it actually returns a predicate function for checking if an IP is trusted.\n\nThe real issue is the performance bottleneck, not the API behavior.\n\n## Priority\n\n**High** - This affects production applications with large trust lists, causing significant startup delays and poor user experience.",
      "solution": "@mikeal @peters @nick @mcolyer \nI've identified and fixed a MAJOR PERFORMANCE ISSUE in `compileTrust()` that was causing 2+ second startup delays for applications with large IP lists.\n\nThe issue was :\nWhile investigating issue #6611 (which turned out to be an API misunderstanding), I discovered the real problem: `compileTrust()` has O(n^2) complexity when processing large IP lists, causing severe startup delays:\n\n- 100,000 IPs:  254ms startup delay\n- 1,000,000 IPs:  2,365ms startup delay\n\nThis affects CDNs, large enterprises, and cloud providers using extensive trust lists.\n\nThe fix I implemented :\nImplemented **Lazy compilation** for IP lists >1000 addresses:\n- Compile trust function only on first access (not during app initialization)\n- Cache compiled function for subsequent calls (19.5x speedup)\n\n## Performance Results\n| IP Count | Before | After | Improvement |\n|----------|--------|-------|-------------|\n| 10,000   | 46ms   | 2ms   | **23x faster** |\n| 100,000  | 254ms  | 15ms  | **17x faster** |\n| 1,000,000| 2,365ms| ~50ms | **47x faster** |\n\n## PR Details\n- **PR**: #6851 - Clean implementation with comprehensive tests\n- **Issue**: #6849 - Performance analysis and benchmarks\n- **Tests**: All 1245 existing tests pass + new performance tests\n\nThis fix will significantly improve startup times for production applications with large trust lists. Would appreciate your review when you have a chance! \ud83d\ude4f\n\n**Related**: Also clarified the original issue #6611 - it was an API\n\n---\n\nI haven't worked for any CDN/cloud provider, but I don't think 10000+ separate proxies makes sense in any scenario - that would be at least /19 subnet worth of individual IPs. If you need to mark so many IPs as proxies, those addresses most likely will be from one of a few subsets and **Express supports subnet notation** for [`\"trust proxy\"` setting](https://expressjs.com/en/5x/api.html#trust.proxy.options.table). Matching few large subnets will also be much faster than individual /32 or /128 subnets (aka individual IP addresses) that will have to be done at runtime.\n\n[`compileTrust`](https://github.com/expressjs/express/blob/64e7373d6976dd3ec32a92dbc31ff19700fe152a/lib/utils.js#L194-L214) only handles some most basic cases (custom function, `true`, number of hops), splits strings into array (if necessary) and hands over processing to the [`proxy-addr`](https://github.com/jshttp/proxy-addr) package. If you think there's a problem with its performance, please report it there.\n\nFinally, let me do some math based on your own results:\n\n| IP count | n change | Time | Time change | Expected change for O(n^2) | Expected change for O(n) |\n| --------- | --------- | ----- | --------- | ------------------------------- | ----------------------- |\n| 10,000 | - | 46ms | - | - | - |\n| 100,000 | x10 | 254ms | x5.5 | x100 | x10 |\n| 1,000,000 | x10 | 2,365ms | x9.3 | x100 | x10 |\n\n## Your own results show that the measured time complexity is linear!\n\nThe first x5.5 is due to some constant overhead.\n\n---\n\n> I haven't worked for any CDN/cloud provider, but I don't think 10000+ separate proxies makes sense in any scenario - that would be at least /19 subnet worth of individual IPs. If you need to mark so many IPs as proxies, those addresses most likely will be from one of a few subsets and **Express supports subnet notation** for [`\"trust proxy\"` setting](https://expressjs.com/en/5x/api.html#trust.proxy.options.table). Matching few large subnets will also be much faster than individual /32 or /128 subnets (aka individual IP addresses) that will have to be done at runtime.\n> \n> [`compileTrust`](https://github.com/expressjs/express/blob/64e7373d6976dd3ec32a92dbc31ff19700fe152a/lib/utils.js#L194-L214) only handles some most basic cases (custom function, `true`, number of hops), splits strings into array (if necessary) and hands over processing to the [`proxy-addr`](https://github.com/jshttp/proxy-addr) package. If you think there's a problem with its performance, please report it there.\n> \n> Finally, let me do some math based on your own results:\n> \n> IP count\tn change\tTime\tTime change\tExpected change for O(n^2)\tExpected change for O(n)\n> 10,000\t-\t46ms\t-\t-\t-\n> 100,000\tx10\t254ms\tx5.5\tx100\tx10\n> 1,000,000\tx10\t2,365ms\tx9.3\tx100\tx10\n> ## Your own results show that the measured time complexity is linear!\n> The first x5.5 is due to some constant overhead.\n\n\nplease see this kindly >\nhttps://github.com/expressjs/express/pull/6851#issuecomment-3442412856\n\nthanks",
      "labels": [],
      "created_at": "2025-10-21T20:17:03Z",
      "closed_at": "2025-11-13T17:53:10Z",
      "url": "https://github.com/expressjs/express/issues/6849",
      "comments_count": 4
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 6611,
      "title": "The compileTrust function exhibits issues when processing exceptionally long strings.",
      "problem": "## Environment information\n\n**Version**: express 5.0.0-beta.3\n\n**Platform**: Linux\n\n**Node.js version**: 18.x\n\n**Any other relevant information**: N/A\n\n## What steps will reproduce the bug?\n\n1. Create a very long string with many IP addresses separated by commas\n2. Call the `compileTrust` function with this string\n3. Call the returned function and check its length\n\n```js\nconst utils = require('./lib/utils');\n\n// Create a string with 1,000,001 IP addresses\nconst longString = Array(1000001).fill('127.0.0.1').join(',');\n\n// Call compileTrust with the long string\nconst result = utils.compileTrust(longString);\n\n// Check the length of the result when called\nconsole.log(result().length); // Expected: 1000001, Actual: undefined\n\n```\n\n## What is the expected behavior?\nThe function should correctly process the very long string, split it into an array of IP addresses, and return a function that, when called, returns an array with 1,000,001 elements (one for each IP address in the original string).\n\n## What happens instead?\nWhen calling the result function and checking its length, it returns undefined instead of the expected value of 1,000,001.\n\nWhen processing a string input, the function splits the string and maps the values, but doesn't ensure the returned function properly returns the processed array for very long inputs. There's inconsistency in how the function handles string inputs compared to other input types.",
      "solution": "After investigating this issue, I found that the reported behavior is actually **working as intended**.\n\n## The Misunderstanding\n\nThe issue reporter expected `compileTrust()` to return a function that provides the list of IPs:\n\n```javascript\nconst result = utils.compileTrust(longString);\nresult().length; // Expected: 1000001\n```\n\nHowever, `compileTrust()` returns a **predicate function** that checks if an IP should be trusted:\n\n```javascript\nconst trustFn = utils.compileTrust('127.0.0.1,192.168.1.1');\ntrustFn('127.0.0.1', 0); // true (IP is trusted)\ntrustFn('8.8.8.8', 0);   // false (IP not trusted)\ntrustFn();               // false (no IP provided)\n```\n\n## The Real Issue: Performance\n\nWhile investigating, I discovered a **legitimate performance issue** with large IP lists:\n\n| IP Count | Processing Time |\n|----------|----------------|\n| 100,000  | 213ms          |\n| 500,000  | 1,019ms        |\n| 1,000,000| 2,365ms        |\n\nThis causes significant startup delays for applications with large trust lists.\n\n## New Issue Created\n\nI've created a separate issue (#6849) to address the performance problem: https://github.com/expressjs/express/issues/6849\n\n## Recommendation\n\nThis issue should be closed as \"working as intended\" since the API behavior is correct. The performance concern is now tracked in #6849.",
      "labels": [
        "bug"
      ],
      "created_at": "2025-06-30T15:32:14Z",
      "closed_at": "2025-11-13T17:58:00Z",
      "url": "https://github.com/expressjs/express/issues/6611",
      "comments_count": 3
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 6432,
      "title": "Express 5.1: req.body is `undefined` when using express.json() for requests that do not have a request body",
      "problem": "Simply put, with express 5.1 req.body is `undefined` when not explicitly including a request body. This is unexpected! In express 4.x, req.body is guaranteed to be at least `{}` even when the request body is missing.\n\n# Express 5.1 Bug Report: express.json() Middleware Body Initialization\n\n## Environment information\n\n**Version**:\n```json\n\"dependencies\": {\n  \"express\": \"5.1.0\"  // Broken behavior\n  \"express\": \"4.21.2\" // Working behavior\n}\n```\n\n**Platform**:\n```bash\n\n$ uname -a\n 24.2.0 Darwin Kernel Version 24.2.0: Fri Dec  6 18:40:14 PST 2024; root:xnu11215.61.5~2/RELEASE_ARM64_T8103 arm64\n```\n\n**Node.js version**:\n```bash\n$ node -v\nv22.9.0\n```\n\n## What steps will reproduce the bug?\n\n1. Create minimal Express server:\n```javascript\nconst express = require('express');\nconst app = express();\n\napp.use(express.json());\n\napp.post('/test', (req, res) => {\n  console.log('Body:', req.body);\n  res.json({ receivedBody: req.body });\n});\n\napp.listen(3000, () => console.log('Server running on port 3000'));\n```\n\n2. Send POST request with no body:\n```bash\n$ npm install express@5\n$ curl -X POST http://localhost:3000/test\nServer Output: `Body: undefined`\n$ curl -X POST http://localhost:3000/test -H \"Content-Type: application/json\"\nServer Output: `Body: undefined`\n\n$ npm install express@4\n$ curl -X POST http://localhost:3000/test\nServer Output: `Body: {}`\n$ curl -X POST http://localhost:3000/test -H \"Content-Type: application/json\"\nServer Output: `Body: {}`\n```\n\n3. Observe behavior:\n- Express 4.x: `req.body` is `{}`\n- Express 5.x: `req.body` is `undefined`\n\n## Expected Behavior\nWhen a POST request is made with no body (with or without `Content-Type: application/json`), the `express.json()` middleware should initialize `req.body` to an empty object `{}`, as it did in Express 4.x.\n\n## Actual Behavior\nIn Express 5.x, `req.body` is `undefined` when no body is sent, even when the request includes `Content-Type: application/json`.\n\nThis change breaks backward compatibility and affects common REST API patterns where routes expect `req.body` to be an object, even if empty.\n\nExample output:\n``` bash\n$ npm install express@5\n$ curl -X POST http://localhost:3000/test\n# Server Output: `Body: undefined`\n$ npm install express@4\n# Server Output: `Body: {}`\n```",
      "solution": "There are multiple workarounds\n### Solution 1\nTo force it behave like `Express 4.x` Change the line `app.use(express.json());` to\n\n```\napp.use((req, res, next) => {\n  express.json()(req, res, (err) => {\n    if (!req.body) req.body = {};\n    next(err);\n  });\n});\n```\n\n### Or Solution 2\nAdd a global middleware to ensure `req.body` is never undefined\n```\napp.use((req, res, next) => {\n  req.body = req.body || {};\n  next();\n});\napp.use(express.json());\n\n```",
      "labels": [],
      "created_at": "2025-04-03T18:01:56Z",
      "closed_at": "2025-04-23T14:43:20Z",
      "url": "https://github.com/expressjs/express/issues/6432",
      "comments_count": 10
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 4888,
      "title": "Error: Route.get() requires a callback function but got a [object Undefined]",
      "problem": "I am using typescript in my project and when I develop with nodemon I get the following error:\r\n```console\r\nError: Route.get() requires a callback function but got a [object Undefined]\r\n    at Route.<computed> [as get] (M:\\repos\\web\\api-rest\\node_modules\\express\\lib\\router\\route.js:202:15)\r\n    at Function.proto.<computed> [as get] (M:\\repos\\web\\api-rest\\node_modules\\express\\lib\\router\\index.js:516:19)                                                                               :202:15)\r\n    at Object.<anonymous> (M:\\repos\\web\\api-rest\\src\\routes\\users.ts:11:8)                      \\index.js:516:19)\r\n    at Module._compile (node:internal/modules/cjs/loader:1101:14)\r\n    at Module.m._compile (M:\\repos\\web\\api-rest\\node_modules\\ts-node\\src\\index.ts:1455:23)      \r\n    at Module._extensions..js (node:internal/modules/cjs/loader:1153:10)\r\n    at Object.require.extensions.<computed> [as .ts] (M:\\repos\\web\\api-rest\\node_modules\\ts-node\\src\\index.ts:1458:12)                                                                          \\src\\index.ts:1458:12)\r\n    at Module.load (node:internal/modules/cjs/loader:981:32)\r\n    at Function.Module._load (node:internal/modules/cjs/loader:822:12)\r\n    at Module.require (node:internal/modules/cjs/loader:1005:19)\r\n```\r\nbut when I compile it with \"tsc\" and executor normally with node it works correctly\r\nBelow is the code I am using:\r\n```typescript\r\n//controllers/users.ts\r\n\r\nimport users from './users.json'\r\nimport type { Request, Response } from 'express'\r\n\r\nconst getUsers = (_: Request, res: Response): void => {\r\n  res.status(200).json(users)\r\n}\r\n\r\nconst getUserById = (req: Request, res: Response): void => {\r\n  const { id } = req.params\r\n  const user = users.find(user => user.id === Number(id))\r\n  if (user != null) {\r\n    res.status(200).json(user)\r\n  } else {\r\n    res.status(404).json({ message: 'User not found' })\r\n  }\r\n}\r\n\r\nexport { getUsers, getUserById }\r\n```\r\n```typescript\r\n//routes/users.ts\r\n\r\nimport express from 'express'\r\n// import users from '../controllers/users.json'\r\nimport { getUsers } from '../controllers/users'\r\n// import type { Request, Response } from 'express'\r\nconst router = express.Router()\r\n\r\n// export const getUsersHere = (_: Request, res: Response): void => {\r\n//   res.status(200).json(users)\r\n// }\r\n\r\nrouter.get('/', getUsers) <--- Here is the error\r\n// router.get(':id', users.getUserById)\r\n\r\nexport default router\r\n```",
      "solution": "Hi @JoShMiQueL. Could you kindly let me know how you resolved this issue?\n\n---\n\n> Hi @JoShMiQueL. Could you kindly let me know how you resolved this issue?\n\nI never actually figured it out",
      "labels": [],
      "created_at": "2022-04-08T16:40:43Z",
      "closed_at": "2022-04-08T16:45:09Z",
      "url": "https://github.com/expressjs/express/issues/4888",
      "comments_count": 4
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 6038,
      "title": "throw new TypeError(`Missing parameter name at ${i}: ${DEBUG_URL}`); at node_modules/path-to-regexp/dist/index.js:73",
      "problem": "<!-- The process for bug fixing is:\r\n\r\n- We will first assess if the behavior is different from what should occur\r\n- Confirm the bug is reproducible\r\n- Discuss how to best fix the bug\r\n- Work towards a fix\r\n-->\r\n\r\n/rbd/pnpm-volume/11269fa9-8ee1-4ee3-b2e8-df916e2b43b7/node_modules/path-to-regexp/dist/index.js:73\r\n            throw new TypeError(`Missing parameter name at ${i}: ${DEBUG_URL}`);\r\n            ^\r\nTypeError: Missing parameter name at 6: https://git.new/pathToRegexpError\r\n    at name (/rbd/pnpm-volume/11269fa9-8ee1-4ee3-b2e8-df916e2b43b7/node_modules/path-to-regexp/dist/index.js:73:19)\r\n    at lexer (/rbd/pnpm-volume/11269fa9-8ee1-4ee3-b2e8-df916e2b43b7/node_modules/path-to-regexp/dist/index.js:91:27)\r\n    at lexer.next (<anonymous>)\r\n    at Iter.peek (/rbd/pnpm-volume/11269fa9-8ee1-4ee3-b2e8-df916e2b43b7/node_modules/path-to-regexp/dist/index.js:106:38)\r\n    at Iter.tryConsume (/rbd/pnpm-volume/11269fa9-8ee1-4ee3-b2e8-df916e2b43b7/node_modules/path-to-regexp/dist/index.js:112:28)\r\n    at Iter.text (/rbd/pnpm-volume/11269fa9-8ee1-4ee3-b2e8-df916e2b43b7/node_modules/path-to-regexp/dist/index.js:128:30)\r\n    at consume (/rbd/pnpm-volume/11269fa9-8ee1-4ee3-b2e8-df916e2b43b7/node_modules/path-to-regexp/dist/index.js:152:29)\r\n    at parse (/rbd/pnpm-volume/11269fa9-8ee1-4ee3-b2e8-df916e2b43b7/node_modules/path-to-regexp/dist/index.js:183:20)\r\n    at /rbd/pnpm-volume/11269fa9-8ee1-4ee3-b2e8-df916e2b43b7/node_modules/path-to-regexp/dist/index.js:294:74\r\n    at Array.map (<anonymous>)",
      "solution": "im fetching same error, is it have quick solution to solve the problem?\n\n\n---\n\n> im fetching same error, is it have quick solution to solve the problem?\n\n/api/*placeholderValue\nOR\n/*{{placeholderValue}}\n*{{placeholderValue}}\n\nthis should help you\n\n---\n\nThank you for this conversation, this helps me when i am stuck at \n```\n\n             throw new TypeError(`Missing parameter name at ${i}: ${DEBUG_URL}`);\n            ^\n\nTypeError: Missing parameter name at 1: https://git.new/pathToRegexpError\n    at name (D:\\PROJECTS\\LIVE-SCOREBOARD-API\\node_modules\\path-to-regexp\\dist\\index.js:73:19)\n    at lexer (D:\\PROJECTS\\LIVE-SCOREBOARD-API\\node_modules\\path-to-regexp\\dist\\index.js:91:27)\n    at lexer.next (<anonymous>)\n    at Iter.peek (D:\\PROJECTS\\LIVE-SCOREBOARD-API\\node_modules\\path-to-regexp\\dist\\index.js:106:38)\n    at Iter.tryConsume (D:\\PROJECTS\\LIVE-SCOREBOARD-API\\node_modules\\path-to-regexp\\dist\\index.js:112:28)\n    at Iter.text (D:\\PROJECTS\\LIVE-SCOREBOARD-API\\node_modules\\path-to-regexp\\dist\\index.js:128:30)\n    at consume (D:\\PROJECTS\\LIVE-SCOREBOARD-API\\node_modules\\path-to-regexp\\dist\\index.js:152:29)\n    at parse (D:\\PROJECTS\\LIVE-SCOREBOARD-API\\node_modules\\path-to-regexp\\dist\\index.js:183:20)\n    at D:\\PROJECTS\\LIVE-SCOREBOARD-API\\node_modules\\path-to-regexp\\dist\\index.js:294:74\n    at Array.map (<anonymous>)\n\nNode.js v20.17.0\n\n```\n\nRoot cause\n\n// 404 handler\napp.use('*', (req, res) => {\n    res.status(404).json({\n        error: 'Route not found',\n        message: `Cannot ${req.method} ${req.originalUrl}`\n    });\n});\n\n\ni am just remove it ",
      "labels": [
        "bug"
      ],
      "created_at": "2024-10-10T16:09:13Z",
      "closed_at": "2024-10-10T17:15:42Z",
      "url": "https://github.com/expressjs/express/issues/6038",
      "comments_count": 21
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 6222,
      "title": " CVE-2024-10491 ",
      "problem": "<!-- The process for bug fixing is:\r\n\r\n- We will first assess if the behavior is different from what should occur\r\n- Confirm the bug is reproducible\r\n- Discuss how to best fix the bug\r\n- Work towards a fix\r\n-->\r\nA vulnerability has been identified in the Express response.links&nbsp;function, allowing for arbitrary resource injection in the Link&nbsp;header when unsanitized data is used.\r\n\r\nThe issue arises from improper sanitization in Link header values, which can allow a combination of characters like ,, ;, and <> to preload malicious resources.\r\n\r\nThis vulnerability is especially relevant for dynamic parameters.",
      "solution": "Can you confirm this is fixed as our NexusIQ scan is flagging this and we are currently on 4.21.2:\r\n\r\n> ### Advisory Deviation Notice:\r\n> The Sonatype Security Research team discovered that this vulnerability was introduced in version `3.0.0-beta4` and does not affect all versions through `3.1.24` as stated in the advisory. Additionally, the team discovered that this vulnerability affects all available `4.x` and `5.x` versions.\n\n---\n\nAs you can read from the vulnerability [report here](https://nvd.nist.gov/vuln/detail/CVE-2024-10491) or [here](https://www.herodevs.com/vulnerability-directory/cve-2024-10491), this does not impact express after version `3.21.2`. If Nexus or some other vendor thinks they have a new report or this impacts more than the verified range they should use our standard security reporting methods so we can properly address it. Please reach out to them and tell them to either correct their mistake or responsibly disclose the issue. \n\n---\n\n@wesleytodd - thanks for your prompt response and clarification regarding this issue and its impact on Express `4.21.2`.\r\n\r\nYour explanation confirms what I had begun to suspect - that this issue was indeed fixed after version `3.21.2` and is **NOT** a current vulnerability requiring a fix for `4.x` or `5.x`.\r\n\r\nI will escalate this with our cyber team to address the discrepancy in NexusIQ's scan and request that they contact the vendor to highlight the incorrect flagging of this issue. I will also ask them to suggest that the vendor use your standard security reporting methods if they genuinely believe this to be a valid concern identified by their team.\r\n\r\nIt is frustrating that our software could be incorrectly flagged as vulnerable due to potential inaccuracies within NexusIQ as this undermines trust in a tool that should be relied upon to provide accurate and verified data.",
      "labels": [
        "3.x"
      ],
      "created_at": "2024-12-13T06:03:33Z",
      "closed_at": "2024-12-13T15:29:37Z",
      "url": "https://github.com/expressjs/express/issues/6222",
      "comments_count": 10
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 6428,
      "title": "TypeError: Missing parameter name at 2: https://git.new/pathToRegexpError",
      "problem": "<!-- The process for bug fixing is:\n\n- We will first assess if the behavior is different from what should occur\n- Confirm the bug is reproducible\n- Discuss how to best fix the bug\n- Work towards a fix\n-->\n\n## Environment information\n\n**Version**: [v5.1.0](https://github.com/expressjs/express/releases/tag/v5.1.0)\n\n**Platform**: Linux 6.8.0-1021-aws 23~22.04.1-Ubuntu SMP Tue Dec 10 16:50:46 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux\n\n**Node.js version**: v16.20.2\n\n**Any other relevant information**: N/A\n\n## What steps will reproduce the bug?\n\n```bash\n# Installing the latest express cause the issue\nnpm install express\n```\nthen throws error\n```bash\nTypeError: Missing parameter name at 2: https://git.new/pathToRegexpError\n    at name (/var/www/<project>/node_modules/path-to-regexp/dist/index.js:73:19)\n    at lexer (/var/www/<project>/node_modules/path-to-regexp/dist/index.js:91:27)\n    at lexer.next (<anonymous>)\n    at Iter.peek (/var/www/<project>/node_modules/path-to-regexp/dist/index.js:106:38)\n    at Iter.tryConsume (/var/www/<project>/node_modules/path-to-regexp/dist/index.js:112:28)\n    at Iter.text (/var/www/<project>/node_modules/path-to-regexp/dist/index.js:128:30)\n    at consume (/var/www/<project>/node_modules/path-to-regexp/dist/index.js:152:29)\n    at parse (/var/www/<project>/node_modules/path-to-regexp/dist/index.js:183:20)\n    at /var/www/<project>/node_modules/path-to-regexp/dist/index.js:294:74\n    at Array.map (<anonymous>)\n/var/www/<project>/node_modules/path-to-regexp/dist/index.js:73\n            throw new TypeError(`Missing parameter name at ${i}: ${DEBUG_URL}`);\n```\n\n### Temporary Fix\nDowngrade to [4.21.2](https://github.com/expressjs/express/releases/tag/4.21.2)\n```bash\nnpm install express@4.21.2\n```\n",
      "solution": "I had this problem aswell and can confirm that this is \"simply\" a breaking change from the V5 update. \nAs described in the changelogs [here](https://expressjs.com/en/guide/migrating-5.html#path-syntax) routes like `app.use('/**', ...)` are not supported anymore.\n\nThis problem came up while updating express in an Angular SSR application for me. When setting up SSR, Angular generated exactly the example mentioned above for me. But changing it to `app.use('/{*path}', ...)` fixed the problem for me. \ud83d\udc4d \n\nScreenshot from the migration section of the express homepage:\n\n![Image](https://github.com/user-attachments/assets/4806a882-e67b-4df4-92a9-2b74c49feca1)\n\n\n---\n\nThanks @wesleytodd and @slagiewka, I confirmed that it is a breaking issue with the new version.\n\nAs for our *usecase* we only have this one route, that accepts all requests and send in the `index.html` of our frontend build, and you guys are correct, this one is causing the issue.\n\n```js\napp.get('/*', function (req, res) {\n  res.sendFile(path.join(__dirname, '../build', 'index.html'));\n});\n```\n\nI opened this issue yesterday because we have to rollback our production and it cost us downtime and delay our app release thinking it's a fault on our recent changes.\n\nFunny enough, it's our mistake, we have this untouched code in our `Dockerfile` for ~5 years now and a `package-lock.json` is not generated until deployment as we do the `expressjs` installation on docker build, basically it's always installing the latest version \ud83d\ude2d.\n\n```bash\nRUN npm install express\n```\n\nWith this, I conclude that this issue is **resolved**, you guys can close this anytime if no one wants to add more. I just hope no major impact to production servers out there because of silly mistakes like what we did.\n\n---\n\nThis is true of all libraries, you have benefited these past years from the stability (and stagnation \ud83d\ude09) of the project. If you are doing something like `npm install express` in your build you need to specify a version you rely on. Change that to `npm install express@5` now and then when we release v6 which will also include breaking changes you will avoid this pain.\n\nI will still keep this open for now, because it avoids more duplicate issues, but glad to hear your issue is resolved @eru123!",
      "labels": [
        "awaiting more info"
      ],
      "created_at": "2025-04-01T03:09:19Z",
      "closed_at": "2025-08-23T01:29:23Z",
      "url": "https://github.com/expressjs/express/issues/6428",
      "comments_count": 30
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 5936,
      "title": "[DEPLOY-ERROR] - TypeError: Missing parameter name at 1: https://git.new/pathToRegexpError - Firebase Functions Project - Updated to Version 5 - Firebase Deploy Failed",
      "problem": "<!-- The process for bug fixing is:\r\n\r\n- We will first assess if the behavior is different from what should occur\r\n- Confirm the bug is reproducible\r\n- Discuss how to best fix the bug\r\n- Work towards a fix\r\n-->\r\n\r\n[DEPLOY-ERROR] - TypeError: Missing parameter name at 1: https://git.new/pathToRegexpError - Firebase Functions Project - Updated to Version 5 - Firebase Deploy Failed\r\n\r\n\r\n\r\n<img width=\"1407\" alt=\"Screenshot 2024-09-10 at 3 37 19\u202fPM\" src=\"https://github.com/user-attachments/assets/e0bc4432-c23d-4b2f-b6bb-a4b8229e8a64\">\r\n\r\n\r\nkingsman97:functions ahsan$ nmr deploy:specific\r\n\r\n> deploy:specific\r\n> npm run pre-deploy && firebase deploy --only functions:get_generic_in_app_notifications,functions:get_user_role_specific_in_app_notifications,functions:get_user_specific_in_app_notifications\r\n\r\n\r\n> pre-deploy\r\n> npm run unlink-packages\r\n\r\n\r\n> unlink-packages\r\n> yarn unlink zaions-tool-kit && yarn unlink zaions-express-tool-kit && yarn install --force\r\n\r\nyarn unlink v1.22.22\r\nsuccess Removed linked package \"zaions-tool-kit\".\r\ninfo You will need to run `yarn install --force` to re-install the package that was linked.\r\n\u2728  Done in 0.04s.\r\nyarn unlink v1.22.22\r\nsuccess Removed linked package \"zaions-express-tool-kit\".\r\ninfo You will need to run `yarn install --force` to re-install the package that was linked.\r\n\u2728  Done in 0.05s.\r\nyarn install v1.22.22\r\n[1/5] \ud83d\udd0d  Validating package.json...\r\n[2/5] \ud83d\udd0d  Resolving packages...\r\n[3/5] \ud83d\ude9a  Fetching packages...\r\n[4/5] \ud83d\udd17  Linking dependencies...\r\nwarning \" > firebase-functions-test@3.3.0\" has unmet peer dependency \"jest@>=28.0.0\".\r\n[5/5] \ud83d\udd28  Rebuilding all packages...\r\nsuccess Saved lockfile.\r\n\u2728  Done in 4.55s.\r\n\r\n=== Deploying to 'aoneahsan-learn-p2'...\r\n\r\ni  deploying functions\r\nRunning command: npm --prefix \"$RESOURCE_DIR\" run build\r\n\r\n> build\r\n> npm run cleanOutputDir && tsc && tsc-alias\r\n\r\n\r\n> cleanOutputDir\r\n> rimraf ./lib\r\n\r\n\u2714  functions: Finished running predeploy script.\r\ni  functions: preparing codebase default for deployment\r\ni  functions: ensuring required API cloudfunctions.googleapis.com is enabled...\r\ni  functions: ensuring required API cloudbuild.googleapis.com is enabled...\r\ni  artifactregistry: ensuring required API artifactregistry.googleapis.com is enabled...\r\n\u2714  functions: required API cloudfunctions.googleapis.com is enabled\r\n\u2714  functions: required API cloudbuild.googleapis.com is enabled\r\n\u2714  artifactregistry: required API artifactregistry.googleapis.com is enabled\r\ni  functions: Loading and analyzing source code for codebase default to determine what to deploy\r\nServing at port 8526\r\n\r\nTypeError: Missing parameter name at 1: https://git.new/pathToRegexpError\r\n    at name (/Volumes/Personal/01-code-work/firebase-projects/01-zaions/100-ahsan-learn-p1-v1/functions/node_modules/router/node_modules/path-to-regexp/dist/index.js:85:19)\r\n    at lexer (/Volumes/Personal/01-code-work/firebase-projects/01-zaions/100-ahsan-learn-p1-v1/functions/node_modules/router/node_modules/path-to-regexp/dist/index.js:103:27)\r\n    at lexer.next (<anonymous>)\r\n    at Iter.peek (/Volumes/Personal/01-code-work/firebase-projects/01-zaions/100-ahsan-learn-p1-v1/functions/node_modules/router/node_modules/path-to-regexp/dist/index.js:119:38)\r\n    at Iter.tryConsume (/Volumes/Personal/01-code-work/firebase-projects/01-zaions/100-ahsan-learn-p1-v1/functions/node_modules/router/node_modules/path-to-regexp/dist/index.js:125:28)\r\n    at Iter.text (/Volumes/Personal/01-code-work/firebase-projects/01-zaions/100-ahsan-learn-p1-v1/functions/node_modules/router/node_modules/path-to-regexp/dist/index.js:141:30)\r\n    at consume (/Volumes/Personal/01-code-work/firebase-projects/01-zaions/100-ahsan-learn-p1-v1/functions/node_modules/router/node_modules/path-to-regexp/dist/index.js:166:29)\r\n    at parse (/Volumes/Personal/01-code-work/firebase-projects/01-zaions/100-ahsan-learn-p1-v1/functions/node_modules/router/node_modules/path-to-regexp/dist/index.js:197:20)\r\n    at /Volumes/Personal/01-code-work/firebase-projects/01-zaions/100-ahsan-learn-p1-v1/functions/node_modules/router/node_modules/path-to-regexp/dist/index.js:308:74\r\n    at Array.map (<anonymous>)\r\n\r\n\r\nError: Functions codebase could not be analyzed successfully. It may have a syntax or runtime error",
      "solution": "yes i know this is \"next\" major release, and current stable version on NPM is \"4.20.0\", just want to bring this to attention as well, so once on NPM we can the stable version to \"5.*.*\" it will be fixed before that, thanks :) \n\n---\n\n> Express 5.0.0 uses [`router` 2.0.0](https://github.com/pillarjs/router/blob/2e7fb67ad1b0c1cd2d9eb35c2244439c5c57891a/HISTORY.md#200--2024-09-09), which uses [`path-to-regexp` 8.0.0](https://github.com/pillarjs/path-to-regexp/releases/tag/v8.0.0) and it brings some breaking changes to path handling.\r\n> \r\n> The error that you got is caused by using `:` or `*` in one of your paths, which is not followed by parameter name. In Express 5 the wildcard `*` means something different than in 4.x. In 4.x it would match anything, but in 5.0 it behaves like `:` and is a named parameter.\r\n> \r\n> You should check your paths, especially ones with `:`, `*`, `?` and `+` to make sure that they are compatible with the new changes.\r\n> \r\n> You can find more details in changelogs and the link shown in the error message:\r\n> \r\n>     * https://github.com/pillarjs/router/blob/master/HISTORY.md#200--2024-09-09\r\n> \r\n>     * https://github.com/pillarjs/path-to-regexp/releases/tag/v8.0.0\r\n> \r\n>     * https://github.com/pillarjs/path-to-regexp?tab=readme-ov-file#missing-parameter-name\r\n\r\nI think that Express 5.x documentation may have an error: \r\n![image](https://github.com/user-attachments/assets/16539c0c-ae87-42fc-9565-f4d6dd5176aa)\r\n\r\nIf you try this you'll trigger the \r\n`TypeError: Unexpected ( at 0, expected END: https://git.new/pathToRegexpError` error. Would be worth fixing this to prevent confusion.\n\n---\n\n\n  ### \ud83d\ude80 Bounty Alert!\n\n\ud83d\udcb0 **A bounty of** $30.00 **has been created by** [omarsoufiane](https://github.com/omarsoufiane) **on BountyHub**. \n\n\ud83d\udd17 **[Claim this bounty by submitting a pull request that solves the issue!](https://www.bountyhub.dev/bounty/view/eedbb4e8-a23e-43e6-9cb5-8815fcdc0c94)**\n\nGood luck, and happy coding! \ud83d\udcbb\n",
      "labels": [
        "bug"
      ],
      "created_at": "2024-09-10T10:37:34Z",
      "closed_at": "2024-10-23T20:28:04Z",
      "url": "https://github.com/expressjs/express/issues/5936",
      "comments_count": 17
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 6545,
      "title": "No overload matches this call error ts(2769)",
      "problem": "This is my indexWithZod.ts file:\n\n``` import express from 'express';\nimport { z } from 'zod';\n\nexport const appWithZod = express();\nappWithZod.use(express.json());\n\n\nconst sumInput = z.object({\n  a: z.number(),\n  b: z.number()\n});\n\nappWithZod.post('/sum', (req, res) => {\n\n  const parsedResponse = sumInput.safeParse(req.body);\n\n  if (!parsedResponse.success) {\n    return res.status(411).json({\n      message: 'Incorrect inputs'\n    });\n  }\n  \n  const answer = parsedResponse.data.a + parsedResponse.data.b;\n\n  return res.json({\n    answer\n  });\n});\n```\n\nI am getting error at `(req, res) => {` where the exact error message is \n\n> No overload matches this call.\n  The last overload gave the following error.\n    Argument of type '(req: Request<{}, any, any, ParsedQs, Record<string, any>>, res: Response<any, Record<string, any>, number>) => Response<any, Record<...>, number>' is not assignable to parameter of type 'Application<Record<string, any>>'.\n      Type '(req: Request<{}, any, any, ParsedQs, Record<string, any>>, res: Response<any, Record<string, any>, number>) => Response<any, Record<...>, number>' is missing the following properties from type 'Application<Record<string, any>>': init, defaultConfiguration, engine, set, and 63 more.ts(2769)\n\nHere is the additional bin.ts file for reference:\n```\nimport { appWithZod } from './indexWithZod';\n\nappWithZod.listen(3001, () => {\n  console.log('Server (appWithZod) is running on PORT 3001');\n});\n\n```\n",
      "solution": "This should be fixed with the latest version of the express-server-static types.",
      "labels": [
        "question"
      ],
      "created_at": "2025-05-30T13:10:45Z",
      "closed_at": "2025-07-15T02:40:22Z",
      "url": "https://github.com/expressjs/express/issues/6545",
      "comments_count": 2
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 6633,
      "title": "req.query is now a getter and can't use as custom server in front of Next.js",
      "problem": "When using Express as a custom server in front of Next.js, accessing req.query directly no longer returns a mutable plain object. Instead, it is now a getter, and attempts to modify it or use it as a plain object lead to unexpected behavior or errors.\n\nThis change affects routes where the server modifies or depends on the mutability of req.query before passing it to Next.js, breaking compatibility with existing implementations.\n\n\nNext.js Issue:\nhttps://github.com/vercel/next.js/issues/79158\n\n---\n\n### Context\n\nWe applied the following patch, but we do not believe it is the best solution:\n\n```js\n// Express 5 makes the 'query' property read-only, but Next.js needs to write to it\ntry {\n  Object.defineProperty(req, 'query', descriptor);\n} catch (error) {\n  // If property already exists and is configurable, delete and recreate it\n  try {\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    delete (req as any).query;\n    Object.defineProperty(req, 'query', descriptor);\n    logger.logInfo(`query property fixed ${error}`, requestID);\n  } catch (error) {\n    logger.logWarn(\n      `Could not fix query property for Next.js compatibility: ${error}`,\n      requestID\n    );\n  }\n}\n```\n\nIs it possible to know why Express decided to change? This would help us understand where we need to make adjustments (on the Next.js side or the Express.js side).\n",
      "solution": "While I wasn't around at the time when that [commit](https://github.com/expressjs/express/commit/dcc4eaabe86a4309437db2a853c5ef788a854699) was made and possibly discussed, I found reasons (https://github.com/expressjs/express/issues/3472#issuecomment-341783875, https://github.com/expressjs/express/issues/2752#issuecomment-139111402) from the former maintainer for making this just a getter. \n\nThe long-term solution is to use req.locals to store the query and modify it later, i'm not sure how this could work for Next.js, https://github.com/expressjs/express/issues/3472#issuecomment-341820984\n",
      "labels": [],
      "created_at": "2025-07-12T21:11:54Z",
      "closed_at": "2025-07-13T18:34:07Z",
      "url": "https://github.com/expressjs/express/issues/6633",
      "comments_count": 1
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 5238,
      "title": "timeout abnormal ",
      "problem": "when i use node:18.13.0 and express:4.18.2 to upload file, Uploading will fail between 5 minutes and 6 minutes.\r\nFirstly, response will emit \"close\". Secondly, request will be aborted.\r\n",
      "solution": "```\r\napp.post('/_/file/upload/put_file', (req, res) => {\r\n    const fileStream = fs.createWriteStream('afse');\r\n    \r\n    req.pipe(fileStream);\r\n\r\n    fileStream.on('finish', () => {\r\n        res.send('success');\r\n    });\r\n\r\n    fileStream.on('error', (err) => {\r\n        console.error(`${new Date()} File stream error: ${err}`);\r\n        res.status(500).send('failed');\r\n    });\r\n\r\n    req.on('error', (err) => {\r\n        console.error(`${new Date()} Request error: ${err}`);\r\n    });\r\n\r\n    req.on('close', () => {\r\n        console.log(`${new Date()} Request closed`);\r\n        fileStream.destroy();\r\n    });\r\n\r\n    res.on('error', (err) => {\r\n        console.error(`${new Date()} Response error: ${err}`);\r\n        fileStream.end();\r\n    });\r\n\r\n    res.on('close', () => {\r\n        console.log(`${new Date()} Response closed`);\r\n    });\r\n});\r\n``` \r\n### Try this \r\n>  i think these changes will help and ensure that the file stream is properly closed and that errors are handled more gracefully, reducing the chances of upload failures.\r\n> if will not work please specifie more info\r\n> and info. about the content or format. which you trying to upload .\r\n",
      "labels": [],
      "created_at": "2023-08-06T05:47:39Z",
      "closed_at": "2025-07-12T19:16:52Z",
      "url": "https://github.com/expressjs/express/issues/5238",
      "comments_count": 3
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 6458,
      "title": "5.1.0 Throws error on route with a parameter in it",
      "problem": "\n```\ncory@redbarchetta[03:41:13]~/.../Projects/YoctoVideo/backend$ npm run dev\n\n> backend@0.0.1 dev\n> npx dotenvx run -f .env.development -f .env -- node ./src/index.js\n\n[dotenvx@1.38.4] injecting env (9) from .env.development, .env\n/home/cory/Documents/Projects/YoctoVideo/backend/node_modules/path-to-regexp/dist/index.js:73\n            throw new TypeError(`Missing parameter name at ${i}: ${DEBUG_URL}`);\n                  ^\n\nTypeError: Missing parameter name at 1: https://git.new/pathToRegexpError\n    at name (/home/cory/Documents/Projects/YoctoVideo/backend/node_modules/path-to-regexp/dist/index.js:73:19)\n    at lexer (/home/cory/Documents/Projects/YoctoVideo/backend/node_modules/path-to-regexp/dist/index.js:91:27)\n    at lexer.next (<anonymous>)\n    at Iter.peek (/home/cory/Documents/Projects/YoctoVideo/backend/node_modules/path-to-regexp/dist/index.js:106:38)\n    at Iter.tryConsume (/home/cory/Documents/Projects/YoctoVideo/backend/node_modules/path-to-regexp/dist/index.js:112:28)\n    at Iter.text (/home/cory/Documents/Projects/YoctoVideo/backend/node_modules/path-to-regexp/dist/index.js:128:30)\n    at consume (/home/cory/Documents/Projects/YoctoVideo/backend/node_modules/path-to-regexp/dist/index.js:152:29)\n    at parse (/home/cory/Documents/Projects/YoctoVideo/backend/node_modules/path-to-regexp/dist/index.js:183:20)\n    at /home/cory/Documents/Projects/YoctoVideo/backend/node_modules/path-to-regexp/dist/index.js:294:74\n    at Array.map (<anonymous>)\n\nNode.js v23.11.0\nCommand failed with exit code 1: /usr/bin/node ./src/index.js\n```\n\nThrough bisecting, the offending code in my app is\n\n```js\nimport express from 'express';\nimport session from 'express-session';\nimport { sql } from 'slonik';\nimport { dbPool } from '../database/index.js';\nconst router = express.Router();\n\n// API route for profile info\nrouter.get('/api/users/:nickname', async (request, response) => {\n  try {\n    let nickname = request.params.nickname;\n    // If no nickname is provided, check if the user is logged in\n    if (!nickname && request.session.nickname) {\n      nickname = request.session.nickname;\n    }\n\n    // If no nickname and no session, return an error\n    if (!nickname) {\n      response.status(401).json({ success: false, message: 'Not logged in or no nickname provided.' });\n      return;\n    }\n\n    // Fetch the profile details for the given nickname\n    const user = await dbPool.maybeOne(sql.unsafe`\n\t\t\tSELECT email, nickname, full_name, birthday FROM users WHERE nickname = ${nickname};\n\t\t`);\n\n    // Check if the logged-in user is viewing their own profile\n    const isOwner = request.session.nickname === nickname;\n\n    // If the logged-in user is the owner, send full profile details\n    if (isOwner) {\n      response.status(200).json({\n        success: true,\n        profile: {\n          email: user.email,\n          nickname: user.nickname,\n          fullName: user.full_name,\n          dateOfBirth: user.birthday,\n          isOwner: true,\n        },\n      });\n      return;\n    } else {\n      // If it's a public view, send limited profile details\n      response.status(200).json({\n        success: true,\n        data: {\n          email: user.email,\n          nickname: user.nickname,\n          isOwner: false,\n        },\n      });\n      return;\n    }\n  } catch (error) {\n    console.error('Error fetching profile:', error);\n    response.status(500).json({ success: false, message: 'Internal server error.' });\n    return;\n  }\n});\n\nexport default router;\n```\n\nIt was originally '/api/users/:nickname?', but when I updated to express 5.1.0, I changed the route path to '/api/users/{:nickname}' like the migration guide suggests. When that didn't work I got rid of the braces ('/api/users/:nickname') to make it a required parameter, but that still did not work.\n\n<!-- The process for bug fixing is:\n\n- We will first assess if the behavior is different from what should occur\n- Confirm the bug is reproducible\n- Discuss how to best fix the bug\n- Work towards a fix\n-->\n\n## Environment information\n\n**Version**:\n<!-- \nTo find the installed version of a package, you can check the `package.json` file in the root directory of your project. The version will be listed under `dependencies` or `devDependencies`, like this:\n\n```json\n\"dependencies\": {\n    \"connect-pg-simple\": \"^10.0.0\",\n    \"cors\": \"^2.8.5\",\n    \"express\": \"^5.1.0\",\n    \"express-session\": \"^1.18.1\",\n    \"pg\": \"^8.14.1\",\n    \"slonik\": \"^46.4.0\"\n  }-->\n\n**Platform**:\n<!-- \ncory@redbarchetta[03:53:20]~/.../Projects/YoctoVideo/backend$ uname -a\nLinux redbarchetta.cory.albrecht.name 6.8.0-58-generic #60~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Fri Mar 28 16:09:21 UTC 2 x86_64 x86_64 x86_64 GNU/Linux\n-->\n\n**Node.js version**:\n<!-- \ncory@redbarchetta[03:53:28]~/.../Projects/YoctoVideo/backend$ node -v\nv23.11.0\n-->\n\n**Any other relevant information**:\n\n## What steps will reproduce the bug?\n\nAfter trying a few arbitrary paths, it seems that any replaceable param in a 5.x router.get() path will do this.\n",
      "solution": "Are you sure that this path is the problem? Don't you maybe have `/*` somewhere?\n\nIf you want to see which path causes the exception you can increase the stack trace limit (e.g. `node --stack-trace-limit=50 index.js`) and find the last line pointing at your source, before entering `node_modules/express` or `node_modules/router`.\n\n---\n\n> I am facing the same error with express.static, with previous version the code below worked fine but it no longer works with the 5.x version\n> \n> ```\n> app.get(\"*\", (req, res) => {\n>       res.sendFile(\"index.html\", { root: \"build/dist/\" });\n>     });\n> ```\n\ntry this instead\n ```\n app.get(/(.*)/, (req, res) =>  { \n     res.sendFile(\"index.html\", { root: \"build/dist/\" });\n });\n```",
      "labels": [],
      "created_at": "2025-04-16T07:56:36Z",
      "closed_at": "2025-07-11T23:59:18Z",
      "url": "https://github.com/expressjs/express/issues/6458",
      "comments_count": 8
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 6556,
      "title": "Native support for Valkey (cache) in Express",
      "problem": "**Context:**\n\nAfter exhaustive analysis of different caching options for my express boilerplate ive found that:\n\n- Redis cant be trusted (relicensing drama)\n- Redis was forked into free OSS and by AWS & GCP and named to [Valkey](https://valkey.io/)\n- Valkey is more performant [Video proof](https://www.youtube.com/watch?v=9hDvWVJtljE&t=248s)\n\nAnd thus, id like to request a Expressjs-valkey plugin, similar to [what Fastify has](https://github.com/fastify/fastify-redis/issues/226) - In my case, im using [Valkey Glide](https://github.com/valkey-io/valkey-glide) so that would be desired, but [iovalkey](https://github.com/valkey-io/iovalkey) would make sense too\n\n\n**- The use case it helps solve and goal of the new feature**\nThe use case for this would be streamlined access to Valkey.io cache service\n\n\n**- Example code for how you would use or interact with the feature**\n```\nimport express from 'express'\nimport expressValkey from '@express/valkey-glide'\n\nconst express = express()\n\n// create by specifying address\nexpress.register(expressValkey, {\n  addresses: [{ host: '127.0.0.1' }]\n})\n\n// OR with more options\nexpress.register(expressValkey, {\n  addresses: [{ host: '127.0.0.1', port: 6379 }],\n  credentials: {username: \"user1\", password: \"password\"},\n  useTLS: true\n})\n\n```\n\n**- The expected behavior, in as much details as you can provide**\n\nExpressjs framework has access to Valkey client - similar to [Fastify's plugin](https://github.com/fastify/fastify-valkey-glide)\n\n\n**- Do you want to work on this feature yourself? Would to look for help opening the PR? Or are you asking for others to work on the PR?**\n\ni do NOT want to on this feature myself - Help by others would be welcome",
      "solution": "Hi, valkey-glide maintainer.\n\nIf someone sees it and wants to contribute a middleware please engage me and I'll be happy to support with whatever is needed.\n\nAdditionally, if it's something that you want, regardless of whether you want to contribute it, please comment and tag me.\n\nWhile limited in resources to create integration with any project, we do care primarily about our users needs, and if there are growing interests of users with direct integration with express, we will prioritize it.\n\n@wesleytodd @bjohansebas thanks for your comments. In case express will come to a point it wants to create some larger integration with a cache system, which can combine using local cache plus cache server please let me know and we can plan something.\n\n@SergioNR thanks for the issue, appreciate your interest in our project, and I'll be happy to support anything needed.",
      "labels": [],
      "created_at": "2025-06-03T20:18:33Z",
      "closed_at": "2025-06-03T21:45:48Z",
      "url": "https://github.com/expressjs/express/issues/6556",
      "comments_count": 3
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 5615,
      "title": "Support HTTP `QUERY` method",
      "problem": "Honestly we might already, but opening this issue to track landing support and writing tests.\r\n\r\nNode 21 added support for a new method! `QUERY` which is awesome, looks to have landed here https://github.com/nodejs/node/pull/51719 in `21.7.2` ([changelog](https://github.com/nodejs/node/blob/main/doc/changelogs/CHANGELOG_V21.md#21.7.2))\r\n\r\n@wesleytodd any idea what needs to be done to router to support QUERY? If this Issue belongs at `router` plz do move it!\r\nWe may actually already support this \u00af\\_(\u30c4)_/\u00af and it's just that a test is failing.\r\n\r\nIm unable to get it to work in Node `21.7.3` currently, and left a comment in the node tracking issue for support https://github.com/nodejs/node/issues/51562\r\n\r\n\r\nTODO:\r\n- [x] Comment out failing QUERY test until fixed in https://github.com/nodejs/node/pull/51719\r\n- [x] Add back in QUERY method test, only run on versions at and above the version that includes the fix to the above\r\n",
      "solution": "Yeah that is quite possible. I thought the version with this fixed was supposed to go out with the security patch but maybe I misunderstood and it was set to land later? Based on this yes I am thinking that is the case: https://github.com/nodejs/node/issues/51562#issuecomment-2067948749",
      "labels": [
        "enhancement",
        "4.x",
        "5.x",
        "module:router"
      ],
      "created_at": "2024-04-20T21:10:58Z",
      "closed_at": "2024-06-05T23:22:28Z",
      "url": "https://github.com/expressjs/express/issues/5615",
      "comments_count": 9
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 6359,
      "title": "ERROR:- TS2769: No overload matches this call.",
      "problem": "I'm trying to use an error-handling middleware with Express and TypeScript, but I'm getting a TypeScript error:\n\n```typescript\nI'm trying to use an error-handling middleware with Express and TypeScript, but I'm getting a TypeScript error:\n\n```typescript\nconst errorHandler = (err: any, req: Request, res: Response, next: NextFunction) => {\n    res.status(500).json({ error: 'Something went wrong!' });\n};\n\napp.use(errorHandler); // Error occurs here\n\nAnd the Error Message is:-\nsrc/app.ts:30:9 - error TS2769: No overload matches this call.\n  The last overload gave the following error.\n    Argument of type '(err: any, req: Request<ParamsDictionary, any, any, ParsedQs, Record<string, any>>, res: Response<any, Record<string, any>>, next: NextFunction) => Response<...>' is not assignable to parameter of type 'PathParams'.\n\n30 app.use(errorHandler);\n           ~~~~~~~~~~~~\n\n  node_modules/@types/express-serve-static-core/index.d.ts:157:5\n    157     <\n            ~\n    158         P = ParamsDictionary,\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    ...\n    166         ...handlers: Array<RequestHandlerParams<P, ResBody, ReqBody, ReqQuery, LocalsObj>>\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    167     ): T;\n        ~~~~~~~~~\n    The last overload is declared here.\n\n",
      "solution": "> The error suggests that your error handler returns `Response`, but the `errorHandler` above has no return and its return type is `void`.\n> \n> I'm assuming that you are using types for Express 5, which require return type of `void | Promise<void>` for [error] request handlers, the code that you really use is different from your example above and it returns `Response` (e.g. `return res.json({ /* ... */ }`). If that is the case, this comment explains the behaviour: [#5987 (comment)](https://github.com/expressjs/express/issues/5987#issuecomment-2428333462)\n\nIt solved Now Thanks \ud83d\udc4d\n\n---\n\nThanks it solved.\n\n```\n// \u274c Old pattern (causes TS error in express@5 types)\n// \u274c Invalid: returning `Response` in a function expecting `void`\n\nif (!user) return res.status(401).json({ message: 'Unauthorized' });\n```\n\nNow you must write one of:\n```\n// \u2705 Option 1:\n// Separate the statement\n// Separate the response from the return\n\nif (!user) {\n  res.status(401).json({ message: 'Unauthorized' });\n  return;\n}\n\n```\nor\n```\n// \u2705 Option 2: \n// Suppress the return with `void`\n// Use `void` to suppress the return value\n\nif (!user) return void res.status(401).json({ message: 'Unauthorized' });\n```\n\n---\n\nThank a ton sir , same issue I've come up with now ....After removing the return keyword and putting it in next line the issue got solved.",
      "labels": [
        "question",
        "awaiting more info",
        "typescript"
      ],
      "created_at": "2025-02-23T10:47:01Z",
      "closed_at": "2025-03-01T15:05:56Z",
      "url": "https://github.com/expressjs/express/issues/6359",
      "comments_count": 8
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 5205,
      "title": "Express v5 blockers",
      "problem": "_I started writing this as a reply to https://github.com/expressjs/express/issues/4920#issuecomment-1575599117 but the issue was locked just before I finishing typing my comment._\r\n\r\n@dougwilson Thanks for your continued work on express. I'm sorry you've had to deal with reports of bogus security vulnerabilities as well as folks who don't know better assuming that you've abandoned the project. I hope it doesn't get you down. \u2764\ufe0f\r\n\r\nI do have a question re:\r\n> reach out and I would be happy to Zoom with you or similar and we can work on the remaining items together, like see what you want to help with or hear what there is and you can let me know what you can do.\r\n\r\nAre you open to the idea of enumerating the remaining work to be done in public, rather than in private Zoom calls, e.g. on this issue or another one?\r\n\r\nIt may seem like asking folks to get on a Zoom call is not a big ask, but time zones and other factors could make that a nontrivial barrier. Moreover, documenting this in a public issue would eliminate the need for repeating yourself over `N` Zoom calls.\r\n\r\nThe last update said:\r\n> Express 5 is pretty much completed at this point, and we're just finishing up the last code merges in upstream modules in order to bump the dependencies finally in the 5.0 branch.\r\n\r\nIf this is still the case, perhaps you could provide more details about which modules are currently blocked and link to any relevant PRs/issues that are stalled. Even if you'd still prefer to for folks that want to help to first reach out to you, providing this additional detail could help to make those conversations more fruitful.\r\n\r\nAnyway, just wanted to pose the question for you to consider. Once again, thanks for your continued work on express and I hope you are doing well!",
      "solution": "I tried to add error-handling middleware to a web application recently, found out that it doesn't work with async request handlers, and fell down this rabbit hole.\r\n\r\nExpress 5.0's release date has been continually pushed back [since **2015**](https://github.com/expressjs/express/pull/2237#issuecomment-115311279); it seems like people have been told that 5.0's release is just around the corner for the last 8-9 years.\r\n\r\nAre there *any* specific tasks - code, documentation, triaging, etc. - that others can help out with in order to get 5.0 out the door? I understand that you have no specific obligations since this is volunteer work, but it seems like many others are willing to help out with this if they can get an up-to-date roadmap. I'm not sure when the 5.0 milestone or #2237 were last updated, or to what extent they accurately reflect the work left to do.\r\n\r\nIf it's [lack of usage](https://github.com/expressjs/express/issues/4920#issuecomment-1149874852) that's blocking the release, you're going to be waiting forever--very few people want to use beta software in production, and it becomes a catch-22 of not wanting to release until more people have used 5.0, but people not wanting to use 5.0 until it's a stable release.\r\n\r\nIf there's any info on what people can do to help (e.g. what documentation needs to be written, what bugs need to be fixed, what features need to be implemented), I think it'd go a long way towards helping people both understand and help out with 5.0.",
      "labels": [
        "meta"
      ],
      "created_at": "2023-06-04T16:53:07Z",
      "closed_at": "2024-05-13T15:30:26Z",
      "url": "https://github.com/expressjs/express/issues/5205",
      "comments_count": 30
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 6316,
      "title": "Release Plan: 5.1.0",
      "problem": "## Remaining Work\n\n- [ ] https://github.com/expressjs/express/pull/6095\n- [x] https://github.com/expressjs/express/pull/6285\n\n## Dependency work\n\n- [x] accepts\n- [x] body-parser\n  - [x] https://github.com/expressjs/body-parser/pull/578\n  - [x] Update type-is \n    - [x] update mime-types\n      - [x] update mime-db\n- [ ] content-disposition\n  - [ ] https://github.com/jshttp/content-disposition/pulls\n  - [ ] https://github.com/jshttp/content-disposition/issues/47\n  - [ ] https://github.com/jshttp/content-disposition/pull/68\n  - [ ] https://github.com/jshttp/content-disposition/pull/58\n- [x] content-type\n  - [x] `^` range https://github.com/expressjs/express/commit/d2de128a32f1ce3d360bbe3fad56afa026fc8832\n- [x] cookie\n  - [x] `^` range https://github.com/expressjs/express/commit/af7cd90893f4619212e01f271fbaa10f3176fb33\n- [x] debug: \n  - [x] `^` range https://github.com/expressjs/express/commit/85e48bb8c109d2200077bf8d50f3a163488d0fa5\n  - [x] https://github.com/expressjs/express/pull/6313\n- [x] encodeurl\n  - [x] `^` range https://github.com/expressjs/express/commit/af7cd90893f4619212e01f271fbaa10f3176fb33\n- [x] escape-html\n  - [x] `^` range https://github.com/expressjs/express/commit/af7cd90893f4619212e01f271fbaa10f3176fb33\n- [x] etag\n  - [x] `^` range https://github.com/expressjs/express/commit/af7cd90893f4619212e01f271fbaa10f3176fb33\n- [x] finalhandler\n  - [x] https://github.com/expressjs/express/pull/6373\n- [x] fresh\n  - [x] `^` range https://github.com/expressjs/express/commit/af7cd90893f4619212e01f271fbaa10f3176fb33\n- [ ] http-errors\n  - [x] `^` range https://github.com/expressjs/express/commit/af7cd90893f4619212e01f271fbaa10f3176fb33\n  - https://github.com/jshttp/http-errors/pulls\n- [x] merge-descriptors: nothing\n- [x] mime-types\n  - [x] https://github.com/jshttp/mime-db/releases/tag/v1.54.0\n  - [x] Update mime-db to 1.54.0 (https://github.com/jshttp/mime-types/pull/133)\n- [x] on-finished\n  - [x] `^` range https://github.com/expressjs/express/commit/af7cd90893f4619212e01f271fbaa10f3176fb33\n- [x] once\n  -  [x] `^` range https://github.com/expressjs/express/commit/af7cd90893f4619212e01f271fbaa10f3176fb33\n- [x] parseurl\n  - [x] `^` range https://github.com/expressjs/express/commit/af7cd90893f4619212e01f271fbaa10f3176fb33\n- [ ] proxy-addr\n  - [x] `^` range https://github.com/expressjs/express/commit/af7cd90893f4619212e01f271fbaa10f3176fb33\n  - [ ] https://github.com/jshttp/proxy-addr/issues/28 ???\n- [x] qs\n  - [x] https://github.com/expressjs/express/pull/6374\n- [x] range-parser\n  - [x] `^` range https://github.com/expressjs/express/commit/af7cd90893f4619212e01f271fbaa10f3176fb33\n- [ ] router\n  - [ ] release: https://github.com/pillarjs/router/pull/155\n  - [x] https://github.com/expressjs/express/issues/6280\n- [x] send\n  - [x] https://github.com/pillarjs/send/pulls\n  - [x] update mime-types (https://github.com/pillarjs/send/pull/251)\n    - [x] update mime-db\n- [x] serve-static\n  - [x] https://github.com/expressjs/serve-static/pulls\n  - [x] update send \n    - [x] update mime-types\n      - [x] update mime-db\n- [x] statuses\n  - [x] `^` range https://github.com/expressjs/express/commit/af7cd90893f4619212e01f271fbaa10f3176fb33\n- [x] type-is\n- [x] vary\n  - [x] `^` range https://github.com/expressjs/express/commit/af7cd90893f4619212e01f271fbaa10f3176fb33\n\n## Not planned for this release\n\n- [ ] cookie-signature\n  - [ ] https://github.com/tj/node-cookie-signature/issues/36",
      "solution": "> Do you have some node issues I can link to when we talk about this? It is near impossible to get folks to upgrade even if it is `latest` but we can try with good messaging.\n\n@wesleytodd Having express as a dependency of a passport strategy - it is a real struggle to be able to determine the request's original href without both replicating express' internals and changing the resolution code based on the app's `trust proxy` setting. This is stemming from the fact that `req.host` in v4 is the hostname and not the host and there's no host getter that would take `trust proxy` into consideration.\n\nrefs: https://github.com/panva/openid-client/issues/767, https://github.com/panva/openid-client/issues/743, https://github.com/panva/openid-client/issues/733, https://github.com/panva/openid-client/issues/746, https://github.com/panva/openid-client/pull/713, https://github.com/panva/openid-client/discussions/714\n\nv5 behaves as expected but unfortunately in v4 the code doesn't work for development on localhost with arbitrary ports, and in production with non http scheme default ports (i.e. other than 80 and 443).\n\nI've resisted the pressure of accomodating the bugged v4 behaviour and just explained to users they need to either upgrade to v5 or overload the method that returns the current URL in development setups. In v4 the `req.host` should have been fixed instead of being deprecated and console warned upon use in the first place. I get a few issues opened every now and then because of it.\n\n> It is near impossible to get folks to upgrade even if it is latest but we can try with good messaging.\n\nI get that. But at least having latest be v5 means new apps will default to v5 with `npm i express` and the transition to v5 can begin for real in userland.",
      "labels": [
        "5.x"
      ],
      "created_at": "2025-02-03T21:52:04Z",
      "closed_at": "2025-03-28T01:23:13Z",
      "url": "https://github.com/expressjs/express/issues/6316",
      "comments_count": 14
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 6485,
      "title": "Missing next() in Middleware Causes Request to Get Stuck",
      "problem": "---\nname: Missing `next()` in Middleware Causes Request to Get Stuck\nabout: Report an issue where missing `next()` in middleware causes requests to get stuck.\n---\n\n## Description\nIn Express, if the `next()` function is not called in a middleware, the request will get stuck because Express doesn't know how to proceed to the next middleware or route handler. This can lead to issues where requests hang indefinitely, as no response is sent to the client.\n\n## Steps to Reproduce:\n1. Create a middleware function that does not call `next()` or send a response.\n2. Use the middleware in an Express app.\n3. Make a request to the server.\n4. Observe that the request is stuck, and no further middleware or route handlers are executed.\n\n### Example Code:\n```javascript\napp.use((req, res) => {\n  // Middleware without `next()`\n  console.log('Middleware without next()');\n  // No response is sent, and no next() is called, causing the request to get stuck.\n});\n",
      "solution": "I totally agree \u2014 enforcing return next() would break many existing apps, and Express's flexibility is a key part of its design.\n\nThat said, in large apps or teams, it's easy to forget to call next() or res.end(), which can lead to hanging requests without any obvious errors. I'm wondering if there's a way we could catch these scenarios more easily during development \u2014 maybe through optional tooling or patterns that warn when a request remains unresolved for too long.\n\nNot proposing a breaking change to core \u2014 more like: how can we help developers detect these issues early without disrupting the current ecosystem?\n\nCurious what your thoughts are on this, or if this has been discussed before?\n\n---\n\nIMO this isn't just isolated to this issue too, we also have the error where data is still trying to be sent once res.end() has been called. \n\nI don't see any clear way to get be able to enforce that res.end() or next() be called, other than by adding timeouts which trigger an error if they haven't been called yet. In terms of static analysis, while it may be possible to force `next()` to be called inside the middleware, this ignores cases where the function is passed as a callback to other functions.\n\n(imo, a good solution is to create `express/promise`, and creating adapters to use the existing middleware ecosystem. but this is a massive project in and of itself)\n\nI'm not sure if this has been discussed before though, hopefully other people can chime in here.",
      "labels": [],
      "created_at": "2025-04-28T09:37:50Z",
      "closed_at": "2025-04-30T15:16:00Z",
      "url": "https://github.com/expressjs/express/issues/6485",
      "comments_count": 4
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 5944,
      "title": "Express v4 -> v5 Migration",
      "problem": "<!-- We can't debug your app for you, but you can ask questions and we will try to answer them.\r\n\r\nYou can also ask a question in the Node.js Slack community's #express-js channel:\r\n\r\n* Slack invite form: http://www.nodeslackers.com/invite\r\n* #express-js channel: https://node-js.slack.com/archives/C0L827VV4\r\n\r\nIt is super important that you paste in samples of your code (no screenshots of code, [use markdown](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks))!\r\nWithout seeing what your code looks like, we won't be able to help you very much.\r\nMore is better when it comes to sharing code samples if you're having a problem. -->\r\n\r\nCopied from [my comment](https://github.com/expressjs/express/pull/2237#issuecomment-2340006831) in the [Release 5.0 PR](https://github.com/expressjs/express/pull/2237):\r\n\r\nMaybe I missed a v5 planning document somewhere - I'm looking for a few details to help determine how and when to migrate our projects:\r\n\r\n1. When is it planned that `express@5.0.0` be published to the npm `latest` tag?\r\n2. Is there a plan to publish an Express v4 -> v5 Migration Guide? (or a visible list of breaking changes with short suggestions about what to do about them) The closest thing that I can see is [this small `breaking` note in `History.md`](https://github.com/expressjs/express/blob/5.x/History.md#500--2024-09-10:~:text=path.isAbsolute%20instead-,breaking%3A,-res.status())\r\n3. What's the current plan for the TypeScript types in DefinitelyTyped [`@types/express`](https://www.npmjs.com/package/@types/express) and [`@types/express-serve-static-core`](https://www.npmjs.com/package/@types/express-serve-static-core)? Does anything need to be changed here? Specifically, things like https://github.com/DefinitelyTyped/DefinitelyTyped/pull/69660#issuecomment-2179358937 cc DT maintainers @borisyankov @samijaber @NatoBoram @bombillazo @AndrewLeedham @gabritto @sandersn\r\n",
      "solution": "The author of the [original PR to DefinitelyTyped](https://github.com/DefinitelyTyped/DefinitelyTyped/pull/69846) is sick.\r\nI began my own approach [here](https://github.com/DefinitelyTyped/DefinitelyTyped/pull/70563), but it's promising to be massive: dozens of packages have internal references to express' types, declaring `*` compatibility, having discrepancies in tests, such as returning something where `void` was expected. That did actually work with `void` before, but no longer works with `void | Promise<void>` and that is the issue, but not the only one \u2014 so that each package failing tests seems to be requiring an individual handling to fix it.\r\n\n\n---\n\nOK. I fixed all issues in my PR to types of Express 5 \u2014 https://github.com/DefinitelyTyped/DefinitelyTyped/pull/70563\r\nI welcome advises and suggestions on how to make it better and release faster.\n\n---\n\n> I have not\r\n> I would\r\n> I could\r\n\r\nYou know, in _my_ system of values getting things done is more important and cherished rather than \r\n\r\n- ruminating for 10 years\r\n  - on how to arrange the seats \r\n    - for a potential opportunity\r\n      - to begin negotiations\r\n        - on a proposed cooperation\r\n          - in matters of shared responsibility \r\n            - towards finding a suggested solution\r\n              - to **_the elementary problem_**.\r\n\r\nSo, [I have done it](https://github.com/DefinitelyTyped/DefinitelyTyped/pull/70563).\r\nIf you're having your own solution _in mind_, which _perhaps_ you believe _could be_ better than mine \u2014 don't hesitate and do it @jonkoops ",
      "labels": [
        "5.x",
        "question"
      ],
      "created_at": "2024-09-10T16:53:54Z",
      "closed_at": "2025-04-01T15:52:55Z",
      "url": "https://github.com/expressjs/express/issues/5944",
      "comments_count": 28
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 6467,
      "title": "Github tag naming",
      "problem": "I noticed that with the release of 5.1.0 the github tag was labeled `v5.1.0` which is different from how version 4 was tagged.  Also there isn't any real consistency in the version labels for version 5.\n\nWe have nightly jobs that get the latest version number from npm and then use that to pull this repo so we can test it on our platforms.  It has been recently failing because of the labelling.  I'm fine with updating our code to use the `v` prefix, i just wanted to know if it will be consistent in the future?\n",
      "solution": "I just pushed the missing `v5.0.1` tag, deleted the incorrect `5.0.1` tag, and updated the release to use the correct tag. We will be automating this sometime this year and until then we can make sure the `npm version` tag format it kept consistent. Sorry for the issue.\n\nEDIT: I went and read that thread in slack and while there I said maybe we can use both, I was sure there was a prior conversation about this where we decided we want to use the normal version. I can't find it, but since this is a rather minor issue and one soon to be resolved by the automation, I don't think we need more thorough docs on this. If someone thinks we do, feel free to either re-open this or open a follow up issue in the discussions repo.",
      "labels": [
        "question",
        "tc agenda",
        "top priority"
      ],
      "created_at": "2025-04-18T14:46:44Z",
      "closed_at": "2025-04-20T14:32:31Z",
      "url": "https://github.com/expressjs/express/issues/6467",
      "comments_count": 6
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 5610,
      "title": "Using colon as a character instead of path parameter (express 5)",
      "problem": "### Discussed in https://github.com/expressjs/express/discussions/5609\r\n\r\n<div type='discussions-op-text'>\r\n\r\n<sup>Originally posted by **Tanavya** April 18, 2024</sup>\r\nI am trying to follow google's API design guide here for custom methods https://cloud.google.com/apis/design/custom_methods. I already looked through the discussion here https://github.com/expressjs/express/issues/3857 but it did not solve my problem.\r\n\r\nOn express version 5.0.0-beta.3.\r\n \r\nThis works\r\n```\r\nrouter.get('/tasks/:id/([:])action', async (req, res, _next) => {\r\n    const id = req.params[\"id\"];\r\n    return res.status(200).send(id);\r\n  });\r\n```\r\n\r\nSo I can do a simple GET on `tasks/50/:action`\r\n\r\nHowever, the moment I remove the `/` it breaks.\r\n\r\n```\r\nrouter.get('/tasks/:id([:])action', async (req, res, _next) => {\r\n    const id = req.params[\"id\"];\r\n    return res.status(200).send(id);\r\n});\r\n```\r\nGET /tasks/50:action -> not found.\r\n\r\nHow do I solve this?</div>",
      "solution": "> Oh interestingly, after some hit and trial I figured it out. Looks like this works:\n> \n> ```\n> router.get(\"/tasks/:id\\\\:action\", async (req, res, _next) => {\n>   const id = req.params[\"id\"];\n>   return res.status(200).send(id);\n> });\n> ```\n> \n> Now `GET /tasks/50:action` returns 200 Definitely documentation can be improved for this though.\n\nThank you! This does in fact solve the issue. ",
      "labels": [],
      "created_at": "2024-04-18T23:21:14Z",
      "closed_at": "2024-04-18T23:44:56Z",
      "url": "https://github.com/expressjs/express/issues/5610",
      "comments_count": 2
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 5347,
      "title": "HTTP2 insecure server crashes every time.",
      "problem": "I am trying to write an insecure HTTP2 web server. However it crashes on any inputs.\r\n\r\nI need an insecure HTTP2 server. Here's my code:\r\n\r\n```typescript\r\nimport { createServer } from 'node:http2';\r\n\r\nimport createExpressApp from 'express';\r\nimport http2ExpressBridge from 'http2-express-bridge';\r\n\r\n// Test with: curl --insecure -v -k --http2-prior-knowledge \"http://[::]:8080\"\r\nfunction main(): void {\r\n  const app = http2ExpressBridge(createExpressApp);\r\n\r\n  app.get('*', (request, response) => {\r\n    response.setHeader('Content-Type', 'text/plain');\r\n    response.status(200);\r\n    response.write('Server reached');\r\n    response.end();\r\n  });\r\n\r\n  const server = createServer(app);\r\n\r\n  server.listen({ port: 8080 });\r\n\r\n  // Triggered by CMD/CTRL + C .\r\n  process.on('SIGINT', () => {\r\n    server.close();\r\n    process.exit(0);\r\n  });\r\n}\r\n\r\ntry {\r\n  main();\r\n} catch (error) {\r\n  console.error(error);\r\n  process.exit(1);\r\n}\r\n```\r\n\r\nI'm testing using ` curl --insecure -v -k --http2-prior-knowledge \"http://[::]:8080\"`\r\n\r\nEvery time I execute it, the server crashes with:\r\n\r\n```text\r\n> tsx src/example/http2BugRepro.ts\r\n\r\nnode:events:492\r\n      throw er; // Unhandled 'error' event\r\n      ^\r\n\r\nTypeError: Cannot read properties of undefined (reading 'readable')\r\n    at IncomingMessage._read (node:_http_incoming:214:19)\r\n    at Readable.read (node:internal/streams/readable:547:12)\r\n    at resume_ (node:internal/streams/readable:1048:12)\r\n    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)\r\nEmitted 'error' event on IncomingMessage instance at:\r\n    at emitErrorNT (node:internal/streams/destroy:151:8)\r\n    at errorOrDestroy (node:internal/streams/destroy:214:7)\r\n    at Readable.read (node:internal/streams/readable:549:7)\r\n    at resume_ (node:internal/streams/readable:1048:12)\r\n    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)\r\n```\r\n\r\nWhen I add an SSL certificate and use `createSecureServer` everything works fine. But when I use `createServer` it crashes. \r\n\r\nI think it's a bug in `express` or `http2-express-bridge` because without specifying the app, and handling the stream directly produces no errors.\r\n\r\nMy info:\r\n\r\n```\r\nOS: MacOS\r\nNode: 20.9.0\r\nExpress: I tried v5.0.0-beta and v4.18.2\r\nhttp2-express-bridge: 1.0.7\r\n```\r\n\r\nThank you.",
      "solution": "I will suggest to open an issue on http2-express-bridge, It seems that the issue might be related to the library.\n\n---\n\nYeah, well, a startup I consulted for ran out of cash in December and I need a job :)\r\n\r\nBut no, I also want to see this happen.  The last company I worked at informally sponsored Matteo you could say to continue working on Fastify.  I could go to them, but this seems like it would stir some pots.  If not them, a similar organization.\r\n\r\nI am sitting on a large monorepository solution without a safe way to go to market without getting ripped off by Vercel, the powers and IP thiefs at large.  This seems like a reasonable way to gain some notoriety and while I seriously would just sit down for 3-6 months and do this, I burned out my bank account.\r\n\r\nI authored hot reloading in express without doing a server reload on top, and helping with HTTP2 could build trust to push this and similar changes into Express v5 as a PR(s).  I do more stuff that allows route repositioning and declarative route ordering across a monorepository (where construction of route order is not always known in advance).  Basically I have done a lot of additional work to make route positioning more dynamic that might be interesting to propose into Express.\r\n\r\nThis is me, inventing a job for myself.  Scratching at the food line.  This is tech.",
      "labels": [
        "awaiting more info",
        "http2"
      ],
      "created_at": "2023-12-08T03:04:59Z",
      "closed_at": "2025-03-28T19:41:18Z",
      "url": "https://github.com/expressjs/express/issues/5347",
      "comments_count": 10
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 6401,
      "title": "Operational issues with express dependencies.",
      "problem": "Hi Team,\n\nI wanted to point out that Black Duck is flagging operational vulnerabilities with express. Please update the dependency packages to the latest versions. Refer the attached PDF.\n\nAny updates or plans for future development would be greatly appreciated.\n\nThanks",
      "solution": "I can't transfer the issue to https://github.com/jshttp/accepts as it is in another org (cc: @expressjs/triagers )\n\n---\n\n> Seems like only express/accepts is related to \"express\" but from this report is unclear what are the \"operational vulnerabilities\". Can you provide more information?\n> \n> ![Image](https://github.com/user-attachments/assets/e4050947-a4b1-4de8-91ad-4a4da461ae01)\n\nThanks for the response. \nWe are getting issues since there is not active development for the express/accepts. We will raise the issue there",
      "labels": [
        "awaiting more info"
      ],
      "created_at": "2025-03-18T06:24:18Z",
      "closed_at": "2025-03-27T23:59:44Z",
      "url": "https://github.com/expressjs/express/issues/6401",
      "comments_count": 6
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 6406,
      "title": "vulnerability report (severity: 7.5)",
      "problem": "# Dependency Hierarchy:\n\nexpress-1.1.1.tgz (Root Library)\n  multer-1.4.4.tgz\n    busboy-0.2.14.tgz\n      \u274c dicer-0.2.5.tgz (Vulnerable Library)\n\n# Vulnerability Details\n\nThis affects all versions of package dicer. A malicious attacker can send a modified form to server, and crash the nodejs service. An attacker could sent the payload again and again so that the service continuously crashes.\n\nPublish Date: 2022-05-20\n",
      "solution": "> This is referring to express 1.1.1 which is arguably unsupported?\n\nI can't even find 1.1.1 version of Express on NPM or in tags. I don't think any Express version has a direct dependency on multer.\n\nAs for the issue in multer see https://github.com/expressjs/multer/issues/1122 (especially the last comment).\n\n---\n\nExpress 1.x is currently unsupported since March 2011 ([ref](https://expressjs.com/en/support/)). The only options I can see in this scenario are:\n- Upgrade to Express@4 or Express@5\n- Use a commercial solution: [HeroDevs Never-Ending Support for Express](https://www.herodevs.com/support/express-nes?utm_source=expressjs&utm_medium=link&utm_campaign=express_eol_page)",
      "labels": [
        "bug"
      ],
      "created_at": "2025-03-19T09:20:42Z",
      "closed_at": "2025-03-19T12:13:26Z",
      "url": "https://github.com/expressjs/express/issues/6406",
      "comments_count": 3
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 4851,
      "title": "`res.clearCookie()` does not ignore `maxAge`",
      "problem": "Hi everyone!\r\nI just ran into a bug, where `res.clearCookie()` does not work properly.\r\n\r\n## What happen?\r\nAccording to the typescript definitions, `res.clearCookie()` accepts `CookieOptions` as a second parameter (see [here](https://github.com/DefinitelyTyped/DefinitelyTyped/blob/23e0b071471f0120d7c03c5abf206f41e832133b/types/express-serve-static-core/index.d.ts#L920)) which includes the `maxAge` attribute. But if the `maxAge` is set, the cookie won't be deleted.\r\n\r\n## What do I expect?\r\n`.clearCookie()`should ignore (or delete) the `maxAge` attribute, because it is used to calculate the `expire` attribute afterwards in `.cookie()`;\r\n\r\n## Research\r\nI already located the bug and would like to provide a pr to fix this.\r\n",
      "solution": "Since #4252 is closed, I'll continue the discussion here:\r\n\r\nAs all the other options (domain, sameSite, etc) needs to be the same when clearing the cookie as when setting it, the natural thing to do is use the same `const OPTIONS` when clearing the current cookie as when setting it.  Anything that depends on the current behaviour is obviously broken.  If a new major is needed to fix this, then a new major is needed asap.\r\n\r\nBig thanks to @tjarbo for identifying the problem and provding a PR!",
      "labels": [
        "5.x"
      ],
      "created_at": "2022-03-08T15:58:58Z",
      "closed_at": "2025-03-19T01:56:24Z",
      "url": "https://github.com/expressjs/express/issues/4851",
      "comments_count": 9
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 5567,
      "title": "mime.charsets in Response.js is undefined",
      "problem": "Using Express version 4.19.2 in Response.js at line 790 (var charset = mime.charsets.lookup(value.split(';')[0]);) is throwing an exception stating that mime.charset \"TypeError: Cannot read properties of undefined (reading 'charsets')\"\r\n",
      "solution": "It was a versioning issue! :-( What a nightmare!\r\n\r\nPlease consider the issue closed. Thank you for your help!\n\n---\n\nHey @robertlario any chance you remember the solution to this? I'm currently debugging and suspected a versioning issue as well! Just trying to find the right thing to update. ",
      "labels": [
        "bug"
      ],
      "created_at": "2024-03-27T16:42:40Z",
      "closed_at": "2024-03-27T19:40:03Z",
      "url": "https://github.com/expressjs/express/issues/5567",
      "comments_count": 9
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 4618,
      "title": "No overload matches this call -> use(express.json()) ? ",
      "problem": "app/server.ts:53:8 - error TS2769: No overload matches this call.\r\nThe last overload gave the following error.\r\nArgument of type 'NextHandleFunction' is not assignable to parameter of type 'PathParams'.\r\nType 'NextHandleFunction' is not assignable to type '(string | RegExp)[]'.\r\n\r\n53 .use(express.json())\r\n\r\nconst app = express();\r\napp\r\n.set(\"port\", port)\r\n.set(\"trust proxy\", 2)\r\n.disable(\"x-powered-by\")\r\n.use(requireHttps())\r\n.use(i18n.init)\r\n.use(express.json())",
      "solution": "> Resolved itself after a new npm upgrade.\r\n\r\nI am experiencing the same issue. How did you fix it? The error happened while doing a codebuild on aws.\n\n---\n\nI'm experiencing the same issue with the following installed:\r\n\r\n```json\r\n\"express\": \"^4.18.2\",\r\n\"@types/express\": \"^5.0.0\"\r\n```\r\n\r\nFor the moment, I'm using type assertions to silence the TypeScript compile-time errors in VS Code. \r\n\r\n```ts\r\nimport express, { type Request, type Response, type NextFunction } from \"express\";\r\n\r\napp.use(((req: Request, res: Response, next: NextFunction) => {\r\n  res.header(\"Access-Control-Allow-Origin\", \"*\");\r\n  res.header(\"Access-Control-Allow-Headers\", \"Content-Type\");\r\n  res.header(\"Access-Control-Allow-Methods\", \"GET, OPTIONS\");\r\n  if (req.method === \"OPTIONS\") {\r\n    return res.sendStatus(204);\r\n  }\r\n  next();\r\n}) as express.RequestHandler);\r\n```\r\n\r\nUsing type assertions is a last resort for me, so hopefully this is fixed sooner than later.\r\n\r\n\n\n---\n\n@NazCodeland bro I later fixed it by using \r\n\r\n`\"typescript\": \"5.1.6\"`\r\n\r\n```json\r\n\"devDependencies\": {\r\n    \"@types/express\": \"^4.17.21\",\r\n    \"@types/node\": \"^22.7.5\",\r\n    \"ts-node\": \"^10.9.2\",\r\n    \"typescript\": \"5.1.6\"\r\n  }\r\n```",
      "labels": [
        "invalid"
      ],
      "created_at": "2021-06-22T13:05:26Z",
      "closed_at": "2021-06-22T13:29:43Z",
      "url": "https://github.com/expressjs/express/issues/4618",
      "comments_count": 18
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 5987,
      "title": "Version 4.21.0 and older now pulls in types for 5.0.0 that are incompatible.",
      "problem": "Version 4.21.0 and older now pulls in types for 5.0.0 that are incompatible.\r\n\r\n\r\n```\r\n\r\n     \"node_modules/@types/express\": {\r\n       \"version\": \"5.0.0\",\r\n       \"resolved\": \"https://registry.npmjs.org/@types/express/-/express-5.0.0.tgz\",\r\n       \"integrity\": \"sha512-DvZriSMehGHL1ZNLzi6MidnsDhUZM/x2pRdDIKdwbUNqqwHxMlRdkxtn6/EPKyqKpHqTl/4nRZsRNLpZxZRpPQ==\",\r\n      \"license\": \"MIT\",\r\n       \"dependencies\": {\r\n         \"@types/body-parser\": \"*\",\r\n         \"@types/express-serve-static-core\": \"^5.0.0\",\r\n         \"@types/qs\": \"*\",\r\n         \"@types/serve-static\": \"*\"\r\n       }\r\n     },\r\n```\r\n\r\n\r\n```\r\napp/server.ts:31:11 - error TS2769: No overload matches this call.\r\n  The last overload gave the following error.\r\n    Argument of type '(err: any, req: Request<ParamsDictionary, any, any, ParsedQs, Record<string, any>>, res: Response<any, Record<string, any>>, next: NextFunction) => void | Response<...>' is not assignable to parameter of type 'PathParams'.\r\n\r\n31   app.use(ErrorHandler);\r\n             ~~~~~~~~~~~~\r\n\r\n  node_modules/@types/express-serve-static-core/index.d.ts:153:5\r\n    153     <\r\n            ~\r\n    154         P = ParamsDictionary,\r\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n    ...\r\n    162         ...handlers: Array<RequestHandlerParams<P, ResBody, ReqBody, ReqQuery, LocalsObj>>\r\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n    163     ): T;\r\n        ~~~~~~~~~\r\n    The last overload is declared here.\r\n```\r\n \r\n\r\nAdding the types/express 4.17.21 fixes the issue but it was not needed until today",
      "solution": "Not sure why but the v5 types are pushed to v4.17.21 causing a lot of linting errors for me\r\n\r\nI have to pin types back to 4.17.20 for now:\r\n```\r\n    \"resolutions\": {\r\n        \"@types/express\": \"4.17.21\"\r\n    }\r\n```\n\n---\n\nI'm with the same issue. I would like to ask if will fix it?\r\n\r\nBy the way for who are using npm this could be a solution:\r\n\r\n`\r\n\"overrides\": {\r\n    \"@types/express\": \"4.17.20\"\r\n  }\r\n`\r\n\r\notherwise can use the @Trinovantes solution for yarn\r\n\r\n\n\n---\n\n@JimmyBjorklund what version range did you have for your `@types/express` in package.json before?\r\n\r\nPlease follow jakebailey's steps and report back.\r\n\r\n> Adding the types/express 4.17.21 fixes the issue but it was not needed until today\r\n\r\nThis would always have been necessary to guard against pulling a new major's types when they were published. The difference today is that there is a new major's types on DT",
      "labels": [
        "bug"
      ],
      "created_at": "2024-09-26T06:44:59Z",
      "closed_at": "2024-10-22T11:45:45Z",
      "url": "https://github.com/expressjs/express/issues/5987",
      "comments_count": 22
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 6334,
      "title": "Can not detect cancelled request in post handler when using express.json() middleware",
      "problem": "<!-- The process for bug fixing is:\n\n- We will first assess if the behavior is different from what should occur\n- Confirm the bug is reproducible\n- Discuss how to best fix the bug\n- Work towards a fix\n-->\n\n## Environment information\n\n**Version**:\n```json\n\"dependencies\": {\n  \"express\": \"4.21.2\"\n}\n```\n\n**Platform**:\n```\n$ uname -a\nLinux Corona 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux\n```\n\n**Node.js version**:\n```\n$ node --version\nv20.17.0\n```\n\n\n## What steps will reproduce the bug?\n\nThe bug is that I can not detect if the request is cancelled when posting json. It might not be a documentation bug - either way, I couldn't get it working.\n\nI have tried listening to the `abort` event on the request without success. As I understand it the events go hand-in-hand with node's http request, and this (https://nodejs.org/api/http.html#event-abort) suggests trying with a `destroyed` event. I didn't get that to work either.\n\nWhen searching for a solution, the most common recommendation is to listen to the `close` event, although that is not intuitive.\n\n```javascript\nconst express = require(\"express\");\nconst app = express();\nconst port = 3000;\n\napp.use(express.json());\n\napp.post(\"/sleep\", async (req, res) => {\n  console.log(\"Received JSON:\", req.body);\n\n  const timeoutPromise = new Promise((resolve, reject) => {\n    const timeoutId = setTimeout(() => {\n      resolve(\"Done sleeping\");\n    }, 1000);\n\n    req.on(\"close\", () => {\n      clearTimeout(timeoutId);\n      reject(new Error(\"Request aborted\"));\n    });\n  });\n\n  try {\n    await timeoutPromise;\n    console.log(\"Request ok!\");\n    res.status(200).send(\"ok\");\n  } catch (error) {\n    console.log(\"Request was ABORTED!\");\n    res.status(500).send(\"aborted\");\n  }\n});\n\napp.listen(port, () => {\n  console.log(`Server listening at http://localhost:${port}`);\n});\n```\n\n## Test using curl\n\n* As expected \u2705 \n```\n$ curl -X POST http://localhost:3000/sleep\nok\n\n// Backend output\nReceived JSON: {}\nRequest was ok!\n```\n\n* As expected too \u2705 \nThe argument `-m 0.5` has expected effect.\n```\n$ curl -X POST http://localhost:3000/sleep -m 0.5\ncurl: (28) Operation timed out after 501 milliseconds with 0 bytes received\n\n// Backend output\nReceived JSON: {}\nRequest was ABORTED!\n```\n\n* Also expected \u2705 \nSending json with a timeout.\n```\n$ curl -X POST http://localhost:3000/sleep -H \"Content-Type: application/json\" -d '{\"value\": 42}' -m 0.5\naborted\n\n// Backend output\nReceived JSON: { value: 42 }\nRequest was ABORTED!\n```\n\n* Not expected \u274c \nNo timeout used. It's as if the json middleware reads the request to the end to be able to parse the body, after which the `close` event fires.\n```\n$ curl -X POST http://localhost:3000/sleep -H \"Content-Type: application/json\" -d '{\"value\": 42}'\naborted\n\n// Backend output\nReceived JSON: { value: 42 }\nRequest was ABORTED!\n```",
      "solution": "Thank you for confirming this and providing the reasonable solution.\n\nI though I tried the `close` event on the response object, but must have concluded it didn't work in my development environment, running through a `vite proxy`. It works when bypassing the proxy.\n\nThanks again!",
      "labels": [],
      "created_at": "2025-02-11T10:27:59Z",
      "closed_at": "2025-02-19T18:36:21Z",
      "url": "https://github.com/expressjs/express/issues/6334",
      "comments_count": 2
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 6216,
      "title": "Unpatched `path-to-regexp` ReDoS in 0.1.x (CVE-2024-52798, CVE-2024-45296)",
      "problem": "[CVE-2024-52798](https://github.com/pillarjs/path-to-regexp/security/advisories/GHSA-rhx6-c78j-4q9w)\r\n[CVE-2024-45296](https://github.com/advisories/GHSA-9wv6-86v2-598j)\r\n\r\nA new vulnerability has appeared in a dependent package. \r\nWhen forcibly updating to version 0.1.12, an error occurs: TypeError: pathRegexp is not a function.\r\n",
      "solution": "It does not appear a new issue has been opened. If that is something you would like to do to work toward a resolution please do @fiftyy.",
      "labels": [
        "awaiting more info"
      ],
      "created_at": "2024-12-09T07:44:01Z",
      "closed_at": "2025-01-14T19:58:33Z",
      "url": "https://github.com/expressjs/express/issues/6216",
      "comments_count": 12
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 6292,
      "title": "XSS in examples",
      "problem": "I have found a potential Cross-Site Scripting (XSS) vulnerability in your codebase located at /examples/auth/index.js. In the following code, user input is directly rendered into the page without proper sanitization:\n\n```\napp.use(function(req, res, next){\n    var err = req.session.error;\n    var msg = req.session.success;\n    if (err) res.locals.message = '<p>' + err + '</p>';  // Potential XSS\n    if (msg) res.locals.message = '<p>' + msg + '</p>';\n    next();\n});\n```\nRisks:\nThis exposes the application to XSS attacks, where attackers can inject malicious scripts.\nIt can lead to script injection and potentially allow attackers to steal user data, perform actions on behalf of the user, or hijack user sessions.\nSuggested Fix:\nUse libraries like DOMPurify to sanitize user input before rendering it to prevent XSS attacks.\nEncode or escape text before displaying it to avoid executing embedded scripts.\nConsider using safe HTML templating engines that automatically handle escaping, such as Handlebars or Pug.\nImplementing these measures will significantly enhance the security of the application and protect against XSS vulnerabilities.\n\nThank you for your attention to this matter.\n\nBest Regards, @ZeroXJacks",
      "solution": "Thank you for your responses and clarification.\n\nTo ensure proper handling of potential security vulnerabilities, could you kindly guide me on how to responsibly report security issues directly to the Express.js security team? I would like to follow the correct process to ensure the issue is addressed securely and in accordance with the project's security policies.\n\nAs a reminder, I understand this particular issue exists in the example code and not in the core framework itself, but it may still pose a security risk for developers who use the example without applying proper security measures.",
      "labels": [
        "bug"
      ],
      "created_at": "2025-01-23T17:01:38Z",
      "closed_at": "2025-01-24T14:50:57Z",
      "url": "https://github.com/expressjs/express/issues/6292",
      "comments_count": 5
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 4807,
      "title": "[suggestion] send/respond with blob",
      "problem": "NodeJS now has support for Blob's globally, \r\n...earlier you had to load it from `require('buffer').Blob`\r\n\r\nit would be cool / awesome if it where possible to respond with a Blob by doing something like\r\n\r\n```js\r\nconst str = `<h1>Hello World</h1>`\r\nconst blob = new Blob([str], { type: 'text/html' })\r\nconst file = new File([blob], 'index.html', { type: 'text/html' })\r\n\r\napp.get('/', (req, res) => {\r\n  res.send(blob) // or:\r\n  res.download(blob, 'name.html')\r\n  res.download(file) // name taken from file instead\r\n})\r\n```\r\n\r\nDoing this would take care of \r\n1. Setting the response header `content-type` to the blob's type (only if content-type haven't been set manually)\r\n2. Setting the response header `content-length` to the blob's size\r\n3. and pipe the data from `blob.stream()` to the response\r\n4. if you used `res.download(blob)` then it would also add content-disposition attachment header\r\n\r\n<s>it's also looking like if node will at some point add a way of getting blobs from the filesystem also, but i don't know when.\r\nref: https://github.com/nodejs/node/issues/39015</s>\r\n\r\nedit: NodeJS just shipped `fs.openAsBlob(path, { type })` in v20+\r\n",
      "solution": "@dougwilson It would be convenient to extend this to UInt8Array in general. Currently Express's v5 `res.send(uint8ArrayData)` converts binary data into json(!!) even though the header contains a Content-Type other than `application/json` which is obviously a bug. Any `UInt8Array` or its derivation should be treated as binary data, and no coercion should occur without presence of appropriate content-type or user's request to do so. Current workaround is to do `Buffer.from(uint8ArrayData)` before passing to send method.",
      "labels": [
        "ideas"
      ],
      "created_at": "2022-02-04T12:17:45Z",
      "closed_at": "2025-01-20T16:16:45Z",
      "url": "https://github.com/expressjs/express/issues/4807",
      "comments_count": 10
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 6227,
      "title": "Express.js Module Resolution Errors: Missing Internal Modules Not Utilized in Project",
      "problem": "Hello Express Community,\r\n\r\nI've been encountering persistent issues with my Express.js application that I haven't been able to resolve despite numerous troubleshooting attempts. Below are the details of the problems and the steps I've taken so far.\r\n\r\n\r\nEnvironment:\r\n\r\nNode.js Version: v22.12.0\r\nExpress.js Version: latest\r\nOperating System: Windows 11\r\n\r\n\r\nIssues Encountered:\r\n\r\n1: Error When Running node index.js:\r\n\r\n```\r\nnode:internal/modules/cjs/loader:1252\r\n  throw err;\r\n  ^\r\n\r\nError: Cannot find module './middleware/query'\r\nRequire stack:\r\n- ...\\node_modules\\express\\lib\\application.js\r\n- ...\\node_modules\\express\\lib\\express.js\r\n- ...\\node_modules\\express\\index.js\r\n- ...\\index.js       \r\n    at Function._resolveFilename (node:internal/modules/cjs/loader:1249:15) \r\n    ...\r\n  code: 'MODULE_NOT_FOUND',\r\n  requireStack: [\r\n    '...\\node_modules\\express\\lib\\application.js',\r\n    '...\\node_modules\\express\\lib\\express.js',\r\n    '...\\node_modules\\express\\index.js',\r\n    '...\\index.js'\r\n  ]\r\n}\r\n\r\nNode.js v22.12.0\r\n```\r\n\r\n\r\n\r\n2: Error When Using Nodemon (npm start):\r\n\r\n```\r\nError: Cannot find module '...\\node_modules\\nodemon\\node_modules\\brace-expansion\\index.js'. Please verify that the package.json has a valid \"main\" entry\r\n    at tryPackage (node:internal/modules/cjs/loader:494:19)\r\n    ...\r\n  code: 'MODULE_NOT_FOUND',\r\n  path: '...\\\\brace-expansion\\\\package.json',\r\n  requestPath: 'brace-expansion'\r\n}\r\n\r\nNode.js v22.12.0\r\n```\r\n\r\n\r\n\r\nSteps Taken to Resolve:\r\n\r\nReinstalled node_modules: Deleted the node_modules folder and ran npm install multiple times.\r\n\r\nReinstalled Node.js: Completely removed Node.js and installed the latest version.\r\n\r\nTried Alternative Package Managers: Switched between npm, Yarn, and pnpm without success.\r\n\r\nTested with Other Frameworks: Created simple boilerplate projects using Fastify and NestJS, which run without issues.\r\n\r\nUsed Different Boilerplates: Even with a fresh Express boilerplate, the same errors persist.\r\n\r\n\r\n\r\nAdditional Information:\r\n\r\nThe issue persists even with a fresh Express setup.\r\n\r\nOther Node.js frameworks like Fastify and NestJS work flawlessly on the same environment.\r\n\r\nSuspect it might be related to Express's internal modules or compatibility with the current Node.js version.\r\n.\r\n\r\n\r\nRequest for Assistance:\r\n\r\nHas anyone encountered similar issues with Express.js, particularly related to missing internal modules like ./middleware/query or brace-expansion? Any insights or suggestions on how to resolve these module resolution errors would be greatly appreciated.\r\n\r\nThank you in advance for your help!",
      "solution": "Just to confirm, you're using Express with ES modules, in a .js file, and in the package.json you don't have the type set to module?\n\n```json\n{ //package.json\n  \"type\": \"module\"\n}\n```\n\nnote: As far as I know, in the latest version of Node.js 22.12, where CJ modules can be resolved in ES modules, there are issues with libraries that use monkey patching. This will be fixed in the next version of Node.js. I'm on my phone right now, so I can provide links for this later.\n\n---\n\nThank you for providing some insights into the issue.\r\n\r\nTo clarify, I am creating a new boilerplate HTTP server that essentially serves as a \"Hello World\" application. Here are the details of my setup and the steps I've taken:\r\n\r\n1. Using ES Modules:\r\n\r\n-       Configuration: Set \"type\": \"module\" in my package.json.\r\n-       Issue: Encountering the same MODULE_NOT_FOUND errors as mentioned earlier when running the application.\\\r\n\r\n2.  Using CommonJS (Default Express Behavior):\r\n\r\n-     Configuration: Removed the \"type\": \"module\" from package.json to revert to CommonJS.\r\n-     Issue: Still experiencing the same MODULE_NOT_FOUND errors when executing the server.\r\n\r\n\r\n3. Node.js Versions:\r\n\r\n-       Current Version: v22.12.0\r\n-       Older LTS Versions:(v18) Tested with previous Long-Term Support (LTS) versions of Node.js.\r\n-        Outcome: The errors persist across different Node.js versions, indicating that the issue isn't specific to the Node.js version being used.\r\n\r\n4. Tweaking File Extensions:\r\n\r\n-     Approach: Attempted renaming files to .cjs and .mjs as appropriate to align with CommonJS and ES Module standards.\r\n-     Outcome: The errors persisted despite these changes.\r\n\r\n\r\n\r\nSteps Taken So Far:\r\n\r\n-         Reinstalled Dependencies: Deleted node_modules and package-lock.json, then ran npm install to ensure all packages are correctly installed.\r\n-         Verified Package Integrity: Checked the express package in node_modules to confirm that all necessary files, including ./middleware/query, are present.\r\n-         Simplified Codebase: Created a minimal index.js with just the basic Express setup to rule out any code-related issues.\r\n-         Despite these efforts, the application still fails to locate the required modules, both when using ES modules and CommonJS. \r\n\r\n",
      "labels": [
        "awaiting more info"
      ],
      "created_at": "2024-12-17T06:05:24Z",
      "closed_at": "2025-01-10T18:23:11Z",
      "url": "https://github.com/expressjs/express/issues/6227",
      "comments_count": 3
    },
    {
      "tech": "nodejs",
      "repo": "expressjs/express",
      "issue_number": 6207,
      "title": "In Express 5, how to make a query param parsed as an array when it is an array of string of one element ?",
      "problem": "Hello everybody,\r\n\r\nI am currently migrating my app from Express 4.17.21 to Express 5. I am using Node 22.9. The only problem I have is for parsing the query params when it is array of string of 1 element.\r\n\r\nHere is an example of the url : \r\n\r\n```\r\nhttp://localhost/api/myApi/myBaseUrl?myQueryParam=firstElement\r\n```\r\nThe query param firstElement is not recognized as a an array, and it was the case in Express 4.x.x.\r\n\r\nThis problem can be fixed by this code (found [https://github.com/cdimascio/express-openapi-validator/issues/599](url) thanks to @djMax)\r\n\r\n```\r\napp.use((req, res, next) => {\r\n    // Express 5 re-parses the query string every time. This causes problems with\r\n    // various libraries, namely the express OpenAPI parser. So we \"freeze it\" in place\r\n    // here, which runs right before the routing stuff does.\r\n    const { query } = req;\r\n    if (query) {\r\n      Object.defineProperty(req, 'query', {\r\n        value: query,\r\n      });\r\n    }\r\n  next();\r\n});\r\n```\r\n\r\nBut this is not very clean.\r\n\r\nI know if I use the \"extended\" query parser (app.set(\"query parser\", \"extended\")), and use the url\r\n```\r\nhttp://localhost/api/myApi/myBaseUrl?myQueryParam[]=firstElement\r\n```\r\nthe problem can be fixed but for reasons - such as our APIs are used by consumers - I cannot use this solution.\r\n\r\nSo I wonder whether there is a \"clean\" way for parsing as Express 4 does...?\r\n\r\nThank you !",
      "solution": "First, just to clear up any confusion, it does not parse the url every time: https://github.com/pillarjs/parseurl/blob/master/index.js#L43\r\n\r\nSecond, seems like this lib is breaking some assumptions because of this misunderstanding, but I will not comment more on that other than to say this seems like a bad idea on their part.\r\n\r\nBut this doesn't appear to have anything to do with the question you asked, which is about the breaking changes from `qs`. You can check out the changelogs there: https://github.com/ljharb/qs\r\n\r\nThe query string parser would not treat this as an array because it has no indication of it being an array, so the best solution if you always want that to be an array is to check and make it an array if it is not:\r\n\r\n```\r\nif (req.query.myQueryParam && !Array.isArray(req.query.myQueryParam)) {\r\n  req.query.myQueryParam = [req.query.myQueryParam];\r\n|\r\n```\n\n---\n\nMigrating from Express 4 to Express 5 introduced some changes to how query parameters are parsed, particularly with regards to arrays and single-element arrays. This behavior stems from the underlying `querystring` vs `qs` modules used for parsing.\r\n\r\n### Why the Issue Occurs\r\nExpress 4 uses `qs` by default, which is more robust and recognizes `?param=value` as `param: ['value']` when a consumer expects an array. However, Express 5 defaults to `querystring`, which does not handle this the same way.\r\n\r\n### Solutions\r\nHere are clean and practical solutions for resolving this issue:\r\n\r\n---\r\n\r\n#### 1. **Use the `qs` Module Explicitly**\r\nYou can replicate Express 4 behavior by setting up a custom query parser using the `qs` library.\r\n\r\n**Install `qs`:**\r\n```bash\r\nnpm install qs\r\n```\r\n\r\n**Update your application:**\r\n```javascript\r\nconst qs = require('qs');\r\n\r\n// Set a custom query parser\r\napp.set('query parser', str => qs.parse(str));\r\n```\r\n\r\nThis makes the query parser behave like it did in Express 4.\r\n\r\n---\r\n\r\n#### 2. **Restore the \"Extended\" Parser**\r\nThe `extended` parser uses `qs` under the hood. You can explicitly set it up without requiring clients to change their URL syntax.\r\n\r\n```javascript\r\napp.set('query parser', 'extended');\r\n```\r\n\r\nThis re-enables the extended query parsing without forcing clients to use `[]` for array elements.\r\n\r\n---\r\n\r\n#### 3. **Manually Normalize Query Parameters**\r\nIf you need fine-grained control over how query parameters are handled, you can write middleware to normalize query parameters into arrays.\r\n\r\n```javascript\r\napp.use((req, res, next) => {\r\n  for (const [key, value] of Object.entries(req.query)) {\r\n    if (!Array.isArray(value)) {\r\n      req.query[key] = [value];\r\n    }\r\n  }\r\n  next();\r\n});\r\n```\r\n\r\nThis ensures all query parameters are treated as arrays, even if they are single values.\r\n\r\n---\r\n\r\n### Recommendation\r\nThe cleanest and most backward-compatible solution is **Option 1** (using `qs`). It closely mimics Express 4 behavior and avoids unexpected surprises for consumers of your API.\r\n\r\n### Additional Notes\r\n- Consider documenting the behavior change for your API consumers if it's relevant.\r\n- Test your API thoroughly after the migration, especially with complex query strings.\r\n\n\n---\n\nHi @kerlos-alfy,\r\n\r\nThank you for your solutions ! However they don't work... Before asking my question I had tried the first one already, and the second one does not work. I didn't try the last one since I don't want that all the query params to be treaded as array.\r\n\r\nI think it is because the problem is not the query parser, the problem is about the parameter \"query\" of the object \"request\" - which is actually not an object param in Express 5, but a getter. And this getter does not only \"get\" the param, it parses the query from the url and makes some transformation - and I think this is why I have the problem.\r\n\r\nHow the query param is returned in Express 4\r\n````\r\nreq.param = function param(name, defaultValue) {\r\n  var params = this.params || {};\r\n  var body = this.body || {};\r\n  var query = this.query || {};\r\n\r\n  var args = arguments.length === 1\r\n    ? 'name'\r\n    : 'name, default';\r\n  deprecate('req.param(' + args + '): Use req.params, req.body, or req.query instead');\r\n\r\n  if (null != params[name] && params.hasOwnProperty(name)) return params[name];\r\n  if (null != body[name]) return body[name];\r\n  if (null != query[name]) return query[name];\r\n\r\n  return defaultValue;\r\n};\r\n````\r\n\r\nHow the query param is returned in Express 5\r\n````\r\ndefineGetter(req, 'query', function query(){\r\n  var queryparse = this.app.get('query parser fn');\r\n\r\n  if (!queryparse) {\r\n    // parsing is disabled\r\n    return Object.create(null);\r\n  }\r\n\r\n  var querystring = parse(this).query;\r\n\r\n  return queryparse(querystring);\r\n});\r\n````\r\n\r\nThank you for your answer ! ",
      "labels": [
        "question"
      ],
      "created_at": "2024-12-05T15:45:38Z",
      "closed_at": "2024-12-28T19:19:33Z",
      "url": "https://github.com/expressjs/express/issues/6207",
      "comments_count": 5
    }
  ]
}