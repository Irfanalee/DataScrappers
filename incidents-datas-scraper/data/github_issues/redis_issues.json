{
  "tech": "redis",
  "count": 62,
  "examples": [
    {
      "tech": "redis",
      "repo": "redis/redis",
      "issue_number": 14762,
      "title": "[BUG] Memory Leakage Issue in the Redis Sentinel ",
      "problem": "**Describe the bug**\nThese Bug Details are already  shared in this Link \nhttps://github.com/redis/RedisInsight/issues/5420 \nA short description of the bug.\n\n**To reproduce**\n\nSteps to reproduce the behavior and/or a minimal code sample.\n\n**Expected behavior**\n\nA description of what you expected to happen.\n\n**Additional information**\n\nAny additional information that is relevant to the problem.\n",
      "solution": "The Memory Leakages  on the slaves node it is Fixed in the Recent Redis Releases 8.4",
      "labels": [],
      "created_at": "2026-02-03T05:03:04Z",
      "closed_at": "2026-02-04T04:20:40Z",
      "url": "https://github.com/redis/redis/issues/14762",
      "comments_count": 1
    },
    {
      "tech": "redis",
      "repo": "redis/redis",
      "issue_number": 11784,
      "title": "[BUG] SUSCRIBE in RESP3 gets no proper reply, only push",
      "problem": "**Describe the bug**\r\n\r\nAfter sending SUBSCRIBE and UNSUBSCRIBE (and P and S variants), the message `[\"subscribe\", \"ch1\", 1]` comes as a push message, not a proper reply. If \"SUBSCRIBE ch1 ch2\", you get two push messages, but no reply. Clients get out of sync due to this.\r\n\r\n**To reproduce**\r\n\r\n```sh\r\n$ telnet localhost 6379\r\nTrying ::1...\r\nConnected to localhost.\r\nEscape character is '^]'.\r\nhello 3\r\n%7\r\n$6\r\nserver\r\n$5\r\nredis\r\n$7\r\nversion\r\n$11\r\n255.255.255\r\n$5\r\nproto\r\n:3\r\n$2\r\nid\r\n:211\r\n$4\r\nmode\r\n$10\r\nstandalone\r\n$4\r\nrole\r\n$6\r\nmaster\r\n$7\r\nmodules\r\n*0\r\nsubscribe ch1 ch2\r\n>3\r\n$9\r\nsubscribe\r\n$3\r\nch1\r\n:1\r\n>3\r\n$9\r\nsubscribe\r\n$3\r\nch2\r\n:2\r\nping\r\n+PONG\r\n```\r\n\r\n**Expected behavior**\r\n\r\nSUBSCRIBE should get +OK or something like that, so that each command gets a reply. (Push are out-of-band.)\r\n\r\n**Additional information**\r\n\r\nThis was reported in #7026, which was assumed to be a problem in redis-cli, but it has nothing to do with redis-cli.\r\n\r\nI guess we can't fix this now, so perhaps it should instead be clearly documented? RESP3-capable clients need to compensate for this or else they will get out of sync...",
      "solution": "@zuiderkwast can you please explain what you mean by \"Clients get out of sync due to this\"?\r\nAFAIU, in RESP2, once a client issues a SUBSCRIBE (or the other variants) it enters a special mode where you can only handle pubsub commands and PING\r\nin RESP3 there are no limits, push notifications can arrive at any time, so what's the difference between a message that came via push from the SUBSCRIBE command itself, or from another client doing PUBLISH\r\n\r\nif anything, what's weird is that UNSUBSCRIBE also replies with push. the problem here is that the client is considered CLIENT_PUBSUB while it is subscribed to at least one channel. that means that if I execute UNSUBSCRIBE with no args (or the UNSUBSCRIBE that causes the client not to be subscribed to anything anymore) i get a push reply while not in pusub mode.\r\n\r\nso yes, it does look weird, but does it cause an actual issue with clients?\n\n---\n\nI do not think this is a resp3 protocol intended issue but simply a bug.\r\nFor example take the issue reported in #2967. we allow resp3 to issue different commands while in subscribed mode since clients are able to set different handler on the pushed commands, but it makes no sense that command reply will get into push handler from client implementation POV. I understand the breaking change for clients, but I support fixing this, even if late as redis 8",
      "labels": [],
      "created_at": "2023-02-06T08:02:10Z",
      "closed_at": "2023-03-08T17:05:09Z",
      "url": "https://github.com/redis/redis/issues/11784",
      "comments_count": 16
    },
    {
      "tech": "redis",
      "repo": "redis/redis",
      "issue_number": 14724,
      "title": "[BUG] RediSearch Index Missing and Duplicated Results After Cluster Reshard",
      "problem": "[resharing.log](https://github.com/user-attachments/files/24767133/resharing.log)\n[node-8.log](https://github.com/user-attachments/files/24767137/node-8.log)\n[node-7.log](https://github.com/user-attachments/files/24767131/node-7.log)\n[node-6.log](https://github.com/user-attachments/files/24767136/node-6.log)\n[node-5.log](https://github.com/user-attachments/files/24767138/node-5.log)\n[node-4.log](https://github.com/user-attachments/files/24767135/node-4.log)\n[node-3.log](https://github.com/user-attachments/files/24767134/node-3.log)\n[node-2.log](https://github.com/user-attachments/files/24767139/node-2.log)\n[node-1.log](https://github.com/user-attachments/files/24767132/node-1.log)\n\nWe encountered two critical issues when adding nodes and resharding a Redis cluster with RediSearch secondary indexes. Both availability of indexes and result are affected. Below are reproducible steps and observations.\n\n#### \ud83d\udea8 Issue 1: RediSearch index missing on new nodes after reshard\n\n- Index is not present on new master/replica after slot migration.\n- Querying index on any node leads to \"No such index idx\".\n\n#### \ud83d\udea8 Issue 2: Duplicate/incorrect search results after index recreation\n\n- After manually recreating the index, search on migrated slots returns **inflated counts** (e.g., 6 instead of 2).\n- Duplicate document entries in search results (same doc-id appears multiple times).\n- Other keys and data are not duplicated (`KEYS` is correct).\n\n### Questions\n\n1. According to [RediSearch documentation](https://redis.io/docs/latest/operate/oss_and_stack/stack-with-enterprise/search/#resharding-indexed-data), the index should be created on new shards *automatically* during reshard. Why does the index go missing?\n2. Why does manually recreating the index on the new node result in **duplicate entries** and inflated counts for affected tenants?\n3. What is the correct procedure to avoid these issues when scaling out (adding nodes and resharding) with RediSearch indexes in Redis Cluster?\n4. Is there a safe recovery or repair method for restoring correct index state if this happens?\n\nThank you for your help! Please let us know if additional logs or metrics are needed.\n\n\n# Steps to reproduce\n\nSome utility functions and variable:\n\n```bash\nrun-node() {\n  for name in \"$@\"; do\n    docker run -d \\\n      --name \"$name\" \\\n      --net redis \\\n      redis:8.2.0-bookworm \\\n      redis-server \\\n        --cluster-enabled yes \\\n        --cluster-preferred-endpoint-type hostname \\\n        --cluster-announce-hostname \"$name\"\n  done\n}\n\nget-logs() {\n  for name in \"$@\"; do\n    local output=\"${name}.log\"\n    docker logs \"$name\" > \"$output\"\n    echo \"$output\"\n  done\n}\n\nredis-cli() {\n  docker exec node-1 redis-cli -c \"$@\"\n}\n\nnode-id() {\n  redis-cli CLUSTER NODES | grep node-$1 | cut -d' ' -f 1\n}\n\nEXISTING_NODE=\"node-1:6379\"\n```\n\n\n**1.** Start with 6 Redis nodes:\n\n```bash\ndocker network create redis\nrun-node node-{1,2,3,4,5,6}\n```\n\n**2.** Create the cluster: 3 masters and 3 replicas:\n\n```bash\nredis-cli --cluster create node-{1,2,3,4,5,6}:6379 --cluster-replicas 1 --cluster-yes\n```\n\n**3.** Create a RediSearch seconday index\n\n```bash\nredis-cli FT.CREATE idx ON HASH PREFIX 1 doc: SCHEMA tenant_id TAG title TEXT category TAG\n```\n\n**4.** Load some data: 2 documents for each of 5 tenants. tenant_id is used to compute the hash slot, so all documents of a tenant_id is on the same node (so an HNSWS index can index tenant document on each node).\n\n```bash\nredis-cli HSET doc:{t00}:00 tenant_id \"t00\" title \"title-00\" category \"foo\"\nredis-cli HSET doc:{t00}:01 tenant_id \"t00\" title \"title-01\" category \"bar\"\nredis-cli HSET doc:{t01}:02 tenant_id \"t01\" title \"title-02\" category \"baz\"\nredis-cli HSET doc:{t01}:03 tenant_id \"t01\" title \"title-03\" category \"foo\"\nredis-cli HSET doc:{t02}:04 tenant_id \"t02\" title \"title-04\" category \"bar\"\nredis-cli HSET doc:{t02}:05 tenant_id \"t02\" title \"title-05\" category \"baz\"\nredis-cli HSET doc:{t03}:06 tenant_id \"t03\" title \"title-06\" category \"foo\"\nredis-cli HSET doc:{t03}:07 tenant_id \"t03\" title \"title-07\" category \"baz\"\nredis-cli HSET doc:{t04}:08 tenant_id \"t04\" title \"title-08\" category \"baz\"\nredis-cli HSET doc:{t04}:09 tenant_id \"t04\" title \"title-09\" category \"foo\"\n```\n\n**5.** Each tenant has 2 documents:\n\n```bash\nredis-cli FT.SEARCH idx \"@tenant_id:{t00}\" RETURN 0 LIMIT 0 0\nredis-cli FT.SEARCH idx \"@tenant_id:{t01}\" RETURN 0 LIMIT 0 0\nredis-cli FT.SEARCH idx \"@tenant_id:{t02}\" RETURN 0 LIMIT 0 0\nredis-cli FT.SEARCH idx \"@tenant_id:{t03}\" RETURN 0 LIMIT 0 0\nredis-cli FT.SEARCH idx \"@tenant_id:{t04}\" RETURN 0 LIMIT 0 0\n2\n2\n2\n2\n2\n```\n\n**6.** Start 2 new Redis containers:\n\n```bash\nrun-node node-{7,8}\n```\n\n**7.** Add the new nodes to the cluster:\n\n```bash\nredis-cli --cluster add-node node-7:6379 $EXISTING_NODE\nredis-cli --cluster add-node node-8:6379 $EXISTING_NODE --cluster-slave --cluster-master-id $(node-id 7)\n```\n\n\n**8.** Reshard\n\n```bash\nredis-cli --cluster reshard $EXISTING_NODE --cluster-from all --cluster-to $(node-id 7) --cluster-slots 4096 --cluster-yes &> resharing.log\n```\n\n\n**9.** Index is missing\n\nThe index should have been created during the resharding according to the [documentation](https://redis.io/docs/latest/operate/oss_and_stack/stack-with-enterprise/search/#resharding-indexed-data):\n\n> By moving the index out of the keyspace and structuring the data as hashes, RediSearch 2.x makes it possible to reshard the database. When half of the data moves to the new shard, the index related to that data is created synchronously and Redis removes the keys from the index when it detects that the keys were deleted. Because the index on the new shard is created synchronously though, it's expected that the resharding process will take longer than resharding of a database without search and query enabled.\n\n\ud83d\udea8 **ISSUE 1**:  But index missing on nodes 7 and 8:\n\n```bash\ndocker exec node-6 redis-cli FT._LIST\nidx\ndocker exec node-7 redis-cli FT._LIST\n\ndocker exec node-8 redis-cli FT._LIST\n\n```\n\nThe search is no more possible:\n\n```bash\ndocker exec node-1 redis-cli FT.SEARCH idx \"@tenant_id:{t00}\" RETURN 0 LIMIT 0 0\nNo such index idx\n```\n\n**10.** Create the index\n\nCreate the index on the master node:\n\n```bash\ndocker exec node-7 redis-cli FT.CREATE idx ON HASH PREFIX 1 doc: SCHEMA tenant_id TAG title TEXT category TAG\nIndex already exists\n```\n\nIt say `Index already exists`, but it's actually now created:\n\n```bash\ndocker exec node-7 redis-cli FT._LIST\nidx\ndocker exec node-8 redis-cli FT._LIST\nidx\n```\n\n**11.** Search\n\n\ud83d\udea8 **ISSUE 2**: Count the documents using the index is incorrect for the document own by the new nodes:\n\n```bash\nredis-cli FT.SEARCH idx \"@tenant_id:{t00}\" RETURN 0 LIMIT 0 0\nredis-cli FT.SEARCH idx \"@tenant_id:{t01}\" RETURN 0 LIMIT 0 0\nredis-cli FT.SEARCH idx \"@tenant_id:{t02}\" RETURN 0 LIMIT 0 0\nredis-cli FT.SEARCH idx \"@tenant_id:{t03}\" RETURN 0 LIMIT 0 0\nredis-cli FT.SEARCH idx \"@tenant_id:{t04}\" RETURN 0 LIMIT 0 0\n2\n2\n6\n2\n2\n```\n\nThe data are correct:\n\n```bash\ndocker exec node-7 redis-cli KEYS \"*\"\ndoc:{t02}:04\ndoc:{t02}:05\n```\n\n\n```bash\nredis-cli FT.SEARCH idx \"@tenant_id:{t02}\"\n6\ndoc:{t02}:04\ntenant_id\nt02\ntitle\ntitle-04\ncategory\nbar\ndoc:{t02}:04\ntenant_id\nt02\ntitle\ntitle-04\ncategory\nbar\ndoc:{t02}:04\ntenant_id\nt02\ntitle\ntitle-04\ncategory\nbar\ndoc:{t02}:05\ntenant_id\nt02\ntitle\ntitle-05\ncategory\nbaz\ndoc:{t02}:05\ntenant_id\nt02\ntitle\ntitle-05\ncategory\nbaz\ndoc:{t02}:05\ntenant_id\nt02\ntitle\ntitle-05\ncategory\nbaz\n```\n\n**12.** Get the logs\n\n```bash\nget-logs node-{1,2,3,4,5,6,7,8}\n\nredis-cli CLUSTER NODES\n8a76b826b7ae1a948cf61c34e78ada24438ad7e7 172.64.3.9:6379@16379,node-8 slave 921856e96365cb6d5359f3655c56071c95157977 0 1768998390000 8 connected\nafea05c19ac407c48659acdabdde661e96c7570f 172.64.3.3:6379@16379,node-2 master - 0 1768998391000 2 connected 6827-10922\nf6e42dbf574ab849dc666ebaefdc7714bef4de25 172.64.3.7:6379@16379,node-6 slave afea05c19ac407c48659acdabdde661e96c7570f 0 1768998392333 2 connected\n921856e96365cb6d5359f3655c56071c95157977 172.64.3.8:6379@16379,node-7 master - 0 1768998389279 8 connected 0-1364 5461-6826 10923-12287\n97e1f3a7f28e47dd61791e3c54b78181ba22e917 172.64.3.2:6379@16379,node-1 myself,master - 0 0 1 connected 1365-5460\n5acc82353fd0934bd93bcaf89cfc341675a8e986 172.64.3.4:6379@16379,node-3 master - 0 1768998391315 3 connected 12288-16383\nb0a6157c5be553e6f66ffd97deab4cf36510916b 172.64.3.5:6379@16379,node-4 slave 5acc82353fd0934bd93bcaf89cfc341675a8e986 0 1768998390000 3 connected\n5811e9ab29685e41ce9f2295d8911898c1a2e55c 172.64.3.6:6379@16379,node-5 slave 97e1f3a7f28e47dd61791e3c54b78181ba22e917 0 1768998393347 1 connected\n\nredis-cli CLUSTER SHARDS\nslots\n1365\n5460\nnodes\nid\n97e1f3a7f28e47dd61791e3c54b78181ba22e917\nport\n6379\nip\n172.64.3.2\nendpoint\nnode-1\nhostname\nnode-1\nrole\nmaster\nreplication-offset\n774\nhealth\nonline\nid\n5811e9ab29685e41ce9f2295d8911898c1a2e55c\nport\n6379\nip\n172.64.3.6\nendpoint\nnode-5\nhostname\nnode-5\nrole\nreplica\nreplication-offset\n760\nhealth\nonline\nslots\n0\n1364\n5461\n6826\n10923\n12287\nnodes\nid\n921856e96365cb6d5359f3655c56071c95157977\nport\n6379\nip\n172.64.3.8\nendpoint\nnode-7\nhostname\nnode-7\nrole\nmaster\nreplication-offset\n716\nhealth\nonline\nid\n8a76b826b7ae1a948cf61c34e78ada24438ad7e7\nport\n6379\nip\n172.64.3.9\nendpoint\nnode-8\nhostname\nnode-8\nrole\nreplica\nreplication-offset\n716\nhealth\nonline\nslots\n12288\n16383\nnodes\nid\n5acc82353fd0934bd93bcaf89cfc341675a8e986\nport\n6379\nip\n172.64.3.4\nendpoint\nnode-3\nhostname\nnode-3\nrole\nmaster\nreplication-offset\n970\nhealth\nonline\nid\nb0a6157c5be553e6f66ffd97deab4cf36510916b\nport\n6379\nip\n172.64.3.5\nendpoint\nnode-4\nhostname\nnode-4\nrole\nreplica\nreplication-offset\n984\nhealth\nonline\nslots\n6827\n10922\nnodes\nid\nafea05c19ac407c48659acdabdde661e96c7570f\nport\n6379\nip\n172.64.3.3\nendpoint\nnode-2\nhostname\nnode-2\nrole\nmaster\nreplication-offset\n1021\nhealth\nonline\nid\nf6e42dbf574ab849dc666ebaefdc7714bef4de25\nport\n6379\nip\n172.64.3.7\nendpoint\nnode-6\nhostname\nnode-6\nrole\nreplica\nreplication-offset\n1021\nhealth\nonline\n```\n",
      "solution": "Just tested with `8.4.0`, still with `--cluster reshard`:\n- issue 1 is still here\n- issue 2 is fixed, good news\n\nI'll experiment with `CLUSTER MIGRATION`.\n\n---\n\nOk, clear for me! I think I can close the issue if it's ok for you too?\n\n---\n\nNote that while the migration happens (near the end), there is a small chance some results will be missing/duplicated. This will be fixed in `8.4.1`.",
      "labels": [],
      "created_at": "2026-01-21T13:10:20Z",
      "closed_at": "2026-01-22T14:14:52Z",
      "url": "https://github.com/redis/redis/issues/14724",
      "comments_count": 8
    },
    {
      "tech": "redis",
      "repo": "redis/redis",
      "issue_number": 14570,
      "title": "[CRASH] crash in moduleTimerHandler  on versions 8.2-8.4 ",
      "problem": "### Summary\nRedis crashes on Redis 8.4.0. I had similar crashes on 8.2 and 8.3 as well. \n\n\n**Crash report from Redis 8.4.0**\n```\n10967:M 25 Nov 2025 16:24:01.164 * <search> DocTable capacity increase from 438565 to 657848\n10967:M 25 Nov 2025 16:33:15.889 # <search> ForkGC - got timeout while reading from pipe (No such process)\n\n\n=== REDIS BUG REPORT START: Cut & paste starting from here ===\n10967:M 25 Nov 2025 16:37:07.904 # Redis 8.4.0 crashed by signal: 11, si_code: 128\n10967:M 25 Nov 2025 16:37:07.904 # Accessing address: (nil)\n10967:M 25 Nov 2025 16:37:07.904 # Crashed running the instruction at: 0x55e427033653\n\n------ STACK TRACE ------\nEIP:\n/usr/bin/redis-server 0.0.0.0:6379(moduleTimerHandler+0x153)[0x55e427033653]\n\n10969 bio_close_file\n/lib/x86_64-linux-gnu/libc.so.6(+0x98d71)[0x7f40da4ffd71]\n/lib/x86_64-linux-gnu/libc.so.6(pthread_cond_wait+0x20d)[0x7f40da5027ed]\n/usr/bin/redis-server 0.0.0.0:6379(bioProcessBackgroundJobs+0x1ea)[0x55e426ffdcda]\n/lib/x86_64-linux-gnu/libc.so.6(+0x9caa4)[0x7f40da503aa4]\n/lib/x86_64-linux-gnu/libc.so.6(__clone+0x44)[0x7f40da590a64]\n\n10971 bio_lazy_free\n/lib/x86_64-linux-gnu/libc.so.6(+0x98d71)[0x7f40da4ffd71]\n/lib/x86_64-linux-gnu/libc.so.6(pthread_cond_wait+0x20d)[0x7f40da5027ed]\n/usr/bin/redis-server 0.0.0.0:6379(bioProcessBackgroundJobs+0x1ea)[0x55e426ffdcda]\n/lib/x86_64-linux-gnu/libc.so.6(+0x9caa4)[0x7f40da503aa4]\n/lib/x86_64-linux-gnu/libc.so.6(__clone+0x44)[0x7f40da590a64]\n\n10970 bio_aof\n/lib/x86_64-linux-gnu/libc.so.6(+0x98d71)[0x7f40da4ffd71]\n/lib/x86_64-linux-gnu/libc.so.6(pthread_cond_wait+0x20d)[0x7f40da5027ed]\n/usr/bin/redis-server 0.0.0.0:6379(bioProcessBackgroundJobs+0x1ea)[0x55e426ffdcda]\n/lib/x86_64-linux-gnu/libc.so.6(+0x9caa4)[0x7f40da503aa4]\n/lib/x86_64-linux-gnu/libc.so.6(__clone+0x44)[0x7f40da590a64]\n\n10967 redis-server *\n/lib/x86_64-linux-gnu/libc.so.6(+0x45330)[0x7f40da4ac330]\n/usr/bin/redis-server 0.0.0.0:6379(moduleTimerHandler+0x153)[0x55e427033653]\n/usr/bin/redis-server 0.0.0.0:6379(+0x995d3)[0x55e426f135d3]\n/usr/bin/redis-server 0.0.0.0:6379(aeMain+0x2d)[0x55e426f1388d]\n/usr/bin/redis-server 0.0.0.0:6379(main+0x3cb)[0x55e426f0cddb]\n/lib/x86_64-linux-gnu/libc.so.6(+0x2a1ca)[0x7f40da4911ca]\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x8b)[0x7f40da49128b]\n/usr/bin/redis-server 0.0.0.0:6379(_start+0x25)[0x55e426f0e645]\n\n11102 gc-7224\n/lib/x86_64-linux-gnu/libc.so.6(+0x98d71)[0x7f40da4ffd71]\n/lib/x86_64-linux-gnu/libc.so.6(pthread_cond_wait+0x20d)[0x7f40da5027ed]\n/usr/lib/redis/modules/redisearch.so(+0x27e18b)[0x7f40d85bd18b]\n/usr/lib/redis/modules/redisearch.so(+0x27de19)[0x7f40d85bce19]\n/lib/x86_64-linux-gnu/libc.so.6(+0x9caa4)[0x7f40da503aa4]\n/lib/x86_64-linux-gnu/libc.so.6(__clone+0x44)[0x7f40da590a64]\n\n11104 reindex-3249\n/lib/x86_64-linux-gnu/libc.so.6(+0x98d71)[0x7f40da4ffd71]\n/lib/x86_64-linux-gnu/libc.so.6(pthread_cond_wait+0x20d)[0x7f40da5027ed]\n/usr/lib/redis/modules/redisearch.so(+0x27e18b)[0x7f40d85bd18b]\n/usr/lib/redis/modules/redisearch.so(+0x27de19)[0x7f40d85bce19]\n/lib/x86_64-linux-gnu/libc.so.6(+0x9caa4)[0x7f40da503aa4]\n/lib/x86_64-linux-gnu/libc.so.6(__clone+0x44)[0x7f40da590a64]\n\n6/6 expected stacktraces.\n\n------ STACK TRACE DONE ------\n\n------ REGISTERS ------\n10967:M 25 Nov 2025 16:37:07.918 #\nRAX:0000000066900000 RBX:00007fff83d06120\nRCX:00007f40d9c45800 RDX:000055e427260360\nRDI:00007fff83d06120 RSI:0041da49689696c4\nRBP:00007fff83d063c0 RSP:00007fff83d06100\nR8 :00000000038e7840 R9 :00007fff83d3f080\nR10:00007fff83d060b0 R11:00007fff83d3f080\nR12:00007fff83d061a0 R13:0006446c36dd7419\nR14:000055e4271563df R15:00007f4032ada049\nRIP:000055e427033653 EFL:0000000000010206\nCSGSFS:002b000000000033\n10967:M 25 Nov 2025 16:37:07.919 # (00007fff83d0610f) -> 0000000000000000\n10967:M 25 Nov 2025 16:37:07.919 # (00007fff83d0610e) -> 0000000000000000\n10967:M 25 Nov 2025 16:37:07.919 # (00007fff83d0610d) -> 0000000000000000\n10967:M 25 Nov 2025 16:37:07.919 # (00007fff83d0610c) -> 0000000000000000\n10967:M 25 Nov 2025 16:37:07.919 # (00007fff83d0610b) -> 0000000000000000\n10967:M 25 Nov 2025 16:37:07.919 # (00007fff83d0610a) -> 0000000000000040\n10967:M 25 Nov 2025 16:37:07.919 # (00007fff83d06109) -> 0000000000000000\n10967:M 25 Nov 2025 16:37:07.919 # (00007fff83d06108) -> 0000000000000000\n10967:M 25 Nov 2025 16:37:07.919 # (00007fff83d06107) -> 0000000000000000\n10967:M 25 Nov 2025 16:37:07.919 # (00007fff83d06106) -> 00007f40d9c45800\n10967:M 25 Nov 2025 16:37:07.919 # (00007fff83d06105) -> 5800003631383433\n10967:M 25 Nov 2025 16:37:07.919 # (00007fff83d06104) -> 000055e4270200b0\n10967:M 25 Nov 2025 16:37:07.919 # (00007fff83d06103) -> 00007fff83d061d0\n10967:M 25 Nov 2025 16:37:07.919 # (00007fff83d06102) -> 00007fff83d06270\n10967:M 25 Nov 2025 16:37:07.919 # (00007fff83d06101) -> 000055e427260360\n10967:M 25 Nov 2025 16:37:07.919 # (00007fff83d06100) -> 000055e42724c490\n\n------ INFO OUTPUT ------\n# Server\nredis_version:8.4.0\nredis_git_sha1:00000000\nredis_git_dirty:1\nredis_build_id:27c2d682528c8769\nredis_mode:standalone\nos:Linux 5.15.167.4-microsoft-standard-WSL2 x86_64\narch_bits:64\nmonotonic_clock:POSIX clock_gettime\nmultiplexing_api:epoll\natomicvar_api:c11-builtin\ngcc_version:13.3.0\nprocess_id:10967\nprocess_supervised:systemd\nrun_id:810cd2057fa6d475a3fc0d2f789890b95ab5313a\ntcp_port:6379\nserver_time_usec:1764081427903521\nuptime_in_seconds:9361\nuptime_in_days:0\nhz:10\nconfigured_hz:10\nlru_clock:2473747\nexecutable:/usr/bin/redis-server\nconfig_file:/etc/redis/redis.conf\nio_threads_active:0\nlistener0:name=tcp,bind=0.0.0.0,bind=-::1,port=6379\n\n# Clients\nconnected_clients:12\ncluster_connections:0\nmaxclients:10000\nclient_recent_max_input_buffer:202\nclient_recent_max_output_buffer:0\nblocked_clients:0\ntracking_clients:0\npubsub_clients:4\nwatching_clients:0\nclients_in_timeout_table:0\ntotal_watched_keys:0\ntotal_blocking_keys:0\ntotal_blocking_keys_on_nokey:0\n\n# Memory\nused_memory:2347355816\nused_memory_human:2.19G\nused_memory_rss:2393575424\nused_memory_rss_human:2.23G\nused_memory_peak:2347375912\nused_memory_peak_human:2.19G\nused_memory_peak_time:1764081427\nused_memory_peak_perc:100.00%\nused_memory_overhead:119759082\nused_memory_startup:1072384\nused_memory_dataset:2227596734\nused_memory_dataset_perc:94.94%\nallocator_allocated:2348347952\nallocator_active:2353790976\nallocator_resident:2390450176\nallocator_muzzy:0\ntotal_system_memory:16614010880\ntotal_system_memory_human:15.47G\nused_memory_lua:34816\nused_memory_vm_eval:34816\nused_memory_lua_human:34.00K\nused_memory_scripts_eval:320\nnumber_of_cached_scripts:1\nnumber_of_functions:0\nnumber_of_libraries:0\nused_memory_vm_functions:34816\nused_memory_vm_total:69632\nused_memory_vm_total_human:68.00K\nused_memory_functions:192\nused_memory_scripts:512\nused_memory_scripts_human:512B\nmaxmemory:0\nmaxmemory_human:0B\nmaxmemory_policy:noeviction\nallocator_frag_ratio:1.00\nallocator_frag_bytes:5364304\nallocator_rss_ratio:1.02\nallocator_rss_bytes:36659200\nrss_overhead_ratio:1.00\nrss_overhead_bytes:3125248\nmem_fragmentation_ratio:1.02\nmem_fragmentation_bytes:46225312\nmem_not_counted_for_evict:0\nmem_replication_backlog:0\nmem_total_replication_buffers:0\nmem_replica_full_sync_buffer:0\nmem_clients_slaves:0\nmem_clients_normal:24978\nmem_cluster_slot_migration_output_buffer:0\nmem_cluster_slot_migration_input_buffer:0\nmem_cluster_slot_migration_input_buffer_peak:0\nmem_cluster_links:0\nmem_aof_buffer:0\nmem_allocator:jemalloc-5.3.0\nmem_overhead_db_hashtable_rehashing:0\nactive_defrag_running:0\nlazyfree_pending_objects:0\nlazyfreed_objects:0\n\n# Persistence\nloading:0\nasync_loading:0\ncurrent_cow_peak:0\ncurrent_cow_size:0\ncurrent_cow_size_age:0\ncurrent_fork_perc:0.00\ncurrent_save_keys_processed:0\ncurrent_save_keys_total:0\nrdb_changes_since_last_save:3442370\nrdb_bgsave_in_progress:0\nrdb_last_save_time:1764072066\nrdb_last_bgsave_status:ok\nrdb_last_bgsave_time_sec:-1\nrdb_current_bgsave_time_sec:-1\nrdb_saves:0\nrdb_saves_consecutive_failures:0\nrdb_last_cow_size:0\nrdb_last_load_keys_expired:0\nrdb_last_load_keys_loaded:0\naof_enabled:0\naof_rewrite_in_progress:0\naof_rewrite_scheduled:0\naof_last_rewrite_time_sec:-1\naof_current_rewrite_time_sec:-1\naof_last_bgrewrite_status:ok\naof_rewrites:0\naof_rewrites_consecutive_failures:0\naof_last_write_status:ok\naof_last_cow_size:0\nmodule_fork_in_progress:0\nmodule_fork_last_cow_size:0\n\n# Threads\nio_thread_0:clients=12,reads=182360,writes=251561\n\n# Stats\ntotal_connections_received:38\ntotal_commands_processed:3529771\ninstantaneous_ops_per_sec:4\ntotal_net_input_bytes:1323888411\ntotal_net_output_bytes:87414313\ntotal_net_repl_input_bytes:0\ntotal_net_repl_output_bytes:0\ninstantaneous_input_kbps:0.46\ninstantaneous_output_kbps:0.16\ninstantaneous_input_repl_kbps:0.00\ninstantaneous_output_repl_kbps:0.00\nrejected_connections:0\nsync_full:0\nsync_partial_ok:0\nsync_partial_err:0\nexpired_subkeys:0\nexpired_keys:0\nexpired_stale_perc:0.00\nexpired_time_cap_reached_count:0\nexpire_cycle_cpu_milliseconds:2882\nevicted_keys:0\nevicted_clients:0\nevicted_scripts:0\ntotal_eviction_exceeded_time:0\ncurrent_eviction_exceeded_time:0\nkeyspace_hits:1255692\nkeyspace_misses:0\npubsub_channels:1\npubsub_patterns:0\npubsubshard_channels:0\nlatest_fork_usec:18871\ntotal_forks:10\nmigrate_cached_sockets:0\nslave_expires_tracked_keys:0\nactive_defrag_hits:0\nactive_defrag_misses:0\nactive_defrag_key_hits:0\nactive_defrag_key_misses:0\ntotal_active_defrag_time:0\ncurrent_active_defrag_time:0\ntracking_total_keys:0\ntracking_total_items:0\ntracking_total_prefixes:0\nunexpected_error_replies:0\ntotal_error_replies:29\ndump_payload_sanitizations:0\ntotal_reads_processed:182360\ntotal_writes_processed:251561\nio_threaded_reads_processed:0\nio_threaded_writes_processed:0\nio_threaded_total_prefetch_batches:218501\nio_threaded_total_prefetch_entries:3410447\nclient_query_buffer_limit_disconnections:0\nclient_output_buffer_limit_disconnections:0\nreply_buffer_shrinks:3731\nreply_buffer_expands:3704\neventloop_cycles:270040\neventloop_duration_sum:136364247\neventloop_duration_cmd_sum:105569236\ninstantaneous_eventloop_cycles_per_sec:13\ninstantaneous_eventloop_duration_usec:317\nacl_access_denied_auth:0\nacl_access_denied_cmd:0\nacl_access_denied_key:0\nacl_access_denied_channel:0\n\n# Replication\nrole:master\nconnected_slaves:0\nmaster_failover_state:no-failover\nmaster_replid:fa25c0c6c373fcc78004b1b6af74b2ce22911b1b\nmaster_replid2:0000000000000000000000000000000000000000\nmaster_repl_offset:0\nsecond_repl_offset:-1\nrepl_backlog_active:0\nrepl_backlog_size:1048576\nrepl_backlog_first_byte_offset:0\nrepl_backlog_histlen:0\n\n# CPU\nused_cpu_sys:22.513633\nused_cpu_user:193.707047\nused_cpu_sys_children:4.347761\nused_cpu_user_children:5.197986\nused_cpu_sys_main_thread:18.928022\nused_cpu_user_main_thread:130.103686\n\n# Modules\nmodule:name=bf,ver=80400,api=1,filters=0,usedby=[],using=[],options=[handle-io-errors]\nmodule:name=vectorset,ver=1,api=1,filters=0,usedby=[],using=[],options=[handle-io-errors|handle-repl-async-load]\nmodule:name=ReJSON,ver=80400,api=1,filters=0,usedby=[search],using=[],options=[handle-io-errors]\nmodule:name=timeseries,ver=80400,api=1,filters=0,usedby=[],using=[],options=[handle-io-errors]\nmodule:name=search,ver=80402,api=1,filters=0,usedby=[],using=[ReJSON],options=[handle-io-errors]\n\n# Commandstats\ncmdstat_evalsha:calls=2,usec=554,usec_per_call=277.00,rejected_calls=0,failed_calls=1\ncmdstat_del:calls=1,usec=46,usec_per_call=46.00,rejected_calls=0,failed_calls=0\ncmdstat_config|get:calls=3,usec=2,usec_per_call=0.67,rejected_calls=0,failed_calls=0\ncmdstat_xreadgroup:calls=381,usec=106019,usec_per_call=278.27,rejected_calls=0,failed_calls=0\ncmdstat_json.set:calls=2156124,usec=51965941,usec_per_call=24.10,rejected_calls=0,failed_calls=0\ncmdstat_client|setname:calls=10,usec=18,usec_per_call=1.80,rejected_calls=0,failed_calls=0\ncmdstat_client|setinfo:calls=54,usec=79,usec_per_call=1.46,rejected_calls=0,failed_calls=0\ncmdstat_publish:calls=17350,usec=155042,usec_per_call=8.94,rejected_calls=0,failed_calls=0\ncmdstat_ping:calls=34269,usec=75175,usec_per_call=2.19,rejected_calls=0,failed_calls=0\ncmdstat_command|docs:calls=1,usec=1476,usec_per_call=1476.00,rejected_calls=0,failed_calls=0\ncmdstat_xgroup|create:calls=6,usec=203,usec_per_call=33.83,rejected_calls=0,failed_calls=5\ncmdstat_xadd:calls=15449,usec=87026,usec_per_call=5.63,rejected_calls=0,failed_calls=0\ncmdstat_expire:calls=1254757,usec=37794439,usec_per_call=30.12,rejected_calls=0,failed_calls=0\ncmdstat_FT.AGGREGATE:calls=86,usec=113562,usec_per_call=1320.49,rejected_calls=0,failed_calls=0\ncmdstat_subscribe:calls=5,usec=24,usec_per_call=4.80,rejected_calls=0,failed_calls=0\ncmdstat_set:calls=419,usec=882,usec_per_call=2.11,rejected_calls=0,failed_calls=0\ncmdstat_multi:calls=188,usec=395,usec_per_call=2.10,rejected_calls=0,failed_calls=0\ncmdstat__FT.CREATE:calls=15,usec=1693,usec_per_call=112.87,rejected_calls=0,failed_calls=0\ncmdstat_FT.INFO:calls=70,usec=5146,usec_per_call=73.51,rejected_calls=0,failed_calls=15\ncmdstat_get:calls=7,usec=310,usec_per_call=44.29,rejected_calls=0,failed_calls=0\ncmdstat_exec:calls=188,usec=43969,usec_per_call=233.88,rejected_calls=0,failed_calls=0\ncmdstat_xack:calls=159,usec=34837,usec_per_call=219.10,rejected_calls=0,failed_calls=0\ncmdstat_script|load:calls=1,usec=813,usec_per_call=813.00,rejected_calls=0,failed_calls=0\ncmdstat_FT.CREATE:calls=15,usec=1926,usec_per_call=128.40,rejected_calls=0,failed_calls=0\ncmdstat_FT.SEARCH:calls=46500,usec=14687957,usec_per_call=315.87,rejected_calls=0,failed_calls=7\ncmdstat_info:calls=3711,usec=533380,usec_per_call=143.73,rejected_calls=0,failed_calls=0\n\n# Errorstats\nerrorstat_BUSYGROUP:count=5\nerrorstat_ERR:count=1\nerrorstat_NOSCRIPT:count=1\nerrorstat_No:count=7\nerrorstat_monitoring_check_groups_:count=1\nerrorstat_monitoring_profiles_:count=1\nerrorstat_velocity_bin_lists_:count=1\nerrorstat_velocity_card_lists_:count=1\nerrorstat_velocity_convertation_cache_:count=1\nerrorstat_velocity_currency_mastercard_:count=1\nerrorstat_velocity_currency_visa_:count=1\nerrorstat_velocity_lists_of_countries_:count=1\nerrorstat_velocity_merchant_levels_:count=1\nerrorstat_velocity_merchants_groups_:count=1\nerrorstat_velocity_rule_hits_:count=1\nerrorstat_velocity_rules_:count=1\nerrorstat_velocity_transactions_:count=1\nerrorstat_veloroute_mc_ip0040t1_:count=1\nerrorstat_veloroute_visa_ardef_:count=1\n\n# Latencystats\nlatency_percentiles_usec_evalsha:p50=252.927,p99=303.103,p99.9=303.103\nlatency_percentiles_usec_del:p50=46.079,p99=46.079,p99.9=46.079\nlatency_percentiles_usec_config|get:p50=1.003,p99=1.003,p99.9=1.003\nlatency_percentiles_usec_xreadgroup:p50=35.071,p99=1474.559,p99.9=1761.279\nlatency_percentiles_usec_json.set:p50=28.031,p99=73.215,p99.9=380.927\nlatency_percentiles_usec_client|setname:p50=1.003,p99=5.023,p99.9=5.023\nlatency_percentiles_usec_client|setinfo:p50=1.003,p99=4.015,p99.9=5.023\nlatency_percentiles_usec_publish:p50=9.023,p99=16.063,p99.9=87.039\nlatency_percentiles_usec_ping:p50=2.007,p99=4.015,p99.9=33.023\nlatency_percentiles_usec_command|docs:p50=1482.751,p99=1482.751,p99.9=1482.751\nlatency_percentiles_usec_xgroup|create:p50=8.031,p99=153.599,p99.9=153.599\nlatency_percentiles_usec_xadd:p50=5.023,p99=14.015,p99.9=28.031\nlatency_percentiles_usec_expire:p50=27.007,p99=55.039,p99.9=118.271\nlatency_percentiles_usec_FT.AGGREGATE:p50=1245.183,p99=3096.575,p99.9=3473.407\nlatency_percentiles_usec_subscribe:p50=3.007,p99=9.023,p99.9=9.023\nlatency_percentiles_usec_set:p50=1.003,p99=6.015,p99.9=557.055\nlatency_percentiles_usec_multi:p50=2.007,p99=4.015,p99.9=33.023\nlatency_percentiles_usec__FT.CREATE:p50=67.071,p99=428.031,p99.9=428.031\nlatency_percentiles_usec_FT.INFO:p50=45.055,p99=313.343,p99.9=1019.903\nlatency_percentiles_usec_get:p50=6.015,p99=259.071,p99.9=259.071\nlatency_percentiles_usec_exec:p50=270.335,p99=423.935,p99.9=1409.023\nlatency_percentiles_usec_xack:p50=235.519,p99=483.327,p99.9=511.999\nlatency_percentiles_usec_script|load:p50=815.103,p99=815.103,p99.9=815.103\nlatency_percentiles_usec_FT.CREATE:p50=73.215,p99=440.319,p99.9=440.319\nlatency_percentiles_usec_FT.SEARCH:p50=182.271,p99=663.551,p99.9=2146.303\nlatency_percentiles_usec_info:p50=147.455,p99=372.735,p99.9=548.863\n\n# Cluster\ncluster_enabled:0\n\n# Keyspace\ndb0:keys=2031433,expires=1251770,avg_ttl=5439562648,subexpiry=0\n\n# Keysizes\ndb0_distrib_strings_sizes:0=1,2=204\n\n------ CLIENT LIST OUTPUT ------\nid=14 addr=127.0.0.1:46844 laddr=127.0.0.1:6379 fd=26 name=sentinel-a8b75413-cmd age=9361 idle=1 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=11 obl=0 oll=0 omem=0 tot-mem=2048 events=r cmd=ping user=default redir=-1 resp=2 lib-name= lib-ver= io-thread=0 tot-net-in=718244 tot-net-out=6809638 tot-cmds=13816\nid=15 addr=127.0.0.1:46852 laddr=127.0.0.1:6379 fd=27 name=sentinel-a8b75413-pubsub age=9361 idle=0 flags=P db=0 sub=1 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=135 obl=0 oll=0 omem=0 tot-mem=2104 events=r cmd=subscribe user=default redir=-1 resp=2 lib-name= lib-ver= io-thread=0 tot-net-in=104 tot-net-out=2329290 tot-cmds=2\nid=16 addr=127.0.0.1:46854 laddr=127.0.0.1:6379 fd=28 name=sentinel-b93ade23-cmd age=9360 idle=1 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=11 obl=0 oll=0 omem=0 tot-mem=2048 events=r cmd=ping user=default redir=-1 resp=2 lib-name= lib-ver= io-thread=0 tot-net-in=713963 tot-net-out=6810066 tot-cmds=13820\nid=18 addr=127.0.0.1:46864 laddr=127.0.0.1:6379 fd=30 name=sentinel-b93ade23-cmd age=9360 idle=0 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=11 obl=0 oll=0 omem=0 tot-mem=2048 events=r cmd=publish user=default redir=-1 resp=2 lib-name= lib-ver= io-thread=0 tot-net-in=714265 tot-net-out=6810786 tot-cmds=13833\nid=19 addr=127.0.0.1:46866 laddr=127.0.0.1:6379 fd=31 name=sentinel-b93ade23-pubsub age=9360 idle=0 flags=P db=0 sub=1 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=135 obl=0 oll=0 omem=0 tot-mem=2104 events=r cmd=subscribe user=default redir=-1 resp=2 lib-name= lib-ver= io-thread=0 tot-net-in=104 tot-net-out=2329290 tot-cmds=2\nid=64 addr=127.0.0.1:41280 laddr=127.0.0.1:6379 fd=34 name= age=473 idle=411 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2048 events=r cmd=xadd user=default redir=-1 resp=2 lib-name=redis-py lib-ver=6.1.0 io-thread=0 tot-net-in=24515624 tot-net-out=23102030 tot-cmds=61767\nid=17 addr=127.0.0.1:46858 laddr=127.0.0.1:6379 fd=29 name=sentinel-b93ade23-pubsub age=9360 idle=0 flags=P db=0 sub=1 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=135 obl=0 oll=0 omem=0 tot-mem=2104 events=r cmd=subscribe user=default redir=-1 resp=2 lib-name= lib-ver= io-thread=0 tot-net-in=104 tot-net-out=2329290 tot-cmds=2\nid=10 addr=127.0.0.1:46806 laddr=127.0.0.1:6379 fd=24 name=sentinel-b93ade23-cmd age=9361 idle=0 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=11 obl=0 oll=0 omem=0 tot-mem=2048 events=r cmd=publish user=default redir=-1 resp=2 lib-name= lib-ver= io-thread=0 tot-net-in=714251 tot-net-out=6810584 tot-cmds=13832\nid=11 addr=127.0.0.1:46816 laddr=127.0.0.1:6379 fd=25 name=sentinel-b93ade23-pubsub age=9361 idle=0 flags=P db=0 sub=1 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=135 obl=0 oll=0 omem=0 tot-mem=2104 events=r cmd=subscribe user=default redir=-1 resp=2 lib-name= lib-ver= io-thread=0 tot-net-in=104 tot-net-out=2329290 tot-cmds=2\nid=62 addr=127.0.0.1:35166 laddr=127.0.0.1:6379 fd=32 name= age=480 idle=154 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2048 events=r cmd=exec user=default redir=-1 resp=2 lib-name=redis-py lib-ver=6.1.0 io-thread=0 tot-net-in=12543348 tot-net-out=215993 tot-cmds=36779\nid=63 addr=127.0.0.1:35176 laddr=127.0.0.1:6379 fd=33 name= age=480 idle=0 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=5 obl=0 oll=0 omem=0 tot-mem=2048 events=r cmd=xreadgroup user=default redir=-1 resp=2 lib-name=redis-py lib-ver=6.1.0 io-thread=0 tot-net-in=400912 tot-net-out=11300345 tot-cmds=466\nid=65 addr=127.0.0.1:51402 laddr=127.0.0.1:6379 fd=35 name= age=144 idle=0 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=22 obl=0 oll=0 omem=0 tot-mem=2048 events=r cmd=xadd user=default redir=-1 resp=2 lib-name=redis-py lib-ver=6.1.0 io-thread=0 tot-net-in=81312 tot-net-out=75817 tot-cmds=460\n\n------ MODULES INFO OUTPUT ------\n# ReJSON_trace\nReJSON_backtrace:   0: <unknown>\n   1: <unknown>\n   2: modulesCollectInfo\n   3: logModulesInfo\n   4: printCrashReport\n   5: <unknown>\n   6: <unknown>\n   7: moduleTimerHandler\n   8: <unknown>\n   9: aeMain\n  10: main\n  11: <unknown>\n  12: __libc_start_main\n  13: _start\n\n\n# search_version\nsearch_version:8.4.2\nsearch_redis_version:8.4.0 - oss\n\n# search_indexes\nsearch_number_of_indexes:15\nsearch_number_of_active_indexes:0\nsearch_number_of_active_indexes_running_queries:0\nsearch_number_of_active_indexes_indexing:0\nsearch_total_active_write_threads:0\nsearch_total_indexing_time:77126176\n\n# search_fields_statistics\nsearch_fields_text:Text=8,IndexErrors=0\nsearch_fields_numeric:Numeric=59,Sortable=8,IndexErrors=0\nsearch_fields_tag:Tag=55,IndexErrors=98\n\n# search_memory\nsearch_used_memory_indexes:504304078\nsearch_used_memory_indexes_human:480.94184684753418\nsearch_smallest_memory_index:16096\nsearch_smallest_memory_index_human:0.015350341796875\nsearch_largest_memory_index:294779535\nsearch_largest_memory_index_human:281.12367153167725\nsearch_used_memory_vector_index:0\n\n# search_cursors\nsearch_global_idle_user:0\nsearch_global_idle_internal:0\nsearch_global_total_user:0\nsearch_global_total_internal:0\n\n# search_garbage_collector\nsearch_gc_bytes_collected:79523342\nsearch_gc_total_cycles:10\nsearch_gc_total_ms_run:554530\nsearch_gc_total_docs_not_collected:13835\nsearch_gc_marked_deleted_vectors:0\n\n# search_queries\nsearch_total_queries_processed:46579\nsearch_total_query_commands:46579\nsearch_total_query_execution_time_ms:8302\nsearch_total_active_queries:0\n\n# search_warnings_and_errors\nsearch_errors_indexing_failures:98\nsearch_errors_for_index_with_max_failures:98\nsearch_OOM_indexing_failures_indexes_count:0\n\n# search_dialect_statistics\nsearch_dialect_1:0\nsearch_dialect_2:1\nsearch_dialect_3:0\nsearch_dialect_4:0\n\n# search_runtime_configurations\nsearch_extension_load:\nsearch_friso_ini:\nsearch_default_scorer:BM25STD\nsearch_enableGC:ON\nsearch_minimal_term_prefix:2\nsearch_minimal_stem_length:4\nsearch_maximal_prefix_expansions:200\nsearch_query_timeout_ms:500\nsearch_timeout_policy:return\nsearch_oom_policy:return\nsearch_cursor_read_size:1000\nsearch_cursor_max_idle_time:300000\nsearch_max_doc_table_size:1000000\nsearch_max_search_results:1000000\nsearch_max_aggregate_results:2147483648\nsearch_gc_scan_size:100\nsearch_min_phonetic_term_length:3\nsearch_bm25std_tanh_factor:4\n\n# search_current_thread\n\n# search_blocked_queries\n\n# search_blocked_cursors\n\n------ CONFIG DEBUG OUTPUT ------\nslave-read-only yes\nlist-compress-depth 0\nrepl-diskless-load disabled\nsanitize-dump-payload no\nlazyfree-lazy-eviction no\nreplica-read-only yes\nlazyfree-lazy-server-del no\nlazyfree-lazy-expire no\nrepl-diskless-sync yes\nproto-max-bulk-len 512mb\nactivedefrag no\nio-threads 1\nlazyfree-lazy-user-del no\nlazyfree-lazy-user-flush no\nclient-query-buffer-limit 1gb\n\n------ FAST MEMORY TEST ------\n10967:M 25 Nov 2025 16:37:07.929 # Bio worker thread #0 terminated\n10967:M 25 Nov 2025 16:37:07.929 # Bio worker thread #1 terminated\n10967:M 25 Nov 2025 16:37:07.929 # Bio worker thread #2 terminated\n*** Preparing to test memory region 55e42724d000 (2318336 bytes)\n*** Preparing to test memory region 55e4624f6000 (401408 bytes)\n*** Preparing to test memory region 7f4024000000 (135168 bytes)\n*** Preparing to test memory region 7f402a600000 (12582912 bytes)\n*** Preparing to test memory region 7f402b380000 (2041053184 bytes)\n*** Preparing to test memory region 7f40a4f00000 (588251136 bytes)\n*** Preparing to test memory region 7f40c8000000 (135168 bytes)\n*** Preparing to test memory region 7f40cc200000 (2097152 bytes)\n*** Preparing to test memory region 7f40cc4ff000 (26214400 bytes)\n*** Preparing to test memory region 7f40cde00000 (8388608 bytes)\n*** Preparing to test memory region 7f40ce600000 (4194304 bytes)\n*** Preparing to test memory region 7f40cec00000 (8388608 bytes)\n*** Preparing to test memory region 7f40cf400000 (2097152 bytes)\n*** Preparing to test memory region 7f40cf800000 (8523776 bytes)\n*** Preparing to test memory region 7f40d413a000 (27262976 bytes)\n*** Preparing to test memory region 7f40d5b3b000 (8388608 bytes)\n*** Preparing to test memory region 7f40d633c000 (8388608 bytes)\n*** Preparing to test memory region 7f40d6b3d000 (8388608 bytes)\n*** Preparing to test memory region 7f40d733e000 (8388608 bytes)\n*** Preparing to test memory region 7f40d7b3f000 (8388608 bytes)\n*** Preparing to test memory region 7f40d8fe3000 (12288 bytes)\n*** Preparing to test memory region 7f40d93ff000 (4096 bytes)\n*** Preparing to test memory region 7f40d9400000 (12582912 bytes)\n*** Preparing to test memory region 7f40da16b000 (8192 bytes)\n*** Preparing to test memory region 7f40da1a0000 (4096 bytes)\n*** Preparing to test memory region 7f40da1a8000 (28672 bytes)\n*** Preparing to test memory region 7f40da2c0000 (8192 bytes)\n*** Preparing to test memory region 7f40da42b000 (4096 bytes)\n*** Preparing to test memory region 7f40da66c000 (61440 bytes)\n*** Preparing to test memory region 7f40dab8b000 (12288 bytes)\n*** Preparing to test memory region 7f40dad17000 (4096 bytes)\n*** Preparing to test memory region 7f40daf92000 (16384 bytes)\n*** Preparing to test memory region 7f40db084000 (8192 bytes)\n.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O\nFast memory test PASSED, however your memory can still be broken. Please run a memory test for several hours if possible.\n\n------ DUMPING CODE AROUND EIP ------\nSymbol: moduleTimerHandler (base: 0x55e427033500)\nModule: /usr/bin/redis-server 0.0.0.0:6379 (base 0x55e426e7a000)\n$ xxd -r -p /tmp/dump.hex /tmp/dump.bin\n$ objdump --adjust-vma=0x55e427033500 -D -b binary -m i386:x86-64 /tmp/dump.bin\n------\n10967:M 25 Nov 2025 16:37:17.030 # dump of function (hexdump of 467 bytes):\nf30f1efa5531f64889e5415741564c8d35ca2e1200415541544c8da5e0fdffff53488d9d60fdffff4889df4881ec98020000660f6f05a64f160064488b042528000000488945c831c0488b0518474400c745b0000000000f298500feffff660f6f058a4f1600488985e8fdffff488d8510feffff48898558fdffff488985f0fdffff488d85b0feffffc785e0fdffff0200000048c785f8fdffff0000000048c745b80000000048c745c00000000048898550fdffff48898598feffff0f2985a0feffffe888c5ecff4c69ad60fdffff40420f004c03ad68fdffffe9a4000000904c8bbdf0fdfffff685e0fdffff020f853c010000498b17480fca4939d50f823d0100004c8bbdf8fdffffba400000004889df498b37e8d6fcfeff49634718488b8d70fdffff85c07823488d1530cd22003b82541900007d14488d3480488d3470488b4240488d04f048894120498b77104889df41ff57084889dfe8b148ffff488b3d02464400488b9500feffff31c9488bb5f0fdffffe8b5b501004c89ffe82da5f0ff31c931d24c89f64c89e7e82ea7010031f64c89e7e8d49c010085c00f853cffffffe847cdecff4c8bbdf0fdffffc7000c000000bb010000004d85ff7414488b8558fdffff4939c774084c89ffe8dca4f0\n\n=== REDIS BUG REPORT END. Make sure to include from START to END. ===\n```\n\n**Crash report from Redis 8.2.3**\n```\n=== REDIS BUG REPORT START: Cut & paste starting from here ===\n1661497:M 11 Nov 2025 07:37:44.496 # Redis 8.2.3 crashed by signal: 11, si_code: 128\n1661497:M 11 Nov 2025 07:37:44.496 # Accessing address: (nil)\n1661497:M 11 Nov 2025 07:37:44.496 # Crashed running the instruction at: 0x653fbd269fe1\n\n------ STACK TRACE ------\nEIP:\n/usr/bin/redis-server 0.0.0.0:6380(moduleTimerHandler+0x1c1)[0x653fbd269fe1]\n\n1661510 bio_close_file\n/lib/x86_64-linux-gnu/libc.so.6(+0x98d71)[0x7b52f9c98d71]\n/lib/x86_64-linux-gnu/libc.so.6(pthread_cond_wait+0x20d)[0x7b52f9c9b7ed]\n/usr/bin/redis-server 0.0.0.0:6380(bioProcessBackgroundJobs+0x1ea)[0x653fbd234fba]\n/lib/x86_64-linux-gnu/libc.so.6(+0x9caa4)[0x7b52f9c9caa4]\n/lib/x86_64-linux-gnu/libc.so.6(+0x129c6c)[0x7b52f9d29c6c]\n\n1661511 bio_aof\n/lib/x86_64-linux-gnu/libc.so.6(+0x98d71)[0x7b52f9c98d71]\n/lib/x86_64-linux-gnu/libc.so.6(pthread_cond_wait+0x20d)[0x7b52f9c9b7ed]\n/usr/bin/redis-server 0.0.0.0:6380(bioProcessBackgroundJobs+0x1ea)[0x653fbd234fba]\n/lib/x86_64-linux-gnu/libc.so.6(+0x9caa4)[0x7b52f9c9caa4]\n/lib/x86_64-linux-gnu/libc.so.6(+0x129c6c)[0x7b52f9d29c6c]\n\n1661497 redis-server *\n/lib/x86_64-linux-gnu/libc.so.6(+0x45330)[0x7b52f9c45330]\n/usr/bin/redis-server 0.0.0.0:6380(moduleTimerHandler+0x1c1)[0x653fbd269fe1]\n/usr/bin/redis-server 0.0.0.0:6380(+0x94813)[0x653fbd15c813]\n/usr/bin/redis-server 0.0.0.0:6380(aeMain+0x2d)[0x653fbd15cacd]\n/usr/bin/redis-server 0.0.0.0:6380(main+0x3cb)[0x653fbd155cdb]\n/lib/x86_64-linux-gnu/libc.so.6(+0x2a1ca)[0x7b52f9c2a1ca]\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x8b)[0x7b52f9c2a28b]\n/usr/bin/redis-server 0.0.0.0:6380(_start+0x25)[0x653fbd157535]\n\n1661512 bio_lazy_free\n/lib/x86_64-linux-gnu/libc.so.6(+0x98d71)[0x7b52f9c98d71]\n/lib/x86_64-linux-gnu/libc.so.6(pthread_cond_wait+0x20d)[0x7b52f9c9b7ed]\n/usr/bin/redis-server 0.0.0.0:6380(bioProcessBackgroundJobs+0x1ea)[0x653fbd234fba]\n/lib/x86_64-linux-gnu/libc.so.6(+0x9caa4)[0x7b52f9c9caa4]\n/lib/x86_64-linux-gnu/libc.so.6(+0x129c6c)[0x7b52f9d29c6c]\n\n1661573 gc-3454\n/lib/x86_64-linux-gnu/libc.so.6(+0x98d71)[0x7b52f9c98d71]\n/lib/x86_64-linux-gnu/libc.so.6(pthread_cond_wait+0x20d)[0x7b52f9c9b7ed]\n/etc/redis/modules/redisearch.so(+0x32c9f3)[0x7b52f7f2c9f3]\n/etc/redis/modules/redisearch.so(+0x32c8d7)[0x7b52f7f2c8d7]\n/lib/x86_64-linux-gnu/libc.so.6(+0x9caa4)[0x7b52f9c9caa4]\n/lib/x86_64-linux-gnu/libc.so.6(+0x129c6c)[0x7b52f9d29c6c]\n\n5/5 expected stacktraces.\n\n------ STACK TRACE DONE ------\n\n------ REGISTERS ------\n1661497:M 11 Nov 2025 07:37:44.510 #\nRAX:000000006968890c RBX:00007ffce647d310\nRCX:00007b52f974b700 RDX:0000653fbd4845a0\nRDI:00007ffce647d310 RSI:6574616572635f6e\nRBP:00007ffce647d5b0 RSP:00007ffce647d300\nR8 :0000000000000000 R9 :0000000000000000\nR10:0000000000000000 R11:0000000000000000\nR12:00007ffce647d390 R13:0006434cb93b1663\nR14:0000653fbd38340b R15:00007b5247528000\nRIP:0000653fbd269fe1 EFL:0000000000010206\nCSGSFS:002b000000000033\n1661497:M 11 Nov 2025 07:37:44.510 # (00007ffce647d30f) -> 000013b7e17bc890\n1661497:M 11 Nov 2025 07:37:44.510 # (00007ffce647d30e) -> 0000000000000000\n1661497:M 11 Nov 2025 07:37:44.510 # (00007ffce647d30d) -> 0000000000000000\n1661497:M 11 Nov 2025 07:37:44.510 # (00007ffce647d30c) -> 0000000000000000\n1661497:M 11 Nov 2025 07:37:44.510 # (00007ffce647d30b) -> 0000000000000000\n1661497:M 11 Nov 2025 07:37:44.510 # (00007ffce647d30a) -> 0000000000000000\n1661497:M 11 Nov 2025 07:37:44.510 # (00007ffce647d309) -> 0000000000000000\n1661497:M 11 Nov 2025 07:37:44.510 # (00007ffce647d308) -> 0000000000000040\n1661497:M 11 Nov 2025 07:37:44.510 # (00007ffce647d307) -> 0000000000000000\n1661497:M 11 Nov 2025 07:37:44.510 # (00007ffce647d306) -> 0000000000000000\n1661497:M 11 Nov 2025 07:37:44.510 # (00007ffce647d305) -> 0000000000000000\n1661497:M 11 Nov 2025 07:37:44.510 # (00007ffce647d304) -> 00007b52f974b700\n1661497:M 11 Nov 2025 07:37:44.511 # (00007ffce647d303) -> 0106003100000e51\n1661497:M 11 Nov 2025 07:37:44.511 # (00007ffce647d302) -> 0000653fbd256d40\n1661497:M 11 Nov 2025 07:37:44.511 # (00007ffce647d301) -> 00007ffce647d460\n1661497:M 11 Nov 2025 07:37:44.511 # (00007ffce647d300) -> 00007ffce647d3c0\n\n------ INFO OUTPUT ------\n# Server\nredis_version:8.2.3\nredis_git_sha1:00000000\nredis_git_dirty:1\nredis_build_id:8ef393086911f0f6\nredis_mode:standalone\nos:Linux 6.8.0-1024-aws x86_64\narch_bits:64\nmonotonic_clock:POSIX clock_gettime\nmultiplexing_api:epoll\natomicvar_api:c11-builtin\ngcc_version:13.3.0\nprocess_id:1661497\nprocess_supervised:no\nrun_id:01f61f7c299f9d519b12dcabbdefee6f3007f8e0\ntcp_port:6380\nserver_time_usec:1762846664496741\nuptime_in_seconds:594\nuptime_in_days:0\nhz:10\nconfigured_hz:10\nlru_clock:1238984\nexecutable:/usr/bin/redis-server\nconfig_file:/etc/redis/6380/redis.conf\nio_threads_active:0\nlistener0:name=tcp,bind=0.0.0.0,port=6380\n\n# Clients\nconnected_clients:9\ncluster_connections:0\nmaxclients:10000\nclient_recent_max_input_buffer:20512\nclient_recent_max_output_buffer:20632\nblocked_clients:0\ntracking_clients:0\npubsub_clients:3\nwatching_clients:0\nclients_in_timeout_table:0\ntotal_watched_keys:0\ntotal_blocking_keys:0\ntotal_blocking_keys_on_nokey:0\n\n# Memory\nused_memory:2839127088\nused_memory_human:2.64G\nused_memory_rss:2895347712\nused_memory_rss_human:2.70G\nused_memory_peak:2845157072\nused_memory_peak_human:2.65G\nused_memory_peak_time:1762846193\nused_memory_peak_perc:99.79%\nused_memory_overhead:160391784\nused_memory_startup:879808\nused_memory_dataset:2678735304\nused_memory_dataset_perc:94.38%\nallocator_allocated:2840174936\nallocator_active:2844983296\nallocator_resident:2888929280\nallocator_muzzy:0\ntotal_system_memory:33263804416\ntotal_system_memory_human:30.98G\nused_memory_lua:32768\nused_memory_vm_eval:32768\nused_memory_lua_human:32.00K\nused_memory_scripts_eval:0\nnumber_of_cached_scripts:0\nnumber_of_functions:0\nnumber_of_libraries:0\nused_memory_vm_functions:33792\nused_memory_vm_total:66560\nused_memory_vm_total_human:65.00K\nused_memory_functions:192\nused_memory_scripts:192\nused_memory_scripts_human:192B\nmaxmemory:0\nmaxmemory_human:0B\nmaxmemory_policy:noeviction\nallocator_frag_ratio:1.00\nallocator_frag_bytes:4732328\nallocator_rss_ratio:1.02\nallocator_rss_bytes:43945984\nrss_overhead_ratio:1.00\nrss_overhead_bytes:6418432\nmem_fragmentation_ratio:1.02\nmem_fragmentation_bytes:56201896\nmem_not_counted_for_evict:13472\nmem_replication_backlog:1066224\nmem_total_replication_buffers:1066208\nmem_replica_full_sync_buffer:0\nmem_clients_slaves:0\nmem_clients_normal:41000\nmem_cluster_links:0\nmem_aof_buffer:0\nmem_allocator:jemalloc-5.3.0\nmem_overhead_db_hashtable_rehashing:0\nactive_defrag_running:0\nlazyfree_pending_objects:0\nlazyfreed_objects:0\n\n# Persistence\nloading:0\nasync_loading:0\ncurrent_cow_peak:0\ncurrent_cow_size:0\ncurrent_cow_size_age:0\ncurrent_fork_perc:0.00\ncurrent_save_keys_processed:0\ncurrent_save_keys_total:0\nrdb_changes_since_last_save:1887\nrdb_bgsave_in_progress:0\nrdb_last_save_time:1762846633\nrdb_last_bgsave_status:ok\nrdb_last_bgsave_time_sec:28\nrdb_current_bgsave_time_sec:-1\nrdb_saves:1\nrdb_last_cow_size:11452416\nrdb_last_load_keys_expired:3\nrdb_last_load_keys_loaded:2810514\naof_enabled:0\naof_rewrite_in_progress:0\naof_rewrite_scheduled:0\naof_last_rewrite_time_sec:-1\naof_current_rewrite_time_sec:-1\naof_last_bgrewrite_status:ok\naof_rewrites:0\naof_rewrites_consecutive_failures:0\naof_last_write_status:ok\naof_last_cow_size:0\nmodule_fork_in_progress:0\nmodule_fork_last_cow_size:0\n\n# Threads\nio_thread_0:clients=9,reads=10668,writes=15523\n\n# Stats\ntotal_connections_received:143\ntotal_commands_processed:20172\ninstantaneous_ops_per_sec:55\ntotal_net_input_bytes:5134614\ntotal_net_output_bytes:8038138\ntotal_net_repl_input_bytes:0\ntotal_net_repl_output_bytes:3935461\ninstantaneous_input_kbps:14.29\ninstantaneous_output_kbps:19.41\ninstantaneous_input_repl_kbps:0.00\ninstantaneous_output_repl_kbps:0.00\nrejected_connections:0\nsync_full:0\nsync_partial_ok:1\nsync_partial_err:0\nexpired_subkeys:0\nexpired_keys:0\nexpired_stale_perc:0.00\nexpired_time_cap_reached_count:0\nexpire_cycle_cpu_milliseconds:139\nevicted_keys:0\nevicted_clients:0\nevicted_scripts:0\ntotal_eviction_exceeded_time:0\ncurrent_eviction_exceeded_time:0\nkeyspace_hits:2812030\nkeyspace_misses:4071\npubsub_channels:1\npubsub_patterns:0\npubsubshard_channels:0\nlatest_fork_usec:69097\ntotal_forks:2\nmigrate_cached_sockets:0\nslave_expires_tracked_keys:0\nactive_defrag_hits:0\nactive_defrag_misses:0\nactive_defrag_key_hits:0\nactive_defrag_key_misses:0\ntotal_active_defrag_time:0\ncurrent_active_defrag_time:0\ntracking_total_keys:0\ntracking_total_items:0\ntracking_total_prefixes:0\nunexpected_error_replies:0\ntotal_error_replies:597\ndump_payload_sanitizations:0\ntotal_reads_processed:10668\ntotal_writes_processed:15523\nio_threaded_reads_processed:0\nio_threaded_writes_processed:0\nio_threaded_total_prefetch_batches:0\nio_threaded_total_prefetch_entries:0\nclient_query_buffer_limit_disconnections:0\nclient_output_buffer_limit_disconnections:0\nreply_buffer_shrinks:150\nreply_buffer_expands:141\neventloop_cycles:13381\neventloop_duration_sum:53654352\neventloop_duration_cmd_sum:49431738\ninstantaneous_eventloop_cycles_per_sec:32\ninstantaneous_eventloop_duration_usec:3916\nacl_access_denied_auth:0\nacl_access_denied_cmd:0\nacl_access_denied_key:0\nacl_access_denied_channel:0\n\n# Replication\nrole:master\nconnected_slaves:0\nmaster_failover_state:no-failover\nmaster_replid:d865f87722f4ca6c61807e47d09ed1b00059bd1c\nmaster_replid2:7f5459ea4ae50cb961c8e1fb730168e86baa94db\nmaster_repl_offset:625002459422\nsecond_repl_offset:624998219543\nrepl_backlog_active:1\nrepl_backlog_size:1048576\nrepl_backlog_first_byte_offset:625001408183\nrepl_backlog_histlen:1051240\n\n# CPU\nused_cpu_sys:5.307908\nused_cpu_user:171.913945\nused_cpu_sys_children:5.450361\nused_cpu_user_children:28.554061\nused_cpu_sys_main_thread:5.006469\nused_cpu_user_main_thread:171.890080\n\n# Modules\nmodule:name=ReJSON,ver=80201,api=1,filters=0,usedby=[search],using=[],options=[handle-io-errors]\nmodule:name=search,ver=999999,api=1,filters=0,usedby=[],using=[ReJSON],options=[handle-io-errors]\nmodule:name=vectorset,ver=1,api=1,filters=0,usedby=[],using=[],options=[handle-io-errors|handle-repl-async-load]\n\n# Commandstats\ncmdstat_get:calls=1,usec=7,usec_per_call=7.00,rejected_calls=0,failed_calls=0\ncmdstat_publish:calls=894,usec=8796,usec_per_call=9.84,rejected_calls=0,failed_calls=0\ncmdstat_xgroup|create:calls=2,usec=26,usec_per_call=13.00,rejected_calls=0,failed_calls=2\ncmdstat_xadd:calls=1510,usec=37700,usec_per_call=24.97,rejected_calls=0,failed_calls=0\ncmdstat_expire:calls=4924,usec=145026,usec_per_call=29.45,rejected_calls=0,failed_calls=0\ncmdstat_FT.INFO:calls=14,usec=2819,usec_per_call=201.36,rejected_calls=0,failed_calls=0\ncmdstat_multi:calls=202,usec=585,usec_per_call=2.90,rejected_calls=0,failed_calls=0\ncmdstat_ping:calls=1364,usec=3202,usec_per_call=2.35,rejected_calls=473,failed_calls=0\ncmdstat_xack:calls=202,usec=11169,usec_per_call=55.29,rejected_calls=0,failed_calls=0\ncmdstat_psync:calls=1,usec=108,usec_per_call=108.00,rejected_calls=122,failed_calls=0\ncmdstat_config|get:calls=2,usec=7,usec_per_call=3.50,rejected_calls=0,failed_calls=0\ncmdstat_auth:calls=143,usec=824,usec_per_call=5.76,rejected_calls=0,failed_calls=0\ncmdstat_replconf:calls=795,usec=1429,usec_per_call=1.80,rejected_calls=0,failed_calls=0\ncmdstat_info:calls=193,usec=22545,usec_per_call=116.81,rejected_calls=0,failed_calls=0\ncmdstat_exec:calls=202,usec=74426,usec_per_call=368.45,rejected_calls=0,failed_calls=0\ncmdstat_json.set:calls=4925,usec=283383,usec_per_call=57.54,rejected_calls=0,failed_calls=0\ncmdstat_xreadgroup:calls=226,usec=33128,usec_per_call=146.58,rejected_calls=0,failed_calls=0\ncmdstat_FT.SEARCH:calls=4535,usec=48877809,usec_per_call=10777.91,rejected_calls=0,failed_calls=0\ncmdstat_client|setinfo:calls=28,usec=40,usec_per_call=1.43,rejected_calls=0,failed_calls=0\ncmdstat_client|setname:calls=6,usec=4,usec_per_call=0.67,rejected_calls=0,failed_calls=0\ncmdstat_subscribe:calls=3,usec=9,usec_per_call=3.00,rejected_calls=0,failed_calls=0\n\n# Errorstats\nerrorstat_BUSYGROUP:count=2\nerrorstat_LOADING:count=472\nerrorstat_NOAUTH:count=123\n\n# Latencystats\nlatency_percentiles_usec_get:p50=7.007,p99=7.007,p99.9=7.007\nlatency_percentiles_usec_publish:p50=9.023,p99=35.071,p99.9=166.911\nlatency_percentiles_usec_xgroup|create:p50=10.047,p99=16.063,p99.9=16.063\nlatency_percentiles_usec_xadd:p50=20.095,p99=144.383,p99.9=260.095\nlatency_percentiles_usec_expire:p50=1.003,p99=315.391,p99.9=1187.839\nlatency_percentiles_usec_FT.INFO:p50=158.719,p99=651.263,p99.9=651.263\nlatency_percentiles_usec_multi:p50=3.007,p99=16.063,p99.9=61.183\nlatency_percentiles_usec_ping:p50=2.007,p99=10.047,p99.9=77.311\nlatency_percentiles_usec_xack:p50=42.239,p99=294.911,p99.9=479.231\nlatency_percentiles_usec_psync:p50=108.031,p99=108.031,p99.9=108.031\nlatency_percentiles_usec_config|get:p50=3.007,p99=4.015,p99.9=4.015\nlatency_percentiles_usec_auth:p50=5.023,p99=17.023,p99.9=66.047\nlatency_percentiles_usec_replconf:p50=1.003,p99=4.015,p99.9=78.335\nlatency_percentiles_usec_info:p50=109.055,p99=282.623,p99.9=581.631\nlatency_percentiles_usec_exec:p50=294.911,p99=1458.175,p99.9=1548.287\nlatency_percentiles_usec_json.set:p50=11.007,p99=557.055,p99.9=1859.583\nlatency_percentiles_usec_xreadgroup:p50=93.183,p99=452.607,p99.9=5767.167\nlatency_percentiles_usec_FT.SEARCH:p50=261.119,p99=49283.071,p99.9=77070.335\nlatency_percentiles_usec_client|setinfo:p50=1.003,p99=3.007,p99.9=3.007\nlatency_percentiles_usec_client|setname:p50=0.001,p99=2.007,p99.9=2.007\nlatency_percentiles_usec_subscribe:p50=2.007,p99=6.015,p99.9=6.015\n\n# Cluster\ncluster_enabled:0\n\n# Keyspace\ndb0:keys=2807990,expires=1123168,avg_ttl=6117468128,subexpiry=0\ndb2:keys=3119,expires=0,avg_ttl=0,subexpiry=0\ndb4:keys=952,expires=0,avg_ttl=0,subexpiry=0\n\n# Keysizes\ndb0_distrib_strings_sizes:2=204\ndb2_distrib_strings_sizes:16=3,256=420,512=798,1K=79,2K=1807,4K=7\ndb2_distrib_lists_items:1=5\ndb4_distrib_strings_sizes:256=420,512=532\n\n------ CLIENT LIST OUTPUT ------\nid=167 addr=18.193.193.235:57241 laddr=172.31.14.123:6380 fd=28 name= age=442 idle=1 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=4 obl=0 oll=0 omem=0 tot-mem=1920 events=r cmd=xreadgroup user=default redir=-1 resp=2 lib-name=redis-py lib-ver=6.1.0 io-thread=0 tot-net-in=83885 tot-net-out=1033424 tot-cmds=433\nid=166 addr=18.193.193.235:57237 laddr=172.31.14.123:6380 fd=26 name= age=443 idle=1 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=20474 argv-mem=0 multi-mem=0 rbs=1024 rbp=496 obl=0 oll=0 omem=0 tot-mem=22400 events=r cmd=expire user=default redir=-1 resp=2 lib-name=redis-py lib-ver=6.1.0 io-thread=0 tot-net-in=2594371 tot-net-out=107810 tot-cmds=10256\nid=28 addr=35.157.48.48:41680 laddr=172.31.14.123:6380 fd=20 name=sentinel-9149f551-cmd age=593 idle=0 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=2048 rbp=1024 obl=0 oll=0 omem=0 tot-mem=2944 events=r cmd=info user=default redir=-1 resp=2 lib-name= lib-ver= io-thread=0 tot-net-in=51075 tot-net-out=428458 tot-cmds=814\nid=165 addr=18.193.193.235:57231 laddr=172.31.14.123:6380 fd=19 name= age=457 idle=0 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=22 obl=0 oll=0 omem=0 tot-mem=1920 events=r cmd=FT.SEARCH user=default redir=-1 resp=2 lib-name=redis-py lib-ver=6.1.0 io-thread=0 tot-net-in=2241075 tot-net-out=1283045 tot-cmds=6066\nid=29 addr=35.157.48.48:41694 laddr=172.31.14.123:6380 fd=23 name=sentinel-9149f551-pubsub age=593 idle=1 flags=P db=0 sub=1 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=142 obl=0 oll=0 omem=0 tot-mem=1976 events=r cmd=subscribe user=default redir=-1 resp=2 lib-name= lib-ver= io-thread=0 tot-net-in=142 tot-net-out=126425 tot-cmds=3\nid=30 addr=18.196.187.37:45272 laddr=172.31.14.123:6380 fd=24 name=sentinel-bfafff2d-pubsub age=593 idle=1 flags=P db=0 sub=1 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=142 obl=0 oll=0 omem=0 tot-mem=1976 events=r cmd=subscribe user=default redir=-1 resp=2 lib-name= lib-ver= io-thread=0 tot-net-in=142 tot-net-out=126425 tot-cmds=3\nid=31 addr=18.196.187.37:45264 laddr=172.31.14.123:6380 fd=25 name=sentinel-bfafff2d-cmd age=593 idle=0 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=2048 rbp=1024 obl=0 oll=0 omem=0 tot-mem=2944 events=r cmd=ping user=default redir=-1 resp=2 lib-name= lib-ver= io-thread=0 tot-net-in=51331 tot-net-out=415555 tot-cmds=815\nid=26 addr=63.177.164.20:43894 laddr=172.31.14.123:6380 fd=21 name=sentinel-812f4f62-pubsub age=594 idle=1 flags=P db=0 sub=1 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=142 obl=0 oll=0 omem=0 tot-mem=1976 events=r cmd=subscribe user=default redir=-1 resp=2 lib-name= lib-ver= io-thread=0 tot-net-in=142 tot-net-out=126708 tot-cmds=3\nid=27 addr=63.177.164.20:43892 laddr=172.31.14.123:6380 fd=22 name=sentinel-812f4f62-cmd age=594 idle=0 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=2048 rbp=1024 obl=0 oll=0 omem=0 tot-mem=2944 events=r cmd=ping user=default redir=-1 resp=2 lib-name= lib-ver= io-thread=0 tot-net-in=50975 tot-net-out=441587 tot-cmds=819\n\n------ MODULES INFO OUTPUT ------\n# ReJSON_trace\nReJSON_backtrace:   0: <unknown>\n   1: <unknown>\n   2: modulesCollectInfo\n   3: logModulesInfo\n   4: printCrashReport\n   5: <unknown>\n   6: <unknown>\n   7: moduleTimerHandler\n   8: <unknown>\n   9: aeMain\n  10: main\n  11: <unknown>\n  12: __libc_start_main\n  13: _start\n\n\n# search_version\nsearch_version:99.99.99\nsearch_redis_version:8.2.3 - oss\n\n# search_indexes\nsearch_number_of_indexes:17\nsearch_number_of_active_indexes:0\nsearch_number_of_active_indexes_running_queries:0\nsearch_number_of_active_indexes_indexing:0\nsearch_total_active_write_threads:0\nsearch_total_indexing_time:83466432\n\n# search_fields_statistics\nsearch_fields_text:Text=8,IndexErrors=0\nsearch_fields_numeric:Numeric=64,Sortable=16,IndexErrors=0\nsearch_fields_tag:Tag=55,Sortable=1,IndexErrors=0\n\n# search_memory\nsearch_used_memory_indexes:642989844\nsearch_used_memory_indexes_human:613.20289993286133\nsearch_smallest_memory_index:16096\nsearch_smallest_memory_index_human:0.015350341796875\nsearch_largest_memory_index:210102595\nsearch_largest_memory_index_human:200.3694486618042\nsearch_used_memory_vector_index:0\n\n# search_cursors\nsearch_global_idle_user:0\nsearch_global_idle_internal:0\nsearch_global_total_user:0\nsearch_global_total_internal:0\n\n# search_garbage_collector\nsearch_gc_bytes_collected:9881\nsearch_gc_total_cycles:1\nsearch_gc_total_ms_run:282355\nsearch_gc_total_docs_not_collected:1291\nsearch_gc_marked_deleted_vectors:0\n\n# search_queries\nsearch_total_queries_processed:4535\nsearch_total_query_commands:4535\nsearch_total_query_execution_time_ms:48606\nsearch_total_active_queries:0\n\n# search_warnings_and_errors\nsearch_errors_indexing_failures:0\nsearch_errors_for_index_with_max_failures:0\nsearch_OOM_indexing_failures_indexes_count:0\n\n# search_dialect_statistics\nsearch_dialect_1:0\nsearch_dialect_2:1\nsearch_dialect_3:0\nsearch_dialect_4:0\n\n# search_runtime_configurations\nsearch_extension_load:\nsearch_friso_ini:\nsearch_enableGC:ON\nsearch_minimal_term_prefix:2\nsearch_minimal_stem_length:4\nsearch_maximal_prefix_expansions:200\nsearch_query_timeout_ms:500\nsearch_timeout_policy:return\nsearch_cursor_read_size:1000\nsearch_cursor_max_idle_time:300000\nsearch_max_doc_table_size:1000000\nsearch_max_search_results:1000000\nsearch_max_aggregate_results:2147483648\nsearch_gc_scan_size:100\nsearch_min_phonetic_term_length:3\nsearch_bm25std_tanh_factor:4\n\n# search_current_thread\n\n# search_blocked_queries\n\n# search_blocked_cursors\n\n------ CONFIG DEBUG OUTPUT ------\nlazyfree-lazy-expire no\nreplica-read-only yes\nlazyfree-lazy-user-flush no\nslave-read-only yes\nactivedefrag no\nlazyfree-lazy-user-del no\nsanitize-dump-payload no\nrepl-diskless-load disabled\nlazyfree-lazy-server-del no\nlist-compress-depth 0\nclient-query-buffer-limit 1gb\nrepl-diskless-sync yes\nlazyfree-lazy-eviction no\nio-threads 1\nproto-max-bulk-len 512mb\n\n------ FAST MEMORY TEST ------\n1661497:M 11 Nov 2025 07:37:44.516 # Bio worker thread #0 terminated\n1661497:M 11 Nov 2025 07:37:44.516 # Bio worker thread #1 terminated\n1661497:M 11 Nov 2025 07:37:44.517 # Bio worker thread #2 terminated\n*** Preparing to test memory region 653fbd471000 (2318336 bytes)\n*** Preparing to test memory region 653fea7b9000 (401408 bytes)\n*** Preparing to test memory region 7b5230000000 (135168 bytes)\n*** Preparing to test memory region 7b5236400000 (4194304 bytes)\n*** Preparing to test memory region 7b5236801000 (8388608 bytes)\n*** Preparing to test memory region 7b5237200000 (3075473408 bytes)\n*** Preparing to test memory region 7b52ee800000 (11534336 bytes)\n*** Preparing to test memory region 7b52ef400000 (7864320 bytes)\n*** Preparing to test memory region 7b52efc00000 (3145728 bytes)\n*** Preparing to test memory region 7b52f0000000 (135168 bytes)\n*** Preparing to test memory region 7b52f4200000 (2621440 bytes)\n*** Preparing to test memory region 7b52f4601000 (8388608 bytes)\n*** Preparing to test memory region 7b52f5001000 (8388608 bytes)\n*** Preparing to test memory region 7b52f5a01000 (8388608 bytes)\n*** Preparing to test memory region 7b52f6401000 (8388608 bytes)\n*** Preparing to test memory region 7b52f6e01000 (8388608 bytes)\n*** Preparing to test memory region 7b52f7bf4000 (4096 bytes)\n*** Preparing to test memory region 7b52f8934000 (8192 bytes)\n*** Preparing to test memory region 7b52f8a00000 (4194304 bytes)\n*** Preparing to test memory region 7b52f9200000 (8388608 bytes)\n*** Preparing to test memory region 7b52f9e05000 (53248 bytes)\n*** Preparing to test memory region 7b52f9fff000 (4096 bytes)\n*** Preparing to test memory region 7b52fa510000 (12288 bytes)\n*** Preparing to test memory region 7b52fa550000 (28672 bytes)\n*** Preparing to test memory region 7b52fa5ae000 (8192 bytes)\n*** Preparing to test memory region 7b52fa87a000 (16384 bytes)\n*** Preparing to test memory region 7b52fa88b000 (8192 bytes)\n*** Preparing to test memory region 7b52faa16000 (4096 bytes)\n*** Preparing to test memory region 7b52fab07000 (8192 bytes)\n.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O\nFast memory test PASSED, however your memory can still be broken. Please run a memory test for several hours if possible.\n\n------ DUMPING CODE AROUND EIP ------\nSymbol: moduleTimerHandler (base: 0x653fbd269e20)\nModule: /usr/bin/redis-server 0.0.0.0:6380 (base 0x653fbd0c8000)\n$ xxd -r -p /tmp/dump.hex /tmp/dump.bin\n$ objdump --adjust-vma=0x653fbd269e20 -D -b binary -m i386:x86-64 /tmp/dump.bin\n------\n1661497:M 11 Nov 2025 07:37:54.835 # dump of function (hexdump of 480 bytes):\nf30f1efa5531f64889e5415741564c8d35d6951100415541544c8da5e0fdffff53488d9d60fdffff4889df4881ec88020000660f6f05167b150064488b042528000000488945c831c0488b05781e4300c745b0000000000f298500feffff660f6f05fa7a1500488985e8fdffff488d8510feffff48898550fdffff488985f0fdffff488d85b0feffffc785e0fdffff0200000048c785f8fdffff0000000048c745b80000000048c745c00000000048898558fdffff48898598feffff0f2985a0feffffe868ecedff4c69ad60fdffff40420f004c03ad68fdffffe912010000904c8bbdf0fdfffff685e0fdffff020f859c010000498b170fb6c6440fb6da4989d14989d04989c24c89d849c1e9104889d748c1e008450fb6c949c1e8184889d64c09d0450fb6c048c1ef204889d148c1e008400fb6ff48c1ee284c09c8400fb6f648c1e93048c1e0080fb6c94c09c048c1e0084809f848c1e0084809f048c1e0084809c8480fa4d0084989c74939c50f822b0100004c8bbdf8fdffffba400000004889df498b37e82403ffff49634718488b8d70fdffff85c0781f488d15dea521003b82181900007d10488d048048c1e0044803424048894120498b77104889df41ff57084889dfe89350ffff488b3df41c4300488b9500feffff31c9488bb5\n\n=== REDIS BUG REPORT END. Make sure to include from START to END. ===\n\n```\n\n**Additional information**\n\n1. OS distribution and version\nDistributor ID: Ubuntu\nDescription:    Ubuntu 24.04.2 LTS\nRelease:        24.04\nCodename:       noble\n\n2. Steps to reproduce (if any)\nCan't provide reliable way to reproduce\nRedis was used in following way: \nFirst process reads data puts into xgroup\nSecond process processed data and save into redis search indexes \nThird process is actively using redis search \n3 Sentinels are used as well\n\nlargest index has `1240409` keys \nDBSIZE `1279320`\n",
      "solution": "seem that this issue is similar to https://github.com/RediSearch/RediSearch/issues/7156#issuecomment-3575163124\ncan you try it to check if it's fixed?\n\n---\n\nAlso duplicates #14469.\nShould be fixed in the next patches of 8.4 and 8.2 (8.4.1 and 8.2.4)\n\n---\n\n@sundb I built redis-search from master and tried to reproduce with new .so module.\nWhile loading data into index I've got another crash at least 2 times: \n\n```\n19372:C 26 Nov 2025 15:44:13.645 * Fork CoW for Module fork: current 26 MB, peak 26 MB, average 25 MB\n19062:M 26 Nov 2025 15:44:21.942 * <search> DocTable capacity increase from 129944 to 194917\n19062:M 26 Nov 2025 15:44:36.458 * <search> DocTable capacity increase from 194917 to 292376\n19062:M 26 Nov 2025 15:44:44.028 * <search> Scanning index veloroute_mc_ip0040t1 in background: done (scanned=1383289)\n\n\n=== REDIS BUG REPORT START: Cut & paste starting from here ===\n19062:M 26 Nov 2025 15:45:06.863 # Redis 8.4.0 crashed by signal: 11, si_code: 1\n19062:M 26 Nov 2025 15:45:06.863 # Accessing address: 0x18\n19062:M 26 Nov 2025 15:45:06.863 # Crashed running the instruction at: 0x7f9815757670\n\n------ STACK TRACE ------\nEIP:\n/usr/lib/redis/modules/redisearch2.so(GcScanDelta_LastBlockIdx+0x0)[0x7f9815757670]\n\n19064 bio_close_file\n/lib/x86_64-linux-gnu/libc.so.6(+0x98d71)[0x7f98172d6d71]\n/lib/x86_64-linux-gnu/libc.so.6(pthread_cond_wait+0x20d)[0x7f98172d97ed]\n/usr/bin/redis-server 0.0.0.0:6379(bioProcessBackgroundJobs+0x1ea)[0x5612a9090cda]\n/lib/x86_64-linux-gnu/libc.so.6(+0x9caa4)[0x7f98172daaa4]\n/lib/x86_64-linux-gnu/libc.so.6(__clone+0x44)[0x7f9817367a64]\n\n19066 bio_lazy_free\n/lib/x86_64-linux-gnu/libc.so.6(+0x98d71)[0x7f98172d6d71]\n/lib/x86_64-linux-gnu/libc.so.6(pthread_cond_wait+0x20d)[0x7f98172d97ed]\n/usr/bin/redis-server 0.0.0.0:6379(bioProcessBackgroundJobs+0x1ea)[0x5612a9090cda]\n/lib/x86_64-linux-gnu/libc.so.6(+0x9caa4)[0x7f98172daaa4]\n/lib/x86_64-linux-gnu/libc.so.6(__clone+0x44)[0x7f9817367a64]\n\n19065 bio_aof\n/lib/x86_64-linux-gnu/libc.so.6(+0x98d71)[0x7f98172d6d71]\n/lib/x86_64-linux-gnu/libc.so.6(pthread_cond_wait+0x20d)[0x7f98172d97ed]\n/usr/bin/redis-server 0.0.0.0:6379(bioProcessBackgroundJobs+0x1ea)[0x5612a9090cda]\n/lib/x86_64-linux-gnu/libc.so.6(+0x9caa4)[0x7f98172daaa4]\n/lib/x86_64-linux-gnu/libc.so.6(__clone+0x44)[0x7f9817367a64]\n\n19062 redis-server\n/lib/x86_64-linux-gnu/libc.so.6(epoll_wait+0x52)[0x7f9817368072]\n/usr/bin/redis-server 0.0.0.0:6379(+0x99348)[0x5612a8fa6348]\n/usr/bin/redis-server 0.0.0.0:6379(aeMain+0x2d)[0x5612a8fa688d]\n/usr/bin/redis-server 0.0.0.0:6379(main+0x3cb)[0x5612a8f9fddb]\n/lib/x86_64-linux-gnu/libc.so.6(+0x2a1ca)[0x7f98172681ca]\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x8b)[0x7f981726828b]\n/usr/bin/redis-server 0.0.0.0:6379(_start+0x25)[0x5612a8fa1645]\n\n19309 reindex-6733\n/lib/x86_64-linux-gnu/libc.so.6(+0x98d71)[0x7f98172d6d71]\n/lib/x86_64-linux-gnu/libc.so.6(pthread_cond_wait+0x20d)[0x7f98172d97ed]\n/usr/lib/redis/modules/redisearch2.so(+0x33acd3)[0x7f9815377cd3]\n/usr/lib/redis/modules/redisearch2.so(+0x33ab8b)[0x7f9815377b8b]\n/lib/x86_64-linux-gnu/libc.so.6(+0x9caa4)[0x7f98172daaa4]\n/lib/x86_64-linux-gnu/libc.so.6(__clone+0x44)[0x7f9817367a64]\n\n19075 gc-1336 *\n/lib/x86_64-linux-gnu/libc.so.6(+0x45330)[0x7f9817283330]\n/usr/lib/redis/modules/redisearch2.so(GcScanDelta_LastBlockIdx+0x0)[0x7f9815757670]\n/usr/lib/redis/modules/redisearch2.so(+0x370251)[0x7f98153ad251]\n/usr/lib/redis/modules/redisearch2.so(FGC_parentHandleFromChild+0x228)[0x7f98153ada88]\n/usr/lib/redis/modules/redisearch2.so(+0x371d32)[0x7f98153aed32]\n/usr/lib/redis/modules/redisearch2.so(+0x373f65)[0x7f98153b0f65]\n/usr/lib/redis/modules/redisearch2.so(+0x33abbf)[0x7f9815377bbf]\n/lib/x86_64-linux-gnu/libc.so.6(+0x9caa4)[0x7f98172daaa4]\n/lib/x86_64-linux-gnu/libc.so.6(__clone+0x44)[0x7f9817367a64]\n\n6/6 expected stacktraces.\n\n------ STACK TRACE DONE ------\n\n------ REGISTERS ------\n19062:M 26 Nov 2025 15:45:06.872 #\nRAX:0000000000000000 RBX:00007f97f339af00\nRCX:0000000000000010 RDX:0000000000000040\nRDI:0000000000000000 RSI:0000000000000ae5\nRBP:00007f97af23b9a0 RSP:00007f9810a7ddf8\nR8 :0000000000a9a1a0 R9 :00007f9810ea9ec0\nR10:0000000000000000 R11:0000000000060000\nR12:00007f981008a1c0 R13:00007f97af23fe00\nR14:00007f9810085758 R15:00007f9810a7deb0\nRIP:00007f9815757670 EFL:0000000000010246\nCSGSFS:002b000000000033\n19062:M 26 Nov 2025 15:45:06.872 # (00007f9810a7de07) -> 00007f97f2e469f0\n19062:M 26 Nov 2025 15:45:06.872 # (00007f9810a7de06) -> 00007f9810a7df40\n19062:M 26 Nov 2025 15:45:06.872 # (00007f9810a7de05) -> 0000003000000018\n19062:M 26 Nov 2025 15:45:06.872 # (00007f9810a7de04) -> 00005612a90c4e8c\n19062:M 26 Nov 2025 15:45:06.872 # (00007f9810a7de03) -> 00007f9810a7df30\n19062:M 26 Nov 2025 15:45:06.872 # (00007f9810a7de02) -> 00000000deadbeef\n19062:M 26 Nov 2025 15:45:06.872 # (00007f9810a7de01) -> 0000000000000031\n19062:M 26 Nov 2025 15:45:06.872 # (00007f9810a7de00) -> 00007f9810082460\n19062:M 26 Nov 2025 15:45:06.872 # (00007f9810a7ddff) -> 000000000000000a\n19062:M 26 Nov 2025 15:45:06.872 # (00007f9810a7ddfe) -> 0000000000000ae5\n19062:M 26 Nov 2025 15:45:06.872 # (00007f9810a7ddfd) -> 00007f9810082460\n19062:M 26 Nov 2025 15:45:06.872 # (00007f9810a7ddfc) -> 00007f9810a7de70\n19062:M 26 Nov 2025 15:45:06.872 # (00007f9810a7ddfb) -> 0000000000000b04\n19062:M 26 Nov 2025 15:45:06.872 # (00007f9810a7ddfa) -> 00007f981008a1d0\n19062:M 26 Nov 2025 15:45:06.872 # (00007f9810a7ddf9) -> 0000000000000002\n19062:M 26 Nov 2025 15:45:06.872 # (00007f9810a7ddf8) -> 00007f98153ad251\n\n------ INFO OUTPUT ------\n# Server\nredis_version:8.4.0\nredis_git_sha1:00000000\nredis_git_dirty:1\nredis_build_id:27c2d682528c8769\nredis_mode:standalone\nos:Linux 5.15.167.4-microsoft-standard-WSL2 x86_64\narch_bits:64\nmonotonic_clock:POSIX clock_gettime\nmultiplexing_api:epoll\natomicvar_api:c11-builtin\ngcc_version:13.3.0\nprocess_id:19062\nprocess_supervised:systemd\nrun_id:4155776599ff6557afd899a82a6ddb2b83f26a9c\ntcp_port:6379\nserver_time_usec:1764164706868653\nuptime_in_seconds:658\nuptime_in_days:0\nhz:10\nconfigured_hz:10\nlru_clock:2557026\nexecutable:/usr/bin/redis-server\nconfig_file:/etc/redis/redis.conf\nio_threads_active:0\nlistener0:name=tcp,bind=0.0.0.0,bind=-::1,port=6379\n\n# Clients\nconnected_clients:9\ncluster_connections:0\nmaxclients:10000\nclient_recent_max_input_buffer:20517\nclient_recent_max_output_buffer:0\nblocked_clients:0\ntracking_clients:0\npubsub_clients:4\nwatching_clients:0\nclients_in_timeout_table:0\ntotal_watched_keys:0\ntotal_blocking_keys:0\ntotal_blocking_keys_on_nokey:0\n\n# Memory\nused_memory:1873792128\nused_memory_human:1.75G\nused_memory_rss:1924665344\nused_memory_rss_human:1.79G\nused_memory_peak:1876099600\nused_memory_peak_human:1.75G\nused_memory_peak_time:1764164705\nused_memory_peak_perc:99.88%\nused_memory_overhead:64051480\nused_memory_startup:1072384\nused_memory_dataset:1809740648\nused_memory_dataset_perc:96.64%\nallocator_allocated:1876442712\nallocator_active:1879588864\nallocator_resident:1908043776\nallocator_muzzy:0\ntotal_system_memory:16614010880\ntotal_system_memory_human:15.47G\nused_memory_lua:32768\nused_memory_vm_eval:32768\nused_memory_lua_human:32.00K\nused_memory_scripts_eval:0\nnumber_of_cached_scripts:0\nnumber_of_functions:0\nnumber_of_libraries:0\nused_memory_vm_functions:34816\nused_memory_vm_total:67584\nused_memory_vm_total_human:66.00K\nused_memory_functions:192\nused_memory_scripts:192\nused_memory_scripts_human:192B\nmaxmemory:0\nmaxmemory_human:0B\nmaxmemory_policy:noeviction\nallocator_frag_ratio:1.00\nallocator_frag_bytes:3070120\nallocator_rss_ratio:1.02\nallocator_rss_bytes:28454912\nrss_overhead_ratio:1.01\nrss_overhead_bytes:16621568\nmem_fragmentation_ratio:1.03\nmem_fragmentation_bytes:49317128\nmem_not_counted_for_evict:0\nmem_replication_backlog:0\nmem_total_replication_buffers:0\nmem_replica_full_sync_buffer:0\nmem_clients_slaves:0\nmem_clients_normal:39136\nmem_cluster_slot_migration_output_buffer:0\nmem_cluster_slot_migration_input_buffer:0\nmem_cluster_slot_migration_input_buffer_peak:0\nmem_cluster_links:0\nmem_aof_buffer:0\nmem_allocator:jemalloc-5.3.0\nmem_overhead_db_hashtable_rehashing:0\nactive_defrag_running:0\nlazyfree_pending_objects:0\nlazyfreed_objects:0\n\n# Persistence\nloading:0\nasync_loading:0\ncurrent_cow_peak:0\ncurrent_cow_size:0\ncurrent_cow_size_age:0\ncurrent_fork_perc:0.00\ncurrent_save_keys_processed:0\ncurrent_save_keys_total:1438276\nrdb_changes_since_last_save:1509555\nrdb_bgsave_in_progress:0\nrdb_last_save_time:1764164048\nrdb_last_bgsave_status:ok\nrdb_last_bgsave_time_sec:-1\nrdb_current_bgsave_time_sec:-1\nrdb_saves:0\nrdb_saves_consecutive_failures:0\nrdb_last_cow_size:0\nrdb_last_load_keys_expired:0\nrdb_last_load_keys_loaded:0\naof_enabled:0\naof_rewrite_in_progress:0\naof_rewrite_scheduled:0\naof_last_rewrite_time_sec:-1\naof_current_rewrite_time_sec:-1\naof_last_bgrewrite_status:ok\naof_rewrites:0\naof_rewrites_consecutive_failures:0\naof_last_write_status:ok\naof_last_cow_size:0\nmodule_fork_in_progress:1\nmodule_fork_last_cow_size:0\n\n# Threads\nio_thread_0:clients=9,reads=59198,writes=64231\n\n# Stats\ntotal_connections_received:135\ntotal_commands_processed:1513838\ninstantaneous_ops_per_sec:2486\ntotal_net_input_bytes:1072399386\ntotal_net_output_bytes:11112817\ntotal_net_repl_input_bytes:0\ntotal_net_repl_output_bytes:0\ninstantaneous_input_kbps:792.59\ninstantaneous_output_kbps:13.16\ninstantaneous_input_repl_kbps:0.00\ninstantaneous_output_repl_kbps:0.00\nrejected_connections:0\nsync_full:0\nsync_partial_ok:0\nsync_partial_err:0\nexpired_subkeys:0\nexpired_keys:0\nexpired_stale_perc:0.00\nexpired_time_cap_reached_count:0\nexpire_cycle_cpu_milliseconds:8\nevicted_keys:0\nevicted_clients:0\nevicted_scripts:0\ntotal_eviction_exceeded_time:0\ncurrent_eviction_exceeded_time:0\nkeyspace_hits:0\nkeyspace_misses:0\npubsub_channels:1\npubsub_patterns:0\npubsubshard_channels:0\nlatest_fork_usec:10762\ntotal_forks:2\nmigrate_cached_sockets:0\nslave_expires_tracked_keys:0\nactive_defrag_hits:0\nactive_defrag_misses:0\nactive_defrag_key_hits:0\nactive_defrag_key_misses:0\ntotal_active_defrag_time:0\ncurrent_active_defrag_time:0\ntracking_total_keys:0\ntracking_total_items:0\ntracking_total_prefixes:0\nunexpected_error_replies:0\ntotal_error_replies:14\ndump_payload_sanitizations:0\ntotal_reads_processed:59198\ntotal_writes_processed:64231\nio_threaded_reads_processed:0\nio_threaded_writes_processed:0\nio_threaded_total_prefetch_batches:114745\nio_threaded_total_prefetch_entries:1509305\nclient_query_buffer_limit_disconnections:0\nclient_output_buffer_limit_disconnections:0\nreply_buffer_shrinks:275\nreply_buffer_expands:261\neventloop_cycles:65224\neventloop_duration_sum:50526872\neventloop_duration_cmd_sum:46662948\ninstantaneous_eventloop_cycles_per_sec:59\ninstantaneous_eventloop_duration_usec:385\nacl_access_denied_auth:0\nacl_access_denied_cmd:0\nacl_access_denied_key:0\nacl_access_denied_channel:0\n\n# Replication\nrole:master\nconnected_slaves:0\nmaster_failover_state:no-failover\nmaster_replid:075dfb7a5c8c52d5a233b0605cf303b4c68dcaa9\nmaster_replid2:0000000000000000000000000000000000000000\nmaster_repl_offset:0\nsecond_repl_offset:-1\nrepl_backlog_active:0\nrepl_backlog_size:1048576\nrepl_backlog_first_byte_offset:0\nrepl_backlog_histlen:0\n\n# CPU\nused_cpu_sys:6.375001\nused_cpu_user:113.892041\nused_cpu_sys_children:0.641399\nused_cpu_user_children:0.123008\nused_cpu_sys_main_thread:2.062907\nused_cpu_user_main_thread:0.332077\n\n# Modules\nmodule:name=ReJSON,ver=80400,api=1,filters=0,usedby=[search],using=[],options=[handle-io-errors]\nmodule:name=bf,ver=80400,api=1,filters=0,usedby=[],using=[],options=[handle-io-errors]\nmodule:name=vectorset,ver=1,api=1,filters=0,usedby=[],using=[],options=[handle-io-errors|handle-repl-async-load]\nmodule:name=timeseries,ver=80400,api=1,filters=0,usedby=[],using=[],options=[handle-io-errors]\nmodule:name=search,ver=999999,api=1,filters=0,usedby=[],using=[ReJSON],options=[handle-io-errors]\n\n# Commandstats\ncmdstat_ping:calls=2538,usec=4477,usec_per_call=1.76,rejected_calls=0,failed_calls=0\ncmdstat_subscribe:calls=4,usec=90,usec_per_call=22.50,rejected_calls=0,failed_calls=0\ncmdstat_flushall:calls=1,usec=660,usec_per_call=660.00,rejected_calls=0,failed_calls=0\ncmdstat_FT.CREATE:calls=14,usec=1786,usec_per_call=127.57,rejected_calls=0,failed_calls=0\ncmdstat_config|get:calls=3,usec=2,usec_per_call=0.67,rejected_calls=0,failed_calls=0\ncmdstat_FT.INFO:calls=14,usec=112,usec_per_call=8.00,rejected_calls=0,failed_calls=14\ncmdstat__FT.CREATE:calls=14,usec=1688,usec_per_call=120.57,rejected_calls=0,failed_calls=0\ncmdstat_json.set:calls=1509335,usec=46608298,usec_per_call=30.88,rejected_calls=0,failed_calls=0\ncmdstat_command|docs:calls=3,usec=4289,usec_per_call=1429.67,rejected_calls=0,failed_calls=0\ncmdstat_publish:calls=1292,usec=10877,usec_per_call=8.42,rejected_calls=0,failed_calls=0\ncmdstat_client|setinfo:calls=6,usec=9,usec_per_call=1.50,rejected_calls=0,failed_calls=0\ncmdstat_client|setname:calls=8,usec=116,usec_per_call=14.50,rejected_calls=0,failed_calls=0\ncmdstat_set:calls=206,usec=103,usec_per_call=0.50,rejected_calls=0,failed_calls=0\ncmdstat_info:calls=400,usec=32614,usec_per_call=81.54,rejected_calls=0,failed_calls=0\n\n# Errorstats\nerrorstat_monitoring_check_groups_:count=1\nerrorstat_monitoring_profiles_:count=1\nerrorstat_velocity_bin_lists_:count=1\nerrorstat_velocity_card_lists_:count=1\nerrorstat_velocity_convertation_cache_:count=1\nerrorstat_velocity_currency_mastercard_:count=1\nerrorstat_velocity_currency_visa_:count=1\nerrorstat_velocity_lists_of_countries_:count=1\nerrorstat_velocity_merchant_levels_:count=1\nerrorstat_velocity_merchants_groups_:count=1\nerrorstat_velocity_rules_:count=1\nerrorstat_velocity_transactions_:count=1\nerrorstat_veloroute_mc_ip0040t1_:count=1\nerrorstat_veloroute_visa_ardef_:count=1\n\n# Latencystats\nlatency_percentiles_usec_ping:p50=2.007,p99=4.015,p99.9=15.039\nlatency_percentiles_usec_subscribe:p50=5.023,p99=75.263,p99.9=75.263\nlatency_percentiles_usec_flushall:p50=663.551,p99=663.551,p99.9=663.551\nlatency_percentiles_usec_FT.CREATE:p50=75.263,p99=446.463,p99.9=446.463\nlatency_percentiles_usec_config|get:p50=1.003,p99=1.003,p99.9=1.003\nlatency_percentiles_usec_FT.INFO:p50=5.023,p99=27.007,p99.9=27.007\nlatency_percentiles_usec__FT.CREATE:p50=69.119,p99=434.175,p99.9=434.175\nlatency_percentiles_usec_json.set:p50=30.079,p99=81.407,p99.9=307.199\nlatency_percentiles_usec_command|docs:p50=1359.871,p99=1949.695,p99.9=1949.695\nlatency_percentiles_usec_publish:p50=9.023,p99=19.071,p99.9=105.471\nlatency_percentiles_usec_client|setinfo:p50=1.003,p99=5.023,p99.9=5.023\nlatency_percentiles_usec_client|setname:p50=4.015,p99=75.263,p99.9=75.263\nlatency_percentiles_usec_set:p50=0.001,p99=1.003,p99.9=35.071\nlatency_percentiles_usec_info:p50=55.039,p99=242.687,p99.9=501.759\n\n# Cluster\ncluster_enabled:0\n\n# Keyspace\ndb0:keys=1440377,expires=0,avg_ttl=0,subexpiry=0\n\n# Keysizes\ndb0_distrib_strings_sizes:0=1,2=204\n\n------ CLIENT LIST OUTPUT ------\nid=15 addr=127.0.0.1:58988 laddr=127.0.0.1:6379 fd=29 name=sentinel-b93ade23-pubsub age=657 idle=1 flags=P db=0 sub=1 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=135 obl=0 oll=0 omem=0 tot-mem=2104 events=r cmd=subscribe user=default redir=-1 resp=2 lib-name= lib-ver= io-thread=0 tot-net-in=104 tot-net-out=173101 tot-cmds=2\nid=109 addr=127.0.0.1:52834 laddr=127.0.0.1:6379 fd=32 name= age=94 idle=0 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=20474 argv-mem=0 multi-mem=0 rbs=1024 rbp=310 obl=0 oll=0 omem=0 tot-mem=22528 events=r cmd=json.set user=default redir=-1 resp=2 lib-name=redis-py lib-ver=6.1.0 io-thread=0 tot-net-in=66554863 tot-net-out=1020104 tot-cmds=204009\nid=10 addr=127.0.0.1:58934 laddr=127.0.0.1:6379 fd=24 name=sentinel-a8b75413-cmd age=658 idle=0 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=7 obl=0 oll=0 omem=0 tot-mem=2048 events=r cmd=ping user=default redir=-1 resp=2 lib-name= lib-ver= io-thread=0 tot-net-in=53462 tot-net-out=474503 tot-cmds=1024\nid=11 addr=127.0.0.1:58948 laddr=127.0.0.1:6379 fd=25 name=sentinel-a8b75413-pubsub age=658 idle=1 flags=P db=0 sub=1 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=135 obl=0 oll=0 omem=0 tot-mem=2104 events=r cmd=subscribe user=default redir=-1 resp=2 lib-name= lib-ver= io-thread=0 tot-net-in=104 tot-net-out=173369 tot-cmds=2\nid=12 addr=127.0.0.1:58960 laddr=127.0.0.1:6379 fd=26 name=sentinel-b93ade23-cmd age=657 idle=0 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=7 obl=0 oll=0 omem=0 tot-mem=2048 events=r cmd=ping user=default redir=-1 resp=2 lib-name= lib-ver= io-thread=0 tot-net-in=53125 tot-net-out=474498 tot-cmds=1023\nid=13 addr=127.0.0.1:58968 laddr=127.0.0.1:6379 fd=27 name=sentinel-b93ade23-pubsub age=657 idle=1 flags=P db=0 sub=1 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=135 obl=0 oll=0 omem=0 tot-mem=2104 events=r cmd=subscribe user=default redir=-1 resp=2 lib-name= lib-ver= io-thread=0 tot-net-in=104 tot-net-out=173235 tot-cmds=2\nid=16 addr=127.0.0.1:58996 laddr=127.0.0.1:6379 fd=30 name=sentinel-b93ade23-cmd age=657 idle=0 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=7 obl=0 oll=0 omem=0 tot-mem=2048 events=r cmd=ping user=default redir=-1 resp=2 lib-name= lib-ver= io-thread=0 tot-net-in=53139 tot-net-out=474565 tot-cmds=1024\nid=17 addr=127.0.0.1:59012 laddr=127.0.0.1:6379 fd=31 name=sentinel-b93ade23-pubsub age=657 idle=1 flags=P db=0 sub=1 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=135 obl=0 oll=0 omem=0 tot-mem=2104 events=r cmd=subscribe user=default redir=-1 resp=2 lib-name= lib-ver= io-thread=0 tot-net-in=104 tot-net-out=172967 tot-cmds=2\nid=14 addr=127.0.0.1:58974 laddr=127.0.0.1:6379 fd=28 name=sentinel-b93ade23-cmd age=657 idle=0 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=7 obl=0 oll=0 omem=0 tot-mem=2048 events=r cmd=ping user=default redir=-1 resp=2 lib-name= lib-ver= io-thread=0 tot-net-in=53153 tot-net-out=474517 tot-cmds=1025\n\n------ MODULES INFO OUTPUT ------\n19062:M 26 Nov 2025 15:45:06.880 # <search> Failed to acquire read lock on index veloroute_mc_ip0040t1: rc=35 (Resource deadlock avoided). Cannot continue getting Index info\n# ReJSON_trace\nReJSON_backtrace:   0: <unknown>\n   1: <unknown>\n   2: modulesCollectInfo\n   3: logModulesInfo\n   4: printCrashReport\n   5: <unknown>\n   6: <unknown>\n   7: <unknown>\n   8: FGC_parentHandleNumeric\n   9: FGC_parentHandleFromChild\n  10: periodicCb\n  11: taskCallback\n  12: thread_do\n  13: <unknown>\n  14: clone\n\n\n# search_version\nsearch_version:99.99.99\nsearch_redis_version:8.4.0 - oss\n\n# search_indexes\nsearch_number_of_indexes:14\nsearch_number_of_active_indexes:0\nsearch_number_of_active_indexes_running_queries:0\nsearch_number_of_active_indexes_indexing:0\nsearch_total_active_write_threads:0\nsearch_total_indexing_time:37823864\n\n# search_fields_statistics\nsearch_fields_text:Text=8,IndexErrors=0\nsearch_fields_numeric:Numeric=57,Sortable=8,IndexErrors=0\nsearch_fields_tag:Tag=50,IndexErrors=98\n\n# search_memory\nsearch_used_memory_indexes:314140109\nsearch_used_memory_indexes_human:299.58735370635986\nsearch_smallest_memory_index:16096\nsearch_smallest_memory_index_human:0.015350341796875\nsearch_largest_memory_index:303431398\nsearch_largest_memory_index_human:289.37473106384277\nsearch_used_memory_vector_index:0\n\n# search_cursors\nsearch_global_idle_user:0\nsearch_global_idle_internal:0\nsearch_global_total_user:0\nsearch_global_total_internal:0\n\n# search_garbage_collector\nsearch_gc_bytes_collected:0\nsearch_gc_total_cycles:0\nsearch_gc_total_ms_run:0\nsearch_gc_total_docs_not_collected:113160\nsearch_gc_marked_deleted_vectors:0\n\n# search_queries\nsearch_total_queries_processed:0\nsearch_total_query_commands:0\nsearch_total_query_execution_time_ms:0\nsearch_total_active_queries:0\n\n# search_warnings_and_errors\nsearch_errors_indexing_failures:98\nsearch_errors_for_index_with_max_failures:98\nsearch_OOM_indexing_failures_indexes_count:0\nsearch_shard_total_query_errors_syntax:0\nsearch_shard_total_query_errors_arguments:0\nsearch_shard_total_query_errors_timeout:0\nsearch_shard_total_query_warnings_timeout:0\n\n# search_coordinator_warnings_and_errors\nsearch_coord_total_query_errors_syntax:0\nsearch_coord_total_query_errors_arguments:0\nsearch_coord_total_query_errors_timeout:0\nsearch_coord_total_query_warnings_timeout:0\n\n# search_dialect_statistics\nsearch_dialect_1:0\nsearch_dialect_2:0\nsearch_dialect_3:0\nsearch_dialect_4:0\n\n# search_runtime_configurations\nsearch_extension_load:\nsearch_friso_ini:\nsearch_default_scorer:BM25STD\nsearch_enableGC:ON\nsearch_minimal_term_prefix:2\nsearch_minimal_stem_length:4\nsearch_maximal_prefix_expansions:200\nsearch_query_timeout_ms:500\nsearch_timeout_policy:return\nsearch_oom_policy:return\nsearch_cursor_read_size:1000\nsearch_cursor_max_idle_time:300000\nsearch_max_doc_table_size:1000000\nsearch_max_search_results:1000000\nsearch_max_aggregate_results:2147483648\nsearch_gc_scan_size:100\nsearch_min_phonetic_term_length:3\nsearch_bm25std_tanh_factor:4\n\n# search_current_thread\nsearch_index:veloroute_mc_ip0040t1\n\n# search_blocked_queries\n\n# search_blocked_cursors\n\n------ CONFIG DEBUG OUTPUT ------\nclient-query-buffer-limit 1gb\nlazyfree-lazy-user-flush no\nslave-read-only yes\nlazyfree-lazy-eviction no\nlazyfree-lazy-server-del no\nlazyfree-lazy-user-del no\nrepl-diskless-load disabled\nreplica-read-only yes\nlazyfree-lazy-expire no\nproto-max-bulk-len 512mb\nrepl-diskless-sync yes\nio-threads 1\nactivedefrag no\nlist-compress-depth 0\nsanitize-dump-payload no\n\n------ FAST MEMORY TEST ------\n19062:M 26 Nov 2025 15:45:06.880 # main thread terminated\n19062:M 26 Nov 2025 15:45:06.881 # Bio worker thread #0 terminated\n19062:M 26 Nov 2025 15:45:06.881 # Bio worker thread #1 terminated\n19062:M 26 Nov 2025 15:45:06.881 # Bio worker thread #2 terminated\n\nFast memory test PASSED, however your memory can still be broken. Please run a memory test for several hours if possible.\n\n------ DUMPING CODE AROUND EIP ------\nSymbol: GcScanDelta_LastBlockIdx (base: 0x7f9815757670)\nModule: /usr/lib/redis/modules/redisearch2.so (base 0x7f981503d000)\n$ xxd -r -p /tmp/dump.hex /tmp/dump.bin\n$ objdump --adjust-vma=0x7f9815757670 -D -b binary -m i386:x86-64 /tmp/dump.bin\n------\n19062:M 26 Nov 2025 15:45:06.881 # dump of function (hexdump of 128 bytes):\n488b4718c3662e0f1f84000000000090488b4720c3662e0f1f840000000000900fb74728c3662e0f1f84000000000090488b4708c3662e0f1f84000000000090554157415641554154534881ecb8000000488d8424f0000000488b0f488d15d946470048630c8a4801d1ffe18038010f858d080000488b4810488b4018488d57\n\n=== REDIS BUG REPORT END. Make sure to include from START to END. ===\n```\n\nIn cases when I was able to upload data I didnt get previous problem. But I'm not sure 100% that problem was solved due to randomness of this issue. ",
      "labels": [],
      "created_at": "2025-11-25T15:03:14Z",
      "closed_at": "2025-11-29T12:47:14Z",
      "url": "https://github.com/redis/redis/issues/14570",
      "comments_count": 7
    },
    {
      "tech": "redis",
      "repo": "redis/redis",
      "issue_number": 8339,
      "title": "[NEW] Expiring Members inside Sets",
      "problem": "**The problem/use-case that the feature addresses**\r\n\r\nI use Redis as a place to store objects in a web application. These objects belong to a class. I access them via some ORM Library I developed in Ruby. I need these objects to have some TTL time.\r\nI store the keys of a class objects inside a set. And I store the info of each object, each in its own Key/Value\r\nFor example, a Redis DB can look like this\r\n```\r\n[SET] 'users' => ['user:1', 'user:2', 'user:3']\r\n[STRING] 'user:1' => '{ JSON REPRESENTATION OF user1 }'\r\n[STRING] 'user:2' => '{ JSON REPRESENTATION OF user2 }'\r\n[STRING] 'user:3' => '{ JSON REPRESENTATION OF user3 }'\r\n```\r\n\r\nA description of the problem that the feature will solve, or the use-case with which the feature will be used.\r\n\r\n**Description of the feature**\r\n\r\nBy adding this feature, I can easily expire both the object info (already available), and **the Object Membership inside the set**.\r\n\r\nIt can be done using a command like\r\n```\r\nsmemberexpire users user1\r\n```\r\n\r\n**Alternatives you've considered**\r\n\r\nMy alternative solution is that, each time I see the object inside the set, I check for its key if it exists, or already expired.\r\n\r\n**Additional information**\r\n\r\nIf this feature is welcome, I would be happy to implement it and make a PR for it myself.",
      "solution": "How is this issue? Is this being implemented? \r\nI have a similar problem here. I needed a sort of relationships between keys on my redis database.   \r\nSo, I found [this blog post about full text search](https://redis.com/redis-best-practices/indexing-patterns/full-text-search/) which, works like a charm ...\r\n\r\nSo, when I add a key to the database, I split it into parts and store the parts in a set.  \r\nExample:   \r\n```txt\r\nSET USER_ID=10|COUNTRY=20| '{\"user\": {\"name\": \"Matheus\", \"salary\": 1899}, \"country\": {\"name\": \"Brazil}}' \r\nSADD USER_ID=10| USER_ID=10|COUNTRY=20| \r\nSADD COUNTRY=20| USER_ID=10|COUNTRY=20| \r\n```\r\nSometimes, I do not use ttls, so, all the keys/members are deleted/overwrite manually  \r\nSo, I do execute:   \r\n```txt\r\nSREM USER_ID=10| USER_ID=10|COUNTRY=20|\r\nSREM COUNTRY=20| USER_ID=10|COUNTRY=20| \r\nDEL USER_ID=10|COUNTRY=20|\r\n```\r\n\r\nBut, the problem is when, I have some keys with ttl .    \r\nSo,  I would have something like:  \r\n```txt\r\nSET USER_ID=10|COUNTRY=20| '{\"user\": {\"name\": \"Matheus\", \"salary\": 1899}, \"country\": {\"name\": \"Brazil}}'  EX 3600\r\nSADD USER_ID=10| USER_ID=10|COUNTRY=20| \r\nSADD COUNTRY=20| USER_ID=10|COUNTRY=20| \r\n``` \r\nBut, I can not set the ttl to a member of the set. So, in the worst case, I could have some trash.   \r\nOne thing that I can do to bypass this, is to use the Publish & Subscriber, which works. But, its harder to do, and, I do not think that clients must implement this to get this behavior, cause, this is too much error prone ... \r\n\r\nWhats my idea:  To add ttl to set members. Something that I do know, is that, if it is not implemented, it must affect server performance, or be a bit too hard. So, I am just thinking as a naive redis user.    \r\n```txt\r\nSET USER_ID=10|COUNTRY=20| '{\"user\": {\"name\": \"Matheus\", \"salary\": 1899}, \"country\": {\"name\": \"Brazil}}'  EX 3600\r\nSADD USER_ID=10| USER_ID=10|COUNTRY=20|  EX 3600\r\nSADD COUNTRY=20| USER_ID=10|COUNTRY=20|  EX 3600\r\n```\r\n\r\n@itamarhaber  does it make sense for you? I might try to implement it on my spare time, and then, show you my solution\r\n\r\n\r\nThanks in advance!\r\n\r\n\n\n---\n\nHello, any possible workarounds or implementation roadmap?",
      "labels": [],
      "created_at": "2021-01-14T17:00:23Z",
      "closed_at": "2026-01-13T09:44:46Z",
      "url": "https://github.com/redis/redis/issues/8339",
      "comments_count": 7
    },
    {
      "tech": "redis",
      "repo": "redis/redis",
      "issue_number": 14656,
      "title": "[BUG] XTRIM/XADD MAXLEN ~ (approximate) ACKED does note delete anything from stream",
      "problem": "**Describe the bug**\n\nNoticed in production that our stream size was exploding, because our XADD MAXLEN ~ ACKED command was not deleting anything, even though the items were acked AFAICS, and not stuck in the PEL. Then reproduced in a isolation using XTRIM as below.\n\nFirst seen with docker-based 8.2.2-alpine. Upgraded to 8.4.0-alpine showing the same problem. Below reproduction was made with 8.4.0.\n\n**To reproduce**\n\n```\n# Create a stream of 200 entries\nfor i in $(seq 200); do docker compose exec redis redis-cli XADD teststream \\* num $i; done\n[snip output]\n# Create a consumer group\ndocker compose exec redis redis-cli XGROUP CREATE teststream groupid 0\nOK\n# Read all messages and ack them (NOACK should be equivalent to read + immediately ACK)\ndocker compose exec redis redis-cli XREADGROUP GROUP groupid consumerid COUNT 200 NOACK STREAMS teststream '>'\n[snip output]\n# Show info on the stream\ndocker compose exec redis redis-cli XINFO STREAM teststream FULL COUNT 1\n 1) \"length\"\n 2) (integer) 200\n 3) \"radix-tree-keys\"\n 4) (integer) 2\n 5) \"radix-tree-nodes\"\n 6) (integer) 6\n 7) \"last-generated-id\"\n 8) \"1767444446387-0\"\n 9) \"max-deleted-entry-id\"\n10) \"0-0\"\n11) \"entries-added\"\n12) (integer) 200\n13) \"recorded-first-entry-id\"\n14) \"1767444425939-0\"\n15) \"entries\"\n16) 1) 1) \"1767444425939-0\"\n       2) 1) \"num\"\n          2) \"1\"\n17) \"groups\"\n18) 1)  1) \"name\"\n        2) \"groupid\"\n        3) \"last-delivered-id\"\n        4) \"1767444446387-0\"\n        5) \"entries-read\"\n        6) (integer) 200\n        7) \"lag\"\n        8) (integer) 0\n        9) \"pel-count\"\n       10) (integer) 0\n       11) \"pending\"\n       12) (empty array)\n       13) \"consumers\"\n       14) 1)  1) \"name\"\n               2) \"consumerid\"\n               3) \"seen-time\"\n               4) (integer) 1767444488906\n               5) \"active-time\"\n               6) (integer) -1\n               7) \"pel-count\"\n               8) (integer) 0\n               9) \"pending\"\n              10) (empty array)\n# Try to trim, deletes 0 entries\ndocker compose exec redis redis-cli XTRIM teststream MAXLEN '~' 10 ACKED\n(integer) 0\n# Show stream info again\ndocker compose exec redis redis-cli XINFO STREAM teststream FULL COUNT 1\n 1) \"length\"\n 2) (integer) 200\n 3) \"radix-tree-keys\"\n 4) (integer) 2\n 5) \"radix-tree-nodes\"\n 6) (integer) 6\n 7) \"last-generated-id\"\n 8) \"1767444446387-0\"\n 9) \"max-deleted-entry-id\"\n10) \"0-0\"\n11) \"entries-added\"\n12) (integer) 200\n13) \"recorded-first-entry-id\"\n14) \"1767444425939-0\"\n15) \"entries\"\n16) 1) 1) \"1767444425939-0\"\n       2) 1) \"num\"\n          2) \"1\"\n17) \"groups\"\n18) 1)  1) \"name\"\n        2) \"groupid\"\n        3) \"last-delivered-id\"\n        4) \"1767444446387-0\"\n        5) \"entries-read\"\n        6) (integer) 200\n        7) \"lag\"\n        8) (integer) 0\n        9) \"pel-count\"\n       10) (integer) 0\n       11) \"pending\"\n       12) (empty array)\n       13) \"consumers\"\n       14) 1)  1) \"name\"\n               2) \"consumerid\"\n               3) \"seen-time\"\n               4) (integer) 1767444488906\n               5) \"active-time\"\n               6) (integer) -1\n               7) \"pel-count\"\n               8) (integer) 0\n               9) \"pending\"\n              10) (empty array)\n```\n\nNote that this uses `NOACK` when reading, which should be equivalent reading the entry and immediately acking. In our production setup, we used a separate ACK.\n\n**Expected behavior**\n\nThe XTRIM should delete some of the entries (probably not all of them, because of the approximate).\n\n**Additional information**\n\nThe values of `radix-tree-nodes` and `radix-tree-keys` are both > 1, which AFAIU means that redis should be able to delete some values by deleting a complete node (or key?).\n\nThis is also confirmed by dropping the `ACKED`, then redis deletes half of the entries, so approximate trimming is possible:\n\n```\ndocker compose exec redis redis-cli XTRIM teststream MAXLEN '~' 10\n(integer) 100\n```\n\nThis would suggest that maybe the entries are not properly ACKED, but switching to exact trimming instead of approximate, shows that it will happily remove entries, so they must be acked (I ran this command after redoing the above steps to reproduce):\n\n```\ndocker compose exec redis redis-cli XTRIM teststream MAXLEN '=' 10 ACKED\n(integer) 190\n```",
      "solution": "@matthijskooijman, thanks for your report. This bug will be fixed by https://github.com/redis/redis/pull/14623 in the next version.",
      "labels": [],
      "created_at": "2026-01-03T13:00:58Z",
      "closed_at": "2026-01-05T13:18:48Z",
      "url": "https://github.com/redis/redis/issues/14656",
      "comments_count": 2
    },
    {
      "tech": "redis",
      "repo": "redis/redis",
      "issue_number": 14586,
      "title": "[CRASH] Randomly crash and after few rebots goes well",
      "problem": "\n\n**Crash report**\n\nPaste the complete crash log between the quotes below. Please include a few lines from the log preceding the crash report to provide some context.\n\n```\n === REDIS BUG REPORT START: Cut & paste starting from here ===\nredis-1  | 1:M 27 Nov 2025 11:21:20.727 # Redis 8.4.0 crashed by signal: 11, si_code: 1\nredis-1  | 1:M 27 Nov 2025 11:21:20.727 # Accessing address: 0xd8\nredis-1  | 1:M 27 Nov 2025 11:21:20.727 # Crashed running the instruction at: 0x560c1cc0503f\nredis-1  | \nredis-1  | ------ STACK TRACE ------\nredis-1  | EIP:\nredis-1  | redis-server *:6379(+0x2a603f)[0x560c1cc0503f]\nredis-1  | \nredis-1  | 25 bio_lazy_free\nredis-1  | /lib/x86_64-linux-gnu/libc.so.6(+0x85f16)[0x7fbd8ef36f16]\nredis-1  | /lib/x86_64-linux-gnu/libc.so.6(pthread_cond_wait+0x1e8)[0x7fbd8ef395d8]\nredis-1  | redis-server *:6379(bioProcessBackgroundJobs+0x1a1)[0x560c1cafda01]\nredis-1  | /lib/x86_64-linux-gnu/libc.so.6(+0x891f5)[0x7fbd8ef3a1f5]\nredis-1  | /lib/x86_64-linux-gnu/libc.so.6(__clone+0x40)[0x7fbd8efb9b40]\nredis-1  | \nredis-1  | 23 bio_close_file\nredis-1  | /lib/x86_64-linux-gnu/libc.so.6(+0x85f16)[0x7fbd8ef36f16]\nredis-1  | /lib/x86_64-linux-gnu/libc.so.6(pthread_cond_wait+0x1e8)[0x7fbd8ef395d8]\nredis-1  | redis-server *:6379(bioProcessBackgroundJobs+0x1a1)[0x560c1cafda01]\nredis-1  | /lib/x86_64-linux-gnu/libc.so.6(+0x891f5)[0x7fbd8ef3a1f5]\nredis-1  | /lib/x86_64-linux-gnu/libc.so.6(__clone+0x40)[0x7fbd8efb9b40]\nredis-1  | \nredis-1  | 1 redis-server\nredis-1  | /lib/x86_64-linux-gnu/libc.so.6(epoll_wait+0x56)[0x7fbd8efb9f26]\nredis-1  | redis-server *:6379(+0x98184)[0x560c1c9f7184]\nredis-1  | redis-server *:6379(aeMain+0xc0)[0x560c1c9f7840]\nredis-1  | redis-server *:6379(main+0x4a8)[0x560c1c9f1bf8]\nredis-1  | /lib/x86_64-linux-gnu/libc.so.6(+0x2724a)[0x7fbd8eed824a]\nredis-1  | /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x85)[0x7fbd8eed8305]\nredis-1  | redis-server *:6379(_start+0x21)[0x560c1c9f3581]\nredis-1  | \nredis-1  | 24 bio_aof\nredis-1  | /lib/x86_64-linux-gnu/libc.so.6(+0x85f16)[0x7fbd8ef36f16]\nredis-1  | /lib/x86_64-linux-gnu/libc.so.6(pthread_cond_wait+0x1e8)[0x7fbd8ef395d8]\nredis-1  | redis-server *:6379(bioProcessBackgroundJobs+0x1a1)[0x560c1cafda01]\nredis-1  | /lib/x86_64-linux-gnu/libc.so.6(+0x891f5)[0x7fbd8ef3a1f5]\nredis-1  | /lib/x86_64-linux-gnu/libc.so.6(__clone+0x40)[0x7fbd8efb9b40]\nredis-1  | \nredis-1  | 28 gc-9322 *\nredis-1  | /lib/x86_64-linux-gnu/libc.so.6(+0x3c050)[0x7fbd8eeed050]\nredis-1  | redis-server *:6379(+0x2a603f)[0x560c1cc0503f]\nredis-1  | redis-server *:6379(RM_Free+0x16)[0x560c1cb220e6]\nredis-1  | /usr/local/lib/redis/modules//redisearch.so(FGC_parentHandleFromChild+0x98c)[0x7fbd8d848fcc]\nredis-1  | /usr/local/lib/redis/modules//redisearch.so(+0x353b52)[0x7fbd8d849b52]\nredis-1  | /usr/local/lib/redis/modules//redisearch.so(+0x355c01)[0x7fbd8d84bc01]\nredis-1  | /usr/local/lib/redis/modules//redisearch.so(+0x320e8b)[0x7fbd8d816e8b]\nredis-1  | /lib/x86_64-linux-gnu/libc.so.6(+0x891f5)[0x7fbd8ef3a1f5]\nredis-1  | /lib/x86_64-linux-gnu/libc.so.6(__clone+0x40)[0x7fbd8efb9b40]\nredis-1  | \nredis-1  | 5/5 expected stacktraces.\nredis-1  | \nredis-1  | ------ STACK TRACE DONE ------\nredis-1  | \nredis-1  | ------ REGISTERS ------\nredis-1  | 1:M 27 Nov 2025 11:21:20.739 # \nredis-1  | RAX:00000000000000d8 RBX:00007fbd27bfec10\nredis-1  | RCX:00007fbd27bfedd0 RDX:00007fbd27bfedd0\nredis-1  | RDI:00007fbd27bfeee0 RSI:0000000000000000\nredis-1  | RBP:000000000001b203 RSP:00007fbd27bfded0\nredis-1  | R8 :0000000000000001 R9 :0000000000000000\nredis-1  | R10:0000000000000001 R11:0000000000000293\nredis-1  | R12:00007fbd27bfec10 R13:00007fbd27bfdfb8\nredis-1  | R14:0000000000000000 R15:00007fbd27bfef78\nredis-1  | RIP:0000560c1cc0503f EFL:0000000000010202\nredis-1  | CSGSFS:002b000000000033\nredis-1  | 1:M 27 Nov 2025 11:21:20.739 # (00007fbd27bfdedf) -> 00007fbd8eb5e300\nredis-1  | 1:M 27 Nov 2025 11:21:20.739 # (00007fbd27bfdede) -> 00007fbd27bfe028\nredis-1  | 1:M 27 Nov 2025 11:21:20.739 # (00007fbd27bfdedd) -> 0000560c1cb40e50\nredis-1  | 1:M 27 Nov 2025 11:21:20.739 # (00007fbd27bfdedc) -> 0000000200000428\nredis-1  | 1:M 27 Nov 2025 11:21:20.739 # (00007fbd27bfdedb) -> 00007fbd43fb2320\nredis-1  | 1:M 27 Nov 2025 11:21:20.739 # (00007fbd27bfdeda) -> 0000000000000000\nredis-1  | 1:M 27 Nov 2025 11:21:20.739 # (00007fbd27bfded9) -> 00007fbd8ef4fa26\nredis-1  | 1:M 27 Nov 2025 11:21:20.739 # (00007fbd27bfded8) -> ffffffffffffffff\nredis-1  | 1:M 27 Nov 2025 11:21:20.739 # (00007fbd27bfded7) -> 00007fbd43fb2320\nredis-1  | 1:M 27 Nov 2025 11:21:20.740 # (00007fbd27bfded6) -> 00007fbd8f085560\nredis-1  | 1:M 27 Nov 2025 11:21:20.740 # (00007fbd27bfded5) -> 0000000000000000\nredis-1  | 1:M 27 Nov 2025 11:21:20.740 # (00007fbd27bfded4) -> fffffffffffff4a8\nredis-1  | 1:M 27 Nov 2025 11:21:20.740 # (00007fbd27bfded3) -> 00007fbd8f048200\nredis-1  | 1:M 27 Nov 2025 11:21:20.740 # (00007fbd27bfded2) -> 00007fbd8ea44800\nredis-1  | 1:M 27 Nov 2025 11:21:20.740 # (00007fbd27bfded1) -> 00007fbd27bfedd0\nredis-1  | 1:M 27 Nov 2025 11:21:20.740 # (00007fbd27bfded0) -> 00007fbd8f085a80\nredis-1  | \nredis-1  | ------ INFO OUTPUT ------\nredis-1  | # Server\nredis-1  | redis_version:8.4.0\nredis-1  | redis_git_sha1:00000000\nredis-1  | redis_git_dirty:1\nredis-1  | redis_build_id:a4baaa1fa9dc2610\nredis-1  | redis_mode:standalone\nredis-1  | os:Linux 5.4.0-200-generic x86_64\nredis-1  | arch_bits:64\nredis-1  | monotonic_clock:POSIX clock_gettime\nredis-1  | multiplexing_api:epoll\nredis-1  | atomicvar_api:c11-builtin\nredis-1  | gcc_version:12.2.0\nredis-1  | process_id:1\nredis-1  | process_supervised:no\nredis-1  | run_id:0532cff2427d90bdf8800ca923622079ea5740e5\nredis-1  | tcp_port:6379\nredis-1  | server_time_usec:1764242480738713\nredis-1  | uptime_in_seconds:38827\nredis-1  | uptime_in_days:0\nredis-1  | hz:10\nredis-1  | configured_hz:10\nredis-1  | lru_clock:2634800\nredis-1  | executable:/data/redis-server\nredis-1  | config_file:\nredis-1  | io_threads_active:0\nredis-1  | listener0:name=tcp,bind=*,bind=-::*,port=6379\nredis-1  | \nredis-1  | # Clients\nredis-1  | connected_clients:32\nredis-1  | cluster_connections:0\nredis-1  | maxclients:10000\nredis-1  | client_recent_max_input_buffer:192\nredis-1  | client_recent_max_output_buffer:20608\nredis-1  | blocked_clients:0\nredis-1  | tracking_clients:0\nredis-1  | pubsub_clients:16\nredis-1  | watching_clients:0\nredis-1  | clients_in_timeout_table:0\nredis-1  | total_watched_keys:0\nredis-1  | total_blocking_keys:0\nredis-1  | total_blocking_keys_on_nokey:0\nredis-1  | \nredis-1  | # Memory\nredis-1  | used_memory:1357549776\nredis-1  | used_memory_human:1.26G\nredis-1  | used_memory_rss:1417347072\nredis-1  | used_memory_rss_human:1.32G\nredis-1  | used_memory_peak:1368524824\nredis-1  | used_memory_peak_human:1.27G\nredis-1  | used_memory_peak_time:1764208918\nredis-1  | used_memory_peak_perc:99.20%\nredis-1  | used_memory_overhead:31794536\nredis-1  | used_memory_startup:1072640\nredis-1  | used_memory_dataset:1325755240\nredis-1  | used_memory_dataset_perc:97.74%\nredis-1  | allocator_allocated:1359223400\nredis-1  | allocator_active:1386803200\nredis-1  | allocator_resident:1418682368\nredis-1  | allocator_muzzy:0\nredis-1  | total_system_memory:16773480448\nredis-1  | total_system_memory_human:15.62G\nredis-1  | used_memory_lua:926720\nredis-1  | used_memory_vm_eval:926720\nredis-1  | used_memory_lua_human:905.00K\nredis-1  | used_memory_scripts_eval:2272\nredis-1  | number_of_cached_scripts:2\nredis-1  | number_of_functions:0\nredis-1  | number_of_libraries:0\nredis-1  | used_memory_vm_functions:34816\nredis-1  | used_memory_vm_total:961536\nredis-1  | used_memory_vm_total_human:939.00K\nredis-1  | used_memory_functions:192\nredis-1  | used_memory_scripts:2464\nredis-1  | used_memory_scripts_human:2.41K\nredis-1  | maxmemory:0\nredis-1  | maxmemory_human:0B\nredis-1  | maxmemory_policy:noeviction\nredis-1  | allocator_frag_ratio:1.02\nredis-1  | allocator_frag_bytes:26798168\nredis-1  | allocator_rss_ratio:1.02\nredis-1  | allocator_rss_bytes:31879168\nredis-1  | rss_overhead_ratio:1.00\nredis-1  | rss_overhead_bytes:-1335296\nredis-1  | mem_fragmentation_ratio:1.04\nredis-1  | mem_fragmentation_bytes:59799272\nredis-1  | mem_not_counted_for_evict:0\nredis-1  | mem_replication_backlog:0\nredis-1  | mem_total_replication_buffers:0\nredis-1  | mem_replica_full_sync_buffer:0\nredis-1  | mem_clients_slaves:0\nredis-1  | mem_clients_normal:67136\nredis-1  | mem_cluster_slot_migration_output_buffer:0\nredis-1  | mem_cluster_slot_migration_input_buffer:0\nredis-1  | mem_cluster_slot_migration_input_buffer_peak:0\nredis-1  | mem_cluster_links:0\nredis-1  | mem_aof_buffer:0\nredis-1  | mem_allocator:jemalloc-5.3.0\nredis-1  | mem_overhead_db_hashtable_rehashing:0\nredis-1  | active_defrag_running:0\nredis-1  | lazyfree_pending_objects:0\nredis-1  | lazyfreed_objects:0\nredis-1  | \nredis-1  | # Persistence\nredis-1  | loading:0\nredis-1  | async_loading:0\nredis-1  | current_cow_peak:1262833664\nredis-1  | current_cow_size:1262833664\nredis-1  | current_cow_size_age:5\nredis-1  | current_fork_perc:40.00\nredis-1  | current_save_keys_processed:0\nredis-1  | current_save_keys_total:632790\nredis-1  | rdb_changes_since_last_save:199989\nredis-1  | rdb_bgsave_in_progress:0\nredis-1  | rdb_last_save_time:1764242236\nredis-1  | rdb_last_bgsave_status:ok\nredis-1  | rdb_last_bgsave_time_sec:8\nredis-1  | rdb_current_bgsave_time_sec:-1\nredis-1  | rdb_saves:97\nredis-1  | rdb_saves_consecutive_failures:0\nredis-1  | rdb_last_cow_size:8990720\nredis-1  | rdb_last_load_keys_expired:3448\nredis-1  | rdb_last_load_keys_loaded:631428\nredis-1  | aof_enabled:0\nredis-1  | aof_rewrite_in_progress:0\nredis-1  | aof_rewrite_scheduled:0\nredis-1  | aof_last_rewrite_time_sec:-1\nredis-1  | aof_current_rewrite_time_sec:-1\nredis-1  | aof_last_bgrewrite_status:ok\nredis-1  | aof_rewrites:0\nredis-1  | aof_rewrites_consecutive_failures:0\nredis-1  | aof_last_write_status:ok\nredis-1  | aof_last_cow_size:0\nredis-1  | module_fork_in_progress:1\nredis-1  | module_fork_last_cow_size:879230976\nredis-1  | \nredis-1  | # Threads\nredis-1  | io_thread_0:clients=32,reads=25734338,writes=25846052\nredis-1  | \nredis-1  | # Stats\nredis-1  | total_connections_received:3917\nredis-1  | total_commands_processed:55249876\nredis-1  | instantaneous_ops_per_sec:3631\nredis-1  | total_net_input_bytes:10514365302\nredis-1  | total_net_output_bytes:11489978673\nredis-1  | total_net_repl_input_bytes:0\nredis-1  | total_net_repl_output_bytes:0\nredis-1  | instantaneous_input_kbps:970.26\nredis-1  | instantaneous_output_kbps:80.82\nredis-1  | instantaneous_input_repl_kbps:0.00\nredis-1  | instantaneous_output_repl_kbps:0.00\nredis-1  | rejected_connections:0\nredis-1  | sync_full:0\nredis-1  | sync_partial_ok:0\nredis-1  | sync_partial_err:0\nredis-1  | expired_subkeys:0\nredis-1  | expired_keys:1264831\nredis-1  | expired_stale_perc:6.66\nredis-1  | expired_time_cap_reached_count:0\nredis-1  | expire_cycle_cpu_milliseconds:27414\nredis-1  | evicted_keys:0\nredis-1  | evicted_clients:0\nredis-1  | evicted_scripts:0\nredis-1  | total_eviction_exceeded_time:0\nredis-1  | current_eviction_exceeded_time:0\nredis-1  | keyspace_hits:5386540\nredis-1  | keyspace_misses:1281146\nredis-1  | pubsub_channels:1\nredis-1  | pubsub_patterns:0\nredis-1  | pubsubshard_channels:0\nredis-1  | latest_fork_usec:60371\nredis-1  | total_forks:195\nredis-1  | migrate_cached_sockets:0\nredis-1  | slave_expires_tracked_keys:0\nredis-1  | active_defrag_hits:0\nredis-1  | active_defrag_misses:0\nredis-1  | active_defrag_key_hits:0\nredis-1  | active_defrag_key_misses:0\nredis-1  | total_active_defrag_time:0\nredis-1  | current_active_defrag_time:0\nredis-1  | tracking_total_keys:0\nredis-1  | tracking_total_items:0\nredis-1  | tracking_total_prefixes:0\nredis-1  | unexpected_error_replies:0\nredis-1  | total_error_replies:3755\nredis-1  | dump_payload_sanitizations:0\nredis-1  | total_reads_processed:25734338\nredis-1  | total_writes_processed:25846052\nredis-1  | io_threaded_reads_processed:0\nredis-1  | io_threaded_writes_processed:0\nredis-1  | io_threaded_total_prefetch_batches:1729780\nredis-1  | io_threaded_total_prefetch_entries:4503981\nredis-1  | client_query_buffer_limit_disconnections:0\nredis-1  | client_output_buffer_limit_disconnections:0\nredis-1  | reply_buffer_shrinks:909\nredis-1  | reply_buffer_expands:1270\nredis-1  | eventloop_cycles:25953069\nredis-1  | eventloop_duration_sum:5309464815\nredis-1  | eventloop_duration_cmd_sum:3985650696\nredis-1  | instantaneous_eventloop_cycles_per_sec:1819\nredis-1  | instantaneous_eventloop_duration_usec:127\nredis-1  | acl_access_denied_auth:0\nredis-1  | acl_access_denied_cmd:0\nredis-1  | acl_access_denied_key:0\nredis-1  | acl_access_denied_channel:0\nredis-1  | \nredis-1  | # Replication\nredis-1  | role:master\nredis-1  | connected_slaves:0\nredis-1  | master_failover_state:no-failover\nredis-1  | master_replid:5fdfdb6823d6919717925012a51f79ec9fb55fd7\nredis-1  | master_replid2:0000000000000000000000000000000000000000\nredis-1  | master_repl_offset:393561\nredis-1  | second_repl_offset:-1\nredis-1  | repl_backlog_active:0\nredis-1  | repl_backlog_size:1048576\nredis-1  | repl_backlog_first_byte_offset:0\nredis-1  | repl_backlog_histlen:0\nredis-1  | \nredis-1  | # CPU\nredis-1  | used_cpu_sys:1338.363967\nredis-1  | used_cpu_user:4729.392915\nredis-1  | used_cpu_sys_children:553.297907\nredis-1  | used_cpu_user_children:1215.201179\nredis-1  | used_cpu_sys_main_thread:199.870988\nredis-1  | used_cpu_user_main_thread:155.716289\nredis-1  | \nredis-1  | # Modules\nredis-1  | module:name=timeseries,ver=80400,api=1,filters=0,usedby=[],using=[],options=[handle-io-errors]\nredis-1  | module:name=search,ver=80402,api=1,filters=0,usedby=[],using=[ReJSON],options=[handle-io-errors]\nredis-1  | module:name=bf,ver=80400,api=1,filters=0,usedby=[],using=[],options=[handle-io-errors]\nredis-1  | module:name=vectorset,ver=1,api=1,filters=0,usedby=[],using=[],options=[handle-io-errors|handle-repl-async-load]\nredis-1  | module:name=ReJSON,ver=80400,api=1,filters=0,usedby=[search],using=[],options=[handle-io-errors]\nredis-1  | \nredis-1  | # Commandstats\nredis-1  | cmdstat_hmset:calls=1153,usec=21467,usec_per_call=18.62,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_zremrangebyscore:calls=309,usec=1951,usec_per_call=6.31,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_info:calls=14275,usec=2267864,usec_per_call=158.87,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_echo:calls=34,usec=12,usec_per_call=0.35,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_latency|histogram:calls=3883,usec=2529919,usec_per_call=651.54,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_latency|latest:calls=3883,usec=11106,usec_per_call=2.86,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_get:calls=1486126,usec=7316158,usec_per_call=4.92,rejected_calls=1366,failed_calls=0\nredis-1  | cmdstat_config|get:calls=3920,usec=1561270,usec_per_call=398.28,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_cluster|nodes:calls=1,usec=407,usec_per_call=407.00,rejected_calls=16,failed_calls=1\nredis-1  | cmdstat_expire:calls=1153,usec=8088,usec_per_call=7.01,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_FT.CURSOR:calls=77,usec=307939,usec_per_call=3999.21,rejected_calls=0,failed_calls=2\nredis-1  | cmdstat_watch:calls=1635,usec=7503,usec_per_call=4.59,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat__FT.TAGVALS:calls=2,usec=4089,usec_per_call=2044.50,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_eval:calls=2,usec=504700,usec_per_call=252350.00,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_unlink:calls=1894,usec=18767,usec_per_call=9.91,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_zscan:calls=259,usec=1140,usec_per_call=4.40,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_exec:calls=1635,usec=28133,usec_per_call=17.21,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_slowlog|get:calls=3883,usec=39708,usec_per_call=10.23,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_slowlog|len:calls=3883,usec=11603,usec_per_call=2.99,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_FT.SEARCH:calls=2216526,usec=2239379428,usec_per_call=1010.31,rejected_calls=2345,failed_calls=6\nredis-1  | cmdstat_auth:calls=3917,usec=44728,usec_per_call=11.42,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_setex:calls=1267511,usec=33612186,usec_per_call=26.52,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_select:calls=25850676,usec=16514357,usec_per_call=0.64,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_subscribe:calls=17,usec=93,usec_per_call=5.47,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_script|load:calls=2,usec=388,usec_per_call=194.00,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_scan:calls=128,usec=911336,usec_per_call=7119.81,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_json.get:calls=3190722,usec=20529610,usec_per_call=6.43,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_hmget:calls=13284,usec=153526,usec_per_call=11.56,rejected_calls=1,failed_calls=0\nredis-1  | cmdstat_FT.AGGREGATE:calls=1,usec=7135,usec_per_call=7135.00,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_json.set:calls=11935814,usec=1640534324,usec_per_call=137.45,rejected_calls=0,failed_calls=1\nredis-1  | cmdstat_persist:calls=9225068,usec=17441464,usec_per_call=1.89,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_ping:calls=10335,usec=30975,usec_per_call=3.00,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_zadd:calls=98,usec=1436,usec_per_call=14.65,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_client|id:calls=34,usec=17,usec_per_call=0.50,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_client|setname:calls=3917,usec=12456,usec_per_call=3.18,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_client|setinfo:calls=68,usec=7,usec_per_call=0.10,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_evalsha:calls=139,usec=9482344,usec_per_call=68218.30,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_multi:calls=1635,usec=1549,usec_per_call=0.95,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_set:calls=1899,usec=45597,usec_per_call=24.01,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_json.mget:calls=76,usec=791705,usec_per_call=10417.17,rejected_calls=0,failed_calls=0\nredis-1  | cmdstat_FT.TAGVALS:calls=2,usec=5631,usec_per_call=2815.50,rejected_calls=0,failed_calls=0\nredis-1  | \nredis-1  | # Errorstats\nredis-1  | errorstat_Cursor:count=2\nredis-1  | errorstat_ERR:count=18\nredis-1  | errorstat_Err:count=1\nredis-1  | errorstat_LOADING:count=3728\nredis-1  | errorstat_Syntax:count=6\nredis-1  | \nredis-1  | # Latencystats\nredis-1  | latency_percentiles_usec_hmset:p50=17.023,p99=53.247,p99.9=74.239\nredis-1  | latency_percentiles_usec_zremrangebyscore:p50=4.015,p99=34.047,p99.9=76.287\nredis-1  | latency_percentiles_usec_info:p50=28.031,p99=708.607,p99.9=831.487\nredis-1  | latency_percentiles_usec_echo:p50=0.001,p99=1.003,p99.9=1.003\nredis-1  | latency_percentiles_usec_latency|histogram:p50=634.879,p99=1122.303,p99.9=1368.063\nredis-1  | latency_percentiles_usec_latency|latest:p50=2.007,p99=12.031,p99.9=26.111\nredis-1  | latency_percentiles_usec_get:p50=4.015,p99=22.015,p99.9=50.175\nredis-1  | latency_percentiles_usec_config|get:p50=356.351,p99=1630.207,p99.9=2457.599\nredis-1  | latency_percentiles_usec_cluster|nodes:p50=407.551,p99=407.551,p99.9=407.551\nredis-1  | latency_percentiles_usec_expire:p50=7.007,p99=22.015,p99.9=40.191\nredis-1  | latency_percentiles_usec_FT.CURSOR:p50=3915.775,p99=5767.167,p99.9=5963.775\nredis-1  | latency_percentiles_usec_watch:p50=3.007,p99=24.063,p99.9=49.151\nredis-1  | latency_percentiles_usec__FT.TAGVALS:p50=1572.863,p99=2523.135,p99.9=2523.135\nredis-1  | latency_percentiles_usec_eval:p50=29753.343,p99=476053.503,p99.9=476053.503\nredis-1  | latency_percentiles_usec_unlink:p50=9.023,p99=27.007,p99.9=38.143\nredis-1  | latency_percentiles_usec_zscan:p50=3.007,p99=16.063,p99.9=25.087\nredis-1  | latency_percentiles_usec_exec:p50=15.039,p99=46.079,p99.9=53.247\nredis-1  | latency_percentiles_usec_slowlog|get:p50=9.023,p99=29.055,p99.9=47.103\nredis-1  | latency_percentiles_usec_slowlog|len:p50=3.007,p99=15.039,p99.9=30.079\nredis-1  | latency_percentiles_usec_FT.SEARCH:p50=413.695,p99=806.911,p99.9=176160.767\nredis-1  | latency_percentiles_usec_auth:p50=9.023,p99=34.047,p99.9=59.135\nredis-1  | latency_percentiles_usec_setex:p50=25.087,p99=63.231,p99.9=103.423\nredis-1  | latency_percentiles_usec_select:p50=1.003,p99=3.007,p99.9=11.007\nredis-1  | latency_percentiles_usec_subscribe:p50=5.023,p99=27.007,p99.9=27.007\nredis-1  | latency_percentiles_usec_script|load:p50=97.279,p99=292.863,p99.9=292.863\nredis-1  | latency_percentiles_usec_scan:p50=7274.495,p99=10354.687,p99.9=34865.151\nredis-1  | latency_percentiles_usec_json.get:p50=5.023,p99=25.087,p99.9=51.199\nredis-1  | latency_percentiles_usec_hmget:p50=11.007,p99=27.007,p99.9=55.039\nredis-1  | latency_percentiles_usec_FT.AGGREGATE:p50=7143.423,p99=7143.423,p99.9=7143.423\nredis-1  | latency_percentiles_usec_json.set:p50=130.047,p99=296.959,p99.9=438.271\nredis-1  | latency_percentiles_usec_persist:p50=2.007,p99=5.023,p99.9=20.095\nredis-1  | latency_percentiles_usec_ping:p50=2.007,p99=16.063,p99.9=31.103\nredis-1  | latency_percentiles_usec_zadd:p50=12.031,p99=40.191,p99.9=382.975\nredis-1  | latency_percentiles_usec_client|id:p50=0.001,p99=2.007,p99.9=2.007\nredis-1  | latency_percentiles_usec_client|setname:p50=3.007,p99=14.015,p99.9=37.119\nredis-1  | latency_percentiles_usec_client|setinfo:p50=0.001,p99=1.003,p99.9=1.003\nredis-1  | latency_percentiles_usec_evalsha:p50=34865.151,p99=513802.239,p99.9=541065.215\nredis-1  | latency_percentiles_usec_multi:p50=1.003,p99=3.007,p99.9=3.007\nredis-1  | latency_percentiles_usec_set:p50=21.119,p99=63.231,p99.9=79.359\nredis-1  | latency_percentiles_usec_json.mget:p50=9961.471,p99=14483.455,p99.9=20971.519\nredis-1  | latency_percentiles_usec_FT.TAGVALS:p50=2506.751,p99=3129.343,p99.9=3129.343\nredis-1  | \nredis-1  | # Cluster\nredis-1  | cluster_enabled:0\nredis-1  | \nredis-1  | # Keyspace\nredis-1  | db0:keys=633577,expires=31277,avg_ttl=279365869,subexpiry=0\nredis-1  | \nredis-1  | # Keysizes\nredis-1  | db0_distrib_strings_sizes:1=43,4=20,32=2,64=289,128=1978,256=4856,512=21125,1K=1087,2K=39,4K=25,8K=127,16K=144,32K=26,64K=96,128K=114,256K=2\nredis-1  | db0_distrib_zsets_items:1=2\nredis-1  | db0_distrib_hashes_items:2=1542,8K=2\nredis-1  | \nredis-1  | ------ CLIENT LIST OUTPUT ------\nredis-1  | id=42 addr=172.18.0.16:50414 laddr=172.18.0.2:6379 fd=52 name=be03c7b5b5c7(SE.Redis-v2.9.32.54708) age=38812 idle=40 flags=P db=0 sub=1 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2104 events=r cmd=ping user=default redir=-1 resp=2 lib-name=SE.Redis lib-ver=2.9.32.54708 io-thread=0 tot-net-in=9354 tot-net-out=12984 tot-cmds=651\nredis-1  | id=43 addr=172.18.0.3:47812 laddr=172.18.0.2:6379 fd=53 name=55cbe49f399f(SE.Redis-v2.9.32.54708) age=38811 idle=38 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2048 events=r cmd=info user=default redir=-1 resp=2 lib-name=SE.Redis-aspnet-DC lib-ver=2.9.32.54708 io-thread=0 tot-net-in=22096 tot-net-out=235732 tot-cmds=667\nredis-1  | id=44 addr=172.18.0.3:47816 laddr=172.18.0.2:6379 fd=54 name=55cbe49f399f(SE.Redis-v2.9.32.54708) age=38811 idle=41 flags=P db=0 sub=1 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2104 events=r cmd=ping user=default redir=-1 resp=2 lib-name=SE.Redis-aspnet-DC lib-ver=2.9.32.54708 io-thread=0 tot-net-in=9365 tot-net-out=12984 tot-cmds=651\nredis-1  | id=12 addr=172.18.0.7:46690 laddr=172.18.0.2:6379 fd=23 name=b3c625d956c8(SE.Redis-v2.9.32.54708) age=38825 idle=0 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=649 obl=0 oll=0 omem=0 tot-mem=2048 events=r cmd=setex user=default redir=-1 resp=2 lib-name=SE.Redis-aspnet-OC lib-ver=2.9.32.54708 io-thread=0 tot-net-in=741755867 tot-net-out=62990884 tot-cmds=2712764\nredis-1  | id=13 addr=172.18.0.7:46700 laddr=172.18.0.2:6379 fd=24 name=b3c625d956c8(SE.Redis-v2.9.32.54708) age=38825 idle=47 flags=P db=0 sub=1 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2104 events=r cmd=ping user=default redir=-1 resp=2 lib-name=SE.Redis-aspnet-OC lib-ver=2.9.32.54708 io-thread=0 tot-net-in=9365 tot-net-out=12984 tot-cmds=651\nredis-1  | id=14 addr=172.18.0.3:51002 laddr=172.18.0.2:6379 fd=25 name=55cbe49f399f(SE.Redis-v2.9.32.54708) age=38825 idle=6 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=188 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2240 events=r cmd=setex user=default redir=-1 resp=2 lib-name=SE.Redis-aspnet-OC lib-ver=2.9.32.54708 io-thread=0 tot-net-in=4449161 tot-net-out=1427190 tot-cmds=2101\nredis-1  | id=15 addr=172.18.0.3:51014 laddr=172.18.0.2:6379 fd=26 name=55cbe49f399f(SE.Redis-v2.9.32.54708) age=38825 idle=54 flags=P db=0 sub=1 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2104 events=r cmd=ping user=default redir=-1 resp=2 lib-name=SE.Redis-aspnet-OC lib-ver=2.9.32.54708 io-thread=0 tot-net-in=9365 tot-net-out=12984 tot-cmds=651\nredis-1  | id=16 addr=172.18.0.16:56572 laddr=172.18.0.2:6379 fd=27 name=be03c7b5b5c7(SE.Redis-v2.9.32.54708) age=38824 idle=4 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2048 events=r cmd=exec user=default redir=-1 resp=2 lib-name=SE.Redis lib-ver=2.9.32.54708 io-thread=0 tot-net-in=3570771 tot-net-out=1912418 tot-cmds=17006\nredis-1  | id=17 addr=172.18.0.16:56580 laddr=172.18.0.2:6379 fd=28 name=be03c7b5b5c7(SE.Redis-v2.9.32.54708) age=38824 idle=53 flags=P db=0 sub=1 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2104 events=r cmd=ping user=default redir=-1 resp=2 lib-name=SE.Redis lib-ver=2.9.32.54708 io-thread=0 tot-net-in=9354 tot-net-out=12984 tot-cmds=651\nredis-1  | id=19 addr=172.18.0.4:35734 laddr=172.18.0.2:6379 fd=29 name=ef0039aaeff4(SE.Redis-v2.9.32.54708) age=38812 idle=39 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2048 events=r cmd=info user=default redir=-1 resp=2 lib-name=SE.Redis-aspnet-DC lib-ver=2.9.32.54708 io-thread=0 tot-net-in=22704 tot-net-out=236372 tot-cmds=675\nredis-1  | id=20 addr=172.18.0.4:35736 laddr=172.18.0.2:6379 fd=30 name=ef0039aaeff4(SE.Redis-v2.9.32.54708) age=38812 idle=38 flags=P db=0 sub=1 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2104 events=r cmd=ping user=default redir=-1 resp=2 lib-name=SE.Redis-aspnet-DC lib-ver=2.9.32.54708 io-thread=0 tot-net-in=9365 tot-net-out=12984 tot-cmds=651\nredis-1  | id=21 addr=172.18.0.19:36050 laddr=172.18.0.2:6379 fd=31 name=8a71f337cabf(SE.Redis-v2.9.32.54708) age=38812 idle=30 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2048 events=r cmd=info user=default redir=-1 resp=2 lib-name=SE.Redis-aspnet-DC lib-ver=2.9.32.54708 io-thread=0 tot-net-in=841511 tot-net-out=1490940 tot-cmds=12507\nredis-1  | id=22 addr=172.18.0.19:36066 laddr=172.18.0.2:6379 fd=32 name=8a71f337cabf(SE.Redis-v2.9.32.54708) age=38812 idle=39 flags=P db=0 sub=1 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2104 events=r cmd=ping user=default redir=-1 resp=2 lib-name=SE.Redis-aspnet-DC lib-ver=2.9.32.54708 io-thread=0 tot-net-in=9365 tot-net-out=12984 tot-cmds=651\nredis-1  | id=23 addr=172.18.0.16:50378 laddr=172.18.0.2:6379 fd=33 name=be03c7b5b5c7(SE.Redis-v2.9.32.54708) age=38812 idle=4 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2048 events=r cmd=expire user=default redir=-1 resp=2 lib-name=SE.Redis-aspnet-DC lib-ver=2.9.32.54708 io-thread=0 tot-net-in=296328 tot-net-out=313509 tot-cmds=4202\nredis-1  | id=24 addr=172.18.0.16:50392 laddr=172.18.0.2:6379 fd=34 name=be03c7b5b5c7(SE.Redis-v2.9.32.54708) age=38812 idle=39 flags=P db=0 sub=1 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2104 events=r cmd=ping user=default redir=-1 resp=2 lib-name=SE.Redis-aspnet-DC lib-ver=2.9.32.54708 io-thread=0 tot-net-in=9365 tot-net-out=12984 tot-cmds=651\nredis-1  | id=25 addr=172.18.0.7:55506 laddr=172.18.0.2:6379 fd=35 name=b3c625d956c8(SE.Redis-v2.9.32.54708) age=38812 idle=34 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2048 events=r cmd=info user=default redir=-1 resp=2 lib-name=SE.Redis-aspnet-DC lib-ver=2.9.32.54708 io-thread=0 tot-net-in=33068 tot-net-out=246748 tot-cmds=810\nredis-1  | id=26 addr=172.18.0.7:55522 laddr=172.18.0.2:6379 fd=36 name=b3c625d956c8(SE.Redis-v2.9.32.54708) age=38812 idle=34 flags=P db=0 sub=1 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2104 events=r cmd=ping user=default redir=-1 resp=2 lib-name=SE.Redis-aspnet-DC lib-ver=2.9.32.54708 io-thread=0 tot-net-in=9365 tot-net-out=12984 tot-cmds=651\nredis-1  | id=27 addr=172.18.0.5:38166 laddr=172.18.0.2:6379 fd=37 name=e0efe3b92242(SE.Redis-v2.9.32.54708) age=38812 idle=0 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1536 rbp=5 obl=0 oll=0 omem=0 tot-mem=2560 events=r cmd=FT.SEARCH user=default redir=-1 resp=2 lib-name=SE.Redis lib-ver=2.9.32.54708 io-thread=0 tot-net-in=154195621 tot-net-out=672383871 tot-cmds=1878762\nredis-1  | id=28 addr=172.18.0.5:38178 laddr=172.18.0.2:6379 fd=38 name=e0efe3b92242(SE.Redis-v2.9.32.54708) age=38812 idle=35 flags=P db=0 sub=1 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2104 events=r cmd=ping user=default redir=-1 resp=2 lib-name=SE.Redis lib-ver=2.9.32.54708 io-thread=0 tot-net-in=9340 tot-net-out=12964 tot-cmds=650\nredis-1  | id=29 addr=172.18.0.3:47796 laddr=172.18.0.2:6379 fd=39 name=55cbe49f399f(SE.Redis-v2.9.32.54708) age=38812 idle=2 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=5 obl=0 oll=0 omem=0 tot-mem=2048 events=r cmd=FT.SEARCH user=default redir=-1 resp=2 lib-name=SE.Redis lib-ver=2.9.32.54708 io-thread=0 tot-net-in=928906 tot-net-out=10133106 tot-cmds=10244\nredis-1  | id=30 addr=172.18.0.3:47802 laddr=172.18.0.2:6379 fd=40 name=55cbe49f399f(SE.Redis-v2.9.32.54708) age=38812 idle=40 flags=P db=0 sub=1 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2104 events=r cmd=ping user=default redir=-1 resp=2 lib-name=SE.Redis lib-ver=2.9.32.54708 io-thread=0 tot-net-in=9354 tot-net-out=12984 tot-cmds=651\nredis-1  | id=31 addr=172.18.0.19:36076 laddr=172.18.0.2:6379 fd=41 name=8a71f337cabf(SE.Redis-v2.9.32.54708) age=38812 idle=0 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=10 obl=0 oll=0 omem=0 tot-mem=2048 events=r cmd=persist user=default redir=-1 resp=2 lib-name=SE.Redis lib-ver=2.9.32.54708 io-thread=0 tot-net-in=9403979442 tot-net-out=9498177166 tot-cmds=48022180\nredis-1  | id=32 addr=172.18.0.19:36080 laddr=172.18.0.2:6379 fd=42 name=8a71f337cabf(SE.Redis-v2.9.32.54708) age=38812 idle=33 flags=P db=0 sub=1 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2104 events=r cmd=ping user=default redir=-1 resp=2 lib-name=SE.Redis lib-ver=2.9.32.54708 io-thread=0 tot-net-in=9354 tot-net-out=12984 tot-cmds=651\nredis-1  | id=33 addr=172.18.0.10:35438 laddr=172.18.0.2:6379 fd=43 name=d393b8540f4a(SE.Redis-v2.9.32.54708) age=38812 idle=38 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2048 events=r cmd=info user=default redir=-1 resp=2 lib-name=SE.Redis-aspnet-DC lib-ver=2.9.32.54708 io-thread=0 tot-net-in=21488 tot-net-out=235092 tot-cmds=659\nredis-1  | id=34 addr=172.18.0.10:35444 laddr=172.18.0.2:6379 fd=44 name=d393b8540f4a(SE.Redis-v2.9.32.54708) age=38812 idle=41 flags=P db=0 sub=1 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2104 events=r cmd=ping user=default redir=-1 resp=2 lib-name=SE.Redis-aspnet-DC lib-ver=2.9.32.54708 io-thread=0 tot-net-in=9365 tot-net-out=12984 tot-cmds=651\nredis-1  | id=35 addr=172.18.0.7:55528 laddr=172.18.0.2:6379 fd=45 name=b3c625d956c8(SE.Redis-v2.9.32.54708) age=38812 idle=0 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=5 obl=0 oll=0 omem=0 tot-mem=2048 events=r cmd=FT.SEARCH user=default redir=-1 resp=2 lib-name=SE.Redis lib-ver=2.9.32.54708 io-thread=0 tot-net-in=196543583 tot-net-out=1063694555 tot-cmds=2532311\nredis-1  | id=36 addr=172.18.0.7:55544 laddr=172.18.0.2:6379 fd=46 name=b3c625d956c8(SE.Redis-v2.9.32.54708) age=38812 idle=37 flags=P db=0 sub=1 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2104 events=r cmd=ping user=default redir=-1 resp=2 lib-name=SE.Redis lib-ver=2.9.32.54708 io-thread=0 tot-net-in=9354 tot-net-out=12984 tot-cmds=651\nredis-1  | id=37 addr=172.18.0.10:35452 laddr=172.18.0.2:6379 fd=47 name=d393b8540f4a(SE.Redis-v2.9.32.54708) age=38812 idle=6 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2048 events=r cmd=FT.SEARCH user=default redir=-1 resp=2 lib-name=SE.Redis lib-ver=2.9.32.54708 io-thread=0 tot-net-in=141102 tot-net-out=946969 tot-cmds=2160\nredis-1  | id=38 addr=172.18.0.10:35456 laddr=172.18.0.2:6379 fd=48 name=d393b8540f4a(SE.Redis-v2.9.32.54708) age=38812 idle=39 flags=P db=0 sub=1 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2104 events=r cmd=ping user=default redir=-1 resp=2 lib-name=SE.Redis lib-ver=2.9.32.54708 io-thread=0 tot-net-in=9354 tot-net-out=12984 tot-cmds=651\nredis-1  | id=39 addr=172.18.0.4:35740 laddr=172.18.0.2:6379 fd=49 name=ef0039aaeff4(SE.Redis-v2.9.32.54708) age=38812 idle=36 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2048 events=r cmd=info user=default redir=-1 resp=2 lib-name=SE.Redis lib-ver=2.9.32.54708 io-thread=0 tot-net-in=224772 tot-net-out=1165704 tot-cmds=2717\nredis-1  | id=40 addr=172.18.0.4:35750 laddr=172.18.0.2:6379 fd=50 name=ef0039aaeff4(SE.Redis-v2.9.32.54708) age=38812 idle=36 flags=P db=0 sub=1 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2104 events=r cmd=ping user=default redir=-1 resp=2 lib-name=SE.Redis lib-ver=2.9.32.54708 io-thread=0 tot-net-in=9354 tot-net-out=12984 tot-cmds=651\nredis-1  | id=41 addr=172.18.0.16:50406 laddr=172.18.0.2:6379 fd=51 name=be03c7b5b5c7(SE.Redis-v2.9.32.54708) age=38812 idle=4 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2048 events=r cmd=json.set user=default redir=-1 resp=2 lib-name=SE.Redis lib-ver=2.9.32.54708 io-thread=0 tot-net-in=2703929 tot-net-out=2627005 tot-cmds=8385\nredis-1  | \nredis-1  | ------ MODULES INFO OUTPUT ------\nredis-1  | # search_version\nredis-1  | search_version:8.4.2\nredis-1  | search_redis_version:8.4.0 - oss\nredis-1  | \nredis-1  | # search_indexes\nredis-1  | search_number_of_indexes:2\nredis-1  | search_number_of_active_indexes:0\nredis-1  | search_number_of_active_indexes_running_queries:0\nredis-1  | search_number_of_active_indexes_indexing:0\nredis-1  | search_total_active_write_threads:0\nredis-1  | search_total_indexing_time:1132583168\nredis-1  | \nredis-1  | # search_fields_statistics\nredis-1  | search_fields_text:Text=2,Sortable=1,IndexErrors=0\nredis-1  | search_fields_numeric:Numeric=7,Sortable=6,IndexErrors=0\nredis-1  | search_fields_tag:Tag=6,Sortable=1,IndexErrors=0\nredis-1  | \nredis-1  | # search_memory\nredis-1  | search_used_memory_indexes:440349940\nredis-1  | search_used_memory_indexes_human:419.9504280090332\nredis-1  | search_smallest_memory_index:16096\nredis-1  | search_smallest_memory_index_human:0.015350341796875\nredis-1  | search_largest_memory_index:440333844\nredis-1  | search_largest_memory_index_human:419.93507766723633\nredis-1  | search_used_memory_vector_index:0\nredis-1  | \nredis-1  | # search_cursors\nredis-1  | search_global_idle_user:0\nredis-1  | search_global_idle_internal:0\nredis-1  | search_global_total_user:0\nredis-1  | search_global_total_internal:0\nredis-1  | \nredis-1  | # search_garbage_collector\nredis-1  | search_gc_bytes_collected:1048862703\nredis-1  | search_gc_total_cycles:97\nredis-1  | search_gc_total_ms_run:34101327\nredis-1  | search_gc_total_docs_not_collected:213860\nredis-1  | search_gc_marked_deleted_vectors:0\nredis-1  | \nredis-1  | # search_queries\nredis-1  | search_total_queries_processed:2216521\nredis-1  | search_total_query_commands:2216596\nredis-1  | search_total_query_execution_time_ms:1670732\nredis-1  | search_total_active_queries:0\nredis-1  | \nredis-1  | # search_warnings_and_errors\nredis-1  | search_errors_indexing_failures:0\nredis-1  | search_errors_for_index_with_max_failures:0\nredis-1  | search_OOM_indexing_failures_indexes_count:0\nredis-1  | \nredis-1  | # search_dialect_statistics\nredis-1  | search_dialect_1:1\nredis-1  | search_dialect_2:0\nredis-1  | search_dialect_3:0\nredis-1  | search_dialect_4:0\nredis-1  | \nredis-1  | # search_runtime_configurations\nredis-1  | search_extension_load:\nredis-1  | search_friso_ini:\nredis-1  | search_default_scorer:BM25STD\nredis-1  | search_enableGC:ON\nredis-1  | search_minimal_term_prefix:2\nredis-1  | search_minimal_stem_length:4\nredis-1  | search_maximal_prefix_expansions:200\nredis-1  | search_query_timeout_ms:500\nredis-1  | search_timeout_policy:return\nredis-1  | search_oom_policy:return\nredis-1  | search_cursor_read_size:1000\nredis-1  | search_cursor_max_idle_time:300000\nredis-1  | search_max_doc_table_size:1000000\nredis-1  | search_max_search_results:1000000\nredis-1  | search_max_aggregate_results:2147483648\nredis-1  | search_gc_scan_size:100\nredis-1  | search_min_phonetic_term_length:3\nredis-1  | search_bm25std_tanh_factor:4\nredis-1  | \nredis-1  | # search_current_thread\nredis-1  | \nredis-1  | # search_blocked_queries\nredis-1  | \nredis-1  | # search_blocked_cursors\nredis-1  | \nredis-1  | # ReJSON_trace\nredis-1  | ReJSON_backtrace:   0: redis_module::basic_info_command_handler\nredis-1  |    1: rejson::__info_func\nredis-1  |    2: modulesCollectInfo\nredis-1  |              at /usr/src/redis/src/module.c:10847:9\nredis-1  |    3: logModulesInfo\nredis-1  |              at /usr/src/redis/src/debug.c:2191:22\nredis-1  |       printCrashReport\nredis-1  |              at /usr/src/redis/src/debug.c:2559:5\nredis-1  |    4: sigsegvHandler\nredis-1  |              at /usr/src/redis/src/debug.c:2478:32\nredis-1  |    5: <unknown>\nredis-1  |    6: rtree_leaf_elm_lookup\nredis-1  |              at /usr/src/redis/deps/jemalloc/include/jemalloc/internal/rtree.h:371:10\nredis-1  |       rtree_metadata_read\nredis-1  |              at /usr/src/redis/deps/jemalloc/include/jemalloc/internal/rtree.h:443:26\nredis-1  |       emap_alloc_ctx_lookup\nredis-1  |              at /usr/src/redis/deps/jemalloc/include/jemalloc/internal/emap.h:238:30\nredis-1  |       ifree\nredis-1  |              at /usr/src/redis/deps/jemalloc/src/jemalloc.c:2887:2\nredis-1  |       je_free_default\nredis-1  |              at /usr/src/redis/deps/jemalloc/src/jemalloc.c:3025:4\nredis-1  |    7: RM_Free\nredis-1  |              at /usr/src/redis/src/module.c:575:6\nredis-1  |    8: FGC_parentHandleFromChild\nredis-1  |    9: periodicCb\nredis-1  |   10: taskCallback\nredis-1  |   11: thread_do\nredis-1  |   12: <unknown>\nredis-1  |   13: __clone\nredis-1  | \nredis-1  | \nredis-1  | ------ CONFIG DEBUG OUTPUT ------\nredis-1  | activedefrag no\nredis-1  | lazyfree-lazy-server-del no\nredis-1  | proto-max-bulk-len 512mb\nredis-1  | slave-read-only yes\nredis-1  | repl-diskless-load disabled\nredis-1  | lazyfree-lazy-user-del no\nredis-1  | lazyfree-lazy-eviction no\nredis-1  | io-threads 1\nredis-1  | repl-diskless-sync yes\nredis-1  | sanitize-dump-payload no\nredis-1  | list-compress-depth 0\nredis-1  | lazyfree-lazy-expire no\nredis-1  | client-query-buffer-limit 1gb\nredis-1  | lazyfree-lazy-user-flush no\nredis-1  | replica-read-only yes\nredis-1  | \nredis-1  | ------ FAST MEMORY TEST ------\nredis-1  | 1:M 27 Nov 2025 11:21:20.805 # main thread terminated\nredis-1  | 1:M 27 Nov 2025 11:21:20.805 # Bio worker thread #0 terminated\nredis-1  | 1:M 27 Nov 2025 11:21:20.806 # Bio worker thread #1 terminated\nredis-1  | 1:M 27 Nov 2025 11:21:20.806 # Bio worker thread #2 terminated\nredis-1  | \nredis-1  | Fast memory test PASSED, however your memory can still be broken. Please run a memory test for several hours if possible.\nredis-1  | \nredis-1  | ------ DUMPING CODE AROUND EIP ------\nredis-1  | Symbol: (null) (base: (nil))\nredis-1  | Module: redis-server *:6379 (base 0x560c1c95f000)\nredis-1  | $ xxd -r -p /tmp/dump.hex /tmp/dump.bin\nredis-1  | $ objdump --adjust-vma=(nil) -D -b binary -m i386:x86-64 /tmp/dump.bin\nredis-1  | ------\nredis-1  | \nredis-1  | === REDIS BUG REPORT END. Make sure to include from START to END. ===\n```\n\n**Additional information**\n\n1. Ubuntu, redis hosted on docker\n2. Steps to reproduce (if any) - i don't know, randomly crashes sometimes.\n",
      "solution": "close via https://github.com/redis/redis/issues/14469#issuecomment-3581656015, feel free to reopen it if the issue still exists.",
      "labels": [],
      "created_at": "2025-11-27T12:53:59Z",
      "closed_at": "2025-12-10T12:25:42Z",
      "url": "https://github.com/redis/redis/issues/14586",
      "comments_count": 2
    },
    {
      "tech": "redis",
      "repo": "redis/redis",
      "issue_number": 14469,
      "title": "[CRASH] RedisSearch module crash: Redis 8.2.1 segfault in raxGenericInsert (signal 11)",
      "problem": "## Summary\nRedis crashed with SIGSEGV (segmentation fault) while running with the RedisSearch module (redisearch.so) on Redis 8.2.1 (Bitnami container). The crash occurred in core Redis's radix tree code (`raxGenericInsert`), called from within the RedisSearch module. This appears to be a module bug, module/core incompatibility, or a rare concurrency/memory issue.\n\n## Key Log Excerpts\n```\n1:M 26 Oct 2025 07:45:51.549 # Redis 8.2.1 crashed by signal: 11, si_code: 1\n1:M 26 Oct 2025 07:45:51.549 # Accessing address: (nil)\n1:M 26 Oct 2025 07:45:51.549 # Crashed running the instruction at: 0x55b3f56a8817\n------ STACK TRACE ------\nEIP:\nredis-server *:6379(raxGenericInsert+0xb7)[0x55b3f56a8817]\n1 redis-server\n/usr/lib/libc.so.6(epoll_wait+0x56)[0x7f348e40a7f6]\nredis-server *:6379(+0x9d364)[0x55b3f5548364]\nredis-server *:6379(aeMain+0xd0)[0x55b3f5548a60]\nredis-server *:6379(main+0x4c2)[0x55b3f5542942]\n/usr/lib/libc.so.6(+0x2724a)[0x7f348e32924a]\n/usr/lib/libc.so.6(__libc_start_main+0x85)[0x7f348e329305]\nredis-server *:6379(_start+0x21)[0x55b3f5544321]\n\n17 io_thd_2\n/usr/lib/libc.so.6(epoll_wait+0x56)[0x7f348e40a7f6]\nredis-server *:6379(+0x9d364)[0x55b3f5548364]\nredis-server *:6379(aeMain+0xd0)[0x55b3f5548a60]\nredis-server *:6379(IOThreadMain+0x95)[0x55b3f5559ec5]\n/usr/lib/libc.so.6(+0x890c4)[0x7f348e38b0c4]\n/usr/lib/libc.so.6(__clone+0x40)[0x7f348e40a410]\n\n18 io_thd_3\n/usr/lib/libc.so.6(epoll_wait+0x56)[0x7f348e40a7f6]\nredis-server *:6379(+0x9d364)[0x55b3f5548364]\nredis-server *:6379(aeMain+0xd0)[0x55b3f5548a60]\nredis-server *:6379(IOThreadMain+0x95)[0x55b3f5559ec5]\n/usr/lib/libc.so.6(+0x890c4)[0x7f348e38b0c4]\n/usr/lib/libc.so.6(__clone+0x40)[0x7f348e40a410]\n\n23 cleanPool-8028\n/usr/lib/libc.so.6(+0x85e26)[0x7f348e387e26]\n/usr/lib/libc.so.6(pthread_cond_wait+0x1e8)[0x7f348e38a4e8]\n/opt/bitnami/redis/lib/redis/modules/redisearch.so(+0x3363db)[0x7f348c8253db]\n/opt/bitnami/redis/lib/redis/modules/redisearch.so(+0x3362b9)[0x7f348c8252b9]\n/usr/lib/libc.so.6(+0x890c4)[0x7f348e38b0c4]\n/usr/lib/libc.so.6(__clone+0x40)[0x7f348e40a410]\n\n16 io_thd_1\n/usr/lib/libc.so.6(epoll_wait+0x56)[0x7f348e40a7f6]\nredis-server *:6379(+0x9d364)[0x55b3f5548364]\nredis-server *:6379(aeMain+0xd0)[0x55b3f5548a60]\nredis-server *:6379(IOThreadMain+0x95)[0x55b3f5559ec5]\n/usr/lib/libc.so.6(+0x890c4)[0x7f348e38b0c4]\n/usr/lib/libc.so.6(__clone+0x40)[0x7f348e40a410]\n\n13 bio_close_file\n/usr/lib/libc.so.6(+0x85e26)[0x7f348e387e26]\n/usr/lib/libc.so.6(pthread_cond_wait+0x1e8)[0x7f348e38a4e8]\nredis-server *:6379(bioProcessBackgroundJobs+0x30b)[0x55b3f56486ab]\n/usr/lib/libc.so.6(+0x890c4)[0x7f348e38b0c4]\n/usr/lib/libc.so.6(__clone+0x40)[0x7f348e40a410]\n\n15 bio_lazy_free\n/usr/lib/libc.so.6(+0x85e26)[0x7f348e387e26]\n/usr/lib/libc.so.6(pthread_cond_wait+0x1e8)[0x7f348e38a4e8]\nredis-server *:6379(bioProcessBackgroundJobs+0x30b)[0x55b3f56486ab]\n/usr/lib/libc.so.6(+0x890c4)[0x7f348e38b0c4]\n/usr/lib/libc.so.6(__clone+0x40)[0x7f348e40a410]\n\n24 gc-0214 *\n/usr/lib/libc.so.6(+0x3c050)[0x7f348e33e050]\nredis-server *:6379(raxGenericInsert+0xb7)[0x55b3f56a8817]\nredis-server *:6379(RM_GetServerInfo+0x1ec)[0x55b3f5690aec]\n/opt/bitnami/redis/lib/redis/modules/redisearch.so(+0x36f55d)[0x7f348c85e55d]\n/opt/bitnami/redis/lib/redis/modules/redisearch.so(+0x371e6a)[0x7f348c860e6a]\n/opt/bitnami/redis/lib/redis/modules/redisearch.so(+0x3362e8)[0x7f348c8252e8]\n/usr/lib/libc.so.6(+0x890c4)[0x7f348e38b0c4]\n/usr/lib/libc.so.6(__clone+0x40)[0x7f348e40a410]\n\n14 bio_aof\n/usr/lib/libc.so.6(+0x85e26)[0x7f348e387e26]\n/usr/lib/libc.so.6(pthread_cond_wait+0x1e8)[0x7f348e38a4e8]\nredis-server *:6379(bioProcessBackgroundJobs+0x30b)[0x55b3f56486ab]\n/usr/lib/libc.so.6(+0x890c4)[0x7f348e38b0c4]\n/usr/lib/libc.so.6(__clone+0x40)[0x7f348e40a410]\n\n9/9 expected stacktraces.\n\n1:M 26 Oct 2025 07:45:51.557 # \nRAX:0000000000000156 RBX:0000000000000075\nRCX:00007f33b3aa7b58 RDX:000000000f17221c\nRDI:00000000000000d0 RSI:00007f33a4934e8c\nRBP:00007f3461dfe0a0 RSP:00007f3461dfdfe0\nR8 :0000000000000000 R9 :0000000000000000\nR10:00007f3478b910d0 R11:000000000f17221a\nR12:0000000000000001 R13:000000000000000b\nR14:00007f3478a322f1 R15:0000000000000000\nRIP:000055b3f56a8817 EFL:0000000000010212\nCSGSFS:002b000000000033\n1:M 26 Oct 2025 07:45:51.557 # (00007f3461dfdfef) -> 00007f3478a322fd\n1:M 26 Oct 2025 07:45:51.557 # (00007f3461dfdfee) -> 000000000000000a\n1:M 26 Oct 2025 07:45:51.557 # (00007f3461dfdfed) -> af983bef272f8600\n1:M 26 Oct 2025 07:45:51.557 # (00007f3461dfdfec) -> 0000000000000010\n1:M 26 Oct 2025 07:45:51.557 # (00007f3461dfdfeb) -> 000055b3f5578961\n1:M 26 Oct 2025 07:45:51.557 # (00007f3461dfdfea) -> 00007f3461dfe0f4\n1:M 26 Oct 2025 07:45:51.557 # (00007f3461dfdfe9) -> 00007f33a1cac671\n1:M 26 Oct 2025 07:45:51.557 # (00007f3461dfdfe8) -> 00007f3461dfe0a0\n1:M 26 Oct 2025 07:45:51.557 # (00007f3461dfdfe7) -> 000000000000060e\n1:M 26 Oct 2025 07:45:51.557 # (00007f3461dfdfe6) -> 0000000000000038\n1:M 26 Oct 2025 07:45:51.557 # (00007f3461dfdfe5) -> 0000000000000038\n1:M 26 Oct 2025 07:45:51.557 # (00007f3461dfdfe4) -> 000000000000060e\n1:M 26 Oct 2025 07:45:51.557 # (00007f3461dfdfe3) -> af983bef272f8600\n1:M 26 Oct 2025 07:45:51.557 # (00007f3461dfdfe2) -> 0000000000000008\n1:M 26 Oct 2025 07:45:51.557 # (00007f3461dfdfe1) -> 000055b3f570d4e7\n1:M 26 Oct 2025 07:45:51.557 # (00007f3461dfdfe0) -> 00007f3461dfe020\n\n------ INFO OUTPUT ------\n# Server\nredis_version:8.2.1\nredis_mode:standalone\n[...]\n# Modules\nmodule:name=ReJSON,ver=80200,api=1,filters=0,usedby=[search],using=[],options=[handle-io-errors]\nmodule:name=vectorset,ver=1,api=1,filters=0,usedby=[],using=[],options=[handle-io-errors|handle-repl-async-load]\nmodule:name=search,ver=80201,api=1,filters=0,usedby=[],using=[ReJSON],options=[handle-io-errors]\n\n# search_version\nsearch_version:8.2.1\nsearch_redis_version:8.2.1 - oss\n[...]\n------ FAST MEMORY TEST ------\n1:M 26 Oct 2025 07:45:51.577 # main thread terminated\n[...]\n=== REDIS BUG REPORT END. Make sure to include from START to END. ===\n```\n\n## Observations\n- Crash occurs in Redis core radix tree (rax) logic, called from RedisSearch module code.\n- Repeated `ForkGC - got timeout while reading from pipe (Success)` messages prior to crash\u2014appears related to RedisSearch index GC.\n- Heavy usage of FT.SEARCH and JSON commands (high call counts, high p99 latency).\n- IO threads enabled (io-threads 4), both RedisSearch and ReJSON loaded.\n- No OOM or maxmemory reached; RSS and fragmentation normal.\n\n## Environment\n- Redis: 8.2.1 (Bitnami container)\n- RedisSearch: 8.2.1 (as per module info)\n- ReJSON: 8.2.0\n- vectorset: 1\n- OS: Linux 5.14.0-427.85.1.el9_4.x86_64 x86_64\n- Memory: ~3GB used, 31GB total\n- Workload: High concurrency, heavy FT.SEARCH and JSON.GET/SET, pubsub activity\n\n## Steps to Reproduce\n1. Run Redis 8.2.1 with RedisSearch 8.2.1 & ReJSON 8.2.0 modules loaded (Bitnami container)\n2. Enable IO threads (io-threads 4)\n3. Heavy usage of FT.SEARCH and JSON commands\n4. Observe repeated ForkGC timeouts, then eventual segfault\n\n## Attachments\n- Full crash log (START to END) available upon request\n\n## Additional Diagnostics\n- Fast memory test: PASSED\n- RDB persistence (AOF disabled)\n- No maxmemory set\n- No recent OOM/killer events in dmesg\n\n## Recommendation\nThis appears to be a bug in the RedisSearch module or a module/core compatibility issue. Please advise on:\n- Known issues with RedisSearch 8.2.1 and Redis 8.2.1\n- Safe upgrade path or workaround\n- Further debugging steps (can provide core dump if needed)\n\n---\nLet me know if further logs or configs are needed. This is a production-impacting crash with RedisSearch.\n[](url)",
      "solution": "We believe we found the root cause. Two bugs join forces, leading to corruption and a later crash.\n1. https://github.com/RediSearch/RediSearch/issues/7423 \n2. https://github.com/RediSearch/RediSearch/issues/7441\n\nWill be fixed in the next patch releases, 8.2.4 for Redis 8.2, and 8.4.1 for 8.4",
      "labels": [
        "crash report"
      ],
      "created_at": "2025-10-27T03:23:40Z",
      "closed_at": "2025-11-26T15:02:25Z",
      "url": "https://github.com/redis/redis/issues/14469",
      "comments_count": 12
    },
    {
      "tech": "redis",
      "repo": "redis/redis",
      "issue_number": 14339,
      "title": "[BUG] Assertion failed: rep && RedisModule_CallReplyType(rep) == REDISMODULE_REPLY_ARRAY && RedisModule_CallReplyLength(rep) == 2",
      "problem": "**Describe the bug**\n\nsystemctl start redis-server\uff0cbut failed:\n\nJob for redis-server.service failed because a fatal signal was delivered causing the control process to dump core.\nSee \"systemctl status redis-server.service\" and \"journalctl -xeu redis-server.service\" for details.\n\nA short description of the bug.\n\n1. OS: Ubuntu24.04\n2. Add the repository to the APT index, update it, and install Redis v=8.2.1\n3. vim /etc/redis/redis.conf, uncomment the following line\n     rename-command CONFIG \"\"  \n\n4. view redis-server.log\n    \n10057:M 08 Sep 2025 17:39:07.417 * <timeseries> Enabled diskless replication\n10057:M 08 Sep 2025 17:39:07.417 * Module 'timeseries' loaded from /usr/lib/redis/modules/redistimeseries.so\n10057:M 08 Sep 2025 17:39:07.419 # <search> Assertion failed: rep && RedisModule_CallReplyType(rep) == REDISMODULE_REPLY_ARRAY && RedisModule_CallReplyLength(rep) == 2\n\n\n=== REDIS BUG REPORT START: Cut & paste starting from here ===\n10057:M 08 Sep 2025 17:39:07.419 # === ASSERTION FAILED ===\n10057:M 08 Sep 2025 17:39:07.419 # ==> ./modules/redisearch/src/src/module.c:3666 'rep && RedisModule_CallReplyType(rep) == 3 && RedisModule_CallReplyLength(rep) == 2' is not true\n\n------ STACK TRACE ------\n\n10057 redis-server *\n/usr/lib/redis/modules/redisearch.so(RedisModule_OnLoad+0x2920)[0x7cceb0ac1f40]\n/usr/bin/redis-server 127.0.0.1:6379(moduleOnLoad+0x69)[0x57089a099ad9]\n/usr/bin/redis-server 127.0.0.1:6379(moduleLoad+0x92)[0x57089a099f02]\n/usr/bin/redis-server 127.0.0.1:6379(moduleLoadFromQueue+0x5e)[0x57089a09a01e]\n/usr/bin/redis-server 127.0.0.1:6379(main+0x693)[0x570899f79fa3]\n/lib/x86_64-linux-gnu/libc.so.6(+0x2a1ca)[0x7cceb3a2a1ca] \n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x8b)[0x7cceb3a2a28b]\n/usr/bin/redis-server 127.0.0.1:6379(_start+0x25)[0x570899f7b535] \n\n1/1 expected stacktraces.\n\n------ STACK TRACE DONE ------\n\n\n",
      "solution": "It's a known issue: https://github.com/RediSearch/RediSearch/issues/6098\nyou can remove the `rename config` temporally, i'll notify you when it's fixed. thx.",
      "labels": [],
      "created_at": "2025-09-08T09:43:54Z",
      "closed_at": "2025-11-06T06:13:39Z",
      "url": "https://github.com/redis/redis/issues/14339",
      "comments_count": 2
    },
    {
      "tech": "redis",
      "repo": "redis/redis",
      "issue_number": 14491,
      "title": "[CRASH] xackdel multiples messages",
      "problem": "**Crash report**\n\n```\n2025-10-30 10:50:02 Starting Redis Server\n2025-10-30 10:50:02 1:C 30 Oct 2025 09:50:02.831 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n2025-10-30 10:50:02 1:C 30 Oct 2025 09:50:02.831 * Redis version=8.2.2, bits=64, commit=00000000, modified=1, pid=1, just started\n2025-10-30 10:50:02 1:C 30 Oct 2025 09:50:02.831 * Configuration loaded\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.831 * monotonic clock: POSIX clock_gettime\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.832 * Running mode=standalone, port=6379.\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.833 * <bf> RedisBloom version 8.2.3 (Git=unknown)\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.833 * <bf> Registering configuration options: [\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.833 * <bf>         { bf-error-rate       :      0.01 }\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.833 * <bf>         { bf-initial-size     :       100 }\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.833 * <bf>         { bf-expansion-factor :         2 }\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.833 * <bf>         { cf-bucket-size      :         2 }\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.834 * <bf>         { cf-initial-size     :      1024 }\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.834 * <bf>         { cf-max-iterations   :        20 }\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.834 * <bf>         { cf-expansion-factor :         1 }\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.834 * <bf>         { cf-max-expansions   :        32 }\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.834 * <bf> ]\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.834 * Module 'bf' loaded from /usr/local/lib/redis/modules//redisbloom.so\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.837 * <search> Redis version found by RedisSearch : 8.2.2 - oss\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.837 * <search> RediSearch version 8.2.5 (Git=222ad3b)\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.837 * <search> Low level api version 1 initialized successfully\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.837 * <search> gc: ON, prefix min length: 2, min word length to stem: 4, prefix max expansions: 200, query timeout (ms): 500, timeout policy: return, cursor read size: 1000, cursor max idle (ms): 300000, max doctable size: 1000000, max number of search results:  1000000, \n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.837 * <search> Initialized thread pools!\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.837 * <search> Disabled workers threadpool of size 0\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.837 * <search> Subscribe to config changes\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.837 * <search> Enabled role change notification\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.837 * <search> Cluster configuration: AUTO partitions, type: 0, coordinator timeout: 0ms\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.837 * <search> Register write commands\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.838 * Module 'search' loaded from /usr/local/lib/redis/modules//redisearch.so\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.838 * <timeseries> RedisTimeSeries version 80200, git_sha=1439d4a439ca9c063e6ef124a510abff09a5d493\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.838 * <timeseries> Redis version found by RedisTimeSeries : 8.2.2 - oss\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.838 * <timeseries> Registering configuration options: [\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.838 * <timeseries>         { ts-compaction-policy   :              }\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.838 * <timeseries>         { ts-num-threads         :            3 }\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.838 * <timeseries>         { ts-retention-policy    :            0 }\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.838 * <timeseries>         { ts-duplicate-policy    :        block }\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.838 * <timeseries>         { ts-chunk-size-bytes    :         4096 }\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.838 * <timeseries>         { ts-encoding            :   compressed }\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.838 * <timeseries>         { ts-ignore-max-time-diff:            0 }\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.838 * <timeseries>         { ts-ignore-max-val-diff :     0.000000 }\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.838 * <timeseries> ]\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.838 * <timeseries> Detected redis oss\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.838 * <timeseries> Enabled diskless replication\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.838 * Module 'timeseries' loaded from /usr/local/lib/redis/modules//redistimeseries.so\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.839 * <ReJSON> Created new data type 'ReJSON-RL'\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.839 * <ReJSON> version: 80201 git sha: unknown branch: unknown\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.839 * <ReJSON> Exported RedisJSON_V1 API\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.839 * <ReJSON> Exported RedisJSON_V2 API\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.839 * <ReJSON> Exported RedisJSON_V3 API\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.839 * <ReJSON> Exported RedisJSON_V4 API\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.839 * <ReJSON> Exported RedisJSON_V5 API\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.839 * <ReJSON> Enabled diskless replication\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.839 * <ReJSON> Initialized shared string cache, thread safe: false.\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.839 * Module 'ReJSON' loaded from /usr/local/lib/redis/modules//rejson.so\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.839 * <search> Acquired RedisJSON_V5 API\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.871 * Server initialized\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.871 * <search> Loading event starts\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.871 * <search> Enabled workers threadpool of size 4\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.871 * Loading RDB produced by version 8.2.2\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.871 * RDB age 3695 seconds\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.871 * RDB memory usage when created 1.10 Mb\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.871 * Done loading RDB, keys loaded: 1, keys expired: 0.\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.872 * <search> Disabled workers threadpool of size 4\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.872 * <search> Loading event ends\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.872 * DB loaded from disk: 0.001 seconds\n2025-10-30 10:50:02 1:M 30 Oct 2025 09:50:02.872 * Ready to accept connections tcp\n2025-10-30 10:51:36 === REDIS BUG REPORT START: Cut & paste starting from here ===\n2025-10-30 10:51:36 1:M 30 Oct 2025 09:51:36.662 # Redis 8.2.2 crashed by signal: 11, si_code: 128\n2025-10-30 10:51:36 1:M 30 Oct 2025 09:51:36.662 # Accessing address: 0\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 ------ INFO OUTPUT ------\n2025-10-30 10:51:36 # Server\n2025-10-30 10:51:36 redis_version:8.2.2\n2025-10-30 10:51:36 redis_git_sha1:00000000\n2025-10-30 10:51:36 redis_git_dirty:1\n2025-10-30 10:51:36 redis_build_id:705dfe06669e4ffb\n2025-10-30 10:51:36 redis_mode:standalone\n2025-10-30 10:51:36 os:Linux 6.12.5-linuxkit x86_64\n2025-10-30 10:51:36 arch_bits:64\n2025-10-30 10:51:36 monotonic_clock:POSIX clock_gettime\n2025-10-30 10:51:36 multiplexing_api:epoll\n2025-10-30 10:51:36 atomicvar_api:c11-builtin\n2025-10-30 10:51:36 gcc_version:14.2.0\n2025-10-30 10:51:36 process_id:1\n2025-10-30 10:51:36 process_supervised:no\n2025-10-30 10:51:36 run_id:ced058f8f35eebf337233a0dd7ab71df017aff7b\n2025-10-30 10:51:36 tcp_port:6379\n2025-10-30 10:51:36 server_time_usec:1761817896661509\n2025-10-30 10:51:36 uptime_in_seconds:94\n2025-10-30 10:51:36 uptime_in_days:0\n2025-10-30 10:51:36 hz:10\n2025-10-30 10:51:36 configured_hz:10\n2025-10-30 10:51:36 lru_clock:210216\n2025-10-30 10:51:36 executable:/data/redis-server\n2025-10-30 10:51:36 config_file:\n2025-10-30 10:51:36 io_threads_active:0\n2025-10-30 10:51:36 listener0:name=tcp,bind=*,bind=-::*,port=6379\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 # Clients\n2025-10-30 10:51:36 connected_clients:2\n2025-10-30 10:51:36 cluster_connections:0\n2025-10-30 10:51:36 maxclients:10000\n2025-10-30 10:51:36 client_recent_max_input_buffer:178\n2025-10-30 10:51:36 client_recent_max_output_buffer:0\n2025-10-30 10:51:36 blocked_clients:1\n2025-10-30 10:51:36 tracking_clients:0\n2025-10-30 10:51:36 pubsub_clients:0\n2025-10-30 10:51:36 watching_clients:0\n2025-10-30 10:51:36 clients_in_timeout_table:1\n2025-10-30 10:51:36 total_watched_keys:0\n2025-10-30 10:51:36 total_blocking_keys:1\n2025-10-30 10:51:36 total_blocking_keys_on_nokey:1\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 # Memory\n2025-10-30 10:51:36 used_memory:1184144\n2025-10-30 10:51:36 used_memory_human:1.13M\n2025-10-30 10:51:36 used_memory_rss:27471872\n2025-10-30 10:51:36 used_memory_rss_human:26.20M\n2025-10-30 10:51:36 used_memory_peak:1222512\n2025-10-30 10:51:36 used_memory_peak_human:1.17M\n2025-10-30 10:51:36 used_memory_peak_time:1761817896\n2025-10-30 10:51:36 used_memory_peak_perc:96.86%\n2025-10-30 10:51:36 used_memory_overhead:1041058\n2025-10-30 10:51:36 used_memory_startup:967336\n2025-10-30 10:51:36 used_memory_dataset:143086\n2025-10-30 10:51:36 used_memory_dataset_perc:66.00%\n2025-10-30 10:51:36 allocator_allocated:1884448\n2025-10-30 10:51:36 allocator_active:2240512\n2025-10-30 10:51:36 allocator_resident:7991296\n2025-10-30 10:51:36 allocator_muzzy:0\n2025-10-30 10:51:36 total_system_memory:8319983616\n2025-10-30 10:51:36 total_system_memory_human:7.75G\n2025-10-30 10:51:36 used_memory_lua:32768\n2025-10-30 10:51:36 used_memory_vm_eval:32768\n2025-10-30 10:51:36 used_memory_lua_human:32.00K\n2025-10-30 10:51:36 used_memory_scripts_eval:0\n2025-10-30 10:51:36 number_of_cached_scripts:0\n2025-10-30 10:51:36 number_of_functions:0\n2025-10-30 10:51:36 number_of_libraries:0\n2025-10-30 10:51:36 used_memory_vm_functions:33792\n2025-10-30 10:51:36 used_memory_vm_total:66560\n2025-10-30 10:51:36 used_memory_vm_total_human:65.00K\n2025-10-30 10:51:36 used_memory_functions:192\n2025-10-30 10:51:36 used_memory_scripts:192\n2025-10-30 10:51:36 used_memory_scripts_human:192B\n2025-10-30 10:51:36 maxmemory:0\n2025-10-30 10:51:36 maxmemory_human:0B\n2025-10-30 10:51:36 maxmemory_policy:noeviction\n2025-10-30 10:51:36 allocator_frag_ratio:1.20\n2025-10-30 10:51:36 allocator_frag_bytes:280032\n2025-10-30 10:51:36 allocator_rss_ratio:3.57\n2025-10-30 10:51:36 allocator_rss_bytes:5750784\n2025-10-30 10:51:36 rss_overhead_ratio:3.44\n2025-10-30 10:51:36 rss_overhead_bytes:19480576\n2025-10-30 10:51:36 mem_fragmentation_ratio:23.21\n2025-10-30 10:51:36 mem_fragmentation_bytes:26288048\n2025-10-30 10:51:36 mem_not_counted_for_evict:0\n2025-10-30 10:51:36 mem_replication_backlog:0\n2025-10-30 10:51:36 mem_total_replication_buffers:0\n2025-10-30 10:51:36 mem_replica_full_sync_buffer:0\n2025-10-30 10:51:36 mem_clients_slaves:0\n2025-10-30 10:51:36 mem_clients_normal:4010\n2025-10-30 10:51:36 mem_cluster_links:0\n2025-10-30 10:51:36 mem_aof_buffer:0\n2025-10-30 10:51:36 mem_allocator:jemalloc-5.3.0\n2025-10-30 10:51:36 mem_overhead_db_hashtable_rehashing:0\n2025-10-30 10:51:36 active_defrag_running:0\n2025-10-30 10:51:36 lazyfree_pending_objects:0\n2025-10-30 10:51:36 lazyfreed_objects:0\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 # Persistence\n2025-10-30 10:51:36 loading:0\n2025-10-30 10:51:36 async_loading:0\n2025-10-30 10:51:36 current_cow_peak:0\n2025-10-30 10:51:36 current_cow_size:0\n2025-10-30 10:51:36 current_cow_size_age:0\n2025-10-30 10:51:36 current_fork_perc:0.00\n2025-10-30 10:51:36 current_save_keys_processed:0\n2025-10-30 10:51:36 current_save_keys_total:0\n2025-10-30 10:51:36 rdb_changes_since_last_save:137\n2025-10-30 10:51:36 rdb_bgsave_in_progress:0\n2025-10-30 10:51:36 rdb_last_save_time:1761817802\n2025-10-30 10:51:36 rdb_last_bgsave_status:ok\n2025-10-30 10:51:36 rdb_last_bgsave_time_sec:-1\n2025-10-30 10:51:36 rdb_current_bgsave_time_sec:-1\n2025-10-30 10:51:36 rdb_saves:0\n2025-10-30 10:51:36 rdb_last_cow_size:0\n2025-10-30 10:51:36 rdb_last_load_keys_expired:0\n2025-10-30 10:51:36 rdb_last_load_keys_loaded:1\n2025-10-30 10:51:36 aof_enabled:0\n2025-10-30 10:51:36 aof_rewrite_in_progress:0\n2025-10-30 10:51:36 aof_rewrite_scheduled:0\n2025-10-30 10:51:36 aof_last_rewrite_time_sec:-1\n2025-10-30 10:51:36 aof_current_rewrite_time_sec:-1\n2025-10-30 10:51:36 aof_last_bgrewrite_status:ok\n2025-10-30 10:51:36 aof_rewrites:0\n2025-10-30 10:51:36 aof_rewrites_consecutive_failures:0\n2025-10-30 10:51:36 aof_last_write_status:ok\n2025-10-30 10:51:36 aof_last_cow_size:0\n2025-10-30 10:51:36 module_fork_in_progress:0\n2025-10-30 10:51:36 module_fork_last_cow_size:0\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 # Threads\n2025-10-30 10:51:36 io_thread_0:clients=2,reads=183,writes=135\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 # Stats\n2025-10-30 10:51:36 total_connections_received:48\n2025-10-30 10:51:36 total_commands_processed:145\n2025-10-30 10:51:36 instantaneous_ops_per_sec:4\n2025-10-30 10:51:36 total_net_input_bytes:18526\n2025-10-30 10:51:36 total_net_output_bytes:10954\n2025-10-30 10:51:36 total_net_repl_input_bytes:0\n2025-10-30 10:51:36 total_net_repl_output_bytes:0\n2025-10-30 10:51:36 instantaneous_input_kbps:0.70\n2025-10-30 10:51:36 instantaneous_output_kbps:0.42\n2025-10-30 10:51:36 instantaneous_input_repl_kbps:0.00\n2025-10-30 10:51:36 instantaneous_output_repl_kbps:0.00\n2025-10-30 10:51:36 rejected_connections:0\n2025-10-30 10:51:36 sync_full:0\n2025-10-30 10:51:36 sync_partial_ok:0\n2025-10-30 10:51:36 sync_partial_err:0\n2025-10-30 10:51:36 expired_subkeys:0\n2025-10-30 10:51:36 expired_keys:0\n2025-10-30 10:51:36 expired_stale_perc:0.00\n2025-10-30 10:51:36 expired_time_cap_reached_count:0\n2025-10-30 10:51:36 expire_cycle_cpu_milliseconds:1\n2025-10-30 10:51:36 evicted_keys:0\n2025-10-30 10:51:36 evicted_clients:0\n2025-10-30 10:51:36 evicted_scripts:0\n2025-10-30 10:51:36 total_eviction_exceeded_time:0\n2025-10-30 10:51:36 current_eviction_exceeded_time:0\n2025-10-30 10:51:36 keyspace_hits:234\n2025-10-30 10:51:36 keyspace_misses:0\n2025-10-30 10:51:36 pubsub_channels:0\n2025-10-30 10:51:36 pubsub_patterns:0\n2025-10-30 10:51:36 pubsubshard_channels:0\n2025-10-30 10:51:36 latest_fork_usec:0\n2025-10-30 10:51:36 total_forks:0\n2025-10-30 10:51:36 migrate_cached_sockets:0\n2025-10-30 10:51:36 slave_expires_tracked_keys:0\n2025-10-30 10:51:36 active_defrag_hits:0\n2025-10-30 10:51:36 active_defrag_misses:0\n2025-10-30 10:51:36 active_defrag_key_hits:0\n2025-10-30 10:51:36 active_defrag_key_misses:0\n2025-10-30 10:51:36 total_active_defrag_time:0\n2025-10-30 10:51:36 current_active_defrag_time:0\n2025-10-30 10:51:36 tracking_total_keys:0\n2025-10-30 10:51:36 tracking_total_items:0\n2025-10-30 10:51:36 tracking_total_prefixes:0\n2025-10-30 10:51:36 unexpected_error_replies:0\n2025-10-30 10:51:36 total_error_replies:1\n2025-10-30 10:51:36 dump_payload_sanitizations:0\n2025-10-30 10:51:36 total_reads_processed:183\n2025-10-30 10:51:36 total_writes_processed:135\n2025-10-30 10:51:36 io_threaded_reads_processed:0\n2025-10-30 10:51:36 io_threaded_writes_processed:0\n2025-10-30 10:51:36 io_threaded_total_prefetch_batches:0\n2025-10-30 10:51:36 io_threaded_total_prefetch_entries:0\n2025-10-30 10:51:36 client_query_buffer_limit_disconnections:0\n2025-10-30 10:51:36 client_output_buffer_limit_disconnections:0\n2025-10-30 10:51:36 reply_buffer_shrinks:2\n2025-10-30 10:51:36 reply_buffer_expands:0\n2025-10-30 10:51:36 eventloop_cycles:1150\n2025-10-30 10:51:36 eventloop_duration_sum:856144\n2025-10-30 10:51:36 eventloop_duration_cmd_sum:3367\n2025-10-30 10:51:36 instantaneous_eventloop_cycles_per_sec:18\n2025-10-30 10:51:36 instantaneous_eventloop_duration_usec:665\n2025-10-30 10:51:36 acl_access_denied_auth:0\n2025-10-30 10:51:36 acl_access_denied_cmd:0\n2025-10-30 10:51:36 acl_access_denied_key:0\n2025-10-30 10:51:36 acl_access_denied_channel:0\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 # Replication\n2025-10-30 10:51:36 role:master\n2025-10-30 10:51:36 connected_slaves:0\n2025-10-30 10:51:36 master_failover_state:no-failover\n2025-10-30 10:51:36 master_replid:e5cb36ad6dccbe7bcedc21ff03597fab29e40ba8\n2025-10-30 10:51:36 master_replid2:0000000000000000000000000000000000000000\n2025-10-30 10:51:36 master_repl_offset:0\n2025-10-30 10:51:36 second_repl_offset:-1\n2025-10-30 10:51:36 repl_backlog_active:0\n2025-10-30 10:51:36 repl_backlog_size:1048576\n2025-10-30 10:51:36 repl_backlog_first_byte_offset:0\n2025-10-30 10:51:36 repl_backlog_histlen:0\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 # CPU\n2025-10-30 10:51:36 used_cpu_sys:0.077896\n2025-10-30 10:51:36 used_cpu_user:0.866207\n2025-10-30 10:51:36 used_cpu_sys_children:0.003749\n2025-10-30 10:51:36 used_cpu_user_children:0.005951\n2025-10-30 10:51:36 used_cpu_sys_main_thread:0.077769\n2025-10-30 10:51:36 used_cpu_user_main_thread:0.864798\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 # Modules\n2025-10-30 10:51:36 module:name=vectorset,ver=1,api=1,filters=0,usedby=[],using=[],options=[handle-io-errors|handle-repl-async-load]\n2025-10-30 10:51:36 module:name=timeseries,ver=80200,api=1,filters=0,usedby=[],using=[],options=[handle-io-errors]\n2025-10-30 10:51:36 module:name=search,ver=80205,api=1,filters=0,usedby=[],using=[ReJSON],options=[handle-io-errors]\n2025-10-30 10:51:36 module:name=bf,ver=80203,api=1,filters=0,usedby=[],using=[],options=[]\n2025-10-30 10:51:36 module:name=ReJSON,ver=80201,api=1,filters=0,usedby=[search],using=[],options=[handle-io-errors]\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 # Commandstats\n2025-10-30 10:51:36 cmdstat_xinfo|consumers:calls=18,usec=348,usec_per_call=19.33,rejected_calls=0,failed_calls=0\n2025-10-30 10:51:36 cmdstat_info:calls=6,usec=57,usec_per_call=9.50,rejected_calls=0,failed_calls=0\n2025-10-30 10:51:36 cmdstat_config|get:calls=2,usec=6,usec_per_call=3.00,rejected_calls=0,failed_calls=0\n2025-10-30 10:51:36 cmdstat_hello:calls=2,usec=21,usec_per_call=10.50,rejected_calls=0,failed_calls=0\n2025-10-30 10:51:36 cmdstat_xgroup|create:calls=1,usec=6,usec_per_call=6.00,rejected_calls=0,failed_calls=1\n2025-10-30 10:51:36 cmdstat_xreadgroup:calls=55,usec=2809,usec_per_call=51.07,rejected_calls=0,failed_calls=0\n2025-10-30 10:51:36 cmdstat_xadd:calls=46,usec=1141,usec_per_call=24.80,rejected_calls=0,failed_calls=0\n2025-10-30 10:51:36 cmdstat_xackdel:calls=11,usec=850,usec_per_call=77.27,rejected_calls=0,failed_calls=0\n2025-10-30 10:51:36 cmdstat_client|setinfo:calls=4,usec=3,usec_per_call=0.75,rejected_calls=0,failed_calls=0\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 # Errorstats\n2025-10-30 10:51:36 errorstat_BUSYGROUP:count=1\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 # Latencystats\n2025-10-30 10:51:36 latency_percentiles_usec_xinfo|consumers:p50=16.063,p99=92.159,p99.9=92.159\n2025-10-30 10:51:36 latency_percentiles_usec_info:p50=7.007,p99=20.095,p99.9=20.095\n2025-10-30 10:51:36 latency_percentiles_usec_config|get:p50=3.007,p99=3.007,p99.9=3.007\n2025-10-30 10:51:36 latency_percentiles_usec_hello:p50=10.047,p99=11.007,p99.9=11.007\n2025-10-30 10:51:36 latency_percentiles_usec_xgroup|create:p50=6.015,p99=6.015,p99.9=6.015\n2025-10-30 10:51:36 latency_percentiles_usec_xreadgroup:p50=45.055,p99=141.311,p99.9=171.007\n2025-10-30 10:51:36 latency_percentiles_usec_xadd:p50=20.095,p99=117.247,p99.9=117.247\n2025-10-30 10:51:36 latency_percentiles_usec_xackdel:p50=49.151,p99=245.759,p99.9=245.759\n2025-10-30 10:51:36 latency_percentiles_usec_client|setinfo:p50=0.001,p99=2.007,p99.9=2.007\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 # Cluster\n2025-10-30 10:51:36 cluster_enabled:0\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 # Keyspace\n2025-10-30 10:51:36 db0:keys=1,expires=0,avg_ttl=0,subexpiry=0\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 # Keysizes\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 ------ CLIENT LIST OUTPUT ------\n2025-10-30 10:51:36 id=10 addr=192.168.65.1:64858 laddr=172.18.0.15:6379 fd=22 name= age=90 idle=0 flags=b db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=0 argv-mem=82 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=2090 events=r cmd=xreadgroup user=default redir=-1 resp=3 lib-name=go-redis(,go1.25.1) lib-ver=9.14.1 io-thread=0 tot-net-in=9032 tot-net-out=7413 tot-cmds=59\n2025-10-30 10:51:36 id=11 addr=192.168.65.1:41181 laddr=172.18.0.15:6379 fd=23 name= age=85 idle=0 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=314 qbuf-free=20160 argv-mem=201 multi-mem=0 rbs=1024 rbp=90 obl=46 oll=0 omem=0 tot-mem=22729 events=r cmd=xackdel user=default redir=-1 resp=3 lib-name=go-redis(,go1.25.1) lib-ver=9.14.1 io-thread=0 tot-net-in=3634 tot-net-out=2529 tot-cmds=32\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 ------ CURRENT CLIENT INFO ------\n2025-10-30 10:51:36 id=11 addr=192.168.65.1:41181 laddr=172.18.0.15:6379 fd=23 name= age=85 idle=0 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=314 qbuf-free=20160 argv-mem=201 multi-mem=0 rbs=1024 rbp=90 obl=46 oll=0 omem=0 tot-mem=22729 events=r cmd=xackdel user=default redir=-1 resp=3 lib-name=go-redis(,go1.25.1) lib-ver=9.14.1 io-thread=0 tot-net-in=3634 tot-net-out=2529 tot-cmds=32\n2025-10-30 10:51:36 argc: '16'\n2025-10-30 10:51:36 argv[0]: '\"xackdel\"'\n2025-10-30 10:51:36 argv[1]: '\"webhook_events\"'\n2025-10-30 10:51:36 argv[2]: '\"webhook_processors\"'\n2025-10-30 10:51:36 argv[3]: '\"KEEPREF\"'\n2025-10-30 10:51:36 argv[4]: '\"ids\"'\n2025-10-30 10:51:36 argv[5]: '\"10\"'\n2025-10-30 10:51:36 argv[6]: '\"1761817891726-0\"'\n2025-10-30 10:51:36 argv[7]: '\"1761817892448-0\"'\n2025-10-30 10:51:36 argv[8]: '\"1761817893085-0\"'\n2025-10-30 10:51:36 argv[9]: '\"1761817893734-0\"'\n2025-10-30 10:51:36 argv[10]: '\"1761817894254-0\"'\n2025-10-30 10:51:36 argv[11]: '\"1761817894877-0\"'\n2025-10-30 10:51:36 argv[12]: '\"1761817895388-0\"'\n2025-10-30 10:51:36 argv[13]: '\"1761817895761-0\"'\n2025-10-30 10:51:36 argv[14]: '\"1761817896171-0\"'\n2025-10-30 10:51:36 argv[15]: '\"1761817896529-0\"'\n2025-10-30 10:51:36 1:M 30 Oct 2025 09:51:36.663 # key 'webhook_events' found in DB containing the following object:\n2025-10-30 10:51:36 1:M 30 Oct 2025 09:51:36.663 # Object type: 6\n2025-10-30 10:51:36 1:M 30 Oct 2025 09:51:36.663 # Object encoding: 10\n2025-10-30 10:51:36 1:M 30 Oct 2025 09:51:36.663 # Object refcount: 1\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 ------ EXECUTING CLIENT INFO ------\n2025-10-30 10:51:36 id=11 addr=192.168.65.1:41181 laddr=172.18.0.15:6379 fd=23 name= age=85 idle=0 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=314 qbuf-free=20160 argv-mem=201 multi-mem=0 rbs=1024 rbp=90 obl=46 oll=0 omem=0 tot-mem=22729 events=r cmd=xackdel user=default redir=-1 resp=3 lib-name=go-redis(,go1.25.1) lib-ver=9.14.1 io-thread=0 tot-net-in=3634 tot-net-out=2529 tot-cmds=32\n2025-10-30 10:51:36 argc: '16'\n2025-10-30 10:51:36 argv[0]: '\"xackdel\"'\n2025-10-30 10:51:36 argv[1]: '\"webhook_events\"'\n2025-10-30 10:51:36 argv[2]: '\"webhook_processors\"'\n2025-10-30 10:51:36 argv[3]: '\"KEEPREF\"'\n2025-10-30 10:51:36 argv[4]: '\"ids\"'\n2025-10-30 10:51:36 argv[5]: '\"10\"'\n2025-10-30 10:51:36 argv[6]: '\"1761817891726-0\"'\n2025-10-30 10:51:36 argv[7]: '\"1761817892448-0\"'\n2025-10-30 10:51:36 argv[8]: '\"1761817893085-0\"'\n2025-10-30 10:51:36 argv[9]: '\"1761817893734-0\"'\n2025-10-30 10:51:36 argv[10]: '\"1761817894254-0\"'\n2025-10-30 10:51:36 argv[11]: '\"1761817894877-0\"'\n2025-10-30 10:51:36 argv[12]: '\"1761817895388-0\"'\n2025-10-30 10:51:36 argv[13]: '\"1761817895761-0\"'\n2025-10-30 10:51:36 argv[14]: '\"1761817896171-0\"'\n2025-10-30 10:51:36 argv[15]: '\"1761817896529-0\"'\n2025-10-30 10:51:36 1:M 30 Oct 2025 09:51:36.663 # key 'webhook_events' found in DB containing the following object:\n2025-10-30 10:51:36 1:M 30 Oct 2025 09:51:36.663 # Object type: 6\n2025-10-30 10:51:36 1:M 30 Oct 2025 09:51:36.663 # Object encoding: 10\n2025-10-30 10:51:36 1:M 30 Oct 2025 09:51:36.663 # Object refcount: 1\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 ------ MODULES INFO OUTPUT ------\n2025-10-30 10:51:36 # search_version\n2025-10-30 10:51:36 search_version:8.2.5\n2025-10-30 10:51:36 search_redis_version:8.2.2 - oss\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 # search_indexes\n2025-10-30 10:51:36 search_number_of_indexes:0\n2025-10-30 10:51:36 search_number_of_active_indexes:0\n2025-10-30 10:51:36 search_number_of_active_indexes_running_queries:0\n2025-10-30 10:51:36 search_number_of_active_indexes_indexing:0\n2025-10-30 10:51:36 search_total_active_write_threads:0\n2025-10-30 10:51:36 search_total_indexing_time:0\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 # search_fields_statistics\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 # search_memory\n2025-10-30 10:51:36 search_used_memory_indexes:0\n2025-10-30 10:51:36 search_used_memory_indexes_human:0\n2025-10-30 10:51:36 search_smallest_memory_index:0\n2025-10-30 10:51:36 search_smallest_memory_index_human:0\n2025-10-30 10:51:36 search_largest_memory_index:0\n2025-10-30 10:51:36 search_largest_memory_index_human:0\n2025-10-30 10:51:36 search_used_memory_vector_index:0\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 # search_cursors\n2025-10-30 10:51:36 search_global_idle_user:0\n2025-10-30 10:51:36 search_global_idle_internal:0\n2025-10-30 10:51:36 search_global_total_user:0\n2025-10-30 10:51:36 search_global_total_internal:0\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 # search_garbage_collector\n2025-10-30 10:51:36 search_gc_bytes_collected:0\n2025-10-30 10:51:36 search_gc_total_cycles:0\n2025-10-30 10:51:36 search_gc_total_ms_run:0\n2025-10-30 10:51:36 search_gc_total_docs_not_collected:0\n2025-10-30 10:51:36 search_gc_marked_deleted_vectors:0\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 # search_queries\n2025-10-30 10:51:36 search_total_queries_processed:0\n2025-10-30 10:51:36 search_total_query_commands:0\n2025-10-30 10:51:36 search_total_query_execution_time_ms:0\n2025-10-30 10:51:36 search_total_active_queries:0\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 # search_warnings_and_errors\n2025-10-30 10:51:36 search_errors_indexing_failures:0\n2025-10-30 10:51:36 search_errors_for_index_with_max_failures:0\n2025-10-30 10:51:36 search_OOM_indexing_failures_indexes_count:0\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 # search_dialect_statistics\n2025-10-30 10:51:36 search_dialect_1:0\n2025-10-30 10:51:36 search_dialect_2:0\n2025-10-30 10:51:36 search_dialect_3:0\n2025-10-30 10:51:36 search_dialect_4:0\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 # search_runtime_configurations\n2025-10-30 10:51:36 search_extension_load:\n2025-10-30 10:51:36 search_friso_ini:\n2025-10-30 10:51:36 search_enableGC:ON\n2025-10-30 10:51:36 search_minimal_term_prefix:2\n2025-10-30 10:51:36 search_minimal_stem_length:4\n2025-10-30 10:51:36 search_maximal_prefix_expansions:200\n2025-10-30 10:51:36 search_query_timeout_ms:500\n2025-10-30 10:51:36 search_timeout_policy:return\n2025-10-30 10:51:36 search_cursor_read_size:1000\n2025-10-30 10:51:36 search_cursor_max_idle_time:300000\n2025-10-30 10:51:36 search_max_doc_table_size:1000000\n2025-10-30 10:51:36 search_max_search_results:1000000\n2025-10-30 10:51:36 search_max_aggregate_results:2147483648\n2025-10-30 10:51:36 search_gc_scan_size:100\n2025-10-30 10:51:36 search_min_phonetic_term_length:3\n2025-10-30 10:51:36 search_bm25std_tanh_factor:4\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 # search_current_thread\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 # search_blocked_queries\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 # search_blocked_cursors\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 # ReJSON_trace\n2025-10-30 10:51:36 ReJSON_backtrace:   0: redis_module::basic_info_command_handler\n2025-10-30 10:51:36    1: rejson::__info_func\n2025-10-30 10:51:36    2: modulesCollectInfo\n2025-10-30 10:51:36              at /usr/src/redis/src/module.c:10631:9\n2025-10-30 10:51:36    3: logModulesInfo\n2025-10-30 10:51:36              at /usr/src/redis/src/debug.c:2154:22\n2025-10-30 10:51:36       printCrashReport\n2025-10-30 10:51:36              at /usr/src/redis/src/debug.c:2516:5\n2025-10-30 10:51:36    4: sigsegvHandler\n2025-10-30 10:51:36              at /usr/src/redis/src/debug.c:2441:32\n2025-10-30 10:51:36    5: sigwaitinfo\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 \n2025-10-30 10:51:36 ------ CONFIG DEBUG OUTPUT ------\n2025-10-30 10:51:36 lazyfree-lazy-user-flush no\n2025-10-30 10:51:36 lazyfree-lazy-eviction no\n2025-10-30 10:51:36 sanitize-dump-payload no\n2025-10-30 10:51:36 slave-read-only yes\n2025-10-30 10:51:36 lazyfree-lazy-expire no\n2025-10-30 10:51:36 io-threads 1\n2025-10-30 10:51:36 replica-read-only yes\n2025-10-30 10:51:36 repl-diskless-sync yes\n2025-10-30 10:51:36 activedefrag no\n2025-10-30 10:51:36 lazyfree-lazy-user-del no\n2025-10-30 10:51:36 lazyfree-lazy-server-del no\n2025-10-30 10:51:36 repl-diskless-load disabled\n2025-10-30 10:51:36 proto-max-bulk-len 512mb\n2025-10-30 10:51:36 client-query-buffer-limit 1gb\n2025-10-30 10:51:36 list-compress-depth 0\n2025-10-30 10:51:36 \n2025-10-30 10:51:36 ------ FAST MEMORY TEST ------\n2025-10-30 10:51:36 1:M 30 Oct 2025 09:51:36.688 # Bio worker thread #0 terminated\n2025-10-30 10:51:36 1:M 30 Oct 2025 09:51:36.689 # Bio worker thread #1 terminated\n2025-10-30 10:51:36 1:M 30 Oct 2025 09:51:36.689 # Bio worker thread #2 terminated\n2025-10-30 10:51:36 *** Preparing to test memory region 55c6e8f37000 (2322432 bytes)\n2025-10-30 10:51:36 *** Preparing to test memory region 55c71de4b000 (16384 bytes)\n2025-10-30 10:51:36 *** Preparing to test memory region 7fdf8a3b2000 (6815744 bytes)\n2025-10-30 10:51:36 *** Preparing to test memory region 7fdf8bcab000 (2621440 bytes)\n2025-10-30 10:51:36 *** Preparing to test memory region 7fdf8c410000 (57344 bytes)\n2025-10-30 10:51:36 *** Preparing to test memory region 7fdf8c420000 (139264 bytes)\n2025-10-30 10:51:36 *** Preparing to test memory region 7fdf8c444000 (139264 bytes)\n2025-10-30 10:51:36 *** Preparing to test memory region 7fdf8d461000 (4096 bytes)\n2025-10-30 10:51:36 *** Preparing to test memory region 7fdf8d4ff000 (8192 bytes)\n2025-10-30 10:51:36 *** Preparing to test memory region 7fdf8e1fd000 (12288 bytes)\n2025-10-30 10:51:36 *** Preparing to test memory region 7fdf8e200000 (12582912 bytes)\n2025-10-30 10:51:36 *** Preparing to test memory region 7fdf8ee0a000 (65536 bytes)\n2025-10-30 10:51:36 *** Preparing to test memory region 7fdf8ee23000 (4096 bytes)\n2025-10-30 10:51:36 *** Preparing to test memory region 7fdf8ee4b000 (483328 bytes)\n2025-10-30 10:51:36 *** Preparing to test memory region 7fdf8f3af000 (12288 bytes)\n2025-10-30 10:51:36 *** Preparing to test memory region 7fdf8f725000 (16384 bytes)\n2025-10-30 10:51:36 *** Preparing to test memory region 7fdf8f7d2000 (12288 bytes)\n2025-10-30 10:51:36 .O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O\n2025-10-30 10:51:36 Fast memory test PASSED, however your memory can still be broken. Please run a memory test for several hours if possible.\n```\n\n**Additional information**\n\n1. Win10 / Docker Desktop 4.38.0 (181591)\n2. Start Redis server\n3. launch a Go program that read a stream and basically log the message ID received\n4. accumulate message ID in a batch for some milliseconds and ACKDEL them in batch\n5. Redis crash\n",
      "solution": "@dynamicnet this issue is fixed in version [8.2.3](https://github.com/redis/redis/releases/tag/8.2.3)",
      "labels": [],
      "created_at": "2025-10-30T10:06:09Z",
      "closed_at": "2025-11-05T06:57:48Z",
      "url": "https://github.com/redis/redis/issues/14491",
      "comments_count": 5
    },
    {
      "tech": "redis",
      "repo": "redis/redis",
      "issue_number": 13402,
      "title": "[QUESTION] Proper ACL policy for create a read-only user that can perform `SCAN`s",
      "problem": "I am not totally sure if this a `[QUESTION]` or a `[BUG]` , and if the latter, if the bug is on **Redis** directly or on **AWS**.\r\n\r\n-----\r\n\r\n### Environment\r\n\r\n- **Redis** version: `7.1`.\r\n- Connecting to an **AWS** **Elasticache** _serverless_ **Redis** cluster.\r\n- Using a _read-only_ user: `on ~* -@all +@connection +@read +@keyspace +@slow`\r\n\r\n-----\r\n\r\nWhen running a `SCAN` against the database using the _read-only_ user, we receive the following error:\r\n\r\n> ERR internal error\r\n\r\nNote: other read-only commands like `ZRANGE` work as expected. Also, using a user whose ACL is `on ~* +@all`, then the `SCAN` also works as expected.\r\n\r\nMy gut feeling is that considering that the same command works when using a user with all permissions, it feels only natural to assume that the issue is related to permissions. Yet, when checking the [official **Redis** docs for `SCAN`](https://redis.io/docs/latest/commands/scan/) it only mentions the following ACL categories: `@keyspace, @read, @slow`, all of which are assigned to the _read-only_ user.\r\nThus, I wonder if the problem is that the documentation is outdated and we need an extra or different set of permissions? Or if **AWS** is doing something weird on their side?",
      "solution": "Hiya @BalmungSan - did AWS get back to you with a work around? it doesn't look like it's fixed still :(",
      "labels": [],
      "created_at": "2024-07-08T17:05:51Z",
      "closed_at": "2024-07-29T18:01:08Z",
      "url": "https://github.com/redis/redis/issues/13402",
      "comments_count": 6
    },
    {
      "tech": "redis",
      "repo": "redis/redis",
      "issue_number": 14360,
      "title": "Probable division by zero in /src/redis-cli.c:9847 ( v. 8.0.3)",
      "problem": "Variable ((float( 64))hits + misses) with a floating-point type whose possible value set allows a zero value at redis-cli.c:9847 is used as a denominator at redis-cli.c:9847. The result of division is further used without a check for NaN, which leads to undefined program behavior.\n\nSince LRUTestMode is a special testing mode for evaluating the performance and efficiency of the LRU algorithm (Least Recently Used), which displaces data from the cache and deletes those items that have not been used for the longest time, hits and misses can be equal to 0 simultaneously if the test has not performed any read operations, and all get operations have completed with errors\n\nLine in /src/redis-cli.c:9847 https://github.com/redis/redis/blob/8.0.3/src/redis-cli.c#L9847\n```c\n hits, (double)hits/(hits+misses)*100, \n``` \nIt is recommended to add null check.\n\nFound by Linux Verification Center ( linuxtesting.org ) with SVACE\nReporter: Gushchin Egor",
      "solution": "Is this also fixed due to PR [14369](https://github.com/redis/redis/pull/14369) ?",
      "labels": [],
      "created_at": "2025-09-16T18:31:28Z",
      "closed_at": "2025-09-19T06:53:35Z",
      "url": "https://github.com/redis/redis/issues/14360",
      "comments_count": 4
    },
    {
      "tech": "redis",
      "repo": "redis/redis",
      "issue_number": 14026,
      "title": "[BUG] Vector Sets and Search commands are missing docs in redis-cli",
      "problem": "**Describe the bug**\n\nIn Redis 8.0.0, the `help` command in redis-cli gives useful information for most commands -- enough that I don't have to look at the docs to remember an argument. However, both vector sets and search-related commands (FT.CREATE, etc.) show \"null\":\n\n```\n127.0.0.1:6379> info\n# Server\nredis_version:8.0.0\nredis_git_sha1:00000000\nredis_git_dirty:1\nredis_build_id:205b0ec41d42d8ec\nredis_mode:standalone\nos:Linux 6.10.14-linuxkit aarch64\narch_bits:64\n# ... (output elided)\n\n127.0.0.1:6379> help vsim\n\n  VSIM (null)\n  summary: (null)\n  group: module\n\n127.0.0.1:6379> help ft.create\n\n  FT.CREATE (null)\n  summary: (null)\n  group: module\n\n```\n\n**To reproduce**\n\n- Open `redis-cli`\n- Run `help ft.create`, etc.\n\n**Expected behavior**\n\nUseful output, such as this:\n```\n127.0.0.1:6379> help set\n\n  SET key value [NX|XX] [GET] [EX seconds|PX milliseconds|EXAT unix-time-seconds|PXAT unix-time-milliseconds|KEEPTTL]\n  summary: Sets the string value of a key, ignoring its type. The key is created if it doesn't exist.\n  since: 1.0.0\n  group: string\n```\n\n**Additional information**\n\nNA",
      "solution": "Yes, you need to specify it explicitly. The default (for now) is without modules.",
      "labels": [],
      "created_at": "2025-05-08T21:14:02Z",
      "closed_at": "2025-09-11T03:49:43Z",
      "url": "https://github.com/redis/redis/issues/14026",
      "comments_count": 5
    },
    {
      "tech": "redis",
      "repo": "redis/redis",
      "issue_number": 13612,
      "title": "[BUG] High RSS Memory Usage Even with Active Defrag",
      "problem": "**Describe the bug**\r\n\r\nHello! We have been observing a strange issue since upgrading our redis standalone servers from 6.2.5 to 7.2.5. Occasionally, one of our redis servers will start growing in RSS memory until the system is running out of memory. We have activedefrag enabled and it starts running during this time but it doesn't seem to help. Can someone help take a look and see what is going on? I captured some data when this occurred last. If we need more data I will have to wait until it happens again to get back with it.\r\n\r\n\r\n`MEMORY STATS`\r\n```\r\n 1) \"peak.allocated\"\r\n 2) (integer) 9155310640\r\n 3) \"total.allocated\"\r\n 4) (integer) 9088238672\r\n 5) \"startup.allocated\"\r\n 6) (integer) 859496\r\n 7) \"replication.backlog\"\r\n 8) (integer) 0\r\n 9) \"clients.slaves\"\r\n10) (integer) 0\r\n11) \"clients.normal\"\r\n12) (integer) 18185184\r\n13) \"cluster.links\"\r\n14) (integer) 0\r\n15) \"aof.buffer\"\r\n16) (integer) 0\r\n17) \"lua.caches\"\r\n18) (integer) 1416\r\n19) \"functions.caches\"\r\n20) (integer) 184\r\n21) \"db.0\"\r\n22) 1) \"overhead.hashtable.main\"\r\n    2) (integer) 547207768\r\n    3) \"overhead.hashtable.expires\"\r\n    4) (integer) 382011752\r\n    5) \"overhead.hashtable.slot-to-keys\"\r\n    6) (integer) 0\r\n23) \"overhead.total\"\r\n24) (integer) 948265800\r\n25) \"keys.count\"\r\n26) (integer) 10324751\r\n27) \"keys.bytes-per-key\"\r\n28) (integer) 880\r\n29) \"dataset.bytes\"\r\n30) (integer) 8139972872\r\n31) \"dataset.percentage\"\r\n32) \"89.57447814941406\"\r\n33) \"peak.percentage\"\r\n34) \"99.26740264892578\"\r\n35) \"allocator.allocated\"\r\n36) (integer) 9088373168\r\n37) \"allocator.active\"\r\n38) (integer) 12528521216\r\n39) \"allocator.resident\"\r\n40) (integer) 12667953152\r\n41) \"allocator-fragmentation.ratio\"\r\n42) \"1.3785219192504883\"\r\n43) \"allocator-fragmentation.bytes\"\r\n44) (integer) 3440148048\r\n45) \"allocator-rss.ratio\"\r\n46) \"1.0111291408538818\"\r\n47) \"allocator-rss.bytes\"\r\n48) (integer) 139431936\r\n49) \"rss-overhead.ratio\"\r\n50) \"0.9998315572738647\"\r\n51) \"rss-overhead.bytes\"\r\n52) (integer) -2134016\r\n53) \"fragmentation\"\r\n54) \"1.393649697303772\"\r\n55) \"fragmentation.bytes\"\r\n56) (integer) 3577581536\r\n```\r\n\r\n`MALLOC-STATS`\r\n```\r\n___ Begin jemalloc statistics ___\r\nVersion: \"5.3.0-0-g0\"\r\nBuild-time option settings\r\n  config.cache_oblivious: false\r\n  config.debug: false\r\n  config.fill: true\r\n  config.lazy_lock: false\r\n  config.malloc_conf: \"\"\r\n  config.opt_safety_checks: false\r\n  config.prof: false\r\n  config.prof_libgcc: false\r\n  config.prof_libunwind: false\r\n  config.stats: true\r\n  config.utrace: false\r\n  config.xmalloc: false\r\nRun-time option settings\r\n  opt.abort: false\r\n  opt.abort_conf: false\r\n  opt.cache_oblivious: false\r\n  opt.confirm_conf: false\r\n  opt.retain: true\r\n  opt.dss: \"secondary\"\r\n  opt.narenas: 8\r\n  opt.percpu_arena: \"disabled\"\r\n  opt.oversize_threshold: 8388608\r\n  opt.hpa: false\r\n  opt.hpa_slab_max_alloc: 65536\r\n  opt.hpa_hugification_threshold: 1992294\r\n  opt.hpa_hugify_delay_ms: 10000\r\n  opt.hpa_min_purge_interval_ms: 5000\r\n  opt.hpa_dirty_mult: \"0.25\"\r\n  opt.hpa_sec_nshards: 4\r\n  opt.hpa_sec_max_alloc: 32768\r\n  opt.hpa_sec_max_bytes: 262144\r\n  opt.hpa_sec_bytes_after_flush: 131072\r\n  opt.hpa_sec_batch_fill_extra: 0\r\n  opt.metadata_thp: \"disabled\"\r\n  opt.mutex_max_spin: 600\r\n  opt.background_thread: false (background_thread: true)\r\n  opt.dirty_decay_ms: 10000 (arenas.dirty_decay_ms: 10000)\r\n  opt.muzzy_decay_ms: 0 (arenas.muzzy_decay_ms: 0)\r\n  opt.lg_extent_max_active_fit: 6\r\n  opt.junk: \"false\"\r\n  opt.zero: false\r\n  opt.tcache: true\r\n  opt.tcache_max: 32768\r\n  opt.tcache_nslots_small_min: 20\r\n  opt.tcache_nslots_small_max: 200\r\n  opt.tcache_nslots_large: 20\r\n  opt.lg_tcache_nslots_mul: 1\r\n  opt.tcache_gc_incr_bytes: 65536\r\n  opt.tcache_gc_delay_bytes: 0\r\n  opt.lg_tcache_flush_small_div: 1\r\n  opt.lg_tcache_flush_large_div: 1\r\n  opt.thp: \"default\"\r\n  opt.stats_print: false\r\n  opt.stats_print_opts: \"\"\r\n  opt.stats_print: false\r\n  opt.stats_print_opts: \"\"\r\n  opt.stats_interval: -1\r\n  opt.stats_interval_opts: \"\"\r\n  opt.zero_realloc: \"free\"\r\nArenas: 9\r\nQuantum size: 8\r\nPage size: 4096\r\nMaximum thread-cached size class: 32768\r\nNumber of bin size classes: 39\r\nNumber of thread-cache bin size classes: 44\r\nNumber of large size classes: 196\r\nAllocated: 9088438136, active: 12537638912, metadata: 139436448 (n_thp 0), resident: 12677103616, mapped: 12690853888, retained: 1994452992\r\nCount of realloc(non-null-ptr, 0) calls: 0\r\nBackground threads: 1, num_runs: 74773, run_interval: 9706952082 ns\r\n                           n_lock_ops (#/sec)       n_waiting (#/sec)      n_spin_acq (#/sec)  n_owner_switch (#/sec)   total_wait_ns   (#/sec)     max_wait_ns  max_n_thds\r\nbackground_thread            15920251      21               0       0               0       0               1       0               0         0               0           0\r\nmax_per_bg_thd                8447672      11               0       0               0       0               3       0               0         0               0           0\r\nctl                          31840495      43               0       0               0       0               1       0               0         0               0           0\r\nprof                                0       0               0       0               0       0               0       0               0         0               0           0\r\nprof_thds_data                      0       0               0       0               0       0               0       0               0         0               0           0\r\nprof_dump                           0       0               0       0               0       0               0       0               0         0               0           0\r\nprof_recent_alloc                   0       0               0       0               0       0               0       0               0         0               0           0\r\nprof_recent_dump                    0       0               0       0               0       0               0       0               0         0               0           0\r\nprof_stats                          0       0               0       0               0       0               0       0               0         0               0           0\r\nMerged arenas stats:\r\nassigned threads: 1\r\nuptime: 725830571650076\r\ndss allocation precedence: \"N/A\"\r\ndecaying:  time       npages       sweeps     madvises       purged\r\n   dirty:   N/A           30        59808       593498      1893267\r\n   muzzy:   N/A            0            0            0            0\r\n                            allocated         nmalloc   (#/sec)         ndalloc   (#/sec)       nrequests   (#/sec)           nfill   (#/sec)          nflush   (#/sec)\r\nsmall:                     8760041336       554242742       763       446073706       614     67182704643     92559        85734598       118        29525034        40\r\nlarge:                      328396800         1215150         1         1214086         1        11320091        15         1215150         1          859012         1\r\ntotal:                     9088438136       555457892       765       447287792       616     67194024734     92575        86949748       119        30384046        41\r\n\r\nactive:                   12537638912\r\nmapped:                   12690853888\r\nretained:                  1994452992\r\nbase:                       139338144\r\ninternal:                       98304\r\nmetadata_thp:                       0\r\ntcache_bytes:                  103760\r\ntcache_stashed_bytes:               0\r\nresident:                 12677103616\r\nabandoned_vm:                       0\r\nextent_avail:                       1\r\n                           n_lock_ops (#/sec)       n_waiting (#/sec)      n_spin_acq (#/sec)  n_owner_switch (#/sec)   total_wait_ns   (#/sec)     max_wait_ns  max_n_thds\r\nlarge                        15829375      21               0       0               0       0               2       0               0         0               0           0\r\nextent_avail                 17998331      24               0       0               2       0          104942       0               0         0               0           0\r\nextents_dirty                22838844      31               0       0              58       0          119674       0               0         0               0           0\r\nextents_muzzy                15864762      21               0       0               0       0               2       0               0         0               0           0\r\nextents_retained             17626774      24               0       0               6       0          119698       0               0         0               0           0\r\ndecay_dirty                  17250502      23               0       0               1       0          297186       0               0         0               0           0\r\ndecay_muzzy                  15978550      22               0       0               0       0          296934       0               0         0               0           0\r\nbase                         32529371      44               0       0               0       0               2       0               0         0               0           0\r\ntcache_list                  15829376      21               0       0               0       0               2       0               0         0               0           0\r\nhpa_shard                           0       0               0       0               0       0               0       0               0         0               0           0\r\nhpa_shard_grow                      0       0               0       0               0       0               0       0               0         0               0           0\r\nhpa_sec                             0       0               0       0               0       0               0       0               0         0               0           0\r\nbins:           size ind    allocated      nmalloc (#/sec)      ndalloc (#/sec)    nrequests   (#/sec)  nshards      curregs     curslabs  nonfull_slabs regs pgs   util       nfills (#/sec)     nflushes (#/sec)       nslabs     nreslabs (#/sec)      n_lock_ops (#/sec)       n_waiting (#/sec)      n_spin_acq (#/sec)  n_owner_switch (#/sec)   total_wait_ns   (#/sec)     max_wait_ns  max_n_thds\r\n                   8   0         5992      1460121       2      1459372       2    165227552       227        1          749            2              0  512   1  0.731       908665       1       751437       1           19        15835       0        17489496      24               0       0               0       0               2       0               0         0               0           0\r\n                  16   1    165063952     53338498      73     43022001      59  19912092842     27433        1     10316497        42768          21276  256   1  0.942     10855872      14      1551039       2        78512     23817166      32        73137144     100               0       0               0       0         6578224       9               0         0               0           0\r\n                  24   2    865151040    164045682     226    127997722     176   8437774623     11625        1     36047960        73048          55411  512   3  0.963     14373712      19      2039254       2        95160     65526118      90       198481414     273               0       0               0       0        24699694      34               0         0               0           0\r\n                  32   3    328121216     47508388      65     37254600      51  11295660858     15562        1     10253788        85030          40353  128   1  0.942      6842004       9      1592562       2       158625     18131374      24        69605730      95               0       0               0       0         7114564       9               0         0               0           0\r\n                  40   4         5440       534324       0       534188       0   2931833591      4039        1          136            1              0  512   5  0.265       521512       0       251277       0            1            0       0        16602165      22               0       0               0       0               2       0               0         0               0           0\r\n                  48   5       109056      1985674       2      1983402       2    354711849       488        1         2272           21             15  256   3  0.422      1258241       1      1048496       1          353       817728       1        18185250      25               0       0               0       0           57184       0               0         0               0           0\r\n                  56   6    905802688     71069706      97     54894658      75   1922065392      2648        1     16175048        33453          17199  512   7  0.944      9502332      13      1586140       2        54004     24549924      33        94025222     129               0       0               0       0        10067686      13               0         0               0           0\r\n                  64   7    220889984     20255554      27     16804148      23   9163599655     12624        1      3451406        57263          27030   64   1  0.941      3259472       4      1498164       2       134042      7486391      10        37275666      51               0       0               0       0         2912754       4               0         0               0           0\r\n                  80   8     68633040     10648106      14      9790193      13   4291824716      5912        1       857913         3689           3351  256   5  0.908      2192822       3      1504358       2        13474      4109182       5        26367533      36               0       0               0       0         1574130       2               0         0               0           0\r\n                  96   9     17339232      2499640       3      2319023       3    859528950      1184        1       180617         1532            682  128   3  0.921      1000405       1      1121574       1         4790      1638823       2        18579568      25               0       0               0       0          370174       0               0         0               0           0\r\n                 112  10     62473936      5700197       7      5142394       7    574210007       791        1       557803         2320           1654  256   7  0.939      2136234       2      1322845       1         5747      3710376       5        20741666      28               0       0               0       0          476220       0               0         0               0           0\r\n                 128  11     80135040     18561135      25     17935080      24    337172693       464        1       626055        22087          19550   32   1  0.885      6767882       9      1566604       2       107037     12767896      17        32374751      44               0       0               0       0         1755476       2               0         0               0           0\r\n                 160  12   2426561600     56990302      78     41824292      57   2579470684      3553        1     15166010       119315          44864  128   5  0.993      7166304       9      1513051       2       186127     23672807      32        88689395     122               0       0               0       0        12148526      16               0         0               0           0\r\n                 192  13   1408278144     31675569      43     24340787      33   2049712618      2823        1      7334782       119689          58758   64   3  0.957      5002488       6      1479207       2       200693     12090507      16        52419929      72               0       0               0       0         6362682       8               0         0               0           0\r\n                 224  14    949042304     29418908      40     25182112      34   2048964999      2822        1      4236796        74111          69745  128   7  0.446      2849561       3      1491051       2       110749      8303720      11        60875629      83               0       0               0       0         1408028       1               0         0               0           0\r\n                 256  15     20811776      1491703       2      1410407       1     44440621        61        1        81296         5296           1595   16   1  0.959       747211       1       776031       1        21314      1062821       1        17647301      24               0       0               0       0          228230       0               0         0               0           0\r\n                 320  16    322758720     17105458      23     16096837      22    163920020       225        1      1008621       115337         111938   64   5  0.136      1210962       1      1267071       1       206011      1561018       2        48252296      66               0       0               0       0         1153456       1               0         0               0           0\r\n                 384  17    492847104      4230320       5      2946864       4     22604066        31        1      1283456        40108              0   32   3      1      1338847       1      1067491       1        67691      1704949       2        20171049      27               0       0               0       0          333398       0               0         0               0           0\r\n                 448  18     13349056      1277906       1      1248109       1      3152267         4        1        29797          475            213   64   7  0.980      1040379       1       630996       0         1311      1153062       1        17614587      24               0       0               0       0           63762       0               0         0               0           0\r\n                 512  19     40167936      1413530       1      1335077       1      3221982         4        1        78453         9822            119    8   1  0.998       968131       1       722665       0        27368      1087814       1        17760997      24               0       0               0       0           98412       0               0         0               0           0\r\n                 640  20    116151680      1510154       2      1328667       1      3868401         5        1       181487         5672              0   32   5  0.999       984093       1       630889       0        11067      1151388       1        17813467      24               0       0               0       0           51560       0               0         0               0           0\r\n                 768  21    130656000       800815       1       630690       0      2335944         3        1       170125        10633              0   16   3  0.999       507694       0       285925       0        17375       566498       0        16834672      23               0       0               0       0           25354       0               0         0               0           0\r\n                 896  22     84373632       633803       0       539636       0      1370478         1        1        94167         2943              0   32   7  0.999       443954       0       367760       0         4943       224736       0        16705338      23               0       0               0       0           14328       0               0         0               0           0\r\n                1024  23     28119040      4382005       6      4354545       5      6779850         9        1        27460         6869             15    4   1  0.999      1908121       2      1613523       2        32073      4316967       5        19406739      26               0       0               0       0           13934       0               0         0               0           0\r\n                1280  24      4357120        68112       0        64708       0        78116         0        1         3404          213              1   16   5  0.998        61798       0        39568       0          408        52685       0        15937093      21               0       0               0       0            7270       0               0         0               0           0\r\n                1536  25      1015296        50198       0        49537       0        43369         0        1          661           87             20    8   3  0.949        38045       0        36189       0          342        38659       0        15906588      21               0       0               0       0            3350       0               0         0               0           0\r\n                1792  26       684544        41289       0        40907       0        53208         0        1          382           26             12   16   7  0.918        38242       0        29987       0          107        23204       0        15899656      21               0       0               0       0            2710       0               0         0               0           0\r\n                2048  27      1288192      4957078       6      4956449       6      6486777         8        1          629          319              9    2   1  0.985      1431194       1      1285085       1       360643      4650579       6        18909840      26               0       0               0       0            6118       0               0         0               0           0\r\n                2560  28       768000        14111       0        13811       0        13860         0        1          300           43             24    8   5  0.872        13441       0        10157       0           86         8192       0        15854369      21               0       0               0       0            1646       0               0         0               0           0\r\n                3072  29       439296        34197       0        34054       0        32242         0        1          143           40             13    4   3  0.893        31317       0        30906       0          153         9327       0        15892405      21               0       0               0       0             850       0               0         0               0           0\r\n                3584  30       340480        17907       0        17812       0        17699         0        1           95           13              2    8   7  0.913        17454       0        16204       0           55         3026       0        15863489      21               0       0               0       0             532       0               0         0               0           0\r\n                4096  31      1421312       461127       0       460780       0       376549         0        1          347          347              0    1   1      1       259944       0       344160       0       461127            0       0        16896310      23               0       0               0       0            3288       0               0         0               0           0\r\n                5120  32       378880        15199       0        15125       0        15139         0        1           74           21              5    4   5  0.880        14958       0        13920       0           85         2254       0        15858668      21               0       0               0       0             400       0               0         0               0           0\r\n                6144  33       344064         4787       0         4731       0         4623         0        1           56           31              5    2   3  0.903         4363       0         2799       0          467         3298       0        15837168      21               0       0               0       0             238       0               0         0               0           0\r\n                7168  34       157696         2176       0         2154       0         2143         0        1           22            7              4    4   7  0.785         1950       0         1161       0          110         1194       0        15832719      21               0       0               0       0             110       0               0         0               0           0\r\n                8192  35      1540096        21667       0        21479       0        18668         0        1          188          188              0    1   2      1        17908       0        19952       0        21667            0       0        15889896      21               0       0               0       0            1888       0               0         0               0           0\r\n               10240  36       276480         2210       0         2183       0         2255         0        1           27           16              4    2   5  0.843         2092       0         1407       0          253         1760       0        15833207      21               0       0               0       0              96       0               0         0               0           0\r\n               12288  37       110592        13676       0        13667       0        13732         0        1            9            9              0    1   3      1        13509       0        13024       0        13676            0       0        15869608      21               0       0               0       0              48       0               0         0               0           0\r\n               14336  38        71680         1510       0         1505       0         1605         0        1            5            4              2    2   7  0.625         1485       0         1105       0          282          813       0        15832266      21               0       0               0       0              34       0               0         0               0           0\r\nlarge:          size ind    allocated      nmalloc (#/sec)      ndalloc (#/sec)    nrequests (#/sec)  curlextents\r\n               16384  39      1769472       316178       0       316070       0       364172       0          108\r\n               20480  40     15667200       832600       1       831835       1     10874836      14          765\r\n               24576  41       737280         5624       0         5594       0         7639       0           30\r\n               28672  42       802816        17034       0        17006       0        29725       0           28\r\n               32768  43      1277952         1189       0         1150       0         1194       0           39\r\n               40960  44        40960          248       0          247       0          248       0            1\r\n               49152  45            0         2541       0         2541       0         2541       0            0\r\n               57344  46            0        35906       0        35906       0        35906       0            0\r\n               65536  47      1900544          957       0          928       0          957       0           29\r\n               81920  48        81920         1249       0         1248       0         1249       0            1\r\n               98304  49            0          113       0          113       0          113       0            0\r\n              114688  50            0          301       0          301       0          301       0            0\r\n              131072  51      1835008          266       0          252       0          266       0           14\r\n              163840  52            0          298       0          298       0          298       0            0\r\n              196608  53            0           90       0           90       0           90       0            0\r\n              229376  54            0          125       0          125       0          125       0            0\r\n              262144  55      6553600          133       0          108       0          133       0           25\r\n              327680  56       327680          112       0          111       0          112       0            1\r\n              393216  57            0           50       0           50       0           50       0            0\r\n              458752  58            0            8       0            8       0            8       0            0\r\n              524288  59      5242880           43       0           33       0           43       0           10\r\n              655360  60       655360            8       0            7       0            8       0            1\r\n              786432  61            0            8       0            8       0            8       0            0\r\n              917504  62            0           20       0           20       0           20       0            0\r\n             1048576  63      2097152           18       0           16       0           18       0            2\r\n                     ---\r\n             2097152  67     12582912           12       0            6       0           12       0            6\r\n                     ---\r\n             4194304  71      8388608            5       0            3       0            5       0            2\r\n                     ---\r\n             8388608  75            0            3       0            3       0            3       0            0\r\n                     ---\r\n            16777216  79            0            3       0            3       0            3       0            0\r\n                     ---\r\n            33554432  83            0            3       0            3       0            3       0            0\r\n                     ---\r\n            67108864  87            0            3       0            3       0            3       0            0\r\n                     ---\r\n           134217728  91    268435456            2       0            0       0            2       0            2\r\n                     ---\r\nextents:        size ind       ndirty        dirty       nmuzzy        muzzy    nretained     retained       ntotal        total\r\n                4096   0           10        40960            0            0        21144     86605824        21154     86646784\r\n                8192   1           10        81920            0            0        15494    126926848        15504    127008768\r\n                     ---\r\n             8388608  39            0            0            0            0            1      8388608            1      8388608\r\n            10485760  40            0            0            0            0            1     10485760            1     10485760\r\n                     ---\r\n            16777216  43            0            0            0            0            1     16777216            1     16777216\r\n            20971520  44            0            0            0            0            1     20971520            1     20971520\r\n                     ---\r\n            33554432  47            0            0            0            0            2     67108864            2     67108864\r\n            41943040  48            0            0            0            0            1     41943040            1     41943040\r\n                     ---\r\n            67108864  51            0            0            0            0            1     67108864            1     67108864\r\n            83886080  52            0            0            0            0            1     83886080            1     83886080\r\n                     ---\r\n          1342177280  68            0            0            0            0            1   1464250368            1   1464250368\r\n                     ---\r\nBytes in small extent cache: 0\r\nHPA shard stats:\r\n  Purge passes: 0 (0 / sec)\r\n  Purges: 0 (0 / sec)\r\n  Hugeifies: 0 (0 / sec)\r\n  Dehugifies: 0 (0 / sec)\r\n\r\n  In full slabs:\r\n      npageslabs: 0 huge, 0 nonhuge\r\n      nactive: 0 huge, 0 nonhuge\r\n      ndirty: 0 huge, 0 nonhuge\r\n      nretained: 0 huge, 0 nonhuge\r\n  In empty slabs:\r\n      npageslabs: 0 huge, 0 nonhuge\r\n      nactive: 0 huge, 0 nonhuge\r\n      ndirty: 0 huge, 0 nonhuge\r\n      nretained: 0 huge, 0 nonhuge\r\n\r\n                size ind npageslabs_huge    nactive_huge     ndirty_huge  npageslabs_nonhuge     nactive_nonhuge      ndirty_nonhuge   nretained_nonhuge\r\n                     ---\r\narenas[0]:\r\nassigned threads: 1\r\nuptime: 725830571650076\r\ndss allocation precedence: \"secondary\"\r\ndecaying:  time       npages       sweeps     madvises       purged\r\n   dirty: 10000           30        59808       593486      1801107\r\n   muzzy:     0            0            0            0            0\r\n                            allocated         nmalloc   (#/sec)         ndalloc   (#/sec)       nrequests   (#/sec)           nfill   (#/sec)          nflush   (#/sec)\r\nsmall:                     8760041336       554242742       763       446073706       614     67182704643     92559        85734598       118        29525034        40\r\nlarge:                       59961344         1215136         1         1214074         1        11320077        15         1215136         1          859012         1\r\ntotal:                     8820002680       555457878       765       447287780       616     67194024720     92575        86949734       119        30384046        41\r\n\r\nactive:                   12269203456\r\nmapped:                   12420321280\r\nretained:                  1677783040\r\nbase:                       139244752\r\ninternal:                       98304\r\nmetadata_thp:                       0\r\ntcache_bytes:                  103760\r\ntcache_stashed_bytes:               0\r\nresident:                 12408573952\r\nabandoned_vm:                       0\r\nextent_avail:                       0\r\n                           n_lock_ops (#/sec)       n_waiting (#/sec)      n_spin_acq (#/sec)  n_owner_switch (#/sec)   total_wait_ns   (#/sec)     max_wait_ns  max_n_thds\r\nlarge                         7960125      10               0       0               0       0               1       0               0         0               0           0\r\nextent_avail                 10129062      13               0       0               2       0          104941       0               0         0               0           0\r\nextents_dirty                14969568      20               0       0              58       0          119673       0               0         0               0           0\r\nextents_muzzy                 7995512      11               0       0               0       0               1       0               0         0               0           0\r\nextents_retained              9757493      13               0       0               6       0          119697       0               0         0               0           0\r\ndecay_dirty                   9234703      12               0       0               1       0          148985       0               0         0               0           0\r\ndecay_muzzy                   8034898      11               0       0               0       0          148831       0               0         0               0           0\r\nbase                         16790858      23               0       0               0       0               1       0               0         0               0           0\r\ntcache_list                   7960126      10               0       0               0       0               1       0               0         0               0           0\r\nhpa_shard                           0       0               0       0               0       0               0       0               0         0               0           0\r\nhpa_shard_grow                      0       0               0       0               0       0               0       0               0         0               0           0\r\nhpa_sec                             0       0               0       0               0       0               0       0               0         0               0           0\r\nbins:           size ind    allocated      nmalloc (#/sec)      ndalloc (#/sec)    nrequests   (#/sec)  nshards      curregs     curslabs  nonfull_slabs regs pgs   util       nfills (#/sec)     nflushes (#/sec)       nslabs     nreslabs (#/sec)      n_lock_ops (#/sec)       n_waiting (#/sec)      n_spin_acq (#/sec)  n_owner_switch (#/sec)   total_wait_ns   (#/sec)     max_wait_ns  max_n_thds\r\n                   8   0         5992      1460121       2      1459372       2    165227552       227        1          749            2              0  512   1  0.731       908665       1       751437       1           19        15835       0         9620246      13               0       0               0       0               1       0               0         0               0           0\r\n                  16   1    165063952     53338498      73     43022001      59  19912092842     27433        1     10316497        42768          21276  256   1  0.942     10855872      14      1551039       2        78512     23817166      32        65267894      89               0       0               0       0         6578223       9               0         0               0           0\r\n                  24   2    865151040    164045682     226    127997722     176   8437774623     11625        1     36047960        73048          55411  512   3  0.963     14373712      19      2039254       2        95160     65526118      90       190612164     262               0       0               0       0        24699693      34               0         0               0           0\r\n                  32   3    328121216     47508388      65     37254600      51  11295660858     15562        1     10253788        85030          40353  128   1  0.942      6842004       9      1592562       2       158625     18131374      24        61736480      85               0       0               0       0         7114563       9               0         0               0           0\r\n                  40   4         5440       534324       0       534188       0   2931833591      4039        1          136            1              0  512   5  0.265       521512       0       251277       0            1            0       0         8732915      12               0       0               0       0               1       0               0         0               0           0\r\n                  48   5       109056      1985674       2      1983402       2    354711849       488        1         2272           21             15  256   3  0.422      1258241       1      1048496       1          353       817728       1        10316000      14               0       0               0       0           57183       0               0         0               0           0\r\n                  56   6    905802688     71069706      97     54894658      75   1922065392      2648        1     16175048        33453          17199  512   7  0.944      9502332      13      1586140       2        54004     24549924      33        86155972     118               0       0               0       0        10067685      13               0         0               0           0\r\n                  64   7    220889984     20255554      27     16804148      23   9163599655     12624        1      3451406        57263          27030   64   1  0.941      3259472       4      1498164       2       134042      7486391      10        29406416      40               0       0               0       0         2912753       4               0         0               0           0\r\n                  80   8     68633040     10648106      14      9790193      13   4291824716      5912        1       857913         3689           3351  256   5  0.908      2192822       3      1504358       2        13474      4109182       5        18498283      25               0       0               0       0         1574129       2               0         0               0           0\r\n                  96   9     17339232      2499640       3      2319023       3    859528950      1184        1       180617         1532            682  128   3  0.921      1000405       1      1121574       1         4790      1638823       2        10710318      14               0       0               0       0          370173       0               0         0               0           0\r\n                 112  10     62473936      5700197       7      5142394       7    574210007       791        1       557803         2320           1654  256   7  0.939      2136234       2      1322845       1         5747      3710376       5        12872416      17               0       0               0       0          476219       0               0         0               0           0\r\n                 128  11     80135040     18561135      25     17935080      24    337172693       464        1       626055        22087          19550   32   1  0.885      6767882       9      1566604       2       107037     12767896      17        24505501      33               0       0               0       0         1755475       2               0         0               0           0\r\n                 160  12   2426561600     56990302      78     41824292      57   2579470684      3553        1     15166010       119315          44864  128   5  0.993      7166304       9      1513051       2       186127     23672807      32        80820145     111               0       0               0       0        12148525      16               0         0               0           0\r\n                 192  13   1408278144     31675569      43     24340787      33   2049712618      2823        1      7334782       119689          58758   64   3  0.957      5002488       6      1479207       2       200693     12090507      16        44550679      61               0       0               0       0         6362681       8               0         0               0           0\r\n                 224  14    949042304     29418908      40     25182112      34   2048964999      2822        1      4236796        74111          69745  128   7  0.446      2849561       3      1491051       2       110749      8303720      11        53006379      73               0       0               0       0         1408027       1               0         0               0           0\r\n                 256  15     20811776      1491703       2      1410407       1     44440621        61        1        81296         5296           1595   16   1  0.959       747211       1       776031       1        21314      1062821       1         9778051      13               0       0               0       0          228229       0               0         0               0           0\r\n                 320  16    322758720     17105458      23     16096837      22    163920020       225        1      1008621       115337         111938   64   5  0.136      1210962       1      1267071       1       206011      1561018       2        40383046      55               0       0               0       0         1153455       1               0         0               0           0\r\n                 384  17    492847104      4230320       5      2946864       4     22604066        31        1      1283456        40108              0   32   3      1      1338847       1      1067491       1        67691      1704949       2        12301799      16               0       0               0       0          333397       0               0         0               0           0\r\n                 448  18     13349056      1277906       1      1248109       1      3152267         4        1        29797          475            213   64   7  0.980      1040379       1       630996       0         1311      1153062       1         9745337      13               0       0               0       0           63761       0               0         0               0           0\r\n                 512  19     40167936      1413530       1      1335077       1      3221982         4        1        78453         9822            119    8   1  0.998       968131       1       722665       0        27368      1087814       1         9891747      13               0       0               0       0           98411       0               0         0               0           0\r\n                 640  20    116151680      1510154       2      1328667       1      3868401         5        1       181487         5672              0   32   5  0.999       984093       1       630889       0        11067      1151388       1         9944217      13               0       0               0       0           51559       0               0         0               0           0\r\n                 768  21    130656000       800815       1       630690       0      2335944         3        1       170125        10633              0   16   3  0.999       507694       0       285925       0        17375       566498       0         8965422      12               0       0               0       0           25353       0               0         0               0           0\r\n                 896  22     84373632       633803       0       539636       0      1370478         1        1        94167         2943              0   32   7  0.999       443954       0       367760       0         4943       224736       0         8836088      12               0       0               0       0           14327       0               0         0               0           0\r\n                1024  23     28119040      4382005       6      4354545       5      6779850         9        1        27460         6869             15    4   1  0.999      1908121       2      1613523       2        32073      4316967       5        11537489      15               0       0               0       0           13933       0               0         0               0           0\r\n                1280  24      4357120        68112       0        64708       0        78116         0        1         3404          213              1   16   5  0.998        61798       0        39568       0          408        52685       0         8067843      11               0       0               0       0            7269       0               0         0               0           0\r\n                1536  25      1015296        50198       0        49537       0        43369         0        1          661           87             20    8   3  0.949        38045       0        36189       0          342        38659       0         8037338      11               0       0               0       0            3349       0               0         0               0           0\r\n                1792  26       684544        41289       0        40907       0        53208         0        1          382           26             12   16   7  0.918        38242       0        29987       0          107        23204       0         8030406      11               0       0               0       0            2709       0               0         0               0           0\r\n                2048  27      1288192      4957078       6      4956449       6      6486777         8        1          629          319              9    2   1  0.985      1431194       1      1285085       1       360643      4650579       6        11040590      15               0       0               0       0            6117       0               0         0               0           0\r\n                2560  28       768000        14111       0        13811       0        13860         0        1          300           43             24    8   5  0.872        13441       0        10157       0           86         8192       0         7985119      11               0       0               0       0            1645       0               0         0               0           0\r\n                3072  29       439296        34197       0        34054       0        32242         0        1          143           40             13    4   3  0.893        31317       0        30906       0          153         9327       0         8023155      11               0       0               0       0             849       0               0         0               0           0\r\n                3584  30       340480        17907       0        17812       0        17699         0        1           95           13              2    8   7  0.913        17454       0        16204       0           55         3026       0         7994239      11               0       0               0       0             531       0               0         0               0           0\r\n                4096  31      1421312       461127       0       460780       0       376549         0        1          347          347              0    1   1      1       259944       0       344160       0       461127            0       0         9027060      12               0       0               0       0            3287       0               0         0               0           0\r\n                5120  32       378880        15199       0        15125       0        15139         0        1           74           21              5    4   5  0.880        14958       0        13920       0           85         2254       0         7989418      11               0       0               0       0             399       0               0         0               0           0\r\n                6144  33       344064         4787       0         4731       0         4623         0        1           56           31              5    2   3  0.903         4363       0         2799       0          467         3298       0         7967918      10               0       0               0       0             237       0               0         0               0           0\r\n                7168  34       157696         2176       0         2154       0         2143         0        1           22            7              4    4   7  0.785         1950       0         1161       0          110         1194       0         7963469      10               0       0               0       0             109       0               0         0               0           0\r\n                8192  35      1540096        21667       0        21479       0        18668         0        1          188          188              0    1   2      1        17908       0        19952       0        21667            0       0         8020646      11               0       0               0       0            1887       0               0         0               0           0\r\n               10240  36       276480         2210       0         2183       0         2255         0        1           27           16              4    2   5  0.843         2092       0         1407       0          253         1760       0         7963957      10               0       0               0       0              95       0               0         0               0           0\r\n               12288  37       110592        13676       0        13667       0        13732         0        1            9            9              0    1   3      1        13509       0        13024       0        13676            0       0         8000358      11               0       0               0       0              47       0               0         0               0           0\r\n               14336  38        71680         1510       0         1505       0         1605         0        1            5            4              2    2   7  0.625         1485       0         1105       0          282          813       0         7963016      10               0       0               0       0              33       0               0         0               0           0\r\nlarge:          size ind    allocated      nmalloc (#/sec)      ndalloc (#/sec)    nrequests (#/sec)  curlextents\r\n               16384  39      1769472       316178       0       316070       0       364172       0          108\r\n               20480  40     15667200       832600       1       831835       1     10874836      14          765\r\n               24576  41       737280         5624       0         5594       0         7639       0           30\r\n               28672  42       802816        17034       0        17006       0        29725       0           28\r\n               32768  43      1277952         1189       0         1150       0         1194       0           39\r\n               40960  44        40960          248       0          247       0          248       0            1\r\n               49152  45            0         2541       0         2541       0         2541       0            0\r\n               57344  46            0        35906       0        35906       0        35906       0            0\r\n               65536  47      1900544          957       0          928       0          957       0           29\r\n               81920  48        81920         1249       0         1248       0         1249       0            1\r\n               98304  49            0          113       0          113       0          113       0            0\r\n              114688  50            0          301       0          301       0          301       0            0\r\n              131072  51      1835008          266       0          252       0          266       0           14\r\n              163840  52            0          298       0          298       0          298       0            0\r\n              196608  53            0           90       0           90       0           90       0            0\r\n              229376  54            0          125       0          125       0          125       0            0\r\n              262144  55      6553600          133       0          108       0          133       0           25\r\n              327680  56       327680          112       0          111       0          112       0            1\r\n              393216  57            0           50       0           50       0           50       0            0\r\n              458752  58            0            8       0            8       0            8       0            0\r\n              524288  59      5242880           43       0           33       0           43       0           10\r\n              655360  60       655360            8       0            7       0            8       0            1\r\n              786432  61            0            8       0            8       0            8       0            0\r\n              917504  62            0           20       0           20       0           20       0            0\r\n             1048576  63      2097152           18       0           16       0           18       0            2\r\n                     ---\r\n             2097152  67     12582912           12       0            6       0           12       0            6\r\n                     ---\r\n             4194304  71      8388608            5       0            3       0            5       0            2\r\n                     ---\r\nextents:        size ind       ndirty        dirty       nmuzzy        muzzy    nretained     retained       ntotal        total\r\n                4096   0           10        40960            0            0        21144     86605824        21154     86646784\r\n                8192   1           10        81920            0            0        15494    126926848        15504    127008768\r\n                     ---\r\n          1342177280  68            0            0            0            0            1   1464250368            1   1464250368\r\n                     ---\r\nBytes in small extent cache: 0\r\nHPA shard stats:\r\n  Purge passes: 0 (0 / sec)\r\n  Purges: 0 (0 / sec)\r\n  Hugeifies: 0 (0 / sec)\r\n  Dehugifies: 0 (0 / sec)\r\n\r\n  In full slabs:\r\n      npageslabs: 0 huge, 0 nonhuge\r\n      nactive: 0 huge, 0 nonhuge\r\n      ndirty: 0 huge, 0 nonhuge\r\n      nretained: 0 huge, 0 nonhuge\r\n  In empty slabs:\r\n      npageslabs: 0 huge, 0 nonhuge\r\n      nactive: 0 huge, 0 nonhuge\r\n      ndirty: 0 huge, 0 nonhuge\r\n      nretained: 0 huge, 0 nonhuge\r\n\r\n                size ind npageslabs_huge    nactive_huge     ndirty_huge  npageslabs_nonhuge     nactive_nonhuge      ndirty_nonhuge   nretained_nonhuge\r\n                     ---\r\narenas[8]:\r\nassigned threads: 0\r\nuptime: 717554192152229\r\ndss allocation precedence: \"secondary\"\r\ndecaying:  time       npages       sweeps     madvises       purged\r\n   dirty:     0            0            0           12        92160\r\n   muzzy:     0            0            0            0            0\r\n                            allocated         nmalloc   (#/sec)         ndalloc   (#/sec)       nrequests   (#/sec)           nfill   (#/sec)          nflush   (#/sec)\r\nsmall:                              0               0         0               0         0               0         0               0         0               0         0\r\nlarge:                      268435456              14         0              12         0              14         0              14         0               0         0\r\ntotal:                      268435456              14         0              12         0              14         0              14         0               0         0\r\n\r\nactive:                     268435456\r\nmapped:                     270532608\r\nretained:                   316669952\r\nbase:                           93392\r\ninternal:                           0\r\nmetadata_thp:                       0\r\ntcache_bytes:                       0\r\ntcache_stashed_bytes:               0\r\nresident:                   268529664\r\nabandoned_vm:                       0\r\nextent_avail:                       1\r\n                           n_lock_ops (#/sec)       n_waiting (#/sec)      n_spin_acq (#/sec)  n_owner_switch (#/sec)   total_wait_ns   (#/sec)     max_wait_ns  max_n_thds\r\nlarge                         7869250      10               0       0               0       0               1       0               0         0               0           0\r\nextent_avail                  7869269      10               0       0               0       0               1       0               0         0               0           0\r\nextents_dirty                 7869276      10               0       0               0       0               1       0               0         0               0           0\r\nextents_muzzy                 7869250      10               0       0               0       0               1       0               0         0               0           0\r\nextents_retained              7869281      10               0       0               0       0               1       0               0         0               0           0\r\ndecay_dirty                   8015799      11               0       0               0       0          148201       0               0         0               0           0\r\ndecay_muzzy                   7943652      11               0       0               0       0          148103       0               0         0               0           0\r\nbase                         15738513      21               0       0               0       0               1       0               0         0               0           0\r\ntcache_list                   7869250      10               0       0               0       0               1       0               0         0               0           0\r\nhpa_shard                           0       0               0       0               0       0               0       0               0         0               0           0\r\nhpa_shard_grow                      0       0               0       0               0       0               0       0               0         0               0           0\r\nhpa_sec                             0       0               0       0               0       0               0       0               0         0               0           0\r\nbins:           size ind    allocated      nmalloc (#/sec)      ndalloc (#/sec)    nrequests   (#/sec)  nshards      curregs     curslabs  nonfull_slabs regs pgs   util       nfills (#/sec)     nflushes (#/sec)       nslabs     nreslabs (#/sec)      n_lock_ops (#/sec)       n_waiting (#/sec)      n_spin_acq (#/sec)  n_owner_switch (#/sec)   total_wait_ns   (#/sec)     max_wait_ns  max_n_thds\r\n                     ---\r\nlarge:          size ind    allocated      nmalloc (#/sec)      ndalloc (#/sec)    nrequests (#/sec)  curlextents\r\n                     ---\r\n             8388608  75            0            3       0            3       0            3       0            0\r\n                     ---\r\n            16777216  79            0            3       0            3       0            3       0            0\r\n                     ---\r\n            33554432  83            0            3       0            3       0            3       0            0\r\n                     ---\r\n            67108864  87            0            3       0            3       0            3       0            0\r\n                     ---\r\n           134217728  91    268435456            2       0            0       0            2       0            2\r\n                     ---\r\nextents:        size ind       ndirty        dirty       nmuzzy        muzzy    nretained     retained       ntotal        total\r\n                     ---\r\n             8388608  39            0            0            0            0            1      8388608            1      8388608\r\n            10485760  40            0            0            0            0            1     10485760            1     10485760\r\n                     ---\r\n            16777216  43            0            0            0            0            1     16777216            1     16777216\r\n            20971520  44            0            0            0            0            1     20971520            1     20971520\r\n                     ---\r\n            33554432  47            0            0            0            0            2     67108864            2     67108864\r\n            41943040  48            0            0            0            0            1     41943040            1     41943040\r\n                     ---\r\n            67108864  51            0            0            0            0            1     67108864            1     67108864\r\n            83886080  52            0            0            0            0            1     83886080            1     83886080\r\n                     ---\r\nBytes in small extent cache: 0\r\nHPA shard stats:\r\n  Purge passes: 0 (0 / sec)\r\n  Purges: 0 (0 / sec)\r\n  Hugeifies: 0 (0 / sec)\r\n  Dehugifies: 0 (0 / sec)\r\n\r\n  In full slabs:\r\n      npageslabs: 0 huge, 0 nonhuge\r\n      nactive: 0 huge, 0 nonhuge\r\n      ndirty: 0 huge, 0 nonhuge\r\n      nretained: 0 huge, 0 nonhuge\r\n  In empty slabs:\r\n      npageslabs: 0 huge, 0 nonhuge\r\n      nactive: 0 huge, 0 nonhuge\r\n      ndirty: 0 huge, 0 nonhuge\r\n      nretained: 0 huge, 0 nonhuge\r\n\r\n                size ind npageslabs_huge    nactive_huge     ndirty_huge  npageslabs_nonhuge     nactive_nonhuge      ndirty_nonhuge   nretained_nonhuge\r\n                     ---\r\n--- End jemalloc statistics ---\r\n```\r\n\r\n**To reproduce**\r\n\r\nUnsure what causes this issue to reproduce.\r\n\r\n**Expected behavior**\r\n\r\nI expect active defrag running to help with external memory fragmentation\r\n",
      "solution": "I'm facing the same problem...\n\nMy Redis server worked well for some days but after fourteen days the problem appeared again:\n\n```\n# uptime\n 14:08:19 up 14 days,  5:50,  1 user,  load average: 0.00, 0.00, 0.00\n\nredis-server --version\nRedis server v=6.2.18 sha=00000000:0 malloc=jemalloc-5.1.0 bits=64 build=43c2df230c37e8ca\n```\n\n```\n127.0.0.1:6379> CONFIG GET activedefrag\n1) \"activedefrag\"\n2) \"yes\"\n```\n\n```\n127.0.0.1:6379> CONFIG GET active-defrag-ignore-bytes\n1) \"active-defrag-ignore-bytes\"\n2) \"104857600\"\n```\n\n```\n127.0.0.1:6379> info memory\n# Memory\nused_memory:6442054120\nused_memory_human:6.00G\nused_memory_rss:6777532416\nused_memory_rss_human:6.31G\nused_memory_peak:6454464936\nused_memory_peak_human:6.01G\nused_memory_peak_perc:99.81%\nused_memory_overhead:32587504\nused_memory_startup:812000\nused_memory_dataset:6409466616\nused_memory_dataset_perc:99.51%\nallocator_allocated:6443080088\nallocator_active:6933839872\nallocator_resident:7006519296\ntotal_system_memory:8025862144\ntotal_system_memory_human:7.47G\nused_memory_lua:30720\nused_memory_lua_human:30.00K\nused_memory_scripts:0\nused_memory_scripts_human:0B\nnumber_of_cached_scripts:0\nmaxmemory:6442450944\nmaxmemory_human:6.00G\nmaxmemory_policy:allkeys-lru\nallocator_frag_ratio:1.08\nallocator_frag_bytes:490759784\nallocator_rss_ratio:1.01\nallocator_rss_bytes:72679424\nrss_overhead_ratio:0.97\nrss_overhead_bytes:-228986880\nmem_fragmentation_ratio:1.05\n**mem_fragmentation_bytes:335519320**\nmem_not_counted_for_evict:0\nmem_replication_backlog:0\nmem_clients_slaves:0\nmem_clients_normal:20496\nmem_aof_buffer:0\nmem_allocator:jemalloc-5.1.0\nactive_defrag_running:0\nlazyfree_pending_objects:0\nlazyfreed_objects:0\n```\n\nThanks for any help!\n\n\n---\n\n> [@reporter4u](https://github.com/reporter4u) this issue happened when upgrading to 7.2, but the version you're using is 6.2. do you mean the defrag is ineffective? i saw that the fragmention ratio is 8% from your input, which didn't look too high. maybe you can share the `INFO ALL` and `MEMORY malloc-stats`\n\nThe defrag worked for nearly two weeks and now is ineffective. I haven't restarted the service: i have now 800 Mbyte of available RAM.\n\nMaybe the defragmentation is not too high but is three time *active-defrag-ignore-bytes* which I espected to trig active defrag procedures.\nThe problem is when I gave 6 gbyte of *maxmemory* and the rest to the SO and if Redis does not honor defragmentation I risk to go out of memory when i run other commands (for example python scripts running as Ansible host target).\n\n\n\n```\n# free\n              total        used        free      shared  buff/cache   available\nMem:        7837756     6809952      423168        6480      604636      810160\nSwap:      16777212       14848    16762364\n\n```\n\n\nThese are the results o the commands:\n```\n124.0.0.1:6379> INFO ALL\n# Server\nredis_version:6.2.18\nredis_git_sha1:00000000\nredis_git_dirty:0\nredis_build_id:43c2df230c37e8ca\nredis_mode:standalone\nos:Linux 5.4.17-2136.344.4.1.el8uek.x86_64 x86_64\narch_bits:64\nmonotonic_clock:POSIX clock_gettime\nmultiplexing_api:epoll\natomicvar_api:c11-builtin\ngcc_version:8.5.0\nprocess_id:1052\nprocess_supervised:systemd\nrun_id:0fe15f89734ca0a953ff225a2c928a85c6074f8f\ntcp_port:6379\nserver_time_usec:1751027796302842\nuptime_in_seconds:1232317\nuptime_in_days:14\nhz:10\nconfigured_hz:10\nlru_clock:6197332\nexecutable:/usr/bin/redis-server\nconfig_file:/etc/redis.conf\nio_threads_active:0\n\n# Clients\nconnected_clients:2\ncluster_connections:0\nmaxclients:10000\nclient_recent_max_input_buffer:40976\nclient_recent_max_output_buffer:0\nblocked_clients:0\ntracking_clients:0\nclients_in_timeout_table:0\n\n# Memory\nused_memory:6441984384\nused_memory_human:6.00G\nused_memory_rss:6778433536\nused_memory_rss_human:6.31G\nused_memory_peak:6454464936\nused_memory_peak_human:6.01G\nused_memory_peak_perc:99.81%\nused_memory_overhead:32625728\nused_memory_startup:812000\nused_memory_dataset:6409358656\nused_memory_dataset_perc:99.51%\nallocator_allocated:6443142664\nallocator_active:6936264704\nallocator_resident:7008944128\ntotal_system_memory:8025862144\ntotal_system_memory_human:7.47G\nused_memory_lua:30720\nused_memory_lua_human:30.00K\nused_memory_scripts:0\nused_memory_scripts_human:0B\nnumber_of_cached_scripts:0\nmaxmemory:6442450944\nmaxmemory_human:6.00G\nmaxmemory_policy:allkeys-lru\nallocator_frag_ratio:1.08\nallocator_frag_bytes:493122040\nallocator_rss_ratio:1.01\nallocator_rss_bytes:72679424\nrss_overhead_ratio:0.97\nrss_overhead_bytes:-230510592\nmem_fragmentation_ratio:1.05\nmem_fragmentation_bytes:336451000\nmem_not_counted_for_evict:0\nmem_replication_backlog:0\nmem_clients_slaves:0\nmem_clients_normal:40992\nmem_aof_buffer:0\nmem_allocator:jemalloc-5.1.0\nactive_defrag_running:0\nlazyfree_pending_objects:0\nlazyfreed_objects:0\n\n# Persistence\nloading:0\ncurrent_cow_size:0\ncurrent_cow_size_age:0\ncurrent_fork_perc:0.00\ncurrent_save_keys_processed:0\ncurrent_save_keys_total:0\nrdb_changes_since_last_save:42820421\nrdb_bgsave_in_progress:0\nrdb_last_save_time:1749795479\nrdb_last_bgsave_status:ok\nrdb_last_bgsave_time_sec:-1\nrdb_current_bgsave_time_sec:-1\nrdb_last_cow_size:0\naof_enabled:0\naof_rewrite_in_progress:0\naof_rewrite_scheduled:0\naof_last_rewrite_time_sec:-1\naof_current_rewrite_time_sec:-1\naof_last_bgrewrite_status:ok\naof_last_write_status:ok\naof_last_cow_size:0\nmodule_fork_in_progress:0\nmodule_fork_last_cow_size:0\n\n# Stats\ntotal_connections_received:1202638\ntotal_commands_processed:127211852\ninstantaneous_ops_per_sec:45\ntotal_net_input_bytes:90373398960\ntotal_net_output_bytes:1081949875940\ninstantaneous_input_kbps:4.50\ninstantaneous_output_kbps:1125.48\nrejected_connections:0\nsync_full:0\nsync_partial_ok:0\nsync_partial_err:0\nexpired_keys:0\nexpired_stale_perc:0.00\nexpired_time_cap_reached_count:0\nexpire_cycle_cpu_milliseconds:119230\nevicted_keys:1979482\nkeyspace_hits:95739699\nkeyspace_misses:9809100\npubsub_channels:0\npubsub_patterns:0\nlatest_fork_usec:0\ntotal_forks:0\nmigrate_cached_sockets:0\nslave_expires_tracked_keys:0\nactive_defrag_hits:28751529\nactive_defrag_misses:9111180309\nactive_defrag_key_hits:8660483\nactive_defrag_key_misses:303746067\ntracking_total_keys:0\ntracking_total_items:0\ntracking_total_prefixes:0\nunexpected_error_replies:0\ntotal_error_replies:0\ndump_payload_sanitizations:0\ntotal_reads_processed:135565830\ntotal_writes_processed:128530744\nio_threaded_reads_processed:0\nio_threaded_writes_processed:0\n\n# Replication\nrole:master\nconnected_slaves:0\nmaster_failover_state:no-failover\nmaster_replid:2180719d49cad4360b7436b2ba7400d7602167cf\nmaster_replid2:0000000000000000000000000000000000000000\nmaster_repl_offset:0\nsecond_repl_offset:-1\nrepl_backlog_active:0\nrepl_backlog_size:1048576\nrepl_backlog_first_byte_offset:0\nrepl_backlog_histlen:0\n\n# CPU\nused_cpu_sys:3036.200037\nused_cpu_user:5680.056479\nused_cpu_sys_children:0.000000\nused_cpu_user_children:0.000000\nused_cpu_sys_main_thread:3018.363009\nused_cpu_user_main_thread:5675.180528\n\n# Modules\n\n# Commandstats\ncmdstat_incr:calls=181,usec=600,usec_per_call=3.31,rejected_calls=0,failed_calls=0\ncmdstat_expire:calls=4280016,usec=7632663,usec_per_call=1.78,rejected_calls=0,failed_calls=0\ncmdstat_hgetall:calls=99264901,usec=899855907,usec_per_call=9.07,rejected_calls=0,failed_calls=0\ncmdstat_slowlog:calls=2,usec=892,usec_per_call=446.00,rejected_calls=0,failed_calls=0\ncmdstat_hmset:calls=4280016,usec=72473749,usec_per_call=16.93,rejected_calls=0,failed_calls=0\ncmdstat_del:calls=11024,usec=122909,usec_per_call=11.15,rejected_calls=0,failed_calls=0\ncmdstat_info:calls=24,usec=9697,usec_per_call=404.04,rejected_calls=0,failed_calls=0\ncmdstat_hget:calls=384,usec=1627,usec_per_call=4.24,rejected_calls=0,failed_calls=0\ncmdstat_config:calls=26,usec=1395,usec_per_call=53.65,rejected_calls=0,failed_calls=0\ncmdstat_set:calls=9050,usec=26391,usec_per_call=2.92,rejected_calls=0,failed_calls=0\ncmdstat_ping:calls=5390,usec=3175,usec_per_call=0.59,rejected_calls=0,failed_calls=0\ncmdstat_dbsize:calls=23,usec=19,usec_per_call=0.83,rejected_calls=0,failed_calls=0\ncmdstat_hset:calls=106,usec=235,usec_per_call=2.22,rejected_calls=0,failed_calls=0\ncmdstat_get:calls=6276266,usec=16947480,usec_per_call=2.70,rejected_calls=0,failed_calls=0\ncmdstat_exec:calls=6539548,usec=252017362,usec_per_call=38.54,rejected_calls=0,failed_calls=0\ncmdstat_multi:calls=6539548,usec=3367873,usec_per_call=0.52,rejected_calls=0,failed_calls=0\ncmdstat_mget:calls=834,usec=9649,usec_per_call=11.57,rejected_calls=0,failed_calls=0\ncmdstat_command:calls=3,usec=5065,usec_per_call=1688.33,rejected_calls=0,failed_calls=0\ncmdstat_scan:calls=4509,usec=4505489,usec_per_call=999.22,rejected_calls=0,failed_calls=0\ncmdstat_monitor:calls=1,usec=1,usec_per_call=1.00,rejected_calls=0,failed_calls=0\n\n# Errorstats\n\n# Cluster\ncluster_enabled:0\n\n# Keyspace\ndb0:keys=365386,expires=365362,avg_ttl=31275787546\n```\n\n```\n127.0.0.1:6379> MEMORY malloc-stats\n___ Begin jemalloc statistics ___\nVersion: \"5.1.0-0-g0\"\nBuild-time option settings\n  config.cache_oblivious: true\n  config.debug: false\n  config.fill: true\n  config.lazy_lock: false\n  config.malloc_conf: \"\"\n  config.prof: false\n  config.prof_libgcc: false\n  config.prof_libunwind: false\n  config.stats: true\n  config.utrace: false\n  config.xmalloc: false\nRun-time option settings\n  opt.abort: false\n  opt.abort_conf: false\n  opt.retain: true\n  opt.dss: \"secondary\"\n  opt.narenas: 8\n  opt.percpu_arena: \"disabled\"\n  opt.metadata_thp: \"disabled\"\n  opt.background_thread: false (background_thread: true)\n  opt.dirty_decay_ms: 10000 (arenas.dirty_decay_ms: 10000)\n  opt.muzzy_decay_ms: 10000 (arenas.muzzy_decay_ms: 10000)\n  opt.junk: \"false\"\n  opt.zero: false\n  opt.tcache: true\n  opt.lg_tcache_max: 15\n  opt.thp: \"default\"\n  opt.stats_print: false\n  opt.stats_print_opts: \"\"\nArenas: 8\nQuantum size: 8\nPage size: 65536\nMaximum thread-cached size class: 229376\nNumber of bin size classes: 55\nNumber of thread-cache bin size classes: 55\nNumber of large size classes: 180\nAllocated: 6443459960, active: 6936133632, metadata: 71668792 (n_thp 0), resident: 7007764480, mapped: 7011631104, retained: 4863492096\nBackground threads: 1, num_runs: 184514, run_interval: 6679229711 ns\n                           n_lock_ops       n_waiting      n_spin_acq  n_owner_switch   total_wait_ns     max_wait_ns  max_n_thds\nbackground_thread            26930891               0               0               1               0               0           0\nctl                          53861775               0               0               1               0               0           0\nprof                                0               0               0               0               0               0           0\narenas[0]:\nassigned threads: 1\nuptime: 1232438213195905\ndss allocation precedence: \"secondary\"\ndecaying:  time       npages       sweeps     madvises       purged\n   dirty: 10000            0       175979       418577      1997875\n   muzzy: 10000            0       157709       358361      1543163\n                            allocated     nmalloc     ndalloc   nrequests\nsmall:                     5309949304   110747398   100151558  1403153747\nlarge:                     1133510656     1413192     1409303     1413192\ntotal:                     6443459960   112160590   101560861  1404566939\n                                     \nactive:                    6936133632\nmapped:                    7011631104\nretained:                  4863492096\nbase:                        71611440\ninternal:                       57352\nmetadata_thp:                       0\ntcache_bytes:                 1268752\nresident:                  7007764480\n                           n_lock_ops       n_waiting      n_spin_acq  n_owner_switch   total_wait_ns     max_wait_ns  max_n_thds\nlarge                        13465445               0               0               1               0               0           0\nextent_avail                 14844385               0               0          234407               0               0           0\nextents_dirty                18412111               1              17          351987               0               0           1\nextents_muzzy                14622686               0               2          351207               0               0           0\nextents_retained             14454779               3               4          311825               0               0           1\ndecay_dirty                  14400287               0               2          369423               0               0           0\ndecay_muzzy                  14302984               0               1          357361               0               0           0\nbase                         26991021               0               0               3               0               0           0\ntcache_list                  13465446               0               0               1               0               0           0\nbins:           size ind    allocated      nmalloc      ndalloc    nrequests      curregs     curslabs regs pgs   util       nfills     nflushes       nslabs     nreslabs      n_lock_ops       n_waiting      n_spin_acq  n_owner_switch   total_wait_ns     max_wait_ns  max_n_thds\n                   8   0     25130896     23897258     20755896     58562025      3141362          386 8192   1  0.993       825748       212596          750      2512240      2756247594               0               0        24012853               0               0           0\n                  16   1     28781760     23665284     21866424    267002082      1798860          441 4096   1  0.995      1731870       264989         1015      3822131      1560163612               0               0        15251407               0               0           0\n                  24   2     86151024     31031232     27441606    237956553      3589626          441 8192   3  0.993       866973       271016          832      3143190      3136088740               0               0        25003473               0               0           0\n                  32   3      1665984      4210957      4158895    143453896        52062           26 2048   1  0.977      2338532       182244          124       505570        54391461               0               0         6136011               0               0           0\n                  40   4       624480       153096       137484      4925617        15612            2 8192   5  0.952        75806        81704            2        13970        28161019               0               0         5870745               0               0           0\n                  48   5      1685472       616418       581304    487080361        35114            9 4096   3  0.952       326496       161964           27        97509        48326474               0               0         5996265               0               0           0\n                  56   6      2673384      1585735      1537996     24612360        47739            6 8192   7  0.971       901441       171856            6       223019        61582896               0               0         5986701               0               0           0\n                  64   7     24593856      3814460      3430181     39599864       384279          377 1024   1  0.995       745181       176425         1012      1168855       346035484               0               0         7993997               0               0           0\n                  80   8      4930640      2344684      2283051     21038399        61633           16 4096   5  0.940      1073170       176463           39       408161        61173301               0               0         6164559               0               0           0\n                  96   9     40889184      3725714      3299785     16753441       425929          209 2048   3  0.995       478637       174149          576       935161       372230460               0               0         8244243               0               0           0\n                 112  10      5773600       792541       740991      5307312        51550           13 4096   7  0.968       271595       158024           44       199503        53980303               0               0         6187331               0               0           0\n                 128  11     10226432       977840       897946      6710989        79894          159  512   1  0.981       194879       149899          550       359271        86645558               0               0         6529597               0               0           0\n                 160  12      9878880       918142       856399     13185566        61743           31 2048   5  0.972       199239       154886          120       251727        78074851               0               0         6404195               0               0           0\n                 192  13     13732416       800720       729197      8191298        71523           71 1024   3  0.983       170475       149326          314       223523        72103185               0               0         6326937               0               0           0\n                 224  14      8845984       545799       506308      7134481        39491           20 2048   7  0.964       155711       132232           72       150833        49787336               0               0         6146019               0               0           0\n                 256  15      7822336       419232       388676      3790739        30556          122  256   1  0.978       130795       117264          439       170066        43772848               0               0         6139317               0               0           0\n                 320  16     24160640       680815       605313      8350497        75502           74 1024   5  0.996       205731       133886          203       218754        59482712               0               0         6257711               0               0           0\n                 384  17     46997376      1431182      1308793      6930374       122389          242  512   3  0.987       261340       153710          804       401724        99875731               0               0         7059907               0               0           0\n                 448  18     32565120       979506       906816      4644203        72690           75 1024   7  0.946       242535       152710          330       340839        78563431               0               0         6456017               0               0           0\n                 512  19     28993536      1239385      1182757      4366970        56628          464  128   1  0.953       229917       157151         3262       386123        80497264               0               0         6702269               0               0           0\n                 640  20     29203200       823818       778188      3417148        45630           90  512   5  0.990       222382       153468          445       278607        62187923               0               0         6340357               0               0           0\n                 768  21     12021504       256198       240545       719834        15653           62  256   3  0.986       111098       106780          266       108164        27838346               0               0         5918685               0               0           0\n                 896  22      7077504       218074       210175       540929         7899           30  512   7  0.514       112703       102723           97        74226        26283745               0               0         5670305               0               0           0\n                1024  23      4122624       105998       101972       304856         4026           63   64   1  0.998        75796        75791          304        31958        16290678               0               0         3425767               0               0           0\n                1280  24     20364800       331119       315209       650686        15910           63  256   5  0.986       108649       121270          482       118868        26398799               0               0         5934847               0               0           0\n                1536  25     45869568       395776       365913       840477        29863          237  128   3  0.984       120015       124471          960       172499        35486212               0               0         6125225               0               0           0\n                1792  26     72308992       539752       499401      1029437        40351          158  256   7  0.997       138558       137564          660       228710        45764737               0               0         6238545               0               0           0\n                2048  27     60315648       479251       449800       930450        29451          964   32   1  0.954       121804       133440         6091       187621        47240066               0               0         6139269               0               0           0\n                2560  28     34216960       148988       135622       243473        13366          107  128   5  0.975        66045        64302          345        73277        24901357               0               0         5805579               0               0           0\n                3072  29     48251904       222294       206587       467525        15707          472   64   3  0.519        91666        76120          981        94454        40434713               0               0         6029049               0               0           0\n                3584  30     29392384        97458        89257       169007         8201           65  128   7  0.985        53613        47316          245        41507        22926089               0               0         5642043               0               0           0\n                4096  31     15986688        65148        61245        83107         3903          260   16   1  0.938        34741        34727         1092        41073        18193131               0               0         4610167               0               0           0\n                5120  32    310978560       664271       603533      1576232        60738          953   64   5  0.995       164171       141336         3093       292059        58401736               0               0         6362759               0               0           0\n                6144  33     60739584       148772       138886       310982         9886          320   32   3  0.965        80191        72099          902        93768        23809581               0               0         5720603               0               0           0\n                7168  34     20063232        42482        39683        68142         2799           45   64   7  0.971        26316        24561          181        25052        15548617               0               0         2913331               0               0           0\n                8192  35     47349760        97604        91824       177283         5780          749    8   1  0.964        59347        59956         3227        59242        17701337               0               0         4406967               0               0           0\n               10240  36     47380480       112487       107860       203833         4627          255   32   5  0.567        64225        63210          749        60062        20646607               0               0         5170139               0               0           0\n               12288  37     34308096        56822        54030        85836         2792          192   16   3  0.908        32171        31521          840        36572        16726593               0               0         3845515               0               0           0\n               14336  38     24242176        34442        32751        56394         1691           55   32   7  0.960        19461        19529          252        19795        15141021               0               0         2471291               0               0           0\n               16384  39     25296896        27717        26173        47621         1544          411    4   1  0.939        19118        18918         1870        18394        14863121               0               0         2124761               0               0           0\n               20480  40     37621760       505175       503338      4378588         1837          121   16   5  0.948       321313       153699         6417       223571        15593645               0               0         2470717               0               0           0\n               24576  41     30081024        49862        48638      1080311         1224          159    8   3  0.962        37585        40367         1615        17885        14561323               0               0         1705735               0               0           0\n               28672  42     19353600        63731        63056       555544          675           46   16   7  0.917        55259        56579          942        14071        14148910               0               0         1013767               0               0           0\n               32768  43     19431424        68924        68331       377409          593          320    2   1  0.926        65001        65821         7257         8412        14018167               0               0          751895               0               0           0\n               40960  44   1224007680       374474       344591      9980955        29883         3738    8   5  0.999       150224       137112        16042       211251        27006344               0               0         5916211               0               0           0\n               49152  45     60899328        61310        60071       535399         1239          321    4   3  0.964        51123        52788         3147        18452        14194663               0               0         1103053               0               0           0\n               57344  46    484384768       206937       198490       575577         8447         1234    8   7  0.855       111064       107922         7008       123638        21124984               0               0         5419691               0               0           0\n               65536  47    429654016       191470       184914       359198         6556         6556    1   1      1       101098       109957       191470            0        20942293               0               0         5198569               0               0           0\n               81920  48    392151040       101153        96366      1692660         4787         1197    4   5  0.999        59881        65673         8610        57398        17360630               0               0         4142529               0               0           0\n               98304  49    103710720        78358        77303       288051         1055          574    2   3  0.918        70006        70267         8551        18446        14156107               0               0          953363               0               0           0\n              114688  50     58949632        88463        87949       250276          514          137    4   7  0.937        78192        78566         3724        19512        14286059               0               0         1150875               0               0           0\n              131072  51     66191360        57291        56786       127535          505          505    1   2      1        52697        53220        57291            0        14212407               0               0          960421               0               0           0\n              163840  52     18841600        58546        58431       336753          115           62    2   5  0.927        57207        57298         9086         2974        13685339               0               0          168987               0               0           0\n              196608  53     16318464        31162        31079       840232           83           83    1   3      1        29837        30044        31162            0        13639356               0               0          102885               0               0           0\n              229376  54    992739328       112071       107743       254980         4328         2205    2   7  0.981        81168        83325        14495        41431        16590478               0               0         3325727               0               0           0\nlarge:          size ind    allocated      nmalloc      ndalloc    nrequests  curlextents\n              262144  55    783810560      1109666      1106676      1109666         2990\n              327680  56    269352960        99905        99083        99905          822\n              393216  57      4718592         1400         1388         1400           12\n              458752  58      6422528        21550        21536        21550           14\n              524288  59      4194304        39986        39978        39986            8\n              655360  60     14417920       138168       138146       138168           22\n              786432  61      2359296          370          367          370            3\n              917504  62            0          629          629          629            0\n             1048576  63            0           38           38           38            0\n             1310720  64     10485760          492          484          492            8\n             1572864  65            0           52           52           52            0\n             1835008  66            0            3            3            3            0\n             2097152  67            0            2            2            2            0\n             2621440  68            0           41           41           41            0\n             3145728  69            0            1            1            1            0\n             3670016  70     29360128          780          772          780            8\n             4194304  71      8388608            3            1            3            2\n             5242880  72            0          106          106          106            0\n                     ---\n--- End jemalloc statistics ---\n\n```\n\nThanks.\n\n---\n\n> [@reporter4u](https://github.com/reporter4u) `active-defrag-ignore-bytes` does not mean that Redis will definitely defragment to such a low level. Instead, it indicates that defragmentation will only be executed if the fragmentation is higher than this. from the malloc-stats, the vast majority of bins have been defragmented, and 8% fragmentation rate is actually not too high.\n\nOk, but the memory is retained and who is doing this is Redis, despite INFO statistics. There are no other process who are using it.\n\nThe total of used memory does not match what Redis INFO is saying.\n7.4 Gbyte (total ram) - 6 Gbyte (Redis maxmemory) - 200Mbyte (SO) = 1.2 Gbyte free \n\nBut now  I have 800 Mbyte of available memory which is going down. If we add 300 Mbyte of fragmented ram we reach almost 1.2Gbyte that is ok.\n\nIf I restart Redis the count is ok for several days but when Redis stops to defragment the SO shows that there is few free memory.\n\nAt the moment the problem is not 800 Mbyte of free ram is that Redis does not start defragmentation process. And this will be a problem... It is not so good to do a manually service restart each time defragmentation stops to work because it is not predictable.",
      "labels": [
        "memory efficiency"
      ],
      "created_at": "2024-10-21T23:08:10Z",
      "closed_at": "2025-08-20T06:40:17Z",
      "url": "https://github.com/redis/redis/issues/13612",
      "comments_count": 27
    },
    {
      "tech": "redis",
      "repo": "redis/redis",
      "issue_number": 14267,
      "title": "[CRASH] Redis 8.0.2 slave keep crashing when it startup.",
      "problem": "Notice!\n- If a Redis module was involved, please open an issue in the module's repo instead!\n- If you're using docker on Apple M1, please make sure the image you're using was compiled for ARM!\n\n\n**Crash report**\n\nPaste the complete crash log between the quotes below. Please include a few lines from the log preceding the crash report to provide some context.\n\n```\n\n=== REDIS BUG REPORT START: Cut & paste starting from here ===\n1027239:S 12 Aug 2025 10:41:05.050 # Redis 8.0.2 crashed by signal: 11, si_code: 128\n1027239:S 12 Aug 2025 10:41:05.050 # Accessing address: (nil)\n1027239:S 12 Aug 2025 10:41:05.050 # Crashed running the instruction at: 0x466e68\n\n------ STACK TRACE ------\nEIP:\n/usr/local/bin/redis-server 127.0.0.1:6379(dictSdsDestructor+0x8)[0x466e68]\n\n1027240 bio_close_file\n/lib64/libc.so.6(+0x8722a)[0x7fad0048722a]\n/lib64/libc.so.6(pthread_cond_wait+0x132)[0x7fad00489782]\n/usr/local/bin/redis-server 127.0.0.1:6379(bioProcessBackgroundJobs+0x1e1)[0x53a0f1]\n/lib64/libc.so.6(+0x8a19a)[0x7fad0048a19a]\n/lib64/libc.so.6(+0x10f210)[0x7fad0050f210]\n\n1027243 io_thd_1\n/lib64/libc.so.6(epoll_wait+0x5e)[0x7fad0050e84e]\n/usr/local/bin/redis-server 127.0.0.1:6379[0x4631c7]\n/usr/local/bin/redis-server 127.0.0.1:6379(aeMain+0xa4)[0x463864]\n/usr/local/bin/redis-server 127.0.0.1:6379(IOThreadMain+0x67)[0x46c067]\n/lib64/libc.so.6(+0x8a19a)[0x7fad0048a19a]\n/lib64/libc.so.6(+0x10f210)[0x7fad0050f210]\n\n1027244 io_thd_2\n/lib64/libc.so.6(epoll_wait+0x5e)[0x7fad0050e84e]\n/usr/local/bin/redis-server 127.0.0.1:6379[0x4631c7]\n/usr/local/bin/redis-server 127.0.0.1:6379(aeMain+0xa4)[0x463864]\n/usr/local/bin/redis-server 127.0.0.1:6379(IOThreadMain+0x67)[0x46c067]\n/lib64/libc.so.6(+0x8a19a)[0x7fad0048a19a]\n/lib64/libc.so.6(+0x10f210)[0x7fad0050f210]\n\n1027241 bio_aof\n/lib64/libc.so.6(+0x8722a)[0x7fad0048722a]\n/lib64/libc.so.6(pthread_cond_wait+0x132)[0x7fad00489782]\n/usr/local/bin/redis-server 127.0.0.1:6379(bioProcessBackgroundJobs+0x1e1)[0x53a0f1]\n/lib64/libc.so.6(+0x8a19a)[0x7fad0048a19a]\n/lib64/libc.so.6(+0x10f210)[0x7fad0050f210]\n\n1027242 bio_lazy_free\n/lib64/libc.so.6(+0x8722a)[0x7fad0048722a]\n/lib64/libc.so.6(pthread_cond_wait+0x132)[0x7fad00489782]\n/usr/local/bin/redis-server 127.0.0.1:6379(bioProcessBackgroundJobs+0x1e1)[0x53a0f1]\n/lib64/libc.so.6(+0x8a19a)[0x7fad0048a19a]\n/lib64/libc.so.6(+0x10f210)[0x7fad0050f210]\n\n1027239 redis-server *\n/lib64/libc.so.6(+0x3ebf0)[0x7fad0043ebf0]\n/usr/local/bin/redis-server 127.0.0.1:6379(dictSdsDestructor+0x8)[0x466e68]\n/usr/local/bin/redis-server 127.0.0.1:6379[0x5dc6fc]\n/usr/local/bin/redis-server 127.0.0.1:6379(kvstoreEmpty+0xe9)[0x468aa9]\n/usr/local/bin/redis-server 127.0.0.1:6379(emptyDbStructure+0x6d)[0x4ac46d]\n/usr/local/bin/redis-server 127.0.0.1:6379[0x5def06]\n/usr/local/bin/redis-server 127.0.0.1:6379(rdbLoadWithEmptyFunc+0xd3)[0x4cb203]\n/usr/local/bin/redis-server 127.0.0.1:6379(readSyncBulkPayload+0xa8d)[0x4bc04d]\n/usr/local/bin/redis-server 127.0.0.1:6379[0x5af4b1]\n/usr/local/bin/redis-server 127.0.0.1:6379(aeMain+0xea)[0x4638aa]\n/usr/local/bin/redis-server 127.0.0.1:6379(main+0x43f)[0x457b0f]\n/lib64/libc.so.6(+0x295d0)[0x7fad004295d0]\n/lib64/libc.so.6(__libc_start_main+0x80)[0x7fad00429680]\n/usr/local/bin/redis-server 127.0.0.1:6379(_start+0x25)[0x4591d5]\n\n6/6 expected stacktraces.\n\n------ STACK TRACE DONE ------\n\n------ REGISTERS ------\n1027239:S 12 Aug 2025 10:41:05.053 #\nRAX:00007facca831488 RBX:00007facca831488\nRCX:0000000000466e60 RDX:00000000a7e532a0\nRDI:5f726f7461726570 RSI:5f726f7461726570\nRBP:00007ffdd12d1e00 RSP:00007ffdd12d1db8\nR8 :00007fad00000f70 R9 :00007fad00bf3780\nR10:00007fad00000f78 R11:0000000000000002\nR12:8f4c00f40569736d R13:0000000000000000\nR14:000000000054d541 R15:00007facca4d8000\nRIP:0000000000466e68 EFL:0000000000010202\nCSGSFS:002b000000000033\n1027239:S 12 Aug 2025 10:41:05.053 # (00007ffdd12d1dc7) -> 000000000000003f\n1027239:S 12 Aug 2025 10:41:05.053 # (00007ffdd12d1dc6) -> 000000000093d642\n1027239:S 12 Aug 2025 10:41:05.053 # (00007ffdd12d1dc5) -> 0000000000000004\n1027239:S 12 Aug 2025 10:41:05.053 # (00007ffdd12d1dc4) -> 00007fad000c6150\n1027239:S 12 Aug 2025 10:41:05.053 # (00007ffdd12d1dc3) -> 00007fad0000c0f8\n1027239:S 12 Aug 2025 10:41:05.053 # (00007ffdd12d1dc2) -> 0000000000468aa9\n1027239:S 12 Aug 2025 10:41:05.053 # (00007ffdd12d1dc1) -> 00007ffdd12d1e40\n1027239:S 12 Aug 2025 10:41:05.053 # (00007ffdd12d1dc0) -> 0000000000000000\n1027239:S 12 Aug 2025 10:41:05.053 # (00007ffdd12d1dbf) -> 0000000000000000\n1027239:S 12 Aug 2025 10:41:05.053 # (00007ffdd12d1dbe) -> 00000000004b9150\n1027239:S 12 Aug 2025 10:41:05.053 # (00007ffdd12d1dbd) -> 00007fad00113400\n1027239:S 12 Aug 2025 10:41:05.053 # (00007ffdd12d1dbc) -> 00007facca4d8000\n1027239:S 12 Aug 2025 10:41:05.053 # (00007ffdd12d1dbb) -> 00000000000001b6\n1027239:S 12 Aug 2025 10:41:05.053 # (00007ffdd12d1dba) -> 00000000004b9150\n1027239:S 12 Aug 2025 10:41:05.053 # (00007ffdd12d1db9) -> 3832313138323730\n1027239:S 12 Aug 2025 10:41:05.053 # (00007ffdd12d1db8) -> 00000000005dc6fc\n\n\n------ CONFIG DEBUG OUTPUT ------\nio-threads 3\nrepl-diskless-load disabled\nrepl-diskless-sync no\nreplica-read-only yes\nlazyfree-lazy-expire no\nslave-read-only yes\nproto-max-bulk-len 512mb\nlist-compress-depth 0\nlazyfree-lazy-eviction no\nlazyfree-lazy-user-del no\nactivedefrag yes\nlazyfree-lazy-server-del no\nsanitize-dump-payload no\nclient-query-buffer-limit 512mb\nlazyfree-lazy-user-flush no\n\n------ FAST MEMORY TEST ------\n1027239:S 12 Aug 2025 10:41:05.053 # Bio worker thread #0 terminated\n1027239:S 12 Aug 2025 10:41:05.053 # Bio worker thread #1 terminated\n1027239:S 12 Aug 2025 10:41:05.053 # Bio worker thread #2 terminated\n1027239:S 12 Aug 2025 10:41:05.053 # IO thread(tid:140380968306240) terminated\n1027239:S 12 Aug 2025 10:41:05.054 # IO thread(tid:140380959913536) terminated\n*** Preparing to test memory region 792000 (487424 bytes)\n*** Preparing to test memory region 1a34000 (135168 bytes)\n*** Preparing to test memory region 7fabfa000000 (8388608 bytes)\n*** Preparing to test memory region 7fabfa800000 (4185915392 bytes)\n*** Preparing to test memory region 7facf4000000 (135168 bytes)\n*** Preparing to test memory region 7facf82ff000 (22020096 bytes)\n*** Preparing to test memory region 7facf9800000 (8388608 bytes)\n*** Preparing to test memory region 7facfa000000 (8388608 bytes)\n*** Preparing to test memory region 7facfa9f9000 (18874368 bytes)\n*** Preparing to test memory region 7facfbbfa000 (8388608 bytes)\n*** Preparing to test memory region 7facfc3fb000 (8388608 bytes)\n*** Preparing to test memory region 7facfcbfc000 (8388608 bytes)\n*** Preparing to test memory region 7facfd3fd000 (8388608 bytes)\n*** Preparing to test memory region 7facfdbfe000 (8388608 bytes)\n*** Preparing to test memory region 7facfe3ff000 (8388608 bytes)\n*** Preparing to test memory region 7facff400000 (8388608 bytes)\n*** Preparing to test memory region 7fad00000000 (4194304 bytes)\n*** Preparing to test memory region 7fad005fb000 (53248 bytes)\n*** Preparing to test memory region 7fad00a26000 (12288 bytes)\n*** Preparing to test memory region 7fad00bf1000 (28672 bytes)\n*** Preparing to test memory region 7fad00cf8000 (8192 bytes)\n.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O\nFast memory test PASSED, however your memory can still be broken. Please run a memory test for several hours if possible.\n\n------ DUMPING CODE AROUND EIP ------\nSymbol: dictSdsDestructor (base: 0x466e60)\nModule: /usr/local/bin/redis-server 127.0.0.1:6379 (base 0x400000)\n$ xxd -r -p /tmp/dump.hex /tmp/dump.bin\n$ objdump --adjust-vma=0x466e60 -D -b binary -m i386:x86-64 /tmp/dump.bin\n------\n1027239:S 12 Aug 2025 10:41:13.295 # dump of function (hexdump of 136 bytes):\n4889f74885f674300fb646ff83e0073c047e0de958b601000f1f8400000000000fb6c048630485c05e69004829c7e93db601000f1f440000c30f1f800000000049b873657479626465744889f8488b3d24a7320048b96172656e6567796c48be6d6f646e61726f6448330d01a7320048333502a7320048ba75657370656d6f734831c7483315e6a6\n\n=== REDIS BUG REPORT END. Make sure to include from START to END. ===\n\n```\n\n**Additional information**\n\nOS distribution and version: Rocky Linux release 9.6 (Blue Onyx)\nKernel: Linux 5.14.0-570.22.1.el9_6.x86_64\n\nThis is the slave bug report, master didn't crash. \nWhen I found it is crash, I try to restart it but it crash at the same place. \nBelow is the last log before it crash. \n\n```\n1027239:S 12 Aug 2025 10:40:39.502 * Full resync from master: 403f5eadaf40b46fa5a90ccde54b462606b52604:15123965342078\n1027239:S 12 Aug 2025 10:40:51.718 * MASTER <-> REPLICA sync: receiving 1190754337 bytes from master to disk\n1027239:S 12 Aug 2025 10:41:01.847 * MASTER <-> REPLICA sync: Loading DB in memory\n1027239:S 12 Aug 2025 10:41:01.848 * MASTER <-> REPLICA sync: Flushing old data\n```\n\n",
      "solution": "@onghongyao can you try this fix: https://github.com/redis/redis/pull/14274, thx.",
      "labels": [
        "crash report"
      ],
      "created_at": "2025-08-12T03:27:46Z",
      "closed_at": "2025-08-15T03:25:04Z",
      "url": "https://github.com/redis/redis/issues/14267",
      "comments_count": 8
    },
    {
      "tech": "redis",
      "repo": "redis/redis",
      "issue_number": 13257,
      "title": "[NEW] Support for Ubuntu 24.04",
      "problem": "**The problem/use-case that the feature addresses & description of the feature**\r\n\r\nWith the new release of 24.04 it would be great if we could get apt package support\r\nCurrently: \r\nE: Failed to fetch https://packages.redis.io/deb/dists/noble/InRelease  403  Forbidden [IP: 108.158.20.6 443]\r\nE: The repository 'https://packages.redis.io/deb noble InRelease' is not signed.\r\nN: Updating from such a repository can't be done securely, and is therefore disabled by default.\r\nN: See apt-secure(8) manpage for repository creation and user configuration details.\r\n\r\n\r\n**Alternatives you've considered**\r\n\r\nInstalling via snap works, but is not as desirable in my use case.\r\n\r\n\r\n",
      "solution": "This is fixed and the issue here can be closed.\r\n\r\n<img width=\"388\" alt=\"image\" src=\"https://github.com/redis/redis/assets/2775739/79dd630d-c168-46fb-98d3-39d7fcf150fa\">\r\n",
      "labels": [],
      "created_at": "2024-05-10T12:33:23Z",
      "closed_at": "2024-05-27T00:37:22Z",
      "url": "https://github.com/redis/redis/issues/13257",
      "comments_count": 4
    },
    {
      "tech": "redis",
      "repo": "redis/redis",
      "issue_number": 14196,
      "title": "[BUG] Frequent test TIMEOUTs in 7.2, 7.4 and 8.0",
      "problem": "I am experiencing frequent test TIMEOUTs in version lines 7.2, 7.4 and 8.0, when I run them in Gentoo test environment. I noticed that after the latest security release of versions 7.2.10, 7.4.5 and 8.0.3. I don't think that I experienced the similar timeouts when I was testing the previous security batch, when 7.2.9, 7.4.4 and 8.0.2 was released but they are now also affected. `grep -C5 TIMEOUT redis-{7.2.10,7.4.5,8.0.3}/temp/build.log` gives me this:\n```\nredis-7.2.10/temp/build.log-[ignore]: hash with one huge field: large memory flag not provided\nredis-7.2.10/temp/build.log-[91/91 done]: violations (1 seconds)\nredis-7.2.10/temp/build.log-Testing solo test\nredis-7.2.10/temp/build.log-[ok]: Active defrag (46825 ms)\nredis-7.2.10/temp/build.log-[skip]: Active defrag eval scripts\nredis-7.2.10/temp/build.log:[TIMEOUT]: clients state report follows.\nredis-7.2.10/temp/build.log-sock561b0c21d610 => (IN PROGRESS) Active defrag big keys\nredis-7.2.10/temp/build.log-Killing still running Redis server 1751\nredis-7.2.10/temp/build.log-Killing still running Redis server 1797\nredis-7.2.10/temp/build.log-Killing still running Redis server 1794\nredis-7.2.10/temp/build.log-Killing still running Redis server 1967\n--\nredis-7.2.10/temp/build.log-  0 seconds - bitops-large-memory\nredis-7.2.10/temp/build.log-  1 seconds - violations\nredis-7.2.10/temp/build.log-\nredis-7.2.10/temp/build.log-!!! WARNING The following tests failed:\nredis-7.2.10/temp/build.log-\nredis-7.2.10/temp/build.log:*** [TIMEOUT]: clients state report follows.\nredis-7.2.10/temp/build.log-Cleanup: may take some time... OK\nredis-7.2.10/temp/build.log- * ERROR: dev-db/redis-7.2.10::gentoo failed (test phase):\nredis-7.2.10/temp/build.log- *   Failed to run command: ./runtest\nredis-7.2.10/temp/build.log- *\nredis-7.2.10/temp/build.log- * Call stack:\n--\nredis-7.4.5/temp/build.log-[ok]: Active defrag big keys: cluster (28088 ms)\nredis-7.4.5/temp/build.log-[ok]: Active defrag pubsub: cluster (35031 ms)\nredis-7.4.5/temp/build.log-[ok]: Active Defrag HFE: cluster (8165 ms)\nredis-7.4.5/temp/build.log-[ok]: Active defrag main dictionary: standalone (48346 ms)\nredis-7.4.5/temp/build.log-[ok]: Active defrag eval scripts: standalone (5361 ms)\nredis-7.4.5/temp/build.log:[TIMEOUT]: clients state report follows.\nredis-7.4.5/temp/build.log-sock5555f20636e0 => (IN PROGRESS) Active defrag big keys: standalone\nredis-7.4.5/temp/build.log-Killing still running Redis server 20333\nredis-7.4.5/temp/build.log-\nredis-7.4.5/temp/build.log-                   The End\nredis-7.4.5/temp/build.log-\n--\nredis-7.4.5/temp/build.log-  162 seconds - integration/replication-psync\nredis-7.4.5/temp/build.log-  1 seconds - bitops-large-memory\nredis-7.4.5/temp/build.log-\nredis-7.4.5/temp/build.log-!!! WARNING The following tests failed:\nredis-7.4.5/temp/build.log-\nredis-7.4.5/temp/build.log:*** [TIMEOUT]: clients state report follows.\nredis-7.4.5/temp/build.log-Cleanup: may take some time... OK\nredis-7.4.5/temp/build.log- * ERROR: dev-db/redis-7.4.5::gentoo failed (test phase):\nredis-7.4.5/temp/build.log- *   Failed to run command: ./runtest\nredis-7.4.5/temp/build.log- *\nredis-7.4.5/temp/build.log- * Call stack:\n--\nredis-8.0.3/temp/build.log-[ignore]: SETBIT values larger than UINT32_MAX and lzf_compress/lzf_decompress correctly: large memory flag not provided\nredis-8.0.3/temp/build.log-[95/95 done]: bitops-large-memory (0 seconds)\nredis-8.0.3/temp/build.log-Testing solo test\nredis-8.0.3/temp/build.log-[ok]: Active defrag main dictionary: cluster (53920 ms)\nredis-8.0.3/temp/build.log-[ok]: Active defrag eval scripts: cluster (7182 ms)\nredis-8.0.3/temp/build.log:[TIMEOUT]: clients state report follows.\nredis-8.0.3/temp/build.log-sock55bd580abee0 => (IN PROGRESS) Active defrag big keys: cluster\nredis-8.0.3/temp/build.log-Killing still running Redis server 23878\nredis-8.0.3/temp/build.log-\nredis-8.0.3/temp/build.log-                   The End\nredis-8.0.3/temp/build.log-\n--\nredis-8.0.3/temp/build.log-  240 seconds - integration/replication-psync\nredis-8.0.3/temp/build.log-  0 seconds - bitops-large-memory\nredis-8.0.3/temp/build.log-\nredis-8.0.3/temp/build.log-!!! WARNING The following tests failed:\nredis-8.0.3/temp/build.log-\nredis-8.0.3/temp/build.log:*** [TIMEOUT]: clients state report follows.\nredis-8.0.3/temp/build.log-Cleanup: may take some time... OK\nredis-8.0.3/temp/build.log- * ERROR: dev-db/redis-8.0.3::gentoo failed (test phase):\nredis-8.0.3/temp/build.log- *   Failed to run command: ./runtest\nredis-8.0.3/temp/build.log- *\nredis-8.0.3/temp/build.log- * Call stack:\n```\nInterestingly, 6.2.19 versions is not affected, therefore, I bisected the code between 8.0.3 and 6.2.19. The git bisect pointed to the commit 98b3f52599cc - `add test suite infra to test RESP3 attributes (#10247)` and tests pass if I revert this change in 8.0.3. The tests are executed with following parameters in gentoo ebuild:\n```sh\n./runtest --clients 16 --skiptest '/Active defrag for argv retained by the main thread from IO thread.*' --skipunit unit/oom-score-adj --skiptest 'CONFIG SET rollback on apply error' --tls\n```\nand it is related to `--tls` parameter, because tests pass without it.",
      "solution": "@arkamar thanks a lot, this thing has been bothering me for a long time, and I still don't know the root cause.\nThe reason why 6.x is ok might be that https://github.com/redis/redis/pull/10247 was introduced since 7.0.\n\n---\n\nThis regression was introduced in https://github.com/torvalds/linux/commit/8c670bdfa58e48abad1d5b6ca1ee843ca91f7303 commit, which is part of linux kernel 6.14. It is a solution for [CVE-2025-21710](https://lore.kernel.org/all/2025022644-CVE-2025-21710-5e28@gregkh/). I guess [this thread](https://lore.kernel.org/all/CADVnQyn=MXohOf1vskJcm9VTOeP31Y5AqCPu7B=zZuTB8nh8Eg@mail.gmail.com/) might be also related as it describes what is most probably happening, but I didn't investigated it deeper.\n\n---\n\n@arkamar thanks for your help.\nTest `Active defrag big list` still needs to be fixed. I saw in your initial comment that it was Test `Active defrag big keys`, which was fixed by https://github.com/redis/redis/pull/14217",
      "labels": [],
      "created_at": "2025-07-14T19:57:21Z",
      "closed_at": "2025-07-31T06:08:04Z",
      "url": "https://github.com/redis/redis/issues/14196",
      "comments_count": 21
    },
    {
      "tech": "redis",
      "repo": "redis/redis",
      "issue_number": 14137,
      "title": "[BUG] cluster node config has been corrupted",
      "problem": "**Describe the bug**\n\ncluster nod has been corrupted. In node Id, I has a extra digit and d is missing from _connected_  text. This is in 7.2.5\n\n```\n[::1]:6379> cluster nodes\n3c4ab1c4d7196e9f4eb0a280647a1440a4ea7cc0 240b:c0e0:201:5ec9:b434:2:0:ffdb:6379@16379 slave b69cbb8075a6ea0e84009f978a0c66cbbf7a105 0 1750157496522 8 connected\n1931d7397151a679c021d12bcbd8081319e70a10 240b:c0e0:201:5ec9:b434:2:0:fbeb:6379@16379 myself,slave c8c3a7cd496a838d4e73a2b3f110fe177c8e079 0 1750157495000 6 connected\n65f3183cf5f56504825facb1a16d485d1e2d1e3f 240b:c0e0:201:5ec9:b434:2:0:a0f9:6379@16379 master - 0 1750157496000 2 connecte 5461-10922\nb69cbb8075a6ea0e84009f978a0c626cbbf7a105 240b:c0e0:201:5ec9:b434:2:0:d3b3:6379@16379 master - 0 1750157496000 8 connecte 10923-16383\nc8c3a7cd496a838d4e73a27b3f110fe177c8e079 240b:c0e0:201:5ec9:b434:2:0:badb:6379@16379 master - 0 1750157496000 6 connecte 0-5460\n6904a2d146af11e4ca53d88747997021769d4710 240b:c0e0:201:5ec9:b434:2:0:9517:6379@16379 slave 65f3183cf5f56504825facb1a16d45d1e2d1e3f 0 1750157496823 2 connected\n```\n\n**To reproduce**\n\nSteps to reproduce the behavior and/or a minimal code sample.\n\n**Expected behavior**\n\nA description of what you expected to happen.\n\n**Additional information**\n\nAny additional information that is relevant to the problem.\n",
      "solution": "@SarthakSahu there is a bug that might cause the cluster node config to be corrupted, you can upgrade to 7.2.7 to fix it.\nFixed by: https://github.com/redis/redis/pull/13468\nhttps://github.com/redis/redis/pull/13877",
      "labels": [],
      "created_at": "2025-06-19T06:05:05Z",
      "closed_at": "2025-07-28T13:46:35Z",
      "url": "https://github.com/redis/redis/issues/14137",
      "comments_count": 3
    },
    {
      "tech": "redis",
      "repo": "redis/redis",
      "issue_number": 13994,
      "title": "[QUESTION]",
      "problem": "In redis/src/cli_common.c\n`ssize_t cliWriteConn(redisContext *c, const char *buf, size_t buf_len)\n{\n    int done = 0;\n\n    /* Append data to buffer which is *usually* expected to be empty\n     * but we don't assume that, and write.\n     */\n    c->obuf = sdscatlen(c->obuf, buf, buf_len);\n    if (redisBufferWrite(c, &done) == REDIS_ERR) {\n        if (!(c->flags & REDIS_BLOCK))\n            errno = EAGAIN;\n\n        /* On error, we assume nothing was written and we roll back the\n         * buffer to its original state.\n         */\n        if (sdslen(c->obuf) > buf_len)\n            sdsrange(c->obuf, 0, -(buf_len+1));\n        else\n            sdsclear(c->obuf);\n\n        return -1;\n    }`\nline 131 might return a null value to `c->obuf`\nif the if branch in line 132 can be triggered, in line 139, `c->obuf` may be dereference in `redis/deps/hiredis/sds.h` line 94.\nThis may cause a NPD problem.",
      "solution": "Thanks for your issue.\n\nIf you\u2019d like, you can try sharing the relevant code like this \u2014 it can help make the discussion a bit clearer:\n1. Navigate to the source file on GitHub.\n2. Click the line number you\u2019d like to reference, then click \u201cCopy permalink\u201d.\nExample:\n<img width=\"987\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/fa156327-79a6-4dea-80b9-e9a7320dd26f\" />\n\n\nThen the permalink like this:`https://github.com/redis/redis/blob/de16bee70a7533119f45c8f7b62a36861f1b2193/src/cli_common.c#L124`\n\nIt can be displayed in the issue like this:https://github.com/redis/redis/blob/de16bee70a7533119f45c8f7b62a36861f1b2193/src/cli_common.c#L124\n\n\n\n\nIf you want to link multiple lines, you can select a range like this:\n`https://github.com/redis/redis/blob/de16bee70a7533119f45c8f7b62a36861f1b2193/src/cli_common.c#L124-L145`\nhttps://github.com/redis/redis/blob/de16bee70a7533119f45c8f7b62a36861f1b2193/src/cli_common.c#L124-L145\n\n\n\nSharing code this way can often make conversations easier to follow. Feel free to give it a try if you think it helps!\n",
      "labels": [],
      "created_at": "2025-04-30T12:06:32Z",
      "closed_at": "2025-07-22T13:03:18Z",
      "url": "https://github.com/redis/redis/issues/13994",
      "comments_count": 3
    },
    {
      "tech": "redis",
      "repo": "redis/redis",
      "issue_number": 14207,
      "title": "[BUG] Abnormal Expiration Time Update after Data Synchronization with redis-shake",
      "problem": "**Describe the bug**\n\nDuring the process of using the redis - shake tool to synchronize data from a Redis 3.0 cluster to a Redis 6.0 cluster, an abnormal situation regarding the update of expiration times has occurred.\nSpecifically, after the data is synchronized from the Redis 3.0 cluster to the Redis 6.0 cluster, when checking the expiration time of the data using the TTL command in the Redis 6.0 cluster environment, it appears that the expiration time has been successfully updated. However, in reality, the data is cleared according to the initially set expiration time, rather than being processed based on the updated expiration time.\n\nI'm curious about the possible underlying reasons. Why does the TTL command show normal and as expected, yet the actual result is different?\n\n**To reproduce**\n\nThe following is a detailed example:\nExecute the SETEX mykey 300 Hello command in the Redis 3.0 cluster. At this time, the expiration time of the mykey data is set to 5 minutes. After synchronization to the Redis 6.0 cluster via redis - shake, the data is displayed normally.\nTwo minutes later, execute SETEX mykey 300 Hello2 on the Redis 6.0 cluster to reset the expiration time of mykey to 5 minutes. When using the TTL command to check at this time, it shows that the expiration time is normal, with 5 minutes remaining.\nHowever, after 3 minutes, mykey is cleared. When mykey is newly added again on the Redis 6.0 cluster and the expiration time is set, the expiration time mechanism operates normally.\n\n\n**Additional information**\n\nEnvironment\n\u25cf RedisShake Version\uff1a4.3.0\n\u25cf Redis Source Version\uff1a3.2.12, cluster\n\u25cf Redis Destination Version\uff1a6.2.14, cluster",
      "solution": "The problem is solved. The correct sequence of events is:\n\n1. In the 3.0 cluster, execute \"SETEX mykey 300 Hello\".\n2. Use redis - shake to synchronize \"SETEX mykey 300 Hello\" to the 6.0 cluster.\n3. Renew the expiration time of \"mykey\" in the 6.0 cluster.\n4. The \"mykey\" in the 3.0 cluster expires, generating the \"del mykey\" command.\n5. Use redis - shake to synchronize the \"del mykey\" command to the 6.0 cluster.",
      "labels": [],
      "created_at": "2025-07-18T06:04:06Z",
      "closed_at": "2025-07-18T06:43:21Z",
      "url": "https://github.com/redis/redis/issues/14207",
      "comments_count": 1
    },
    {
      "tech": "redis",
      "repo": "redis/redis",
      "issue_number": 13885,
      "title": "Why not Consider Moving Client Handling Tasks to IO Threads for Simplified Implementation and Improved Performance",
      "problem": "cilentsCron:\n\n        if (clientsCronHandleTimeout(c,now)) continue;\n        if (clientsCronResizeQueryBuffer(c)) continue;\n        if (clientsCronFreeArgvIfIdle(c)) continue;\n        if (clientsCronResizeOutputBuffer(c,now)) continue;\n\n        if (clientsCronTrackExpansiveClients(c, curr_peak_mem_usage_slot)) continue;\n\nCurrently, Redis handles client-related tasks such as timeout checks, buffer resizing, and memory management within the main thread. While this design ensures efficiency and simplicity, it may lead to the main thread waiting for IO threads to pause, potentially impacting performance in certain scenarios.\n\nI suggest moving these client handling tasks to IO threads. This approach could offer several advantages:\n\n1. Simplified Main Thread: Reducing the load on the main thread by offloading non-critical, non-real-time tasks to IO threads could streamline its operations.\n2. Enhanced Performance: By distributing tasks across IO threads, we might achieve better utilization of system resources and reduce potential bottlenecks.\n3. Easier Implementation: Handling client-related tasks in IO threads could simplify the implementation by avoiding complex synchronization between the main thread and IO operations.\n\n\n",
      "solution": "The first thing i think you need to confirm is if the main thread handling `clientsCron` does bring performance degradation from benchmark, and how much?",
      "labels": [],
      "created_at": "2025-03-25T08:57:07Z",
      "closed_at": "2025-06-30T01:37:19Z",
      "url": "https://github.com/redis/redis/issues/13885",
      "comments_count": 30
    },
    {
      "tech": "redis",
      "repo": "redis/redis",
      "issue_number": 13256,
      "title": "[BUG] There is a memory leak defect at line 5981 of the file redis-cli.c in /redis/src/.",
      "problem": "**Describe the bug**\r\n\r\nA variable named \"slot_nodes\" is defined at line 5967 of the file redis-cli.c in /redis/src/. It allocates a block of dynamic memory using the listCreate function. When the if statement at line 5978 evaluates to true, the program jumps to the \"cleanup\" label at line 5981 using a goto statement. During this process, the dynamic memory allocated to slot_nodes is neither used as shown in line 5994 nor released at the \"cleanup\" label, resulting in a memory leak defect.as shown in the figure below:\r\nhttps://github.com/LuMingYinDetect/redis_defects/blob/main/redis_1.png\r\n\r\n**To reproduce**\r\n\r\nThe detection tool I'm using is the Clang Static Analyzer, which employs static analysis techniques. The tool's defect reports provide the path that triggers the defect. Based on the aforementioned path, the defect can be reproduced.\r\n\r\n**Expected behavior**\r\n\r\nIf the defect is confirmed, it is advisable to address it by making necessary fixes.\r\n",
      "solution": "@LuMingYinDetect a minor patch, if you want welcome to make PR for it, also i can do it if you need me.\r\n```diff\r\ndiff --git a/src/redis-cli.c b/src/redis-cli.c\r\nindex 0c9f088da..e8484956c 100644\r\n--- a/src/redis-cli.c\r\n+++ b/src/redis-cli.c\r\n@@ -5978,6 +5978,7 @@ static int clusterManagerFixSlotsCoverage(char *all_slots) {\r\n                 if (!clusterManagerCheckRedisReply(n, reply, NULL)) {\r\n                     fixed = -1;\r\n                     if (reply) freeReplyObject(reply);\r\n+                    if (slot_nodes) listRelease(slot_nodes);\r\n                     goto cleanup;\r\n                 }\r\n                 assert(reply->type == REDIS_REPLY_ARRAY);\r\n```\n\n---\n\n> @LuMingYinDetect a minor patch, if you want welcome to make PR for it, also i can do it if you need me.\r\n> \r\n> ```diff\r\n> diff --git a/src/redis-cli.c b/src/redis-cli.c\r\n> index 0c9f088da..e8484956c 100644\r\n> --- a/src/redis-cli.c\r\n> +++ b/src/redis-cli.c\r\n> @@ -5978,6 +5978,7 @@ static int clusterManagerFixSlotsCoverage(char *all_slots) {\r\n>                  if (!clusterManagerCheckRedisReply(n, reply, NULL)) {\r\n>                      fixed = -1;\r\n>                      if (reply) freeReplyObject(reply);\r\n> +                    if (slot_nodes) listRelease(slot_nodes);\r\n>                      goto cleanup;\r\n>                  }\r\n>                  assert(reply->type == REDIS_REPLY_ARRAY);\r\n> ```\r\n\r\nThank you for your patient explanation! I have submitted a pull request.\n\n---\n\nIssue seems fixed, is there a reason it's still open?",
      "labels": [],
      "created_at": "2024-05-10T08:56:34Z",
      "closed_at": "2025-06-05T01:45:49Z",
      "url": "https://github.com/redis/redis/issues/13256",
      "comments_count": 6
    },
    {
      "tech": "redis",
      "repo": "redis/redis",
      "issue_number": 14094,
      "title": "[BUG] Online Redis Repo Blocking Connections",
      "problem": "**Describe the bug**\n\nI'm trying to connect to the Redis Repo to download the latest files and Redis Cloudfront is blocking my connection.\n\n**To reproduce**\n\n`curl -vL https://packages.redis.io/rpm/redis-stable/redis.repo`\n`curl -vcurl -vL https://packages.redis.io/rpm/redis-stable/rhel8-x86_64/repodata/repomd.xml`\n\n**Expected behavior**\n\nAllow the connection to download the latest files.\n\n**Additional information**\n\n```\n*   Trying 18.154.144.29...\n* TCP_NODELAY set\n* Connected to packages.redis.io (18.154.144.29) port 443 (#0)\n* ALPN, offering h2\n* ALPN, offering http/1.1\n* successfully set certificate verify locations:\n*   CAfile: /etc/pki/tls/certs/ca-bundle.crt\n  CApath: none\n* TLSv1.3 (OUT), TLS handshake, Client hello (1):\n* TLSv1.3 (IN), TLS handshake, Server hello (2):\n* TLSv1.3 (IN), TLS handshake, [no content] (0):\n* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):\n* TLSv1.3 (IN), TLS handshake, [no content] (0):\n* TLSv1.3 (IN), TLS handshake, Certificate (11):\n* TLSv1.3 (IN), TLS handshake, [no content] (0):\n* TLSv1.3 (IN), TLS handshake, CERT verify (15):\n* TLSv1.3 (IN), TLS handshake, [no content] (0):\n* TLSv1.3 (IN), TLS handshake, Finished (20):\n* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):\n* TLSv1.3 (OUT), TLS handshake, [no content] (0):\n* TLSv1.3 (OUT), TLS handshake, Finished (20):\n* SSL connection using TLSv1.3 / TLS_AES_128_GCM_SHA256\n* ALPN, server accepted to use h2\n* Server certificate:\n*  subject: CN=packages.redis.io\n*  start date: Mar 21 00:00:00 2025 GMT\n*  expire date: Apr 18 23:59:59 2026 GMT\n*  subjectAltName: host \"packages.redis.io\" matched cert's \"packages.redis.io\"\n*  issuer: C=US; O=Amazon; CN=Amazon RSA 2048 M02\n*  SSL certificate verify ok.\n* Using HTTP2, server supports multi-use\n* Connection state changed (HTTP/2 confirmed)\n* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0\n* TLSv1.3 (OUT), TLS app data, [no content] (0):\n* TLSv1.3 (OUT), TLS app data, [no content] (0):\n* TLSv1.3 (OUT), TLS app data, [no content] (0):\n* Using Stream ID: 1 (easy handle 0x558f75e0b6f0)\n* TLSv1.3 (OUT), TLS app data, [no content] (0):\n> GET /rpm/redis-stable/redis.repo HTTP/2\n> Host: packages.redis.io\n> User-Agent: curl/7.61.1\n> Accept: */*\n> \n* TLSv1.3 (IN), TLS handshake, [no content] (0):\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\n* TLSv1.3 (IN), TLS app data, [no content] (0):\n* Connection state changed (MAX_CONCURRENT_STREAMS == 128)!\n* TLSv1.3 (OUT), TLS app data, [no content] (0):\n* TLSv1.3 (IN), TLS app data, [no content] (0):\n< HTTP/2 403 \n< content-type: application/xml\n< server: AmazonS3\n< date: Sat, 31 May 2025 13:11:04 GMT\n< x-cache: Error from cloudfront\n< via: 1.1 b0cf88fce5b426f643a724856a8060ea.cloudfront.net (CloudFront)\n< x-amz-cf-pop: LAX50-P4\n< x-amz-cf-id: kdbzd6_qs4Gii9Le8zI29641aKUbZm7J0liAj8zfQaSlrJRCq0rIsA==\n< \n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n* TLSv1.3 (IN), TLS app data, [no content] (0):\n* Connection #0 to host packages.redis.io left intact\n<Error><Code>AccessDenied</Code><Message>Access Denied</Message><RequestId>GKFDFQDYH1VQQ1GC</RequestId><HostId>fNDt5VZChGK/uFFYRnSPNAM0zFcjqddM8jPwNoarqpuzB6DqSGsoiJMNlKGmEB1E9yNJulXN71Y=</HostId></Error>\n```\n",
      "solution": "Fixed by using the new updated repository URL per the documentation",
      "labels": [],
      "created_at": "2025-05-31T13:14:14Z",
      "closed_at": "2025-06-01T11:55:36Z",
      "url": "https://github.com/redis/redis/issues/14094",
      "comments_count": 1
    },
    {
      "tech": "redis",
      "repo": "redis/redis",
      "issue_number": 13866,
      "title": "[BUG] redis/sentinel can probably run into tilt mode because long time waiting for dns reply",
      "problem": "Long time blocking DNS request can cause sentinel enter tile mode.\nDuring sentinel's DNS request, the libc function getaddrinfo(), can possibly block sentinel/redis thread for more than 2.5 seconds, which exceeds the SENTINEL_TILT_TRIGGER threshold of 2000 ms. As a result, sentinel tilts.\n\n\n\n```\n1:X 21 Aug 2024 16:19:15.621 # Failed to resolve hostname 'redis-persistent-master-0.redis-persistent-master-svc.service-software'\n1:X 21 Aug 2024 16:19:15.621 # +sdown master mymaster redis-persistent-master-0.redis-persistent-master-svc.service-software 6379\n1:X 21 Aug 2024 16:19:25.633 # Failed to resolve hostname 'redis-persistent-sentinel-1.redis-persistent-sentinel-svc.service-software'\n1:X 21 Aug 2024 16:19:25.634 # +sdown sentinel 89f6ad0dbb453989e9071edd6ada4ec15ac66c09 redis-persistent-sentinel-1.redis-persistent-sentinel-svc.service-software 26379 @ mymaster redis-persistent-master-0.redis-persistent-master-svc.service-software 6379\n1:X 21 Aug 2024 16:19:25.634 # +sdown sentinel e0e60aafa5fad733a7176e2d4c28d0f6d9106cf5 redis-persistent-sentinel-2.redis-persistent-sentinel-svc.service-software 26379 @ mymaster redis-persistent-master-0.redis-persistent-master-svc.service-software 6379\n1:X 21 Aug 2024 16:19:45.655 # +tilt #tilt mode entered\n1:X 21 Aug 2024 16:20:05.673 # Failed to resolve hostname 'redis-persistent-sentinel-2.redis-persistent-sentinel-svc.service-software'\n1:X 21 Aug 2024 16:20:05.763 # +tilt #tilt mode entered\n1:X 21 Aug 2024 16:20:35.821 # -tilt #tilt mode exited\n1:X 21 Aug 2024 16:20:59.379 # +sdown slave redis-persistent-master-2.redis-persistent-master-svc.service-software:6379 redis-persistent-master-2.redis-persistent-master-svc.service-software 6379 @ mymaster redis-persistent-master-0.redis-persistent-master-svc.service-software 6379\n1:X 21 Aug 2024 16:21:16.333 # -sdown slave redis-persistent-master-2.redis-persistent-master-svc.service-software:6379 redis-persistent-master-2.redis-persistent-master-svc.service-software 6379 @ mymaster redis-persistent-master-0.redis-persistent-master-svc.service-software 6379\n1:X 21 Aug 2024 16:21:20.529 # +sdown slave redis-persistent-master-2.redis-persistent-master-svc.service-software:6379 redis-persistent-master-2.redis-persistent-master-svc.service-software 6379 @ mymaster redis-persistent-master-0.redis-persistent-master-svc.service-software 6379\n\n```\n\n\n\n",
      "solution": "version 6.2.x @ShooterIT \nI find that getaddrinfo() still exists in branch unstable\nMaybe it can be fixed by solution of [libcares](https://github.com/c-ares/c-ares)\nIf needed, I can help\n\n---\n\nyes, `getaddrinfo()` is a synchronous blocking call, and it will block the calling thread until the DNS resolution is completed or times out, but i am not sure if we want to involve a new lib to resolve this issue. ping @oranagra ",
      "labels": [],
      "created_at": "2025-03-17T08:20:00Z",
      "closed_at": "2025-05-30T09:02:56Z",
      "url": "https://github.com/redis/redis/issues/13866",
      "comments_count": 5
    },
    {
      "tech": "redis",
      "repo": "redis/redis",
      "issue_number": 13067,
      "title": "Implement HSETXX ",
      "problem": "**The problem/use-case that the feature addresses**\r\n```\r\nlocal exists = redis.call('EXISTS', KEYS[1])\r\nif exists == 1 then\r\n    redis.call('HSET', KEYS[1], 'expire', ARGV[1])\r\nend\r\nreturn exists\r\n```\r\n\r\nA description of the problem that the feature will solve, or the use-case with which the feature will be used.\r\n```I'm currently using this script via EVAL. But I think the best option would be if we have the HSETXX command.```\r\n\r\n**Description of the feature**\r\n```HSET if keys exist and return true/false if keys exist```\r\n\r\nA description of what you want to happen.\r\n```HSETXX only for existing keys and return true/false if key exist```\r\n\r\n**Alternatives you've considered**\r\n```HSETNX, but it is only for non-existing keys```\r\n\r\nAny alternative solutions or features you've considered, including references to existing open and closed feature requests in this repository.\r\n```HSETXX searching do nothing```\r\n\r\n**Additional information**\r\n\r\nAny additional information that is relevant to the feature request.\r\n",
      "solution": "resolved by https://github.com/redis/redis/pull/13798",
      "labels": [],
      "created_at": "2024-02-19T14:39:54Z",
      "closed_at": "2025-05-21T22:24:50Z",
      "url": "https://github.com/redis/redis/issues/13067",
      "comments_count": 4
    },
    {
      "tech": "redis",
      "repo": "redis/redis",
      "issue_number": 14047,
      "title": "[BUG] in redis cluster, if an instance restart, it cannot use psync, but always full sync from master.",
      "problem": "**Describe the bug**\n\nin cluster-mode, if an instanse restart, it will generate an new master-replid, when it connect to the master, the requested master-replid mismatch the info replication of the master. so the master will do bgsave, and the slave always reload the data from the master by using full sync.\n\n**To reproduce**\n\nbuilld an redis cluster, such an using 3 masters and 3 slave as a cluster.  you can select an instance to restart, then analysis the master log. you can get mismath, and full sync information.\n\n**Expected behavior**\n\nwhen the instance restart, it can psync from master.\n\n**Additional information**\n\nAny additional information that is relevant to the problem.\n",
      "solution": "> could you show me the log of replica?\n\nthe version 5.0.3 has the bug. so i make install the 5.0.8 version. the psync log is ok. maybe it has be fixed. ",
      "labels": [],
      "created_at": "2025-05-15T08:33:28Z",
      "closed_at": "2025-05-20T02:50:11Z",
      "url": "https://github.com/redis/redis/issues/14047",
      "comments_count": 4
    },
    {
      "tech": "redis",
      "repo": "redis/redis",
      "issue_number": 13973,
      "title": "[BUG] used_memory_dataset can be exorbitantly higher than used_memory",
      "problem": "**Describe the bug**\n\nAfter running for a while, our redis cluster nodes are showing a giant number for `used_memory_dataset` and `used_memory_dataset_perc`.\n\n```\n> redis-cli info | grep used_memory\nused_memory:551167304\nused_memory_human:525.63M\nused_memory_rss:575311872\nused_memory_rss_human:548.66M\nused_memory_peak:556545744\nused_memory_peak_human:530.76M\nused_memory_peak_perc:99.03%\nused_memory_overhead:636860908\nused_memory_startup:7638912\nused_memory_dataset:18446744073623858012\nused_memory_dataset_perc:3393887928320.00%\nused_memory_lua:31744\nused_memory_vm_eval:31744\nused_memory_lua_human:31.00K\nused_memory_scripts_eval:0\nused_memory_vm_functions:32768\nused_memory_vm_total:64512\nused_memory_vm_total_human:63.00K\nused_memory_functions:296\nused_memory_scripts:296\nused_memory_scripts_human:296B\n```\n\n**To reproduce**\n\nThe clusters that are exhibiting this behavior have clients that are using the EVAL command, while our other clusters do not, so I would assume that is related.\n\n**Expected behavior**\n\nused_memory_dataset shouldn't be higher than the total memory, and used_memory_dataset_perc shouldn't be higher than 100%.\n\n**Additional information**\n\nRunning redis 7.4.2 in docker on an ubuntu 24.04 VM.\n\nThe number seems really close to the max for an unsigned long long, so maybe something is underflowing?\n",
      "solution": "```\n22) 1) \"overhead.hashtable.main\"\n    2) (integer) 47395384\n    3) \"overhead.hashtable.expires\"\n    4) (integer) 46884904\n```\nseems that the problem is here, the total number of keys is 45452, which means that the dict LUT overhead for single key is almost 1k.\n@nosammai are there any special operations? I saw that you enabled key eviction and maxmemory. Is there anything else I missed? thanks.",
      "labels": [],
      "created_at": "2025-04-23T15:38:40Z",
      "closed_at": "2025-05-07T08:45:25Z",
      "url": "https://github.com/redis/redis/issues/13973",
      "comments_count": 7
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 2365,
      "title": "Redisearch + Cluster seems unsupported",
      "problem": "**Version**: 4.3.4\r\n\r\n**Platform**: Python 3.10.6 on Archlinux with `5.19.5-arch1-1` kernel\r\n\r\n**Description**:\r\n\r\n<details><summary>\r\nDockerfile\r\n</summary>\r\n\r\n```dockerfile\r\nFROM redis/redis-stack-server:latest\r\n\r\nRUN mkdir \"/configs\" \\\r\n        \\\r\n\t&& echo \"port 7000\" >> \"/configs/7000\"  \\\r\n\t&& echo \"cluster-enabled yes\" >> \"/configs/7000\"  \\\r\n\t&& echo \"cluster-config-file /configs/7000nodes.conf\" >> \"/configs/7000\"  \\\r\n\t&& echo \"cluster-node-timeout 5000\" >> \"/configs/7000\"  \\\r\n\t&& echo \"appendonly yes\" >> \"/configs/7000\"  \\\r\n\t&& echo \"protected-mode no\" >> \"/configs/7000\"  \\\r\n\t\\\r\n\t&& echo \"port 7001\" >> \"/configs/7001\"  \\\r\n\t&& echo \"cluster-enabled yes\" >> \"/configs/7001\"  \\\r\n\t&& echo \"cluster-config-file /configs/7001nodes.conf\" >> \"/configs/7001\"  \\\r\n\t&& echo \"cluster-node-timeout 5000\" >> \"/configs/7001\"  \\\r\n\t&& echo \"appendonly yes\" >> \"/configs/7001\"  \\\r\n\t&& echo \"protected-mode no\" >> \"/configs/7001\"  \\\r\n\t\\\r\n\t&& echo \"port 7002\" >> \"/configs/7002\"  \\\r\n\t&& echo \"cluster-enabled yes\" >> \"/configs/7002\"  \\\r\n\t&& echo \"cluster-config-file /configs/7002nodes.conf\" >> \"/configs/7002\"  \\\r\n\t&& echo \"cluster-node-timeout 5000\" >> \"/configs/7002\"  \\\r\n\t&& echo \"appendonly yes\" >> \"/configs/7002\"  \\\r\n\t&& echo \"protected-mode no\" >> \"/configs/7002\"  \\\r\n\t\\\r\n\t&& echo \"port 7003\" >> \"/configs/7003\"  \\\r\n\t&& echo \"cluster-enabled yes\" >> \"/configs/7003\"  \\\r\n\t&& echo \"cluster-config-file /configs/7003nodes.conf\" >> \"/configs/7003\"  \\\r\n\t&& echo \"cluster-node-timeout 5000\" >> \"/configs/7003\"  \\\r\n\t&& echo \"appendonly yes\" >> \"/configs/7003\"  \\\r\n\t&& echo \"protected-mode no\" >> \"/configs/7003\"  \\\r\n\t\\\r\n\t&& echo \"port 7004\" >> \"/configs/7004\"  \\\r\n\t&& echo \"cluster-enabled yes\" >> \"/configs/7004\"  \\\r\n\t&& echo \"cluster-config-file /configs/7004nodes.conf\" >> \"/configs/7004\"  \\\r\n\t&& echo \"cluster-node-timeout 5000\" >> \"/configs/7004\"  \\\r\n\t&& echo \"appendonly yes\" >> \"/configs/7004\"  \\\r\n\t&& echo \"protected-mode no\" >> \"/configs/7004\"  \\\r\n\t\\\r\n\t&& echo \"port 7005\" >> \"/configs/7005\"  \\\r\n\t&& echo \"cluster-enabled yes\" >> \"/configs/7005\"  \\\r\n\t&& echo \"cluster-config-file /configs/7005nodes.conf\" >> \"/configs/7005\"  \\\r\n\t&& echo \"cluster-node-timeout 5000\" >> \"/configs/7005\"  \\\r\n\t&& echo \"appendonly yes\" >> \"/configs/7005\"  \\\r\n\t&& echo \"protected-mode no\" >> \"/configs/7005\"  \\\r\n\t\\\r\n\t&& echo \"#!/usr/bin/env bash\" >> \"/entrypoint\"  \\\r\n\t&& echo \"redis-server /configs/7000 --loadmodule /opt/redis-stack/lib/redisearch.so &\" >> \"/entrypoint\"  \\\r\n\t&& echo 'R7000=$!' >> \"/entrypoint\"  \\\r\n\t&& echo \"redis-server /configs/7001 --loadmodule /opt/redis-stack/lib/redisearch.so &\" >> \"/entrypoint\"  \\\r\n\t&& echo 'R7001=$!' >> \"/entrypoint\"  \\\r\n\t&& echo \"redis-server /configs/7002 --loadmodule /opt/redis-stack/lib/redisearch.so &\" >> \"/entrypoint\"  \\\r\n\t&& echo 'R7002=$!' >> \"/entrypoint\"  \\\r\n\t&& echo \"redis-server /configs/7003 --loadmodule /opt/redis-stack/lib/redisearch.so &\" >> \"/entrypoint\"  \\\r\n\t&& echo 'R7003=$!' >> \"/entrypoint\"  \\\r\n\t&& echo \"redis-server /configs/7004 --loadmodule /opt/redis-stack/lib/redisearch.so &\" >> \"/entrypoint\"  \\\r\n\t&& echo 'R7004=$!' >> \"/entrypoint\"  \\\r\n\t&& echo \"redis-server /configs/7005 --loadmodule /opt/redis-stack/lib/redisearch.so &\" >> \"/entrypoint\"  \\\r\n\t&& echo 'R7005=$!' >> \"/entrypoint\"  \\\r\n\t&& echo \"_term() {\" >> \"/entrypoint\" \\\r\n\t&& echo \"  echo 'Stopping redisearch cluster'\" >> \"/entrypoint\" \\\r\n\t&& echo '  kill -TERM \"$R7000\"' >> \"/entrypoint\" \\\r\n\t&& echo '  kill -TERM \"$R7001\"' >> \"/entrypoint\" \\\r\n\t&& echo '  kill -TERM \"$R7002\"' >> \"/entrypoint\" \\\r\n\t&& echo '  kill -TERM \"$R7003\"' >> \"/entrypoint\" \\\r\n\t&& echo '  kill -TERM \"$R7004\"' >> \"/entrypoint\" \\\r\n\t&& echo '  kill -TERM \"$R7005\"' >> \"/entrypoint\" \\\r\n\t&& echo \"}\" >> \"/entrypoint\" \\\r\n\t&& echo \"sleep 1\" >> \"/entrypoint\"  \\\r\n\t&& echo \"redis-cli --cluster create 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 --cluster-replicas 1 --cluster-yes\" >> \"/entrypoint\"  \\\r\n\t&& echo \"trap '_term' TERM\" >> \"/entrypoint\" \\\r\n\t&& echo \"trap '_term' INT\" >> \"/entrypoint\" \\\r\n\t&& echo \"wait -n\" >> \"/entrypoint\" \\\r\n\t&& echo 'exit $?' >> \"/entrypoint\" \\\r\n\t\\\r\n\t&& chmod 755 \"/entrypoint\"\r\n\r\nEXPOSE 7000\r\nEXPOSE 7001\r\nEXPOSE 7002\r\nEXPOSE 7003\r\nEXPOSE 7004\r\nEXPOSE 7005\r\n\r\nCMD [ \"/entrypoint\" ]\r\n\r\n```\r\n\r\n</details>\r\n\r\nFor the redis cluster I use a docker image build from the Dockerfile and start it with:\r\n\r\n```\r\ndocker run -p 7000-7005:7000-7005 [imagename]\r\n```\r\n\r\nSetup:\r\n\r\n```python\r\nfrom redis.cluster import RedisCluster, ClusterNode\r\nfrom redis.commands.search.indexDefinition import IndexDefinition, IndexType\r\nfrom redis.commands.search.field import TextField\r\n\r\nr = RedisCluster(startup_nodes=[\r\n    ClusterNode('localhost', '7000'),\r\n    ClusterNode('localhost', '7001'),\r\n    ClusterNode('localhost', '7002'),\r\n    ClusterNode('localhost', '7003'),\r\n    ClusterNode('localhost', '7004'),\r\n    ClusterNode('localhost', '7005')\r\n])\r\n```\r\n\r\nWhen I try to create a index the response is `b'OK'`, but you can see that there was an error. Here my IPython logs:\r\n\r\n```\r\nIn [2]: r.ft('foo').create_index([TextField('bar')])\r\nMovedError\r\nTraceback (most recent call last):\r\n  File \"path/venv/lib/python3.10/site-packages/redis/cluster.py\", line 1074, in _execute_command\r\n    response = redis_node.parse_response(connection, command, **kwargs)\r\n  File \"path/venv/lib/python3.10/site-packages/redis/client.py\", line 1254, in parse_response\r\n    response = connection.read_response()\r\n  File \"path/venv/lib/python3.10/site-packages/redis/connection.py\", line 839, in read_response\r\n    raise response\r\nredis.exceptions.MovedError: 12182 127.0.0.1:7002\r\nOut[2]: b'OK'\r\n```\r\n\r\nWhen I run `r.ft('foo').create_index([TextField('bar')])` again I get:\r\n\r\n```\r\n...\r\n\r\nResponseError: Index already exists\r\n```\r\n\r\nSo probably the first try worked as expected, but only prints an error.\r\n\r\nBy looking into the code I think that these two lines of `RedisCluster._determine_nodes` are causeing the issue because they always use the default node and not the node of slot of the index name:\r\n\r\nhttps://github.com/redis/redis-py/blob/e6cd4fdf3b159d7f118f154e28e884069da89d7e/redis/cluster.py#L836-L837\r\n\r\nWhen I removed these lines the error was gone, but I think the lines have to be there for a reason.\r\n\r\nI while looking into the code I also saw, that `Search.pipeline` always (for clusters and single servers) returns a `redis.commands.search.Pipeline` object, which does not inharit `ClusterPipeline`.\r\n\r\nBecause of this `r.ft('foo').pipeline()` fails. Because `Search.sugadd` uses this function, `r.ft('foo').sugadd('bar')` fails as well.",
      "solution": "Hi, has any resolved this issue? \nWhat understood about this issue is that it is basically occurring due to use of cluster Redis servers, when session key created on any node, due to change of node, it checks on different node, where key is not created, and through that error.\nHere is my error\n```\nredis.exceptions.MovedError: 3132 Ip_address:port\n```",
      "labels": [
        "stale",
        "cluster",
        "RediSearch"
      ],
      "created_at": "2022-09-01T18:32:45Z",
      "closed_at": "2024-03-14T17:33:16Z",
      "url": "https://github.com/redis/redis-py/issues/2365",
      "comments_count": 3
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3008,
      "title": "Overall revamp of connection pool, retries and timeouts",
      "problem": "I'm experiencing many issues (already reported here by other people) related with the (blocking) connection pool in the asyncio version.\r\n\r\nI had to rollback and pin my dependency version to 4.5.5 currently. (lablup/backend.ai#1620)\r\n\r\nSince there are already many reports, here I would instead suggest some high-level design suggestions:\r\n\r\n* The redis-py users should be able to distinguish:\r\n  - The connection is closed by myself.\r\n  - The connection is actively closed by the server after sending responses.\r\n  - The connection is abruptly closed by the server without sending responses.\r\n  - [The client is evicted due to a server-side limit.](https://redis.io/docs/reference/clients/#client-eviction)\r\n* Improvements for timeouts\r\n  - Distinguish:\r\n    - connection ready timeout in the blocking connection pool\r\n    - socket connection timeout\r\n    - response timeout\r\n  - These timeouts should be treated differently in the intrinsic retry subsystem and distinguishable by subclasses of `ConnectionError` and/or `TimeoutError` to ease writing user-defined retry mechanisms.\r\n    - e.g., Within the connection ready timeout, we should silenty wait and retry until we get a connection from a blocking connection pool.\r\n    - The response timeout for non-blocking (`GET`, `SET`, ...) and blocking commands (`BLPOP`, `XREAD`, ...) should be considered a different condition: active error vs. polling.\r\n      - It would be nice to have explicit examples/docs on how to write a polling loop around blocking commands with proper timeout and retry handling.\r\n      - Please refer: https://github.com/lablup/backend.ai/blob/833ed5477d57846e568b17fec35c82300111a519/src/ai/backend/common/redis_helper.py#L174\r\n    - #2807\r\n    - #2973\r\n    - #2663\r\n  - Maybe we could refer the design of [`aiohttp.ClientTimeout`](https://docs.aiohttp.org/en/stable/client_reference.html#aiohttp.ClientTimeout).\r\n* `BlockingConnectionPool` should be the default.\r\n  - `ConnectionPool`'s default `max_connections` should be a more reasonable number.\r\n    - #2220\r\n  - #2522\r\n  - #3034\r\n  - #3056\r\n  - There are issues to resolve first, though...\r\n    - #2995\r\n    - #2983\r\n    - #2998\r\n      - #2997\r\n      - #2859\r\n        - #2755\r\n          - #2749\r\n    - #2992\r\n      - #445\r\n    - #3124\r\n* Better connection pool design and abstraction to embrace underyling transport type differences and errors with connections\r\n  - #2523\r\n  - #2773\r\n  - #2832\r\n  - #2727\r\n  - #2636\r\n  - #2695\r\n  - #3000 (though this is a thread usecase)\r\n    - #2883\r\n  - #3014\r\n  - #3026\r\n  - #3043\r\n* The sentinel's connection pool should also have the blocking version.\r\n  - Currently there is no `BlockingSentinelConnectionPool`.\r\n  - #2956\r\n  - We need to clearly define whether the delay after connecting to the sentinel but before connecting to the target master/slave is included in the socket connection timeout or not.\r\n* The new `CLIENT SETINFO` mechanism should be generalized.\r\n  - #2682\r\n  - What if a failure occurs during sending this command?\r\n    - In my test cases which test retries with a sentinel master failover or a redis server restart, this `CLIENT SETINFO` breaks the retry semantic.\r\n    - Could we enforce the intrinsic/user-defined retry on such event?\r\n  - What if a user wants to insert additional commands like `CLIENT SETNAME` or `CLIENT NO-EVICT on`?\r\n  - Maybe we could refactor it as a connection-establishment callback, whose failure is ignored before disposing the faulty connection.\r\n    - #2980\r\n      - #2965\r\n* Backport policy for bug fixes\r\n  - Since I had to rollback to 4.5.5 after trying 4.6.0 \u2192 5.0.1 upgrade with connection leak experience, it would be nice to have a backport policy for critical bug fixes.",
      "solution": "I'm not sure how to fix these problems without breaking the existing users. It seems that many of the problems are intertwined and having a better design and abstraction of connection and connection pools would be the way to go.\r\n\r\nI understand that the list may look intimidating for the maintainers\u2014I'm not urging to fix them ASAP, but hoping that this list would be a guide to embrace the issues in a more holistic view.",
      "labels": [
        "stale"
      ],
      "created_at": "2023-10-16T03:31:45Z",
      "closed_at": "2026-01-19T00:25:53Z",
      "url": "https://github.com/redis/redis-py/issues/3008",
      "comments_count": 8
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3692,
      "title": "Async performance issue since version 5.3.0",
      "problem": "The unannounced introduction of `asyncio.Lock` when acquiring a connection from the pool in #3445 is causing noticeable performance degradation:\n\nhttps://github.com/redis/redis-py/blob/653d9ef00fc687e72c0bb065d2c61c664b13b63b/redis/asyncio/connection.py#L1095-L1105\n\nThis becomes especially evident under high load, where lock contention becomes a bottleneck. The issue affects version 5.3.0 and later. Others have also reported performance impacts, as noted in [#3624](https://github.com/redis/redis-py/issues/3624).\n\nI kind of understand the rationale behind this change, but personally, I would acquire the lock only to pop a connection and then performs `ensure_connection` outside the lock. Alternatively, if it didn\u2019t appear to cause issues in earlier versions, may be no lock at all?",
      "solution": "+1 for this issue. In the meantime, we found a solution wherein we warm up the redis pool on service startup:\n\n```python\nself.redis_pool = redis.ConnectionPool.from_url(...)\n\n# Create a Redis client using the connection pool\nself.redis_client = redis.Redis(connection_pool=self.redis_pool)\n\n# warm up to create all connections..\nconns = []\nfor _ in range(self.max_connections):\n    conn = await self.redis_pool.get_connection('warmup')\n    await conn.connect()  # actually opens the TCP connection\n    conns.append(conn)\n\n# Release all connections back to the pool\nfor conn in conns:\n    await self.redis_pool.release(conn)\n```\n\nThis way, the `ensure_connection` call returns immediately and the lock does not cause a lot of queue up.",
      "labels": [],
      "created_at": "2025-07-03T10:53:53Z",
      "closed_at": "2025-12-17T10:13:59Z",
      "url": "https://github.com/redis/redis-py/issues/3692",
      "comments_count": 2
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3450,
      "title": "Async retry needs to capture OSError exception in retry",
      "problem": "**Version**: What redis-py and what redis version is the issue happening on?\r\n\r\nredis-py 5.20\r\n\r\n**Platform**: What platform / version? (For example Python 3.5.1 on Windows 7 / Ubuntu 15.10 / Azure)\r\n\r\nFedora 40, Python 3.12\r\n\r\n**Description**: Description of your issue, stack traces from errors and code that reproduces the issue\r\n\r\nI tried the program for asyncio version from [Redis doc](https://redis.io/kb/doc/22wxq63j93/how-to-manage-client-reconnections-in-case-of-errors-with-redis-py) by taking down Redis server.\r\n\r\nThe first exception was raised is built-in `OSError` rather than `redis.exceptions.ConnectionError`.  The exception regarding to the lost connection from Redis sever behave differently between the asnycio version and the normal version.\r\n\r\n```Python\r\nimport asyncio\r\nimport redis.asyncio as redis\r\nfrom redis.asyncio.retry import Retry\r\nfrom redis.backoff import ExponentialBackoff\r\nfrom redis.exceptions import BusyLoadingError, ConnectionError, TimeoutError\r\n\r\nimport logging\r\n\r\n# Configure the logging module\r\nlogging.basicConfig(\r\n    format='%(asctime)s - %(levelname)s - %(message)s',\r\n    level=logging.INFO\r\n)\r\n\r\n\r\nasync def main():\r\n    logging.info(f\"Creating async Redis client...\")\r\n    r = await redis.from_url(\"redis://127.0.0.1\",\r\n                             retry=Retry(ExponentialBackoff(8, 1), 25),\r\n                             retry_on_error=[BusyLoadingError, ConnectionError, TimeoutError, ConnectionResetError, ])\r\n    logging.info(f\"Created async Redis client...\")\r\n    logging.info(f\"Redis client pinging...\")\r\n    await r.ping()\r\n    logging.info(f\"Redis client pinged!\")\r\n    logging.info(f\"Closing async Redis client...\")\r\n    await r.aclose()\r\n    logging.info(f\"Closed async Redis client...\")\r\n\r\n\r\n# start the asyncio program\r\nasyncio.run(main())\r\n```\r\n\r\n```log\r\n/home/Ricky/.virtualenv/pytool/bin/python /home/Ricky/private/repo/pytool/asyncio/demo_asyncio_redis_client_retry.py \r\n2024-12-05 14:18:09,802 - INFO - Creating async Redis client...\r\n2024-12-05 14:18:09,803 - INFO - Created async Redis client...\r\n2024-12-05 14:18:09,803 - INFO - Redis client pinging...\r\nTraceback (most recent call last):\r\n  File \"/home/Ricky/.virtualenv/pytool/lib/python3.12/site-packages/redis/asyncio/connection.py\", line 275, in connect\r\n    await self.retry.call_with_retry(\r\n  File \"/home/Ricky/.virtualenv/pytool/lib/python3.12/site-packages/redis/asyncio/retry.py\", line 59, in call_with_retry\r\n    return await do()\r\n           ^^^^^^^^^^\r\n  File \"/home/Ricky/.virtualenv/pytool/lib/python3.12/site-packages/redis/asyncio/connection.py\", line 691, in _connect\r\n    reader, writer = await asyncio.open_connection(\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib64/python3.12/asyncio/streams.py\", line 48, in open_connection\r\n    transport, _ = await loop.create_connection(\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib64/python3.12/asyncio/base_events.py\", line 1121, in create_connection\r\n    raise exceptions[0]\r\n  File \"/usr/lib64/python3.12/asyncio/base_events.py\", line 1103, in create_connection\r\n    sock = await self._connect_sock(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib64/python3.12/asyncio/base_events.py\", line 1006, in _connect_sock\r\n    await self.sock_connect(sock, address)\r\n  File \"/usr/lib64/python3.12/asyncio/selector_events.py\", line 651, in sock_connect\r\n    return await fut\r\n           ^^^^^^^^^\r\n  File \"/usr/lib64/python3.12/asyncio/selector_events.py\", line 691, in _sock_connect_cb\r\n    raise OSError(err, f'Connect call failed {address}')\r\nConnectionRefusedError: [Errno 111] Connect call failed ('127.0.0.1', 6379)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/Ricky/private/repo/pytool/asyncio/demo_asyncio_redis_client_retry.py\", line 31, in <module>\r\n    asyncio.run(main())\r\n  File \"/usr/lib64/python3.12/asyncio/runners.py\", line 194, in run\r\n    return runner.run(main)\r\n           ^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib64/python3.12/asyncio/runners.py\", line 118, in run\r\n    return self._loop.run_until_complete(task)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib64/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\r\n    return future.result()\r\n           ^^^^^^^^^^^^^^^\r\n  File \"/home/Ricky/private/repo/pytool/asyncio/demo_asyncio_redis_client_retry.py\", line 23, in main\r\n    await r.ping()\r\n  File \"/home/Ricky/.virtualenv/pytool/lib/python3.12/site-packages/redis/asyncio/client.py\", line 611, in execute_command\r\n    conn = self.connection or await pool.get_connection(command_name, **options)\r\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/Ricky/.virtualenv/pytool/lib/python3.12/site-packages/redis/asyncio/connection.py\", line 1058, in get_connection\r\n    await self.ensure_connection(connection)\r\n  File \"/home/Ricky/.virtualenv/pytool/lib/python3.12/site-packages/redis/asyncio/connection.py\", line 1091, in ensure_connection\r\n    await connection.connect()\r\n  File \"/home/Ricky/.virtualenv/pytool/lib/python3.12/site-packages/redis/asyncio/connection.py\", line 283, in connect\r\n    raise ConnectionError(self._error_message(e))\r\nredis.exceptions.ConnectionError: Error 111 connecting to 127.0.0.1:6379. Connect call failed ('127.0.0.1', 6379).\r\n\r\nProcess finished with exit code 1\r\n\r\n```",
      "solution": "Hi! Client instance accept a list of exceptions you want to Retry on, you can specify OSError there\n\n```\nretry_on_error: Optional[list] = None,\n```\n\nClosing issue for now. Let me know if it doesn't resolved\n\n---\n\nWhy was this even closed @vladvildanov ? The provided response does not address the issue at hand. This behaviour needs to be documented somewhere. ",
      "labels": [
        "needs-information"
      ],
      "created_at": "2024-12-05T19:27:55Z",
      "closed_at": "2025-12-16T07:25:33Z",
      "url": "https://github.com/redis/redis-py/issues/3450",
      "comments_count": 7
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3184,
      "title": "PubSub error when running in threads with redis-py 5.0.3",
      "problem": "pubsub.py:\r\n```python\r\nimport sys\r\nimport time\r\n\r\nimport redis\r\n\r\n# Global variable to indicate if the event handler is invoked\r\nevent_handler_invoked = False\r\n\r\n\r\ndef event_handler(message):\r\n    # Event handler function\r\n    global event_handler_invoked\r\n    print(\"Event handler invoked with message:\", message)\r\n    event_handler_invoked = True\r\n\r\n\r\ndef setup_pubsub():\r\n    # Function to set up Redis pub/sub\r\n    # Connect to Redis\r\n    redis_client = redis.Redis(host=\"localhost\", port=12000)\r\n\r\n    # Subscribe to a channel pattern\r\n    pubsub = redis_client.pubsub()\r\n    pubsub.psubscribe(**{'test_channel*': event_handler})\r\n\r\n    # Start a thread to handle messages\r\n    pubsub.run_in_thread(sleep_time=0.1)\r\n\r\n\r\ndef main():\r\n    setup_pubsub()\r\n\r\n    # Publish a test message\r\n    redis_client = redis.Redis(host=\"localhost\", port=12000)\r\n    redis_client.publish('test_channel', 'Hello, Redis!')\r\n\r\n    # Wait for the event handler to be invoked\r\n    timeout = 5  # Timeout in seconds\r\n    start_time = time.time()\r\n    while not event_handler_invoked:\r\n        if time.time() - start_time > timeout:\r\n            print(\"Timeout reached. Event handler not invoked.\")\r\n            sys.exit(1)\r\n        time.sleep(0.1)\r\n\r\n    print(\"Event handler successfully invoked.\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nIn redis-py 4.6.0 it works as expected:\r\n```bash\r\n$ python3 pubsub.py\r\nEvent handler invoked with message: {'type': 'pmessage', 'pattern': b'test_channel*', 'channel': b'test_channel', 'data': b'Hello, Redis!'}\r\nEvent handler successfully invoked.\r\n```\r\n\r\nWith redis-py 5.0.3 it constantly fails with:\r\n```bash\r\n$ python3 pubsub.py\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/my-project/venv3.8/lib/python3.8/site-packages/redis/_parsers/socket.py\", line 69, in _read_from_socket\r\n    buf.write(data)\r\nValueError: I/O operation on closed file.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\r\n    self.run()\r\n  File \"/home/ubuntu/my-project/venv3.8/lib/python3.8/site-packages/redis/client.py\", line 1162, in run\r\n    pubsub.get_message(ignore_subscribe_messages=True, timeout=sleep_time)\r\n  File \"/home/ubuntu/my-project/venv3.8/lib/python3.8/site-packages/redis/client.py\", line 1024, in get_message\r\n    response = self.parse_response(block=(timeout is None), timeout=timeout)\r\n  File \"/home/ubuntu/my-project/venv3.8/lib/python3.8/site-packages/redis/client.py\", line 835, in parse_response\r\n    response = self._execute(conn, try_read)\r\n  File \"/home/ubuntu//venv3.8/lib/python3.8/site-packages/redis/client.py\", line 811, in _execute\r\n    return conn.retry.call_with_retry(\r\n  File \"/home/ubuntu/my-project/venv3.8/lib/python3.8/site-packages/redis/retry.py\", line 46, in call_with_retry\r\n    return do()\r\n  File \"/home/ubuntu/my-project/venv3.8/lib/python3.8/site-packages/redis/client.py\", line 812, in <lambda>\r\n    lambda: command(*args, **kwargs),\r\n  File \"/home/ubuntu/my-project/venv3.8/lib/python3.8/site-packages/redis/client.py\", line 829, in try_read\r\n    if not conn.can_read(timeout=timeout):\r\n  File \"/home/ubuntu/my-project/venv3.8/lib/python3.8/site-packages/redis/connection.py\", line 490, in can_read\r\n    return self._parser.can_read(timeout)\r\n  File \"/home/ubuntu/my-project/venv3.8/lib/python3.8/site-packages/redis/_parsers/base.py\", line 128, in can_read\r\n    return self._buffer and self._buffer.can_read(timeout)\r\n  File \"/home/ubuntu/my-project/venv3.8/lib/python3.8/site-packages/redis/_parsers/socket.py\", line 95, in can_read\r\n    return bool(self.unread_bytes()) or self._read_from_socket(\r\n  File \"/home/ubuntu/my-project/venv3.8/lib/python3.8/site-packages/redis/_parsers/socket.py\", line 90, in _read_from_socket\r\n    buf.seek(current_pos)\r\nValueError: I/O operation on closed file.\r\n^CTraceback (most recent call last):\r\n  File \"pubsub.py\", line 54, in <module>\r\n    main()\r\n  File \"pubsub.py\", line 48, in main\r\n    time.sleep(0.1)\r\n```\r\n\r\nTested in Python3.8 and Python3.11, same result. ",
      "solution": "Running into a similar issue with Celery/Redis. Downgrading to 4.6.0 didn't help. For me, the issue triggers when I try to get the results of celery tasks\r\n\r\nStack trace when gathering celery task results.\r\n\r\n```python\r\n     results = await asyncio.gather(*[asyncio.to_thread(task.get) for task in tasks])\r\n   File \"/usr/local/lib/python3.9/asyncio/threads.py\", line 25, in to_thread\r\n     return await loop.run_in_executor(None, func_call)\r\n   File \"/usr/local/lib/python3.9/concurrent/futures/thread.py\", line 58, in run\r\n     result = self.fn(*self.args, **self.kwargs)\r\n   File \"/usr/local/lib/python3.9/site-packages/celery/result.py\", line 251, in get\r\n     return self.backend.wait_for_pending(\r\n   File \"/usr/local/lib/python3.9/site-packages/celery/backends/asynchronous.py\", line 221, in wait_for_pending\r\n     for _ in self._wait_for_pending(result, **kwargs):\r\n   File \"/usr/local/lib/python3.9/site-packages/celery/backends/asynchronous.py\", line 287, in _wait_for_pending\r\n     for _ in self.drain_events_until(\r\n   File \"/usr/local/lib/python3.9/site-packages/celery/backends/asynchronous.py\", line 54, in drain_events_until\r\n     yield self.wait_for(p, wait, timeout=interval)\r\n   File \"/usr/local/lib/python3.9/site-packages/celery/backends/asynchronous.py\", line 63, in wait_for\r\n     wait(timeout=timeout)\r\n   File \"/usr/local/lib/python3.9/site-packages/celery/backends/redis.py\", line 161, in drain_events\r\n     message = self._pubsub.get_message(timeout=timeout)\r\n   File \"/usr/local/lib/python3.9/site-packages/redis/client.py\", line 1690, in get_message\r\n     response = self.parse_response(block=(timeout is None), timeout=timeout)\r\n   File \"/usr/local/lib/python3.9/site-packages/redis/client.py\", line 1542, in parse_response\r\n     response = self._execute(conn, try_read)\r\n   File \"/usr/local/lib/python3.9/site-packages/redis/client.py\", line 1518, in _execute\r\n     return conn.retry.call_with_retry(\r\n  File \"/usr/local/lib/python3.9/site-packages/redis/retry.py\", line 46, in call_with_retry\r\n    return do()\r\n   File \"/usr/local/lib/python3.9/site-packages/redis/client.py\", line 1519, in <lambda>\r\n     lambda: command(*args, **kwargs),\r\n   File \"/usr/local/lib/python3.9/site-packages/redis/client.py\", line 1536, in try_read\r\n    if not conn.can_read(timeout=timeout):\r\n   File \"/usr/local/lib/python3.9/site-packages/redis/connection.py\", line 869, in can_read\r\n    return self._parser.can_read(timeout)\r\n   File \"/usr/local/lib/python3.9/site-packages/redis/connection.py\", line 344, in can_read\r\n     return self._buffer and self._buffer.can_read(timeout)\r\n   File \"/usr/local/lib/python3.9/site-packages/redis/connection.py\", line 242, in can_read\r\n     return bool(self.unread_bytes()) or self._read_from_socket(\r\n   File \"/usr/local/lib/python3.9/site-packages/redis/connection.py\", line 237, in _read_from_socket\r\n     buf.seek(current_pos)\r\nValueError: I/O operation on closed file.\r\n```\n\n---\n\nI think I'm having the same / similar problem (redis==5.2.0).\r\n\r\nMy scenario is a FastAPI application, where I listen to Redis PubSub messages in a separate asyncio thread. The error occurs if the application is run with more than 1 (uvicorn) workers. However the problem **occurs only sometimes / most of the times** (so maybe a nasty timing / concurrency issue).\r\n\r\nSee my stack trace here (line 14 has a comment with `# PROBLEM ...`):\r\n\r\n```python\r\n2024-11-24 17:34:38,799 - routers.myservice.mymodule.websocket_manager - INFO - initialize: end\r\n2024-11-24 17:34:38,799 - app - INFO - FastAPI service startup complete\r\nINFO:     Application startup complete.\r\n2024-11-24 17:34:38,801 - routers.myservice.mymodule.websocket_manager - ERROR - I/O operation on closed file.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.13/site-packages/redis/_parsers/socket.py\", line 68, in _read_from_socket\r\n    raise ConnectionError(SERVER_CLOSED_CONNECTION_ERROR)\r\nredis.exceptions.ConnectionError: Connection closed by server.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/routers/myservice/mymodule/websocket_manager.py\", line 88, in _redis_listening_thread_function\r\n    for redis_msg in _redis_pubsub.listen():  # PROBLEM occurs if app runs with more than 1 (uvicorn) worker, but only _sometimes_ (on FastAPI application startup)\r\n                     ~~~~~~~~~~~~~~~~~~~~^^\r\n  File \"/usr/local/lib/python3.13/site-packages/redis/client.py\", line 1026, in listen\r\n    response = self.handle_message(self.parse_response(block=True))\r\n                                   ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.13/site-packages/redis/client.py\", line 865, in parse_response\r\n    response = self._execute(conn, try_read)\r\n  File \"/usr/local/lib/python3.13/site-packages/redis/client.py\", line 841, in _execute\r\n    return conn.retry.call_with_retry(\r\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~^\r\n        lambda: command(*args, **kwargs),\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        lambda error: self._disconnect_raise_connect(conn, error),\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    )\r\n    ^\r\n  File \"/usr/local/lib/python3.13/site-packages/redis/retry.py\", line 62, in call_with_retry\r\n    return do()\r\n  File \"/usr/local/lib/python3.13/site-packages/redis/client.py\", line 842, in <lambda>\r\n    lambda: command(*args, **kwargs),\r\n            ~~~~~~~^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.13/site-packages/redis/client.py\", line 863, in try_read\r\n    return conn.read_response(disconnect_on_error=False, push_request=True)\r\n           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.13/site-packages/redis/connection.py\", line 592, in read_response\r\n    response = self._parser.read_response(disable_decoding=disable_decoding)\r\n  File \"/usr/local/lib/python3.13/site-packages/redis/_parsers/resp2.py\", line 15, in read_response\r\n    result = self._read_response(disable_decoding=disable_decoding)\r\n  File \"/usr/local/lib/python3.13/site-packages/redis/_parsers/resp2.py\", line 25, in _read_response\r\n    raw = self._buffer.readline()\r\n  File \"/usr/local/lib/python3.13/site-packages/redis/_parsers/socket.py\", line 115, in readline\r\n    self._read_from_socket()\r\n    ~~~~~~~~~~~~~~~~~~~~~~^^\r\n  File \"/usr/local/lib/python3.13/site-packages/redis/_parsers/socket.py\", line 90, in _read_from_socket\r\n    buf.seek(current_pos)\r\n    ~~~~~~~~^^^^^^^^^^^^^\r\nValueError: I/O operation on closed file.\r\n```\r\n\r\nI would be glad about any suggestions and/or ideas what's the reason and if this is intended behavior.\r\n\r\n~~I think I will now try to use [aioredis](https://github.com/aio-libs-abandoned/aioredis-py) (has async support) instead of redis-py (maybe I should use this anyway with FastAPI?). I hope that my stuff will work then.~~  \r\nEDIT: no, I won't \ud83d\ude06 (I only then noticed that this is archived and now integrated into redis-py.)\n\n---\n\nUpdate: for what it's worth, I solved my problem that I described in my comment above.\r\n\r\nTurns out the reason was that I did the `redis_pubsub = redis_client.pubsub()` and the `redis_pubsub.subscribe(\"channel\")` **outside the thread** at first. So probably different threads tried to access the same pubsub object or something, which could explain the `ValueError: I/O operation on closed file.`.\r\n\r\nAfter putting these things inside the thread, it works just like expected for me.\r\n\r\nI don't know if my experience here is any help to you at all, but I sure hope it is! (And I hope the errors are related at all. \ud83d\ude06)  \r\nGood luck with everything, and always focus on the fun part.",
      "labels": [
        "stale"
      ],
      "created_at": "2024-03-14T13:48:18Z",
      "closed_at": "2025-12-08T14:02:11Z",
      "url": "https://github.com/redis/redis-py/issues/3184",
      "comments_count": 5
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3555,
      "title": "Retry Mechanism Fails When Redis Container is Paused",
      "problem": "### Expected behavior\n\nWhen the Redis container is **paused** (not stopped), the connection attempt should fail, triggering the retry mechanism. The retry number should increase monotonically until the specified maximum number of retries is reached.\n\n#### Example logs of expected behavior:\n\n```logs\nINFO - Attempt 1/5. Backing off for 0.5 seconds\nINFO - Attempt 2/5. Backing off for 1.0 seconds\nINFO - Attempt 3/5. Backing off for 2.0 seconds\nINFO - Attempt 4/5. Backing off for 4.0 seconds\n```\n\nThe print statement was added at the end of the following `except`  block:\nhttps://github.com/redis/redis-py/blob/ea01a303ab54e9698689b796dc5c67644425ba51/redis/retry.py#L60-L70\n\n\n### Actual behavior\n\nInstead of progressing through the retry attempts, the retry mechanism gets stuck at the first attempt, repeating indefinitely.\n\n#### Example logs of actual behavior:\n```logs\nINFO - Attempt 1/5. Backing off for 0.5 seconds\nINFO - Attempt 1/5. Backing off for 0.5 seconds\nINFO - Attempt 1/5. Backing off for 0.5 seconds\nINFO - Attempt 1/5. Backing off for 0.5 seconds\n```\n\n### Root Cause\n\nThe issue occurs because the `sock.connect` (line 575 of the `_connect` method) succeeds even when the container is `paused`. However, subsequent read operations fail with `Timeout`.\nhttps://github.com/redis/redis-py/blob/ea01a303ab54e9698689b796dc5c67644425ba51/redis/connection.py#L728-L763 \n\n### Possible solution\n\nTo properly detect when the connection is truly established, we can send a `PING` command immediately after `connect()` and verify the response.\nAdd the following after `sock.connect()` to ensure the connection is functional:\n```python\nping_parts = self._command_packer.pack(\"PING\")\nfor part in ping_parts:\n    sock.sendall(part)\n\n    response = sock.recv(7)\n\n    if not str_if_bytes(response).startswith(\"+PONG\"):\n        raise OSError(f\"Redis handshake failed: unexpected response {response!r}\")\n```\n\n### Additional Comments\n\n- There may be a better way to handle the read operation for the `PING` response using existing methods, but calling `_send_ping` directly does not work in this case.\n\n- This issue also affects the **asynchronous version** of `redis-py`.\n\n-----\n\nLet me know if you'd like a clearer example to reproduce the behavior.",
      "solution": "Following up on this issue, I've noticed that it also occurs when a container is still booting up in the background, making the port available before the Redis instance has fully initialized.\n\nAn improved approach for my Proposed solution would look like this:\n```python\ntry:\n    ping_parts = self._command_packer.pack(\"PING\")\n    for part in ping_parts:\n        sock.sendall(part)\n\n    response = sock.recv(7)\n    assert str_if_bytes(response).startswith(\"+PONG\")\nexcept Exception:\n    raise OSError(f\"Redis handshake failed: {socket_address}\")\n```\nHowever, there are two small points that require consideration:\n1. I'm not entirely sure about the most appropriate error type to raise here. Using OSError might compel users to explicitly handle this exception in their retry logic (which might not be ideal for the async case where the retry says that supported errors should be of type `Tuple[Type[RedisError], ...]`).\n2. Although performing this PING check by default seems beneficial, it might be valuable to include a flag allowing users to disable this behavior, especially in scenarios where each message incurs a cost (similar to how the library currently handles setting lib name/version by default but allows disabling it\u2014see lines 509\u2013516).\n\n---\n\nHey @vladvildanov , I opened a PR with a solution to problem mentioned in the comment. if you want I can add you as a reviewer",
      "labels": [],
      "created_at": "2025-03-12T14:21:59Z",
      "closed_at": "2025-12-04T14:31:21Z",
      "url": "https://github.com/redis/redis-py/issues/3555",
      "comments_count": 4
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3203,
      "title": "Bug in retry-handling in the PubSub-client",
      "problem": "**Version**: \r\nredis-py: 5.0.3\r\nredis: 7.2.4\r\n\r\n**Platform**: \r\nPython 3.12.2 running on Debian bookworm\r\n\r\n**Description**: \r\nFirst off I want to say thanks for maintaining this library :) \r\n\r\nSo to the issue at hand:\r\nIt seems like the retry-functionality in the PubSub-client (both async and sync) is not working correctly. \r\nTo simulate a crash of redis I am closing and restarting redis while our system is running. Even though our connection is set up with an infinite amount of retry, the PubSub-client always crashes and never recovers.\r\n\r\nHaving dug though the source code I have found the pathways causing the crash.\r\nhttps://github.com/redis/redis-py/blob/1784b37f548076aca16d7236882c085428f9b23e/redis/asyncio/client.py#L885-L896\r\nThe `_execute`-function runs periodicially, and whenever redis is closed the command fails.\r\nThis causes the `_disconnect_raise_connect`-function to be called:\r\nhttps://github.com/redis/redis-py/blob/1784b37f548076aca16d7236882c085428f9b23e/redis/asyncio/client.py#L870-L883\r\n`_disconnect_raise_connect` closes the connection, `conn.retry_on_error` is `True` making the function skip raising error. \r\nThis is followed by an attempt at reconnecting.\r\nhttps://github.com/redis/redis-py/blob/1784b37f548076aca16d7236882c085428f9b23e/redis/asyncio/connection.py#L269-L284\r\nSince we are not currently connected, the first if-statement is `False`. A call with retry is made to the `_connect`-function.\r\nhttps://github.com/redis/redis-py/blob/1784b37f548076aca16d7236882c085428f9b23e/redis/asyncio/connection.py#L678-L700\r\nInside the `_connect`-function an attempt is made to connect to redis. Since redis is unavailable an `OSError` is created. `OSError` is not in the list of  `supported_errors` causing it to bypass the error handling in the `call_with_retry`-function and is propagated to the `connect`-function. Here it is converted to a `ConnectionError` and is propagated all the way to the top unhandled. \r\n\r\nIs this intended behaviour?\r\nIs this something that could be handled in a different way?\r\n\r\nWe have temporarily fixed this by adding `OSError` to the list of `supported_errors`, but this feels like quite a hacky solution.\r\n\r\n",
      "solution": "Hey,\n\nBased on your description, I believe I've encountered the same issue with the synchronous version (as you said).\n\n### Minimal Reproducible Example\n\nThe following pseudo-code snippet helps to reproduce the issue:\n```python\ndef event_handler(msg):\n    print(msg)\n\nredis_pub_sub.psubscribe(**{\"test\": event_handler})\n\ndef exception_handler(ex, pubsub, thread):\n    print(traceback.format_exc())\n\nredis_pub_sub.run_in_thread(sleep_time=0.1, exception_handler=exception_handler)\n\n# Close Redis connection\n```\n\n### Error Trace\n\n```log\nTraceback (most recent call last):\n  File \"/path/to/env/lib/python3.10/site-packages/redis/retry.py\", line 62, in call_with_retry\n    return do()\n  File \"/path/to/env/lib/python3.10/site-packages/redis/client.py\", line 842, in <lambda>\n    lambda: command(*args, **kwargs),\n  File \"/path/to/env/lib/python3.10/site-packages/redis/client.py\", line 859, in try_read\n    if not conn.can_read(timeout=timeout):\n  File \"/path/to/env/lib/python3.10/site-packages/redis/connection.py\", line 600, in can_read\n    return self._parser.can_read(timeout)\n  File \"/path/to/env/lib/python3.10/site-packages/redis/_parsers/hiredis.py\", line 79, in can_read\n    return self.read_from_socket(timeout=timeout, raise_on_timeout=False)\n  File \"/path/to/env/lib/python3.10/site-packages/redis/_parsers/hiredis.py\", line 90, in read_from_socket\n    raise ConnectionError(SERVER_CLOSED_CONNECTION_ERROR)\nredis.exceptions.ConnectionError: Connection closed by server.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/path/to/env/lib/python3.10/site-packages/redis/connection.py\", line 622, in read_response\n    response = self._parser.read_response(disable_decoding=disable_decoding)\n  File \"/path/to/env/lib/python3.10/site-packages/redis/_parsers/hiredis.py\", line 128, in read_response\n    self.read_from_socket()\n  File \"/path/to/env/lib/python3.10/site-packages/redis/_parsers/hiredis.py\", line 88, in read_from_socket\n    bufflen = self._sock.recv_into(self._buffer)\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/path/to/env/lib/python3.10/site-packages/redis/client.py\", line 1192, in run\n    pubsub.get_message(ignore_subscribe_messages=True, timeout=sleep_time)\n  File \"/path/to/env/lib/python3.10/site-packages/redis/client.py\", line 1054, in get_message\n    response = self.parse_response(block=(timeout is None), timeout=timeout)\n  File \"/path/to/env/lib/python3.10/site-packages/redis/client.py\", line 865, in parse_response\n    response = self._execute(conn, try_read)\n  File \"/path/to/env/lib/python3.10/site-packages/redis/client.py\", line 841, in _execute\n    return conn.retry.call_with_retry(\n  File \"/path/to/env/lib/python3.10/site-packages/redis/retry.py\", line 65, in call_with_retry\n    fail(error)\n  File \"/path/to/env/lib/python3.10/site-packages/redis/client.py\", line 843, in <lambda>\n    lambda error: self._disconnect_raise_connect(conn, error),\n  File \"/path/to/env/lib/python3.10/site-packages/redis/client.py\", line 831, in _disconnect_raise_connect\n    conn.connect()\n  File \"/path/to/env/lib/python3.10/site-packages/redis/connection.py\", line 370, in connect\n    self.on_connect()\n  File \"/path/to/env/lib/python3.10/site-packages/redis/connection.py\", line 501, in on_connect\n    self.read_response()\n  File \"/path/to/env/lib/python3.10/site-packages/redis/connection.py\", line 630, in read_response\n    raise ConnectionError(\nredis.exceptions.ConnectionError: Error while reading from localhost:6379 : (104, 'Connection reset by peer')\n```\n\n### Possible Cause\n\nIt seems that the issue in this version is more related to the `connect` method since the second part of the error trace occurs outside the retry scope.\n\n### Quick Patch\nTemporarily adding `OSError` to the list of `supported_errors` doesn't solve the problem, but a quick workaround is to wrap everything inside a retry call, as shown below. However, this might introduce unexpected side effects.\n```python\ndef connect(self):\n    \"\"\"Connects to the Redis server if not already connected.\"\"\"\n    if self._sock:\n        return\n\n    def _full_connect():\n        self._sock = self._connect()\n        if self.redis_connect_func is None:\n            # Use the default on_connect function\n            self.on_connect()\n        else:\n            # Use the passed function redis_connect_func\n            self.redis_connect_func(self)\n\n        # Run any user callbacks. Currently, the only internal callback\n        # is for pubsub channel/pattern resubscription.\n        # First, remove any dead weak references\n        self._connect_callbacks = [ref for ref in self._connect_callbacks if ref()]\n        for ref in self._connect_callbacks:\n            callback = ref()\n            if callback:\n                callback(self)\n\n    try:\n        sock = self.retry.call_with_retry(\n            lambda: _full_connect(),\n            lambda error: self.disconnect(error)\n        )\n    except socket.timeout:\n        raise TimeoutError(\"Timeout connecting to server\")\n    except OSError as e:\n        raise ConnectionError(self._error_message(e))\n\n```\n\nI see that this issue is already assigned to you, @gerzse. Do you have any updates on its progress?\n\n---\n\nHaving the same issue with pipelines. Solved using this hack:\n\n```python\n    async def redis_connect_func_patch(self: AbstractConnection):\n        async def _noop(_):\n            pass\n\n        await self.retry.call_with_retry(\n            do=lambda: self.on_connect_check_health(check_health=True),\n            fail=_noop,\n        )\n\n    redis = Redis(\n        protocol=3,\n        retry_on_error=[\n            asyncio.TimeoutError,\n            ConnectionError,\n            exceptions.ConnectionError,\n            exceptions.TimeoutError,\n            TimeoutError,\n        ],\n        retry=RedisRetry(backoff=ExponentialBackoff(cap=10, base=0.1), retries=100),\n        redis_connect_func=redis_connect_func_patch, # the hack\n    )\n``` \n\n---\n\nBtw @petyaslavova , I have a better explanation of the problem [here](https://github.com/valkey-io/valkey-py/issues/169#issuecomment-2704939737) ",
      "labels": [],
      "created_at": "2024-04-10T08:41:11Z",
      "closed_at": "2025-12-04T14:31:20Z",
      "url": "https://github.com/redis/redis-py/issues/3203",
      "comments_count": 3
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3815,
      "title": "AttributeError in aioredis.RedisCluster when the connection is lost: 'ConnectionError' object has no attribute 'host'",
      "problem": "I noticed an unexpected behavior for aioredis.RedisCluster after losing a connection to the cluster node (the maintenance had been started by the service provider):\n\nAttributeError:'ConnectionError' object has no attribute 'host'\nFile \"<venv_path>/lib/python3.11/site-packages/redis/asyncio/cluster.py\", line 787, in execute_command\nraise e\nFile \"<venv_path>/lib/python3.11/site-packages/redis/asyncio/cluster.py\", line 748, in execute_command\ntarget_nodes = await self._determine_nodes(\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"<venv_path>/lib/python3.11/site-packages/redis/asyncio/cluster.py\", line 624, in _determine_nodes\nself.nodes_manager.get_node_from_slot(\nFile \"<venv_path>/lib/python3.11/site-packages/redis/asyncio/cluster.py\", line 1342, in get_node_from_slot\nself._update_moved_slots()\nFile \"<venv_path>/lib/python3.11/site-packages/redis/asyncio/cluster.py\", line 1299, in _update_moved_slots\nredirected_node = self.get_node(host=e.host, port=e.port)\n^^^^^^\n\n\nPython: 3.11\nRedis-py: 6.4.0",
      "solution": "Hi @klymenkosergiy, thanks for notifying us! We will investigate the issue.\n\n---\n\nHi @klymenkosergiy,\n\nIt turns out my initial assumption that the issue was related to transactions was incorrect. I attempted to reproduce the problem locally by executing simple SET and GET commands and then replacing a cluster node (with all requests targeting slots on that node), but I wasn\u2019t able to observe the described behavior.\n\nI also reviewed the code to check whether we ever assign an error that isn\u2019t an AskError or MovedError to the `_moved_exception` property, but I couldn\u2019t find any such cases.\n\nCould you please share more details about your use case\u2014specifically, how your client is initialized and what configuration it uses? If possible, providing a minimal example of your app\u2019s main flow that triggers the issue would greatly help me reproduce the problem.\n\n---\n\nHi @klymenkosergiy,\nI was able to reproduce the problem \u2014 and it turns out my initial suspicion was correct, just not for the master branch. \ud83d\ude42\n\nIn the master branch, the issue was already fixed by the following commit:\nhttps://github.com/redis/redis-py/commit/f3806fad528052c7aad202c356fdcf9668ef9ec1\n\nWith this change, connection errors raised during a transaction are no longer mapped to the `_moved_exception` property.\n\nSince the fix is already available in the latest releases, I\u2019m going to close the issue.",
      "labels": [
        "bug"
      ],
      "created_at": "2025-10-24T13:49:34Z",
      "closed_at": "2025-12-04T12:27:29Z",
      "url": "https://github.com/redis/redis-py/issues/3815",
      "comments_count": 6
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3237,
      "title": "MaxConnectionsError not used consistently",
      "problem": "**Version**: redis-py 5.0.4\r\n**Platform**: Ubuntu 22.04\r\n\r\nThe docs around the `max_connections` argument in various places suggest that `MaxConnectionsError` should be used to indicate a lack of available connections, yet that exception type seems only to be used in one place: https://github.com/redis/redis-py/blob/6751de20a0cad597a2a024cdae2a1518ea1b1580/redis/asyncio/cluster.py#L1051\r\nOther places use `ConnectionError(\"Too many connections\")`.\r\n\r\nIs this expected? Should these all use `MaxConnectionsError`?",
      "solution": "This looks like it's _mostly_ been resolved (as part of addressing https://github.com/redis/redis-py/issues/3684), though there's still one place where it's not:\nhttps://github.com/redis/redis-py/blob/440465bb2904e2b90993f9c8733360479e95dd81/redis/asyncio/connection.py#L1211\n\nPerhaps this was just an oversight?\n\ncc @petyaslavova as someone who may have context.\n\nI'd be happy to have a go at putting up a PR to change over if that's useful.",
      "labels": [],
      "created_at": "2024-05-17T14:05:21Z",
      "closed_at": "2025-12-01T15:11:23Z",
      "url": "https://github.com/redis/redis-py/issues/3237",
      "comments_count": 4
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3745,
      "title": "RecursionError in redis.asyncio when using retry_on_timeout or health_check_interval",
      "problem": "This might be related to #2893 (Sentinel recursion), but it also happens in non-Sentinel setups\n\nBug Description\nredis.asyncio enters infinite recursion when retry_on_timeout=True or health_check_interval>0 are set.\n\nEnvironment\n\nPython: 3.11.x, 3.12.x (both affected)\n\nredis-py: 5.0.1 (also tested with 4.6.0)\n\nParser: default (no hiredis)\n\nOS: Docker (python:3.12-slim-bookworm)\n\nuvloop: disabled\n\nMinimal Reproduction\n\nimport asyncio\nimport redis.asyncio as redis\n\nasync def reproduce():\n    client = redis.from_url(\n        \"redis://default:password@localhost:6379/0\",\n        retry_on_timeout=True,\n        health_check_interval=30,\n    )\n    await client.ping()  # RecursionError\n\nasyncio.run(reproduce())\n\n\nWorking Configuration\n\nclient = redis.from_url(\n    \"redis://default:password@localhost:6379/0\",\n    retry_on_timeout=False,\n    health_check_interval=0,\n)\n\n\nStack Trace Pattern\nRecursionError: maximum recursion depth exceeded\n\nconnect() \u2192 on_connect() \u2192 send_command(\"CLIENT SETINFO\")\n\n\u2192 check_health() \u2192 _send_ping() \u2192 send_command(\"PING\")\n\n\u2192 back to connect() (infinite loop)\n\nKey Findings\n\nTrigger: enabling retry or health check during initial connection.\n\nNot Python-version-specific, reproducible on 3.11 and 3.12.\n\nNot parser-specific (occurs with PythonParser too).\n\nRoot cause: circular dependency between connection setup and health check.\n\nWorkaround\nDisable retry and health check:\n\nredis.from_url(..., retry_on_timeout=False, health_check_interval=0)\n\n\nExpected Behavior\nHealth checks and retries should not trigger recursion during connection initialization.",
      "solution": "Hi @everpoint-saas, it looks like this issue has been resolved with PR #3557.\nI was able to reproduce the max-recursion exception on older versions that don\u2019t include this commit, but not on the latest master.\n\nI\u2019ll go ahead and close this issue for now.\nPlease feel free to reopen it if you\u2019re able to reproduce the problem with the latest `redis-py` versions.",
      "labels": [],
      "created_at": "2025-08-19T05:59:07Z",
      "closed_at": "2025-11-28T14:00:29Z",
      "url": "https://github.com/redis/redis-py/issues/3745",
      "comments_count": 2
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3291,
      "title": "The timeout settings are not working",
      "problem": "Hello! I am using Python 3.11.6, redis-py version 4.6.0 and hiredis 2.0.0. My application I run in Docker Python:3.11.6-slim-bookworm. I set up the timeout settings as follows: socket_connect_timeout=0.008 and socket_timeout=0.01. In addition to this, I set the following parameters: socket_keepalive=True, decode_responses=False. After which I began to measure the operation time of MGET. According to the measurement results, I clearly see that the response time sometimes exceeds 0.1 seconds. See screenshot below.\r\n\r\n<img width=\"1782\" alt=\"\u0421\u043d\u0438\u043c\u043e\u043a \u044d\u043a\u0440\u0430\u043d\u0430 2024-06-25 \u0432 11 37 05\" src=\"https://github.com/redis/redis-py/assets/55485145/ae1b5136-4f76-42d9-bcee-bdd5c9b88c7c\">\r\n\r\nMy question is why the timeout settings did not work and the MGET operation took 0.1 seconds instead of 0.01 seconds.\r\nTo solve this problem, I tried:\r\n1) Disabling hiredis. This didn't help. The response time also sometimes exceeds 0.01 seconds.\r\n2) Passing the setting 'socket_keepalive_options': {socket.TCP_USER_TIMEOUT, 3}. This also didn't help.\r\n3) Studying the original client code. I discovered that in the Connection class there is a method called can_read, which has an argument for timeout, which by default is set to 0, meaning that the timeout is absent and the completion of the operation will be awaited infinitely.\r\nI used monkey patching and redefined this method to my own, in which case when the timeout is equal to 0, the socket_timeout setting was used.\r\nBut this also didn't help.\r\n\r\nWhat else can I do? Can this be a bug in the original client code?\r\n",
      "solution": "@harshit98 in the third point I tried this solution\r\n\r\n```\r\nfrom redis.connection import Connection\r\n\r\n\r\noriginal_function = Connection.can_read\r\n\r\ndef function(self: Connection, timeout: float = 0) -> bool:\r\n    if not isinstance(timeout, (int, float)) or timeout <= 0:\r\n        if self.socket_timeout is not None:\r\n            timeout = self.socket_timeout\r\n    return original_function(self, timeout)\r\n\r\nConnection.can_read = function\r\n```",
      "labels": [
        "stale"
      ],
      "created_at": "2024-06-25T08:59:50Z",
      "closed_at": "2025-11-21T00:21:48Z",
      "url": "https://github.com/redis/redis-py/issues/3291",
      "comments_count": 4
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3836,
      "title": "BLMOVE throws syntax error",
      "problem": "I'm pretty sure I'm not doing anything weird here.\n\n`payload = await redis.blmove(\"dispatch-queue\", \"processing-queue\", \"LEFT\", \"RIGHT\", 1)`\n\nThis throws a syntax error on v5.3.1, with Redis 7.0.15\n\n```\n  File \"dispatcher/main.py\", line 83, in run_redis\n    payload = await redis.blmove(\"room-dispatcher.queue\", \"room-dispatcher.processing\", \"LEFT\", \"RIGHT\", 1)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniel/.cache/pypoetry/virtualenvs/dispatcher-jV_AsfwX-py3.12/lib/python3.12/site-packages/redis/asyncio/client.py\", line 677, in execute_command\n    return await conn.retry.call_with_retry(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniel/.cache/pypoetry/virtualenvs/dispatcher-jV_AsfwX-py3.12/lib/python3.12/site-packages/redis/asyncio/retry.py\", line 50, in call_with_retry\n    return await do()\n           ^^^^^^^^^^\n  File \"/home/daniel/.cache/pypoetry/virtualenvs/dispatcher-jV_AsfwX-py3.12/lib/python3.12/site-packages/redis/asyncio/client.py\", line 652, in _send_command_parse_response\n    return await self.parse_response(conn, command_name, **options)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniel/.cache/pypoetry/virtualenvs/dispatcher-jV_AsfwX-py3.12/lib/python3.12/site-packages/redis/asyncio/client.py\", line 698, in parse_response\n    response = await connection.read_response()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniel/.cache/pypoetry/virtualenvs/dispatcher-jV_AsfwX-py3.12/lib/python3.12/site-packages/redis/asyncio/connection.py\", line 627, in read_response\n    raise response from None\n\n```\n\nLMOVE works fine, it's just the blocking variant that does this.",
      "solution": "Thanks for the questions. I am using Hiredis, so I tested agin without it on the latest version and everything worked. I then added Hiredis back and it still works, lol. The thing is, I definitely tested 7.0.1 originally; downgrading to 5.x was my attempt to fix the issue. So I guess something somewhere got mixed up in my packages and broke, who knows. Anyway, thanks :) No issue then.\n\n---\n\nHi @Pleochism, I think I found the problem in your command... \n\n`payload = await redis.blmove(\"dispatch-queue\", \"processing-queue\", \"LEFT\", \"RIGHT\", 1)`\n\nThis is the definition of the command:\n```\ndef blmove(\n        self,\n        first_list: str,\n        second_list: str,\n        timeout: int,\n        src: str = \"LEFT\",\n        dest: str = \"RIGHT\",\n    )\n```\n\nThe timeout should be before the src and dest arguments - in your case you have set it after them.\nCan you please try to change the order of the args and test again?",
      "labels": [],
      "created_at": "2025-11-06T07:07:36Z",
      "closed_at": "2025-11-19T06:21:01Z",
      "url": "https://github.com/redis/redis-py/issues/3836",
      "comments_count": 7
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3852,
      "title": "Maintenance notification warning if protocol=3",
      "problem": "Hi,\n\nsince redis-py v7 a warning is issued if `protocol=3` but the server doesn't support the latest maintenance notification feature:\n\n```python\n>>> import redis\n>>> r = redis.Redis()\n>>> r.get('foo')\n>>> r = redis.Redis(protocol=3)\n>>> r.get('foo')\nFailed to enable maintenance notifications: unknown subcommand 'MAINT_NOTIFICATIONS'. Try CLIENT HELP.\n```\n\nBy explicitly disabling the feature the warning also disappears:\n\n```python\n>>> import redis\n>>> from redis.maint_notifications import MaintNotificationsConfig\n>>> r = redis.Redis(protocol=3, maint_notifications_config=MaintNotificationsConfig(enabled=False))\n>>> r.get('foo')\n```\n\nThe default for maintenance configuration is \"auto\". IMHO, this implies a more graceful handling of unsupported servers, i.e., not logging anything. Maybe this warning could simply be removed?",
      "solution": "Closing the issue, since it has already been resolved.",
      "labels": [],
      "created_at": "2025-11-18T11:59:55Z",
      "closed_at": "2025-11-18T16:28:33Z",
      "url": "https://github.com/redis/redis-py/issues/3852",
      "comments_count": 3
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3194,
      "title": "Inconsistency default behaviour on Sync / Async clients against ConnectionError",
      "problem": "**Version**: Redis docker redis:latest. redis-py: 5.0.3\r\n\r\n**Platform**: Ubuntu 22.04.4 LTS\r\n\r\n**Description**: \r\nDuring a migration from sync to async, we found an important inconsistency on the default behavior between sync/async clients when a  `ConnectionError` happens . This is a really common situation, considering that the `redis-ha-proxy` [is by default configured to close the connections after a while iddle](https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.0/html/external_load_balancing_for_the_overcloud/example_default_haproxy_configuration), so this is an absolutely normal situation.\r\n\r\nSync client by default is handling the retry on `ConnectionError` while the Async client is failing. Probably it's because by default the Redis Retry schema supports those errors -> https://github.com/redis/redis-py/blob/master/redis/retry.py#L14.\r\n\r\nThis inconsistency happens on all commands, we will explain how to reproduce even in the most simple `ping` case. \r\n\r\nThis error impact is that migration from sync to async clients introduces extra complexity by having to deal with inconsistent behavior against network issues.\r\n\r\n**Reproduce** :\r\n\r\nFirst, start a docker container with latest redis image: `docker run --name my-redis -p 6379:6379 -d redis`  \r\n\r\nThose are the necessary commands to list / kill connections from clients to redis docker image:  \r\n\r\n- List existing connection to redis: `ss -tp '( dport = :6379 )'` \r\n- Kill all connections to redis: `sudo ss -K -tp '( dport = :6379 )'`\r\n\r\n**Sync client behavior:**\r\nExecute standard `python` command: \r\n\r\n```Python 3.10.10 (main, Mar 30 2023, 22:45:48) [GCC 11.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import redis\r\n>>> redisCli = redis.Redis(\r\n...      host='localhost',\r\n...      port=6379,\r\n...      charset=\"utf-8\",\r\n...      decode_responses=True\r\n...      )\r\n>>> redisCli.ping()\r\nTrue\r\n>>> print('here we kill connections with the ss command!')\r\nhere we kill connections with the ss command!\r\n>>> redisCli.ping()\r\nTrue\r\n>>> \r\n```\r\n\r\n\r\n**Async client behavior:** \r\n\r\nExecute python with `python -m asyncio` to be able to await in the python shell.\r\n\r\n```\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import asyncio\r\n>>> import redis.asyncio as aioredis\r\n>>> redisAsync = aioredis.Redis(host='localhost',\r\n... port = 6379,\r\n... decode_responses=True)\r\n>>> await redisAsync.ping()\r\nTrue\r\n>>> print('HERE KILL connection with ss command!')\r\nHERE KILL connection with ss command!\r\n>>> await redisAsync.ping()\r\nTraceback (most recent call last):\r\n  File \"/home/jonortiz/.pyenv/versions/ais/lib/python3.10/site-packages/redis/asyncio/connection.py\", line 473, in send_packed_command\r\n    await self._writer.drain()\r\n  File \"/home/jonortiz/.pyenv/versions/3.10.10/lib/python3.10/asyncio/streams.py\", line 371, in drain\r\n    await self._protocol._drain_helper()\r\n  File \"/home/jonortiz/.pyenv/versions/3.10.10/lib/python3.10/asyncio/streams.py\", line 167, in _drain_helper\r\n    raise ConnectionResetError('Connection lost')\r\nConnectionResetError: Connection lost\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/jonortiz/.pyenv/versions/3.10.10/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\r\n    return self.__get_result()\r\n  File \"/home/jonortiz/.pyenv/versions/3.10.10/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\r\n    raise self._exception\r\n  File \"<console>\", line 1, in <module>\r\n  File \"/home/jonortiz/.pyenv/versions/ais/lib/python3.10/site-packages/redis/asyncio/client.py\", line 610, in execute_command\r\n    return await conn.retry.call_with_retry(\r\n  File \"/home/jonortiz/.pyenv/versions/ais/lib/python3.10/site-packages/redis/asyncio/retry.py\", line 62, in call_with_retry\r\n    await fail(error)\r\n  File \"/home/jonortiz/.pyenv/versions/ais/lib/python3.10/site-packages/redis/asyncio/client.py\", line 597, in _disconnect_raise\r\n    raise error\r\n  File \"/home/jonortiz/.pyenv/versions/ais/lib/python3.10/site-packages/redis/asyncio/retry.py\", line 59, in call_with_retry\r\n    return await do()\r\n  File \"/home/jonortiz/.pyenv/versions/ais/lib/python3.10/site-packages/redis/asyncio/client.py\", line 583, in _send_command_parse_response\r\n    await conn.send_command(*args)\r\n  File \"/home/jonortiz/.pyenv/versions/ais/lib/python3.10/site-packages/redis/asyncio/connection.py\", line 497, in send_command\r\n    await self.send_packed_command(\r\n  File \"/home/jonortiz/.pyenv/versions/ais/lib/python3.10/site-packages/redis/asyncio/connection.py\", line 484, in send_packed_command\r\n    raise ConnectionError(\r\nredis.exceptions.ConnectionError: Error UNKNOWN while writing to socket. Connection lost.\r\n>>> \r\n```\r\n\r\n\r\n\r\n\r\n",
      "solution": "For anyone experiencing this issue, it can be resolved by manually adding the retry parameter to the Redis constructor, as follows:\r\n\r\n```\r\nredisAsync = aioredis.Redis(\r\n    host='localhost',\r\n    port = 6379,\r\n    decode_responses=True,\r\n    retry_on_error=[aioredis.ConnectionError]\r\n)\r\n```\r\n\r\nHowever, this solution is far from optimal in a migration scenario where we expect consistent behavior across both variants.",
      "labels": [],
      "created_at": "2024-03-27T10:48:59Z",
      "closed_at": "2025-11-14T15:59:31Z",
      "url": "https://github.com/redis/redis-py/issues/3194",
      "comments_count": 2
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3595,
      "title": "Unable to connect to redis sentinel. Getting: unexpected keyword argument 'connection_pool'",
      "problem": "I am using python client to connect with sentinel master. But getting this error\n\n`Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/site-packages/ecprt/agents/cli/cli.py\", line 266, in _worker_run\n    await worker.run()\n  File \"/usr/local/lib/python3.12/site-packages/ecprt/agents/worker.py\", line 550, in run\n    self.connect_to_redis()\n  File \"/usr/local/lib/python3.12/site-packages/ecprt/agents/worker.py\", line 476, in connect_to_redis\n    redis_client.ping()\n  File \"/usr/local/lib/python3.12/site-packages/redis/commands/core.py\", line 1212, in ping\n    return self.execute_command(\"PING\", **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/redis/client.py\", line 559, in execute_command\n    return self._execute_command(*args, **options)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/redis/client.py\", line 565, in _execute_command\n    conn = self.connection or pool.get_connection(command_name, **options)\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/redis/connection.py\", line 1417, in get_connection\n    connection = self.make_connection()\n                 ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/redis/connection.py\", line 1463, in make_connection\n    return self.connection_class(**self.connection_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/redis/connection.py\", line 1013, in __init__\n    super().__init__(**kwargs)\n  File \"/usr/local/lib/python3.12/site-packages/redis/connection.py\", line 684, in __init__\n    super().__init__(**kwargs)\nTypeError: AbstractConnection.__init__() got an unexpected keyword argument 'connection_pool'\n{\"message\": \"worker failed\", \"level\": \"ERROR\", \"name\": \"livekit.agents\", \"exc_info\": \"Traceback (most recent call last):\\n  File \\\"/usr/local/lib/python3.12/site-packages/redis/connection.py\\\", line 1415, in get_connection\\n    connection = self._available_connections.pop()\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nIndexError: pop from empty list\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \\\"/usr/local/lib/python3.12/site-packages/myapp/cli/cli.py\\\", line 266, in _worker_run\\n    await worker.run()\\n  File \\\"/usr/local/lib/python3.12/site-packages/myapp/worker.py\\\", line 550, in run\\n    self.connect_to_redis()\\n  File \\\"/usr/local/lib/python3.12/site-packages/myapp/worker.py\\\", line 476, in connect_to_redis\\n    redis_client.ping()\\n  File \\\"/usr/local/lib/python3.12/site-packages/redis/commands/core.py\\\", line 1212, in ping\\n    return self.execute_command(\\\"PING\\\", **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/local/lib/python3.12/site-packages/redis/client.py\\\", line 559, in execute_command\\n    return self._execute_command(*args, **options)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/local/lib/python3.12/site-packages/redis/client.py\\\", line 565, in _execute_command\\n    conn = self.connection or pool.get_connection(command_name, **options)\\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/local/lib/python3.12/site-packages/redis/connection.py\\\", line 1417, in get_connection\\n    connection = self.make_connection()\\n                 ^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/local/lib/python3.12/site-packages/redis/connection.py\\\", line 1463, in make_connection\\n    return self.connection_class(**self.connection_kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/local/lib/python3.12/site-packages/redis/connection.py\\\", line 1013, in __init__\\n    super().__init__(**kwargs)\\n  File \\\"/usr/local/lib/python3.12/site-packages/redis/connection.py\\\", line 684, in __init__\\n    super().__init__(**kwargs)\\nTypeError: AbstractConnection.__init__() got an unexpected keyword argument 'connection_pool'\", \"timestamp\": \"2025-04-11T08:47:09.997584+00:00\"}`\n\nBelow is my code for connecting with sentinel master node.\n\n`def connect_to_redis(self):\n\n    logger.info(f\"Redis version in use: {redis.__version__}\")\n        logger.info(\"Inside connect_to_redis\")\n\n        use_tls = os.getenv(\"REDIS_USE_TLS\", \"true\").lower() == \"true\"\n        cacert_path = os.getenv(\"REDIS_CACERT_PATH\", \"/etc/rediscert/ca.crt\")\n\n        sentinel_addresses_raw = os.getenv(\"REDIS_SENTINEL_ADDRESSES\", \"sentinel.redis-ptdev5-ns.svc.cluster.local:26379\")\n        sentinel_addresses = sentinel_addresses_raw.strip(\"[]\").split(\",\")\n        sentinel_master_name = os.getenv(\"REDIS_SENTINEL_MASTER_NAME\", \"master\")\n\n        dial_timeout = int(os.getenv(\"REDIS_DIAL_TIMEOUT\", \"2000\")) / 1000\n        read_timeout = int(os.getenv(\"REDIS_READ_TIMEOUT\", \"200\")) / 1000\n        write_timeout = int(os.getenv(\"REDIS_WRITE_TIMEOUT\", \"200\")) / 1000\n\n        if not (sentinel_addresses and sentinel_addresses[0]):\n            raise Exception(\"Redis is not configured - REDIS_SENTINEL_ADDRESSES required\")\n\n        sentinel_addrs = []\n        for addr in sentinel_addresses:\n            addr = addr.strip()\n            if addr:\n                if ':' in addr:\n                    host, port_str = addr.rsplit(':', 1)\n                    port = int(port_str.strip(\"]'\\\"\"))\n                else:\n                    host = addr\n                    port = 26379\n                sentinel_addrs.append((host, port))\n\n        try:\n            connection_class = redis.SSLConnection if use_tls else redis.Connection\n\n            sentinel = Sentinel(\n                sentinel_addrs,\n                socket_timeout=read_timeout,\n                connection_class=connection_class,\n                ssl=use_tls if use_tls else None,\n                ssl_ca_certs=cacert_path if use_tls else None,\n            )\n\n            global redis_client\n            redis_client = sentinel.master_for(\n                service_name=sentinel_master_name,\n                socket_timeout=read_timeout,\n            )\n\n            try:\n                redis_client.ping()\n                logger.info(\"Successfully connected to Redis\")\n            except redis.RedisError as e:\n                logger.warning(f\"Redis client initialized, but ping failed: {e}\")\n                pass\n\n        except Exception as e:\n            logger.error(f\"Redis sentinel connection setup failed: {e}\")\n            raise`",
      "solution": "Hi @Manhar0911, thank you for bringing this to our attention. I've identified the root cause and will be posting a fix shortly.",
      "labels": [],
      "created_at": "2025-04-11T08:21:17Z",
      "closed_at": "2025-11-14T13:32:13Z",
      "url": "https://github.com/redis/redis-py/issues/3595",
      "comments_count": 6
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3353,
      "title": "Race condition Airflow's Celery executor timeout and import redis leave a broken import",
      "problem": "**Version**: What redis-py and what redis version is the issue happening on?\r\nredis-py: 4.6.0 and 5.0.7\r\nredis: 7.2.4\r\n\r\n**Platform**: What platform / version? (For example Python 3.5.1 on Windows 7 / Ubuntu 15.10 / Azure)\r\npython 3.11.9 on Debian 12 (bookworm)\r\n\r\n**Description**: Description of your issue, stack traces from errors and code that reproduces the issue\r\n\r\nHello,\r\n\r\nWe are sorry if this is a long text and some aren't related to redis-py. This a copy-paste issue we also posted on apache/airflow and celery/kombu. We include a MCVE to reproduce this bug.\r\n\r\nTL;DR: The `with timeout(seconds=OPERATION_TIMEOUT):` in `airflow.executors.celery_executor.send_task_to_executor` might leave a very broken import of redis & may affect another package that we haven't discovered yet. This is a race condition and very hard to debug at first. To reproduce this bug, we have to\r\n\r\n- have a celery timeout (we kept it at default 1.0 second)\r\n- the timeout should happen during an import\r\n- a long import (we aren't sure the import of redis is long and this bug happens mostly with redis package, maybe this is a confirmation bias)\r\n\r\nRelates:\r\n\r\n- [**apache/airflow** discussion #36097 _CeleryExecutor is failing to launch tasks with redis error_](https://github.com/apache/airflow/discussions/36097)\r\n- [**apache/airflow** issue #33744 _Celery Executor is not working with redis-py 5.0.0_](https://github.com/apache/airflow/issues/33744)\r\n- [**celery/kombu** issue #1815 _import exception raised in transport/redis \"module 'redis' has no attribute 'client' \"_](https://github.com/celery/kombu/issues/1815)\r\n\r\nOur environment:\r\n\r\n```\r\nairflow: this happens with both 2.6 and latest 2.9.3 version\r\nhelm chart: 1.9, 1.14 or 1.15\r\npython: 3.11.9\r\nredis: 4.6.0 (airflow 2.6) and 5.0.7 (airflow 2.9)\r\nkombu: 5.3.1 (airflow 2.6) and 5.3.7 (airflow 2.9)\r\n```\r\n\r\nWe 've observed this issue since at least several months ago with our airflow deployment using official helm chart, we have the same issue as in related issues/discussion:\r\n\r\n```\r\nAug 8 08:29:02 airflow-XXX-scheduler-XXX-zhtb8 scheduler ERROR {timeout.py:68} ERROR - Process timed out, PID: 7\r\nAug 8 08:29:02 airflow-XXX-scheduler-XXX-zhtb8 scheduler {celery_executor.py:279} INFO - [Try 1 of 3] Task Timeout Error for Task: (TaskInstanceKey(dag_id='XXX', task_id='XXX', run_id='manual__2024-06-19T14:06:39+00:00', try_number=51, map_index=-1)).\r\nAug 8 08:29:03 airflow-XXX-scheduler-XXX-zhtb8 scheduler {celery_executor.py:290} ERROR - Error sending Celery task: module 'redis' has no attribute 'client'\r\nAug 8 08:29:03 airflow-XXX-scheduler-XXX-zhtb8 scheduler Celery Task ID: TaskInstanceKey(dag_id='XXX', task_id='XXX', run_id='manual__2024-06-19T14:06:39+00:00', try_number=51, map_index=-1)\r\nAug 8 08:29:03 airflow-XXX-scheduler-XXX-zhtb8 scheduler Traceback (most recent call last):\r\n  File \"/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/celery/executors/celery_executor_utils.py\", line 220, in send_task_to_executor\r\n    result = task_to_run.apply_async(args=[command], queue=queue)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/airflow/.local/lib/python3.11/site-packages/celery/app/task.py\", line 594, in apply_async\r\n    return app.send_task(\r\n           ^^^^^^^^^^^^^^\r\n  File \"/home/airflow/.local/lib/python3.11/site-packages/celery/app/base.py\", line 797, in send_task\r\n    with self.producer_or_acquire(producer) as P:\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/airflow/.local/lib/python3.11/site-packages/celery/app/base.py\", line 932, in producer_or_acquire\r\n    producer, self.producer_pool.acquire, block=True,\r\n              ^^^^^^^^^^^^^^^^^^\r\n  File \"/home/airflow/.local/lib/python3.11/site-packages/celery/app/base.py\", line 1354, in producer_pool\r\n    return self.amqp.producer_pool\r\n           ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/airflow/.local/lib/python3.11/site-packages/celery/app/amqp.py\", line 591, in producer_pool\r\n    self.app.connection_for_write()]\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/airflow/.local/lib/python3.11/site-packages/celery/app/base.py\", line 829, in connection_for_write\r\n    return self._connection(url or self.conf.broker_write_url, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/airflow/.local/lib/python3.11/site-packages/celery/app/base.py\", line 880, in _connection\r\n    return self.amqp.Connection(\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/airflow/.local/lib/python3.11/site-packages/kombu/connection.py\", line 201, in __init__\r\n    if not get_transport_cls(transport).can_parse_url:\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/airflow/.local/lib/python3.11/site-packages/kombu/transport/__init__.py\", line 90, in get_transport_cls\r\n    _transport_cache[transport] = resolve_transport(transport)\r\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/airflow/.local/lib/python3.11/site-packages/kombu/transport/__init__.py\", line 75, in resolve_transport\r\n    return symbol_by_name(transport)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/airflow/.local/lib/python3.11/site-packages/kombu/utils/imports.py\", line 59, in symbol_by_name\r\n    module = imp(module_name, package=package, **kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<frozen importlib._XXX>\", line 1204, in _gcd_import\r\n  File \"<frozen importlib._XXX>\", line 1176, in _find_and_load\r\n  File \"<frozen importlib._XXX>\", line 1147, in _find_and_load_unlocked\r\n  File \"<frozen importlib._XXX>\", line 690, in _load_unlocked\r\n  File \"<frozen importlib._XXX_external>\", line 940, in exec_module\r\n  File \"<frozen importlib._XXX>\", line 241, in _call_with_frames_removed\r\n  File \"/home/airflow/.local/lib/python3.11/site-packages/kombu/transport/redis.py\", line 285, in <module>\r\n    class PrefixedRedisPipeline(GlobalKeyPrefixMixin, redis.client.Pipeline):\r\n                                                      ^^^^^^^^^^^^\r\nAug 8 08:29:03 airflow-XXX-scheduler-XXX-zhtb8 scheduler AttributeError: module 'redis' has no attribute 'client'\r\n```\r\n\r\nWe've verified and we have neither redis folder nor `redis.py` file from our dev, this is a very sporadic error where most of the time it works, then it stops working for unknown reason, and once if happens, the scheduler is broken and couldn't schedule anything (same error message) until we restart the scheduler process (restart the pod)\r\n\r\nThis happens quite randomly (one in tens or fifty deployments of helm chart), and we couldn't reproduce it for sure for debugging purpose.\r\n\r\nWhat we found out is that if this happens, this bug won't disappear until we restart (kill) the scheduler pod.\r\nWe could reproduce randomly with these steps in a test airflow:\r\n\r\n- `kubectl delete po -l release=airflow-XXX,component=scheduler --force --grace-period 0`\r\n- Clear a DAG task and hope that the bug happens, this should be immediate, if not, repeat the whole process\r\n\r\nAt first, we suspect that this is a case of race condition in importing redis package, because we inject debug code before the line `class PrefixedRedisPipeline(GlobalKeyPrefixMixin, redis.client.Pipeline):` with `print(sys.path)`, `print(redis)`, `print(redis.__version__)`, ... and everything is okay, except `print(dir(redis))` gives a different result:\r\n\r\n```\r\nsys.path=['/home/airflow/.local/bin', '/usr/local/lib/python311.zip', '/usr/local/lib/python3.11', '/usr/local/lib/python3.11/lib-dynload', '/home/airflow/.local/lib/python3.11/site-packages', '/opt/airflow/dags/repo/', '/opt/airflow/config', '/opt/airflow/plugins']\r\n\r\nredis=<module 'redis' from '/home/airflow/.local/lib/python3.11/site-packages/redis/__init__.py'>\r\n\r\nredis.__version__=5.0.7\r\n\r\ndir(redis)=['AuthenticationError', 'AuthenticationWrongNumberOfArgsError', 'BlockingConnectionPool', 'BusyLoadingError', 'ChildDeadlockedError', 'Connection', 'ConnectionError', 'ConnectionPool', 'CredentialProvider', 'DataError', 'InvalidResponse', 'OutOfMemoryError', 'PubSubError', 'ReadOnlyError', 'Redis', 'RedisCluster', 'RedisError', 'ResponseError', 'SSLConnection', 'Sentinel', 'SentinelConnectionPool', 'SentinelManagedConnection', 'SentinelManagedSSLConnection', 'StrictRedis', 'TimeoutError', 'UnixDomainSocketConnection', 'UsernamePasswordCredentialProvider', 'VERSION', 'WatchError', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'asyncio', 'cluster', 'default_backoff', 'from_url', 'int_or_str', 'metadata', 'sentinel', 'sys']\r\n```\r\n\r\ncompared to a python shell session inside the same container:\r\n\r\n```\r\nPython 3.11.9 (main, Jul 23 2024, 07:22:56) [GCC 12.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import redis\r\n>>> redis\r\n<module 'redis' from '/home/airflow/.local/lib/python3.11/site-packages/redis/__init__.py'>\r\n>>> redis.__version__\r\n'5.0.7'\r\n>>> dir(redis)\r\n['AuthenticationError', 'AuthenticationWrongNumberOfArgsError', 'BlockingConnectionPool', 'BusyLoadingError', 'ChildDeadlockedError', 'Connection', 'ConnectionError', 'ConnectionPool', 'CredentialProvider', 'DataError', 'InvalidResponse', 'OutOfMemoryError', 'PubSubError', 'ReadOnlyError', 'Redis', 'RedisCluster', 'RedisError', 'ResponseError', 'SSLConnection', 'Sentinel', 'SentinelConnectionPool', 'SentinelManagedConnection', 'SentinelManagedSSLConnection', 'StrictRedis', 'TimeoutError', 'UnixDomainSocketConnection', 'UsernamePasswordCredentialProvider', 'VERSION', 'WatchError', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_parsers', 'asyncio', 'backoff', 'client', 'cluster', 'commands', 'compat', 'connection', 'crc', 'credentials', 'default_backoff', 'exceptions', 'from_url', 'int_or_str', 'lock', 'metadata', 'retry', 'sentinel', 'sys', 'typing', 'utils']\r\n```\r\n\r\nWe noted that `dir(redis)` inside the troublesome scheduler lacks several attributes, notably `redis.client`\r\n\r\nAnother thing we discovered is that in every case, there is always a Timeout (as you could see the log above), and sure enough, we found out later that the bug always happens while the process of importing `redis` is interrupted by Timeout (we print line number in `redis/__init__.py` and the importing didn't run till the end). In very rare case, `airflow.utils.timeout` doesn't work as inteded, the timeout error is printed out in the middle of `import redis` but the `import redis` still run till the end, in this case, the bug couldn't happens. But most of the time, the timeout interrupt the import.\r\n\r\nWith this idea, we injected a `sleep` at the end of `redis/__init__.py` and sure enough, we could reproduce this bug every time.\r\n\r\nSo we made a quick minimal, complete and verifiable example (MCVE) by timeout the very long first import of redis.\r\n(This is a race condition which is hard to duplicate in local machine, so what we did is to introduce a delay to the very first import of redis to reproduce this bug for sure. )\r\n\r\nPlease find below a compressed file of three file:\r\n- mock_kombu_transport_redis.py\r\n- mock_airflow.py\r\n- and redis.patch/init.py, we need to add the content of this file at the end of redis-py package's  __init__.py to add the delay in import\r\n[mcve.zip](https://github.com/user-attachments/files/16563669/mcve.zip)\r\n\r\n`python -m mock_airflow` gives the result:\r\n\r\n```\r\nFunctionTimedOut\r\n\r\nAfter failed import redis\r\n'redis' in sys.modules: True\r\n'redis.client' in sys.modules: True\r\n\r\nReimport mock_kombu_transport_redis\r\nAfter reimport redis\r\n'redis' in sys.modules: True\r\n'redis.client' in sys.modules: True\r\n'client' in dir(redis): False\r\ngetattr(redis, 'client', None)=None\r\n\r\nAfter reimport redis.client\r\n'client' in dir(redis): False\r\ngetattr(redis, 'client', None)=None\r\n\r\nAfter reimport: from redis import client\r\nclient=<module 'redis.client' from '***/site-packages/redis/client.py'>\r\n'client' in dir(redis): False\r\ngetattr(redis, 'client', None)=None\r\nclient.Pipeline=<class 'redis.client.Pipeline'>\r\n```\r\n\r\nThe line `print(redis.client)` will raise an error:\r\n\r\n```\r\nAttributeError: module 'redis' has no attribute 'client'\r\n```\r\n\r\nSo an interrupted import give a different import than a normal `import`, it seems that the broken import doesn't import not-public member in package, such as redis.client in this case, Redis, StrictRedis are exposed explicitly but redis.client is set \"impliciteley\"\r\n\r\n```python\r\n# redis/__init__.py\r\nfrom redis.client import Redis, StrictRedis\r\n```\r\n\r\nI've found one comment from discuss.python.org:\r\n\r\n> ```python\r\n> import asyncio\r\n> original = id(asyncio)\r\n> from asyncio.base_events import BaseEventLoop\r\n> assert id(asyncio) == original\r\n> assert asyncio.base_events.BaseEventLoop is BaseEventLoop\r\n> ```\r\n>\r\n> From which it should be clear that asyncio.base_events is indeed guaranteed to be set after the from-import.\r\n>\r\n> -- <cite>[discuss.python.org](https://discuss.python.org/t/why-do-relative-from-imports-also-add-the-submodule-itself-to-the-namespace/24440/2)</cite>\r\n\r\nIn our case, if we try to reimport an interrupted import, this isn't true anymore, the submodule isn't set at all. We didn't dig further in internal python to find out why this happens.\r\n\r\nWe see at least four options to fix this bug:\r\n\r\n- Increase celery's operation_timeout (by config or env var). This isn't error-proof but at least reduce drastically the number of this bug\r\n- inject `from redis import client` to `redis/__init__.py`\r\n- patch `kombu/transport/redis.py` with\r\n  ```python\r\n  from redis import client\r\n  ```\r\n  and replace every `redis.client` by `client`\r\n\r\nWe opt for the second method in our dev at the moment, with the same script above we have the diffent output:\r\n\r\n```\r\n...\r\nAfter reimport redis\r\n'redis' in sys.modules: True\r\n'redis.client' in sys.modules: True\r\n'client' in dir(redis): True\r\ngetattr(redis, 'client', None)=<module 'redis.client' from 'XXX/site-packages/redis/client.py'>\r\n...\r\n```\r\n\r\nWe aren't sure that this bug happens enough to be taken into consideration in upstream? But at least other dev won't loose days of debugging session as us ^^\r\n\r\nThis raise another question: Could the Timeout or another mechanism break the import and introduce this bug in another package or another hard-to-catch race condition bug?\r\n[mcve.zip](https://github.com/user-attachments/files/16563669/mcve.zip)\r\n",
      "solution": "The issue downstream seems to be relevant still:\n- https://github.com/apache/airflow/issues/41359#issuecomment-3109676393\n- https://github.com/apache/airflow/issues/41359#issuecomment-3347579368\n- https://github.com/apache/airflow/issues/41359#issuecomment-3484894225",
      "labels": [
        "stale"
      ],
      "created_at": "2024-08-09T15:25:31Z",
      "closed_at": "2025-10-26T00:22:48Z",
      "url": "https://github.com/redis/redis-py/issues/3353",
      "comments_count": 4
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3750,
      "title": "TypeError in sentinel_master on 6.3.0: parse_sentinel_master() got unexpected keyword argument 'return_responses'",
      "problem": "## Description\nCalling `sentinel_master(\"mymaster\")` via StrictRedis (alias of Redis) fails on redis-py 6.3.0 with:\n`TypeError: parse_sentinel_master() got an unexpected keyword argument 'return_responses'`\n\nOther Sentinel calls like `sentinel_masters()` succeed.\n\n## Reproducing\nRedis-py version: 6.3.0 and 6.4.0. \nPython: 3.10\nParser: default & hiredis\n\nMinimal repro:\n```Python\nfrom redis import StrictRedis\nconn = StrictRedis(host='127.0.0.1', port=26379)\nconn.sentinel_masters()       # OK\nconn.sentinel_master('mymaster')  # TypeError in parser\n\n```\n\nthrows the following error:\n```\nTraceback (most recent call last):\n  File \"/home/user/test-redispy/testbug.py\", line 13, in <module>\n    print(conn.sentinel_master(\"mymaster\"))\n  File \"/usr/lib/python3.10/site-packages/redis/commands/sentinel.py\", line 31, in sentinel_master\n    return self.execute_command(\n  File \"/usr/lib/python3.10/site-packages/redis/client.py\", line 621, in execute_command\n    return self._execute_command(*args, **options)\n  File \"/usr/lib/python3.10/site-packages/redis/client.py\", line 632, in _execute_command\n    return conn.retry.call_with_retry(\n  File \"/usr/lib/python3.10/site-packages/redis/retry.py\", line 105, in call_with_retry\n    return do()\n  File \"/usr/lib/python3.10/site-packages/redis/client.py\", line 633, in <lambda>\n    lambda: self._send_command_parse_response(\n  File \"/usr/lib/python3.10/site-packages/redis/client.py\", line 604, in _send_command_parse_response\n    return self.parse_response(conn, command_name, **options)\n  File \"/usr/lib/python3.10/site-packages/redis/client.py\", line 664, in parse_response\n    return self.response_callbacks[command_name](response, **options)\nTypeError: parse_sentinel_master() got an unexpected keyword argument 'return_responses'\n```\n\n## Workaround \n- Pin redis to <=`6.2.0`\n- Or use Sentinel client API:\n  ```Python\n  from redis.sentinel import Sentinel\n  sentinel = Sentinel([(\"127.0.0.1\", 26379)], socket_timeout=1.0)\n  print(sentinel.discover_master(\"mymaster\"))\n  ```\n## Likely Cause (https://github.com/redis/redis-py/pull/3191)\n- `sentinel_master` calls `execute_command` with return_responses in `**options`.\n- `parse_response` forwards **options to the response callback.\n- The callback for `SENTINEL MASTER` does not accept `return_responses`.\n\n### Possible fixes\n- Update the callback to accept `**options`, or\n- Prevent forwarding `return_responses` to the callback for this command.",
      "solution": "Hi @miberl, thanks for reporting this! We will investigate the issue soon.\n\n---\n\nFixed with [#3831](https://github.com/redis/redis-py/pull/3831)",
      "labels": [],
      "created_at": "2025-08-21T14:59:41Z",
      "closed_at": "2025-11-03T16:56:07Z",
      "url": "https://github.com/redis/redis-py/issues/3750",
      "comments_count": 2
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 2883,
      "title": "A deadlock is encountered when invoking the 'subscribe' function.",
      "problem": "**Version**: \r\nredis-py 4.5.5 used in a multi-threads program\r\n\r\n**Platform**: \r\nPython 3.9.7 on Linux 5.10 aarch64\r\n\r\n**Description**: \r\nAt times, invoking the 'subscribe' function may result in a hang, and based on the stack analysis, it appears that a deadlock might be occurring.\r\n\r\n```\r\n...\r\nThread 0x0000ffffaca0f150 (most recent call first):\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/redis/connection.py\", line 1497 in release # try lock self._lock again\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/redis/client.py\", line 1426 in reset\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/redis/client.py\", line 1418 in __del__\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/redis/connection.py\", line 949 in __init__\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/redis/connection.py\", line 1492 in make_connection\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/redis/connection.py\", line 1452 in get_connection # with self._lock\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/redis/client.py\", line 1469 in execute_command\r\n  File \"/root/miniconda3/lib/python3.9/site-packages/redis/client.py\", line 1634 in subscribe\r\n...\r\n```\r\n\r\nClient is created with:\r\n```python\r\nhost=\"localhost\",\r\nport=self._port,\r\npassword=\"*****\",\r\nsingle_connection_client=True,  # might be helpful \r\nsocket_timeout=30,\r\nsocket_connect_timeout=10,\r\nssl=self._is_tls,\r\nssl_ca_certs=f\"xxxxxx/tls/ca.crt\"\r\n```\r\n\r\nMy program:\r\n```python\r\n...\r\npubsub = c.pubsub()\r\npubsub.subscribe(channel) # hang\r\n...\r\n```",
      "solution": "@lan17 i'm running in the same problem with celery and redis as a result backend.\nDid you find any solution?\n\n---\n\n> [@lan17](https://github.com/lan17) i'm running in the same problem with celery and redis as a result backend. Did you find any solution?\n\nI just ended up building https://github.com/lan17/celery-redis-poll and use polling instead of subscribe.\n\nIn my case it was due to a single connection that is issuing subscribe also being used for other operations.  Poll is sync calls to redis, so they don't run into multi threaded issues.\n\n---\n\n> > [@lan17](https://github.com/lan17) i'm running in the same problem with celery and redis as a result backend. Did you find any solution?\n> \n> I just ended up building https://github.com/lan17/celery-redis-poll and use polling instead of subscribe.\n> \n> In my case it was due to a single connection that is issuing subscribe also being used for other operations. Poll is sync calls to redis, so they don't run into multi threaded issues.\n\nYou saved my life! your solution is working beautifully",
      "labels": [],
      "created_at": "2023-08-09T02:31:10Z",
      "closed_at": "2025-06-27T13:08:10Z",
      "url": "https://github.com/redis/redis-py/issues/2883",
      "comments_count": 9
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3591,
      "title": "Dynamically route commands to master or replica",
      "problem": "Hello. I'm trying to understand if there's any way to programmatically send a command to a replica node using a `RedisCluster`. I'm aware of the `read_from_replicas` configuration, but there are a few issues:\n* I can't send all reads to the replicas. There are a few use cases in my application that have strong consistency requirements, and others that don't.\n* Alternatively, I could have one client that uses `read_from_replicas=True` and one that uses `read_from_replicas=False`. However, now every process in my fleet has 2 connections to every node, doubling overall connections. For a large fleet, this is a problem as you can easily exceed the max number of connections.\n\nIs there any workaround for this? If not, is it something you'd be open to supporting in future versions?",
      "solution": "@petyaslavova thank you. I'll take a look.\n\nJust to clarify, I'm not looking to send a subset of Redis commands to replicas. Essentially I want to enable/disable `read_from_replicas` depending on the use case.\n\nThe problem is pretty simply that the client has a single global config. If it could be controlled on a per-command basis, it would be solved.\n\nPseudo-code:\n```\nmy_redis_client = RedisCluster(..., read_from_replicas=False)\n\ndef function_that_can_be_eventually_consistent():\n  return redis.get(\"key\", read_from_replicas=True)\n\ndef function_that_must_hit_leader():\n  return redis.get(\"key\")\n```\n",
      "labels": [],
      "created_at": "2025-04-09T00:10:14Z",
      "closed_at": "2025-10-16T16:46:08Z",
      "url": "https://github.com/redis/redis-py/issues/3591",
      "comments_count": 5
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3780,
      "title": "*arg types in redis/commands/core.py and redis/commands/json/commands.py appear to be wrong",
      "problem": "PEP484 says  \n\n> When using the short form, for *args and **kwds, put 1 or 2 stars in front of the corresponding type annotation. (As with Python 3 annotations, the annotation here denotes the type of the individual argument values, not of the tuple/dict that you receive as the special argument value args or kwd\n\nhttps://peps.python.org/pep-0484/#arbitrary-argument-lists-and-default-argument-values\n\nExample mypy error:\n` Argument 3 to \"blmpop\" of \"ListCommands\" has incompatible type \"str\"; expected \"list[str]\"`\n\nSo I think this change should fix it:\n\n\n```diff\ndiff --git c/redis/commands/core.py w/redis/commands/core.py\nindex 737b098..4ba6430 100644\n--- c/redis/commands/core.py\n+++ w/redis/commands/core.py\n@@ -830,7 +830,7 @@ class ManagementCommands(CommandsProtocol):\n \n         return self.execute_command(\"COMMAND LIST\", *pieces)\n \n-    def command_getkeysandflags(self, *args: List[str]) -> List[Union[str, List[str]]]:\n+    def command_getkeysandflags(self, *args: str) -> List[Union[str, List[str]]]:\n         \"\"\"\n         Returns array of keys from a full Redis command and their usage flags.\n \n@@ -848,7 +848,7 @@ class ManagementCommands(CommandsProtocol):\n         )\n \n     def config_get(\n-        self, pattern: PatternT = \"*\", *args: List[PatternT], **kwargs\n+        self, pattern: PatternT = \"*\", *args: PatternT, **kwargs\n     ) -> ResponseT:\n         \"\"\"\n         Return a dictionary of configuration based on the ``pattern``\n@@ -861,7 +861,7 @@ class ManagementCommands(CommandsProtocol):\n         self,\n         name: KeyT,\n         value: EncodableT,\n-        *args: List[Union[KeyT, EncodableT]],\n+        *args: Union[KeyT, EncodableT],\n         **kwargs,\n     ) -> ResponseT:\n         \"\"\"Set config item ``name`` with ``value``\n@@ -987,9 +987,7 @@ class ManagementCommands(CommandsProtocol):\n         \"\"\"\n         return self.execute_command(\"SELECT\", index, **kwargs)\n \n-    def info(\n-        self, section: Optional[str] = None, *args: List[str], **kwargs\n-    ) -> ResponseT:\n+    def info(self, section: Optional[str] = None, *args: str, **kwargs) -> ResponseT:\n         \"\"\"\n         Returns a dictionary containing information about the Redis server\n \n@@ -2599,7 +2597,7 @@ class ListCommands(CommandsProtocol):\n         self,\n         timeout: float,\n         numkeys: int,\n-        *args: List[str],\n+        *args: str,\n         direction: str,\n         count: Optional[int] = 1,\n     ) -> Optional[list]:\n@@ -2619,7 +2617,7 @@ class ListCommands(CommandsProtocol):\n     def lmpop(\n         self,\n         num_keys: int,\n-        *args: List[str],\n+        *args: str,\n         direction: str,\n         count: Optional[int] = 1,\n     ) -> Union[Awaitable[list], list]:\ndiff --git c/redis/commands/json/commands.py w/redis/commands/json/commands.py\nindex 48849e1..f278daf 100644\n--- c/redis/commands/json/commands.py\n+++ w/redis/commands/json/commands.py\n@@ -14,7 +14,7 @@ class JSONCommands:\n     \"\"\"json commands.\"\"\"\n \n     def arrappend(\n-        self, name: str, path: Optional[str] = Path.root_path(), *args: List[JsonType]\n+        self, name: str, path: Optional[str] = Path.root_path(), *args: JsonType\n     ) -> List[Optional[int]]:\n         \"\"\"Append the objects ``args`` to the array under the\n         ``path` in key ``name``.\n@@ -52,7 +52,7 @@ class JSONCommands:\n         return self.execute_command(\"JSON.ARRINDEX\", *pieces, keys=[name])\n \n     def arrinsert(\n-        self, name: str, path: str, index: int, *args: List[JsonType]\n+        self, name: str, path: str, index: int, *args: JsonType\n     ) -> List[Optional[int]]:\n         \"\"\"Insert the objects ``args`` to the array at index ``index``\n         under the ``path` in key ``name``.\n```\n",
      "solution": "Hi @jogo-openai, thanks for reporting this! We will have a look at the problem soon.",
      "labels": [],
      "created_at": "2025-10-01T17:34:14Z",
      "closed_at": "2025-10-14T15:00:01Z",
      "url": "https://github.com/redis/redis-py/issues/3780",
      "comments_count": 1
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3430,
      "title": "Connection pool garbage collection of idle connections",
      "problem": "**Version**: What redis-py and what redis version is the issue happening on?\r\nredis-py 5.2.0 with redis 7.2.4\r\n\r\n**Platform**: What platform / version?\r\nPython 3.10.12 on Ubuntu 22.04\r\n\r\n**Description**:\r\n\r\nHey everyone,\r\n\r\nI have an application that uses a Redis connection created from a connection from a connection pool. The pool works as expected, but there is a point during the lifetime of my application where a connection is not used for some time (i.e., it stays in the pool as an available connection). There is some configuration that I am not finding to discard unused connections after some time? Or do I have to manually close those connections? \r\n\r\nBest regards,\r\nJorge",
      "solution": "Since the question has been answered, I\u2019ll close the issue for now. Feel free to reopen it if you need further assistance.",
      "labels": [
        "question"
      ],
      "created_at": "2024-11-08T13:37:30Z",
      "closed_at": "2025-10-09T13:12:18Z",
      "url": "https://github.com/redis/redis-py/issues/3430",
      "comments_count": 2
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3715,
      "title": "how to override decode_responses on a client instance when using a shared ConnectionPool",
      "problem": "I'm opening this issue not to report a bug, but to suggest a documentation improvement based on a common point of confusion for users, including myself.\n\n```\nimport redis\n\npool = redis.ConnectionPool(host='localhost', port=6379, db=0, decode_responses=True)\n\nr_bytes = redis.Redis(connection_pool=pool, decode_responses=False)\nkey = 'my_binary_data'\npoison_value = b'data-\\x80-bytes'\nr_bytes.set(key, poison_value)\n\nr_bytes.get(key)\n```",
      "solution": "Hi! Since we haven\u2019t received any response for a while, I\u2019ll go ahead and close this issue for now.\nIf you\u2019re still experiencing the problem or have additional details to share, please reopen it or create a new one.\n",
      "labels": [
        "needs-information"
      ],
      "created_at": "2025-07-21T07:24:34Z",
      "closed_at": "2025-10-09T13:01:17Z",
      "url": "https://github.com/redis/redis-py/issues/3715",
      "comments_count": 4
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3759,
      "title": "Difficulties with Async Sentinel Connections",
      "problem": "Hello!\n\nWe met some difficulties with async sentinel connection.\n\nIn some module we generate global Redis connection like this:\n\n```\nfrom redis.retry import Retry\nfrom redis.backoff import ExponentialBackoff\nfrom redis.asyncio.sentinel import Sentinel\nfrom redis.asyncio import Redis, BlockingConnectionPool\nimport redis.exceptions as redis_exceptions\n\nfrom config.vars import app_vars\n\nretry = Retry(ExponentialBackoff(), 3)\n\nsentinel = Sentinel(\n    sentinels=[\n        (\n            host[0],\n            int(app_vars['REDIS_SENTINEL_PORT']) if app_vars.get('REDIS_SENTINEL_PORT') else 26379\n        )\n        for host in app_vars['REDIS_HOST']\n    ],\n    socket_timeout=REDIS_SOCKET_TIMEOUT\n)\n\n\nredis_client = sentinel.master_for(\n    app_vars['REDIS_SERVICE_NAME'],\n    socket_timeout=REDIS_SOCKET_TIMEOUT,\n    username=app_vars['REDIS_USERNAME'],\n    password=app_vars['REDIS_PASSWORD'],\n    db=app_vars['REDIS_DB'],\n    retry_on_error=[\n        redis_exceptions.BusyLoadingError,\n        redis_exceptions.ConnectionError,\n        redis_exceptions.TimeoutError,\n        redis_exceptions.ReadOnlyError,\n        redis_exceptions.MasterDownError\n    ],\n    redis_class=Redis,\n    retry_on_timeout=True,\n    socket_keepalive=True\n)\n```\n\nAfter importing this connection into other modules and simply using the await redis_client.set() type, we encounter an overflow of the maximum number of clients in Redis.\nIf we use the async with redis_client, then quite often we get an error that the client is already closed.\n\nWhat are we doing wrong and how to fix this situation?\n\nIt is worth adding that in one of the modules we generate many asynchronous tasks, in each of which we access Redis and wait for them via asyncio.gather.",
      "solution": "Hi @vorobevnd,\n\nWhen you call `sentinel.master_for`, a new client is created each time. From your explanation and example, it\u2019s not entirely clear whether you\u2019re executing this call multiple times on every import or reusing the same client. My guess is that you might be creating a new client for each task import, which could lead to the error you mentioned.\n\nRegarding the issue you described \u2014 `\u201cIf we use the async with redis_client, then quite often we get an error that the client is already closed\u201d` \u2014 we recently implemented a fix for this, and it should be released soon.\n\n---\n\nHi! Since we haven\u2019t received any response for a while, I\u2019ll go ahead and close this issue for now.\nIf you\u2019re still experiencing the problem or have additional details to share, please reopen it or create a new one.",
      "labels": [
        "needs-information"
      ],
      "created_at": "2025-08-29T14:43:58Z",
      "closed_at": "2025-10-08T06:05:58Z",
      "url": "https://github.com/redis/redis-py/issues/3759",
      "comments_count": 3
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3685,
      "title": "Asking for an option to deactivate x509 strict mode for TLS connections",
      "problem": "Hello,\n\nWe are using Google Cloud for our python jobs using Redis. When we tried to upgrade some of our code to Python 3.13 we noticed that our jobs could not access our hosted Redis instance.\n\nThis is related to the Python 3.13 change: https://docs.python.org/3/whatsnew/3.13.html#ssl\n\nThe `VERIFY_X509_STRICT` is now used by default by Python 3.13 and unfortunately the certificates generated by the cloud provider don't have that TLS x509 extension.\n\nCurrently:\n\n- I can't recreate the server certificate which is handled by the cloud provider\n- I can't find a parameter on the redis-py client to deactivate the x509 strict mode\n- I'm stuck with Python 3.12\n\nOne solution would be to completely deactivate the certificate checking but this is something we can't do for security purpose.\n\nWould it be possible to have a way to deactivate x509 strict mode inside Redis-py ?\n\nI looked at existing parameters and didn't see something helpful.\n\nThe error message we get:\n\n`redis.exceptions.ConnectionError: Error 1 connecting to redis-production:6378. [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Missing Authority Key Identifier (_ssl.c:1028).`\n\nBest regards",
      "solution": "Hacky workaround I'm using temporarily until this is fixed:\n\n```python\nimport os\nimport ssl\nfrom typing import Mapping\n\nfrom redis.asyncio import Redis as AsyncRedis\nfrom redis.asyncio import SSLConnection\n\nclass RelaxedSSLConnection(SSLConnection):\n    def _connection_arguments(self) -> Mapping:\n        kwargs = super()._connection_arguments()\n        ssl_context = self.ssl_context.get()\n        if hasattr(ssl, \"VERIFY_X509_STRICT\"):\n            ssl_context.verify_flags = ssl_context.verify_flags & ~ssl.VERIFY_X509_STRICT\n\n        if hasattr(ssl, \"VERIFY_X509_PARTIAL_CHAIN\"):\n            ssl_context.verify_flags = ssl_context.verify_flags & ~ssl.VERIFY_X509_PARTIAL_CHAIN\n\n        kwargs[\"ssl\"] = ssl_context  # type: ignore\n        return kwargs\n\n\n_redis_connection = AsyncRedis(\n    host=HOST,\n    port=PORT,\n    ssl=True,\n    ssl_ca_certs=os.path.expanduser(REDIS_TLS_CERT_LOCATION),\n    ssl_cert_reqs=\"required\",\n)\n_redis_connection.connection_pool.connection_class = RelaxedSSLConnection\n```",
      "labels": [
        "feature"
      ],
      "created_at": "2025-06-25T14:31:53Z",
      "closed_at": "2025-09-26T11:32:07Z",
      "url": "https://github.com/redis/redis-py/issues/3685",
      "comments_count": 4
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 2871,
      "title": "pubsub.get_message() sometimes doesn't receive messages",
      "problem": "**Version**: redis-py 4.5.1, redis-stack \r\n\r\n**Platform**: Python 3.9.12 on Ubuntu 20.4\r\n\r\n**Description**: \r\nTo give some context, I have a notifier that publishes a user_info dict in redis every time there is an update in a user. I can see this working fine using redis-stack.\r\n\r\nThen I have a websocket that receives a user connection and upon connecting it connects to redis and subscribes to that user's channel like so:\r\n\r\n```python\r\n async def subscribe_redis(self, channel: str):\r\n        try:\r\n            r = aioredis.from_url(settings.REDIS_HOST)\r\n            self.redis = r\r\n            \r\n            pubsub = r.pubsub()\r\n            await pubsub.subscribe(channel)\r\n\r\n            asyncio.create_task(self.reader(pubsub))\r\n\r\n        except Exception as ex:\r\n            logging.info(f\"Error while subscribing to Redis: {str(ex)}\")\r\n        \r\n        return True\r\n```\r\n\r\nUpon subscribing to the channel, it waits for messages (in this case, a user_info). The reader function:\r\n\r\n```python\r\nasync def reader(self, pubsub: aioredis.client.PubSub):\r\n        while True:\r\n            try:                \r\n                if not self.websocket:\r\n                    break\r\n\r\n                message = await pubsub.get_message(\r\n                    ignore_subscribe_messages=True,\r\n                    timeout=1.0\r\n                )\r\n\r\n                if message is not None:\r\n                    user_info = eval(message[\"data\"])\r\n                    await self.send_user_info(user_info)\r\n            \r\n            except Exception as ex:\r\n                logging.info(str(ex))\r\n                break\r\n\r\n        return True\r\n``` \r\n\r\nThe expected functioning is: I make a change in the user inside my database, the notifier publishes the new user_info in the user's redis pubsub channel, the get_message() receives the user_info.\r\n\r\nHowever, something weird happens: sometimes it works, sometimes it doesn't. \r\nWithin the same websocket connection, when it works, it always works, meaning the get_message() always gets every new message; and within the same websocket connection, when it doesn't work, it never works, meaning the get_message never receives a message from the channel, even though the new message is being successfully published in redis.\r\n\r\nAnother weird thing is that in the case when it doesn't work, the redis connection is there and pubsub as well, if I make a \r\n```redis.ping() ```\r\nand a\r\n``` pubsub.ping() ```\r\ninside the while loop I get a response.\r\n\r\nNote that I call the subscribe_redis() function like this:\r\n```python\r\nself.task = asyncio.create_task(\r\n            self.subscribe_redis(redis_channel)\r\n        ) \r\n```\r\n\r\nIs there something I am not seeing?\r\n",
      "solution": "Any Solution ?\r\nI am also facing the same problem, even when I am not doing anything in async\n\n---\n\n> Any Solution ? I am also facing the same problem, even when I am not doing anything in async\r\n\r\nBecause I also have control over the publishers, and my business need allows for it, I added a small throttle before each publish to space them out a bit and it has resolved it... But that's definitely a workaround, as pubsub should be able to handle millions of ops/sec and the PubSub should buffer the messages, but I couldn't get it to work otherwise.\n\n---\n\nThe Redis Stack UI is able to pick up these publishes without any time delay, I think there must be a solution too this without time delays ",
      "labels": [
        "stale",
        "question"
      ],
      "created_at": "2023-08-02T16:44:31Z",
      "closed_at": "2025-09-01T00:25:51Z",
      "url": "https://github.com/redis/redis-py/issues/2871",
      "comments_count": 7
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3729,
      "title": "At least one redis.io URL in a docstring was 404, not found",
      "problem": "While copy editing docstrings, @petyaslavova and I happened to find a URL that redirected to a 404 Not Found error.\r\n\r\nFurthermore, all of the URLs had 301, permanent redirects.\r\n\r\nAnd, all of the permanent redirects were written without a trailing slash, so those triggered a 302, temporary redirect, to add the trailing slash.\r\n\r\nIf you don't already have processes, y'all may want to consider link-rot tools.\r\n\r\n## Summary of final URLs\r\n\r\n```cmd\r\nC:\\>curl -sL https://redis.io/commands/vadd -o /dev/null -w \"%{http_code} %{url_effective}\\n\"\r\n200 https://redis.io/docs/latest/commands/vadd/\r\n\r\nC:\\>curl -sL https://redis.io/commands/vcard -o /dev/null -w \"%{http_code} %{url_effective}\\n\"\r\n200 https://redis.io/docs/latest/commands/vcard/\r\n\r\nC:\\>curl -sL https://redis.io/commands/vdim -o /dev/null -w \"%{http_code} %{url_effective}\\n\"\r\n200 https://redis.io/docs/latest/commands/vdim/\r\n\r\nC:\\>curl -sL https://redis.io/commands/vembed -o /dev/null -w \"%{http_code} %{url_effective}\\n\"\r\n404 https://redis.io/docs/latest/commands/vembed/\r\n\r\nC:\\>curl -sL https://redis.io/commands/vgetattr -o /dev/null -w \"%{http_code} %{url_effective}\\n\"\r\n200 https://redis.io/docs/latest/commands/vgetattr/\r\n\r\nC:\\>curl -sL https://redis.io/commands/vinfo -o /dev/null -w \"%{http_code} %{url_effective}\\n\"\r\n200 https://redis.io/docs/latest/commands/vinfo/\r\n\r\nC:\\>curl -sL https://redis.io/commands/vlinks -o /dev/null -w \"%{http_code} %{url_effective}\\n\"\r\n200 https://redis.io/docs/latest/commands/vlinks/\r\n\r\nC:\\>curl -sL https://redis.io/commands/vrandmember -o /dev/null -w \"%{http_code} %{url_effective}\\n\"\r\n200 https://redis.io/docs/latest/commands/vrandmember/\r\n\r\nC:\\>curl -sL https://redis.io/commands/vrem -o /dev/null -w \"%{http_code} %{url_effective}\\n\"\r\n200 https://redis.io/docs/latest/commands/vrem/\r\n\r\nC:\\>curl -sL https://redis.io/commands/vsetattr -o /dev/null -w \"%{http_code} %{url_effective}\\n\"\r\n200 https://redis.io/docs/latest/commands/vsetattr/\r\n\r\nC:\\>curl -sL https://redis.io/commands/vsim -o /dev/null -w \"%{http_code} %{url_effective}\\n\"\r\n200 https://redis.io/docs/latest/commands/vsim/\r\n```\r\n\r\n## Extended URL headers\r\n\r\n<details><summary>All URLs have 301 to 302 re-writes</summary>\r\n<p>\r\n\r\n```cmd\r\nC:\\>curl -sLI https://redis.io/commands/vadd\r\nHTTP/1.1 301 Moved Permanently\r\nDate: Wed, 06 Aug 2025 15:27:36 GMT\r\nContent-Type: text/html\r\nContent-Length: 167\r\nConnection: keep-alive\r\nCache-Control: max-age=3600\r\nExpires: Wed, 06 Aug 2025 16:27:36 GMT\r\nLocation: https://redis.io/docs/latest/commands/vadd\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nServer: cloudflare\r\nCF-RAY: 96af8f2e9979a527-MIA\r\n\r\nHTTP/1.1 302 Moved Temporarily\r\nDate: Wed, 06 Aug 2025 15:27:36 GMT\r\nContent-Type: text/html\r\nContent-Length: 143\r\nConnection: keep-alive\r\nCache-Control: private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0\r\nExpires: Thu, 01 Jan 1970 00:00:01 GMT\r\nLocation: https://redis.io/docs/latest/commands/vadd/\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nServer: cloudflare\r\nCF-RAY: 96af8f2f4a2aa527-MIA\r\n\r\nHTTP/1.1 200 OK\r\nDate: Wed, 06 Aug 2025 15:27:37 GMT\r\nContent-Type: text/html\r\nConnection: keep-alive\r\nCF-Ray: 96af8f2feb15a527-MIA\r\nCF-Cache-Status: DYNAMIC\r\nAccept-Ranges: bytes\r\nCache-Control: public, max-age=3600\r\nExpires: Wed, 06 Aug 2025 16:27:37 GMT\r\nLast-Modified: Wed, 06 Aug 2025 15:23:47 GMT\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nx-goog-generation: 1754493827031567\r\nx-goog-hash: crc32c=ULxP6g==\r\nx-goog-hash: md5=2kMJtcBYhROPLTPo/vxISg==\r\nx-goog-meta-goog-reserved-file-mtime: 1754493585\r\nx-goog-metageneration: 1\r\nx-goog-storage-class: STANDARD\r\nx-goog-stored-content-encoding: identity\r\nx-goog-stored-content-length: 194155\r\nx-guploader-uploadid: ABgVH889z9HVYKi3c2vzCu1a3eOBmH08XXlzJcWOzhuKQA3z5g5oeiwMT0QsRYGezslMxbAs9uayWqk\r\nServer: cloudflare\r\n\r\n\r\nC:\\>curl -sLI https://redis.io/commands/vcard\r\nHTTP/1.1 301 Moved Permanently\r\nDate: Wed, 06 Aug 2025 15:27:37 GMT\r\nContent-Type: text/html\r\nContent-Length: 167\r\nConnection: keep-alive\r\nCache-Control: max-age=3600\r\nExpires: Wed, 06 Aug 2025 16:27:37 GMT\r\nLocation: https://redis.io/docs/latest/commands/vcard\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nServer: cloudflare\r\nCF-RAY: 96af8f34ba77ac95-MIA\r\n\r\nHTTP/1.1 302 Moved Temporarily\r\nDate: Wed, 06 Aug 2025 15:27:37 GMT\r\nContent-Type: text/html\r\nContent-Length: 143\r\nConnection: keep-alive\r\nCache-Control: private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0\r\nExpires: Thu, 01 Jan 1970 00:00:01 GMT\r\nLocation: https://redis.io/docs/latest/commands/vcard/\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nServer: cloudflare\r\nCF-RAY: 96af8f356c65ac95-MIA\r\n\r\nHTTP/1.1 200 OK\r\nDate: Wed, 06 Aug 2025 15:27:38 GMT\r\nContent-Type: text/html\r\nConnection: keep-alive\r\nCF-Ray: 96af8f360de5ac95-MIA\r\nCF-Cache-Status: DYNAMIC\r\nAccept-Ranges: bytes\r\nCache-Control: public, max-age=3600\r\nExpires: Wed, 06 Aug 2025 16:27:38 GMT\r\nLast-Modified: Wed, 06 Aug 2025 15:23:47 GMT\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nx-goog-generation: 1754493827081902\r\nx-goog-hash: crc32c=5BBFsA==\r\nx-goog-hash: md5=CXURYGSpgI4b9M0IcrxXvw==\r\nx-goog-meta-goog-reserved-file-mtime: 1754493585\r\nx-goog-metageneration: 1\r\nx-goog-storage-class: STANDARD\r\nx-goog-stored-content-encoding: identity\r\nx-goog-stored-content-length: 188571\r\nx-guploader-uploadid: ABgVH8_m6mMe7vwep7kcxG6iz3WngPTY4x8qHHSd4j4URlSKykKRek2UCXXSn4AhNPao0vvM1jIm0Ek\r\nServer: cloudflare\r\n\r\n\r\nC:\\>curl -sLI https://redis.io/commands/vdim\r\nHTTP/1.1 301 Moved Permanently\r\nDate: Wed, 06 Aug 2025 15:27:38 GMT\r\nContent-Type: text/html\r\nContent-Length: 167\r\nConnection: keep-alive\r\nCache-Control: max-age=3600\r\nExpires: Wed, 06 Aug 2025 16:27:38 GMT\r\nLocation: https://redis.io/docs/latest/commands/vdim\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nServer: cloudflare\r\nCF-RAY: 96af8f3a5dd43d8b-MIA\r\n\r\nHTTP/1.1 302 Moved Temporarily\r\nDate: Wed, 06 Aug 2025 15:27:38 GMT\r\nContent-Type: text/html\r\nContent-Length: 143\r\nConnection: keep-alive\r\nCache-Control: private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0\r\nExpires: Thu, 01 Jan 1970 00:00:01 GMT\r\nLocation: https://redis.io/docs/latest/commands/vdim/\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nServer: cloudflare\r\nCF-RAY: 96af8f3af8233d8b-MIA\r\n\r\nHTTP/1.1 200 OK\r\nDate: Wed, 06 Aug 2025 15:27:39 GMT\r\nContent-Type: text/html\r\nConnection: keep-alive\r\nServer: cloudflare\r\nCF-Ray: 96af8f3b8a413d8b-MIA\r\nCF-Cache-Status: DYNAMIC\r\nAccept-Ranges: bytes\r\nCache-Control: public, max-age=3600\r\nExpires: Wed, 06 Aug 2025 16:27:39 GMT\r\nLast-Modified: Wed, 06 Aug 2025 15:23:47 GMT\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nx-goog-generation: 1754493827124710\r\nx-goog-hash: crc32c=0vuxHA==\r\nx-goog-hash: md5=k87GrRQAVGbCxm816lmKjA==\r\nx-goog-meta-goog-reserved-file-mtime: 1754493585\r\nx-goog-metageneration: 1\r\nx-goog-storage-class: STANDARD\r\nx-goog-stored-content-encoding: identity\r\nx-goog-stored-content-length: 189072\r\nx-guploader-uploadid: ABgVH8_Qsi3h6FXXxs5bdiHvTgHeJ1UvgbFVq-TzBKk0H5LBiWdJ2hy5UFH7i5zbCftqp_E6w95yQdM\r\n\r\n\r\nC:\\>curl -sLI https://redis.io/commands/vembed\r\nHTTP/1.1 301 Moved Permanently\r\nDate: Wed, 06 Aug 2025 15:27:39 GMT\r\nContent-Type: text/html\r\nContent-Length: 167\r\nConnection: keep-alive\r\nCache-Control: max-age=3600\r\nExpires: Wed, 06 Aug 2025 16:27:39 GMT\r\nLocation: https://redis.io/docs/latest/commands/vembed\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nServer: cloudflare\r\nCF-RAY: 96af8f404ef660eb-MIA\r\n\r\nHTTP/1.1 302 Moved Temporarily\r\nDate: Wed, 06 Aug 2025 15:27:39 GMT\r\nContent-Type: text/html\r\nContent-Length: 143\r\nConnection: keep-alive\r\nCache-Control: private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0\r\nExpires: Thu, 01 Jan 1970 00:00:01 GMT\r\nLocation: https://redis.io/docs/latest/commands/vembed/\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nServer: cloudflare\r\nCF-RAY: 96af8f41599060eb-MIA\r\n\r\nHTTP/1.1 404 Not Found\r\nDate: Wed, 06 Aug 2025 15:27:40 GMT\r\nContent-Type: text/html\r\nConnection: keep-alive\r\nX-Frame-Options: SAMEORIGIN\r\nReferrer-Policy: same-origin\r\nCache-Control: private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0\r\nExpires: Thu, 01 Jan 1970 00:00:01 GMT\r\nCF-RAY: 96af8f41eb0260eb-MIA\r\nServer: cloudflare\r\n\r\n\r\nC:\\>curl -sLI https://redis.io/commands/vgetattr\r\nHTTP/1.1 301 Moved Permanently\r\nDate: Wed, 06 Aug 2025 15:27:40 GMT\r\nContent-Type: text/html\r\nContent-Length: 167\r\nConnection: keep-alive\r\nCache-Control: max-age=3600\r\nExpires: Wed, 06 Aug 2025 16:27:40 GMT\r\nLocation: https://redis.io/docs/latest/commands/vgetattr\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nServer: cloudflare\r\nCF-RAY: 96af8f45c938d327-MIA\r\n\r\nHTTP/1.1 302 Moved Temporarily\r\nDate: Wed, 06 Aug 2025 15:27:40 GMT\r\nContent-Type: text/html\r\nContent-Length: 143\r\nConnection: keep-alive\r\nCache-Control: private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0\r\nExpires: Thu, 01 Jan 1970 00:00:01 GMT\r\nLocation: https://redis.io/docs/latest/commands/vgetattr/\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nServer: cloudflare\r\nCF-RAY: 96af8f466b2ad327-MIA\r\n\r\nHTTP/1.1 200 OK\r\nDate: Wed, 06 Aug 2025 15:27:40 GMT\r\nContent-Type: text/html\r\nConnection: keep-alive\r\nCF-Ray: 96af8f46fcb4d327-MIA\r\nCF-Cache-Status: DYNAMIC\r\nAccept-Ranges: bytes\r\nCache-Control: public, max-age=3600\r\nExpires: Wed, 06 Aug 2025 16:27:40 GMT\r\nLast-Modified: Wed, 06 Aug 2025 15:23:47 GMT\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nx-goog-generation: 1754493827229397\r\nx-goog-hash: crc32c=bQJwvA==\r\nx-goog-hash: md5=eR+EcDq42MuWINwLnK0nsw==\r\nx-goog-meta-goog-reserved-file-mtime: 1754493585\r\nx-goog-metageneration: 1\r\nx-goog-storage-class: STANDARD\r\nx-goog-stored-content-encoding: identity\r\nx-goog-stored-content-length: 189052\r\nx-guploader-uploadid: ABgVH8-fzjw_RTlJUrCG659XMF_R2Xv7Bf-qt1ixYj-1TM1XZP40cRQJL2KtEM_k4ViKKQLNml0rZJM\r\nServer: cloudflare\r\n\r\n\r\nC:\\>curl -sLI https://redis.io/commands/vinfo\r\nHTTP/1.1 301 Moved Permanently\r\nDate: Wed, 06 Aug 2025 15:27:41 GMT\r\nContent-Type: text/html\r\nContent-Length: 167\r\nConnection: keep-alive\r\nCache-Control: max-age=3600\r\nExpires: Wed, 06 Aug 2025 16:27:41 GMT\r\nLocation: https://redis.io/docs/latest/commands/vinfo\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nServer: cloudflare\r\nCF-RAY: 96af8f4a69d6c3a9-MIA\r\n\r\nHTTP/1.1 302 Moved Temporarily\r\nDate: Wed, 06 Aug 2025 15:27:41 GMT\r\nContent-Type: text/html\r\nContent-Length: 143\r\nConnection: keep-alive\r\nCache-Control: private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0\r\nExpires: Thu, 01 Jan 1970 00:00:01 GMT\r\nLocation: https://redis.io/docs/latest/commands/vinfo/\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nServer: cloudflare\r\nCF-RAY: 96af8f4b1c39c3a9-MIA\r\n\r\nHTTP/1.1 200 OK\r\nDate: Wed, 06 Aug 2025 15:27:41 GMT\r\nContent-Type: text/html\r\nConnection: keep-alive\r\nCF-Ray: 96af8f4bbe6dc3a9-MIA\r\nCF-Cache-Status: DYNAMIC\r\nAccept-Ranges: bytes\r\nCache-Control: public, max-age=3600\r\nExpires: Wed, 06 Aug 2025 16:27:41 GMT\r\nLast-Modified: Wed, 06 Aug 2025 15:23:47 GMT\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nx-goog-generation: 1754493827047698\r\nx-goog-hash: crc32c=YW061A==\r\nx-goog-hash: md5=yD7ce3no8aDmkhx9a73peA==\r\nx-goog-meta-goog-reserved-file-mtime: 1754493585\r\nx-goog-metageneration: 1\r\nx-goog-storage-class: STANDARD\r\nx-goog-stored-content-encoding: identity\r\nx-goog-stored-content-length: 190266\r\nx-guploader-uploadid: ABgVH89VcNxoeRGA7Y5YwUrbB7o87c8ZzrSgpf3L6PkpNRLiK4mf-jqZ2nWk3J9AVW6vCAzcecndzo0\r\nServer: cloudflare\r\n\r\n\r\nC:\\>curl -sLI https://redis.io/commands/vlinks\r\nHTTP/1.1 301 Moved Permanently\r\nDate: Wed, 06 Aug 2025 15:27:41 GMT\r\nContent-Type: text/html\r\nContent-Length: 167\r\nConnection: keep-alive\r\nCache-Control: max-age=3600\r\nExpires: Wed, 06 Aug 2025 16:27:41 GMT\r\nLocation: https://redis.io/docs/latest/commands/vlinks\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nServer: cloudflare\r\nCF-RAY: 96af8f4f4b4174a6-MIA\r\n\r\nHTTP/1.1 302 Moved Temporarily\r\nDate: Wed, 06 Aug 2025 15:27:42 GMT\r\nContent-Type: text/html\r\nContent-Length: 143\r\nConnection: keep-alive\r\nCache-Control: private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0\r\nExpires: Thu, 01 Jan 1970 00:00:01 GMT\r\nLocation: https://redis.io/docs/latest/commands/vlinks/\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nServer: cloudflare\r\nCF-RAY: 96af8f4ffc0574a6-MIA\r\n\r\nHTTP/1.1 200 OK\r\nDate: Wed, 06 Aug 2025 15:27:42 GMT\r\nContent-Type: text/html\r\nConnection: keep-alive\r\nCF-Ray: 96af8f50acee74a6-MIA\r\nCF-Cache-Status: DYNAMIC\r\nAccept-Ranges: bytes\r\nCache-Control: public, max-age=3600\r\nExpires: Wed, 06 Aug 2025 16:27:42 GMT\r\nLast-Modified: Wed, 06 Aug 2025 15:23:47 GMT\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nx-goog-generation: 1754493827109394\r\nx-goog-hash: crc32c=o5aR2g==\r\nx-goog-hash: md5=ROXj7JMgQaUni5FvxfTp/g==\r\nx-goog-meta-goog-reserved-file-mtime: 1754493585\r\nx-goog-metageneration: 1\r\nx-goog-storage-class: STANDARD\r\nx-goog-stored-content-encoding: identity\r\nx-goog-stored-content-length: 190847\r\nx-guploader-uploadid: ABgVH8_sI_7sEMh-bSMxkwp9nWWR48CPyS8TKWlf0O_mNHP90vDHkaPnDqfgjvYS1Puxrn7hhFNddLY\r\nServer: cloudflare\r\n\r\n\r\nC:\\>curl -sLI https://redis.io/commands/vrandmember\r\nHTTP/1.1 301 Moved Permanently\r\nDate: Wed, 06 Aug 2025 15:27:42 GMT\r\nContent-Type: text/html\r\nContent-Length: 167\r\nConnection: keep-alive\r\nCache-Control: max-age=3600\r\nExpires: Wed, 06 Aug 2025 16:27:42 GMT\r\nLocation: https://redis.io/docs/latest/commands/vrandmember\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nServer: cloudflare\r\nCF-RAY: 96af8f53dda63f41-MIA\r\n\r\nHTTP/1.1 302 Moved Temporarily\r\nDate: Wed, 06 Aug 2025 15:27:43 GMT\r\nContent-Type: text/html\r\nContent-Length: 143\r\nConnection: keep-alive\r\nCache-Control: private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0\r\nExpires: Thu, 01 Jan 1970 00:00:01 GMT\r\nLocation: https://redis.io/docs/latest/commands/vrandmember/\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nServer: cloudflare\r\nCF-RAY: 96af8f55ca9f3f41-MIA\r\n\r\nHTTP/1.1 200 OK\r\nDate: Wed, 06 Aug 2025 15:27:43 GMT\r\nContent-Type: text/html\r\nConnection: keep-alive\r\nCF-Ray: 96af8f565c283f41-MIA\r\nCF-Cache-Status: DYNAMIC\r\nAccept-Ranges: bytes\r\nCache-Control: public, max-age=3600\r\nExpires: Wed, 06 Aug 2025 16:27:43 GMT\r\nLast-Modified: Wed, 06 Aug 2025 15:23:47 GMT\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nx-goog-generation: 1754493827150308\r\nx-goog-hash: crc32c=nE2HfQ==\r\nx-goog-hash: md5=P9A7xtMO9wHiANo3lVIZmQ==\r\nx-goog-meta-goog-reserved-file-mtime: 1754493585\r\nx-goog-metageneration: 1\r\nx-goog-storage-class: STANDARD\r\nx-goog-stored-content-encoding: identity\r\nx-goog-stored-content-length: 194527\r\nx-guploader-uploadid: ABgVH8_MsDHB0JQn6s5LPpBA6CU0rDSjDQMTa2n2af0hXy0fR_8N3LcmQAHg5fD8NARXxgvbtgH4obM\r\nServer: cloudflare\r\n\r\n\r\nC:\\>curl -sLI https://redis.io/commands/vrem\r\nHTTP/1.1 301 Moved Permanently\r\nDate: Wed, 06 Aug 2025 15:27:43 GMT\r\nContent-Type: text/html\r\nContent-Length: 167\r\nConnection: keep-alive\r\nCache-Control: max-age=3600\r\nExpires: Wed, 06 Aug 2025 16:27:43 GMT\r\nLocation: https://redis.io/docs/latest/commands/vrem\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nServer: cloudflare\r\nCF-RAY: 96af8f58ee6055ca-MIA\r\n\r\nHTTP/1.1 302 Moved Temporarily\r\nDate: Wed, 06 Aug 2025 15:27:43 GMT\r\nContent-Type: text/html\r\nContent-Length: 143\r\nConnection: keep-alive\r\nCache-Control: private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0\r\nExpires: Thu, 01 Jan 1970 00:00:01 GMT\r\nLocation: https://redis.io/docs/latest/commands/vrem/\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nServer: cloudflare\r\nCF-RAY: 96af8f59983a55ca-MIA\r\n\r\nHTTP/1.1 200 OK\r\nDate: Wed, 06 Aug 2025 15:27:43 GMT\r\nContent-Type: text/html\r\nConnection: keep-alive\r\nCF-Ray: 96af8f5a39c455ca-MIA\r\nCF-Cache-Status: DYNAMIC\r\nAccept-Ranges: bytes\r\nCache-Control: public, max-age=3600\r\nExpires: Wed, 06 Aug 2025 16:27:43 GMT\r\nLast-Modified: Wed, 06 Aug 2025 15:23:47 GMT\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nx-goog-generation: 1754493827125359\r\nx-goog-hash: crc32c=D8Ehxw==\r\nx-goog-hash: md5=2tiWREJaUAPgsu9I2Bs8Fg==\r\nx-goog-meta-goog-reserved-file-mtime: 1754493585\r\nx-goog-metageneration: 1\r\nx-goog-storage-class: STANDARD\r\nx-goog-stored-content-encoding: identity\r\nx-goog-stored-content-length: 189471\r\nx-guploader-uploadid: ABgVH8_gigHqExBhWG3UVQCZ2q46n_ULaGu0QDEOuMshkV_mgO2mE-oWGabbWR-a-_CfzkJ9O3QzwKw\r\nServer: cloudflare\r\n\r\n\r\nC:\\>curl -sLI https://redis.io/commands/vsetattr\r\nHTTP/1.1 301 Moved Permanently\r\nDate: Wed, 06 Aug 2025 15:27:44 GMT\r\nContent-Type: text/html\r\nContent-Length: 167\r\nConnection: keep-alive\r\nCache-Control: max-age=3600\r\nExpires: Wed, 06 Aug 2025 16:27:44 GMT\r\nLocation: https://redis.io/docs/latest/commands/vsetattr\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nServer: cloudflare\r\nCF-RAY: 96af8f5cc83c88da-MIA\r\n\r\nHTTP/1.1 302 Moved Temporarily\r\nDate: Wed, 06 Aug 2025 15:27:44 GMT\r\nContent-Type: text/html\r\nContent-Length: 143\r\nConnection: keep-alive\r\nCache-Control: private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0\r\nExpires: Thu, 01 Jan 1970 00:00:01 GMT\r\nLocation: https://redis.io/docs/latest/commands/vsetattr/\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nServer: cloudflare\r\nCF-RAY: 96af8f5d7a1c88da-MIA\r\n\r\nHTTP/1.1 200 OK\r\nDate: Wed, 06 Aug 2025 15:27:44 GMT\r\nContent-Type: text/html\r\nConnection: keep-alive\r\nServer: cloudflare\r\nCF-Ray: 96af8f5e0bc388da-MIA\r\nCF-Cache-Status: DYNAMIC\r\nAccept-Ranges: bytes\r\nCache-Control: public, max-age=3600\r\nExpires: Wed, 06 Aug 2025 16:27:44 GMT\r\nLast-Modified: Wed, 06 Aug 2025 15:23:47 GMT\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nx-goog-generation: 1754493827248553\r\nx-goog-hash: crc32c=X+GTbA==\r\nx-goog-hash: md5=1p/nYLQfKbpLoQLf9M73Xg==\r\nx-goog-meta-goog-reserved-file-mtime: 1754493585\r\nx-goog-metageneration: 1\r\nx-goog-storage-class: STANDARD\r\nx-goog-stored-content-encoding: identity\r\nx-goog-stored-content-length: 189912\r\nx-guploader-uploadid: ABgVH899aEYf3gSNnbSb6PfYSDIlrqmhZL1pyAkmap_VtW6NLbve1ixpuxYmMxG8gG8argrlFJJEgVA\r\n\r\n\r\nC:\\>curl -sLI https://redis.io/commands/vsim\r\nHTTP/1.1 301 Moved Permanently\r\nDate: Wed, 06 Aug 2025 15:27:47 GMT\r\nContent-Type: text/html\r\nContent-Length: 167\r\nConnection: keep-alive\r\nCache-Control: max-age=3600\r\nExpires: Wed, 06 Aug 2025 16:27:47 GMT\r\nLocation: https://redis.io/docs/latest/commands/vsim\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nServer: cloudflare\r\nCF-RAY: 96af8f6fa971a53f-MIA\r\n\r\nHTTP/1.1 302 Moved Temporarily\r\nDate: Wed, 06 Aug 2025 15:27:47 GMT\r\nContent-Type: text/html\r\nContent-Length: 143\r\nConnection: keep-alive\r\nCache-Control: private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0\r\nExpires: Thu, 01 Jan 1970 00:00:01 GMT\r\nLocation: https://redis.io/docs/latest/commands/vsim/\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nServer: cloudflare\r\nCF-RAY: 96af8f706a6ba53f-MIA\r\n\r\nHTTP/1.1 200 OK\r\nDate: Wed, 06 Aug 2025 15:27:47 GMT\r\nContent-Type: text/html\r\nConnection: keep-alive\r\nServer: cloudflare\r\nCF-Ray: 96af8f710b49a53f-MIA\r\nCF-Cache-Status: DYNAMIC\r\nAccept-Ranges: bytes\r\nCache-Control: public, max-age=3600\r\nExpires: Wed, 06 Aug 2025 16:27:47 GMT\r\nLast-Modified: Wed, 06 Aug 2025 15:23:47 GMT\r\nStrict-Transport-Security: max-age=10368000; includeSubDomains; preload\r\nx-goog-generation: 1754493827236063\r\nx-goog-hash: crc32c=snJFhQ==\r\nx-goog-hash: md5=vVK/pC5F9xWATmwT5SfhNQ==\r\nx-goog-meta-goog-reserved-file-mtime: 1754493585\r\nx-goog-metageneration: 1\r\nx-goog-storage-class: STANDARD\r\nx-goog-stored-content-encoding: identity\r\nx-goog-stored-content-length: 198325\r\nx-guploader-uploadid: ABgVH8_QdZF4r3O5SGajDkwoJl8oM7RW08oGj_uJyiwfgAML_TZKcz-3YbkZ27_yUDhrFVA0egeSvLc\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n_Originally posted by @hunterhogan in https://github.com/redis/redis-py/issues/3719#issuecomment-3160721363_\r\n            ",
      "solution": "The wrong link was fixed as part of PR# 3719, so I'm closing this issue.",
      "labels": [],
      "created_at": "2025-08-06T16:02:45Z",
      "closed_at": "2025-08-19T16:20:06Z",
      "url": "https://github.com/redis/redis-py/issues/3729",
      "comments_count": 2
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3310,
      "title": "RedisCluster client failing to reconnect to AWS Elasticache cluster after node failover",
      "problem": "lib - redis py redis==5.0.0 python version 3.7 redis cluster - 1 shard, 2 nodes\r\n\r\nwhen we do node failovers, the client cannot reconnect and throw RedisClusterException:Redis Cluster cannot be connected. Please provide at least one reachable node: <None, or some IP, Timeout connecting to server>.\r\n\r\ncreating redis cluster connection like this - client = redis.cluster.RedisCluster.from_url(url, read_from_replicas=True) Then using this client as a singleton object across application. want the client to know when a failover happens about updated node information. how to do this properly?",
      "solution": "Hi @mohits1007 , with redis-py 6.0, reconnection to cluster nodes and update of cluster topology information should have a significant improvement.\nIn case you have changing ip addresses, you may be interested in a recent issue where the proper configuration of the RedisCluster client was discussed #3604.\nClosing the issue for now. Please feel free to reopen it if further assistance is needed.",
      "labels": [],
      "created_at": "2024-07-10T08:54:40Z",
      "closed_at": "2025-05-09T12:40:31Z",
      "url": "https://github.com/redis/redis-py/issues/3310",
      "comments_count": 6
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3604,
      "title": "Redis Cluster Connection Fails After Cluster Restart with Dynamic IPs, redis.exceptions.ConnectionError: Error -3 connecting to 10.201.116.181:6379. Temporary failure in name resolution.",
      "problem": "# Redis Cluster Connection Fails After Cluster Restart with Dynamic IPs\n\n## Problem Description\n\nOur Python application loses connection to our Redis cluster whenever the cluster restarts because the nodes get assigned new dynamic IPs. The current implementation caches the initial connection and reuses it throughout the application lifecycle, but this approach fails when the cluster node IPs change.\n\nWhen the Redis cluster restarts:\n1. The nodes receive new IPs\n2. Our application still tries to connect to the previously cached IPs\n3. All Redis operations fail until the application is restarted\n\n## Current Implementation\n\n```python\nfrom redis import RedisCluster\nfrom redis.backoff import ExponentialBackoff\nfrom redis.cluster import ClusterNode\nfrom redis.retry import Retry\n\nREDIS_NODES = [\n    {\n        \"host\": redis_host,\n        \"port\": redis_port,\n    },\n]\n\ncluster_nodes = [\n    ClusterNode(host=node[\"host\"], port=node[\"port\"])\n    for node in REDIS_NODES\n]\n\nredis_instance = RedisCluster(\n    startup_nodes=cluster_nodes,\n    decode_responses=True,\n    username=redis_username,\n    password=redis_password,\n    retry=Retry(ExponentialBackoff(), 3),\n    retry_on_timeout=True,\n    socket_keepalive=True,\n)\n```\n\n## Attempted Solutions\n\n### 1. Periodic Connection Check (Cronjob)\n- Created a cronjob running every 30 seconds to ping the cluster\n- If ping fails, establish a new connection\n- **Problem**: Team rejected due to overhead concerns\n\n### 2. Connection Check Before Every Operation\n- Check connection before every Redis operation\n- Recreate connection on failure\n- **Problem**: Too costly as each API call performs 20-30 Redis operations\n- Additional complication: Ping sometimes hangs causing timeout exceptions\n- **Attempted workaround**: Created dual instances (normal and low-timeout) which led to inconsistent behavior\n\n### 3. Threaded Ping Approach\n- Executed ping in separate thread with timeout\n- **Problem**: Still creates overhead for each operation\n\n## Notable Requirements/Constraints\n\n1. The Redis cluster team cannot implement static IPs due to their own constraints\n2. We need a solution that minimizes overhead\n3. We are specifically using a Redis Bitnami cluster configuration\n4. We must use Redis cluster mode and cannot switch to a Redis standalone server due to operational constraints\n5. Similar issue was solved in our Java service using DNS cache configuration:\nhttps://redis.io/docs/latest/develop/clients/lettuce/produsage/#dns-cache-and-redis\n   ```java\n   java.security.Security.setProperty(\"networkaddress.cache.negative.ttl\", \"0\");\n   ```\n   But we haven't found an equivalent solution for Python\n\n## Questions\n\n1. Is there a way to configure Python's Redis client to not cache DNS/IPs or automatically reconnect when node IPs change?\n2. Are there alternatives to manually checking connections that would be more efficient?\n3. Does Python's Redis client have a feature similar to Java's Lettuce client DNS cache configuration?\n\n## Environment Details\n\n- Python version:3.11.4\n- redis-py version: redis==5.0.8\n- OS: Ubuntu\n\n## Impact\n\nThis issue affects our application's reliability and requires manual intervention after each Redis cluster restart. For a production service, this creates unacceptable downtime.",
      "solution": "Thank you for suggesting the `dynamic_startup_nodes = False` configuration. I implemented this change:\n\n```python\nredis_instance = RedisCluster(\n    startup_nodes=cluster_nodes,\n    decode_responses=True,\n    username=redis_username,\n    password=redis_password,\n    retry=Retry(ExponentialBackoff(), 3),\n    retry_on_timeout=True,\n    socket_keepalive=True,\n    dynamic_startup_nodes=False  # Added this line\n)\n```\n\nUnfortunately, this didn't resolve the issue in our environment. The problem persists because in our Bitnami Redis cluster setup, **all** node IPs change after a restart, including the initial startup nodes.\n\nAfter a cluster restart:\n1. The initial startup nodes referenced in `settings.redis_host` get new IPs\n2. All other cluster nodes also receive new IPs\n3. Even with `dynamic_startup_nodes=False`, the client can't reconnect because it can't reach any of the originally configured nodes to rediscover the topology\n\nIs there a configuration option in redis-py that allows the client to handle scenarios where all node addresses change? In the Java client, there was a DNS cache configuration option that helped with this issue, but we haven't found an equivalent for Python.\n\nAre there any other approaches you'd recommend for handling completely dynamic Redis cluster topologies where all nodes can change addresses?\n\n---\n\nHi @talha927, were you able to resolve the issue with changing IPs and the RedisCluster client?\n\n---\n\nHi @petyaslavova ,\nI found a solution to the Redis Cluster reconnection issue with dynamically changing IPs!\nThe solution combines two key Redis client configuration options:\n\n```\ndef remap_cluster_nodes(node_addr: Tuple[str, int]) -> Optional[Tuple[str, int]]:\n    \"\"\"\n    Remap all cluster node addresses to a primary node\n    \n    This function ensures all connections route through the primary configured node,\n    maintaining connection resilience when node IPs change.\n    \"\"\"\n    if not cluster_config:\n        return node_addr\n        \n    # Always redirect to the primary node defined in configuration\n    primary_host = cluster_config[0].get(\"host\")\n    primary_port = cluster_config[0].get(\"port\")\n    \n    return (primary_host, int(primary_port))\n\n\n# Create the Redis Cluster client with remapping enabled\nredis_client = RedisCluster(\n    startup_nodes=initial_nodes,\n    decode_responses=True,\n    username=redis_username,\n    password=redis_password,\n    retry=Retry(ExponentialBackoff(), 3),\n    retry_on_timeout=True,\n    socket_keepalive=True,\n    # Enable address remapping to maintain connection resilience\n    address_remap=remap_cluster_nodes,\n    # Skip full coverage check improves connection reliability\n    skip_full_coverage_check=True,\n)\n```\n\n\n**The solution works by:**\n\nUsing **_address_remap_** to redirect all node connections through the primary configured node (which in our case is the stable Kubernetes service hostname)\nEnabling **_skip_full_coverage_check=True_** to prevent the client from strictly verifying complete cluster topology during initialization\n\nWith this configuration, even when the underlying pod IPs change after cluster restarts, the client maintains a stable connection by consistently routing through the service endpoint rather than attempting to directly access potentially stale node addresses.",
      "labels": [],
      "created_at": "2025-04-16T22:38:12Z",
      "closed_at": "2025-05-07T14:25:38Z",
      "url": "https://github.com/redis/redis-py/issues/3604",
      "comments_count": 10
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3675,
      "title": "Why Async is not actually async?",
      "problem": "line 4074 of `redis/commands/core.py` (https://github.com/redis/redis-py/blob/8325ce2859b28e61086da904fba6ba87a4ee3d1b/redis/commands/core.py#L4074C1-L4074C37)\n`AsyncStreamCommands = StreamCommands`\n\nalso this in `redismodulecommands` line 94:\n`class AsyncRedisModuleCommands(RedisModuleCommands)`\n\nand we can go on and on\n\nAs I see, if we want asynchronous code, now we need to use `redis.asyncio.execute_command` method every time, because all wrappers are synchronous (with some exceptions).\n\nWhy we don't have asynchronous wrappers?",
      "solution": "also as was pointed out here #3569 we don't have async TimeSeries, but the problem is bigger, we don't have async wrappers for most of the features\n\n---\n\n@petyaslavova Hi, sorry for the later response. I think you nailed it, thank you! But please don't close the issue yet, I want to look into a few more things in the couple of days. If there will be no questions regarding this topic, I will close this issue",
      "labels": [],
      "created_at": "2025-06-12T11:59:12Z",
      "closed_at": "2025-07-29T05:34:15Z",
      "url": "https://github.com/redis/redis-py/issues/3675",
      "comments_count": 6
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3700,
      "title": "deprecated_args decorator strips typing",
      "problem": "After moving to use `redis` v6 from v5, `mypy` began complaining that `redis.Redis()` was returning type `Any`. I investigated and pinned down that up through `v6.0.0b2`, it correctly knew the return type was `redis.client.Redis`, and starting in `v6.0.0`, it was `Any`.\n\nRunning `mypy` on the following script shows the issue:\n\n```python\nfrom typing import reveal_type\nimport redis\n\nreveal_type(redis.Redis(host=\"harf\", port=100, db=0))\n```\n\nI noticed that the `@deprecated_args` decorator was added to the `redis.client.Redis.__init__()` method with [this commit](https://github.com/redis/redis-py/commit/fe13ed10c5330245050ebbc14142bb1c9786f119) that fell in the gap between those two versions. I verified that removing that decorator solved the problem. I then looked at the definition of that decorator, and I can see why it is losing the type information. I have tested a fix for this, and will be opening a PR for consideration.",
      "solution": "Hi @mharding-hpe, thank you for taking the time to report the issue and implement a code change to fix it!",
      "labels": [],
      "created_at": "2025-07-07T17:32:34Z",
      "closed_at": "2025-07-09T15:26:56Z",
      "url": "https://github.com/redis/redis-py/issues/3700",
      "comments_count": 2
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3686,
      "title": "openSUSE Tumbleweed i586 with redis 8 don't pass test_add_elem_no_quant tests",
      "problem": "https://build.opensuse.org/package/show/home:13ilya/redis\nhttps://build.opensuse.org/package/show/home:13ilya/python-redis\n\nIn openSUSE Tumbleweed with the latest versions of redis 8.0.2 and python-redis 6.2.0 on the i586 architecture, three tests (test_add_elem_no_quant) fail, although they pass on x86_64 and aarch64.\n\n```\n[  189s] =================================== FAILURES ===================================\n[  189s] ________________________ test_add_elem_no_quant[single] ________________________\n[  189s] \n[  189s] d_client = <redis.asyncio.client.Redis(<redis.asyncio.connection.ConnectionPool(<redis.asyncio.connection.Connection(host=localhost,port=6379,db=0)>)>)>\n[  189s] \n[  189s]     @skip_if_server_version_lt(\"7.9.0\")\n[  189s]     async def test_add_elem_no_quant(d_client):\n[  189s]         float_array = [1, 4.32, 0.11, 0.5, 0.9]\n[  189s]         resp = await d_client.vset().vadd(\n[  189s]             \"myset\",\n[  189s]             vector=float_array,\n[  189s]             element=\"elem1\",\n[  189s]             quantization=QuantizationOptions.NOQUANT,\n[  189s]         )\n[  189s]         assert resp == 1\n[  189s]     \n[  189s]         emb = await d_client.vset().vemb(\"myset\", \"elem1\")\n[  189s] >       assert _validate_quantization(float_array, emb, tolerance=0.0)\n[  189s] E       assert False\n[  189s] E        +  where False = _validate_quantization([1, 4.32, 0.11, 0.5, 0.9], [1.0000001192092896, 4.320000171661377, 0.11000000685453415, 0.5000000596046448, 0.9000000357627869], tolerance=0.0)\n[  189s] \n[  189s] tests/test_asyncio/test_vsets.py:86: AssertionError\n[  189s] _________________________ test_add_elem_no_quant[pool] _________________________\n[  189s] \n[  189s] d_client = <redis.asyncio.client.Redis(<redis.asyncio.connection.ConnectionPool(<redis.asyncio.connection.Connection(host=localhost,port=6379,db=0)>)>)>\n[  189s] \n[  189s]     @skip_if_server_version_lt(\"7.9.0\")\n[  189s]     async def test_add_elem_no_quant(d_client):\n[  189s]         float_array = [1, 4.32, 0.11, 0.5, 0.9]\n[  189s]         resp = await d_client.vset().vadd(\n[  189s]             \"myset\",\n[  189s]             vector=float_array,\n[  189s]             element=\"elem1\",\n[  189s]             quantization=QuantizationOptions.NOQUANT,\n[  189s]         )\n[  189s]         assert resp == 1\n[  189s]     \n[  189s]         emb = await d_client.vset().vemb(\"myset\", \"elem1\")\n[  189s] >       assert _validate_quantization(float_array, emb, tolerance=0.0)\n[  189s] E       assert False\n[  189s] E        +  where False = _validate_quantization([1, 4.32, 0.11, 0.5, 0.9], [1.0000001192092896, 4.320000171661377, 0.11000000685453415, 0.5000000596046448, 0.9000000357627869], tolerance=0.0)\n[  189s] \n[  189s] tests/test_asyncio/test_vsets.py:86: AssertionError\n[  189s] ____________________________ test_add_elem_no_quant ____________________________\n[  189s] \n[  189s] d_client = <redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(host=localhost,port=6379,db=0)>)>)>\n[  189s] \n[  189s]     @skip_if_server_version_lt(\"7.9.0\")\n[  189s]     def test_add_elem_no_quant(d_client):\n[  189s]         float_array = [1, 4.32, 0.11, 0.5, 0.9]\n[  189s]         resp = d_client.vset().vadd(\n[  189s]             \"myset\",\n[  189s]             vector=float_array,\n[  189s]             element=\"elem1\",\n[  189s]             quantization=QuantizationOptions.NOQUANT,\n[  189s]         )\n[  189s]         assert resp == 1\n[  189s]     \n[  189s]         emb = d_client.vset().vemb(\"myset\", \"elem1\")\n[  189s] >       assert _validate_quantization(float_array, emb, tolerance=0.0)\n[  189s] E       assert False\n[  189s] E        +  where False = _validate_quantization([1, 4.32, 0.11, 0.5, 0.9], [1.0000001192092896, 4.320000171661377, 0.11000000685453415, 0.5000000596046448, 0.9000000357627869], tolerance=0.0)\n[  189s] \n[  189s] tests/test_vsets.py:90: AssertionError\n[  189s] =============================== warnings summary ===============================\n[  189s] tests/test_asyncio/test_connection_pool.py::TestConnection::test_busy_loading_from_pipeline[single]\n[  189s] tests/test_asyncio/test_connection_pool.py::TestConnection::test_busy_loading_from_pipeline[single]\n[  189s]   /home/abuild/rpmbuild/BUILD/python-redis-test-6.2.0-build/redis-6.2.0/redis/asyncio/client.py:612: ResourceWarning: Unclosed client session <redis.asyncio.client.Pipeline(<redis.asyncio.connection.ConnectionPool(<redis.asyncio.connection.Connection(host=localhost,port=6379,db=0)>)>)>\n[  189s]     _warn(f\"Unclosed client session {self!r}\", ResourceWarning, source=self)\n[  189s]   Enable tracemalloc to get traceback where the object was allocated.\n[  189s]   See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.\n[  189s] \n[  189s] tests/test_asyncio/test_sentinel.py::test_repr_correctly_represents_connection_object\n[  189s]   /home/abuild/rpmbuild/BUILD/python-redis-test-6.2.0-build/redis-6.2.0/redis/asyncio/connection.py:232: ResourceWarning: unclosed Connection <redis.asyncio.sentinel.SentinelManagedConnection,host=127.0.0.1,port=6379)>\n[  189s]     _warnings.warn(\n[  189s]   Enable tracemalloc to get traceback where the object was allocated.\n[  189s]   See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.\n[  189s] \n[  189s] -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n[  189s] =========================== short test summary info ============================\n[  189s] FAILED tests/test_asyncio/test_vsets.py::test_add_elem_no_quant[single] - ass...\n[  189s] FAILED tests/test_asyncio/test_vsets.py::test_add_elem_no_quant[pool] - asser...\n[  189s] FAILED tests/test_vsets.py::test_add_elem_no_quant - assert False\n[  189s] = 3 failed, 1161 passed, 51 skipped, 665 deselected, 22 xpassed, 3 warnings in 164.14s (0:02:44) =\n```\n\nWhat could be the cause of this and how can it be fixed?\n\n[python-redis.spec.txt](https://github.com/user-attachments/files/20961062/python-redis.spec.txt)\n[python-redis-i586.log.txt](https://github.com/user-attachments/files/20961064/python-redis-i586.log.txt)\n[python-redis-x86_64.log.txt](https://github.com/user-attachments/files/20961063/python-redis-x86_64.log.txt)",
      "solution": "Hi @13ilya, these test failures in `openSUSE` on `i586` (32-bit architecture) are very likely caused by **floating-point precision differences**, which are more prominent or behave differently on 32-bit systems compared to 64-bit ones.\n\nThe issue arises from the use of `tolerance=0.0` in three specific tests. This zero-tolerance setting is applied in cases of:\n\n- binary quantization, where the expected output consists of exact 0s and 1s,\n\n- and no quantization, where values are expected to remain unchanged.\n\nSince these tests had not previously been executed on 32-bit architectures, the platform-specific floating-point precision issues were not encountered earlier.\n\nA practical workaround is to relax the tolerance threshold slightly \u2014 setting it to something like `0.00001` (i.e., `1e-5`) should be sufficient to account for minor float discrepancies while preserving test integrity.",
      "labels": [],
      "created_at": "2025-06-28T13:42:23Z",
      "closed_at": "2025-07-01T07:30:50Z",
      "url": "https://github.com/redis/redis-py/issues/3686",
      "comments_count": 2
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 2835,
      "title": "Is async version of redis-py coroutine safe?",
      "problem": "Hello\r\n\r\nI am wondering that async version of redis-py supports coroutine-safety.\r\n\r\nI created one redis client instance as a global, and then I used it without any\r\nlocking primitives in many coroutines, like in FastAPI request handlers.\r\n\r\n```python\r\nfrom redis.asyncio.cluster import RedisCluster\r\n\r\nredis = None\r\n\r\n\r\nasync def initialize_redis():\r\n      global redis\r\n      redis = RedisCluster.from_url(...)\r\n      await redis.initialize()\r\n\r\n\r\n@app.get('/some/path')\r\nasync def handle_get(key: str):\r\n      ...\r\n      v = await redis.get(key)\r\n      ...\r\n\r\n\r\n\r\n@app.put('/some/path')\r\nasync def handle_put(key: str, value: str):\r\n      ...\r\n      await redis.set(key, value)\r\n      ...\r\n```\r\n\r\nI didn't experience some kind of bugs with this scenario, but I just want to\r\nmake sure that coroutine-safety for redis-py is guranteed.\r\n\r\n\r\nThanks,",
      "solution": "I am a little confused by the question and the answer here.\nThe library is an asyncio library, simply making network requests with asyncio primitives. It should be as safe as any other asyncio library.\n\nObviously, your code can create race conditions in the logic, and you need to keep that in mind (i.e. where you see an `await` or `async` in your code, remember that the code may yield to the event loop and run another task), but there should be no issues with the library itself.\n\nThe global shouldn't make any difference in this regard, that just seems like a bit of general advice on your programming. For FastAPI, they usually recommend some dependency injection thing in their docs (I think that's a bad idea though, as you won't know that the Redis login is wrong until you've deployed the app and user requests start coming in and failing). In aiohttp, we'd just add it to the app object in a cleanup_ctx:\n\n```\nasync def redis_client(app: Application):\n    async with redis.Redis() as redis:\n        app[\"redis\"] = redis\n        yield\n\napp.cleanup_ctx.append(redis_client)\n```\n\nOr similar, and then it can easily be accessed in your handlers without any globals.",
      "labels": [
        "async",
        "question"
      ],
      "created_at": "2023-07-06T02:09:50Z",
      "closed_at": "2025-05-14T05:29:35Z",
      "url": "https://github.com/redis/redis-py/issues/2835",
      "comments_count": 9
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 2995,
      "title": "\"No connection available\" errors since 5.0.1",
      "problem": "**Version**: 5.0.1\r\n\r\n**Platform**:\r\nPython 3.11.5(3.11.5 (main, Sep  4 2023, 15:30:52) [GCC 10.2.1 20210110])\r\n\r\n**Description**:\r\n\r\nSince 5.0.1, we noticed a significant increase in `No connection available.` errors from redis. We are using cashews (6.3.0) + redis-py for request caching for our fastapi application. We don't experience this with 5.0.0.\r\n\r\nStack trace:\r\n```\r\nCancelledError: null\r\n  File \"redis/asyncio/connection.py\", line 1170, in get_connection\r\n    async with self._condition:\r\n  File \"asyncio/locks.py\", line 15, in __aenter__\r\n    await self.acquire()\r\n  File \"asyncio/locks.py\", line 114, in acquire\r\n    await fut\r\nTimeoutError: null\r\n  File \"redis/asyncio/connection.py\", line 1169, in get_connection\r\n    async with async_timeout(self.timeout):\r\n  File \"asyncio/timeouts.py\", line 111, in __aexit__\r\n    raise TimeoutError from exc_val\r\nConnectionError: No connection available.\r\n  File \"cashews/backends/redis/client.py\", line 26, in execute_command\r\n    return await super().execute_command(command, *args, **kwargs)\r\n  File \"redis/asyncio/client.py\", line 601, in execute_command\r\n    conn = self.connection or await pool.get_connection(command_name, **options)\r\n  File \"redis/asyncio/connection.py\", line 1174, in get_connection\r\n    raise ConnectionError(\"No connection available.\") from err\r\n```",
      "solution": "@kristjanvalur Thanks for being so reactive ! I'm currently trying to reproduce the issue as I did not yet find a consistent trigger for the \"No connection available\" error. I'll keep you posted !\n\n---\n\n> @kristjanvalur Thanks for being so reactive ! I'm currently trying to reproduce the issue as I did not yet find a consistent trigger for the \"No connection available\" error. I'll keep you posted !\r\n\r\nMy hypothesis is this:  You create a pool, and immediately start a few hundred requests.  However, the current implementation will make the socket connection _within_ the condition variable lock.  and so, all the tasks are waiting, on the timeout, while one after the other finishes their connections.  A few hundred socket connects can easily take up to 5 seconds, and then finally one of them times out.  Maybe you can repro using that, maybe your server was far away or something?\r\n\r\nMy PR should fix this, now the condition variable is only around pool management, not the socket connection code.\n\n---\n\n@kristjanvalur Thank you for your responsiveness! Feeling a little embarrassed and I'll understand if you keep my question unanswered as you're not obliged to and it's clearaly not a SO or some education course, and even this issue has nothing to do with my problem.\r\n\r\n> If you are seing this problem now, it means probably that there is some incorrect use of a connection pool, or a connection is not being correctly returned to the pool. What is your use case?\r\n\r\nwe're using redis-py 5.0.0 and problem came with the update of a perfomance requirements.\r\n\r\nIt's a web service which has to hit redis to process requests. Recently we got our requirements updated - now we have to handle 2k RPS on average.\r\n\r\nI have a class which encapsulates redis handling:\r\n\r\n```python\r\nclass RedisCodeClient:\r\n\r\n    def __init__(self, pool: ConnectionPool):\r\n        self._conn_pool = pool\r\n        self._active_db_number = 0\r\n    \r\n    async def select_active_db(self):\r\n        async with Redis(connection_pool=self._conn_pool, db=0) as redis:\r\n            active_db_number = await redis.get('active_db')\r\n            if not active_db_number:\r\n                return\r\n            self._active_db_number = active_db_number\r\n\r\n    async def get_code_data(self, code: str) -> list[dict[str, str]]:\r\n        async with Redis(connection_pool=self._conn_pool, db=self._active_db_number) as redis:\r\n            async with redis.pipeline(transaction=False) as pipe:\r\n                for number in [code[:i] for i in range(len(code), 0, -1)]:\r\n                    pipe.hgetall(number)\r\n                return await pipe.execute()\r\n```\r\n\r\nThis is a class, instance of which are created on the first request when we ask for it in our handlers:\r\n\r\n```python\r\n@functools.lru_cache()\r\ndef get_code_client():\r\n    settings = get_settings()\r\n    pool = ConnectionPool(\r\n        host=settings.CODE_REDIS_HOST,\r\n        port=settings.CODE_REDIS_PORT,\r\n        db=settings.CODE_REDIS_DB,\r\n        decode_responses=True,\r\n    )\r\n    return RedisCodeClient(pool=pool)\r\n    \r\n ...\r\n \r\n async def get_code_data(\r\n        client: RedisCodeClient, code: str\r\n) -> list[dict[str, str]]:\r\n    await client.select_active_db()\r\n    return await client.get_code_data(code)\r\n  \r\n ...\r\n \r\n async def example(code: str):\r\n    redis = get_code_client()\r\n    code_data = await get_code_data(redis, code)\r\n ```\r\n\r\nWhen we're testing our service at high RPS (~1.5k) - we hit the redis `maxclients` limit (in our case more than 5k connections) and redis become unresponsive untill we stop our application as connections are never closed (and we don't really want them to be closed as request rate is somewhat constant) \r\nThe solution were to use a `BlockingConnectionPool` with `max_connections` param set, but now we're facing timeouts :(\r\nI understand the nature of a problem but to me it seems it can't be solved without tweaking hardware or rewriting everything to use a single pipelined connection and callback like interfaces. \r\n\r\n\r\n**EDIT:**\r\n\r\nIf somebody stumbles upon my code and for any reason decide to use it for their own projects **be careful** there is **nasty bug** with selecting logical database in this code. \r\nparameter `db` passed to `Redis` class initializer won't do anything if you pass in Connection pool. This parameter have to be defined **on the pool itself**. So for selecting logical database - you need to use another approach, my approach was to use two separate connection pools and asyncio synchronizing primitives",
      "labels": [
        "stale"
      ],
      "created_at": "2023-10-10T00:41:57Z",
      "closed_at": "2025-06-22T00:24:51Z",
      "url": "https://github.com/redis/redis-py/issues/2995",
      "comments_count": 30
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3011,
      "title": "Type Error Duing redis cluster rebalance operation.",
      "problem": "**Version**: What redis-py and what redis version is the issue happening on?\r\n4.5.3\r\n\r\nBut this error appears to exist on all versions of the client library\r\n\r\n**Platform**: What platform / version? (For example Python 3.5.1 on Windows 7 / Ubuntu 15.10 / Azure)\r\nPython 3.10 on ubuntu docker image\r\n\r\n**Description**: Description of your issue, stack traces from errors and code that reproduces the issue\r\n\r\nDuring our cluster rebalancing, calls made using the SET command with the GET option result in failures. The redis cluster gets an asking command, but forwards the original options to the parsing, which result in a type error.\r\n\r\n```\r\nTypeError: bool_ok() got an unexpected keyword argument 'get'\r\n  File \"root/event_processing/unique_service/server/server/interceptors.py\", line 51, in intercept\r\n    result = method(request_or_iterator, context)\r\n  File \"root/event_processing/unique_service/server/monitoring.py\", line 218, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"root/event_processing/unique_service/server/server/routers.py\", line 68, in RecordBatch\r\n    responses = self.unique_service.record_or_get_event_batch(unique_service_requests)\r\n  File \"root/event_processing/unique_service/server/monitoring.py\", line 218, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"root/event_processing/unique_service/server/services/unique.py\", line 312, in record_or_get_event_batch\r\n    active_results = self._active_record_batch(active_requests)\r\n  File \"root/event_processing/unique_service/server/services/unique.py\", line 158, in _active_record_batch\r\n    response_batch = self.active.record_or_get_event_batch(\r\n  File \"root/event_processing/unique_service/server/monitoring.py\", line 218, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"root/event_processing/unique_service/server/repository/active.py\", line 203, in record_or_get_event_batch\r\n    return [self.record_or_get_event(request) for request in request_batch]\r\n  File \"root/event_processing/unique_service/server/repository/active.py\", line 203, in <listcomp>\r\n    return [self.record_or_get_event(request) for request in request_batch]\r\n  File \"root/event_processing/unique_service/server/monitoring.py\", line 218, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"root/event_processing/unique_service/server/repository/active.py\", line 252, in record_or_get_event\r\n    result = self._get_redis_cluster().set(\r\n  File \"redis/commands/core.py\", line 2302, in set\r\n    return self.execute_command(\"SET\", *pieces, **options)\r\n  File \"/root/.pex/venvs/88a5344e0040931bbcf0ac82f021c059eb108950/779eb2cc0ca9e2fdd204774cbc41848e4e7c5055/lib/python3.10/site-packages/pex-ns-pkgs/14/opentelemetry/instrumentation/redis/__init__.py\", line 162, in _traced_execute_command\r\n    response = func(*args, **kwargs)\r\n  File \"redis/cluster.py\", line 1074, in execute_command\r\n    raise e\r\n  File \"redis/cluster.py\", line 1060, in execute_command\r\n    res[node.name] = self._execute_command(node, *args, **kwargs)\r\n  File \"root/event_processing/unique_service/server/tracing_redis_cluster.py\", line 65, in _traced_execute_command\r\n    response = func(*args, **kwargs)\r\n  File \"redis/cluster.py\", line 1169, in _execute_command\r\n    raise e\r\n  File \"redis/cluster.py\", line 1106, in _execute_command\r\n    redis_node.parse_response(connection, \"ASKING\", **kwargs)\r\n  File \"redis/client.py\", line 1285, in parse_response\r\n    return self.response_callbacks[command_name](response, **options)\r\n```",
      "solution": "Solution for the 5.0.0 series: https://github.com/redis/redis-py/pull/3012\n\n---\n\nSolution for the 4.6.0 series: https://github.com/redis/redis-py/pull/3013",
      "labels": [
        "stale",
        "bug"
      ],
      "created_at": "2023-10-16T15:00:16Z",
      "closed_at": "2025-06-21T00:22:04Z",
      "url": "https://github.com/redis/redis-py/issues/3011",
      "comments_count": 3
    },
    {
      "tech": "redis",
      "repo": "redis/redis-py",
      "issue_number": 3640,
      "title": "Deadlock in `PubSub` class after update to 5.3.0",
      "problem": "Hi again!\n\nI just updated redis-py from 5.2.1 to 5.3.0 and I noticed that while adding the `Dispatchers` there was deadlock introduced.\n\n## Summary\nWhen using a client with `Retries` enabled and subscribing to a pattern using `psubscribe`, a deadlock can occur under certain conditions:\n- If a `punsubscribe` is attempted and a disconnection (such as a broken pipe) happens during this process, a reconnection is triggered (via the `PubSub._execute` error handling).\n- During reconnection, the client tries to re-subscribe to the previously subscribed patterns, but this process can result in a deadlock.\n\n## Full explanation\nThere was a new lock introduced before each `PubSub` command execution on commit [`40e5fc1`](https://github.com/redis/redis-py/commit/40e5fc131b457edc368637c1034424567553ba55), which is part of the 5.3.0 release:\nhttps://github.com/redis/redis-py/blob/7130e1ad913f51abc0de1886a52cfd6338e1beee/redis/client.py#L874-L875\n\nWhen a command execution fails, if `Retries` are enabled, there will be a reconnection attempt:\nhttps://github.com/redis/redis-py/blob/7130e1ad913f51abc0de1886a52cfd6338e1beee/redis/client.py#L895-L916\n\nHowever, when executing the `connect` we get to the final lines where the resubscriptions callbacks are triggered for `pubsub`:\nhttps://github.com/redis/redis-py/blob/7130e1ad913f51abc0de1886a52cfd6338e1beee/redis/connection.py#L406-L413\n\nThis will hit the same lock again a cause a deadlock.\n\n---\n\nLet me know if you need any other information. \ud83d\ude42\n ",
      "solution": "@matejsp I don't know which is the lock that causes the issue on `django-celery` but the one mentioned in the released 6.2.0 is the cluster one and not the PubSub that i mention here\n\n---\n\n> [@Darek07](https://github.com/Darek07) did you try just released 6.2.0 ? It says in release notes that it fixes one deadlock.\n\nUnfortunately it doesn't help. With redis 6.2.0 the problem still appears.",
      "labels": [],
      "created_at": "2025-05-09T14:26:30Z",
      "closed_at": "2025-06-16T12:22:14Z",
      "url": "https://github.com/redis/redis-py/issues/3640",
      "comments_count": 10
    }
  ]
}