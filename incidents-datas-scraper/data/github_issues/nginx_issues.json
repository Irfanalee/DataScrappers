{
  "tech": "nginx",
  "count": 66,
  "examples": [
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 1103,
      "title": "nginx v1.28 log rotate not create new log files",
      "problem": "### Bug Overview\n\nHello.\nI deployed nginx docker container on EC2, and set the log rotate.\n\nrotate value is 1, so i checked after one day and confirmed that the history was recorded in the status file\n\n> **root@66c5e6f19929:/var/log/nginx# cat /var/lib/logrotate/status\n> logrotate state -- version 2\n> \"/var/log/nginx/error.log\" 2026-1-26-6:0:0\n> \"/var/log/nginx/access.log\" 2026-1-26-6:0:0**\n\n\nbut logrotate not create new access log file\n\n> **root@66c5e6f19929:/var/log/nginx# ls -alh\n> total 16M\n> drwxr-xr-x. 2 nginx root   41 Jan 25 16:39 .\n> drwxr-xr-x. 1 root  root   57 Jan 26 14:23 ..\n> -rw-r--r--. 1 nginx root  16M Jan 26 22:40 access.log\n> -rw-r--r--. 1 nginx root 9.3K Jan 25 16:41 error.log**\n\nIs there anything else I should consider?\n\n\n_Dockerfile_\n```\nFROM nginx:1.28.0\n\nRUN apt-get -qq update \\\n    && apt-get install -qq --no-install-recommends -y logrotate\n\nADD nginx.conf /etc/nginx/nginx.conf\nADD ./conf.d/* /etc/nginx/conf.d/\n\nADD ./logrotate.conf /etc/logrotate.conf\n\nRUN mkdir -p /opt/nginx/html\nADD error.html /opt/nginx/html/error.html\n\nRUN chmod 640 /etc/nginx/nginx.conf\nRUN chmod 640 /etc/nginx/conf.d/*\n\nCOPY entrypoint.sh /usr/local/bin/entrypoint.sh\nRUN chmod +x /usr/local/bin/entrypoint.sh\n\nENTRYPOINT [\"/usr/local/bin/entrypoint.sh\"]\n```\n\n_/etc/logrotate.conf_\n```\n/var/log/nginx/*.log {\n    daily\n    rotate 1\n    create 0640 nginx adm\n    missingok\n    copytruncate\n    dateext\n    sharedscripts\n    postrotate\n        [ -s /run/nginx.pid ] && kill -USR1 `cat /run/nginx.pid`\n    endscript\n} \n```\n\n_entrypoint.sh_\n```\n#!/bin/sh\n\nset -e \n\nchown -R nginx:root /var/log/nginx\nchmod -R 755 /var/log/nginx\n\ntouch /var/log/nginx/access.log /var/log/nginx/error.log\nchown nginx:root /var/log/nginx/access.log /var/log/nginx/error.log\n\nservice cron start\n\nnginx -g 'daemon off;'\n```\n\n### Expected Behavior\n\nroot@66c5e6f19929:/var/log/nginx# ls -alh\ntotal 16M\ndrwxr-xr-x. 2 nginx root   41 Jan 25 16:39 .\ndrwxr-xr-x. 1 root  root   57 Jan 26 14:23 ..\n-rw-r--r--. 1 nginx root  0 Jan 26 22:40 access.log\n-rw-r--r--. 1 nginx root 0 Jan 25 16:41 error.log\n-rw-r--r--. 1 nginx root  16M Jan 26 22:40 access.log.260125 <<<---\n-rw-r--r--. 1 nginx root 9.3K Jan 25 16:41 error.log.260125 <<<---\n\n### Steps to Reproduce the Bug\n\nThere were no issues when I ran it manually\n\n> $ logrorate -f /etc/logrotate.conf\n\n### NGINX Configuration\n\n```\nuser  nginx;\nworker_processes  auto;\n\nerror_log  /var/log/nginx/error.log notice;\npid        /var/run/nginx.pid;\n\nevents {\n    worker_connections  1024;\n}\n\nhttp {\n    include       /etc/nginx/mime.types;\n    default_type  application/octet-stream;\n\n    log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n                      '$status $body_bytes_sent \"$http_referer\" '\n                      '\"$http_user_agent\" \"$http_x_forwarded_for\" \"$upstream_cache_status\" \"expire:$upstream_http_x_accel_expires\"';\n\n    access_log  /var/log/nginx/access.log  main;\n\n    ## Security Action ##\n    disable_symlinks on;\n    #####################\n\n    sendfile        on;\n    #tcp_nopush     on;\n\n    keepalive_timeout  65;\n\n    #gzip  on;\n\n    include /etc/nginx/conf.d/*.conf;\n}\n```\n\n### NGINX version and build configuration options\n\nroot@66c5e6f19929:/var/log/nginx# nginx -v\nnginx version: nginx/1.28.0\n\n### Environment where NGINX is being built and/or deployed\n\n- Target deployment platform: AWS\n- Target OS: AL2023\n\n\n### Architecture where NGINX is being built and/or deployed\n\nLinux ip-10-172-xx-xx.ap-northeast-2.compute.internal 6.1.59-84.139.amzn2023.x86_64 #1 SMP PREEMPT_DYNAMIC Tue Oct 24 20:57:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\n\n### NGINX Debug Log\n\n```\n# Your NGINX debug log\n```\n\n\n### Additional Context\n\n_No response_",
      "solution": "logrotate explained in small steps:\n\n1. Logrotate would rotate access.log to `access.log.suffix`\n2. nginx continues to write to its file-handle. Therefore it continues to write to `access.log.<suffix>`\n3. Logrotate sends HUP signal to nginx\n4. nginx reopens access log and now writes to `access.log`\n\nYour logrotate is not triggering a rename. If logrotate would run, `access.log.<suffix>` and `error.log.<suffix>` would exist.\n\nYou need to find the bug in your logrotate/docker container configuration.",
      "labels": [
        "bug"
      ],
      "created_at": "2026-01-26T13:56:13Z",
      "closed_at": "2026-02-05T04:29:44Z",
      "url": "https://github.com/nginx/nginx/issues/1103",
      "comments_count": 2
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 455,
      "title": "Support for populating $http_host with the :authority header in HTTP/3 (QUIC)",
      "problem": "### Description\nI would like to report a technical issue regarding the use of HTTP/3 (also known as QUIC) with all versions of NGINX. The problem involves the $http_host variable, which remains empty in HTTP/3 because it is not populated using the values from the :authority header. This behavior differs from HTTP/1 and HTTP/2, where $http_host is correctly populated using client-provided data.\n\n### Technical Configuration\nIn my case, I am using NGINX 1.23.7 for a Magento 2 installation that hosts three multilingual sites. The configuration uses a map directive to determine the language code of the site (e.g., Italian, English, or German) based on the value of $http_host. Below is a simplified example of the configuration:\n\n```\nmap $http_host $MAGE_RUN_CODE {\n    www.bselfie.it website_it;\n    www.bselfie.it/en website_en;\n    www.bselfie.de website_de;\n}\n```\nThis configuration works perfectly with HTTP/2: the $http_host value is correctly passed and allows the appropriate site or language to be served. However, with HTTP/3, $http_host remains empty because this protocol uses the :authority header, which does not automatically populate $http_host.\n\n### The Problem\nIn HTTP/3, the lack of a value for $http_host causes routing logic to fail. For instance:\n\nRequests to www.bselfie.it/en are not correctly mapped because $http_host is empty.\nMagento cannot identify the specific site and always defaults to the primary site (Italian in this case).\nThis behavior makes it impossible to use HTTP/3 alongside configurations that rely on $http_host unless HTTP/3 support is completely disabled, which is not an ideal solution.\n\n### Technical Considerations\nWhile the $host variable could be considered an alternative, it is not suitable in this scenario. $host only contains the main domain (www.bselfie.it) and does not include critical parts of the URL, such as /en, which are necessary for differentiating configurations. Therefore, $host cannot replace $http_host for setups that depend on advanced routing logic or mapping.\n\n### Proposal\nI propose that NGINX be updated to support the automatic population of $http_host in HTTP/3 by retrieving values from the :authority header, as is done for HTTP/1 and HTTP/2. This improvement would ensure consistent behavior across HTTP protocol versions and resolve issues like the one described.\n\n### Conclusion\nI believe this functionality is crucial for improving the adoption and compatibility of HTTP/3 in complex scenarios. I am available to provide further details or examples if needed.\n\nThank you,\nMarco Marcoaldi\nMANAGED SERVER SRL\n",
      "solution": ">$host only contains the main domain ([www.bselfie.it](http://www.bselfie.it/)) and does not include critical parts of the URL, such as /en, which are necessary for differentiating configurations.\n\nI don't understand your case, why $http_host includes the uri part. \n\n$http_ prefixed variables are used to point to http request headers. Therefore, even if nginx needs to support getting pseudo header values, h2/h3 pseudo headers should not be associated with $http_host unless the request is really initiated with a host request header. Instead, I think nginx should provide another set of variables to get pseudo header values, such as $pseudo_http_authority.\n\nThe $host variable exists to parse and normalize the host name, which is why nginx specifically distinguishes between $host and $http_host variables. $host only contains the host name part, separating the port number from $http_host, and also supports parsing the host name in the request line and :authority in h2/h3. It is also the basis for finding virtual hosts in nginx.\n\nI noticed that your $http_host has a uri part. I guess this should be your custom behavior? Since $http_host only faithfully records the host header of the original http request, you can get the host name with the path from this variable. Incorrect use of the $http_host variable to execute routing or functional logic may pose a security risk. \n\nIn the rfc, the host header only allows the host name and optional port number to appear, and there should be no uri part. \nhttps://www.rfc-editor.org/rfc/rfc9110.html#name-host-and-authority\n\nAlso in your use case I believe you can combine the $host header with something else and do a map like.\n\n```\nmap $host$uri $MAGE_RUN_CODE {\n~*^www\\.bselfie\\.it/en website_en;\n~*^www\\.bselfie\\.it website_it;\n~*^www\\.bselfie\\.de website_de;\n}\n```",
      "labels": [
        "feature"
      ],
      "created_at": "2025-01-16T10:36:10Z",
      "closed_at": "2026-01-15T23:08:02Z",
      "url": "https://github.com/nginx/nginx/issues/455",
      "comments_count": 7
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 1053,
      "title": "In some cases, `ngx_http_grpc_ctx_t` may contain a dangling pointer reference to `ngx_http_grpc_conn_t*`, leading to Nginx crash.",
      "problem": "### Bug Overview\n\nWe encountered this issue\uff1a\n\nwhen Nginx fails to forward gRPC requests to rs1 and triggers a retry to rs2, in some cases, the `connection` in `ngx_http_grpc_ctx_t` is not set to NULL.  Since this `connection` is based on memory allocated by rs1, after rs1 releases its memory, the `connection` becomes a dangling pointer, leading to a memory crash.\n\nScene: when retring next upstream,  but `u->request_sent is` false,  and `ngx_http_grpc_reinit_request` will not have a chance to execute\n\n<img width=\"797\" height=\"223\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/2ee27ae9-c433-4e0b-ac87-532e75c51b2a\" />\n\nThis is not so easy to reproduce.\n\n#### details\n\n1\u3001coredump file:\n<img width=\"1209\" height=\"366\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/cd57d606-b9d5-4bd8-8ea7-08f5ca45cbaa\" />\n\n\n2\u3001we can see ctx->connection has the same memory address with  one part of new upstream peer's pool\n<img width=\"918\" height=\"228\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/bb94b94b-14cf-4282-a618-551c732852c1\" />\n\n3\u3001Writing to the memory pointed to by `ctx->connection` corrupted the memory of `r->pool`, triggering a crash.\n\n<img width=\"1209\" height=\"970\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/89280593-94bd-429c-8dd0-47214bd5a96e\" />\n\n\n\n### Steps to Reproduce the Bug\n\n1. gRPC servers rs1 and rs2 listen on plaintext.\n2. Nginx listens on HTTP/2 and is configured for gRPC forwarding.\n3. The client initiates a gRPC request.\n4. Nginx forwards the request to rs1. The TCP handshake is successful, and rs1 sends a settings frame to Nginx. After Nginx parses the frame, it creates an `ngx_http_grpc_ctx_t*ctx` and `ctx->connection`.  \n5. Then, Nginx initiates a TLS handshake with rs1. Because rs1 only listens on plaintext, the TLS handshake fails. rs1's `c->pool` is released, but `ctx->connection` is not set to null.\n6. Subsequently, Nginx retries the request with rs2. rs2 sends a settings frame to Nginx. After Nginx parses the frame, it updates the values \u200b\u200bin `ctx->connection`. \n7. Then Nginx sends a TLS handshake with rs2, which fails. rs2's `c->pool` is released, leading to a crash.\n\nfollow is the tcpdump file:\n\n<img width=\"1622\" height=\"751\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e226943c-5e6a-4871-b6e2-35deaecb6c5f\" />\n\n### NGINX Configuration\n\n```\nhttp {\n\n    proxy_next_upstream_tries 2;\n\n    upstream grpc_server {\n      server 9.134.212.144:50003;\n      server 9.134.212.144:50004;\n    }\n\n    server {\n        listen 80 http2 reuseport;\n \n        location /hello.Greeter/SayHello\n        {\n            grpc_ssl_verify off;\n            \n            grpc_ssl_server_name off;\n            grpc_ssl_verify_depth 0;\n            grpc_ssl_protocols TLSv1.2 TLSv1.1;\n            grpc_pass grpcs://grpc_server;\n        }\n    }\n}\n```\n\nthis is my mock server:\n1\u3001send setting frame to server after tcp handshake\n2\u3001then, after recv nginx's first request, close the socket\n\n``` \nimport socket\nimport binascii\n\ndef tcp_server():\n    # 1. Configure basic server parameters\n    HOST = '0.0.0.0'  # Listen on all network interfaces (use 127.0.0.1 for local only)\n    PORT = 50003       # Listener port, customizable as needed\n    # Target 46-byte hex string of setting's frame (to be converted to binary bytes)\n    hex_data = \"000018040000000000000400400000000500400000000600004000fe0300000001000004080000000000003f0001\"\n    send_data = binascii.unhexlify(hex_data)  # Convert hex string to binary bytes\n    # Assert to verify data length (ensure it's exactly 46 bytes)\n    assert len(send_data) == 46, f\"Invalid data length: {len(send_data)} bytes (expected 46 bytes)\"\n\n    # 2. Create TCP socket (AF_INET = IPv4 protocol, SOCK_STREAM = stream-oriented TCP)\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as server_socket:\n        # Enable port reuse (avoid \"Address already in use\" error on server restart)\n        server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        # Bind the socket to specified host and port\n        server_socket.bind((HOST, PORT))\n        # Start listening for client connections (backlog=5: max pending connections)\n        server_socket.listen(5)\n        print(f\"TCP Server started, listening on {HOST}:{PORT}\")\n\n        # Continuously listen for incoming connections (infinite loop)\n        while True:\n            # 3. Wait for client connection (blocking until a connection is received)\n            # accept() returns: (client connection object, client address tuple (IP, port))\n            client_conn, client_addr = server_socket.accept()\n            print(f\"\\nClient {client_addr} connected (TCP 3-way handshake completed)\")\n            \n            # Manage client connection with 'with' statement (auto-closes connection when done)\n            with client_conn:\n                try:\n                    # 4. Immediately send 46-byte data after handshake completion\n                    client_conn.sendall(send_data)  # sendall() ensures full data transmission\n                    print(f\"Sent 46-byte data to client: {hex_data}\")\n\n                    # 5. Receive next data from client (blocking until data/connection closure)\n                    # Buffer size set to 1024 bytes (adjust based on expected client data size)\n                    client_data = client_conn.recv(1024)\n                    if client_data:\n                        print(f\"Received data from client: {client_data.hex()} (raw bytes: {client_data})\")\n                    else:\n                        print(\"Client closed connection (no data sent)\")\n\n                    # 6. Immediately close connection after receiving client data\n                    # 'with' statement auto-closes, but explicit log for clarity\n                    print(f\"Closing connection with {client_addr}\")\n\n                except Exception as e:\n                    # Handle exceptions (e.g., connection reset by client)\n                    print(f\"Error processing client {client_addr}: {str(e)}\")\n\nif __name__ == \"__main__\":\n    try:\n        # Start the TCP server\n        tcp_server()\n    except KeyboardInterrupt:\n        # Handle manual server termination (Ctrl+C)\n        print(\"\\nServer terminated manually by user\")\n\n```\n\n\n### NGINX version and build configuration options\n\nMy Nginx version is 1.17.3, but I checked the latest code, and the logic hasn't changed, so it should still have the same problem.\n\n\n### Environment where NGINX is being built and/or deployed\n\ndeployed in linux VM.\nbuilt with asan and cov\n\n\n### Architecture where NGINX is being built and/or deployed\n\nThe output of `uname -a`: [...]\n\n\n### NGINX Debug Log\n\nwhen built with asan, also can found the use-after-free error log\n```\nSUMMARY: AddressSanitizer: heap-use-after-free src/http/modules/ngx_http_grpc_module.c:3589 in ngx_http_grpc_parse_settings\n==3054129==ERROR: AddressSanitizer: heap-use-after-free on address 0x60d0000113e8 at pc 0x000000b488f4 bp 0x7ffda817e7c0 sp 0x7ffda817e7b0\n```\n\n\n### Additional Context\n\n_No response_",
      "solution": "The root cause is that HTTP/2 modules (grpc and proxy_v2) allocate data from upstream connection pool, but store the pointer in request context.\nWhen upstream connection is closed for retry, the pool is destroyed but request context still alive with a dangling pointer.\nIt means proxy_v2 module has the same issue too.\n\n---\n\nHi @hongzhidao ,  I have tested  that both the two patches can fix the problem by giving more chance to do reinit.\n\n",
      "labels": [
        "bug"
      ],
      "created_at": "2025-12-23T11:36:55Z",
      "closed_at": "2026-02-04T15:13:40Z",
      "url": "https://github.com/nginx/nginx/issues/1053",
      "comments_count": 10
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 1085,
      "title": "NGINX exits with exitcode 1 when OpenSSL has PKCS#11 module",
      "problem": "### Bug Overview\n\nWhen I setup my Docker image with NGINX in such a way that everything is installed to start using PKCS#11, NGINX exists with an exitcode 1 eventually. As soon as comment out `activate = 1` in `openssl.cnf`, NGINX starts acting as normal.\n\n### Expected Behavior\n\nA NGINX instance that starts up and keeps running.\n\n### Steps to Reproduce the Bug\n\nI use Docker for my reproduction with base image `nginx:alpine`:\n```Dockerfile\nFROM nginx:alpine\n\nRUN apk add git\nRUN apk add libp11\nRUN apk add softhsm\nRUN apk add opensc\nRUN apk add strace\nRUN apk add nano openssl\n\nRUN mkdir -p /var/log/nginx\n\nRUN apk add meson gcc g++ openssl-dev\nRUN git clone https://github.com/latchset/pkcs11-provider.git\nRUN cd pkcs11-provider && meson build\nRUN cd pkcs11-provider && ninja -C build\nRUN cd pkcs11-provider && ninja -C build install\n\nRUN echo \"\" >> /etc/ssl/openssl.cnf\nRUN echo \"[provider_sect]\" >> /etc/ssl/openssl.cnf\nRUN echo \"pkcs11 = pkcs11_sect\" >> /etc/ssl/openssl.cnf\n\nRUN echo \"\" >> /etc/ssl/openssl.cnf\nRUN echo \"[pkcs11_sect]\" >> /etc/ssl/openssl.cnf\n# Commenting out the next line prevents the module from activating\nRUN echo \"activate = 1\" >> /etc/ssl/openssl.cnf\nRUN echo \"module = /usr/lib/ossl-modules/pkcs11.so\" >> /etc/ssl/openssl.cnf\nRUN echo \"pkcs11-module-load-behavior = early\" >> /etc/ssl/openssl.cnf\nRUN echo \"pkcs11-module-path = /usr/lib/softhsm/libsofthsm2.so\" >> /etc/ssl/openssl.cnf\n```\n\n### NGINX Configuration\n\nDefault config files. I didn't even add `ssl_engine` or used `ssl_certificate_key` with `engine:pkcs11:***`.\n\n### NGINX version and build configuration options\n\nThe base image `nginx:alpine`, which at time of writing is:\n```\nnginx version: nginx/1.29.4\nbuilt by gcc 15.2.0 (Alpine 15.2.0)\nbuilt with OpenSSL 3.5.4 30 Sep 2025\nTLS SNI support enabled\n```\n\n### Environment where NGINX is being built and/or deployed\n\n- Target deployment platform: Docker\n- Target OS: Alpine (inside Docker)\n\n\n### Architecture where NGINX is being built and/or deployed\n\n`Linux vps1 5.10.0-37-amd64 #1 SMP Debian 5.10.247-1 (2025-12-11) x86_64 GNU/Linux` (Executed inside the container)\n\n### NGINX Debug Log\n\n```\n/docker-entrypoint.sh: Sourcing /docker-entrypoint.d/15-local-resolvers.envsh\n/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh\n/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh\n/docker-entrypoint.sh: Configuration complete; ready for start up\n/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration\n/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/\n/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh\n10-listen-on-ipv6-by-default.sh: info: /etc/nginx/conf.d/default.conf is not a file or does not exist\n/docker-entrypoint.sh: Sourcing /docker-entrypoint.d/15-local-resolvers.envsh\n/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh\n/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh\n/docker-entrypoint.sh: Configuration complete; ready for start up\n/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration\n/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/\n/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh\n10-listen-on-ipv6-by-default.sh: info: /etc/nginx/conf.d/default.conf is not a file or does not exist\n/docker-entrypoint.sh: Sourcing /docker-entrypoint.d/15-local-resolvers.envsh\n/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh\n/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh\n/docker-entrypoint.sh: Configuration complete; ready for start up\n/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration\n/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/\n/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh\n10-listen-on-ipv6-by-default.sh: info: /etc/nginx/conf.d/default.conf is not a file or does not exist\n/docker-entrypoint.sh: Sourcing /docker-entrypoint.d/15-local-resolvers.envsh\n/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh\n/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh\n/docker-entrypoint.sh: Configuration complete; ready for start up\n```\n(This repeatingly)\n\n### Additional Context\n\nI am aware that there is a difference between OpenSSL Engine and OpenSSL Provider. However, information on this topic of connecting SSL/TLS-systems to a PKCS#11, especially NGINX, is scarce.",
      "solution": "Okay, it seems that it is because the key doesn't exist. When using `pkcs11-tool` to generate a keypair and use the URIs that are given back, I see SoftHSM react when reloading NGINX, so that is a good sign. I think this issue is indeed fixed now. However, having more feedback logging wouldn't be bad for future NGINX versions.",
      "labels": [
        "bug"
      ],
      "created_at": "2026-01-19T01:37:48Z",
      "closed_at": "2026-01-20T18:14:38Z",
      "url": "https://github.com/nginx/nginx/issues/1085",
      "comments_count": 8
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 867,
      "title": "About dns and resolver",
      "problem": "I want to confirm whether Nginx can run without the host machine having DNS configured, when I use a domain name in an upstream block.\n\nI have set the resolver directive in my Nginx configuration file, but it seems I still need to set up DNS on the host for Nginx to start properly.\n\nHere's my environment:\n\nI'm running Nginx in a Docker container\n\nThe host machine does not have DNS configured (I can't even ping [www.google.com](http://www.google.com/) from it)\n\nI expected that setting resolver in the Nginx config would be sufficient\n\nHowever, Nginx fails to launch with an '[emerg] host not found in...' error",
      "solution": "> I expected that setting resolver in the Nginx config would be sufficient\n> \n> However, Nginx fails to launch with an '[emerg] host not found in...' error\n\nRuntime name resolution with nginx resolver should be enabled explicitly with [`server ... resolve`](https://nginx.org/en/docs/http/ngx_http_upstream_module.html#resolve) directive argument in the upstream block.\n\n---\n\n> > I expected that setting resolver in the Nginx config would be sufficient\n> > However, Nginx fails to launch with an '[emerg] host not found in...' error\n> \n> Runtime name resolution with nginx resolver should be enabled explicitly with [`server ... resolve`](https://nginx.org/en/docs/http/ngx_http_upstream_module.html#resolve) directive argument in the upstream block.\n\nThank you, I am trying. It seems can work.",
      "labels": [
        "feature"
      ],
      "created_at": "2025-08-26T09:22:10Z",
      "closed_at": "2025-12-19T18:34:08Z",
      "url": "https://github.com/nginx/nginx/issues/867",
      "comments_count": 2
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 1018,
      "title": "[VULN] Multiple High Severity CVEs in nginx:1.29.3 Docker Image Dependencies",
      "problem": "### Feature Overview\n\nWe are currently utilizing the nginx:1.29.3 official Docker image as our production-grade web server and reverse proxy foundation. We chose this image due to its stability and the expectation of up-to-date dependencies.\n\nA routine security audit performed with the Trivy scanner revealed the following four High Severity vulnerabilities present in the image's components:\n\nCVE-2025-7425\n\nCVE-2025-64720\n\nCVE-2025-65018\n\nCVE-2025-66293\n\nThese vulnerabilities are not related to the core Nginx application itself, but rather to the underlying operating system packages or third-party libraries bundled within the container environment.\n\n### Alternatives Considered\n\nBefore filing this report, we considered the following alternatives:\n\nSwitching to the alpine variant (e.g., nginx:1.29.3-alpine): This might resolve some Debian-based package vulnerabilities, but the Alpine image often requires additional build steps to include necessary tools or modules, adding complexity to our build process.\n\nUsing an older Nginx version: This is not a viable option as it would likely expose us to older, known vulnerabilities and miss out on recent bug fixes and features in version 1.29.3.\n\nBuilding a custom image: We could attempt to patch the underlying packages ourselves, but this would increase our maintenance overhead significantly and deviates from using the official, supported base image.\n\nWe believe the most efficient and safest solution is for the official Nginx team to update the vulnerable packages in the base nginx:1.29.3 image, ensuring all dependencies meet current security standards.\n\n### Additional Context\n\nYou can reproduce this finding by running the following command:\n\nPull the affected image:\ndocker pull nginx:1.29.3\n\nRun the Trivy scan:\ntrivy image nginx:1.29.3\n\nThe report should clearly list the four CVEs mentioned above, along with the specific affected package for each one.\nWe urge the Nginx team to prioritize updating the base image dependencies (such as the OS packages, e.g., libc, openssl, etc.) to their fixed versions to mitigate these High Severity risks immediately.\nPlease let us know if you require the full Trivy scan output for detailed package information.\n",
      "solution": "Hello!\n\nLooking at the Debian's CVE tracker all of those are still marked as \"vulnerable\" for Trixie, which is used as a base image for `nginx:1.29.3`:\n\n- https://security-tracker.debian.org/tracker/CVE-2025-64720\n- https://security-tracker.debian.org/tracker/CVE-2025-7425\n- https://security-tracker.debian.org/tracker/CVE-2025-65018\n- https://security-tracker.debian.org/tracker/CVE-2025-66293\n\nWe cant fix those without them being fixed upstream in Debian.  When the fixed versions are updated to their repos, and Debian issues a point release (or Docker Official Library team does a ~monthly Debian bump, whichever comes earlier), those will be fixed in the image as well.\n\n---\n\n> Hello!\n> \n> Looking at the Debian's CVE tracker all of those are still marked as \"vulnerable\" for Trixie, which is used as a base image for `nginx:1.29.3`:\n> \n> * https://security-tracker.debian.org/tracker/CVE-2025-64720\n> * https://security-tracker.debian.org/tracker/CVE-2025-7425\n> * https://security-tracker.debian.org/tracker/CVE-2025-65018\n> * https://security-tracker.debian.org/tracker/CVE-2025-66293\n> \n> We cant fix those without them being fixed upstream in Debian. When the fixed versions are updated to their repos, and Debian issues a point release (or Docker Official Library team does a ~monthly Debian bump, whichever comes earlier), those will be fixed in the image as well.\n\n@thresheek \nThank you for your previous update regarding the Debian Trixie base image.\n\nI would like to inform you that the fixed version 1.6.48-1+deb13u1 has been released in the Debian upstream repositories to address the previously mentioned CVEs.\n\nSince the fix is now available, could you please trigger a rebuild and release a new version of the nginx image? This will allow us to pull the updated base image and resolve these vulnerabilities in our environment.\n\nThank you for your support!",
      "labels": [
        "invalid"
      ],
      "created_at": "2025-12-05T10:32:04Z",
      "closed_at": "2025-12-18T22:19:26Z",
      "url": "https://github.com/nginx/nginx/issues/1018",
      "comments_count": 3
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 279,
      "title": "Reuse of UDP socket for DNS resolution incompatible with Kubernetes virtual IPs",
      "problem": "### Environment\n\nInclude the result of the following commands:\n  - `nginx -V` - nginx/1.20.2\n  - `uname -a` - 5.10.226 x86_64 (containerized CentOS 7 on AL2 host)\n\n### Description\n\nIn Kubernetes, cluster defaults dictate that DNS is to be resolved by a Kubernetes ClusterIP service identified as `172.20.0.10`. The ClusterIP can be backed by one or more resolvers running in separate Kubernetes pods, and the list of endpoints (pods) can change over time. In the most common configuration, the `kube-proxy` component of Kubernetes [uses netfilter rules (through iptables)](https://kubernetes.io/docs/reference/networking/virtual-ips/#proxy-modes) to translate ClusterIPs to individual pod IP addresses through NAT.\n\nIf I understand correctly, nginx [reuses](https://github.com/nginx/nginx/blob/f45c2707ea1bf6fde3bd0c045cc4a63cdc4a966a/src/core/ngx_resolver.c#L1326) UDP sockets for DNS resolution. This introduces an incompatibility with the ClusterIP virtual IP scheme in Kubernetes because the endpoints of the kube-dns service ClusterIP can change after the socket has been established. This can result in timeouts for subsequent requests if the endpoint that was initially selected is replaced. Timeouts don't result in the socket being reestablished, so this problem can cause errors in name resolution for an extended duration until the socket is reestablished for some other reason.\n\nIn addition to causing errors when an endpoint used by the socket is terminated, the socket reuse also prevents DNS request load balancing between endpoints. If there are `n` endpoints, requests will be sent to whichever was selected at the time the socket was established, until the socket is reestablished. Ideally the built-in ClusterIP load balancing would function, as it would if a separate socket was opened for each request.\n\nFor comparison, `getaddrinfo()` from glibc does not suffer from this issue because it opens a socket for each request. I assume nginx's approach is an optimization, but without a way to configure this behavior it introduces a problem that cannot easily be worked around without more involved infrastructure changes.",
      "solution": "Hi @JacobHenner Did the solution above resolve the issue for you?",
      "labels": [
        "question"
      ],
      "created_at": "2024-10-25T21:37:31Z",
      "closed_at": "2025-12-18T22:07:03Z",
      "url": "https://github.com/nginx/nginx/issues/279",
      "comments_count": 3
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 995,
      "title": "HTTP2 framing layer errors after upgrading from 1.22 to 1.28",
      "problem": "### Environment\n\nnginx version: nginx/1.28.0\nbuilt by gcc 12.2.0 (Debian 12.2.0-14)\nbuilt with OpenSSL 3.0.15 3 Sep 2024 (running with OpenSSL 3.0.17 1 Jul 2025)\nTLS SNI support enabled\nconfigure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/run/nginx.pid --lock-path=/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-http_v3_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-g -O2 -ffile-prefix-map=/home/builder/debuild/nginx-1.28.0/debian/debuild-base/nginx-1.28.0=. -fstack-protector-strong -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fPIC' --with-ld-opt='-Wl,-z,relro -Wl,-z,now -Wl,--as-needed -pie'\n\n\nLinux ws-a1 6.1.0-23-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.99-1 (2024-07-15) x86_64 GNU/Linux\n\n\n### Description\n\nWe are monitoring our site using curl requests on static files\nSince the last Nginx upgrade we have rare (0.1 %) failures on the site.\nCurl returns error 16 (CURLE_HTTP2 )\nWe never had any error with version 1.22\n\n\n\n",
      "solution": "Update:\n\nWe fixed the problem on the Curl side forcing HTTP version to 1.1 \ncurl_setopt($ch, CURLOPT_HTTP_VERSION, CURL_HTTP_VERSION_1_1);\n\nWe have multiple IPs for a same hostame, our monitoring pings every IP to be able to check every server\ncurl_setopt($ch, CURLOPT_RESOLVE, [\"www.domain.com:80:$ip\", \"www.domain.com:443:$ip\"]);\n\nSo it may be Curl problem.\n\nSince the monitoring worked with Nginx 1.22 we assume that the default http version changed between version 1.22 and 1.28\nCan somebody confirm that ?\n\n",
      "labels": [
        "bug",
        "need more info"
      ],
      "created_at": "2025-11-20T06:57:04Z",
      "closed_at": "2025-12-12T13:20:32Z",
      "url": "https://github.com/nginx/nginx/issues/995",
      "comments_count": 2
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 474,
      "title": "When error_log is set to stderr, nginx still logs \"signal process started\" to default error.log file",
      "problem": "### Environment\n\nInclude the result of the following commands:\n```\nnginx version: nginx/1.26.2\nbuilt by gcc 11.4.1 20231218 (Red Hat 11.4.1-3) (GCC) \nbuilt with OpenSSL 3.0.7 1 Nov 2022 (running with OpenSSL 3.2.2 4 Jun 2024)\nTLS SNI support enabled\nconfigure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib64/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-http_v3_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-O2 -flto=auto -ffat-lto-objects -fexceptions -g -grecord-gcc-switches -pipe -Wall -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -Wp,-D_GLIBCXX_ASSERTIONS -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -fstack-protector-strong -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1 -m64 -march=x86-64-v2 -mtune=generic -fasynchronous-unwind-tables -fstack-clash-protection -fcf-protection -fPIC' --with-ld-opt='-Wl,-z,relro -Wl,-z,now -pie'\n```\n```\nLinux example.org 5.14.0-503.22.1.el9_5.x86_64 #1 SMP PREEMPT_DYNAMIC Wed Jan 22 13:59:07 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux\n```\n\n### Description\n\nAttempting to configure nginx to log only to syslog (see configuration below), it still logs the following type of message into the default error.log file on start:\n```\n[notice] 17583#17583: signal process started\n```\nIt seem it is currently impossible to stop nginx from writing to the default error.log file entirely.\n\n- [x] The bug is reproducible with the latest version of nginx\n- [x] The nginx configuration is minimized to the smallest possible\nto reproduce the issue and doesn't contain third-party modules\n\n#### nginx configuration\n\n```\nerror_log stderr emerg;\n#error_log /dev/null emerg; # same result\nerror_log syslog:server=unix:/dev/log,nohostname warn;\n```\n\nEdit:\nWhen adding `-e stderr` to the command line, the message appears only in the syslog. In the early execution before parsing the configuration files, using stdout and stderr seems like a more appropriate default though.",
      "solution": "Even with the workaround of having `-e stderr` in the command line for ExecStart, ExecReload and ExecStop, the following is still written to error.log during a dnf upgrade of nginx and nginx-module-njs:\n\n```\n[notice] 245174#245174: js vm init njs: 000055D946A33380\n[notice] 245174#245174: js vm init njs: 000055D946A69A00\n\n```\n\nMight be similar or related to https://github.com/nginx/pkg-oss/issues/24.",
      "labels": [
        "bug"
      ],
      "created_at": "2025-01-26T03:41:25Z",
      "closed_at": "2025-12-12T10:37:45Z",
      "url": "https://github.com/nginx/nginx/issues/474",
      "comments_count": 2
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 983,
      "title": "crash dump when both try_files and proxy_pass are configured",
      "problem": "### Environment\n\nInclude the result of the following commands:\n  - `nginx -V`\n       all the versions from day1\n       root@nginx-dev:~/workspace/nginx-base# ./objs/nginx -v\n      nginx version: nginx/1.29.1\n  - `uname -a`\n      platform is independent\n     root@nginx-dev:~/workspace/nginx-base# uname -a\nLinux nginx-dev 6.8.0-79-generic #79-Ubuntu SMP PREEMPT_DYNAMIC Tue Aug 12 14:42:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux\n\n### Description\n\nCrash dump occurs at some conditions if both try_files and proxy_pass are configured.\n\n- [x] The bug is reproducible with the latest version of nginx\n      yes\n- [x] The nginx configuration is minimized to the smallest possible\nto reproduce the issue and doesn't contain third-party modules\n\n       location /ttll/ {\n            root /var/www/;\n            try_files  /abcd /abc12 /abc123 /abcd; # =404;\n            proxy_pass http://10.197.32.191:9090/;\n       }\n#### nginx configuration\n       location /ttll/ {\n            root /var/www/;\n            try_files  /abcd /abc12 /abc123 /abcd; # =404;\n            proxy_pass http://10.197.32.191:9090/;\n       }\n\n```\n# Your nginx configuration here\n\n       location /ttll/ {\n            root /var/www/;\n            try_files  /abcd /abc12 /abc123 /abcd; # =404;\n            proxy_pass http://10.197.32.191:9090/;\n       }\n```\nor share the configuration in [gist](https://gist.github.com/).\n\n#### nginx debug log\n\nIt is advised to enable\n[debug logging](http://nginx.org/en/docs/debugging_log.html).\n```\n# Your nginx debug log here\n```\nor share the debug log in [gist](https://gist.github.com/).\n\n",
      "solution": "For the above configuration when we try to access the /ttll/ location via \"curl http://server:port/ttll/\" and when the file /abcd exists at /var/www/ directory, the crash dump will happen. \n\n- If the existed file's name length is larger than the matched location /ttll/ then the filename will be extracted and pass to the backend server. \n\n- If the existed file's name is smaller than the length of matched location /ttll/ , the crash or Internal Server Error happens.\n\n- What's more the proxy_pass directive MUST ends up with /. for example proxy_pass http://10.197.32.191/;\n\n\nThe root cause is clear, when try_files detects there is an exists file in the location defined in either root or alias, then r->uri is changed to the existing file name.  When proxy_pass is executed, it will use the modified r->uri to create the new location which will introduce this bug.\n\n\nI have a basic fix and verify it works. The theory is quit simple, just try to change the r->uri when nginx is NOT configured the proxy_pass series directives and only serves as a static file server:\n\n\nAttach the diff file for your reference.\n\n[try_files_proxy_pass.txt](https://github.com/user-attachments/files/23619352/try_files_proxy_pass.txt)",
      "labels": [
        "bug"
      ],
      "created_at": "2025-11-19T05:01:03Z",
      "closed_at": "2025-11-26T18:46:23Z",
      "url": "https://github.com/nginx/nginx/issues/983",
      "comments_count": 2
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 878,
      "title": "Very slow nginx compilation using recent GCC version",
      "problem": "## Description\n\nI build nginx for RHEL 7, 8, 9 and 10. Since el10, compilation is extremely slow. While it takes ~5 minutes for el7 to el9, it takes around an hour to build nginx on el10. It seems to be related to recent GCC version (14) and LTO as lto-ltrans is taking all the CPU. By disabling LTO during the process using `_lto_cflags` macro, build time returns to normal on el10. Same result for both x86_64 and aarch64 architectures.\n\nAre you aware of what is causing this?\n\nAlso, I see that with recent gcc, there's no more _built by gcc version_ in `nginx -V`, is that a normal behaviour?\n\n```\nnginx version: nginx/1.29.1\nbuilt by gcc 11.5.0 20240719 (Red Hat 11.5.0-5) (GCC) \n```\n\nThanks for your help.\n\n\n## Benchmarks\n\nHere are the following benchmarks, including default build settings, LTO optimizations, and LTO disabled, along with their respective build times:\n\n**el10 aarch64 default GCC settings: 41 minutes**\n\n```bash\nCFLAGS='-O2 -flto=auto -ffat-lto-objects -fexceptions -g -grecord-gcc-switches -pipe -Wall -Werror=format-security -Wp,-U_FORTIFY_SOURCE,-D_FORTIFY_SOURCE=3 -Wp,-D_GLIBCXX_ASSERTIONS -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -fstack-protector-strong -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1  -mbranch-protection=standard -fasynchronous-unwind-tables -fstack-clash-protection   '\n\nLDFLAGS='-Wl,-z,relro -Wl,--as-needed  -Wl,-z,pack-relative-relocs -Wl,-z,now -specs=/usr/lib/rpm/redhat/redhat-hardened-ld -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1  -Wl,--build-id=sha1 -specs=/usr/lib/rpm/redhat/redhat-package-notes '\n\n./configure --prefix=/usr/share/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib64/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --http-client-body-temp-path=/var/lib/nginx/cache/client_body --http-proxy-temp-path=/var/lib/nginx/cache/proxy --http-fastcgi-temp-path=/var/lib/nginx/cache/fastcgi --http-uwsgi-temp-path=/var/lib/nginx/cache/uwsgi --http-scgi-temp-path=/var/lib/nginx/cache/scgi --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --user=nginx --group=nginx --with-compat --with-file-aio --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_image_filter_module --with-http_sub_module --with-http_dav_module --with-http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_random_index_module --with-http_secure_link_module --with-http_degradation_module --with-http_stub_status_module --with-http_auth_request_module --with-http_xslt_module --with-http_v2_module --with-http_v3_module --with-mail --with-mail_ssl_module --with-threads --with-stream --with-stream_ssl_module --with-stream_realip_module --with-http_slice_module --with-stream_ssl_preread_module --with-debug '--with-cc-opt=-O2 -flto=auto -ffat-lto-objects -fexceptions -g -grecord-gcc-switches -pipe -Wall -Wno-complain-wrong-lang -Werror=format-security -Wp,-U_FORTIFY_SOURCE,-D_FORTIFY_SOURCE=3 -Wp,-D_GLIBCXX_ASSERTIONS -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -fstack-protector-strong -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1  -mbranch-protection=standard -fasynchronous-unwind-tables -fstack-clash-protection    -DTCP_FASTOPEN=23' --with-openssl=modules/openssl-3.5.2 --with-openssl-opt=enable-ktls --add-dynamic-module=modules/ngx_modsecurity-1.0.4 --add-module=modules/ngx_headers_more-0.39 --add-module=modules/ngx_cache_purge-2.3 --add-module=modules/ngx_brotli-1.0.0rc-2-g6e97 --add-module=modules/ngx_module_vts-0.2.4 --add-module=modules/ngx_http_geoip2_module-3.4 --add-module=modules/ngx_echo-0.63\n```\n\n**el10 aarch64 `nofat` LTO: 35 minutes**\n\n```bash\n%global _lto_cflags -flto=auto -fno-fat-lto-objects\n\nCFLAGS='-O2 -flto=auto -fno-fat-lto-objects -fexceptions -g -grecord-gcc-switches -pipe -Wall -Werror=format-security -Wp,-U_FORTIFY_SOURCE,-D_FORTIFY_SOURCE=3 -Wp,-D_GLIBCXX_ASSERTIONS -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -fstack-protector-strong -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1  -mbranch-protection=standard -fasynchronous-unwind-tables -fstack-clash-protection   '\n\nLDFLAGS='-Wl,-z,relro -Wl,--as-needed  -Wl,-z,pack-relative-relocs -Wl,-z,now -specs=/usr/lib/rpm/redhat/redhat-hardened-ld -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1  -Wl,--build-id=sha1 -specs=/usr/lib/rpm/redhat/redhat-package-notes '\n\n./configure --prefix=/usr/share/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib64/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --http-client-body-temp-path=/var/lib/nginx/cache/client_body --http-proxy-temp-path=/var/lib/nginx/cache/proxy --http-fastcgi-temp-path=/var/lib/nginx/cache/fastcgi --http-uwsgi-temp-path=/var/lib/nginx/cache/uwsgi --http-scgi-temp-path=/var/lib/nginx/cache/scgi --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --user=nginx --group=nginx --with-compat --with-file-aio --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_image_filter_module --with-http_sub_module --with-http_dav_module --with-http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_random_index_module --with-http_secure_link_module --with-http_degradation_module --with-http_stub_status_module --with-http_auth_request_module --with-http_xslt_module --with-http_v2_module --with-http_v3_module --with-mail --with-mail_ssl_module --with-threads --with-stream --with-stream_ssl_module --with-stream_realip_module --with-http_slice_module --with-stream_ssl_preread_module --with-debug '--with-cc-opt=-O2 -flto=auto -fno-fat-lto-objects -fexceptions -g -grecord-gcc-switches -pipe -Wall -Wno-complain-wrong-lang -Werror=format-security -Wp,-U_FORTIFY_SOURCE,-D_FORTIFY_SOURCE=3 -Wp,-D_GLIBCXX_ASSERTIONS -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -fstack-protector-strong -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1  -mbranch-protection=standard -fasynchronous-unwind-tables -fstack-clash-protection    -DTCP_FASTOPEN=23' --with-openssl=modules/openssl-3.5.2 --with-openssl-opt=enable-ktls --add-dynamic-module=modules/ngx_modsecurity-1.0.4 --add-module=modules/ngx_headers_more-0.39 --add-module=modules/ngx_cache_purge-2.3 --add-module=modules/ngx_brotli-1.0.0rc-2-g6e97 --add-module=modules/ngx_module_vts-0.2.4 --add-module=modules/ngx_http_geoip2_module-3.4 --add-module=modules/ngx_echo-0.63\n```\n\n**el10 aarch64 `nofat partition=one` LTO: 141 minutes**\n\n```bash\n%global _lto_cflags -flto=auto -fno-fat-lto-objects -flto-partition=one\n\nCFLAGS='-O2 -flto=auto -fno-fat-lto-objects -fexceptions -g -grecord-gcc-switches -pipe -Wall -Werror=format-security -Wp,-U_FORTIFY_SOURCE,-D_FORTIFY_SOURCE=3 -Wp,-D_GLIBCXX_ASSERTIONS -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -fstack-protector-strong -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1  -mbranch-protection=standard -fasynchronous-unwind-tables -fstack-clash-protection   '\n\nLDFLAGS='-Wl,-z,relro -Wl,--as-needed  -Wl,-z,pack-relative-relocs -Wl,-z,now -specs=/usr/lib/rpm/redhat/redhat-hardened-ld -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1  -Wl,--build-id=sha1 -specs=/usr/lib/rpm/redhat/redhat-package-notes '\n\n./configure --prefix=/usr/share/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib64/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --http-client-body-temp-path=/var/lib/nginx/cache/client_body --http-proxy-temp-path=/var/lib/nginx/cache/proxy --http-fastcgi-temp-path=/var/lib/nginx/cache/fastcgi --http-uwsgi-temp-path=/var/lib/nginx/cache/uwsgi --http-scgi-temp-path=/var/lib/nginx/cache/scgi --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --user=nginx --group=nginx --with-compat --with-file-aio --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_image_filter_module --with-http_sub_module --with-http_dav_module --with-http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_random_index_module --with-http_secure_link_module --with-http_degradation_module --with-http_stub_status_module --with-http_auth_request_module --with-http_xslt_module --with-http_v2_module --with-http_v3_module --with-mail --with-mail_ssl_module --with-threads --with-stream --with-stream_ssl_module --with-stream_realip_module --with-http_slice_module --with-stream_ssl_preread_module --with-debug '--with-cc-opt=-O2 -flto=auto -fno-fat-lto-objects -flto-partition=one -fexceptions -g -grecord-gcc-switches -pipe -Wall -Wno-complain-wrong-lang -Werror=format-security -Wp,-U_FORTIFY_SOURCE,-D_FORTIFY_SOURCE=3 -Wp,-D_GLIBCXX_ASSERTIONS -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -fstack-protector-strong -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1  -mbranch-protection=standard -fasynchronous-unwind-tables -fstack-clash-protection    -DTCP_FASTOPEN=23' --with-openssl=modules/openssl-3.5.2 --with-openssl-opt=enable-ktls --add-dynamic-module=modules/ngx_modsecurity-1.0.4 --add-module=modules/ngx_headers_more-0.39 --add-module=modules/ngx_cache_purge-2.3 --add-module=modules/ngx_brotli-1.0.0rc-2-g6e97 --add-module=modules/ngx_module_vts-0.2.4 --add-module=modules/ngx_http_geoip2_module-3.4 --add-module=modules/ngx_echo-0.63\n```\n\n**el10 aarch64 without LTO: 4 minutes**\n\n```bash\n%global _lto_cflags %{nil}\n\nCFLAGS='-O2  -fexceptions -g -grecord-gcc-switches -pipe -Wall -Werror=format-security -Wp,-U_FORTIFY_SOURCE,-D_FORTIFY_SOURCE=3 -Wp,-D_GLIBCXX_ASSERTIONS -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -fstack-protector-strong -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1  -mbranch-protection=standard -fasynchronous-unwind-tables -fstack-clash-protection   '\n\nLDFLAGS='-Wl,-z,relro -Wl,--as-needed  -Wl,-z,pack-relative-relocs -Wl,-z,now -specs=/usr/lib/rpm/redhat/redhat-hardened-ld -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1  -Wl,--build-id=sha1 -specs=/usr/lib/rpm/redhat/redhat-package-notes '\n\n./configure --prefix=/usr/share/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib64/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --http-client-body-temp-path=/var/lib/nginx/cache/client_body --http-proxy-temp-path=/var/lib/nginx/cache/proxy --http-fastcgi-temp-path=/var/lib/nginx/cache/fastcgi --http-uwsgi-temp-path=/var/lib/nginx/cache/uwsgi --http-scgi-temp-path=/var/lib/nginx/cache/scgi --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --user=nginx --group=nginx --with-compat --with-file-aio --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_image_filter_module --with-http_sub_module --with-http_dav_module --with-http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_random_index_module --with-http_secure_link_module --with-http_degradation_module --with-http_stub_status_module --with-http_auth_request_module --with-http_xslt_module --with-http_v2_module --with-http_v3_module --with-mail --with-mail_ssl_module --with-threads --with-stream --with-stream_ssl_module --with-stream_realip_module --with-http_slice_module --with-stream_ssl_preread_module --with-debug '--with-cc-opt=-O2  -fexceptions -g -grecord-gcc-switches -pipe -Wall -Wno-complain-wrong-lang -Werror=format-security -Wp,-U_FORTIFY_SOURCE,-D_FORTIFY_SOURCE=3 -Wp,-D_GLIBCXX_ASSERTIONS -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -fstack-protector-strong -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1  -mbranch-protection=standard -fasynchronous-unwind-tables -fstack-clash-protection    -DTCP_FASTOPEN=23' --with-openssl=modules/openssl-3.5.2 --with-openssl-opt=enable-ktls --add-dynamic-module=modules/ngx_modsecurity-1.0.4 --add-module=modules/ngx_headers_more-0.39 --add-module=modules/ngx_cache_purge-2.3 --add-module=modules/ngx_brotli-1.0.0rc-2-g6e97 --add-module=modules/ngx_module_vts-0.2.4 --add-module=modules/ngx_http_geoip2_module-3.4 --add-module=modules/ngx_echo-0.63\n``` ",
      "solution": "Based on your tests, I tried to further narrow down the source of the issue. I replicated what Mock does when building the package by manually running a rockylinux:10 Docker image and executing the same configure and make process. With this approach, the build completes in about 3 minutes, and the \"built by gcc\" string appears in the nginx -V output.\n\n```bash\ntime make -j4\nmake[1]: Leaving directory '/workspace/nginx-1.29.1'\n\nreal\t2m48.490s\nuser\t4m37.551s\nsys\t1m11.690s\n\n[root@ad31c6b6225a nginx-1.29.1]# ./objs/nginx -V\nnginx version: nginx/1.29.1\ncustom build maintained on github.com/karljohns0n/nginx-more\nbuilt by gcc 14.2.1 20250110 (Red Hat 14.2.1-7) (GCC) \nbuilt with OpenSSL 3.5.2 5 Aug 2025\nTLS SNI support enabled\nconfigure arguments: --prefix=/usr/share/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib64/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --http-client-body-temp-path=/var/lib/nginx/cache/client_body --http-proxy-temp-path=/var/lib/nginx/cache/proxy --http-fastcgi-temp-path=/var/lib/nginx/cache/fastcgi --http-uwsgi-temp-path=/var/lib/nginx/cache/uwsgi --http-scgi-temp-path=/var/lib/nginx/cache/scgi --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --user=nginx --group=nginx --with-compat --with-file-aio --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_image_filter_module --with-http_sub_module --with-http_dav_module --with-http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_random_index_module --with-http_secure_link_module --with-http_degradation_module --with-http_stub_status_module --with-http_auth_request_module --with-http_xslt_module --with-http_v2_module --with-http_v3_module --with-mail --with-mail_ssl_module --with-threads --with-stream --with-stream_ssl_module --with-stream_realip_module --with-http_slice_module --with-stream_ssl_preread_module --with-debug --with-cc-opt='-O2 -flto=auto -ffat-lto-objects -fexceptions -g -grecord-gcc-switches -pipe -Wall -Wno-complain-wrong-lang -Werror=format-security -Wp,-U_FORTIFY_SOURCE,-D_FORTIFY_SOURCE=3 -Wp,-D_GLIBCXX_ASSERTIONS -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -fstack-protector-strong -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1 -mbranch-protection=standard -fasynchronous-unwind-tables -fstack-clash-protection -DTCP_FASTOPEN=23' --with-openssl=modules/openssl-3.5.2 --with-openssl-opt=enable-ktls --add-module=modules/ngx_headers_more-0.39 --add-module=modules/ngx_cache_purge-2.3 --add-module=modules/ngx_brotli-1.0.0rc-2-g6e97 --add-module=modules/ngx_module_vts-0.2.4 --add-module=modules/ngx_http_geoip2_module-3.4 --add-module=modules/ngx_echo-0.63\n```\n\nThis confirms that the very slow compilation occurs only in one specific scenario: building nginx with **Mock on RHEL10 using the OpenSSL source.**\n\nI\u2019m using the exact same configure options across all RHEL versions, as well as in my manual build. So what could be different when using Mock on RHEL10 with OpenSSL source? I use default [Mock config](https://github.com/rpm-software-management/mock/blob/main/mock-core-configs/etc/mock/templates/rocky-10.tpl) and the usual [rpmspec file](https://github.com/karljohns0n/nginx-more/blob/master/SPECS/nginx-more.spec).\n\nWith these confirmation, do you have any idea in mind? \nIf you have a minute to check the [build.log](https://github.com/karljohns0n/nginx-more/actions/runs/17655331171/artifacts/3990463543), do you see anything that would lead to the slow gcc build and missing nginx line (#ifdef NGX_COMPILER  ngx_write_stderr(\"built by \" NGX_COMPILER NGX_LINEFEED);) ?\n\nThank you!\n\n\n\n---\n\nHi @ac000 \n\nI made a lot of progress in my troubleshooting.\n\n## Flags auto injection\n\nSince RHEL10, redhat-rpm-config seems to have a new behaviour of injecting all these flags when building packages with Mock. I have no control over them.\n\n```bash\n+ CFLAGS='-O2 -flto=auto -ffat-lto-objects -fexceptions -g -grecord-gcc-switches -pipe -Wall -Werror=format-security -Wp,-U_FORTIFY_SOURCE,-D_FORTIFY_SOURCE=3 -Wp,-D_GLIBCXX_ASSERTIONS -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -fstack-protector-strong -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1  -m64 -march=x86-64-v3 -mtune=generic -fasynchronous-unwind-tables -fstack-clash-protection -fcf-protection -mtls-dialect=gnu2   '\n+ export CFLAGS\n+ CXXFLAGS='-O2 -flto=auto -ffat-lto-objects -fexceptions -g -grecord-gcc-switches -pipe -Wall -Werror=format-security -Wp,-U_FORTIFY_SOURCE,-D_FORTIFY_SOURCE=3 -Wp,-D_GLIBCXX_ASSERTIONS -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -fstack-protector-strong -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1  -m64 -march=x86-64-v3 -mtune=generic -fasynchronous-unwind-tables -fstack-clash-protection -fcf-protection -mtls-dialect=gnu2   '\n+ export CXXFLAGS\n+ FFLAGS='-O2 -flto=auto -ffat-lto-objects -fexceptions -g -grecord-gcc-switches -pipe -Wall -Wp,-U_FORTIFY_SOURCE,-D_FORTIFY_SOURCE=3 -Wp,-D_GLIBCXX_ASSERTIONS -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -fstack-protector-strong -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1  -m64 -march=x86-64-v3 -mtune=generic -fasynchronous-unwind-tables -fstack-clash-protection -fcf-protection -mtls-dialect=gnu2   -I/usr/lib64/gfortran/modules '\n+ export FFLAGS\n+ FCFLAGS='-O2 -flto=auto -ffat-lto-objects -fexceptions -g -grecord-gcc-switches -pipe -Wall -Wp,-U_FORTIFY_SOURCE,-D_FORTIFY_SOURCE=3 -Wp,-D_GLIBCXX_ASSERTIONS -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -fstack-protector-strong -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1  -m64 -march=x86-64-v3 -mtune=generic -fasynchronous-unwind-tables -fstack-clash-protection -fcf-protection -mtls-dialect=gnu2   -I/usr/lib64/gfortran/modules '\n+ export FCFLAGS\n+ VALAFLAGS=-g\n+ export VALAFLAGS\n+ LDFLAGS='-Wl,-z,relro -Wl,--as-needed  -Wl,-z,pack-relative-relocs -Wl,-z,now -specs=/usr/lib/rpm/redhat/redhat-hardened-ld -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1  -Wl,--build-id=sha1 -specs=/usr/lib/rpm/redhat/redhat-package-notes '\n+ export LDFLAGS\n+ LT_SYS_LIBRARY_PATH=/usr/lib64:\n+ export LT_SYS_LIBRARY_PATH\n+ CC=gcc\n+ export CC\n+ CXX=g++\n+ export CXX\n```\n\nI've spawned a mock shell to output all the default vars, it's pretty much the same as RHEL9 and lower, however I don't understand why the behaviour is not the same when compiling manually then using Mock. When a manually build nginx with openssl source, with the same RHEL10 buildroot, there's no LTO enforcement and the \"built by gcc\" line is there. I suppose this is more of a RHEL question than a Nginx question.\n\n```bash\ncat /etc/redhat-release\ngcc --version\nrpm -q gcc redhat-rpm-config\nrpm --eval '%{?rhel}'\nrpm --eval '%{_lto_cflags}'\nrpm --eval '%{optflags}'\nrpm --eval '%{build_cflags}'\nrpm --eval '%{build_ldflags}'\nrpm --eval '%{__global_ldflags}'\nrpm --eval '%{_smp_mflags}'\n\nRocky Linux release 10.0 (Red Quartz)\ngcc (GCC) 14.2.1 20250110 (Red Hat 14.2.1-7)\nCopyright (C) 2024 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\ngcc-14.2.1-7.el10.aarch64\nredhat-rpm-config-288-1.el10_0.1.noarch\n10\n-flto=auto -ffat-lto-objects\n-O2 -flto=auto -ffat-lto-objects -fexceptions -g -grecord-gcc-switches -pipe -Wall -Wno-complain-wrong-lang -Werror=format-security -Wp,-U_FORTIFY_SOURCE,-D_FORTIFY_SOURCE=3 -Wp,-D_GLIBCXX_ASSERTIONS -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -fstack-protector-strong -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1  -mbranch-protection=standard -fasynchronous-unwind-tables -fstack-clash-protection  \n-O2 -flto=auto -ffat-lto-objects -fexceptions -g -grecord-gcc-switches -pipe -Wall -Werror=format-security -Wp,-U_FORTIFY_SOURCE,-D_FORTIFY_SOURCE=3 -Wp,-D_GLIBCXX_ASSERTIONS -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -fstack-protector-strong -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1  -mbranch-protection=standard -fasynchronous-unwind-tables -fstack-clash-protection   \n-Wl,-z,relro -Wl,--as-needed  -Wl,-z,pack-relative-relocs -Wl,-z,now -specs=/usr/lib/rpm/redhat/redhat-hardened-ld -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1  -Wl,--build-id=sha1  \n-Wl,-z,relro -Wl,--as-needed  -Wl,-z,pack-relative-relocs -Wl,-z,now -specs=/usr/lib/rpm/redhat/redhat-hardened-ld -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1  -Wl,--build-id=sha1  \n-j${RPM_BUILD_NCPUS}\n```\n\n\n## Disable LTO when building OpenSSL\n\n[I've modified ](https://github.com/karljohns0n/nginx-more/commit/fe56ba2800a56a3d0ab40896ff58a38b9682d6c6)the `with-openssl-opt` with `no-lto`;\n\n```bash\n--with-openssl-opt=\"-fno-lto -fPIC enable-ktls\"\n```\n\n```bash\n&& ./config --prefix=/builddir/build/BUILD/nginx-1.29.1/modules/openssl-3.5.3/.openssl no-shared no-threads -fno-lto -fPIC enable-ktls \\\n````\n\nThis leads from 40 minutes of build time to 4 minutes with OpenSSL 3.5.3 from source. Do these options seem fine with you?\n\n\n## GCC version line (NGX_COMPILER) missing from nginx -V\n\nI haven't been able to fix that one. When using Mock on RHEL10, nginx doesn't seem to find NGX_COMPILER. Moreover, I checked the nginx official packages from RHEL10 and it's the same behaviour, also the same with Debian. Should this issue be fixed in nginx? It's still showing fine on RHEL7 to RHEL9.\n\n```bash\n[nginx-more/Build el10 aarch64 rpm]\n\n| Complete!\n| nginx version: nginx/1.29.1\n| custom build maintained on github.com/karljohns0n/nginx-more\n| built with OpenSSL 3.5.3 16 Sep 2025\n| TLS SNI support enabled\n| configure arguments: --prefix=/usr/share/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib64/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --http-client-body-temp-path=/var/lib/nginx/cache/client_body --http-proxy-temp-path=/var/lib/nginx/cache/proxy --http-fastcgi-temp-path=/var/lib/nginx/cache/fastcgi --http-uwsgi-temp-path=/var/lib/nginx/cache/uwsgi --http-scgi-temp-path=/var/lib/nginx/cache/scgi --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --user=nginx --group=nginx --with-compat --with-file-aio --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_image_filter_module --with-http_sub_module --with-http_dav_module --with-http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_random_index_module --with-http_secure_link_module --with-http_degradation_module --with-http_stub_status_module --with-http_auth_request_module --with-http_xslt_module --with-http_v2_module --with-http_v3_module --with-mail --with-mail_ssl_module --with-threads --with-stream --with-stream_ssl_module --with-stream_realip_module --with-http_slice_module --with-stream_ssl_preread_module --with-debug --with-cc-opt='-O2 -flto=auto -ffat-lto-objects -fexceptions -g -grecord-gcc-switches -pipe -Wall -Wno-complain-wrong-lang -Werror=format-security -Wp,-U_FORTIFY_SOURCE,-D_FORTIFY_SOURCE=3 -Wp,-D_GLIBCXX_ASSERTIONS -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -fstack-protector-strong -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1 -mbranch-protection=standard -fasynchronous-unwind-tables -fstack-clash-protection -DTCP_FASTOPEN=23' --with-ld-opt='-Wl,-z,relro -Wl,--as-needed -Wl,-z,pack-relative-relocs -Wl,-z,now -specs=/usr/lib/rpm/redhat/redhat-hardened-ld -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1 -Wl,--build-id=sha1 -specs=/usr/lib/rpm/redhat/redhat-package-notes -Wl,-E -O2' --with-openssl=modules/openssl-3.5.3 --with-openssl-opt='-fno-lto -fPIC enable-ktls' --add-dynamic-module=modules/ngx_modsecurity-1.0.4 --add-module=modules/ngx_headers_more-0.39 --add-module=modules/ngx_cache_purge-2.3 --add-module=modules/ngx_brotli-1.0.0rc-2-g6e97 --add-module=modules/ngx_module_vts-0.2.4 --add-module=modules/ngx_http_geoip2_module-3.4 --add-module=modules/ngx_echo-0.63\n```\n\n```bash\n[16:00:21 =>  pkg-nginx-more git:(master) \u2717 ] docker run --rm rockylinux/rockylinux:9 bash -c \"dnf -y -q install nginx && nginx -V\"\n\nnginx version: nginx/1.20.1\nbuilt by gcc 11.5.0 20240719 (Red Hat 11.5.0-5) (GCC) \nbuilt with OpenSSL 3.2.2 4 Jun 2024\n````\n\n```bash\n[16:03:12 =>  pkg-nginx-more git:(master) \u2717 ] docker run --rm rockylinux/rockylinux:10 bash -c \"dnf -y -q install nginx && nginx -V\"    \n\nnginx version: nginx/1.26.3\nbuilt with OpenSSL 3.2.2 4 Jun 2024\n````\n\n```bash\n[16:04:40 =>  pkg-nginx-more git:(master) \u2717 ] docker run --rm debian:13 bash -c \\                                                  \n'apt-get update && apt-get install -y --no-install-recommends nginx && nginx -V'\n\nnginx version: nginx/1.26.3\nbuilt with OpenSSL 3.5.1 1 Jul 2025\n````\n\nThanks again,\n\n---\n\n> --with-openssl-opt=\"-fno-lto -fPIC enable-ktls\"\n> ...\n> This leads from 40 minutes of build time to 4 minutes with OpenSSL\n> 3.5.3 from source.\n\nYes, that's in line with what I see...\n\n> Do these options seem fine with you?\n\nYes, though I don't think you need `-fPIC`.\n\n> GCC version line (NGX_COMPILER) missing from nginx -V\n>\n> I haven't been able to fix that one. When using Mock on RHEL10, nginx\n> doesn't seem to find NGX_COMPILER.    Moreover, I checked the nginx\n> official packages from RHEL10 and it's the same behaviour, also the\n> same with Debian. Should this issue be fixed in nginx? It's still\n> showing fine on RHEL7 to RHEL9.\n>\n> Should this issue be fixed in nginx\n\nNot sure, needs more investigation, but I don't think it's an issue with\nnginx per se, but perhaps more about build environment/process.\n\nMy RHEL 10 build of nginx has it...\n\n```\n$ ./objs/nginx -V\n...\nbuilt by gcc 14.2.1 20250110 (Red Hat 14.2.1-7) (GCC)\n...\n```\n\nMy Fedora 42 build has it...\n\n```\n$ ./objs/nginx -V\n...\nbuilt by gcc 15.2.1 20250808 (Red Hat 15.2.1-1) (GCC)\n...\n```\n\nYet the Fedora 42 packaged nginx doesn't...\n",
      "labels": [],
      "created_at": "2025-09-05T22:35:30Z",
      "closed_at": "2025-11-14T03:54:29Z",
      "url": "https://github.com/nginx/nginx/issues/878",
      "comments_count": 14
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 964,
      "title": "Add Support for Variables in Stream Module for Certificates, Keys, and Server Names",
      "problem": "1. Issue Description\n\nCurrently, the stream module in Nginx does not support the use of variables for SSL/TLS certificate paths, private keys, and hostnames. This limitation means that SSL/TLS configurations in the stream module must be static, which can make it difficult to manage dynamic environments where certificates and keys are based on the hostname or other dynamic values.\n\nIt would be beneficial to add support for variables in the stream module, similar to the capabilities available in the http module. This feature would allow Nginx to dynamically load certificates, keys, and configure hostnames based on variables like $hostname, $server_name, etc.\n\n2. Current Behavior\n\nIn the stream module, SSL certificates and keys must be specified with static paths, which limits flexibility.\n\nThe current configuration does not allow for dynamic changes to certificate or key paths based on variables.\n\n3. Expected Behavior\n\nThe ability to use variables such as $hostname or $server_name in SSL configuration directives within the stream module.\n\nExample of desired configuration:\n\nstream {\n\tmap $ssl_server_name $targetBackend {\n\t\twww.google.com unix:/run/nginx_port443.socket;\n\t}\n\t\tmap $ssl_server_name $secondBackend {\n\t\twww.google.com unix:/run/nginx_port21.socket;\n\t}\n\tmap $ssl_server_name $targetCert {\n\t\twww.google.com /etc/ssl/nginx/www_google_com/www_google_com.crt;\n\t}\n\tmap $ssl_server_name $targetKey {\n\t\twww.google.com /etc/ssl/nginx/www_google_com/www_google_com.key;\n\t}\n\tserver {\n\t\tlisten 8.8.8.8:443 ssl;\n\t\tserver_name $ssl_server_name;\n\t\tpass $targetBackend;\n\t\tssl_certificate $targetCert;\n\t\tssl_certificate_key $targetKey;\n\t}\n\tserver {\n\t\tlisten 8.8.8.8:21 ssl;\n\t\tserver_name $ssl_server_name;\n\t\tpass $secondBackend;\n\t\tssl_certificate $targetCert;\n\t\tssl_certificate_key $targetKey;\n\t}\n}\n\nhttp {\n\tserver {\n\t\tlisten unix:/run/nginx_port443.socket;\n\t\tdefault_type \"text/html\";\n\t\tlocation / {\n\t\t\treturn 200 \"port 443 request completed.\";\n\t\t}\n\tserver {\n\t\tlisten unix:/run/nginx_port21.socket;\n\t\tdefault_type \"text/html\";\n\t\tlocation / {\n\t\t\treturn 200 \"port 21 request completed.\";\n\t\t}\n}\n\n4. Proposed Solution\n\nModify the stream module\u2019s SSL configuration logic to support variable substitution for ssl_certificate, ssl_certificate_key, and server_name.\n\nEnhance the configuration parsing in Nginx to allow dynamic substitution of certificate paths and keys based on variables.\n\nEnsure that the feature is backward compatible, so existing configurations without variables continue to work as expected.\n\n5. Motivation\n\nThe ability to use dynamic variables for SSL certificates and keys would greatly improve the flexibility of Nginx in multi-host or multi-environment configurations. For example, this change would simplify managing multiple domains with different certificates in environments where hostnames may change dynamically, such as in containerized deployments, multi-tenant applications, or systems with automated certificate management (e.g., Let's Encrypt).\n\n6. Additional Information\n\nNginx Version: 1.28.0\n\nOperating System: archlinux20251001\n\nIt would be helpful to have the option to dynamically configure SSL certificates and keys in the stream module just as we can with the http module.",
      "solution": "> The log is as follows: 2025/11/03 09:42:16 [error] 361#361: *316 cannot load certificate \"/etc/nginx/\": PEM_read_bio_X509_AUX() failed (SSL: error:0480006C:PEM routines::no start line:Expecting: TRUSTED CERTIFICATE) while SSL handshaking\n\nAssuming that your configuration is similar to what you added to the issue: if the client sends an unrecognized server name or does not send an SNI extension, `$targetCert` and `$targetKey` will evaluate to an empty value. That's what the error means.\n\nTo fix that, you can\n* add `map` branches for any hostsnames you plan to serve, and `default` for connections without SNI or with unrecognized names.\n* use something like `ssl_certificate /etc/ssl/nginx/$ssl_server_name.crt;` and ensure that you have certificate files or symbolic links for any hostnames you want to serve.",
      "labels": [
        "feature"
      ],
      "created_at": "2025-11-06T01:31:38Z",
      "closed_at": "2025-11-06T03:25:59Z",
      "url": "https://github.com/nginx/nginx/issues/964",
      "comments_count": 4
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 485,
      "title": "DNS resolver caches host resolution with valid=0s",
      "problem": "### Environment\n\nRunning NGINX in Docker\n\nInclude the result of the following commands:\n  - `nginx -V`\n```\nnginx version: nginx/1.27.3\nbuilt by gcc 12.2.0 (Debian 12.2.0-14) \nbuilt with OpenSSL 3.0.11 19 Sep 2023 (running with OpenSSL 3.0.15 3 Sep 2024)\nTLS SNI support enabled\nconfigure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-http_v3_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-g -O2 -ffile-prefix-map=/data/builder/debuild/nginx-1.27.3/debian/debuild-base/nginx-1.27.3=. -fstack-protector-strong -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fPIC' --with-ld-opt='-Wl,-z,relro -Wl,-z,now -Wl,--as-needed -pie'\n```\n\n  - `uname -a`\n```\nLinux e2b-orch-api-q01k 6.8.0-1020-gcp #22~22.04.1-Ubuntu SMP Mon Dec  9 20:42:57 UTC 2024 x86_64 GNU/Linux\n```\n\n### Description\n\nI am trying to setup dynamic hostname resolution. I have my DNS server, which either returns IP address of local VM or \"dummy\" server, which returns error instead of generic 502.\n\nI found if I try request VM before it starts, it correctly returns the error from \"dummy\" server. If I start the VM (it takes around 100ms) and do the request again, it doesn't hit my DNS server and returns error again.\n \nI would expect it hits my DNS server on each request. \n\n- [x] The bug is reproducible with the latest version of nginx\n- [x] The nginx configuration is minimized to the smallest possible\nto reproduce the issue and doesn't contain third-party modules\n\n#### nginx configuration\n\n\n`nginx,conf`\n```conf\nuser  nginx;\nworker_processes  auto;\npid        /var/run/nginx.pid;\n\nevents {\n    worker_connections  1024;\n}\n\nhttp {\n    include /etc/nginx/conf.d/*.conf;\n}\n```\n\n`proxy.conf`\n```conf\nmap $host $node_ip {\n  default         \"\";\n  \"~-(?<s>\\w+)-\"  $s;\n}\n\n# DNS server resolved addreses as to <sandbox-id> <ip-address>\nresolver 127.0.0.4 valid=0s;\nresolver_timeout 100ms;\n\nserver {\n  listen 3002;\n\n  proxy_set_header Host $host;\n\n  proxy_cache_bypass 1;\n  proxy_no_cache 1;\n\n  # gzip off;\n\n  location / {\n    proxy_pass $scheme://$node_ip:3003$request_uri;\n  }\n}\n```\n\nor share the configuration in [gist](https://gist.github.com/).\n\n#### nginx debug log\n\nIt is advised to enable\n[debug logging](http://nginx.org/en/docs/debugging_log.html).\n```\n# Your nginx debug log here\n```\nHere's a [gist](https://gist.github.com/jakubno/ee03bbfc3b839f87abc0dac02a6e9623) with logs.\n\nThere are 4 request to `49983-iotpu4b0lvkc1jm142mfd-fa7da789.*.dev`. There's retry on 502, so these correspond to 2 before the VM is started and 2 after VM is started.  You can see the name was always resolved to `127.0.0.1`, but there was only 1 request to DNS server in total. Which results in returning 502 even though the VM is already running. The VM start time is under 100ms, so it's all during 1 second, if I add wait time everything works correctly, so I think there's some small period of time when the DNS response is cached.\n\nI've tested that DNS server isn't caching it. If i use `dig` command it register each DNS request correctly.  \n\nIf you need more information, I am happy to help.",
      "solution": "Thanks for your fast answer. I get it my use case isn't standard. I guess I will have to go with custom solution. Can I create PR which will add this information to docs?",
      "labels": [
        "invalid"
      ],
      "created_at": "2025-01-30T02:36:53Z",
      "closed_at": "2025-04-01T17:37:13Z",
      "url": "https://github.com/nginx/nginx/issues/485",
      "comments_count": 4
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 930,
      "title": "nginx randomly \"forgets\" to add proxy protocol header",
      "problem": "### Environment\n\nInclude the result of the following commands:\n  - `nginx -V`\n  - `uname -a`\n\nI'll skip these commands as I have tested different versions in different setups:\n - Nginx versions 1.23.x, 1.25.x and 1.29.x\n - In Docker official regular and alpine images on Debian 13, Ubuntu server 24.04 and Raspberry Pi OS hosts\n - On bare metal Ubuntu server 24.04\n\n### Description\n\nI have used Nginx as load balancer for various services that I am running in my homelab using http(s) without issues. Now I tried to load balance my dns servers. To get the actual client ips on the dns servers I configured Nginx to use the proxy protocol (port 53 tcp & udp being proxied to a port solely for proxied connections on the dns servers).\nThat caused the log files on the dns servers to grow in size from few kb to multiple mb with errors about missing proxy protocol headers with Nginx's ip as source.\nRan a tcpdump on the corresponding port on the dns servers and there are mostly correctly proxied upd packets with proxy protocol headers, but now and then regular dns packets without.\n\nI then isolated one of the dns servers and started some testing with the above mentioned Nginx instances. For testing I ran loops of nslookups from the shell. The result was always the same: roughly 3-4% of the lookups timed out because they were missing the proxy protocol header. At some point I completely removed my config and just used a minimal config to see if that changed anything but it didn't.\nAside from the obvious timeouts there was absolutely nothing in Nginx logs.\n\n- [x] The bug is reproducible with the latest version of nginx\n- [x] The nginx configuration is minimized to the smallest possible\nto reproduce the issue and doesn't contain third-party modules\n\n#### nginx configuration\n\n```\nworker_processes  auto;\n\nerror_log  /var/log/nginx/error.log notice;\npid        /tmp/nginx.pid;\n\n\nevents {\n    # increased from 1024 to avoid congestions\n    worker_connections  4096;\n}\n\n# use the stream directive\nstream {\n    # for DNS-over-UDP-PROXY\n    server {\n        listen              53 udp;\n        proxy_pass          xxx.xxx.xxx.xxx:538;\n        proxy_protocol      on;\n    }\n    server {\n    # for DNS-over-TCP-PROXY\n        listen              53;\n        proxy_pass          xxx.xxx.xxx.xxx:538;\n        proxy_protocol      on;\n    }\n}\n```\n\nFor now I had to switch to Caddy with layer 4 addon because I wasn't able to solve the issue. No more Errors in the dns server logs since then.",
      "solution": "Hi, I couldn't reproduce this bug on Ubuntu 24.04 and nginx 1.29.3 I built from source, could you provide more info on the issue? \n\n---\n\nI looked through the stream proxy module code and noticed your config is missing `reuseport` on the UDP listener. Without it, different worker processes can handle packets from the same client independently, which could mess up connection state tracking. Try changing to:\n```\nlisten 53 udp reuseport;\n```\nThe 3-4% failure rate makes me think it's a race condition between workers. When `ngx_stream_proxy_next_upstream()` reconnects, it clears the output chain before reinitializing, and if timing is off, packets might skip getting the proxy protocol header added.\n\nAlso worth trying `proxy_responses 1;` to force new sessions per request, and/or temporarily set `worker_processes 1;` to see if the issue goes away with a single worker. If you can enable debug logging and grep for \"proxy protocol\", that would show exactly when headers are being added vs skipped.",
      "labels": [
        "bug"
      ],
      "created_at": "2025-10-16T15:22:57Z",
      "closed_at": "2025-11-03T00:34:52Z",
      "url": "https://github.com/nginx/nginx/issues/930",
      "comments_count": 5
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 916,
      "title": "NGINX refuses to serve HTTPS using anything but ECC ciphers, even though they are enabled",
      "problem": "I am not sure if this is a bug, or a configuration problem and if the problem is related to NGINX, or OpenSSL.\n\nHTTPS is enabled via certbot and it created the file `/etc/letsencrypt/options-ssl-nginx.conf` which is included by NGINX. It has the following content (without comments)\n```\nssl_session_cache shared:le_nginx_SSL:10m;\nssl_session_timeout 1440m;\nssl_session_tickets off;\n\nssl_protocols SSLv2 SSLv3 TLSv1 TLSv1.1 TLSv1.2 TLSv1.3;\nssl_prefer_server_ciphers on;\n\nssl_ciphers \"ALL:COMPLEMENTOFDEFAULT:!aNULL:!eNULL:!NULL\";\n```\nIf i do `openssl ciphers -v 'ALL:COMPLEMENTOFDEFAULT:!aNULL:!eNULL:!NULL'`, OpenSSL outputs this list, so it seems to me, that there are other available ciphers than ECC ones: [openssl_ciphers.log](https://github.com/user-attachments/files/22703901/openssl_ciphers.log)\n\nOpenSSL is - theoretically - set to enable them in `/etc/ssl/openssl.cnf`:\n```\n[openssl_init]\nssl_conf = ssl_sect\n\n[ssl_sect]\nsystem_default = system_default_sect\n\n[system_default_sect]\nMinProtocol = SSLv2\nCipherString = ALL@SECLEVEL=0\n```\nNGINX's own SSL settings in `/etc/nginx/nginx.conf` are commented out, they are not in effect. The default site has these SSL settings in `/etc/nginx/sites-available/default`:\n```\n        listen 80;\n        listen 443 ssl; # managed by Certbot\n        ssl_certificate /etc/letsencrypt/live/example.com/fullchain.pem; # managed by Certbot\n        ssl_certificate_key /etc/letsencrypt/live/example.com/privkey.pem; # managed by Certbot\n        include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot\n        ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot\n        add_header Strict-Transport-Security \"max-age=31536000\" always; # managed by Certbot\n        ssl_trusted_certificate /etc/letsencrypt/live/example.com/chain.pem; # managed by Certbot\n        ssl_stapling on; # managed by Certbot\n        ssl_stapling_verify on; # managed by Certbot\n```\nMy non-ECC-cipher-capable Opera 12.16 still dies with error 40 and ssllabs.com also reports that there are no other available ciphers than ECC ones.\n\nWell, except from 3 TLSv1.3 one, but i need these on TLSv1.2, or below.",
      "solution": "I recon you can downgrade to any version you like, as long as you keep the OpenSSL binary path the same as the current one. But I really discourage you form doing so.\n\nI have one question tho, almost any software or device supports at least TLSv1.0. In your case I know Opera 12.16 supports it. You may have better luck with explaining the problem you're facing instead of downgrading OpenSSL. ",
      "labels": [],
      "created_at": "2025-10-04T19:54:50Z",
      "closed_at": "2025-11-02T03:19:19Z",
      "url": "https://github.com/nginx/nginx/issues/916",
      "comments_count": 20
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 591,
      "title": "Provide an alternative to `add_header` that doesn\u2019t clobber headers from outer scope",
      "problem": "### Describe the feature you'd like to add to nginx\n\nProvide an alternative to `add_header` that doesn\u2019t clobber headers from the outer scope.\n\n### Describe the problem this feature solves\n\nAn `add_header` in an inner scope (such as `location`) causes `add_header` directives in an outer scope (such as `server`) to be ignored.  There are third-party modules that solve this problem, but a feature such as this should not require a third-party module.\n\n### Additional context\n\nThis issue made it onto F5\u2019s corporate blog: [Avoiding the Top 10 NGINX Configuration Mistakes][1].\n\n[1]: https://www.f5.com/company/blog/nginx/avoiding-top-10-nginx-configuration-mistakes",
      "solution": "I hope this issue receives attention and resolution.\n\nThe `fastcgi_param` directive, along with many \u201carray-like\u201d directives, also suffers from this problem.\n\nThe `add_header`, `fastcgi_param`, and other directives come with a significant configuration pitfall out of the box, which can be considered a design flaw that has existed for years.\n\nThis issue has resulted in numerous wasted hours for individuals due to the unintuitive nature of the configuration.\n\n---\n\n> Could there be a global directive (`merge_array_directives`?) that causes array directives to be merged instead of the inner one overwriting the outer one?\n\nIt does not apply to all array directives. For example, allow and deny directives should not be inherited. Therefore, if a solution is needed, it must be handled separately for each specific directive.\n\n---\n\nThe issue has been addressed in #918 and will be available in nginx-1.29.3.",
      "labels": [
        "feature"
      ],
      "created_at": "2025-03-24T03:02:51Z",
      "closed_at": "2025-10-27T16:06:56Z",
      "url": "https://github.com/nginx/nginx/issues/591",
      "comments_count": 8
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 924,
      "title": "nginx returns 5-byte sized responses with status code 200",
      "problem": "### Environment\n\nDetails:\n\n- Nginx version: 1.27.3\n- OS kernel version - linux (amd64)\n\n### Description\n\nnginx returns absurdly small responses (5 bytes) with status 200 following CDN cache purges when trying to access a single page application (`/pay`). It only happens in mobile platforms; not in web-based ones:\n\n```\n<redacted_ip_address> - - [04/Oct/2025:10:03:40 +0000] \"GET /pay?packageId=<redacted_client_related_info> HTTP/1.1\" 200 5 \"-\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 18_7 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148\" \"<redacted_ip_address_2>, <redacted_ip_address_3>\"\n```\n\nThe problem is that I can't seem to reproduce this bug. Here are all of the methods I've tried:\n\n- Trying to replicate in our dev environment (which doesn't have caching) simply doesn't work. When there's no cache, nginx returns totally valid responses.\n- Increasing the disk space of the nginx server\n- Ruled out any inherent infrastructure issues - it works just fine even without caching, the only exception is mobile platforms. Plus, there are no inherent errors; just absurdly small status 200 responses.\n- Configuring the following parameters:\n  - `client_max_body_size`: when setting it to an absurdly small value, it returns an actual error, not status 200.\n  - `lingering_time`: same as the above ^.\n  - `keepalive_timeout`: same as the above ^.\n  - `worker_processes`: makes no difference whether I keep the default value or increase it to 4/8.\n  - `client_header_buffer_size`: same as the above ^ - no difference.\n  - `large_client_header_buffers`: error 400 is returned if the value is too small.\n  - `proxy_buffering`: if it's on and the correlating values are too small, it returns an actual error, not status 200. If it's off, it makes no difference - the situation remains as it is.\n  - `proxy_buffer_size`: returns an error if `proxy_buffering` is on and value is too small.\n  - `proxy_busy_buffers`: same as the above ^.\n  - `proxy_temp_file_write_size`: returns an error if value is too small; default value makes no difference.\n  - `proxy_max_temp_file_size`: same as the above ^.\n- Listening port is not misconfigured - else it wouldn't have worked in all platforms other than mobile.\n- SSL issue - that's not it, because the server is configured to listen in HTTP.\n- Not an IP issue either, as the IP remains static.\n- Not the application buffers as well - if the value is too small, nginx will return error 400.\n- Midflight cache purges - unfortunately, that's not it, either. Tested by sending multiple concurrent request mid cache-purge.\n- HTTP/2 to HTTP/1.1 mismatch - not the issue. Nginx only supports 1.1. Even when I tried to test with explicit HTTP/2.0 requests, but nginx simply responds with HTTP/1.1 and the issue isn't replicated.\n\n#### Small Hints of Promise\n\nI created a mirroring environment with the same infrastructure and settings, but in a different domain. I couldn't reproduce the 5 byte-sized responses with status code 200, but two interesting things happened:\n\n- Tried mobile requests to the specific endpoint from the log (which I redacted). Pre-cache/post cache-purge return different-sized responses. In some mobile browsers, it doesn't even load at all. In firefox, however, it loads up just fine. On PC-based clients, cache purges didn't affect anything, regardless of the browswer.\n- I modified the config a bit to check if the upstream header is empty/missing. If it is, I set a variable to 1. If it's not empty, the variable defaults to 0. Then, in the individual `location` blocks, I'd check if that varible was equal to 1 - if so, I'd return status 404. What ended up happening when I tried the specific endpoint from the log (the one I redacted) is that I got error 404. But when I dropped that setting, I was getting the correct page that the endpoint references.\n\n##### Checklist\n\n- [ ] ~The bug is reproducible with the latest version of nginx~. Couldn't reproduce this bug myself.\n- [x] The nginx configuration is minimized to the smallest possible to reproduce the issue and doesn't contain third-party modules\n\n#### nginx configuration\n\n```\nresolver <private_ip> ipv6=off valid=300s;\nserver {\n  server_name <my_domain>.com;\n\n  set $s3_app_bucket_url \"<mydomain>.com.s3-website-us-west-2.amazonaws.com\";\n  set $pay_url \"pay.<mydomain>.com.s3-website-us-west-2.amazonaws.com\";\n\n  add_header Referrer-Policy \"no-referrer-when-downgrade\";\n\n  add_header Content-Security-Policy \"default-src * 'unsafe-inline' 'unsafe-eval'; script-src * 'unsafe-inline' 'unsafe-eval'; connect-src * 'unsafe-inline'; img-src * data: blob: 'unsafe-inline'; frame-src *; style-src * 'unsafe-inline';\" ;\n\n  location /pay {\n      add_header Cache-Control \"no-cache, no-store, must-revalidate\";\n      rewrite ^/pay/(.*)$ /$1 break;\n      proxy_pass             http://$pay_url;\n      proxy_redirect         off;\n      proxy_http_version     1.1;\n      proxy_set_header       X-Real-IP $remote_addr;\n      proxy_buffering        on;\n      proxy_buffer_size      128k;\n      proxy_buffers          256 128k;\n      proxy_busy_buffers_size 256k;\n      proxy_temp_file_write_size 256k;\n      proxy_max_temp_file_size 1024m;\n      proxy_hide_header      x-amz-id-2;\n      proxy_hide_header      x-amz-request-id;\n      proxy_hide_header      x-amz-meta-server-side-encryption;\n      proxy_hide_header      x-amz-server-side-encryption;\n      proxy_hide_header      Set-Cookie;\n      proxy_ignore_headers   Set-Cookie;\n      add_header X-Frame-Options \"SAMEORIGIN\" always;\n      add_header Content-Security-Policy \"frame-ancestors 'self'\" always;\n      proxy_intercept_errors on;\n      error_page 404 =200 @pay_index;\n  }\n\n  location @pay_index {\n      add_header Cache-Control \"no-cache, no-store, must-revalidate\";\n      add_header X-Frame-Options \"SAMEORIGIN\" always;\n      add_header Content-Security-Policy \"frame-ancestors 'self'\" always;\n      proxy_pass http://$pay_url/index.html;\n  }\n\n  location / {\n      add_header Cache-Control \"no-cache, no-store, must-revalidate\";\n      add_header X-Frame-Options \"SAMEORIGIN\" always;\n      add_header Content-Security-Policy \"frame-ancestors 'self'\" always;\n      proxy_http_version     1.1;\n      proxy_redirect         off;\n      proxy_set_header       X-Real-IP $remote_addr;\n      proxy_hide_header      x-amz-id-2;\n      proxy_hide_header      x-amz-request-id;\n      proxy_hide_header      x-amz-meta-server-side-encryption;\n      proxy_hide_header      x-amz-server-side-encryption;\n      proxy_hide_header      Set-Cookie;\n      proxy_ignore_headers   Set-Cookie;\n      proxy_intercept_errors on;\n\n      proxy_pass http://$s3_app_bucket_url;\n      error_page 404 =200 @default_index;\n  }\n\n  location @default_index {\n      add_header Cache-Control \"no-cache, no-store, must-revalidate\";\n      add_header X-Frame-Options \"SAMEORIGIN\" always;\n      add_header Content-Security-Policy \"frame-ancestors 'self'\" always;\n      proxy_pass http://$s3_app_bucket_url/index.html;\n  }\n}\n```\n",
      "solution": "I couldn't reproduce the bug due to the scale of the issue (more on that later), but there's a definitive solution I found based solely in nginx (and more outside of it, but it's irrelevant for now, I guess). \n\nThe solution relies on utilizing a custom lua script to catch responses/requests with sizes smaller than an `x` amount of bytes and retry the request (which works in my case). Something like so:\n\n```\nlocation /securepay {\n    # ... your existing config ...\n    \n    header_filter_by_lua_block {\n        local body_size = tonumber(ngx.var.body_bytes_sent) or 0\n        if body_size < 98 then\n            ngx.var.retry_needed = \"1\"\n        end\n    }\n    \n    body_filter_by_lua_block {\n        if ngx.var.retry_needed == \"1\" then\n            -- Trigger retry by setting error status\n            ngx.exit(ngx.HTTP_SERVICE_UNAVAILABLE)\n        end\n    }\n    \n    proxy_next_upstream error timeout http_503;\n    proxy_next_upstream_tries 3;\n}\n```\n\nRegarding the scale of the issue - due to the sheer scale of the user base that translates into significant network traffic, reproducing this bug is easier said than done. ",
      "labels": [
        "bug"
      ],
      "created_at": "2025-10-09T19:48:36Z",
      "closed_at": "2025-10-19T08:10:22Z",
      "url": "https://github.com/nginx/nginx/issues/924",
      "comments_count": 1
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 394,
      "title": "After enabling mTLS, the HTTP3 protocol fails to take effect.",
      "problem": "### Environment\n\nInclude the result of the following commands:\n  - `nginx version: nginx/1.27.3\nbuilt by gcc 13.2.1 20240309 (Alpine 13.2.1_git20240309) \nbuilt with OpenSSL 3.3.0 9 Apr 2024 (running with OpenSSL 3.3.2 3 Sep 2024)\nTLS SNI support enabled\nconfigure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --with-perl_modules_path=/usr/lib/perl5/vendor_perl --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-http_v3_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-Os -fstack-clash-protection -Wformat -Werror=format-security -fno-plt -g' --with-ld-opt='-Wl,--as-needed,-O1,--sort-common -Wl,-z,pack-relative-relocs'\n`\n  - `Linux OpenWrt 5.15.28 #0 SMP Mon Mar 14 06:45:28 2022 x86_64 Linux`\n\n### Description\n\nI enabled TLS on my server using a normal Let's Encrypt certificate and used a self-signed certificate for client authentication. When \"ssl_client_verify\" is set to \"off\", the Edge browser can communicate with the server normally using the HTTP3 protocol. However, when \"ssl_client_verify\" is set to \"on\", the browser always uses the HTTP2 protocol to communicate with the server.\n\n#### nginx configuration\n\n```\nserver {\n    listen       8087 ssl;\n    http2        on;\n    http3        on;\n    ssl_early_data on;\n    quic_retry on;\n    listen 8087 quic reuseport;\n    server_name  xxxx.xxxx.com;\n\n    ssl_certificate      /etc/nginx/conf.d/certs/xxxx.xxxx.com/cert.pem;\n    ssl_certificate_key  /etc/nginx/conf.d/certs/xxxx.xxxx.com/key.key;\n\n    ssl_session_cache    shared:SSL:1m;\n    ssl_session_timeout  5m;\n    ssl_client_certificate /etc/nginx/conf.d/certs/xxxx.xxxx.com/client.crt;\n    ssl_verify_client on;\n\n    ssl_protocols       TLSv1.2 TLSv1.3;\n    ssl_ciphers  TLS13-AES-256-GCM-SHA384:TLS13-CHACHA20-POLY1305-SHA256:TLS13-AES-128-GCM-SHA256:TLS13-AES-128-CCM-8-SHA256:TLS13-AES-128-CCM-SHA256:EECDH+CHACHA20:EECDH+CHACHA20-draft:EECDH+ECDSA+AES128:EECDH+aRS\n    ssl_prefer_server_ciphers  on;\n\n    client_max_body_size 4321M;\n\n    error_page 495 496 497 =444 /444.html;\n\n    location = /444.html {\n        internal;\n        return 444;\n    }\n\n    location / {\n        proxy_pass http://10.0.0.13:8080/;\n        add_header Alt-Svc 'h3=\":8087\"; ma=86400';\n    }\n}\n\n```\n\n",
      "solution": "Hello @Maryna-f5 \n\nplease reopen the issue.\n\nI have a NGINX setup with http3 and mTLS, where http3 only works without mTLS and mTLS only works with http2.\nAfter authenticating via mTLS all websites are not loaded, it seems to be an issue with $ssl_client_verify variable that is always set to false when http3 is enabled.\n\nThe issue is already known and should have been fixed with release 1.27, but it's still present:\nhttps://trac.nginx.org/nginx/ticket/2626\n\nPlease let me know which logs to collect for further troubleshooting.\n\n**My tested config examples:**\n\n**Non-working mTLS config with HTTP3**\n\n```\nserver {\n    listen 443 ssl;\n    listen 443 quic;\n\n    server_name tn.*;\n\n    include /config/nginx/ssl.conf;\n\n    client_max_body_size 0;\n\n    ssl_client_certificate /config/custom_ssl/ca.crt;\n    ssl_verify_client optional;\n    set $verify $intranet$ssl_client_verify;\n    if ($verify ~ (0NONE|0FAILED)) {\n      return 444;\n    }\n\n    location / {\n        include /config/nginx/proxy.conf;\n        include /config/nginx/resolver.conf;\n        set $upstream_app triliumnext;\n        set $upstream_port 8080;\n        set $upstream_proto http;\n        proxy_pass $upstream_proto://$upstream_app:$upstream_port;\n\n    }\n\n```\n**Working mTLS config with HTTP2:**\n\n```\nserver {\n    listen 443 ssl;\n#    listen 443 quic;\n\n    server_name tn.*;\n\n    include /config/nginx/ssl.conf;\n\n    client_max_body_size 0;\n\n    ssl_client_certificate /config/custom_ssl/ca.crt;\n    ssl_verify_client optional;\n    set $verify $intranet$ssl_client_verify;\n    if ($verify ~ (0NONE|0FAILED)) {\n      return 444;\n    }\n\n    location / {\n        include /config/nginx/proxy.conf;\n        include /config/nginx/resolver.conf;\n        set $upstream_app triliumnext;\n        set $upstream_port 8080;\n        set $upstream_proto http;\n        proxy_pass $upstream_proto://$upstream_app:$upstream_port;\n\n    }\n```\n",
      "labels": [
        "need more info"
      ],
      "created_at": "2024-12-18T10:03:30Z",
      "closed_at": "2025-05-30T17:58:01Z",
      "url": "https://github.com/nginx/nginx/issues/394",
      "comments_count": 4
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 826,
      "title": "Compile error on Debian 13 (trixie): `./configure: no supported file AIO was found`",
      "problem": "### Environment\n   \n  - `nginx -V`\n\nN/A - does not compile from source. Attempting to compile Nginx 1.29.0.\n\n  - `uname -a`\n\n`Linux hobby-deb13 6.12.38+deb13-arm64 #1 SMP Debian 6.12.38-1 (2025-07-16) aarch64 GNU/Linux`\n\n### Description\n\n- [X] The bug is reproducible with the latest version of nginx\n\n- [ ] The nginx configuration is minimized to the smallest possible to reproduce the issue and doesn't contain third-party modules\n\nN/A - does not compile from source.\n\nI am able to successfully compile from source on Debian 12 (`bookworm`). My compile script yields errors on Debian 13 (`trixie`). I am using a vanilla Debian 12 and performing an in-place upgrade via `apt` as follows:\n\n```\nsudo apt update \\\n&& sudo apt -y dist-upgrade \\\n&& sudo apt -y full-upgrade \\\n&& sudo apt -y autoclean  \\\n&& sudo apt -y autoremove --purge \\\n&& sudo sed -i 's/bookworm/trixie/g' /etc/apt/sources.list \\\n&& sudo sed -i -- 's/bookworm/trixie/g' /etc/apt/sources.list.d/*.list \\\n&& sudo apt clean \\\n&& sudo apt update \\\n&& sudo apt -y -o \"Dpkg::Options::=--force-confdef\" -o \"Dpkg::Options::=--force-confold\" dist-upgrade \\\n&& sudo apt -y -o \"Dpkg::Options::=--force-confdef\" -o \"Dpkg::Options::=--force-confold\" full-upgrade \\\n&& sudo apt -y autoclean  \\\n&& sudo apt -y autoremove --purge\n```\n\nOn a vanilla Debian 12 instance, I can install the Nginx dependencies and compile successfully. On Debian 13 upgraded via the above `apt` dance, I confirm a full reboot takes place before installing the Nginx dependencies and Nginx source compile begins.\n\nDebian 13 compile errors out at `configure` checks with error:\n\n```\nchecking for accept4() ... not found\nchecking for kqueue AIO support ... not found\nchecking for Linux AIO support ... not found\nchecking for Linux AIO support (SYS_eventfd) ... not found\n\n./configure: no supported file AIO was found\nCurrently file AIO is supported on FreeBSD 4.3+ and Linux 2.6.22+ only\n```\n\nDebian 12 compile completes without issue. Here's the `configure` output from the same section:\n\n```\nchecking for accept4() ... found\nchecking for kqueue AIO support ... not found\nchecking for Linux AIO support ... found\n```\n\nDebian 12 (working) `dpkg -l *aio*` output:\n\n```\n$ sudo dpkg -l *aio*\nDesired=Unknown/Install/Remove/Purge/Hold\n| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend\n|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)\n||/ Name             Version      Architecture Description\n+++-================-============-============-===================================================\nii  libaio-dev:arm64 0.3.113-4    arm64        Linux kernel AIO access library - development files\nii  libaio1:arm64    0.3.113-4    arm64        Linux kernel AIO access library - shared library\n```\n\nDebian 13 (failing) `dpkg -l *aio*` output:\n\n```\n$ sudo dpkg -l *aio*\nDesired=Unknown/Install/Remove/Purge/Hold\n| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend\n|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)\n||/ Name             Version      Architecture Description\n+++-================-============-============-===================================================\nii  libaio-dev:arm64 0.3.113-8+b1 arm64        Linux kernel AIO access library - development files\nii  libaio1t64:arm64 0.3.113-8+b1 arm64        Linux kernel AIO access library - shared library\n```\n\n#### nginx configuration\n\n```\n# Your nginx configuration here\n```\n\nN/A - does not compile from source.\n\n#### nginx debug log\n\nN/A - does not compile from source.",
      "solution": "Hi @petecooper !  You should check the probable causes for the issue in `objs/autoconf.err` file as created by nginx' configure.  It will likely to have the debug information on why this fails in your case.",
      "labels": [
        "bug"
      ],
      "created_at": "2025-08-05T18:05:02Z",
      "closed_at": "2025-08-05T22:08:47Z",
      "url": "https://github.com/nginx/nginx/issues/826",
      "comments_count": 2
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 818,
      "title": "Alpine Signing key returns HTTP/404 - Not Found",
      "problem": "### Environment\n\nInclude the result of the following commands:\n  - ~~`nginx -V`~~ N/A, cant install\n  - ~~`uname -a`~~ N/A, cant install\n\n### Description\n\nDescribe the bug in full detail including expected and actual behavior.\nSpecify conditions that caused it. Provide the relevant part of nginx\nconfiguration and debug log.\n\n- ~~[ ] The bug is reproducible with the latest version of nginx~~ N/A cant install due to signing key missing\n- ~~[ ] The nginx configuration is minimized to the smallest possible\nto reproduce the issue and doesn't contain third-party modules~~ N/A cant install due to signing key missing\n\nFor setting up Alpine the [docs](https://nginx.org/en/linux_packages.html#Alpine) state to download the signing key from `https://nginx.org/keys/nginx_signing.rsa.pub`, however as of now it returns `HTTP/404` Not Found.\n\nHas this key moved? if so where is it? if not can we get it fixed. As this key is missing you can't verify the binaries.\n\n#### nginx configuration\n\nN/A\n\n#### nginx debug log\n\nN/A\n",
      "solution": "https://nginx.org/keys/nginx_signing.rsa.pub is available again, which solved the problem for us.\n\n---\n\n> https://nginx.org/keys/nginx_signing.rsa.pub is available again, which solved the problem for us.\n\nOur build worked this morning too!\n\nInterestingly we have two builds:\n- the arm64 build was **not** able to download the file at 2025-08-05 08:08:41Z\n- the amd64 build was able to download the file at 2025-08-05 09:40:34Z\n\nWe emulate arm64 on amd64 hardware using QEMU so the arm64 job takes considerably longer and finished after amd64 even though arm64 starts first.\n\nSo the issue must have been solved after 08:08:41Z but before 09:40:34Z.",
      "labels": [
        "bug"
      ],
      "created_at": "2025-08-03T04:58:14Z",
      "closed_at": "2025-08-05T16:29:54Z",
      "url": "https://github.com/nginx/nginx/issues/818",
      "comments_count": 6
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 132,
      "title": "Stream's server_name not taking in regex capture",
      "problem": "### Environment\r\n\r\nI'm trying everything in a docker-compose environment to keep things simple and repeatable. I've tried this with nginx=1.25.5 and newer.\r\n\r\n<details>\r\n\r\n<summary>docker-compose.yaml</summary>\r\n\r\n```\r\nservices:\r\n  nginx:\r\n    platform: linux/amd64\r\n    image: \"nginx:1.25.5-bookworm\"\r\n    command: [nginx-debug, '-g', 'daemon off;']\r\n    ports:\r\n      - \"33306:33306\"\r\n      - \"8080:80\"\r\n    volumes:\r\n      - ${PWD}/nginx.conf:/etc/nginx/nginx.conf\r\n  mysql:\r\n    platform: linux/amd64\r\n    image: \"mysql:8.0.39-bookworm\"\r\n    environment:\r\n      - MYSQL_ROOT_PASSWORD=123456\r\n    ports:\r\n      - '3306:3306'\r\n```\r\n\r\n</details>\r\n\r\n\r\n<details>\r\n\r\n<summary>nginx.conf</summary>\r\n\r\n```\r\nuser  nginx;\r\nworker_processes  auto;\r\n\r\nerror_log  /var/log/nginx/error.log notice;\r\npid        /var/run/nginx.pid;\r\n\r\n\r\nevents {\r\n    worker_connections  512;\r\n}\r\n\r\n\r\n\r\n\r\nhttp {\r\n    include       /etc/nginx/mime.types;\r\n    default_type  application/octet-stream;\r\n\r\n    log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\r\n                      '$status $body_bytes_sent \"$http_referer\" '\r\n                      '\"$http_user_agent\" \"$http_x_forwarded_for\"';\r\n\r\n    access_log  /var/log/nginx/access.log  main;\r\n\r\n    sendfile        on;\r\n    #tcp_nopush     on;\r\n\r\n    keepalive_timeout  65;\r\n\r\n    #gzip  on;\r\n\r\n    # include /etc/nginx/conf.d/*.conf;\r\n\r\n}\r\n\r\nstream {\r\n    server {\r\n        listen 33306;\r\n\r\n        server_name ~^(?<service>.+).local.docker$;\r\n\r\n        proxy_pass $service:3306;\r\n    }\r\n}\r\n\r\n```\r\n\r\n</details>\r\n\r\n\r\n<details>\r\n\r\n<summary>/etc/hosts</summary>\r\n\r\n```\r\n127.0.0.1 mysql.local.docker\r\n```\r\n\r\n</details>\r\n\r\n### Description\r\n\r\nThe [`server_name` being added to the streams module](https://nginx.org/en/docs/stream/ngx_stream_core_module.html#server_name), I'm very interested in using it to create a rudimentary dynamic proxy to containerized databases for development environments. However, the regex capture as described in the documentation is not working for me. Perhaps I'm missing some more context about how this is supposed to work, and it might not actually work. \ud83e\udd37 \r\n\r\nWhen I make a request using telnet, mysql, or curl to `mysql.local.docker` to the stream port 33306, I'm expecting the regex `server_name ~^(?<service>.+).local.docker$;` to capture the host and route the service to `mysql`, and I don't see that happening. The connection is not passed through.\r\n\r\n#### nginx configuration\r\n\r\nThe relevant nginx configuration:\r\n```\r\nstream {\r\n    server {\r\n        listen 33306;\r\n\r\n        server_name ~^(?<service>.+).local.docker$;\r\n\r\n        proxy_pass $service:3306;\r\n    }\r\n}\r\n```\r\n\r\n\r\n#### nginx debug log\r\n\r\nI can see no `$service` is captured here when I execute `telnet mysql.local.docker 33306`. It ends up being the blank string.\r\n\r\n```\r\nnginx-1  | 2024/09/06 22:00:52 [error] 29#29: *1 no host in upstream \":3306\", client: 192.168.65.1, server: 0.0.0.0:33306, bytes from/to client:0/0, bytes from/to upstream:0/0\r\n``` \r\n\r\n#### other things\r\n\r\nI've tried a few different kind of regex captures, the numbered capture or even a complete capture\r\n```\r\n        server_name ~^(.+)$;\r\n        proxy_pass $1:3306;\r\n```\r\n\r\nwith no luck.",
      "solution": "In Stream server name always comes from SSL, which is missing in your configuration. There are two ways to use it:\r\n- ssl termination; in this case you need to enable `ssl` in `listen`\r\n- `ssl_preread`; in this case you need to enable `ssl_preread` in the default server\r\n\r\nIn HTTP server name may also come from HTTP Host header, but this option is unavailable in Stream for obvious reasons.",
      "labels": [
        "help wanted"
      ],
      "created_at": "2024-09-06T22:06:22Z",
      "closed_at": "2025-01-14T23:52:09Z",
      "url": "https://github.com/nginx/nginx/issues/132",
      "comments_count": 4
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 807,
      "title": "Early hints frame is not sent until the complete response from upstream",
      "problem": "We got the same issue recently upon testing the early hints with HTTP/2 enabled, and it looks like the frame is queued but never sent to the client until the full response from upstream arrives.\n\nI managed to find a workaround using call to `ngx_http_v2_send_output_queue`, but it smells and I'm not sure it's the right way to fix it.\n\nNevertheless, here is the snippet:\n```c\ndiff --git a/src/http/v2/ngx_http_v2_filter_module.c b/src/http/v2/ngx_http_v2_filter_module.c\nindex 907906a88..fa72cc4f6 100644\n--- a/src/http/v2/ngx_http_v2_filter_module.c\n+++ b/src/http/v2/ngx_http_v2_filter_module.c\n@@ -787,7 +787,14 @@ ngx_http_v2_early_hints_filter(ngx_http_request_t *r)\n         return NGX_ERROR;\n     }\n \n-    return ngx_http_v2_filter_send(fc, stream);\n+    fc->buffered |= NGX_HTTP_V2_BUFFERED;\n+\n+    if (ngx_http_v2_send_output_queue(stream->connection) == NGX_ERROR) {\n+        fc->error = 1;\n+        return NGX_ERROR;\n+    }\n+\n+    return NGX_OK;\n }\n \n \n\n```\n\n_Originally posted by @vpotseluyko in https://github.com/nginx/nginx/issues/326#issuecomment-3111364338_\n\nHello everyone, just tested this feature on localhost in following setup:\n\n1) nginx taken from docker (1.29.0) configured with self signed ssl + http2 \n2) proxy pass to node.js web application which is implementing early hints on its side. \n\nNode.js is configured as following: it returns early hints headers immediately, but sends body with 3 seconds delay. I can see this behaviour works correctly via curl\n\nWhen making request to node.js application via nginx - I get early hints in same time as everything else after 3 seconds\n\n**expected behaviour:** nginx sends early hints headers as soon as gets. \n**current behaviour:** nginx sends early hints with whole response.\n\nP.S. this is relevant for nginx with `http2 on`. While on http1.1 it works as expected.",
      "solution": "Thanks for reporting and proposing a solution. Manipulating HTTP/2 stream fake connection like that is not a good idea. Please try the proposed solution in #808.",
      "labels": [],
      "created_at": "2025-07-24T09:59:20Z",
      "closed_at": "2025-07-24T14:49:27Z",
      "url": "https://github.com/nginx/nginx/issues/807",
      "comments_count": 2
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 785,
      "title": "Critical Bug: Regex `location ~ ^/[^/]+$` incorrectly matches all paths, including root `/`",
      "problem": "### Environment\n\n* **nginx -V output:** *nginx version: nginx/1.22.1*\n* **uname -a output:** *Linux lingyi 6.1.0-37-cloud-amd64 SMP PREEMPT_DYNAMIC Debian 6.1.140-1 (2025-05-22) x86_64 GNU/Linux*\n* **Also reproducible on:** Windows 10, with official Nginx build (e.g., 1.22.0). This bug is cross-platform.\n\n### Description\n\nThere is a critical bug in how Nginx handles regex-based `location` blocks. A standard regex pattern `~ ^/[^/]+$` which is intended to match only single-level paths (e.g., `/login`, `/api`) incorrectly matches **all** paths, including the root path (`/`).\n\nThis appears to be caused by a bug in the location matching engine when a regex pattern for a path segment begins with a meta-character (like `[` or `(`).\n\n**Expected Behavior:**\n\nA request to the root path `/` should be handled by the `location / { ... }` block.\n\n**Actual Behavior:**\n\nThe request for the root path `/` is incorrectly captured and handled by the `location ~ ^/[^/]+$ { ... }` block.\n\n- [x] The bug is reproducible with the latest version of nginx\n- [x] The nginx configuration is minimized to the smallest possible to reproduce the issue and doesn't contain third-party modules\n\n### nginx configuration\n\n```nginx\nworker_processes  1;\nerror_log stderr; # Log to stderr for easy debugging\nevents {\n    worker_connections  1024;\n}\nhttp {\n    server {\n        listen 80;\n        server_name localhost;\n\n        # This block is supposed to match single-level paths like /page\n        # But it incorrectly matches EVERYTHING.\n        location ~ ^/[^/]+$ {\n            return 502; # Use a simple return for easy debugging\n        }\n\n        # This block should handle the root `/` and multi-level paths.\n        # It is never reached when the regex location is present.\n        location / {\n            return 200 'This is the root location.';\n        }\n    }\n}\n```\nSteps to Reproduce\nUse the minimal nginx.conf provided above.\n\nStart Nginx: nginx -c /path/to/your/nginx.conf\n\nExecute curl -I http://localhost/ in a terminal.\n\nObserve the server returns HTTP/1.1 502 Bad Gateway. It should have returned HTTP/1.1 200 OK.\n\nAdditional Context (Workaround)\nThe bug can be successfully worked around by moving the regex logic into a map directive in the http block and using a variable with an if condition inside location /. This strongly suggests the bug is in the location block parsing logic itself, not the PCRE library.",
      "solution": "@arut \nOkay, I've figured it out. The problem was with my configuration. Here is my actual configuration logic:\n```nginx\nworker_processes  1;\nevents {\n    worker_connections  1024;\n}\nhttp {\n    server {\n        listen 80;\n        server_name localhost;\n\n        location ~ ^/[^/]+$ {\n            return 502;\n        }\n\n        location / {\n            # return 200;\n            try_files $uri $uri/ /404.html;\n        }\n    }\n}\n```\nThis is how the requests were being processed:\n\n/ -> matches location / -> try_files internally redirects to /404.html -> which then matches location ~ ^/[^/]+$ -> returns 502\n\n/test -> matches location ~ ^/[^/]+$ -> returns 502\n\n/test/test -> matches location / -> try_files internally redirects to /404.html -> which then matches location ~ ^/[^/]+$ -> returns 502\n\nThis led me to mistakenly believe that location ~ ^/[^/]+$ was matching all paths.",
      "labels": [
        "bug"
      ],
      "created_at": "2025-07-13T14:49:06Z",
      "closed_at": "2025-07-14T12:54:34Z",
      "url": "https://github.com/nginx/nginx/issues/785",
      "comments_count": 3
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 342,
      "title": "NGINX Cache Stuck in Updating: File Not Updating",
      "problem": "### Environment\nnginx version: Telewebion/1.21.4-Debian\n\nLinux RP 5.15.0-124-generic #134-Ubuntu SMP Fri Sep 27 20:20:17 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux\n\nconfigure arguments: --prefix=/etc/nginx/nginx --with-debug --with-cc-opt='-DNGX_LUA_USE_ASSERT -DNGX_LUA_ABORT_AT_PANIC -O2 -O3' --add-module=../ngx_devel_kit-0.3.1 --add-module=../echo-nginx-module-0.62 --add-module=../xss-nginx-module-0.06 --add-module=../ngx_coolkit-0.2 --add-module=../set-misc-nginx-module-0.33 --add-module=../form-input-nginx-module-0.12 --add-module=../encrypted-session-nginx-module-0.09 --add-module=../srcache-nginx-module-0.32 --add-module=../ngx_lua-0.10.21 --add-module=../ngx_lua_upstream-0.07 --add-module=../headers-more-nginx-module-0.33 --add-module=../array-var-nginx-module-0.05 --add-module=../memc-nginx-module-0.19 --add-module=../redis2-nginx-module-0.15 --add-module=../redis-nginx-module-0.3.9 --add-module=../ngx_stream_lua-0.0.11 --with-ld-opt=-Wl,-rpath,/etc/nginx/luajit/lib --build=Telewebion --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --user=nginx --group=nginx --build=Debian --builddir=nginx-1.18.0 --without-http_autoindex_module --with-select_module --with-poll_module --with-threads --with-file-aio --with-http_ssl_module --with-http_v2_module --with-http_realip_module --with-http_addition_module --with-http_xslt_module=dynamic --with-http_image_filter_module=dynamic --with-http_geoip_module=dynamic --with-http_sub_module --with-http_dav_module --with-http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_auth_request_module --with-http_random_index_module --with-http_secure_link_module --with-http_degradation_module --with-http_slice_module --with-http_stub_status_module --with-http_perl_module=dynamic --with-perl_modules_path=/usr/share/perl/5.26.1 --with-perl=/usr/bin/perl --http-log-path=/var/log/nginx/access.log --with-mail=dynamic --with-mail_ssl_module --with-stream=dynamic --with-stream_ssl_module --with-stream_realip_module --with-stream_geoip_module=dynamic --with-stream_ssl_preread_module --with-compat --with-pcre-jit --with-openssl-opt='-g no-nextprotoneg enable-ktls' --add-module=/tmp/openresty-1.21.4.1/../nginx-module-vts-master --add-module=/tmp/openresty-1.21.4.1/../ngx_cache_purge-master --add-module=/tmp/openresty-1.21.4.1/../ngx_http_geoip2_module-master --with-stream --with-stream --without-mail_pop3_module --without-mail_imap_module --without-mail_smtp_module --add-module=/tmp/openresty-1.21.4.1/../ngx_brotli-master --add-module=/tmp/openresty-1.21.4.1/../ngx_http_substitutions_filter_module-master --add-dynamic-module=/tmp/openresty-1.21.4.1/../nginx-vod-module-master --add-dynamic-module=/tmp/openresty-1.21.4.1/../nginx-rtmp-module-master --with-pcre=/tmp/openresty-1.21.4.1/../pcre-8.45 --with-zlib=/tmp/openresty-1.21.4.1/../zlib-1.2.13 --with-openssl=/tmp/openresty-1.21.4.1/../openssl-3.0.8 --with-pcre-opt=-g --with-zlib-opt=-g --with-stream\n\n### Description\n\nThere are 2 cache files for the same cach keys and one of them have some suffixes with uncompleted content. when this issue happen in my system for the same request cache status stayed in Updating mode for a long time and then return hit but cache file does not update any more on this server. always the cache state retuned to updating but no checges wriite in actual cache file and cache files with suffix\n\nto reproduce the issue and doesn't contain third-party modules\n\ntwo cache filesname:\n```\n-rw------- 1 www-data www-data 5.9K Nov  2 16:28 /mnt/ramdisk/cache/e/71/c7/eb56fcea7dd4757743e70b274dfc771e\n-rw------- 1 www-data www-data 5.9K Nov  2 16:50 /mnt/ramdisk/cache/e/71/c7/eb56fcea7dd4757743e70b274dfc771e.0005819463\n```\n\n\n\n#### Cache config\n\n```\nMy cache config:\nproxy_cache_path /mnt/ramdisk/cache use_temp_path=off levels=1:2:2\n    keys_zone=all_ramcache:1024m inactive=1y loader_threshold=60000\n    manager_files=800 manager_threshold=1500 manager_sleep=25\n    loader_files=10000000 min_free=50G max_size=300g;\n```\nor share the configuration in [gist](https://gist.github.com/).\n\n#### nginx debug log\nPlease notice some informations and variable changed in order to be more secure for sharing log. so the cache_key and hashed cache_key does not match.\n\n```\n024/11/19 12:23:35 [debug] 297927#297927: *77429690 http cl:-1 max:0\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 rewrite phase: 4\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 rewrite phase: 5\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 post rewrite phase: 6\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 generic phase: 7\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http vts limit handler\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 generic phase: 8\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 generic phase: 9\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 generic phase: 10\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 generic phase: 11\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 access phase: 12\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 vts set filter variables\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 access phase: 13\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 access phase: 14\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 access phase: 15\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 post access phase: 16\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 generic phase: 17\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 generic phase: 18\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 posix_memalign: 000055639487F300:4096 @16\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http init upstream, client timer: 0\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http map started\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http script var: \"gzip, br, deflate, zstd\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http script copy: \":\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http script var: \"/kandoo/program/getProgram/\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http map: \"gzip, br, deflate, zstd:/kandoo/program/getProgram/\" \"br\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http map started\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http map started\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http map started\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http script var: \"xxx.xxx.xxx.x\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http map: \"\" \"xxx.xxx.xxx.x\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http script var: \"xxx.xxx.xxx.x\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http script var: \"GB\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http map: \"\" \"GB\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http script var: \"GB\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http map: \"IR\" \"GB\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http script copy: \"v42_\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http script var: \"br\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http script var: \"GB\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http script var: \"gateway.telewebion.com\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http script var: \"/kandoo/program/getProgram/\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http script var: \"ProgramID=0x1b26cd1&EpisodeOffset=0&FirstEpisodes=10\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http cache key: \"v42_brGBgateway.telewebion.com/kandoo/program/getProgram/ProgramID=0x1b26cd1&EpisodeOffset=0&FirstEpisodes=10\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 add cleanup: 000055639487FAD8\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http file cache exists: 0 e:1\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 cache file: \"/mnt/ramdisk/cache/e/71/c7/eb56fcea7dd4757743e70b274dfc771e\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 add cleanup: 000055639487FB30\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 malloc: 00005563953291D0:144\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 malloc: 0000556394683F30:60\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 cached open file: /mnt/ramdisk/cache/e/71/c7/eb56fcea7dd4757743e70b274dfc771e, fd:62, c:1, e:0, u:1\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http file cache fd: 62\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 thread read: 62, 000055639487FBB8, 841, 0\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http upstream cache: -2\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http finalize request: -4, \"/kandoo/program/getProgram/?ProgramID=0x1b26cd1&EpisodeOffset=0&FirstEpisodes=10\" a:1, c:2\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http request count:2 blk:1\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http file cache thread: \"/kandoo/program/getProgram/?ProgramID=0x1b26cd1&EpisodeOffset=0&FirstEpisodes=10\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 thread read: 62, 000055639487FBB8, 841, 0\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http file cache vary: \"Origin\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http file cache vary: origin\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http file cache expired: 5 1730553496 1732006415\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http upstream cache: 5\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http proxy status 200 \"200 OK\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http proxy header: \"Access-Control-Allow-Credentials: true\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http proxy header: \"Access-Control-Allow-Origin: *\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http proxy header: \"Access-Control-Expose-Headers: Content-Length, Content-Type, X-Accel-Expires\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http proxy header: \"Cache-Control: public,max-age=1200,stale-if-error=300\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 posix_memalign: 0000556394B1F360:4096 @16\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http proxy header: \"Content-Type: application/json\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http proxy header: \"Vary: Origin\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http proxy header: \"X-Krakend: Version x.x.x\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http proxy header: \"X-Krakend-Completed: false\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http proxy header: \"Date: Sat, 02 Nov 2024 12:58:16 GMT\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http proxy header: \"Transfer-Encoding: chunked\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http proxy header done\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http file cache send: /mnt/ramdisk/cache/e/71/c7/eb56fcea7dd4757743e70b274dfc771e\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 headers more header filter, uri \"/kandoo/program/getProgram/\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http script var: \"UPDATING\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http script var: \"GB\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http2 header filter\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http2 output header: \":status: 200\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http2 output header: \"date: Tue, 19 Nov 2024 08:53:35 GMT\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http2 output header: \"content-type: application/json\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http2 output header: \"vary: Accept-Encoding\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http2 output header: \"access-control-expose-headers: Content-Length, Content-Type, X-Accel-Expires\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http2 output header: \"cache-control: public,max-age=1200,stale-if-error=300\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http2 output header: \"vary: Origin\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http2 output header: \"access-control-allow-origin: *\"\n2024/11/19 12:23:35 [debug] 297927#297927: *77429690 http2 output header: \"access-control-allow-credentials: true\"\n```\n\nseems the fd became -1 in nginx open file:\n```\nclose cached open file: /mnt/ramdisk/cache/e/71/c7/eb56fcea7dd4757743e70b274dfc771e, fd:-1, c:0, u:1, 1\n```\n\n",
      "solution": "How reproducible is this?\n\nWhat type of filesystem is the cache residing on?\n\n> There are 2 cache files for the same cach keys and one of them have some suffixes with uncompleted content. when this\n\n> -rw------- 1 www-data www-data 5.9K Nov  2 16:28 /mnt/ramdisk/cache/e/71/c7/eb56fcea7dd4757743e70b274dfc771e\n-rw------- 1 www-data www-data 5.9K Nov  2 16:50 /mnt/ramdisk/cache/e/71/c7/eb56fcea7dd4757743e70b274dfc771e.000581946\n\nThe file with the suffix is a temporary file with the updated contents.\n\nIt should get `rename(2)`'d (as both old and new file are under the same mountpoint) to the non-suffixed file.\n\n> but cache file does not update any more on this server. always the cache state retuned to updating but no checges wriite in actual cache file and cache files with suffix\n\nYeah, because the temporary file is somehow left behind, nginx is unable to create a new temporary file to update it as we use the `O_EXCL` flag along with `O_CREAT` on the `open(2)` call\n\n```c\n// src/os/unix/ngx_files.c::ngx_open_tempfile()\n\n    fd = open((const char *) name, O_CREAT|O_EXCL|O_RDWR,                       \n              access ? access : 0600);\n```\n\n_Maybe_ deleting the temporary file would get things going again...\n\nIf this is reproducible it may be informative to capture the contents of the `/proc/<PID>/fd` directories of the nginx worker processes (though wouldn't hurt to check all the nginx processes). \n\nShould look something like\n\n```shell\n$ ls -l /proc/211637/fd\ntotal 0\nlrwx------ 1 andrew andrew 64 Jun 11 23:22 0 -> /dev/pts/23\nlrwx------ 1 andrew andrew 64 Jun 11 23:22 1 -> /dev/pts/23\nl-wx------ 1 andrew andrew 64 Jun 11 23:22 2 -> /opt/nginx/logs/error.log\nlrwx------ 1 andrew andrew 64 Jun 11 23:22 3 -> 'socket:[48694922]'\nl-wx------ 1 andrew andrew 64 Jun 11 23:22 4 -> /dev/pts/23\nl-wx------ 1 andrew andrew 64 Jun 11 23:22 5 -> /opt/nginx/logs/error.log\nlrwx------ 1 andrew andrew 64 Jun 11 23:22 6 -> 'socket:[48694919]'\nlrwx------ 1 andrew andrew 64 Jun 11 23:22 7 -> 'socket:[48694921]'\nlrwx------ 1 andrew andrew 64 Jun 11 23:22 8 -> 'anon_inode:[eventpoll]'\nlrwx------ 1 andrew andrew 64 Jun 11 23:22 9 -> 'anon_inode:[eventfd]'\n```\n\nWhat would be interesting is if you see an entry for the temporary cache file.\n\n---\n\nThank you. I am trying to reproduce the issue and post required info and logs here. ",
      "labels": [
        "bug"
      ],
      "created_at": "2024-11-20T13:32:10Z",
      "closed_at": "2025-07-13T18:03:18Z",
      "url": "https://github.com/nginx/nginx/issues/342",
      "comments_count": 2
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 762,
      "title": "Question about 499 status code and HTTP/2",
      "problem": "Nginx version: nginx/1.18.0-1\n\nHello!\n\nWe use Nginx as a proxy for gprc. If client request times out, I can see in Nginx access log that $status is 200 (and grpc-status is obviously absent, because upstream is still handling request).\n\nI was expecting, that for such a case Nginx will output 499 status code. Can you tell me, should I investigate the problem more thoroughly or this behavior is expected (I mean using 200 code in such situation).\n\nIf it is an expected behavior, can you please explain, why 200 code is used in this case instead of 499 ?",
      "solution": "Ok, I've checked again and can't reproduce this. So closing the issue.",
      "labels": [],
      "created_at": "2025-06-27T11:31:29Z",
      "closed_at": "2025-07-10T13:37:42Z",
      "url": "https://github.com/nginx/nginx/issues/762",
      "comments_count": 1
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 711,
      "title": "OpenSSL 3.5 QUIC support doesn't seem to be working with SNI",
      "problem": "### Environment\n\n  - `nginx -V`\n```\nnginx version: nginx/1.29.0 (Ubuntu)\nbuilt with OpenSSL 3.5.0 8 Apr 2025\nTLS SNI support enabled\nconfigure arguments: --with-cc-opt='-g -O2 -Werror=implicit-function-declaration -fno-omit-frame-pointer -mno-omit-leaf-frame-pointer -ffile-prefix-map=/build/nginx-1.28.0=. -flto=auto -ffat-lto-objects -fstack-protector-strong -fstack-clash-protection -Wformat -Werror=format-security -fcf-protection -march=x86-64-v3 -mtune=icelake-server -fdebug-prefix-map=/build/nginx-1.28.0=/usr/src/nginx-1.28.0-0ubuntu0+git20250526~ppa0~25.10.0 -fPIC -Wdate-time -D_FORTIFY_SOURCE=3 -march=x86-64-v3 -mtune=icelake-server' --with-ld-opt='-Wl,-Bsymbolic-functions -flto=auto -ffat-lto-objects -Wl,-z,relro -Wl,-z,now -fPIC' --prefix=/usr/share/nginx --conf-path=/etc/nginx/nginx.conf --http-log-path=/var/log/nginx/access.log --error-log-path=stderr --lock-path=/var/lock/nginx.lock --pid-path=/run/nginx.pid --modules-path=/usr/lib/nginx/modules --http-client-body-temp-path=/var/lib/nginx/body --http-fastcgi-temp-path=/var/lib/nginx/fastcgi --http-proxy-temp-path=/var/lib/nginx/proxy --http-scgi-temp-path=/var/lib/nginx/scgi --http-uwsgi-temp-path=/var/lib/nginx/uwsgi --with-compat --with-debug --with-pcre-jit --with-http_ssl_module --with-http_stub_status_module --with-http_realip_module --with-http_auth_request_module --with-http_v2_module --with-http_v3_module --with-http_dav_module --with-http_slice_module --with-threads --build=Ubuntu --with-http_addition_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_secure_link_module --with-http_sub_module --with-mail_ssl_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-stream_realip_module --with-http_geoip_module=dynamic --with-http_image_filter_module=dynamic --with-http_perl_module=dynamic --with-http_xslt_module=dynamic --with-mail=dynamic --with-stream=dynamic --with-stream_geoip_module=dynamic\n```\n  - `uname -a`\n`Linux mamarley-oldlaptop 6.15.0-061500-generic #202505252019 SMP PREEMPT_DYNAMIC Sun May 25 20:45:06 EDT 2025 x86_64 x86_64 x86_64 GNU/Linux`\n\n### Description\n\nIt seems that the new OpenSSL 3.5 QUIC support in git master isn't working correctly with configurations that use SNI.  Browsers won't connect (they just use HTTP2 instead) and `gtlsclient` reports `peer does not allow at least 3 unidirectional streams` due to an apparent lack of transport_parameters being received from the server.  The configuration I posted below doesn't work if I do `gtlsclient localhost 443 https://localhost/` (obviously on the same box).  However, if I comment out the `listen` directives in the first `server` block, it works and I get a 404 as expected.  I tried a few different combinations of `server_name` and `default_server`, but I never could get it to respond correctly with multiple `listen` directives and SNI.  This same configuration works fine with 1.28.0 and also with git master if I hack the code to build the OpenSSL compatibility layer instead of the native support in 3.5.\n\n#### nginx configuration\n\n```\ndaemon off;\nerror_log stderr debug;\n\nevents {}\n\nhttp {\n\tserver {\n\t\tlisten [::]:443 quic;\n\t\tlisten 443 quic;\n\n\t\tssl_certificate /etc/ssl/certs/ssl-cert-snakeoil.pem;\n\t\tssl_certificate_key /etc/ssl/private/ssl-cert-snakeoil.key;\n\n\t\tserver_name localhost;\n\n\t\treturn 404;\n\t}\n\tserver {\n\t\tlisten [::]:443 quic reuseport default_server;\n\t\tlisten 443 quic reuseport default_server;\n\n\t\tssl_certificate /etc/ssl/certs/ssl-cert-snakeoil.pem;\n\t\tssl_certificate_key /etc/ssl/private/ssl-cert-snakeoil.key;\n\n\t\treturn 404;\n\t}\n}\n\n```\n\n#### nginx debug log\n\n```\n2025/05/26 20:56:08 [debug] 14099#14099: bind() [::]:443 #4 \n2025/05/26 20:56:08 [debug] 14099#14099: bind() 0.0.0.0:443 #5 \n2025/05/26 20:56:08 [debug] 14099#14099: add cleanup: 00005D329D8E4108\n2025/05/26 20:56:08 [notice] 14099#14099: using the \"epoll\" event method\n2025/05/26 20:56:08 [debug] 14099#14099: counter: 000073A4C2151080, 1\n2025/05/26 20:56:08 [notice] 14099#14099: nginx/1.29.0 (Ubuntu)\n2025/05/26 20:56:08 [notice] 14099#14099: OS: Linux 6.15.0-061500-generic\n2025/05/26 20:56:08 [notice] 14099#14099: getrlimit(RLIMIT_NOFILE): 1024:1073741816\n2025/05/26 20:56:08 [debug] 14099#14099: write: 6, 00007FFD4D82C010, 6, 0\n2025/05/26 20:56:08 [debug] 14099#14099: setproctitle: \"nginx: master process nginx\"\n2025/05/26 20:56:08 [notice] 14099#14099: start worker processes\n2025/05/26 20:56:08 [debug] 14099#14099: channel 6:7\n2025/05/26 20:56:08 [notice] 14099#14099: start worker process 14100\n2025/05/26 20:56:08 [debug] 14099#14099: sigsuspend\n2025/05/26 20:56:08 [debug] 14100#14100: add cleanup: 00005D329D8E4168\n2025/05/26 20:56:08 [debug] 14100#14100: malloc: 00005D329D8DCAD0:16\n2025/05/26 20:56:08 [debug] 14100#14100: add cleanup: 00005D329D8E4180\n2025/05/26 20:56:08 [debug] 14100#14100: malloc: 00005D329D8DC930:16\n2025/05/26 20:56:08 [debug] 14100#14100: notify eventfd: 9\n2025/05/26 20:56:08 [debug] 14100#14100: testing the EPOLLRDHUP flag: success\n2025/05/26 20:56:08 [debug] 14100#14100: malloc: 00005D329D84A810:6144\n2025/05/26 20:56:08 [debug] 14100#14100: malloc: 00005D329D8EA380:126976\n2025/05/26 20:56:08 [debug] 14100#14100: malloc: 00005D329D909390:49152\n2025/05/26 20:56:08 [debug] 14100#14100: malloc: 00005D329D9153A0:49152\n2025/05/26 20:56:08 [debug] 14100#14100: epoll add event: fd:4 op:1 ev:00002001\n2025/05/26 20:56:08 [debug] 14100#14100: epoll add event: fd:5 op:1 ev:00002001\n2025/05/26 20:56:08 [debug] 14100#14100: epoll add event: fd:7 op:1 ev:00002001\n2025/05/26 20:56:08 [debug] 14100#14100: setproctitle: \"nginx: worker process\"\n2025/05/26 20:56:08 [debug] 14100#14100: worker cycle\n2025/05/26 20:56:08 [debug] 14100#14100: epoll timer: -1\n2025/05/26 20:56:11 [debug] 14100#14100: epoll: fd:4 ev:0001 d:00005D329D8EA380\n2025/05/26 20:56:11 [debug] 14100#14100: quic recvmsg on [::]:443, ready: 0\n2025/05/26 20:56:11 [debug] 14100#14100: posix_memalign: 00005D329D838740:512 @16\n2025/05/26 20:56:11 [debug] 14100#14100: malloc: 00005D329D84C020:1200\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic recvmsg: [::1]:47205 fd:4 n:1200\n2025/05/26 20:56:11 [debug] 14100#14100: posix_memalign: 00005D329D8270F0:512 @16\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic run\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic packet rx long flags:ca version:1\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic packet rx init len:1153\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic packet rx dcid len:18 e10a9493cc78705b48302e2375c437d224a9\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic packet rx scid len:17 0230472e01bad9e2a4e59ace310dcef583\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic address validation token len:0 \n2025/05/26 20:56:11 [debug] 14100#14100: *1 malloc: 00005D329D8403A0:2512\n2025/05/26 20:56:11 [debug] 14100#14100: *1 malloc: 00005D329D840D80:1528\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic ngx_quic_set_initial_secret\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic socket seq:0 listening at sid:000000000000100c5ce2a98a835a4517f86f64e1 nsock:1\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic cid seq:0 received id:17:0230472e01bad9e2a4e59ace310dcef583:00000000000000000000000000000000\n2025/05/26 20:56:11 [debug] 14100#14100: *1 posix_memalign: 00005D329D8220A0:512 @16\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic path seq:0 created addr:[::1]:47205\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic path seq:0 set active tx:0 rx:0 valid:0 st:0 mtu:1200\n2025/05/26 20:56:11 [debug] 14100#14100: *1 posix_memalign: 00005D329D842320:512 @16\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic socket seq:-1 listening at sid:e10a9493cc78705b48302e2375c437d224a9 nsock:2\n2025/05/26 20:56:11 [debug] 14100#14100: *1 reusable connection: 1\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic connection created\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic packet rx clearflags:c3\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic packet rx number:781125443 len:4\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic packet len:1200 via sock seq:0 path seq:0\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic path seq:0 status tx:0 rx:1200 valid:0 st:0 mtu:1200\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic stateless reset token 7a2facbcd76bd8dcb5ac38a4062e4dc4\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic frame rx init:781125443 CRYPTO len:362 off:0\n2025/05/26 20:56:11 [debug] 14100#14100: *1 posix_memalign: 00005D329D923750:512 @16\n2025/05/26 20:56:11 [debug] 14100#14100: *1 malloc: 00005D329D923960:4096\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic ngx_quic_cbs_recv_rcd\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic ngx_quic_cbs_release_rcd len:362\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic ngx_quic_cbs_got_transport_params() len:63\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic unknown transport param id:0x2ab2, skipped\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic unknown transport param id:0x11, skipped\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic transport parameters parsed ok\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic tp disable active migration: 0\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic tp idle_timeout:30000\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic tp max_udp_payload_size:65527\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic tp max_data:25165824\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic tp max_stream_data_bidi_local:16777216\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic tp max_stream_data_bidi_remote:0\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic tp max_stream_data_uni:16777216\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic tp initial_max_streams_bidi:0\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic tp initial_max_streams_uni:100\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic tp ack_delay_exponent:3\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic tp max_ack_delay:25\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic tp active_connection_id_limit:7\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic tp initial source_connection_id len:17 0230472e01bad9e2a4e59ace310dcef583\n2025/05/26 20:56:11 [debug] 14100#14100: *1 SSL server name: \"localhost\"\n2025/05/26 20:56:11 [debug] 14100#14100: *1 SSL ALPN supported by client: h3\n2025/05/26 20:56:11 [debug] 14100#14100: *1 SSL ALPN selected: h3\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic ngx_quic_cbs_send len:90\n2025/05/26 20:56:11 [debug] 14100#14100: *1 malloc: 00005D329D92B0C0:4096\n2025/05/26 20:56:11 [debug] 14100#14100: *1 posix_memalign: 00005D329D92A820:512 @16\n2025/05/26 20:56:11 [debug] 14100#14100: *1 post event 00005D329D8409A8\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic ngx_quic_cbs_yield_secret() level:2\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic ngx_quic_cbs_yield_secret() level:2\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic ngx_quic_cbs_send len:41\n2025/05/26 20:56:11 [debug] 14100#14100: *1 malloc: 00005D329D92CDA0:4096\n2025/05/26 20:56:11 [debug] 14100#14100: *1 update posted event 00005D329D8409A8\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic ngx_quic_cbs_send len:752\n2025/05/26 20:56:11 [debug] 14100#14100: *1 malloc: 00005D329D92DDB0:4096\n2025/05/26 20:56:11 [debug] 14100#14100: *1 posix_memalign: 00005D329D92EDC0:512 @16\n2025/05/26 20:56:11 [debug] 14100#14100: *1 update posted event 00005D329D8409A8\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic ngx_quic_cbs_send len:264\n2025/05/26 20:56:11 [debug] 14100#14100: *1 malloc: 00005D329D931260:4096\n2025/05/26 20:56:11 [debug] 14100#14100: *1 update posted event 00005D329D8409A8\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic ngx_quic_cbs_send len:36\n2025/05/26 20:56:11 [debug] 14100#14100: *1 malloc: 00005D329D932270:4096\n2025/05/26 20:56:11 [debug] 14100#14100: *1 posix_memalign: 00005D329D931050:512 @16\n2025/05/26 20:56:11 [debug] 14100#14100: *1 update posted event 00005D329D8409A8\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic ngx_quic_cbs_yield_secret() level:3\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic ngx_quic_cbs_recv_rcd\n2025/05/26 20:56:11 [debug] 14100#14100: *1 SSL_do_handshake: -1\n2025/05/26 20:56:11 [debug] 14100#14100: *1 SSL_get_error: 2\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic frame rx init:781125443 PADDING\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic ngx_quic_ack_packet pn:781125443 largest -1 fr:0 nranges:0\n2025/05/26 20:56:11 [debug] 14100#14100: *1 update posted event 00005D329D8409A8\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic packet done rc:0 level:init decr:1 pn:781125443 perr:0\n2025/05/26 20:56:11 [debug] 14100#14100: *1 event timer add: 4: 30000:83414547\n2025/05/26 20:56:11 [debug] 14100#14100: *1 event timer add: 4: 60000:83444547\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic state: read:30000 close:60000\n2025/05/26 20:56:11 [debug] 14100#14100: timer delta: 3509\n2025/05/26 20:56:11 [debug] 14100#14100: posted event 00005D329D8409A8\n2025/05/26 20:56:11 [debug] 14100#14100: *1 delete posted event 00005D329D8409A8\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic push handler\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic output init packet max:1200 min:0\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic frame tx init:0 ACK n:0 delay:0 781125443\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic frame tx init:0 CRYPTO len:90 off:0\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic packet tx init bytes:102 need_ack:1 number:0 encoded nl:1 trunc:0x0\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic output hs packet max:1034 min:1034\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic frame tx hs:0 CRYPTO len:41 off:0\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic frame tx hs:0 CRYPTO len:752 off:41\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic split frame now:269 need:171 shrink:98\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic frame tx hs:0 CRYPTO len:166 off:793\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic packet tx hs bytes:971 need_ack:1 number:0 encoded nl:1 trunc:0x0\n2025/05/26 20:56:11 [debug] 14100#14100: *1 sendmsg: 1200 of 1200\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic congestion send if:1200\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic congestion idle:0\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic output hs packet max:1200 min:0\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic frame tx hs:1 CRYPTO len:98 off:959\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic frame tx hs:1 CRYPTO len:36 off:1057\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic packet tx hs bytes:143 need_ack:1 number:1 encoded nl:1 trunc:0x1\n2025/05/26 20:56:11 [debug] 14100#14100: *1 sendmsg: 206 of 206\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic congestion send if:1406\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic congestion idle:1\n2025/05/26 20:56:11 [debug] 14100#14100: *1 event timer: 4, old: 83414547, new: 83414547\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic lost timer pto:997\n2025/05/26 20:56:11 [debug] 14100#14100: *1 event timer add: 4: 997:83385544\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic state: send:30000 pto:997 close:60000\n2025/05/26 20:56:11 [debug] 14100#14100: worker cycle\n2025/05/26 20:56:11 [debug] 14100#14100: epoll timer: 997\n2025/05/26 20:56:11 [debug] 14100#14100: epoll: fd:4 ev:0001 d:00005D329D8EA380\n2025/05/26 20:56:11 [debug] 14100#14100: quic recvmsg on [::]:443, ready: 0\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic recvmsg: fd:4 n:73\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic input handler\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic packet rx long flags:e1 version:1\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic packet rx hs len:25\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic packet rx dcid len:20 000000000000100c5ce2a98a835a4517f86f64e1\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic packet rx scid len:17 0230472e01bad9e2a4e59ace310dcef583\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic packet rx clearflags:e3\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic packet rx number:781125443 len:4\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic packet len:73 via sock seq:0 path seq:0\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic path seq:0 status tx:1406 rx:1273 valid:0 st:0 mtu:1200\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic congestion ack idle t:83384552 win:12000 if:1240\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic socket seq:-1 closed nsock:1\n2025/05/26 20:56:11 [debug] 14100#14100: *1 event timer del: 4: 83385544\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic lost timer pto:992\n2025/05/26 20:56:11 [debug] 14100#14100: *1 event timer add: 4: 992:83385544\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic path seq:0 in handshake tx:1406 rx:1273 valid:1 st:0 mtu:1200\n2025/05/26 20:56:11 [debug] 14100#14100: *1 post event 00005D329D8409A8\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic frame rx hs:781125443 CONNECTION_CLOSE err:336 ft:0\n2025/05/26 20:56:11 [debug] 14100#14100: *1 post event 00005D329D840A68\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic ngx_quic_ack_packet pn:781125443 largest -1 fr:0 nranges:0\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic packet done rc:0 level:hs decr:1 pn:781125443 perr:0\n2025/05/26 20:56:11 [debug] 14100#14100: *1 event timer: 4, old: 83414547, new: 83414552\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic state: draining read:29995 pto:992 close:59995\n2025/05/26 20:56:11 [debug] 14100#14100: timer delta: 5\n2025/05/26 20:56:11 [debug] 14100#14100: posted event 00005D329D8409A8\n2025/05/26 20:56:11 [debug] 14100#14100: *1 delete posted event 00005D329D8409A8\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic push handler\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic state: draining read:29995 pto:992 close:59995\n2025/05/26 20:56:11 [debug] 14100#14100: posted event 00005D329D840A68\n2025/05/26 20:56:11 [debug] 14100#14100: *1 delete posted event 00005D329D840A68\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic close handler\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic close initiated rc:0\n2025/05/26 20:56:11 [debug] 14100#14100: *1 event timer del: 4: 83444547\n2025/05/26 20:56:11 [debug] 14100#14100: *1 quic close immediate term:0 drain:1 error:0 \"\"\n2025/05/26 20:56:11 [debug] 14100#14100: *1 event timer add: 4: 2991:83387543\n2025/05/26 20:56:11 [debug] 14100#14100: *1 event timer: 4, old: 83387543, new: 83387543\n2025/05/26 20:56:11 [debug] 14100#14100: *1 event timer del: 4: 83385544\n2025/05/26 20:56:11 [debug] 14100#14100: worker cycle\n2025/05/26 20:56:11 [debug] 14100#14100: epoll timer: 2991\n2025/05/26 20:56:14 [debug] 14100#14100: timer delta: 2994\n2025/05/26 20:56:14 [debug] 14100#14100: *1 event timer del: 4: 83387543\n2025/05/26 20:56:14 [debug] 14100#14100: *1 quic close handler\n2025/05/26 20:56:14 [debug] 14100#14100: *1 quic close resumed rc:0\n2025/05/26 20:56:14 [debug] 14100#14100: *1 quic socket seq:0 closed nsock:0\n2025/05/26 20:56:14 [debug] 14100#14100: *1 quic close completed\n2025/05/26 20:56:14 [debug] 14100#14100: *1 event timer del: 4: 83414547\n2025/05/26 20:56:14 [debug] 14100#14100: *1 reusable connection: 0\n2025/05/26 20:56:14 [debug] 14100#14100: *1 free: 00005D329D932270\n2025/05/26 20:56:14 [debug] 14100#14100: *1 free: 00005D329D931260\n2025/05/26 20:56:14 [debug] 14100#14100: *1 free: 00005D329D92DDB0\n2025/05/26 20:56:14 [debug] 14100#14100: *1 free: 00005D329D92CDA0\n2025/05/26 20:56:14 [debug] 14100#14100: *1 free: 00005D329D92B0C0\n2025/05/26 20:56:14 [debug] 14100#14100: *1 free: 00005D329D923960\n2025/05/26 20:56:14 [debug] 14100#14100: *1 free: 00005D329D840D80\n2025/05/26 20:56:14 [debug] 14100#14100: *1 free: 00005D329D8403A0\n2025/05/26 20:56:14 [debug] 14100#14100: *1 free: 00005D329D84C020\n2025/05/26 20:56:14 [debug] 14100#14100: *1 free: 00005D329D838740, unused: 8\n2025/05/26 20:56:14 [debug] 14100#14100: *1 free: 00005D329D8270F0, unused: 0\n2025/05/26 20:56:14 [debug] 14100#14100: *1 free: 00005D329D8220A0, unused: 8\n2025/05/26 20:56:14 [debug] 14100#14100: *1 free: 00005D329D842320, unused: 8\n2025/05/26 20:56:14 [debug] 14100#14100: *1 free: 00005D329D923750, unused: 0\n2025/05/26 20:56:14 [debug] 14100#14100: *1 free: 00005D329D92A820, unused: 32\n2025/05/26 20:56:14 [debug] 14100#14100: *1 free: 00005D329D92EDC0, unused: 32\n2025/05/26 20:56:14 [debug] 14100#14100: *1 free: 00005D329D931050, unused: 48\n2025/05/26 20:56:14 [debug] 14100#14100: worker cycle\n2025/05/26 20:56:14 [debug] 14100#14100: epoll timer: -1\n^C2025/05/26 20:56:17 [notice] 14100#14100: signal 2 (SIGINT) received, exiting\n2025/05/26 20:56:17 [notice] 14099#14099: signal 2 (SIGINT) received, exiting\n2025/05/26 20:56:17 [debug] 14099#14099: wake up, sigio 0\n2025/05/26 20:56:17 [debug] 14099#14099: child: 0 14100 e:0 t:0 d:0 r:1 j:0\n2025/05/26 20:56:17 [info] 14100#14100: epoll_wait() failed (4: Interrupted system call)\n2025/05/26 20:56:17 [debug] 14100#14100: timer delta: 2988\n2025/05/26 20:56:17 [notice] 14100#14100: exiting\n2025/05/26 20:56:17 [debug] 14099#14099: termination cycle: 50\n2025/05/26 20:56:17 [debug] 14100#14100: flush files\n2025/05/26 20:56:17 [debug] 14099#14099: sigsuspend\n2025/05/26 20:56:17 [debug] 14100#14100: run cleanup: 00005D329D8E4180\n2025/05/26 20:56:17 [debug] 14100#14100: run cleanup: 00005D329D8E4168\n2025/05/26 20:56:17 [debug] 14100#14100: run cleanup: 00005D329D8E4108\n2025/05/26 20:56:17 [debug] 14100#14100: run cleanup: 00005D329D85A848\n2025/05/26 20:56:17 [debug] 14100#14100: run cleanup: 00005D329D85A6E8\n2025/05/26 20:56:17 [debug] 14100#14100: run cleanup: 00005D329D859BF0\n2025/05/26 20:56:17 [debug] 14100#14100: cleanup resolver\n2025/05/26 20:56:17 [debug] 14100#14100: run cleanup: 00005D329D83E4A0\n2025/05/26 20:56:17 [debug] 14100#14100: run cleanup: 00005D329D83E458\n2025/05/26 20:56:17 [debug] 14100#14100: free: 00005D329D8E8510\n2025/05/26 20:56:17 [debug] 14100#14100: free: 00005D329D854870\n2025/05/26 20:56:17 [debug] 14100#14100: free: 00005D329D853860\n2025/05/26 20:56:17 [debug] 14100#14100: free: 00005D329D84E790\n2025/05/26 20:56:17 [debug] 14100#14100: free: 00005D329D84D6D0\n2025/05/26 20:56:17 [debug] 14100#14100: free: 00005D329D84C610\n2025/05/26 20:56:17 [debug] 14100#14100: free: 00005D329D83C390, unused: 0\n2025/05/26 20:56:17 [debug] 14100#14100: free: 00005D329D8453C0, unused: 1\n2025/05/26 20:56:17 [debug] 14100#14100: free: 00005D329D84F850, unused: 0\n2025/05/26 20:56:17 [debug] 14100#14100: free: 00005D329D856880, unused: 6\n2025/05/26 20:56:17 [debug] 14100#14100: free: 00005D329D8E04F0, unused: 856\n2025/05/26 20:56:17 [debug] 14100#14100: free: 00005D329D8E4500, unused: 13576\n2025/05/26 20:56:17 [notice] 14100#14100: exit\n2025/05/26 20:56:17 [notice] 14099#14099: signal 17 (SIGCHLD) received from 14100\n2025/05/26 20:56:17 [notice] 14099#14099: worker process 14100 exited with code 0\n2025/05/26 20:56:17 [debug] 14099#14099: shmtx forced unlock\n2025/05/26 20:56:17 [debug] 14099#14099: wake up, sigio 3\n2025/05/26 20:56:17 [debug] 14099#14099: reap children\n2025/05/26 20:56:17 [debug] 14099#14099: child: 0 14100 e:1 t:1 d:0 r:1 j:0\n2025/05/26 20:56:17 [notice] 14099#14099: exit\n2025/05/26 20:56:17 [debug] 14099#14099: run cleanup: 00005D329D8E4108\n2025/05/26 20:56:17 [debug] 14099#14099: run cleanup: 00005D329D85A848\n2025/05/26 20:56:17 [debug] 14099#14099: run cleanup: 00005D329D85A6E8\n2025/05/26 20:56:17 [debug] 14099#14099: run cleanup: 00005D329D859BF0\n2025/05/26 20:56:17 [debug] 14099#14099: cleanup resolver\n2025/05/26 20:56:17 [debug] 14099#14099: run cleanup: 00005D329D83E4A0\n2025/05/26 20:56:17 [debug] 14099#14099: run cleanup: 00005D329D83E458\n2025/05/26 20:56:17 [debug] 14099#14099: free: 00005D329D8E8510\n2025/05/26 20:56:17 [debug] 14099#14099: free: 00005D329D854870\n2025/05/26 20:56:17 [debug] 14099#14099: free: 00005D329D853860\n2025/05/26 20:56:17 [debug] 14099#14099: free: 00005D329D84E790\n2025/05/26 20:56:17 [debug] 14099#14099: free: 00005D329D84D6D0\n2025/05/26 20:56:17 [debug] 14099#14099: free: 00005D329D84C610\n2025/05/26 20:56:17 [debug] 14099#14099: free: 00005D329D83C390, unused: 0\n2025/05/26 20:56:17 [debug] 14099#14099: free: 00005D329D8453C0, unused: 1\n2025/05/26 20:56:17 [debug] 14099#14099: free: 00005D329D84F850, unused: 0\n2025/05/26 20:56:17 [debug] 14099#14099: free: 00005D329D856880, unused: 6\n2025/05/26 20:56:17 [debug] 14099#14099: free: 00005D329D8E04F0, unused: 938\n2025/05/26 20:56:17 [debug] 14099#14099: free: 00005D329D8E4500, unused: 13576\n```\n",
      "solution": "You can try this workaround.\n\nhttps://github.com/pluknet/nginx/commit/94ff5c57aa84643c2bc2af981cda749b5672afcc\n\nIt won't work though in configurations with mixed default/non-default virtual servers.",
      "labels": [
        "bug"
      ],
      "created_at": "2025-05-27T01:06:26Z",
      "closed_at": "2025-06-22T13:54:03Z",
      "url": "https://github.com/nginx/nginx/issues/711",
      "comments_count": 7
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 534,
      "title": "An error exists in nginx 1.22-1.27",
      "problem": "hello  \n\n\nAs long as User.agent contains the keyword Chrome/. And if the 'connection' header does not contain 'close'. It will cause a delay of 1 second.\n\nI don't know specifically what caused it. Delay by 1 second.\n\nNginx 1.27.4 started with Docker\n\n\n\n![Image](https://github.com/user-attachments/assets/e39df07f-7e52-4e59-a6f8-860944dcf224)\n\n\nIf the connection contains a 'close' condition, this issue will not be triggered\n\n\n![Image](https://github.com/user-attachments/assets/bb82dc00-7f6d-41e1-9625-42f00f319e1f)\n\n\n",
      "solution": "To better understand what happens, you need to provide [debug logs](https://nginx.org/en/docs/debugging_log.html) and include the relevant part of configuration as well.",
      "labels": [
        "need more info"
      ],
      "created_at": "2025-02-20T12:15:01Z",
      "closed_at": "2025-05-30T18:12:54Z",
      "url": "https://github.com/nginx/nginx/issues/534",
      "comments_count": 3
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 497,
      "title": "Nginx Suddenly Stops Routing Requests to My Domain",
      "problem": "**Issue Description:**\nI have been using Nginx without any issues, but suddenly my domain stopped working. However, when I directly access the service using `http://myserver:8459`, it works fine, which means the web service itself is running correctly.\n\nIt seems that Nginx is no longer routing requests to the correct port. I have checked the following:\n- My SSL certificates (`.pem` files) have the correct permissions and are not expired.\n- The `nginx.conf` syntax is correct.\n- No recent changes were made before the issue occurred.\n\nI would appreciate any guidance on how to debug this issue further.\n\n**System Information:**\n- Nginx Version: nginx/1.24.0 (Ubuntu)\n- OS: Linux ip-172-31-36-163 6.8.0-1021-aws #23-Ubuntu SMP\n\n**Steps to Reproduce:**\n1. Run the Nginx server with the current configuration.\n2. Access `http://myserver:8459` directly (works fine).\n3. Try accessing via domain (fails).\n\n**Expected Behavior:**\nRequests to my domain should be routed correctly as before.\n\n**Actual Behavior:**\nRequests to my domain do not go through, but direct port access works.\n\n**Logs & Error Messages:**\n- /var/log/nginx/error.log is empty.\n\nAny help would be greatly appreciated!",
      "solution": "Any solution for this? Facing a similar situation.",
      "labels": [
        "need more info"
      ],
      "created_at": "2025-02-06T10:02:25Z",
      "closed_at": "2025-05-30T18:12:14Z",
      "url": "https://github.com/nginx/nginx/issues/497",
      "comments_count": 3
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 384,
      "title": "Unable to upload files to nginx server",
      "problem": "### Environment\n\nInclude the result of the following commands:\n  - `nginx -V`\n```\nnginx version: nginx/1.27.3 (nginx-quic)\nbuilt by gcc 11.4.0 (Ubuntu 11.4.0-1ubuntu1~22.04)\nbuilt with OpenSSL 1.1.1 (compatible; BoringSSL) (running with BoringSSL)\nTLS SNI support enabled\nconfigure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-http_v3_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc=c++ --build=nginx-quic --with-debug --with-http_v3_module --with-cc-opt='-I/src/boringssl/include -x c' --with-ld-opt='-L/src/boringssl/build/ssl -L/src/boringssl/build/crypto'\nroot@ubuntu:~#\n```\n  - `uname -a`\n```\nLinux ubuntu 5.15.0-124-generic #134-Ubuntu SMP Fri Sep 27 20:20:17 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux\nroot@ubuntu:~#\n```\n\n### Description\n\nI was trying to upload my files to nginx server but unable to do.\n\nFiles location\n\n```\nroot@ubuntu:/upload# ls -lrth Sample.doc\n-rw-r--r-- 1 root root 1.1M Dec  5 14:10 Sample.doc\nroot@ubuntu:/upload#\n```\nClient commands used\n```\nroot@ubuntu:~# curl -v -k -# --limit-rate 1M https://127.0.0.1:8443/upload/Sample.doc_1 -T /upload/Sample.doc\n*   Trying 127.0.0.1:8443...\n* ALPN: curl offers h2,http/1.1\n* TLSv1.3 (OUT), TLS handshake, Client hello (1):\n* TLSv1.3 (IN), TLS handshake, Server hello (2):\n* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):\n* TLSv1.3 (IN), TLS handshake, Certificate (11):\n* TLSv1.3 (IN), TLS handshake, CERT verify (15):\n* TLSv1.3 (IN), TLS handshake, Finished (20):\n* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):\n* TLSv1.3 (OUT), TLS handshake, Finished (20):\n* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384 / X25519 / RSASSA-PSS\n* ALPN: server accepted h2\n* Server certificate:\n*  subject: CN=ubuntu\n*  start date: Nov  6 06:31:08 2024 GMT\n*  expire date: Nov  4 06:31:08 2034 GMT\n*  issuer: CN=ubuntu\n*  SSL certificate verify result: self-signed certificate (18), continuing anyway.\n*   Certificate level 0: Public key type RSA (2048/112 Bits/secBits), signed using sha256WithRSAEncryption\n* Connected to 127.0.0.1 (127.0.0.1) port 8443\n* using HTTP/2\n* [HTTP/2] [1] OPENED stream for https://127.0.0.1:8443/upload/Sample.doc_1\n* [HTTP/2] [1] [:method: PUT]\n* [HTTP/2] [1] [:scheme: https]\n* [HTTP/2] [1] [:authority: 127.0.0.1:8443]\n* [HTTP/2] [1] [:path: /upload/Sample.doc_1]\n* [HTTP/2] [1] [user-agent: curl/8.11.1-DEV]\n* [HTTP/2] [1] [accept: */*]\n* [HTTP/2] [1] [content-length: 1110528]\n> PUT /upload/Sample.doc_1 HTTP/2\n> Host: 127.0.0.1:8443\n> User-Agent: curl/8.11.1-DEV\n> Accept: */*\n> Content-Length: 1110528\n>\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\n< HTTP/2 405\n< server: nginx/1.27.3\n< date: Thu, 05 Dec 2024 14:14:14 GMT\n< content-type: text/html\n< content-length: 157\n< x-protocol: HTTP/2.0\n* HTTP error before end of send, stop sending\n* abort upload after having sent 65412 bytes\n<\n<html>\n<head><title>405 Not Allowed</title></head>\n<body>\n<center><h1>405 Not Allowed</h1></center>\n<hr><center>nginx/1.27.3</center>\n</body>\n</html>\n* Connection #0 to host 127.0.0.1 left intact\nroot@ubuntu:~#\nroot@ubuntu:~# curl -v -k -# --http3-only --limit-rate 1M https://127.0.0.1:8443/upload/Sample.doc_1 -T /upload/Sample.doc\n*   Trying 127.0.0.1:8443...\n* Server certificate:\n*  subject: CN=ubuntu\n*  start date: Nov  6 06:31:08 2024 GMT\n*  expire date: Nov  4 06:31:08 2034 GMT\n*  issuer: CN=ubuntu\n*  SSL certificate verify result: self-signed certificate (18), continuing anyway.\n*   Certificate level 0: Public key type RSA (2048/112 Bits/secBits), signed using sha256WithRSAEncryption\n* Connected to 127.0.0.1 (127.0.0.1) port 8443\n* using HTTP/3\n* [HTTP/3] [0] OPENED stream for https://127.0.0.1:8443/upload/Sample.doc_1\n* [HTTP/3] [0] [:method: PUT]\n* [HTTP/3] [0] [:scheme: https]\n* [HTTP/3] [0] [:authority: 127.0.0.1:8443]\n* [HTTP/3] [0] [:path: /upload/Sample.doc_1]\n* [HTTP/3] [0] [user-agent: curl/8.11.1-DEV]\n* [HTTP/3] [0] [accept: */*]\n* [HTTP/3] [0] [content-length: 1110528]\n> PUT /upload/Sample.doc_1 HTTP/3\n> Host: 127.0.0.1:8443\n> User-Agent: curl/8.11.1-DEV\n> Accept: */*\n> Content-Length: 1110528\n>\n< HTTP/3 405\n< server: nginx/1.27.3\n< date: Thu, 05 Dec 2024 14:14:32 GMT\n< content-type: text/html\n< content-length: 157\n< x-protocol: HTTP/3.0\n* HTTP error before end of send, stop sending\n* abort upload after having sent 65412 bytes\n<\n<html>\n<head><title>405 Not Allowed</title></head>\n<body>\n<center><h1>405 Not Allowed</h1></center>\n<hr><center>nginx/1.27.3</center>\n</body>\n</html>\n* Connection #0 to host 127.0.0.1 left intact\nroot@ubuntu:~#\nroot@ubuntu:~#\n```\n\nEven if I make ```client_max_body_size 0```(which disables check), I am facing same issue\n\n#### nginx configuration\n\n```\nroot@ubuntu:~# cat /etc/nginx/nginx.conf\nuser root;\nworker_processes auto;\npid /run/nginx.pid;\n#include /etc/nginx/modules-enabled/*.conf;\n#quic_bpf off;\n#working_directory /tmp/;\n#worker_rlimit_core 500M;\n\nevents {\n           worker_connections 768;\n           multi_accept on;\n           accept_mutex on;\n}\n\nhttp {\n\n   ##\n   # Basic Settings\n   ##\n\n   sendfile on;\n   tcp_nopush on;\n   tcp_nodelay on;\n   keepalive_timeout 65;\n   types_hash_max_size 2048;\n   # server_tokens off;\n\n   # server_names_hash_bucket_size 64;\n   # server_name_in_redirect off;\n\n   include /etc/nginx/mime.types;\n   default_type application/octet-stream;\n\n   ##\n   # SSL Settings\n   ##\n\n   ssl_protocols TLSv1 TLSv1.1 TLSv1.2 TLSv1.3; # Dropping SSLv3, ref: POODLE\n   ssl_prefer_server_ciphers on;\n\n   ##\n   # Logging Settings\n   ##\n\n   access_log /var/log/nginx/access.log;\n   error_log /var/log/nginx/error.log debug;\n\n   ##\n   # Gzip Settings\n   ##\n\n   gzip on;\n\n   # gzip_vary on;\n   # gzip_proxied any;\n   # gzip_comp_level 6;\n   # gzip_buffers 16 8k;\n   # gzip_http_version 1.1;\n   # gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;\n\n   ##\n   # Virtual Host Configs\n   ##\n\n   #include /etc/nginx/conf.d/*.conf;\n   #include /etc/nginx/sites-enabled/*;\n\n   ## From QUIC/HTTP3 README file\n   log_format quic '$remote_addr - $remote_user [$time_local] '\n                   '\"$server_protocol\" \"$request\" $status $body_bytes_sent '\n                   '\"$http_referer\" \"$http_user_agent\" \"$http3\" \"$http_x_forwarded_for\"'\n                   '\"$request_method $scheme://$host$request_uri '\n                   '$request_id $pid $msec $request_time '\n                   '$upstream_connect_time $upstream_header_time '\n                   '$upstream_response_time \"$request_filename\" '\n                   '$request_completion' '$ssl_protocol/$ssl_cipher '\n                   ' Proxy: \"$proxy_host\" \"$upstream_addr\"';\n\n   access_log /var/log/nginx/access.log quic;\n\n   server {\n        http3 on;\n        http3_hq off;\n        http2 on;\n        # for better compatibility it's recommended\n        # to use the same port for quic and https\n        listen 8443 quic reuseport;\n        listen [::]:8443 quic reuseport;\n        listen 8443 ssl;\n        listen [::]:8443 ssl;\n        #default timeout is 75sec\n        #keepalive_timeout   70;\n\n        ssl_certificate     /etc/ssl/certs/ssl-cert-snakeoil.pem;\n        ssl_certificate_key /etc/ssl/private/ssl-cert-snakeoil.key;\n        ssl_protocols       TLSv1.2 TLSv1.3;\n        ssl_ciphers         ALL:COMPLEMENTOFALL;\n        #ssl_ciphers \"ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384\";\n        ssl_session_cache   shared:SSL:10m;\n        ssl_session_tickets on;\n        ssl_session_timeout 5m;\n        #to enable 0-rtt\n        ssl_early_data      off;\n        #retransmission of quic\n        #quic_retry          on;\n        #to disable hrr with ngtcp2 as client(curl)\n        #ssl_ecdh_curve auto;\n        #to disable hrr with curl as client(ngtcp2 or aioquic)\n        #ssl_ecdh_curve prime256v1:secp384r1;\n        #deafult files directory is /usr/share/nginx/html/\n        #to change to /var/www/html/\n        root  /var/www/html/;\n        #to enable gso\n        #quic_gso on;\n        #http3_stream_buffer_size 64k;\n        #http3_max_concurrent_streams 128;\n        #quic_active_connection_id_limit 2;\n\n        client_max_body_size 1G;\n\n        limit_rate   100M;\n        lingering_close always;\n\n        location / {\n            # required for browsers to direct them into quic port\n            add_header Alt-Svc 'h3=\":$server_port\"; ma=86400';\n            #add_header Alt-Svc 'h3=\":8443\"; ma=86400';\n\n            # signal whether we are using QUIC+HTTP/3\n            add_header X-protocol $server_protocol always;\n            #add_header Last-Modified $date_gmt;\n            #add_header Cache-Control 'no-store, no-cache,max-age=0';\n\n        }\n\n        location /rproxy {\n            proxy_pass http://localhost:80;\n            proxy_buffering off;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-Host $host;\n            proxy_set_header X-Forwarded-Port $server_port;\n            proxy_set_header Host $http_host;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n\n        }\n\n        location ~ \"/upload/([0-9a-zA-Z-.]*)$\" {\n            dav_methods PUT DELETE MKCOL COPY MOVE;\n            dav_access             group:rw all:rw;\n\n            alias     /root/incoming/$1;\n            #root /root/incoming;\n\n            #rewrite ^ /file-$request_id break;\n            client_body_temp_path /tmp;\n            client_body_buffer_size 256K;\n            client_max_body_size 1G;\n            create_full_put_path   on;\n            client_body_in_file_only on;\n        }\n   }\n}\nroot@ubuntu:~#\n```\n\n#### nginx debug log\n\nRefer below comment for logs\n",
      "solution": "You want me to figure it out which directive is causing the issue?\n\n---\n\nAh, OK, so it may not even be a problem any more... some possibilities are\n\n1) If there was a bug, it's since been fixed.\n2) If you were to try recreating the issue you may inadvertently fix whatever the issue was. \n\nBut anyway I did a quick check with a bunch of stuff added from your config and it still works..\n\n```\ndaemon off;                                                                     \npid nginx.pid;                                                                  \nworker_processes auto;                                                          \n                                                                                \nevents {                                                                        \n        worker_connections 768;                                                 \n        multi_accept on;                                                        \n        accept_mutex on;                                                        \n}                                                                               \n                                                                                \nhttp {                                                                          \n        access_log /dev/stdout;                                                 \n                                                                                \n        sendfile on;                                                            \n        tcp_nopush on;                                                          \n        tcp_nodelay on;                                                         \n        keepalive_timeout 65;                                                   \n        types_hash_max_size 2048;                                               \n                                                                                \n        gzip on;                                                                \n                                                                                \n        server {                                                                \n                http2 on;                                                       \n                                                                                \n                server_name localhost;                                          \n                listen [::1]:4040 ssl;                                          \n                                                                                \n                ssl_certificate                 /opt/nginx/cert.pem;            \n                ssl_certificate_key             /opt/nginx/key.pem;             \n                ssl_protocols                   TLSv1.2 TLSv1.3;                \n                ssl_session_tickets             on;                             \n                ssl_session_timeout             5m;                             \n                ssl_early_data                  off;                            \n                                                                                \n                client_max_body_size    1G;                                     \n                limit_rate              100M;                                   \n                lingering_close         always;                                 \n                                                                                \n                location / {                                                    \n                        root                  /srv/nginx/dav;                   \n                                                                                \n                        client_max_body_size     1G;                            \n                        client_body_buffer_size  256K;                          \n                        client_body_temp_path    /tmp;                          \n                        client_body_in_file_only on;                            \n                                                                                \n                        dav_methods PUT DELETE MKCOL COPY MOVE;                 \n                                                                                \n                        create_full_put_path  on;                                                               \n                        dav_access            group:rw  all:r;                  \n                }                                                               \n        }                                                                       \n}\n```\n\nBut then again it could be a config issue...\n\nYou have\n\n```\nlocation ~ \"/upload/([0-9a-zA-Z-.]*)$\" {\n```\n\nand are trying to put up\n\n```\nhttps://127.0.0.1:8443/upload/Sample.doc_1\n```\n\nWhich I don't think will match due to the `_`.\n\nSo I'm going to go ahead and close this. Feel free to re-open if you ever hit this again.",
      "labels": [
        "bug"
      ],
      "created_at": "2024-12-12T04:01:02Z",
      "closed_at": "2025-05-28T21:35:39Z",
      "url": "https://github.com/nginx/nginx/issues/384",
      "comments_count": 12
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 696,
      "title": "Listen on 443 requires ssl_certificate even if the ssl parameter is not specified",
      "problem": "### Environment\n\n```\n# nginx -V\nnginx version: nginx/1.26.3\nbuilt with OpenSSL 3.0.12 24 Oct 2023\nTLS SNI support enabled\nconfigure arguments: --user=nginx --group=nginx --with-ld-opt='-L/usr/pkg/lib -Wl,-R/usr/pkg/lib' --prefix=/usr/pkg --sbin-path=/usr/pkg/sbin --conf-path=/usr/pkg/etc/nginx/nginx.conf --pid-path=/var/run/nginx.pid --lock-path=/var/db/nginx/nginx.lock --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --http-client-body-temp-path=/var/db/nginx/client_body_temp --http-proxy-temp-path=/var/db/nginx/proxy_temp --http-fastcgi-temp-path=/var/db/nginx/fstcgi_temp --http-scgi-temp-path=/var/db/nginx/scgi_temp --with-pcre --with-mail_ssl_module --with-http_ssl_module --with-http_v2_module --with-http_v3_module --with-http_realip_module --http-uwsgi-temp-path=/var/db/nginx/uwsgi_temp --with-http_slice_module --with-http_stub_status_module --with-http_gzip_static_module --with-http_auth_request_module\n```\n\n```\n# uname -a\nNetBSD ghost.triaxx.online 10.1 NetBSD 10.1 (GENERIC) #0: Mon Dec 16 13:08:11 UTC 2024  mkrepro@mkrepro.NetBSD.org:/usr/src/sys/arch/amd64/compile/GENERIC amd64\n```\n\n### Description\n\nNginx claims ssl certificate even for server that doesn't listen to ssl port. The first server definition is only to redirect to www. The error message is not consistent with my configuration file since there is no `listen` directive ending with `ssl`. It is like `listen 443`was implicitly `listen 443 ssl`.\n\n#### nginx configuration\n\n```\nhttp {\n  server {\n    listen                     80;\n    listen                     443;\n    listen                     [::]:80;\n    listen                     [::]:443;\n    server_name                triaxx.online;\n    return             301     $scheme://www.$server_name$request_uri;\n  }\n\n  server {\n    listen                     80;\n    listen                     [::]:80;\n    server_name                www.triaxx.online;\n    return             301     https://www.$server_name$request_uri;\n  }\n\n  server {\n    listen                     443             ssl;\n    listen                     [::]:443        ssl;\n    server_name                www.triaxx.online;\n    ssl_certificate            /etc/openssl/certs/TriaxxOnline.pem;\n    ssl_certificate_key        /etc/openssl/private/TriaxxOnline.pem;\n    }\n}\n```\n\n#### nginx debug log\n\n```\n2025/05/15 11:33:29 [emerg] 18221#0: no \"ssl_certificate\" is defined for the \"listen ... ssl\" directive in /usr/pkg/etc/nginx/nginx.conf:18\n```\n",
      "solution": "Your configuration is wrong and invalid. You cannot assign SSL and non-SSL modes to the same port. As long as you configure listen 443 ssl in any server, nginx will think that port 443 supports SSL, even if you don't configure SSL parameters in other servers.\n\nSolution: Always configure SSL parameters for the same port, or do not configure SSL parameters.",
      "labels": [
        "invalid"
      ],
      "created_at": "2025-05-15T09:50:39Z",
      "closed_at": "2025-05-27T21:56:18Z",
      "url": "https://github.com/nginx/nginx/issues/696",
      "comments_count": 2
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 639,
      "title": "Build failed with boringssl",
      "problem": "I am trying to build nginx(1.27.5) with boringssl(main) but getting failed with below error\n\n```\nsrc/event/modules -I src/event/quic -I src/os/unix -I objs \\\n        -o objs/src/event/quic/ngx_event_quic.o \\\n        src/event/quic/ngx_event_quic.c\nIn file included from src/event/quic/ngx_event_quic_connection.h:29,\n                 from src/event/quic/ngx_event_quic.c:10:\nsrc/event/quic/ngx_event_quic_openssl_compat.h:23:6: error: redeclaration of \u2018enum ssl_encryption_level_t\u2019\n   23 | enum ssl_encryption_level_t {\n      |      ^~~~~~~~~~~~~~~~~~~~~~\nIn file included from src/event/ngx_event_openssl.h:17,\n                 from src/core/ngx_core.h:86,\n                 from src/event/quic/ngx_event_quic.c:8:\n/src/boringssl/include/openssl/ssl.h:3855:6: note: originally defined here\n 3855 | enum ssl_encryption_level_t BORINGSSL_ENUM_INT {\n      |      ^~~~~~~~~~~~~~~~~~~~~~\nIn file included from src/event/quic/ngx_event_quic_connection.h:29,\n                 from src/event/quic/ngx_event_quic.c:10:\nsrc/event/quic/ngx_event_quic_openssl_compat.h:24:5: error: redeclaration of enumerator \u2018ssl_encryption_initial\u2019\n   24 |     ssl_encryption_initial = 0,\n      |     ^~~~~~~~~~~~~~~~~~~~~~\nIn file included from src/event/ngx_event_openssl.h:17,\n                 from src/core/ngx_core.h:86,\n                 from src/event/quic/ngx_event_quic.c:8:\n/src/boringssl/include/openssl/ssl.h:3856:3: note: previous definition of \u2018ssl_encryption_initial\u2019 with type \u2018enum ssl_encryption_level_t\u2019\n 3856 |   ssl_encryption_initial = 0,\n      |   ^~~~~~~~~~~~~~~~~~~~~~\nIn file included from src/event/quic/ngx_event_quic_connection.h:29,\n                 from src/event/quic/ngx_event_quic.c:10:\nsrc/event/quic/ngx_event_quic_openssl_compat.h:25:5: error: redeclaration of enumerator \u2018ssl_encryption_early_data\u2019\n   25 |     ssl_encryption_early_data,\n      |     ^~~~~~~~~~~~~~~~~~~~~~~~~\nIn file included from src/event/ngx_event_openssl.h:17,\n                 from src/core/ngx_core.h:86,\n                 from src/event/quic/ngx_event_quic.c:8:\n/src/boringssl/include/openssl/ssl.h:3857:3: note: previous definition of \u2018ssl_encryption_early_data\u2019 with type \u2018enum ssl_encryption_level_t\u2019\n 3857 |   ssl_encryption_early_data = 1,\n      |   ^~~~~~~~~~~~~~~~~~~~~~~~~\nIn file included from src/event/quic/ngx_event_quic_connection.h:29,\n                 from src/event/quic/ngx_event_quic.c:10:\nsrc/event/quic/ngx_event_quic_openssl_compat.h:26:5: error: redeclaration of enumerator \u2018ssl_encryption_handshake\u2019\n   26 |     ssl_encryption_handshake,\n      |     ^~~~~~~~~~~~~~~~~~~~~~~~\nIn file included from src/event/ngx_event_openssl.h:17,\n                 from src/core/ngx_core.h:86,\n                 from src/event/quic/ngx_event_quic.c:8:\n/src/boringssl/include/openssl/ssl.h:3858:3: note: previous definition of \u2018ssl_encryption_handshake\u2019 with type \u2018enum ssl_encryption_level_t\u2019\n 3858 |   ssl_encryption_handshake = 2,\n      |   ^~~~~~~~~~~~~~~~~~~~~~~~\nIn file included from src/event/quic/ngx_event_quic_connection.h:29,\n                 from src/event/quic/ngx_event_quic.c:10:\nsrc/event/quic/ngx_event_quic_openssl_compat.h:27:5: error: redeclaration of enumerator \u2018ssl_encryption_application\u2019\n   27 |     ssl_encryption_application\n      |     ^~~~~~~~~~~~~~~~~~~~~~~~~~\nIn file included from src/event/ngx_event_openssl.h:17,\n                 from src/core/ngx_core.h:86,\n                 from src/event/quic/ngx_event_quic.c:8:\n/src/boringssl/include/openssl/ssl.h:3859:3: note: previous definition of \u2018ssl_encryption_application\u2019 with type \u2018enum ssl_encryption_level_t\u2019\n 3859 |   ssl_encryption_application = 3,\n      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~\nIn file included from src/event/quic/ngx_event_quic_connection.h:29,\n                 from src/event/quic/ngx_event_quic.c:10:\nsrc/event/quic/ngx_event_quic_openssl_compat.h:31:16: error: redefinition of \u2018struct ssl_quic_method_st\u2019\n   31 | typedef struct ssl_quic_method_st {\n      |                ^~~~~~~~~~~~~~~~~~\nIn file included from src/event/ngx_event_openssl.h:17,\n                 from src/core/ngx_core.h:86,\n                 from src/event/quic/ngx_event_quic.c:8:\n/src/boringssl/include/openssl/ssl.h:3863:8: note: originally defined here\n 3863 | struct ssl_quic_method_st {\n      |        ^~~~~~~~~~~~~~~~~~\nIn file included from src/event/quic/ngx_event_quic_connection.h:29,\n                 from src/event/quic/ngx_event_quic.c:10:\nsrc/event/quic/ngx_event_quic_openssl_compat.h:43:3: error: conflicting types for \u2018SSL_QUIC_METHOD\u2019; have \u2018struct ssl_quic_method_st\u2019\n   43 | } SSL_QUIC_METHOD;\n      |   ^~~~~~~~~~~~~~~\nIn file included from /src/boringssl/include/openssl/ssl.h:20,\n                 from src/event/ngx_event_openssl.h:17,\n                 from src/core/ngx_core.h:86,\n                 from src/event/quic/ngx_event_quic.c:8:\n/src/boringssl/include/openssl/base.h:347:35: note: previous declaration of \u2018SSL_QUIC_METHOD\u2019 with type \u2018SSL_QUIC_METHOD\u2019 {aka \u2018struct ssl_quic_method_st\u2019}\n  347 | typedef struct ssl_quic_method_st SSL_QUIC_METHOD;\n      |                                   ^~~~~~~~~~~~~~~\nIn file included from src/event/quic/ngx_event_quic_connection.h:29,\n                 from src/event/quic/ngx_event_quic.c:10:\nsrc/event/quic/ngx_event_quic_openssl_compat.h:48:5: error: conflicting types for \u2018SSL_set_quic_method\u2019; have \u2018int(SSL *, const SSL_QUIC_METHOD *)\u2019 {aka \u2018int(struct ssl_st *, const struct ssl_quic_method_st *)\u2019}\n   48 | int SSL_set_quic_method(SSL *ssl, const SSL_QUIC_METHOD *quic_method);\n      |     ^~~~~~~~~~~~~~~~~~~\nIn file included from src/event/ngx_event_openssl.h:17,\n                 from src/core/ngx_core.h:86,\n                 from src/event/quic/ngx_event_quic.c:8:\n/src/boringssl/include/openssl/ssl.h:3977:20: note: previous declaration of \u2018SSL_set_quic_method\u2019 with type \u2018int(SSL *, const SSL_QUIC_METHOD *)\u2019 {aka \u2018int(struct ssl_st *, const struct ssl_quic_method_st *)\u2019}\n 3977 | OPENSSL_EXPORT int SSL_set_quic_method(SSL *ssl,\n      |                    ^~~~~~~~~~~~~~~~~~~\nmake[1]: *** [objs/Makefile:1140: objs/src/event/quic/ngx_event_quic.o] Error 1\nmake[1]: Leaving directory '/src/nginx-quic'\nmake: *** [Makefile:10: build] Error 2\n```\nBoringssl build using below steps:-\n\n```\ngit clone https://boringssl.googlesource.com/boringssl \ncd /src/boringssl/\nmkdir build\ncd build/\ncmake ..\nmake\n\ncommit ea482ed0e7431fd65ed2bb9954c5ce47a6b3fb8f (HEAD -> main, origin/main, origin/HEAD)\nAuthor: David Benjamin <davidben@google.com>\nDate:   Thu Apr 17 01:29:03 2025 -0400\n\n    Switch a bit more of libcrypto to scopers\n```\n\nNginx built using below steps\n\n```\ncd /src/\ngit clone https://github.com/nginx/nginx.git -b release-1.27.5 nginx-quic\ncd nginx-quic/\nauto/configure `nginx -V 2>&1 | sed \"s/ \\-\\-/ \\\\\\ \\n\\t--/g\" | grep -v -e 'http-geoip2' | grep \"\\-\\-\" | grep -ve opt= -e param= -e build=` --with-cc=c++ --build=nginx-quic --with-debug  --with-http_v3_module --with-cc-opt=\"-I/src/boringssl/include -x c\" --with-ld-opt=\"-L/src/boringssl/build/ssl -L/src/boringssl/build/crypto\"\nmake\n\ncommit 6ac8b69f06bc10d5503f636da888fa70095b151c (HEAD, tag: release-1.27.5)\nAuthor: Sergey Kandaurov <pluknet@nginx.com>\nDate:   Mon Apr 14 22:35:27 2025 +0400\n\n    nginx-1.27.5-RELEASE\n\ncommit aa49a416b8ea762558211de25d6ee70ca73bb373\nAuthor: Roman Arutyunyan <arut@nginx.com>\nDate:   Mon Apr 14 17:16:47 2025 +0400\n\n    QUIC: dynamic packet threshold.\n```\n\nGo version\n```\nroot@ubuntu:/src/nginx-quic# go version\ngo version go1.24.2 linux/amd64\nroot@ubuntu:/src/nginx-quic#\n```",
      "solution": "> This below paths are correct\n> \n> ```\n> --with-cc-opt=\"-I/src/boringssl/include -x c\" --with-ld-opt=\"-L/src/boringssl/build/ssl -L/src/boringssl/build/crypto\"\n> ```\n\nBoringSSL since https://github.com/google/boringssl/commit/5e3ba4c15b67ed00fe8a71dc3a45ebd6211d299f does not use `build/crypto` and `build/ssl`. You need to pass `-L/src/boringssl/build` instead.\n\n---\n\n> [@bavshin-f5](https://github.com/bavshin-f5) [google/boringssl@5e3ba4c](https://github.com/google/boringssl/commit/5e3ba4c15b67ed00fe8a71dc3a45ebd6211d299f)\n\nAfter testing, when compiling BoringSSL, on the test machine with GCC 12,you need to execute the command `sed -i 's/-Werror/-Werror -Wno-array-bounds/' CMakeLists.txt`for it to compile successfully. However, no issues were encountered during compilation with GCC 13.",
      "labels": [],
      "created_at": "2025-04-18T09:46:55Z",
      "closed_at": "2025-04-18T18:31:38Z",
      "url": "https://github.com/nginx/nginx/issues/639",
      "comments_count": 11
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 119,
      "title": "upload a mp3 file get http 400 error on nginx accect.log",
      "problem": "This is my conf\r\nhttp {\r\n\tsendfile on; \r\n        tcp_nodelay on;        \r\n        keepalive_timeout  1800; \r\n\tlarge_client_header_buffers 9999 9999k;\r\n\tclient_max_body_size    100M;\r\n\tclient_body_buffer_size 100M;\r\n\tproxy_connect_timeout   120s;\r\n\tproxy_send_timeout 120s; \r\n\tproxy_read_timeout 120s; \r\n\tproxy_buffers           9999 9999k;\r\n\r\nserver {\r\n   listen 80;\r\n \r\n   location / {\r\n       proxy_pass XXX\r\n       proxy_http_version 1.1;\r\n       proxy_set_header   Upgrade $http_upgrade;\r\n       proxy_set_header   Connection keep-alive;\r\n       proxy_set_header   Host $http_host;\r\n       proxy_cache_bypass $http_upgrade;\r\n       proxy_set_header   X-Forwarded-For $proxy_add_x_forwarded_for;\r\n       proxy_set_header   X-Forwarded-Proto $scheme;\r\n       proxy_set_header   X-Real-IP $remote_addr;\r\n   }\r\n}\r\n\r\nWhen I upload a mp3 file from device, I get this error on accect.log\r\n<img width=\"286\" alt=\"nginx 400\" src=\"https://github.com/user-attachments/assets/b09f1a84-4c9c-425f-9e76-66cea7a5da41\">\r\n\r\nAnd device error log\r\ndevice get *** select timeout or error *** error\r\n![image](https://github.com/user-attachments/assets/e760128f-23c3-4037-98f3-647a8dd5056a)\r\n\r\nThis my debug log\r\n![image](https://github.com/user-attachments/assets/b720a45b-dcbd-4c58-bba4-334cbdadafb4)\r\n\r\n",
      "solution": "Thanks for reporting this issue, @dfengpo.\n\nWe don't have sufficient information to proceed with further troubleshooting. Closing this issue for now. \nPlease reopen the issue if you have more details about this problem. \n\n---\n\nHas this problem been solved? I also encountered a similar problem @dfengpo ",
      "labels": [
        "need more info"
      ],
      "created_at": "2024-09-01T02:04:20Z",
      "closed_at": "2024-10-23T19:58:09Z",
      "url": "https://github.com/nginx/nginx/issues/119",
      "comments_count": 3
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 666,
      "title": "proxy-body-size allow gigabyte indentifier",
      "problem": "### Describe the feature you'd like to add to nginx\n\nproxy-body-size allow for example 1024m as value. It would be great for better readability to allow value like 1Gi\n\nI dont know it is already implemented but the documentation example only show megabyte as value identifier\nhttps://nginx.org/en/docs/http/ngx_http_core_module.html#client_max_body_size\n\n### Describe the problem this feature solves\n\nAllow values like 10gi or then already implemented add information to documentation \n",
      "solution": "The client_max_body_size directive supports G or g suffixes since nginx release-0.3.53 \n\nNotably, the syntax page (https://nginx.org/en/docs/syntax.html) already describes that certain offset-like directives\nsupport G or g prefix, but that may not be clear from reading e.g. the client_max_body_size directive documentation.\n\nThis is clearly a documentation update request, not related to nginx code.\nUnlike in trac, I cannot reclassify the issue; please open a new one for https://github.com/nginx/nginx.org.",
      "labels": [
        "feature"
      ],
      "created_at": "2025-04-30T08:58:47Z",
      "closed_at": "2025-05-06T10:43:03Z",
      "url": "https://github.com/nginx/nginx/issues/666",
      "comments_count": 6
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 661,
      "title": "Default 'server_tokens' to 'off'",
      "problem": "### Describe the feature you'd like to add to nginx\n\nMake `off` the default value for the directive `server_tokens`.\n\n### Describe the problem this feature solves\n\nCurrently, the directive `server_tokens` defaults to `on`. From a security perspective, this exposes an additional attack surface, as this emits the build version.\n\nBy default the configuration should be as secure as possible. Using `off` as default value would help to make the default configuration more secure.\n\n### Additional context\n\nI'm fully aware that the server header in the open-source version also exposes the version. I would be useful to patch this behaviour at the same time.\n\nIf needed, I would provide the necessary patches to help bring this live as soon as possible\n",
      "solution": "Attitudes toward server version disclosure are not unequivocal. Today it's easier to run a scanner for all possible vulnerabilities for all kinds of servers, versions and backends hiding behind proxies. To be clear, you are not hiding by hiding the version of nginx.\nOn the other hand, the `Server` header allows public services to collect statistics about the products used on the Internet. This also allows you to track the version of your nginx if you have multiple copies running.\nIn addition, this directive can be changed to suit the needs. Therefore, I would say that this question is more relevant to the discussions instead of the issues.",
      "labels": [
        "feature"
      ],
      "created_at": "2025-04-28T09:34:45Z",
      "closed_at": "2025-05-01T12:10:05Z",
      "url": "https://github.com/nginx/nginx/issues/661",
      "comments_count": 1
    },
    {
      "tech": "nginx",
      "repo": "nginx/nginx",
      "issue_number": 290,
      "title": "Caching dynamically compressed files",
      "problem": "### Describe the feature you'd like to add to nginx\n\nStoring compressed files returned by on-the-fly brotli filter in cache path.\n\n### Describe the problem this feature solves\n\nThis is one of the remaining areas where nginx falls short and can\u2019t match Apache httpd\u2019s offering at present. Whereas Apache [caches and serves compressed files](https://serverfault.com/a/220418), nginx only saves the uncompressed response and requires on-the-fly compression _for each subsequent request_ (or else manual compression beforehand).\n\nThis also makes `SSL_sendfile()` much less effective, having to be disabled whenever such repeated compression happens.\n\n### Additional context\n\nPrevious request: https://forum.nginx.org/read.php?11,283440,283440\n",
      "solution": "> you can do this on one instance, using different listening ports to distinguish between layer 1 and layer 2.\n\nAh good point - didn't think about that.\n\nOther than overhead, it still has a few warts, for example, if you have a min_uses directive, the upstream nginx won't be able to act on it.  But it could suffice as a hacky workaround for some.\n\n---\n\n> > you can do this on one instance, using different listening ports to distinguish between layer 1 and layer 2.\n> \n> Ah good point - didn't think about that.\n> \n> Other than overhead, it still has a few warts, for example, if you have a min_uses directive, the upstream nginx won't be able to act on it. But it could suffice as a hacky workaround for some.\n\nThe two levels use different cache keys, so there will be no min_use problem\n\n---\n\n> Sorry, I still don't understand you. If you're caching in the upstream nginx, you're not caching the compressed response. The reason for two layers is that you need the first to do the compression so that the second can do the caching. Doing caching in the first layer defeats the purpose of having two layers.\n\nWhat I mean is, assuming you need two layers of cache, using different cache keys will not affect the min_uses count. If you only configure cache on the first layer, there will be no impact on the min_uses count at all. \n\nI am just explaining this issue: `it still has a few warts, for example, if you have a min_uses directive`\n\nSo far, apart from the extra overhead, no other problems have been found. Or the problems found can be solved.",
      "labels": [
        "feature"
      ],
      "created_at": "2024-10-31T06:36:10Z",
      "closed_at": "2024-11-02T00:10:11Z",
      "url": "https://github.com/nginx/nginx/issues/290",
      "comments_count": 11
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 8708,
      "title": "[Bug]: DNSEndpoint recordType is incorrect for dual-stack services",
      "problem": "### Version\n\nedge\n\n### What Kubernetes platforms are you running on?\n\nAKS Azure\n\n### Steps to reproduce\n\nWhen using a dual-stack LoadBalancer Service, the NGINX Ingress Controller generates an invalid DNSEndpoint.\n\nInstead of creating separate DNS records for IPv4 (A) and IPv6 (AAAA), a single A record is created with both IPv4 and IPv6 addresses as targets.\n\nActual behavior:\n```\napiVersion: externaldns.nginx.org/v1\nkind: DNSEndpoint\nmetadata:\n  name: podinfo.example.com\n  namespace: podinfo\nspec:\n  endpoints:\n    - dnsName: podinfo.example.com\n      recordType: A\n      targets:\n        - 127.0.0.1\n        - ::1\n```\n\nExpected behavior:\n```\napiVersion: externaldns.nginx.org/v1\nkind: DNSEndpoint\nmetadata:\n  name: podinfo.example.com\n  namespace: podinfo\nspec:\n  endpoints:\n    - dnsName: podinfo.example.com\n      recordType: A\n      targets:\n        - 127.0.0.1\n    - dnsName: podinfo.example.com\n      recordType: AAAA\n      targets:\n        - ::1\n```\n\n",
      "solution": "Hey @bj0rn,\n\nThank you for the report. I managed to reproduce the issue, you're correct, it's a bug. We'll add it to planning.\n\n## The issue\n\nIn the [`getValidTargets`](https://github.com/nginx/kubernetes-ingress/blob/main/internal/externaldns/sync.go#L103) function the incoming slice of `ExternalEndpoints` entries are iterated over, their IP / Hostnames parsed, and the single `target` string slice gets populated, and the type of the last entry wins.\n\nThere's a test to make sure that's actually what happens: https://github.com/nginx/kubernetes-ingress/blob/main/internal/externaldns/sync_test.go#L64-L76\n\nBoth the function and the test are incorrect though, the desired functionality is that multiple types of targets returned grouped by their types (`A`, `AAAA`, `CNAME`)\n",
      "labels": [
        "bug",
        "backlog"
      ],
      "created_at": "2025-12-12T10:29:34Z",
      "closed_at": "2026-01-22T15:08:11Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/8708",
      "comments_count": 2
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 2698,
      "title": "Ingress returns 301 on ACME challenge",
      "problem": "**Describe the bug**\r\nThe ACME challenge for Cert-Manager keeps returning a 301 on which the cert cant be approved. I guess it is not really a bug though however i can not find anything related in docs nor online. The ingress should not redirect on an acme challenge if im correct.\r\n\r\n**To Reproduce**\r\nHard to say the exact steps. Below is at least the ingress that i have deployed\r\n\r\n```yaml\r\napiVersion: networking.k8s.io/v1\r\nkind: Ingress\r\nmetadata:\r\n  name: argocd-http\r\n  namespace: argocd\r\n  annotations:\r\n    cert-manager.io/cluster-issuer: letsencrypt-http\r\nspec:\r\n  ingressClassName: nginx\r\n  tls:\r\n    - hosts:\r\n        - argocd.<domain>.nl\r\n      secretName: argocd.<domain>.nl-tls\r\n  rules:\r\n    - host: argocd.<domain>.nl\r\n      http:\r\n        paths:\r\n          - path: /\r\n            pathType: Prefix\r\n            backend:\r\n              service:\r\n                name: argocd-server\r\n                port: \r\n                  name: https\r\n```\r\n\r\nThe actual challenge which keeps pending:\r\n\r\n```\r\nStatus:\r\n  Presented:   true\r\n  Processing:  true\r\n  Reason:      Waiting for HTTP-01 challenge propagation: failed to perform self check GET request 'http://argocd.<domain>.nl/.well-known/acme-challenge/KwUul2vb_qbb-f-': Get \"https://argocd.<domain>.nl:443/.well-known/acme-challenge/KwUul2vb_qbb-f-\": remote error: tls: unrecognized name\r\n  State:       pending\r\nEvents:        <none>\r\n```\r\n\r\n**Expected behavior**\r\nThe challenge to be handled properly\r\n\r\n**Your environment**\r\n* Version of the Ingress Controller - release version or a specific commit: 2.1.1\r\n* Version of Kubernetes: 1.21.9\r\n* Kubernetes platform (e.g. Mini-kube or GCP): AKS\r\n* Using NGINX or NGINX Plus: nginx\r\n\r\n**Additional context**\r\nAdd any other context about the problem here. Any log files you want to share.\r\n```\r\n    spec:\r\n      containers:\r\n        - name: nginx-ingress\r\n          image: nginx/nginx-ingress:2.1.1\r\n          args:\r\n            - '-enable-tls-passthrough'\r\n            - '-enable-custom-resources'\r\n            - '-nginx-configmaps=$(POD_NAMESPACE)/nginx-config'\r\n            - '-default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret'\r\n            - '-enable-snippets'\r\n            - '-report-ingress-status'\r\n            - '-external-service=nginx-ingress'\r\n```",
      "solution": "I solved it by using `acme.cert-manager.io/http01-edit-in-place: \"true\"` on my ingess\n\n---\n\nadding `ingress.kubernetes.io/ssl-redirect: \"false\"` to `argocd-http-master` does not seem to be a solution, right?\r\nI do not want to disable the ssl redirect for all minions. \r\nit would be better i can set there only for the minion that the cert manager creates\r\n\n\n---\n\nNote that this solution will not work with `VirtualServer` resources.",
      "labels": [
        "question"
      ],
      "created_at": "2022-05-18T18:23:00Z",
      "closed_at": "2022-07-20T12:15:08Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/2698",
      "comments_count": 7
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 8529,
      "title": "[Bug]: Ingress Controller is not able to restart correctly after OOM",
      "problem": "### Version\n\n5.2.1\n\n### What Kubernetes platforms are you running on?\n\nEKS Amazon\n\n### Steps to reproduce\n\n1. Deploy an ingress controller with a lower memory request (just for test, so it will be easier to observe) like\nRequests: 200Mi, Limits: 512Mi. or so.\n2. Generate some traffic, wait for the OOM event, and we want the container to be restarted by Kubernetes.\n3. Observe nginx logs:\n\nI20251112 09:42:27.813418   1 flags.go:287] Starting with flags: [\"-nginx-plus=false\" \"-nginx-reload-timeout=60000\" \"-enable-app-protect=false\" \"-enable-app-protect-dos=false\" \"-nginx-configmaps=nginx/nginx-lb-nginx-ingress\" \"-ingress-class=nginx\" \"-health-status=true\" \"-health-status-uri=/nginx-health\" \"-nginx-debug=false\" \"-log-level=info\" \"-log-format=glog\" \"-nginx-status=true\" \"-nginx-status-port=8080\" \"-nginx-status-allow-cidrs=127.0.0.1\" \"-report-ingress-status\" \"-enable-leader-election=true\" \"-leader-election-lock-name=nginx-lb-nginx-ingress-leader-election\" \"-enable-prometheus-metrics=true\" \"-prometheus-metrics-listen-port=9113\" \"-prometheus-tls-secret=\" \"-enable-service-insight=false\" \"-service-insight-listen-port=9114\" \"-service-insight-tls-secret=\" \"-enable-custom-resources=true\" \"-enable-snippets=true\" \"-disable-ipv6=true\" \"-enable-tls-passthrough=false\" \"-enable-cert-manager=false\" \"-enable-oidc=false\" \"-enable-external-dns=true\" \"-default-http-listener-port=80\" \"-default-https-listener-port=443\" \"-ready-status=true\" \"-ready-status-port=8081\" \"-enable-latency-metrics=true\" \"-ssl-dynamic-reload=true\" \"-enable-telemetry-reporting=false\" \"-weight-changes-dynamic-reload=false\"]\n[[[ some more lines here ]]]\n2025/11/12 09:42:27 [notice] 21#21: js vm init njs: 0000556A38EE7780\n2025/11/12 09:42:27 [emerg] 21#21: bind() to unix:/var/lib/nginx/nginx-status.sock failed (98: Address already in use)\n2025/11/12 09:42:27 [emerg] 21#21: bind() to unix:/var/lib/nginx/nginx-config-version.sock failed (98: Address already in use)\n2025/11/12 09:42:27 [emerg] 21#21: bind() to unix:/var/lib/nginx/nginx-502-server.sock failed (98: Address already in use)\n2025/11/12 09:42:27 [emerg] 21#21: bind() to unix:/var/lib/nginx/nginx-418-server.sock failed (98: Address already in use)\n2025/11/12 09:42:27 [notice] 21#21: try again to bind() after 500ms\n\n\nNginx can't bind to a port unless the pod is removed and recreated; after that, it works fine.\n\nThis issue definitely didn't start with this version. We've been seeing it for some time. I might be mistaken, but I believe it began after upgrading the Helm chart to version 2.0+.\n\nWhat is worth to mention is that we are using linkerd, so we have a native sidecar running the whole time with linkerd's proxy.",
      "solution": "hey @glieske, this looks similar to https://github.com/nginx/kubernetes-ingress/issues/7622, which will be fixed in next release (v5.3.0)",
      "labels": [
        "bug",
        "waiting for response"
      ],
      "created_at": "2025-11-12T09:48:11Z",
      "closed_at": "2026-01-12T16:29:03Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/8529",
      "comments_count": 5
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 8521,
      "title": "Permit Source configuration for weighted routing in VirtualServer",
      "problem": "**Is your feature request related to a problem? Please describe.**\n\nHi! my team is looking to enable traffic splitting for our NGINX gateway using Virtualserver/VSR. One thing that we are currently interested in is configuring a custom key for the split_client hash to enable weighted routing based on our a custom string/key. \n\nOne such as a case is when we have a inc header \"http_call_id\", for which we would like to route calls to the same upstream, but still split our calls (uniquely identified by call_id) between several upstreams for A/B testing, Blue/green, and Canary testing.\n\n**Describe the solution you'd like**\n\nOne solution could be to allow this field to be configured under each route - for example something like below:\n\n```\n\tpath: /tea\n\tsplitsKey: \"$http_call_id\"\n\tsplits:\n\t- weight: 80\n\taction:\n\t\tpass: coffee-v1\n\t- weight: 20\n\taction:\n\t\tpass: coffee-v2\n```\n\n**Describe alternatives you've considered**\nAlternatives such as http-snippets works - but its impractical when you want to avoid enabling snippets in general since we have a lot of partner teams using our gateway, and snippets tend to break the overall IC configuration generation.\n\n**Additional context**\nI would be happy to make a contribution myself, but I don't want to waste time implementing something if you guys think its a waste.",
      "solution": "Thanks for opening the issue @eanveden. I'm trying to understand your proposal for the traffic-splitting feature.\n\nI deployed the [traffic-splitting example](https://github.com/nginx/kubernetes-ingress/tree/v5.2.1/examples/custom-resources/traffic-splitting), and this is the generated NGINX conf:\n\n```\nsplit_clients $request_id $vs_default_cafe_splits_0 {\n    90% /internal_location_splits_0_split_0;\n    10% /internal_location_splits_0_split_1;\n}\n```\nHaving looked at our current implementation, I see that the string`$request_id` is hardcoded. Did you mean that you would like it to be customisable?\n\nYou mentioned that using snippets worked. Could you share your snippets so we can better understand the enhancement you would like to see in the traffic splitting feature?\n\n---\n\nHi @eanveden, \n\nAre you happy with the solution provided by @haywoodsh ? ",
      "labels": [
        "proposal",
        "waiting for response"
      ],
      "created_at": "2025-11-10T20:44:45Z",
      "closed_at": "2026-01-12T16:27:15Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/8521",
      "comments_count": 6
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 7332,
      "title": "[Bug]: client-max-body-size on (upstream) VirtualServer is never considered",
      "problem": "### Version\n\nedge\n\n### What Kubernetes platforms are you running on?\n\nOther\n\n### Steps to reproduce\n\n# Bug\n\nWhen applying the `client-max-body-size` option on the upstream, via VirtualServer it ultimately has no effect. This is not say that it's not applied to the conf because it is, this is more to say that it doesn't work - this seems to be that the client max body size is then set on the sub location resource and not the originally matching location.\n\nThis may be either intended behaviour or a bug, but since it's nginx that's interpreting that conf, that would be a bug in nginx, however I think there would be ways to workaround this in the operator.\n\n# Reproduction and testing\n\nI've included some examples here but, they're really to show what's going on more than to be things you can just take and run (as the upstream itself is not provided).\n\n```yaml\napiVersion: k8s.nginx.org/v1\nkind: VirtualServer\nmetadata:\n  name: vs-test\nspec:\n  ingressClassName: nginx  \n  upstreams:\n  - name: max-body-test\n    service: max-body-test-service\n    port: 3000\n    keepalive: 32\n    client-max-body-size: 15m\n  routes:\n  - path: ~* '^/max-body-test$'\n    matches:\n      - conditions:\n        - variable: $request_method\n          value: POST\n        action:\n          proxy:\n            upstream: max-body-test\n```\n\nOnce this is deployed, this becomes the following nginx conf; (this is a snippet as the actual conf is quite a bit longer but the important parts are included);\n\n```\nupstream vs-test-max-body-size {\n    zone vs-test-max-body-size256k;\n    random two least_conn;\n    server 10.244.221.35:3000 max_fails=1 fail_timeout=10s max_conns=0;\n    keepalive 32;\n}\n\nmap $request_method $vs_test_matches_0_match_0_cond_0 {\n    \"POST\" 1;\n    default 0;\n}\nmap $vs_test_matches_0_match_0_cond_0 $vs_test_matches_0 {\n    ...\n}\nserver {\n    listen 80;\n    ...\n    location ~* '^/max-body-size$' {\n        rewrite ^ $vs_test_matches_0 last;\n    }\n\n    location @return_0 {\n        ...\n    }\n\n    location /internal_location_matches_0_match_0 {\n        set $service \"max-body-size-service\";\n        internal;\n\n        ...\n        client_max_body_size 15m;\n        ...\n    }\n    location /internal_location_matches_0_default {\n        ...\n    }\n}\n```\n\nIn the above example, we can see that our `client_max_body_size 15m;` was added to `location /internal_location_matches_0_match_0 {` as that's the match that proxies to our upstream. However, the default of 1m is still used so we're unable to send anything larger still.\n\nIf we modify this slightly to change the originally matching location block;\n\n```\nlocation ~* '^/max-body-size$' {\n        rewrite ^ $vs_test_matches_0 last;\n    }\n```\n\nAnd add the max body size here;\n\n```\nlocation ~* '^/max-body-size$' {\n        client_max_body_size 15m;\n        rewrite ^ $vs_test_matches_0 last;\n    }\n```\n\nAnd we copy this into nginx;\n\n```bash\nkubectl cp ./vs-test.conf nginx-ingress/nginx-ingress-controller-pod:/etc/nginx/conf.d/vs-test.conf\nkubectl exec -it -n nginx-ingress nginx-ingress-controller-pod -- sh -c \"nginx -s reload\"\n```\n\nWe're now able to send bodies that are larger than 1m and upto our defined 15m.\n\nI have no doubts that sever and http snippets would also fix this but, this would then apply to all defined endpoints and upstreams within that scope which may not be what we want.\n\n# Possible Solutions\n\n1. We can work around this potential bug by adding this configuration into the originally matching location block. It would be a case of adding this configuration option to the `s.InternalRedirectLocations` struct or if the option is within the loop of another struct that could be used, the template for this here;\n\nhttps://github.com/nginx/kubernetes-ingress/blob/278f84154b694f27f1c61ebb302c43755d6105ff/internal/configs/version2/nginx.virtualserver.tmpl#L190\n\n2. This is reported as a nginx bug and ultimately is fixed there without any changes to this operator.\n\n# Related issues\n\nhttps://github.com/nginx/kubernetes-ingress/issues/5859\nhttps://github.com/nginx/kubernetes-ingress/issues/5317",
      "solution": "@jjngx @shaun-nx Is it possible to get this reopened? Looks like it was closed automatically, I'm happy to contribute on a solution but I'm unsure which direction we would go in with regards to this?",
      "labels": [
        "bug",
        "stale",
        "backlog",
        "ready for refinement"
      ],
      "created_at": "2025-02-08T17:42:03Z",
      "closed_at": "2026-01-01T02:13:49Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/7332",
      "comments_count": 9
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 8709,
      "title": "[Bug]: ipFamilies list may not be rendered correctly in Service template",
      "problem": "### Version\n\nedge\n\n### What Kubernetes platforms are you running on?\n\nKind\n\n### Steps to reproduce\n\nipFamilies is defined as a list in the values file, but it appears that the Helm chart currently only supports rendering a single value. It is defined here https://github.com/nginx/kubernetes-ingress/blob/main/charts/nginx-ingress/values.yaml#L502 and used here https://github.com/nginx/kubernetes-ingress/blob/main/charts/nginx-ingress/templates/controller-service.yaml#L42C3-L42C58. \n\nIn the Service template, ipFamilies seems to be rendered as a scalar rather than a list, which may prevent proper support for multiple IP families.\n\nI would have expected ipFamilies to be rendered as a list, for example:\n\n```\nipFamilies:\n{{- range $ipFamily := .Values.controller.service.ipFamilies }}\n  - {{ $ipFamily }}\n{{- end }}\n```",
      "solution": "Hi @bj0rn!\n\n~I tried to reproduce this issue, but couldn't. Or rather my testing resulted in the expected output.~ I think I figured out what the issue is. Here's what I did on current main (26e208ba):\n\n1. adjusted the `values.yaml` file to have the following `ipFamilies` entry:\n   \n   ```yaml\n   ...\n   \n    ## List of IP families assigned to this service.\n    ## Valid values: IPv4, IPv6\n    ipFamilies:\n      - IPv6\n      - IPv4\n\n    httpPort:\n    ...\n   ```\n2. in the `charts` directory I ran the following helm command (I'm using version 4.0.2):\n   ```\n   $ helm template nginx-ingress --values nginx-ingress/values.yaml --show-only templates/controller-service.yaml\n   ```shell\n   \n   This renders only the `controller-service.yaml` template using values from the `values.yaml` file\n3. I got the following output:\n\n   ```yaml\n   level=INFO msg=\"found symbolic link in path. Contents of linked file included and used\" path=/Users/g.javorszky/Projects/NIC/kubernetes-ingress/charts/nginx-ingress/crds resolved=/Users/g.javorszky/Projects/NIC/kubernetes-ingress/config/crd/bases\n   ---\n   # Source: nginx-ingress/templates/controller-service.yaml\n   apiVersion: v1\n   kind: Service\n   metadata:\n     name: release-name-nginx-ingress-controller\n     namespace: default\n     labels:\n       helm.sh/chart: nginx-ingress-2.5.0 # this version is due to main already preparing for next minor release\n       app.kubernetes.io/name: nginx-ingress\n       app.kubernetes.io/instance: release-name\n       app.kubernetes.io/version: \"5.4.0\" # this version is due to main already preparing for next minor release\n       app.kubernetes.io/managed-by: Helm\n   spec:\n     externalTrafficPolicy: Local\n     type: LoadBalancer\n     ipFamilies: [IPv6 IPv4] # <- this would be the expected array value if it had a , (comma) between them, but it does not...\n     ports:\n     - port: 80\n       targetPort: 80\n       protocol: TCP\n       name: http\n       nodePort:\n     - port: 443\n       targetPort: 443\n       protocol: TCP\n       name: https\n       nodePort:\n     selector:\n       app.kubernetes.io/name: nginx-ingress\n       app.kubernetes.io/instance: release-name\n   ```\n\n~Can you double check whether you get the same output?~\n\nWe'll add this to our backlog as well soon.\n\nUPDATE: confirmed using `yq`, the value of the `ipFamilies` is a single string that is `IPv4 IPv6`...",
      "labels": [
        "bug",
        "backlog"
      ],
      "created_at": "2025-12-12T12:06:14Z",
      "closed_at": "2025-12-16T11:26:58Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/8709",
      "comments_count": 3
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 8593,
      "title": "[Bug]: slow-start config defined in virtualserverroute seems not work",
      "problem": "### Version\n\nedge\n\n### What Kubernetes platforms are you running on?\n\nKind\n\n### Steps to reproduce\n\nI tried to set up slow start in my virtualserverroute defination but it seems the nginx does not respect it:\n```\n  upstreams:\n    - name: my-service\n      service: my-service\n      port: 9000\n      keepalive: 64\n      connect-timeout: 10s\n      read-timeout: 30s\n      send-timeout: 30s\n      next-upstream-tries: 1\n      fail-timeout: 15s\n      max-fails: 1\n      lb-method: least_conn\n      slow-start: 45s\n      healthCheck:\n        enable: true\n        path: /alive\n        port: 8558\n        interval: 5s\n        passes: 1\n        fails: 3\n        mandatory: true\n```\n\nI tried to use lb-method:round_robin and removed healthCheck and set mandatory to false, but it all did not work\n\nclaude said that: \n> The actual problem is that slow-start requires a server state transition (down\u2192up), but new endpoints appear already \"up\". Whether the controller uses API calls or config reloads, the result is the same - no transition, no slow-start.\n\n\nis that correct?",
      "solution": "The nginx.conf and the virtual server fies look fine to me.. \n\n\nTheres a few things I notice in the logs, one is \n```\nW20251201 06:28:48.272890   1 configurator.go:1155] Couldn't update the endpoints via the API: couldn't update the endpoints for vs_staging-data-eng_ingestion-v3-analytics-tubitv-com_vsr_staging-data-eng_ingestion-v3-analytics-tubitv-com_ingestion-v3-analytics: error verifying config version: API returned non-success status: 503; reloading configuration instead\n```\n\nSince you are running NGINX Plus, I would recommend you reach out to F5 support to assist with this further. \n\n\nanother is a spam of \n```\nW20251201 10:09:11.591344   1 nginx_plus.go:831] wrong number of labels for upstream peer, empty labels will be used instead\n```\n\nWe've had this pop up before, the solution can be found here.\nhttps://github.com/nginx/kubernetes-ingress/issues/7425\n",
      "labels": [
        "bug"
      ],
      "created_at": "2025-12-01T08:10:37Z",
      "closed_at": "2025-12-05T06:40:17Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/8593",
      "comments_count": 19
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 8290,
      "title": "[Bug]: Upstream conf resolves to 127.0.0.1:8181 when an additional port is added to an upstream Deploy & Service",
      "problem": "### Version\n\n4.0.1\n\n### What Kubernetes platforms are you running on?\n\nAKS Azure\n\n### Steps to reproduce\n\nI've spotted that if you modify an existing deployment in a certain way _(more on this below)_ then sometimes Nginx fails to resolve the correct IP address for the destination and so the conf file contains something like this at the top:\n\n```\nupstream example-minion-example.com-example.com-example-443 {zone example-minion-example.com-example.com-example-443 256k;\n        random two least_conn;\n        server 127.0.0.1:8181 max_fails=1 fail_timeout=10s max_conns=0;\n}\n\n# all content below removed from the example but also completely fine\n```\n\nThe problem is this bit: ` server 127.0.0.1:8181`\n\nIn my setup and in the environment where I'm testing this I have 2x replicas of the Nginx Ingress Controller pods.\nOn all occasions where I've replicated this issue, only 1 pod retains this invalid conf. During a config reload I can observe both pods initially with ` server 127.0.0.1:8181` but shortly after one of the Pods resolves to the correct address. i.e.\n\n```\nupstream example-minion-example.com-example.com-example-443 {zone example-minion-example.com-example.com-example-443 256k;\n        random two least_conn;\n        server 10.156.67.181:5001 max_fails=1 fail_timeout=10s max_conns=0;\n}\n```\n\nIn order to replicate this, I deploy a K8s Deployment, Service, and Ingress resource. The deployment spec configures 1 containerPort, and the Service also has 1 port (I start with 80), and the Ingress resource is configured to route all traffic to the Service on port 80.\n\nEverything is fine at this point.\n\nThen to trigger the issue, redeploy the aforementioned resources but the Deployment now configures 2x containerPorts, and the Service has 2 ports (80 + 443 - each mapping to one of the container ports), and the Ingress resource is updated to route all traffic to the Service on port 443.\n\nOne of the two Nginx Pods will handle this config just fine and will continue to route to the workload as expected; but the other will fail and start issuing 502 responses. Upon doing so the following logs will be presented (with debug loglevel set):\n\n```\n{\"time\":\"2025-09-23T10:24:20.59820435Z\",\"level\":\"DEBUG\",\"source\":{\"file\":\"verify.go\",\"line\":90},\"msg\":\"success, version 27 ensured. took: 128.491111ms\"}\n2025/09/23 10:24:20 [notice] 18#18: signal 17 (SIGCHLD) received from 204\n2025/09/23 10:24:20 [notice] 18#18: worker process 204 exited with code 0\n2025/09/23 10:24:20 [notice] 18#18: worker process 206 exited with code 0\n2025/09/23 10:24:20 [notice] 18#18: signal 29 (SIGIO) received\n2025/09/23 10:24:20 [notice] 18#18: signal 17 (SIGCHLD) received from 207\n2025/09/23 10:24:20 [notice] 18#18: worker process 207 exited with code 0\n2025/09/23 10:24:20 [notice] 18#18: signal 29 (SIGIO) received\n2025/09/23 10:24:20 [notice] 18#18: signal 17 (SIGCHLD) received from 205\n2025/09/23 10:24:20 [notice] 18#18: worker process 205 exited with code 0\n2025/09/23 10:24:20 [notice] 18#18: signal 29 (SIGIO) received\n{\"time\":\"2025-09-23T10:24:20.846495916Z\",\"level\":\"DEBUG\",\"source\":{\"file\":\"endpoint_slice.go\",\"line\":33},\"msg\":\"Removing EndpointSlice: example-pod-km6xm\"}\n{\"time\":\"2025-09-23T10:24:20.846534116Z\",\"level\":\"DEBUG\",\"source\":{\"file\":\"task_queue.go\",\"line\":65},\"msg\":\"Adding an element with a key: example-ns/example-pod-km6xm\"}\n{\"time\":\"2025-09-23T10:24:20.846551817Z\",\"level\":\"DEBUG\",\"source\":{\"file\":\"task_queue.go\",\"line\":98},\"msg\":\"Syncing example-ns/example-pod-km6xm\"}\n{\"time\":\"2025-09-23T10:24:20.846561717Z\",\"level\":\"DEBUG\",\"source\":{\"file\":\"task_queue.go\",\"line\":77},\"msg\":\"The queue has 0 element(s)\"}\n{\"time\":\"2025-09-23T10:24:20.846568217Z\",\"level\":\"DEBUG\",\"source\":{\"file\":\"controller.go\",\"line\":1019},\"msg\":\"Syncing example-ns/example-pod-km6xm\"}\n2025/09/23 10:25:58 [error] 211#211: *403 connect() failed (111: Connection refused) while connecting to upstream, client: 10.157.116.134, server: example.com, request: \"GET /api/v1/example-path HTTP/1.1\", upstream: \"https://127.0.0.1:8181/api/v1/example-path\", host: \"example.com\"\n10.157.116.134 - - [23/Sep/2025:10:25:58 +0000] \"GET /api/v1/example-path HTTP/1.1\" 502 559 \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36\" \"xxx.xxx.xxx.xxx,yyy.yyy.yyy.yyy:37220,zzz.zzz.zzz.zzz\"\n\n_# Subsequent log lines then repeatedly show the 502 and connect refused entries as more requests come in_\n```\n \nIn order to resolve the issue I simply need to trigger a config reload. I've demonstrated this works by either restarting the affected Nginx Pod; restarting the upstream workload; redeploying the upstream workload.\n\nNote that I've so far been unable to replicate this issue when applying subsequent deployments. It seems to only affect the very first deployment which adds the additional port to the Service (or maybe it's the flip from port 80 to port 443 in the Ingress resource that might specifically cause it).\n",
      "solution": "HI @AlexFenlon \nThank you for your attention.\n\nIt doesn't happen on 'every' first deployment - but to be clear, first deployment here I mean the first deployment which adds the extra ports and changes the port which the existing ingress resource maps to - so more of an update than a first deploy.\n\nToday for example we took a change through a couple of environments. I was on hand to restart any instances which might be affected as we were expecting the issue to manifest itself. In the first cluster we deployed the change into, both instances of the ingress controller correctly populated the correct upstream IP and with the revised container port. But on the subsequent cluster which we deployed into ~5 min later the issue repeated itself. One instance had a correct conf file, but the other had `127.0.0.1:8181` as the upstream IP. \n\nI would say that I have about a 80% success rate in replicating the issue when bouncing between 2x deployments with the different port configurations. \n\nI didn't mention it as I didn't think it would be relevant, but I'll add that this change also included some annotations on the ingress resource to enforce the use of TLS with the upstream.\n\nIt might help if I show you the ingress resource before and after. _Service names redacted _ :\n\nBefore / From:\n```\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations\n    nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"\n    nginx.org/mergeable-ingress-type: minion\n  name: minion-example.sub.example.com\n  namespace: example\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: example.sub.example.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: example-app\n            port:\n              number: 80\n        path: /\n        pathType: Prefix\n```\n\nAfter / To:\n```\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"\n    nginx.org/location-snippets: |\n      proxy_ssl_verify on;\n      proxy_ssl_verify_depth 3;\n      proxy_ssl_name example-app.example.svc.cluster.local;\n      proxy_ssl_trusted_certificate /etc/ssl/certs/issuer.crt;\n    nginx.org/mergeable-ingress-type: minion\n    nginx.org/ssl-services: example-app\n  name: minion-example.sub.example.com\n  namespace: example\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: example.sub.example.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: example-app\n            port:\n              number: 443\n        path: /\n        pathType: Prefix\n\n```\n\nI won't present the manifests, but to recap that in addition the Service went from 1 port (80) to 2 ports (80 + 443), and the pod spec on the deployment was also updated with an additional port.\n\nSome thoughts from myself and a colleague:\n\n1. Note we're using Helm for the deployment. Helm will deploy a Service before a Deployment. If Kubernetes creates the Service fractionally before the Deployment, then there is a small window during which the new Service will have a port (the new one) which maps to a non-existent container port since the Deployment with the new extra port is not yet in place. Maybe there is something in this which causes the issue.\n\n2. A colleague who has previously contributed to this project wonders if it is an issue that only occurs when you have a HA control-plane. We didn't get chance to discuss much but I guess he was getting at 2x different API Server instances handling requests possible contributing to the issue. We're using Azure and they will deploy the control plane with HA. Could be worth you trying to replicate in that setup if you can and didn't already do so.\n\n\n\n\n\n\n---\n\n> Hey [@MarkTopping](https://github.com/MarkTopping), i've tried similar example but haven't been able to reproduce this likely race condition, can you please confirm if:\n> \n> 1. You're using helm chart for you application and Ingress deployments,\n> 2. config of your cluster (platform and number of nodes etc)\n\nSorry for the delayed response.\n\nAh, that's a shame. Given how often I saw it I'd rather hoped it would be pretty replicatable for you. \n\nFor your questions:\n\n1. Yes we are using Helm for those.\n\n2. Not too sure how much detail you're looking for here. As mentioned it was within Azure AKS clusters that the issue was spotted. Roughly 15 Nodes across multiple Nodepools. ~750 ingress resources (as per code pasted previously you can see I'm using mergeable ingress types - so those 750 ingress resources equate to ~375 hosts) \n\n---\n\nHi @MarkTopping I have been looking into this issue.  So far I have not been able to replicate the problem you have seen, I have not yet tried the AKS HA setup you mentioned.\n\nOne thing to highlight (if you're not already aware), 127.0.0.1:8181 is our default upstream if the `Service` mentioned in the backend does not have any `Endpoints` associated with it.\n\n1. I see from the debug logs you provided, that just prior to getting the 502's a watched EndpointSlice was removed.  This could well be the reason the service no longer had any `Endpoints` available and fell back to the default upstream.  I would be interested to see how these logs differ between pods, where one pod has a working config and the other doesn't.\n2. Whilst I cannot rule out the TLS annotations being relevant (due to not being able to replicate the issue), I do not see how it might affect the Kubernetes events triggering this problem.\n3. All NIC pods watch for and process the same events from the Kubernetes API, if one pod is getting a different view of events due to the HA scenario you mentioned, it could explain things.  I can see that AKS has a few different HA options.  Can you provide a link to the AKS HA docs of the service you are using so I can do some more research.\n\nThanks",
      "labels": [
        "bug",
        "waiting for response"
      ],
      "created_at": "2025-09-23T22:29:18Z",
      "closed_at": "2025-12-02T14:43:36Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/8290",
      "comments_count": 10
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 4687,
      "title": "When using Keda as Autoscaler - Deployments Cause All but 1 Pod to Terminate",
      "problem": "**Describe the bug**\r\nWe are utilizing Keda as an autoscaler to be based upon current connections. However, when a deployment occurs, all but 1 NGINX Ingress pods are terminated due to `replicas: <value>` being included within the deployment manifest. This is documented at https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#migrating-deployments-and-statefulsets-to-horizontal-autoscaling\r\n\r\n**To Reproduce**\r\nYou can follow the steps listed in this blog post: https://www.nginx.com/blog/microservices-march-reduce-kubernetes-latency-with-autoscaling/\r\n\r\n**Expected behavior**\r\nWhen doing a deployment, it should respect the rolling strategy as defined, and not include `replicas: <value>`.\r\n\r\n**Your environment**\r\n\r\n- Version of the Ingress Controller - 3.3.1\r\n- Version of Kubernetes - 1.27\r\n- Kubernetes platform (e.g. Mini-kube or GCP) - Azure Kubernetes \r\n\r\n**Additional context**\r\nIf there was a way to enable autoscaling without including the built-in hpa\r\n",
      "solution": "Hi @matthawley \r\n\r\nI'm working on understanding the problem you're facing. Are you encountering this issue while deploying the ingress controller utilizing Helm, and subsequently finding that Helm doesn't generate the manifest required for KEDA?\r\n\r\nAs a workaround, have you tried deploying the ingress controller directly using the provided manifests? You can find detailed instructions in this documentation: [Installation with Manifests | NGINX Ingress Controller](https://docs.nginx.com/nginx-ingress-controller/installation/installing-nic/installation-with-manifests/)\r\n\r\nWhen the autoscaling feature is disabled in Helm (which is the default configuration), the `spec:replicas` field would be present in the deployment manifest and defaults to 1. This scenario can lead to the termination of the pods. Is that what you observed?\r\nhttps://github.com/nginxinc/kubernetes-ingress/blob/v3.3.1/deployments/helm-chart/templates/controller-deployment.yaml#L13-L15\r\n\r\nIf this mirrors your scenario, it seems we might need to adjust our Helm chart to ensure it is fully compatible with KEDA.\n\n---\n\nHi @haywoodsh - yes, when we upgrade from helm is when we experience this issue. Helm does generate the right manifests, except that because we have autoscaling disabled (because we're using Keda), that `replicas: 1` in the deployment is causing the issue described in the Kubernetes documentation (all but 1 pods are terminated). At this point, our workaround has been to fork the chart and remove the hpa from it so that we can enable \"autoscaling\" in hopes that the stable chart can support this scenario soon.\r\n\r\n",
      "labels": [
        "backlog"
      ],
      "created_at": "2023-11-20T23:51:10Z",
      "closed_at": "2025-10-29T16:12:24Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/4687",
      "comments_count": 4
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 7428,
      "title": "More context in OIDC logs",
      "problem": "**Is your feature request related to a problem? Please describe.**\n\nThe problem is log messages like this:\n\n```\n2024/12/11  13:44:54 [error] 86#86: *3762 js: OIDC ID Token validation error: nonce\n from token (3-wSxtTGFsip_PQNftt4S5ty-vHIkiG0PsfdMEqyu11ds) does not match\n client () \n```\n\n**Describe the solution you'd like**\n\nIt would be nice if at least the name of the VirtualServer object, or URL, was included. It would also be nice if this logging was configurable, in JSON format. Missing context is the most required feature, though. \n\nIt would be nice if the context was not limited to the OIDC module, but was added for all modules/components. For example the JWT policy.\n\n**Describe alternatives you've considered**\n\nNone, very open to suggestions. \n\n**Additional context**\n\nUsing Nginx Ingress Controller 4.0.1 (the example is a bit older, though).\n",
      "solution": "This can be resolved by merging https://github.com/nginxinc/nginx-openid-connect/pull/111 and updating the implementation on NIC.\n\n---\n\nFixed by [#8207](https://github.com/nginx/kubernetes-ingress/issues/8207)",
      "labels": [
        "proposal",
        "ready for refinement",
        "area/security"
      ],
      "created_at": "2025-02-28T11:13:45Z",
      "closed_at": "2025-10-29T11:07:43Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/7428",
      "comments_count": 6
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 8069,
      "title": "[Bug]: JWT validation does not work with requests larger than 1MB using JWKSUri as source for keys - POC",
      "problem": "### Version\n\nedge\n\n### What Kubernetes platforms are you running on?\n\nAKS Azure\n\n### Steps to reproduce\n\nWhen using JWKs URI as source for JWKs a subrequest is used. The request limit for subrequests is the default 1MB and the client gets a 413 Payload to large.\n\nThe configuration is similar to:\n\n```\njwt:\n  realm: Realm\n  token: $http_token\n  jwksURI: https://location_of_jwt\n  keyCache: 1h\n```\n\nThis is a similar bug  as https://github.com/nginx/kubernetes-ingress/issues/7876",
      "solution": "@vepatel correct\n\n```\n upstreams:\n  - client-max-body-size: 0m\n     name: sizetest-upstream\n     port: 80\n     use-cluster-ip: true\n```\n\nSmaller requests (under 1MB) works fine. It should be quite easy to reproduce.\n\nThe genereated code for fetching the JWKs is:\n\n```\n    location = /_jwks_uri_server_XXXX {\n        internal;\n        proxy_method GET;\n        proxy_set_header Content-Length \"\";\n        proxy_cache jwks_uri_XXXX;\n        proxy_cache_valid 200 12h;\n        proxy_set_header Host XXXX\n        set $idp_backend XXXX;\n        proxy_pass https://$idp_backend/jwks;\n    }\n```\n\nI would also expect  the following directives to be set:\n\n```\nproxy_pass_request_headers off;\nproxy_pass_request_body off;\n```\n\nAs well SNI validation, but from what I can see, this will be fixed in https://github.com/nginx/kubernetes-ingress/pull/7993\n\n\n\n---\n\n@nixx now that #8098 is resolved, do you see this as resolved too?",
      "labels": [
        "bug",
        "backlog",
        "refined"
      ],
      "created_at": "2025-07-23T11:05:22Z",
      "closed_at": "2025-10-21T08:30:30Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/8069",
      "comments_count": 8
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 6752,
      "title": "[Bug]: bind() to unix:/var/lib/nginx/nginx-status.sock failed (98: Address already in use)",
      "problem": "### Version\n\n3.6.2\n\n### What Kubernetes platforms are you running on?\n\nEKS Amazon\n\n### Steps to reproduce\n\nk8s EKS version: 1.31\n\nDescribe the bug:\nSometimes, the nginx-ingress-controller restarts the process without cleaning the socket files.\nAt first time we meet this problem during massive node restarting in the k8s cluster.\nThen it happens randomly on weekends.\n\nThe problem was noticed in version 3.6.2. Before we used app version 3.0.2 and never had this problem\n\nManual Pod deletion solves the problem, but it can happen again.\n\nHere is deployment yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    meta.helm.sh/release-name: nginx-inc-ingress-controller\n    meta.helm.sh/release-namespace: nginx-ingress\n  labels:\n    app: nginx-inc-ingress-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 3.6.1\n    helm.sh/chart: nginx-ingress-1.3.1\n  name: nginx-inc-ingress-controller\n  namespace: nginx-ingress\nspec:\n  progressDeadlineSeconds: 600\n  replicas: 3\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: nginx-inc-ingress-controller\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 25%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        logs.improvado.io/app: nginx-ingress\n        logs.improvado.io/format: json\n        logs.improvado.io/ingress-class: nginx-stable\n        prometheus.io/port: \"9113\"\n        prometheus.io/scheme: http\n        prometheus.io/scrape: \"true\"\n      creationTimestamp: null\n      labels:\n        app: nginx-inc-ingress-controller\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app: nginx-inc-ingress-controller\n            topologyKey: kubernetes.io/hostname\n      automountServiceAccountToken: true\n      containers:\n      - args:\n        - -nginx-plus=false\n        - -nginx-reload-timeout=60000\n        - -enable-app-protect=false\n        - -enable-app-protect-dos=false\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-inc-ingress-controller\n        - -default-server-tls-secret=$(POD_NAMESPACE)/nginx-inc-ingress-controller-default-server-tls\n        - -ingress-class=nginx-stable\n        - -health-status=true\n        - -health-status-uri=/-/health/lb\n        - -nginx-debug=false\n        - -v=1\n        - -nginx-status=true\n        - -nginx-status-port=8080\n        - -nginx-status-allow-cidrs=127.0.0.1\n        - -report-ingress-status\n        - -enable-leader-election=true\n        - -leader-election-lock-name=nginx-inc-ingress-controller-leader\n        - -enable-prometheus-metrics=true\n        - -prometheus-metrics-listen-port=9113\n        - -prometheus-tls-secret=\n        - -enable-service-insight=false\n        - -service-insight-listen-port=9114\n        - -service-insight-tls-secret=\n        - -enable-custom-resources=true\n        - -enable-snippets=true\n        - -include-year=false\n        - -disable-ipv6=false\n        - -enable-tls-passthrough=false\n        - -enable-cert-manager=false\n        - -enable-oidc=false\n        - -enable-external-dns=false\n        - -default-http-listener-port=80\n        - -default-https-listener-port=443\n        - -ready-status=true\n        - -ready-status-port=8081\n        - -enable-latency-metrics=false\n        - -ssl-dynamic-reload=true\n        - -enable-telemetry-reporting=false\n        - -weight-changes-dynamic-reload=false\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        image: 627003544259.dkr.ecr.us-east-1.amazonaws.com/nginx-inc-ingress:master-3.6.2-2-1\n        imagePullPolicy: IfNotPresent\n        name: ingress-controller\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        - containerPort: 9113\n          name: prometheus\n          protocol: TCP\n        - containerPort: 8081\n          name: readiness-port\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /nginx-ready\n            port: readiness-port\n            scheme: HTTP\n          periodSeconds: 1\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: 1500m\n            memory: 1500Mi\n          requests:\n            cpu: 100m\n            memory: 1500Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 101\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n        volumeMounts:\n        - mountPath: /etc/nginx\n          name: nginx-etc\n        - mountPath: /var/cache/nginx\n          name: nginx-cache\n        - mountPath: /var/lib/nginx\n          name: nginx-lib\n        - mountPath: /var/log/nginx\n          name: nginx-log\n      dnsPolicy: ClusterFirst\n      initContainers:\n      - command:\n        - cp\n        - -vdR\n        - /etc/nginx/.\n        - /mnt/etc\n        image: 627003544259.dkr.ecr.us-east-1.amazonaws.com/nginx-inc-ingress:master-3.6.2-2-1\n        imagePullPolicy: IfNotPresent\n        name: init-ingress-controller\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 101\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n        volumeMounts:\n        - mountPath: /mnt/etc\n          name: nginx-etc\n      nodeSelector:\n        kubernetes.io/arch: amd64\n      priorityClassName: cluster-application-critical\n      restartPolicy: Always\n      schedulerName: default-scheduler\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      serviceAccount: nginx-inc-ingress-controller\n      serviceAccountName: nginx-inc-ingress-controller\n      terminationGracePeriodSeconds: 60\n      tolerations:\n      - effect: NoExecute\n        key: node.kubernetes.io/not-ready\n        operator: Exists\n        tolerationSeconds: 300\n      - effect: NoExecute\n        key: node.kubernetes.io/unreachable\n        operator: Exists\n        tolerationSeconds: 300\n      - effect: NoSchedule\n        key: node.kubernetes.io/memory-pressure\n        operator: Exists\n      topologySpreadConstraints:\n      - labelSelector:\n          matchLabels:\n            app: nginx-inc-ingress-controller\n        maxSkew: 1\n        topologyKey: kubernetes.io/hostname\n        whenUnsatisfiable: DoNotSchedule\n      volumes:\n      - emptyDir: {}\n        name: nginx-etc\n      - emptyDir: {}\n        name: nginx-cache\n      - emptyDir: {}\n        name: nginx-lib\n      - emptyDir: {}\n        name: nginx-log\n```\nLogs with error:\n```\n2024-11-02 10:02:58.543\t2024/11/02 06:02:56 [emerg] 18#18: bind() to unix:/var/lib/nginx/nginx-status.sock failed (98: Address already in use)\n2024-11-02 10:02:58.543\t2024/11/02 06:02:56 [emerg] 18#18: bind() to unix:/var/lib/nginx/nginx-config-version.sock failed (98: Address already in use)\n2024-11-02 10:02:58.543\t2024/11/02 06:02:56 [emerg] 18#18: bind() to unix:/var/lib/nginx/nginx-502-server.sock failed (98: Address already in use)\n2024-11-02 10:02:58.543\t2024/11/02 06:02:56 [emerg] 18#18: bind() to unix:/var/lib/nginx/nginx-418-server.sock failed (98: Address already in use)\n2024-11-02 10:02:58.543\t2024/11/02 06:02:56 [notice] 18#18: try again to bind() after 500ms\n2024-11-02 10:02:59.043\t2024/11/02 06:02:56 [emerg] 18#18: still could not bind()\n```\nHere are logs, containing 1 signal reconfiguring and then a crash loop with socket busy error\n[Explore-logs-2024-11-05 18_40_57.txt](https://github.com/user-attachments/files/17634341/Explore-logs-2024-11-05.18_40_57.txt)\n\n**Expected behavior**\nnginx-ingress controller pod is working.\n",
      "solution": "Hi @granescb,\n\nCan you give more details about the node restarts. Are the nodes on a scheduled restart?\n\nCan you try turn readOnlyRootFilesystem to false and let us know if this changes the behaviour.\n\nIn the meantime, we will do our best to reproduce the issue and get back as soon as we can. \n\n\n\n---\n\nHello @AlexFenlon \nThe node restart was related to cluster component update, so k8s drained all old nodes and migrated all pods to the new ones.\nWe did this operation with 4 k8s clusters but got an ingress problem only for the biggest one. 2 of 3 pods were in CrashLoopBackOff status. The biggest cluster has +- 60 nodes and about 217 ingress resources - maybe it's related to the problem. \n\nYes, I can try readOnlyRootFilesystem, but only in the staging cluster. The main problem - I don't know how to reproduce the issue to check whether readOnlyRootFilesystem will solve the problem. I will try to reproduce the problem with the current settings and then try to do it with readOnlyRootFilesystem.\nUPD: During the update from 3.0.2 to 3.6.2 we set readOnlyRootFilesystem = true\n\n---\n\nI repeated the same behavior by sending 1 signal from the k8s worker node.\n\n1. Run nginx-ingress-controller\n2. exec to k8s worker node\n3. find nginx master process by htop\n4. kill with 1 signal\n5. Now pod in CrashLoopBackOff\n\n```\n2024/11/06 09:35:56 [notice] 14#14: signal 1 (SIGHUP) received from 24, reconfiguring\n2024/11/06 09:35:56 [notice] 14#14: reconfiguring\n2024/11/06 09:35:56 [warn] 14#14: duplicate MIME type \"text/html\" in /etc/nginx/nginx.conf:28\n2024/11/06 09:35:56 [notice] 14#14: using the \"epoll\" event method\n2024/11/06 09:35:56 [notice] 14#14: start worker processes\n2024/11/06 09:35:56 [notice] 14#14: start worker process 25\n2024/11/06 09:35:56 [notice] 14#14: start worker process 26\n2024/11/06 09:35:56 [notice] 14#14: start worker process 27\n2024/11/06 09:35:56 [notice] 14#14: start worker process 28\n2024/11/06 09:35:56 [notice] 16#16: gracefully shutting down\n2024/11/06 09:35:56 [notice] 17#17: gracefully shutting down\n2024/11/06 09:35:56 [notice] 18#18: gracefully shutting down\n2024/11/06 09:35:56 [notice] 15#15: gracefully shutting down\n2024/11/06 09:35:56 [notice] 17#17: exiting\n2024/11/06 09:35:56 [notice] 18#18: exiting\n2024/11/06 09:35:56 [notice] 15#15: exiting\n2024/11/06 09:35:56 [notice] 16#16: exiting\n2024/11/06 09:35:56 [notice] 16#16: exit\n2024/11/06 09:35:56 [notice] 15#15: exit\n2024/11/06 09:35:56 [notice] 17#17: exit\n2024/11/06 09:35:56 [notice] 18#18: exit\nI1106 09:35:56.455595       1 event.go:377] Event(v1.ObjectReference{Kind:\"ConfigMap\", Namespace:\"nginx-ingress\", Name:\"nginx-inc-ingress-controller\", UID:\"a61edba5-ec9b-4024-8649-d88d6d932178\", APIVersion:\"v1\", ResourceVersion:\"404400560\", FieldPath:\"\"}): type: 'Normal' reason: 'Updated' Configuration from nginx-ingress/nginx-inc-ingress-controller was updated\nI1106 09:35:56.455660       1 event.go:377] Event(v1.ObjectReference{Kind:\"Ingress\", Namespace:\"<namespace>\", Name:\"<ingress_name>\", UID:\"2d658178-96ee-4290-a8cb-a04de49f3150\", APIVersion:\"networking.k8s.io/v1\", ResourceVersion:\"381554759\", FieldPath:\"\"}): type: 'Normal' reason: 'AddedOrUpdated' Configuration for <namespace>/<ingress_name> was added or updated\n2024/11/06 09:35:56 [notice] 14#14: signal 17 (SIGCHLD) received from 17\n2024/11/06 09:35:56 [notice] 14#14: worker process 17 exited with code 0\n2024/11/06 09:35:56 [notice] 14#14: signal 29 (SIGIO) received\n2024/11/06 09:35:56 [notice] 14#14: signal 17 (SIGCHLD) received from 16\n2024/11/06 09:35:56 [notice] 14#14: worker process 16 exited with code 0\n2024/11/06 09:35:56 [notice] 14#14: signal 29 (SIGIO) received\n2024/11/06 09:35:56 [notice] 14#14: signal 17 (SIGCHLD) received from 18\n2024/11/06 09:35:56 [notice] 14#14: worker process 18 exited with code 0\n2024/11/06 09:35:56 [notice] 14#14: worker process 15 exited with code 0\n2024/11/06 09:35:56 [notice] 14#14: signal 29 (SIGIO) received\n2024/11/06 09:35:56 [notice] 14#14: signal 17 (SIGCHLD) received from 15\nE1106 09:39:46.190574       1 processes.go:39] unable to collect process metrics : unable to read file /proc/37/cmdline: open /proc/37/cmdline: no such file or directory\n```\nNow Pod is restarting and going to CrashLoopBackOff cause of busy sockets error.\nThe question - who is sending 1 signal in production workload? \n\nI also used **readOnlyRootFilesystem=false** and repeated the same case with 1 signal - now the pod is just restarting and working fine. So looks like this solution will work for us.",
      "labels": [
        "bug",
        "backlog"
      ],
      "created_at": "2024-11-05T14:58:32Z",
      "closed_at": "2025-01-14T11:59:16Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/6752",
      "comments_count": 10
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 7190,
      "title": "Add seccompProfile to securityContext",
      "problem": "**Is your feature request related to a problem? Please describe.**\n\nI think it'd be nice to have this a default under `securityContext` for the nginx Deployment resource.\n```yaml\nseccompProfile:\n  type: RuntimeDefault\n```\n\nhttps://kubernetes.io/docs/tutorials/security/seccomp/#enable-the-use-of-runtimedefault-as-the-default-seccomp-profile-for-all-workloads\n\nDocker has a list of syscalls that are banned with this https://docs.docker.com/engine/security/seccomp/\nThe corresponding capability is listed. Note that one of them are \"NET_BIND_SERVICE\" which is the only one that really appears needed for nginx.\n\nWe have currently set this under our deployment. If you use a managed Kubernetes cluster (AWS, Azure, Google or something else), it may be difficult or undesired to create custom more elaborate profiles.\n\nThere is a bit more motivation here.\nhttps://www.stigviewer.com/stig/mirantis_kubernetes_engine/2024-06-17/finding/V-260937\n\nhttps://www.aquasec.com/products/trivy/ This tool will report this as a shortcoming. There is a free/open version of the tool that can be used.",
      "solution": "closing this one as alternative resolution is provided",
      "labels": [
        "proposal"
      ],
      "created_at": "2025-01-22T15:05:06Z",
      "closed_at": "2025-10-06T14:51:56Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/7190",
      "comments_count": 5
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 7721,
      "title": "[Bug]: ApPolicy CRD missing violations",
      "problem": "### Version\n\nedge\n\n### What Kubernetes platforms are you running on?\n\nOther\n\n### Steps to reproduce\n\n1. Create the following `ApPolicy`, and apply it to the cluster using kubectl\n\n```\napiVersion: appprotect.f5.com/v1beta1\nkind: APPolicy\nmetadata:\n  name: blizzard-bot-exception-policy\nspec:\n  policy:\n    name: blizzard-bot-exception-policy\n    applicationLanguage: utf-8\n    enforcementMode: blocking\n    template:\n      name: POLICY_TEMPLATE_NGINX_BASE\n    blocking-settings:\n      violations:\n        - name: VIOL_BOT_CLIENT\n          alarm: true\n          block: true\n        - name: VIOL_DATA_GUARD\n          alarm: true\n          block: false\n``` \n\n2. Note the following error in the NIC pod log:\n```\n2025-04-28 20:37:12.515036: Error: UPGRADE FAILED: failed to create resource: APPolicy.appprotect.f5.com \"blizzard-bot-exception-policy\" is invalid: spec.policy.blocking-settings.violations[0].name: Unsupported value: \"VIOL_BOT_CLIENT\": supported values: \"VIOL_ACCESS_INVALID\", \"VIOL_ACCESS_MALFORMED\", \"VIOL_ACCESS_MISSING\", \"VIOL_ACCESS_UNAUTHORIZED\", \"VIOL_ASM_COOKIE_HIJACKING\", \"VIOL_ASM_COOKIE_MODIFIED\", \"VIOL_BLACKLISTED_IP\", \"VIOL_COOKIE_EXPIRED\", \"VIOL_COOKIE_LENGTH\", \"VIOL_COOKIE_MALFORMED\", \"VIOL_COOKIE_MODIFIED\", \"VIOL_CSRF\", \"VIOL_DATA_GUARD\", \"VIOL_ENCODING\", \"VIOL_EVASION\", \"VIOL_FILE_UPLOAD\", \"VIOL_FILE_UPLOAD_IN_BODY\", \"VIOL_FILETYPE\", \"VIOL_GRAPHQL_ERROR_RESPONSE\", \"VIOL_GRAPHQL_FORMAT\", \"VIOL_GRAPHQL_INTROSPECTION_QUERY\", \"VIOL_GRAPHQL_MALFORMED\", \"VIOL_GRPC_FORMAT\", \"VIOL_GRPC_MALFORMED\", \"VIOL_GRPC_METHOD\", \"VIOL_HEADER_LENGTH\", \"VIOL_HEADER_METACHAR\", \"VIOL_HEADER_REPEATED\", \"VIOL_HTTP_PROTOCOL\", \"VIOL_HTTP_RESPONSE_STATUS\", \"VIOL_JSON_FORMAT\", \"VIOL_JSON_MALFORMED\", \"VIOL_JSON_SCHEMA\", \"VIOL_MANDATORY_HEADER\", \"VIOL_MANDATORY_PARAMETER\", \"VIOL_MANDATORY_REQUEST_BODY\", \"VIOL_METHOD\", \"VIOL_PARAMETER\", \"VIOL_PARAMETER_ARRAY_VALUE\", \"VIOL_PARAMETER_DATA_TYPE\", \"VIOL_PARAMETER_EMPTY_VALUE\", \"VIOL_PARAMETER_LOCATION\", \"VIOL_PARAMETER_MULTIPART_NULL_VALUE\", \"VIOL_PARAMETER_NAME_METACHAR\", \"VIOL_PARAMETER_NUMERIC_VALUE\", \"VIOL_PARAMETER_REPEATED\", \"VIOL_PARAMETER_STATIC_VALUE\", \"VIOL_PARAMETER_VALUE_BASE64\", \"VIOL_PARAMETER_VALUE_LENGTH\", \"VIOL_PARAMETER_VALUE_METACHAR\", \"VIOL_PARAMETER_VALUE_REGEXP\", \"VIOL_POST_DATA_LENGTH\", \"VIOL_QUERY_STRING_LENGTH\", \"VIOL_RATING_NEED_EXAMINATION\", \"VIOL_RATING_THREAT\", \"VIOL_REQUEST_LENGTH\", \"VIOL_REQUEST_MAX_LENGTH\", \"VIOL_THREAT_CAMPAIGN\", \"VIOL_URL\", \"VIOL_URL_CONTENT_TYPE\", \"VIOL_URL_LENGTH\", \"VIOL_URL_METACHAR\", \"VIOL_XML_FORMAT\", \"VIOL_XML_MALFORMED\"\n2025-04-28 20:37:12.524045: [33;1mWARNING: Command failed. Retrying in 5 seconds.[0m\n```",
      "solution": "Resolved on https://github.com/nginx/kubernetes-ingress/commit/85810e62797ff2439bb7521a8b1cada3abbe3950",
      "labels": [
        "bug",
        "backlog",
        "area/security"
      ],
      "created_at": "2025-04-29T15:31:32Z",
      "closed_at": "2025-10-01T15:55:16Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/7721",
      "comments_count": 2
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 8180,
      "title": "[Bug]: topologySpreadConstraints JSON schema is invalid with helm v3.18.5+",
      "problem": "### Version\n\n4.0.1\n\n### What Kubernetes platforms are you running on?\n\nEKS Amazon\n\n### Steps to reproduce\n\nHelm v3.18.5 changed the Go library for validating JSON Schema https://github.com/helm/helm/releases/tag/v3.18.5\nhttps://github.com/helm/helm/commit/cb8595bc650e2ec7459427d2b0430599431a3dbe\n\nwhich uncovered a problem with the nginx-ingress topologySpreadConstraints JSON schema in the Helm chart.\n\nThe problem is that the JSON schema incorrectly specifies controller.topologySpreadConstraints as `object` https://github.com/nginx/kubernetes-ingress/blob/main/charts/nginx-ingress/values.schema.json#L919\nbut the referenced schema https://raw.githubusercontent.com/nginxinc/kubernetes-json-schema/master/v1.33.1/_definitions.json#/definitions/io.k8s.api.core.v1.PodSpec/properties/topologySpreadConstraints\nspecifies it (correctly) as \"array\"\n\n## How to reproduce\n\n- Prepare `values-array.yaml`\n```yaml \ncontroller:\n  topologySpreadConstraints:\n    - maxSkew: 1\n      topologyKey: \"topology.kubernetes.io/zone\"\n      whenUnsatisfiable: ScheduleAnyway\n      labelSelector:\n        matchLabels:\n          helmapp.kubernetes.io/name: nginx-ingress\n```\n- connect Helm repo \n```shell\nhelm repo add nginx https://helm.nginx.com/stable\n```\n- run local templating with Helm \n```yaml\n\u276f helm version\nversion.BuildInfo{Version:\"v3.18.6\", GitCommit:\"b76a950f6835474e0906b96c9ec68a2eff3a6430\", GitTreeState:\"clean\", GoVersion:\"go1.25.0\"}\n\u276f helm template nginx nginx/nginx-ingress -f values-array.yaml\nError: values don't meet the specifications of the schema(s) in the following chart(s):\nnginx-ingress:\n- at '/controller/topologySpreadConstraints': got array, want object\n\n```\n\nwhen JSON schema validation is disabled, it produces valid manifest\n\n```shell\n\u276f helm template nginx nginx/nginx-ingress -f values-array.yaml --skip-schema-validation | kubeconform -strict -verbose\nstdin - ConfigMap nginx-nginx-ingress-leader-election is valid\nstdin - ServiceAccount nginx-nginx-ingress is valid\nstdin - ConfigMap nginx-nginx-ingress is valid\nstdin - ClusterRole nginx-nginx-ingress is valid\nstdin - Role nginx-nginx-ingress is valid\nstdin - RoleBinding nginx-nginx-ingress is valid\nstdin - ClusterRoleBinding nginx-nginx-ingress is valid\nstdin - Service nginx-nginx-ingress-controller is valid\nstdin - IngressClass nginx is valid\nstdin - Deployment nginx-nginx-ingress-controller is valid\nstdin - Lease nginx-nginx-ingress-leader-election is valid\n```\n\nWith older version of Helm this works just fine \n```shell\n\u276f ~/tmp/helm-old version\nversion.BuildInfo{Version:\"v3.18.4\", GitCommit:\"d80839cf37d860c8aa9a0503fe463278f26cd5e2\", GitTreeState:\"clean\", GoVersion:\"go1.24.4\"}\n\u276f ~/tmp/helm-old template nginx nginx/nginx-ingress -f values-array.yaml\n---\n# Source: nginx-ingress/templates/controller-serviceaccount.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: nginx-nginx-ingress\n  namespace: default\n  labels:\n    helm.sh/chart: nginx-ingress-2.2.2\n    app.kubernetes.io/name: nginx-ingress\n    app.kubernetes.io/instance: nginx\n    app.kubernetes.io/version: \"5.1.1\"\n    app.kubernetes.io/managed-by: Helm\n...\n```",
      "solution": "It should be pretty straightforward. The `type` parameter for any node in the `values.schema.json` that uses a `$ref` is redundant anyway. The `type` and any other details are taken only from the referenced schema. \n\nIn the case of `controller.topologySpreadConstraints` simply removing the `\"type\": \"object\",` line solves the problem for both older and new versions of Helm. `topologySpreadConstraints` can't be an object, as that would produce an invalid K8s manifest. And the referenced schema correctly specifies it as an `array`. \n\n",
      "labels": [
        "bug",
        "backlog",
        "refined"
      ],
      "created_at": "2025-08-21T13:11:20Z",
      "closed_at": "2025-09-29T11:09:44Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/8180",
      "comments_count": 5
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 7768,
      "title": "SSL_do_handshake() failed (SSL: error:14094458:SSL routines:ssl3_read_bytes:tlsv1 unrecognized name:SSL alert number 112) while SSL handshaking to upstream, client: 127.0.0.1, server: , request: \"GET /lab_control HTTP/1.1\", upstream:",
      "problem": "### Version\n\n4.0.1\n\n### What Kubernetes platforms are you running on?\n\nAKS Azure\n\n### Steps to reproduce\n\nIn our aks cluster the nginx version was in 2.0.1 and we upgraded it to version 4.0.1. And the upgrade was through helm chart.  updating the helm dependency, lease upgrade, and crd upgrades\n\nThe upgrade went well but there was a issue for the upstream appliance which was connecting to the application before upgrade with no error but now after the upgrade it has this error\nSSL_do_handshake() failed (SSL: error:14094458:SSL routines:ssl3_read_bytes:tlsv1 unrecognized name:SSL alert number 112) while SSL handshaking to upstream, client: 127.0.0.1, server: , request: \"GET /lab_control HTTP/1.1\", upstream: \n\nnow that we have to fix this error but not at the appliance side but from the cluster side what parameter i should make change to fix this error\n\nnginx-ingress:\n  enabled: true\n  controller:\n    config:\n      entries:\n        default-server-return: 302 https://www..........com/\n        ssl-protocols: TLSv1.2\n        server-tokens: false\n        proxy_ssl_server_name: true\n\nDoes the addition of  proxy_ssl_server_name: true in the values.yaml fix this?",
      "solution": "Also want to highlight that the issue of SSL handshake wasnt there before the upgrade to v.4.0.1\n\n---\n\n@Shivani-shi could you please test the upgrade to NIC v3.0.2 and check if you encounter the same issue? If you don't see the issue with v3.0.2 could you test version v3.1.0?\n\nIn NIC v3.1.0 we set `ssl_reject_handshake on;` by default.\n\nPlease see our release notes here on v3.1.0 https://docs.nginx.com/nginx-ingress-controller/releases/#310",
      "labels": [
        "bug",
        "stale",
        "backlog"
      ],
      "created_at": "2025-05-09T04:11:14Z",
      "closed_at": "2025-09-26T02:03:33Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/7768",
      "comments_count": 19
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 7876,
      "title": "[Bug]: client-max-body-size Ignored When OIDC Policy Is Enabled",
      "problem": "### Version\n\nedge\n\n### What Kubernetes platforms are you running on?\n\nAKS Azure\n\n### Steps to reproduce\n\nWhen using the client-max-body-size setting in a VirtualServer object in combination with an OIDC policy, the setting is ignored and the default limit of 1MB is applied instead. The restriction appears to originate from the following location https://github.com/nginx/kubernetes-ingress/blob/main/internal/configs/oidc/oidc.conf#L9\n\nWe are currently running the latest release of the ingress controller: v5.0.0\n\n\n**Steps to reproduce:** \n1. Create a VirtualServer object with client-max-body-size explicitly set.\n2. Configure the server to use OIDC.\n3. Send a request larger than 1MB.\n\n**Expected Result:**\nThe request should be accepted if it is within the configured client-max-body-size.\n\n**Actual Result:**\nThe request is rejected due to the default 1MB limit, indicating that the custom setting is not being applied.",
      "solution": "Setting the configuration globally works, but it's not an acceptable solution, as it allows all virtual servers connected to the shard to use large requests. We still need the ability to set client-max-body-size individually for each virtual server that requires support for larger requests than the default.",
      "labels": [
        "bug",
        "stale",
        "backlog",
        "ready for refinement"
      ],
      "created_at": "2025-06-10T11:12:16Z",
      "closed_at": "2025-09-26T02:03:31Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/7876",
      "comments_count": 7
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 7846,
      "title": "Guide for permission issue when using NIC with NGINX Agent",
      "problem": "Hi. I've been trying to use NGINX Agent with NIC to integrate NAP WAF V5 with NIM Security Monitoring. When I tried to run NGINX Agent with `-agent=true` args, I found out this error.\n\n`level=fatal msg=\"Unable to load properties from config files (/etc/nginx-agent/nginx-agent.conf, /var/lib/nginx-agent/agent-dynamic.conf) - error attempting to open dynamic config (/var/lib/nginx-agent/agent-dynamic.conf): open /var/lib/nginx-agent/agent-dynamic.conf: permission denied\"`\n\nThis appears to happen because NIC pod runs with 101 User(nginx) and doesn't have permission to read `/var/lib/nginx-agent` dir.\nSo I changed the pod's security context(not container's) to `fsGroup: 101` and mounted emptyDir to `/var/lib/nginx-agent`.\n\nThe following are some of the contents of the `deployment.yaml`\n\n```\nspec:\n  template:\n    spec:\n      securityContext:\n        # Added fsGroup to grant nginx user (101) access to mounted volumes\n        fsGroup: 101               \n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: nginx-plus-ingress\n        args:\n        - -nginx-plus\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -mgmt-configmap=$(POD_NAMESPACE)/nginx-config-mgmt\n        # Activate NGINX Agent\n        - -agent=true\n        - -agent-instance-group=nginx-ingress\n        - -report-ingress-status\n        - -external-service=nginx-ingress\n        - -enable-app-protect\n        - -enable-prometheus-metrics\n        - -global-configuration=$(POD_NAMESPACE)/nginx-configuration\n        - -log-level=error\n        # Maintain container security constraints\n        securityContext: \n          allowPrivilegeEscalation: false\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 101\n        volumeMounts:\n        - mountPath: /opt/app_protect/bd_config\n          name: app-protect-bd-config\n        - mountPath: /opt/app_protect/config\n          name: app-protect-config\n        - mountPath: /etc/app_protect/bundles\n          name: app-protect-bundles\n        - mountPath: /etc/nginx-agent/nginx-agent.conf\n          name: agent-config\n          subPath: nginx-agent.conf\n        # Mount directory for permission\n        - mountPath: /var/lib/nginx-agent\n          name: agent-lib-dir\n      volumes:\n      - name: app-protect-bd-config\n        emptyDir: {}\n      - name: app-protect-config\n        emptyDir: {}      \n      - name: app-protect-bundles\n        persistentVolumeClaim:\n          claimName: waf-bundle-pvc\n      - name: agent-config\n        configMap:\n          name: agent-config\n      # Mount directory for permission\n      - name: agent-lib-dir  \n        emptyDir: {}\n        \n```\n\nIf there's better solution for this, please let me know. And it would be nice to have some guide for using NIC with NGINX Agent.",
      "solution": "Hi @0jsong, \n\nThanks for bringing this to our attention, \n\nNGINX Agent should have already been stated within the pod, \n\nThis is exactly the solution, we must have missed this in our documentation. I will update this and reply.\n\n\n\n\n---\n\nThe doc has been updated, thank you for submitting and hope that solved your issue. \n\nhttps://docs.nginx.com/nginx-ingress-controller/tutorials/security-monitoring/#deploying-nginx-ingress-controller-with-nginx-agent-configuration",
      "labels": [
        "proposal",
        "waiting for response"
      ],
      "created_at": "2025-05-28T01:21:52Z",
      "closed_at": "2025-09-18T12:08:17Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/7846",
      "comments_count": 5
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 8129,
      "title": "[Bug]: Policy dev-tintin-system/jwt-policy is invalid and was rejected",
      "problem": "### Version\n\nedge\n\n### What Kubernetes platforms are you running on?\n\nKind\n\n### Steps to reproduce\n\n1. deploy VS with jwt\n2. see error in description of policy\n```yaml\n\u279c  tintin-voip git:(test-k8s) \u2717 kdes policy jwt-policy                 \nName:         jwt-policy\nNamespace:    dev-tintin-system\nLabels:       project=tintin\n              version=v1.0.0\nAnnotations:  <none>\nAPI Version:  k8s.nginx.org/v1\nKind:         Policy\nMetadata:\n  Creation Timestamp:  2025-08-13T07:35:04Z\n  Generation:          1\n  Resource Version:    6094225\n  UID:                 b2b5b1e3-bdd1-4f53-9dc3-f7c667560a3e\nSpec:\n  Jwt:\n    Realm:   MyProductAPI\n    Secret:  jwk-secret\n    Token:   $http_token\nStatus:\n  Message:  Policy dev-tintin-system/jwt-policy is invalid and was rejected: spec.jwt: Forbidden: jwt secrets are only supported in NGINX Plus\n  Reason:   Rejected\n  State:    Invalid\nEvents:     <none>\n```",
      "solution": "> [@jxs1211](https://github.com/jxs1211) does it solve your issue?\n\nYes, I moved to the F5 inc. for more details on the ingress with JWT, thanks for your patient and support. Please close the issue.",
      "labels": [
        "bug",
        "waiting for response"
      ],
      "created_at": "2025-08-13T08:47:16Z",
      "closed_at": "2025-09-08T15:10:50Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/8129",
      "comments_count": 7
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 8025,
      "title": "[Bug]: golang:1.24 is required when building image for 4.0.1 version",
      "problem": "### Version\n\n4.0.1\n\n### What Kubernetes platforms are you running on?\n\nKind\n\n### Steps to reproduce\n\nThe golang:1.23 is giving an issue when building the images , below is the error we are facing \n```#70 0.203 go: go.mod requires go >= 1.24.2 (running go 1.23.6; GOTOOLCHAIN=local)\n#70 ERROR: process \"/bin/sh -c go mod download\" did not complete successfully: exit code: 1\n------\n > [builder 3/4] RUN --mount=type=bind,target=/go/src/github.com/nginx/kubernetes-ingress/ --mount=type=cache,target=/root/.cache/go-build \tgo mod download:\n0.203 go: go.mod requires go >= 1.24.2 (running go 1.23.6; GOTOOLCHAIN=local)\n------\nERROR: failed to build: failed to solve: process \"/bin/sh -c go mod download\" did not complete successfully: exit code: 1\nmake: *** [Makefile:148: alpine-image-plus-fips] Error 1```\n\nSo we need to update the FROM golang:1.23-alpine@sha256:2c49857f2295e89b23b28386e57e018a86620a8fede5003900f2d138ba9c4037 AS golang-builder\n in dockerfile",
      "solution": "Hi @AlexFenlon. \nThe Go version is coming from this https://github.com/nginx/kubernetes-ingress/blob/v4.0.1/build/Dockerfile repo, right? I think you guys need to update your Docker file (line no. 20) . But for now, I have patched your Docker file(1.24.2 version) after cloning it in my pipeline and resolved the version mismatch issue.\n\nUpon further investigation , However, even with Go 1.24.2, we are consistently encountering the following error during the go build step inside the Docker container:\n\ninternal/telemetry/collector.go:15:2: no required module provides package github.com/nginx/telemetry-exporter/pkg/telemetry; to add it: go get github.com/nginx/telemetry-exporter/pkg/telemetry\n\nThis indicates an inconsistency in vendoring/module resolution within the project's dependency graph as perceived by the Go build system, particularly concerning the telemetry-exporter module. Even when go mod tidy and go mod vendor are run on the host machine, the go build inside the Dockerfile still reports this missing module. (This same kind of issue we are facing in both 4.0.1 and 5.1.0 ).\n\nTo resolve this directly within the Docker build process, we suggest modifying the build/Dockerfile to explicitly ensure Go modules are correctly synced before compilation.\nSo please make the necessary changes to your Docker file  (https://github.com/nginx/kubernetes-ingress/blob/v4.0.1/build/Dockerfile)",
      "labels": [
        "bug",
        "waiting for response"
      ],
      "created_at": "2025-07-16T08:28:57Z",
      "closed_at": "2025-08-25T15:10:35Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/8025",
      "comments_count": 5
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 7384,
      "title": "[Bug]: When deleting or changing Api Key name NGINX Breaks/refuses update.",
      "problem": "### Version\n\n3.7.0\n\n### What Kubernetes platforms are you running on?\n\nOpenshift\n\n### Steps to reproduce\n\nCreating the following secret:\n\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: apikey-secret\ntype: nginx.org/apikey\nstringData:\n  client1: \"demo\"\n  client2: \"showcase\"\n```\n\nThen attaching it to a policy:\n\n```\napiVersion: k8s.nginx.org/v1\nkind: Policy\nmetadata:\n  name: apikey-policy\nspec:\n  apiKey:\n    clientSecret: apikey-secret\n    suppliedIn:\n      header:\n        - x-api-key\n```\n\nWhen deleting an api key using `oc apply` the object does not update, when deleting via OCP UI it deletes.\n\nIn addition when changing the name of a client the configuration will break and remove the api key authentication from the config file.\n\nOnly way that i have managed to delete/change name of API Keys is to completely delete the secret and re-create it.",
      "solution": "@vepatel \n\nThe problem persists when:\n\nYou delete a key and value (e.g deleting completetly client1 and the vlaue for it and applying.)\n\nAnd when you change the `key`, for example changing clienttests to demo will break it, not the value `demo`, changing only the value and not the key itself works.\n\n\nAnd we will try changing to Data from stringData, it's just easier to configure like that when you see the api key itself.\n\n---\n\nI tried to reproduce this with [our example](https://github.com/nginx/kubernetes-ingress/tree/main/examples/custom-resources/api-key) in a local k8s cluster with `kubectl apply` but I also did not encounter the issue.\n\nBy default, the `api-key-secret.yaml` is\n\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: api-key-client-secret\ntype: nginx.org/apikey\ndata:\n    client1: cGFzc3dvcmQ= # password\n    client2: YW5vdGhlci1wYXNzd29yZA== # another-password\n```\n\nand the map in the generated NGINX config is\n```\nmap $apikey_auth_token $apikey_auth_client_name_default_cafe_api_key_policy {\n    default \"\";\n    \"5e884898da28047151d0e56f8dc6292773603d0d6aabbdd62a11ef721d1542d8\" \"client1\";\n    \"5b6cb866b79cfaffe4162718f97eacafa6732e3d340622fcbda84582840eb9ec\" \"client2\";\n}\n\n```\n\nThen I renamed the key from `client1` to `client1b`, i.e.\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: api-key-client-secret\ntype: nginx.org/apikey\ndata:\n    client1b: cGFzc3dvcmQ= # password\n    client2: YW5vdGhlci1wYXNzd29yZA== # another-password\n```\n\nthe NGINX config becomes:\n```\nmap $apikey_auth_token $apikey_auth_client_name_default_cafe_api_key_policy {\n    default \"\";\n    \"5e884898da28047151d0e56f8dc6292773603d0d6aabbdd62a11ef721d1542d8\" \"client1b\";\n    \"5b6cb866b79cfaffe4162718f97eacafa6732e3d340622fcbda84582840eb9ec\" \"client2\";\n}\n```\n\nAnd if I delete client1\n\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: api-key-client-secret\ntype: nginx.org/apikey\ndata:\n    client2: YW5vdGhlci1wYXNzd29yZA== # another-password\n```\n\nit is gone from the NGINX config as well\n\n```\nmap $apikey_auth_token $apikey_auth_client_name_default_cafe_api_key_policy {\n    default \"\";\n    \"5b6cb866b79cfaffe4162718f97eacafa6732e3d340622fcbda84582840eb9ec\" \"client2\";\n}\n```\nAre these the operation you were trying to perform at the OpenShift cluster? Since we could not reproduce the issue, I wonder if it could be an environment or configuration related issue?",
      "labels": [
        "bug",
        "waiting for response"
      ],
      "created_at": "2025-02-19T13:59:03Z",
      "closed_at": "2025-08-25T15:05:50Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/7384",
      "comments_count": 15
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 3770,
      "title": "Unable to use CRDs imported as modules",
      "problem": "**Describe the bug**\r\nWhen automating the configuration for NGINX Ingress Controller, we need access to the CRDs. Importing them as a golang module, is preferrable.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\nRunning  \" go get -u github.com/nginxinc/kubernetes-ingress\" will fail with \" require github.com/nginxinc/kubernetes-ingress: version \"v3.1.0\" invalid: should be v0 or v1, not v3\"\r\n\r\nThis is fixable with adding /v3 in the module name in go.mod.\r\n\r\n**Expected behavior**\r\nExpecting to be able to import the module.\r\nI do get around it with psedo-versions (go get -u github.com/nginxinc/kubernetes-ingress@v0.0.0-20230414120628-96d28b25e15b)\r\n\r\n**Your environment**\r\nv3.1.0\r\nKubernetes: N/A\r\n\r\n",
      "solution": "Hi @nixx,\r\nThanks for reporting the issue. We are planning to make the module importable, so packages could be used by referencing `v3`.\r\n",
      "labels": [
        "stale",
        "backlog"
      ],
      "created_at": "2023-04-14T14:38:01Z",
      "closed_at": "2025-08-09T02:12:34Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/3770",
      "comments_count": 9
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 1782,
      "title": "Add PKCE, Scopes, and Logout Redirect URL Support to the OIDC Policy",
      "problem": " **Is your feature request related to a problem? Please describe:**<br/>To have feature parity with the [NGINX Plus OIDC Reference Implementation](https://github.com/nginxinc/nginx-openid-connect) I would like to see a key/value pair added to the OIDC Policy declaration to include enabling PKCE, updating scopes, and adding a Logout Redirect URL.\n\n**Describe the solution you'd like:**<br/>I would like to see a key/value pair added to the OIDC Policy declaration to include enabling PKCE ($oidc\\_pkce\\_enable 1), updating scopes ($oidc\\_scopes), and adding a Logout Redirect URL ($oidc\\_logout\\_redirect).\n\n**Describe alternatives you've considered:**<br/>Not sure if we could use Server/Location Snippets to meet this requirement with the map directives in the openid\\_connect\\_configuration.conf file.\n\n**Additional context:**<br/>PKCE could be automatically marked true (Implemented) if the clientSecret value is left blank, unless there is a use case where both would be needed? Scopes and Logout URI would use the defaults if not updated and therefore be optional.\n\nSome more detail, I'm looking more for feature parity with this [OIDC Reference Implementation](https://github.com/nginxinc/nginx-openid-connect) which looks like the same implementation added to the latest version of KIC (If you exec into the KIC pod and look in the /etc/nginx/oidc folder, you will see the same code used). We are just missing these 3x variables to make the 2 solutions equivalent. All the code is already in /etc/nginx/oidc, on KIC, but these 3x variables ($oidc\\_pkce\\_enable, $oidc\\_scopes, and $oidc\\_logout\\_redirect) were not included in the OIDC Policy configuration which is what I am asking about.\n\n<br/>",
      "solution": "Some more detail, I'm looking more for feature parity with this [OIDC Reference Implementation](https://github.com/nginxinc/nginx-openid-connect) which looks like the same implementation added to the latest version of KIC (If you exec into the KIC pod and look in the /etc/nginx/oidc folder, you will see the same code used). We are just missing these 3x variables to make the 2 solutions equivalent. All the code is already in  /etc/nginx/oidc, on KIC, but these 3x variables ($oidc_pkce_enable, $oidc_scopes, and $oidc_logout_redirect) were not included in the OIDC Policy configuration which is what I am asking about.",
      "labels": [
        "proposal",
        "backlog"
      ],
      "created_at": "2021-07-27T08:05:53Z",
      "closed_at": "2025-08-08T11:12:04Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/1782",
      "comments_count": 11
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 7427,
      "title": "Add SNI for JWT policy",
      "problem": "**Is your feature request related to a problem? Please describe.**\n\nNetwork rules often rely on SNI to work, and we noticed that Nginx is not always sending the server name.\n\n**Describe the solution you'd like**\n\nOne example is here: https://github.com/nginx/kubernetes-ingress/blob/main/internal/configs/version2/nginx-plus.virtualserver.tmpl#L230\n\nIt would be nice if all proxy_pass / external subrequests set SNI, like it is done here:\n\n`        proxy_ssl_server_name on;                     # For SNI to the IdP`\n\n**Describe alternatives you've considered**\n\nNone. Workaround is to use IP-addresses in firewalls etc.\n\n**Additional context**\n\nAcceptance Criteria:\n\nEnable users to set:\n\nproxy_ssl_server_name must be off by default, as it is the NGINX default.\n\nproxy_ssl_name should be configurable, and the default value should be the NGINX default value.\n\n\n",
      "solution": "Should be fixed by https://github.com/nginx/kubernetes-ingress/pull/7500",
      "labels": [
        "proposal",
        "refined",
        "area/security"
      ],
      "created_at": "2025-02-28T10:56:16Z",
      "closed_at": "2025-07-22T09:07:47Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/7427",
      "comments_count": 2
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 8049,
      "title": "[Bug]: failed to install release: create: failed to create: Secret \"sh.helm.release.v1.my-nginx-ingress.v1\" is invalid: data: Too long: must have at most 1048576 bytes",
      "problem": "### Version\n\nedge\n\n### What Kubernetes platforms are you running on?\n\nOpenshift\n\n### Steps to reproduce\n\nWhen deploying the NGINX Ingress Controller via the Operator on OpenShift 4.18, the installation fails with the following error:\n\n`failed to install release: create: failed to create: Secret \"sh.helm.release.v1.my-nginx-ingress.v1\" is invalid: data: Too long: must have at most 1048576 bytes`\n\nThis appears to be due to the Helm release metadata exceeding the 1MiB limit imposed on Kubernetes Secrets. \n\nSteps to Reproduce\n\n1. Deploy the operator in an OpenShift 4.18 cluster.  \n2. Apply a moderately complex custom resource  (I used the template [here](https://github.com/redhat-cop/agnosticd/blob/development/ansible/roles_ocp_workloads/ocp4_workload_nginxplus/templates/nginx-ingress-controller.j2))\n3. Observe the failure with the Secret size error.",
      "solution": "Hi @gejames ,\n\nThanks for the bug report, this is a bug with Operator that will be fixed in the next version v3.2.2. \n\n",
      "labels": [
        "bug",
        "needs triage"
      ],
      "created_at": "2025-07-18T18:27:12Z",
      "closed_at": "2025-07-21T08:58:47Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/8049",
      "comments_count": 3
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 7988,
      "title": "[Bug]: NIC does not accept day-unit value for controller.mgmt.usageReport.interval",
      "problem": "### Version\n\n5.0.0\n\n### What Kubernetes platforms are you running on?\n\nKind\n\n### Steps to reproduce\n\nDeploy NIC v5.0.0 with `alues.yaml` that has `controller.mgmt.usageReport.interval` set to `30d`\n\nObserved Error:\n`E20250630 17:05:38.655714 1 configmaps.go:818] Configmap nginx-plus/ingress-nginx-plus-mgmt: Invalid value for the interval key: got \"30d\": time: unknown unit \"d\" in duration \"30d\". Ignoring.\nE20250630 17:05:38.655764 1 configmaps.go:824] Configmap nginx-plus/ingress-nginx-plus-mgmt: Value too low for the interval key, got: 30d, need higher than 60s. Ignoring.`\n\nExpected Outcome:\nA value of `30d` for the usage report interval applies successfully in the nginx configuration\n\n### Workaround\nUse the hours as the units, eg 30 days as 30*24=720h",
      "solution": "It turns out that the [usage_report](https://nginx.org/en/docs/ngx_mgmt_module.html#usage_report) directive only allows the interval value to be set to a maximum of 24 hours.\nTherefore, our recommendation to the customer is to set this to 24h.\n \nHope this clarifies the limits of `controller.mgmt.usageReport.interval` parameter.\nI have requested the docs to provide limits for the `interval` value of the `usage_report` directive in the [nginx.org](https://nginx.org/) docs.\n\n@AlexFenlon, please confirm if we have assessed this appropriately.\nWe ought to improve the logging message that NIC writes to indicate the limits of the `interval` value.\nMaybe the schema needs to be update to drop the `day` resolution as it can only be 1 day as the interval when 24h should suffice.",
      "labels": [
        "bug"
      ],
      "created_at": "2025-07-04T07:36:10Z",
      "closed_at": "2025-07-18T14:25:31Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/7988",
      "comments_count": 5
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 8020,
      "title": "[Bug]: v2.2.0 of chart does not appear to be released",
      "problem": "### Version\n\n5.1.0\n\n### What Kubernetes platforms are you running on?\n\nk3s\n\n### Steps to reproduce\n\nThe [Releases](https://docs.nginx.com/nginx-ingress-controller/releases/) page says v5.1.0 is out, and to use v2.2.0 of the Helm chart to install it, but:\n\n```\nError: chart \"nginx-ingress\" version \"2.2.0\" not found in https://helm.nginx.com/stable repository\n```",
      "solution": "Hi @kyrofa, \n\nAs mentioned above, for future reference please pull from OCI as this is the new prefered way.\n\nThough, the issue should be resolved. \n\n```\nhelm search repo nginx-ingress\nNAME                      \tCHART VERSION\tAPP VERSION\tDESCRIPTION             \nnginx-stable/nginx-ingress\t2.2.0        \t5.1.0      \tNGINX Ingress Controller\n```\n",
      "labels": [
        "bug",
        "waiting for response"
      ],
      "created_at": "2025-07-14T19:18:32Z",
      "closed_at": "2025-07-15T13:49:32Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/8020",
      "comments_count": 4
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 7960,
      "title": "[Bug]: example/custom-resources/cross-namespace-configuration missing secret namespace",
      "problem": "### Version\n\nedge\n\n### What Kubernetes platforms are you running on?\n\nGKE Google Cloud\n\n### Steps to reproduce\n\nWhile going through the example the VirtualServer kept coming up with error. Turns out that the VS is deployed in the `cafe` namespace, but the secret's definition does not contain the namespace, so that gets deployed to `default`.\n\nThis causes an issue where the VS can't make use of the secret, because it's in a different namespace.\n\n### Recommended fix\n\nChange the command in the [example](https://github.com/nginx/kubernetes-ingress/tree/dd0727dc4ccb1bb7e07503b509d6862e130ef387/examples/custom-resources/cross-namespace-configuration) to include the namespace:\n\n```diff\n- kubectl create -f cafe-secret.yaml\n+ kubectl create -f cafe-secret.yaml -n cafe\n```\n\nThe `cafe-secret.yaml` is a symlink, so we can't change the file contents without breaking other examples.",
      "solution": "https://github.com/nginx/kubernetes-ingress/pull/7942 its fixed!",
      "labels": [
        "bug",
        "good first issue",
        "needs triage"
      ],
      "created_at": "2025-06-25T18:35:04Z",
      "closed_at": "2025-06-30T15:29:43Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/7960",
      "comments_count": 2
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 7968,
      "title": "[Bug]: Incorrect link to installation in contributing.md",
      "problem": "### Version\n\nedge\n\n### What Kubernetes platforms are you running on?\n\nNot applicable\n\n### Steps to reproduce\n\nThe `Installation Guide` link is not working for the getting started [guide](https://github.com/nginx/kubernetes-ingress/blob/main/CONTRIBUTING.md#getting-started).",
      "solution": "fixed by https://github.com/nginx/documentation/pull/753",
      "labels": [
        "bug",
        "needs triage"
      ],
      "created_at": "2025-06-27T02:24:37Z",
      "closed_at": "2025-06-27T12:56:18Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/7968",
      "comments_count": 3
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 7278,
      "title": "[Bug]: OIDC Adds \":\" to redirect URI even though no port is specified in URL.",
      "problem": "### Version\n\n3.7.0\n\n### What Kubernetes platforms are you running on?\n\nOpenshift\n\n### Steps to reproduce\n\nWe are deploying the NGINX as usual and using the \"NGINX OIDC\" policy.\nWhen using it and diving into some of the code we saw that the redirect URI automatically adds a `:` in the redirect URI which can break it and doesn't make much sense. (In our env it's a problem since specifying ports is not allowed in redirect_uris.)\n\nAs seen in the following file on line 6-9: [internal/configs/oidc/oidc_common.conf](https://github.com/nginx/kubernetes-ingress/blob/05861e2376df5a69b46814ec64c762999ecec0d8/internal/configs/oidc/oidc_common.conf#L6)\n\nWe saw that there is mapping of the redirect URI based on `$server_port` and `$http_x_forwarded_port`.\n\nWhen using HTTPS there is no port \"specified\" thus if our domain is: `my-oidc-tests.com` it will result in the following redirect URI: `https://my-oidc-tests.com:/_codexch` which breaks our SSO provider.\n\nWe think a better solution is adding the `:` and a port only when it actually exists or even have an option to toggle it on or off.\n\nI opened under a bug because we are not sure if this is intended behaviour (the default adding of `:` into the URI or not.)\n\nThank you :)",
      "solution": "We recently introduced the integration test for OIDC in our pipeline to ensure it functions correctly in our standard environment. https://github.com/nginx/kubernetes-ingress/blob/main/tests/suite/test_oidc.py\nTherefore, I would assume the issue you are experiencing is specific to your environment, and it would be difficult for us to accommodate every possible deployment scenario. That said, would you mind sharing more information about your environment, such as which identity platform you are using?\n\n---\n\n@haywoodsh \n\nI have specified in the issue the lines of code where the mapping adds the `:` by default, it's here: [internal/configs/oidc/oidc_common.conf](https://github.com/nginx/kubernetes-ingress/blob/05861e2376df5a69b46814ec64c762999ecec0d8/internal/configs/oidc/oidc_common.conf#L6)\n\n```\nmap $http_x_forwarded_port $redirect_base {\n    \"\"      $proto://$host:$server_port;\n    default $proto://$host:$http_x_forwarded_port;\n}\n```\n\nThis means that if no server port or http_x_forwared port is specified it will still add the \":\" hardcoded hence resulting the the bad redirect URI containing a `:` in it even though no port is specified.\n\nThe OIDC works and redirects, it just adds to the redirect URI the `:` which causes problems when adding redirect URIs in our env\n\n---\n\n@benshalev849 the code you pointed out is actually [a reference implementation of oidc with nginx](https://github.com/nginxinc/nginx-openid-connect/blob/main/openid_connect_configuration.conf#L87-L89) which we use so not something we can modify and also I'm unable to repro the issue with keycloak unfortunately",
      "labels": [
        "bug",
        "waiting for response"
      ],
      "created_at": "2025-02-05T13:59:23Z",
      "closed_at": "2025-06-25T12:20:01Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/7278",
      "comments_count": 30
    },
    {
      "tech": "nginx",
      "repo": "nginxinc/kubernetes-ingress",
      "issue_number": 7866,
      "title": "[Bug]: unknown directive large_client_header_buffers in nginx 5.0.0",
      "problem": "### Version\n\nedge\n\n### What Kubernetes platforms are you running on?\n\nAKS Azure\n\n### Steps to reproduce\n\n1. Deploy using a manifest file\n2. Using configmap to add:\n  server-snippets: |\n    large_client_header_buffers: 4 16k;\n3. Nginx pod throws errors: unknown directive large_client_header_buffers",
      "solution": "> [@nghia-nguyen-pe](https://github.com/nghia-nguyen-pe) please remove the : from the snippets as snippets are raw nginx configuration\n\nThanks a lot!\nThe issue is resolved as suggested by Vepatel:\n\n```\n  server-snippets: |\n    large_client_header_buffers 4 16k;\n```",
      "labels": [
        "bug",
        "waiting for response"
      ],
      "created_at": "2025-06-05T15:51:37Z",
      "closed_at": "2025-06-05T16:04:31Z",
      "url": "https://github.com/nginx/kubernetes-ingress/issues/7866",
      "comments_count": 3
    }
  ]
}