{
  "tech": "influxdb",
  "count": 104,
  "examples": [
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26110,
      "title": "[influxdb1.x/2.x]: purger code may prevent compactions from finishing",
      "problem": "Copied from this comment:\n* https://github.com/influxdata/influxdb/pull/26089#issuecomment-2701989707\n\nRequires investigation to confirm the issues noted below.\n\n* `purger` uses a go map, which doesn't release backing memory ever - so it'll be as large as the largest set of compaction input files (which might be a lot for a full compaction). Minor issue, but could remedy by [allocating a new map when](https://github.com/influxdata/influxdb/blob/60e49d854c35ddf373531352d67633a5920a2353/tsdb/engine/tsm1/file_store.go#L1657) `len(p.files) == 0`.\n* The purger seems to suffer from a similar issue that @gwossum  identified with the retention service. It calls `tsmfile.InUse()` and then later `tsmfile.Close()` without blocking new readers in the inbetween - a time of check, time of use flaw. Therefore `tsmfile.Close()` which waits on readers to finish (`Unref()`) has an unbounded runtime. The [purger's lock is held over the close() call](https://github.com/influxdata/influxdb/blob/60e49d854c35ddf373531352d67633a5920a2353/tsdb/engine/tsm1/file_store.go#L1636-L1644), which means the lock is held for an unbounded time, which you might say is ok because this happens in the purger's \"purge\" goroutine, but the `purger.Add(files[])` [call needs to get that lock too](https://github.com/influxdata/influxdb/blob/60e49d854c35ddf373531352d67633a5920a2353/tsdb/engine/tsm1/file_store.go#L1616-L1617) and `purger.Add()` [is called synchronously](https://github.com/influxdata/influxdb/blob/60e49d854c35ddf373531352d67633a5920a2353/tsdb/engine/tsm1/file_store.go#L1036-L1037) in `Filestore.replace()` which means _replace()_ has an unbounded runtime. All this means, if i'm reading it right, that purging tsm files could freeze up compaction.\n    * One could make it `go f.purger.add(inuse)` to decouple the purger and compaction which is nice, but it'd be better to use a sync map and/or reduce the purger lock holding time and/or put tsm file closing into a goroutine and/or call `SetNewReadersBlocked` before the InUse check, like the retention service does.",
      "solution": "Could the result of such an issue result in something like the issue logged here? \n\nhttps://github.com/influxdata/influxdb/issues/25296\n\n\n---\n\n> Could the result of such an issue result in something like the issue logged here?\n> \n> [#25296](https://github.com/influxdata/influxdb/issues/25296)\n\nNot entirely sure without further exploration - do you by chance have any profiles that I can run pprof on to get a better idea of the stack traces at that time? It may add in the exploration to have some more reproducer profiles. \n\n---\n\nI have gathered some of the profile traces, but they are not traces for when the problem occurred. The problem occurred on this particular server today at 00:00. Please have a look to see if you can find anything related. I will setup a script to capture this same output next time it crashes.\n\n[cpu-profile.pb.gz](https://github.com/user-attachments/files/19165437/cpu-profile.pb.gz)\n[goroutine-dump.txt](https://github.com/user-attachments/files/19165438/goroutine-dump.txt)\n[heap-profile.pb.gz](https://github.com/user-attachments/files/19165435/heap-profile.pb.gz)\n[trace.pb.gz](https://github.com/user-attachments/files/19165436/trace.pb.gz)",
      "labels": [
        "area/tsm",
        "1.x",
        "kind/perf",
        "team/edge"
      ],
      "created_at": "2025-03-07T17:11:42Z",
      "closed_at": "2026-01-30T17:53:26Z",
      "url": "https://github.com/influxdata/influxdb/issues/26110",
      "comments_count": 6
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26150,
      "title": "Influxdb 3 Core/Enterprise 'illegal instruction' on x86_64 CPU with CPUs older than Intel Haswell",
      "problem": "https://github.com/influxdata/influxdb/blob/main/.cargo/config.toml#L21 lists:\n\n```\n[target.x86_64-unknown-linux-gnu]\nrustflags = [\n    ...\n    # Enable all features supported by CPUs more recent than haswell (2013)\n    \"-C\", \"target-cpu=haswell\",\n```\n\nI have an old test machine (Intel(R) Xeon(R) CPU E3-1230 V2 @ 3.30GHz) with up to date Ubuntu 18.04 LTS, but despite having proper glibc compatibility, it fails to run:\n\n```\n$ ./influxdb3 -V\nIllegal instruction (core dumped)\n\n$ gdb ./influxdb3\n...\nProgram received signal SIGILL, Illegal instruction.\n(gdb) run -V\n\n(gdb) x/i $pc\n=> 0x555558233124 <_ZN12clap_builder5error14Error$LT$F$GT$5print17hcabd755f43c3aba1E+8484>:\tshrx   %r12d,%eax,%r12d\n```\n\nGoogling tells me that the `shrx` instruction was introduced with the Intel BMI2 (Bit Manipulation Instruction Set 2) extension, which was first supported in Intel's Haswell microarchitecture, released in 2013.\n\nIt appears that this was introduced because we wanted the `avx2` instruction, first introduced in haswell: https://github.com/influxdata/influxdb_iox/pull/2119 states \"I think that the current setting for ROARING_ARCH of x86-64 may not be specific enough to ensure that things are built with the avx2 instructions. ~~Ivy Bridge~~ Haswell has those so I think that this should work better.\" https://github.com/influxdata/influxdb_iox/pull/2140 has additional information.\n\nIt's apparently quite intentional that we don't support CPUs older than haswell. We probably need a documentation note for this. I expect this issue to be closed but wanted to file it for posterity since others will likely run into this.\n\ncc @pauldix, @peterbarnett03, @alamb (since you wrote https://github.com/influxdata/influxdb_iox/pull/2140), @sanderson  and @jstirnaman \n\nReferences:\n* https://github.com/influxdata/influxdb_iox/pull/2119\n* https://github.com/influxdata/influxdb_iox/pull/2140",
      "solution": "@thexeos's [post](https://github.com/influxdata/influxdb/issues/26150#issuecomment-2799905129) led me to the right option to change in the code. I applied this knowledge to build a Docker version that runs on older CPUs. If you are willing to use the Docker build then this is an easy DIY fix for now:\n\n1. Clone the repo. Optionally check out a stable version (e.g. `git checkout 3.1`)\n2. Edit `.cargo/config.toml` to change `haswell` to `sandybridge`.\n3. Build the docker image using build args: `docker build -t influxdb:3-core-sandybridge --build-arg PBS_DATE=20250106 --build-arg PBS_VERSION=3.11.11 --build-arg PBS_TARGET=x86_64-unknown-linux-gnu .`\n\n    > I used the values given in the example seen in `.circleci/scripts/fetch-python-standalone.bash`. I'm not sure exactly what version numbers are used in the official releases though. In either case these values seemed to work for me.\n\nThe compile takes a while, it doesn't seem to multithread the build process very much, but the resulting Docker image should work just like the official one, but runs fine on older 64-bit CPUs!\n\n-----\n\nIt might be worth publishing this info/workaround officially - I know of many people who use older servers in homelabs or educational settings (myself included). My main \"big iron\" homelab server is a PowerEdge R720 with 24 cores, 256GB RAM and dual GPUs. I use it for AI workloads and database clustering experiments, but since its CPUs are Ivy Bridge based (Xeon E5-2697 v2) I can't run the stock image. Of course it's easy to say \"nobody needs to worry about a 13 year old CPU\" but consider that practical CPU advancements have actually slowed down quite a bit - compare today's CPUs with Ivy Bridge, then compare Ivy Bridge with the Pentium 3 (which was king of the hill - at least sometimes - in the late 90s!). \n\n---\n\n@hesperaux Follow the instructions in my post https://github.com/influxdata/influxdb/issues/26150#issuecomment-2941699164 to build an image capable of running on pre-Haswell hardware. I've tested this image successfully on my Ivy Bridge based PowerEdge R720 and it runs just fine and performs well.\n\nThe Dockerfile linked above only downloads the precompiled binary and packages it in a Docker container - that won't help with the CPU instruction issue. You need to rebuild from source to make that work, but luckily it's not difficult - just takes a bit of time.\n\n\n---\n\n@wuan thanks for the builds but I get \"unsupported media type application/vnd.oci.empty.v1+json\" when I try to pull either. I have no idea what the issue is. Can you tell me?",
      "labels": [
        "v3"
      ],
      "created_at": "2025-03-17T12:53:02Z",
      "closed_at": "2025-03-17T14:25:11Z",
      "url": "https://github.com/influxdata/influxdb/issues/26150",
      "comments_count": 29
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 27116,
      "title": "--config-toml flag is unexpected but defaults from /lib/systemd/system/influxdb3-enterprise.service",
      "problem": "<!--\n\nThank you for reporting a bug in InfluxDB IOx.\n\nHave you read the contributing seaction of the README? Please do if you haven't.\nhttps://github.com/influxdata/influxdb/blob/main/README.md\n\n* Please ask usage questions in the Influx Slack (there is an #influxdb-iox channel).\n    * https://influxdata.com/slack\n* Please don't open duplicate issues; use the search. If there is an existing issue please don't add \"+1\" or \"me too\" comments; only add comments with new information.\n* Please check whether the bug can be reproduced with tip of main.\n* The fastest way to fix a bug is to open a Pull Request.\n    * https://github.com/influxdata/influxdb/pulls\n\n-->\n\n__Steps to reproduce:__\nList the minimal actions needed to reproduce the behaviour.\n\n1. install influxdb3-enterprise\n2. enable influxdb3-enterprise with systemctl\n3. notice config file in /etc/influxdb3/influxdb3-enterprise.config is not picked up and service is restarting\n\n__Expected behaviour:__\nInfluxdb3-enterprise should start up and process requests\n\n__Actual behaviour:__\nSystemctl tries to restart several times but --config-toml is not an accepted flag.\n\n__Environment info:__\nLinux 5.15.0-164-generic x86_64\nUbuntu\n\n__Config:__\nOnly license flag with own email\n\n__Logs:__\nerror: unexpected argument '--config-toml' found",
      "solution": "```\n# /lib/systemd/system/influxdb3-enterprise.service\n[Unit]\nDescription=InfluxDB 3 Enterprise\nAfter=network-online.target\n\n[Service]\n# Type=exec needs systemd \u2265240; simple keeps RHEL7 (systemd 219) working\nType=simple\nRestart=on-failure\nKillMode=control-group\n# Use launcher to assist with migrations, etc\nExecStart=/usr/lib/influxdb3/python/bin/python3 /usr/lib/influxdb3/influxdb3-launcher --exec=/usr/bin/influxdb3 --flavor=enterprise --stamp-dir=/var/lib/influxdb3 --config-toml=/etc/influxdb3/influxdb3-enterprise.conf -- serve\n\n# Reload not supported; use restart after config changes\nExecReload=\nWorkingDirectory=/var/lib/influxdb3\nStateDirectory=influxdb3\n# Logs default to journald (StandardOutput/Error); you may optionally set\n# LOG_DESTINATION=file:/var/log/influxdb3/influxdb3.log to use LogsDirectory.\nLogsDirectory=influxdb3\nStandardOutput=journal\nStandardError=journal\nLimitNOFILE=65536\nRemoveIPC=true\n\n#\n# Basic security\n#\nUser=influxdb3\nGroup=influxdb3\nSupplementaryGroups=\nUMask=0027\n\n#\n# Additional security options commonly safe for use. In general, these assume\n# User/Group/Supplementary groups are set to an unprivileged user and as a\n# result, we do not have to exhaustively add directives for blocking privileged\n# actions since they are already blocked by the kernel.\n# https://www.freedesktop.org/software/systemd/man/latest/systemd.exec.html\n#\n# Disallow gaining privileges via setuid/setgid/fscaps/residual caps\nNoNewPrivileges=true\nRestrictSUIDSGID=true\nCapabilityBoundingSet=\nAmbientCapabilities=\n# Allow only pseudo devices with no host mount propagation\nPrivateDevices=true\n# Use separate SysV IPC from host\nPrivateIPC=true\n# Use separate /tmp and /var/tmp from host (implies PrivateMounts=)\nPrivateTmp=true\n# Use separate /dev/shm (override with size= to limit size too)\nTemporaryFileSystem=/dev/shm:mode=1777\n# Disallow access to /home (implies PrivateMounts=)\nProtectHome=true\n# Disallow access to the kernel log ring buffer (needed if PrivateDevices=false)\nProtectKernelLogs=true\n# Make host files read-only\nProtectSystem=strict\n# Limit to IPv4, IPv6 and AF_UNIX (eg, disallow AF_NETLINK, AF_PACKET, etc).\n# AF_UNIX is required for DNS resolution but is a tradeoff. In practice:\n#  a) named sockets are generally available if the socket file's permissions\n#     allow it, in which case, the serving process must mediate access.\n#     InaccessiblePaths will be used to disallow access to well-known services\n#     with world-writable permissions\n#  b) abstract sockets can be connected to and the serving process must mediate\n#     access. AppArmor or SELinux can be used to mediate these further\n#  c) anonymous sockets aren't a concern since this process won't have the fd\nRestrictAddressFamilies=AF_INET AF_INET6 AF_UNIX\n# Disallow well-known system services' sockets\nInaccessiblePaths=-/run/avahi-daemon -/run/cups -/run/snapd.socket -/run/dbus/system_bus_socket\n# Disallow well-known desktop services' sockets\nInaccessiblePaths=-/tmp/.X11-unix -/tmp/.XIM-unix -/tmp/.ICE-unix -/tmp/.font-unix -/run/user\n# Disallow use of all namespaces\nRestrictNamespaces=true\n# Limit syscalls (systemd-analyze syscall-filter) to reduce kernel surface.\n# See allowed syscalls for this service with:\n#  $ PAGER= systemctl show -p SystemCallFilter influxdb3.service\nSystemCallArchitectures=native\nSystemCallErrorNumber=EPERM\n# Allowed syscalls:\nSystemCallFilter=@system-service\n# Disallow often abused unprivileged syscalls from @system-service:\n# io_uring_setup - create an io_uring instance\n# keyctl - use kernel keyrings\n# userfaultfd - page faults to fd (often disabled in vm.unprivileged_userfaultfd)\nSystemCallFilter=~io_uring_setup keyctl userfaultfd\nLockPersonality=true\n# Allow creating memory mappings that are writable and executable at the same\n# time, which is required by the InfluxDB python processing engine.\nMemoryDenyWriteExecute=false\n\n#\n# Security options known to affect certain plugins\n#\n# Hide processes not owned by this user (see User=, above); note, ProcSubset\n# intentionally not set\nProtectProc=invisible\n\n#\n# Site-specific security controls\n#\n\n# Network filtering via BPF. IMPORTANT: rule order is:\n#  1) Access # is granted if matches entry in IPAddressAllow\n#  2) otherwise access is denied if matches entry in IPAddressDeny\n#  3) otherwise access is granted\n# For egress, matches against sender; for ingress, matches against receiver.\n# This filtering only matches on IP addresses, not ports (use firewall\n# tools/cloud security groups if you need more flexibility).\n# https://www.freedesktop.org/software/systemd/man/latest/systemd.resource-control.html#Network%20Accounting%20and%20Control\n#\n# Eg: limit communications to only localhost (all ports)\n#IPAddressDeny=any\n#IPAddressAllow=localhost\n#\n# Eg: limit to only public IP ranges\n#IPAddressDeny=0.0.0.0/32      # 0.0.0.0 treated as 127.0.0.1\n#IPAddressDeny=127.0.0.0/8     # IPv4 loopback\n#IPAddressDeny=10.0.0.0/8      # IPv4 internal (RFC1918)\n#IPAddressDeny=172.16.0.0/12   # IPv4 internal (RFC1918)\n#IPAddressDeny=192.168.0.0/16  # IPv4 internal (RFC1918)\n#IPAddressDeny=169.254.0.0/16  # IPv4 link-local (RFC3927)\n#IPAddressDeny=224.0.0.0/4     # IPv4 multicast\n#IPAddressDeny=::1/128         # IPv6 loopback\n#IPAddressDeny=fe80::/64       # IPv6 link-local\n#IPAddressDeny=fc00::/7        # IPv6 unique local addr\n#IPAddressDeny=ff00::/8        # IPv6 multicast\n\n[Install]\nWantedBy=multi-user.target\n```\n\n```\nJan 13 20:52:12 lemony-lemon systemd[1]: Started InfluxDB 3 Enterprise.\nJan 13 20:52:12 lemony-lemon python3[1051017]: E: Required configuration not found. At least one of the following TOML key groups must be set:\nJan 13 20:52:12 lemony-lemon python3[1051017]: E: - All of: object-store, license-file\nJan 13 20:52:12 lemony-lemon python3[1051017]: E: - All of: object-store, license-email, license-type\nJan 13 20:52:12 lemony-lemon systemd[1]: influxdb3-enterprise.service: Main process exited, code=exited, status=1/FAILURE\nJan 13 20:52:12 lemony-lemon systemd[1]: influxdb3-enterprise.service: Failed with result 'exit-code'.\nJan 13 20:52:12 lemony-lemon systemd[1]: influxdb3-enterprise.service: Scheduled restart job, restart counter is at 2.\nJan 13 20:52:12 lemony-lemon systemd[1]: Stopped InfluxDB 3 Enterprise.\nJan 13 20:52:12 lemony-lemon systemd[1]: Started InfluxDB 3 Enterprise.\nJan 13 20:52:13 lemony-lemon python3[1051021]: E: Required configuration not found. At least one of the following TOML key groups must be set:\nJan 13 20:52:13 lemony-lemon python3[1051021]: E: - All of: object-store, license-file\nJan 13 20:52:13 lemony-lemon python3[1051021]: E: - All of: object-store, license-email, license-type\nJan 13 20:52:13 lemony-lemon systemd[1]: influxdb3-enterprise.service: Main process exited, code=exited, status=1/FAILURE\nJan 13 20:52:13 lemony-lemon systemd[1]: influxdb3-enterprise.service: Failed with result 'exit-code'.\nJan 13 20:52:13 lemony-lemon systemd[1]: influxdb3-enterprise.service: Scheduled restart job, restart counter is at 3.\nJan 13 20:52:13 lemony-lemon systemd[1]: Stopped InfluxDB 3 Enterprise.\nJan 13 20:52:13 lemony-lemon systemd[1]: Started InfluxDB 3 Enterprise.\nJan 13 20:52:13 lemony-lemon python3[1051034]: E: Required configuration not found. At least one of the following TOML key groups must be set:\nJan 13 20:52:13 lemony-lemon python3[1051034]: E: - All of: object-store, license-file\nJan 13 20:52:13 lemony-lemon python3[1051034]: E: - All of: object-store, license-email, license-type\nJan 13 20:52:13 lemony-lemon systemd[1]: influxdb3-enterprise.service: Main process exited, code=exited, status=1/FAILURE\nJan 13 20:52:13 lemony-lemon systemd[1]: influxdb3-enterprise.service: Failed with result 'exit-code'.\n```",
      "labels": [
        "v3"
      ],
      "created_at": "2026-01-13T20:18:17Z",
      "closed_at": "2026-01-16T16:32:26Z",
      "url": "https://github.com/influxdata/influxdb/issues/27116",
      "comments_count": 20
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26835,
      "title": "Version reported by 1.12.2-1 from deb packages incorrect",
      "problem": "<!--\n\nThank you for reporting a bug in InfluxDB IOx.\n\nHave you read the contributing section of the README? Please do if you haven't.\nhttps://github.com/influxdata/influxdb/blob/main/README.md\n\n* Please ask usage questions in the Influx Slack (there is an #influxdb-iox channel).\n    * https://influxdata.com/slack\n* Please don't open duplicate issues; use the search. If there is an existing issue please don't add \"+1\" or \"me too\" comments; only add comments with new information.\n* Please check whether the bug can be reproduced with tip of main.\n* The fastest way to fix a bug is to open a Pull Request.\n    * https://github.com/influxdata/influxdb/pulls\n\n-->\n\nVersion number reported in deb/apt installed influxdb v1.12.2-1 is reported incorrectly.\n\n__Steps to reproduce:__\nList the minimal actions needed to reproduce the behaviour.\n\n1. Install v1.11.8-2\n    1. `sudo apt-get install influxdb=1.11.8-2`\n3. Access influx shell from host   \n    1. `$ influx`\n    2. `Connected to http://localhost:8086 version v1.11.8`\n    3. `InfluxDB shell version: v1.11.8`\n3. Upgrade influxdb to v1.12.2-1\n    1. `sudo apt-get install influxdb=1.12.2-1`\n    2. `lvl=info msg=\"InfluxDB starting\" log_id=0zCKPfkG000 version=1.x-c9a9af2d63 branch=1.12 commit=95e86be3f4a18bf9b4a2a63b660109bc291b0775`\n4. Access influx shell from host\n    1. `$ influx`\n    2. `Connected to http://localhost:8086 version 1.x-c9a9af2d63`\n    3. `InfluxDB shell version: 1.x-c9a9af2d63`\n \n__Expected behaviour:__\nVersion number to display accurately.\n```\n$ influx\nConnected to http://localhost:8086 version v1.12.2\nInfluxDB shell version: v1.12.2\n```\n\n__Actual behaviour:__\nVersion number shows a build or commit identifier.\n```\n$ influx\nConnected to http://localhost:8086 version 1.x-c9a9af2d63\nInfluxDB shell version: 1.x-c9a9af2d63\n```\n\n__Environment info:__\n* Ubuntu 22.04, both amd64 and arm64.\n* Assume consistent across other Ubuntu LTS/Debian releases\n\n__Config:__\nDefault values\n\n<!-- The following sections are only required if relevant. -->\n\n__Logs:__\n```\nlvl=info msg=\"InfluxDB starting\" log_id=0zCOISd0000 version=v1.11.8 branch=unknown commit=199a607e1c7a5a687f96a636bb3bab9a61e4ae31\nlvl=info msg=\"InfluxDB starting\" log_id=0zCKPfkG000 version=1.x-c9a9af2d63 branch=1.12 commit=95e86be3f4a18bf9b4a2a63b660109bc291b0775\n```\n",
      "solution": "This appears to be fixed now:\n```\n$ influx --version\nInfluxDB shell version: 1.12.2\n```\n\n@devanbenz - can this be closed?",
      "labels": [],
      "created_at": "2025-09-25T15:42:44Z",
      "closed_at": "2025-12-18T16:21:48Z",
      "url": "https://github.com/influxdata/influxdb/issues/26835",
      "comments_count": 3
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26698,
      "title": "[1.x] ubuntu packaging - the service init.sh script for SystemV requires root but it runs `buildtsi` which shouldn't be run with root",
      "problem": "*Note*: performing this in a docker container means I don't have systemctl available and i am violating container philosophy somewhat by the entry point being `bash` and then installing and running influxd in the container.\n\n**NOTE**: This only impacts the SystemV init.sh script. The [systemd script](https://github.com/influxdata/influxdb/blob/61f965a4b6145342611dd9aa16f1c14cf47f161b/.circleci/packages/influxdb/fs/usr/lib/influxdb/scripts/influxd-systemd-start.sh#L1) doesn't have this issue.\n\n\nI noticed an inconsistency when running the 1.x ubuntu packaging. The exact file I encountered the issue on is [1.11.8 for arm](https://download.influxdata.com/influxdb/releases/influxdb-1.11.8-arm64.deb).\n\nI'm using a ubuntu container. I run `service influxdb start` which let's me know `You must be root to run this script` and terminates. so I run `sudo service influxdb start`  but then the `buildtsi` tool runs and says.\n\n```\nYou are currently running as root. This will build your\nindex files with root ownership and will be inaccessible\nif you run influxd as a non-root user. You should run\nbuildtsi as the same user you are running influxd.\nAre you sure you want to continue? (y/N):\n```\n\nThe init script passes `yes` to build tsi so it then automatically creates the tsi files with root ownership which influxdb running as the influxdb user (which is the package's default) can't read those files and fails to start. \n\nI worked around the issue by commenting out `buildtsi` as that is important for version upgrades. Influxdb will automatically build tsi and series index files if they are missing. `buildtsi` forces rebuild if they are already present.\n\nHere are the sections of the init.sh script in question.\n\nhttps://github.com/influxdata/influxdb/blob/61f965a4b6145342611dd9aa16f1c14cf47f161b/.circleci/packages/influxdb/fs/usr/lib/influxdb/scripts/init.sh#L27-L31\n\nhttps://github.com/influxdata/influxdb/blob/61f965a4b6145342611dd9aa16f1c14cf47f161b/.circleci/packages/influxdb/fs/usr/lib/influxdb/scripts/init.sh#L156-L164",
      "solution": "I think the systemV script can be fixed by running buildtsi this way:\n\n```\n        sudo -u influxdb /usr/bin/influx_inspect buildtsi -compact-series-file \\\n            -datadir \"${DATA_DIR}\"                                  \\\n            -waldir  \"${WAL_DIR}\"\n```\n\nThe script requires root so sudo should work and then the script runs as the influxdb user and generates files owned by the influxdb user (just like the systemd script does).\n\n---\n\nThis was fixed by https://github.com/influxdata/influxdb/pull/26699",
      "labels": [
        "1.x"
      ],
      "created_at": "2025-08-18T17:42:46Z",
      "closed_at": "2025-12-18T14:21:15Z",
      "url": "https://github.com/influxdata/influxdb/issues/26698",
      "comments_count": 2
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 24799,
      "title": "Critical cyber vulnerabilities (CVE) on latest of version 2 (vault)",
      "problem": "On version 2.7.5, some vulnerabilities are critical:\r\n\r\n![image](https://github.com/influxdata/influxdb/assets/16835638/803d7dd7-ca82-4db8-9688-2f73d5319fd0)\r\n\r\nThey are visible on dockerhub also so I imagine it is well known...\r\n\r\n![image](https://github.com/influxdata/influxdb/assets/16835638/ef3f1c35-b8cf-456d-b2b7-5d0381626fba)\r\n\r\nDo you plan to resolve these issues? Are they false positive?\r\n\r\n_Sorry it is not a bug at all but I don't know how to have a reactive response :'(_",
      "solution": "@omarmarquez - these CVEs are all already addressed in influxdb 2.7.10. Eg:\r\n```\r\n$ apt-get download influxdb2\r\n$ ar x ./influxdb2_2.7.10-1_amd64.deb\r\n$ tar -zxvf ./data.tar.gz\r\n...\r\n./usr/bin/influxd\r\n...\r\n$ ./usr/bin/influxd version\r\nInfluxDB v2.7.10 (git: f302d9730c) build_date: 2024-08-16T20:19:28Z\r\n$ go version ./usr/bin/influxd\r\n./usr/bin/influxd: go1.21.12\r\n```\r\n- CVE-2024-24790 - fixed in 1.22.4 and 1.21.11\r\n- CVE-2023-24540 - fixed in 1.20.4 and 1.19.9\r\n- CVE-2023-24538 - fixed in 1.20.3 and 1.19.8\n\n---\n\n> > @omarmarquez - these CVEs are all already addressed in influxdb 2.7.10. Eg:\r\n> \r\n> Thanks @jdstrand , What about [CVE-2024-21626](https://github.com/advisories/GHSA-xr7r-f8xq-vfvv) ?\r\n\r\nIt does not apply to influxdb. `github.com/opencontainers/runc` is an indirect dependency used in tests and the issue is when `runc` is used to drive containers, which influxdb does not do.",
      "labels": [],
      "created_at": "2024-03-21T14:44:27Z",
      "closed_at": "2025-12-18T14:19:32Z",
      "url": "https://github.com/influxdata/influxdb/issues/24799",
      "comments_count": 9
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 24883,
      "title": "InfluxDB 1.8.10 Not Responding, Too many open files Error",
      "problem": "Hi All and @davidby-influx \r\n\r\n\r\nI am observing below issue for my InfluxDB 1.8.10.\r\n\r\n**Issue**: InfluxDB not responding and stopping. Getting too many open files error.\r\n\r\n**Influxdb.conf** : every thing is default except cache_max_memory: 32GB\r\n\r\nSystem Environment:\r\nOperating System: Ubuntu 22.04 on Azure\r\nInfluxDB Version: 1.8.10\r\nCPU: 32vCPU\r\nRAM: 128GB\r\nDisk Type: SSD\r\ncache-max-memory: 32GB\r\nIndex Type: TSM\r\n\r\n**ulimit -a**\r\ncore file size          (blocks, -c) 0\r\ndata seg size           (kbytes, -d) unlimited\r\nscheduling priority             (-e) 0\r\nfile size               (blocks, -f) unlimited\r\npending signals                 (-i) 515580\r\nmax locked memory       (kbytes, -l) 65536\r\nmax memory size         (kbytes, -m) unlimited\r\nopen files                      (-n) 65535\r\npipe size            (512 bytes, -p) 8\r\nPOSIX message queues     (bytes, -q) 819200\r\nreal-time priority              (-r) 0\r\nstack size              (kbytes, -s) 8192\r\ncpu time               (seconds, -t) unlimited\r\nmax user processes              (-u) 515580\r\nvirtual memory          (kbytes, -v) unlimited\r\nfile locks                      (-x) unlimited\r\n\r\n\r\n **sysctl -p**\r\nvm.max_map_count = 262144\r\nfs.file-max = 524288\r\n\r\n**cat /etc/security/limits.conf**\r\n* soft  nproc           65535\r\n* hard  nproc           65535\r\n* soft  nofile          65535\r\n* hard  nofile          65535\r\nroot soft  nofile          65535\r\nroot hard  nofile          65535\r\n\r\n\r\n**Error in Logfile**:\r\n\r\nApr  4 15:17:21 influx-opc influxd-systemd-start.sh[2478]: ts=2024-04-04T09:47:21.565072Z lvl=info msg=\"Error writing snapshot from compactor\" log_id=0o1m93RG000 engine=tsm1 trace_id=0oM7wUN0001 op_name=tsm1_cache_snapshot error=\"compaction in progress: open /data/influx-data/data/Datastore2623/autogen/121662/000005994-000000001.tsm.tmp: too many open files\"\r\nApr  4 15:17:21 influx-opc influxd-systemd-start.sh[2478]: ts=2024-04-04T09:47:21.565104Z lvl=info msg=\"Cache snapshot (end)\" log_id=0o1m93RG000 engine=tsm1 trace_id=0oM7wUN0001 op_name=tsm1_cache_snapshot op_event=end op_elapsed=0.181ms\r\nApr  4 15:17:21 influx-opc influxd-systemd-start.sh[2478]: ts=2024-04-04T09:47:21.565113Z lvl=info msg=\"Error writing snapshot\" log_id=0o1m93RG000 engine=tsm1 error=\"compaction in progress: open /data/influx-data/data/Datastore2623/autogen/121662/000005994-000000001.tsm.tmp: too many open files\"\r\nApr  4 15:17:21 influx-opc influxd-systemd-start.sh[2478]: ts=2024-04-04T09:47:21.565415Z lvl=info msg=\"Cache snapshot (start)\" log_id=0o1m93RG000 engine=tsm1 trace_id=0oM7wUNG000 op_name=tsm1_cache_snapshot op_event=start\r\nApr  4 15:17:21 influx-opc influxd-systemd-start.sh[2478]: ts=2024-04-04T09:47:21.565439Z lvl=info msg=\"Cache snapshot (end)\" log_id=0o1m93RG000 engine=tsm1 trace_id=0oM7wUNG000 op_name=tsm1_cache_snapshot op_event=end op_elapsed=0.027ms\r\nApr  4 15:17:21 influx-opc influxd-systemd-start.sh[2478]: ts=2024-04-04T09:47:21.565444Z lvl=info msg=\"Error writing snapshot\" log_id=0o1m93RG000 engine=tsm1 error=\"error opening new segment file for wal (1): close /data/influx-data/wal/Datastore2623/autogen/121589/_00001.wal: file already closed\"\r\nApr  4 15:17:21 influx-opc influxd-systemd-start.sh[2478]: ts=2024-04-04T09:47:21.566183Z lvl=info msg=\"failed to store statistics\" log_id=0o1m93RG000 service=monitor error=\"engine: error rolling WAL segment: error opening new segment file for wal (2): close /data/influx-data/wal/_internal/monitor/120287/_12279.wal: file already closed\"\r\nApr  4 15:17:21 influx-opc influxd-systemd-start.sh[2478]: ts=2024-04-04T09:47:21.566190Z lvl=info msg=\"Cache snapshot (end)\" log_id=0o1m93RG000 engine=tsm1 trace_id=0oM7wU3W000 op_name=tsm1_cache_snapshot op_event=end op_elapsed=80.186ms\r\n.\r\n.\r\n.\r\npr  4 15:17:21 influx-opc influxd-systemd-start.sh[2478]: ts=2024-04-04T09:47:21.633456Z lvl=info msg=\"failed to store statistics\" log_id=0o1m93RG000 service=monitor error=\"engine: error rolling WAL segment: error opening new segment file for wal (2): close /data/influx-data/wal/_internal/monitor/120287/_12279.wal: file already closed\"\r\nApr  4 15:17:21 influx-opc influxd-systemd-start.sh[2478]: ts=2024-04-04T09:47:21.633502Z lvl=info msg=\"Terminating storage of statistics\" log_id=0o1m93RG000 service=monitor\r\nApr  4 15:17:21 influx-opc influxd-systemd-start.sh[2478]: ts=2024-04-04T09:47:21.633530Z lvl=info msg=\"Terminating precreation service\" log_id=0o1m93RG000 service=shard-precreation\r\nApr  4 15:17:21 influx-opc influxd-systemd-start.sh[2478]: ts=2024-04-04T09:47:21.633549Z lvl=info msg=\"Terminating continuous query service\" log_id=0o1m93RG000 service=continuous_querier\r\nApr  4 15:17:21 influx-opc influxd-systemd-start.sh[2478]: ts=2024-04-04T09:47:21.642937Z lvl=info msg=\"Closing retention policy enforcement service\" log_id=0o1m93RG000 service=retention\r\nApr  4 15:17:21 influx-opc influxd-systemd-start.sh[2478]: ts=2024-04-04T09:47:21.686959Z lvl=info msg=\"TSM compaction (start)\" log_id=0o1m93RG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0oM7wUqW000 op_name=tsm1_compact_group db_shard_id=106896 op_event=start\r\nApr  4 15:17:21 influx-opc influxd-systemd-start.sh[2478]: ts=2024-04-04T09:47:21.686992Z lvl=info msg=\"Beginning compaction\" log_id=0o1m93RG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0oM7wUqW000 op_name=tsm1_compact_group db_shard_id=106896 tsm1_files_n=2\r\nApr  4 15:17:21 influx-opc influxd-systemd-start.sh[2478]: ts=2024-04-04T09:47:21.687011Z lvl=info msg=\"Compacting file\" log_id=0o1m93RG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0oM7wUqW000 op_name=tsm1_compact_group db_shard_id=106896 tsm1_index=0 tsm1_file=/data/influx-data/data/Datastore2518/autogen/106896/000000001-000000001.tsm\r\nApr  4 15:17:21 influx-opc influxd-systemd-start.sh[2478]: ts=2024-04-04T09:47:21.687017Z lvl=info msg=\"Compacting file\" log_id=0o1m93RG000 engine=tsm1 tsm1_strategy=full tsm1_optimize=false trace_id=0oM7wUqW000 op_name=tsm1_compact_group db_shard_id=106896 tsm1_index=1 tsm1_file=/data/influx-data/data/Datastore2518/autogen/106896/000000003-000000001.tsm\r\n\r\nAny help/suggestions are appreciated in advance.\r\n\r\nWith Regards",
      "solution": "Not sure if following is applicable to me too : #22824\r\n\r\nAnyway i will try this too, but any suggestions/help is appreciated \r\n\r\nbelow is [monitor] setting: \r\n[monitor]\r\n  --#Whether to record statistics internally.\r\n  --# store-enabled = true\r\n\r\n -- # The destination database for recorded statistics\r\n -- # store-database = \"_internal\"\r\n\r\n--  # The interval at which to record statistics\r\n  --# store-interval = \"10s\"\r\n\r\n--###\r\n--### [http]\r\n\n\n---\n\nIssue has been resolved by increasing fs.file-max,  nofile, nr_open and LimitOnFile. Thank you @davidby-influx  once again.",
      "labels": [],
      "created_at": "2024-04-04T19:00:57Z",
      "closed_at": "2024-04-12T13:50:16Z",
      "url": "https://github.com/influxdata/influxdb/issues/24883",
      "comments_count": 3
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26997,
      "title": "influxdata-archive-keyring RPM installs .repo file, breaking system updates for anyone in restricted network environments",
      "problem": "Description:\ninfluxdata-archive-keyring 2025.09.11-1 appears to include and place a repository configuration file under /etc/yum.repos.d/influxdata.repo\n\nExpected Behavior:\nIn general, RPM keyring packages should only populate trust stores, never actual repository configuration files and in worst case such changes should only be handled by a release package.\n\nIssue:\nBy bundling a .repo file into the keyring package, any local repository management strategy gets overwritten by a well intentioned quality of life update.\n\nIn many enterprise environments, servers reside on restricted networks with no direct internet access and rely on internal mirrors and or vetted content sources in .repo files that follow standard conventions.\n\nImpact:\nIn short, when influxdata-archive-keyring installs a new influxdata.repo pointing to the public https://repos.influxdata.com, all subsequent dnf/yum operations in such an enterprise environment will fail.\n\nCommentary:\ninfluxdata-archive-keyring should only install the GPG keys, not create or modify files in /etc/yum.repos.d\n\nIf a repo file must be provided, it should be in a separate release package that the user/org explicitly chooses to install rather than a forced dependency of a keyring.",
      "solution": "> Please consider decoupling keys, repos and \"automation\" into separate opt-in components so we can choose the level of magic appropriate for our environments.\n\nKey rotations are always difficult for some people.\n\nThe last forced key rotation almost 3 years ago (due to the CircleCI issue where they leaked all customer's envs and private keys) showed that we needed a better way to manage keys because everyone complained: users of deb, rpm, docker and anyone doing automation. As described in the [blog](https://www.influxdata.com/blog/package-signing-key-rotation/), we wanted to update our processes for security (ie, regular rotations) and usability. For usability, we designed the primary key with signing subkeys approach since a GPG key's fingerprint will be for its primary key and you can add/revoke signing subkeys as needed without changing the key's fingerprint (and you of course can't change the sub key without having the primary key's private key). Once everyone adjusts their automation, apt/yum configuration, docker files, etc to use this new key, then we have an opportunity to be able to rotate the key automatically so long as we sign the new package delivering new key material with the old key (this is all common practice with Linux distros). With just the primary key and signing subkeys approach, Docker and non-deb/rpm automated installs using the key can now be rotated securely and without interruption.\n\nThe problem with deb and rpm installs as that the packages themselves do not automatically refresh the signing key, so people can break in two different ways: 1) when the signing subkey expires and 2) when we rotate the signing subkey and start signing with a new key. We wanted to address this situation and so we looked around at how Linux distros and other 3rd party repos handled this sort of thing. Some vendors do nothing to help the user, which we learned 3 years ago is not an option. Some ship key material and apt/yum configuration in the same package as the software... we strongly considered this but opted to not do this precisely because we knew that some users would want custom configuration or to opt out so we chose a separate keyring approach. We considering having users opt into the keyring package, but as evidenced from this and the prior rotation, we can't reach all users through notifications (blog, package issues, slack, discourse, discord, etc) and as a result, most deb/rpm users would find themselves in a different but equivalent situation where their installs/upgrades fail due to the key expiring or what they are installing needs the new key.\n\nSo, we were left with the decision to either: a) do nothing extra for deb/rpm users, b) to have users opt into a keyring package, c) adjust packaging to use the keyring package by default with the ability to opt out.\n\nGetting back to my first point: key rotations are always difficult for some people. If we did nothing for deb/rpm users then most deb/rpm users (including people like those commenting in this thread) would experience difficulty this time and whenever we rotated in the future. If we chose opting in, then most deb/rpm users (including people like those commenting in this thread) would experience difficulty this time (because they didn't know to opt in), like they did already. By changing the deb/rpm packaging, we knew many users would be able to avoid difficulty this time (and going forward). We also knew some people (ie, people like those commenting in this thread) would unfortunately still experience difficulty (albeit different from the key expiring/not being present), but the packaging allows them to configure their systems in a way to avoid it going forward.\n\nConsidering the comments in this thread and the points made in this comment, there are currently no plans to change away from the overall approach as more users would be affected by key rotations than they will be now. However and importantly, it's clear that there is an opportunity to improve the documentation on how to most effectively configure the system to use a mirror.",
      "labels": [],
      "created_at": "2025-11-27T12:05:08Z",
      "closed_at": "2025-12-08T22:46:59Z",
      "url": "https://github.com/influxdata/influxdb/issues/26997",
      "comments_count": 14
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26993,
      "title": "[v3][core] InfluxDB3: datafusion runtime hanging; process killed",
      "problem": "__Steps to reproduce:__\nList the minimal actions needed to reproduce the behaviour.\n\n1. Collect a 10 GB Icinga2 metrics database (created by InfluxDBWriter)\n2. Run `SHOW TAG VALUES FROM icinga WITH KEY = hostname`\n\n__Expected behaviour:__\nI get the data back.\n\n__Actual behaviour:__\nInfluxDB3 crashes with `2025-11-24T15:48:51.933255Z DEBUG tokio_watchdog: tokio starts hanging runtime=\"datafusion\"`\n\nNote:\n* We've had several OOM crashes in the past. I'm not sure if this can lead to data corruption. I have raised the available memory though and currently InfluxDB does not OOM anymore.\n* The setup worked fine for ca. 2 Weeks, *however* since the data retention is ~62 days the amout of data is still growing and hasn't reached a steady state yet (currently ca. 10 GB). Therefore an explanation why this worked before and now doesn't anymore could be the growing database size.\n\n__Environment info:__\n\n* Docker image `dockerhub.vshn.net/library/influxdb:3-core` running on K8s/OpenShift\n* imageID: 'dockerhub.vshn.net/library/influxdb@sha256:0c0b5c6f96799a0bd0a086199285fad1d8c637634d67c173de9a21dbc5652268'\n\n__Config:__\n\n* File system storage\n* Start command: `bash -c 'mkdir -p /tmp/plugins/.env/bin; touch /tmp/plugins/.env/bin/activate; influxdb3 serve --node-id node0 --object-store file --data-dir /data --plugin-dir /tmp/plugins'` (workarounds required to make image work on OpenShift)\n* INFLUXDB3_AUTH_TOKEN=REDACTED\n* INFLUXDB3_QUERY_FILE_LIMIT=10000 (we want to be able to reliably query a range of 30 days. We've tested this setting and it worked, albeit with a smaller database)\n* INFLUXDB3_EXEC_MEM_POOL_BYTES=2147483648 (I've tried 4 GB, no difference)\n* LOG_FILTER=debug\n* Single icinga2 DB, retention set to 62 days\n\n\n__Logs:__\n\nNon-verbose log output:\n```\n2025-11-24T14:58:57.886839Z  INFO influxdb3_wal::object_store: flushing WAL buffer to object store host=\"node0\" n_ops=1 min_timestamp_ns=1763996311000000000 max_timestamp_ns=1763996337000000000 wal_file_number=1029917\n2025-11-24T14:59:01.871500Z  INFO iox_query::query_log: query when=\"received\" id=5dd3c32f-a20d-4e91-8aa0-be3fb44a7437 namespace_id=11 namespace_name=\"icinga2\" query_type=\"influxql\" query_text=SHOW TAG VALUES FROM icinga WITH KEY = hostname query_params=Params { } issue_time=2025-11-24T14:59:01.871491005+00:00 success=false running=true cancelled=false\n2025-11-24T14:59:01.871574Z  INFO iox_v1_query_api::handler: InfluxQL request planning namespace_name=icinga2 query=SHOW TAG VALUES FROM icinga WITH KEY = hostname trace=\"\" variant=\"influxql\" request_protocol=\"v1_http_query\"\n2025-11-24T14:59:01.910804Z  WARN datafusion::physical_planner: Physical input schema should be the same as the one converted from logical input schema, but did not match for logical plan:\nProjection: icinga.hostname\n  Filter: icinga.time >= TimestampNanosecond(1763909941874974943, None)\n    TableScan: icinga projection=[hostname, time], partial_filters=[icinga.time >= TimestampNanosecond(1763909941874974943, None)]\n2025-11-24T14:59:01.911123Z  WARN iox_query::physical_optimizer::dedup::split: cannot split dedup operation, fanout too wide n_split=133 max_dedup_split=100\n2025-11-24T14:59:01.921736Z  INFO iox_query::query_log: query when=\"planned\" id=5dd3c32f-a20d-4e91-8aa0-be3fb44a7437 namespace_id=11 namespace_name=\"icinga2\" query_type=\"influxql\" query_text=SHOW TAG VALUES FROM icinga WITH KEY = hostname query_params=Params { } issue_time=2025-11-24T14:59:01.871491005+00:00 partitions=142 parquet_files=271 deduplicated_partitions=142 deduplicated_parquet_files=271 plan_duration_secs=0.049970371 success=false running=true cancelled=false\n2025-11-24T14:59:01.921858Z  INFO iox_query::query_log: query when=\"permit\" id=5dd3c32f-a20d-4e91-8aa0-be3fb44a7437 namespace_id=11 namespace_name=\"icinga2\" query_type=\"influxql\" query_text=SHOW TAG VALUES FROM icinga WITH KEY = hostname query_params=Params { } issue_time=2025-11-24T14:59:01.871491005+00:00 partitions=142 parquet_files=271 deduplicated_partitions=142 deduplicated_parquet_files=271 plan_duration_secs=0.049970371 permit_duration_secs=0.000395557 success=false running=true cancelled=false\n```\n\nInfluxDB stops at this point (process exits, *not* OOM or similar)\n\nNote that there are warnings associated with this problem (but no errors). I've looked at the warnings but Google doesn't really bring something up for them and I don't understand what they really mean or whether they're indicating an actual problem.\n\n\n\n\n\n\n\nThe debug output end (full debug output available privately on request):\n```\n2025-11-24T15:48:51.800898Z DEBUG object_store_metrics::log: object store operation store_type=\"main\" op=\"get_opts\" result=\"success\" location=\"node0/dbs/11/2/2025-11-23/16-00/0001028730.parquet\" duration_secs=0.000324657 bytes=9737 headers_secs=0.000324657 first_byte_secs=0.000324657\n2025-11-24T15:48:51.802512Z DEBUG datafusion_datasource_parquet::opener: DataSourceExec Ignoring unknown extension specified for node0/dbs/11/2/2025-11-23/16-00/0001028730.parquet\n2025-11-24T15:48:51.802591Z DEBUG datafusion_datasource_parquet::page_filter: Use filter and page index to create RowSelection RowSelection { selectors: [RowSelector { row_count: 650, skip: false }] } from predicate: BinaryExpr { left: BinaryExpr { left: Column { name: \"time_null_count\", index: 1 }, op: NotEq, right: Column { name: \"row_count\", index: 2 }, fail_on_overflow: false }, op: And, right: BinaryExpr { left: Column { name: \"time_max\", index: 0 }, op: GtEq, right: Literal { value: TimestampNanosecond(1763912930515309301, None), field: Field { name: \"lit\", data_type: Timestamp(Nanosecond, None), nullable: false, dict_id: 0, dict_is_ordered: false, metadata: {} } }, fail_on_overflow: false }, fail_on_overflow: false }\n2025-11-24T15:48:51.805294Z DEBUG datafusion_datasource_parquet::opener: DataSourceExec Ignoring unknown extension specified for node0/dbs/11/2/2025-11-23/16-20/0001028730.parquet\n2025-11-24T15:48:51.805429Z DEBUG datafusion_datasource_parquet::page_filter: Use filter and page index to create RowSelection RowSelection { selectors: [RowSelector { row_count: 650, skip: false }] } from predicate: BinaryExpr { left: BinaryExpr { left: Column { name: \"time_null_count\", index: 1 }, op: NotEq, right: Column { name: \"row_count\", index: 2 }, fail_on_overflow: false }, op: And, right: BinaryExpr { left: Column { name: \"time_max\", index: 0 }, op: GtEq, right: Literal { value: TimestampNanosecond(1763912930515309301, None), field: Field { name: \"lit\", data_type: Timestamp(Nanosecond, None), nullable: false, dict_id: 0, dict_is_ordered: false, metadata: {} } }, fail_on_overflow: false }, fail_on_overflow: false }\n2025-11-24T15:48:51.806450Z DEBUG object_store_metrics::log: object store operation store_type=\"main\" op=\"get_opts\" result=\"success\" location=\"node0/dbs/11/2/2025-11-23/15-50/0001028730.parquet\" duration_secs=0.000677604 bytes=9651 headers_secs=0.000677604 first_byte_secs=0.000677604\n2025-11-24T15:48:51.807414Z DEBUG datafusion_datasource_parquet::opener: DataSourceExec Ignoring unknown extension specified for node0/dbs/11/2/2025-11-23/15-50/0001028730.parquet\n2025-11-24T15:48:51.807514Z DEBUG datafusion_datasource_parquet::page_filter: Use filter and page index to create RowSelection RowSelection { selectors: [RowSelector { row_count: 650, skip: false }] } from predicate: BinaryExpr { left: BinaryExpr { left: Column { name: \"time_null_count\", index: 1 }, op: NotEq, right: Column { name: \"row_count\", index: 2 }, fail_on_overflow: false }, op: And, right: BinaryExpr { left: Column { name: \"time_max\", index: 0 }, op: GtEq, right: Literal { value: TimestampNanosecond(1763912930515309301, None), field: Field { name: \"lit\", data_type: Timestamp(Nanosecond, None), nullable: false, dict_id: 0, dict_is_ordered: false, metadata: {} } }, fail_on_overflow: false }, fail_on_overflow: false }\n2025-11-24T15:48:51.933255Z DEBUG tokio_watchdog: tokio starts hanging runtime=\"datafusion\"\n```\n\n\nThank you for looking into this!",
      "solution": "Hi @philjb , Thank you very much for your insights!\n\nI wish I could be more helpful but today it doesn't crash anymore. Without me changing anything... Nevertheless I'll try to answer some of your questions.\n\n> Do you (or did you) experience the same termination issue with the SHOW TAG VALUES query on 3.6?\n\nI don't remember having any such issues before yesterday, hence I think not. We're definitely on 3.7 at least for a few days now (our logs don't go back further) and the problem only started yesterday, hence it did *not* coincide with an upgrade from 3.6 to 3.7.\n\n> How many cpu cores and threads is Influxdb3 recognizing?\n\nIt says `num_cpus=6` at startup.\n\n> Adjust thread configuration and/or your cpu core in your vm/pod. We recommend at least 2 threads/cores for io and 2 for datafusion/query for a decent workload. So that's a true 4 core cpu.\n\nYou're touching on an interesting point.\n\nOur K8s quota for InfluxDB was requests=1 core, limits=10 cores. Which means that the K8s scheduler only guarantees the equivalent of 1 core to be available, but *if* there is unused CPU capacity available on the server then the process can use up to 10 cores. Presumably this also means that InfluxDB has to see 10 cores even if it only gets the performance of a single core. This sort of configuration has the advantage that we can over-provision the CPU; InfluxDB can use a lot of CPU power if available but we don't need to provide much actual hardware, at the expense of unpredictable performance at times.\n\nHowever, this also means that depending on which node the pod is scheduled and depending on what else is going on  InfluxDB can experience very slow cores (i.e. it gets 10 cores but each only has the performance of 1/10 of a core; or worse, some cores are even slower while others are faster - not sure how exactly the scheduler works), which I guess could lead to a situation where the watchdog doesn't get pinged regularly enough and thus thinks the process is dead but in reality it's just slow.\n\nMaybe in this situation it would be better to give InfluxDB predictable performance by setting request and limit to the same value...\n\n(At the same time it might be worth tuning the watchdog to better deal with very unpredictable CPU performance which can happen in a cloud environment)\n\n---\n\nHi @philjb \n\nThanks again for the response. I've asked around for more details on how the CPU detection works in K8s and I see some problems.\n\nFirst of all: The k8s \"limits\" quota does not influence the number of CPU cores the application sees. The application always sees all cores of the underlying k8s node.\n\nThe K8s \"requests\" and \"limits\" quotas only restrict the time share the process gets on the CPU.\n\n\nWhat this means is:\nInfluxDB3's `num_cpus` is based on the k8s node's CPU core count **which is not indicative of the CPU resources actually available to InfluxDB3**. E.g. on a 64 core k8s host the `num_cpus` will be 64 but the CPU capacity actually available to InfluxDB3 may only be a fraction of that (e.g. it could be 2 cores). Therefore InfluxDB tunes itself for a high core count and ends up starting way too many processes which then have to share very little CPU capacity.\n\nNow the \"--num-cores\" option of the enterprise version would allow fixing this by setting it to 2, but the Core version of InfluxDB doesn't seem to have that option. IMHO at the very least the \"--num-cores\" option needs to be made available to the InfluxDB3 Core version.\n\n(Side note: I'm not a fan of the situation that a k8s container with a low CPU quota sees all the cores because that inevitably leads to software tuning itself for way too many cores, as it happens here. This should be fixed in k8s' handling of cgroups.)\n\n\nFor our particular case: We're only using 6 core k8s nodes and I was able to allocate almost all 6 cores to InfluxDB3, so it basically has a dedicated node with all the CPU power available. But in the general case this needs to be fixed in InfluxDB3 because the current behavior just isn't suitable for a k8s environment.\n\n---\n\n> we believe influxdb3 was killed by k8s, which isn't an issue with the db.\n\nI do not agree with that conclusion at this point but as I'm unable to provide further evidence and the problem seems to be solved for us feel free to close the ticket anyway, thanks!",
      "labels": [
        "v3",
        "v3/core"
      ],
      "created_at": "2025-11-24T16:18:41Z",
      "closed_at": "2025-12-01T19:14:20Z",
      "url": "https://github.com/influxdata/influxdb/issues/26993",
      "comments_count": 7
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26347,
      "title": "influxdb 2.7.1 OOM",
      "problem": "<!--\n\nThank you for reporting a bug in InfluxDB IOx.\n\nHave you read the contributing section of the README? Please do if you haven't.\nhttps://github.com/influxdata/influxdb/blob/main/README.md\n\n* Please ask usage questions in the Influx Slack (there is an #influxdb-iox channel).\n    * https://influxdata.com/slack\n* Please don't open duplicate issues; use the search. If there is an existing issue please don't add \"+1\" or \"me too\" comments; only add comments with new information.\n* Please check whether the bug can be reproduced with tip of main.\n* The fastest way to fix a bug is to open a Pull Request.\n    * https://github.com/influxdata/influxdb/pulls\n<!-- The following sections are only required if relevant. -->\n\n__Description:__\nWhen running the Influx, host was killed by OOM.\n\n__Expected behaviour:__\nwe use bucket retention period of 7 days. so, memory consumption should not increase after 7 days. but we can still see the rise in memory usage. it eventually leads to OOM.\n\n__Actual behaviour:__\nMemory should approximately stop increasing after 7 days and thus OOM should not happen.\n\n__Environment info:__\nInfluxDB v2.7.1 (git: 407fa622e9) directly running on host\nhost: rockylinux 9.5 (Linux 5.14.0-284.18.1.x7.2.000.47.x86_64 x86_64)\n\n__Config:__\n\n`# cat /etc/influxdb/config.toml`\n```\nbolt-path = \"/mnt/mgmt/var/lib/influxdb/influxd.bolt\"\nengine-path = \"/mnt/mgmt/var/lib/influxdb/engine\"\nquery-concurrency = 100\nquery-queue-size = 100\nstorage-no-validate-field-size = true\npprof-disabled = true\nmetrics-disabled = true\nui-disabled = true\n```\n\n\n__Logs:__\n```\ntop - 15:08:05 up 8 days, 3:28, 2 users, load average: 41.32, 34.40, 31.27\nTasks: 2624 total, 6 running, 2618 sleeping, 0 stopped, 0 zombie\n%Cpu(s): 9.7 us, 32.8 sy, 0.6 ni, 33.9 id, 21.5 wa, 1.2 hi, 0.4 si, 0.0 st\nMiB Mem : 257022.1 total, 565.7 free, 163613.6 used, 92842.8 buff/cache\nMiB Swap: 21372.0 total, 0.0 free, 21372.0 used. 7199.6 avail Mem\n\nPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND\n2762590 influxdb 20 0 537.2g 128.3g 4.7g D 516.7 51.1 3963:57 /usr/bin/influxd\n```\n\nWhat can we do to debug this issue? ",
      "solution": "Memory growth is surprisingly often caused by slow IO to disk.  That can cause writes to buffer up in RAM waiting for disk availability.  It's not always the root cause, but often enough to warrant `iostat`",
      "labels": [
        "area/2.x"
      ],
      "created_at": "2025-05-02T06:47:04Z",
      "closed_at": "2025-11-26T23:25:50Z",
      "url": "https://github.com/influxdata/influxdb/issues/26347",
      "comments_count": 3
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 20883,
      "title": "TSM compactions: Recover from runtime errors",
      "problem": "__Proposal:__\r\nInfluxDB should be able to recover from runtime errors during compaction.\r\n\r\nCompaction is a complex process that involves reading, processing and writing large amounts of data, sooner or later a disk or memory error could lead to unexpected errors during this process.\r\n\r\n__Current behavior:__\r\nInfluxDB panics and it's unable to fully restart until corrupt TSM files are manually removed.\r\n\r\n__Desired behavior:__\r\nInfluxDB to carry on. \r\n\r\n__Use case:__\r\nBeen able to record new data should be a priority over optimising the access to the old one.\r\n\r\n\r\n\r\n```\r\npanic: runtime error: slice bounds out of range\r\n\r\ngoroutine 671 [running]:\r\ngithub.com/influxdata/influxdb/tsdb/cursors.(*BooleanArray).Include(0xc00933aff0, 0x1648042fc1f9fd28, 0x1648046305449fec)\r\n        /go/src/github.com/influxdata/influxdb/tsdb/cursors/arrayvalues.gen.go:918 +0x263\r\ngithub.com/influxdata/influxdb/tsdb/engine/tsm1.(*tsmBatchKeyIterator).combineBoolean(0xc012438f00, 0xc032c04501, 0xe3, 0xc032c045a0, 0x1)\r\n        /go/src/github.com/influxdata/influxdb/tsdb/engine/tsm1/compact.gen.go:1967 +0x2f7\r\ngithub.com/influxdata/influxdb/tsdb/engine/tsm1.(*tsmBatchKeyIterator).mergeBoolean(0xc012438f00)\r\n        /go/src/github.com/influxdata/influxdb/tsdb/engine/tsm1/compact.gen.go:1910 +0x1aa\r\ngithub.com/influxdata/influxdb/tsdb/engine/tsm1.(*tsmBatchKeyIterator).merge(0xc012438f00)\r\n        /go/src/github.com/influxdata/influxdb/tsdb/engine/tsm1/compact.go:1833 +0x1d5\r\ngithub.com/influxdata/influxdb/tsdb/engine/tsm1.(*tsmBatchKeyIterator).Next(0xc012438f00, 0x2182c07dd)\r\n        /go/src/github.com/influxdata/influxdb/tsdb/engine/tsm1/compact.go:1695 +0x119f\r\ngithub.com/influxdata/influxdb/tsdb/engine/tsm1.(*Compactor).write(0xc0001e2900, 0xc000040620, 0x66, 0x21765e0, 0xc012438f00, 0x153c501, 0x0, 0x0)\r\n        /go/src/github.com/influxdata/influxdb/tsdb/engine/tsm1/compact.go:1140 +0x1c8\r\ngithub.com/influxdata/influxdb/tsdb/engine/tsm1.(*Compactor).writeNewFiles(0xc0001e2900, 0x1c1, 0x3, 0xc011cb8200, 0x5, 0x8, 0x21765e0, 0xc012438f00, 0x1, 0x21765e0, ...)\r\n        /go/src/github.com/influxdata/influxdb/tsdb/engine/tsm1/compact.go:1044 +0x1d2\r\ngithub.com/influxdata/influxdb/tsdb/engine/tsm1.(*Compactor).compact(0xc0001e2900, 0x1f8cc00, 0xc011cb8200, 0x5, 0x8, 0x0, 0x0, 0x0, 0x0, 0x0)\r\n        /go/src/github.com/influxdata/influxdb/tsdb/engine/tsm1/compact.go:952 +0x40e\r\ngithub.com/influxdata/influxdb/tsdb/engine/tsm1.(*Compactor).CompactFull(0xc0001e2900, 0xc011cb8200, 0x5, 0x8, 0x0, 0x0, 0x0, 0x0, 0x0)\r\n        /go/src/github.com/influxdata/influxdb/tsdb/engine/tsm1/compact.go:970 +0x170\r\ngithub.com/influxdata/influxdb/tsdb/engine/tsm1.(*compactionStrategy).compactGroup(0xc000152af0)\r\n        /go/src/github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:2158 +0x17db\r\ngithub.com/influxdata/influxdb/tsdb/engine/tsm1.(*compactionStrategy).Apply(0xc000152af0)\r\n        /go/src/github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:2135 +0x54\r\ngithub.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).compactHiPriorityLevel.func1(0xc014990190, 0xc0001a2280, 0x2, 0xc000152af0)\r\n        /go/src/github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:2052 +0xea\r\ncreated by github.com/influxdata/influxdb/tsdb/engine/tsm1.(*Engine).compactHiPriorityLevel\r\n        /go/src/github.com/influxdata/influxdb/tsdb/engine/tsm1/engine.go:2047 +0x12a\r\n```",
      "solution": "A number of the problems that caused unrecoverable compaction errors have now been fixed, lowering the importance of this requested feature\n\nSee these PRs:\n- https://github.com/influxdata/influxdb/pull/26025\n- https://github.com/influxdata/influxdb/pull/26022\n- https://github.com/influxdata/influxdb/pull/24520\n- https://github.com/influxdata/influxdb/pull/24565",
      "labels": [
        "panic",
        "area/storage",
        "1.x",
        "area/2.x"
      ],
      "created_at": "2021-03-08T11:52:35Z",
      "closed_at": "2025-11-26T21:48:59Z",
      "url": "https://github.com/influxdata/influxdb/issues/20883",
      "comments_count": 2
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 24434,
      "title": "Replication error - Queue is full",
      "problem": "<!--\r\n\r\nThank you for reporting a bug in InfluxDB IOx. \r\n\r\nHave you read the contributing section of the README? Please do if you haven't.\r\nhttps://github.com/influxdata/influxdb_iox/blob/main/README.md\r\n\r\n* Please ask usage questions in the Influx Slack (there is an #influxdb-iox channel).\r\n    * https://influxdata.com/slack\r\n* Please don't open duplicate issues; use the search. If there is an existing issue please don't add \"+1\" or \"me too\" comments; only add comments with new information.\r\n* Please check whether the bug can be reproduced with tip of main.\r\n* The fastest way to fix a bug is to open a Pull Request.\r\n    * https://github.com/influxdata/influxdb/pulls\r\n\r\n-->\r\n\r\n__Steps to reproduce:__\r\nList the minimal actions needed to reproduce the behaviour.\r\n\r\n1. Setup remote connection between two nodes (both running InfluxDB 2.7 OSS)\r\n2. Setup replication from raw data bucket in edge node to bucket in cloud node\r\n3. Perform a high insertion rate at the edge node (e.g. 48 distinct connections inserting 1 batch of 100 measurements with 30 fields each per second)\r\n\r\n__Expected behaviour:__\r\nAll data inserted at the edge node is replicated to the cloud node\r\n\r\n__Actual behaviour:__\r\nA significant amount of data fails to be replicated due to the replication queue being full. Increasing the size of the replication queue simply delays the problem. No feedback is provided to the clients performing insertions.\r\n\r\n```\r\n{\r\n    \"replications\": [\r\n        {\r\n            \"id\": \"0c0c57dd86471000\",\r\n            \"orgID\": \"ce0886412ad86f64\",\r\n            \"name\": \"replication_0\",\r\n            \"remoteID\": \"0c0c57dcb3515000\",\r\n            \"localBucketID\": \"0391452d31c77515\",\r\n            \"remoteBucketID\": \"6593fa8e235bb3cf\",\r\n            \"RemoteBucketName\": \"\",\r\n            \"maxQueueSizeBytes\": 67108860,\r\n            \"currentQueueSizeBytes\": 67067089,\r\n            \"remainingBytesToBeSynced\": 67067033,\r\n            \"latestResponseCode\": 204,\r\n            \"latestErrorMessage\": \"\",\r\n            \"dropNonRetryableData\": false,\r\n            \"maxAgeSeconds\": 0\r\n        }\r\n    ]\r\n}\r\n```\r\n\r\nEven though the replication queue is filled and failing to replicate data, the replication endpoint still shows _latestResponseCode_ with 204 indicating replication success\r\n\r\n__Environment info:__\r\n\r\n* InfluxDB version: InfluxDB OSS 2.7 \r\n* System info: Linux 5.4.0-156-generic x86_64\r\n* Edge node running in a Docker container limited to 4 CPU cores, 4GB of RAM, 44MB/s disk reads, 40MB/s disk writes, 2700 read IOps, 1200 write IOps\r\n* Cloud node running in a Docker container with no limitations. Host machine has a 6 core CPU and 16GB of RAM\r\n\r\n__Logs:__\r\n```\r\nts=2023-10-30T16:25:37.915491Z lvl=error msg=\"Failed to enqueue points for replication\" log_id=0lCLyGJG000 service=replications id=0c0c57dd86471000 error=\"queue is full\"\r\nts=2023-10-30T16:25:37.915627Z lvl=error msg=\"Failed to enqueue points for replication\" log_id=0lCLyGJG000 service=replications id=0c0c57dd86471000 error=\"queue is full\"\r\nts=2023-10-30T16:25:37.915758Z lvl=error msg=\"Failed to enqueue points for replication\" log_id=0lCLyGJG000 service=replications id=0c0c57dd86471000 error=\"queue is full\"\r\nts=2023-10-30T16:25:37.915875Z lvl=error msg=\"Failed to enqueue points for replication\" log_id=0lCLyGJG000 service=replications id=0c0c57dd86471000 error=\"queue is full\"\r\n```\r\n\r\nLogs at the edge node show consecutive errors with the message \"Failed to enqueue points for replication\"\r\n\r\n__Questions:__\r\n* Should the replication endpoint not report some failure since data is not being successfully replicated?\r\n* Should the insert operations not return a failure when the inserted data could not be added to the replication queue?\r\n\r\n\r\nAlthough some other issues related to replication exist, I was not able to find any reporting this behavior.\r\n",
      "solution": "I think I understood the problem:\n\n- Our transaction adds one \"record\" to the DB, the data size of it is 170-180 bytes. \n- Replication picks this write from a queue and generates separate HTTP request for it.\n- I am not familiar enough with Go to determine whether Keep-Alive or any other HTTP features are used to avoid authentication and sending other headers on each request - but it seems that there is some serious overhead. \n- When I tried to block the destination with a firewall rule - it took the writer 30 seconds to figure the issue (logs attached). \n[logs.txt](https://github.com/user-attachments/files/18580011/logs.txt)\n- When the destination was made available again - a replication seems to not having a mechanism of combining multiple writes into a single HTTP request, but continues to submit the queue one by one. \n\n Please correct me if I am wrong...\n\nThanks,\nDmitry\n\n---\n\nWe are also having this issue of queues running full. The setup is pretty similar (several edge containers replicating to one central cloud all on OSS 2.7.5). If I get it right there is a limit of 2,5 MB of data in 5 minutes that can be replicated?\n\nhttps://github.com/influxdata/influxdb/blob/v2.7.5/replications/service.go#L28-L31\n\nI think this might be the problem in our case. Any chance to increase this value?\n\n---\n\nDisclaimer - I am not associated with Influx, just a user and everything expressed below are my impressions from using 2.7.5-2.7.11 versions and digging a bit in the code (with almost no experience with Go). Feel free to correct me if I wrong. \n\n1. Built-in replication is more like PoC than production-grade feature - not only because all its parameters are hardcoded ;)\n2. It uses the same Write API you've called to insert the data - just at the remote server\n3. Data is transferred in JSON format - causing it to explode on the line, compared to the way it is stored. ZIP compression improves the situation for multi-row transactions, but turns into a waste of CPU when the data is small and includes no redundancies.\n4. New HTTP request is created for every replicated transaction - meaning that to ~170 bytes of data in our case, more than 2k of headers are added. Multi-part, Web sockets - ?\n5. Each bucket has its own replication queue - shipping of data from which starts on a first write. So if your DB \"works\", say, between 9AM and 5PM, and for some reason it is being restarted 5:01 with non-empty queue - no replication will happen until 9AM next day, with old data waiting in the queue\n6. \"Unlimited\" queue means \"size=67M\"\n\nHaving all this said - I am really enjoying working with InfluxDB, and hope that mentioned issues will be fixed in 3.x. I would like to contribute, but neither Go not Rust are the languages I have enough experience with. ",
      "labels": [
        "area/2.x",
        "team/edge",
        "area/replications"
      ],
      "created_at": "2023-10-30T17:20:57Z",
      "closed_at": "2025-11-26T21:34:19Z",
      "url": "https://github.com/influxdata/influxdb/issues/24434",
      "comments_count": 12
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26680,
      "title": "Issue with `findGenerations` caching depending on input",
      "problem": "In the following code path https://github.com/influxdata/influxdb/blob/22bec4f046a28e3f1fa815705362767151407e1b/tsdb/engine/tsm1/compact.go#L706-L716\n\nWe see a problem when `skipInUse` alternates between `true` and `false` which can cause invalid caching behavior. \n\n__Steps to reproduce:__\nList the minimal actions needed to reproduce the behavior.\n\n1. `findGenerations` gets called with `skipInUse` set to true\n2. Data is returned and set in `last` &  `lastGen`\n3. `findGenerations` is called with `skipInUse` set to false\n4. `lastGen` will be returned due to cached data\n\n__Expected behaviour:__\nI would expect changing the input to invalidate the cache\n\n",
      "solution": "I have a hypothesis / fear that the portion of this caching bug that ignores `skipInUse` could be important for the compaction planning. My reasoning is that  ignoring `skipInUse` causes higher compaction level planners to include files that would be excluded without the bug. This in turn could lead to more compaction groups for the higher level planners. In [nextByQueueDepths](https://github.com/influxdata/influxdb/blob/22bec4f046a28e3f1fa815705362767151407e1b/tsdb/engine/tsm1/scheduler.go#L36), higher level plans need more groups in order to \"win\" over level groups due to the weighting. If `skipInUse` is honored properly in `findGenerations`, this could lead to less groups in the higher level plans, which could prevent them from getting scheduled as often, or at all.\n\nI don't know that this happens, but its something we should be watchful as we fix the caching bugs in here. I do believe there are at least some caching bugs here that should be fixed.",
      "labels": [
        "kind/bug",
        "1.x",
        "team/edge"
      ],
      "created_at": "2025-08-08T17:25:32Z",
      "closed_at": "2025-11-26T20:51:23Z",
      "url": "https://github.com/influxdata/influxdb/issues/26680",
      "comments_count": 2
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26694,
      "title": "OSS Version 1.12.1 is reported as available, but no tar for download",
      "problem": "<!--\n\nThank you for reporting a bug in InfluxDB IOx.\n\nHave you read the contributing section of the README? Please do if you haven't.\nhttps://github.com/influxdata/influxdb/blob/main/README.md\n\n* Please ask usage questions in the Influx Slack (there is an #influxdb-iox channel).\n    * https://influxdata.com/slack\n* Please don't open duplicate issues; use the search. If there is an existing issue please don't add \"+1\" or \"me too\" comments; only add comments with new information.\n* Please check whether the bug can be reproduced with tip of main.\n* The fastest way to fix a bug is to open a Pull Request.\n    * https://github.com/influxdata/influxdb/pulls\n\n-->\n\n__Steps to reproduce:__\nList the minimal actions needed to reproduce the behaviour.\n\n1. wget https://download.influxdata.com/influxdb/releases/influxdb-1.12.1-linux-amd64.tar.gz\n\n__Expected behaviour:__\nvalid tar file with version inside\n\n__Actual behaviour:__\n404 Not found\n\nI even went into your AI assistant, asked how to get it, and it gave me the same directions as #1 above. :)\n\n__Environment info:__\n\nUbuntu 22.04\n\n__Config:__\nN/A\n\n",
      "solution": "Hi @kamenmackay and @ellisroll-b  - \n\nYes, you're right - the [1.12.1 release notes were published](https://docs.influxdata.com/influxdb/v1/about_the_project/release-notes/). We've added a \"pre-release/TDB\" note in the last few days: we observed [an issue](https://github.com/influxdata/influxdb/issues/26648#event-19180635179) in 1.12.1 during final internal testing that delayed publishing prebuilt packages but we had already made the release notes. To fix the issue, we've needed to go through the bake-in testing again which can be a few weeks. Again we expect 1.12.x soon (this week), but of course pending anything we might see in testing. It'll likely be 1.12.2. \n\nI will try to remember to comment here again when the packages are available. If you can't wait, you may build from source.",
      "labels": [
        "1.x"
      ],
      "created_at": "2025-08-14T18:08:44Z",
      "closed_at": "2025-11-26T20:38:44Z",
      "url": "https://github.com/influxdata/influxdb/issues/26694",
      "comments_count": 3
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 24938,
      "title": "query doesn't return result / returns partial result, when there is change in datatype of a field in that time range.",
      "problem": "<!--\r\n\r\nThank you for reporting a bug in InfluxDB IOx.\r\n\r\nHave you read the contributing section of the README? Please do if you haven't.\r\nhttps://github.com/influxdata/influxdb/blob/main/README.md\r\n\r\n* Please ask usage questions in the Influx Slack (there is an #influxdb-iox channel).\r\n    * https://influxdata.com/slack\r\n* Please don't open duplicate issues; use the search. If there is an existing issue please don't add \"+1\" or \"me too\" comments; only add comments with new information.\r\n* Please check whether the bug can be reproduced with tip of main.\r\n* The fastest way to fix a bug is to open a Pull Request.\r\n    * https://github.com/influxdata/influxdb/pulls\r\n\r\n-->\r\n\r\n__Steps to reproduce:__\r\n\r\n1. Need a bucket where there is a change in datatype of a field.\r\n2. Run Query for the time-range which will consist of data where datatype of field has changed.\r\n\r\n\r\n__Expected behaviour:__\r\n\r\n1. The query should return all of the data available in the specified time range.\r\n\r\n__Actual behaviour:__\r\n\r\n1. The query returns either no data, or partial data.\r\n- Query returns No data.\r\n![Screenshot No data](https://github.com/influxdata/influxdb/assets/167964532/bae59f93-a8c8-478b-b222-13dd6323acf6)\r\n- Query returns data & data type is int (time range is subset of above time range)\r\n![Screenshot datatype integer](https://github.com/influxdata/influxdb/assets/167964532/d9373b2d-b84a-404c-b5f3-552e9ed28258)\r\n- Datatype is string (screenshot also shows partial data returned , as this time range has more data as can be seen in above screenshot)\r\n![Screenshot_string_datatype](https://github.com/influxdata/influxdb/assets/167964532/107b5a7d-0dac-42f1-90fd-fe1905df3e59)\r\n\r\n\r\n__Link to issue on Influx forum__ : \r\nhttps://community.influxdata.com/t/flux-query-does-not-return-results-if-wider-time-range-is-selected/33750/9\r\n\r\n\r\n__Environment info:__\r\n> Note: Upgrading to the latest version didn't fix the issue\r\n```bash \r\ndocker run --name influxdb -d -p 8086:8086 --volume c:\\projects\\test_influxdata_3:/var/lib/influxdb2 --volume c:\\projects\\test_influxdata_3\\config:/etc/influxdb2 influxdb:2.7.1 \r\n```\r\n\r\n__Config:__\r\nDefault config.\r\n\r\n\r\n__Logs:__\r\nNo error messages in the Logs as well as flux query logs.\r\n",
      "solution": "I had same problem with v2.6.1. There was integer/double type change of `value` field which was unnoticed. \r\nThe only workaround was to export data, clean table and import it back.\n\n---\n\n> I had same problem with v2.6.1. There was integer/double type change of `value` field which was unnoticed. The only workaround was to export data, clean table and import it back.\r\n\r\nHi @latonita How did you exactly clean the data? and also how did you export and import it ?",
      "labels": [
        "area/2.x"
      ],
      "created_at": "2024-04-24T10:55:14Z",
      "closed_at": "2025-11-26T20:38:14Z",
      "url": "https://github.com/influxdata/influxdb/issues/24938",
      "comments_count": 6
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 20704,
      "title": "[2.x OSS]: tags not disappearing after deleting by predicate",
      "problem": "<!--\r\n\r\nThank you for reporting a bug in InfluxDB. \r\n\r\n* Please ask usage questions on the Influx Community site.\r\n    * https://community.influxdata.com/\r\n* Please add a :+1: or comment on a similar existing bug report instead of opening a new one.\r\n    * https://github.com/influxdata/influxdb/issues?utf8=%E2%9C%93&q=is%3Aissue+is%3Aopen+is%3Aclosed+sort%3Aupdated-desc+label%3Akind%2Fbug+\r\n* Please check whether the bug can be reproduced with the latest release.\r\n* The fastest way to fix a bug is to open a Pull Request.\r\n    * https://github.com/influxdata/influxdb/pulls\r\n\r\n-->\r\n\r\n__Steps to reproduce:__\r\nList the minimal actions needed to reproduce the behavior.\r\n\r\n1. Have data with tags in a measurement.\r\n2. Delete Data with a specific tag value:\r\n\r\n```\r\ninflux delete --bucket \"telegraf/autogen\" --predicate '_measurement=\"devax_forecast\" AND fcdate=\"fc_21012021\"' -o NETZConsult --start '1970-01-01T00:00:00Z' --stop '2025-12-31T23:59:00Z')\r\n```\r\n\r\n3. Check of Tag value is removed from the DB:\r\n\r\n```\r\nshow tag values from devax_forecast WITH key=\"fcdate\"\r\n```\r\n\r\nand it's still showing up:\r\n\r\n```\r\nkey    |value\r\nfcdate |fc_01022021\r\nfcdate |fc_02022021\r\nfcdate |fc_03022021\r\nfcdate |fc_04022021\r\nfcdate |fc_21012021\r\nfcdate |fc_22012021\r\nfcdate |fc_23012021\r\nfcdate |fc_24012021\r\nfcdate |fc_25012021\r\nfcdate |fc_26012021\r\nfcdate |fc_27012021\r\nfcdate |fc_27012021_erst\r\nfcdate |fc_28012021\r\nfcdate |fc_29012021\r\nfcdate |fc_30012021\r\n```\r\n\r\n__Expected behavior:__\r\nThe tag should disappear.\r\n\r\n__Actual behavior:__\r\nThe tag is still present, even if no records are assigned that tag.\r\n\r\n__Environment info:__\r\n\r\n* System info: Linux 4.4.0-198-generic x86_64\r\n* InfluxDB version: InfluxDB 2.0.3 (git: fe04d346df) build_date: 2020-12-15T01:00:16Z\r\n* Other relevant environment details: Runs from debian Packages on a Ubuntu 16.04.7\r\n",
      "solution": "@nickwbaker the way the influxdb storage engine works, deleting by a specific field is not possible. \r\n\r\nAs for a workaround, you will need to write only the data you need to a new bucket and drop the old one.\r\n\r\n```\r\nfrom(bucket: \"old_bucket\") \r\n  |> range(start: -100y) \r\n  |> filter(fn: (r) =>  _field != \"foo\") \r\n  |> to(bucket: \"new_bucket\")\r\n```\r\nyou can also do it from the command line by exporting the data from the old bucket as LP using the https://docs.influxdata.com/influxdb/v2.0/reference/cli/influxd/inspect/export-lp/ command and then ingesting that into a new bucket.\r\n\r\nI know it's not the answer you wanted, but hopefully this can unblock you in the short term.\n\n---\n\nAnyone manage to fix this ? \r\n@lephisto \r\n\r\nI have the issue on 2.3.0",
      "labels": [
        "area/storage",
        "area/2.x"
      ],
      "created_at": "2021-02-05T09:59:02Z",
      "closed_at": "2025-11-26T20:32:29Z",
      "url": "https://github.com/influxdata/influxdb/issues/20704",
      "comments_count": 22
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 23016,
      "title": "runtime error: invalid memory address or nil pointer dereference",
      "problem": "System: Docker\r\nVersion: influxDB 2.1\r\nBuilt from official Docker Image\r\n\r\nWhen I try to load different fields from existing buckets I get the below error:\r\n\r\n`ts=2021-12-21T15:47:17.646101Z lvl=info msg=\"Execute source panic\" log_id=0YZQdeMG000 service=storage-reads error=\"panic: runtime error: invalid memory address or nil pointer dereference\" stacktrace=\"goroutine 1639 [running]:\\nruntime/debug.Stack()\\n\\t/home\r\n/circleci/.tools/go/src/runtime/debug/stack.go:24 +0x65\\ngithub.com/influxdata/flux/execute.(*executionState).do.func1.1()\\n\\t/home/circleci/go/pkg/mod/github.com/influxdata/flux@v0.139.0/execute/executor.go:286 +0x205\\npanic({0x3c52600, 0x53e34d0})\\n\\t/home/circleci/.to\r\nols/go/src/runtime/panic.go:1038 +0x215\\ngithub.com/influxdata/influxdb/v2/v1/services/storage.(*Store).measurementFields.func1()\\n\\t/home/circleci/go/src/github.com/influxdata/influxdb/v1/services/storage/store.go:546 +0x1c\\npanic({0x3c7bc60, 0xc0024e7ec0})\\n\\t/home/cir\r\ncleci/.tools/go/src/runtime/panic.go:1038 +0x215\\ngithub.com/influxdata/influxdb/v2/v1/services/storage.(*Store).measurementFields(0xc0009b61b0, {0x40a6b18, 0xc0024ea780}, 0xc001634bd0)\\n\\t/home/circleci/go/src/github.com/influxdata/influxdb/v1/services/storage/store.go:549 +0x33f\\ngithub.com/influxdata/influxdb/v2/v1/services/storage.(*Store).TagValues(0x4079ad0, {0x40a6b18, 0xc0024ea780}, 0xc0024e7d70)\\n\\t/home/circleci/go/src/github.com/influxdata/influxdb/v1/services/storage/store.go:406 +0x390\\ngithub.com/influxdata/influxdb/v2/storage/flux.(*tagValuesIterator).Do(0xc0024a4e80, 0x61c1f705)\\n\\t/home/circleci/go/src/github.com/influxdata/influxdb/storage/flux/reader.go:970 +0x1c6\\ngithub.com/influxdata/influxdb/v2/query/stdlib/influxdata/influxdb.(*Source).processTables(0xc0004695f0, {0x40a6b18, 0xc0024ea780}, {0x406f510, 0xc0024a4e80}, 0xc0020e4bb8)\\n\\t/home/circleci/go/src/github.com/influxdata/influxdb/query/stdlib/influxdata/influxdb/source.go:76 +0xbf\\ngithub.com/influxdata/influxdb/v2/query/stdlib/influxdata/influxdb.(*readTagValuesSource).run(0xc0004695f0, {0x40a6b18, 0xc0024ea780})\\n\\t/home/circleci/go/src/github.com/influxdata/influxdb/query/stdlib/influxdata/influxdb/source.go:492 +0x114\\ngithub.com/influxdata/influxdb/v2/query/stdlib/influxdata/influxdb.(*Source).Run(0xc0004695f0, {0x40a6b18, 0xc0024ea780})\\n\\t/home/circleci/go/src/github.com/influxdata/influxdb/query/stdlib/influxdata/influxdb/source.go:56 +0x168\\ngithub.com/influxdata/flux/execute.(*executionState).do.func1({0x40a86e0, 0xc0004695f0})\\n\\t/home/circleci/go/pkg/mod/github.com/influxdata/flux@v0.139.0/execute/executor.go\r\n:291 +0x20e\\ncreated by github.com/influxdata/flux/execute.(*executionState).do\\n\\t/home/circleci/go/pkg/mod/github.com/influxdata/flux@v0.139.0/execute/executor.go:261 +0x228\\n\"`\r\n\r\n![image](https://user-images.githubusercontent.com/4213032/146960808-55a329ec-7a03-4a47-9a30-c5f52b1f2ddc.png)\r\n\r\nThanks in advance for any further help!",
      "solution": "[These panics](https://github.com/influxdata/influxdb/issues/23016#issuecomment-1039479819) and [this panic](https://github.com/influxdata/influxdb/issues/23016#issue-1085982770) have been fixed: [fix: handle a potential nil iterator leading to a panic](https://github.com/influxdata/influxdb/pull/23520)\n\n[This](https://github.com/influxdata/influxdb/issues/23016#issuecomment-1038740267) is a different panic.  Please open a new issue for it @bcare-bergia if you still encounter it with v2.7.12\n",
      "labels": [],
      "created_at": "2021-12-21T16:01:27Z",
      "closed_at": "2025-11-26T20:04:06Z",
      "url": "https://github.com/influxdata/influxdb/issues/23016",
      "comments_count": 4
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 24882,
      "title": "service=scraper tls: failed to verify certificate: x509: certificate signed by unknown authority",
      "problem": "__Steps to reproduce:__\r\nList the minimal actions needed to reproduce the behaviour.\r\n\r\n1. Image : docker.io/influxdb:2.7.5 used in OpenShift with Grafana 10.2.4\r\n2. When I start the Pod everything is working well but I have this error : ts=2024-04-04T17:56:16.772948Z lvl=error msg=\"Unable to gather\" log_id=0oMYhyyG000 service=scraper scraper-name=\"new target\" error=\"Get \\\"https://influxdb-cp.XXX.com/metrics\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"\r\n\r\nIt's a private certificate from the PKI of my company, I checked the box \"Skip TLS Verify\" in influxdb datasource without success.\r\n\r\nThanks for your help",
      "solution": "@ilario, I am afraid, the above linked issues and related solutions are addressing other issues. \n\nI am having a very similar issue. \nI created a self signed certificate for InfluxDB (including the subjectAltName/SAN). \nI created a Scraper in InfluxDB-UI to https://mylocalhostname:8086/metrics and get also the following error:\n\ninfluxd[15479]: ts=2025-10-12T20:48:04.834025Z lvl=error msg=\"Unable to gather\" log_id=0zYXi28l000 service=scraper scraper-name=\"Name this Scraper\" error=\"Get \\\"https://mylocalhostname:8086/metrics\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"\ninfluxd[15479]: ts=2025-10-12T20:48:04.834043Z lvl=info msg=\"http: TLS handshake error from 192.168.x.200:33546: remote error: tls: bad certificate\" log_id=0zYXi28l000 service=http\n\nIt looks to me as if the scraper function cannot handle a self-signed certificate. The scraper would need a configuration such as \nhttps-insecure-tls/skip-verify/unsafeSSL.\n\nWould you please investigate.\n",
      "labels": [],
      "created_at": "2024-04-04T18:26:14Z",
      "closed_at": "2025-11-26T19:35:12Z",
      "url": "https://github.com/influxdata/influxdb/issues/24882",
      "comments_count": 6
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26799,
      "title": "panic: interface conversion: tsm1.Value is tsm1.FloatValue, not tsm1.IntegerValue",
      "problem": "On a v2.7.12 docker container, I get the following message:\n<!--\n\nThank you for reporting a bug in InfluxDB IOx.\n\nHave you read the contributing section of the README? Please do if you haven't.\nhttps://github.com/influxdata/influxdb/blob/main/README.md\n\n* Please ask usage questions in the Influx Slack (there is an #influxdb-iox channel).\n    * https://influxdata.com/slack\n* Please don't open duplicate issues; use the search. If there is an existing issue please don't add \"+1\" or \"me too\" comments; only add comments with new information.\n* Please check whether the bug can be reproduced with tip of main.\n* The fastest way to fix a bug is to open a Pull Request.\n    * https://github.com/influxdata/influxdb/pulls\n\n-->\n```\ninfluxdb-1  | panic: interface conversion: tsm1.Value is tsm1.FloatValue, not tsm1.IntegerValue\n[...]\ninfluxdb-1 exited with code 2\n```\n\n__Steps to reproduce:__\n1. Insert data with the following telegraf input:\n```\n[[inputs.mqtt_consumer]]\n  name_override = \"foo\"\n  servers = [\"tcp://some-mqtt:1883\"]\n  topics = [ \"foo/#\" ] # some topics send floats, others  send integer\n  data_format = \"value\"\n  data_type = \"float\" # force to use float as data type\n```\n2. send some data on different topics with consistent type per topic.\n3. query for data:\n```\ninfluxdb-1  | ts=2025-09-13T11:00:15.668461Z lvl=info msg=\"Executing query\" log_id=0yxgIKrG000 service=query query=\"SELECT last(value) FROM telegraf.autogen.foo WHERE topic::tag = 'foo/battery1/soc' AND time >= 1757156415575ms AND time <= 1757761215575ms GROUP BY time(5m) ORDER BY time ASC\"\n```\n\n__Expected behaviour:__\nRetrieval of data (floats).\n\n__Actual behaviour:__\nInfluxdb panics and dies.\n\n__Environment info:__\n\n* dockerized TIG setup with 'latest' tags\n* docker host is a Debian 13\n\n\n<!-- The following sections are only required if relevant. -->\n\n__Logs:__\n\ninfluxdb-1  | panic: interface conversion: tsm1.Value is tsm1.FloatValue, not tsm1.IntegerValue\ninfluxdb-1  |\ninfluxdb-1  | goroutine 3905 [running]:\ninfluxdb-1  | github.com/influxdata/influxdb/v2/tsdb/engine/tsm1.(*integerAscendingCursor).peekCache(0x10?)\ninfluxdb-1  | \t/root/project/tsdb/engine/tsm1/iterator.gen.go:892 +0x8a\ninfluxdb-1  | github.com/influxdata/influxdb/v2/tsdb/engine/tsm1.(*integerAscendingCursor).nextInteger(0xc0020185f0)\ninfluxdb-1  | \t/root/project/tsdb/engine/tsm1/iterator.gen.go:923 +0x1c\ninfluxdb-1  | github.com/influxdata/influxdb/v2/tsdb/engine/tsm1.(*integerIterator).Next(0xc00655ec00)\ninfluxdb-1  | \t/root/project/tsdb/engine/tsm1/iterator.gen.go:724 +0x4c\ninfluxdb-1  | github.com/influxdata/influxdb/v2/influxql/query.(*integerInterruptIterator).Next(0xc00580a0f0?)\ninfluxdb-1  | \t/root/project/influxql/query/iterator.gen.go:3607 +0x53\ninfluxdb-1  | github.com/influxdata/influxdb/v2/influxql/query.(*bufIntegerIterator).Next(0x7f73fc877938?)\ninfluxdb-1  | \t/root/project/influxql/query/iterator.gen.go:2756 +0x53\ninfluxdb-1  | github.com/influxdata/influxdb/v2/influxql/query.(*integerReduceIntegerIterator).reduce(0xc0000554a0)\ninfluxdb-1  | \t/root/project/influxql/query/iterator.gen.go:4012 +0x3a\ninfluxdb-1  | github.com/influxdata/influxdb/v2/influxql/query.(*integerReduceIntegerIterator).Next(0xc0000554a0)\ninfluxdb-1  | \t/root/project/influxql/query/iterator.gen.go:3980 +0x26\ninfluxdb-1  | github.com/influxdata/influxdb/v2/influxql/query.(*integerParallelIterator).monitor(0xc0037e7d80)\ninfluxdb-1  | \t/root/project/influxql/query/iterator.gen.go:3297 +0xaa\ninfluxdb-1  | created by github.com/influxdata/influxdb/v2/influxql/query.newIntegerParallelIterator in goroutine 3897\ninfluxdb-1  | \t/root/project/influxql/query/iterator.gen.go:3267 +0xf6\ninfluxdb-1 exited with code 2",
      "solution": "This looks like you may have written some data that was an integer into the value field which is typed as float.  \n\nDid you get Partial Write Errors with field type conflicts when ingesting the data?\n\nWe fixed one bug around this for 2.7.12: [fix: prevent differing field types in the same shard (#26025)](https://github.com/influxdata/influxdb/pull/26403), so I am surprised to see this error again.\n\n",
      "labels": [],
      "created_at": "2025-09-13T11:12:15Z",
      "closed_at": "2025-11-26T19:32:35Z",
      "url": "https://github.com/influxdata/influxdb/issues/26799",
      "comments_count": 3
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 24772,
      "title": "Fix FlightSQL queries from Grafana",
      "problem": "When testing out the InfluxDB Grafana plugin with the v1 query API in Edge (v3), I found that when using SQL to query the target server does not work.\n\n`hyper` is rejecting the request with an _\"invalid HTTP method\"_ error.\n\nYou can see this error in the logs here:\n```\n2024-03-14T18:45:59.267691Z TRACE hyper::proto::h1::conn: Conn::read_head\n2024-03-14T18:45:59.267793Z TRACE hyper::proto::h1::io: received 232 bytes\n2024-03-14T18:45:59.268304Z TRACE parse_headers: hyper::proto::h1::role: Request.parse bytes=232\n2024-03-14T18:45:59.268633Z TRACE hyper::proto::h1::conn: State::close_read()\n2024-03-14T18:45:59.268659Z DEBUG hyper::proto::h1::conn: parse error (invalid HTTP method parsed) with 232 bytes\n2024-03-14T18:45:59.268743Z DEBUG hyper::proto::h1::role: sending automatic response (400 Bad Request) for parse error\n2024-03-14T18:45:59.268989Z TRACE encode_headers: hyper::proto::h1::role: Server::encode status=400, body=None, req_method=None\n2024-03-14T18:45:59.269576Z DEBUG hyper::proto::h1::io: flushed 84 bytes\n2024-03-14T18:45:59.269609Z TRACE hyper::proto::h1::conn: flushed({role=server}): State { reading: Closed, writing: Closed, keep_alive: Disabled, error: hyper::Error(Parse(Method)) }\n2024-03-14T18:45:59.269653Z TRACE hyper::proto::h1::conn: shut down IO complete\n2024-03-14T18:45:59.269724Z DEBUG hyper::server::server::new_svc: connection error: invalid HTTP method parsed\n```",
      "solution": "The issue appears to be that Grafana is attempting a TLS handshake when establishing the connection, and because I am running my `influxdb3` server locally, I am not serving TLS.\r\n\r\nThis is the error showing in Grafana:\r\n```\r\nERROR: flightsql: rpc error: code = Unavailable desc = connection error: desc = \"transport: authentication handshake failed: tls: first record does not look like a TLS handshake\"\r\n```\r\nI attempted to disable all manner of TLS in my local Grafana, to no avail.\r\n\r\nAs it turns out, there was recently an [issue](https://github.com/grafana/grafana/issues/83732) opened for this very thing, and a [PR](https://github.com/grafana/grafana/pull/83834) that closes it. So, once that is released, we can try this out again with TLS properly disabled.\n\n---\n\nI'm using the latest version and I have the same issue here, is there any solution ?\n\n---\n\nI'm using `grafana/grafana:11.0.0` with `docker compose` to test it. The problem seems to still exist.\r\n\r\nIs there anyone else like that?\r\n\r\n![Xnip2024-05-29_16-46-06](https://github.com/influxdata/influxdb/assets/1525018/0ca5bebf-adae-49c5-b523-11cc871bb8b5)\r\n![Xnip2024-05-29_16-57-07](https://github.com/influxdata/influxdb/assets/1525018/272cdea0-af33-4de7-ad42-d4c7b843ec32)\r\n",
      "labels": [
        "v3"
      ],
      "created_at": "2024-03-15T15:20:43Z",
      "closed_at": "2024-11-12T17:40:14Z",
      "url": "https://github.com/influxdata/influxdb/issues/24772",
      "comments_count": 8
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26572,
      "title": "influxdb version 1.11.8 (and possibly others) download is not gzipped, but filename ends in gz and documentation specifies gzip decompression in `tar` command",
      "problem": "\n\n```\nzsh/4 6006 % wget https://download.influxdata.com/influxdb/releases/influxdb-1.11.8-linux-amd64.tar.gz\n--2025-06-30 19:39:59--  https://download.influxdata.com/influxdb/releases/influxdb-1.11.8-linux-amd64.tar.gz\nLoaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\nResolving download.influxdata.com (download.influxdata.com)... 52.33.86.107, 34.213.189.139, 54.244.195.224\nConnecting to download.influxdata.com (download.influxdata.com)|52.33.86.107|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://dl.influxdata.com/influxdb/releases/influxdb-1.11.8-linux-amd64.tar.gz [following]\n--2025-06-30 19:39:59--  https://dl.influxdata.com/influxdb/releases/influxdb-1.11.8-linux-amd64.tar.gz\nResolving dl.influxdata.com (dl.influxdata.com)... 3.163.165.72, 3.163.165.15, 3.163.165.129, ...\nConnecting to dl.influxdata.com (dl.influxdata.com)|3.163.165.72|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 114954240 (110M) [application/x-tar]\nSaving to: \u2018influxdb-1.11.8-linux-amd64.tar.gz\u2019\n\ninfluxdb-1.11.8-linux-amd64.tar.gz                                   100%[====================================================================================================================================================================>] 109.63M  7.39MB/s    in 20s\n\n2025-06-30 19:40:19 (5.55 MB/s) - \u2018influxdb-1.11.8-linux-amd64.tar.gz\u2019 saved [114954240/114954240]\n\nzsh/4 6007 % tar xzf influxdb-1.11.8-linux-amd64.tar.gz\n\ngzip: stdin: not in gzip format\ntar: Child returned status 1\ntar: Error is not recoverable: exiting now\n```\n\n`file` utility confirms that it's a plain tar archive:\n\n```\nzsh/4 6008 [2] % file influxdb-1.11.8-linux-amd64.tar.gz\ninfluxdb-1.11.8-linux-amd64.tar.gz: POSIX tar archive (GNU)\n```",
      "solution": "```\n\u276f file influxdb-1.11.7-linux-amd64.tar.gz                                                                                                                                                                          \ninfluxdb-1.11.7-linux-amd64.tar.gz: POSIX tar archive (GNU)             \n```\n\nAppears 1.11.7 is also a tar archive instead of gzip\n\n```\nDocuments/InfluxData/test_over_256_tags via \ud83d\udc0d v3.10.12 on \u2601\ufe0f  dbenz@influxdata.com                                                                                                                                 \n\u276f gzip influxdb-1.11.7-linux-amd64.tar                                                                                                                                                                             \n\u276f file influxdb-1.11.7-linux-amd64.tar.gz                                                                                                                                                                         \ninfluxdb-1.11.7-linux-amd64.tar.gz: gzip compressed data, was \"influxdb-1.11.7-linux-amd64.tar\", last modified: Thu Oct 24 08:57:02 2024, from Unix, original size modulo 2^32 114964480   \n```\nrunning gzip manually resolves the issue. I'm going to investigate our circle ci config and see what's up.",
      "labels": [
        "area/packaging",
        "kind/bug",
        "1.x"
      ],
      "created_at": "2025-07-01T01:40:08Z",
      "closed_at": "2025-11-03T19:26:34Z",
      "url": "https://github.com/influxdata/influxdb/issues/26572",
      "comments_count": 4
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26896,
      "title": "RPM package upgrade from 1.11.8 -> 1.12.2 fails and removes systemd unit file",
      "problem": "__Steps to reproduce:__\nList the minimal actions needed to reproduce the behaviour.\n\n1. download influxdb 1.11.8 and 1.12.2 rpm packages from https://repos.influxdata.com/packages/\n2. Install version 1.11.8 of influxDB on your choice of RHEL system (i.e. Rocky, CentOS, RHEL) `yum localinstall <influxdb1.11.8>.rpm`\n3. Run `systemctl enable --now influxdb`\n4. Verify influxdb is running `systemctl status influxdb`\n5. Upgrade to version 1.12.2 using `yum localinstall <influxdb1.12.2>.rpm`\n6. Run `systemctl daemon-reload` \n7. Run `systemctl status influxdb` and see an error that there is no unit file\n\n\n__Expected behaviour:__\nInfluxDB upgrades from 1.11.8 to 1.12.2 successfully. \n\n__Actual behaviour:__\nDuring the upgrade process we see the following:\n\n```\n# yum install influxdb-1.11.8\n...\n# yum upgrade\n...\n(9/10): influxdb-1.12.2-1.x86_64.rpm\n...\n  Running scriptlet: influxdb-1.12.2-1.x86_64                                                                                                                           9/20 \n  Upgrading        : influxdb-1.12.2-1.x86_64                                                                                                                           9/20 \n  Running scriptlet: influxdb-1.12.2-1.x86_64                                                                                                                           9/20 \nSynchronizing state of influxdb.service with SysV service script with /usr/lib/systemd/systemd-sysv-install.\nExecuting: /usr/lib/systemd/systemd-sysv-install enable influxdb\nCreated symlink /etc/systemd/system/influxd.service \u2192 /usr/lib/systemd/system/influxdb.service.\nCreated symlink /etc/systemd/system/multi-user.target.wants/influxdb.service \u2192 /usr/lib/systemd/system/influxdb.service.\n...\n\n# systemctl enable influxdb.service\nFailed to enable unit: Unit file influxdb.service does not exist.\n```\n\nIf you take a look at the 1.11.8 vs 1.12.2 post install files you'll notice that we use sysV previously where we are now using systemd. This is currently where I hypothesize the issue stems from. ",
      "solution": "Same problem here.\n\nForce a reinstall on my side solve the issue:\n\n```\nsudo dnf reinstall influxdb-1.12.2\n```",
      "labels": [
        "1.x"
      ],
      "created_at": "2025-10-09T16:53:46Z",
      "closed_at": "2025-10-31T19:51:56Z",
      "url": "https://github.com/influxdata/influxdb/issues/26896",
      "comments_count": 2
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26937,
      "title": "Data appears to be truncated when querying earlier than random date: Field data type conflicts",
      "problem": "I have a timeseries which is continuously being filled by a python client without any issues.\n\nI'm now encountering weird issues trying to query that data:\n\n- When querying data earlier than a specific date (about 70 days in the past), I only get data until that date: \n\n<img width=\"705\" height=\"431\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/bff12807-ab38-4409-adc5-d630674bc5e2\" />\n\n- When querying data after that date, I have no issues at all:\n\n<img width=\"1039\" height=\"517\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/7c13137b-b432-481f-8334-1cf556bf6f15\" />\n\n__Steps to reproduce:__\nList the minimal actions needed to reproduce the behaviour.\n\nn/a\n\n__Expected behaviour:__\nI always get all the data I'm querying for and don't have to split queries in half\n\n__Actual behaviour:__\nWhen querying data before August 18th 2025, I only get data until August 18th. When querying with start after August 18th I have no issues.\n\n__Environment info:__\n\n* Please provide the command you used to build the project, including any `RUSTFLAGS`: Debian version influxdb2 2.7.12-1\n* System info: Run `uname -srm` or similar and copy the output here (we want to know your OS, architecture etc): Linux 6.1.0-25-amd64 x86_64\n* If you're running IOx in a containerised environment then details about that would be helpful.\n* Other relevant environment details: vm inside proxmox\n\n__Config:__\nCopy any non-default config values here or attach the full config as a gist or file.\n\n<!-- The following sections are only required if relevant. -->\n\n__Logs:__\nInclude snippet of errors in logs or stack traces here.\nSometimes you can get useful information by running the program with the `RUST_BACKTRACE=full` environment variable.\nFinally, the IOx server has a `-vv` for verbose logging.\n",
      "solution": "Can you provide more details through segmenting the problem? If you provide a start and end time range, can you narrow to the smallest window that shows the issue? \n\nalso please try your queries with curl directly to determine if the data is being returned and the ui isn't showing it or if the data isn't being returned at all. \n\n> Debian version influxdb2 2.17.12-1\n\nyou must mean version 2.7.12, yes?\n\n---\n\nThank you for the quick reply\n\n> you must mean version 2.7.12, yes?\n\nSorry for that, you're right. I changed it.\n\n> Can you provide more details through segmenting the problem? If you provide a start and end time range, can you narrow to the smallest window that shows the issue?\n\n> also please try your queries with curl directly to determine if the data is being returned and the ui isn't showing it or if the data isn't being returned at all.\n\nI did reproduce the issue with curl and it seems to be \"located\" exactly at midnight between 2025-08-17 and 2025-08-18:\n\n```bash\ncurl -XPOST 192.168.99.244:8086/api/v2/query -sS \\\n  -H 'Accept:application/csv' \\\n  -H 'Content-type:application/vnd.flux' \\\n  -d 'from (bucket: \"home\")\n\t\t\t|> range(start: 2025-08-17T23:45:00Z, stop: 2025-08-18T00:15:00Z)\n\t\t\t|> filter (fn: (r) => r[\"_measurement\"] == \"sensors\")\n\t\t\t|> filter(fn: (r) => r[\"_field\"] == \"heating_operation_mode\")'  \n\n# ,result,table,_start,_stop,_time,_value,_field,_measurement,location,sensor_id\n# ,_result,0,2025-08-17T23:45:00Z,2025-08-18T00:15:00Z,2025-08-17T23:50:10.203637Z,0,heating_operation_mode,sensors,heat_pump,heating_control\n# ,_result,0,2025-08-17T23:45:00Z,2025-08-18T00:15:00Z,2025-08-17T23:55:25.223255Z,0,heating_operation_mode,sensors,heat_pump,heating_control\n\ncurl -XPOST 192.168.99.244:8086/api/v2/query -sS \\\n  -H 'Accept:application/csv' \\\n  -H 'Content-type:application/vnd.flux' \\\n  -d 'from (bucket: \"home\")\n\t\t\t|> range(start: 2025-08-18T00:00:00Z, stop: 2025-08-18T00:15:00Z)\n\t\t\t|> filter (fn: (r) => r[\"_measurement\"] == \"sensors\")\n\t\t\t|> filter(fn: (r) => r[\"_field\"] == \"heating_operation_mode\")'  \n\n# ,_result,0,2025-08-18T00:00:00Z,2025-08-18T00:15:00Z,2025-08-18T00:00:40.215186Z,0,heating_operation_mode,sensors,heat_pump,heating_control\n# ,_result,0,2025-08-18T00:00:00Z,2025-08-18T00:15:00Z,2025-08-18T00:05:55.226771Z,0,heating_operation_mode,sensors,heat_pump,heating_control\n# ,_result,0,2025-08-18T00:00:00Z,2025-08-18T00:15:00Z,2025-08-18T00:11:10.232669Z,0,heating_operation_mode,sensors,heat_pump,heating_control\n```\n\n---\n\nVery interesting -- what's the retention on the `home` bucket? \n\nCan you make a [backup](https://docs.influxdata.com/influxdb/v2/reference/cli/influx/backup/) of your tsm files -- I'm not sure when i'll be able to give this some deep time and i'm worried the problem will disappear in the mean time. (you can also use rsync if you want). \n\nI'd be interested to know if you also see the issue with queries via influxql: flux and influxql share part of the query path but also have unshared implementations; it might help further narrow down the issue.\n\nhttps://docs.influxdata.com/influxdb/v2/query-data/influxql/\n\ndb=home `select heating_operation_mode, location, sensor_id from sensors where time > 2025-08-18T00:00:00Z and time < 2025-08-18T00:15:00Z` (likely needs some quoting around the times)\n\nAnd I assume the issue stays at midnight 8-17? it doesn't walk forward one day at a time or anything matching wall clock time?\n\nlast id; does adding more filtering preserve the issue or make it go away? \n\nOh; I think i know what this is; the field heating_operation_mode must be changing data type at the shard boundary. \n\n*What's the field datatype before and after midnight? does it happen to be a float in one interval and a integer in another?*\n\nThe \"raw\" view in the flux ui is one way to see the datatype:\n\n<img width=\"434\" height=\"70\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/98f4e8d4-cf9e-4f22-9a50-214411e20f43\" />",
      "labels": [],
      "created_at": "2025-10-27T21:46:52Z",
      "closed_at": "2025-10-29T16:57:42Z",
      "url": "https://github.com/influxdata/influxdb/issues/26937",
      "comments_count": 7
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 23105,
      "title": "Missing data from influx",
      "problem": "# Problem/Motivation\r\nWhen I link grafana to influx I am missing data. In influx I can query the data and all but grafana is only showing some of the data.\r\nGrafana:\r\n![image](https://user-images.githubusercontent.com/3063928/152644221-e9dcc659-b68d-4d29-a68b-bae449c818dc.png)\r\n(end of the list)\r\n\r\nInflux:\r\n![image](https://user-images.githubusercontent.com/3063928/152644236-97a05efb-6c8f-417c-bda8-d267610d9044.png)\r\n(and the list is much longer)\r\n\r\n__Expected behavior__\r\n\r\nShow all the data needed\r\n\r\n__Actual behavior__\r\n\r\nIt only shows some data\r\n\r\n__Steps to reproduce:__\r\n\r\n1. Install influx\r\n2. Hit start (I did not change anything\r\n3. Add configuration to home assistant so you get data into influx\r\n4. Instal grafana\r\n5. Start it\r\n6. Add datasource using the readme\r\n7. Observere there is a difference in data. \r\n\r\n__Environment info:__\r\n\r\n* System info: running this: https://github.com/hassio-addons/addon-grafana/issues/246 (cross posting the issue from there)\r\n* InfluxDB version: 4.3.0 (https://github.com/hassio-addons/addon-influxdb)\r\n* Other relevant environment details: totally default without any modifications \r\n\r\n## Log file\r\n```\r\n[s6-init] making user provided files available at /var/run/s6/etc...exited 0.\r\n[s6-init] ensuring user provided files have correct perms...exited 0.\r\n[fix-attrs.d] applying ownership & permissions fixes...\r\n[fix-attrs.d] done.\r\n[cont-init.d] executing container initialization scripts...\r\n[cont-init.d] 00-banner.sh: executing... \r\n-----------------------------------------------------------\r\n Add-on: Grafana\r\n The open platform for beautiful analytics and monitoring\r\n-----------------------------------------------------------\r\n Add-on version: 7.4.1\r\n You are running the latest version of this add-on.\r\n System: Home Assistant OS 7.2  (amd64 / generic-x86-64)\r\n Home Assistant Core: 2022.2.2\r\n Home Assistant Supervisor: 2022.01.1\r\n-----------------------------------------------------------\r\n Please, share the above information when looking for help\r\n or support in, e.g., GitHub, forums or the Discord chat.\r\n-----------------------------------------------------------\r\n[cont-init.d] 00-banner.sh: exited 0.\r\n[cont-init.d] 01-log-level.sh: executing... \r\nLog level is set to WARNING\r\n[cont-init.d] 01-log-level.sh: exited 0.\r\n[cont-init.d] grafana.sh: executing... \r\n[cont-init.d] grafana.sh: exited 0.\r\n[cont-init.d] nginx.sh: executing... \r\n[cont-init.d] nginx.sh: exited 0.\r\n[cont-init.d] done.\r\n[services.d] starting services\r\n[services.d] done.\r\nGrafana server is running with elevated privileges. This is not recommended\r\nt=2022-02-05T14:34:51+0100 lvl=warn msg=\"falling back to legacy setting of 'min_interval_seconds'; please use the configuration option in the `unified_alerting` section if Grafana 8 alerts are enabled.\" logger=settings\r\nt=2022-02-05T14:35:03+0100 lvl=warn msg=\"Plugin process is running with elevated privileges. This is not recommended\" logger=plugin.initializer pluginID=grafana-image-renderer\r\n```",
      "solution": "@bruvv -- sorry you were having trouble with grafana. It looks like you said that influxdb, when queried directly, was showing the data you expect? but grafana wasn't? \n\nIf you're still having issues now/today, please let us know what version of influxdb you are using and grafana (please test with the latest versions of each -- there have been updates to influxdb and the grafana plugin). And provide a sample data set that reproduces the problem. \n\nIt's odd to me that grafana isn't showing all your measurements, but there might be some caching in grafana i don't know about. ",
      "labels": [],
      "created_at": "2022-02-05T21:06:50Z",
      "closed_at": "2025-10-25T15:52:59Z",
      "url": "https://github.com/influxdata/influxdb/issues/23105",
      "comments_count": 4
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26810,
      "title": "InfluxDB 3 freezing at startup with 100% CPU usage",
      "problem": "Since a few weeks, InfluxDB 3 Core has a difficult time to start up.\nAfter calling `influxdb3 serve`, it often freezes before the server is up and ends up using 100% of one CPU core.\n\nI can sometimes revive it by restarting it repeatedly until I get lucky and it somehow starts successfully.\nThe bug occoured to me first a few weeks ago with InfluxDB 3.2.1 and the issue is still there with 3.4.2.\n\n__Steps to reproduce:__\n\nI'm not sure if it is easy to reproduce the issue on a clean system.\nI'm running InfluxDB via a Docker Compose file, with the default command. (environment settings and configuration see below)\n\n__Expected behaviour:__\n\nI expect InfluxDB 3 to start and take requests. CPU usage of InfluxDB 3 is &lt;1%.\n\n__Actual behaviour:__\n\nInfluxDB 3 freezes somewhere during the boot process and takes up 100% CPU according to the \"htop\" monitoring tool.\n\n__Environment info:__\n\nMy current setup is as follows:\n\n- Host OS: Ubuntu 24.04.3 LTS with all updates installed, `Linux 6.8.0-63-generic x86_64`\n- RAM: 64 GiB total, 58 free\n- Disk space: over 500 GiB free\n- InfluxDB running inside a Docker container, using the official `docker.io/library/influxdb:3.4.2-core` image\n\n__Config:__\n\nI'm using the default settings of the Docker image, except:\n\n- `INFLUXDB3_OBJECT_STORE=file`\n- `INFLUXDB3_DB_DIR=/var/lib/influxdb3` (This folder is a Docker volume.)\n\n__Logs:__\n\nWhen starting the Docker container with `serve -vv`, these are the last few log lines it prints:\n\n```\n2025-09-16T13:36:35.291563Z  INFO influxdb3_write::table_index_cache: split snapshot at Path { raw: \"abc/snapshots/18446744073709548166.info.json\" } into table snaphots\n2025-09-16T13:36:35.291601Z DEBUG object_store_metrics::log: object store operation store_type=\"main\" op=\"put\" result=\"success\" location=\"abc/table-index-conversion-completed\" duration_secs=3.2918e-5 bytes=34\n2025-09-16T13:36:35.291606Z  INFO influxdb3_write::table_index_cache: Completed splitting persisted snapshots duration_ms=2984 snapshots_processed=3449 snapshots_skipped=0 last_sequence=3449\n2025-09-16T13:36:35.291610Z  INFO influxdb3_write::table_index_cache: Starting table index cache synchronization from object store node_prefix=abc\n2025-09-16T13:36:35.291635Z DEBUG object_store_metrics::log: object store operation store_type=\"main\" op=\"list\" result=\"success\" prefix=\"abc/db-indices\" duration_secs=2.0887e-5 count=0\n2025-09-16T13:36:35.291644Z DEBUG influxdb3_write::table_index_cache: Found 0 existing table indices to update\n```\n\nAfter that it freezes. Trying to execute a `influxdb3 show databases` inside the Docker container fails with a \"Connection refused\" error.\n\nThese are the contents of the influxdb3 data folder:\n\n```\ninfluxdb3@d60c67963dd4:/var/lib/influxdb3/abc$ du -sh *\n88K     _catalog_checkpoint\n92K     catalog\n1.3M    catalogs\n18G     dbs\n551M    snapshots\n4.0K    table-index-conversion-completed\n1.2G    table-snapshots\n50M     wal\n```\n",
      "solution": "I'm also experiencing a similar issue. When will it be fixed?\n\n\n---\n\nI've written some scripts that attempt to reproduce this issue based on the description I'm seeing in this issue. The current script I've got does the following:\n\n* Runs version 3.0.3 for 15 seconds with the load-generator writing data to it, then kills the process.\n* Upgrades to version 3.3.0 then writes to it for 15 seconds at a time then checks for a log message that I expect to be missing (based on logs shared in this issue and #26802) before restarting it at the same version. Repeat this about 30 times to give race conditions a chance to occur. \n* Upgrades to version 3.4.2 then performs the same write/stop/restart loop.\n\nIf at any time the most recently-killed process's stdout is missing the log that indicates the TableIndexCache finished initializing is missing then the reproducer script breaks out of the loop with a message indicating it's missing.\n\nSo far I have only seen successful TableIndexCache initializations. I also did some exploratory testing where I've explicitly deleted the `table-index-conversion-completed` marker object or one of the table index objects or table snapshot object then restarted the process. The TableIndexCache is meant to be fault tolerant to index files missing (since the index state can always be rebuilt from snapshots) so it's expected in my view that it can overcome any of the object types i mentioned being deleted.\n\nTomorrow I'll push up a draft PR with some additional trace-level logging that users who are experiencing this issue can try running.\n\nIn the meantime, it would be helpful if anyone who has reported this issue could provide the following:\n\n* a tarball containing a data directory that I can use to directly reproduce the issue myself\n* more information about your setup -- CLI flags, environment variables\n* the number of snapshot files in your data directory when the process hangs on startup\n\n---\n\nOkay, while writing my previous comment I did actually see one of the restart iterations lead to a run with the log I was looking for missing (after 24 restarts). Here are the logs I'm looking at, filtered for the `table_index_cache` module prefix with the `server starting` lines included to show when the process restarts:\n\n```\nzsh/3 8796  (git)-[3.4]-% cat test-data/logs/influxdb3-oss.log | grep -E -a \"(table_index_cache|server starting)\"\n\\nstarting new process\\n\n\n2025-09-23T06:44:02.255774Z  INFO influxdb3_lib::commands::serve: InfluxDB 3 Core server starting node_id=writer git_hash=02d7ee1e6fec5b62debbe862881562e451624de6 version=3.3.0 uuid=f247b91a-95f2-4208-9428-3322eb7dc6a2 num_cpus=12\n2025-09-23T06:44:02.274332Z  INFO influxdb3_write::table_index_cache: Initializing table index cache node_id=writer max_entries=Some(100) concurrency_limit=20\n2025-09-23T06:44:02.274346Z  INFO influxdb3_write::table_index_cache: creating table indices from split snapshots\n2025-09-23T06:44:02.284394Z  INFO influxdb3_write::table_index_cache: loading snapshot object metas starting from snapshot sequence SnapshotSequenceNumber(10)\n2025-09-23T06:44:02.284826Z  INFO influxdb3_write::table_index_cache: splitting snapshots into table snapshots\n2025-09-23T06:44:02.286353Z DEBUG influxdb3_write::table_index_cache: Skipping persisted snapshot snapshot_seq=1 reason=\"already_processed\" last_completed=10\n2025-09-23T06:44:02.286472Z DEBUG influxdb3_write::table_index_cache: Skipping persisted snapshot snapshot_seq=2 reason=\"already_processed\" last_completed=10\n2025-09-23T06:44:02.286540Z DEBUG influxdb3_write::table_index_cache: Skipping persisted snapshot snapshot_seq=3 reason=\"already_processed\" last_completed=10\n2025-09-23T06:44:02.286604Z DEBUG influxdb3_write::table_index_cache: Skipping persisted snapshot snapshot_seq=4 reason=\"already_processed\" last_completed=10\n2025-09-23T06:44:02.286665Z DEBUG influxdb3_write::table_index_cache: Skipping persisted snapshot snapshot_seq=5 reason=\"already_processed\" last_completed=10\n2025-09-23T06:44:02.286729Z DEBUG influxdb3_write::table_index_cache: Skipping persisted snapshot snapshot_seq=6 reason=\"already_processed\" last_completed=10\n2025-09-23T06:44:02.286791Z DEBUG influxdb3_write::table_index_cache: Skipping persisted snapshot snapshot_seq=7 reason=\"already_processed\" last_completed=10\n2025-09-23T06:44:02.286860Z DEBUG influxdb3_write::table_index_cache: Skipping persisted snapshot snapshot_seq=8 reason=\"already_processed\" last_completed=10\n2025-09-23T06:44:02.286925Z DEBUG influxdb3_write::table_index_cache: Skipping persisted snapshot snapshot_seq=9 reason=\"already_processed\" last_completed=10\n2025-09-23T06:44:02.286990Z DEBUG influxdb3_write::table_index_cache: Skipping persisted snapshot snapshot_seq=10 reason=\"already_processed\" last_completed=10\n2025-09-23T06:44:02.287052Z DEBUG influxdb3_write::table_index_cache: Processing persisted snapshot snapshot_seq=11 snapshot_path=writer/snapshots/18446744073709551604.info.json\n2025-09-23T06:44:02.288018Z  INFO influxdb3_write::table_index_cache: split snapshot at Path { raw: \"writer/snapshots/18446744073709551604.info.json\" } into table snaphots\n2025-09-23T06:44:02.288285Z  INFO influxdb3_write::table_index_cache: Completed splitting persisted snapshots duration_ms=13 snapshots_processed=1 snapshots_skipped=10 last_sequence=11\n2025-09-23T06:44:02.288309Z  INFO influxdb3_write::table_index_cache: Starting table index cache synchronization from object store node_prefix=writer\n2025-09-23T06:44:02.289205Z DEBUG influxdb3_write::table_index_cache: Found 1 existing table indices to update\n2025-09-23T06:44:02.289257Z DEBUG influxdb3_write::table_index_cache: Updating table index from object store table_id=(writer, DbId(1), TableId(0)) cached=false\n2025-09-23T06:44:02.291019Z DEBUG influxdb3_write::table_index_cache: Found 1 tables without indices to create\n2025-09-23T06:44:02.291047Z DEBUG influxdb3_write::table_index_cache: Cache miss for table index, loading from object store table_id=(writer, DbId(1), TableId(0)) cache_size_before=0\n2025-09-23T06:44:02.291777Z DEBUG influxdb3_write::table_index_cache: Completed table index update table_id=(writer, DbId(1), TableId(0)) duration_ms=2\n2025-09-23T06:44:02.291871Z  INFO influxdb3_write::table_index_cache: updated table index for TableIndexId { node_id: \"writer\", db_id: DbId(1), table_id: TableId(0) }\n\n\\nstarting new process\\n\n\n2025-09-23T06:53:59.449221Z  INFO influxdb3_lib::commands::serve: InfluxDB 3 Core server starting node_id=writer git_hash=571299afed3644c69811df9a71816446af64dec0 version=3.4.2 uuid=9abbcbca-74c5-4b80-bc7a-663cda0dbf3a num_cpus=12\n2025-09-23T06:53:59.483302Z  INFO influxdb3_write::table_index_cache: creating table indices from split snapshots\n2025-09-23T06:53:59.483402Z  INFO influxdb3_write::table_index_cache: loading snapshot object metas starting from snapshot sequence SnapshotSequenceNumber(11)\n2025-09-23T06:53:59.483686Z  INFO influxdb3_write::table_index_cache: splitting snapshots into table snapshots\n2025-09-23T06:53:59.491911Z DEBUG influxdb3_write::table_index_cache: Skipping persisted snapshot snapshot_seq=1 reason=\"already_processed\" last_completed=11\n2025-09-23T06:53:59.492058Z DEBUG influxdb3_write::table_index_cache: Skipping persisted snapshot snapshot_seq=2 reason=\"already_processed\" last_completed=11\n2025-09-23T06:53:59.492080Z DEBUG influxdb3_write::table_index_cache: Skipping persisted snapshot snapshot_seq=3 reason=\"already_processed\" last_completed=11\n2025-09-23T06:53:59.492098Z DEBUG influxdb3_write::table_index_cache: Skipping persisted snapshot snapshot_seq=4 reason=\"already_processed\" last_completed=11\n2025-09-23T06:53:59.492115Z DEBUG influxdb3_write::table_index_cache: Skipping persisted snapshot snapshot_seq=5 reason=\"already_processed\" last_completed=11\n2025-09-23T06:53:59.492136Z DEBUG influxdb3_write::table_index_cache: Skipping persisted snapshot snapshot_seq=6 reason=\"already_processed\" last_completed=11\n2025-09-23T06:53:59.492161Z DEBUG influxdb3_write::table_index_cache: Skipping persisted snapshot snapshot_seq=7 reason=\"already_processed\" last_completed=11\n2025-09-23T06:53:59.492188Z DEBUG influxdb3_write::table_index_cache: Skipping persisted snapshot snapshot_seq=8 reason=\"already_processed\" last_completed=11\n2025-09-23T06:53:59.492213Z DEBUG influxdb3_write::table_index_cache: Skipping persisted snapshot snapshot_seq=9 reason=\"already_processed\" last_completed=11\n2025-09-23T06:53:59.492244Z DEBUG influxdb3_write::table_index_cache: Skipping persisted snapshot snapshot_seq=10 reason=\"already_processed\" last_completed=11\n2025-09-23T06:53:59.492269Z DEBUG influxdb3_write::table_index_cache: Skipping persisted snapshot snapshot_seq=11 reason=\"already_processed\" last_completed=11\n2025-09-23T06:53:59.492526Z  INFO influxdb3_write::table_index_cache: Completed splitting persisted snapshots duration_ms=9 snapshots_processed=0 snapshots_skipped=11 last_sequence=11\n2025-09-23T06:53:59.492544Z  INFO influxdb3_write::table_index_cache: Starting table index cache synchronization from object store node_prefix=writer\n2025-09-23T06:53:59.493456Z DEBUG influxdb3_write::table_index_cache: Found 1 existing table indices to update\n2025-09-23T06:53:59.493513Z DEBUG influxdb3_write::table_index_cache: Found 0 tables without indices to create\n2025-09-23T06:53:59.493969Z DEBUG influxdb3_write::table_index_cache: Updating table index from object store table_id=(writer, DbId(1), TableId(0)) cached=false\n2025-09-23T06:53:59.495821Z DEBUG influxdb3_write::table_index_cache: Completed table index update table_id=(writer, DbId(1), TableId(0)) duration_ms=1\n2025-09-23T06:53:59.496125Z DEBUG influxdb3_write::table_index_cache: updated table index for TableIndexId { node_id: \"writer\", db_id: DbId(1), table_id: TableId(0) }\n2025-09-23T06:53:59.496296Z  INFO influxdb3_write::table_index_cache: Completed table index cache synchronization duration_ms=3 indices_updated=1 indices_created=0 total_cached=1 cache_capacity=Some(100\n```\n\nThe line I look for to indicate a successful table index cache initialization is\n```\n2025-09-23T06:53:59.496296Z  INFO influxdb3_write::table_index_cache: Completed table index cache synchronization duration_ms=3 indices_updated=1 indices_created=0 total_cached=1\n```\n\nAt the point i the logs where I expect this message but don't see it (just prior to the restart) I see (unfiltered):\n\n```\n2025-09-23T06:44:02.290995Z DEBUG object_store_metrics::log: object store operation store_type=\"main\" op=\"list\" result=\"success\" prefix=\"writer/table-snapshots\" duration_secs=0.001757752 count=1\n2025-09-23T06:44:02.291019Z DEBUG influxdb3_write::table_index_cache: Found 1 tables without indices to create\n2025-09-23T06:44:02.291047Z DEBUG influxdb3_write::table_index_cache: Cache miss for table index, loading from object store table_id=(writer, DbId(1), TableId(0)) cache_size_before=0\n2025-09-23T06:44:02.291195Z DEBUG object_store_metrics::log: object store operation store_type=\"main\" op=\"get_opts\" result=\"success\" location=\"writer/db-indices/1/0/index.info.json\" duration_secs=0.000126064 bytes=3214 headers_secs=0.000126064 first_byte_secs=0.000126064\n2025-09-23T06:44:02.291426Z DEBUG object_store_metrics::log: object store operation store_type=\"main\" op=\"list\" result=\"success\" prefix=\"writer/table-snapshots/1/0\" duration_secs=0.000142994 count=1\n2025-09-23T06:44:02.291487Z DEBUG object_store_metrics::log: object store operation store_type=\"main\" op=\"put\" result=\"success\" location=\"writer/db-indices/1/0/index.info.json\" duration_secs=0.000428848 bytes=3512\n2025-09-23T06:44:02.291685Z DEBUG object_store_metrics::log: object store operation store_type=\"main\" op=\"delete\" result=\"success\" location=\"writer/table-snapshots/1/0/00000000000000000011.info.json\" duration_secs=0.000178539\n2025-09-23T06:44:02.291669Z DEBUG object_store_metrics::log: object store operation store_type=\"main\" op=\"get_opts\" result=\"success\" location=\"writer/table-snapshots/1/0/00000000000000000011.info.json\" duration_secs=0.00021137 bytes=557 headers_secs=0.00021137 first_byte_secs=0.00021137\n2025-09-23T06:44:02.291777Z DEBUG influxdb3_write::table_index_cache: Completed table index update table_id=(writer, DbId(1), TableId(0)) duration_ms=2\n2025-09-23T06:44:02.291871Z  INFO influxdb3_write::table_index_cache: updated table index for TableIndexId { node_id: \"writer\", db_id: DbId(1), table_id: TableId(0) }\n2025-09-23T06:44:02.292377Z DEBUG object_store_metrics::log: object store operation store_type=\"main\" op=\"put\" result=\"success\" location=\"writer/db-indices/1/0/index.info.json\" duration_secs=0.000256186 bytes=3512\n2025-09-23T06:44:02.292492Z DEBUG object_store_metrics::log: object store operation store_type=\"main\" op=\"delete\" result=\"error\" location=\"writer/table-snapshots/1/0/00000000000000000011.info.json\" duration_secs=6.0536e-5\n2025-09-23T06:44:02.292846Z  WARN executor: DedicatedExecutor dropped without calling shutdown()\n2025-09-23T06:44:02.292884Z  WARN executor: DedicatedExecutor dropped without waiting for worker termination\n2025-09-23T06:44:02.293889Z  WARN executor: DedicatedExecutor dropped without calling shutdown()\n2025-09-23T06:44:02.293928Z  WARN executor: DedicatedExecutor dropped without waiting for worker termination\n2025-09-23T06:44:02.294995Z  INFO influxdb3_cache::parquet_cache: cache request handler closed\n```\n\nThe problem seems to be a second attempt to delete a file that was already deleted earlier in the process lifetime:\n\n```\n# the successful deletion attempt:\n2025-09-23T06:44:02.292492Z DEBUG object_store_metrics::log: object store operation store_type=\"main\" op=\"delete\" result=\"error\" location=\"writer/table-snapshots/1/0/00000000000000000011.info.json\" duration_secs=6.0536e-5\n\n# the failed deletion attempt:\n2025-09-23T06:44:02.292492Z DEBUG object_store_metrics::log: object store operation store_type=\"main\" op=\"delete\" result=\"error\" location=\"writer/table-snapshots/1/0/00000000000000000011.info.json\" duration_secs=6.0536e-5\n```\n\nThis seems to be related to an index update operation:\n\n```\n2025-09-23T06:44:02.291777Z DEBUG influxdb3_write::table_index_cache: Completed table index update table_id=(writer, DbId(1), TableId(0)) duration_ms=2\n```\n\nThis is should be enough information for me to figure out the bug i'm reproducing tomorrow. At minimum, deletion attempts should tolerate the target file being missing. It's also not yet clear to me that this is exactly the same issue that users have been reporting; I'm guessing since users report the issue happening more readily (eg it took 24 restarts on version 3.3.0 for me to see it) that there is another problem. \n\nSo it will still be useful for users experiencing the problem to provide more details about their setup.",
      "labels": [
        "blocked-on/external",
        "v3"
      ],
      "created_at": "2025-09-16T15:18:02Z",
      "closed_at": "2025-10-26T17:46:11Z",
      "url": "https://github.com/influxdata/influxdb/issues/26810",
      "comments_count": 8
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26906,
      "title": "panic: runtime error: invalid memory address or nil pointer dereference",
      "problem": "Version: 2.7.12\n\nWe get these errors often while jmeter plugin is sending data to influxdb2:\n\n`ts=2025-10-14T09:04:01.993266Z lvl=info msg=Starting log_id=0z_VEtBW000 service=telemetry interval=8h ts=2025-10-14T09:04:01.993612Z lvl=info msg=Listening log_id=0z_VEtBW000 service=tcp-listener transport=http addr=:8086 port=8086 ts=2025-10-14T09:04:08.770837Z lvl=warn msg=\"internal error not returned to client\" log_id=0z_VEtBW000 handler=error_logger error=\"context canceled\" ts=2025-10-14T09:04:08.777111Z lvl=warn msg=\"internal error not returned to client\" log_id=0z_VEtBW000 handler=error_logger error=\"context canceled\" ts=2025-10-14T09:04:11.269444Z lvl=warn msg=\"internal error not returned to client\" log_id=0z_VEtBW000 handler=error_logger error=\"context canceled\" ts=2025-10-14T09:04:11.269411Z lvl=info msg=\"Dispatcher panic\" log_id=0z_VEtBW000 service=storage-reads component=dispatcher error=\"panic: runtime error: invalid memory address or nil pointer dereference\" stacktrace=\"goroutine 4246 [running]:\\nruntime/debug.Stack()\\n\\t/go/src/runtime/debug/stack.go:26 +0x5e\\ngithub.com/influxdata/flux/execute.(*poolDispatcher).recover(0xc02af12af0)\\n\\t/go/pkg/mod/github.com/influxdata/flux@v0.196.1/execute/recover.go:53 +0x1e5\\npanic({0x7fe571ee8440?, 0x7fe573bc45f0?})\\n\\t/go/src/runtime/panic.go:791 +0x132\\ngithub.com/influxdata/influxdb/v2/storage/flux.(*integerGroupTable).advance(0xc00cfaa380)\\n\\t/root/project/storage/flux/table.gen.go:1716 +0xa9\\ngithub.com/influxdata/influxdb/v2/storage/flux.(*table).do(0xc01c1359c8?, 0xc01f6201c8, 0xc01c1359b8)\\n\\t/root/project/storage/flux/table.go:95 +0x1e2\\ngithub.com/influxdata/influxdb/v2/storage/flux.(*integerGroupTable).Do(0x7fe57219dc20?, 0x7fe570f51101?)\\n\\t/root/project/storage/flux/table.gen.go:1690 +0x2b\\ngithub.com/influxdata/flux/execute.(*consecutiveTransportTable).Do(0xc01f620198, 0xc01f640460)\\n\\t/go/pkg/mod/github.com/influxdata/flux@v0.196.1/execute/transport.go:554 +0x76\\ngithub.com/influxdata/flux/execute.(*simpleAggregateTransformation).Process(0xc022154cc0, {0x40, 0x22, 0x1c, 0xf8, 0xbf, 0xc4, 0x50, 0xff, 0xad, ...}, ...)\\n\\t/go/pkg/mod/github.com/influxdata/flux@v0.196.1/execute/aggregate.go:505 +0x65d\\ngithub.com/influxdata/flux/execute.(*transformationTransportAdapter).ProcessMessage(0xc00c1a1710, {0x7fe5723ba1d8, 0xc01f6301e0})\\n\\t/go/pkg/mod/github.com/influxdata/flux@v0.196.1/execute/transport.go:652 +0xa6f\\ngithub.com/influxdata/flux/execute.(*consecutiveTransport).processMessage(0xc00d51a4b0, {0x7fe5723ba1d8, 0xc01f6301e0})\\n\\t/go/pkg/mod/github.com/influxdata/flux@v0.196.1/execute/transport.go:297 +0xb5\\ngithub.com/influxdata/flux/execute.(*consecutiveTransport).processMessages(0xc00d51a4b0, {0x7fe5723b7830?, 0xc0155a2370?}, 0xa)\\n\\t/go/pkg/mod/github.com/influxdata/flux@v0.196.1/execute/transport.go:251 +0xe5\\ngithub.com/influxdata/flux/execute.(*poolDispatcher).doWork(0xc02af12af0, {0x7fe5723b7830, 0xc0155a2370})\\n\\t/go/pkg/mod/github.com/influxdata/flux@v0.196.1/execute/dispatcher.go:157 +0x18c\\ngithub.com/influxdata/flux/execute.(*poolDispatcher).run(0xc02af12af0, {0x7fe5723b7830, 0xc0155a2370})\\n\\t/go/pkg/mod/github.com/influxdata/flux@v0.196.1/execute/dispatcher.go:135 +0x49\\ngithub.com/influxdata/flux/execute.(*poolDispatcher).Start.func1()\\n\\t/go/pkg/mod/github.com/influxdata/flux@v0.196.1/execute/dispatcher.go:75 +0x74\\ncreated by github.com/influxdata/flux/execute.(*poolDispatcher).Start in goroutine 840\\n\\t/go/pkg/mod/github.com/influxdata/flux@v0.196.1/execute/dispatcher.go:71 +0x51\\n\" ts=2025-10-14T09:04:11.269777Z lvl=warn msg=\"internal error not returned to client\" log_id=0z_VEtBW000 handler=error_logger error=\"context canceled\"`\n\nWe have to store around 19gb of data for this to appear.\n\nHow we start it in docker-compose:\n\n```\nservices:\n  jmeterinfluxdb:\n    container_name: jmeterinfluxdb\n    hostname: jmeterinfluxdb.my.gr\n    image: influxdb:2.7.12\n    build:\n      context: .\n      dockerfile: Dockerfile\n    cpus: \"6.0\"\n    mem_limit: 10g\n    environment:\n      - DOCKER_INFLUXDB_INIT_MODE=setup\n      - DOCKER_INFLUXDB_INIT_USERNAME=admin\n      - DOCKER_INFLUXDB_INIT_PASSWORD=admin\n      - DOCKER_INFLUXDB_INIT_ORG=my.gr\n      - DOCKER_INFLUXDB_INIT_BUCKET=my-bucket\n      - DOCKER_INFLUXDB_INIT_RETENTION=16w\n      - DOCKER_INFLUXDB_INIT_ADMIN_TOKEN=my-token\n      - INFLUXD_STORAGE_CACHE_MAX_MEMORY_SIZE=4g\n      - INFLUXD_STORAGE_WAL_FSYNC_DELAY=50ms\n      - INFLUXD_STORAGE_MAX_CONCURRENT_COMPACTIONS=3\n      - INFLUXD_STORAGE_COMPACT_FULL_WRITE_COLD_DURATION=4h\n      - INFLUXD_QUERY_MEMORY_BYTES=3g\n      - INFLUXD_QUERY_CONCURRENCY=3\n      - INFLUXD_LOG_LEVEL=info\n    volumes:\n      - /data/influxdb2:/var/lib/influxdb2\n      - /data/backup:/backup\n    network_mode: host\n    restart: unless-stopped\n```\n\n\n",
      "solution": "The problem we really experienced was outage. Influxdb after these error exploded in CPU and was using all 6cpus while we had not reached 10gb RAM and no more metrics could be accepted resulting in grafana to not be able to show metrics from influxdb anymore. I don't know how that relates with the above bug but I had to stop influxdb and start it again and still these errors were appearing. I stopped again and I  run all inspect verify-* cmds without issue and no corruption detected. I ended up backing up and restoring to another directory to continue operations also using these extra env vars:\n\n      - INFLUXD_STORAGE_CACHE_MAX_MEMORY_SIZE=4g\n      - INFLUXD_STORAGE_WAL_FSYNC_DELAY=50ms\n      - INFLUXD_STORAGE_MAX_CONCURRENT_COMPACTIONS=3\n      - INFLUXD_STORAGE_COMPACT_FULL_WRITE_COLD_DURATION=4h\n      - INFLUXD_QUERY_MEMORY_BYTES=3g\n      - INFLUXD_QUERY_CONCURRENCY=3\n      - INFLUXD_LOG_LEVEL=info\n\n That solved the incoming metrics issue....the original error still appears but without issues now. Most likely this bug is indeed unrelated to the outage I had and I got it noticed at that time of the outage thinking it is causing this issue but in reality the above tuning saved the day. :) Assumption is the mother of all f****ups :) Thanks for your time!",
      "labels": [
        "area/2.x"
      ],
      "created_at": "2025-10-14T11:12:36Z",
      "closed_at": "2025-10-15T13:13:08Z",
      "url": "https://github.com/influxdata/influxdb/issues/26906",
      "comments_count": 2
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26662,
      "title": "ERROR influxdb3_server::http: cannot authenticate token e=MissingToken",
      "problem": "__Steps to reproduce:__\nTrying to install InfluxDB 3 using Portainer Stacks\n\n__Expected behaviour:__\nWorking installation of InfluxDB3\n\n__Actual behaviour:__\nContainer is starting up, but then shutting down due to an error.\n\n\n\n__Config:__\nPortainer Stack Editor text:\nversion: '3.8'\nservices:\n  influxdb3:\n    image: influxdb:3-core\n    container_name: influxdb3\n    restart: always\n    ports:\n      - \"8181:8181\"\n\n    volumes:\n      - /docker-volumes/influxdb3:/var/lib/influxdb\n    command: >\n      influxdb3 serve\n      --object-store file\n      --data-dir /var/lib/influxdb\n      --node-id influxdb3-node\n      --log-filter info  \n    environment:\n      # Basic InfluxDB 3 Core configuration\n      - INFLUXD_BOLT_PATH=/var/lib/influxdb/influxd.bolt\n      - INFLUXD_ENGINE_PATH=/var/lib/influxdb/engine\n      #Copied from 2\n      - DOCKER_INFLUXDB_INIT_MODE=setup\n      - DOCKER_INFLUXDB_INIT_USERNAME=${INFLUXDB_USERNAME}\n      - DOCKER_INFLUXDB_INIT_PASSWORD=${INFLUXDB_PASSWORD}\n      - DOCKER_INFLUXDB_INIT_ORG=${INFLUXDB_ORG}\n      - DOCKER_INFLUXDB_INIT_BUCKET=${INFLUXDB_BUCKET}\n\n\n    networks:\n      - webserver_default\n\nnetworks:\n  webserver_default:\n    external: true\n\n__Logs:__\n2025-08-02T11:27:23.320987Z  INFO influxdb3_lib::commands::serve: InfluxDB 3 Core server starting node_id=influxdb3-node git_hash=02d7ee1e6fec5b62debbe862881562e451624de6 version=3.3.0 uuid=501fc729-2dcd-4846-b016-40b6424faba1 num_cpus=4\n2025-08-02T11:27:23.321180Z  INFO influxdb3_clap_blocks::object_store: Object Store db_dir=\"/var/lib/influxdb\" object_store_type=\"Directory\"\n2025-08-02T11:27:23.321420Z  INFO influxdb3_lib::commands::serve: Creating shared query executor num_threads=4\n2025-08-02T11:27:23.329687Z  INFO influxdb3_write::table_index_cache: Initializing table index cache node_id=influxdb3-node max_entries=Some(100) concurrency_limit=20\n2025-08-02T11:27:23.329729Z  INFO influxdb3_write::table_index_cache: creating table indices from split snapshots\n2025-08-02T11:27:23.331766Z  INFO influxdb3_write::table_index_cache: loading snapshot object metas starting from snapshot sequence SnapshotSequenceNumber(0)\n2025-08-02T11:27:23.331957Z  INFO influxdb3_write::table_index_cache: Starting table index cache synchronization from object store node_prefix=influxdb3-node\n2025-08-02T11:27:23.332205Z  INFO influxdb3_write::table_index_cache: Completed table index cache synchronization duration_ms=0 indices_updated=0 indices_created=0 total_cached=0 cache_capacity=Some(100)\n2025-08-02T11:27:23.336323Z  INFO influxdb3_catalog::catalog::update: create database name=\"_internal\"\n2025-08-02T11:27:23.336505Z  INFO influxdb3_lib::commands::serve: catalog initialized catalog_uuid=437f1c73-da92-402b-842e-900b86286d03\n2025-08-02T11:27:23.336563Z  INFO influxdb3_catalog::catalog::update: register node node_id=\"influxdb3-node\" core_count=4 mode=[Core]\n2025-08-02T11:27:23.336610Z  INFO influxdb3_write::retention_period_handler: Starting retention period handler background task check_interval_seconds=1800\n2025-08-02T11:27:23.337263Z  INFO influxdb3_catalog::object_store: persisted next catalog sequence put_result=PutResult { e_tag: Some(\"930628-63b60288a96f2-174\"), version: None } object_path=CatalogFilePath(Path { raw: \"influxdb3-node/catalogs/00000000000000000007.catalog\" })\n2025-08-02T11:27:23.337489Z  INFO influxdb3_lib::commands::serve: catalog initialized instance_id=\"6e707bee-64a3-4e9d-961c-74a52aa19c34\"\n2025-08-02T11:27:23.337549Z  INFO influxdb3_catalog::catalog::update: set gen1 duration duration_ns=600000000000\n2025-08-02T11:27:23.337669Z  INFO influxdb3_wal::object_store: replaying WAL files\n2025-08-02T11:27:23.337683Z  INFO influxdb3_wal::object_store: completed replaying wal files time_taken=15.299\u00b5s\n2025-08-02T11:27:23.337711Z  INFO influxdb3_lib::commands::serve: setting up background mem check for query buffer\n2025-08-02T11:27:23.337716Z  INFO influxdb3_lib::commands::serve: setting up telemetry store\n2025-08-02T11:27:23.337781Z  INFO influxdb3_write::deleter: Started catalog hard deleter task. delete_grace_period=86400s\n2025-08-02T11:27:26.960090Z  INFO influxdb3_lib::commands::serve: setting up server with authz disabled for paths paths_without_authz=[]\n2025-08-02T11:27:26.960149Z  INFO influxdb3_server: startup time: 3639ms address=0.0.0.0:8181\n2025-08-02T11:27:26.960311Z ERROR influxdb3_server::http: cannot authenticate token e=MissingToken\n2025-08-02T11:27:30.097784Z ERROR influxdb3_server::http: cannot authenticate token e=MissingToken\n",
      "solution": "As @Marukome0743 pointed out, you need to follow the steps in the docs to create an operator token. Were you able to get your server up and running after that @Johannf78?\n\n---\n\n@philjb You solved my problem.\n\nI already tried with `influxdb3 --host HOSTNAME create token` since the syntax was `influxdb3 [GLOBAL-OPTIONS] [COMMAND]` and not `[COMMAND] [GLOBAL-OPTIONS]`\n\n Thanks! \ud83d\udc4d",
      "labels": [
        "v3"
      ],
      "created_at": "2025-08-02T11:37:04Z",
      "closed_at": "2025-10-14T18:21:57Z",
      "url": "https://github.com/influxdata/influxdb/issues/26662",
      "comments_count": 13
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26689,
      "title": "After upgrading InfluxDB from v1.8.10 to v1.11.8, a `show tag values` query has become extremely slow",
      "problem": "I\u2019m in the process of upgrading our production InfluxDB from v1.8.10 to v1.11.8 as a step toward eventually migrating to version 3. I was able to follow [the upgrade procedure][1], and the database starts up without any errors afterward. However, post-upgrade, a `SHOW TAG KEYS` query that we rely on heavily has become significantly slower compared to v1.8.10. The query result is exactly the same regardless of the InfluxDB version, but in v1.8.10 the query executes in 300 milliseconds whereas in v1.11.8 it takes 70 seconds to complete! The tag key I'm querying in my tests has ca. 11000 values. ~~Note that it does not make a difference whether I'm using the `tsi1` or `inmem` index type~~ (edit: I was wrong here). Below are the exact results from the tests I ran on both versions in a local environment.\n\n\n#### Test result with version 1.8.10\n\n```\n> hyperfine \"http -a $INFLUX_USER:$INFLUX_PASS get 'http://localhost:8086/query?db=tenant_X&epoch=ms&q=show tag values with key = \\\"device_id\\\"'\"\n\nBenchmark 1: http -a <REDACTED>:<REDACTED> get 'http://localhost:8086/query?db=tenant_X&epoch=ms&q=show tag values with key = \"device_id\"'\n  Time (mean \u00b1 \u03c3):     347.8 ms \u00b1   9.0 ms    [User: 229.5 ms, System: 26.8 ms]\n  Range (min \u2026 max):   337.1 ms \u2026 364.7 ms    10 runs\n```\n\n#### Test result with version 1.11.8\n\n```\n> hyperfine \"http -a $INFLUX_USER:$INFLUX_PASS get 'http://localhost:8086/query?db=tenant_X&epoch=ms&q=show tag values with key = \\\"device_id\\\"'\n\nBenchmark 1: http -a <REDACTED>:<REDACTED> get 'http://localhost:8086/query?db=tenant_X&epoch=ms&q=show tag values with key = \"device_id\"'\n  Time (mean \u00b1 \u03c3):     75.883 s \u00b1  3.790 s    [User: 0.280 s, System: 0.034 s]\n  Range (min \u2026 max):   67.936 s \u2026 82.327 s    10 runs\n```\n\n\n\n\n\n#### Additional information regarding the test conditions and upgrade steps\n\nI'm using a Docker setup with a snapshot of the production database to test the upgrade. The production data resides in a Docker volume called `influxdb`. \n\n```\n# This is how I started version 1.8.10\ndocker run -it --rm -p 8086:8086 -e INFLUXDB_ADMIN_USER=$INFLUXDB_ADMIN_USER -e INFLUXDB_ADMIN_PASSWORD=$INFLUXDB_ADMIN_PASSWORD -v influxdb:/var/lib/influxdb influxdb:1.8.10\n\n# I ran the test for v1.8.10 via a separate terminal. You can find the result above.\n# Afterwards, I terminated the container to prepare the upgrade to v1.11.8\n\n# I changed the owner of all files in the InfluxDB docker volume because\n# the user in the Docker image for InfluxDB 1.11.8 has a different uid and gid (1500)\n\ncd /var/lib/docker/volumes/influxdb/_data/\nchown -R 1500:1500 .\n\n# I removed all \"_series\" and \"index\" directories in the volume\nfind data -name _series -exec rm -rv {} +\nfind data -name index -exec rm -rv {} +\n\n# I started an interactive container with InfluxDB 1.11.8 and a Bash shell to migrate the index\ndocker run -it --rm -e INFLUXDB_DATA_INDEX_VERSION=tsi1 -v influxdb:/var/lib/influxdb influxdb:1.11.8 /bin/bash\n\n# I ran the command that builds the TSI inside the interactive container\ninflux_inspect buildtsi -datadir /var/lib/influxdb/data -waldir /var/lib/influxdb/wal\n\n# I exited the interactive container and started database version 1.11.8 in a new container\ndocker run -it --rm -p 8086:8086 -e INFLUXDB_ADMIN_USER=$INFLUXDB_ADMIN_USER -e INFLUXDB_ADMIN_PASSWORD=$INFLUXDB_ADMIN_PASSWORD -e INFLUXDB_DATA_INDEX_VERSION=tsi1 -v influxdb:/var/lib/influxdb influxdb:1.11.8\n\n# I ran the test for v1.11.8 via a separate terminal. You can find the result above.\n```\n\n[1]: https://docs.influxdata.com/influxdb/v1/administration/upgrading/\n\n",
      "solution": "@philjb: Thanks a lot for your response and tests! I tried your suggestion with the `-compact-series-file` flag, and this resolves the issue. The performance of the `SHOW TAG KEYS` query in v1.11.8 is now comparable to v1.8.10. I hadn\u2019t considered this flag because the [upgrade instructions][1] said we needed to rebuild the index, and the [buildtsi documentation][2] stated that using the flag does not rebuild the index.\n\nFor additional context, in case you\u2019d like to run further tests: our device_ids are spread across multiple measurements. In fact, we create a separate measurement for each device to avoid field type conflicts across devices with different software versions. The IDs range from 19 to 32 characters in length. We use the `SHOW TAG KEYS` query to populate a Grafana dashboard variable, which is then used throughout the dashboard to query the measurements of the devices the user selects.\n\n\n[1]: https://docs.influxdata.com/influxdb/v1/administration/upgrading/#upgrade-to-influxdb-111x\n[2]: https://docs.influxdata.com/influxdb/v1/tools/influx_inspect/#buildtsi\n\n\n---\n\nHi @usommerl - That's great! I'm really glad the tsi compaction resolved the performance issue in the metadata query. Thanks for taking the time to test my suggestions and report back. This helps me and other OSS users too. \n\nWe recommend periodic offline TSI compactions as the online compaction sometimes doesn't get a chance to run if conditions for compaction aren't met. In fact, [our start scripts for our Linux packages](https://github.com/influxdata/influxdb/blob/7ca78d1342c46a26759c62af6e86f237b4d3734a/.circleci/packages/influxdb/fs/usr/lib/influxdb/scripts/influxd-systemd-start.sh#L42C53-L50C3) include running buildtsi with the compaction option before every startup. \n\n1.11 has [index improvements](https://docs.influxdata.com/influxdb/v1/about_the_project/release-notes/#bug-fixes-2) (first in 1.10 I believe) that I hope make tsi more stable for you (maybe a minor performance improvement) now that you're on 1.11. We're in the final testing phases of 1.12 too -- that should be out this month.\n\nI noticed too the upgrade docs need some improvements. The [docs team is working on them now](https://github.com/influxdata/docs-v2/pull/6323#issue-comment-box). As you saw, [the `buildtsi` code](https://github.com/influxdata/influxdb/blob/75eb209f7205f46d3a00e93bce116112575cbb44/cmd/influx_inspect/buildtsi/buildtsi.go#L118-L130) doesn't rebuild when compacting. I agree it should be clearer in which situations one should rebuild or compact or rebuild and then compact. I'll see if we can clarify.\n\nEach tag in its own measurement is an interesting strategy. Thanks for explaining how you've set up your schema. This is useful information if I want to recreate your scenario.\n\nI hope for smooth sailing but reach out if you encounter anything else. Cheers.\n\n---\n\n@philjb I'm sorry, I was so relieved last week because the issue seemed resolved that I didn't realize I made a mistake while testing your suggestion. I forgot to rebuild the index before compacting the series file! \n\nAfter rerunning the procedure correctly, I noticed that query performance was slow again. **Compacting the series file doesn\u2019t appear to have a positive effect** \u2014 the slowdown seems to be caused by rebuilding the index. You probably should consider this in influxdata/docs-v2#6328. I ran some additional tests using the same database snapshot as before:\n\n#### Test 1: Reproducing my mistake (Only compacting the series file without rebuilding the index)\n```sh\n# This is the command that I ran before starting the database:\ninflux_inspect buildtsi -compact-series-file -datadir /var/lib/influxdb/data -waldir /var/lib/influxdb/wal\n\n# Test result:\nBenchmark 1: http get 'http://localhost:8086/query?db=tenant_X&q=show tag values with key = \"device_id\"'\n  Time (mean \u00b1 \u03c3):     322.2 ms \u00b1  25.2 ms    [User: 216.8 ms, System: 27.5 ms]\n  Range (min \u2026 max):   305.2 ms \u2026 390.5 ms    10 runs\n```\n\n#### Test 2: Rebuild the index and compact the series file\n```sh\n# These are the commands that I ran before starting the database:\ninflux_inspect buildtsi -datadir /var/lib/influxdb/data -waldir /var/lib/influxdb/wal\ninflux_inspect buildtsi -compact-series-file -datadir /var/lib/influxdb/data -waldir /var/lib/influxdb/wal\n\n# Test result:\nBenchmark 1: http get 'http://localhost:8086/query?db=tenant_X&q=show tag values with key = \"device_id\"'\n  Time (mean \u00b1 \u03c3):     71.637 s \u00b1  2.053 s    [User: 0.263 s, System: 0.088 s]\n  Range (min \u2026 max):   68.601 s \u2026 76.103 s    10 runs\n```\n\n#### Test 3: Don't run any buildtsi commands at all before starting v1.11.8\n```sh\n# Test result:\nBenchmark 1: http get 'http://localhost:8086/query?db=tenant_X&q=show tag values with key = \"device_id\"'\n  Time (mean \u00b1 \u03c3):     320.6 ms \u00b1  21.8 ms    [User: 220.6 ms, System: 24.7 ms]\n  Range (min \u2026 max):   304.2 ms \u2026 378.5 ms    10 runs\n```\n\n> [!NOTE]\n> I restored the InfluxDB file system data from a v1.8.10 backup before each test to ensure all tests started from the same state.\n\n\nIn all three tests, the database started without any obvious errors. The query result was also identical in all tests. I wonder what the implications are when I run v1.11.8 without rebuilding the index?\n\n    I suspect that the first occurrence of each of your device ids is spread out in time and therefore across shards or at least tsm files.\n\nThis is correct. Since we used the default `autogen` retention policy, this database has its points spread out over multiple years of time.\n",
      "labels": [
        "1.x"
      ],
      "created_at": "2025-08-12T14:37:46Z",
      "closed_at": "2025-10-10T14:54:29Z",
      "url": "https://github.com/influxdata/influxdb/issues/26689",
      "comments_count": 15
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26517,
      "title": "Published x86_64 v3 binaries require AVX?",
      "problem": "I'm unsure whether this is deliberate and this bug should instead apply to https://github.com/influxdata/docs-v2/blob/67963c8e560ca38e7f61162ffbd2682b50300635/content/influxdb3/core/install.md , which simply offers `amd64`/`x86_64` builds of Influxdb v3, suggesting pretty wide architectural support, but here goes...\n\nAttempting to run the published InfluxDB v3 binaries [^0] on a lovely little machine with a `Intel(R) Atom(TM) CPU  C2758  @ 2.40GHz` CPU dies a rather immediate and horrible death thus:\n```\n$ gdb --args /usr/bin/influxdb3\n(gdb) r\nStarting program: /usr/bin/influxdb3\nwarning: Error disabling address space randomization: Function not implemented\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\n\nProgram received signal SIGILL, Illegal instruction.\n0x0000563fab2296b2 in influxdb3::main ()\n(gdb) disass\n   0x0000563fab229680 <+0>:     push   %rbp                       \n   0x0000563fab229681 <+1>:     mov    %rsp,%rbp                              \n   0x0000563fab229684 <+4>:     push   %r15                                   \n   0x0000563fab229686 <+6>:     push   %r14\n   0x0000563fab229688 <+8>:     push   %r13\n   0x0000563fab22968a <+10>:    push   %r12\n   0x0000563fab22968c <+12>:    push   %rbx\n   0x0000563fab22968d <+13>:    mov    %rsp,%r11\n   0x0000563fab229690 <+16>:    sub    $0xc000,%r11\n   0x0000563fab229697 <+23>:    sub    $0x1000,%rsp\n   0x0000563fab22969e <+30>:    movq   $0x0,(%rsp)\n   0x0000563fab2296a6 <+38>:    cmp    %r11,%rsp\n   0x0000563fab2296a9 <+41>:    jne    0x563fab229697 <_ZN9influxdb34main17h7952dff2a5a8128eE+23>\n   0x0000563fab2296ab <+43>:    sub    $0x628,%rsp\n=> 0x0000563fab2296b2 <+50>:    vpxor  %xmm0,%xmm0,%xmm0\n```\n\nLooking over at https://www.felixcloutier.com/x86/pxor, we see that this is indeed an AVX instruction, which this processor certainly does not have.  The need for AVX was seemingly previously discovered in https://github.com/influxdata/influxdb/issues/26346 but apparently without prompting an update to the documentation.\n\nETA: Somehow I missed https://github.com/influxdata/influxdb/issues/26150 .  Sorry!\n\n[^0]: From the docker image `sha256:62276b5841c54271f8f61aa75754dd094ac8889e0e3df13143eb2efa2f3436e3` specifically, but I tried again with https://dl.influxdata.com/influxdb/releases/influxdb3-core-3.1.0_linux_amd64.tar.gz just in case.",
      "solution": "@nwf - was #26150 able to lead you to a solution? I am inclined to close this issue as a duplicate of that one.",
      "labels": [
        "v3"
      ],
      "created_at": "2025-06-13T04:16:17Z",
      "closed_at": "2025-07-02T20:40:07Z",
      "url": "https://github.com/influxdata/influxdb/issues/26517",
      "comments_count": 3
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 25054,
      "title": "Retention service hangs and does not remove old shards",
      "problem": "Under certain conditions, the retention service can become hung waiting on a shard's reference count to drop to zero. When this happens, no other shards can be removed by the retention service. This can eventually result in high disk usage.\r\n\r\nThe attached goroutine trace shows a system exhibiting the issue. The retention service is stuck on [waiting on the WaitGroup used to indicate that the references to the shard have dropped to zero](https://github.com/influxdata/influxdb/blob/b09e4b751f9ac24e53558764df64b74e9b06fc3d/tsdb/store.go#L541).\r\n[goroutine.txt](https://github.com/user-attachments/files/15793116/goroutine.txt)\r\n",
      "solution": "* I think this ticket was fixed by https://github.com/influxdata/influxdb/pull/25055\n\nclose this one?\n\n---\n\nYes please\r\n\r\nOn Wed, Oct 1, 2025, 2:48 PM Phil Bracikowski ***@***.***>\r\nwrote:\r\n\r\n> *philjb* left a comment (influxdata/influxdb#25054)\r\n> <https://github.com/influxdata/influxdb/issues/25054#issuecomment-3358277795>\r\n>\r\n>    - I think this ticket was fixed by #25055\r\n>    <https://github.com/influxdata/influxdb/pull/25055>\r\n>\r\n> close this one?\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/influxdata/influxdb/issues/25054#issuecomment-3358277795>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/ARIQHJHEY32O2ZE3EJ4DNWT3VRD4BAVCNFSM6AAAAACIBRO4KGVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZTGNJYGI3TONZZGU>\r\n> .\r\n> You are receiving this because you commented.Message ID:\r\n> ***@***.***>\r\n>\r\n",
      "labels": [
        "kind/bug",
        "area/storage",
        "1.x",
        "team/edge"
      ],
      "created_at": "2024-06-11T18:00:54Z",
      "closed_at": "2025-10-02T15:35:44Z",
      "url": "https://github.com/influxdata/influxdb/issues/25054",
      "comments_count": 3
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26297,
      "title": "Perf issues between HTTP vs FlightRPC v3",
      "problem": "I'm still experiencing performance issues between calling CLI that calls HTTP API under the hood vs FlightRPC with pyarrow client.\n\nI'm gonna try to create a minimal example for this.\n\nI'm using the v3.0.0 build enterprise, with compaction enabled.\n\nIn the mean time here is the log calling the same SQL query:\n\nHere using CLI: 32ms\n```bash\ninfluxdb3  | SELECT DISTINCT ON (tid) lat, lon, tid, battery, altitude, time\ninfluxdb3  | FROM telemetry\ninfluxdb3  | WHERE tid IN ('1', '2', '3', '4', '5', '6', '7', '8', '9', '10')\ninfluxdb3  | ORDER BY tid, time DESC;\ninfluxdb3  |  query_params=Params { } issue_time=2025-04-20T17:52:56.673480632+00:00 partitions=8 parquet_files=16 deduplicated_partitions=3 deduplicated_parquet_files=11 plan_duration_secs=0.031543458 permit_duration_secs=0.000114208 execute_duration_secs=0.000655875 end2end_duration_secs=0.032463916 compute_duration_secs=0.00020381 max_memory=0 ingester_metrics=IngesterMetrics { latency_to_plan = 0ns, latency_to_full_data = 0ns, response_rows = 0, partition_count = 0, response_size = 0 } success=true running=false cancelled=false\n```\n\nUsing FLightRPC: 617ms\n```bash\ninfluxdb3  | SELECT DISTINCT ON (tid) lat, lon, tid, battery, altitude, time\ninfluxdb3  | FROM telemetry\ninfluxdb3  | WHERE tid IN ('1', '2', '3', '4', '5', '6', '7', '8', '9', '10')\ninfluxdb3  | ORDER BY tid, time DESC;\ninfluxdb3  |  query_params=Params { } issue_time=2025-04-20T17:53:48.931954711+00:00 partitions=8 parquet_files=16 deduplicated_partitions=3 deduplicated_parquet_files=11 plan_duration_secs=0.060829459 permit_duration_secs=0.001200208 execute_duration_secs=0.555666209 end2end_duration_secs=0.617759376 compute_duration_secs=0.095443782 max_memory=25699024 ingester_metrics=IngesterMetrics { latency_to_plan = 0ns, latency_to_full_data = 0ns, response_rows = 0, partition_count = 0, response_size = 0 } success=true running=false cancelled=false\n```\n\n__Expected behaviour:__\nSame performance between the two paths\n\n__Actual behaviour:__\n\n\n__Environment info:__\n\n* Please provide the command you used to build the project, including any `RUSTFLAGS`.\n* System info: Darwin 24.3.0 arm64\n* I'm running influxdb3 using docker with a volume, object store is setup as `file`\n* Other relevant environment details: disk info, hardware setup etc.\n\n__Config:__\n      - INFLUXDB3_OBJECT_STORE=file\n      - INFLUXDB3_DB_DIR=/var/lib/influxdb3\n      - INFLUXDB3_ENTERPRISE_LICENSE_EMAIL=<redacted>\n      - INFLUXDB3_ENTERPRISE_MODE=all\n      - INFLUXDB3_HTTP_BIND_ADDR=0.0.0.0:8181\n      - INFLUXDB3_MAX_HTTP_REQUEST_SIZE=20971520\n      - LOG_FILTER=info\n      - INFLUXDB3_WAL_FLUSH_INTERVAL=1000ms\n      - INFLUXDB3_NODE_IDENTIFIER_PREFIX=node-1\n      - INFLUXDB3_ENTERPRISE_CLUSTER_ID=cluster-1\n\n__Logs:__\nInclude snippet of errors in logs or stack traces here.\nSometimes you can get useful information by running the program with the `RUST_BACKTRACE=full` environment variable.\nFinally, the IOx server has a `-vv` for verbose logging.\n",
      "solution": "@jules-ch - I've ran your script instead of queries directly and I've managed to reproduce the issue. I just wanted to clarify if this is something that you've done intentionally, in your http call you seem to print the time as soon as you get the status but not `await` the json response.\n\n```python\nasync def v3_http_call(query: str):\n    influxdb_url = \"http://127.0.0.1:8181\"\n    headers = {\n        \"Authorization\": f\"Bearer {TOKEN}\",\n        \"accept\": \"application/json\",\n        \"Content-Type\": \"application/json\",\n    }\n    content = json.dumps(\n        {\n            \"db\": \"location\",\n            \"q\": query,\n            \"format\": \"json\",\n        },\n    )\n    async with aiohttp.ClientSession(headers=headers) as session:\n        start_time = time.perf_counter()\n        async with session.post(\n            influxdb_url + \"/api/v3/query_sql\", data=content\n        ) as response:\n            print(\"Status:\", response.status)\n            print(time.perf_counter() - start_time, \"s\") # <---- this print seems to be before you get the json body\n            response = await response.json()\n```\n\nWhen you compare that with flight call, you're measuring the time _after_ `table = reader.read_all()`. I'm not sure if it is a fair comparison if one is measured after reading all the data and other is before reading the response. If I move the timer `print` statement in `v3_http_call` function after waiting for the response I get the following as result,\n\n```\nSQL query:\n--sql\nSELECT DISTINCT ON (tid) lat, lon, tid, time\nFROM telemetry\nWHERE tid IN ('1', '2', '3', '4', '5', '6', '7', '8', '9', '10')\nORDER BY tid ASC, time DESC;\n\nCalling FlightRPC\n-----------------\n0.2590100329834968 s\nCalling HTTP V3\n-----------------\nStatus: 200\n0.0907010689843446 s\n\nSQL query:\n--sql\nSELECT lat, lon, tid, time\nFROM telemetry\nORDER BY time DESC\nLIMIT 200\n\nCalling FlightRPC\n-----------------\n0.01946758001577109 s\nCalling HTTP V3\n-----------------\nStatus: 200\n0.020495977951213717 s\n\nSQL query:\n--sql\nSELECT AVG(lat)\nFROM telemetry\nGROUP BY tid\n\nCalling FlightRPC\n-----------------\n3.2022568699903786 s\nCalling HTTP V3\n-----------------\nStatus: 200\n6.923285491997376 s\n\nSQL query:\n--sql\nSELECT\n  DATE_BIN(INTERVAL '10 minutes', time) AS time,\n  tid,\n  selector_max(lat, time)['value'] AS 'max lat',\n  selector_min(lat, time)['value'] AS 'min lat',\n  avg(lat) AS 'average lat'\nFROM telemetry\nGROUP BY 1, tid\nORDER BY tid, 1\n\nCalling FlightRPC\n-----------------\n4.450999940978363 s\nCalling HTTP V3\n-----------------\nStatus: 200\n5.365592924063094 s\n``` \n\nNow the first query is slightly slower than the HTTP response (which I'm looking into atm), but flight seems to be quicker for all other queries. I'll continue to investigate to see if there are any other issues.",
      "labels": [
        "v3"
      ],
      "created_at": "2025-04-20T17:58:15Z",
      "closed_at": "2025-10-01T20:47:29Z",
      "url": "https://github.com/influxdata/influxdb/issues/26297",
      "comments_count": 11
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 24901,
      "title": "after disk full was resolved, InfluxDB2 continues to write just to RAM",
      "problem": "I was [asked](https://community.influxdata.com/t/all-data-gone-when-influxdb-2-7-is-restarted/33812) to post this here.\r\n\r\nI\u2019ve been running influxdb 2.7 on my Ubuntu 22.04.4 LTS VM for over 6 months without any issues.\r\n\r\nLast week the system ran out of diskspace which I noticed when I received an alert about high RAM usage (I suspect influxdb was writing just to RAM for 2 days).\r\n\r\nI then expanded the disk to resove the problem.\r\n\r\n**However** whenever I restarted the VM, or just restarted the influxdb service, influxdb lost all data it had logged since the last start.\r\nIt appears as if influxdb continued to just log data to RAM instead of the disk as if the disk was still full.\r\n\r\nBut \u201csystemctl status influxdb\u201d as well as \u201cjournalctl -u influxdb.service\u201d showed no errors.\r\n\r\nMy solution then was to restore a VM backup from the day before the disk was full.\r\nI then expanded the disk like before and influxdb is writing data to the disk just fine now - restarts are no issue.\r\n\r\nIt looks like running out of diskspace upset influxdb, which caused it to only write to RAM even after the diskspace issue was resolved.",
      "solution": "Thanks for your report - It appears the issue has been resolved for yourself? yes?\r\n\r\nDid you happen to capture any logs while you suspected that influxd was only writing to memory? I would not expect influx to continue to work after the disk runs out. The last wal file or tsm file influxd tried to write when the disk filled could have been truncated and thus corrupt. That situation might have needed the restore from a backup as you did to resolve/remove any problematic files on disk. \r\n\r\nFor a future reader finding this issue, I would do this procedure:\r\n\r\n* Realize the disk is full.\r\n* Shutdown influxd\r\n* expand/increase available disk\r\n* restart influxd and watch the logs and file system to ensure that influxd is writing wal files and snapshots into new tsm files. Check write requests for errors. If there are no errors or warning in the logs and wal/tsm files are appearing, influxd should be operating correctly (try a query too). \r\n\r\n\r\nAs it seems your issue is resolved, I'm going to close this ticket. Please reopen and provide more detail if you still have an issue.",
      "labels": [],
      "created_at": "2024-04-09T14:14:22Z",
      "closed_at": "2024-04-10T18:13:08Z",
      "url": "https://github.com/influxdata/influxdb/issues/24901",
      "comments_count": 2
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26776,
      "title": "Panic when expecting `Arc::get_mut` to return but there are more than one reference",
      "problem": "<!--\n\nThank you for reporting a bug in InfluxDB IOx.\n\nHave you read the contributing section of the README? Please do if you haven't.\nhttps://github.com/influxdata/influxdb/blob/main/README.md\n\n* Please ask usage questions in the Influx Slack (there is an #influxdb-iox channel).\n    * https://influxdata.com/slack\n* Please don't open duplicate issues; use the search. If there is an existing issue please don't add \"+1\" or \"me too\" comments; only add comments with new information.\n* Please check whether the bug can be reproduced with tip of main.\n* The fastest way to fix a bug is to open a Pull Request.\n    * https://github.com/influxdata/influxdb/pulls\n\n-->\n\n__Steps to reproduce:__\nList the minimal actions needed to reproduce the behaviour.\n\n1. be running influx via docker\n2. accumulate some data\n3. influxdb3 starts panicking and stops writing data\n4. restarting influxdb3 doesn't resolve the issue\n\n__Expected behaviour:__\nIt should keep writing data when I send it.\n\n__Actual behaviour:__\nPanicked with no useful info about what went wrong or how to fix it.\n\n__Environment info:__\n\ndocker image: influxdb:3-core\n\n__Config:__\nCopy any non-default config values here or attach the full config as a gist or file.\n\n<!-- The following sections are only required if relevant. -->\n\n__Logs:__\nInclude snippet of errors in logs or stack traces here.\nSometimes you can get useful information by running the program with the `RUST_BACKTRACE=full` environment variable.\nFinally, the IOx server has a `-vv` for verbose logging.\n\n```\ninflux-1  | thread 'InfluxDB 3 Core Tokio IO 16' panicked at influxdb3_catalog/src/catalog/versions/v2/update.rs:1421:41:\ninflux-1  | no other references\ninflux-1  | \ninflux-1  | thread 'InfluxDB 3 Core Tokio IO 16' panicked at influxdb3_catalog/src/catalog/versions/v2/update.rs:1421:41:\ninflux-1  | no other references\ninflux-1  | 2025-09-04T14:37:47.858070Z ERROR panic_logging: Thread panic panic_type=\"unknown\" panic_message=\"no other references\" panic_file=\"influxdb3_catalog/src/catalog/versions/v2/update.rs\" panic_line=1421 panic_column=41\ninflux-1  | 2025-09-04T14:37:47.858094Z ERROR panic_logging: Thread panic panic_type=\"unknown\" panic_message=\"no other references\" panic_file=\"influxdb3_catalog/src/catalog/versions/v2/update.rs\" panic_line=1421 panic_column=41\ninflux-1  | 2025-09-04T14:37:48.797505Z ERROR panic_logging: Thread panic panic_type=\"unknown\" panic_message=\"no other references\" panic_file=\"influxdb3_catalog/src/catalog/versions/v2/update.rs\" panic_line=1421 panic_column=41\n```\n",
      "solution": "Thanks for opening the issue @jakkarth - I updated the title . Someone from the team will take a look into this bug.",
      "labels": [
        "v3"
      ],
      "created_at": "2025-09-04T14:38:55Z",
      "closed_at": "2025-09-16T13:24:32Z",
      "url": "https://github.com/influxdata/influxdb/issues/26776",
      "comments_count": 5
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26785,
      "title": "Query exceeds parquet file limit (432) even with small seed batches in InfluxDB v3",
      "problem": "\n**Description:**\nI\u2019m working with InfluxDB v3 and seeding test data into a measurement called `greenspaces`.\nEven though I only insert **100\u20131000 records per batch**, queries quickly hit the error:\n\ni ran this command to setup influxdb using this command \n\n```bash\ndocker run -d \\\n  --name influxdb3-core \\\n  -p 8181:8181 \\\n  quay.io/influxdb/influxdb3-core:latest serve \\\n    --object-store=file \\\n    --data-dir=/var/lib/influxdb3 \\\n    --node-id=node0\n```\n\n```\nError while planning query: External error: Query would exceed file limit of 432 parquet files.\n```\n\nThis happens even when querying relatively small ranges in the dashboard.\n\n---\n\n**Steps to Reproduce:**\n\n1. Run a seeding script that inserts \\~1000 records into InfluxDB.\n\n   * Tags include `areaName`, `cameraId`, `timeRange`, `crowdDensityLevel`, `trashLevel`, etc.\n   * Fields include `latitude`, `longitude`, `crowdCount`, and optionally `alertImage`.\n2. Repeat a few times (seeding a few thousand rows total).\n3. Query data in dashboard (time range = 1 day or more).\n4. Observe parquet file limit error.\n\n---\n\n**Expected Behavior:**\n\n* Small seed datasets (a few thousand points) should not trigger file limit errors.\n* Queries should run without exceeding the default 432 parquet file scan limit.\n\n---\n\n**Environment:**\n\n* InfluxDB v3 version: (fill in)\n* Deployment: Docker container\n* OS: (fill in, e.g., Ubuntu 22.04)\n* Query type: (SQL / InfluxQL)\n\n---\n\n<img width=\"1151\" height=\"773\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/7b610dd5-9260-44f2-9bb3-e5bbbfed054e\" />\n",
      "solution": "\n@philjb i am storing the base64 in the db as it is as alertImage field Name ?\nis it causing this issue ?\n\ncheck this function i have used to seed the data \n\n\n```\nasync seedIncidentManagementData(count: number = 1000) {\n    try {\n      const measurement = \"incident_management\";\n      const batchSize = this.batchSize;\n\n      const processRecord = async (_, index) => {\n        const timestamp = this.randomTimestamp();\n\n        // Generate incident ID and reference number\n        const incident_id = `INC-${String(this.randomInt(1, 999999)).padStart(6, \"0\")}`;\n        const ref_no = `REF-${String(this.randomInt(1000, 9999))}`;\n        // Randomly select truck file and read base64 image\n        let base64Image = \"\";\n        try {\n          const randomTruckFile = this.getRandomTruckFile();\n          const imagePath = path.join(process.cwd(), randomTruckFile);\n          base64Image = fs.readFileSync(imagePath, \"utf8\").trim();\n        } catch (error) {\n          console.warn(`Warning: Could not read truck file for image data: ${error.message}`);\n        }\n\n        // Select random classification\n        const mainClasses = Object.keys(this.incidentClassifications);\n        const class_main = mainClasses[this.randomInt(0, mainClasses.length - 1)];\n        const subClasses = Object.keys(this.incidentClassifications[class_main]);\n        const class_sub = subClasses[this.randomInt(0, subClasses.length - 1)];\n        const detailClasses = this.incidentClassifications[class_main][class_sub];\n        const class_detail = detailClasses[this.randomInt(0, detailClasses.length - 1)];\n\n        // Select location\n        const cities = Object.keys(this.saudiLocations);\n        const city = cities[this.randomInt(0, cities.length - 1)];\n        const locationData = this.saudiLocations[city];\n        const street = locationData.streets[this.randomInt(0, locationData.streets.length - 1)];\n\n        // Generate address\n        const building = this.randomInt(1, 100);\n        const block = this.randomInt(1, 20);\n        const address = `Building ${building}, Block ${block}`;\n\n        // Generate coordinates with realistic variation\n        const lat = locationData.coordinates.lat + this.randomFloat(-0.05, 0.05);\n        const lon = locationData.coordinates.lon + this.randomFloat(-0.05, 0.05);\n\n        // Generate severity based on incident type (more realistic distribution)\n        let severity;\n        if (class_main === \"Safety\" || class_detail.includes(\"Fire\") || class_detail.includes(\"Damage\")) {\n          // Safety incidents tend to be higher severity\n          severity = this.severityLevels[this.randomInt(1, 3)]; // Medium to Critical\n        } else if (class_main === \"Visual Pollution\" && class_sub === \"Cleanliness\") {\n          // Cleanliness issues tend to be lower severity\n          severity = this.severityLevels[this.randomInt(0, 1)]; // Low to Medium\n        } else {\n          severity = this.severityLevels[this.randomInt(0, 2)]; // Low to High\n        }\n\n        // Generate status (realistic distribution: 40% Open, 30% In Progress, 20% Resolved, 10% Closed)\n        const statusRand = Math.random();\n        let status;\n        if (statusRand < 0.4) status = \"Open\";\n        else if (statusRand < 0.7) status = \"In Progress\";\n        else if (statusRand < 0.9) status = \"Resolved\";\n        else status = \"Closed\";\n        const fields = {\n          lat: lat,\n          lon: lon,\n        };\n\n        const tags = {\n          incident_id: incident_id,\n          ref_no: ref_no,\n          class_main: class_main,\n          class_sub: class_sub,\n          class_detail: class_detail,\n          country: \"Saudi Arabia\",\n          city: city,\n          street: street,\n          address: address,\n          severity: severity,\n          status: status,\n          image: base64Image,\n        };\n\n        const point = Point.measurement(measurement).setTimestamp(timestamp);\n        Object.entries(tags).forEach(([key, value]) => point.setTag(key, value));\n        Object.entries(fields).forEach(([key, value]) => {\n          if (typeof value === \"number\") {\n            point.setFloatField(key, value);\n          } else {\n            point.setStringField(key, String(value));\n          }\n        });\n\n        const batchIndex = Math.floor(index / batchSize);\n        return { point, batchIndex };\n      };\n\n      const pointsWithBatches = await Promise.all(Array(count).fill(null).map(processRecord));\n\n      const batches = pointsWithBatches.reduce<Record<number, Point[]>>((acc, { point, batchIndex }) => {\n        if (!acc[batchIndex]) {\n          acc[batchIndex] = [];\n        }\n        acc[batchIndex].push(point);\n        return acc;\n      }, {});\n\n      for (const batchPoints of Object.values(batches)) {\n        await this.client.write(batchPoints);\n      }\n\n      console.log(`\u2705 Seeded ${count} incident management records`);\n    } catch (error) {\n      console.error(`\u274c Failed to seed incident management data: ${error.message}`);\n      throw new Error(`Error seeding incident management data: ${error.message}`);\n    }\n  }\n```",
      "labels": [],
      "created_at": "2025-09-10T10:51:57Z",
      "closed_at": "2025-09-11T17:39:25Z",
      "url": "https://github.com/influxdata/influxdb/issues/26785",
      "comments_count": 4
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26752,
      "title": "thread 'InfluxDB 3 Enterprise Tokio Datafusion 4' panicked",
      "problem": "I've updated my influxdb3 docker instance to the latest 3.4. For license issues I've deleted all old data.\nI've created 4 \"last_cache\" to speed up queries.\n\nI the logs I can see this message reoccouring every ~10 sec:\n\n```\nthread 'InfluxDB 3 Enterprise Tokio Datafusion 3' panicked at influxdb3_cache/src/last_cache/cache.rs:446:14:\nshould only be calling to_record_batch when using a store\nContact trial@influxdata.com for assistance\n\nthread 'InfluxDB 3 Enterprise Tokio Datafusion 1' panicked at influxdb3_cache/src/last_cache/cache.rs:446:14:\nshould only be calling to_record_batch when using a store\nContact trial@influxdata.com for assistance\n\nthread 'InfluxDB 3 Enterprise Tokio Datafusion 1' panicked at influxdb3_cache/src/last_cache/cache.rs:446:14:\nshould only be calling to_record_batch when using a store\nContact trial@influxdata.com for assistance\n\nthread 'InfluxDB 3 Enterprise Tokio Datafusion 3' panicked at influxdb3_cache/src/last_cache/cache.rs:446:14:\nshould only be calling to_record_batch when using a store\nContact trial@influxdata.com for assistance\n```\n\n\nIs this a known issue? I can't tell if this is a new one, as I've never really checked the logs from the older docker containers.\n\nEdit: Looking further I did see this:\n\n```\n2025-08-27T18:32:46.965564Z ERROR panic_logging: Thread panic panic_type=\"unknown\" panic_message=\"should only be calling to_record_batch when using a store\" panic_file=\"influxdb3_cache/src/last_cache/cache.rs\" panic_line=446 panic_column=14\n2025-08-27T18:32:46.965575Z ERROR panic_logging: Thread panic panic_type=\"unknown\" panic_message=\"should only be calling to_record_batch when using a store\" panic_file=\"influxdb3_cache/src/last_cache/cache.rs\" panic_line=446 panic_column=14\n\nthread 'InfluxDB 3 Enterprise Tokio Datafusion 6' panicked at influxdb3_cache/src/last_cache/cache.rs:446:14:\nshould only be calling to_record_batch when using a store\nContact trial@influxdata.com for assistance\n\n[...]\n\n2025-08-27T18:32:46.965668Z  WARN service_grpc_flight: e=Error while planning query: Join Error: External error: Panic: should only be calling to_record_batch when using a store namespace=inverter query=SELECT value FROM last_cache('inverter_battery_charge', 'inverter_battery_charge__last_cache'); msg=\"Error handling Flight gRPC request\"\n```\n\nThe service, which is using this query is Grafana.",
      "solution": "@jaal2001 thanks for opening the issue. Are you running a query against the last cache every ten seconds? If there are any additional logs you can share that show the query, or if you can share the query directly, that would be helpful.\n\nIn addition, can you provide details about the last caches in your database? That can be found with:\n```sql\nselect * from system.last_caches\n```",
      "labels": [
        "v3"
      ],
      "created_at": "2025-08-27T18:29:59Z",
      "closed_at": "2025-08-31T12:30:36Z",
      "url": "https://github.com/influxdata/influxdb/issues/26752",
      "comments_count": 3
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 25281,
      "title": "influxdb:2.7.10 results in timeouts with high storage-wal-fsync-delay",
      "problem": "```\r\nfor datapoint in lots_of_datapoints:\r\n    point = Point(measurement).field(\"value\", datapoint[0]).time(datapoint[1])\r\n    await write_api.write(bucket=BUCKET, record=points)\r\n```\r\n_(I am aware of batching, this is a test)_\r\n\r\nTrying to increase `storage-wal-fsync-delay` to `60s` results in `raise asyncio.TimeoutError from None` on the client. \r\n\r\nIt shouldn't. WAL writes shouldn't affect receiving data from the client at all.\r\n\r\nThe [documentation](https://docs.influxdata.com/influxdb/v2/reference/config-options/#storage-wal-fsync-delay) does not indicate such issue.",
      "solution": "Why is this closed if the problem persists?  `wal-fsync-delay` should not be tied to the ACK responses to clients.  As soon as Influxdb retrieves the data, it should respond with ACK, and then hold this data in RAM before saving to disk as per `wal-fsync-delay`.",
      "labels": [],
      "created_at": "2024-09-03T23:41:40Z",
      "closed_at": "2025-07-25T17:24:27Z",
      "url": "https://github.com/influxdata/influxdb/issues/25281",
      "comments_count": 7
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26676,
      "title": "[doc]",
      "problem": "Generated doc is buggy: https://docs.influxdata.com/influxdb/cloud/write-data/replication/replicate-data/\n\nExample:\n<img width=\"716\" height=\"372\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/6d1947c1-5433-47c8-8a9f-de625a6ccdf9\" />\n\nHope it's easy to fix.",
      "solution": "* fixed by https://github.com/influxdata/docs-v2/pull/6315",
      "labels": [],
      "created_at": "2025-08-07T07:31:07Z",
      "closed_at": "2025-08-18T17:18:31Z",
      "url": "https://github.com/influxdata/influxdb/issues/26676",
      "comments_count": 2
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26452,
      "title": "#26443 introduced breaking change to WAL format",
      "problem": "### Problem\n\nhttps://github.com/influxdata/influxdb/pull/26443 removed the `Key` variant from the `FieldData` type in the WAL.\n\nThis invalidates existing WAL files, because they use `bitcode` as a serialization format, which is not self-describing and therefore does not support changes like this.\n\n### Solution\n\nRevert the changes from https://github.com/influxdata/influxdb/pull/26443, specifically [this](https://github.com/influxdata/influxdb/pull/26443/files#diff-b37e5c9491170a168e32f74b0553b0cab89eb4e35f2e5a2a7f5306d15e95d3a3L391) line.\n\nBeyond that, if we would like to remove that variant, a new version for the `.wal` file format will need to be introduced.\n\n### To reproduce\n\n* Start the `influxdb3 serve` command with the build at 1ec063b0c4 (commit immediately prior to the breaking change).\n* Perform a write to the server.\n* Stop the server\n* Start the `influxdb3 serve` command with the build at 4a917c5a9f - it will fail to start with the following message:\n```\nServe command failed: failed to initialized write buffer: error from wal: deserialize error: bitcode error: bitcode error\n```",
      "solution": "Fixed by https://github.com/influxdata/influxdb/pull/26453",
      "labels": [
        "v3"
      ],
      "created_at": "2025-05-23T16:39:48Z",
      "closed_at": "2025-08-19T13:59:04Z",
      "url": "https://github.com/influxdata/influxdb/issues/26452",
      "comments_count": 1
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26671,
      "title": "InfluxDB 3.2 fails to start after rebooting long-running high-volume instance: \"Too many open files (os error 24)\"",
      "problem": "<!--\n\nThank you for reporting a bug in InfluxDB IOx.\n\nHave you read the contributing section of the README? Please do if you haven't.\nhttps://github.com/influxdata/influxdb/blob/main/README.md\n\n* Please ask usage questions in the Influx Slack (there is an #influxdb-iox channel).\n    * https://influxdata.com/slack\n* Please don't open duplicate issues; use the search. If there is an existing issue please don't add \"+1\" or \"me too\" comments; only add comments with new information.\n* Please check whether the bug can be reproduced with tip of main.\n* The fastest way to fix a bug is to open a Pull Request.\n    * https://github.com/influxdata/influxdb/pulls\n\n-->\n\n__Steps to reproduce:__\nList the minimal actions needed to reproduce the behaviour.\n\n1. Run InfluxDB 3.2.0 continuously for ~1 month using the following command:\n```\n/opt/influxdb3-core/influxdb3 serve --node-id tux --object-store file --data-dir /opt/data/influxdb/data\n```\n2. Insert large volumes of time-series data (estimated hundreds of millions of rows daily).\n3. Reboot the server.\n4. Attempt to restart InfluxDB using the same command.\n5. Observe startup failure.\n\n__Expected behaviour:__\nInfluxDB should start successfully after a system reboot, even after long runtime and large-scale data ingestion.\n\n__Actual behaviour:__\nAfter rebooting, InfluxDB fails to start and logs:\n```\n2025-08-06T00:40:00.460464Z  INFO influxdb3_lib::commands::serve: InfluxDB 3 Core server starting node_id=tux git_hash=1ca3168beee35b22fe62a3998cd3ed0d4ab65ac6 version=3.2.0 uuid=b1dd3da4-7aaf-4ba8-bbf3-cb6b5e32191a num_cpus=2\n2025-08-06T00:40:00.465060Z  INFO influxdb3_clap_blocks::object_store: Object Store db_dir=\"/opt/data/influxdb/data\" object_store_type=\"Directory\"\n2025-08-06T00:40:00.465338Z  INFO influxdb3_lib::commands::serve: Creating shared query executor num_threads=2\n2025-08-06T00:40:00.495298Z  INFO influxdb3_catalog::catalog::update: create database name=\"_internal\"\n2025-08-06T00:40:00.495344Z  INFO influxdb3_lib::commands::serve: catalog initialized catalog_uuid=f57a921e-a8f5-4611-a195-f8baf74cfe65\n2025-08-06T00:40:00.495351Z  INFO influxdb3_catalog::catalog::update: register node node_id=\"tux\" core_count=2 mode=[Core]\n2025-08-06T00:40:00.495361Z  INFO influxdb3_catalog::catalog::update: registering node to catalog that was not previously de-registered node_id=\"tux\" instance_id=\"d83a79a3-b8ab-46ee-9464-cccf8507b993\"\n2025-08-06T00:40:00.495927Z  INFO influxdb3_catalog::object_store: persisted next catalog sequence put_result=PutResult { e_tag: Some(\"80c92b2-63ba794b164d2-15f\"), version: None } object_path=CatalogFilePath(Path { raw: \"tux/catalogs/00000000000000000021.catalog\" })\n2025-08-06T00:40:00.496041Z  INFO influxdb3_lib::commands::serve: catalog initialized instance_id=\"d83a79a3-b8ab-46ee-9464-cccf8507b993\"\n2025-08-06T00:40:00.496073Z  INFO influxdb3_catalog::catalog::update: set gen1 duration duration_ns=600000000000\n2025-08-06T00:40:00.582419Z  WARN executor: DedicatedExecutor dropped without calling shutdown()\n2025-08-06T00:40:00.582423Z  INFO influxdb3_cache::parquet_cache: cache request handler closed\n2025-08-06T00:40:00.582594Z  WARN executor: DedicatedExecutor dropped without waiting for worker termination\n2025-08-06T00:40:00.582798Z  WARN executor: DedicatedExecutor dropped without calling shutdown()\n2025-08-06T00:40:00.582815Z  WARN executor: DedicatedExecutor dropped without waiting for worker termination\nServe command failed: failed to initialized write buffer: error from persister: object_store error: Generic LocalFileSystem error: Unable to open file /opt/data/influxdb/data/tux/snapshots/18446744073709551343.info.json: Too many open files (os error 24)\n```\n\n__Environment info:__\n\n* Please provide the command you used to build the project, including any `RUSTFLAGS`.\n* System info: Run `uname -srm` or similar and copy the output here (we want to know your OS, architecture etc).\n* If you're running IOx in a containerised environment then details about that would be helpful.\n* Other relevant environment details: disk info, hardware setup etc.\n\nLinux 6.12.31-gentoo x86_64\nCPUs: 2\nDisk: Local SSD, sufficient free space\n\n__Config:__\nCopy any non-default config values here or attach the full config as a gist or file.\n\n<!-- The following sections are only required if relevant. -->\n\n__Logs:__\nInclude snippet of errors in logs or stack traces here.\nSometimes you can get useful information by running the program with the `RUST_BACKTRACE=full` environment variable.\nFinally, the IOx server has a `-vv` for verbose logging.\n\n\n```\nRUST_BACKTRACE=full /opt/influxdb3-core/influxdb3 serve --node-id tux --object-store file --data-dir /opt/data/influxdb/data/ -vv\n```\n[debug.txt](https://github.com/user-attachments/files/21609100/debug.txt)\n",
      "solution": "You need to configure your system to allow more open file handles for the process. These are notoriously low across most operating systems given modern hardware capabilities. How to do it depends on what OS you're running on. A search will turn up many hundreds of articles about this for each OS. If you're on OSX, it's kind of a pain, but it can be done.\n\n---\n\nI\u2019m not on macOS, I'm running on Linux. After increasing the file descriptor limit to 65535, the issue is resolved. Thank you very much for your help!",
      "labels": [],
      "created_at": "2025-08-06T01:01:36Z",
      "closed_at": "2025-08-06T13:38:50Z",
      "url": "https://github.com/influxdata/influxdb/issues/26671",
      "comments_count": 2
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26598,
      "title": "PGP key checksum verification error",
      "problem": "__Steps to reproduce:__\nMinimal actions needed to reproduce the behaviour.\n\n1. Run step 2 for Ubuntu/Debian from [the installation instruction](https://docs.influxdata.com/influxdb/v2/install/?t=Linux#install-influxdb-as-a-service-with-systemd)\n2. Observe checksum validation failure\n\n__Expected behaviour:__\n\nChecksum to match, key to be written to disk\n\n__Actual behaviour:__\n\nChecksum failed, key was not written to disk\n\n__Environment info:__\n\n* System info: Debian 12 (x86_64)\n\n__Config:__\n\nn/a\n\n__Logs:__\n\nHere's the command and output of the command from the documentation.\n\n```\ncurl --silent --location -O \\\nhttps://repos.influxdata.com/influxdata-archive.key\necho \"943666881a1b8d9b849b74caebf02d3465d6beb716510d86a39f6c8e8dac7515  influxdata-archive.key\" \\\n| sha256sum --check - && cat influxdata-archive.key \\\n| gpg --dearmor \\\n| sudo tee /etc/apt/trusted.gpg.d/influxdata-archive.gpg > /dev/null \\\n&& echo 'deb [signed-by=/etc/apt/trusted.gpg.d/influxdata-archive.gpg] https://repos.influxdata.com/debian stable main' \\\n| sudo tee /etc/apt/sources.list.d/influxdata.list\ninfluxdata-archive.key: FAILED\nsha256sum: WARNING: 1 computed checksum did NOT match\n\n\ncurl --silent --location -O \\\nhttps://repos.influxdata.com/influxdata-archive.key | sha256sum\ne3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855  -\n```",
      "solution": "Thanks for the reply and explanation. That URL does not serve up the key for me unless I use a VPN or Tor, which is not a practical solution for Ansible playbooks that download the key to my servers and set up the repo. Here's what I see:\n\n```sh\n$ curl --location https://repos.influxdata.com/influxdata-archive.key\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n<HTML><HEAD><META HTTP-EQUIV=\"Content-Type\" CONTENT=\"text/html; charset=iso-8859-1\">\n<TITLE>ERROR: The request could not be satisfied</TITLE>\n</HEAD><BODY>\n<H1>403 ERROR</H1>\n<H2>The request could not be satisfied.</H2>\n<HR noshade size=\"1px\">\nRequest blocked.\nWe can't connect to the server for this app or website at this time. There might be too much traffic or a configuration error. Try again later, or contact the app or website owner.\n<BR clear=\"all\">\nIf you provide content to customers through CloudFront, you can find steps to troubleshoot and help prevent this error by reviewing the CloudFront documentation.\n<BR clear=\"all\">\n<HR noshade size=\"1px\">\n<PRE>\nGenerated by cloudfront (CloudFront)\nRequest ID: s9T8-1MRBXDBkgruquyUmOupjS19fsz7aNMg33cbHHg2SDN5Z7lqAA==\n</PRE>\n<ADDRESS>\n</ADDRESS>\n</BODY></HTML>\n```\n\nAnd it's that exact same command that works fine over Tor or a VPN (it matches what you posted and the SHA256 matches the one which is published in the documentation).\n\nCan someone fix or disable Cloudfront for that particular URL? It's clearly not functioning properly.\n\n---\n\n> Thanks for the reply and explanation. That URL does not serve up the key for me unless I use a VPN or Tor, which is not a practical solution for Ansible playbooks that download the key to my servers and set up the repo.\n> And it's that exact same command that works fine over Tor or a VPN (it matches what you posted and the SHA256 matches the one which is published in the documentation).\n> ...\n> Can someone fix or disable Cloudfront for that particular URL? It's clearly not functioning properly.\n\n@bnpfeife - can you take a look at this?\n\n---\n\nThanks for making those changes. I can confirm that I am now able to download the key and add the repo.\n\nI have my servers set up to update every 24 hours at a random time of day. The total number of hardware machines and VMs is around 48 machines, which means one machine is checking for and installing updates every 30 minutes, on average.\n\nBut that's an average, and it's not only possible, but likely, that at some point two machines might try to update within 10 minutes of each other.  Now, if there aren't any new releases, this won't be an issue. But given a long enough timeline, there's bound to be some day when there is a new release and two machines will just happen to hit the same 10 minute window.\n\nWould two machines trigger this CloutFront ban? Three?\n\nIf it is triggered, does the error page provide instructions on how get unbanned and what rules they need to abide by to prevent getting banned in the future?\n\n\nHow can we reach some kind of balance here?\n\nI'm going to update my scripts to not retry on failure. Instead it'll just fail and notify me. That should reduce the traffic from me in the event something goes wrong. That will prevent what happen this time, which was that script got into an infinite loop after your apt server started giving me 403 errors. That did not cause the issue, but it certainly made it worse on my side after it started happening.\n\nI use HTTPS when connecting to your apt server, so using a caching proxy isn't really viable unless I both break TLS and introduce a single point of failure for updates across my entire fleet. Neither of those are appealing, and there are [good reasons to use HTTPS](https://blog.cloudflare.com/apt-transports/#apt-over-the-internet) even through the metadata is signed).\n\nWould you be willing to **wait 24 hours** before applying this policy of banning people who download updates multiple times? That would make sure the policy never interferes with people like me who simply want to make sure they have the latest security patches. This is approach could guarantee that there will not be any false positives (legitimate users getting banned) instead of just making it less frequent that legitimate users will get banned today (which is the best that can be done with changing the thresholds for how many downloads are acceptable or how long the window is.\n\nI don't have the context of what problem you're trying to solve with this CloudFront rule. My best guess is you pay a lot for bandwidth and scrapers have run been going wild trying to download every single .deb file that you have to offer and running up your costs. I can't think of any other reason to put a rule like this on non-sensitive, publicly available files. If I misunderstood the issue, then my suggestion may be off base.\n\nI want to find a solution that works for both of us and makes as little work for you as possible (e.g. investigating false positives).",
      "labels": [
        "security",
        "security/misc"
      ],
      "created_at": "2025-07-09T17:46:25Z",
      "closed_at": "2025-07-25T10:43:50Z",
      "url": "https://github.com/influxdata/influxdb/issues/26598",
      "comments_count": 10
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26524,
      "title": "InfluxDB OSS v2.7.10 unresponsive, eventually crashing with OOM",
      "problem": "<!--\n\nThank you for reporting a bug in InfluxDB IOx.\n\nHave you read the contributing section of the README? Please do if you haven't.\nhttps://github.com/influxdata/influxdb/blob/main/README.md\n\n* Please ask usage questions in the Influx Slack (there is an #influxdb-iox channel).\n    * https://influxdata.com/slack\n* Please don't open duplicate issues; use the search. If there is an existing issue please don't add \"+1\" or \"me too\" comments; only add comments with new information.\n* Please check whether the bug can be reproduced with tip of main.\n* The fastest way to fix a bug is to open a Pull Request.\n    * https://github.com/influxdata/influxdb/pulls\n\n-->\n\n__Steps to reproduce:__\nList the minimal actions needed to reproduce the behaviour.\n\n1. Workload with no deviation from long-term trends\n2. No outside action (increased writes to buckets, read query with many datapoints in result executed etc.)\n\n__Expected behaviour:__\nNo outage of Influx service seemingly without outside action\n\n__Actual behaviour:__\nOnce every one or two weeks around 00:00 UTC most of write (Telegraf) and read (Grafana) requests ends with timeout error on the clients and memory consumption increases until the influxd process is reaped by oom killer. I'm suspecting some task around compaction, shard rotation or similar background task on the Telegraf bucket as the outages seems to have fixed period of appearance, traffic volume is not outside of long term trends that could cause increased usage of system resources and write / read requests to other buckets have normal (2xx) http status.\n\nThe Telegraf bucket i'm suspecting could cause the problem is reporting ~350G storage size (storage_shard_disk_size) and have \"forever\" retention policy, which means shard duration 7 days as i haven't configured custom shard duration.\n\nSimilar issue has been discussed in this thread https://github.com/influxdata/influxdb/issues/24406 I couldn't reopen the issue, but the problem seems very similar.\n\n__Environment info:__\n\n* Influx running in podman from image docker.io/library/influxdb:2.7 (amd64 2.7.10 version), no other container on the host is running\n* Podman host is virtual machine with 16vcpu, 32G ram\n* Custom environment variables:\n```\nINFLUXD_LOG_LEVEL=debug\nINFLUXD_STORAGE_MAX_CONCURRENT_COMPACTIONS=2\nINFLUXD_STORAGE_COMPACT_THROUGHPUT_BURST=10331648\n```\n\n__Logs:__\nHeap images before the problem occurs, 10 minutes after and last minute before the oom killer reaped the process\n\n00:01\n![Image](https://github.com/user-attachments/assets/69a1bfbb-fd35-4502-9313-ccde2a47c787)\n00:10\n![Image](https://github.com/user-attachments/assets/57a9f487-8ab6-416a-abde-da43987a7824)\n00:26\n![Image](https://github.com/user-attachments/assets/346e3b42-28a7-4146-aba4-5f791d8d56fc)\n\nLogs from Telegraf agent (write)\n```\n2025-06-16T00:00:18Z E! [outputs.influxdb_v2] When writing to [http://influxhost:8086/api/v2/write]: Post \"http://influxhost:8086/api/v2/write?bucket=telegraf&org=custom_org\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\n2025-06-16T00:00:18Z E! [agent] Error writing to outputs.influxdb_v2: failed to send metrics to any configured server(s)\n...\n2025-06-16T00:28:03Z E! [outputs.influxdb_v2] When writing to [http://influxhost:8086/api/v2/write]: Post \"http://influxhost:8086/api/v2/write?bucket=telegraf&org=custom_org\": dial tcp influxhost:8086: connect: connection refused\n2025-06-16T00:28:03Z E! [agent] Error writing to outputs.influxdb_v2: failed to send metrics to any configured server(s)\n```\n\nLogs from Grafana (read)\n```\n2025-06-16T00:00:40.726475259Z logger=tsdb.influx_flux endpoint=queryData pluginId=influxdb dsName=influxdb-v2 dsUID=xxx uname=grafana_scheduler rule_uid=xxx org_id=1 t=2025-06-16T00:00:40.726475259Z level=warn msg=\"Flux query failed\" err=\"Post \\\"http://influxhost:8086/api/v2/query?org=custom_org\\\": context deadline exceeded\" query=\"from(bucket: \\\"telegraf\\\")...\"\n...\n2025-06-16T00:28:06.297792863Z logger=tsdb.influx_flux endpoint=queryData pluginId=influxdb dsName=influxdb-v2 dsUID=xxx uname=grafana_scheduler rule_uid=xxx org_id=1 t=2025-06-16T00:28:06.297792863Z level=warn msg=\"Flux query failed\" err=\"Post \\\"http://influxhost:8086/api/v2/query?org=custom_org\\\": dial tcp influxhost:8086: connect: connection refused\" query=\"from(bucket: \\\"telegraf\\\")...\"\n```\n\nLogs from Influx\n```\nts=2025-06-16T00:00:13.147125Z lvl=info msg=\"Write failed\" log_id=0wwmuKel000 service=storage-engine service=write shard=6443 error=\"engine: context canceled\"\nts=2025-06-16T00:00:13.147342Z lvl=debug msg=Request log_id=0wwmuKel000 service=http method=POST host=influxhost:8086 path=/api/v2/write query=\"bucket=telegraf&org=custom_org\" proto=HTTP/1.1 status_code=499 response_size=107 content_length=-1 referrer= remote=linuxserver:44892 user_agent=Telegraf authenticated_id=0b26d84447adf000 user_id=0ac7beb338afa000 took=10117.641ms error=\"internal error\" error_code=\"internal error\"\n...\nts=2025-06-16T00:26:36.086156Z lvl=debug msg=Request log_id=0wwmuKel000 service=http method=POST host=influxhost:8086 path=/api/v2/write query=\"bucket=telegraf&org=custom_org\" proto=HTTP/1.1 status_code=499 response_size=90 content_length=-1 referrer= remote=linuxhost:44006 user_agent=Telegraf authenticated_id=0b26d84447adf000 user_id=0ac7beb338afa000 took=10024.616ms error=\"internal error\" error_code=\"internal error\"\n...\n2025-06-16T00:26:36 systemd[1]: libpod-9d2217b610be3ef850f79a65a2d743487c9cf71d806c53c39320039fe05fd325.scope: A process of this unit has been killed by the OOM killer.\n```\n\n",
      "solution": "> Are you seeing increased write request latency before the OOM kills?\n\nAlmost immediately after 02:00 all Telegraf agents started to report errors `context deadline exceeded (Client.Timeout exceeded while awaiting headers)` until new instance of Influx has been started after OOM ~02:30. Almost all of write traffic comes from Telegraf agents.\n\n> CPU profiles when memory usage grows from 8->20GB would be helpful to see what the DB is spending its time on.\n\nBelow is telemetry from Influx host time-wise around incident. I forgot to scroll legend on memory usage; red line is \"used\" memory.\n\n![Image](https://github.com/user-attachments/assets/a8f84953-4753-431e-b0a5-d5d2add9a1ce)\n![Image](https://github.com/user-attachments/assets/e252b475-3d37-4b06-b61c-365686bdde36)\n![Image](https://github.com/user-attachments/assets/74c36578-2ba9-4130-ab20-8e26d7e13db1)\n![Image](https://github.com/user-attachments/assets/ec1e1760-4ec2-4344-8d5b-d850e97c5ad7)\n\n* Heavy disk usage with high cpu iowait can be seen between 02:25 and 02:30 but i attribute it to swap usage.\n* Low to non disk usage can be seen between 02:00 and 02:25\n* It seems that \"user\" cpu load between 02:00 and 02:30 is lower than before the outage\n* Incoming network volume on enp7s0 is increased between 02:00 and 02:30 but the increase started after the issue 02:00 so to me it seems like consequence rather than cause\n\nFrom the look on the telemetry and your answer @philjb i get a feeling that around 02:00 something went wrong with Influx service writing data to disk, clients (mostly Telegraf services) started to time out during POST requests to api/v2/write, but Influx kept the incoming POST data in memory until it couldn't.\n\n> This suggests the DB is not writing to disk as fast as the data arrives. You said the write rate is the same so it must be something about getting to disk.\n\nMaybe i haven't expressed myself correctly. What i meant was that there is no reason why there should be elevated traffic / write rate coming from Telegraf agents. Almost all of the write traffic to Influx we have from Telegraf agents.\n\nThere is elevated incoming traffic during the outage but combined with the fact that disk usage went down, i think it might be Grafana alerts with Influx datasource trying to recover from error state or maybe data in write requests from Telegraf agents due to buffering datapoints while getting error `context deadline exceeded (Client.Timeout exceeded while awaiting headers)`. Around 300% increase of incoming traffic is strange nevertheless, for now i can't figure it out, but i think this is dead end road for determining root cause.\n\nThe only line in Influx log indicating something went wrong while writing to disk is this\n```\nts=2025-06-16T00:00:13.095461Z lvl=info msg=\"Reindexing WAL data\" log_id=0wwmuKel000 service=storage-engine engine=tsm1 db_shard_id=6443\n...\nts=2025-06-16T00:00:13.147125Z lvl=info msg=\"Write failed\" log_id=0wwmuKel000 service=storage-engine service=write shard=6443 error=\"engine: context canceled\"\n```\n\nAfter these lines only errors come from `service=http` with `status_code=499  took=10145.564ms error=\"internal error\" error_code=\"internal error\"`.\n\nThe shard id 6443 is also from different bucket than the one Telegraf agents are writing to. The bucket with shard id 6443 has only 90kB on fs.\n\n---\n\nYou have an excellent monitoring setup - nice. \n\nI reviewed the charts and your description quickly. I'll let it work in the back of my mind through the day.\n\nA couple initial thoughts:\n* do you have a chart of telegraf request success/error rates and/or request latencies? If you have a proxy, it might record this information. Telegraf itself has some metrics available: [see the internal plugin](https://github.com/influxdata/telegraf/blob/release-1.34/plugins/inputs/internal/README.md), but of course those need to be written into influxdb too.\n* You mentioned that the network volume increase is likely an effect and not a cause: How would an issue with influxdb cause more network traffic to it? I can imagine that happening if your telegraf agents are retrying the failed requests. Or i could imagine the increase in traffic being the cause of the issue. You can adjust the telegraf timeout (default is 5s I believe) - a larger number might help telegraf get through an influxdb issue. You can imagine that telegraf reaches its 5s limit, cancels its request while influxd is still working on it. Telegraf than tries again and now influxd has even more write traffic load to process. Your network in traffic has tripled, clearly making the issue worse and explaining the heap usage charts - more write traffic, more data in memory to write to disk. It is interesting that you have plenty of cpu capacity - ~10% baseline, which goes down slightly when the problem occurs. \n* It is beginning to look like a deadlock or stall someplace (low cpu usage, doesn't self recover, disk write rate unchanged, cleared by process restart). Version 2.7.12 addressed a locking issue. Upgrading would be good - at least it would eliminate a variable. \n* I don't recommend using disk swap with influxdb - it causes the kernel to increase disk io to page out memory and therefore takes disk bandwidth away from influxdb. Writing to disk (and reading) is generally the bottleneck so swap makes it worse under load. If it is needing to swap, influxd will almost certainly not recover on its own as that \"memory\" is now slow. I would recommend you let influxd oom earlier instead of giving it swap.\n* I'm surprised there's not more interesting logging from around UTC 00:00 at debug level - nothing about shard creation?\n* While you likely don't want to write to a new bucket, you could create a new bucket with a longer shard duration and configure telegraf agents to write to the new bucket (probably don't want to do this until the issue is resolved).\n* You can try collecting lock/mutex profiles before and after UTC 00:00 whenever the next issue is expected (this weekend?).\n* Lastly, I would change only one variable at a time. I would probably upgrade influx first.\n\n---\n\nThis may be fixed in 2.7.12.  Please upgrade.\n\n[feat: TagValueIterator holds RLock for too long (#26369)](https://github.com/influxdata/influxdb/pull/26414)\n",
      "labels": [],
      "created_at": "2025-06-16T11:41:21Z",
      "closed_at": "2025-07-25T16:43:05Z",
      "url": "https://github.com/influxdata/influxdb/issues/26524",
      "comments_count": 6
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 25266,
      "title": "inconsistencies build.rs and git hash handling",
      "problem": "1. the main function checks for different environment variables than the `get_git_hash*` functions\r\n2. `get_git_hash` checks for the wrong variable compared to main\r\n3. `get_git_hash_short` doesnt even check for the environment variable before trying to run git.\r\n\r\n```\r\ndiff --git a/influxdb3_process/build.rs b/influxdb3_process/build.rs\r\nindex e4b4f992ff..33343f989b 100644\r\n--- a/influxdb3_process/build.rs\r\n+++ b/influxdb3_process/build.rs\r\n@@ -15,7 +15,7 @@ fn main() -> Result<(), Box<dyn std::error::Error>> {\r\n }\r\n \r\n fn get_git_hash() -> String {\r\n-    let out = match std::env::var(\"VERSION_HASH\") {\r\n+    let out = match std::env::var(\"GIT_HASH\") {\r\n         Ok(v) => v,\r\n         Err(_) => {\r\n             let output = Command::new(\"git\")\r\n@@ -32,9 +32,17 @@ fn get_git_hash() -> String {\r\n }\r\n \r\n fn get_git_hash_short() -> String {\r\n-    let output = Command::new(\"git\")\r\n-        .args([\"rev-parse\", \"--short\", \"HEAD\"])\r\n-        .output()\r\n-        .expect(\"failed to execute git rev-parse to read the current git hash\");\r\n-    String::from_utf8(output.stdout).expect(\"non-utf8 found in git hash\")\r\n+    let out = match std::env::var(\"GIT_HASH_SHORT\") {\r\n+        Ok(v) => v,\r\n+        Err(_) => {\r\n+            let output = Command::new(\"git\")\r\n+                .args([\"rev-parse\", \"--short\", \"HEAD\"])\r\n+                .output()\r\n+                .expect(\"failed to execute git rev-parse to read the current git hash\");\r\n+            String::from_utf8(output.stdout).expect(\"non-utf8 found in git hash\")\r\n+        }\r\n+    };\r\n+\r\n+    assert!(!out.is_empty(), \"attempting to embed empty git hash\");\r\n+    out\r\n }\r\n\r\n```",
      "solution": "Hey @darix - thanks for raising the issue. It appears that `VERSION_HASH` is an artifact of an older iteration of our build process, and that it is no longer needed.\r\n\r\nI think the solution here would be to remove reference to `VERSION_HASH`, and rely strictly on the git commands to produce values for `GIT_HASH` and `GIT_HASH_SHORT`; there is no need to check for their existence first using the match statement. I can open a PR.",
      "labels": [
        "v3"
      ],
      "created_at": "2024-08-25T03:33:21Z",
      "closed_at": "2025-07-28T14:36:43Z",
      "url": "https://github.com/influxdata/influxdb/issues/25266",
      "comments_count": 9
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26549,
      "title": "InfluxDB3 Core failed to initialized write buffer: error from wal: join error",
      "problem": "<!--\n\nThank you for reporting a bug in InfluxDB IOx.\n\nHave you read the contributing section of the README? Please do if you haven't.\nhttps://github.com/influxdata/influxdb/blob/main/README.md\n\n* Please ask usage questions in the Influx Slack (there is an #influxdb-iox channel).\n    * https://influxdata.com/slack\n* Please don't open duplicate issues; use the search. If there is an existing issue please don't add \"+1\" or \"me too\" comments; only add comments with new information.\n* Please check whether the bug can be reproduced with tip of main.\n* The fastest way to fix a bug is to open a Pull Request.\n    * https://github.com/influxdata/influxdb/pulls\n\n-->\n\n__Steps to reproduce:__\nList the minimal actions needed to reproduce the behaviour.\n\n1. Server abnormal power outage\n2. Restart InfluxDB\n\n__Expected behaviour:__\nStart InfluxDB service normally.\n\n__Actual behaviour:__\nUnable to start InfluxDB service. Error: Serve command failed: failed to initialized write buffer: error from wal: join error: task 1276 panicked with message \"range end index 8 out of range for slice of length 0\"\n\n__Environment info:__\n\nSystem info: \nLinux 5.10.209-rt101 aarch64\n\ndocker-compose.yaml\n```\ninfluxdb3-core:\n    image: influxdb:3.1.0-core\n    container_name: influxdb3-core\n    restart: always\n    ports:\n      - 8181:8181\n    command:\n      - influxdb3\n      - serve\n\t\n      - --node-id=node0\n      - --log-filter=debug\n      - --object-store=file\n      - --data-dir=/var/lib/influxdb3\n      - --max-http-request-size=20971520\n    environment:\n      - TZ=Asia/Shanghai\n    volumes:\n      - /app/data/influxdb3/data:/var/lib/influxdb3\n```\n\n<!-- The following sections are only required if relevant. -->\n\n__Logs:__\n2025-06-20T06:20:39.219208Z  INFO influxdb3_wal::snapshot_tracker: timestamps passed in and wal file num min_time=Timestamp(1750345682239000000) max_time=Timestamp(1750345682239000000) wal_file_number=WalFileSequenceNumber(34053)\n2025-06-20T06:20:39.219219Z  INFO influxdb3_wal::object_store: replaying WAL file n_ops=1 min_timestamp_ns=1750345682239000000 max_timestamp_ns=1750345682239000000 wal_file_number=34053 snapshot_details=None\n2025-06-20T06:20:39.219689Z  INFO influxdb3_cache::parquet_cache: cache request handler closed\n2025-06-20T06:20:39.219696Z  WARN executor: DedicatedExecutor dropped without calling shutdown()\n2025-06-20T06:20:39.219734Z  WARN executor: DedicatedExecutor dropped without waiting for worker termination\nServe command failed: failed to initialized write buffer: error from wal: join error: task 1276 panicked with message \"range end index 8 out of range for slice of length 0\"\n",
      "solution": "> * Should this be configurable as opt-in / opt-out?\n\nI vote for an opt-in-style automated clean-up feature.\n\nHit this last weekend. It was a dev environment and I deleted it, unknowingly \"fixing\" the problem. I wouldn't want such fixes applied automatically to production system without my knowledge or a prior approval (via opt-in), though.\n\n",
      "labels": [
        "kind/bug",
        "v3"
      ],
      "created_at": "2025-06-20T07:14:42Z",
      "closed_at": "2025-06-24T11:57:42Z",
      "url": "https://github.com/influxdata/influxdb/issues/26549",
      "comments_count": 8
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26573,
      "title": "influxdb version 1.11.8 debian package uses incorrect 'useradd' arguments",
      "problem": "I already wrote up a bunch of detail on this but my browser tab was closed by\nmistake so I am going to leave this pretty barebones.\n\nFirst, start a debian container. I've tried `oldstable`, `bullseye`, and `bookworm`:\n```\nzsh/4 6016 [130] % docker run --network host --rm -it debian:bookworm /bin/bash\n```\n\nThen in the container, install `wget`:\n```\nroot@thing4:/# (apt -qq update 2>&1)>/dev/null && (apt -yqq install wget 2>&1)>/dev/null\n```\n\nThen still in the container, `wget` the debian package:\n```\nroot@thing4:/# wget https://download.influxdata.com/influxdb/releases/influxdb-1.11.8-amd64.deb\n--2025-07-01 01:54:07--  https://download.influxdata.com/influxdb/releases/influxdb-1.11.8-amd64.deb\nResolving download.influxdata.com (download.influxdata.com)... 34.213.189.139, 52.33.86.107, 54.244.195.224\nConnecting to download.influxdata.com (download.influxdata.com)|34.213.189.139|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://dl.influxdata.com/influxdb/releases/influxdb-1.11.8-amd64.deb [following]\n--2025-07-01 01:54:07--  https://dl.influxdata.com/influxdb/releases/influxdb-1.11.8-amd64.deb\nResolving dl.influxdata.com (dl.influxdata.com)... 3.163.165.15, 3.163.165.40, 3.163.165.129, ...\nConnecting to dl.influxdata.com (dl.influxdata.com)|3.163.165.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 26628196 (25M) [application/vnd.debian.binary-package]\nSaving to: 'influxdb-1.11.8-amd64.deb'\n\ninfluxdb-1.11.8-amd64.deb                                            100%[====================================================================================================================================================================>]  25.39M  5.84MB/s    in 5.6s\n\n2025-07-01 01:54:13 (4.55 MB/s) - 'influxdb-1.11.8-amd64.deb' saved [26628196/26628196]\n```\n\nThen still in the container, demonstrate the apparent bug:\n```\nroot@thing4:/# dpkg -i influxdb-1.11.8-amd64.deb\nSelecting previously unselected package influxdb.\n(Reading database ... 6698 files and directories currently installed.)\nPreparing to unpack influxdb-1.11.8-amd64.deb ...\nuseradd: invalid user ID '-m'\ndpkg: error processing archive influxdb-1.11.8-amd64.deb (--install):\n new influxdb package pre-installation script subprocess returned error exit status 3\nErrors were encountered while processing:\n influxdb-1.11.8-amd64.deb\n```\n\nThen check out the `preinst` script:\n\n```\nar x ./influxdb-1.11.8-amd64.deb\ntar --xz -xf control.tar.xz ./preinst\ngrep useradd preinst\n    useradd --system -u -m influxdb -s /bin/false -d \"${data_dir}\"\n```\n\nI tried several different linux distros and couldn't find an instance where the docs for the `-u` flag didn't match those found [here](https://linux.die.net/man/8/useradd).\n\nSo the issue probably is that `-u` is erroneously included since I don't think this script should even care what UUID the `influxdb` user ends up with.\n",
      "solution": "I can verify the issue in 1.11.8.\n\n1.11.7 has the right caps form of -M and -U. ",
      "labels": [
        "area/packaging",
        "kind/bug",
        "1.x"
      ],
      "created_at": "2025-07-01T02:10:32Z",
      "closed_at": "2025-07-18T19:00:57Z",
      "url": "https://github.com/influxdata/influxdb/issues/26573",
      "comments_count": 3
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 26610,
      "title": "Admin token issue on InfluxDB3 Core",
      "problem": "I just restarted my instance of InfluxDB Core to use the processing engine. Things are going well untill I try to connect to the Explorer UI or Grafana. What I use my previous token to connect to the server from there, I get: Invalid API token for Core/Enterprise product (plus an error line on the terminal where I'm running Influx:  ERROR influxdb3_server::http: cannot authenticate token e=InvalidToken) \nWhen I try to create a new admin token using \"influxdb3 create token --admin\" it throws the following: Failed to create token, error: ApiError { code: 409, message: \"token name already exists, _admin\" }\n",
      "solution": "Okay, @LPellerin31 - appreciate the transparency. We are planning on shipping an admin token recovery feature (https://github.com/influxdata/influxdb/pull/26594) in a n upcoming release, so if you decide to try out Core again, that would provide a solution to this situation.",
      "labels": [
        "v3",
        "v3-data-explorer"
      ],
      "created_at": "2025-07-15T14:58:45Z",
      "closed_at": "2025-07-18T07:47:08Z",
      "url": "https://github.com/influxdata/influxdb/issues/26610",
      "comments_count": 9
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/influxdb",
      "issue_number": 23956,
      "title": "[2.x] InfluxDB bucket stops reading+writing every couple of days",
      "problem": "__Steps to reproduce:__\r\nList the minimal actions needed to reproduce the behavior.\r\n\r\n1. Run influxdb2\r\n2. Insert metrics (with telegraf)\r\n3. Wait for some time\r\n\r\n__Expected behavior:__\r\nThings keep working\r\n\r\n__Actual behavior:__\r\nInfluxDB2 main index (\"telegraf\") stops reading/writing data.\r\nOther indexes work fine - including one which is 5m aggregates of the telegraf raw index. (obviously this does not get any new data)\r\n\r\nWe have had this in the past randomly, but in the last few weeks has happened every few days.\r\nIn the past it seemed to happen at 00:00UTC when influx did some internal DB maintenance - but now happens at random times.\r\n\r\n__Environment info:__\r\n\r\n* System info: Linux 3.10.0-1160.66.1.el7.x86_64 x86_64\r\n* InfluxDB version: InfluxDB v2.3.0+SNAPSHOT.090f681737 (git: 090f681737) build_date: 2022-06-16T19:33:50Z\r\n* Other relevant environment details: CentOS 7 on vmware - lots of spare IO, CPU, memory.\r\n\r\nOur database is 170GB, mostly metrics inserted every 60s, some every 600s.\r\nstorage_writer_ok_points is around 2.5k/s for 7mins, then ~25k/s for 3mins for the every-600s burst.\r\n\r\nVM has 32G RAM, 28G of which is in buffers/cache.\r\n4 cores, and typically sits at around 90% idle.\r\n~ 24IOPS, 8MiB/s\r\n\r\n__Config:__\r\n\r\n```\r\nbolt-path = \"/var/lib/influxdb/influxd.bolt\"\r\nengine-path = \"/var/lib/influxdb/engine\"\r\nflux-log-enabled = \"true\"\r\n```\r\n\r\nWe have enabled flux-log to see if specific queries are causing this - but it doesn't seem to be.\r\n\r\n__Logs:__\r\nInclude snippet of errors in log.\r\n\r\n__Performance:__\r\n\r\nI captured a 10s pprof which I will attach.\r\n\r\nI also have a core dump, and a 60s dump of debug/pprof/trace (though not sure if this has sensitive info but can share privately - the core dump certainly will)\r\n",
      "solution": "Hey @nward, I've looked through the profiles attached and haven't seen anything that immediately highlights the issue. If you are able to share more data as mentioned in the issue, I can provide you with a private sftp link. If that works for you, please email me at jsmith @ influxdata.com and I'll get those details to you. Thanks!\n\n---\n\n> Hey @nward, I've looked through the profiles attached and haven't seen anything that immediately highlights the issue. If you are able to share more data as mentioned in the issue, I can provide you with a private sftp link. If that works for you, please email me at jsmith @ influxdata.com and I'll get those details to you. Thanks!\r\n\r\nI've emailed you now - cheers!\n\n---\n\nGood catch, that would only apply for a crash. The core file appears to be corrupted (getting a lot of `unreadable error derefing *G error while reading spliced memory at ...`), can you check that the ulimit is set properly before you generate it?\r\n\r\nA couple of other thoughts:\r\n- Do you have compactions running regularly and succeeding?\r\n- Do you have the ability to test on a later version and see if the issue is still present?\r\n- You mentioned a 60s trace file, if you can throw that on the ftp server too that may provide some insight.",
      "labels": [
        "area/2.x"
      ],
      "created_at": "2022-11-29T10:45:40Z",
      "closed_at": "2025-06-09T20:52:10Z",
      "url": "https://github.com/influxdata/influxdb/issues/23956",
      "comments_count": 30
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17904,
      "title": "[[inputs.procstat]] plugin does not show Systemd children",
      "problem": "### Relevant telegraf.conf\n\n```toml\n[[inputs.procstat]]\nsystemd_unit = \"falcon-sensor.service\"\ninclude_systemd_children = true\n\n[[inputs.procstat]]\nsystemd_unit = \"splunkforwarder.service\"\npid_tag = false\n\n[[inputs.procstat]]\nsystemd_unit = \"qualys-cloud-agent.service\"\npid_tag = false\n\n[[inputs.procstat]]\nsystemd_unit = \"sshd.service\"\npid_tag = false\n\n[[inputs.procstat]]\nsystemd_unit = \"cron.service\"\npid_tag = false\n\n[[inputs.procstat]]\nsystemd_unit = \"telegraf.service\"\npid_tag = false\n\n[[inputs.procstat]]\nsystemd_unit = \"ntp.service\"\npid_tag = false\n\n[[inputs.procstat]]\nsystemd_unit = \"nss_updatedb.service\"\npid_tag = false\n\n[[outputs.graphite]]\nservers = [\"xxxx:2003\"]\nprefix = \"\"\ngraphite_tag_support = true\ntimeout = 2\n\n[[outputs.prometheus_client]]\n  listen = \":9100\"\n  path = \"/metrics\"\n  metric_version = 1\n  expiration_interval = \"60s\"\n```\n\n### Logs from Telegraf\n\n```text\nOct 29 04:13:50 imvaultstg-co-1 systemd[1]: Starting Telegraf...\nOct 29 04:13:50 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:13:50Z I! Loading config: /etc/telegraf/telegraf.conf\nOct 29 04:13:50 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:13:50Z I! Loading config: /etc/telegraf/telegraf.d/graphite_output.conf\nOct 29 04:13:50 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:13:50Z I! Loading config: /etc/telegraf/telegraf.d/prometheus_output.conf\nOct 29 04:13:50 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:13:50Z I! Loading config: /etc/telegraf/telegraf.d/system_debian_systemd.conf\nOct 29 04:13:50 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:13:50Z I! Loading config: /etc/telegraf/telegraf.d/system_metrics.conf\nOct 29 04:13:50 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:13:50Z I! Loading config: /etc/telegraf/telegraf.d/vault_network.conf\nOct 29 04:13:50 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:13:50Z I! Loading config: /etc/telegraf/telegraf.d/vault_ping.conf\nOct 29 04:13:50 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:13:50Z I! Loading config: /etc/telegraf/telegraf.d/vault_service.conf\nOct 29 04:13:50 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:13:50Z I! Starting Telegraf 1.35.2 brought to you by InfluxData the makers of InfluxDB\nOct 29 04:13:50 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:13:50Z I! Available plugins: 238 inputs, 9 aggregators, 34 processors, 26 parsers, 65 outputs, 6 secret-stores\nOct 29 04:13:50 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:13:50Z I! Loaded inputs: cpu disk diskio file (2x) kernel kernel_vmstat mem net net_response (2x) netstat ping processes procstat (8x) swap system systemd_units (3x)\nOct 29 04:13:50 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:13:50Z I! Loaded aggregators:\nOct 29 04:13:50 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:13:50Z I! Loaded processors:\nOct 29 04:13:50 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:13:50Z I! Loaded secretstores:\nOct 29 04:13:50 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:13:50Z I! Loaded outputs: graphite prometheus_client\nOct 29 04:13:50 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:13:50Z I! Tags enabled: datacenter=co host=imvaultstg-co-1 zzz_team=psa\nOct 29 04:13:50 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:13:50Z I! [agent] Config: Interval:10s, Quiet:false, Hostname:\"imvaultstg-co-1\", Flush Interval:10s\nOct 29 04:13:50 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:13:50Z W! [agent] The default value of 'skip_processors_after_aggregators' will change to 'true' with Telegraf v1.40.0! If you need the current default behavior, please explicitly set the option t>\nOct 29 04:13:50 imvaultstg-co-1 systemd[1]: Started Telegraf.\nOct 29 04:13:50 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:13:50Z D! [agent] Initializing plugins\nOct 29 04:13:50 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:13:50Z W! DeprecationWarning: Value \"false\" for option \"ignore_protocol_stats\" of plugin \"inputs.net\" deprecated since version 1.27.3 and will be removed in 1.36.0: use the 'inputs.nstat' plugin i>\nOct 29 04:13:50 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:13:50Z D! [agent] Connecting outputs\nOct 29 04:13:50 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:13:50Z D! [agent] Attempting connection to [outputs.graphite]\nOct 29 04:13:50 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:13:50Z D! [outputs.graphite] Successful connections: 1 of 1\nOct 29 04:13:50 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:13:50Z D! [agent] Successfully connected to outputs.graphite\nOct 29 04:13:50 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:13:50Z D! [agent] Attempting connection to [outputs.prometheus_client]\nOct 29 04:13:50 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:13:50Z I! [outputs.prometheus_client] Listening on http://[::]:9100/metrics\nOct 29 04:13:50 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:13:50Z D! [agent] Successfully connected to outputs.prometheus_client\nOct 29 04:13:50 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:13:50Z D! [agent] Starting service inputs\nOct 29 04:14:00 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:14:00Z D! [outputs.prometheus_client] Wrote batch of 77 metrics in 1.654693ms\nOct 29 04:14:00 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:14:00Z D! [outputs.prometheus_client] Buffer fullness: 0 / 10000 metrics\nOct 29 04:14:00 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:14:00Z D! [outputs.graphite] Wrote batch of 77 metrics in 15.971111ms\nOct 29 04:14:00 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:14:00Z D! [outputs.graphite] Buffer fullness: 0 / 10000 metrics\nOct 29 04:14:10 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:14:10Z D! [outputs.prometheus_client] Wrote batch of 78 metrics in 2.924033ms\nOct 29 04:14:10 imvaultstg-co-1 telegraf[4130045]: 2025-10-29T11:14:10Z D! [outputs.prometheus_client] Buffer fullness: 0 / 10000 metrics\n```\n\n### System info\n\n\"Debian GNU/Linux 11 (bullseye)\", Telegraf 1.35.2\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\nWe have a falcon systemd unit running a process that spawns several child processes\n\n```\n\u2500falcon-sensor.service \u2026\n             \u2502 \u2514\u2500sensor.falcon\n             \u2502   \u251c\u25001856439 /opt/CrowdStrike/falcond\n             \u2502   \u2514\u25001856440 falcon-sensor-bpf\n```\n\nPID ```1856439``` is a parent process of PID ```1856440 ```\n\n```\nroot     1856439       1  0 Oct14 ?        00:00:00 /opt/CrowdStrike/falcond\nroot     1856440 1856439  0 Oct14 ?        03:04:22 falcon-sensor-bpf\n```\n\n\n### Expected behavior\n\nWe expect that we can see metrics from both processes\n\n- falcond\n- falcon-sensor-bpf\n\n### Actual behavior\n\nWe don't see any metrics at all for any processes from ```falcon``` systemd unit.\n\n```\n# TYPE procstat_child_major_faults untyped\nprocstat_child_major_faults{cmdline=\"/usr/bin/telegraf -config /etc/telegraf/telegraf.conf -config-directory /etc/telegraf/telegraf.d\",datacenter=\"co\",host=\"imvaultstg-co-1\",process_name=\"telegraf\",status=\"sleep\",systemd_unit=\"telegraf.service\",user=\"telegraf\",zzz_team=\"psa\"} 2\nprocstat_child_major_faults{cmdline=\"/usr/local/qualys/cloud-agent/bin/qualys-cloud-agent\",datacenter=\"co\",host=\"imvaultstg-co-1\",process_name=\"qualys-cloud-agent\",status=\"sleep\",systemd_unit=\"qualys-cloud-agent.service\",user=\"root\",zzz_team=\"psa\"} 2739\nprocstat_child_major_faults{cmdline=\"/usr/sbin/cron -f\",datacenter=\"co\",host=\"imvaultstg-co-1\",process_name=\"cron\",status=\"sleep\",systemd_unit=\"cron.service\",user=\"root\",zzz_team=\"psa\"} 575\nprocstat_child_major_faults{cmdline=\"/usr/sbin/ntpd -p /var/run/ntpd.pid -g -u 106:112\",datacenter=\"co\",host=\"imvaultstg-co-1\",process_name=\"ntpd\",status=\"sleep\",systemd_unit=\"ntp.service\",user=\"ntp\",zzz_team=\"psa\"} 0\nprocstat_child_major_faults{cmdline=\"splunkd --under-systemd --systemd-delegate=no -p 8089 _internal_launch_under_systemd\",datacenter=\"co\",host=\"imvaultstg-co-1\",process_name=\"splunkd\",status=\"sleep\",systemd_unit=\"splunkforwarder.service\",user=\"splunkfwd\",zzz_team=\"psa\"} 846\nprocstat_child_major_faults{cmdline=\"sshd: /usr/sbin/sshd -D [listener] 0 of 10-100 startups\",datacenter=\"co\",host=\"imvaultstg-co-1\",process_name=\"sshd\",status=\"sleep\",systemd_unit=\"sshd.service\",user=\"root\",zzz_team=\"psa\"} 150\n```\n\n### Additional info\n\n_No response_",
      "solution": "I've got the issue too in Ubuntu 24.04. Maybee it's due to the generated path : \n\n`/sys/fs/cgroup/systemd/system.slice/myservice.service` generated [here](https://github.com/influxdata/telegraf/blob/d7a4ac661fecfffff4505b2150c7830b19dc564b/plugins/inputs/procstat/procstat.go#L586) and [here](https://github.com/influxdata/telegraf/blob/d7a4ac661fecfffff4505b2150c7830b19dc564b/plugins/inputs/procstat/procstat.go#L632). \n\nOn my OS the path should be : `/sys/fs/cgroup/system.slice/myservice.service`\n\nAs a workaround, I put this config :\n```\n[[inputs.procstat]]\n  cgroup = \"/sys/fs/cgroup/system.slice/myservice.service\"\n  tag_with = [ \"pid\" ]\n```\n\n\n\n\n---\n\n@imaximov89 and @jbarotin please test the binary in PR #18233, available as soon as CI finished the tests, and let me know if this fixes the issue!",
      "labels": [
        "bug"
      ],
      "created_at": "2025-10-29T11:24:44Z",
      "closed_at": "2026-01-28T19:47:58Z",
      "url": "https://github.com/influxdata/telegraf/issues/17904",
      "comments_count": 4
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 18247,
      "title": "[telegraf packaging] Package name error for Ubuntu (valid semver identifier)",
      "problem": "### Relevant telegraf.conf\n\n```toml\nThe configuration is never read.\n```\n\n### Logs from Telegraf\n\n```text\ntelegraf --config /etc/telegraf/telegraf.conf --config-directory /etc/telegraf/telegraf.d --test\n2026-01-21T16:52:19Z I! Starting Telegraf 1.21.4+ds1-0ubuntu2+esm2\npanic: failed to validate metadata: ds1-0ubuntu2+esm2 is not a valid semver identifier\n\ngoroutine 1 [running]:\ngithub.com/coreos/go-semver/semver.Must(...)\n\t/build/telegraf-k12Ia1/telegraf-1.21.4+ds1/vendor/github.com/coreos/go-semver/semver/semver.go:65\ngithub.com/coreos/go-semver/semver.New({0x5609defc93a0, 0x0})\n\t/build/telegraf-k12Ia1/telegraf-1.21.4+ds1/vendor/github.com/coreos/go-semver/semver/semver.go:49 +0x45\ngithub.com/influxdata/telegraf/config.NewConfig()\n\t/build/telegraf-k12Ia1/telegraf-1.21.4+ds1/config/config.go:118 +0x2d3\nmain.runAgent({0x5609e265c408, 0xc0004dd880}, {0x5609e5307810, 0x0, 0x0}, {0x5609e5307810, 0x0, 0x0})\n\t/build/telegraf-k12Ia1/telegraf-1.21.4+ds1/cmd/telegraf/telegraf.go:201 +0xea\nmain.reloadLoop({0x5609e5307810, 0x0, 0x0}, {0x5609e5307810, 0x0, 0x0})\n\t/build/telegraf-k12Ia1/telegraf-1.21.4+ds1/cmd/telegraf/telegraf.go:147 +0x28a\nmain.run(...)\n\t/build/telegraf-k12Ia1/telegraf-1.21.4+ds1/cmd/telegraf/telegraf_posix.go:8\nmain.main()\n\t/build/telegraf-k12Ia1/telegraf-1.21.4+ds1/cmd/telegraf/telegraf.go:485 +0xa9a\n```\n\n### System info\n\nTelegraf 1.21.4, Ubuntu 22.04\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1. Install with apt from Ubuntu official repository for Ubuntu 22.04\n2. Try to run it\n\n\n### Expected behavior\n\nProgram runs.\n\n### Actual behavior\n\nPanic.\n\n### Additional info\n\nStarting Telegraf 1.21.4+ds1-0ubuntu2+esm2\npanic: failed to validate metadata: ds1-0ubuntu2+esm2 is not a valid semver identifier\n\nAccording to SemVer\n+ds1-0ubuntu2+esm2 \u2192 double + \u2192 not valid",
      "solution": "No, I'm not sure it's that repository, since we didn't add it to our sources.list.\n\nWe simply do \"apt install telegraf\" from Ubuntu 22.04.\n\nBut:\nds1 \u2192 Debian source revision. This package originated in Debian.\n0ubuntu2 \u2192 Ubuntu\u2019s second revision of that Debian package.\n+esm2 \u2192 Canonical ESM patch level. This means the package is being maintained via the Ubuntu Pro / ESM repositories.\n\nSo the repository is: Ubuntu ESM (Ubuntu Pro) \u2013 Main or Universe\n\nIt seems then that the problem is caused by ESM, because indeed, we have Ubuntu pro enabled for security updates.\n\nSorry for the inconvenience, we will add your official repository.",
      "labels": [
        "bug"
      ],
      "created_at": "2026-01-21T17:11:01Z",
      "closed_at": "2026-01-22T16:40:51Z",
      "url": "https://github.com/influxdata/telegraf/issues/18247",
      "comments_count": 5
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 15404,
      "title": "SQL Server AAD Azure Auth method doesn't work if multiple User assigned identies are existing.",
      "problem": "### Relevant telegraf.conf\n\n```toml\ninputs:\r\n    - sqlserver:\r\n        interval: \"30s\"\r\n        servers:\r\n          -\"Server=dbserver.database.windows.net;Port=1433;database=dbname;hostNameInCertificate=*.database.windows.net;TrustServerCertificate=true;app name=telegraf;log=1;\"\r\n        auth_method: \"AAD\"\r\n        database_type: \"AzureSQLDB\"\r\n        exclude_query: \r\n          - \"AzureSQLDBSchedulers\"\r\n          - \"AzureSQLDBRequests\"\n```\n\n\n### Logs from Telegraf\n\n```text\n[inputs.sqlserver] Error in plugin: error creating AAD token provider for system assigned Azure managed identity : adal: Refresh request failed. Status Code = '400'. Response body: {\"error\":\"invalid_request\",\"error_description\":\"Multiple user assigned identities exist, please specify the clientId / resourceId of the identity in the token request\"}\n```\n\n\n### System info\n\nTelegraf 1.25.0-alpine; Kubernetes 1.30; Azure VM with two MIs\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1. Create Azure VM and install Kubernetes\r\n2. Assign two identities to the created VM.\r\n3. Create Azure SQL DB\r\n4. Deploy and setup Telegraf\r\n\n\n### Expected behavior\n\nThe option to select the desired Managed Identity(MI) or to specify its name in the Conf, if more then one MI is assigned to a vm.\r\n\r\nAt the top of the error log it is called clientId / resourceId. \n\n### Actual behavior\n\nAuthentication to the SQL DB isn't possible.\n\n### Additional info\n\n_No response_",
      "solution": "I have the same connection problem.\n\nInside a Telegraf container with sqlcmd, I run the following command:\n\n```bash\nsqlcmd -S <server>.database.windows.net -d <database> -G\n```\n\nIt works and I\u2019m able to connect to the database.\n\nBut in Telegraf, with this configuration:\n\n```\ninputs:\n  - sqlserver:\n      interval: \"1m\"\n      database_type: \"AzureSQLDB\"\n      auth_method: \"AAD\"\n      servers:\n        - \"Server=<server>.database.windows.net;Database=<database>;\"\n```\n\nIt throws the error mentioned in the issue. Is anyone else experiencing this problem?\n\nNote: Telegraf is not using AZURE_CLIENT_ID.",
      "labels": [
        "bug"
      ],
      "created_at": "2024-05-27T09:55:13Z",
      "closed_at": "2024-06-17T08:24:54Z",
      "url": "https://github.com/influxdata/telegraf/issues/15404",
      "comments_count": 4
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 10062,
      "title": "OPC-UA input plugin roadmap",
      "problem": "Consolidation of OPC-UA plugin issues/feature requests. A lot of bugs and feature requests have arisen since inputs.opcua was released. In this issue would like to consolidate what are library issues vs Telegraf bugs vs plugin feature requests\r\n\r\nBackground:\r\n- A good number of issues seem to be caused by the gopcua library. However this seems to be the only Go OPC UA library out there. Will need to determine the most efficient way on moving forward with these issues. \r\n- the number of OPC-UA devices out there are endless. The plugin was built and tested using the Prosys Simulator and that is what Telegraf maintainers use to troubleshoot as well. We need to create a group of community users that are willing to troubleshoot and test OPC-UA plugin issues. \r\n\r\n### Bugs / issues\r\n- [x] #11559\r\n\r\n### gopcua Library issues\r\n- [x] #8890 \r\n  - Status: More info in comment https://github.com/influxdata/telegraf/issues/8890#issuecomment-888618593\r\n- [x] #9163\r\n  - Status: opened https://github.com/gopcua/opcua/issues/435\r\n- [x] #9164\r\n  - Status: Waiting on testing feedback - May have been addressed from library upgrades\r\n- [x] #8583 - Receives `StatusBadSessionIDInvalid` error message even though everything is working fine. \r\n  - Status: Waiting on user response - but issue may possibly due to gopcua, `getData` function within the opcua client on line 455\r\n- [x] #9900 \r\n  - Status: opened https://github.com/gopcua/opcua/issues/478\r\n\r\n### Feature Requests\r\n- [x] **Title: Workarounds settings subtable**\r\nDescription: Add an [inputs.opcua.workarounds] subtable similar to the [Modbus input plugin workarounds](https://github.com/influxdata/telegraf/tree/master/plugins/inputs/modbus#workarounds) to address device settings that may differ from the OPC foundation specifications. This will help us address settings that may differ for non-tested devices. \r\nType: Feature Request, Device\r\nStatus: \r\nPriority: High\r\nrelated issues: https://github.com/influxdata/telegraf/issues/8826#issuecomment-970580640\r\nrelated PRs: https://github.com/influxdata/telegraf/pull/10384\r\n\r\n- [x] **Title: `Quality` should be a tag not field**\r\nDescription: Quality is a commonly filtered metric and should be stored as a tag.\r\nType: Data type\r\nStatus: Workaround - use converter processor\r\nPriority: Low\r\nrelated issues: #9405\r\nrelated PRs: https://github.com/influxdata/telegraf/pull/10458\r\n\r\n- [x] **Title: Events / subscription based plugin**\r\nDescription: OPC UA is subscription based, this behavior will be expected by a lot of plugin users\r\nType: Feature Request\r\nStatus: Ideally would like to implement in existing plugin\r\nPriority: High but large work\r\nrelated issues: #8083\r\nrelated PRs: \r\n\r\n- [x] **Title: Collect additional attributes such as datatype**\r\nDescription: Plugin currently only collects main value, but would be useful to also read in other attributes such as datatype.\r\nType: Metric collection\r\nStatus: \r\nPriority: Low\r\nrelated issues: #8669\r\nrelated PRs: \r\n\r\n- [ ] **Title: Support for ExtensionObjects**\r\nDescription: \r\nType: Feature Request\r\nStatus: \r\nPriority: Medium / High\r\nrelated issues: #9911 \r\nrelated PRs: \r\n\r\n- [ ] **Title: Add Hierarchical configuration**\r\nDescription: Be able to set node configuration as `tag_name=foo_*` to read a stream of available tags from OPC UA server.\r\nType: Metric collection\r\nStatus: \r\nPriority: Medium\r\nrelated issues: #8275 \r\nrelated PRs: \r\n\r\n### Device types\r\nFanuc - @joschott on community.influxdata.com\r\nKUKA DeviceConnector 2 - @mirkocomparetti-synthesis in #9163\r\nOPC Red Lion DA 10 servers - @jnangle server issue addressed in #9807",
      "solution": "Hi, I am one of the main authors and maintainers of the https://github.com/gopcua/opcua library and I am really happy that this library is being used in more areas. I was caught a bit by surprise by the inclusion to influx.\r\n\r\nSo far we have been using and testing this mostly in our factories at Northvolt with Siemens and Beckhoff PLCs and other maintainers were able to test with a different setup. But we for sure didn't cover the entire OPC/UA landscape with all their edge cases. \r\n\r\nYesterday, we have released the v0.2.0 version of the library which includes auto-reconnect and a cleanup of the subscription code. This was a year in the making since it was a big refactor but we've finally released this and I would like to go back to a faster cadence of smaller releases and bug fixes. \r\n\r\nIt would be great if we can address the issues you are finding and make the library more stable and useful for a greater community but we need your help since we usually don't have access to the equipment or the software. \n\n---\n\nWhat usually helps to troubleshoot the issues are tcpdumps.\n\n---\n\n@LarsStegman @R290 \r\n\r\nLooking for some wisdom and guidance on the remaining four remaining OPC-UA issues listed in the original issue report:\r\n\r\n- https://github.com/influxdata/telegraf/issues/8583\r\n    - There was an upstream issue filed around this that is now closed.\r\n    - Seemed to come from a user using the wrong identifier type, but also a change to allow auto-reconnect. Telegraf has that ability now via #11559. \r\n    - Should this issue be marked closed/resolved?  \r\n- https://github.com/influxdata/telegraf/issues/9911\r\n    - Based on one of the responses from the library maintainer, these objects need to be declared and registered at compile time. As a result, I am not sure how telegraf would support this without first adding the dynamic definition in the underlying library. How useful/important is this support? \r\n- https://github.com/influxdata/telegraf/issues/9900\r\n    - Is this one referenceing support for `extensionObject` as well?\r\n- https://github.com/influxdata/telegraf/issues/8275\r\n    - This final issue revolves around setting and using regex values to avoid more complex configurations. Is there a way to do discovery of what fields are there to support this type of syntax?\r\n\r\nThanks!",
      "labels": [
        "feature request",
        "area/opcua"
      ],
      "created_at": "2021-11-04T20:59:56Z",
      "closed_at": "2026-01-14T19:53:13Z",
      "url": "https://github.com/influxdata/telegraf/issues/10062",
      "comments_count": 19
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 16193,
      "title": "[inputs.tail] Plugin stop collect data when file is removed during the collect",
      "problem": "### Relevant telegraf.conf\n\n```toml\n[[inputs.filestat]]\r\n  files = [\"/opt/application/44otest/random_file/*.txt\"]\n```\n\n\n### Logs from Telegraf\n\n```text\n2024-11-14T20:51:07Z E! [inputs.tail] Tailing \"/opt/application/44otest/random_file/number_joffmgwfwv.txt\": Error watching for changes on /opt/application/44otest/random_file/number_joffmgwfwv.txt: no such file or directory\r\n\r\n2024-11-14T20:51:07Z E! [inputs.tail] Tailing \"/opt/application/44otest/random_file/number_jynjyxhlyv.txt\": Error watching for changes on /opt/application/44otest/random_file/number_jynjyxhlyv.txt: no such file or directory\n```\n\n\n### System info\n\nTelegraf 1.32.1, Redhat 8.4\n\n### Docker\n\nN/A\n\n### Steps to reproduce\n\n1. Use tail plugin for pattern file\r\n2. Execute telegraf\r\n3. Run script to generate and remove file very often\r\n4. Wait ;-)\r\n\n\n### Expected behavior\n\nThe plugin should generate an Warning event and continue to collect\n\n### Actual behavior\n\nThe plugin stop collect data\n\n### Additional info\n\n_No response_",
      "solution": "Thanks for reporting this issue. I've investigated the current state of the tail plugin and believe this has been addressed by recent fixes.                                                                \n                                                                                                                                                                                                              \n  Several PRs have improved file removal handling in the tail plugin:                                                                                                                                         \n                                                                                                                                                                                                              \n  - #16879 - Added `cleanupUnusedTailers()` to properly remove tailers for files that no longer match the glob pattern                                                                                        \n  - #17613 - Fixed data race conditions when cleaning up unused tailers                                                                                                                                       \n  - #17908 - Improved handling of permission issues during directory globbing                                                                                                                                 \n                                                                                                                                                                                                              \n                                                                                                                                                                  \n                                                                                                                                                                                                              \n  I tested the exact scenario described (rapid file creation/deletion while tailing a glob pattern) and confirmed:                                                                                            \n  - The plugin **continues collecting metrics** from other files that match the pattern                                                                                                                       \n  - New files that appear and match the pattern are picked up correctly                                                                                                                                       \n  - Tailers for removed files are cleaned up on subsequent gather cycles                                                                                                                                      \n                                                                                                                                                                                                              \n  ### About the Error Messages                                                                                                                                                                                 \n                                                                                                                                                                                                              \n  The error log `E! [inputs.tail] Tailing \"...\": Error watching for changes on ...: no such file or directory` will still appear when a file disappears while being watched. This is expected informational logging - it doesn't indicate the plugin has stopped working.                                                                                                                                               \n                                                                                                                                                                                                              \n  If these messages are too noisy for environments with rapidly rotating files, consider using `watch_method = \"poll\"` which can be more tolerant of this scenario.                                           \n                                                                                                                                                                                                              \n  Closing as the core issue (plugin stopping data collection) has been fixed. Please reopen if you're still experiencing the plugin completely stopping collection on a recent version (1.33+).               \n                                                                                                                                                                                                              ",
      "labels": [
        "bug"
      ],
      "created_at": "2024-11-15T08:30:50Z",
      "closed_at": "2026-01-13T18:15:05Z",
      "url": "https://github.com/influxdata/telegraf/issues/16193",
      "comments_count": 2
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17828,
      "title": "[[outputs.execd]] execd is not returning an error if an external plugin fails to write metrics",
      "problem": "### Relevant telegraf.conf\n\n```toml\n[agent]\n  interval = \"5s\"\n  round_interval = true\n  metric_batch_size = 1000\n  metric_buffer_limit = 10000\n  collection_jitter = \"0s\"\n  flush_interval = \"5s\"\n  flush_jitter = \"0s\"\n  precision = \"\"\n  hostname = \"\"\n  omit_hostname = false\n  quiet = false\n  debug = true\n\n[[inputs.influxdb_v2_listener]]\n  service_address = \":8086\"\n  bucket_tag = \"bucket\"\n\n[[outputs.execd]]\n  command = [\"./queuetestplugin\", \"-config\", \"plugins/outputs/queuetestplugin/sample.conf\"]\n```\n\n### Logs from Telegraf\n\n```text\ntelegraf --config telegraf.conf\n2025-10-14T16:24:55Z I! Loading config: telegraf.conf\n2025-10-14T16:24:55Z I! Starting Telegraf 1.36.2 brought to you by InfluxData the makers of InfluxDB\n2025-10-14T16:24:55Z I! Available plugins: 239 inputs, 9 aggregators, 35 processors, 26 parsers, 65 outputs, 6 secret-stores\n2025-10-14T16:24:55Z I! Loaded inputs: influxdb_v2_listener\n2025-10-14T16:24:55Z I! Loaded aggregators:\n2025-10-14T16:24:55Z I! Loaded processors:\n2025-10-14T16:24:55Z I! Loaded secretstores:\n2025-10-14T16:24:55Z I! Loaded outputs: execd\n2025-10-14T16:24:55Z I! Tags enabled: host=nico-laptop\n2025-10-14T16:24:55Z I! [agent] Config: Interval:5s, Quiet:false, Hostname:\"nico-laptop\", Flush Interval:5s\n2025-10-14T16:24:55Z W! [agent] The default value of 'skip_processors_after_aggregators' will change to 'true' with Telegraf v1.40.0! If you need the current default behavior, please explicitly set the option to 'false'!\n2025-10-14T16:24:55Z D! [agent] Initializing plugins\n2025-10-14T16:24:55Z D! [agent] Connecting outputs\n2025-10-14T16:24:55Z D! [agent] Attempting connection to [outputs.execd]\n2025-10-14T16:24:55Z I! [outputs.execd] Starting process: ./queuetestplugin [-config plugins/outputs/queuetestplugin/sample.conf]\n2025-10-14T16:24:55Z D! [agent] Successfully connected to outputs.execd\n2025-10-14T16:24:55Z D! [agent] Starting service inputs\n2025-10-14T16:24:55Z I! [inputs.influxdb_v2_listener] Started HTTP listener service on :8086\n2025-10-14T16:24:55Z I! [outputs.execd] Initializing queuetestplugin\n2025-10-14T16:24:55Z I! [outputs.execd] Queue TestPlugin Connect\n2025-10-14T16:25:00Z D! [outputs.execd] Buffer fullness: 0 / 10000 metrics\n2025-10-14T16:25:05Z D! [outputs.execd] Buffer fullness: 0 / 10000 metrics\n2025-10-14T16:25:10Z D! [outputs.execd] Wrote batch of 1 metrics in 84.658\u00b5s\n2025-10-14T16:25:10Z D! [outputs.execd] Buffer fullness: 0 / 10000 metrics\n2025-10-14T16:25:10Z I! [outputs.execd] Queue TestPlugin Write\n2025-10-14T16:25:10Z I! [outputs.execd] [measurement1 map[bucket:bucket1 host:nico-laptop region:france] map[float_field:0.64] 1760459106000000000]\n2025-10-14T16:25:10Z E! [outputs.execd] stderr: Failed to write metrics: failed to write the metrics\n2025-10-14T16:25:15Z D! [outputs.execd] Buffer fullness: 0 / 10000 metrics\n^C2025-10-14T16:25:19Z D! [agent] Stopping service inputs\n2025-10-14T16:25:19Z D! [agent] Input channel closed\n2025-10-14T16:25:19Z I! [agent] Hang on, flushing any cached metrics before shutdown\n2025-10-14T16:25:19Z D! [outputs.execd] Buffer fullness: 0 / 10000 metrics\n2025-10-14T16:25:19Z I! [agent] Stopping running outputs\n2025-10-14T16:25:19Z I! [outputs.execd] Process ./queuetestplugin shut down\n2025-10-14T16:25:19Z D! [agent] Stopped Successfully\n```\n\n### System info\n\nTelegraf 1.36.2 (git: HEAD@8bdd0265), Ubuntu 24.04\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1. Create a minimal output plugin using the Execd Go Shim, returning an error on each call to `Write(metrics []telegraf.Metric)`\n2. Run it as an external plugin\n3. The plugin buffer status log won't increase when sending metrics\n4. Put the same plugin in the Telegraf outputs plugin directory, and build Telegraf with it\n5. The plugin buffer status log will increase when sending metrics\n...\n\n\n### Expected behavior\n\nThe execd plugin is returning an error if the plugin  `Write(metrics []telegraf.Metric)` function is returning an error.\n\n### Actual behavior\n\nThe execd plugin is **NOT** returning an error if the plugin `Write(metrics []telegraf.Metric)` function is returning an error.\n\n### Additional info\n\nIf this is considered as a normal behavior, it should be written in the execd documentation.",
      "solution": "Investigated this and here's what's happening:\n\n  The `Write()` method in `outputs.execd` only checks if serialisation succeeds and if writing to the external process's stdin pipe succeeds. The problem is stdin writes are non-blocking up to the OS pipe buffer (~64KB), so Write() returns success as soon as bytes hit the pipe, it doesn't wait for the external plugin to actually process them.\n\n  The shim does report errors to stderr when `Write()` fails:\n```\n  if err := s.Output.Write(batch); err != nil {\n      fmt.Fprintf(os.Stderr, \"Failed to write metrics: %s\\n\", err)\n  }\n```\n\n  But the execd output just logs these stderr messages, it doesn't propagate them back to trigger retries. So Telegraf thinks the write succeeded, removes metrics from the buffer, and they're lost.\n\n  Fixing this properly would need an acknowledgment protocol between Telegraf and external plugins, which is a breaking change to the shim. For now I think the right approach is to document this limitation in the README so users are aware that:\n  - Metrics are considered \"written\" once they reach the stdin pipe, not once processed\n  - Errors from the external plugin are logged but don't trigger buffer retries\n  - This can lead to metric loss if the external plugin fails\n\n  Will open a PR with README updates.\n",
      "labels": [
        "bug"
      ],
      "created_at": "2025-10-14T16:39:15Z",
      "closed_at": "2026-01-12T16:22:10Z",
      "url": "https://github.com/influxdata/telegraf/issues/17828",
      "comments_count": 1
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 18105,
      "title": "Plugin label cannot contain capitals",
      "problem": "### Relevant telegraf.conf\n\n```toml\n[[inputs.ping]]\n  [inputs.ping.labels]\n    key = \"FOObar\"\n```\n\n### Logs from Telegraf\n\n```text\n2025-12-09T11:42:24Z E! loading config file telegraf.conf failed: error parsing ping, invalid label in plugin inputs.ping: invalid value \"FOObar\"\n```\n\n### System info\n\nTelegraf 1.37.0\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1. Run `telegraf config check`\n\n\n### Expected behavior\n\nAccording to [TSD-010](https://github.com/influxdata/telegraf/blob/master/docs/specs/tsd-010-labels-and-selectors.md), the labels and their value could contain capital letters.\n\n### Actual behavior\n\nTelegraf errors out when using capital letters in key or value for a label.\n\n### Additional info\n\n_No response_",
      "solution": "I don't feel like #18108 completely resolved this issue, see my comments.\n\nCurrent nightly builds accepts this label:\n```toml\n[[inputs.ping]]\n  [inputs.ping.labels]\n    key = \"FOObar?*\"\n```",
      "labels": [
        "bug"
      ],
      "created_at": "2025-12-09T11:53:51Z",
      "closed_at": "2025-12-12T17:55:50Z",
      "url": "https://github.com/influxdata/telegraf/issues/18105",
      "comments_count": 8
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 16401,
      "title": "Telegraf to telegraf communication failures",
      "problem": "### Relevant telegraf.conf\nSender:\n```toml\n[[outputs.influxdb_v2]]\n  alias = \"backbone\"\n  urls = [\"http://127.0.0.1:8086\"]\n  bucket_tag = \"role\"\n```\nReceiver:\n```toml\n# Accept metrics over InfluxDB 2.x HTTP API\n[[inputs.influxdb_v2_listener]]\n  ## Address and port to host InfluxDB listener on\n  ## (Double check the port. Could be 9999 if using OSS Beta)\n  service_address = \":8086\"\n\n  ## Maximum undelivered metrics before rate limit kicks in.\n  ## When the rate limit kicks in, HTTP status 429 will be returned.\n  ## 0 disables rate limiting\n  # max_undelivered_metrics = 0\n\n  ## Maximum duration before timing out read of the request\n  # read_timeout = \"10s\"\n  ## Maximum duration before timing out write of the response\n  # write_timeout = \"10s\"\n\n  ## Maximum allowed HTTP request body size in bytes.\n  ## 0 means to use the default of 32MiB.\n  # max_body_size = \"32MiB\"\n\n  ## Optional tag to determine the bucket.\n  ## If the write has a bucket in the query string then it will be kept in this tag name.\n  ## This tag can be used in downstream outputs.\n  ## The default value of nothing means it will be off and the database will not be recorded.\n  bucket_tag = \"role\"\n\n  ## Set one or more allowed client CA certificate file names to\n  ## enable mutually authenticated TLS connections\n  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## Add service certificate and key\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Optional token to accept for HTTP authentication.\n  ## You probably want to make sure you have TLS configured above for this.\n  # token = \"some-long-shared-secret-token\"\n\n  ## Influx line protocol parser\n  ## 'internal' is the default. 'upstream' is a newer parser that is faster\n  ## and more memory efficient.\n  parser_type = \"upstream\"\n\n```\n\n### Logs from Telegraf\n\n```text\nJan 15 07:00:47 grw-pol-u1 telegraf[664564]: 2025-01-15T06:00:47Z E! [outputs.influxdb_v2::backbone] When writing to [http://127.0.0.1:8086/api/v2/write]: Post \"http://127.0.0.1:8086/api/v2/write?bucket=CARRIER+ETHERNET&org=\": EOF\nJan 15 07:11:47 grw-pol-u1 telegraf[664564]: 2025-01-15T06:11:47Z E! [outputs.influxdb_v2::backbone] When writing to [http://127.0.0.1:8086/api/v2/write]: Post \"http://127.0.0.1:8086/api/v2/write?bucket=CARRIER+ETHERNET&org=\": read tcp 127.0.0.1:45974->127.0.0.1:8086: read: connection reset by peer\nJan 15 07:16:37 grw-pol-u1 telegraf[664564]: 2025-01-15T06:16:37Z E! [outputs.influxdb_v2::backbone] When writing to [http://127.0.0.1:8086/api/v2/write]: Post \"http://127.0.0.1:8086/api/v2/write?bucket=CORE&org=\": net/http: HTTP/1.x transport connection broken: write tcp 127.0.0.1:38982->127.0.0.1:8086: write: broken pipe\nJan 15 07:25:47 grw-pol-u1 telegraf[664564]: 2025-01-15T06:25:47Z E! [outputs.influxdb_v2::backbone] When writing to [http://127.0.0.1:8086/api/v2/write]: Post \"http://127.0.0.1:8086/api/v2/write?bucket=PE&org=\": read tcp 127.0.0.1:33632->127.0.0.1:8086: read: connection reset by peer\nJan 15 07:32:57 grw-pol-u1 telegraf[664564]: 2025-01-15T06:32:57Z E! [outputs.influxdb_v2::backbone] When writing to [http://127.0.0.1:8086/api/v2/write]: Post \"http://127.0.0.1:8086/api/v2/write?bucket=PE&org=\": EOF\nJan 15 07:39:37 grw-pol-u1 telegraf[664564]: 2025-01-15T06:39:37Z E! [outputs.influxdb_v2::backbone] When writing to [http://127.0.0.1:8086/api/v2/write]: Post \"http://127.0.0.1:8086/api/v2/write?bucket=PE&org=\": read tcp 127.0.0.1:38394->127.0.0.1:8086: read: connection reset by peer\nJan 15 07:43:47 grw-pol-u1 telegraf[664564]: 2025-01-15T06:43:47Z E! [outputs.influxdb_v2::backbone] When writing to [http://127.0.0.1:8086/api/v2/write]: Post \"http://127.0.0.1:8086/api/v2/write?bucket=CARRIER+ETHERNET&org=\": EOF\nJan 15 07:47:57 grw-pol-u1 telegraf[664564]: 2025-01-15T06:47:57Z E! [outputs.influxdb_v2::backbone] When writing to [http://127.0.0.1:8086/api/v2/write]: Post \"http://127.0.0.1:8086/api/v2/write?bucket=PE&org=\": net/http: HTTP/1.x transport connection broken: write tcp 127.0.0.1:41082->127.0.0.1:8086: write: broken pipe\nJan 15 07:53:47 grw-pol-u1 telegraf[664564]: 2025-01-15T06:53:47Z E! [outputs.influxdb_v2::backbone] When writing to [http://127.0.0.1:8086/api/v2/write]: Post \"http://127.0.0.1:8086/api/v2/write?bucket=CARRIER+ETHERNET&org=\": write tcp 127.0.0.1:43808->127.0.0.1:8086: use of closed network connection\nJan 15 07:55:37 grw-pol-u1 telegraf[664564]: 2025-01-15T06:55:37Z E! [outputs.influxdb_v2::backbone] When writing to [http://127.0.0.1:8086/api/v2/write]: Post \"http://127.0.0.1:8086/api/v2/write?bucket=CARRIER+ETHERNET&org=\": EOF\nJan 15 08:01:57 grw-pol-u1 telegraf[664564]: 2025-01-15T07:01:57Z E! [outputs.influxdb_v2::backbone] When writing to [http://127.0.0.1:8086/api/v2/write]: Post \"http://127.0.0.1:8086/api/v2/write?bucket=CORE&org=\": read tcp 127.0.0.1:60788->127.0.0.1:8086: read: connection reset by peer\nJan 15 08:02:47 grw-pol-u1 telegraf[664564]: 2025-01-15T07:02:47Z E! [outputs.influxdb_v2::backbone] When writing to [http://127.0.0.1:8086/api/v2/write]: Post \"http://127.0.0.1:8086/api/v2/write?bucket=PE&org=\": read tcp 127.0.0.1:60534->127.0.0.1:8086: read: connection reset by peer\nJan 15 08:45:47 grw-pol-u1 telegraf[664564]: 2025-01-15T07:45:47Z E! [outputs.influxdb_v2::backbone] When writing to [http://127.0.0.1:8086/api/v2/write]: Post \"http://127.0.0.1:8086/api/v2/write?bucket=CARRIER+ETHERNET&org=\": net/http: HTTP/1.x transport connection broken: write tcp 127.0.0.1:53202->127.0.0.1:8086: write: broken pipe\n```\n\n### System info\n\nRed Hat Enterprise Linux release 9.4 (Plow)\nTelegraf 1.33.1 (git: HEAD@44f3a504)\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1. Run 2 telegraf instances\n2. Generate metrics\n3. Observe logs indicating connection resets, broken pipes, ...\n\n### Expected behavior\n\nTelegraf to be able to communicate with another telegraf reliably without issues.\n\n### Actual behavior\n\nConnection issues at random times.\n\n### Additional info\n\n_No response_",
      "solution": "I face the same issue at random times, I have a 10s write from one telegraf to another telegraf docker instance. It fails at random times for the fixed number of metrics being sent every interval, then it revives back again in the next interval and sends the buffered data together.",
      "labels": [
        "bug"
      ],
      "created_at": "2025-01-15T10:45:36Z",
      "closed_at": "2025-02-26T18:09:42Z",
      "url": "https://github.com/influxdata/telegraf/issues/16401",
      "comments_count": 10
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17545,
      "title": "[inputs.sqlserver] support parameter protocol in go-mssqldb",
      "problem": "### Use Case\n\nNow Telegraf can only connect to MSSQL via **TCP**. Sometimes it is necessary to disable **TCP** and use only SharedMemory(Local Procedure Call (LPC)).\nAnd additionally, after the August Windows OS updates, Telegraf stopped working if **TLS+EP (Channel Binding)** is enabled\n\n### Expected behavior\n\nThe telegraf can be connected to **MSSQL** via the **LPC** protocol and **NP**(Named Pipes).\n\n### Actual behavior\n\nWhen trying to add a parameter **protocol** with a value different from **tcp**.\nExample:\n\n> [[inputs.sqlserver]]\n> servers = [\n> \"Server=Localhost;Integrated Security=SSPI;app name=telegraf;protocol=lpc;log=1\"\n\nAn error occurs:\n_2025-08-28T16:23:40Z E! [inputs.sqlserver] Error in plugin: query SQLServerPersistentVersionStore failed for server: LOCALHOST and database: with Error: No protocol handler is available for protocol: 'lpc'_\n\n### Additional info\n\nhttps://github.com/microsoft/go-mssqldb?tab=readme-ov-file#protocol-configuration",
      "solution": "The issue is still relevant.\nTelegraf can only connect via TCP.\nAfter July/August, Microsoft released updates for Windows Server 2019/2022 that break connections to MSSQL if TLS and EP are enabled.\nError: \n> Message:\nSSPI handshake failed with error code 0x80090346, state 45 while establishing a connection with integrated security; the connection has been closed. Reason: The Channel Bindings from this client do not match the established Transport Layer Security (TLS) Channel. The service might be under attack, or the data provider might need to be upgraded to support Extended Protection. Closing the connection. Client's supplied SSPI channel bindings were incorrect.\n\n> Message:\nLogin failed. The login is from an untrusted domain and cannot be used with Integrated authentication.`\n\nIf I disable Extended Protection, everything works, but we can't.\nTelegraf is installed locally and connects to a local instance. I tried using different accounts, including gMSA, with the same result.\nIf we remove these operating system updates, then the telegraf will work without errors again.\nSample config:\n\n> [[inputs.sqlserver]]\nservers = [\n\"Server=ServerName;Integrated Security=SSPI;app name=telegraf;log=1;encrypt=true;hostNameInCertificate=FQDN\",\n]\n\n@srebhan Can you help?\n\n---\n\n@varakspavel looking into the Telegraf code I don't think there is anything we can do from our side regarding the errors shown above ...\n\n This might be a driver issue, however [this issue](https://github.com/microsoft/go-mssqldb/issues/245) suggests that extended protection is supported. Maybe you can join the discussion there and provide the information above in the linked issue!?\n\nFrom digging a bit deeper it seems like your TLS connection is v1.0 as [this article](https://learn.microsoft.com/en-us/answers/questions/2004580/sspi-handshake-failed-with-error-code-0x80090346-s) suggests. Looking at the [driver configuration parameters](https://github.com/microsoft/go-mssqldb/blob/main/README.md#common-parameters) I think you need to use `encrypt=strict` instead of `encrypt=true`...\n\nBut I'm by no means a MS-SQL expert, so asking at the driver repository is probably more fruitful! If you get any insights, please update this issue (and/or the plugin README) and link to the relevant driver-issue if any...\n\n---\n\n@varakspavel please test the binary in PR #17924, available for 72 hours as soon as CI finished the tests, and let me know if this fixes the issue. The PR adds support for the `lpc` and `np` protocols as initially requested in this issue...",
      "labels": [
        "feature request"
      ],
      "created_at": "2025-08-28T19:18:41Z",
      "closed_at": "2025-12-04T18:27:43Z",
      "url": "https://github.com/influxdata/telegraf/issues/17545",
      "comments_count": 4
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 11454,
      "title": "OPC UA Generating new self signed certificates loop",
      "problem": "### Relevant telegraf.conf\r\n\r\n```toml\r\n# Telegraf Configuration\r\n#\r\n# Telegraf is entirely plugin driven. All metrics are gathered from the\r\n# declared inputs, and sent to the declared outputs.\r\n#\r\n# Plugins must be declared in here to be active.\r\n# To deactivate a plugin, comment out the name and any variables.\r\n#\r\n# Use 'telegraf -config telegraf.conf -test' to see what metrics a config\r\n# file would generate.\r\n#\r\n# Environment variables can be used anywhere in this config file, simply surround\r\n# them with ${}. For strings the variable must be within quotes (ie, \"${STR_VAR}\"),\r\n# for numbers and booleans they should be plain (ie, ${INT_VAR}, ${BOOL_VAR})\r\n\r\n\r\n# Global tags can be specified here in key=\"value\" format.\r\n[global_tags]\r\n  # dc = \"us-east-1\" # will tag all metrics with dc=us-east-1\r\n  # rack = \"1a\"\r\n  ## Environment variables can be used as tags, and throughout the config file\r\n  # user = \"$USER\"\r\n\r\n\r\n# Configuration for telegraf agent\r\n[agent]\r\n  ## Default data collection interval for all inputs\r\n  interval = \"5s\"\r\n  ## Rounds collection interval to 'interval'\r\n  ## ie, if interval=\"10s\" then always collect on :00, :10, :20, etc.\r\n  round_interval = true\r\n\r\n  ## Telegraf will send metrics to outputs in batches of at most\r\n  ## metric_batch_size metrics.\r\n  ## This controls the size of writes that Telegraf sends to output plugins.\r\n  metric_batch_size = 1000\r\n\r\n  ## Maximum number of unwritten metrics per output.  Increasing this value\r\n  ## allows for longer periods of output downtime without dropping metrics at the\r\n  ## cost of higher maximum memory usage.\r\n  metric_buffer_limit = 10000\r\n\r\n  ## Collection jitter is used to jitter the collection by a random amount.\r\n  ## Each plugin will sleep for a random time within jitter before collecting.\r\n  ## This can be used to avoid many plugins querying things like sysfs at the\r\n  ## same time, which can have a measurable effect on the system.\r\n  collection_jitter = \"5s\"\r\n\r\n  ## Default flushing interval for all outputs. Maximum flush_interval will be\r\n  ## flush_interval + flush_jitter\r\n  flush_interval = \"10s\"\r\n  ## Jitter the flush interval by a random amount. This is primarily to avoid\r\n  ## large write spikes for users running a large number of telegraf instances.\r\n  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s\r\n  flush_jitter = \"5s\"\r\n\r\n  ## By default or when set to \"0s\", precision will be set to the same\r\n  ## timestamp order as the collection interval, with the maximum being 1s.\r\n  ##   ie, when interval = \"10s\", precision will be \"1s\"\r\n  ##       when interval = \"250ms\", precision will be \"1ms\"\r\n  ## Precision will NOT be used for service inputs. It is up to each individual\r\n  ## service input to set the timestamp at the appropriate precision.\r\n  ## Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\".\r\n  precision = \"\"\r\n\r\n\r\n  ## Override default hostname, if empty use os.Hostname()\r\n  hostname = \"\"\r\n  ## If set to true, do no set the \"host\" tag in the telegraf agent.\r\n  omit_hostname = false\r\n\r\n  debug = true\r\n\r\n  quiet = false\r\n\r\n\r\n###############################################################################\r\n#                            OUTPUT PLUGINS                                   #\r\n###############################################################################\r\n\r\n# Configuration for sending metrics to InfluxDB 2.0\r\n[[outputs.influxdb_v2]]\r\nalias = \"opc-ua-db\"\r\n  ## The URLs of the InfluxDB cluster nodes.\r\n  ##\r\n  ## Multiple URLs can be specified for a single cluster, only ONE of the\r\n  ## urls will be written to each interval.\r\n  ##   ex: urls = [\"https://us-west-2-1.aws.cloud2.influxdata.com\"]\r\n  urls = [\"${INFLUX_HOST}\"]\r\n\r\n  ## Token for authentication.\r\n  token = \"${INFLUX_TOKEN}\"\r\n\r\n  ## Organization is the name of the organization you wish to write to.\r\n  organization = \"${INFLUX_ORG}\"\r\n\r\n  ## Destination bucket to write into.\r\n  bucket = \"${INFLUX_BUCKET}\"\r\n\r\n\r\n\r\n###############################################################################\r\n#                            INPUT PLUGINS                                    #\r\n###############################################################################\r\n[[inputs.opcua]]\r\n  ## Metric name\r\n  name = \"opcua\"\r\n  #\r\n  ## OPC UA Endpoint URL\r\n  endpoint = \"opc.tcp://localhost:49320\"\r\n  #\r\n  ## Maximum time allowed to establish a connect to the endpoint.\r\n  # connect_timeout = \"10s\"\r\n  #\r\n  ## Maximum time allowed for a request over the estabilished connection.\r\n  # request_timeout = \"5s\"\r\n  #\r\n  ## Security policy, one of \"None\", \"Basic128Rsa15\", \"Basic256\",\r\n  ## \"Basic256Sha256\", or \"auto\"\r\n   security_policy = \"auto\"\r\n  #\r\n  ## Security mode, one of \"None\", \"Sign\", \"SignAndEncrypt\", or \"auto\"\r\n   security_mode = \"auto\"\r\n  #\r\n  ## Path to cert.pem. Required when security mode or policy isn't \"None\".\r\n  ## If cert path is not supplied, self-signed cert and key will be generated.\r\n   certificate = \"\"\r\n  #\r\n  ## Path to private key.pem. Required when security mode or policy isn't \"None\".\r\n  ## If key path is not supplied, self-signed cert and key will be generated.\r\n   private_key = \"\"\r\n  #\r\n  ## Authentication Method, one of \"Certificate\", \"UserName\", or \"Anonymous\".  To\r\n  ## authenticate using a specific ID, select 'Certificate' or 'UserName'\r\n   auth_method = \"UserName\"\r\n  #\r\n  ## Username. Required for auth_method = \"UserName\"\r\n   username = \"telegraf\"\r\n  #\r\n  ## Password. Required for auth_method = \"UserName\"\r\n   password = \"telegraftelegraf\"\r\n  #\r\n  ## Option to select the metric timestamp to use. Valid options are:\r\n  ##     \"gather\" -- uses the time of receiving the data in telegraf\r\n  ##     \"server\" -- uses the timestamp provided by the server\r\n  ##     \"source\" -- uses the timestamp provided by the source\r\n  # timestamp = \"gather\"\r\n  #\r\n  ## Node ID configuration\r\n  ## name              - field name to use in the output\r\n  ## namespace         - OPC UA namespace of the node (integer value 0 thru 3)\r\n  ## identifier_type   - OPC UA ID type (s=string, i=numeric, g=guid, b=opaque)\r\n  ## identifier        - OPC UA ID (tag as shown in opcua browser)\r\n  ## tags              - extra tags to be added to the output metric (optional)\r\n  ## Example:\r\n  ## {name=\"ProductUri\", namespace=\"0\", identifier_type=\"i\", identifier=\"2262\", tags=[[\"tag1\",\"value1\"],[\"tag2\",\"value2]]}\r\n   nodes = [\r\n    {name=\"Current\", namespace=\"2\", identifier_type=\"s\", identifier=\"Bender.Bender.Current\"},\r\n    {name=\"Bad_Quantity\", namespace=\"2\", identifier_type=\"s\", identifier=\"Bender.Bender.Bad_Quantity\"},\r\n  ]\r\n  #\r\n  ## Node Group\r\n  ## Sets defaults for OPC UA namespace and ID type so they aren't required in\r\n  ## every node.  A group can also have a metric name that overrides the main\r\n  ## plugin metric name.\r\n  ##\r\n  ## Multiple node groups are allowed\r\n  #[[inputs.opcua.group]]\r\n  ## Group Metric name. Overrides the top level name.  If unset, the\r\n  ## top level name is used.\r\n  # name =\r\n  #\r\n  ## Group default namespace. If a node in the group doesn't set its\r\n  ## namespace, this is used.\r\n  # namespace =\r\n  #\r\n  ## Group default identifier type. If a node in the group doesn't set its\r\n  ## namespace, this is used.\r\n  # identifier_type =\r\n  #\r\n  ## Node ID Configuration.  Array of nodes with the same settings as above.\r\n  # nodes = [\r\n  #  {name=\"\", namespace=\"\", identifier_type=\"\", identifier=\"\"},\r\n  #  {name=\"\", namespace=\"\", identifier_type=\"\", identifier=\"\"},\r\n  #]\r\n```\r\n\r\n\r\n### Logs from Telegraf\r\n\r\n```text\r\n2022-07-05T12:09:18Z I! Starting Telegraf 1.23.0\r\n2022-07-05T12:09:18Z I! Loaded inputs: opcua\r\n2022-07-05T12:09:18Z I! Loaded aggregators: \r\n2022-07-05T12:09:18Z I! Loaded processors: \r\n2022-07-05T12:09:18Z I! Loaded outputs: influxdb_v2\r\n2022-07-05T12:09:18Z I! Tags enabled: host=Jays-MBP\r\n2022-07-05T12:09:18Z I! [agent] Config: Interval:5s, Quiet:false, Hostname:\"Jays-MBP\", Flush Interval:10s\r\n2022-07-05T12:09:18Z D! [agent] Initializing plugins\r\n2022-07-05T12:09:18Z D! [agent] Connecting outputs\r\n2022-07-05T12:09:18Z D! [agent] Attempting connection to [outputs.influxdb_v2::opc-ua-db]\r\n2022-07-05T12:09:18Z D! [agent] Successfully connected to outputs.influxdb_v2::opc-ua-db\r\n2022-07-05T12:09:18Z D! [agent] Starting service inputs\r\n2022-07-05T12:09:23Z E! [inputs.opcua] Error in plugin: error in Client Connection: EOF\r\n2022-07-05T12:09:29Z E! [inputs.opcua] Error in plugin: error in Client Connection: EOF\r\n2022-07-05T12:09:30Z D! [outputs.influxdb_v2::opc-ua-db] Buffer fullness: 0 / 10000 metrics\r\n```\r\n\r\n\r\n### System info\r\n\r\nMacOS, Linux, Windows\r\n\r\n### Docker\r\n\r\n_No response_\r\n\r\n### Steps to reproduce\r\n\r\nHopefully, I can explain it in steps:\r\nTelegraf auto generates a self-signed certificate and sends this to the OPC UA server.\r\n\r\n1. Telegraf then proceeds to request tags but fails since the OPC UA server has rejected the client because the certificates are untrusted.\r\n2. User accepts the certificates within their OPC UA server. Telegraf continues to fall over with the same error message.\r\n3. On telegraf restart a new certificate and key is generated and requires authorisation from the server once again.\r\n4. Some times you get lucky and catch the trust store quick enough and Telegraf will operate as expected. It's hard to gauge however as certificate errors output a EOF error.\r\n\r\n### Expected behavior\r\n\r\nAuto self-signed certificates are generated once and not recreated if they already exist. \r\n\r\n### Actual behavior\r\n\r\nNew certificates are generated on each restart of Telegraf. \r\n\r\n### Additional info\r\n\r\nCurrent workaround. Generate self-signed manually: https://github.com/InfluxCommunity/Telegraf-Community-Configs/tree/master/Kepware/certificates",
      "solution": "@srebhan, I agree with you the issue with regards to the duplicate certificates might well be an issue on Kepware's end. \r\nThough I believe there should be a flag to make the auto-generated certificates permanent rather than temporary. I agree that users should generate their own certificates to normally do this which is why I created the certificate generation script. Though I do not understand why we would generate a new certificate on each restart of Telegraf. \r\n\r\nThis means Telegraf would generate a new certificate under the following scenarios and require acceptance once again by the trust store:\r\n1. Device running Telegraf is restarted\r\n2. Telegraf is restarted (config change, falls over for another unrelated error)\r\n\r\nHappy to change this to a feature change rather than a bug. My initial issue was debunked in my own video :D \n\n---\n\nWell we could do this if we can agree on a _fixed_ location and generate a cert if there is none. However, how do you determine that location without interfering with the usual certs? \n\n---\n\nLet's discuss this in the maintainers meeting. However, please note that on most modern Linux systems `/tmp` content does not survive a reboot, making the \"new certificate generation\" even more unpredictable...\r\nA fixed folder on the contrary might break for docker containers etc... You see this can become complicated. ;-)",
      "labels": [
        "feature request"
      ],
      "created_at": "2022-07-05T12:13:13Z",
      "closed_at": "2025-11-25T20:16:38Z",
      "url": "https://github.com/influxdata/telegraf/issues/11454",
      "comments_count": 25
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 11277,
      "title": "SQLServer Input: Managed Identity token expired but not refreshed",
      "problem": "### Relevant telegraf.conf\r\n\r\n```toml\r\n[agent]\r\n  collection_jitter = \"0s\"\r\n  debug = false\r\n  flush_interval = \"10s\"\r\n  flush_jitter = \"0s\"\r\n  hostname = \"$HOSTNAME\"\r\n  interval = \"10s\"\r\n  logfile = \"\"\r\n  metric_batch_size = 1000\r\n  metric_buffer_limit = 10000\r\n  omit_hostname = false\r\n  precision = \"\"\r\n  quiet = false\r\n  round_interval = true\r\n[[processors.enum]]\r\n   [[processors.enum.mapping]]\r\n    dest = \"status_code\"\r\n    field = \"status\"\r\n    [processors.enum.mapping.value_mappings]\r\n        critical = 3\r\n        healthy = 1\r\n        problem = 2\r\n\r\n[[outputs.prometheus_client]]\r\n  listen = \":9273\"\r\n  path = \"/metrics\"\r\n\r\n[[inputs.sqlserver]]\r\n  auth_method = \"AAD\"\r\n  database_type = \"AzureSQLDB\"\r\n  exclude_query = [\r\n    \"AzureSQLDBSchedulers\",\r\n    \"AzureSQLDBRequests\"\r\n  ]\r\n  interval = \"30s\"\r\n  servers = [ \"Server=azuresqldb.database.windows.net;Port=1433;database=mydb1;hostNameInCertificate=*.database.windows.net;TrustServerCertificate=true;app name=telegraf;log=1;\",\r\n    \"Server=azuresqldb.database.windows.net;Port=1433;database=mydb2;hostNameInCertificate=*.database.windows.net;TrustServerCertificate=true;app name=telegraf;log=1;\"\r\n  ]\r\n\r\n[[inputs.internal]]\r\n  collect_memstats = false\r\n```\r\n\r\n\r\n### Logs from Telegraf\r\n\r\n```text\r\n2022-06-09T13:51:00Z E! [inputs.sqlserver] Error in plugin: query AzureSQLDBWaitStats failed for server: azuresqldb.database.windows.net and database: mydb1 with Msg 18456, Level 14, State 233:, Line 1, Error: mssql: login error: Login failed for user '<token-identified principal>'. Token is expired.\r\n2022-06-09T13:51:00Z E! [inputs.sqlserver] Error in plugin: query AzureSQLDBResourceStats failed for server: azuresqldb.database.windows.net and database: mydb2 with Msg 18456, Level 14, State 233:, Line 1, Error: mssql: login error: Login failed for user '<token-identified principal>'. Token is expired.\r\n```\r\n\r\n\r\n### System info\r\n\r\nTelegraf 1.22.3 (git: HEAD ff950615)\r\n\r\n### Docker\r\n\r\n_No response_\r\n\r\n### Steps to reproduce\r\n\r\n1. Setup Azure SQL DB, permit a Managed Identity as described in manual\r\n2. Setup Azure Linux VM, assign Managed Identity as \"User Assigned\"\r\n3. Start Telegraf\r\n4. After ~24 hours token is expired and messages show up in log\r\n...\r\n\r\n\r\n### Expected behavior\r\n\r\nToken is refreshed (shortly) before the expiration\r\n\r\n### Actual behavior\r\n\r\nToken seems to be not refreshed\r\n\r\n### Additional info\r\n\r\nIn the master branch\r\n- \"refreshToken\" (line 483) is called in function getTokenProvider line 448\r\n- getTokenProvider (line 428) is called in function Start line 426\r\n\r\nWhen/how often is the Start function called?",
      "solution": "Worrying that this bug (that we are now also experiencing) has had no resolution in over a year.\r\n\r\nIs the sqlserver input mothballed ?",
      "labels": [
        "bug",
        "platform/windows",
        "area/sqlserver",
        "waiting for response"
      ],
      "created_at": "2022-06-09T14:06:05Z",
      "closed_at": "2025-11-24T18:09:48Z",
      "url": "https://github.com/influxdata/telegraf/issues/11277",
      "comments_count": 5
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 11372,
      "title": "Tail input plugin does not read line-by-line and sends partial line data",
      "problem": "### Relevant telegraf.conf\n\n```toml\n# Configuration for telegraf agent\r\n[agent]\r\n    interval = \"60s\"\r\n    debug = false\r\n    hostname = \"<hostname>\"\r\n    round_interval = true\r\n    flush_interval = \"60s\"\r\n    flush_jitter = \"0s\"\r\n    collection_jitter = \"0s\"\r\n    metric_batch_size = 3000\r\n    metric_buffer_limit = 15000\r\n    quiet = false\r\n    logtarget = \"file\"\r\n    logfile = \"/opt/logs/telegraf.log\"\r\n    logfile_rotation_interval = \"24h\"\r\n    logfile_rotation_max_size = \"10MB\"\r\n    logfile_rotation_max_archives = 5\r\n    omit_hostname = false\r\n\r\n[[outputs.influxdb]]\r\n    urls = [\"https://<influxdb>:8085\"]\r\n    database = \"monitoring\"\r\n    precision = \"60s\"\r\n    insecure_skip_verify = true\r\n\r\n[[inputs.tail]]\r\n    files = [\"/path/to/file/telegraf-metrics.log*\"]\r\n    data_format = \"influx\"\r\n    path_tag = \"\"\r\n[inputs.tail.tags]\r\n    metric_type = \"manager\"\n```\n\n\n### Logs from Telegraf\n\n```text\n2022-06-22T19:26:00Z E! [inputs.tail] Malformed log line in \"/path/to/file/telegraf-metrics.log_2022-06-22-15-22\": [\"e.partitioncount=2048,offheap.backupcount=27,offheap.entrycount=36,offheap.primarycount=9,offheap.size=0,onheap.entrycount=0,put.rate=1111,rebalance.keyleft=0,rebalance.keyrate=0,rebalance.partitioncount=0,tx.commit.size=0,tx.commit.time=141250,tx.rollback.size=0 1655925960896000000\"]: metric parse error: expected field at 1:284: \"e.partitioncount=2048,offheap.backupcount=27,offheap.entrycount=36,offheap.primarycount=9,offheap.size=0,onheap.entrycount=0,put.rate=1111,rebalance.keyleft=0,rebalance.keyrate=0,rebalance.partitioncount=0,tx.commit.size=0,tx.commit.time=141250,tx.rollback.size=0 1655925960896000000\"\n```\n\n\n### System info\n\nTelegraf 1.18.3 (git: HEAD 6a94f65a), RHEL 7.9 (Maipo) 63.10.0-1160.31.1.el7.x86_64 and RHEL 6.10 (Santiago) 2.6.32-754.35.1.el6.x86_64\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1. Application writes out InfluxDB line protocol formatted metrics to file. \r\n2. Tail plugin reads and publishes to configured InfluxDB output.\r\n3. Malformed lines messages in logs and bad measurements in InfluxDB.\n\n### Expected behavior\n\nFull lines should be sent instead of partially reading lines and sending them. \n\n### Actual behavior\n\nLines are partially being read and sent leading to large number of bad measurements in InfluxDB.\n\n### Additional info\n\n_No response_",
      "solution": "Hi,\r\n\r\nSome questions for you:\r\n\r\n- Can you provide this log file to help reproduce the issue?\r\n- If not, can you at least provide a few of the lines before and after the failing line?\r\n- Have you verified that the log file does not have any extra new lines in this line? Influx line protocol breaks on new lines. It makes me wonder what comes right before the `e.partitioncount=2048`.\r\n\r\nThanks!\r\n\n\n---\n\nI had same issue, \r\nThis fixed it for me:\r\n`[[inputs.tail]]\r\nwatch_method = \"poll\"`\r\n",
      "labels": [
        "bug",
        "waiting for response"
      ],
      "created_at": "2022-06-22T22:11:15Z",
      "closed_at": "2025-11-24T18:09:47Z",
      "url": "https://github.com/influxdata/telegraf/issues/11372",
      "comments_count": 21
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 11457,
      "title": "[[inputs.sql]] Looping telegraf if one instance is down",
      "problem": "### Relevant telegraf.conf\r\n\r\n```toml\r\n[[inputs.sql]]\r\n  driver = \"mysql\"\r\n  dsn = \"xxxx:xxxx@tcp(10.0.0.100:3306)/master\"\r\n[[inputs.sql.query]]\r\n  query=\"SELECT SELECT usrGrp AS usr_grp,FROM current_users WHERE endTime = 0\"\r\n[[inputs.sql]]\r\n  driver = \"mysql\"\r\n  dsn = \"xxxx:xxxx@tcp(10.0.0.101:3306)/master\"\r\n[[inputs.sql.query]]\r\n  query=\"SELECT SELECT usrGrp AS usr_grp,FROM current_users WHERE endTime = 0\"\r\n```\r\n\r\n\r\n### Logs from Telegraf\r\n\r\n```text\r\n! [inputs.sql] Testing connectivity...\r\n! [inputs.sql] Preparing statement \"SELECT usrGrp AS usr_grp,FROM current_users WHERE endTime = 0\"...\r\n! [inputs.sql] Connecting to \"test:test@tcp(10.0.0.100:3306)/master\"...\r\n! [inputs.sql] Testing connectivity...\r\n! [telegraf] Error running agent: starting input inputs.sql: connecting to database failed: dial tcp 10.0.101:3306: i/o timeout\r\n! Starting Telegraf 1.23\r\n```\r\n\r\n\r\n### System info\r\n\r\nTelegraf 1.23.0  - Debian \r\n\r\n### Docker\r\n\r\n_No response_\r\n\r\n### Steps to reproduce\r\n\r\n1. Create 2 [inputs.sql] where one instance is down\r\n\r\n\r\n\r\n### Expected behavior\r\n\r\nExpected telegraf to start \r\n\r\n### Actual behavior\r\n\r\nIt just restarts and loops when it can not connect to the instance that is down and telegraf never starts.\r\n\r\nIf instance goes down while telegraf is running it breaks collecting and it starts to loop again. \r\n\r\n### Additional info\r\n\r\n_No response_",
      "solution": "Hello, \r\n\r\nI do indeed understand the current behavior.\r\nThe issue is I do not expect all my MySQL servers to be online all the time since they are on VSAT links, and if a device is offline I don't expect the whole Telegraf to go down, I would expect it to do a retry and not restart the instance until all servers are back online. \r\nAs an example, if you monitor 100x MySQL and then 1 MySQL goes down the whole telegraf goes down, and not possible to start the Telegraf again so you lose monitoring from all of them instead of only the one that is offline, so it does not really make sense to me. \r\nThe only solution then would be to run 100x Telegraf instances? \r\n\n\n---\n\nHi,\r\n\r\n> I do not expect all my MySQL servers to be online all the time\r\n> The only solution then would be to run 100x Telegraf instances?\r\n\r\nTelegraf was not built with this in mind. We have users who use Telegraf right on the clients/devices and push the data once the connection is restored.\r\n\r\nThis could be a feature request, where a setting is added to the SQL input to not fail on start. It needs to be opt-in, so users know what they are getting into. However, as-is, this is the expected behavior.\n\n---\n\nIs this a duplicate of https://github.com/influxdata/telegraf/issues/13282 and therefore addressed by https://github.com/influxdata/telegraf/pull/13289 ?\n\nAsking because search engines brought me here, which makes it seem like this is an unresolved issue, but I believe `disconnected_servers_behavior = \"ignore\"` addresses this neatly",
      "labels": [
        "bug",
        "help wanted",
        "waiting for response",
        "area/sql",
        "size/m"
      ],
      "created_at": "2022-07-05T19:57:02Z",
      "closed_at": "2025-11-24T18:09:44Z",
      "url": "https://github.com/influxdata/telegraf/issues/11457",
      "comments_count": 8
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 18013,
      "title": "FATAL: [inputs.procstat] panicked",
      "problem": "### Relevant telegraf.conf\n\n```toml\n# Telegraf Configuration\n#\n# Telegraf is entirely plugin driven. All metrics are gathered from the\n# declared inputs, and sent to the declared outputs.\n#\n# Plugins must be declared in here to be active.\n# To deactivate a plugin, comment out the name and any variables.\n#\n# Use 'telegraf -config telegraf.conf -test' to see what metrics a config\n# file would generate.\n#\n# Environment variables can be used anywhere in this config file, simply surround\n# them with ${}. For strings the variable must be within quotes (ie, \"${STR_VAR}\"),\n# for numbers and booleans they should be plain (ie, ${INT_VAR}, ${BOOL_VAR})\n\n\n# Global tags can be specified here in key=\"value\" format.\n[global_tags]\n  # dc = \"us-east-1\" # will tag all metrics with dc=us-east-1\n  # rack = \"1a\"\n  ## Environment variables can be used as tags, and throughout the config file\n  user = \"$USER\"\n\n# Configuration for telegraf agent\n[agent]\n  ## Default data collection interval for all inputs\n  interval = \"10s\"\n  ## Rounds collection interval to 'interval'\n  ## ie, if interval=\"10s\" then always collect on :00, :10, :20, etc.\n  round_interval = true\n\n  ## Telegraf will send metrics to outputs in batches of at most\n  ## metric_batch_size metrics.\n  ## This controls the size of writes that Telegraf sends to output plugins.\n  metric_batch_size = 1000\n\n  ## Maximum number of unwritten metrics per output.  Increasing this value\n  ## allows for longer periods of output downtime without dropping metrics at the\n  ## cost of higher maximum memory usage.\n  metric_buffer_limit = 10000\n\n  ## Collection jitter is used to jitter the collection by a random amount.\n  ## Each plugin will sleep for a random time within jitter before collecting.\n  ## This can be used to avoid many plugins querying things like sysfs at the\n  ## same time, which can have a measurable effect on the system.\n  collection_jitter = \"0s\"\n\n  ## Collection offset is used to shift the collection by the given amount.\n  ## This can be be used to avoid many plugins querying constraint devices\n  ## at the same time by manually scheduling them in time.\n  # collection_offset = \"0s\"\n\n  ## Default flushing interval for all outputs. Maximum flush_interval will be\n  ## flush_interval + flush_jitter\n  flush_interval = \"10s\"\n  ## Jitter the flush interval by a random amount. This is primarily to avoid\n  ## large write spikes for users running a large number of telegraf instances.\n  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s\n  flush_jitter = \"0s\"\n\n  ## Collected metrics are rounded to the precision specified. Precision is\n  ## specified as an interval with an integer + unit (e.g. 0s, 10ms, 2us, 4s).\n  ## Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\".\n  ##\n  ## By default or when set to \"0s\", precision will be set to the same\n  ## timestamp order as the collection interval, with the maximum being 1s:\n  ##   ie, when interval = \"10s\", precision will be \"1s\"\n  ##       when interval = \"250ms\", precision will be \"1ms\"\n  ##\n  ## Precision will NOT be used for service inputs. It is up to each individual\n  ## service input to set the timestamp at the appropriate precision.\n  precision = \"0s\"\n\n  ## Log at debug level.\n  debug = true\n  ## Log only error level messages.\n  quiet = false\n\n  ## Log format controls the way messages are logged and can be one of \"text\",\n  ## \"structured\" or, on Windows, \"eventlog\".\n  logformat = \"text\"\n\n  ## Message key for structured logs, to override the default of \"msg\".\n  ## Ignored if `logformat` is not \"structured\".\n  structured_log_message_key = \"message\"\n\n  ## Name of the file to be logged to or stderr if unset or empty. This\n  ## setting is ignored for the \"eventlog\" format.\n  logfile = \"d://telegraf//telegraf.Log\"\n\n  ## The logfile will be rotated after the time interval specified.  When set\n  ## to 0 no time based rotation is performed.  Logs are rotated only when\n  ## written to, if there is no log activity rotation may be delayed.\n  logfile_rotation_interval = \"2d\"\n\n  ## The logfile will be rotated when it becomes larger than the specified\n  ## size.  When set to 0 no size based rotation is performed.\n  logfile_rotation_max_size = \"5MB\"\n\n  ## Maximum number of rotated archives to keep, any older logs are deleted.\n  ## If set to -1, no archives are removed.\n  logfile_rotation_max_archives = 5\n\n  ## Pick a timezone to use when logging or type 'local' for local time.\n  ## Example: America/Chicago\n  log_with_timezone = \"Europe/Madrid\"\n\n  ## Override default hostname, if empty use os.Hostname()\n  # hostname = \"\"\n  ## If set to true, do no set the \"host\" tag in the telegraf agent.\n  # omit_hostname = false\n\n  ## Method of translating SNMP objects. Can be \"netsnmp\" (deprecated) which\n  ## translates by calling external programs snmptranslate and snmptable,\n  ## or \"gosmi\" which translates using the built-in gosmi library.\n  # snmp_translator = \"netsnmp\"\n\n  ## Name of the file to load the state of plugins from and store the state to.\n  ## If uncommented and not empty, this file will be used to save the state of\n  ## stateful plugins on termination of Telegraf. If the file exists on start,\n  ## the state in the file will be restored for the plugins.\n  # statefile = \"\"\n\n  ## Flag to skip running processors after aggregators\n  ## By default, processors are run a second time after aggregators. Changing\n  ## this setting to true will skip the second run of processors.\n  # skip_processors_after_aggregators = false\n\n\n###############################################################################\n#                            OUTPUT PLUGINS                                   #\n###############################################################################\n\n\n# # Configuration for sending metrics to InfluxDB 2.0\n[[outputs.influxdb_v2]]\n#   ## The URLs of the InfluxDB cluster nodes.\n#   ##\n#   ## Multiple URLs can be specified for a single cluster, only ONE of the\n#   ## urls will be written to each interval.\n#   ##   ex: urls = [\"https://us-west-2-1.aws.cloud2.influxdata.com\"]\nurls = [\"https://192.168.235.150:8086/\"]\n#\n#   ## Local address to bind when connecting to the server\n#   ## If empty or not set, the local address is automatically chosen.\n#   # local_address = \"\"\n#\n#   ## Token for authentication.\ntoken = \"IZw4Qwsoo8AayGQ-cvVWWnqwxxA7V3DRUZ2NsT931QDqfvrD16PEQXC67yuAffyJMhIB5F4eKytDTB3rPSxxFg==\"\n#\n#   ## Organization is the name of the organization you wish to write to.\norganization = \"MordorWorld\"\n#\n#   ## Destination bucket to write into.\nbucket = \"telegraf\"\n#\n#   ## The value of this tag will be used to determine the bucket.  If this\n#   ## tag is not set the 'bucket' option is used as the default.\n#   # bucket_tag = \"\"\n#\n#   ## If true, the bucket tag will not be added to the metric.\n#   # exclude_bucket_tag = false\n#\n#   ## Timeout for HTTP messages.\n#   # timeout = \"5s\"\n#\n#   ## Additional HTTP headers\n#   # http_headers = {\"X-Special-Header\" = \"Special-Value\"}\n#\n#   ## HTTP Proxy override, if unset values the standard proxy environment\n#   ## variables are consulted to determine which proxy, if any, should be used.\n#   # http_proxy = \"http://corporate.proxy:3128\"\n#\n#   ## HTTP User-Agent\n#   # user_agent = \"telegraf\"\n#\n#   ## Content-Encoding for write request body, can be set to \"gzip\" to\n#   ## compress body or \"identity\" to apply no encoding.\n#   # content_encoding = \"gzip\"\n#\n#   ## Enable or disable uint support for writing uints influxdb 2.0.\n#   # influx_uint_support = false\n#\n#   ## When true, Telegraf will omit the timestamp on data to allow InfluxDB\n#   ## to set the timestamp of the data during ingestion. This is generally NOT\n#   ## what you want as it can lead to data points captured at different times\n#   ## getting omitted due to similar data.\n#   # influx_omit_timestamp = false\n#\n#   ## HTTP/2 Timeouts\n#   ## The following values control the HTTP/2 client's timeouts. These settings\n#   ## are generally not required unless a user is seeing issues with client\n#   ## disconnects. If a user does see issues, then it is suggested to set these\n#   ## values to \"15s\" for ping timeout and \"30s\" for read idle timeout and\n#   ## retry.\n#   ##\n#   ## Note that the timer for read_idle_timeout begins at the end of the last\n#   ## successful write and not at the beginning of the next write.\n#   # ping_timeout = \"0s\"\n#   # read_idle_timeout = \"0s\"\n#\n#   ## Optional TLS Config for use on HTTP connections.\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain & host verification\ninsecure_skip_verify = true\n#\n#   ## Rate limits for sending data (disabled by default)\n#   ## Available, uncompressed payload size e.g. \"5MB\"\n#   # rate_limit = \"unlimited\"\n#   ## Fixed time-window for the available payload size e.g. \"5m\"\n#   # rate_limit_period = \"0s\"\n#\n#   ## Number of concurrent writes to the output\n#   ## When set to one sequential sending is used (default).\n#   ## NOTE: When using two or more concurrent writes the sending order of\n#   ##       metrics is not guaranteed!\n#   # concurrent_writes = 1\n\n###############################################################################\n#                            Services Status                                  #\n###############################################################################\n[[inputs.win_services]]\n  ## Configuraci\u00f3n para el plugin de estado de servicios de Windows\n\n  #Se puede usar * como comodin\n  service_names = [\n    \"LanmanServer\",\n    \"Spooler\",\n    \"wuauserv\",\n  ]\n#   # optional, list of service names to exclude\n#   excluded_service_names = ['WinRM']\n  ## El valor del campo es un n\u00famero: \n  ## 0 = running (ejecut\u00e1ndose)\n  ## 1 = cualquier otro estado (detenido, en pausa, etc.)\n\n###############################################################################\n#                            Process Owners                                   #\n###############################################################################\n#[[inputs.exec]]\n#  ## Nombre de la m\u00e9trica que generar\u00e1 el script\n#  name_override = \"win_proc_user\"\n#\n#  ## Intervalo de ejecuci\u00f3n (e.g., cada 60 segundos)\n#  interval = \"60s\"\n#\n#  ## El comando a ejecutar (Llamar a PowerShell y el script)\n#  commands = ['powershell.exe -ExecutionPolicy Bypass -File \"D:\\Telegraf\\get_process_owner.ps1\"']\n#\n#  ## El formato de salida del script debe ser InfluxDB Line Protocol\n#  data_format = \"influx\"\n#  \n###############################################################################\n#                            Process Metrics                                  #\n###############################################################################\n  [[inputs.procstat]]\n\n  pattern = \".*\"\n  pid_finder = \"native\"\n#   ## Add the given information tag instead of a field\n#   ## This allows to create unique metrics/series when collecting processes with\n#   ## otherwise identical tags. However, please be careful as this can easily\n#   ## result in a large number of series, especially with short-lived processes,\n#   ## creating high cardinality at the output.\n#   ## Available options are:\n#   ##   cmdline   -- full commandline\n#   ##   pid       -- ID of the process\n#   ##   ppid      -- ID of the process' parent\n#   ##   status    -- state of the process\n#   ##   user      -- username owning the process\n#   ## socket only options:\n#   ##   protocol  -- protocol type of the process socket\n#   ##   state     -- state of the process socket\n#   ##   src       -- source address of the process socket (non-unix sockets)\n#   ##   src_port  -- source port of the process socket (non-unix sockets)\n#   ##   dest      -- destination address of the process socket (non-unix sockets)\n#   ##   dest_port -- destination port of the process socket (non-unix sockets)\n#   ##   name      -- name of the process socket (unix sockets only)\n#   ## Available for procstat_lookup:\n#   ##   level     -- level of the process filtering\ntag_with = [\"pid\",\"user\",\"src\",\"src_port\",\"dest\",\"dest_port\",\"name\"]\n\n\n#   ## Properties to collect\n#   ## Available options are\n#   ##   cpu     -- CPU usage statistics\n#   ##   limits  -- set resource limits\n#   ##   memory  -- memory usage statistics\n#   ##   mmap    -- mapped memory usage statistics (caution: can cause high load)\n#   ##   sockets -- socket statistics for protocols in 'socket_protocols'\nproperties = [\"cpu\", \"limits\", \"memory\", \"mmap\",\"sockets\"]\n  \n#   ## New-style filtering configuration (multiple filter sections are allowed)\n#   # [[inputs.procstat.filter]]\n#   #    ## Name of the filter added as 'filter' tag\n#   #    name = \"shell\"\n#   #\n#   #    ## Service filters, only one is allowed\n#   #    ## Systemd unit names (wildcards are supported)\n#   #    # systemd_units = []\n#   #    ## CGroup name or path (wildcards are supported)\n#   #    # cgroups = []\n#   #    ## Supervisor service names of hypervisorctl management\n#   #    # supervisor_units = []\n#   #    ## Windows service names\n#   #    # win_service = []\n#   #\n#   #    ## Process filters, multiple are allowed\n#   #    ## Regular expressions to use for matching against the full command\n#   #    # patterns = ['.*']\n#   #    ## List of users owning the process (wildcards are supported)\n#   #    # users = ['*']\n#   #    ## List of executable paths of the process (wildcards are supported)\n#   #    # executables = ['*']\n#   #    ## List of process names (wildcards are supported)\n#   #    # process_names = ['*']\n#   #    ## Recursion depth for determining children of the matched processes\n#   #    ## A negative value means all children with infinite depth\n#   #    # recursion_depth = 0\n  \n###############################################################################\n#                            Metricas Windows                                 #\n###############################################################################\n[[inputs.win_perf_counters]]\n \n# Processor usage\n   [[inputs.win_perf_counters.object]]\n     # Processor usage, alternative to native, reports on a per core.\n     ObjectName = \"Processor\"\n     Instances = [\"*\"]\n     Counters = [\n       \"% Idle Time\",\n       \"% Interrupt Time\",\n       \"% Privileged Time\",\n       \"% User Time\",\n       \"% Processor Time\",\n       \"% DPC Time\",\n     ]\n     Measurement = \"win_cpu\"\n     # Set to true to include _Total instance when querying for all (*).\n     IncludeTotal=true\n   \n   # Disk times and queues\t\n   [[inputs.win_perf_counters.object]]\n   # Disk times and queues\n   ObjectName = \"LogicalDisk\"\n   Instances = [\"*\"]\n   Counters = [\n     \"% Idle Time\",\n     \"% Disk Time\",\n     \"% Disk Read Time\",\n     \"% Disk Write Time\",\n     \"% User Time\",\n     \"% Free Space\",\n     \"Current Disk Queue Length\",\n     \"Free Megabytes\",\n     \"Disk Read Bytes/sec\",\n     \"Disk Write Bytes/sec\"\n   ]\n   Measurement = \"win_disk\"\n   # Set to true to include _Total instance when querying for all (*).\n   #IncludeTotal=false\n   \n   # Network Interface\t\t\n   [[inputs.win_perf_counters.object]]\n   ObjectName = \"Network Interface\"\n   Instances = [\"*\"]\n   Counters = [\n     \"Bytes Received/sec\",\n     \"Bytes Sent/sec\",\n     \"Packets Received/sec\",\n     \"Packets Sent/sec\",\n     \"Packets Received Discarded\",\n     \"Packets Outbound Discarded\",\n     \"Packets Received Errors\",\n     \"Packets Outbound Errors\",\n   ]\n   Measurement = \"win_net\"\n   \n   #System\t\n   [[inputs.win_perf_counters.object]]\n       ObjectName = \"System\"\n       Counters = [\n         \"Context Switches/sec\",\n         \"System Calls/sec\",\n         \"Processor Queue Length\",\n         \"Threads\",\n         \"System Up Time\",\n         \"Processes\"\n       ]\n       Instances = [\"------\"]\n       Measurement = \"win_system\"\n       # Set to true to include _Total instance when querying for all (*).\n       #IncludeTotal=false\n   \n   # Memory\n   [[inputs.win_perf_counters.object]]\n       # Example query where the Instance portion must be removed to get data back,\n       # such as from the Memory object.\n       ObjectName = \"Memory\"\n       Counters = [\n         \"Available Bytes\",\n         \"Cache Faults/sec\",\n         \"Demand Zero Faults/sec\",\n         \"Page Faults/sec\",\n         \"Pages/sec\",\n         \"Transition Faults/sec\",\n         \"Pool Nonpaged Bytes\",\n         \"Pool Paged Bytes\",\n         \"Standby Cache Reserve Bytes\",\n         \"Standby Cache Normal Priority Bytes\",\n         \"Standby Cache Core Bytes\",\n         \"% Committed Bytes In Use\",\n         \"Committed Bytes\",\n       ]\n       # Use 6 x - to remove the Instance bit from the query.\n       Instances = [\"------\"]\n       Measurement = \"win_mem\"\n       # Set to true to include _Total instance when querying for all (*).\n       #IncludeTotal=false\n   \n   # Page File\n   [[inputs.win_perf_counters.object]]\n     # Example query where the Instance portion must be removed to get data back,\n     # such as from the Paging File object.\n     ObjectName = \"Paging File\"\n     Counters = [\n       \"% Usage\",\n     ]\n     Instances = [\"_Total\"]\n     Measurement = \"win_swap\"\n   \n   # PhysicalDisk\t\n   [[inputs.win_perf_counters.object]]\n     ObjectName = \"PhysicalDisk\"\n     Instances = [\"*\"]\n     Counters = [\n       \"Disk Read Bytes/sec\",\n       \"Disk Write Bytes/sec\",\n       \"Current Disk Queue Length\",\n       \"Disk Reads/sec\",\n       \"Disk Writes/sec\",\n       \"% Disk Time\",\n       \"% Disk Read Time\",\n       \"% Disk Write Time\",\n       \"Avg. Disk Queue Length\",\n       \"Avg. Disk sec/Read\",\n       \"Avg. Disk sec/Write\",\n       \"Split IO/sec\",\n     ]\n     Measurement = \"win_diskio\"\n   \n   # Process metrics\n   [[inputs.win_perf_counters.object]]\n    # Process metrics\n    ObjectName = \"Process\"\n    Counters = [\n      \"% Processor Time\",\n      \"Handle Count\",\n      \"Private Bytes\",\n      \"Thread Count\",\n      \"Virtual Bytes\",\n      \"Working Set\"\n      ]\n    Instances = [\"*\"]\n    Measurement = \"win_proc\"\n    #IncludeTotal=false #Set to true to include _Total instance when querying for all (*).\n\n\n#[[inputs.win_eventlog]]\n#[[inputs.win_perf_counters]]\n```\n\n### Logs from Telegraf\n\n```text\n2025-11-17T18:46:26+01:00 D! [agent] Initializing plugins\n2025-11-17T18:46:26+01:00 D! [agent] Connecting outputs\n2025-11-17T18:46:26+01:00 D! [agent] Attempting connection to [outputs.influxdb_v2]\n2025-11-17T18:46:26+01:00 D! [agent] Successfully connected to outputs.influxdb_v2\n2025-11-17T18:46:26+01:00 D! [agent] Starting service inputs\n2025-11-17T18:46:30+01:00 E! FATAL: [inputs.procstat] panicked: interface conversion: interface {} is uint32, not uint16, Stack:\ngoroutine 83 [running]:\ngithub.com/influxdata/telegraf/agent.panicRecover(0xc0003d9ec0)\n        /go/src/github.com/influxdata/telegraf/agent/agent.go:1202 +0x6d\npanic({0x7ff70f1182a0?, 0xc00156f620?})\n        /usr/local/go/src/runtime/panic.go:783 +0x132\ngithub.com/influxdata/telegraf/plugins/inputs/procstat.(*proc).metrics(0xc0008a7650, {0x0?, 0xc000000000?}, 0xc001043a00, {0x2?, 0xc000c8e060?, 0x7ff717b8cc60?})\n        /go/src/github.com/influxdata/telegraf/plugins/inputs/procstat/process.go:360 +0x29f4\ngithub.com/influxdata/telegraf/plugins/inputs/procstat.(*Procstat).gatherOld(0xc001043880, {0x7ff7114101a0, 0xc000d2b960})\n        /go/src/github.com/influxdata/telegraf/plugins/inputs/procstat/procstat.go:284 +0xa4b\ngithub.com/influxdata/telegraf/plugins/inputs/procstat.(*Procstat).Gather(0x7ff71794c580?, {0x7ff7114101a0?, 0xc000d2b960?})\n        /go/src/github.com/influxdata/telegraf/plugins/inputs/procstat/procstat.go:214 +0x27\ngithub.com/influxdata/telegraf/models.(*RunningInput).Gather(0xc0003d9ec0, {0x7ff7114101a0, 0xc000d2b960})\n        /go/src/github.com/influxdata/telegraf/models/running_input.go:263 +0x244\ngithub.com/influxdata/telegraf/agent.(*Agent).gatherOnce.func1()\n        /go/src/github.com/influxdata/telegraf/agent/agent.go:590 +0x58\ncreated by github.com/influxdata/telegraf/agent.(*Agent).gatherOnce in goroutine 100\n        /go/src/github.com/influxdata/telegraf/agent/agent.go:588 +0xf7\n\ngoroutine 1 [sync.WaitGroup.Wait]:\nsync.runtime_SemacquireWaitGroup(0xc000485e50?, 0xc0?)\n        /usr/local/go/src/runtime/sema.go:114 +0x2e\nsync.(*WaitGroup).Wait(0xc00117eae0)\n        /usr/local/go/src/sync/waitgroup.go:206 +0x85\ngithub.com/influxdata/telegraf/agent.(*Agent).Run(0xc00019f940, {0x7ff7113c2930, 0xc000485db0})\n        /go/src/github.com/influxdata/telegraf/agent/agent.go:208 +0xb6a\nmain.(*Telegraf).runAgent(0xc0011082c0, {0x7ff7113c2930, 0xc000485db0}, 0xe0?)\n        /go/src/github.com/influxdata/telegraf/cmd/telegraf/telegraf.go:565 +0x19b3\nmain.(*Telegraf).reloadLoop(0xc0011082c0)\n        /go/src/github.com/influxdata/telegraf/cmd/telegraf/telegraf.go:207 +0x26b\nmain.(*Telegraf).Ru\n2025-11-17T18:46:30+01:00 E! PLEASE REPORT THIS PANIC ON GITHUB with stack trace, configuration, and OS information: https://github.com/influxdata/telegraf/issues/new/choose\n```\n\n### System info\n\nWindows 11  build 26200.7171, telegraf-1.36.3\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1. ejecute .\\telegraf.exe -config .\\telegraf.conf\n2. when running failed and stop\n3.\n...\n\n\n### Expected behavior\n\nrunning telegraf\n\n### Actual behavior\n\nstop when start\n\n### Additional info\n\n_No response_",
      "solution": "@caramontron please test the binary in PR #18036, available for 72h after CI finished the tests, and let me know if this fixes the issue!",
      "labels": [
        "bug",
        "waiting for response"
      ],
      "created_at": "2025-11-17T17:54:09Z",
      "closed_at": "2025-11-24T10:55:10Z",
      "url": "https://github.com/influxdata/telegraf/issues/18013",
      "comments_count": 1
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 18010,
      "title": "[inputs.docker_log] docker_log input breaks with Docker Engine 29.0",
      "problem": "### Relevant telegraf.conf\n\n```toml\n[[inputs.docker_log]]\n  ## Docker Endpoint\n  ##   To use TCP, set endpoint = \"tcp://[ip]:[port]\"\n  ##   To use environment variables, set endpoint = \"ENV\"\n  endpoint = \"unix:///var/run/docker.sock\"\n\n  ## Set the source tag for the metrics to the container ID hostname, eg first 12 chars\n  source_tag = false\n```\n\n### Logs from Telegraf\n\n```text\nmain_telegraf.0.sja25idk88bc@worker-2    | 2025-11-16T21:30:25Z E! [inputs.docker_log] Error in plugin: Error response from daemon: client version 1.24 is too old. Minimum supported API version is 1.44, please upgrade your client to a newer version\n```\n\n### System info\n\nDocker Engine 29.0.1\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1. Install Docker Engine 29.0.x\n2. Start a container that produces logs\n3. Run Telegraf with `docker_log`-input (default config)\n4. `docker_log`-input fails to parse Docker Engine API version and fails with error\n\n### Expected behavior\n\nTelegraf `[inputs.docker_log]` works with Docker Engine 29.0\n\n### Actual behavior\n\nTelegraf `[inputs.docker_log]` does not collect logs with Docker Engine 29.0\n\n### Additional info\n\nThis does not seem to be fixed with Docker Engine 29.0.1, even though this issue would suggest so:\n\nhttps://github.com/docker/cli/pull/6648",
      "solution": "@jaylinski please test the binary in PR #18012, available for 72h after CI finished the tests, and let me know if this fixes the issue!\n\n---\n\n@srebhan I run Telegraf as a container, but couldn't find a pre-built Docker image in the PR.\n\nDo I have to build the container myself and verify the fix in order to get this merged and released? The PR itself looks good to me and I'm pretty sure it would fix the issue.\n\nMaybe align the client with the one from `inputs.docker`?\n\nhttps://github.com/influxdata/telegraf/blob/ea2cb48a2e001aa068b1d8ad3168b35179c18389/plugins/inputs/docker/client.go#L56-L60\n\nThanks for the quick fix by the way! \ud83e\udef6 ",
      "labels": [
        "bug"
      ],
      "created_at": "2025-11-16T21:43:45Z",
      "closed_at": "2025-11-24T10:52:13Z",
      "url": "https://github.com/influxdata/telegraf/issues/18010",
      "comments_count": 7
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 16455,
      "title": "socket listener collectd stops working on v1.33.1",
      "problem": "### Relevant telegraf.conf\n\n```toml\n[[inputs.socket_listener]]\n  service_address = \"udp://127.0.0.1:25826\"\n  data_format = \"collectd\"\n  collectd_typesdb = [\"/usr/share/collectd/types.db\", \"/usr/local/share/freeradius/radsniff_types.db\"]\n  collectd_security_level = \"none\"\n  collectd_parse_multivalue = \"split\"\n```\n\n### Logs from Telegraf\n\n```text\nApologies, I failed to collect debug logs as I encountered this on a production Radius server.\n\nNothing was logged in the systemd journal and when I verified output via stdin nothing was shown.\n```\n\n### System info\n\nUbuntu 20.04\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1. Upgrade from <1.33.x to 1.33.1 and lo\n2. Spend a few hours checking everything but Telegraf\n3. Downgrade to v1.32.3 and metrics are back.\n...\n\n\n### Expected behavior\n\nUnaffected collectd processing\n\n### Actual behavior\n\nNo errors logged, and no output at all.\n\n### Additional info\n\n_No response_",
      "solution": "When I update telegraf to the latest version, the socket_listener with collectd data format stops all output. So 100% loss of metrics without any errors or faults logged by Telegraf. I was looking at the logs as I expected the issue to be with radsniff or collectd. I don't recall seeing anything out of the ordinary.\n\nLogs of Telegraf 1.33.1 starting:\n\n```\nJan 31 03:55:39 rad01 telegraf[388946]: 2025-01-31T02:55:39Z I! Starting Telegraf 1.33.1 brought to you by InfluxData the makers of InfluxDB\nJan 31 03:55:39 rad01 telegraf[388946]: 2025-01-31T02:55:39Z I! Available plugins: 236 inputs, 9 aggregators, 33 processors, 26 parsers, 63 outputs, 6 secret-stores\nJan 31 03:55:39 rad01 telegraf[388946]: 2025-01-31T02:55:39Z I! Loaded inputs: cpu disk diskio exec kernel mem net netstat processes socket_listener swap system\nJan 31 03:55:39 rad01 telegraf[388946]: 2025-01-31T02:55:39Z I! Loaded aggregators:\nJan 31 03:55:39 rad01 telegraf[388946]: 2025-01-31T02:55:39Z I! Loaded processors:\nJan 31 03:55:39 rad01 telegraf[388946]: 2025-01-31T02:55:39Z I! Loaded secretstores:\nJan 31 03:55:39 rad01 telegraf[388946]: 2025-01-31T02:55:39Z I! Loaded outputs: file influxdb_v2 (2x)\nJan 31 03:55:39 rad01 telegraf[388946]: 2025-01-31T02:55:39Z I! Tags enabled: dc=*** host=rad01.***.domain.com\nJan 31 03:55:39 rad01 telegraf[388946]: 2025-01-31T02:55:39Z I! [agent] Config: Interval:10s, Quiet:false, Hostname:\"rad01.***.domain.com\", Flush Interval:10s\nJan 31 03:55:39 rad01 telegraf[388946]: 2025-01-31T02:55:39Z W! [agent] The default value of 'skip_processors_after_aggregators' will change to 'true' with Telegraf v1.40.0! If you need the current default behavior, please explicitly set the option to 'false'!\nJan 31 03:55:39 rad01 telegraf[388946]: 2025-01-31T02:55:39Z W! DeprecationWarning: Value \"false\" for option \"ignore_protocol_stats\" of plugin \"inputs.net\" deprecated since version 1.27.3 and will be removed in 1.36.0: use the 'inputs.nstat' plugin instead for protocol stats\nJan 31 03:55:39 rad01 telegraf[388946]: 2025-01-31T02:55:39Z I! [inputs.socket_listener] Listening on udp://127.0.0.1:25826\nJan 31 03:55:39 rad01 systemd[1]: Started Telegraf.\n```\n\nAnd for 1.32.3 :\n\n```\nJan 31 04:06:51 rad01 telegraf[401736]: 2025-01-31T03:06:51Z I! Starting Telegraf 1.32.3 brought to you by InfluxData the makers of InfluxDB\nJan 31 04:06:51 rad01 telegraf[401736]: 2025-01-31T03:06:51Z I! Available plugins: 235 inputs, 9 aggregators, 32 processors, 26 parsers, 62 outputs, 6 secret-stores\nJan 31 04:06:51 rad01 telegraf[401736]: 2025-01-31T03:06:51Z I! Loaded inputs: cpu disk diskio exec kernel mem net netstat processes socket_listener swap system\nJan 31 04:06:51 rad01 telegraf[401736]: 2025-01-31T03:06:51Z I! Loaded aggregators:\nJan 31 04:06:51 rad01 telegraf[401736]: 2025-01-31T03:06:51Z I! Loaded processors:\nJan 31 04:06:51 rad01 telegraf[401736]: 2025-01-31T03:06:51Z I! Loaded secretstores:\nJan 31 04:06:51 rad01 telegraf[401736]: 2025-01-31T03:06:51Z I! Loaded outputs: influxdb_v2 (2x)\nJan 31 04:06:51 rad01 telegraf[401736]: 2025-01-31T03:06:51Z I! Tags enabled: dc=*** host=rad01.***.domain.com\nJan 31 04:06:51 rad01 telegraf[401736]: 2025-01-31T03:06:51Z I! [agent] Config: Interval:10s, Quiet:false, Hostname:\"rad01.***.domain.com\", Flush Interval:10s\nJan 31 04:06:51 rad01 systemd[1]: Started Telegraf.\nJan 31 04:06:51 rad01 telegraf[401736]: 2025-01-31T03:06:51Z W! DeprecationWarning: Value \"false\" for option \"ignore_protocol_stats\" of plugin \"inputs.net\" deprecated since version 1.27.3 and will be removed in 1.36.0: use the 'inputs.nstat' plugin instead for protocol stats\nJan 31 04:06:51 rad01 telegraf[401736]: 2025-01-31T03:06:51Z I! [inputs.socket_listener] Listening on udp://127.0.0.1:25826\n```\n\n---\n\nIn my case, the collectd issues were actually caused by #17235 and fixed by #17949.\n\n---\n\n@marianrh so this is fixed now at least in your case?\n\n@dmgeurts-mm does this still occur for you?",
      "labels": [
        "bug"
      ],
      "created_at": "2025-01-31T03:13:12Z",
      "closed_at": "2025-11-24T10:21:56Z",
      "url": "https://github.com/influxdata/telegraf/issues/16455",
      "comments_count": 13
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 10894,
      "title": "directory_monitor + unpivot stalls with no error message",
      "problem": "### Relevant telegraf.conf\n\n```toml\n# Global tags can be specified here in key=\"value\" format.\r\n[global_tags]\r\n  # dc = \"us-east-1\" # will tag all metrics with dc=us-east-1\r\n  # rack = \"1a\"\r\n  ## Environment variables can be used as tags, and throughout the config file\r\n  # user = \"$USER\"\r\n\r\n# Configuration for telegraf agent\r\n[agent]\r\n  ## Default data collection interval for all inputs\r\n  interval = \"3s\"\r\n  ## Rounds collection interval to 'interval'\r\n  ## ie, if interval=\"10s\" then always collect on :00, :10, :20, etc.\r\n  round_interval = true\r\n\r\n  ## Telegraf will send metrics to outputs in batches of at most\r\n  ## metric_batch_size metrics.\r\n  ## This controls the size of writes that Telegraf sends to output plugins.\r\n  metric_batch_size = 1000\r\n\r\n  ## Maximum number of unwritten metrics per output.  Increasing this value\r\n  ## allows for longer periods of output downtime without dropping metrics at the\r\n  ## cost of higher maximum memory usage.\r\n  metric_buffer_limit = 100000\r\n  collection_jitter = \"0s\"\r\n  flush_interval = \"10s\"\r\n  flush_jitter = \"0s\"\r\n  precision = \"\"\r\n  ## Log at debug level.\r\n  debug = true\r\n  ## Name of the file to be logged to when using the \"file\" logtarget.  If set to\r\n  ## the empty string then logs are written to stderr.\r\n  logfile = \"\"\r\n  ## Override default hostname, if empty use os.Hostname()\r\n  hostname = \"\"\r\n  ## If set to true, do no set the \"host\" tag in the telegraf agent.\r\n  omit_hostname = false\r\n\r\n#Send telegraf metrics to file(s)\r\n[[outputs.file]]\r\n  ## Files to write to, \"stdout\" is a specially handled file.\r\n  files = [\"/opt/telegraf/metrics.out\"]\r\n  data_format = \"influx\"\r\n  use_batch_format = false\r\n  rotation_interval = \"0\"\r\n  rotation_max_size = \"0MB\"\r\n  rotation_max_archives = 5\r\n\r\n[[inputs.directory_monitor]]\r\n\r\n  name_override = \"mymetric\"\r\n  directory = \"/data/in\"\r\n  #\r\n  ## The directory to move finished files to.\r\n  finished_directory = \"/data/out\"\r\n  error_directory = \"/data/err\"\r\n  files_to_monitor = [\".*\\\\.csv\"]\r\n  directory_duration_threshold = \"10s\"\r\n  max_buffered_metrics = 5000\r\n  file_queue_size = 100000\r\n  \r\n\r\n  data_format = \"csv\"\r\n  csv_delimiter = \"\\t\"\r\n  csv_header_row_count = 0\r\n  #csv_skip_rows = 1\r\n  csv_column_names = [\r\n    \"ts\",\r\n    \"date\",\r\n    \"C\",\r\n    \"D\",\r\n    \"E\",\r\n    \"F\",\r\n    \"G\",\r\n    \"H\",\r\n    \"I\",\r\n    \"J\",\r\n    \"K\",\r\n    \"L\",\r\n    \"M\",\r\n    \"N\",\r\n    \"O\",\r\n    \"P\",\r\n    \"Q\",\r\n    \"R\",\r\n    \"S\",\r\n    \"T\",\r\n    \"U\",\r\n    \"V\",\r\n    \"W\",\r\n    \"X\",\r\n    \"Y\",\r\n    \"Z\",\r\n    \"AA\",\r\n    \"AB\",\r\n    \"AC\",\r\n    \"AD\",\r\n    \"AE\",\r\n    \"AF\",\r\n    \"AG\",\r\n    \"AH\",\r\n    \"AI\",\r\n    \"AJ\",\r\n    \"AK\",\r\n    \"AL\",\r\n    \"AM\",\r\n    \"AN\",\r\n    \"AO\",\r\n    \"AP\",\r\n    \"AQ\",\r\n    \"AR\",\r\n    \"AS\",\r\n    \"AT\",\r\n    \"AU\"\r\n  ]\r\n  csv_timestamp_column = \"date\"\r\n  csv_timestamp_format = \"2006-01-02 15:04:05\"\r\n  file_tag = \"machine\"\r\n\r\n[[processors.unpivot]]\r\n  order = 1\r\n  namepass = [\"mymetric\"]\r\n  tag_key = \"name\"\r\n  value_key = \"value\"\r\n  #tagdrop = [\"machine\"]\n```\n\n\n### Logs from Telegraf\n\n> docker-compose up\r\nStarting telegraf ... done\r\nAttaching to telegraf\r\ntelegraf    | 2022-03-25T13:59:22Z I! Using config file: /etc/telegraf/telegraf.conf\r\ntelegraf    | 2022-03-25T13:59:22Z I! Starting Telegraf 1.22.0\r\ntelegraf    | 2022-03-25T13:59:22Z I! Loaded inputs: directory_monitor\r\ntelegraf    | 2022-03-25T13:59:22Z I! Loaded aggregators:\r\ntelegraf    | 2022-03-25T13:59:22Z I! Loaded processors: unpivot\r\ntelegraf    | 2022-03-25T13:59:22Z I! Loaded outputs: file\r\ntelegraf    | 2022-03-25T13:59:22Z I! Tags enabled: host=61a134236d67\r\ntelegraf    | 2022-03-25T13:59:22Z I! [agent] Config: Interval:3s, Quiet:false, Hostname:\"61a134236d67\", Flush Interval:10s\r\ntelegraf    | 2022-03-25T13:59:22Z D! [agent] Initializing plugins\r\ntelegraf    | 2022-03-25T13:59:22Z D! [agent] Connecting outputs\r\ntelegraf    | 2022-03-25T13:59:22Z D! [agent] Attempting connection to [outputs.file]\r\ntelegraf    | 2022-03-25T13:59:22Z D! [agent] Successfully connected to outputs.file\r\ntelegraf    | 2022-03-25T13:59:22Z D! [agent] Starting service inputs\r\ntelegraf    | 2022-03-25T13:59:24Z D! [outputs.file] Wrote batch of 1000 metrics in 130.74331ms\r\ntelegraf    | 2022-03-25T13:59:24Z D! [outputs.file] Buffer fullness: 85158 / 100000 metrics\r\ntelegraf    | 2022-03-25T13:59:24Z D! [outputs.file] Wrote batch of 1000 metrics in 135.992551ms\r\ntelegraf    | 2022-03-25T13:59:24Z D! [outputs.file] Buffer fullness: 100000 / 100000 metrics\r\ntelegraf    | 2022-03-25T13:59:24Z W! [outputs.file] Metric buffer overflow; 73271 metrics have been dropped\r\ntelegraf    | 2022-03-25T13:59:24Z D! [outputs.file] Wrote batch of 1000 metrics in 108.057716ms\r\ntelegraf    | 2022-03-25T13:59:24Z D! [outputs.file] Buffer fullness: 100000 / 100000 metrics\r\ntelegraf    | 2022-03-25T13:59:24Z W! [outputs.file] Metric buffer overflow; 53729 metrics have been dropped\r\ntelegraf    | 2022-03-25T13:59:24Z D! [outputs.file] Wrote batch of 1000 metrics in 94.145368ms\r\ntelegraf    | 2022-03-25T13:59:24Z D! [outputs.file] Buffer fullness: 99000 / 100000 metrics\r\ntelegraf    | 2022-03-25T13:59:32Z D! [outputs.file] Wrote batch of 1000 metrics in 100.444691ms\r\ntelegraf    | 2022-03-25T13:59:32Z D! [outputs.file] Wrote batch of 1000 metrics in 92.004273ms\r\ntelegraf    | 2022-03-25T13:59:32Z D! [outputs.file] Wrote batch of 1000 metrics in 90.857812ms\r\ntelegraf    | 2022-03-25T13:59:32Z D! [outputs.file] Wrote batch of 1000 metrics in 90.163131ms\r\ntelegraf    | 2022-03-25T13:59:32Z D! [outputs.file] Wrote batch of 1000 metrics in 88.621189ms\r\ntelegraf    | 2022-03-25T13:59:32Z D! [outputs.file] Wrote batch of 1000 metrics in 90.008234ms\r\ntelegraf    | 2022-03-25T13:59:32Z D! [outputs.file] Wrote batch of 1000 metrics in 89.934326ms\r\ntelegraf    | 2022-03-25T13:59:33Z D! [outputs.file] Wrote batch of 1000 metrics in 106.54132ms\r\ntelegraf    | 2022-03-25T13:59:33Z D! [outputs.file] Wrote batch of 1000 metrics in 99.471323ms\r\ntelegraf    | 2022-03-25T13:59:33Z D! [outputs.file] Wrote batch of 1000 metrics in 96.228434ms\r\ntelegraf    | 2022-03-25T13:59:33Z D! [outputs.file] Wrote batch of 1000 metrics in 89.097881ms\r\ntelegraf    | 2022-03-25T13:59:33Z D! [outputs.file] Wrote batch of 1000 metrics in 92.837014ms\r\ntelegraf    | 2022-03-25T13:59:33Z D! [outputs.file] Wrote batch of 1000 metrics in 92.332164ms\r\ntelegraf    | 2022-03-25T13:59:33Z D! [outputs.file] Wrote batch of 1000 metrics in 90.554029ms\r\ntelegraf    | 2022-03-25T13:59:33Z D! [outputs.file] Wrote batch of 1000 metrics in 90.685714ms\r\ntelegraf    | 2022-03-25T13:59:33Z D! [outputs.file] Wrote batch of 1000 metrics in 90.896274ms\r\ntelegraf    | 2022-03-25T13:59:33Z D! [outputs.file] Wrote batch of 1000 metrics in 90.339688ms\r\ntelegraf    | 2022-03-25T13:59:34Z D! [outputs.file] Wrote batch of 1000 metrics in 89.751924ms\r\ntelegraf    | 2022-03-25T13:59:34Z D! [outputs.file] Wrote batch of 1000 metrics in 95.288561ms\r\ntelegraf    | 2022-03-25T13:59:34Z D! [outputs.file] Wrote batch of 1000 metrics in 91.075279ms\r\ntelegraf    | 2022-03-25T13:59:34Z D! [outputs.file] Wrote batch of 1000 metrics in 92.867588ms\r\ntelegraf    | 2022-03-25T13:59:34Z D! [outputs.file] Wrote batch of 1000 metrics in 94.296442ms\r\ntelegraf    | 2022-03-25T13:59:34Z D! [outputs.file] Wrote batch of 1000 metrics in 95.643658ms\r\ntelegraf    | 2022-03-25T13:59:34Z D! [outputs.file] Wrote batch of 1000 metrics in 90.940446ms\r\ntelegraf    | 2022-03-25T13:59:34Z D! [outputs.file] Wrote batch of 1000 metrics in 90.953624ms\r\ntelegraf    | 2022-03-25T13:59:34Z D! [outputs.file] Wrote batch of 1000 metrics in 90.253176ms\r\ntelegraf    | 2022-03-25T13:59:34Z D! [outputs.file] Wrote batch of 1000 metrics in 90.764847ms\r\ntelegraf    | 2022-03-25T13:59:34Z D! [outputs.file] Wrote batch of 1000 metrics in 90.825323ms\r\ntelegraf    | 2022-03-25T13:59:35Z D! [outputs.file] Wrote batch of 1000 metrics in 98.191707ms\r\ntelegraf    | 2022-03-25T13:59:35Z D! [outputs.file] Wrote batch of 1000 metrics in 98.44443ms\r\ntelegraf    | 2022-03-25T13:59:35Z D! [outputs.file] Wrote batch of 1000 metrics in 91.792156ms\r\ntelegraf    | 2022-03-25T13:59:35Z D! [outputs.file] Wrote batch of 1000 metrics in 92.139663ms\r\ntelegraf    | 2022-03-25T13:59:35Z D! [outputs.file] Wrote batch of 1000 metrics in 97.008161ms\r\ntelegraf    | 2022-03-25T13:59:35Z D! [outputs.file] Wrote batch of 1000 metrics in 91.652571ms\r\ntelegraf    | 2022-03-25T13:59:35Z D! [outputs.file] Wrote batch of 1000 metrics in 93.820425ms\r\ntelegraf    | 2022-03-25T13:59:35Z D! [outputs.file] Wrote batch of 1000 metrics in 89.481059ms\r\ntelegraf    | 2022-03-25T13:59:35Z D! [outputs.file] Wrote batch of 1000 metrics in 92.411415ms\r\ntelegraf    | 2022-03-25T13:59:35Z D! [outputs.file] Wrote batch of 1000 metrics in 90.682637ms\r\ntelegraf    | 2022-03-25T13:59:35Z D! [outputs.file] Wrote batch of 1000 metrics in 91.006148ms\r\ntelegraf    | 2022-03-25T13:59:36Z D! [outputs.file] Wrote batch of 1000 metrics in 98.686651ms\r\ntelegraf    | 2022-03-25T13:59:36Z D! [outputs.file] Wrote batch of 1000 metrics in 104.666173ms\r\ntelegraf    | 2022-03-25T13:59:36Z D! [outputs.file] Wrote batch of 1000 metrics in 90.323336ms\r\ntelegraf    | 2022-03-25T13:59:36Z D! [outputs.file] Wrote batch of 1000 metrics in 95.431408ms\r\ntelegraf    | 2022-03-25T13:59:36Z D! [outputs.file] Wrote batch of 1000 metrics in 92.028407ms\r\ntelegraf    | 2022-03-25T13:59:36Z D! [outputs.file] Wrote batch of 1000 metrics in 92.479711ms\r\ntelegraf    | 2022-03-25T13:59:36Z D! [outputs.file] Wrote batch of 1000 metrics in 93.743418ms\r\ntelegraf    | 2022-03-25T13:59:36Z D! [outputs.file] Wrote batch of 1000 metrics in 90.14061ms\r\ntelegraf    | 2022-03-25T13:59:36Z D! [outputs.file] Wrote batch of 1000 metrics in 90.475251ms\r\ntelegraf    | 2022-03-25T13:59:36Z D! [outputs.file] Wrote batch of 1000 metrics in 89.103945ms\r\ntelegraf    | 2022-03-25T13:59:36Z D! [outputs.file] Wrote batch of 1000 metrics in 94.284879ms\r\ntelegraf    | 2022-03-25T13:59:37Z D! [outputs.file] Wrote batch of 1000 metrics in 99.504767ms\r\ntelegraf    | 2022-03-25T13:59:37Z D! [outputs.file] Wrote batch of 1000 metrics in 92.68628ms\r\ntelegraf    | 2022-03-25T13:59:37Z D! [outputs.file] Wrote batch of 1000 metrics in 97.967189ms\r\ntelegraf    | 2022-03-25T13:59:37Z D! [outputs.file] Wrote batch of 1000 metrics in 90.719876ms\r\ntelegraf    | 2022-03-25T13:59:37Z D! [outputs.file] Wrote batch of 1000 metrics in 91.548769ms\r\ntelegraf    | 2022-03-25T13:59:37Z D! [outputs.file] Wrote batch of 1000 metrics in 93.821748ms\r\ntelegraf    | 2022-03-25T13:59:37Z D! [outputs.file] Wrote batch of 1000 metrics in 92.599069ms\r\ntelegraf    | 2022-03-25T13:59:37Z D! [outputs.file] Wrote batch of 1000 metrics in 95.495148ms\r\ntelegraf    | 2022-03-25T13:59:37Z D! [outputs.file] Wrote batch of 1000 metrics in 93.170502ms\r\ntelegraf    | 2022-03-25T13:59:37Z D! [outputs.file] Wrote batch of 1000 metrics in 90.158257ms\r\ntelegraf    | 2022-03-25T13:59:38Z D! [outputs.file] Wrote batch of 1000 metrics in 91.565076ms\r\ntelegraf    | 2022-03-25T13:59:38Z D! [outputs.file] Wrote batch of 1000 metrics in 89.902957ms\r\ntelegraf    | 2022-03-25T13:59:38Z D! [outputs.file] Wrote batch of 1000 metrics in 93.075782ms\r\ntelegraf    | 2022-03-25T13:59:38Z D! [outputs.file] Wrote batch of 1000 metrics in 90.948143ms\r\ntelegraf    | 2022-03-25T13:59:38Z D! [outputs.file] Wrote batch of 1000 metrics in 93.961806ms\r\ntelegraf    | 2022-03-25T13:59:38Z D! [outputs.file] Wrote batch of 1000 metrics in 90.226733ms\r\ntelegraf    | 2022-03-25T13:59:38Z D! [outputs.file] Wrote batch of 1000 metrics in 94.135023ms\r\ntelegraf    | 2022-03-25T13:59:38Z D! [outputs.file] Wrote batch of 1000 metrics in 90.930213ms\r\ntelegraf    | 2022-03-25T13:59:38Z D! [outputs.file] Wrote batch of 1000 metrics in 91.236827ms\r\ntelegraf    | 2022-03-25T13:59:38Z D! [outputs.file] Wrote batch of 1000 metrics in 91.843087ms\r\ntelegraf    | 2022-03-25T13:59:38Z D! [outputs.file] Wrote batch of 1000 metrics in 88.750102ms\r\ntelegraf    | 2022-03-25T13:59:39Z D! [outputs.file] Wrote batch of 1000 metrics in 98.032994ms\r\ntelegraf    | 2022-03-25T13:59:39Z D! [outputs.file] Wrote batch of 1000 metrics in 105.283709ms\r\ntelegraf    | 2022-03-25T13:59:39Z D! [outputs.file] Wrote batch of 1000 metrics in 93.072829ms\r\ntelegraf    | 2022-03-25T13:59:39Z D! [outputs.file] Wrote batch of 1000 metrics in 93.998129ms\r\ntelegraf    | 2022-03-25T13:59:39Z D! [outputs.file] Wrote batch of 1000 metrics in 95.085389ms\r\ntelegraf    | 2022-03-25T13:59:39Z D! [outputs.file] Wrote batch of 1000 metrics in 93.96725ms\r\ntelegraf    | 2022-03-25T13:59:39Z D! [outputs.file] Wrote batch of 1000 metrics in 95.94414ms\r\ntelegraf    | 2022-03-25T13:59:39Z D! [outputs.file] Wrote batch of 1000 metrics in 91.170316ms\r\ntelegraf    | 2022-03-25T13:59:39Z D! [outputs.file] Wrote batch of 1000 metrics in 95.634189ms\r\ntelegraf    | 2022-03-25T13:59:39Z D! [outputs.file] Wrote batch of 1000 metrics in 93.092077ms\r\ntelegraf    | 2022-03-25T13:59:39Z D! [outputs.file] Wrote batch of 1000 metrics in 92.511018ms\r\ntelegraf    | 2022-03-25T13:59:40Z D! [outputs.file] Wrote batch of 1000 metrics in 93.181399ms\r\ntelegraf    | 2022-03-25T13:59:40Z D! [outputs.file] Wrote batch of 1000 metrics in 90.559947ms\r\ntelegraf    | 2022-03-25T13:59:40Z D! [outputs.file] Wrote batch of 1000 metrics in 93.576686ms\r\ntelegraf    | 2022-03-25T13:59:40Z D! [outputs.file] Wrote batch of 1000 metrics in 92.486305ms\r\ntelegraf    | 2022-03-25T13:59:40Z D! [outputs.file] Wrote batch of 1000 metrics in 92.303799ms\r\ntelegraf    | 2022-03-25T13:59:40Z D! [outputs.file] Wrote batch of 1000 metrics in 92.645501ms\r\ntelegraf    | 2022-03-25T13:59:40Z D! [outputs.file] Wrote batch of 1000 metrics in 93.655857ms\r\ntelegraf    | 2022-03-25T13:59:40Z D! [outputs.file] Wrote batch of 1000 metrics in 98.235836ms\r\ntelegraf    | 2022-03-25T13:59:40Z D! [outputs.file] Wrote batch of 1000 metrics in 91.470974ms\r\ntelegraf    | 2022-03-25T13:59:40Z D! [outputs.file] Wrote batch of 1000 metrics in 92.420418ms\r\ntelegraf    | 2022-03-25T13:59:41Z D! [outputs.file] Wrote batch of 1000 metrics in 90.485421ms\r\ntelegraf    | 2022-03-25T13:59:41Z D! [outputs.file] Wrote batch of 1000 metrics in 92.662521ms\r\ntelegraf    | 2022-03-25T13:59:41Z D! [outputs.file] Wrote batch of 1000 metrics in 92.379334ms\r\ntelegraf    | 2022-03-25T13:59:41Z D! [outputs.file] Wrote batch of 1000 metrics in 91.873798ms\r\ntelegraf    | 2022-03-25T13:59:41Z D! [outputs.file] Wrote batch of 1000 metrics in 93.482953ms\r\ntelegraf    | 2022-03-25T13:59:41Z D! [outputs.file] Wrote batch of 1000 metrics in 92.49551ms\r\ntelegraf    | 2022-03-25T13:59:41Z D! [outputs.file] Wrote batch of 1000 metrics in 91.801329ms\r\ntelegraf    | 2022-03-25T13:59:41Z D! [outputs.file] Buffer fullness: 0 / 100000 metrics\r\ntelegraf    | 2022-03-25T13:59:42Z D! [outputs.file] Buffer fullness: 0 / 100000 metrics\r\ntelegraf    | 2022-03-25T13:59:52Z D! [outputs.file] Buffer fullness: 0 / 100000 metrics\r\ntelegraf    | 2022-03-25T14:00:02Z D! [outputs.file] Buffer fullness: 0 / 100000 metrics\r\ntelegraf    | 2022-03-25T14:00:12Z D! [outputs.file] Buffer fullness: 0 / 100000 metrics\r\nGracefully stopping... (press Ctrl+C again to force)\r\nStopping telegraf ... done\n\n### System info\n\nTelegraf 1.21.4 or 1.22, Docker on Debian 10, native Docker on Debian 10 on WSL, Docker Desktop on Win 10 and others\n\n### Docker\n\nversion: '3.4'\r\n\r\nservices:\r\n      telegraf:\r\n            container_name: telegraf\r\n            image: telegraf:1.22\r\n            volumes:\r\n                  - \"./telegraf.conf:/etc/telegraf/telegraf.conf\"\r\n                  - \"./out:/opt/telegraf\"\r\n                  - \"./data:/data\"\r\n\n\n### Steps to reproduce\n\n(More details in https://github.com/meowcat/20220325-telegraf-unpivot-reprex)\r\n* Unzip \"in.zip\" to `data/in/\", it should then contain 334 CSV files\r\n* Run `docker-compose -f up`\r\n  * This starts Telegraf with `telegraf.conf` as config, where `directory_monitoring` input is rotated to long form with the `unpivot` processor and then written to `file` output\r\n* This processes ~20 files (also dropping many metrics, but this is less relevant) and then completely stops. \r\n* Killing and restarting the docker will process another ~10-20 files and stop again; this can be repeated to process more files. So any specific input file is not the problem.\r\n* Note that \r\n  * other inputs would continue to work in parallel with no problems\r\n  * this happens with `file` and `influxdb` output plugins, as well as both in parallel.\n\n### Expected behavior\n\nTelegraf should process all input files in `data/in` and move them to `data/out`. If anything goes wrong (e.g. dropped metrics) it should provide some indication of the error and gracefully continue.\n\n### Actual behavior\n\nTelegraf processes ~10 files from `data/in` to `data/out` and then stops processing files with no error message or other indication of a problem. Putting new files into the input dir will also not trigger processing.\r\n\r\nAfter restarting the Docker, Telegraf will process another ~10 files, and so on. \r\n\n\n### Additional info\n\nhttps://github.com/meowcat/20220325-telegraf-unpivot-reprex \r\nData: https://drive.switch.ch/index.php/s/OqWzQ4rVEEtEc0u\r\n\r\n\r\nThe error only happens when using the `unpivot` processor after `directory_monitor`, not with `directory_monitor` alone (see github for the alternative config.)\r\n\r\nI have not checked `tail` or `file`; I would suspect the large number of metrics generated by unpivoting causes an overflow somewhere.\r\n\r\nI have tried different combinations of buffer sizes, file queue length, batch sizes, intervals etc to no avail.\r\n\r\nThe provided log is from WSL; less metrics are dropped on native debian.",
      "solution": "Hi,\r\n\r\nmy issue is *not* that metrics are dropped; this is expected behaviour if I have bad settings. It is evident from the output, and shows me I have to do some tuning. (In the meantime I found a solution that works with `file_queue_size = 1`.)\r\n\r\nMy issue is that `directory_monitor` stops processing the files (indefinitely) when some internal limit is hit, and no new files ever get processed. This happens without any error message. When the bug/limitation triggers, all unprocessed files stay in the `directory` indefinitely and never move to the `finished_directory` or `error_directory`. I expect the worst case to be that all files end up in the `error_directory`, or that all files end up in the `finished_directory` but no metrics get written. Both would be fine for me, I just don't think the system stalling without any notice is expected behavior. My only workaround for this would be to restart telegraf every x hours. \r\n\r\n> Is there documentation that would be helpful to update this for others in the future?\r\n\r\nI imagine a \"Tuning Telegraf\" section in the documentation (e.g. below \"configure plugins\") could be useful. (I actually had never clicked on the \"videos\" section because it's not a natural way for me to consume content, so I never knew there is a \"Telegraf Agent Configuration Best Practices\" but this level is not covered there.)\r\n\r\n\r\n\r\n\n\n---\n\n> Both would be fine for me, I just don't think the system stalling without any notice is expected behavior. My only workaround for this would be to restart telegraf every x hours.\r\n\r\nI agree that having Telegraf apparently stop parsing the files is undesirable. However, the large number of warnings about dropped metrics is a clear indication that something is not working as it should and is a call to action to the user that something needs to be tuned. \r\n\r\nFor example, I also think your original setting of a smaller `max_buffered_metrics`, which the docs say to match `metric_buffer_limit` in size, may have harmed your attempts as well.\r\n\r\n> I imagine a \"Tuning Telegraf\" section in the documentation \r\n\r\nok I am going to leave this issue open and use some of the important fields in your situation as what config options should be talked about in a tuning section.\r\n\r\nnext steps: add a tuning telegraf section that explains what to do if metrics are getting dropped, things to consider doing, and things to avoid doing.\r\n\n\n---\n\n@meowcat does the issue with directory_monitor stopping to process files after a certain time go away if you increase the metric buffers? Or is the issue still there in that case? We need to know to see if something still needs to be fixed..\r\n\r\n@powersj I was looking into this `max_buffered_metrics` option as well, and was wondering if it is even relevant. In other words, can't we just always make it the same as `metric_buffer_limit`?",
      "labels": [
        "bug",
        "area/tail",
        "waiting for response"
      ],
      "created_at": "2022-03-25T15:03:54Z",
      "closed_at": "2025-11-20T18:09:47Z",
      "url": "https://github.com/influxdata/telegraf/issues/10894",
      "comments_count": 14
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 16631,
      "title": "Lustre2 input causes panic at startup with Lustre 2.16",
      "problem": "### Relevant telegraf.conf\n\n```toml\n`telegraf.conf`\n\n\n[global_tags]\n[agent]\n  interval = \"30s\"\n  round_interval = true\n  metric_batch_size = 2000\n  metric_buffer_limit = 50000\n  collection_jitter = \"0s\"\n  flush_interval = \"10s\"\n  flush_jitter = \"10s\"\n  precision = \"0s\"\n  logfile_rotation_max_size = \"5MB\"\n  logfile_rotation_max_archives = 3\n[[outputs.http]]\n  url = \"http://[REDACTED].net/telegraf\"\n[[inputs.cpu]]\n  percpu = true\n  totalcpu = true\n  collect_cpu_time = false\n  report_active = false\n  core_tags = true\n[[inputs.disk]]\n  ignore_fs = [\"tmpfs\", \"devtmpfs\", \"devfs\", \"iso9660\", \"overlay\", \"aufs\", \"squashfs\"]\n[[inputs.diskio]]\n[[inputs.kernel]]\n[[inputs.mem]]\n[[inputs.processes]]\n[[inputs.swap]]\n[[inputs.system]]\n[[inputs.cgroup]]\n[[inputs.infiniband]]\n[[inputs.interrupts]]\n  cpu_as_tag = true\n[[inputs.kernel_vmstat]]\n[[inputs.linux_cpu]]\n  metrics = [\"cpufreq\"]\n[[inputs.lustre2]]\n[[inputs.netstat]]\n[[inputs.net]]\n  ignore_protocol_stats = true\n[[inputs.nfsclient]]\n[[inputs.sensors]]\n[[inputs.zfs]]\n\n\n`telegraf.d/tags.conf`\n\n\n[global_tags]\n  x-uuid = \"388cb326-4a30-4a8c-938a-f8529e367ad4\"\n  x-name = \"redacted-name\"\n  x-id = \"0\"\n```\n\n### Logs from Telegraf\n\n```text\nroot@redacted-host:/etc/telegraf# telegraf --debug\n2025-03-14T08:44:49Z I! Loading config: /etc/telegraf/telegraf.conf\n2025-03-14T08:44:49Z I! Loading config: /etc/telegraf/telegraf.d/tags.conf\n2025-03-14T08:44:49Z I! Starting Telegraf 1.34.0 brought to you by InfluxData the makers of InfluxDB\n2025-03-14T08:44:49Z I! Available plugins: 239 inputs, 9 aggregators, 33 processors, 26 parsers, 63 outputs, 6 secret-stores\n2025-03-14T08:44:49Z I! Loaded inputs: cgroup cpu disk diskio infiniband interrupts kernel kernel_vmstat linux_cpu lustre2 mem net netstat nfsclient processes sensors swap system zfs\n2025-03-14T08:44:49Z I! Loaded aggregators:\n2025-03-14T08:44:49Z I! Loaded processors:\n2025-03-14T08:44:49Z I! Loaded secretstores:\n2025-03-14T08:44:49Z I! Loaded outputs: http\n2025-03-14T08:44:49Z I! Tags enabled: host=redacted-host x-id=0 x-name=redacted-name  x-uuid=388cb326-4a30-4a8c-938a-f8529e367ad4\n2025-03-14T08:44:49Z I! [agent] Config: Interval:30s, Quiet:false, Hostname:\"redacted-host\", Flush Interval:10s\n2025-03-14T08:44:49Z W! [agent] The default value of 'skip_processors_after_aggregators' will change to 'true' with Telegraf v1.40.0! If you need the current default behavior, please explicitly set the option to 'false'!\n2025-03-14T08:44:49Z D! [agent] Initializing plugins\n2025-03-14T08:44:49Z D! [inputs.nfsclient] using [/proc/self/mountstats] for mountstats\n2025-03-14T08:44:49Z D! [inputs.nfsclient] Including all mounts.\n2025-03-14T08:44:49Z D! [inputs.nfsclient] Not excluding any mounts.\n2025-03-14T08:44:49Z D! [inputs.nfsclient] Including all operations.\n2025-03-14T08:44:49Z D! [inputs.nfsclient] Not excluding any operations.\n2025-03-14T08:44:49Z D! [agent] Connecting outputs\n2025-03-14T08:44:49Z D! [agent] Attempting connection to [outputs.http]\n2025-03-14T08:44:49Z D! [agent] Successfully connected to outputs.http\n2025-03-14T08:44:49Z D! [agent] Starting service inputs\n2025-03-14T08:45:00Z E! [inputs.linux_cpu] Error in plugin: error on reading file: read /sys/devices/system/cpu/cpu100/cpufreq/scaling_cur_freq: device or resource busy\n2025-03-14T08:45:00Z E! [inputs.linux_cpu] Error in plugin: error on reading file: read /sys/devices/system/cpu/cpu101/cpufreq/scaling_cur_freq: device or resource busy\n2025-03-14T08:45:00Z E! [inputs.linux_cpu] Error in plugin: error on reading file: read /sys/devices/system/cpu/cpu102/cpufreq/scaling_cur_freq: device or resource busy\n2025-03-14T08:45:00Z E! [inputs.linux_cpu] Error in plugin: error on reading file: read /sys/devices/system/cpu/cpu103/cpufreq/scaling_max_freq: device or resource busy\n2025-03-14T08:45:00Z E! [inputs.linux_cpu] Error in plugin: error on reading file: read /sys/devices/system/cpu/cpu104/cpufreq/scaling_cur_freq: device or resource busy\n2025-03-14T08:45:00Z E! FATAL: [inputs.lustre2] panicked: runtime error: index out of range [0] with length 0, Stack:\ngoroutine 258 [running]:\ngithub.com/influxdata/telegraf/agent.panicRecover(0xc00206c480)\n\t/go/src/github.com/influxdata/telegraf/agent/agent.go:1202 +0x6d\npanic({0x9d00360?, 0xc003794ee8?})\n\t/usr/local/go/src/runtime/panic.go:792 +0x132\ngithub.com/influxdata/telegraf/plugins/inputs/lustre2.(*Lustre2).getLustreProcStats(0xc0004c9490, {0xa53dbdb?, 0x23?}, {0x11aecf60, 0x1a, 0x2?})\n\t/go/src/github.com/influxdata/telegraf/plugins/inputs/lustre2/lustre2.go:252 +0x8a9\ngithub.com/influxdata/telegraf/plugins/inputs/lustre2.(*Lustre2).Gather(0xc0004c9490, {0xb471160, 0xc002599260})\n\t/go/src/github.com/influxdata/telegraf/plugins/inputs/lustre2/lustre2.go:108 +0x4cf\ngithub.com/influxdata/telegraf/models.(*RunningInput).Gather(0xc00206c480, {0xb471160, 0xc002599260})\n\t/go/src/github.com/influxdata/telegraf/models/running_input.go:260 +0x251\ngithub.com/influxdata/telegraf/agent.(*Agent).gatherOnce.func1()\n\t/go/src/github.com/influxdata/telegraf/agent/agent.go:590 +0x58\ncreated by github.com/influxdata/telegraf/agent.(*Agent).gatherOnce in goroutine 183\n\t/go/src/github.com/influxdata/telegraf/agent/agent.go:588 +0xf7\n\ngoroutine 1 [sync.WaitGroup.Wait]:\nsync.runtime_SemacquireWaitGroup(0xc0021a61e0?)\n\t/usr/local/go/src/runtime/sema.go:110 +0x25\nsync.(*WaitGroup).Wait(0xc001df4c30?)\n\t/usr/local/go/src/sync/waitgroup.go:118 +0x48\ngithub.com/influxdata/telegraf/agent.(*Agent).Run(0xc001df4c30, {0xb424ff0, 0xc0021a6140})\n\t/go/src/github.com/influxdata/telegraf/agent/agent.go:208 +0xb6a\nmain.(*Telegraf).runAgent(0xc001e0d080, {0xb424ff0, 0xc0021a6140}, 0xe0?)\n\t/go/src/github.com/influxdata/telegraf/cmd/telegraf/telegraf.go:486 +0x1993\nmain.(*Telegraf).reloadLoop(0xc001e0d080)\n\t/go/src/github.com/influxdata/telegraf/cmd/telegraf/telegraf.go:206 +0x26b\nmain.(*Telegraf).Run(0xc001e0d080)\n\t/go/src/github.com/influxdata/telegraf/cmd/telegraf/telegraf_posix.go:19 +0xbc\nmain.runApp.func1(0xc002020940)\n\t/go/src/github.com/influxdata/telegraf/cmd/telegraf/main.go:256 +0xd46\ngithub.com/urfave/cli/v2.(*Command).Run(0xc00205edc0, 0xc002020940,\n2025-03-14T08:45:00Z E! PLEASE REPORT THIS PANIC ON GITHUB with stack trace, configuration, and OS information: https://github.com/influxdata/telegraf/issues/new/choose\n```\n\n### System info\n\nTelegraf 1.34.0 (git: HEAD@4b3009fb) but same happens with 1.33.3, Ubuntu 24.04, Lustre 2.16.52\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1. Create a telegraf conf as above (only the Lustre part is important)\n2. mount a Lustre cluster (the exact details of which I cannot give upfront)\n3. start telegraf\n\n### Expected behavior\n\ntelegraf starts and reports data, in particular Lustre data\n\n### Actual behavior\n\nTelegraf crashes in the lustre2 module, with the above stack trace.\n\n### Additional info\n\nThe relevant info seems to be the Lustre version (2.16.52) as this used to work with our previous version (2.15.91).\n\nVisually, one difference I see is in the `job_stats` files:\n\n- in Lustre 2.15\n\n```\n# cat /proc/fs/lustre/mdt/userlstr-MDT0000/job_stats\njob_stats:\n```\n\n- while in Lustre 2.16 it's completely empty\n\n```\n-rw-r--r-- 1 root root 0 Mar 14 09:12 /proc/fs/lustre/mdt/userlstr-MDT0000/job_stats\n```\n\nMy Go is very far from good, but if I understand correctly\n- [this split](https://github.com/influxdata/telegraf/blob/release-1.34/plugins/inputs/lustre2/lustre2.go#L247) will return an array of length 1 with an empty slice in it as it seems to be [what Go does](https://github.com/golang/go/issues/54455)\n- then [this strings.Fields()](https://github.com/influxdata/telegraf/blob/release-1.34/plugins/inputs/lustre2/lustre2.go#L251C13-L251C27) will return an empty slice [as per documentation](https://www.geeksforgeeks.org/strings-fields-function-in-golang-with-examples/)\n> Returns: A slice of substrings of str or an empty slice if str contains only white space.\n- then [this will panic](https://github.com/influxdata/telegraf/blob/release-1.34/plugins/inputs/lustre2/lustre2.go#L252) when accessing `parts[0]`, which is precisely the stack trace I get",
      "solution": "@Ricordel it would be even better to get a dump of `/proc/fs/lustre` or a working and non-working version. This way I can setup some unit-tests and be sure we fix the issue...",
      "labels": [
        "bug",
        "waiting for response"
      ],
      "created_at": "2025-03-14T09:24:19Z",
      "closed_at": "2025-11-19T18:09:49Z",
      "url": "https://github.com/influxdata/telegraf/issues/16631",
      "comments_count": 8
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17570,
      "title": "kafka consumer plugin doesnt' work with kafka 4",
      "problem": "### Relevant telegraf.conf\n\n```toml\n[[inputs.kafka_consumer]]\n  # Local Kafka broker address\n  brokers = [\"localhost:9092\"]\n  # List your Kafka topic(s) here\n  topics = [\"telemetry-events\"]\n  # Kafka messages format: adjust as needed (json is most common)\n  consumer_group = \"telegraf_metrics_consumers\"\n  data_format = \"json\"\n```\n\n### Logs from Telegraf\n\n```text\nSep 05 07:02:36 asset-health-kafka telegraf[161826]: 2025-09-05T07:02:36Z E! [inputs.kafka_consumer] Error in plugin: consume: EOF\nSep 05 07:02:41 asset-health-kafka telegraf[161826]: 2025-09-05T07:02:41Z E! [inputs.kafka_consumer] Error in plugin: consume: EOF\nSep 05 07:02:46 asset-health-kafka telegraf[161826]: 2025-09-05T07:02:46Z E! [inputs.kafka_consumer] Error in plugin: consume: EOF\nSep 05 07:02:51 asset-health-kafka telegraf[161826]: 2025-09-05T07:02:51Z E! [inputs.kafka_consumer] Error in plugin: consume: EOF\nSep 05 07:02:56 asset-health-kafka telegraf[161826]: 2025-09-05T07:02:56Z E! [inputs.kafka_consumer] Error in plugin: consume: EOF\nSep 05 07:03:01 asset-health-kafka telegraf[161826]: 2025-09-05T07:03:01Z E! [inputs.kafka_consumer] Error in plugin: consume: E\n```\n\n### System info\n\nUbuntu 24.04\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1. Install telegraf 1.35.4\n2. Run kafka 4.0 using docker and publish some test messages\n3. connect telegraf with kafka\n\n\n\n### Expected behavior\n\nWith the latest version, telegraf should be able to connect to kafka 4\n\n### Actual behavior\n\nIt is not able to connect\n\n### Additional info\n\n\nI did check this and was hoping that the latest version will work.\nhttps://github.com/influxdata/telegraf/issues/16691\n\nI can also confirm that the version of sarama is latest\n`\tdep\tgithub.com/IBM/sarama\tv1.45.2\th1:8m8LcMCu3REcwpa7fCP6v2fuPuzVwXDAM2DOv3CBrKw=`\n\nKafka logs\n```\n[2025-09-05 07:06:06,266] ERROR Exception while processing request from 127.0.0.1:9092-127.0.0.1:46340-0-3840 (kafka.network.Processor)\norg.apache.kafka.common.errors.UnsupportedVersionException: Received request for api with key 11 (JoinGroup) and unsupported version 1\n```\n",
      "solution": "I can confirm that this solved the issue for me as well. ",
      "labels": [
        "bug",
        "waiting for response"
      ],
      "created_at": "2025-09-05T07:09:12Z",
      "closed_at": "2025-11-19T18:09:45Z",
      "url": "https://github.com/influxdata/telegraf/issues/17570",
      "comments_count": 8
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 13083,
      "title": "Lustre2 Jobstats read_bytes line beeing overwriten by read latency",
      "problem": "### Relevant telegraf.conf\n\n```toml\n[[inputs.lustre2]]\r\n  alias = \"lustre2-ost-jobstats\"\r\n  mds_procfiles = [\"\"]\r\n  ost_procfiles = [\"/proc/fs/lustre/obdfilter/*/job_stats\"]\r\n  # measurement name for OST jobstats\r\n  name_override = \"ost_jobstats\"\r\n\r\n[[inputs.lustre2]]\r\n  alias = \"lustre2-mdt-jobstats\"\r\n  ost_procfiles = [\"\"]\r\n  mds_procfiles = [\"/proc/fs/lustre/mdt/*/job_stats\"]\r\n  # measurement name for MDT jobstats\r\n  name_override = \"mdt_jobstats\"\r\n\r\n[[processors.strings]]\r\n  alias = \"jobstats-strings\"\r\n  # Trim prefix from the field keys (e.g. jobstats_read_bytes => read_bytes)\r\n  [[processors.strings.trim_prefix]]\r\n    measurement = \"*_jobstats\"\r\n    field_key = \"*\"\r\n    prefix = \"jobstats_\"\n```\n\n\n### Logs from Telegraf\n\n```text\nost_jobstats,fs_name=xxxx,host=xxxx,jobid=xxx,jobnode=xxx,jobuid=xxx,target=xxx write_max_size=20601i,quotactl=0i,punch=52491i,ost_sync=0i,read_min_size=1i,ost_statfs=0i,set_info=0i,write_calls=5607608i,ost_getattr=0i,write_min_size=3i,read_max_size=447440i,read_bytes=628435720i,ost_setattr=0i,write_bytes=451436126i,destroy=0i,read_calls=1525515i,get_info=0i,create=0i 1681372300000000000\n```\n\n\n### System info\n\nTelegraf 1.26, CentOS Linux release 7.9.2009, lustre-server-2.12.8\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\nEnable Jobstats\n\n### Expected behavior\n\nJobstats values from line read/write_bytes beeing written to database\n\n### Actual behavior\n\nJobstats read/write_bytes stats is beeing overwriten by read/write line from jobstats (latency) instead. \n\n### Additional info\n\nLustre read_bytes metric is definied in the lustre2 plugin as follows:\r\nhttps://github.com/influxdata/telegraf/blob/302ac88bdbedb11020ba82a8f715f3cb0bd6290a/plugins/inputs/lustre2/lustre2.go#L86-L123\r\n\r\nThe code then overwrites read_bytes by read (latency in line[5] example below) as they have the same 'reportAs' name to keep compability between lustre versions, when both lines exists. This is an issue for \"newer\" lustre builds. Tested on 2.12.8 and 2.14.0:\r\n\r\n```\r\n- job_id:          xxxx:xxx:xxxx\r\n  snapshot_time:   1681372306\r\n  read_bytes:      { samples:     1523680, unit: bytes, min:     4096, max:   524288, sum:     205693874176, sumsq:  41153530404274176 }\r\n  write_bytes:     { samples:     5602760, unit: bytes, min:       48, max:   524288, sum:    2935700676043, sumsq: 1538734750279526901 }\r\n  read:            { samples:     1525515, unit: usecs, min:        1, max:   447440, sum:        628435720, sumsq:     28075770829850 }\r\n  write:           { samples:     5607608, unit: usecs, min:        3, max:    20601, sum:        451436126, sumsq:        40139939376 }\r\n  getattr:         { samples:           0, unit: usecs, min:        0, max:        0, sum:                0, sumsq:                  0 }\r\n  setattr:         { samples:           0, unit: usecs, min:        0, max:        0, sum:                0, sumsq:                  0 }\r\n  punch:           { samples:       52491, unit: usecs, min:       11, max:   685124, sum:        367120431, sumsq:     12123013355015 }\r\n  sync:            { samples:           0, unit: usecs, min:        0, max:        0, sum:                0, sumsq:                  0 }\r\n  destroy:         { samples:           0, unit: usecs, min:        0, max:        0, sum:                0, sumsq:                  0 }\r\n  create:          { samples:           0, unit: usecs, min:        0, max:        0, sum:                0, sumsq:                  0 }\r\n  statfs:          { samples:           0, unit: usecs, min:        0, max:        0, sum:                0, sumsq:                  0 }\r\n  get_info:        { samples:           0, unit: usecs, min:        0, max:        0, sum:                0, sumsq:                  0 }\r\n  set_info:        { samples:           0, unit: usecs, min:        0, max:        0, sum:                0, sumsq:                  0 }\r\n  quotactl:        { samples:           0, unit: usecs, min:        0, max:        0, sum:                0, sumsq:                  0 }\r\n  prealloc:        { samples:           0, unit:  reqs }\r\n```\r\n\r\nOverwritten by:\r\nhttps://github.com/influxdata/telegraf/blob/302ac88bdbedb11020ba82a8f715f3cb0bd6290a/plugins/inputs/lustre2/lustre2.go#L431-L435\r\n\r\nMetric results then to 451436126 micro seconds instead of 2935700676043 bytes:\r\n```\r\nost_jobstats,fs_name=xxxx,host=xxxx,jobid=xxx,jobnode=xxx,jobuid=xxx,target=xxx write_max_size=20601i,quotactl=0i,punch=52491i,ost_sync=0i,read_min_size=1i,ost_statfs=0i,set_info=0i,write_calls=5607608i,ost_getattr=0i,write_min_size=3i,read_max_size=447440i,read_bytes=628435720i,ost_setattr=0i,write_bytes=451436126i,destroy=0i,read_calls=1525515i,get_info=0i,create=0i 1681372300000000000\r\n```\r\n",
      "solution": "Hi,\r\n\r\nThanks for the issue report.\r\n\r\n> This is an issue for \"newer\" lustre builds. Tested on 2.12.8 and 2.14.0:\r\n\r\nDo you know enough about Lustre to know what versions this was used to maintain backwards compatability? Or better yet, what versions of Lustre are a) primarily in use and b) what versions are supported?\r\n\r\nWhat I am thinking is adding a new option that ignores this table from 7 years ago and uses whatever we get as the field name.\r\n\r\nWhat do you think?\n\n---\n\nHey, \r\n\r\ni would assume that most production instances use the LTS release of 2.12 (or higher). \r\n\r\nAlso i just looked through the changelogs and it looks like the format in the lustre LTS community version 2.12 is still the same as before but will be broken with the new LTS release (likely 2.15). From the [Changelogs](https://wiki.lustre.org/Lustre_2.14.0_Changelog) i can see that this change was introduced in 2.14.0 release. \r\n_I was not aware that this change was backported from 2.14 to 2.12 for **our** build as we dont use community version._ \r\n\r\nRegardless this should be fixed for the newer versions but im not sure if dropping support for older systems would be the best solution. Im still looking for better solution to make sure all versions are support and would make a pull request as soon as i have one.\n\n---\n\n> Regardless this should be fixed for the newer versions but im not sure if dropping support for older systems would be the best solution.\r\n\r\nIt does not sound like we can rely on the version then, so I would suggest going with a configuration option that changes the behavior.\r\n\r\n> Im still looking for better solution to make sure all versions are support and would make a pull request as soon as i have one.\r\n\r\nLook forward to a PR let us know if you have any questions :)",
      "labels": [
        "bug",
        "waiting for response"
      ],
      "created_at": "2023-04-13T10:30:32Z",
      "closed_at": "2025-11-13T18:09:48Z",
      "url": "https://github.com/influxdata/telegraf/issues/13083",
      "comments_count": 5
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 16691,
      "title": "kafka output plugin breaks with kafka 4.0",
      "problem": "### Relevant telegraf.conf\n\n```toml\n# # Configuration for the Kafka server to send metrics to\n [[outputs.kafka]]\n#   ## URLs of kafka brokers\n#   ## The brokers listed here are used to connect to collect metadata about a\n#   ## cluster. However, once the initial metadata collect is completed, telegraf\n#   ## will communicate solely with the kafka leader and not all defined brokers.\n   brokers = [\"kafka:9092\"]\n#\n#   ## Kafka topic for producer messages\n   topic = \"metrics\"\n#\n#   ## The value of this tag will be used as the topic.  If not set the 'topic'\n#   ## option is used.\n#   # topic_tag = \"\"\n#\n#   ## If true, the 'topic_tag' will be removed from to the metric.\n#   # exclude_topic_tag = false\n#\n#   ## Optional Client id\n client_id = \"telegraf-influxproxy\"\n#\n#   ## Set the minimal supported Kafka version.  Setting this enables the use of new\n#   ## Kafka features and APIs.  Of particular interested, lz4 compression\n#   ## requires at least version 0.10.0.0.\n#   ##   ex: version = \"1.1.0\"\n#   # version = \"\"\n#\n#   ## The routing tag specifies a tagkey on the metric whose value is used as\n#   ## the message key.  The message key is used to determine which partition to\n#   ## send the message to.  This tag is preferred over the routing_key option.\n#   routing_tag = \"host\"\n#\n#   ## The routing key is set as the message key and used to determine which\n#   ## partition to send the message to.  This value is only used when no\n#   ## routing_tag is set or as a fallback when the tag specified in routing tag\n#   ## is not found.\n#   ##\n#   ## If set to \"random\", a random value will be generated for each message.\n#   ##\n#   ## When unset, no message key is added and each message is routed to a random\n#   ## partition.\n#   ##\n#   ##   ex: routing_key = \"random\"\n#   ##       routing_key = \"telegraf\"\n#   # routing_key = \"\"\n#\n#   ## Compression codec represents the various compression codecs recognized by\n#   ## Kafka in messages.\n#   ##  0 : None\n#   ##  1 : Gzip\n#   ##  2 : Snappy\n#   ##  3 : LZ4\n#   ##  4 : ZSTD\n#   # compression_codec = 0\n#\n#   ## Idempotent Writes\n#   ## If enabled, exactly one copy of each message is written.\n#   # idempotent_writes = false\n#\n#   ##  RequiredAcks is used in Produce Requests to tell the broker how many\n#   ##  replica acknowledgements it must see before responding\n#   ##   0 : the producer never waits for an acknowledgement from the broker.\n#   ##       This option provides the lowest latency but the weakest durability\n#   ##       guarantees (some data will be lost when a server fails).\n#   ##   1 : the producer gets an acknowledgement after the leader replica has\n#   ##       received the data. This option provides better durability as the\n#   ##       client waits until the server acknowledges the request as successful\n#   ##       (only messages that were written to the now-dead leader but not yet\n#   ##       replicated will be lost).\n#   ##   -1: the producer gets an acknowledgement after all in-sync replicas have\n#   ##       received the data. This option provides the best durability, we\n#   ##       guarantee that no messages will be lost as long as at least one in\n#   ##       sync replica remains.\n#   # required_acks = -1\n#\n#   ## The maximum number of times to retry sending a metric before failing\n#   ## until the next flush.\n#   # max_retry = 3\n#\n#   ## The maximum permitted size of a message. Should be set equal to or\n#   ## smaller than the broker's 'message.max.bytes'.\n#   # max_message_bytes = 1000000\n#\n#   ## Optional TLS Config\n#   # enable_tls = false\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain & host verification\n#   # insecure_skip_verify = false\n#\n#   ## Period between keep alive probes.\n#   ## Defaults to the OS configuration if not specified or zero.\n#   # keep_alive_period = \"15s\"\n#\n#   ## Optional SOCKS5 proxy to use when connecting to brokers\n#   # socks5_enabled = true\n#   # socks5_address = \"127.0.0.1:1080\"\n#   # socks5_username = \"alice\"\n#   # socks5_password = \"pass123\"\n#\n#   ## Optional SASL Config\n#   # sasl_username = \"kafka\"\n#   # sasl_password = \"secret\"\n#\n#   ## Optional SASL:\n#   ## one of: OAUTHBEARER, PLAIN, SCRAM-SHA-256, SCRAM-SHA-512, GSSAPI\n#   ## (defaults to PLAIN)\n#   # sasl_mechanism = \"\"\n#\n#   ## used if sasl_mechanism is GSSAPI\n#   # sasl_gssapi_service_name = \"\"\n#   # ## One of: KRB5_USER_AUTH and KRB5_KEYTAB_AUTH\n#   # sasl_gssapi_auth_type = \"KRB5_USER_AUTH\"\n#   # sasl_gssapi_kerberos_config_path = \"/\"\n#   # sasl_gssapi_realm = \"realm\"\n#   # sasl_gssapi_key_tab_path = \"\"\n#   # sasl_gssapi_disable_pafxfast = false\n#\n#   ## Access token used if sasl_mechanism is OAUTHBEARER\n#   # sasl_access_token = \"\"\n#\n#   ## Arbitrary key value string pairs to pass as a TOML table. For example:\n#   # {logicalCluster = \"cluster-042\", poolId = \"pool-027\"}\n#   # sasl_extensions = {}\n#\n#   ## SASL protocol version.  When connecting to Azure EventHub set to 0.\n#   # sasl_version = 1\n#\n#   # Disable Kafka metadata full fetch\n#   # metadata_full = false\n#\n#   ## Data format to output.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   # data_format = \"influx\"\n#\n#   ## NOTE: Due to the way TOML is parsed, tables must be at the END of the\n#   ## plugin definition, otherwise additional config options are read as part of\n#   ## the table\n#\n#   ## Optional topic suffix configuration.\n#   ## If the section is omitted, no suffix is used.\n#   ## Following topic suffix methods are supported:\n#   ##   measurement - suffix equals to separator + measurement's name\n#   ##   tags        - suffix equals to separator + specified tags' values\n#   ##                 interleaved with separator\n#\n#   ## Suffix equals to \"_\" + measurement name\n#   # [outputs.kafka.topic_suffix]\n#   #   method = \"measurement\"\n#   #   separator = \"_\"\n#\n#   ## Suffix equals to \"__\" + measurement's \"foo\" tag value.\n#   ## If there's no such a tag, suffix equals to an empty string\n#   # [outputs.kafka.topic_suffix]\n#   #   method = \"tags\"\n#   #   keys = [\"foo\"]\n#   #   separator = \"__\"\n#\n#   ## Suffix equals to \"_\" + measurement's \"foo\" and \"bar\"\n#   ## tag values, separated by \"_\". If there is no such tags,\n#   ## their values treated as empty strings.\n#   # [outputs.kafka.topic_suffix]\n#   #   method = \"tags\"\n#   #   keys = [\"foo\", \"bar\"]\n#   #   separator = \"_\"\n```\n\n### Logs from Telegraf\n\n```text\n2025-03-25T03:02:05Z I! Loading config: /etc/telegraf/telegraf.conf\n2025-03-25T03:02:05Z I! Loading config: /etc/telegraf/telegraf.d/in_influxv2.conf\n2025-03-25T03:02:05Z I! Loading config: /etc/telegraf/telegraf.d/out_kafka.conf\n2025-03-25T03:02:05Z I! Starting Telegraf 1.34.1 brought to you by InfluxData the makers of InfluxDB\n2025-03-25T03:02:05Z I! Available plugins: 239 inputs, 9 aggregators, 33 processors, 26 parsers, 63 outputs, 6 secret-stores\n2025-03-25T03:02:05Z I! Loaded inputs: influxdb_v2_listener\n2025-03-25T03:02:05Z I! Loaded aggregators:\n2025-03-25T03:02:05Z I! Loaded processors:\n2025-03-25T03:02:05Z I! Loaded secretstores:\n2025-03-25T03:02:05Z I! Loaded outputs: kafka\n2025-03-25T03:02:05Z I! Tags enabled:\n2025-03-25T03:02:05Z I! [agent] Config: Interval:10s, Quiet:false, Hostname:\"\", Flush Interval:10s\n2025-03-25T03:02:05Z W! [agent] The default value of 'skip_processors_after_aggregators' will change to 'true' with Telegraf v1.40.0! If you need the current default behavior, please explicitly set the option to 'false'!\n2025-03-25T03:02:07Z E! [agent] Failed to connect to [outputs.kafka], retrying in 15s, error was \"kafka: client has run out of available brokers to talk to: dial tcp 172.18.0.6:9092: connect: connection refused\"\n2025-03-25T03:02:23Z E! [telegraf] Error running agent: connecting output outputs.kafka: error connecting to output \"outputs.kafka\": kafka: client has run out of available brokers to talk to: dial tcp 172.18.0.6:9092: connect: connection refused\n2025-03-25T03:02:26Z I! Loading config: /etc/telegraf/telegraf.conf\n2025-03-25T03:02:26Z I! Loading config: /etc/telegraf/telegraf.d/in_influxv2.conf\n2025-03-25T03:02:26Z I! Loading config: /etc/telegraf/telegraf.d/out_kafka.conf\n2025-03-25T03:02:26Z I! Starting Telegraf 1.34.1 brought to you by InfluxData the makers of InfluxDB\n2025-03-25T03:02:26Z I! Available plugins: 239 inputs, 9 aggregators, 33 processors, 26 parsers, 63 outputs, 6 secret-stores\n2025-03-25T03:02:26Z I! Loaded inputs: influxdb_v2_listener\n2025-03-25T03:02:26Z I! Loaded aggregators:\n2025-03-25T03:02:26Z I! Loaded processors:\n2025-03-25T03:02:26Z I! Loaded secretstores:\n2025-03-25T03:02:26Z I! Loaded outputs: kafka\n2025-03-25T03:02:26Z I! Tags enabled:\n2025-03-25T03:02:26Z I! [agent] Config: Interval:10s, Quiet:false, Hostname:\"\", Flush Interval:10s\n2025-03-25T03:02:26Z W! [agent] The default value of 'skip_processors_after_aggregators' will change to 'true' with Telegraf v1.40.0! If you need the current default behavior, please explicitly set the option to 'false'!\n2025-03-25T03:02:26Z E! [agent] Failed to connect to [outputs.kafka], retrying in 15s, error was \"kafka: client has run out of available brokers to talk to: dial tcp 172.18.0.6:9092: connect: connection refused\"\n2025-03-25T03:02:42Z E! [telegraf] Error running agent: connecting output outputs.kafka: error connecting to output \"outputs.kafka\": kafka: client has run out of available brokers to talk to: dial tcp 172.18.0.6:9092: connect: connection refused\n2025-03-25T03:02:44Z I! Loading config: /etc/telegraf/telegraf.conf\n2025-03-25T03:02:44Z I! Loading config: /etc/telegraf/telegraf.d/in_influxv2.conf\n2025-03-25T03:02:44Z I! Loading config: /etc/telegraf/telegraf.d/out_kafka.conf\n2025-03-25T03:02:44Z I! Starting Telegraf 1.34.1 brought to you by InfluxData the makers of InfluxDB\n2025-03-25T03:02:44Z I! Available plugins: 239 inputs, 9 aggregators, 33 processors, 26 parsers, 63 outputs, 6 secret-stores\n2025-03-25T03:02:44Z I! Loaded inputs: influxdb_v2_listener\n2025-03-25T03:02:44Z I! Loaded aggregators:\n2025-03-25T03:02:44Z I! Loaded processors:\n2025-03-25T03:02:44Z I! Loaded secretstores:\n2025-03-25T03:02:44Z I! Loaded outputs: kafka\n2025-03-25T03:02:44Z I! Tags enabled:\n2025-03-25T03:02:44Z I! [agent] Config: Interval:10s, Quiet:false, Hostname:\"\", Flush Interval:10s\n2025-03-25T03:02:44Z W! [agent] The default value of 'skip_processors_after_aggregators' will change to 'true' with Telegraf v1.40.0! If you need the current default behavior, please explicitly set the option to 'false'!\n2025-03-25T03:02:45Z E! [agent] Failed to connect to [outputs.kafka], retrying in 15s, error was \"kafka: client has run out of available brokers to talk to: dial tcp 172.18.0.6:9092: connect: connection refused\"\n2025-03-25T03:03:01Z E! [telegraf] Error running agent: connecting output outputs.kafka: error connecting to output \"outputs.kafka\": kafka: client has run out of available brokers to talk to: dial tcp 172.18.0.6:9092: connect: connection refused\n2025-03-25T03:03:04Z I! Loading config: /etc/telegraf/telegraf.conf\n2025-03-25T03:03:04Z I! Loading config: /etc/telegraf/telegraf.d/in_influxv2.conf\n2025-03-25T03:03:04Z I! Loading config: /etc/telegraf/telegraf.d/out_kafka.conf\n2025-03-25T03:03:04Z I! Starting Telegraf 1.34.1 brought to you by InfluxData the makers of InfluxDB\n2025-03-25T03:03:04Z I! Available plugins: 239 inputs, 9 aggregators, 33 processors, 26 parsers, 63 outputs, 6 secret-stores\n2025-03-25T03:03:04Z I! Loaded inputs: influxdb_v2_listener\n2025-03-25T03:03:04Z I! Loaded aggregators:\n2025-03-25T03:03:04Z I! Loaded processors:\n2025-03-25T03:03:04Z I! Loaded secretstores:\n2025-03-25T03:03:04Z I! Loaded outputs: kafka\n2025-03-25T03:03:04Z I! Tags enabled:\n2025-03-25T03:03:04Z I! [agent] Config: Interval:10s, Quiet:false, Hostname:\"\", Flush Interval:10s\n2025-03-25T03:03:04Z W! [agent] The default value of 'skip_processors_after_aggregators' will change to 'true' with Telegraf v1.40.0! If you need the current default behavior, please explicitly set the option to 'false'!\n2025-03-25T03:03:05Z E! [agent] Failed to connect to [outputs.kafka], retrying in 15s, error was \"kafka: client has run out of available brokers to talk to: dial tcp 172.18.0.6:9092: connect: connection refused\"\n2025-03-25T03:03:20Z E! [telegraf] Error running agent: connecting output outputs.kafka: error connecting to output \"outputs.kafka\": kafka: client has run out of available brokers to talk to: dial tcp 172.18.0.6:9092: connect: connection refused\n2025-03-25T03:03:23Z I! Loading config: /etc/telegraf/telegraf.conf\n2025-03-25T03:03:23Z I! Loading config: /etc/telegraf/telegraf.d/in_influxv2.conf\n2025-03-25T03:03:23Z I! Loading config: /etc/telegraf/telegraf.d/out_kafka.conf\n2025-03-25T03:03:23Z I! Starting Telegraf 1.34.1 brought to you by InfluxData the makers of InfluxDB\n2025-03-25T03:03:23Z I! Available plugins: 239 inputs, 9 aggregators, 33 processors, 26 parsers, 63 outputs, 6 secret-stores\n2025-03-25T03:03:23Z I! Loaded inputs: influxdb_v2_listener\n2025-03-25T03:03:23Z I! Loaded aggregators:\n2025-03-25T03:03:23Z I! Loaded processors:\n2025-03-25T03:03:23Z I! Loaded secretstores:\n2025-03-25T03:03:23Z I! Loaded outputs: kafka\n2025-03-25T03:03:23Z I! Tags enabled:\n2025-03-25T03:03:23Z I! [agent] Config: Interval:10s, Quiet:false, Hostname:\"\", Flush Interval:10s\n2025-03-25T03:03:23Z W! [agent] The default value of 'skip_processors_after_aggregators' will change to 'true' with Telegraf v1.40.0! If you need the current default behavior, please explicitly set the option to 'false'!\n2025-03-25T03:03:24Z E! [agent] Failed to connect to [outputs.kafka], retrying in 15s, error was \"kafka: client has run out of available brokers to talk to: dial tcp 172.18.0.6:9092: connect: connection refused\"\n2025-03-25T03:03:40Z I! [inputs.influxdb_v2_listener] Started HTTP listener service on :8086\n```\n\n### System info\n\nTelegraf 1.34.1\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1. Upgrade kafka to 4.0\n2. telegraf kafka output plugin can't connect any more\n3. observe errors in kafka log\n\n\n### Expected behavior\n\nkafka output plugin continues to deliver messages to kafka after an upgrade of kafka to 4.0\n\n### Actual behavior\n\nafter kafka has been upgraded to 4.0, telegraf kafka output plugin fails to connect to the broker.\n\n### Additional info\n\nKafka side logs:\n\n```\norg.apache.kafka.common.errors.UnsupportedVersionException: Received request for api with key 11 (JoinGroup) and unsupported version 1\n```\n\nMaybe the API deprecation in kafka 4.0 is the reason?",
      "solution": "@andrasg please test the binary in PR #16707, available as soon as CI finished the build, and let me know if this fixes the issue! As a background: The underlying library, in the version we use, did not support Kafka 4.x yet and I therefore updated the dependency. Hope this helps.\n\n---\n\n@vanga can you please try setting `version = \"2.1.0\"` and let me know if this fixes the issue!? The problem is that by default we set the version to 0.10.0 and this seems to not be compatible with Kafka 4+...\n\n---\n\nI tested with\n```sh\n# docker run -d -p 9092:9092 --name broker apache/kafka:4.0.0\n```\n\nand \n\n```toml\n[agent]\n  interval = \"1s\"\n  flush_interval = \"1s\"\n\n[[inputs.mock]]\n  metric_name = \"mock\"\n  [[inputs.mock.step]]\n    name = \"counter\"\n\n[[outputs.kafka]]\n  brokers = [\"localhost:9092\"]\n  topic = \"telegraf\"\n  client_id = \"telegraf\"\n  data_format = \"influx\"\n```\n\nand I'm not seeing this issue! \n\n@vanga can you please provide a way to reproduce the issue?!?",
      "labels": [
        "bug",
        "waiting for response"
      ],
      "created_at": "2025-03-25T08:24:23Z",
      "closed_at": "2025-11-13T18:09:45Z",
      "url": "https://github.com/influxdata/telegraf/issues/16691",
      "comments_count": 5
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17205,
      "title": "\"no serializable fields\" causes influxdb_v2 output to drop unrelated metrics in batch",
      "problem": "### Relevant telegraf.conf\n\n```toml\n[[inputs.socket_listener]]\n  alias = \"collectd\"\n  data_format = \"graphite\"\n  read_buffer_size = \"256KiB\"\n  separator = \"_\"\n  service_address = \"tcp://:2003\"\n  templates = [\n    \"collectd.*.iwinfo.* .host.measurement.device.field*\",\n    \"collectd.*.interface.* .host.measurement.device.field*\",\n    \"collectd.*.netlink.* .host.measurement.device.field*\",\n    \"collectd.*.cpu.* .host.measurement.cpu.field*\",\n    \"collectd.*.processes.ps_state .host.measurement.field.state\",\n    \"collectd.*.processes.fork_rate .host.measurement.field\",\n    \"collectd.*.processes.* .host.measurement.process.field*\",\n    \"collectd.*.load.* .host.measurement.field*\",\n    \"collectd.*.memory.* .host.measurement.field*\",\n    \"collectd.*.ping.* .host.measurement.field.target.target.target.target\",\n    \"collectd.*.sensors.* .host.measurement.chip.field.feature\",\n    \"collectd.*.uptime.* .host.measurement\",\n    \".host.measurement.field*\"\n  ]\n  [inputs.socket_listener.tags]\n    collectd = \"true\"\n\n[[outputs.influxdb_v2]]\n  bucket = \"default\"\n  bucket_tag = \"influxdb_target_bucket\"\n  content_encoding = \"identity\"\n  exclude_bucket_tag = true\n  influx_uint_support = true\n  namedrop = [\n    \"log_*\",\n    \"*_log\",\n    \"win_eventlog\",\n    \"syslog\",\n    \"log\"\n  ]\n  organization = \"$INFLUXDB_V2_ORGANIZATION\"\n  token = \"$INFLUXDB_V2_TOKEN\"\n  urls = [\n    \"http://influxdb-influxdb2:8085\"\n  ]\n```\n\n### Logs from Telegraf\n\n```text\n2025-06-17T23:47:36Z D! [outputs.file] Could not serialize metric: \"ping,collectd=hostname,host=hostname,target=ip,telegraf=influxdb/telegraf-74fc7f69cc-dmbl6\": no serializable fields\n2025-06-17T23:47:36Z D! [outputs.file] Could not serialize metric: \"ping,collectd=hostname,host=hostname,influxdb_target_bucket=compute,target=ip\": no serializable fields\n```\n\n### System info\n\nTelegraf 1.34\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1. I have a collectd instance running on my router, performing a ping regularly. I am sending the ping information to Telegraf (to perform postprocessing before writing to Influx) via Graphite encoding.\n2. Recently, one of the routers were decommissioned, and thus the ping metrics were no longer being reported. However, collectd encodes the data with the tags but not fields.\n3. I noticed that the metric points from the host (and only the affected hosts) were missing, even for unrelated measurements.\n\n### Expected behavior\n\nOnly the ping measurements should be dropped, the others should still be submitted. Notably, I was using the system.uptime metric as a kind of heartbeat, and those metrics were being unexpectedly dropped.\n\n### Actual behavior\n\n![Image](https://github.com/user-attachments/assets/776f6296-515f-419b-aeff-c3940e3369fb)\n\nI removed the faulty Ping metric at ~7.55am, then the normal rate (every 30s) was restored. This is the system.uptime as reported by the host, and aggregated in Influx.\n\n### Additional info\n\nI managed to debug this only because I created a duplicate output.file for the affected measurements and encoded it using Influx line protocol. I saw that the metrics were printed for the file output (which was my debug) but not showing up in InfluxDB. The dropped metrics count didn't show anything amiss. I also checked the Influx layer for possible corruption, but exporting the points for the given time frame showed that there were very few metric points for the affected host.\n\nIt seems like the batching code is silently dropping these unrelated metrics. Removing the broken ping metrics from the source restored the frequency of my data points for the affected host.\n\nFor context, the Graphite metrics being sent were:\n\n```\ncollectd.hostname.ping.ping.ip_addr nan 1750205530\ncollectd.hostname.ping.ping_stddev.ip_addr nan 1750205530\ncollectd.hostname.ping.ping_droprate.ip_addr 1 1750205530\n```\n\nI did some tracing, and for 1.34 the code for batching is https://github.com/influxdata/telegraf/blob/v1.34.4/plugins/outputs/influxdb_v2/http.go#L229:\n\n```go\n\t\t// Check if we got a write error and if so, translate the returned\n\t\t// metric indices to return the original indices in case of bucketing\n\t\tvar writeErr *internal.PartialWriteError\n\t\tif errors.As(err, &writeErr) {\n\t\t\twErr.Err = writeErr.Err\n\t\t\tfor _, idx := range writeErr.MetricsAccept {\n\t\t\t\twErr.MetricsAccept = append(wErr.MetricsAccept, batchIndices[bucket][idx])\n\t\t\t}\n\t\t\tfor _, idx := range writeErr.MetricsReject {\n\t\t\t\twErr.MetricsReject = append(wErr.MetricsReject, batchIndices[bucket][idx])\n\t\t\t}\n\t\t\tif !errors.Is(writeErr.Err, internal.ErrSizeLimitReached) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\treturn &wErr\n\t\t}\n\n\t\t// Return the error without special treatment\n\t\twErr.Err = err\n\t\treturn &wErr\n\t}\n\treturn nil\n}\n```\n\nIt seems that c.writeBatch returns the PartialWriteError/SerializationError. The quoted code stores the accepted/rejected metrics and skips to the next batch (!ErrSizeLimitReached). Processes other batches then returns nil.\n\nSince 1.35 there's now #16741 and I'm not sure if the bug still exists.",
      "solution": "I also just ran into this; I was trying to debug what seemed to be randomly dropped metrics (including `internal_*` which made debugging very painful!) when I stumbled on this issue. Adding [this Starlark processor](https://community.influxdata.com/t/telegraf-processor-remove-nan/19518/6) seems to work around the issue for me:\n\n```python\nload(\"logging.star\", \"log\")\nnan = float('nan')\n\ndef apply(metric):\n  for k, v in metric.fields.items():\n    if v == nan:\n      metric.fields.pop(k)\n      log.warn(\"Dropped NaN value: metric {} field {}\".format(metric.name, k))\n  return metric\n```\n\nBy adding a second `file` output just for `internal*` metrics, I was able to see that the dropped metrics were not causing any of the drop counters to increment. So whole batches of valid metrics get dropped by the influxdb_v2 output whenever there is one value that can't be serialized.\n\n---\n\n@lowjoel, @999eagle or @quentinmit please test the binary in PR #17949 and let me know if this fixes the issue!\n\n---\n\ntested the linked pr now and getting these errors every few seconds:\n```\nE! [outputs.influxdb_v2] When writing to [https://[influxdb host]/api/v2/write]: serialization of metric(s) failed\nE! [agent] Error writing to outputs.influxdb_v2: serialization of metric(s) failed\n```\nso far, buffers do not seem to run full though and other metrics are arriving, so at least the issue of dropping unrelated metrics seems to be solved with this, thanks!",
      "labels": [
        "bug"
      ],
      "created_at": "2025-06-18T00:07:18Z",
      "closed_at": "2025-11-10T17:38:50Z",
      "url": "https://github.com/influxdata/telegraf/issues/17205",
      "comments_count": 9
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 16862,
      "title": "namedrop are not honored",
      "problem": "### Relevant telegraf.conf\n\n```toml\n[[inputs.docker]]\n  namedrop = [\"docker_container_cpu_container_id\"]\n  endpoint = \"unix:///var/run/docker.sock\"\n  perdevice = false\n  perdevice_include = [\"cpu\"]\n```\n\n### Logs from Telegraf\n\n```text\n2025-04-23T08:34:29Z E! [serializers.prometheusremotewrite::http] some series were dropped, 1280 series left to send; last recorded error: failed to parse \"docker_container_cpu_container_id\": bad sample value \"d6a4fd0573cfd669aea5dda3ff15eb8069440e2b9a3c8f9b3e071b41c4d491fb\"\n2025-04-23T08:34:29Z E! [serializers.prometheusremotewrite::http] some series were dropped, 1280 series left to send; last recorded error: failed to parse \"docker_container_cpu_container_id\": bad sample value \"d6a4fd0573cfd669aea5dda3ff15eb8069440e2b9a3c8f9b3e071b41c4d491fb\"\n2025-04-23T08:35:29Z E! [serializers.prometheusremotewrite::http] some series were dropped, 1378 series left to send; last recorded error: failed to parse \"docker_container_cpu_container_id\": bad sample value \"34c106efcbc121cf551b108202a3d6dbca68ffabfa1a5724ccbf5869faa19427\"\n2025-04-23T08:35:29Z E! [serializers.prometheusremotewrite::http] some series were dropped, 1378 series left to send; last recorded error: failed to parse \"docker_container_cpu_container_id\": bad sample value \"34c106efcbc121cf551b108202a3d6dbca68ffabfa1a5724ccbf5869faa19427\"\n2025-04-23T08:36:29Z E! [serializers.prometheusremotewrite::http] some series were dropped, 1378 series left to send; last recorded error: failed to parse \"docker_container_cpu_container_id\": bad sample value \"573d548b231f4713bb68df543cc365375be76f33ed1dd683564e6c889763001b\"\n2025-04-23T08:36:29Z E! [serializers.prometheusremotewrite::http] some series were dropped, 1378 series left to send; last recorded error: failed to parse \"docker_container_cpu_container_id\": bad sample value \"573d548b231f4713bb68df543cc365375be76f33ed1dd683564e6c889763001b\"\n2025-04-23T08:37:29Z E! [serializers.prometheusremotewrite::http] some series were dropped, 1378 series left to send; last recorded error: failed to parse \"docker_container_cpu_container_id\": bad sample value \"df50386f5bd614dec7dd85e1843181d5876a6f320344d3f96b2b9641f2e8212c\"\n2025-04-23T08:37:29Z E! [serializers.prometheusremotewrite::http] some series were dropped, 1378 series left to send; last recorded error: failed to parse \"docker_container_cpu_container_id\": bad sample value \"df50386f5bd614dec7dd85e1843181d5876a6f320344d3f96b2b9641f2e8212c\"\n2025-04-23T08:38:29Z E! [serializers.prometheusremotewrite::http] some series were dropped, 1378 series left to send; last recorded error: failed to parse \"docker_container_cpu_container_id\": bad sample value \"a4148c7a257041d75c185eb4594e3cf249df0c9106275bf93d53a2a5f446aad2\"\n2025-04-23T08:38:29Z E! [serializers.prometheusremotewrite::http] some series were dropped, 1378 series left to send; last recorded error: failed to parse \"docker_container_cpu_container_id\": bad sample value \"a4148c7a257041d75c185eb4594e3cf249df0c9106275bf93d53a2a5f446aad2\"\n2025-04-23T08:39:29Z E! [serializers.prometheusremotewrite::http] some series were dropped, 1378 series left to send; last recorded error: failed to parse \"docker_container_cpu_container_id\": bad sample value \"d6a4fd0573cfd669aea5dda3ff15eb8069440e2b9a3c8f9b3e071b41c4d491fb\"\n2025-04-23T08:39:29Z E! [serializers.prometheusremotewrite::http] some series were dropped, 1378 series left to send; last recorded error: failed to parse \"docker_container_cpu_container_id\": bad sample value \"d6a4fd0573cfd669aea5dda3ff15eb8069440e2b9a3c8f9b3e071b41c4d491fb\"\n```\n\n### System info\n\nDocker 28.1.1, OS: Debian Bookworm, Telegraf 1.34.2 (git: HEAD@552f7e20)\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\nRun any output.http with prometheusremotewrite enabled and try to drop metrics that are invalid\n\n\n### Expected behavior\n\nDropped metrics from the input should not be available for any outputs\n\n### Actual behavior\n\nDropped metrics are still processed by the outputs \n\n### Additional info\n\n_No response_",
      "solution": "@jryberg is this a real issue or can we close the issue? If so, can you please provide the \"raw\", unfiltered (and redacted) metrics for me to reproduce the issue?",
      "labels": [
        "bug"
      ],
      "created_at": "2025-04-23T08:44:12Z",
      "closed_at": "2025-11-11T10:39:50Z",
      "url": "https://github.com/influxdata/telegraf/issues/16862",
      "comments_count": 5
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 16188,
      "title": "Tail input silently ignores files in unreadable directories",
      "problem": "### Relevant telegraf.conf\n\n```toml\n[global_tags]\r\n\r\n[agent]\r\n  interval = \"5s\"\r\n  round_interval = true\r\n  metric_batch_size = 1000\r\n  metric_buffer_limit = 10000\r\n  flush_interval = \"1s\"\r\n  precision = \"1ms\"\r\n\r\n[[inputs.tail]]\r\n  files = [ \"/home/myuser/root_only/myfile\" ]\r\n  data_format = \"influx\"\n```\n\n\n### Logs from Telegraf\n\n```text\n$ telegraf --config telegraf-basic.conf  --debug --test\r\n2024-11-13T11:36:48Z I! Loading config: telegraf-basic.conf\r\n2024-11-13T11:36:48Z I! Starting Telegraf 1.32.2 brought to you by InfluxData the makers of InfluxDB\r\n2024-11-13T11:36:48Z I! Available plugins: 235 inputs, 9 aggregators, 32 processors, 26 parsers, 62 outputs, 6 secret-stores\r\n2024-11-13T11:36:48Z I! Loaded inputs: tail\r\n2024-11-13T11:36:48Z I! Loaded aggregators:\r\n2024-11-13T11:36:48Z I! Loaded processors:\r\n2024-11-13T11:36:48Z I! Loaded secretstores:\r\n2024-11-13T11:36:48Z W! Outputs are not used in testing mode!\r\n2024-11-13T11:36:48Z I! Tags enabled: host=thinkpad\r\n2024-11-13T11:36:48Z D! [agent] Initializing plugins\r\n2024-11-13T11:36:48Z D! [agent] Starting service inputs\r\n2024-11-13T11:36:48Z D! [agent] Stopping service inputs\r\n2024-11-13T11:36:48Z D! [agent] Input channel closed\r\n2024-11-13T11:36:48Z D! [agent] Stopped Successfully\n```\n\n\n### System info\n\nFedora 44, Telegraf 1.32.2\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1. Run telegraf with a tail input that references a file in a directory to which there are no access permissions for the executing user\n\n### Expected behavior\n\nIf a file can not be reached because of directory permissions then an error should be reported.\n\n### Actual behavior\n\nNo error is reported, telegraf ignores the input\n\n### Additional info\n\nI couldn't work out in the code at what point the path access is evaluated, it seems to be quite early on as when the code is adding the tail, there are unaviodable error messages there that would be printed had it failed inside the Tail module.\r\n\r\nThis works fine for actual file permissions, but not for directories.",
      "solution": "I think the issue is with the doublestar library `github.com/bmatcuk/doublestar`\r\nThe imported library does not check for director permissions.\n\n---\n\n@neelayu and @cpinflux PR #17908 should workaround the issue for non-wildcard file specifications. Can you please give it a try!?",
      "labels": [
        "bug"
      ],
      "created_at": "2024-11-13T11:42:17Z",
      "closed_at": "2025-11-10T18:05:30Z",
      "url": "https://github.com/influxdata/telegraf/issues/16188",
      "comments_count": 2
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 9129,
      "title": "tail input fails silently when trying to read files inside directories with only execute permissions",
      "problem": "<!--\r\nPlease redirect any questions about Telegraf usage to the InfluxData Community\r\nsite: https://community.influxdata.com\r\n\r\nCheck the documentation for the related plugin including the troubleshooting\r\nsection if available.\r\n-->\r\n\r\nThe `tail` input (and I think that `logparser` does this too) fails silently when attempting to read a file inside a directory which has execute but not read permissions for the `telegraf` user.\r\n\r\nI'm providing a Dockerfile to reproduce the issues below.\r\n\r\nBelow you can see the file permissions schema and proof that the user can read the file:\r\n\r\n```bash\r\nbash-4.2$ whoami\r\ntelegraf\r\n\r\nbash-4.2$ namei -mo /var/log/apache2/error_log \r\nf: /var/log/apache2/error_log\r\n drwxr-xr-x root root /\r\n drwxr-xr-x root root var\r\n drwxr-xr-x root root log\r\n drwx--x--x root root apache2\r\n -rw-r--r-- root root error_log\r\n\r\nbash-4.2$ tail /var/log/apache2/error_log \r\nexample,result=ok value=1i\r\nexample,result=ok value=2i\r\nexample,result=ok value=3i\r\nexample,result=error value=3i\r\n```\r\nBelow you can see that telegraf fails silently when running under the `telegraf` user:\r\n\r\n```shell\r\nbash-4.2$ /usr/bin/telegraf -config /etc/telegraf/telegraf.conf -config-directory /etc/telegraf/telegraf.d --debug --test-wait 15 \r\n2021-04-14T10:15:01Z I! Starting Telegraf 1.18.1\r\n2021-04-14T10:15:01Z D! [agent] Initializing plugins\r\n2021-04-14T10:15:01Z D! [agent] Starting service inputs\r\n2021-04-14T10:15:16Z D! [agent] Stopping service inputs\r\n2021-04-14T10:15:16Z D! [agent] Input channel closed\r\n2021-04-14T10:15:16Z D! [agent] Stopped Successfully\r\n```\r\n\r\nIt works as expected with a `root` user or with a `/var/log/apache2` directory with read and execute permissions:\r\n\r\n```shell\r\n[root@400fb31a3249 /]# /usr/bin/telegraf -config /etc/telegraf/telegraf.conf -config-directory /etc/telegraf/telegraf.d --debug --test-wait 15 \r\n2021-04-14T10:17:07Z I! Starting Telegraf 1.18.1\r\n2021-04-14T10:17:07Z D! [agent] Initializing plugins\r\n2021-04-14T10:17:07Z D! [agent] Starting service inputs\r\n2021-04-14T10:17:07Z D! [inputs.tail] Tail added for \"/var/log/apache2/error_log\"\r\n> example,host=400fb31a3249,path=/var/log/apache2/error_log,result=ok value=1i 1618395427510170897\r\n> example,host=400fb31a3249,path=/var/log/apache2/error_log,result=ok value=2i 1618395427510177215\r\n> example,host=400fb31a3249,path=/var/log/apache2/error_log,result=ok value=3i 1618395427510181769\r\n> example,host=400fb31a3249,path=/var/log/apache2/error_log,result=error value=3i 1618395427510183472\r\n2021-04-14T10:17:22Z D! [agent] Stopping service inputs\r\n2021-04-14T10:17:22Z D! [inputs.tail] Tail removed for \"/var/log/apache2/error_log\"\r\n2021-04-14T10:17:22Z D! [agent] Input channel closed\r\n2021-04-14T10:17:22Z D! [agent] Stopped Successfully\r\n\r\n```\r\n\r\n### Relevant telegraf.conf:\r\n<!-- Place config in the toml code section. -->\r\n```toml\r\n[global_tags]\r\n\r\n[agent]\r\n  interval = \"1s\"\r\n  round_interval = true\r\n  metric_batch_size = 1000\r\n  metric_buffer_limit = 10000\r\n  collection_jitter = \"0s\"\r\n  flush_interval = \"5s\"\r\n  flush_jitter = \"0s\"\r\n  precision = \"\"\r\n  debug = false\r\n  quiet = false\r\n  logfile = \"\"\r\n  hostname = \"\"\r\n  omit_hostname = false\r\n\r\n[[outputs.file]]\r\n  files = [\"stdout\"]\r\n\r\n[[inputs.tail]]\r\n  files = [\"/var/log/apache2/error_log\"]\r\n  from_beginning = true\r\n  watch_method = \"poll\"\r\n  data_format = \"influx\"\r\n\r\n```\r\n\r\n### System info:\r\n\r\n<!-- Include Telegraf version, operating system, and other relevant details -->\r\n\r\n* CentOS 7\r\n* Telegraf 1.18.1 (git: HEAD d3ac9a3f) \r\n\r\n### Docker\r\n\r\n<!-- If your bug involves third party dependencies or services, it can be very helpful to provide a Dockerfile or docker-compose.yml that reproduces the environment you're testing against -->\r\n\r\n### Steps to reproduce:\r\n\r\n<!-- Describe the steps to reproduce the bug. -->\r\n\r\nI've provided a Dockerfile that reproduces the error:\r\n\r\nhttps://github.com/hluaces/telegraf-bug-9129\r\n\r\nIf you run the container (`docker run --rm local/bug-telegraf`) you'll see that no data is gathered after a 15s wait time.\r\n\r\nIf you run with as root inside the container (`docker run --rm -u root local/bug-telegraf`) you'll see that data is gathered.\r\n\r\nYou can move inside the container by using  `docker run --rm -it --entrypoint bash local/bug-telegraf`.\r\n\r\n### Expected behavior:\r\n\r\n* Data should be properly read by the `tail` input.\r\n* A warning or error should be displayed in the case it cannot.\r\n\r\n### Actual behavior:\r\n\r\n* Data is not read.\r\n* A warning is not shown in the file.\r\n\r\n### Additional info:\r\n\r\n<!-- Include gist of relevant config, logs, etc. -->\r\n",
      "solution": "@hluaces Amazing job writing this issue! Providing a dockerfile recreating the issue is super useful and fantastic, I really appreciate it. I was able to re-create the issue locally as well, and the issue is right here: https://github.com/bmatcuk/doublestar/blob/v3/doublestar.go#L441 the error for opening the directory path isn't returned. Although the code (https://github.com/influxdata/telegraf/blob/master/internal/globpath/globpath.go#L53) isn't checking the error either if even it did.\r\n\r\nLooks like the doublestar library recently went through a big overhaul and now has a version 4 available, it might be handled in this new version. I can play around with it and see if it fixes the problem.\r\n \n\n---\n\nAre there any news on this?\r\n\r\nI'm not trying to pressure or anything, just trying to manage expectations with this bug as its presence forces me to do some ugly workarounds. Thank you for your time.\n\n---\n\nThanks for reminding me about this I honestly forgot, I'm afraid no news as I haven't made progress with updating the doublestar library. I will make a note to look at it this week and get back to you, hopefully it will resolve the issue.\r\n\r\nedit: I remember now what stopped me, v4 depends on io/fs and therefore required Go v1.16+. We will be moving away from v1.15 after this pr is merged: https://github.com/influxdata/telegraf/pull/9642. I will go ahead and get a pr ready to make it sure it fixes this issue.",
      "labels": [
        "bug",
        "area/tail"
      ],
      "created_at": "2021-04-14T10:24:18Z",
      "closed_at": "2025-11-10T18:05:30Z",
      "url": "https://github.com/influxdata/telegraf/issues/9129",
      "comments_count": 9
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17235,
      "title": "[outputs.influxdb_v2; common.ratelimiter]: Sometimes metrics are properly sent, sometimes not",
      "problem": "### Use Case\n\nWe are currently stuck in a situation, were we have five `inputs.exec` entries in our config - but out of all the five, _sometimes_ two of them do something and sometimes they do not. There are no errors in the Telegraf log, and none we could find in Event Viewer in general - yes, this is on Windows. All five of them have the same layout:\n\n```toml\n[[inputs.exec]]\n  commands = [ \"powershell.exe -NoProfile -ExecutionPolicy Bypass -Command \\\"&'C:/(...)/backupReplication.ps1' -ReportType backupRepository\\\"\" ]\n  interval = \"1h\"\n  timeout = \"180s\"\n  data_format = \"json_v2\"\n  [[inputs.exec.json_v2]]\n    measurement_name = \"veeam.backupReplication.backupRepository\"\n    [[inputs.exec.json_v2.object]]\n      path = \"@this\"\n      tags = [ \"RepositoryName\" ]\n```\n\nThis is why we need more debugging or tracing. Having gone through the recent releases and their changelogs, we have also played around with `debug = true`, as well as `log_level = \"trace\"` as recently introduced. Unfortunately, to no avail.\n\nHe spent the most time with the actual debugging and I went to read the commits, releases and code to try and find him pointers as to what he could try to do. Interestingly, this _did_ use to work in 1.27.\n\nHis summary:\n- Using Telegraf 1.34.1 with multiple `inputs.exec` plugins scheduled at the same interval (e.g., every 10 minutes).\n- PowerShell processes for each `inputs.exec` are correctly launched (confirmed by observing Task Manager).\n- No errors, timeouts, or scheduling warnings are logged in Telegraf (debug logging enabled).\n- Some `inputs.exec` executions randomly fail to deliver metrics to InfluxDB despite the script always producing valid JSON output (verified via local file logging).\n- Issue did not occur in Telegraf 1.27 with identical configuration and system load. We only recently upgraded to 1.34.\n- Changing GOMAXPROCS had no effect.\n  * This was my idea, as this behaviour was talked about a lot lately on r/golang and was mentioned to have it's behaviour changed. Mainly for containers, but I'd thought to just give it a try.\n- Appears related to Telegraf\u2019s stricter input scheduling since v1.30, where `inputs.exec` commands scheduled at the same interval may conflict or be silently skipped even when system resources are available.\n\n### Expected behavior\n\nTelegraf should either multi-thread `inputs.exec` properly or log skipped executions clearly.\n\nOr, it should produce apropriate logs in debug/trace logging to indicate:\n- When the command (and which one) was started,\n- If it exited with a non-zero exit code\n- If it was perhaps killed due to timeout or alike.\n\n### Actual behavior\n\nIt quietly fails with no indication as to why or what caused it. Observing the logs on both InfluxDB and Telegraf brought us no closer. I will likely monkey-patch a local copy of Telegraf (local build) to use Go's stdlib `fmt.Println(...)` calls to just output _something_ to help us understand more about what is going on - or rather, what isn't.\n\n### Additional info\n\nAs mentioned previously, this used to work just fine in 1.27. But due to other requirements, we opted to update the version of Telegraf we used in hopes to incorporate new plugins and features - but were now met with this. Veeam is a cruicial part of what we monitor here, and not having it reliably return information is a bit of a problem... Aside from wrapping the VBR Cmdlets in scripts and producing JSON out of it which can be parsed and understood by Telegraf, we hadn't found a better alternative. So, we rely on `inputs.exec` for the various information from this instead. It also gives us flexibility in perhaps adding other metadata from other Cmdlet calls in the future if we need or want to.",
      "solution": "@senpro-ingwersenk thanks for reporting and debugging the issue!\n\nFirst of all, you now see the errors in the logs as previously any serialization error was silently ignored and the metric was dropped, so the behavior did not change (or at least should not be). Looking at the errors it seem like\n\n1. you are trying so send a field with an empty name\n2. the server reports some error but we don't see what it is\n\nThe first error is definitively valid as the line-protocol does not allow empty field names. For the second one it's hard to say what is going on without knowing what issue the server sees. Could you provide the metrics of the batch that is causing the issue?\n\nFor the rate-limiter: Do you set a limit? If not, the code should be equivalent to the previous one. Would you be willing to do some debugging if I prepare a PR with debugging output?\n\n---\n\nWell status code `500` indicates that the issue is _not_ on the Telegraf side. Did you check the server logs? Can you maybe increase the debug level there? I can try to check if we get an error body back if you are willing to run a debug version...\n\nBtw: Did you try to run without rate-limit? Does that work?\n\n---\n\nI checked the server logs but unfortunately, even with all logging settings enabled that I could find, InfluxDB just doesn't tell me exactly what the issue is. All I can gather is that it _might_ be an incomplete message.\n\nHere is one I had sent my collegue for showcasing:\n```\nts=2025-06-25T06:56:23.435162Z lvl=debug msg=Request log_id=0xKMi1NW000 service=http method=POST host=<snip> path=/api/v2/write query=\"bucket=veeam&org=SENST\" proto=HTTP/1.1 status_code=500 response_size=88 content_length=0 referrer= remote=10.42.0.48:32938 user_agent=Telegraf authenticated_id=0ba2919f35084000 user_id=0ab0e278cb957000 took=0.357ms error=\"internal error\" error_code=\"internal error\"\n```\n\nPrior to that is just debug logs about authentication. So it literally tells me... nothing. Hence why I just sent it a completely incomplete message, and got back about the same in both logs and cURL.\n\nAs for disabling the rate limit entirely... how, actually? Might've overlooked this in all of this but... how do I actually, fully disable it? Would love to give this a shot!",
      "labels": [
        "bug"
      ],
      "created_at": "2025-06-24T13:45:46Z",
      "closed_at": "2025-11-10T17:38:51Z",
      "url": "https://github.com/influxdata/telegraf/issues/17235",
      "comments_count": 18
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17952,
      "title": "Panic in inputs.zfs after upgrading to 1.36.3-1",
      "problem": "### Relevant telegraf.conf\n\n```toml\n# # Read metrics of ZFS from arcstats, zfetchstats, vdev_cache_stats, pools and datasets\n# # This plugin ONLY supports Linux & FreeBSD\n[[inputs.zfs]]\n  interval=\"300s\"\n#   ## ZFS kstat path. Ignored on FreeBSD\n#   ## If not specified, then default is:\n#   # kstatPath = \"/proc/spl/kstat/zfs\"\n#   \n#   ## By default, telegraf gather all zfs stats\n#   ## Override the stats list using the kstatMetrics array:\n#   ## For FreeBSD, the default is:\n#   # kstatMetrics = [\"arcstats\", \"zfetchstats\", \"vdev_cache_stats\"]\n#   ## For Linux, the default is:\n#   # kstatMetrics = [\"abdstats\", \"arcstats\", \"dnodestats\", \"dbufcachestats\",\n#   #     \"dmu_tx\", \"fm\", \"vdev_mirror_stats\", \"zfetchstats\", \"zil\"]\n  kstatMetrics = [\"\"] \n##  \n#   ## By default, don't gather zpool stats\n#   # poolMetrics = false\n  poolMetrics = true\n#\n#   ## By default, don't gather dataset stats\n#   # datasetMetrics = false\n  datasetMetrics = false\n\n#   ## Report fields as the type defined by ZFS (Linux only)\n#   ## This is disabled for backward compatibility but is STRONGLY RECOMMENDED\n#   ## to be enabled to avoid overflows. This requires UINT support on the output\n#   ## for most fields.\n#   ## useNativeTypes = false\n  useNativeTypes = true\n```\n\n### Logs from Telegraf\n\n```text\nNov 05 17:25:00 hostname.example.com telegraf[839584]: 2025-11-05T22:25:00Z E! FATAL: [inputs.zfs] panicked: runtime error: index out of range [1] with length 0, Stack:\nNov 05 17:25:00 hostname.example.com telegraf[839584]: goroutine 265 [running]:                                                                                                               \nNov 05 17:25:00 hostname.example.com telegraf[839584]: github.com/influxdata/telegraf/agent.panicRecover(0xc0004cec00)    \nNov 05 17:25:00 hostname.example.com telegraf[839584]:         /go/src/github.com/influxdata/telegraf/agent/agent.go:1202 +0x6d        \nNov 05 17:25:00 hostname.example.com telegraf[839584]: panic({0xa0babc0?, 0xc0074d6240?})                                                                                                     \nNov 05 17:25:00 hostname.example.com telegraf[839584]:         /usr/local/go/src/runtime/panic.go:783 +0x132                                                                       \nNov 05 17:25:00 hostname.example.com telegraf[839584]: github.com/influxdata/telegraf/plugins/inputs/zfs.(*Zfs).processProcFile(0xc0074d6210?, {0x0?, 0x1?, 0xc0056e4c60?})                   \nNov 05 17:25:00 hostname.example.com telegraf[839584]:         /go/src/github.com/influxdata/telegraf/plugins/inputs/zfs/zfs_linux.go:252 +0xa13\nNov 05 17:25:00 hostname.example.com telegraf[839584]: github.com/influxdata/telegraf/plugins/inputs/zfs.(*Zfs).Gather(0xc00075b040, {0xb8c5d00, 0xc004908f20})\nNov 05 17:25:00 hostname.example.com telegraf[839584]:         /go/src/github.com/influxdata/telegraf/plugins/inputs/zfs/zfs_linux.go:78 +0x4e7\nNov 05 17:25:00 hostname.example.com telegraf[839584]: github.com/influxdata/telegraf/models.(*RunningInput).Gather(0xc0004cec00, {0xb8c5d00, 0xc004908f20})\nNov 05 17:25:00 hostname.example.com telegraf[839584]:         /go/src/github.com/influxdata/telegraf/models/running_input.go:263 +0x244\nNov 05 17:25:00 hostname.example.com telegraf[839584]: github.com/influxdata/telegraf/agent.(*Agent).gatherOnce.func1()\nNov 05 17:25:00 hostname.example.com telegraf[839584]:         /go/src/github.com/influxdata/telegraf/agent/agent.go:590 +0x58                                              \nNov 05 17:25:00 hostname.example.com telegraf[839584]: created by github.com/influxdata/telegraf/agent.(*Agent).gatherOnce in goroutine 146                                                   \nNov 05 17:25:00 hostname.example.com telegraf[839584]:         /go/src/github.com/influxdata/telegraf/agent/agent.go:588 +0xf7\nNov 05 17:25:00 hostname.example.com telegraf[839584]: goroutine 1 [sync.WaitGroup.Wait]:\nNov 05 17:25:00 hostname.example.com telegraf[839584]: sync.runtime_SemacquireWaitGroup(0xc00403e820?, 0x80?)                    \nNov 05 17:25:00 hostname.example.com telegraf[839584]:         /usr/local/go/src/runtime/sema.go:114 +0x2e\nNov 05 17:25:00 hostname.example.com telegraf[839584]: sync.(*WaitGroup).Wait(0xc00566a610)\nNov 05 17:25:00 hostname.example.com telegraf[839584]:         /usr/local/go/src/sync/waitgroup.go:206 +0x85\nNov 05 17:25:00 hostname.example.com telegraf[839584]: github.com/influxdata/telegraf/agent.(*Agent).Run(0xc001794010, {0xb877710, 0xc00075be50})\nNov 05 17:25:00 hostname.example.com telegraf[839584]:         /go/src/github.com/influxdata/telegraf/agent/agent.go:208 +0xb6a\nNov 05 17:25:00 hostname.example.com telegraf[839584]: main.(*Telegraf).runAgent(0xc001aa0000, {0xb877710, 0xc00075be50}, 0x0?)\nNov 05 17:25:00 hostname.example.com telegraf[839584]:         /go/src/github.com/influxdata/telegraf/cmd/telegraf/telegraf.go:565 +0x19e5\nNov 05 17:25:00 hostname.example.com telegraf[839584]: main.(*Telegraf).reloadLoop(0xc001aa0000) \nNov 05 17:25:00 hostname.example.com telegraf[839584]:         /go/src/github.com/influxdata/telegraf/cmd/telegraf/telegraf.go:207 +0x26b\nNov 05 17:25:00 hostname.example.com telegraf[839584]: main.(*Telegraf).Run(0xc001aa0000)\nNov 05 17:25:00 hostname.example.com telegraf[839584]:         /go/src/github.com/influxdata/telegraf/cmd/telegraf/telegraf_posix.go:20 +0xb8\nNov 05 17:25:00 hostname.example.com telegraf[839584]: main.runApp.func1(0xc000b20800)\nNov 05 17:25:00 hostname.example.com telegraf[839584]:         /go/src/github.com/influxdata/telegraf/cmd/telegraf/main.go:261 +0xdc9\nNov 05 17:25:00 hostname.example.com telegraf[839584]: github.com/urfave/cli/v2.(*Command).Run(0xc001c9c580, 0xc000b20800, {0xc0001ec000, 0x5, 0x5})\nNov 05 17:25:00 hostname.example.com telegraf[839584]:         /go/pkg/\nNov 05 17:25:00 hostname.example.com telegraf[839584]: 2025-11-05T22:25:00Z E! PLEASE REPORT THIS PANIC ON GITHUB with stack trace, configuration, and OS information: https://github.com/infl\nuxdata/telegraf/issues/new/choose\n```\n\n### System info\n\nTelegraf 1.36.3-1 Debian 13.1\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1.  Upgrade telegraf to 1.36.3-1\n2. \n3.\n...\n\n\n### Expected behavior\n\nIt shouldn't panic\n\n### Actual behavior\n\nTelegraf panicks every interval that the zfs plugin is set to run on, and telegraf restarts.\n\n### Additional info\n\nI upgraded telegraf from 1.36.2-1 to 1.36.3-1, and it crashed after restarting.\nAfter reading through recent changes to the ZFS plugin I added the `  useNativeTypes = true` option to the config file, but it had no effect, telegraf still crashed.\nDowngrading to 1.36.2-1 resolved the crashing.",
      "solution": "@acranox please test the binary in PR #17953 and let me know if this fixes the issue!\n\n---\n\n@srebhan Aha, so the problem was self-inflicted by disabling the kstatmetrics!  \ud83d\ude06 \nThe binary in the PR is working for me.  It starts up normally, doesn't crash, and is successfully collecting ZFS metrics.\nThanks for solving that so quickly.  I'm guessing you don't need  `/proc/spl/kstat/zfs` anymore, but if it would still be helpful, let me know and I'll collect that for you.",
      "labels": [
        "bug"
      ],
      "created_at": "2025-11-05T22:30:04Z",
      "closed_at": "2025-11-07T22:16:16Z",
      "url": "https://github.com/influxdata/telegraf/issues/17952",
      "comments_count": 4
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17910,
      "title": "v1.35.4 inputs.http_response panic: <memcall> could not  acquire lock",
      "problem": "### Relevant telegraf.conf\n\n```toml\n[[inputs.http_response]]\n  urls = [\"https://<node1>:5480/api/1.0.0/nodes\",\"https://<node2>:5480/api/1.0.0/nodes\"]\n  username = \"$USER\"\n  password = \"$PWD\"\n  insecure_skip_verify = true\n  response_body_field = \"body\"\n```\n\n### Logs from Telegraf\n\n```text\n2025-10-30T06:50:54Z E! FATAL: [inputs.http_response::xxxxxxx] panicked: <memcall> could not acquire lock on 0x7f1ceb1a6000, limit reached? [Err: cannot allocate memory], Stack:goroutine 269507 [running]:\ngithub.com/influxdata/telegraf/agent.panicRecover(0xc0017ab740)\n        /go/src/github.com/influxdata/telegraf/agent/agent.go:1202 +0x6d\npanic({0x920f100?, 0xc006187790?})\n        /usr/local/go/src/runtime/panic.go:792 +0x132\ngithub.com/awnumar/memguard/core.Panic(...)\n        /go/pkg/mod/github.com/awnumar/memguard@v0.22.5/core/exit.go:86\ngithub.com/awnumar/memguard/core.NewBuffer(0x4)\n        /go/pkg/mod/github.com/awnumar/memguard@v0.22.5/core/buffer.go:73 +0x51a\ngithub.com/awnumar/memguard/core.Open(0xc0005e7050)\n        /go/pkg/mod/github.com/awnumar/memguard@v0.22.5/core/enclave.go:109 +0x25\ngithub.com/awnumar/memguard.(*Enclave).Open(0xc001aaa4a0?)\n        /go/pkg/mod/github.com/awnumar/memguard@v0.22.5/enclave.go:43 +0x1a\ngithub.com/influxdata/telegraf/config.(*protectedSecretContainer).Buffer(0x1?)\n        /go/src/github.com/influxdata/telegraf/config/secret_protected.go:117 +0x25\ngithub.com/influxdata/telegraf/config.(*Secret).Get(0xc0001f85c8)\n        /go/src/github.com/influxdata/telegraf/config/secret.go:193 +0x64\ngithub.com/influxdata/telegraf/plugins/inputs/http_response.(*HTTPResponse).setRequestAuth(0xc0001f8508, 0xc0004c37c0)\n        /go/src/github.com/influxdata/telegraf/plugins/inputs/http_response/http_response.go:459 +0x65\ngithub.com/influxdata/telegraf/plugins/inputs/http_response.(*HTTPResponse).httpGather(0xc0001f8508, {{0xb71e7e0?, 0xc00195911\n0?}, {0xc001931901?, 0x38?}})\n        /go/src/github.com/influxdata/telegraf/plugins/inputs/http_response/http_response.go:345 +0x82a\ngithub.com/influxdata/telegraf/plugins/inputs/http_response.(*HTTPResponse).Gather(0xc0001f8508, {0xb817000, 0xc003465d00})\n        /go/src/github.com/influxdata/telegraf/plugins/inputs/http_response/http_response.go:133 +0x74\ngithub.com/influxdata/telegraf/models.(*RunningInput).Gather(0xc0017ab740, {0xb817000, 0xc003465d00})\n        /go/src/github.com/influxdata/telegraf/models/running_input.go:260 +0x251\ngithub.com/influxdata/telegraf/agent.(*Agent).gatherOnce.func1()\n        /go/src/github.com/influxdata/telegraf/agent/agent.go:590 +0x58\ncreated by github.com/influxdata/teleg\n2025-10-30T06:51:00Z E! PLEASE REPORT THIS PANIC ON GITHUB with stack trace, configuration, and OS information: https://github.com/influxdata/telegraf/issues/new/choose\n```\n\n### System info\n\nTelegraf 1.35.4 on Red Hat Enterprise Linux 8.1\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1. Add above config to telegraf.d folder\n2. Run telegraf systemd service\n3. Observe panic error in telegraf's log file\n\n\n### Expected behavior\n\nHandle errors without panics\n\n### Actual behavior\n\npanic\n\n### Additional info\n\nAlso observed in previous logs while using Telegraf v1.34.3.",
      "solution": "True. Increasing memlock parameter solved the issue. Thank you @srebhan ",
      "labels": [
        "bug"
      ],
      "created_at": "2025-10-30T18:16:32Z",
      "closed_at": "2025-11-07T09:00:00Z",
      "url": "https://github.com/influxdata/telegraf/issues/17910",
      "comments_count": 3
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17204,
      "title": "Telegraf appends _value to all my metrics",
      "problem": "### Relevant telegraf.conf\n\n```toml\n[agent]\n  logfile = \"/var/log/telegraf/telegraf.log\"\n\n[[inputs.exec]]\n  commands = [\"/usr/bin/python3 collector.py\"]\n  data_format = \"xpath_json\"\n  timeout = \"29s\"\n  interval = \"30s\"\n  [[inputs.exec.xpath]]\n    metric_selection = \"/*\"\n    metric_name = \"name\"\n    field_selection = \"value\"\n    tag_selection = \"tags/*\"\n\n[[processors.converter]]\n  namepass = [\"*\"]\n  [processors.converter.fields]\n    float = [\"value\"]\n\n[[outputs.opentelemetry]]\n  service_address = \"127.0.0.1:4317\"\n```\n\n### Logs from Telegraf\n\n```text\n2025-06-17T17:27:13Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\n2025-06-17T17:27:16Z D! [parsers.xpath_json::exec] Number of configs: 1\n2025-06-17T17:27:16Z D! [parsers.xpath_json::exec] Number of selected metric nodes: 726\n2025-06-17T17:27:16Z D! [parsers.xpath_json::exec] Number of selected tag nodes: 0\n2025-06-17T17:27:16Z D! [parsers.xpath_json::exec] Number of selected field nodes: 1\n...\n2025-06-17T17:27:16Z D! [parsers.xpath_json::exec] Number of selected tag nodes: 2\n2025-06-17T17:27:16Z D! [parsers.xpath_json::exec] Number of selected field nodes: 1\n2025-06-17T17:27:23Z D! [outputs.opentelemetry] Received 726 metrics and split into 1 groups by timestamp\n2025-06-17T17:27:23Z D! [outputs.file] Wrote batch of 726 metrics in 28.938597ms\n2025-06-17T17:27:23Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\n2025-06-17T17:27:23Z D! [outputs.opentelemetry] Wrote batch of 726 metrics in 34.88422ms\n2025-06-17T17:27:23Z D! [outputs.opentelemetry] Buffer fullness: 0 / 10000 metrics\n```\n\n### System info\n\nTelegraf: 1.34.2\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1. My script output:\n```\n[\n    {\n        \"name\": \"errors.bolts.failed\",\n        \"value\": 0.0,\n        \"tags\": {\n            \"bolt\": \"bol1\",\n            \"host\": \"host1\"\n        }\n    },\n    {\n        \"name\": \"latency.bolts.process.latency\",\n        \"value\": 0.2,\n        \"tags\": {\n            \"bolt\": \"bol2\",\n            \"host\": \"host2\"\n        }\n    },\n    {\n        \"name\": \"traffic.bolts.transferred\",\n        \"value\": 0.5,\n        \"tags\": {\n            \"bolt\": \"bol3\",\n            \"host\": \"host3\"\n        }\n    }\n]\n```\n2. run telegraf with provided config.\n\n\n### Expected behavior\n\nOn SignalFX I wanted to have metric name 'errors.bolts.failed', 'latency.bolts.process.latency' and 'traffic.bolts.transferred'\n\n### Actual behavior\n\nWhat I actual have is 'errors.bolts.failed_value', 'latency.bolts.process.latency_value' and 'traffic.bolts.transferred_value'\nI have tried with json and the behaviour is the same.\n\n### Additional info\n\nIs there a way to tell telegraf to not add that '_value' suffix before sending to SignalFX?",
      "solution": "The issue you're experiencing is due to how Telegraf's `xpath_json` parser handles field naming. When you specify `field_selection = \"value\"`, Telegraf creates a field called \"value\" and then appends it to your metric name with an underscore, resulting in names like `errors.bolts.failed_value`.\n\nUse the `rename` processor to remove the `_value` suffix, add this processor to your config after the converter:\n\n```\n[[processors.converter]]\n  namepass = [\"*\"]\n  [processors.converter.fields]\n    float = [\"value\"]\n\n[[processors.rename]]\n  namepass = [\"*\"]\n  [[processors.rename.replace]]\n    measurement = \"*\"\n    dest = \"$1\"\n    pattern = \"^(.*)_value$\"\n\n[[outputs.opentelemetry]]\n  service_address = \"127.0.0.1:4317\"\n```\nThis will strip the `_value` suffix from all your metric names before they're sent to your output.\n\n---\n\nI have tried but got this error, I will keep searching for something similar with your solution\n\n`2025-06-18T10:36:49Z E! loading config file /etc/telegraf/telegraf.conf failed: plugin processors.rename: line 17: configuration specified the fields [\"pattern\"], but they were not used. This is either a typo or this config option does not exist in this version.`\n\n---\n\nI have tested both and the result remains the same, we think the issue is in the output, somehow it is adding \"_<value_name>\" to the metric.\n\nif I run with `telegraf --config /etc/telegraf/telegraf.conf --test --input-filter exec`\n```\n> traffic.bolts.executed,bolt=bolt1,host=host1 value=0 1750255053000000000\n> errors.bolts.failed,bolt=bolt2,host=host2 value=0 1750255053000000000\n> latency.bolts.process.latency,bolt=bolt3,host=host3 value=0 1750255053000000000\n```\n\nIf you check the output above the metric does not have the \"_value\" here, only on SignalFX.\n\nThere should be a flag to disable appending \"_value\", but it would only work with 1 field",
      "labels": [
        "bug"
      ],
      "created_at": "2025-06-17T17:36:11Z",
      "closed_at": "2025-11-06T12:12:35Z",
      "url": "https://github.com/influxdata/telegraf/issues/17204",
      "comments_count": 8
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17914,
      "title": "[inputs.sql] No metrics from some queries",
      "problem": "### Relevant telegraf.conf\n\n```toml\n# cat /etc/telegraf/telegraf.conf\n\n###############################################################################\n# AGENT SETTINGS\n###############################################################################\n[agent]\ncollection_jitter = \"0s\"\nflush_interval = \"60s\"\nflush_jitter = \"0s\"\ninterval = \"60s\"\nmetric_batch_size = 1000\nmetric_buffer_limit = 10000\nround_interval = true\ndebug = true\nquiet = false\n\n###############################################################################\n# INPUT PLUGINS\n###############################################################################\n[inputs.sql]\ndriver = \"postgres\"\ndsn = \"host=/run/postgresql user=monitoring dbname=test\"\n[[inputs.sql.query]]\nquery = \"SELECT current_database() AS db, schemaname, relname, pg_total_relation_size(relid) AS total_size_bytes FROM pg_statio_user_tables\"\ntag_columns_include = [\"db\", \"schemaname\", \"relname\"]\n[[inputs.sql.query]]\nquery = \"SELECT CASE WHEN pg_last_wal_receive_lsn() = pg_last_wal_replay_lsn() THEN 0 ELSE EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp())) END AS lag_in_seconds\"\n#query = \"SELECT 1 AS lag_in_seconds\"\nfield_columns_include = [\"lag_in_seconds\"]\n\n[inputs.system]\n\n###############################################################################\n# OUTPUT PLUGINS\n###############################################################################\n[outputs.prometheus_client]\nexpiration_interval = \"0s\"\nlisten = \":9273\"\nmetric_version = 2\npath = \"/metrics\"\ncollectors_exclude = [\"gocollector\", \"process\"]\n```\n\n### Logs from Telegraf\n\n```text\nOct 24 08:09:20 test.mydomain.com systemd[1]: Starting telegraf.service - Telegraf...\n\u2591\u2591 Subject: A start job for unit telegraf.service has begun execution\n\u2591\u2591 Defined-By: systemd\n\u2591\u2591 Support: https://www.debian.org/support\n\u2591\u2591\n\u2591\u2591 A start job for unit telegraf.service has begun execution.\n\u2591\u2591\n\u2591\u2591 The job identifier is 618791.\nOct 24 08:09:20 test.mydomain.com telegraf[418577]: 2025-10-24T08:09:20Z I! Loading config: /etc/telegraf/telegraf.conf\nOct 24 08:09:20 test.mydomain.com telegraf[418577]: 2025-10-24T08:09:20Z I! Starting Telegraf 1.36.3 brought to you by InfluxData the makers of InfluxDB\nOct 24 08:09:20 test.mydomain.com telegraf[418577]: 2025-10-24T08:09:20Z I! Available plugins: 239 inputs, 9 aggregators, 35 processors, 26 parsers, 65 outputs, 6 secret-stores\nOct 24 08:09:20 test.mydomain.com telegraf[418577]: 2025-10-24T08:09:20Z I! Loaded inputs: sql system\nOct 24 08:09:20 test.mydomain.com telegraf[418577]: 2025-10-24T08:09:20Z I! Loaded aggregators:\nOct 24 08:09:20 test.mydomain.com telegraf[418577]: 2025-10-24T08:09:20Z I! Loaded processors:\nOct 24 08:09:20 test.mydomain.com telegraf[418577]: 2025-10-24T08:09:20Z I! Loaded secretstores:\nOct 24 08:09:20 test.mydomain.com telegraf[418577]: 2025-10-24T08:09:20Z I! Loaded outputs: prometheus_client\nOct 24 08:09:20 test.mydomain.com telegraf[418577]: 2025-10-24T08:09:20Z I! Tags enabled: host=test.mydomain.com\nOct 24 08:09:20 test.mydomain.com telegraf[418577]: 2025-10-24T08:09:20Z I! [agent] Config: Interval:1m0s, Quiet:false, Hostname:\"test.mydomain.com\", Flush Interval:1m0s\nOct 24 08:09:20 test.mydomain.com telegraf[418577]: 2025-10-24T08:09:20Z W! [agent] The default value of 'skip_processors_after_aggregators' will change to 'true' with Telegraf v1.40.0! If you need the current default behavior, please explicitly>\nOct 24 08:09:20 test.mydomain.com telegraf[418577]: 2025-10-24T08:09:20Z D! [agent] Initializing plugins\nOct 24 08:09:20 test.mydomain.com systemd[1]: Started telegraf.service - Telegraf.\n\u2591\u2591 Subject: A start job for unit telegraf.service has finished successfully\n\u2591\u2591 Defined-By: systemd\n\u2591\u2591 Support: https://www.debian.org/support\n\u2591\u2591\n\u2591\u2591 A start job for unit telegraf.service has finished successfully.\n\u2591\u2591\n\u2591\u2591 The job identifier is 618791.\nOct 24 08:09:20 test.mydomain.com telegraf[418577]: 2025-10-24T08:09:20Z D! [agent] Connecting outputs\nOct 24 08:09:20 test.mydomain.com telegraf[418577]: 2025-10-24T08:09:20Z D! [agent] Attempting connection to [outputs.prometheus_client]\nOct 24 08:09:20 test.mydomain.com telegraf[418577]: 2025-10-24T08:09:20Z I! [outputs.prometheus_client] Listening on http://[::]:9273/metrics\nOct 24 08:09:20 test.mydomain.com telegraf[418577]: 2025-10-24T08:09:20Z D! [agent] Successfully connected to outputs.prometheus_client\nOct 24 08:09:20 test.mydomain.com telegraf[418577]: 2025-10-24T08:09:20Z D! [agent] Starting service inputs\nOct 24 08:09:20 test.mydomain.com telegraf[418577]: 2025-10-24T08:09:20Z D! [inputs.sql] Connecting...\nOct 24 08:09:20 test.mydomain.com telegraf[418577]: 2025-10-24T08:09:20Z D! [inputs.sql] Testing connectivity...\nOct 24 08:09:20 test.mydomain.com telegraf[418577]: 2025-10-24T08:09:20Z D! [inputs.sql] Preparing statement \"SELECT current_database() AS db, schemaname, relname, pg_total_relation_size(relid) AS total_size_bytes FROM pg_statio_user_tables\"...\nOct 24 08:09:20 test.mydomain.com telegraf[418577]: 2025-10-24T08:09:20Z D! [inputs.sql] Preparing statement \"SELECT CASE WHEN pg_last_wal_receive_lsn() = pg_last_wal_replay_lsn() THEN 0 ELSE EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_times>\nOct 24 08:10:00 test.mydomain.com telegraf[418577]: 2025-10-24T08:10:00Z D! [inputs.sql] Received 1 rows and 1 columns for query \"SELECT CASE WHEN pg_last_wal_receive_lsn() = pg_last_wal_replay_lsn() THEN 0 ELSE EXTRACT(EPOCH FROM (now() - pg_la>\nOct 24 08:10:00 test.mydomain.com telegraf[418577]: 2025-10-24T08:10:00Z D! [inputs.sql] Received 65 rows and 4 columns for query \"SELECT current_database() AS db, schemaname, relname, pg_total_relation_size(relid) AS total_size_bytes FROM pg_st>\nOct 24 08:10:00 test.mydomain.com telegraf[418577]: 2025-10-24T08:10:00Z D! [inputs.sql] Executed 2 queries in 25.086882ms\nOct 24 08:10:20 test.mydomain.com telegraf[418577]: 2025-10-24T08:10:20Z D! [outputs.prometheus_client] Wrote batch of 69 metrics in 234.441\u00b5s\nOct 24 08:10:20 test.mydomain.com telegraf[418577]: 2025-10-24T08:10:20Z D! [outputs.prometheus_client] Buffer fullness: 0 / 10000 metrics\nOct 24 08:11:00 test.mydomain.com telegraf[418577]: 2025-10-24T08:11:00Z D! [inputs.sql] Received 1 rows and 1 columns for query \"SELECT CASE WHEN pg_last_wal_receive_lsn() = pg_last_wal_replay_lsn() THEN 0 ELSE EXTRACT(EPOCH FROM (now() - pg_la>\nOct 24 08:11:00 test.mydomain.com telegraf[418577]: 2025-10-24T08:11:00Z D! [inputs.sql] Received 65 rows and 4 columns for query \"SELECT current_database() AS db, schemaname, relname, pg_total_relation_size(relid) AS total_size_bytes FROM pg_st>\nOct 24 08:11:00 test.mydomain.com telegraf[418577]: 2025-10-24T08:11:00Z D! [inputs.sql] Executed 2 queries in 10.68146ms\nOct 24 08:11:20 test.mydomain.com telegraf[418577]: 2025-10-24T08:11:20Z D! [outputs.prometheus_client] Wrote batch of 69 metrics in 566.844\u00b5s\nOct 24 08:11:20 test.mydomain.com telegraf[418577]: 2025-10-24T08:11:20Z D! [outputs.prometheus_client] Buffer fullness: 0 / 10000 metrics\n```\n\n### System info\n\ntelegraf - 1.36.3-1, debian - 12, kernel - 6.1.140-1, postgres - 16.9-1.pgdg120\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1. setup postgres replication\n2. setup telegraf with same config on replica\n3. check if sql_lag_in_seconds exist \n...\n\n\n### Expected behavior\n\n```\n# curl http://localhost:9273/metrics\n# HELP sql_lag_in_seconds Telegraf collected metric\n# TYPE sql_lag_in_seconds untyped\nsql_lag_in_seconds{host=\"test.mydomain.com\"} 1\n```\n\n\n\n### Actual behavior\n\nno sql_lag_in_seconds metrics\n\n### Additional info\n\npostgres query\n```\n# SELECT CASE WHEN pg_last_wal_receive_lsn() = pg_last_wal_replay_lsn() THEN 0 ELSE EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp())) END AS lag_in_seconds;\n lag_in_seconds\n----------------\n              0\n(1 row)\n```\n\nWith query = \"SELECT CASE WHEN pg_last_wal_receive_lsn() = pg_last_wal_replay_lsn() THEN 0 ELSE EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp())) END AS lag_in_seconds\"\n```\n# curl http://localhost:9273/metrics\n# HELP sql_total_size_bytes Telegraf collected metric\n# TYPE sql_total_size_bytes untyped\nsql_total_size_bytes{db=\"test\",host=\"test.mydomain.com\",relname=\"table1\",schemaname=\"public\"} 106496\nsql_total_size_bytes{db=\"test\",host=\"test.mydomain.com\",relname=\"table2\",schemaname=\"public\"} 483328\nsql_total_size_bytes{db=\"test\",host=\"test.mydomain.com\",relname=\"table3\",schemaname=\"public\"} 73728\n# HELP system_load1 Telegraf collected metric\n# TYPE system_load1 gauge\nsystem_load1{host=\"test.mydomain.com\"} 0.17\n# HELP system_load15 Telegraf collected metric\n# TYPE system_load15 gauge\nsystem_load15{host=\"test.mydomain.com\"} 0.15\n# HELP system_load5 Telegraf collected metric\n# TYPE system_load5 gauge\nsystem_load5{host=\"test.mydomain.com\"} 0.2\n# HELP system_n_cpus Telegraf collected metric\n# TYPE system_n_cpus gauge\nsystem_n_cpus{host=\"test.mydomain.com\"} 128\n# HELP system_n_unique_users Telegraf collected metric\n# TYPE system_n_unique_users gauge\nsystem_n_unique_users{host=\"test.mydomain.com\"} 1\n# HELP system_n_users Telegraf collected metric\n# TYPE system_n_users gauge\nsystem_n_users{host=\"test.mydomain.com\"} 1\n# HELP system_uptime Telegraf collected metric\n# TYPE system_uptime counter\nsystem_uptime{host=\"test.mydomain.com\"} 1.0558808e+07\n```\n\nWith query = \"SELECT 1 AS lag_in_seconds\"\n```\n# curl http://localhost:9273/metrics\n# HELP sql_lag_in_seconds Telegraf collected metric\n# TYPE sql_lag_in_seconds untyped\nsql_lag_in_seconds{host=\"test.mydomain.com\"} 1\n# HELP sql_total_size_bytes Telegraf collected metric\n# TYPE sql_total_size_bytes untyped\nsql_total_size_bytes{db=\"test\",host=\"test.mydomain.com\",relname=\"table1\",schemaname=\"public\"} 106496\nsql_total_size_bytes{db=\"test\",host=\"test.mydomain.com\",relname=\"table2\",schemaname=\"public\"} 483328\nsql_total_size_bytes{db=\"test\",host=\"test.mydomain.com\",relname=\"table3\",schemaname=\"public\"} 73728\n# HELP system_load1 Telegraf collected metric\n# TYPE system_load1 gauge\nsystem_load1{host=\"test.mydomain.com\"} 0.17\n# HELP system_load15 Telegraf collected metric\n# TYPE system_load15 gauge\nsystem_load15{host=\"test.mydomain.com\"} 0.15\n# HELP system_load5 Telegraf collected metric\n# TYPE system_load5 gauge\nsystem_load5{host=\"test.mydomain.com\"} 0.2\n# HELP system_n_cpus Telegraf collected metric\n# TYPE system_n_cpus gauge\nsystem_n_cpus{host=\"test.mydomain.com\"} 128\n# HELP system_n_unique_users Telegraf collected metric\n# TYPE system_n_unique_users gauge\nsystem_n_unique_users{host=\"test.mydomain.com\"} 1\n# HELP system_n_users Telegraf collected metric\n# TYPE system_n_users gauge\nsystem_n_users{host=\"test.mydomain.com\"} 1\n# HELP system_uptime Telegraf collected metric\n# TYPE system_uptime counter\nsystem_uptime{host=\"test.mydomain.com\"} 1.0559588e+07\n```\n",
      "solution": "@srebhan thanks, fixed with \n```\n[[processors.converter]]\n  [processors.converter.fields]\n    integer = [\"lag_in_seconds\"]\n```",
      "labels": [
        "bug"
      ],
      "created_at": "2025-10-31T15:20:26Z",
      "closed_at": "2025-11-04T13:53:17Z",
      "url": "https://github.com/influxdata/telegraf/issues/17914",
      "comments_count": 4
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17871,
      "title": "[[inputs.kinesis_consumer]] Kinesis Consumer does not consume messages since v1.34.0",
      "problem": "### Relevant telegraf.conf\n\n```toml\n[agent]\n  debug = true\n  quiet = false\n  metric_batch_size = 5000\n  metric_buffer_limit = 50000\n\n[[inputs.kinesis_consumer]]\n  region = \"${AWS_REGION}\"\n  streamname = \"${KINESIS_STREAM_NAME}\"\n  role_arn = \"${KINESIS_ROLE_ARN}\"\n  data_format = \"value\"\n  data_type = \"string\"\n\n  [inputs.kinesis_consumer.checkpoint_dynamodb]\n    app_name = \"${KINESIS_CHECKPOINT_DYNAMODB_APP_NAME}\"\n    table_name = \"${KINESIS_CHECKPOINT_DYNAMODB_TABLE_NAME}\"\n```\n\n### Logs from Telegraf\n\n```text\nI! Loading config: /etc/telegraf/telegraf.conf\nI! Starting Telegraf 1.34.0 brought to you by InfluxData the makers of InfluxDB\nI! Available plugins: 239 inputs, 9 aggregators, 33 processors, 26 parsers, 63 outputs, 6 secret-stores\nI! Loaded inputs: kinesis_consumer\nI! Loaded aggregators:\nI! Loaded processors: starlark\nI! Loaded secretstores:\nI! Loaded outputs: file\nI! Tags enabled: host=0df117d65da4\nI! [agent] Config: Interval:10s, Quiet:false, Hostname:\"0df117d65da4\", Flush Interval:10s\nW! [agent] The default value of 'skip_processors_after_aggregators' will change to 'true' with Telegraf v1.40.0! If you need the current default behavior, please explicitly set the option to 'false'!\nD! [agent] Initializing plugins\nD! [agent] Connecting outputs\nD! [agent] Attempting connection to [outputs.file]\nD! [agent] Successfully connected to outputs.file\nD! [agent] Starting service inputs\nD! [outputs.file] Buffer fullness: 0 / 50000 metrics\nD! [outputs.file] Buffer fullness: 0 / 50000 metrics\nD! [outputs.file] Buffer fullness: 0 / 50000 metrics\n```\n\n### System info\n\nTelegraf 1.34.0, Docker on MacOS and/or ECS Fargate\n\n### Docker\n\nThe following PR appears to have broken the Kinesis Consumer: [influxdata/telegraf#16332](https://github.com/influxdata/telegraf/pull/16332)\n\nIn our environment, we are using a Kinesis Data Stream configured in On-Demand mode. Starting from Telegraf v1.34.0, the Kinesis Consumer does not consume messages from streams that have approximately 20 shards. No errors are reported, even with debug logging enabled.\n\nThe consumer appears to work correctly with streams containing fewer than 5 shards.\n\nWhen downgrading to v1.33.3 - everything works as expected.\n\n### Steps to reproduce\n\n1. Create a Kinesis Data Stream in On-Demand mode.\n2. Increase the number of shards (e.g., ~20).\n3. Configure Telegraf with the Kinesis Consumer plugin.\n4. Start Telegraf and monitor the logs.\n\n### Expected behavior\n\nMessages should be consumed from a stream\n\n### Actual behavior\n\nMessages are not consumed from a stream\n\n### Additional info\n\n_No response_",
      "solution": "Thanks @andriyfedorov for the prompt delivery! I found the reason for the weird behavior...\n\nAfter investigating the log above, I realized that resharding took place, thus the `has parent shard` messages. However, none of the parent shards where among the received list of shards from Kinesis. I then found out that _all_ of the parent shards seem to be expired and are therefore not consumable (see [visualization](https://github.com/user-attachments/files/23096912/relations.gv.pdf) with grey nodes are expired nodes).\n\nThe current code assumes that the parent shards are not yet expired and ignore the child shard to start at the earliest message. However, if the parent is expired it will never appear in the processing and therefore none of the shards is consumed as all children are ignored and the parent does not appear.\n\nPR #17877 adresses the issue. Please test the binary in PR #17877, available after CI finished the tests, and let me know if this fixes the issue!",
      "labels": [
        "bug"
      ],
      "created_at": "2025-10-22T03:22:58Z",
      "closed_at": "2025-11-03T11:18:33Z",
      "url": "https://github.com/influxdata/telegraf/issues/17871",
      "comments_count": 5
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 12313,
      "title": "inputs.statsd - Increasing number of UDP pending messages dropped",
      "problem": "### Please direct all support questsions to slack or the forums. Thank you.\n\nOpening up this ticket in response to discussing the issue on slack.  The issue is that despite setting allowed pending messages to a very large number like 1.5M, we're still dropping UDP packages and unsure why.  There was the suggestion that perhaps there were not enough parsers available to drain the channel before packets are dropped.  It seems that once about 500k messages are received we start dropping packets.\r\n<img width=\"1023\" alt=\"image\" src=\"https://user-images.githubusercontent.com/2592630/204909912-1e645bf8-9b50-4462-a233-900049bf3da2.png\">\r\n",
      "solution": "Could any sort of backpressure from flushing to influx cause issues with ingestion? I know at one time there was a bug with that, but I thought it was fixed and did not block each other.\n\n---\n\nHi, \n\nWe faced this same problem where telegraf is unable to keep up fast enough with StatsD input. The system where it is running is only using ~25% of CPU and also 25% of system load.\n\nAfter some research and profiling we found that the main reason preventing the input to fully use the available resources is due to huge mutex contention. I'll try to explain it:\n https://github.com/influxdata/telegraf/blob/master/plugins/inputs/statsd/statsd.go#L98 <- this mutex is used for multiple purposes (during each metric parsing and also for aggregating).\n\n<img width=\"1364\" height=\"167\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b749e628-f529-4554-af4c-cbfa3b01628b\" />\n\nThe only reason every metric parsing needs the lock is because the template engine in the `graphite.Parser` https://github.com/influxdata/telegraf/blob/master/plugins/inputs/statsd/statsd.go#L129 modifies itself when applying a template and since it's running multiple goroutines, without the lock you get data races.\n\nThe problem can be fixed with two approaches:\n\nCreating a new `graphite.Parser` here https://github.com/influxdata/telegraf/blob/master/plugins/inputs/statsd/statsd.go#L798-L805 and removing the lock usage https://github.com/influxdata/telegraf/blob/master/plugins/inputs/statsd/statsd.go#L770-L771\n\nOR\n\nMoving the `graphite.Parser` from the struct to one in each parser goroutine stack (and passing it across the different functions in the call stack) and removing the lock usage https://github.com/influxdata/telegraf/blob/master/plugins/inputs/statsd/statsd.go#L770-L771\n\n\nFor the second approach this is what a profile comparison looks like:\n<img width=\"1496\" height=\"177\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/4dac3a0b-32fa-4a50-a867-7da6fb41e2e7\" />\n",
      "labels": [
        "performance",
        "area/statsd",
        "plugin/input"
      ],
      "created_at": "2022-11-30T21:18:28Z",
      "closed_at": "2025-10-30T16:46:39Z",
      "url": "https://github.com/influxdata/telegraf/issues/12313",
      "comments_count": 21
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17870,
      "title": "[[input.kafka_consumer]] Error ApiVersionsRequest must be disabled when SASL v0 is enabled in Telegraf 1.36.2",
      "problem": "\nAfter updating Telegraf from 1.36.1 to 1.36.2 we have found this issue. Basically, the error is related with the updated Sarama lib to version 1.46.1\n\nSarama Library now includes a new validation method and we don't have control of the `ApiVersionsRequest` var from Telegraf SASL config. https://github.com/IBM/sarama/blob/9bc3d146bc3cb1adb31d11a1447867be813beb24/config.go#L658\n\n### Relevant telegraf.conf\n\n```toml\n[[inputs.kafka_consumer]]\n   brokers = [\"kafka:9092\"]\n   topics = [\"alarm-stream1\"]\n   enable_tls = true\n   insecure_skip_verify = true\n   sasl_username = \"broker\"\n   sasl_password = \"broker-secret\"\n   consumer_group = \"alarm-stream1-group\"\n   metadata_full = false\n```\n\n### Logs from Telegraf\n\n```text\n2025-10-21T16:35:38Z I! Loading config: /etc/telegraf/telegraf_kafka.conf\n2025-10-21T16:35:38Z I! Starting Telegraf 1.36.2 brought to you by InfluxData the makers of InfluxDB\n2025-10-21T16:35:38Z I! Available plugins: 239 inputs, 9 aggregators, 35 processors, 26 parsers, 65 outputs, 6 secret-stores\n2025-10-21T16:35:38Z I! Loaded inputs: kafka_consumer\n2025-10-21T16:35:38Z I! Loaded aggregators:\n2025-10-21T16:35:38Z I! Loaded processors:\n2025-10-21T16:35:38Z I! Loaded secretstores:\n2025-10-21T16:35:38Z W! Outputs are not used in testing mode!\n2025-10-21T16:35:38Z I! Tags enabled: host=6255da055354\n2025-10-21T16:35:38Z W! [agent] The default value of 'skip_processors_after_aggregators' will change to 'true' with Telegraf v1.40.0! If you need the current default behavior, please explicitly set the option to 'false'!\n2025-10-21T16:35:38Z D! [agent] Initializing plugins\n2025-10-21T16:35:38Z D! [agent] Starting service inputs\n2025-10-21T16:35:38Z E! [agent] Starting input inputs.kafka_consumer: create consumer: kafka: invalid configuration (ApiVersionsRequest must be disabled when SASL v0 is enabled)\n2025-10-21T16:35:38Z D! [agent] Stopping service inputs\n2025-10-21T16:35:38Z D! [agent] Input channel closed\n2025-10-21T16:35:38Z D! [agent] Stopped Successfully\n```\n\n### System info\n\nTelegraf 1.36.2\n\n### Expected behavior\n\nAgent working in Telegraf 1.36.2 when SASLHandshakeV0 is used.\n\n### Actual behavior\n\n`E! [agent] Starting input inputs.kafka_consumer: create consumer: kafka: invalid configuration (ApiVersionsRequest must be disabled when SASL v0 is enabled)`\n\n### Additional info\nAs a WA to  keep Telegraf working against SASLHandshakeV0 Kafka broker in my dev env, I've added  `api_version_request` boolean var  in https://github.com/influxdata/telegraf/blob/v1.36.3/plugins/common/kafka/sasl.go to disable Sarama ApiVersionsRequest via config and I've  also initialized Sarama ApiVersionsRequest with `cfg.ApiVersionsRequest  = k.ApiVersionRequest` in `(k *KafkaConsumer) Init() error` function from\nhttps://github.com/influxdata/telegraf/blob/70e4469a12addf4b5d8f34e93b704cd40e0d5433/plugins/inputs/kafka_consumer/kafka_consumer.go\n\nCould be possible to include this functionality on upcoming Telegraf versions?\n",
      "solution": "@carolo87 please test the binary in PR #17873, available as soon as CI finished the tests, and let me know if this fixes the issue for you!\n\n---\n\n@srebhan from the  [plugin documentation](https://docs.influxdata.com/telegraf/v1/input-plugins/kafka_consumer/)   I noticed that setting the `kafka_version` to match the version of the kafka broker fixed the issue for me, I've tested Telegraf 1.36.3 with Kafka 3.9.0, 3.9.1 and 4.0.0.",
      "labels": [
        "bug"
      ],
      "created_at": "2025-10-21T17:00:27Z",
      "closed_at": "2025-10-27T15:14:42Z",
      "url": "https://github.com/influxdata/telegraf/issues/17870",
      "comments_count": 4
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 16875,
      "title": "Allow UNIX timestamp parsing in a Timezone",
      "problem": "### Use Case\n\nI have a configuration:\n\n```toml\n    [[inputs.file]]                                                                                                                                                                                                                                                                                                           \n      files = [\"/config/data.csv\"]                                                                                                                                                                                                                                                                                        \n      csv_delimiter = \";\"                                                                                                                                                                                                                                                                                                     \n      csv_header_row_count = 1                                                                                                                                                                                                                                                                                                \n      csv_timestamp_column = \"(s)\"                                                                                                                                                                                                                                                                                            \n      csv_timestamp_format = \"unix\"                                                                                                                                                                                                                                                                                           \n      csv_timezone = \"Europe/Stockholm\"                                                                                                                                                                                                                                                                                         \n      data_format = \"csv\"                                                                                                                                                                                                                                                                                                     \n      interval = \"60s\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n```\n\nwhich is supposed to read a CSV file with metrics and ingest them.\n\nThe CSV file is below:\n\n```csv\n(s);Temp 1;Temp 2;Temp 3;Temp 4;Temp 5;Temp 6;Temp KB1 Tillopp;Temp 7;Temp 7 RH;Temp 8;Temp 9;Temp 10\n1745482082;22.6;23.2;23.2;22.0;23.2;24.5;10.3;20.9;19.0;20.2;23.0;21.7\n```\n\nThe reading part works well but there is an issue that the UNIX timestamp is not UTC but local time which causes the resulting metric timestamp to be in future.\n\nI see that the CSV parser has [parseTimestamp](https://github.com/influxdata/telegraf/blob/master/plugins/parsers/csv/parser.go#L473) function which calls [ParseTimestamp](https://github.com/influxdata/telegraf/blob/master/internal/internal.go#L263).\n\nThe `ParseTimestamp`  always parses UNIX timestamp in UTC ( [parseUnix](https://github.com/influxdata/telegraf/blob/master/internal/internal.go#L281) ).\n\nIf this is a necessary feature, I can try to provide a PR.\n\n### Expected behavior\n\nIt would be nice to parse UNIX timestamps with respect to the provided Timezone similar to how it is done for non-timestamp formats so that the resulting metric time is correct and the behavior for both unix, unix_ms, unix_ns, unix_us and custom time formats are the same.\n\n### Actual behavior\n\nUNIX timestamps are always parsed in UTC.\n\n### Additional info\n\nAn issue https://github.com/influxdata/telegraf/issues/12674 might be related.",
      "solution": "Hi @srebhan,\n\nI am sorry for long reaction time.\n\nYes, I know that the UNIX timestamp is in UTC.\n\nThe problem is that we have some interestint piece of hardware which allows getting its metrics data via CSV as above\nbut gives the timestamp almost as UNIX timestamp but in \"local time\".\nI suppose that the hardware just has no idea that such a thing as \"Timezone\" exists.\n\nSo, if I understand you well, if we can convert the timestamp to a formatted local time (with timezone info from the TZ database), I think, it will fix my problem.",
      "labels": [
        "feature request"
      ],
      "created_at": "2025-04-24T08:40:34Z",
      "closed_at": "2025-10-23T07:56:26Z",
      "url": "https://github.com/influxdata/telegraf/issues/16875",
      "comments_count": 8
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17578,
      "title": "ZFS Input plugin with zfs v2.3.4: strconv.ParseInt: parsing \"...\": value out of range",
      "problem": "### Relevant telegraf.conf\n\nSince updating proxmox to version 9, I see the following in the logs from my Telegraf inputs.zfs, which I installed on the hypervisor to monitor ZFS metrics:\n\n```log\nSep 08 19:35:50 parrot telegraf[2274]: 2025-09-08T17:35:50Z E! [inputs.zfs] Error in plugin: strconv.ParseInt: parsing \"18446744073709094537\": value out of range\n```\n\nI will add more info in a bit. Apologies for the short bug report in advance!\n\n### Logs from Telegraf\n\n```text\nSep 08 19:35:50 parrot telegraf[2274]: 2025-09-08T17:35:50Z E! [inputs.zfs] Error in plugin: strconv.ParseInt: parsing \"18446744073709094537\": value out of range\nSep 08 19:36:00 parrot telegraf[2274]: 2025-09-08T17:36:00Z E! [inputs.zfs] Error in plugin: strconv.ParseInt: parsing \"18446744073709094631\": value out of range\nSep 08 19:36:10 parrot telegraf[2274]: 2025-09-08T17:36:10Z E! [inputs.zfs] Error in plugin: strconv.ParseInt: parsing \"18446744073709094731\": value out of range\nSep 08 19:36:20 parrot telegraf[2274]: 2025-09-08T17:36:20Z E! [inputs.zfs] Error in plugin: strconv.ParseInt: parsing \"18446744073709094740\": value out of range\nSep 08 19:36:30 parrot telegraf[2274]: 2025-09-08T17:36:30Z E! [inputs.zfs] Error in plugin: strconv.ParseInt: parsing \"18446744073709094748\": value out of range\nSep 08 19:36:40 parrot telegraf[2274]: 2025-09-08T17:36:40Z E! [inputs.zfs] Error in plugin: strconv.ParseInt: parsing \"18446744073709094752\": value out of range\nSep 08 19:36:50 parrot telegraf[2274]: 2025-09-08T17:36:50Z E! [inputs.zfs] Error in plugin: strconv.ParseInt: parsing \"18446744073709094755\": value out of range\nSep 08 19:37:00 parrot telegraf[2274]: 2025-09-08T17:37:00Z E! [inputs.zfs] Error in plugin: strconv.ParseInt: parsing \"18446744073709094759\": value out of range\nSep 08 19:37:10 parrot telegraf[2274]: 2025-09-08T17:37:10Z E! [inputs.zfs] Error in plugin: strconv.ParseInt: parsing \"18446744073709094762\": value out of range\nSep 08 19:37:20 parrot telegraf[2274]: 2025-09-08T17:37:20Z E! [inputs.zfs] Error in plugin: strconv.ParseInt: parsing \"18446744073709094767\": value out of range\nSep 08 19:37:30 parrot telegraf[2274]: 2025-09-08T17:37:30Z E! [inputs.zfs] Error in plugin: strconv.ParseInt: parsing \"18446744073709094772\": value out of range\n```\n\n### System info\n\nDebian trixie, Telegraf 1.35.4-1 & 1.36.0-1\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\nTelegraf Config:\n```\n[agent]\n  interval = \"10s\"\n  round_interval = true\n  metric_batch_size = 1000\n  metric_buffer_limit = 10000\n  collection_jitter = \"0s\"\n  flush_interval = \"10s\"\n  flush_jitter = \"0s\"\n  precision = \"\"\n  hostname = \"\"\n  omit_hostname = false\n  logfile = \"\"\n\n[[inputs.zfs]]\n  ## ZFS kstat path. Ignored on FreeBSD\n  ## If not specified, then default is:\n  # kstatPath = \"/proc/spl/kstat/zfs\"\n\n  ## By default, telegraf gather all zfs stats\n  ## Override the stats list using the kstatMetrics array:\n  ## For FreeBSD, the default is:\n  # kstatMetrics = [\"arcstats\", \"zfetchstats\", \"vdev_cache_stats\"]\n  ## For Linux, the default is:\n  # kstatMetrics = [\"abdstats\", \"arcstats\", \"dnodestats\", \"dbufcachestats\",\n  #     \"dmu_tx\", \"fm\", \"vdev_mirror_stats\", \"zfetchstats\", \"zil\"]\n\n  ## By default, don't gather zpool stats\n  poolMetrics = true\n\n  ## By default, don't gather dataset stats\n  datasetMetrics = true\n```\n\n...\n\n\n### Expected behavior\n\nZFS plugin works without error\n\n### Actual behavior\n\nError is flooding the logs\n\n### Additional info\n\n_No response_",
      "solution": "@Sieboldianus and/or @Finkregh could you please provide the content of `/proc/spl/kstat/zfs` as a zip file or similar so I can reproduce the issue here? Tested on my ZFS server and I'm not seeing the issue on ArchLinux with ZFS 2.3.4 so I guess it is some metric in the files overflowing...\n\n---\n\n@Sieboldianus and @Finkregh please test the binary in PR #17617, available as soon as CI finished the tests, and let me know if this fixes the issue!\n\n> [!NOTE]\n> The issue arises from parsing the ProcFS fields as `int64` instead of the provided `uint64` so without any change you will see an overflow for the fields exceeding the `int64` value.\n\nTo use the correct type provided by ZFS you should set the new `useNativeTypes` setting to `true` in your config.\n\n> [!WARNING]\n> Setting `useNativeTypes` to `true` will change the field-type and might cause field-type conflicts if sent to an existing InfluxDB database!\n\n---\n\n@Finkregh or @Sieboldianus any chance to test the binary and let me know if this fixes the issue!?",
      "labels": [
        "bug"
      ],
      "created_at": "2025-09-08T17:40:53Z",
      "closed_at": "2025-10-20T18:57:53Z",
      "url": "https://github.com/influxdata/telegraf/issues/17578",
      "comments_count": 18
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17370,
      "title": "SMART Input returning exit status 2 for active drives but not when run manually from inside the container",
      "problem": "### Relevant telegraf.conf\n\n```toml\n# Prometheus output\n[[outputs.prometheus_client]]\n  listen = \":9273\"\n\n# SMART monitoring\n[[inputs.smart]]\n  path_smartctl = \"/usr/sbin/smartctl\"\n  nocheck = \"standby\"\n  attributes = true\n  devices = [\"/dev/sdb\", \"/dev/sde\", \"/dev/sdc\"]\n```\n\n### Logs from Telegraf\n\n```text\n2025-07-21T19:31:00Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/932511d76b14\"): no such file or directory\n2025-07-21T19:31:00Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/021e49962264\"): no such file or directory\n2025-07-21T19:31:00Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/0324553c4a51\"): no such file or directory\n2025-07-21T19:31:00Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/4b5c1f380496\"): no such file or directory\n2025-07-21T19:31:00Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/8a2b3b6d6f06\"): no such file or directory\n2025-07-21T19:31:00Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/dc93de697727\"): no such file or directory\n2025-07-21T19:31:00Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/272ed0de122f\"): no such file or directory\n2025-07-21T19:31:00Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/556ea498fb7b\"): no such file or directory\n2025-07-21T19:31:06Z D! [outputs.prometheus_client] Wrote batch of 131 metrics in 8.860129ms\n2025-07-21T19:31:06Z D! [outputs.prometheus_client] Buffer fullness: 0 / 10000 metrics\n2025-07-21T19:31:15Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/hugetlbfs\"): no such file or directory\n2025-07-21T19:31:15Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/mnt/disk1\"): no such file or directory\n2025-07-21T19:31:15Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/mnt/disk2\"): no such file or directory\n2025-07-21T19:31:15Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/mnt/cache\"): no such file or directory\n2025-07-21T19:31:15Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/mnt/user0\"): no such file or directory\n2025-07-21T19:31:15Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/mnt/user\"): no such file or directory\n2025-07-21T19:31:15Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/var/lib/docker\"): no such file or directory\n2025-07-21T19:31:15Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/var/lib/docker/btrfs\"): no such file or directory\n2025-07-21T19:31:15Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/8f116d6e20d2\"): no such file or directory\n2025-07-21T19:31:15Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/3bd80a3cfd1f\"): no such file or directory\n2025-07-21T19:31:15Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/etc/libvirt\"): no such file or directory\n2025-07-21T19:31:15Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/932511d76b14\"): no such file or directory\n2025-07-21T19:31:15Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/021e49962264\"): no such file or directory\n2025-07-21T19:31:15Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/0324553c4a51\"): no such file or directory\n2025-07-21T19:31:15Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/4b5c1f380496\"): no such file or directory\n2025-07-21T19:31:15Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/8a2b3b6d6f06\"): no such file or directory\n2025-07-21T19:31:15Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/dc93de697727\"): no such file or directory\n2025-07-21T19:31:15Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/272ed0de122f\"): no such file or directory\n2025-07-21T19:31:15Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/556ea498fb7b\"): no such file or directory\n2025-07-21T19:31:15Z D! [inputs.system] Reading users: open /var/run/utmp: no such file or directory\n2025-07-21T19:31:16Z D! [outputs.prometheus_client] Wrote batch of 79 metrics in 3.619699ms\n2025-07-21T19:31:16Z D! [outputs.prometheus_client] Buffer fullness: 0 / 10000 metrics\n2025-07-21T19:31:26Z D! [outputs.prometheus_client] Wrote batch of 52 metrics in 5.424221ms\n2025-07-21T19:31:26Z D! [outputs.prometheus_client] Buffer fullness: 0 / 10000 metrics\n2025-07-21T19:31:30Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/hugetlbfs\"): no such file or directory\n2025-07-21T19:31:30Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/mnt/disk1\"): no such file or directory\n2025-07-21T19:31:30Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/mnt/disk2\"): no such file or directory\n2025-07-21T19:31:30Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/mnt/cache\"): no such file or directory\n2025-07-21T19:31:30Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/mnt/user0\"): no such file or directory\n2025-07-21T19:31:30Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/mnt/user\"): no such file or directory\n2025-07-21T19:31:30Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/var/lib/docker\"): no such file or directory\n2025-07-21T19:31:30Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/var/lib/docker/btrfs\"): no such file or directory\n2025-07-21T19:31:30Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/8f116d6e20d2\"): no such file or directory\n2025-07-21T19:31:30Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/3bd80a3cfd1f\"): no such file or directory\n2025-07-21T19:31:30Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/etc/libvirt\"): no such file or directory\n2025-07-21T19:31:30Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/932511d76b14\"): no such file or directory\n2025-07-21T19:31:30Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/021e49962264\"): no such file or directory\n2025-07-21T19:31:30Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/0324553c4a51\"): no such file or directory\n2025-07-21T19:31:30Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/4b5c1f380496\"): no such file or directory\n2025-07-21T19:31:30Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/8a2b3b6d6f06\"): no such file or directory\n2025-07-21T19:31:30Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/dc93de697727\"): no such file or directory\n2025-07-21T19:31:30Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/272ed0de122f\"): no such file or directory\n2025-07-21T19:31:30Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/556ea498fb7b\"): no such file or directory\n2025-07-21T19:31:30Z D! [inputs.system] Reading users: open /var/run/utmp: no such file or directory\n2025-07-21T19:31:36Z D! [outputs.prometheus_client] Wrote batch of 131 metrics in 8.065137ms\n2025-07-21T19:31:36Z D! [outputs.prometheus_client] Buffer fullness: 0 / 10000 metrics\n2025-07-21T19:31:45Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/hugetlbfs\"): no such file or directory\n2025-07-21T19:31:45Z D! [inputs.system] Reading users: open /var/run/utmp: no such file or directory\n2025-07-21T19:31:45Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/mnt/disk1\"): no such file or directory\n2025-07-21T19:31:45Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/mnt/disk2\"): no such file or directory\n2025-07-21T19:31:45Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/mnt/cache\"): no such file or directory\n2025-07-21T19:31:45Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/mnt/user0\"): no such file or directory\n2025-07-21T19:31:45Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/mnt/user\"): no such file or directory\n2025-07-21T19:31:45Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/var/lib/docker\"): no such file or directory\n2025-07-21T19:31:45Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/var/lib/docker/btrfs\"): no such file or directory\n2025-07-21T19:31:45Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/8f116d6e20d2\"): no such file or directory\n2025-07-21T19:31:45Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/3bd80a3cfd1f\"): no such file or directory\n2025-07-21T19:31:45Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/etc/libvirt\"): no such file or directory\n2025-07-21T19:31:45Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/932511d76b14\"): no such file or directory\n2025-07-21T19:31:45Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/021e49962264\"): no such file or directory\n2025-07-21T19:31:45Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/0324553c4a51\"): no such file or directory\n2025-07-21T19:31:45Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/4b5c1f380496\"): no such file or directory\n2025-07-21T19:31:45Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/8a2b3b6d6f06\"): no such file or directory\n2025-07-21T19:31:45Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/dc93de697727\"): no such file or directory\n2025-07-21T19:31:45Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/272ed0de122f\"): no such file or directory\n2025-07-21T19:31:45Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/556ea498fb7b\"): no such file or directory\n2025-07-21T19:31:46Z D! [outputs.prometheus_client] Wrote batch of 108 metrics in 6.337391ms\n2025-07-21T19:31:46Z D! [outputs.prometheus_client] Buffer fullness: 0 / 10000 metrics\n2025-07-21T19:31:56Z D! [outputs.prometheus_client] Wrote batch of 23 metrics in 2.906695ms\n2025-07-21T19:31:56Z D! [outputs.prometheus_client] Buffer fullness: 0 / 10000 metrics\n2025-07-21T19:32:00Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/hugetlbfs\"): no such file or directory\n2025-07-21T19:32:00Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/mnt/disk1\"): no such file or directory\n2025-07-21T19:32:00Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/mnt/disk2\"): no such file or directory\n2025-07-21T19:32:00Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/mnt/cache\"): no such file or directory\n2025-07-21T19:32:00Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/mnt/user0\"): no such file or directory\n2025-07-21T19:32:00Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/mnt/user\"): no such file or directory\n2025-07-21T19:32:00Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/var/lib/docker\"): no such file or directory\n2025-07-21T19:32:00Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/var/lib/docker/btrfs\"): no such file or directory\n2025-07-21T19:32:00Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/8f116d6e20d2\"): no such file or directory\n2025-07-21T19:32:00Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/3bd80a3cfd1f\"): no such file or directory\n2025-07-21T19:32:00Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/etc/libvirt\"): no such file or directory\n2025-07-21T19:32:00Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/932511d76b14\"): no such file or directory\n2025-07-21T19:32:00Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/021e49962264\"): no such file or directory\n2025-07-21T19:32:00Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/0324553c4a51\"): no such file or directory\n2025-07-21T19:32:00Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/4b5c1f380496\"): no such file or directory\n2025-07-21T19:32:00Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/8a2b3b6d6f06\"): no such file or directory\n2025-07-21T19:32:00Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/dc93de697727\"): no such file or directory\n2025-07-21T19:32:00Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/272ed0de122f\"): no such file or directory\n2025-07-21T19:32:00Z D! [inputs.disk] [SystemPS] => unable to get disk usage (\"/run/docker/netns/556ea498fb7b\"): no such file or directory\n2025-07-21T19:32:00Z D! [inputs.system] Reading users: open /var/run/utmp: no such file or directory\n```\n\n### System info\n\nDocker 1.35.2 on UnRAID\n\n### Docker\n\n```yaml\nservices:\n  telegraf:\n    image: telegraf:latest\n    container_name: telegraf\n    restart: unless-stopped\n    privileged: true  # Required for SMART access\n    user: \"0:281\"  # root user, docker group\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n      - /sys:/rootfs/sys:ro\n      - /proc:/rootfs/proc:ro\n      - /etc:/rootfs/etc:ro\n      - /dev:/dev:ro\n      - /var/log:/var/log \n      - /tmp/drive-monitor:/tmp/\n      - /usr/sbin/smartctl:/usr/sbin/smartctl\n      - /mnt/user/appdata/telegraf:/etc/telegraf\n    environment:\n      - HOST_ETC=/rootfs/etc\n      - HOST_PROC=/rootfs/proc\n      - HOST_SYS=/rootfs/sys\n    ports:\n      - \"9273:9273\"  # Prometheus metrics\n```\n\n### Steps to reproduce\n\n1. Install telegraf with the given config\n2. Spin up one of the drives\n3. Wait for the interval and refresh the /metrics interval\n\n\n### Expected behavior\n\n2 of the drives (the spun down ones) should have the exit_status metric as 2. The other drive should get all the attributes.\n\nInside the container:\n```\nroot@c7b3d2c90201:/# smartctl --info --attributes --health -n standby --format=brief /dev/sdc\nsmartctl 7.4 2023-08-01 r5530 [x86_64-linux-6.12.24-Unraid] (local build)\nCopyright (C) 2002-23, Bruce Allen, Christian Franke, www.smartmontools.org\n\nDevice is in STANDBY mode, exit(2)\n```\n```\nroot@c7b3d2c90201:/# smartctl --info --attributes --health -n standby --format=brief /dev/sdb\nsmartctl 7.4 2023-08-01 r5530 [x86_64-linux-6.12.24-Unraid] (local build)\nCopyright (C) 2002-23, Bruce Allen, Christian Franke, www.smartmontools.org\n\n=== START OF INFORMATION SECTION ===\nDevice Model:     ST18000NT001-3NF101\nSerial Number:    ZVTFEY8F\nLU WWN Device Id: 5 000c50 0e8e32474\nFirmware Version: EN01\nUser Capacity:    18,000,207,937,536 bytes [18.0 TB]\nSector Sizes:     512 bytes logical, 4096 bytes physical\nRotation Rate:    7200 rpm\nForm Factor:      3.5 inches\nDevice is:        Not in smartctl database 7.3/5528\nATA Version is:   ACS-4 (minor revision not indicated)\nSATA Version is:  SATA 3.3, 6.0 Gb/s (current: 6.0 Gb/s)\nLocal Time is:    Mon Jul 21 19:37:53 2025 UTC\nSMART support is: Available - device has SMART capability.\nSMART support is: Enabled\nPower mode is:    ACTIVE or IDLE\n\n=== START OF READ SMART DATA SECTION ===\nSMART overall-health self-assessment test result: PASSED\n\nSMART Attributes Data Structure revision number: 10\nVendor Specific SMART Attributes with Thresholds:\nID# ATTRIBUTE_NAME          FLAGS    VALUE WORST THRESH FAIL RAW_VALUE\n  1 Raw_Read_Error_Rate     POSR--   080   064   044    -    95982944\n  3 Spin_Up_Time            PO----   090   089   000    -    0\n  4 Start_Stop_Count        -O--CK   100   100   020    -    113\n  5 Reallocated_Sector_Ct   PO--CK   100   100   010    -    0\n  7 Seek_Error_Rate         POSR--   080   060   045    -    107308876\n  9 Power_On_Hours          -O--CK   094   094   000    -    5471\n 10 Spin_Retry_Count        PO--C-   100   100   097    -    0\n 12 Power_Cycle_Count       -O--CK   100   100   020    -    25\n 18 Unknown_Attribute       PO-R--   100   100   050    -    0\n187 Reported_Uncorrect      -O--CK   100   100   000    -    0\n188 Command_Timeout         -O--CK   100   098   000    -    60130459662\n190 Airflow_Temperature_Cel -O---K   061   045   000    -    39 (Min/Max 34/40)\n192 Power-Off_Retract_Count -O--CK   100   100   000    -    5\n193 Load_Cycle_Count        -O--CK   100   100   000    -    1844\n194 Temperature_Celsius     -O---K   039   055   000    -    39 (0 17 0 0 0)\n197 Current_Pending_Sector  -O--C-   100   100   000    -    0\n198 Offline_Uncorrectable   ----C-   100   100   000    -    0\n199 UDMA_CRC_Error_Count    -OSRCK   200   200   000    -    16\n200 Multi_Zone_Error_Rate   PO---K   100   100   001    -    0\n240 Head_Flying_Hours       ------   100   100   000    -    713 (220 79 0)\n241 Total_LBAs_Written      ------   100   253   000    -    66248646320\n242 Total_LBAs_Read         ------   100   253   000    -    158263807904\n                            ||||||_ K auto-keep\n                            |||||__ C event count\n                            ||||___ R error rate\n                            |||____ S speed/performance\n                            ||_____ O updated online\n                            |______ P prefailure warning\n```\n\n`/metrics` output:\n```\n# HELP smart_device_exit_status Telegraf collected metric\n# TYPE smart_device_exit_status untyped\nsmart_device_exit_status{device=\"sdb\",host=\"big-bertha\"} 2\nsmart_device_exit_status{device=\"sdc\",host=\"big-bertha\"} 2\nsmart_device_exit_status{device=\"sde\",host=\"big-bertha\"} 2\n```\n\n### Actual behavior\n\nThe drive sdb should not be exiting with status code 2. It is spinning as shown by running the smartctl command from inside the container.\n\n### Additional info\n\n_No response_",
      "solution": "@sarpuser Thanks for reporting this, I can confirm it\u2019s an issue, and I\u2019m working on a fix.\n\n**Workaround (for now):**\n\n```toml\n# For active drives - no nocheck\n[[inputs.smart]]\n  path_smartctl = \"/usr/sbin/smartctl\"\n  attributes = true\n  devices = [\"/dev/sdb\"]\n\n# For standby drives - with nocheck\n[[inputs.smart]]\n  path_smartctl = \"/usr/sbin/smartctl\"\n  nocheck = \"standby\"\n  attributes = true\n  devices = [\"/dev/sde\", \"/dev/sdc\"]\n```\n\nThe problem seems to be in the `gatherDisk` function, where the actual `smartctl` exit code is being overridden by text parsing logic.\n",
      "labels": [
        "bug"
      ],
      "created_at": "2025-07-21T19:42:26Z",
      "closed_at": "2025-10-20T09:55:12Z",
      "url": "https://github.com/influxdata/telegraf/issues/17370",
      "comments_count": 1
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17795,
      "title": "CVE false positives since 1.36.2 due to wrong version detection",
      "problem": "### Relevant telegraf.conf\n\n```toml\nN/A\n```\n\n### Logs from Telegraf\n\n```text\nN/A\n```\n\n### System info\n\nTelegraf 1.36.2-alpine container scanned on MacOS Tahoe with Trivy 0.67.0\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1. Install trivy\n2. call `trivy image telegraf:1.36.2-alpine`\n3. Outputs wrong version detection\n\n<img width=\"1357\" height=\"582\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/9f96643f-5956-4d02-b705-7a285255e074\" />\n\n### Expected behavior\n\nCorrect detection of versions\n\n### Actual behavior\n\nDetects v0.0.0-20250911095445-20c4d105d9a0 as version\n\n### Additional info\n\nThese are the trivy scan results for 1.36.1 and 1.36.2\nFor 1.36.2 you can see that the wrong version is detected. This is not a trivy issue but a (currently) very common issues other go-binaries experience like grafana, grafana alloy or hashicorp nomad / consul-template\n\nCompare:\n* https://github.com/hashicorp/consul-template/issues/2087\n* https://github.com/grafana/alloy/issues/3538\n* https://github.com/grafana/grafana/issues/106728\n\n\n<img width=\"1390\" height=\"852\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/dd925a37-625d-4aee-b7b1-7e7583c7793d\" />\n<img width=\"1357\" height=\"582\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/aa40c4d4-a848-4336-95d2-44b7598ee448\" />",
      "solution": "Yes we had to bump to an unreleased version to fix a critical issue leading to panics when using the snowflake driver (#17607). We have bumped to 1.17.0 as of #17725, so this will be resolved in the next release of telegraf",
      "labels": [
        "bug"
      ],
      "created_at": "2025-10-09T20:48:41Z",
      "closed_at": "2025-10-10T15:59:30Z",
      "url": "https://github.com/influxdata/telegraf/issues/17795",
      "comments_count": 2
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17379,
      "title": "[[inputs.docker]] incorrect usage_percent when using podman",
      "problem": "### Relevant telegraf.conf\n\n```toml\n[agent]\n  interval = \"10s\"\n  round_interval = true\n  metric_batch_size = 1000\n  metric_buffer_limit = 10000\n  collection_jitter = \"0s\"\n  flush_interval = \"10s\"\n  flush_jitter = \"0s\"\n  precision = \"\"\n  hostname = \"xxx\"\n  omit_hostname = false\n[[outputs.influxdb]]\n   urls = [\"http://influxdb:8086\"]  # the name of the host with influxdb\n   database = \"telegraf\"\n[[inputs.cpu]]\n  percpu = true\n  totalcpu = true\n  collect_cpu_time = false\n  report_active = false\n[[inputs.disk]]\n  ignore_fs = [\"tmpfs\", \"devtmpfs\", \"devfs\", \"iso9660\", \"overlay\", \"aufs\", \"squashfs\"]\n[[inputs.diskio]]\n[[inputs.kernel]]\n[[inputs.mem]]\n[[inputs.processes]]\n[[inputs.swap]]\n[[inputs.system]]\n[[inputs.docker]]\n   endpoint = \"unix:///run/podman/podman.sock\"\n```\n\n### Logs from Telegraf\n\n```text\nJul 24 08:28:44 xxx systemd[1]: Starting Telegraf...\nJul 24 08:28:44 xxx telegraf[1345004]: 2025-07-24T08:28:44Z I! Loading config: /etc/telegraf/telegraf.conf\nJul 24 08:28:44 xxx telegraf[1345004]: 2025-07-24T08:28:44Z I! Starting Telegraf 1.35.2 brought to you by InfluxData the makers of InfluxDB\nJul 24 08:28:44 xxx telegraf[1345004]: 2025-07-24T08:28:44Z I! Available plugins: 238 inputs, 9 aggregators, 34 processors, 26 parsers, 65 outputs, 6 secret-stores\nJul 24 08:28:44 xxx telegraf[1345004]: 2025-07-24T08:28:44Z I! Loaded inputs: cpu disk diskio docker kernel mem processes swap system\nJul 24 08:28:44 xxx telegraf[1345004]: 2025-07-24T08:28:44Z I! Loaded aggregators:\nJul 24 08:28:44 xxx telegraf[1345004]: 2025-07-24T08:28:44Z I! Loaded processors:\nJul 24 08:28:44 xxx telegraf[1345004]: 2025-07-24T08:28:44Z I! Loaded secretstores:\nJul 24 08:28:44 xxx telegraf[1345004]: 2025-07-24T08:28:44Z I! Loaded outputs: influxdb\nJul 24 08:28:44 xxx telegraf[1345004]: 2025-07-24T08:28:44Z I! [agent] Config: Interval:10s, Quiet:false, Hostname:\"xxx\", Flush Interval:10s\nJul 24 08:28:44 xxx telegraf[1345004]: 2025-07-24T08:28:44Z W! [agent] The default value of 'skip_processors_after_aggregators' will change to 'true' with Telegraf v1.40.0! If >\nJul 24 08:28:44 xxx systemd[1]: Started Telegraf.\n```\n\n### System info\n\nTelegraf 1.35.2, RHEL8.10, podman 4.9.4\n\n### Docker\n\nWhen using `podman` the value recorded in `usage_percent` does not appear to be the actual %CPU but rather the average %CPU since the container was started.\n\n### Steps to reproduce\n\n1. Create a new container and put it under load for several minutes, then let it idle.\n2. Plot the container %CPU\n```\nSELECT \"usage_percent\" FROM \"docker_container_cpu\" WHERE (\"host\" = 'xxx') AND $timeFilter\n```\n3. Plot the system %CPU\n``` \nSELECT \"usage_system\" FROM \"cpu\" WHERE (\"host\" = 'xxx' AND \"cpu\" = 'cpu-total') AND $timeFilter\n```\n\n### Expected behavior\n\nThe container %CPU should peak during a load test and then drop immediately when the test is over, closely matching the system %CPU\n\n### Actual behavior\n\nThe container %CPU remains high after the test, even when the container is idle.\n\n### Additional info\n\nI actually think this might be a bug in `podman-stats` since you get the same behaviour in the 1st sample when streaming. But it does mean that the data reported by Telegraf is -also- wrong.\n\nE.g. this is the output on a test system where I first noticed the data shown in Grafana didn't make much sense.\n\n```\n# podman stats --all --interval 1 --format \"table {{.ID}}\\t{{.CPU }}\\t{{.AvgCPU}}\\t{{.MemUsageBytes}}\" --no-reset\nID            CPU                  AVG CPU             MEM USAGE / LIMIT\n1aa772a0c85c  159.3999657351224    159.3999657351224   39.88GiB / 82GiB\nID            CPU                  AVG CPU             MEM USAGE / LIMIT\n1aa772a0c85c  0.5486888953293112   159.39698221220848  39.88GiB / 82GiB\nID            CPU                  AVG CPU             MEM USAGE / LIMIT\n1aa772a0c85c  0.2593970819529336   159.39399526163533  39.88GiB / 82GiB\nID            CPU                  AVG CPU             MEM USAGE / LIMIT\n1aa772a0c85c  0.24709045580450345  159.39101059737825  39.88GiB / 82GiB\nID            CPU                  AVG CPU             MEM USAGE / LIMIT\n1aa772a0c85c  0.23393135232549853  159.38802545420887  39.88GiB / 82GiB\nID            CPU                  AVG CPU             MEM USAGE / LIMIT\n1aa772a0c85c  0.2443277612159435   159.38503951503566  39.88GiB / 82GiB\n```",
      "solution": "Thank you for reporting this issue. After investigating, I can confirm this is a legitimate bug that affects CPU percentage accuracy when using Podman with the Docker input plugin.\n\n  The issue stems from a fundamental difference between Docker and Podman's stats API behavior:\n\n  Docker: The ContainerStats API (non-streaming) provides:\n  - CPUStats: Current CPU usage snapshot\n  - PreCPUStats: Previous CPU usage snapshot from a recent time interval\n  - This allows calculation of current CPU usage: (CPUStats.TotalUsage - PreCPUStats.TotalUsage) / time_interval\n\n  Podman: The ContainerStats API (non-streaming) provides:\n  - CPUStats: Current CPU usage snapshot\n  - PreCPUStats: CPU usage from container start time (not recent interval)\n  - This results in average CPU since container start: total_cpu_time / container_uptime\n\nThis is a known limitation in Podman itself, tracked in their repository: https://github.com/containers/podman/issues/25709\n\nThe Podman team acknowledges that their single-shot stats show \"average CPU\" instead of current CPU usage, which is the root cause of the incorrect usage_percent values reported by Telegraf when connected to Podman.\n\n\n\n---\n\nOne solution could be to create the missing `previous snapshot` that Podman doesn\u2019t provide, then save it in cache. This would enable accurate CPU percentage calculations without needing API changes.\n\n**Current Problem**\n\n  Podman API call \u2192 Returns stats with PreCPUStats from container START\n  Result: CPU% = total_cpu_since_start / total_time_since_start (WRONG - shows average)\n\n  **Proposed Solution**\n\n  Collection 1: Get stats \u2192 Cache them \u2192 Return CPU% = 0 (no previous data)\n  Collection 2: Get stats \u2192 Use cached stats as PreCPUStats \u2192 Return accurate CPU%\n  Collection 3: Get stats \u2192 Use Collection 2 stats as PreCPUStats \u2192 Return accurate CPU%\n\n  **Step-by-Step Process**\n\n  1. First Collection (10:00 AM):\n    - Telegraf calls Podman API\n    - Gets current stats: CPUStats.TotalUsage = 1000ms\n    - Cache these stats for this container\n    - CPU calculation skipped (no previous data available)\n  2. Second Collection (10:01 AM):\n    - Telegraf calls Podman API\n    - Gets current stats: CPUStats.TotalUsage = 2000ms\n    - Uses cached stats from 10:00 AM as PreCPUStats\n    - CPU% = (2000ms - 1000ms) / 60 seconds = current usage \n    - Cache these new stats for next collection\n  3. Third Collection (10:02 AM):\n    - Gets current stats: CPUStats.TotalUsage = 2100ms\n    - Uses cached stats from 10:01 AM as PreCPUStats\n    - CPU% = (2100ms - 2000ms) / 60 seconds = current usage \n\n  When PreCPUStats Gets Calculated\n\n  - Before each collection: Use previously cached stats as PreCPUStats\n  - After each collection: Cache current stats for next collection\n  - Cache cleanup: Remove old entries for stopped containers\n\nThis transforms Podman's \"cumulative since start\" data into proper \"interval-based\" measurements, just like Docker provides natively.\n",
      "labels": [
        "bug"
      ],
      "created_at": "2025-07-25T09:52:04Z",
      "closed_at": "2025-10-09T20:40:56Z",
      "url": "https://github.com/influxdata/telegraf/issues/17379",
      "comments_count": 2
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17757,
      "title": "Panic in chrony input plugin",
      "problem": "### Relevant telegraf.conf\n\n```toml\n[[inputs.chrony]]\n  server = \"unixgram:///var/run/chrony/chronyd.sock\"\n  dns_lookup = true\n  metrics = [\"activity\", \"tracking\", \"serverstats\", \"sources\", \"sourcestats\"]\n  socket_group = \"wheel\"\n  socket_perms = \"0660\"\n```\n\n### Logs from Telegraf\n\n```text\n2025-10-03T04:00:30Z E! FATAL: [inputs.chrony] panicked: runtime error: index out of range [256] with length 256, Stack:\ngoroutine 236818 [running]:\ngithub.com/influxdata/telegraf/agent.panicRecover(0x1400151c900)\n\tgithub.com/influxdata/telegraf/agent/agent.go:1202 +0x60\npanic({0x10e9e5660?, 0x14001d2e090?})\n\truntime/panic.go:783 +0x120\nencoding/binary.(*encoder).uint8(...)\n\tencoding/binary/binary.go:792\nencoding/binary.(*encoder).value(0x14002ab1d50, {0x10d8a3240?, 0x14000861620?, 0x10d43a6ae?})\n\tencoding/binary/binary.go:953 +0x89c\nencoding/binary.(*encoder).value(0x14002ab1d50, {0x10da17980?, 0x14000861534?, 0x114a092a8?})\n\tencoding/binary/binary.go:919 +0x804\nencoding/binary.(*encoder).value(0x14002ab1d50, {0x10e406d20?, 0x14000861520?, 0x10476e21c?})\n\tencoding/binary/binary.go:928 +0x714\nencoding/binary.Write({0x10f133580, 0x14000b60480}, {0x10f218880, 0x114f0fa00}, {0x10ded1d20, 0x14000861520})\n\tencoding/binary/binary.go:431 +0x254\ngithub.com/facebook/time/ntp/chrony.(*Client).Communicate(0x14000a49050, {0x10f16fc38, 0x14000861520})\n\tgithub.com/facebook/time@v0.0.0-20250903103710-a5911c32cdb9/ntp/chrony/client.go:38 +0x84\ngithub.com/influxdata/telegraf/plugins/inputs/chrony.(*Chrony).gatherServerStats(0x14001380a00, {0x10f2319a0, 0x14000624220})\n\tgithub.com/influxdata/telegraf/plugins/inputs/chrony/chrony.go:298 +0x64\ngithub.com/influxdata/telegraf/plugins/inputs/chrony.(*Chrony).Gather(0x14001380a00, {0x10f2319a0, 0x14000624220})\n\tgithub.com/influxdata/telegraf/plugins/inputs/chrony/chrony.go:156 +0x174\ngithub.com/influxdata/telegraf/models.(*RunningInput).Gather(0x1400151c900, {0x10f2319a0, 0x14000624220})\n\tgithub.com/influxdata/telegraf/models/running_input.go:263 +0x210\ngithub.com/influxdata/telegraf/agent.(*Agent).gatherOnce.func1()\n\tgithub.com/influxdata/telegraf/agent/agent.go:590 +0x50\ncreated by github.com/influxdata/telegraf/agent.(*Agent).gatherOnce in goroutine 93\n\tgithub.com/influxdata/telegraf/agent/agent.go:588 +0xb4\n\ngoroutine 1 [sync.WaitGroup.Wait, 561 minutes]:\nsync.runtime_SemacquireWaitGroup(0x1047458f0?, 0x80?)\n\truntime/sema.go:114 +0x38\nsync.(*WaitGroup).Wait(0x14001517150)\n\tsync/waitgroup.go:206 +0xa8\n2025-10-03T04:00:30Z E! PLEASE REPORT THIS PANIC ON GITHUB with stack trace, configuration, and OS information: https://github.com/influxdata/telegraf/issues/new/choose\n2025-10-03T04:00:30Z W! Telegraf is not permitted to read /opt/homebrew/etc/telegraf.d\n2025-10-03T04:00:30Z I! Loading config: /opt/homebrew/etc/telegraf.conf\n2025-10-03T04:00:30Z I! Starting Telegraf 1.36.2 brought to you by InfluxData the makers of InfluxDB\n2025-10-03T04:00:30Z I! Available plugins: 238 inputs, 9 aggregators, 35 processors, 26 parsers, 65 outputs, 5 secret-stores\n2025-10-03T04:00:30Z I! Loaded inputs: chrony cpu disk diskio dns_query http (4x) http_response internal mem net netstat ping processes swap system temp\n2025-10-03T04:00:30Z I! Loaded aggregators:\n2025-10-03T04:00:30Z I! Loaded processors:\n2025-10-03T04:00:30Z I! Loaded secretstores:\n2025-10-03T04:00:30Z I! Loaded outputs: influxdb_v2\n2025-10-03T04:00:30Z I! Tags enabled: host=homelab.localdomain\n2025-10-03T04:00:30Z I! [agent] Config: Interval:10s, Quiet:false, Hostname:\"homelab.localdomain\", Flush Interval:10s\nruntime: g31113: frame.sp=0x14002389970 top=0x14002389fd0\n\tstack=[0x14002386000-0x1400238a000\nfatal error: traceback did not unwind completely\n\nruntime stack:\nruntime.throw({0x10b526ee0?, 0x10465c960?})\n\truntime/panic.go:1094 +0x34 fp=0x174a3ec00 sp=0x174a3ebd0 pc=0x104678af4\nruntime.(*unwinder).finishInternal(0x174a3ecc8?)\n\truntime/traceback.go:566 +0x10c fp=0x174a3ec40 sp=0x174a3ec00 pc=0x104667bdc\nruntime.(*unwinder).next(0x174a3ed18?)\n\truntime/traceback.go:447 +0x284 fp=0x174a3ecd0 sp=0x174a3ec40 pc=0x104667a34\nruntime.copystack(0x14000a83500, 0x4000)\n\truntime/stack.go:975 +0x2b8 fp=0x174a3edd0 sp=0x174a3ecd0 pc=0x10465d038\nruntime.newstack()\n\truntime/stack.go:1168 +0x35c fp=0x174a3ef00 sp=0x174a3edd0 pc=0x10465d4cc\nruntime.morestack()\n\truntime/asm_arm64.s:392 +0x70 fp=0x174a3ef00 sp=0x174a3ef00 pc=0x10467f470\n\ngoroutine 31113 gp=0x14000a83500 m=19 mp=0x14001d60808 [copystack]:\ngithub.com/shirou/gopsutil/v4/internal/common.NewLibrary({0x105853d00, 0x0})\n\tgithub.com/shirou/gopsutil/v4@v4.25.8/internal/common/common_darwin.go:85 +0xcc fp=0x14002389960 sp=0x14002389960 pc=0x105853d0c\ngithub.com/shirou/gopsutil/v4/internal/common.(*Library).Close(...)\n\tgithub.com/shirou/gopsutil/v4@v4.25.8/internal/common/common_darwin.go:113\ngithub.com/shirou/gopsutil/v4/sensors.TemperaturesWithContext.deferwrap1()\n\tgithub.com/shirou/gopsutil/v4@v4.25.8/sensors/sensors_darwin_arm64.go:23 +0x30 fp=0x14002389970 sp=0x14002389960 pc=0x108960d80\ngithub.com/shirou/gopsutil/v4/sensors.TemperaturesWithContext({0x14000f40e58?, 0x10460d53c?})\n\tgithub.com/shirou/gopsutil/v4@v4.25.8/sensors/sensors_darwin_arm64.go:54 +0x6b4 fp=0x14002389e10 sp=0x14002389970 pc=0x108960bc4\ncreated by github.com/influxdata/telegraf/agent.(*Agent).gatherOnce in goroutine 89\n\tgithub.com/influxdata/telegraf/agent/agent.go:588 +0xb4\n\ngoroutine 1 gp=0x140000021c0 m=nil [sync.WaitGroup.Wait, 72 minutes]:\nruntime.gopark(0x1148d2020?, 0x140015facc0?, 0x80?, 0xe4?, 0x14000658e80?)\n\truntime/proc.go:460 +0xc0 fp=0x1400238cbe0 sp=0x1400238cbc0 pc=0x104678c00\nruntime.goparkunlock(...)\n\truntime/proc.go:466\nruntime.semacquire1(0x140015ffcb8, 0x0, 0x1, 0x0, 0x19)\n\truntime/sema.go:192 +0x204 fp=0x1400238cc30 sp=0x1400238cbe0 pc=0x1046578f4\nsync.runtime_SemacquireWaitGroup(0x10464d8f0?, 0xc0?)\n\truntime/sema.go:114 +0x38 fp=0x1400238cc70 sp=0x1400238cc30 pc=0x10467a848\nsync.(*WaitGroup).Wait(0x140015ffcb0)\n\tsync/waitgroup.go:206 +0xa8 fp=0x1400238cca0 sp=0x1400238cc70 pc=0x10469be28\ngithub.com/influxdata/telegraf/agent.(*Agent).Run(0x1400105df78, {0x10f0ecff0, 0x14001499220})\n\tgithub.com/influxdata/telegraf/agent/agent.go:208 +0x7d0 fp=0x1400238ce60 sp=0x1400238cca0 pc=0x104e43ad0\nmain.(*Telegraf).runAgent(0x140010c9080, {0x10f0ecff0, 0x14001499220}, 0x0?)\n\tgithub.com/influxdata/telegraf/cmd/telegraf/telegraf.go:565 +0x1184 fp=0x1400238d230 sp=0x1400238ce60 pc=0x10b388824\nmain.(*Telegraf).reloadLoop(0x140010c9080)\n\tgithub.com/influxdata/telegraf/cmd/telegraf/telegraf.go:207 +0x1b8 fp=0x1400238d410 sp=0x1400238d230 pc=0x10b385288\nmain.(*Telegraf).Run(0x140010c9080)\n\tgithub.com/influxdata/telegraf/cmd/telegraf/telegraf_posix.go:20 +0xb0 fp=0x1400238d460 sp=0x1400238d410 pc=0x10b3892b0\nmain.runApp.func1(0x1400143cf40)\n\tgithub.com/influxdata/telegraf/cmd/telegraf/main.go:261 +0x9dc fp=0x1400238da10 sp=0x1400238d460 pc=0x10b380e2c\ngithub.com/urfave/cli/v2.(*Command).Run(0x1400149ec60, 0x1400143cf40, {0x140001ce050, 0x5, 0x5})\n\tgithub.com/urfave/cli/v2@v2.27.7/command.go:276 +0x580 fp=0x1400238dc70 sp=0x1400238da10 pc=0x1047c2190\ngithub.com/urfave/cli/v2.(*App).RunContext(0x14000ca5600, {0x10f0ecd00, 0x114e17a00}, {0x140001ce050, 0x5, 0x5})\n\tgithub.com/urfave/cli/v2@v2.27.7/app.go:333 +0x4c8 fp=0x1400238dcd0 sp=0x1400238dc70 pc=0x1047bf318\ngithub.com/urfave/cli/v2.(*App).Run(...)\n\tgithub.com/urfave/cli/v2@v2.27.7/app.go:307\nmain.runApp({0x140001ce050, 0x5, 0x5}, {0x10f03b1e8, 0x1400019c068}, {0x10f075d38, 0x1400105dce0}, {0x10f075d60, 0x140012e6900}, {0x10f0ecbb0, ...})\n\tgithub.com/influxdata/telegraf/cmd/telegraf/main.go:419 +0xd44 fp=0x1400238dec0 sp=0x1400238dcd0 pc=0x10b380244\nmain.main()\n\tgithub.com/influxdata/telegraf/cmd/telegraf/main.go:433 +0xec fp=0x1400238df40 sp=0x1400238dec0 pc=0x10b38174c\nruntime.main()\n\truntime/proc.go:285 +0x278 fp=0x1400238dfd0 sp=0x1400238df40 pc=0x104642d88\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x1400238dfd0 sp=0x1400238dfd0 pc=0x104681834\n\ngoroutine 2 gp=0x14000002c40 m=nil [force gc (idle), 11 minutes]:\nruntime.gopark(0x22406b427fa19?, 0x0?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:460 +0xc0 fp=0x140000f0f90 sp=0x140000f0f70 pc=0x104678c00\nruntime.goparkunlock(...)\n\truntime/proc.go:466\nruntime.forcegchelper()\n\truntime/proc.go:373 +0xb4 fp=0x140000f0fd0 sp=0x140000f0f90 pc=0x1046430d4\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x140000f0fd0 sp=0x140000f0fd0 pc=0x104681834\ncreated by runtime.init.7 in goroutine 1\n\truntime/proc.go:361 +0x24\n\ngoroutine 3 gp=0x14000003180 m=nil [GC sweep wait]:\nruntime.gopark(0x114892c01?, 0x0?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:460 +0xc0 fp=0x140000f1760 sp=0x140000f1740 pc=0x104678c00\nruntime.goparkunlock(...)\n\truntime/proc.go:466\nruntime.bgsweep(0x1400011c000)\n\truntime/mgcsweep.go:323 +0x104 fp=0x140000f17b0 sp=0x140000f1760 pc=0x10462b2c4\nruntime.gcenable.gowrap1()\n\truntime/mgc.go:212 +0x28 fp=0x140000f17d0 sp=0x140000f17b0 pc=0x10461ed88\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x140000f17d0 sp=0x140000f17d0 pc=0x104681834\ncreated by runtime.gcenable in goroutine 1\n\truntime/mgc.go:212 +0x6c\n\ngoroutine 4 gp=0x14000003340 m=nil [GC scavenge wait]:\nruntime.gopark(0x14f0ad?, 0xbb50e?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:460 +0xc0 fp=0x140000f1f60 sp=0x140000f1f40 pc=0x104678c00\nruntime.goparkunlock(...)\n\truntime/proc.go:466\nruntime.(*scavengerState).park(0x1148a35e0)\n\truntime/mgcscavenge.go:425 +0x5c fp=0x140000f1f90 sp=0x140000f1f60 pc=0x104628ddc\nruntime.bgscavenge(0x1400011c000)\n\truntime/mgcscavenge.go:658 +0xac fp=0x140000f1fb0 sp=0x140000f1f90 pc=0x10462937c\nruntime.gcenable.gowrap2()\n\truntime/mgc.go:213 +0x28 fp=0x140000f1fd0 sp=0x140000f1fb0 pc=0x10461ed28\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x140000f1fd0 sp=0x140000f1fd0 pc=0x104681834\ncreated by runtime.gcenable in goroutine 1\n\truntime/mgc.go:213 +0xac\n\ngoroutine 18 gp=0x14000182380 m=nil [GOMAXPROCS updater (idle), 73 minutes]:\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\n\truntime/proc.go:460 +0xc0 fp=0x140000ec770 sp=0x140000ec750 pc=0x104678c00\nruntime.goparkunlock(...)\n\truntime/proc.go:466\nruntime.updateMaxProcsGoroutine()\n\truntime/proc.go:6706 +0xf4 fp=0x140000ec7d0 sp=0x140000ec770 pc=0x104651ba4\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x140000ec7d0 sp=0x140000ec7d0 pc=0x104681834\ncreated by runtime.defaultGOMAXPROCSUpdateEnable in goroutine 1\n\truntime/proc.go:6694 +0x48\n\ngoroutine 19 gp=0x140001828c0 m=nil [finalizer wait, 73 minutes]:\nruntime.gopark(0x140000f05b8?, 0x108cf067c?, 0x0?, 0x64?, 0xa?)\n\truntime/proc.go:460 +0xc0 fp=0x140000f0580 sp=0x140000f0560 pc=0x104678c00\nruntime.runFinalizers()\n\truntime/mfinal.go:210 +0x104 fp=0x140000f07d0 sp=0x140000f0580 pc=0x10461dd74\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x140000f07d0 sp=0x140000f07d0 pc=0x104681834\ncreated by runtime.createfing in goroutine 1\n\truntime/mfinal.go:172 +0x78\n\ngoroutine 34 gp=0x14000884000 m=nil [GC worker (idle)]:\nruntime.gopark(0x114e23320?, 0x1?, 0x62?, 0x2f?, 0x0?)\n\truntime/proc.go:460 +0xc0 fp=0x1400195df10 sp=0x1400195def0 pc=0x104678c00\nruntime.gcBgMarkWorker(0x14000882000)\n\truntime/mgc.go:1463 +0xe0 fp=0x1400195dfb0 sp=0x1400195df10 pc=0x104621400\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1373 +0x28 fp=0x1400195dfd0 sp=0x1400195dfb0 pc=0x1046212e8\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x1400195dfd0 sp=0x1400195dfd0 pc=0x104681834\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1373 +0x140\n\ngoroutine 5 gp=0x14000003c00 m=nil [GC worker (idle)]:\nruntime.gopark(0x114e23320?, 0x1?, 0x72?, 0xee?, 0x0?)\n\truntime/proc.go:460 +0xc0 fp=0x14003018f10 sp=0x14003018ef0 pc=0x104678c00\nruntime.gcBgMarkWorker(0x14000882000)\n\truntime/mgc.go:1463 +0xe0 fp=0x14003018fb0 sp=0x14003018f10 pc=0x104621400\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1373 +0x28 fp=0x14003018fd0 sp=0x14003018fb0 pc=0x1046212e8\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14003018fd0 sp=0x14003018fd0 pc=0x104681834\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1373 +0x140\n\ngoroutine 20 gp=0x14000183340 m=nil [GC worker (idle)]:\nruntime.gopark(0x22486c07fa177?, 0x3?, 0xea?, 0x5d?, 0x0?)\n\truntime/proc.go:460 +0xc0 fp=0x1400301ef10 sp=0x1400301eef0 pc=0x104678c00\nruntime.gcBgMarkWorker(0x14000882000)\n\truntime/mgc.go:1463 +0xe0 fp=0x1400301efb0 sp=0x1400301ef10 pc=0x104621400\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1373 +0x28 fp=0x1400301efd0 sp=0x1400301efb0 pc=0x1046212e8\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x1400301efd0 sp=0x1400301efd0 pc=0x104681834\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1373 +0x140\n\ngoroutine 35 gp=0x140008841c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x22486c07b6a0a?, 0x1?, 0x27?, 0x31?, 0x0?)\n\truntime/proc.go:460 +0xc0 fp=0x1400297ff10 sp=0x1400297fef0 pc=0x104678c00\nruntime.gcBgMarkWorker(0x14000882000)\n\truntime/mgc.go:1463 +0xe0 fp=0x1400297ffb0 sp=0x1400297ff10 pc=0x104621400\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1373 +0x28 fp=0x1400297ffd0 sp=0x1400297ffb0 pc=0x1046212e8\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x1400297ffd0 sp=0x1400297ffd0 pc=0x104681834\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1373 +0x140\n\ngoroutine 21 gp=0x14000183500 m=nil [GC worker (idle), 7 minutes]:\nruntime.gopark(0x224379d1fc4b8?, 0x1?, 0x90?, 0x44?, 0x0?)\n\truntime/proc.go:460 +0xc0 fp=0x14001958f10 sp=0x14001958ef0 pc=0x104678c00\nruntime.gcBgMarkWorker(0x14000882000)\n\truntime/mgc.go:1463 +0xe0 fp=0x14001958fb0 sp=0x14001958f10 pc=0x104621400\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1373 +0x28 fp=0x14001958fd0 sp=0x14001958fb0 pc=0x1046212e8\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14001958fd0 sp=0x14001958fd0 pc=0x104681834\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1373 +0x140\n\ngoroutine 6 gp=0x14000003dc0 m=nil [GC worker (idle), 2 minutes]:\nruntime.gopark(0x2246ad2bdea8d?, 0x1?, 0x93?, 0x20?, 0x0?)\n\truntime/proc.go:460 +0xc0 fp=0x140022f9f10 sp=0x140022f9ef0 pc=0x104678c00\nruntime.gcBgMarkWorker(0x14000882000)\n\truntime/mgc.go:1463 +0xe0 fp=0x140022f9fb0 sp=0x140022f9f10 pc=0x104621400\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1373 +0x28 fp=0x140022f9fd0 sp=0x140022f9fb0 pc=0x1046212e8\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x140022f9fd0 sp=0x140022f9fd0 pc=0x104681834\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1373 +0x140\n\ngoroutine 36 gp=0x14000884380 m=nil [GC worker (idle)]:\nruntime.gopark(0x22486c07f9542?, 0x3?, 0x46?, 0xa?, 0x0?)\n\truntime/proc.go:460 +0xc0 fp=0x14002cb9f10 sp=0x14002cb9ef0 pc=0x104678c00\nruntime.gcBgMarkWorker(0x14000882000)\n\truntime/mgc.go:1463 +0xe0 fp=0x14002cb9fb0 sp=0x14002cb9f10 pc=0x104621400\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1373 +0x28 fp=0x14002cb9fd0 sp=0x14002cb9fb0 pc=0x1046212e8\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14002cb9fd0 sp=0x14002cb9fd0 pc=0x104681834\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1373 +0x140\n\ngoroutine 37 gp=0x14000884540 m=nil [GC worker (idle)]:\nruntime.gopark(0x22486c07be499?, 0x1?, 0x25?, 0x69?, 0x0?)\n\truntime/proc.go:460 +0xc0 fp=0x1400088bf10 sp=0x1400088bef0 pc=0x104678c00\nruntime.gcBgMarkWorker(0x14000882000)\n\truntime/mgc.go:1463 +0xe0 fp=0x1400088bfb0 sp=0x1400088bf10 pc=0x104621400\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1373 +0x28 fp=0x1400088bfd0 sp=0x1400088bfb0 pc=0x1046212e8\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x1400088bfd0 sp=0x1400088bfd0 pc=0x104681834\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1373 +0x140\n\ngoroutine 22 gp=0x140001836c0 m=nil [GC worker (idle)]:\nruntime.gopark(0x22486c07adbf3?, 0x3?, 0x34?, 0x21?, 0x0?)\n\truntime/proc.go:460 +0xc0 fp=0x14001781f10 sp=0x14001781ef0 pc=0x104678c00\nruntime.gcBgMarkWorker(0x14000882000)\n\truntime/mgc.go:1463 +0xe0 fp=0x14001781fb0 sp=0x14001781f10 pc=0x104621400\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1373 +0x28 fp=0x14001781fd0 sp=0x14001781fb0 pc=0x1046212e8\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14001781fd0 sp=0x14001781fd0 pc=0x104681834\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1373 +0x140\n\ngoroutine 23 gp=0x14000183880 m=nil [GC worker (idle)]:\nruntime.gopark(0x22486c07b67ec?, 0x3?, 0x17?, 0x3?, 0x0?)\n\truntime/proc.go:460 +0xc0 fp=0x14000107f10 sp=0x14000107ef0 pc=0x104678c00\nruntime.gcBgMarkWorker(0x14000882000)\n\truntime/mgc.go:1463 +0xe0 fp=0x14000107fb0 sp=0x14000107f10 pc=0x104621400\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1373 +0x28 fp=0x14000107fd0 sp=0x14000107fb0 pc=0x1046212e8\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14000107fd0 sp=0x14000107fd0 pc=0x104681834\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1373 +0x140\n\ngoroutine 7 gp=0x14000138000 m=nil [GC worker (idle)]:\nruntime.gopark(0x22486c07adee1?, 0x1?, 0x84?, 0xca?, 0x0?)\n\truntime/proc.go:460 +0xc0 fp=0x14002cbff10 sp=0x14002cbfef0 pc=0x104678c00\nruntime.gcBgMarkWorker(0x14000882000)\n\truntime/mgc.go:1463 +0xe0 fp=0x14002cbffb0 sp=0x14002cbff10 pc=0x104621400\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1373 +0x28 fp=0x14002cbffd0 sp=0x14002cbffb0 pc=0x1046212e8\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14002cbffd0 sp=0x14002cbffd0 pc=0x104681834\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1373 +0x140\n\ngoroutine 38 gp=0x14000884700 m=nil [GC worker (idle)]:\nruntime.gopark(0x22486c08034c4?, 0x3?, 0x24?, 0xf4?, 0x0?)\n\truntime/proc.go:460 +0xc0 fp=0x14001996f10 sp=0x14001996ef0 pc=0x104678c00\nruntime.gcBgMarkWorker(0x14000882000)\n\truntime/mgc.go:1463 +0xe0 fp=0x14001996fb0 sp=0x14001996f10 pc=0x104621400\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1373 +0x28 fp=0x14001996fd0 sp=0x14001996fb0 pc=0x1046212e8\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14001996fd0 sp=0x14001996fd0 pc=0x104681834\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1373 +0x140\n\ngoroutine 24 gp=0x14000183a40 m=nil [GC worker (idle)]:\nruntime.gopark(0x22486c07f8c4f?, 0x3?, 0xbf?, 0x60?, 0x0?)\n\truntime/proc.go:460 +0xc0 fp=0x140000eef10 sp=0x140000eeef0 pc=0x104678c00\nruntime.gcBgMarkWorker(0x14000882000)\n\truntime/mgc.go:1463 +0xe0 fp=0x140000eefb0 sp=0x140000eef10 pc=0x104621400\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1373 +0x28 fp=0x140000eefd0 sp=0x140000eefb0 pc=0x1046212e8\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x140000eefd0 sp=0x140000eefd0 pc=0x104681834\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1373 +0x140\n\ngoroutine 25 gp=0x14000183c00 m=nil [GC worker (idle)]:\nruntime.gopark(0x114e23320?, 0x1?, 0x7d?, 0x8b?, 0x0?)\n\truntime/proc.go:460 +0xc0 fp=0x14000997f10 sp=0x14000997ef0 pc=0x104678c00\nruntime.gcBgMarkWorker(0x14000882000)\n\truntime/mgc.go:1463 +0xe0 fp=0x14000997fb0 sp=0x14000997f10 pc=0x104621400\nruntime.gcBgMarkStartWorkers.gowrap1()\n\truntime/mgc.go:1373 +0x28 fp=0x14000997fd0 sp=0x14000997fb0 pc=0x1046212e8\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14000997fd0 sp=0x14000997fd0 pc=0x104681834\ncreated by runtime.gcBgMarkStartWorkers in goroutine 1\n\truntime/mgc.go:1373 +0x140\n\ngoroutine 8 gp=0x14000983dc0 m=nil [select, 2 minutes]:\nruntime.gopark(0x14000100f68?, 0x3?, 0x0?, 0xa0?, 0x14000100f62?)\n\truntime/proc.go:460 +0xc0 fp=0x14000100de0 sp=0x14000100dc0 pc=0x104678c00\nruntime.selectgo(0x14000100f68, 0x14000100f5c, 0x14000c9ee00?, 0x0, 0x0?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14000100f20 sp=0x14000100de0 pc=0x104656b4c\ngo.opencensus.io/stats/view.(*worker).start(0x14000c9ee00)\n\tgo.opencensus.io@v0.24.0/stats/view/worker.go:292 +0x84 fp=0x14000100fb0 sp=0x14000100f20 pc=0x105598f14\ngo.opencensus.io/stats/view.init.0.gowrap1()\n\tgo.opencensus.io@v0.24.0/stats/view/worker.go:34 +0x28 fp=0x14000100fd0 sp=0x14000100fb0 pc=0x105598458\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14000100fd0 sp=0x14000100fd0 pc=0x104681834\ncreated by go.opencensus.io/stats/view.init.0 in goroutine 1\n\tgo.opencensus.io@v0.24.0/stats/view/worker.go:34 +0x94\n\ngoroutine 9 gp=0x140001381c0 m=nil [cleanup wait, 9 minutes]:\nruntime.gopark(0x14000b8f200?, 0x1408f9c00?, 0x78?, 0x9f?, 0x10461b544?)\n\truntime/proc.go:460 +0xc0 fp=0x14000889f40 sp=0x14000889f20 pc=0x104678c00\nruntime.goparkunlock(...)\n\truntime/proc.go:466\nruntime.(*cleanupQueue).dequeue(0x1148a5200)\n\truntime/mcleanup.go:439 +0x110 fp=0x14000889f80 sp=0x14000889f40 pc=0x10461ad40\nruntime.runCleanups()\n\truntime/mcleanup.go:635 +0x40 fp=0x14000889fd0 sp=0x14000889f80 pc=0x10461b550\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14000889fd0 sp=0x14000889fd0 pc=0x104681834\ncreated by runtime.(*cleanupQueue).createGs in goroutine 1\n\truntime/mcleanup.go:589 +0x108\n\ngoroutine 26 gp=0x140001396c0 m=nil [chan receive, 2 minutes]:\nruntime.gopark(0x140001d9c10?, 0x140001d9b90?, 0xeb?, 0x35?, 0x2f3295cfbf415ee6?)\n\truntime/proc.go:460 +0xc0 fp=0x140008896d0 sp=0x140008896b0 pc=0x104678c00\nruntime.chanrecv(0x140001d9b90, 0x140008897a8, 0x1)\n\truntime/chan.go:667 +0x428 fp=0x14000889750 sp=0x140008896d0 pc=0x10460e248\nruntime.chanrecv2(0x14000650c60?, 0x14000206098?)\n\truntime/chan.go:514 +0x14 fp=0x14000889780 sp=0x14000889750 pc=0x10460de04\ngithub.com/awnumar/memguard/core.NewCoffer.func1(...)\n\tgithub.com/awnumar/memguard@v0.23.0/core/coffer.go:39\ngithub.com/awnumar/memguard/core.NewCoffer.gowrap1()\n\tgithub.com/awnumar/memguard@v0.23.0/core/coffer.go:44 +0x54 fp=0x140008897d0 sp=0x14000889780 pc=0x104dcf284\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x140008897d0 sp=0x140008897d0 pc=0x104681834\ncreated by github.com/awnumar/memguard/core.NewCoffer in goroutine 1\n\tgithub.com/awnumar/memguard@v0.23.0/core/coffer.go:36 +0x108\n\ngoroutine 27 gp=0x14000902a80 m=nil [select, 72 minutes, locked to thread]:\nruntime.gopark(0x140016557a0?, 0x2?, 0xa8?, 0x56?, 0x14001655790?)\n\truntime/proc.go:460 +0xc0 fp=0x14001655610 sp=0x140016555f0 pc=0x104678c00\nruntime.selectgo(0x140016557a0, 0x1400165578c, 0x0?, 0x0, 0x0?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14001655750 sp=0x14001655610 pc=0x104656b4c\nruntime.ensureSigM.func1()\n\truntime/signal_unix.go:1085 +0x154 fp=0x140016557d0 sp=0x14001655750 pc=0x104672cd4\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x140016557d0 sp=0x140016557d0 pc=0x104681834\ncreated by runtime.ensureSigM in goroutine 1\n\truntime/signal_unix.go:1068 +0xcc\n\ngoroutine 28 gp=0x14000902c40 m=15 mp=0x140003dc808 [syscall, 72 minutes]:\nruntime.sigNoteSleep(0x0?)\n\truntime/os_darwin.go:133 +0x20 fp=0x14001655f90 sp=0x14001655f50 pc=0x10463d310\nos/signal.signal_recv()\n\truntime/sigqueue.go:149 +0x2c fp=0x14001655fb0 sp=0x14001655f90 pc=0x10467b30c\nos/signal.loop()\n\tos/signal/signal_unix.go:23 +0x1c fp=0x14001655fd0 sp=0x14001655fb0 pc=0x104dd1d6c\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14001655fd0 sp=0x14001655fd0 pc=0x104681834\ncreated by os/signal.Notify.func1.1 in goroutine 1\n\tos/signal/signal.go:152 +0x28\n\ngoroutine 29 gp=0x14000902e00 m=nil [select, 72 minutes]:\nruntime.gopark(0x14000886738?, 0x3?, 0x0?, 0x0?, 0x140008866ca?)\n\truntime/proc.go:460 +0xc0 fp=0x14000886540 sp=0x14000886520 pc=0x104678c00\nruntime.selectgo(0x14000886738, 0x140008866c4, 0x0?, 0x0, 0x0?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14000886680 sp=0x14000886540 pc=0x104656b4c\nmain.(*Telegraf).reloadLoop.func1()\n\tgithub.com/influxdata/telegraf/cmd/telegraf/telegraf.go:185 +0x84 fp=0x140008867d0 sp=0x14000886680 pc=0x10b3857b4\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x140008867d0 sp=0x140008867d0 pc=0x104681834\ncreated by main.(*Telegraf).reloadLoop in goroutine 1\n\tgithub.com/influxdata/telegraf/cmd/telegraf/telegraf.go:184 +0x1a4\n\ngoroutine 30 gp=0x14000902fc0 m=nil [chan receive]:\nruntime.gopark(0x14000994e18?, 0x104d4490c?, 0x50?, 0x15?, 0x14002d12290?)\n\truntime/proc.go:460 +0xc0 fp=0x14000994dc0 sp=0x14000994da0 pc=0x104678c00\nruntime.chanrecv(0x1400149d1f0, 0x14000994f50, 0x1)\n\truntime/chan.go:667 +0x428 fp=0x14000994e40 sp=0x14000994dc0 pc=0x10460e248\nruntime.chanrecv2(0x140013e1550?, 0x10f1940f0?)\n\truntime/chan.go:514 +0x14 fp=0x14000994e70 sp=0x14000994e40 pc=0x10460de04\ngithub.com/influxdata/telegraf/agent.(*Agent).runOutputs(0x1400105df78, 0x14000658e80)\n\tgithub.com/influxdata/telegraf/agent/agent.go:864 +0x1c0 fp=0x14000994f90 sp=0x14000994e70 pc=0x104e49710\ngithub.com/influxdata/telegraf/agent.(*Agent).Run.func1()\n\tgithub.com/influxdata/telegraf/agent/agent.go:177 +0x4c fp=0x14000994fd0 sp=0x14000994f90 pc=0x104e440ec\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14000994fd0 sp=0x14000994fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*Agent).Run in goroutine 1\n\tgithub.com/influxdata/telegraf/agent/agent.go:175 +0x5d4\n\ngoroutine 31 gp=0x14000903180 m=nil [sync.WaitGroup.Wait, 72 minutes]:\nruntime.gopark(0x1148d2120?, 0x1?, 0xe0?, 0x41?, 0x1148a2860?)\n\truntime/proc.go:460 +0xc0 fp=0x14000106ce0 sp=0x14000106cc0 pc=0x104678c00\nruntime.goparkunlock(...)\n\truntime/proc.go:466\nruntime.semacquire1(0x140015ffcc8, 0x0, 0x1, 0x0, 0x19)\n\truntime/sema.go:192 +0x204 fp=0x14000106d30 sp=0x14000106ce0 pc=0x1046578f4\nsync.runtime_SemacquireWaitGroup(0x10464d8f0?, 0xe0?)\n\truntime/sema.go:114 +0x38 fp=0x14000106d70 sp=0x14000106d30 pc=0x10467a848\nsync.(*WaitGroup).Wait(0x140015ffcc0)\n\tsync/waitgroup.go:206 +0xa8 fp=0x14000106da0 sp=0x14000106d70 pc=0x10469be28\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs(0x1400105df78, {0x10f0ecff0, 0x14001499220}, {0x0?, 0x0?, 0x1148a2860?}, 0x14000658ec0)\n\tgithub.com/influxdata/telegraf/agent/agent.go:445 +0x468 fp=0x14000106f60 sp=0x14000106da0 pc=0x104e462d8\ngithub.com/influxdata/telegraf/agent.(*Agent).Run.func5()\n\tgithub.com/influxdata/telegraf/agent/agent.go:205 +0x54 fp=0x14000106fd0 sp=0x14000106f60 pc=0x104e43d34\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14000106fd0 sp=0x14000106fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*Agent).Run in goroutine 1\n\tgithub.com/influxdata/telegraf/agent/agent.go:203 +0x7c8\n\ngoroutine 32 gp=0x14000903340 m=nil [select, 2 minutes]:\nruntime.gopark(0x14000887f58?, 0x2?, 0xa0?, 0xd3?, 0x14000887f0c?)\n\truntime/proc.go:460 +0xc0 fp=0x14000887d90 sp=0x14000887d70 pc=0x104678c00\nruntime.selectgo(0x14000887f58, 0x14000887f08, 0x3e926aee6df?, 0x0, 0x0?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14000887ed0 sp=0x14000887d90 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*AlignedTicker).run(0x140016441c0, {0x10f0ecff0, 0x14001499310}, 0x14001499360)\n\tgithub.com/influxdata/telegraf/agent/tick.go:87 +0xd0 fp=0x14000887f80 sp=0x14000887ed0 pc=0x104e4cc70\ngithub.com/influxdata/telegraf/agent.(*AlignedTicker).start.func1()\n\tgithub.com/influxdata/telegraf/agent/tick.go:65 +0x50 fp=0x14000887fd0 sp=0x14000887f80 pc=0x104e4ca30\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14000887fd0 sp=0x14000887fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*AlignedTicker).start in goroutine 31\n\tgithub.com/influxdata/telegraf/agent/tick.go:63 +0x144\n\ngoroutine 33 gp=0x14000903500 m=nil [select, 2 minutes]:\nruntime.gopark(0x14001784f00?, 0x2?, 0x2?, 0x0?, 0x14001784efc?)\n\truntime/proc.go:460 +0xc0 fp=0x14001784d70 sp=0x14001784d50 pc=0x104678c00\nruntime.selectgo(0x14001784f00, 0x14001784ef8, 0x14000659380?, 0x0, 0x10f076250?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14001784eb0 sp=0x14001784d70 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*Agent).gatherLoop(0x1400105df78, {0x10f0ecff0, 0x14001499220}, {0x10f1399a0, 0x14000659380}, 0x14001359e00, {0x10f076250, 0x140016441c0}, 0x2540be400)\n\tgithub.com/influxdata/telegraf/agent/agent.go:573 +0x98 fp=0x14001784f30 sp=0x14001784eb0 pc=0x104e474b8\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs.func1(0x0?)\n\tgithub.com/influxdata/telegraf/agent/agent.go:441 +0x60 fp=0x14001784fb0 sp=0x14001784f30 pc=0x104e466b0\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs.gowrap1()\n\tgithub.com/influxdata/telegraf/agent/agent.go:442 +0x2c fp=0x14001784fd0 sp=0x14001784fb0 pc=0x104e4661c\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14001784fd0 sp=0x14001784fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*Agent).runInputs in goroutine 31\n\tgithub.com/influxdata/telegraf/agent/agent.go:439 +0xc0\n\ngoroutine 66 gp=0x140009036c0 m=nil [select, 2 minutes]:\nruntime.gopark(0x14001786f58?, 0x2?, 0xf0?, 0xd4?, 0x14001786f0c?)\n\truntime/proc.go:460 +0xc0 fp=0x14001786d90 sp=0x14001786d70 pc=0x104678c00\nruntime.selectgo(0x14001786f58, 0x14001786f08, 0x1b10cd36f3d?, 0x0, 0x0?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14001786ed0 sp=0x14001786d90 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*AlignedTicker).run(0x14001644200, {0x10f0ecff0, 0x14001499400}, 0x14001499450)\n\tgithub.com/influxdata/telegraf/agent/tick.go:87 +0xd0 fp=0x14001786f80 sp=0x14001786ed0 pc=0x104e4cc70\ngithub.com/influxdata/telegraf/agent.(*AlignedTicker).start.func1()\n\tgithub.com/influxdata/telegraf/agent/tick.go:65 +0x50 fp=0x14001786fd0 sp=0x14001786f80 pc=0x104e4ca30\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14001786fd0 sp=0x14001786fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*AlignedTicker).start in goroutine 31\n\tgithub.com/influxdata/telegraf/agent/tick.go:63 +0x144\n\ngoroutine 67 gp=0x14000903880 m=nil [select]:\nruntime.gopark(0x14001787f00?, 0x2?, 0x2?, 0x0?, 0x14001787efc?)\n\truntime/proc.go:460 +0xc0 fp=0x14001787d70 sp=0x14001787d50 pc=0x104678c00\nruntime.selectgo(0x14001787f00, 0x14001787ef8, 0x140006593a0?, 0x0, 0x10f076250?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14001787eb0 sp=0x14001787d70 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*Agent).gatherLoop(0x1400105df78, {0x10f0ecff0, 0x14001499220}, {0x10f1399a0, 0x140006593a0}, 0x14001359ec0, {0x10f076250, 0x14001644200}, 0x2540be400)\n\tgithub.com/influxdata/telegraf/agent/agent.go:573 +0x98 fp=0x14001787f30 sp=0x14001787eb0 pc=0x104e474b8\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs.func1(0x0?)\n\tgithub.com/influxdata/telegraf/agent/agent.go:441 +0x60 fp=0x14001787fb0 sp=0x14001787f30 pc=0x104e466b0\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs.gowrap1()\n\tgithub.com/influxdata/telegraf/agent/agent.go:442 +0x2c fp=0x14001787fd0 sp=0x14001787fb0 pc=0x104e4661c\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14001787fd0 sp=0x14001787fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*Agent).runInputs in goroutine 31\n\tgithub.com/influxdata/telegraf/agent/agent.go:439 +0xc0\n\ngoroutine 68 gp=0x14000903a40 m=nil [select, 2 minutes]:\nruntime.gopark(0x14001651f58?, 0x2?, 0x40?, 0xd6?, 0x14001651f0c?)\n\truntime/proc.go:460 +0xc0 fp=0x14001651d90 sp=0x14001651d70 pc=0x104678c00\nruntime.selectgo(0x14001651f58, 0x14001651f08, 0x399fd55f6ec?, 0x0, 0x0?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14001651ed0 sp=0x14001651d90 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*AlignedTicker).run(0x14001644240, {0x10f0ecff0, 0x140014994f0}, 0x14001499540)\n\tgithub.com/influxdata/telegraf/agent/tick.go:87 +0xd0 fp=0x14001651f80 sp=0x14001651ed0 pc=0x104e4cc70\ngithub.com/influxdata/telegraf/agent.(*AlignedTicker).start.func1()\n\tgithub.com/influxdata/telegraf/agent/tick.go:65 +0x50 fp=0x14001651fd0 sp=0x14001651f80 pc=0x104e4ca30\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14001651fd0 sp=0x14001651fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*AlignedTicker).start in goroutine 31\n\tgithub.com/influxdata/telegraf/agent/tick.go:63 +0x144\n\ngoroutine 69 gp=0x14000903c00 m=nil [select]:\nruntime.gopark(0x14001998f00?, 0x2?, 0x2?, 0x0?, 0x14001998efc?)\n\truntime/proc.go:460 +0xc0 fp=0x14001998d70 sp=0x14001998d50 pc=0x104678c00\nruntime.selectgo(0x14001998f00, 0x14001998ef8, 0x140006593c0?, 0x0, 0x10f076250?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14001998eb0 sp=0x14001998d70 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*Agent).gatherLoop(0x1400105df78, {0x10f0ecff0, 0x14001499220}, {0x10f1399a0, 0x140006593c0}, 0x140015fa000, {0x10f076250, 0x14001644240}, 0x6fc23ac00)\n\tgithub.com/influxdata/telegraf/agent/agent.go:573 +0x98 fp=0x14001998f30 sp=0x14001998eb0 pc=0x104e474b8\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs.func1(0x0?)\n\tgithub.com/influxdata/telegraf/agent/agent.go:441 +0x60 fp=0x14001998fb0 sp=0x14001998f30 pc=0x104e466b0\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs.gowrap1()\n\tgithub.com/influxdata/telegraf/agent/agent.go:442 +0x2c fp=0x14001998fd0 sp=0x14001998fb0 pc=0x104e4661c\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14001998fd0 sp=0x14001998fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*Agent).runInputs in goroutine 31\n\tgithub.com/influxdata/telegraf/agent/agent.go:439 +0xc0\n\ngoroutine 70 gp=0x14000903dc0 m=nil [select, 2 minutes]:\nruntime.gopark(0x14001652f58?, 0x2?, 0x90?, 0xd7?, 0x14001652f0c?)\n\truntime/proc.go:460 +0xc0 fp=0x14001652d90 sp=0x14001652d70 pc=0x104678c00\nruntime.selectgo(0x14001652f58, 0x14001652f08, 0x22ec71efd29?, 0x0, 0x0?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14001652ed0 sp=0x14001652d90 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*AlignedTicker).run(0x14001644280, {0x10f0ecff0, 0x140014995e0}, 0x14001499630)\n\tgithub.com/influxdata/telegraf/agent/tick.go:87 +0xd0 fp=0x14001652f80 sp=0x14001652ed0 pc=0x104e4cc70\ngithub.com/influxdata/telegraf/agent.(*AlignedTicker).start.func1()\n\tgithub.com/influxdata/telegraf/agent/tick.go:65 +0x50 fp=0x14001652fd0 sp=0x14001652f80 pc=0x104e4ca30\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14001652fd0 sp=0x14001652fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*AlignedTicker).start in goroutine 31\n\tgithub.com/influxdata/telegraf/agent/tick.go:63 +0x144\n\ngoroutine 71 gp=0x14000a02380 m=nil [select]:\nruntime.gopark(0x14001a00f00?, 0x2?, 0x0?, 0x0?, 0x14001a00efc?)\n\truntime/proc.go:460 +0xc0 fp=0x14001a00d70 sp=0x14001a00d50 pc=0x104678c00\nruntime.selectgo(0x14001a00f00, 0x14001a00ef8, 0x14000659400?, 0x0, 0x10f076250?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14001a00eb0 sp=0x14001a00d70 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*Agent).gatherLoop(0x1400105df78, {0x10f0ecff0, 0x14001499220}, {0x10f1399a0, 0x14000659400}, 0x140015fa0c0, {0x10f076250, 0x14001644280}, 0x6fc23ac00)\n\tgithub.com/influxdata/telegraf/agent/agent.go:573 +0x98 fp=0x14001a00f30 sp=0x14001a00eb0 pc=0x104e474b8\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs.func1(0x0?)\n\tgithub.com/influxdata/telegraf/agent/agent.go:441 +0x60 fp=0x14001a00fb0 sp=0x14001a00f30 pc=0x104e466b0\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs.gowrap1()\n\tgithub.com/influxdata/telegraf/agent/agent.go:442 +0x2c fp=0x14001a00fd0 sp=0x14001a00fb0 pc=0x104e4661c\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14001a00fd0 sp=0x14001a00fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*Agent).runInputs in goroutine 31\n\tgithub.com/influxdata/telegraf/agent/agent.go:439 +0xc0\n\ngoroutine 72 gp=0x14000a02540 m=nil [select]:\nruntime.gopark(0x14001653f58?, 0x2?, 0xe0?, 0xd8?, 0x14001653f0c?)\n\truntime/proc.go:460 +0xc0 fp=0x14001653d90 sp=0x14001653d70 pc=0x104678c00\nruntime.selectgo(0x14001653f58, 0x14001653f08, 0x3ed9fb2154?, 0x0, 0x0?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14001653ed0 sp=0x14001653d90 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*AlignedTicker).run(0x140016442c0, {0x10f0ecff0, 0x140014996d0}, 0x14001499720)\n\tgithub.com/influxdata/telegraf/agent/tick.go:87 +0xd0 fp=0x14001653f80 sp=0x14001653ed0 pc=0x104e4cc70\ngithub.com/influxdata/telegraf/agent.(*AlignedTicker).start.func1()\n\tgithub.com/influxdata/telegraf/agent/tick.go:65 +0x50 fp=0x14001653fd0 sp=0x14001653f80 pc=0x104e4ca30\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14001653fd0 sp=0x14001653fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*AlignedTicker).start in goroutine 31\n\tgithub.com/influxdata/telegraf/agent/tick.go:63 +0x144\n\ngoroutine 73 gp=0x14000a02700 m=nil [select]:\nruntime.gopark(0x1400188ff00?, 0x2?, 0x0?, 0x0?, 0x1400188fefc?)\n\truntime/proc.go:460 +0xc0 fp=0x1400188fd70 sp=0x1400188fd50 pc=0x104678c00\nruntime.selectgo(0x1400188ff00, 0x1400188fef8, 0x14000659420?, 0x0, 0x10f076250?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x1400188feb0 sp=0x1400188fd70 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*Agent).gatherLoop(0x1400105df78, {0x10f0ecff0, 0x14001499220}, {0x10f1399a0, 0x14000659420}, 0x140015fa180, {0x10f076250, 0x140016442c0}, 0x6fc23ac00)\n\tgithub.com/influxdata/telegraf/agent/agent.go:573 +0x98 fp=0x1400188ff30 sp=0x1400188feb0 pc=0x104e474b8\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs.func1(0x0?)\n\tgithub.com/influxdata/telegraf/agent/agent.go:441 +0x60 fp=0x1400188ffb0 sp=0x1400188ff30 pc=0x104e466b0\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs.gowrap1()\n\tgithub.com/influxdata/telegraf/agent/agent.go:442 +0x2c fp=0x1400188ffd0 sp=0x1400188ffb0 pc=0x104e4661c\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x1400188ffd0 sp=0x1400188ffd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*Agent).runInputs in goroutine 31\n\tgithub.com/influxdata/telegraf/agent/agent.go:439 +0xc0\n\ngoroutine 74 gp=0x14000a028c0 m=nil [select, 2 minutes]:\nruntime.gopark(0x14001654f58?, 0x2?, 0x30?, 0xda?, 0x14001654f0c?)\n\truntime/proc.go:460 +0xc0 fp=0x14001654d90 sp=0x14001654d70 pc=0x104678c00\nruntime.selectgo(0x14001654f58, 0x14001654f08, 0x1f6e59dcaf4?, 0x0, 0x0?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14001654ed0 sp=0x14001654d90 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*AlignedTicker).run(0x14001644300, {0x10f0ecff0, 0x140014997c0}, 0x14001499810)\n\tgithub.com/influxdata/telegraf/agent/tick.go:87 +0xd0 fp=0x14001654f80 sp=0x14001654ed0 pc=0x104e4cc70\ngithub.com/influxdata/telegraf/agent.(*AlignedTicker).start.func1()\n\tgithub.com/influxdata/telegraf/agent/tick.go:65 +0x50 fp=0x14001654fd0 sp=0x14001654f80 pc=0x104e4ca30\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14001654fd0 sp=0x14001654fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*AlignedTicker).start in goroutine 31\n\tgithub.com/influxdata/telegraf/agent/tick.go:63 +0x144\n\ngoroutine 75 gp=0x14000a02a80 m=nil [select]:\nruntime.gopark(0x14001b04f00?, 0x2?, 0x0?, 0x0?, 0x14001b04efc?)\n\truntime/proc.go:460 +0xc0 fp=0x14001b04d70 sp=0x14001b04d50 pc=0x104678c00\nruntime.selectgo(0x14001b04f00, 0x14001b04ef8, 0x14000659460?, 0x0, 0x10f076250?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14001b04eb0 sp=0x14001b04d70 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*Agent).gatherLoop(0x1400105df78, {0x10f0ecff0, 0x14001499220}, {0x10f1399a0, 0x14000659460}, 0x140015fa240, {0x10f076250, 0x14001644300}, 0x6fc23ac00)\n\tgithub.com/influxdata/telegraf/agent/agent.go:573 +0x98 fp=0x14001b04f30 sp=0x14001b04eb0 pc=0x104e474b8\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs.func1(0x0?)\n\tgithub.com/influxdata/telegraf/agent/agent.go:441 +0x60 fp=0x14001b04fb0 sp=0x14001b04f30 pc=0x104e466b0\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs.gowrap1()\n\tgithub.com/influxdata/telegraf/agent/agent.go:442 +0x2c fp=0x14001b04fd0 sp=0x14001b04fb0 pc=0x104e4661c\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14001b04fd0 sp=0x14001b04fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*Agent).runInputs in goroutine 31\n\tgithub.com/influxdata/telegraf/agent/agent.go:439 +0xc0\n\ngoroutine 76 gp=0x14000a02c40 m=nil [select, 2 minutes]:\nruntime.gopark(0x14001669f58?, 0x2?, 0x80?, 0xdb?, 0x14001669f0c?)\n\truntime/proc.go:460 +0xc0 fp=0x14001669d90 sp=0x14001669d70 pc=0x104678c00\nruntime.selectgo(0x14001669f58, 0x14001669f08, 0x3ac9d932eeb?, 0x0, 0x0?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14001669ed0 sp=0x14001669d90 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*AlignedTicker).run(0x14001644340, {0x10f0ecff0, 0x140014998b0}, 0x14001499900)\n\tgithub.com/influxdata/telegraf/agent/tick.go:87 +0xd0 fp=0x14001669f80 sp=0x14001669ed0 pc=0x104e4cc70\ngithub.com/influxdata/telegraf/agent.(*AlignedTicker).start.func1()\n\tgithub.com/influxdata/telegraf/agent/tick.go:65 +0x50 fp=0x14001669fd0 sp=0x14001669f80 pc=0x104e4ca30\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14001669fd0 sp=0x14001669fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*AlignedTicker).start in goroutine 31\n\tgithub.com/influxdata/telegraf/agent/tick.go:63 +0x144\n\ngoroutine 77 gp=0x14000a02e00 m=nil [select]:\nruntime.gopark(0x14000990f00?, 0x2?, 0x2?, 0x0?, 0x14000990efc?)\n\truntime/proc.go:460 +0xc0 fp=0x14000990d70 sp=0x14000990d50 pc=0x104678c00\nruntime.selectgo(0x14000990f00, 0x14000990ef8, 0x14000659480?, 0x0, 0x10f076250?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14000990eb0 sp=0x14000990d70 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*Agent).gatherLoop(0x1400105df78, {0x10f0ecff0, 0x14001499220}, {0x10f1399a0, 0x14000659480}, 0x140015fa300, {0x10f076250, 0x14001644340}, 0x2540be400)\n\tgithub.com/influxdata/telegraf/agent/agent.go:573 +0x98 fp=0x14000990f30 sp=0x14000990eb0 pc=0x104e474b8\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs.func1(0x0?)\n\tgithub.com/influxdata/telegraf/agent/agent.go:441 +0x60 fp=0x14000990fb0 sp=0x14000990f30 pc=0x104e466b0\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs.gowrap1()\n\tgithub.com/influxdata/telegraf/agent/agent.go:442 +0x2c fp=0x14000990fd0 sp=0x14000990fb0 pc=0x104e4661c\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14000990fd0 sp=0x14000990fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*Agent).runInputs in goroutine 31\n\tgithub.com/influxdata/telegraf/agent/agent.go:439 +0xc0\n\ngoroutine 39 gp=0x14000a82380 m=nil [select, 2 minutes]:\nruntime.gopark(0x14000995e08?, 0x4?, 0xf0?, 0xd8?, 0x14000995d48?)\n\truntime/proc.go:460 +0xc0 fp=0x14000995b90 sp=0x14000995b70 pc=0x104678c00\nruntime.selectgo(0x14000995e08, 0x14000995d40, 0x10f076228?, 0x0, 0x14002693a40?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14000995cd0 sp=0x14000995b90 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*Agent).flushLoop(0x1400105df78, {0x10f0ecff0, 0x14001682000}, 0x140013e1550, {0x10f076228, 0x1400168a000})\n\tgithub.com/influxdata/telegraf/agent/agent.go:909 +0x12c fp=0x14000995f00 sp=0x14000995cd0 pc=0x104e49cdc\ngithub.com/influxdata/telegraf/agent.(*Agent).runOutputs.func1(0x140013e1550)\n\tgithub.com/influxdata/telegraf/agent/agent.go:860 +0xe4 fp=0x14000995fb0 sp=0x14000995f00 pc=0x104e49a94\ngithub.com/influxdata/telegraf/agent.(*Agent).runOutputs.gowrap1()\n\tgithub.com/influxdata/telegraf/agent/agent.go:861 +0x2c fp=0x14000995fd0 sp=0x14000995fb0 pc=0x104e4997c\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14000995fd0 sp=0x14000995fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*Agent).runOutputs in goroutine 30\n\tgithub.com/influxdata/telegraf/agent/agent.go:854 +0x90\n\ngoroutine 78 gp=0x14000a02fc0 m=nil [select, 2 minutes]:\nruntime.gopark(0x14000887758?, 0x2?, 0xd0?, 0xdc?, 0x1400088770c?)\n\truntime/proc.go:460 +0xc0 fp=0x14000887590 sp=0x14000887570 pc=0x104678c00\nruntime.selectgo(0x14000887758, 0x14000887708, 0x2f956956615?, 0x0, 0x2?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x140008876d0 sp=0x14000887590 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*AlignedTicker).run(0x14001644380, {0x10f0ecff0, 0x140014999a0}, 0x140014999f0)\n\tgithub.com/influxdata/telegraf/agent/tick.go:87 +0xd0 fp=0x14000887780 sp=0x140008876d0 pc=0x104e4cc70\ngithub.com/influxdata/telegraf/agent.(*AlignedTicker).start.func1()\n\tgithub.com/influxdata/telegraf/agent/tick.go:65 +0x50 fp=0x140008877d0 sp=0x14000887780 pc=0x104e4ca30\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x140008877d0 sp=0x140008877d0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*AlignedTicker).start in goroutine 31\n\tgithub.com/influxdata/telegraf/agent/tick.go:63 +0x144\n\ngoroutine 79 gp=0x14000a03180 m=nil [select, 2 minutes]:\nruntime.gopark(0x14001881f00?, 0x2?, 0x2?, 0x0?, 0x14001881efc?)\n\truntime/proc.go:460 +0xc0 fp=0x14001881d70 sp=0x14001881d50 pc=0x104678c00\nruntime.selectgo(0x14001881f00, 0x14001881ef8, 0x140006594a0?, 0x0, 0x10f076250?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14001881eb0 sp=0x14001881d70 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*Agent).gatherLoop(0x1400105df78, {0x10f0ecff0, 0x14001499220}, {0x10f1399a0, 0x140006594a0}, 0x140015fa3c0, {0x10f076250, 0x14001644380}, 0x2540be400)\n\tgithub.com/influxdata/telegraf/agent/agent.go:573 +0x98 fp=0x14001881f30 sp=0x14001881eb0 pc=0x104e474b8\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs.func1(0x0?)\n\tgithub.com/influxdata/telegraf/agent/agent.go:441 +0x60 fp=0x14001881fb0 sp=0x14001881f30 pc=0x104e466b0\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs.gowrap1()\n\tgithub.com/influxdata/telegraf/agent/agent.go:442 +0x2c fp=0x14001881fd0 sp=0x14001881fb0 pc=0x104e4661c\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14001881fd0 sp=0x14001881fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*Agent).runInputs in goroutine 31\n\tgithub.com/influxdata/telegraf/agent/agent.go:439 +0xc0\n\ngoroutine 80 gp=0x14000a03340 m=nil [select, 2 minutes]:\nruntime.gopark(0x14001a05f58?, 0x2?, 0x20?, 0xde?, 0x14001a05f0c?)\n\truntime/proc.go:460 +0xc0 fp=0x14001a05d90 sp=0x14001a05d70 pc=0x104678c00\nruntime.selectgo(0x14001a05f58, 0x14001a05f08, 0x397a9d6de43?, 0x0, 0x0?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14001a05ed0 sp=0x14001a05d90 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*AlignedTicker).run(0x140016443c0, {0x10f0ecff0, 0x14001499a90}, 0x14001499ae0)\n\tgithub.com/influxdata/telegraf/agent/tick.go:87 +0xd0 fp=0x14001a05f80 sp=0x14001a05ed0 pc=0x104e4cc70\ngithub.com/influxdata/telegraf/agent.(*AlignedTicker).start.func1()\n\tgithub.com/influxdata/telegraf/agent/tick.go:65 +0x50 fp=0x14001a05fd0 sp=0x14001a05f80 pc=0x104e4ca30\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14001a05fd0 sp=0x14001a05fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*AlignedTicker).start in goroutine 31\n\tgithub.com/influxdata/telegraf/agent/tick.go:63 +0x144\n\ngoroutine 81 gp=0x14000a03500 m=nil [select, 2 minutes]:\nruntime.gopark(0x14001a06f00?, 0x2?, 0x2?, 0x0?, 0x14001a06efc?)\n\truntime/proc.go:460 +0xc0 fp=0x14001a06d70 sp=0x14001a06d50 pc=0x104678c00\nruntime.selectgo(0x14001a06f00, 0x14001a06ef8, 0x140006594e0?, 0x0, 0x10f076250?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14001a06eb0 sp=0x14001a06d70 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*Agent).gatherLoop(0x1400105df78, {0x10f0ecff0, 0x14001499220}, {0x10f1399a0, 0x140006594e0}, 0x140015fa480, {0x10f076250, 0x140016443c0}, 0x2540be400)\n\tgithub.com/influxdata/telegraf/agent/agent.go:573 +0x98 fp=0x14001a06f30 sp=0x14001a06eb0 pc=0x104e474b8\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs.func1(0x0?)\n\tgithub.com/influxdata/telegraf/agent/agent.go:441 +0x60 fp=0x14001a06fb0 sp=0x14001a06f30 pc=0x104e466b0\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs.gowrap1()\n\tgithub.com/influxdata/telegraf/agent/agent.go:442 +0x2c fp=0x14001a06fd0 sp=0x14001a06fb0 pc=0x104e4661c\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14001a06fd0 sp=0x14001a06fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*Agent).runInputs in goroutine 31\n\tgithub.com/influxdata/telegraf/agent/agent.go:439 +0xc0\n\ngoroutine 82 gp=0x14000a036c0 m=nil [select, 2 minutes]:\nruntime.gopark(0x14001b01f58?, 0x2?, 0x70?, 0xdf?, 0x14001b01f0c?)\n\truntime/proc.go:460 +0xc0 fp=0x14001b01d90 sp=0x14001b01d70 pc=0x104678c00\nruntime.selectgo(0x14001b01f58, 0x14001b01f08, 0x2fbabfeca44?, 0x0, 0x0?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14001b01ed0 sp=0x14001b01d90 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*AlignedTicker).run(0x14001644400, {0x10f0ecff0, 0x14001499b80}, 0x14001499bd0)\n\tgithub.com/influxdata/telegraf/agent/tick.go:87 +0xd0 fp=0x14001b01f80 sp=0x14001b01ed0 pc=0x104e4cc70\ngithub.com/influxdata/telegraf/agent.(*AlignedTicker).start.func1()\n\tgithub.com/influxdata/telegraf/agent/tick.go:65 +0x50 fp=0x14001b01fd0 sp=0x14001b01f80 pc=0x104e4ca30\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14001b01fd0 sp=0x14001b01fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*AlignedTicker).start in goroutine 31\n\tgithub.com/influxdata/telegraf/agent/tick.go:63 +0x144\n\ngoroutine 83 gp=0x14000a03880 m=nil [select, 2 minutes]:\nruntime.gopark(0x14000101f00?, 0x2?, 0x2?, 0x0?, 0x14000101efc?)\n\truntime/proc.go:460 +0xc0 fp=0x14000101d70 sp=0x14000101d50 pc=0x104678c00\nruntime.selectgo(0x14000101f00, 0x14000101ef8, 0x14000659500?, 0x0, 0x10f076250?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14000101eb0 sp=0x14000101d70 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*Agent).gatherLoop(0x1400105df78, {0x10f0ecff0, 0x14001499220}, {0x10f1399a0, 0x14000659500}, 0x140015fa540, {0x10f076250, 0x14001644400}, 0x2540be400)\n\tgithub.com/influxdata/telegraf/agent/agent.go:573 +0x98 fp=0x14000101f30 sp=0x14000101eb0 pc=0x104e474b8\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs.func1(0x0?)\n\tgithub.com/influxdata/telegraf/agent/agent.go:441 +0x60 fp=0x14000101fb0 sp=0x14000101f30 pc=0x104e466b0\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs.gowrap1()\n\tgithub.com/influxdata/telegraf/agent/agent.go:442 +0x2c fp=0x14000101fd0 sp=0x14000101fb0 pc=0x104e4661c\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14000101fd0 sp=0x14000101fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*Agent).runInputs in goroutine 31\n\tgithub.com/influxdata/telegraf/agent/agent.go:439 +0xc0\n\ngoroutine 40 gp=0x14000a82540 m=nil [select, 2 minutes]:\nruntime.gopark(0x14000886f28?, 0x2?, 0x40?, 0xc1?, 0x14000886f24?)\n\truntime/proc.go:460 +0xc0 fp=0x14000886da0 sp=0x14000886d80 pc=0x104678c00\nruntime.selectgo(0x14000886f28, 0x14000886f20, 0x140013e1550?, 0x0, 0x14001682000?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14000886ee0 sp=0x14000886da0 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*RollingTicker).run(0x1400168a000, {0x10f0ecff0, 0x14001682050}, 0x140016820a0)\n\tgithub.com/influxdata/telegraf/agent/tick.go:258 +0x94 fp=0x14000886f80 sp=0x14000886ee0 pc=0x104e4d5c4\ngithub.com/influxdata/telegraf/agent.(*RollingTicker).start.func1()\n\tgithub.com/influxdata/telegraf/agent/tick.go:248 +0x50 fp=0x14000886fd0 sp=0x14000886f80 pc=0x104e4d490\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14000886fd0 sp=0x14000886fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*RollingTicker).start in goroutine 39\n\tgithub.com/influxdata/telegraf/agent/tick.go:246 +0x14c\n\ngoroutine 84 gp=0x14000a03a40 m=nil [select, 2 minutes]:\nruntime.gopark(0x14001a01f58?, 0x2?, 0x40?, 0xc1?, 0x14001a01f0c?)\n\truntime/proc.go:460 +0xc0 fp=0x14001a01d90 sp=0x14001a01d70 pc=0x104678c00\nruntime.selectgo(0x14001a01f58, 0x14001a01f08, 0x399fde9e823?, 0x0, 0x0?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14001a01ed0 sp=0x14001a01d90 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*AlignedTicker).run(0x14001644440, {0x10f0ecff0, 0x14001499c70}, 0x14001499cc0)\n\tgithub.com/influxdata/telegraf/agent/tick.go:87 +0xd0 fp=0x14001a01f80 sp=0x14001a01ed0 pc=0x104e4cc70\ngithub.com/influxdata/telegraf/agent.(*AlignedTicker).start.func1()\n\tgithub.com/influxdata/telegraf/agent/tick.go:65 +0x50 fp=0x14001a01fd0 sp=0x14001a01f80 pc=0x104e4ca30\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14001a01fd0 sp=0x14001a01fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*AlignedTicker).start in goroutine 31\n\tgithub.com/influxdata/telegraf/agent/tick.go:63 +0x144\n\ngoroutine 85 gp=0x14000a03c00 m=nil [select, 2 minutes]:\nruntime.gopark(0x14001a02f00?, 0x2?, 0x2?, 0x0?, 0x14001a02efc?)\n\truntime/proc.go:460 +0xc0 fp=0x14001a02d70 sp=0x14001a02d50 pc=0x104678c00\nruntime.selectgo(0x14001a02f00, 0x14001a02ef8, 0x14000659540?, 0x0, 0x10f076250?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14001a02eb0 sp=0x14001a02d70 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*Agent).gatherLoop(0x1400105df78, {0x10f0ecff0, 0x14001499220}, {0x10f1399a0, 0x14000659540}, 0x140015fa600, {0x10f076250, 0x14001644440}, 0x2540be400)\n\tgithub.com/influxdata/telegraf/agent/agent.go:573 +0x98 fp=0x14001a02f30 sp=0x14001a02eb0 pc=0x104e474b8\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs.func1(0x0?)\n\tgithub.com/influxdata/telegraf/agent/agent.go:441 +0x60 fp=0x14001a02fb0 sp=0x14001a02f30 pc=0x104e466b0\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs.gowrap1()\n\tgithub.com/influxdata/telegraf/agent/agent.go:442 +0x2c fp=0x14001a02fd0 sp=0x14001a02fb0 pc=0x104e4661c\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14001a02fd0 sp=0x14001a02fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*Agent).runInputs in goroutine 31\n\tgithub.com/influxdata/telegraf/agent/agent.go:439 +0xc0\n\ngoroutine 86 gp=0x14000a03dc0 m=nil [select, 2 minutes]:\nruntime.gopark(0x14001889f58?, 0x2?, 0x90?, 0xc2?, 0x14001889f0c?)\n\truntime/proc.go:460 +0xc0 fp=0x14001889d90 sp=0x14001889d70 pc=0x104678c00\nruntime.selectgo(0x14001889f58, 0x14001889f08, 0x2cd19de3688?, 0x0, 0x0?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14001889ed0 sp=0x14001889d90 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*AlignedTicker).run(0x14001644480, {0x10f0ecff0, 0x14001499d60}, 0x14001499db0)\n\tgithub.com/influxdata/telegraf/agent/tick.go:87 +0xd0 fp=0x14001889f80 sp=0x14001889ed0 pc=0x104e4cc70\ngithub.com/influxdata/telegraf/agent.(*AlignedTicker).start.func1()\n\tgithub.com/influxdata/telegraf/agent/tick.go:65 +0x50 fp=0x14001889fd0 sp=0x14001889f80 pc=0x104e4ca30\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14001889fd0 sp=0x14001889fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*AlignedTicker).start in goroutine 31\n\tgithub.com/influxdata/telegraf/agent/tick.go:63 +0x144\n\ngoroutine 87 gp=0x14000b02380 m=nil [select, 2 minutes]:\nruntime.gopark(0x14000102e20?, 0x3?, 0x58?, 0x2c?, 0x14000102dc2?)\n\truntime/proc.go:460 +0xc0 fp=0x14000102c10 sp=0x14000102bf0 pc=0x104678c00\nruntime.selectgo(0x14000102e20, 0x14000102dbc, 0x14000102ea8?, 0x0, 0x14000102f00?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14000102d50 sp=0x14000102c10 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*Agent).gatherOnce(0x14000102f00?, {0x10f1399a0, 0x14000659560}, 0x140015fa6c0, {0x10f076250, 0x14001644480}, 0x2540be400)\n\tgithub.com/influxdata/telegraf/agent/agent.go:600 +0x140 fp=0x14000102eb0 sp=0x14000102d50 pc=0x104e47680\ngithub.com/influxdata/telegraf/agent.(*Agent).gatherLoop(0x1400105df78, {0x10f0ecff0, 0x14001499220}, {0x10f1399a0, 0x14000659560}, 0x140015fa6c0, {0x10f076250, 0x14001644480}, 0x2540be400)\n\tgithub.com/influxdata/telegraf/agent/agent.go:575 +0xbc fp=0x14000102f30 sp=0x14000102eb0 pc=0x104e474dc\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs.func1(0x0?)\n\tgithub.com/influxdata/telegraf/agent/agent.go:441 +0x60 fp=0x14000102fb0 sp=0x14000102f30 pc=0x104e466b0\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs.gowrap1()\n\tgithub.com/influxdata/telegraf/agent/agent.go:442 +0x2c fp=0x14000102fd0 sp=0x14000102fb0 pc=0x104e4661c\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14000102fd0 sp=0x14000102fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*Agent).runInputs in goroutine 31\n\tgithub.com/influxdata/telegraf/agent/agent.go:439 +0xc0\n\ngoroutine 88 gp=0x14000b02540 m=nil [select, 2 minutes]:\nruntime.gopark(0x14000996f58?, 0x2?, 0xe0?, 0xc3?, 0x14000996f0c?)\n\truntime/proc.go:460 +0xc0 fp=0x14000996d90 sp=0x14000996d70 pc=0x104678c00\nruntime.selectgo(0x14000996f58, 0x14000996f08, 0x14f4285e310?, 0x0, 0x0?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14000996ed0 sp=0x14000996d90 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*AlignedTicker).run(0x140016444c0, {0x10f0ecff0, 0x14001499e50}, 0x14001499ea0)\n\tgithub.com/influxdata/telegraf/agent/tick.go:87 +0xd0 fp=0x14000996f80 sp=0x14000996ed0 pc=0x104e4cc70\ngithub.com/influxdata/telegraf/agent.(*AlignedTicker).start.func1()\n\tgithub.com/influxdata/telegraf/agent/tick.go:65 +0x50 fp=0x14000996fd0 sp=0x14000996f80 pc=0x104e4ca30\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14000996fd0 sp=0x14000996fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*AlignedTicker).start in goroutine 31\n\tgithub.com/influxdata/telegraf/agent/tick.go:63 +0x144\n\ngoroutine 89 gp=0x14000b02700 m=nil [select, 2 minutes]:\nruntime.gopark(0x14000105e20?, 0x3?, 0x58?, 0x5c?, 0x14000105dc2?)\n\truntime/proc.go:460 +0xc0 fp=0x14000105c10 sp=0x14000105bf0 pc=0x104678c00\nruntime.selectgo(0x14000105e20, 0x14000105dbc, 0x14000105ea8?, 0x0, 0x14000105f00?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14000105d50 sp=0x14000105c10 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*Agent).gatherOnce(0x14000105f00?, {0x10f1399a0, 0x14000659580}, 0x140015fa780, {0x10f076250, 0x140016444c0}, 0x2540be400)\n\tgithub.com/influxdata/telegraf/agent/agent.go:600 +0x140 fp=0x14000105eb0 sp=0x14000105d50 pc=0x104e47680\ngithub.com/influxdata/telegraf/agent.(*Agent).gatherLoop(0x1400105df78, {0x10f0ecff0, 0x14001499220}, {0x10f1399a0, 0x14000659580}, 0x140015fa780, {0x10f076250, 0x140016444c0}, 0x2540be400)\n\tgithub.com/influxdata/telegraf/agent/agent.go:575 +0xbc fp=0x14000105f30 sp=0x14000105eb0 pc=0x104e474dc\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs.func1(0x0?)\n\tgithub.com/influxdata/telegraf/agent/agent.go:441 +0x60 fp=0x14000105fb0 sp=0x14000105f30 pc=0x104e466b0\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs.gowrap1()\n\tgithub.com/influxdata/telegraf/agent/agent.go:442 +0x2c fp=0x14000105fd0 sp=0x14000105fb0 pc=0x104e4661c\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14000105fd0 sp=0x14000105fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*Agent).runInputs in goroutine 31\n\tgithub.com/influxdata/telegraf/agent/agent.go:439 +0xc0\n\ngoroutine 90 gp=0x14000b028c0 m=nil [select, 2 minutes]:\nruntime.gopark(0x14001664758?, 0x2?, 0x30?, 0xc5?, 0x1400166470c?)\n\truntime/proc.go:460 +0xc0 fp=0x14001664590 sp=0x14001664570 pc=0x104678c00\nruntime.selectgo(0x14001664758, 0x14001664708, 0x356780a047d?, 0x0, 0x0?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x140016646d0 sp=0x14001664590 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*AlignedTicker).run(0x14001644500, {0x10f0ecff0, 0x14001676000}, 0x14001676050)\n\tgithub.com/influxdata/telegraf/agent/tick.go:87 +0xd0 fp=0x14001664780 sp=0x140016646d0 pc=0x104e4cc70\ngithub.com/influxdata/telegraf/agent.(*AlignedTicker).start.func1()\n\tgithub.com/influxdata/telegraf/agent/tick.go:65 +0x50 fp=0x140016647d0 sp=0x14001664780 pc=0x104e4ca30\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x140016647d0 sp=0x140016647d0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*AlignedTicker).start in goroutine 31\n\tgithub.com/influxdata/telegraf/agent/tick.go:63 +0x144\n\ngoroutine 91 gp=0x14000b02a80 m=nil [select, 2 minutes]:\nruntime.gopark(0x14001782f00?, 0x2?, 0x2?, 0x0?, 0x14001782efc?)\n\truntime/proc.go:460 +0xc0 fp=0x14001782d70 sp=0x14001782d50 pc=0x104678c00\nruntime.selectgo(0x14001782f00, 0x14001782ef8, 0x140006595c0?, 0x0, 0x10f076250?, 0x1)\n\truntime/select.go:351 +0x6bc fp=0x14001782eb0 sp=0x14001782d70 pc=0x104656b4c\ngithub.com/influxdata/telegraf/agent.(*Agent).gatherLoop(0x1400105df78, {0x10f0ecff0, 0x14001499220}, {0x10f1399a0, 0x140006595c0}, 0x140015fa840, {0x10f076250, 0x14001644500}, 0x2540be400)\n\tgithub.com/influxdata/telegraf/agent/agent.go:573 +0x98 fp=0x14001782f30 sp=0x14001782eb0 pc=0x104e474b8\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs.func1(0x0?)\n\tgithub.com/influxdata/telegraf/agent/agent.go:441 +0x60 fp=0x14001782fb0 sp=0x14001782f30 pc=0x104e466b0\ngithub.com/influxdata/telegraf/agent.(*Agent).runInputs.gowrap1()\n\tgithub.com/influxdata/telegraf/agent/agent.go:442 +0x2c fp=0x14001782fd0 sp=0x14001782fb0 pc=0x104e4661c\nruntime.goexit({})\n\truntime/asm_arm64.s:1268 +0x4 fp=0x14001782fd0 sp=0x14001782fd0 pc=0x104681834\ncreated by github.com/influxdata/telegraf/agent.(*Agent).runInputs in goroutine 31\n\tgithub.com/influxdata/telegraf/agent/agent.go:439 +0xc0\n\n[truncated because this field can not be longer than 65536 characters)\n```\n\n### System info\n\nTelegraf 1.36.2, Chrony 4.8\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1. add the inputs.chrony snippet above to telegraf.conf\n2. let it run for a few days\n3. eventually observe the panic\n\n### Expected behavior\n\nTelegraf should not panic.\n\n### Actual behavior\n\nThe process panics..\n\n### Additional info\n\nThis had previously been reported as https://github.com/influxdata/telegraf/issues/17453 (released in Telegraf 1.36.0), but it looks like the fix was not sufficient.",
      "solution": "@IngmarStein, thanks for reporting this!\n\nI\u2019ve checked both the upstream library and Telegraf, but I can\u2019t reproduce the panic with the fixed code. To help narrow this down, could you please confirm the following:\n\n1. `go version`\n\n2. `telegraf --version`\n\n3. **Do a clean rebuild** to rule out cache issues:\n\n   ```bash\n   go clean -modcache\n   go mod download\n   go build ./...\n   ```\n\nAlso, could you share the **full panic output with all goroutines**? That will help verify if the panic is actually occurring here or somewhere else.\n\n\n---\n\nThanks for sharing the details. I think we\u2019ve got to the bottom of this.\n\nLooking at your stack trace, the panic shows that `encoding/binary.Write` is being called with three parameters, and the first one isn\u2019t a `bytes.Buffer`. That pattern only exists in the **old code** (before the fix). In the current code, this can\u2019t happen.\n\nSo despite the version number saying `1.36.2`, it looks like you\u2019re actually running a binary that was built before the fix landed. The clue is that you mentioned installing Telegraf via **Homebrew**. Homebrew distributes pre-built binaries, and in this case the `1.36.2` bottle may have been compiled with older dependencies, so it doesn\u2019t include the fix.\n\nThat explains why you\u2019re still seeing the panic.\n\n**Next steps you can try:**\n\n1. Rebuild Telegraf from source (requires Go installed).\n2. Or wait until the Homebrew team updates the pre-built bottle.\n3. Or force Homebrew to build from source immediately:\n\n   ```bash\n   brew reinstall --build-from-source telegraf\n   ```\n\nSo the good news: the fix is already in the codebase. The problem here is with the distributed binary, not with the fix itself.\n",
      "labels": [
        "bug"
      ],
      "created_at": "2025-10-03T06:56:22Z",
      "closed_at": "2025-10-09T19:04:13Z",
      "url": "https://github.com/influxdata/telegraf/issues/17757",
      "comments_count": 5
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17734,
      "title": "[[secretstores.systemd]] Failing to parse systemd version on RHEL 9.6",
      "problem": "### Relevant telegraf.conf\n\n```toml\n# # Secret-store to access systemd secrets\n[[secretstores.systemd]]\n    id = \"systemd\"\n    prefix = \"telegraf.\"\n```\n\n### Logs from Telegraf\n\n```text\nloading config file /etc/telegraf/telegraf.conf failed: error parsing systemd, error initializing secret-store \"systemd\": unable to detect systemd version: strconv.Atoi: parsing \"252-51\": invalid syntax\n```\n\n### System info\n\nTelegraf 1.36.1 (git: HEAD@cbb7f134), Red Hat Enterprise Linux 9.6 (Plow)\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1. Enable the systemd secretstore in telegraf.conf on RHEL 9.6\n2. (re)Start the telegraf service\n\n\n### Expected behavior\n\nTelegraf to parse the systemd version and start as expected\n\n### Actual behavior\n\nTelegraf fails to start with the error\n`loading config file /etc/telegraf/telegraf.conf failed: error parsing systemd, error initializing secret-store \"systemd\": unable to detect systemd version: strconv.Atoi: parsing \"252-51\": invalid syntax`\n\n### Additional info\n\nRunning systemd-creds --version on RHEL 9.6 returns the following version for systemd:\nsystemd 252 (252-51.el9_6.2)\n\nI'm guessing that the version detection looks for the dot in the version string, but RHEL being RHEL, they've edited the version string differently than how it is returned on other distros.",
      "solution": "@AU-Penguin please test the binary in PR #17738, available as soon as CI finished all tests, and let me know if this fixes the issue!\n\n---\n\n> [@AU-Penguin](https://github.com/AU-Penguin) please test the binary in PR [#17738](https://github.com/influxdata/telegraf/pull/17738), available as soon as CI finished all tests, and let me know if this fixes the issue!\n\nConfirming this has indeed fixed the issue, telegraf now starts up and initialises the systemd secretstore as expected.",
      "labels": [
        "bug"
      ],
      "created_at": "2025-10-01T03:57:50Z",
      "closed_at": "2025-10-03T16:49:13Z",
      "url": "https://github.com/influxdata/telegraf/issues/17734",
      "comments_count": 2
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 10646,
      "title": "JSON_v2 parser error when parsing json array",
      "problem": "### Relevant telegraf.conf\r\n\r\n```toml\r\n[agent]\r\n  interval = \"100s\"\r\n  debug = true\r\n\r\n[[inputs.file]]\r\n    files = [\"20211209.json\"]\r\n    data_format = \"json_v2\"\r\n\r\n    [[inputs.file.json_v2]]\r\n        measurement_name = \"metric\"\r\n\r\n        [[inputs.file.json_v2.object]]\r\n            path = \"{time:result.0.data.0.timestamps,metric1:result.0.data.0.values,metric2:result.0.data.1.values}\"\r\n            tags = ['time','metric1','metric2']\r\n            timestamp_format=\"unix_ms\" \r\n[[outputs.file]]\r\n  ## Files to write to, \"stdout\" is a specially handled file.\r\n  files = [\"stdout\", \"metrics4.out\"]\r\n```\r\n\r\n\r\n### Logs from Telegraf\r\n\r\n2022-02-14T12:58:41Z I! Starting Telegraf 1.21.3\r\n2022-02-14T12:58:41Z I! Loaded inputs: file\r\n2022-02-14T12:58:41Z I! Loaded aggregators:\r\n2022-02-14T12:58:41Z I! Loaded processors:\r\n2022-02-14T12:58:41Z I! Loaded outputs: file\r\n2022-02-14T12:58:41Z I! Tags enabled: host=BE1CT887\r\n2022-02-14T12:58:41Z I! [agent] Config: Interval:1m40s, Quiet:false, Hostname:\"BE1CT887\", Flush Interval:10s\r\n2022-02-14T12:58:41Z D! [agent] Initializing plugins\r\n2022-02-14T12:58:41Z D! [agent] Connecting outputs\r\n2022-02-14T12:58:41Z D! [agent] Attempting connection to [outputs.file]\r\n2022-02-14T12:58:41Z D! [agent] Successfully connected to outputs.file\r\n2022-02-14T12:58:41Z D! [agent] Starting service inputs\r\n2022-02-14T12:58:51Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T12:59:01Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T12:59:11Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T12:59:21Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T12:59:31Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T12:59:41Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T12:59:51Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T13:00:01Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T13:00:11Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T13:00:21Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T13:00:31Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T13:00:41Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T13:00:51Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T13:01:01Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T13:01:11Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T13:01:21Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T13:01:31Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T13:01:41Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T13:01:51Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T13:02:01Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T13:02:11Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T13:02:21Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T13:02:31Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T13:02:41Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T13:02:51Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T13:03:01Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T13:03:11Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T13:03:21Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n\r\n### System info\r\n\r\ntelegraf-1.21.3\r\n\r\n### Docker\r\n\r\n_No response_\r\n\r\n### Steps to reproduce\r\n\r\nWhen I run the query I get this json \r\n{\"time\":[\r\n            1638486300000,\r\n            1638486600000,\r\n            1638486900000,\r\n            1638487200000,\r\n            1638487500000,\r\n            1638487800000,\r\n            1638488100000,\r\n            1638488400000,\r\n            1638488700000,\r\n            1638489000000,\r\n            1638489300000,\r\n            1638489600000,\r\n            1638489900000,\r\n            1638490200000,\r\n            1638490500000,\r\n            1638490800000,\r\n            1638491100000,\r\n            1638491400000,\r\n            1638491700000,\r\n            1638492000000,\r\n            1638492300000,\r\n            1638492600000,\r\n           ....\r\n          ],\"metric1\":[\r\n            2140,\r\n            1718,\r\n            1532,\r\n            2003,\r\n            2486,\r\n            1845,\r\n            1807,\r\n            1405,\r\n            1870,\r\n            1040,\r\n            1404,\r\n            886,\r\n            2599,\r\n            1092,\r\n            1349,\r\n            1088,\r\n            1244,\r\n            867,\r\n            1332,\r\n            1328,\r\n            1677,\r\n            1776,\r\n            1782,\r\n            1216,\r\n            1957,\r\n            2432,\r\n            ...\r\n          ],\"metric2\":[\r\n            1306,\r\n            1340,\r\n            898,\r\n            1227,\r\n            1707,\r\n            1553,\r\n            1319,\r\n            1060,\r\n            1122,\r\n            778,\r\n            749,\r\n            701,\r\n            2394,\r\n            810,\r\n            901,\r\n            930,\r\n            904,\r\n            794,\r\n            983,\r\n            1235,\r\n            942,\r\n           ...\r\n          ]}\r\n11Running with default timestamp works but as soon as I try to give time stamp I get this error\r\n\r\n### Expected behavior\r\n\r\nmetric metric1=.. metric2=...\r\n\r\n\r\n### Actual behavior\r\n\r\nI get nothing just \r\n\r\n2022-02-14T13:00:51Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T13:01:01Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T13:01:11Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T13:01:21Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n2022-02-14T13:01:31Z D! [outputs.file] Buffer fullness: 0 / 10000 metrics\r\n\r\n### Additional info\r\n\r\nI tried different variants including using the timestamp key. I also tried the data converter and starlark.",
      "solution": "I figured out a workaround. I used starlark processor. The issue in my opinion is handling the unix_ms with integer. I suspect it is an error while converting int to string. Therefore when I used the starlark I first converted it to int in the first part and then I used the starlark processor to convert it back to string and assign it to metric.time",
      "labels": [
        "bug",
        "area/json"
      ],
      "created_at": "2022-02-14T13:04:16Z",
      "closed_at": "2025-10-01T13:54:20Z",
      "url": "https://github.com/influxdata/telegraf/issues/10646",
      "comments_count": 7
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 9481,
      "title": "Update json_v2 parser to get field values out of arrays ",
      "problem": "<!--\r\nPlease redirect any questions about Telegraf usage to the InfluxData Community\r\nsite: https://community.influxdata.com\r\n\r\nCheck the documentation for the related plugin including the troubleshooting\r\nsection if available.\r\n-->\r\n\r\n### Relevant telegraf.conf:\r\n<!-- Place config in the toml code section. -->\r\n```[[inputs.mqtt_consumer]]\r\ndata_format = \"json_v2\"\r\n    [[inputs.mqtt_consumer.json_v2]]\r\n        [[inputs.mqtt_consumer.json_v2.object]]\r\n            path = \"@this\"\r\n            disable_prepend_keys = true\r\n           # tags = [\"data\"]\r\n        [inputs.mqtt_consumer.json_v2.object.fields]\r\n            cnt  = \"int\"\r\n            data = \"int\"\r\n            format  = \"int\"\r\n```\r\n\r\n### System info:\r\n\r\n<!-- Include Telegraf version, operating system, and other relevant details -->\r\nTelegraf Version 1.19.0, Linux Ubuntu 20.04, InfluxDB 1.8 and Grafana 7.5.4\r\n\r\n### Docker\r\n\r\n<!-- If your bug involves third party dependencies or services, it can be very helpful to provide a Dockerfile or docker-compose.yml that reproduces the environment you're testing against -->\r\n\r\n```\r\nversion: '3.6' \r\nservices: \r\n  telegraf: \r\n    image: telegraf:1.19.0-alpine \r\n\r\n```\r\n### Steps to reproduce:\r\n\r\n<!-- Describe the steps to reproduce the bug. -->\r\n\r\n### Expected behavior:\r\n\r\n<!-- Describe what you expected to happen when you performed the above steps. -->\r\nIm testing the new json_v2 parser with InfluxDB and Grafana in a TIG-Stack configuration managed by docker. The old json parser gave me the array values as field values:\r\n\r\n![old json parser](https://user-images.githubusercontent.com/85989020/124886793-e0a67c00-dfd4-11eb-9955-fcf82b275075.PNG)\r\n\r\n\r\n### Actual behavior:\r\n\r\n<!-- Describe what actually happened when you performed the above steps. -->\r\n\r\nThe new json parser doesn't allow arrays to get trough. \r\nThe solution discussed in this issue #9381 only gives me the array values as tag values: \r\n\r\n```\r\n[[inputs.mqtt_consumer]]\r\ndata_format = \"json_v2\"\r\n    [[inputs.mqtt_consumer.json_v2]]\r\n        [[inputs.mqtt_consumer.json_v2.object]]\r\n            path = \"@this\"\r\n            disable_prepend_keys = true\r\n            tags = [\"data\"]\r\n        [inputs.mqtt_consumer.json_v2.object.fields]\r\n            cnt  = \"int\"\r\n            format  = \"int\"\r\n```\r\n![new json parser](https://user-images.githubusercontent.com/85989020/124887174-38dd7e00-dfd5-11eb-8af3-9dc3b9b5ead2.PNG)\r\n\r\nThe GJSON Path should be \"data\". Thanks for the work @sspaink   \r\n\r\n\r\n### Additional info:\r\n\r\n<!-- Include gist of relevant config, logs, etc. -->\r\n",
      "solution": "Using the json_v2 parser with arrays is a pain and cannot be fixed easily. I suggest switching to the [XPath parser](https://github.com/influxdata/telegraf/tree/master/plugins/parsers/xpath) instead.",
      "labels": [
        "bug",
        "area/json"
      ],
      "created_at": "2021-07-08T08:17:26Z",
      "closed_at": "2025-10-01T13:53:57Z",
      "url": "https://github.com/influxdata/telegraf/issues/9481",
      "comments_count": 5
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17564,
      "title": "[inputs.mqtt_consumer] A panic may occur if the MQTT server loses network connectivity",
      "problem": "### Relevant telegraf.conf\n\n```toml\ntelegraf.conf\n\n\n[agent]\n  interval = \"30s\"\n  round_interval = true\n  metric_batch_size = 1000\n  metric_buffer_limit = 10000\n  collection_jitter = \"0s\"\n  flush_interval = \"10s\"\n  flush_jitter = \"0s\"\n  precision = \"0s\"\n  omit_hostname = true\n\n\ntelegraf.d/input-mqtt-server-1.conf\n\n\n[[inputs.mqtt_consumer]]\n  servers = [\"tcp://mqtt-server-1:9009\"]\n  topics = [\n    \"sensor/#\",\n  ]\n  topic_tag = \"\"\n  qos = 2\n  keepalive = \"30s\"\n  ping_timeout = \"10s\"\n  persistent_session = true\n  client_id = \"t-mqtt-server-1-n10\"\n  username=\"xxx\"\n  password=\"xxx\"\n  data_format = \"json\"\n  json_time_key = \"time\"\n\n\ntelegraf.d/input-mqtt-server-2.conf\n\n\n[[inputs.mqtt_consumer]]\n  servers = [\"wss://mqtt-server-2:443/mqtt\"]\n  topics = [\n    \"sensor/#\",\n  ]\n  topic_tag = \"\"\n  qos = 2\n  keepalive = \"30s\"\n  ping_timeout = \"10s\"\n  persistent_session = true\n  client_id = \"t-mqtt-server-2-n10\"\n  username=\"xxx\"\n  password=\"xxx\"\n  data_format = \"json\"\n  json_time_key = \"time\"\n```\n\n### Logs from Telegraf\n\n```text\nSep  2 17:52:21 tvikgge telegraf: 2025-09-02T09:52:21Z E! [inputs.mqtt_consumer] Error in plugin: connection lost: EOF\nSep  2 17:52:21 tvikgge telegraf: panic: send on closed channel\nSep  2 17:52:21 tvikgge telegraf: goroutine 52 [running]:\nSep  2 17:52:21 tvikgge telegraf: github.com/eclipse/paho%2emqtt%2egolang.(*router).matchAndDispatch.func2.ackFunc.4()\nSep  2 17:52:21 tvikgge telegraf: /home/dev/go/pkg/mod/github.com/eclipse/paho.mqtt.golang@v1.5.0/net.go:464 +0x28e\nSep  2 17:52:21 tvikgge telegraf: sync.(*Once).doSlow(0x0?, 0x56280?)\nSep  2 17:52:21 tvikgge telegraf: /home/dev/go-pkg/src/sync/once.go:78 +0xab\nSep  2 17:52:21 tvikgge telegraf: sync.(*Once).Do(...)\nSep  2 17:52:21 tvikgge telegraf: /home/dev/go-pkg/src/sync/once.go:69\nSep  2 17:52:21 tvikgge telegraf: github.com/eclipse/paho%2emqtt%2egolang.(*message).Ack(0x1047e80?)\nSep  2 17:52:21 tvikgge telegraf: /home/dev/go/pkg/mod/github.com/eclipse/paho.mqtt.golang@v1.5.0/message.go:77 +0x25\nSep  2 17:52:21 tvikgge telegraf: github.com/influxdata/telegraf/plugins/inputs/mqtt_consumer.(*MQTTConsumer).onDelivered(0xc0000ca508, {0x13cf190, 0xc000143c80})\nSep  2 17:52:21 tvikgge telegraf: /home/dev/telegraf-1.35.4/plugins/inputs/mqtt_consumer/mqtt_consumer.go:240 +0x12d\nSep  2 17:52:21 tvikgge telegraf: github.com/influxdata/telegraf/plugins/inputs/mqtt_consumer.(*MQTTConsumer).Start.func1()\nSep  2 17:52:21 tvikgge telegraf: /home/dev/telegraf-1.35.4/plugins/inputs/mqtt_consumer/mqtt_consumer.go:145 +0x79\nSep  2 17:52:21 tvikgge telegraf: created by github.com/influxdata/telegraf/plugins/inputs/mqtt_consumer.(*MQTTConsumer).Start in goroutine 1\nSep  2 17:52:21 tvikgge telegraf: /home/dev/telegraf-1.35.4/plugins/inputs/mqtt_consumer/mqtt_consumer.go:138 +0x147\nSep  2 17:52:21 tvikgge systemd: telegraf.service: main process exited, code=exited, status=2/INVALIDARGUMENT\nSep  2 17:52:21 tvikgge systemd: Unit telegraf.service entered failed state.\nSep  2 17:52:21 tvikgge systemd: telegraf.service failed.\nSep  2 17:52:21 tvikgge systemd: telegraf.service holdoff time over, scheduling restart.\nSep  2 17:52:21 tvikgge systemd: Stopped Telegraf.\nSep  2 17:52:21 tvikgge systemd: Starting Telegraf...\n```\n\n### System info\n\nTelegraf 1.35.4,CentOS Linux release 7.9.2009\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1. Panics may occur when an MQTT server loses network connectivity, but it does not happen every time.\n\n### Expected behavior\n\nAfter the network connection with the MQTT server is closed, no panic should occur.\n\n### Actual behavior\n\nAfter the network connection with the MQTT server is closed, a panic occurs.\n\n### Additional info\n\n\nI modified the code in `telegraf-1.35.4/plugins/inputs/mqtt_consumer/mqtt_consumer.go` like this, but I'm not sure if it's correct. I'm not very familiar with the Telegraf project's code.\n\n```diff\n--- telegraf-1.35.4/plugins/inputs/mqtt_consumer/mqtt_consumer.go   2025-08-19 02:43:39\n+++ mqtt_consumer.go    2025-09-03 10:57:38\n@@ -236,8 +236,15 @@\n        return\n    }\n\n-   if track.Delivered() && m.PersistentSession {\n-       msg.Ack()\n+   if track.Delivered() && m.PersistentSession && m.client.IsConnected() {\n+       func() {\n+           defer func() {\n+               if r := recover(); r != nil {\n+                   m.Log.Warnf(\"ack message failed, connection lost: %v\", r)\n+               }\n+           }()\n+           msg.Ack()\n+       }()\n    }\n\n    delete(m.messages, track.ID())\n```",
      "solution": "Honestly speaking, I would expect the upstream library code to handle this gracefully instead of forcing this down to us especially because your solution (which is the best we can do) is racy if the connection is lost _directly_ after you are checking for the connection but before calling `Ack`...\n\nWould you be willing to file an upstream issue at https://github.com/eclipse-paho/paho.mqtt.golang with your stacktrace?\nIf so, please link the issue here so we can keep track and update the library once this is fixed...\n\n---\n\nI have released version 1.5.1 of `paho.mqtt.golang` which, I believe, will address this issue (via [this [PR](https://github.com/eclipse-paho/paho.mqtt.golang/pull/729)). Would appreciate it if someone is able to test this with telegraf (expect it to take a while to verify the fix as the issue would be pretty intermittent).",
      "labels": [
        "bug",
        "upstream"
      ],
      "created_at": "2025-09-03T13:33:18Z",
      "closed_at": "2025-09-24T10:15:55Z",
      "url": "https://github.com/influxdata/telegraf/issues/17564",
      "comments_count": 6
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17607,
      "title": "Panic at startup with 1.36.1 with snowflake driver",
      "problem": "### Relevant telegraf.conf\n\n```toml\nNot sure which config triggers this.\n```\n\n### Logs from Telegraf\n\n```text\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal SIGSEGV: segmentation violation code=0x2 addr=0x20 pc=0x10312caf0]\n\ngoroutine 1 [running]:\nregexp.(*Regexp).ReplaceAllString(0x0, {0x14001392500, 0x40}, {0x109cf4482, 0xa})\n    regexp/regexp.go:575 +0x50\ngithub.com/snowflakedb/gosnowflake.maskClientSecret(...)\n    github.com/snowflakedb/gosnowflake@v1.16.0/secret_detector.go:56\ngithub.com/snowflakedb/gosnowflake.maskSecrets({0x14001392500?, 0xe?})\n    github.com/snowflakedb/gosnowflake@v1.16.0/secret_detector.go:67 +0x44\ngithub.com/snowflakedb/gosnowflake.(*sfTextFormatter).Format(0x14000a158c0, 0x14000f32540)\n    github.com/snowflakedb/gosnowflake@v1.16.0/log.go:71 +0x28\ngithub.com/sirupsen/logrus.(*Entry).write(0x14000f32540)\n    github.com/sirupsen/logrus@v1.9.3/entry.go:289 +0xc8\ngithub.com/sirupsen/logrus.(*Entry).log(0x14000f324d0, 0x4, {0x14001392500, 0x40})\n    github.com/sirupsen/logrus@v1.9.3/entry.go:252 +0x3dc\ngithub.com/sirupsen/logrus.(*Entry).Log(0x14000f324d0, 0x4, {0x1400109fb38?, 0x1?, 0x1?})\n    github.com/sirupsen/logrus@v1.9.3/entry.go:304 +0x60\ngithub.com/sirupsen/logrus.(*Entry).Logf(0x14000f324d0, 0x4, {0x109ec57c7?, 0x10?}, {0x1400127cbe0?, 0x1089ca370?, 0x140004e8cd8?})\n    github.com/sirupsen/logrus@v1.9.3/entry.go:349 +0x84\ngithub.com/sirupsen/logrus.(*Logger).Logf(0x14000c58900, 0x4, {0x109ec57c7, 0x35}, {0x1400127cbe0, 0x1, 0x1})\n    github.com/sirupsen/logrus@v1.9.3/logger.go:154 +0x74\ngithub.com/sirupsen/logrus.(*Logger).Infof(...)\n    github.com/sirupsen/logrus@v1.9.3/logger.go:168\ngithub.com/snowflakedb/gosnowflake.(*defaultLogger).Infof(0x140004e8cc0, {0x109ec57c7, 0x35}, {0x1400127cbe0, 0x1, 0x1})\n    github.com/snowflakedb/gosnowflake@v1.16.0/log.go:192 +0x5c\ngithub.com/snowflakedb/gosnowflake.newCrlCacheCleaner()\n    github.com/snowflakedb/gosnowflake@v1.16.0/crl.go:97 +0x24c\ngithub.com/snowflakedb/gosnowflake.init()\n    github.com/snowflakedb/gosnowflake@v1.16.0/crl.go:57 +0x470\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal SIGSEGV: segmentation violation code=0x2 addr=0x20 pc=0x102cecaf0]\n```\n\n### System info\n\nTelegraf 1.36.1\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1. Launch\n2. Crash\n3.\n...\n\n\n### Expected behavior\n\nNot crash :)\n\n### Actual behavior\n\nPanic\n\n### Additional info\n\n_No response_",
      "solution": "Yes it appears that this is caused by the above issue in the snowflake library. It looks like there are some listed workarounds in the issue, with the `HOME` and `SNOWFLAKE_CRL_ON_DISK_CACHE_DIR` environment variables. We will likely roll back this library update in the next telegraf update if we are unable to update it to a version that includes the patch from https://github.com/snowflakedb/gosnowflake/pull/1526",
      "labels": [
        "bug"
      ],
      "created_at": "2025-09-09T15:50:21Z",
      "closed_at": "2025-09-29T09:58:12Z",
      "url": "https://github.com/influxdata/telegraf/issues/17607",
      "comments_count": 4
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17662,
      "title": "One of the Mac servers is not reporting to Grafana.",
      "problem": "### Please direct all support questions to Slack or the forums. Thank you.\n\nOne of the Mac servers is not reporting to Grafana.\nTelegraf 1.34.4\n\nthis is conf file\n\n[agent]\n  interval = \"10s\"\n  round_interval = true\n  metric_batch_size = 1000\n  metric_buffer_limit = 10000\n  collection_jitter = \"0s\"\n  flush_interval = \"10s\"\n  flush_jitter = \"0s\"\n  precision = \"\"\n  hostname = \"mac05\"\n  omit_hostname = false\n\n[[outputs.graphite]]\n  servers = [\"1.2.3.4:2003\"]\n  prefix = \"\"\n  template = \"host.tags.measurement.field\"\n  timeout = 2\n\n[[inputs.cpu]]\n  percpu = true\n  totalcpu = true\n  collect_cpu_time = false\n  report_active = false\n\n[[inputs.disk]]\n  ignore_fs = [\"tmpfs\", \"devtmpfs\", \"devfs\", \"iso9660\", \"overlay\", \"aufs\", \"squashfs\"]\n\n[[inputs.mem]]\n  # no configuration\nWhen i check the telegraf.log\n\n2025-09-06T16:33:01Z E! **[agent] Error writing to outputs.graphite: could not write to any server in cluster**\n2025-09-06T16:33:11Z E! [agent] Error writing to outputs.graphite: could not write to any server in cluster\n2025-09-06T16:33:21Z E! [agent] Error writing to outputs.graphite: could not write to any server in cluster\nBut if when i run\ntelegraf --config /usr/local/etc/telegraf.conf --debug\n\nthen it works fine and reports the metrix to grafana. where should i look ???\n\n1> I have restart the server\n2> Restarted the telegraf\n3> I have check the IP server IP address\n\nActivity\n",
      "solution": "Please use Slack or the Forum for these type of questions! Well the error message says that Telegraf cannot write to the server because it's not connected. Please try to run telegraf with `--debug` or debug-mode enabled in the config. Furthermore, there must be more error messages saying what went wrong when connecting.\n\nIn either case, the problem is that Telegraf cannot connect to the server, check if the server can be reached from that machine, if Telegraf has sufficient permissions etc...",
      "labels": [
        "support"
      ],
      "created_at": "2025-09-17T13:49:42Z",
      "closed_at": "2025-09-24T18:09:40Z",
      "url": "https://github.com/influxdata/telegraf/issues/17662",
      "comments_count": 5
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 8818,
      "title": "outputs.kinesis silently drops metrics",
      "problem": "### Steps to reproduce:\r\n\r\nUse Kinesis normally.  The PutRecords API by design may only be partially successful.\r\n\r\n```\r\n{\r\n   \"EncryptionType\": \"string\",\r\n   \"FailedRecordCount\": number,      // The number of unsuccessfully processed records in a PutRecords request.\r\n   \"Records\": [ \r\n      { \r\n         \"ErrorCode\": \"string\",      // The error code for an individual record result. \r\n         \"ErrorMessage\": \"string\",\r\n         \"SequenceNumber\": \"string\",\r\n         \"ShardId\": \"string\"\r\n      }\r\n   ]\r\n}\r\n```\r\n\r\nIt's possible for the request itself to be successul, but in reality not actually process any records.\r\n\r\n### Expected behavior:\r\n\r\nAt a minimum communicate that metrics are being dropped.  Ideally this would be captured by `telegraf_internal_write`.\r\n\r\nBut I'm going to work towards adding retry support. \r\n\r\n### Actual behavior:\r\n\r\nMetrics get silently dropped.\r\n",
      "solution": "@JeffAshton,\r\n\r\nIt looks like you completed a PR via #8817, which says it is part 1. Was there an additional PR that went in such that this is now resolved?\r\n\r\nThanks!\n\n---\n\n> @JeffAshton,\r\n> \r\n> It looks like you completed a PR via #8817, which says it is part 1. Was there an additional PR that went in such that this is now resolved?\r\n> \r\n> Thanks!\r\n\r\nI actually ended up just writing my own output plugin:\r\n\r\nhttps://github.com/Brightspace/telegraf/tree/master/plugins/outputs/d2l_kinesis\r\n\r\nI wanted to gzip multiple metrics into a single kinesis record.  Ended up reducing our kinesis shards drastically and saved a bit of money.  Trying to maintain backwards compatibility with options like the [partition method](https://github.com/Brightspace/telegraf/blob/effa5c5c3665311743dd05b94612164fef1a7653/plugins/outputs/kinesis/kinesis.go#L36) complicated things beyond my needs.\r\n\r\nThe cost of a single metric per kinesis record is actually cost prohibitive from my experience.\n\n---\n\nThanks for the update and it is interesting to hear about the costs.\r\n\r\nIn terms of this bug, and silently dropping metrics, was this at least resolved?",
      "labels": [
        "bug",
        "waiting for response"
      ],
      "created_at": "2021-02-05T18:08:14Z",
      "closed_at": "2025-09-23T18:09:46Z",
      "url": "https://github.com/influxdata/telegraf/issues/8818",
      "comments_count": 6
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 12502,
      "title": "panic: runtime error: invalid memory address or nil pointer dereference",
      "problem": "### Relevant telegraf.conf\n\n```toml\n# Configuration for telegraf agent\r\n\r\n# Global tags can be specified here in key=\"value\" format.\r\n[global_tags]\r\n  dc = \"custom\" # will tag all metrics with dc=pionen\r\n  prod = \"true\"\r\n\r\n# add this to rc.conf and put relevant files there\r\n# telegraf_flags=\"-config-directory /usr/local/etc/telegraf.d\"\r\n\r\n[agent]\r\n  ## Default data collection interval for all inputs\r\n  interval = \"10s\"\r\n  ## Rounds collection interval to 'interval'\r\n  ## ie, if interval=\"10s\" then always collect on :00, :10, :20, etc.\r\n  round_interval = true\r\n\r\n  ## Telegraf will cache metric_buffer_limit metrics for each output, and will\r\n  ## flush this buffer on a successful write.\r\n  metric_buffer_limit = 1000\r\n  ## Flush the buffer whenever full, regardless of flush_interval.\r\n  flush_buffer_when_full = true\r\n\r\n  ## Collection jitter is used to jitter the collection by a random amount.\r\n  ## Each plugin will sleep for a random time within jitter before collecting.\r\n  ## This can be used to avoid many plugins querying things like sysfs at the\r\n  ## same time, which can have a measurable effect on the system.\r\n  collection_jitter = \"1s\"\r\n\r\n  ## Default flushing interval for all outputs. You shouldn't set this below\r\n  ## interval. Maximum flush_interval will be flush_interval + flush_jitter\r\n  flush_interval = \"10s\"\r\n  ## Jitter the flush interval by a random amount. This is primarily to avoid\r\n  ## large write spikes for users running a large number of telegraf instances.\r\n  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s\r\n  flush_jitter = \"5s\"\r\n\r\n  ## Run telegraf in debug mode\r\n  debug = false\r\n  ## Run telegraf in quiet mode\r\n  quiet = true\r\n  ## Override default hostname, if empty use os.Hostname()\r\n  hostname = \"xxx-hostname.yyy.zzz\"\r\n\r\n  ## Log target controls the destination for logs and can be one of \"file\",\r\n  ## \"stderr\" or, on Windows, \"eventlog\".  When set to \"file\", the output file\r\n  ## is determined by the \"logfile\" setting.\r\n  logtarget = \"file\"\r\n\r\n  ## Name of the file to be logged to when using the \"file\" logtarget.  If set to\r\n  ## the empty string then logs are written to stderr.\r\n  logfile = \"/var/log/telegraf/telegraf.log\"\r\n\r\n  ## The logfile will be rotated after the time interval specified.  When set\r\n  ## to 0 no time based rotation is performed.  Logs are rotated only when\r\n  ## written to, if there is no log activity rotation may be delayed.\r\n  logfile_rotation_interval = \"0h\"\r\n\r\n  ## The logfile will be rotated when it becomes larger than the specified\r\n  ## size.  When set to 0 no size based rotation is performed.\r\n  logfile_rotation_max_size = \"1MB\"\r\n\r\n  ## Maximum number of rotated archives to keep, any older logs are deleted.\r\n  ## If set to -1, no archives are removed.\r\n  logfile_rotation_max_archives = 5\r\n\r\n\r\n# Configuration for influxdb server to send metrics to\r\n[[outputs.influxdb]]\r\n  ## The full HTTP or UDP endpoint URL for your InfluxDB instance.\r\n  ## Multiple urls can be specified as part of the same cluster,\r\n  ## this means that only ONE of the urls will be written to each interval.\r\n  # urls = [\"udp://192.168.1.244:8090\"] # UDP endpoint example\r\n  urls = [\"https://influx.cxx.zzz:8086\"] # required\r\n  ## The target database for metrics (telegraf will create it if not exists).\r\n  database = \"pp_prod\" # required\r\n  ## Retention policy to write to.\r\n  retention_policy = \"default\"\r\n  ## Precision of writes, valid values are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\".\r\n  ## note: using \"s\" precision greatly improves InfluxDB compression.\r\n  precision = \"s\"\r\n\r\n  ## Write timeout (for the InfluxDB client), formatted as a string.\r\n  ## If not provided, will default to 5s. 0s means no timeout (not recommended).\r\n  timeout = \"5s\"\r\n\r\n  username = \"xxxxx\"\r\n  password = \"sEcReT\"\r\n  ## Set the user agent for HTTP POSTs (can be useful for log differentiation)\r\n  # user_agent = \"telegraf\"\r\n  ## Set UDP payload size, defaults to InfluxDB UDP Client default (512 bytes)\r\n  # udp_payload = 512\r\n\r\n\r\n# Statsd Server\r\n[[inputs.statsd]]\r\n  ## Address and port to host UDP listener on\r\n  service_address = \"localhost:8125\"\r\n  ## Delete gauges every interval (default=false)\r\n  delete_gauges = true\r\n  ## Delete counters every interval (default=false)\r\n  delete_counters = true\r\n  ## Delete sets every interval (default=false)\r\n  delete_sets = true\r\n  ## Delete timings & histograms every interval (default=true)\r\n  delete_timings = true\r\n  ## Percentiles to calculate for timing & histogram stats\r\n  percentiles = [70.0, 90.0]\r\n\r\n  ## separator to use between elements of a statsd metric\r\n  metric_separator = \"_\"\r\n\r\n  ## Number of UDP messages allowed to queue up, once filled,\r\n  ## the statsd server will start dropping packets\r\n  allowed_pending_messages = 10000\r\n\r\n  ## Number of timing/histogram values to track per-measurement in the\r\n  ## calculation of percentiles. Raising this limit increases the accuracy\r\n  ## of percentiles but also increases the memory usage and cpu time.\r\n  percentile_limit = 1000\r\n\r\n  ## UDP packet size for the server to listen for. This will depend on the size\r\n  ## of the packets that the client is sending, which is usually 1500 bytes.\r\n  udp_packet_size = 1500\r\n```\n```\n\n\n### Logs from Telegraf\n\n```text\npanic: runtime error: invalid memory address or nil pointer dereference\r\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x98b91f]\r\n\r\ngoroutine 1 [running]:\r\ngithub.com/awnumar/memguard/core.Purge.func1(0xc000155930)\r\n\tgithub.com/awnumar/memguard@v0.22.3/core/exit.go:23 +0x3f\r\ngithub.com/awnumar/memguard/core.Purge()\r\n\tgithub.com/awnumar/memguard@v0.22.3/core/exit.go:51 +0x25\r\ngithub.com/awnumar/memguard/core.Panic({0x5093f80, 0xc0001119a0})\r\n\tgithub.com/awnumar/memguard@v0.22.3/core/exit.go:85 +0x25\r\ngithub.com/awnumar/memguard/core.NewBuffer(0x20)\r\n\tgithub.com/awnumar/memguard@v0.22.3/core/buffer.go:73 +0x2d5\r\ngithub.com/awnumar/memguard/core.NewCoffer()\r\n\tgithub.com/awnumar/memguard@v0.22.3/core/coffer.go:30 +0x34\r\ngithub.com/awnumar/memguard/core.init.0()\r\n\tgithub.com/awnumar/memguard@v0.22.3/core/enclave.go:15 +0x2e\r\n\r\n```\n```\n\n\n### System info\n\nTelegraf 1.25.0, FreeBSD-13.1\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1. install latest telegraf 1.25.0 from ports or package\r\n2. start\r\n3. se it crash and restart in loop (due to the start script's daemon restart)\r\n\r\n\n\n### Expected behavior\n\nnot panic :)\n\n### Actual behavior\n\nit panics due to segmentation fault\n\n### Additional info\n\n1.24.x works fine. The problem was introduced with telegraf 1.25.0\r\n\r\nI am the packager for FreeBSD, btw.",
      "solution": "Is there a way simple, or a least not too hard, way to opt-out that module at buildtime? That would mean to opt out the secret stash feature, but as a short term solution, that would be preferred.\r\n\r\nMy other alternative is to let the start script fail and inform the user to reconfigure the jail if it does not have the `allow.sysvipc=1` jail parameter. \r\n\r\nThird alternative would be to downgrade the port until the problem is fixed.\r\n\r\nThe  first alternativ is preferred. Can we fix a patch for the source code that opts out that module?\r\n\r\nBest regards,\r\nPalle\n\n---\n\nAdding `allow.mlock = 1;` in the jail config resolved the panic. Thank you. No other changes needed to be made other than the jail config.\r\n\r\nDetails from my issue (for posterity/relevance):\r\n\r\nRunning FreeBSD 13.1 RELEASE (telegraf-1.25), problems started suddenly around a month ago. Configs have been untouched for a very long time, but telegraf updates are automated.\r\n\r\n```\r\npanic: runtime error: invalid memory address or nil pointer dereference\r\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x98b91f]\r\n\r\ngoroutine 1 [running]:\r\ngithub.com/awnumar/memguard/core.Purge.func1(0xc000143930)\r\n        github.com/awnumar/memguard@v0.22.3/core/exit.go:23 +0x3f\r\ngithub.com/awnumar/memguard/core.Purge()\r\n        github.com/awnumar/memguard@v0.22.3/core/exit.go:51 +0x25\r\ngithub.com/awnumar/memguard/core.Panic({0x5093f80, 0xc0000f59b0})\r\n        github.com/awnumar/memguard@v0.22.3/core/exit.go:85 +0x25\r\ngithub.com/awnumar/memguard/core.NewBuffer(0x20)\r\n        github.com/awnumar/memguard@v0.22.3/core/buffer.go:73 +0x2d5\r\ngithub.com/awnumar/memguard/core.NewCoffer()\r\n        github.com/awnumar/memguard@v0.22.3/core/coffer.go:30 +0x34\r\ngithub.com/awnumar/memguard/core.init.0()\r\n        github.com/awnumar/memguard@v0.22.3/core/enclave.go:15 +0x2e\r\n```\r\n\r\nErrors above are identical to those seen in issue #12403.\n\n---\n\nHi,\r\n\r\nWhile this is workaround, I would still like to pursue the idea of actually change the code to afvoid using the mlock. Is the mlock really necessary?",
      "labels": [
        "bug"
      ],
      "created_at": "2023-01-12T17:17:19Z",
      "closed_at": "2023-08-01T13:25:51Z",
      "url": "https://github.com/influxdata/telegraf/issues/12502",
      "comments_count": 14
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17511,
      "title": "Incorrect values of disk_read_bytes and disk_write_bytes recorded in Linux",
      "problem": "### Relevant telegraf.conf\n\n```toml\n[agent]\n  flush_interval = \"60s\"\n  flush_jitter = \"10s\"\n  collection_jitter = \"10s\"\n  interval = \"1m\"\n  metric_buffer_limit = 100000\n\n[[outputs.influxdb_v2]]\n  urls = // hidden\n  bucket = // hidden\n  token = // hidden\n  organization = // hidden\n\n# Read metrics about memory usage\n[[inputs.mem]]\n\n# Read metrics about disk IO by device\n[[inputs.diskio]]\n\n# Read metrics about cpu usage\n[[inputs.cpu]]\n\n# Read metrics about system load & uptime\n[[inputs.system]]\n\n# Read metrics about disk usage by mount point\n[[inputs.disk]]\n  interval = \"10m\"\n\n# Get kernel statistics from /proc/vmstat\n[[inputs.kernel_vmstat]]\n  # no configuration\n\n# Gathers huge pages measurements.\n[[inputs.hugepages]]\n# capturing only per_node stats as meminfo and root hugepage stats\n# can be calculated from the per_numa stats if needed\ntypes = [\"per_node\"]\n\n# Hacky way to update procstat config by running a cmd\n[[inputs.exec]]\n  commands = [\"python3 /etc/telegraf/scripts/update_procstat_config.py > /dev/null\"]\n  timeout = \"1m\"\n[[inputs.net]]\n[[inputs.nfsclient]]\n  interval = \"10m\"\n```\n\n### Logs from Telegraf\n\n```text\nNo relevant information is present in the logs.\n```\n\n### System info\n\nRed Hat Enterprise Linux 9.6 (Plow), Telegraf 1.33.0 \n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\nWe are recording multiple process statistics. However the values of `disk_read_bytes` and `disk_write_bytes` are recorded incorrectly.\nEg. proc stats.\n```\n[aagarwal@ts-mum1-dsr54 ~]$ cat /proc/196649/io\nrchar: 4508\nwchar: 536\nsyscr: 10\nsyscw: 67\nread_bytes: 12288\nwrite_bytes: 0\ncancelled_write_bytes: 0\n```\n\n\n### Expected behavior\n\ndisk_read_bytes: 12288\ndisk_write_bytes: 0\n\n### Actual behavior\n\ndisk_read_bytes: 4508\ndisk_write_bytes: 536\n\n### Additional info\n\nThere was a [change](https://github.com/shirou/gopsutil/commit/6bdbf6512651dbcd6cd809155b358ecfbebdc86b) in `gopsutil` last year. This changed the following mapping from proc io file.\n```\nread_bytes : ReadBytes -> read_bytes : DiskReadBytes\nrchar : Nothing -> rchar : ReadBytes\n// similarly for writes\n```\nHowever from what I can see in telegraf source code, this corresponding change should also be done in  `plugins/inputs/procstat/process.go`\n```\n\tio, err := p.IOCounters()\n\tif err == nil {\n\t\tfields[prefix+\"read_count\"] = io.ReadCount\n\t\tfields[prefix+\"write_count\"] = io.WriteCount\n\t\tfields[prefix+\"read_bytes\"] = io.ReadBytes\n\t\tfields[prefix+\"write_bytes\"] = io.WriteBytes\n\t}\n\n\t// Linux fixup for gopsutils exposing the disk-only-IO instead of the total\n\t// I/O as for example on Windows\n\tif rc, wc, err := collectTotalReadWrite(p); err == nil {\n\t\tfields[prefix+\"read_bytes\"] = rc\n\t\tfields[prefix+\"write_bytes\"] = wc\n\t\tfields[prefix+\"disk_read_bytes\"] = io.ReadBytes\n\t\tfields[prefix+\"disk_write_bytes\"] = io.WriteBytes\n\t}\n```\nAny help is much appreciated. Thanks!",
      "solution": "@apurbagarwal-ag would you be willing to submit a PR to fix the issue!?",
      "labels": [
        "bug"
      ],
      "created_at": "2025-08-25T11:59:37Z",
      "closed_at": "2025-09-22T10:59:35Z",
      "url": "https://github.com/influxdata/telegraf/issues/17511",
      "comments_count": 2
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17317,
      "title": "Efficient SQLITE configuration",
      "problem": "### Please direct all support questions to Slack or the forums. Thank you.\n\nHello, \n\nI was trying to use SQLite as output of telegraf that do, and the result was not as great as expected.\n(I know this is not the preferred use case, there are a lot of time series database that does exist, but for some reason I do want to use SQLite there).\n\nThe problem is that SQLite doesn't deal correctly with the time telegraf give so we do obtain, this kind of value as string:  \"2025-07-13 23:54:00 +0200 CEST\" for the timestamp row.\n\nIt's both:\n- not efficient\n- hard to convert to something useful (SQLite itself can't read it easily)\n\n```sqlite\n# sqlite version 3.46.1-6 has unixepoch function\nsqlite> SELECT unixepoch('2025-07-13 23:54:00 +0200 CEST') result;\n\n# give an \"empty\" result\u2026\n```\n\nA beginning of solution (thanks to @Hipska):\n\nThere is then 2 solutions:\n\n1. set the timestamp field to DATETIME, it should be handled more efficiently inside as numeric (https://sqlite.org/datatype3.html#affinity_name_examples), but still reflect as this un-optimal string which are hard to parse. THIS\u00a0IS\u00a0NOT\u00a0COMPATIBLE\u00a0WITH STRICT\u00a0MODE\u00a0OF\u00a0SQLITE.\n\n2. set the timestamp field to TEXT. and set _time_format in dsn to SQLite. This will store date in a more readable format but still not be efficient. Format give string like \"2025-07-14 14:55:30+02:00\".\n\n```sqlite\n# sqlite version 3.46.1-6 has unixepoch function\nsqlite> SELECT unixepoch('2025-07-14 14:55:30+02:00') result;\n1752497730\n```\n\n# Config specific for first solution:\n\n```toml\n  [outputs.sql.convert]\n    timestamp     = \"DATETIME\"\n```\n\n\n# Config specific for second solution:\n\n```toml\n[[outputs.sql]]\n  driver = \"sqlite\"\n  data_source_name = \"file:/path/to/my/database.db?_time_format=sqlite\"\n\n  [outputs.sql.convert]\n      timestamp     = \"TEXT\"\n```\n\nIf found that both of these solutions are not really great, I was just expecting timestamp row to be stored as an unixepoch integer.\n\nReading this : https://gitlab.com/cznic/sqlite/-/issues/35#note_454999720, I was a bit more confused about which layers should be fixed to have it working in a better way both efficiently and easy to parse : telegraf or the sqlite go layer, and how ?\n\nNote: if you are trying to use telegraf generated data in grafana, the problem is the same. The first format is hard to \"convert\" for grafana, converting to unixepoch() for the second does work. But not having to do such process seems to me the way things should work to have no so bad performances (we still are is sqlite, so don't expect much).\n\n",
      "solution": "> Hmmm I tested\n> \n> [[outputs.sql]]\n>   driver = \"sqlite\"\n>   data_source_name = \"file:/path/to/my/database.db?_time_format=sqlite\"\n> \n>   [outputs.sql.convert]\n>       timestamp     = \"INTEGER\"\n> \n> and\n> \n> select unixepoch(timestamp) from mytable; \n> \n> gives me the correct timestamp and the database scheme shows `INTEGER` as storage type for sqlite. Is this what you are looking for?\n\n* Using `_time_format` fix the issue of the \"string\" format stored which is not valid for sqlite, but setting integer as storage type is unhelpful in storing the data as integer. The fact is that sqlite when used in non-strict mode use type more as \"label\" than something else. You can put almost anything you want there, it will store your string ... as a string.\n\n* Using \"DATETIME\" is a specific case due to some logic used in the sqlite driver (see https://gitlab.com/cznic/sqlite/-/issues/35#note_454999720) , the _time_format feature seems to be bypassed and i'm not really sure how it is stored. I was thinking it's stored efficiently... but given it return the weird base format ('2025-07-13 23:54:00 +0200 CEST') and sqlite doesn't have any time based format... i'm almost sure now it's just a string because ... sqlite cannot do anything else with that.\n\nWhat i do want is really simple, i wanted just a simple path to store data to sqlite with an efficient format for the date, unixepoch seems to be the best solution, because having to parse string with select unixepoch() is a cool workaround but it's far from efficient.\n\nIt's unclear to me if the driver should do something else here. And i'm not sure if adding an option to \"force\" the timestamp to unixepoch instead of time in telegraf sql output code  is an acceptable fix.\n\nI do think about another \"not really nice but better than nothing\" solution which imply storing just as text and let sqlite do the jobs of convertion using a generated column. It probably imply doing scripts outside of telegraf which is not really nice (i briefly try hacking `table_template` to add the column but it doesn't seems to work this way), but it's currently the best solution i found to avoid doing \"unixepoch()\" for lots of rows during the analyzing phase.\n\nI do hope my comment is somehow clear, sqlite and sqlite driver are unfortunately a bit confusing on this issue.\n\n---\n\n@inkhey well this is a clear limitation of the driver not of Telegraf. We pass in the time and the driver does the conversion according to the SQL scheme. That's the contract here. If the driver does it in an unefficient way we, from the Telegraf side, have no means to change it.\n\nSo IMO this should be a feature-request/issue for the driver as with my solution above the table column type is `INTEGER` and the input to the driver is `time.Time` so I would expect the time to be converted to an integer... Maybe another `_time_format` can be added to the driver but that's out-of-scope for this project...\n\n---\n\nThe fact that you need to use `unixepoch()` [here](https://github.com/influxdata/telegraf/pull/17338/files#diff-ed3e1325ff89183d39e1cc12b9bb1cdf7d6eb89628fe154aa04f389678635c8fR202) to get an integer proves that the time is actually stored as a string.\n\nI'm wondering what the effect is when you choose the `Conversion.Timestamp` to be `DATETIME`?\n\nI agree that in any case it's an issue to be raised on the driver..",
      "labels": [
        "support"
      ],
      "created_at": "2025-07-14T14:16:19Z",
      "closed_at": "2025-07-21T18:09:40Z",
      "url": "https://github.com/influxdata/telegraf/issues/17317",
      "comments_count": 10
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 9265,
      "title": "Multiple wildcards in filter patterns (namepass etc) give inconsistent output",
      "problem": "### Relevant telegraf.conf:\r\n```toml\r\n[[inputs.exec]]\r\n  data_format = \"influx\"\r\n\r\n  commands = [\r\n    \"echo cpu.process32:testcase.exe:caserunner.2222 value=42\",\r\n    \"echo process32:testcase.exe:caserunner.2222 value=42\",\r\n    \"echo otherWantedMetric value=0\",\r\n    \"echo unwantedMetric value=0\",\r\n  ]\r\n\r\n  namepass = [\r\n        '*process32:*.exe:*.*',\r\n        'otherWantedMetric*',\r\n  ]\r\n```\r\n\r\n### System info:\r\n\r\nTelegraf 1.13.3 (git: HEAD da364558)\r\n\r\n### Steps to reproduce:\r\n\r\n1. Add multiple wildcard patterns to any filter clause\r\n2. Feed matching and non-matching lines into input\r\n\r\n### Expected behavior:\r\n\r\nMetrics are correctly filtered:\r\n```\r\n:!/usr/bin/telegraf -config etc.testcase.filter/telegraf.conf --test\r\n2021-05-12T11:09:19Z I! Starting Telegraf 1.13.3\r\n> cpu.process32:testcase.exe:caserunner.2222 value=42 1620817760000000000\r\n> process32:testcase.exe:caserunner.2222 value=42 1620817760000000000\r\n> otherWantedMetric value=0 1620817760000000000\r\n```\r\n\r\n### Actual behavior:\r\n\r\nSome metrics are dropped when they should be passed (or _vice versa_ for \"drop\" rules):\r\n```\r\n2021-05-12T11:10:46Z I! Starting Telegraf 1.13.3\r\n> otherWantedMetric value=0 1620817847000000000\r\n```\r\n\r\n### Additional info:\r\n\r\nRemoving `otherWantedMetric*` from the filter list, permits the \"process32\" ones to pass:\r\n```\r\n2021-05-12T11:11:49Z I! Starting Telegraf 1.13.3\r\n> cpu.process32:testcase.exe:caserunner.2222 value=42 1620817910000000000\r\n> process32:testcase.exe:caserunner.2222 value=42 1620817910000000000\r\n```\r\n\r\nDepending on the specific wildcards used in a set of patterns, there is sometimes also an ordering dependency, where switching two patterns filters correctly.\r\n\r\n> Yes, I know the naming here is an antipattern and they should be tagged like `process32,exe=testcase.exe,activity=caserunner,act_id=2222 cpu=42 ...` but these metrics are from a legacy system, we're having to ingest them using the existing names \"for historical reasons\".\r\n\r\nI believe this behaviour is due to bugs in the [gobwas/glob](https://github.com/gobwas/glob) library (several cases of unexpected pattern behaviour have been reported over its lifetime) and I've created issue gobwas/glob#50 there, but there may also be mitigations or changes to make in Telegraf:\r\n* Telegraf currently composes the individual patterns into a single glob \"alternates\" construct, e.g. `{*process32:*.exe:*.*,otherWantedMetric*}` which seems to be a case where gobwas/glob fails\r\n* it might be worth looking at alternative invocations, like explicit checks on each pattern - if the performance of that isn't dreadful\r\n* I'd suggest trying another glob library, but I've had a look and can't find anything suitable (supporting `*`, `?`, `[]` and `{}` which existing Telegraf users may depend on (although, if they're _not_ reliable, *cough*))\r\n* How much work would it be to implement a suitable/compatible glob function internal to telegraf vs. contributing fixes to the gobwas library ...",
      "solution": "@hackery I reproduced the issue in the upstream library we use with\n\n```golang\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/gobwas/glob\"\n)\n\nfunc main() {\n\ttests := []string{\n\t\t\"cpu.process32:testcase.exe:caserunner.2222\",\n\t\t\"process32:testcase.exe:caserunner.2222\",\n\t\t\"otherWantedMetric\",\n\t\t\"unwantedMetric\",\n\t}\n\n\tg := glob.MustCompile(\"{otherWantedMetric*,*process32:*.exe:*.*}\")\n\tfor _, t := range tests {\n\t\tfmt.Printf(\"%-50s -> %v\\n\", t, g.Match(t))\n\t}\n}\n```\n\nresulting in\n\n```\ncpu.process32:testcase.exe:caserunner.2222         -> false\nprocess32:testcase.exe:caserunner.2222             -> false\notherWantedMetric                                  -> true\nunwantedMetric                                     -> false\n```\n\nHowever, when removing the first asterix (i.e. `{otherWantedMetric,*process32:*.exe:*.*}` the matching is performed correctly. Unfortunately the [upstream library](https://github.com/gobwas/glob/tree/master) seems unmaintained...\n\n---\n\n@hackery I've implemented a workaround. Please test the binary in PR #17609 and let me know if it fixes the issue!",
      "labels": [
        "bug",
        "upstream"
      ],
      "created_at": "2021-05-12T11:40:12Z",
      "closed_at": "2025-09-15T16:36:36Z",
      "url": "https://github.com/influxdata/telegraf/issues/9265",
      "comments_count": 5
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17508,
      "title": "[inputs.diskio] Panic in gopsutil",
      "problem": "### Relevant telegraf.conf\n\n```toml\n[[inputs.diskio]]\n  devices = [\"ada?\", \"da?\"]\n```\n\n### Logs from Telegraf\n\n```text\n2025-08-23T11:07:30Z E! FATAL: [inputs.diskio] panicked: reflect: call of reflect.Value.Field on uint32 Value, Stack:\ngoroutine 124 [running]:\ngithub.com/influxdata/telegraf/agent.panicRecover(0x87cc392c0)\n\tgithub.com/influxdata/telegraf/agent/agent.go:1202 +0x6d\npanic({0xcaa020?, 0x87d9081c8?})\n\truntime/panic.go:792 +0x132\nreflect.Value.Field({0x8f0160?, 0x87cdc3608?, 0x87cdc3608?}, 0x32d2b00?)\n\treflect/value.go:1272 +0xcf\ngithub.com/shirou/gopsutil/v4/internal/common.(*decoder).value(0x87cdc3598, {0x1f5ee60?, 0x87cdc3608?, 0x120?})\n\tgithub.com/shirou/gopsutil/v4@v4.25.7/internal/common/binary.go:475 +0x8be\ngithub.com/shirou/gopsutil/v4/internal/common.Read({0x314eae0, 0x87d96ed50}, {0x3232af0, 0x11fd91a0}, {0x680600, 0x87cdc3608})\n\tgithub.com/shirou/gopsutil/v4@v4.25.7/internal/common/binary.go:221 +0x8c5\ngithub.com/shirou/gopsutil/v4/disk.parsedevstat({_, _, _})\n\tgithub.com/shirou/gopsutil/v4@v4.25.7/disk/disk_freebsd.go:158 +0xfc\ngithub.com/shirou/gopsutil/v4/disk.IOCountersWithContext({0x32012c0, 0x11fd91a0}, {0x0, 0x0, 0x0?})\n\tgithub.com/shirou/gopsutil/v4@v4.25.7/disk/disk_freebsd.go:119 +0x1a7\ngithub.com/shirou/gopsutil/v4/disk.IOCounters(...)\n\tgithub.com/shirou/gopsutil/v4@v4.25.7/disk/disk.go:84\ngithub.com/influxdata/telegraf/plugins/common/psutil.(*SystemPS).DiskIO(0x87c557300?, {0x0?, 0x0?, 0x0?})\n\tgithub.com/influxdata/telegraf/plugins/common/psutil/ps.go:195 +0x30\ngithub.com/influxdata/telegraf/plugins/inputs/diskio.(*DiskIO).Gather(0x87cc39200, {0x3247420, 0x87cf1c820})\n\tgithub.com/influxdata/telegraf/plugins/inputs/diskio/diskio.go:73 +0x6d\ngithub.com/influxdata/telegraf/models.(*RunningInput).Gather(0x87cc392c0, {0x3247420, 0x87cf1c820})\n\tgithub.com/influxdata/telegraf/models/running_input.go:260 +0x251\ngithub.com/influxdata/telegraf/agent.(*Agent).gatherOnce.func1()\n\tgithub.com/influxdata/telegraf/agent/agent.go:590 +0x58\ncreated by github.com/influxdata/telegraf/agent.(*Agent).gatherOnce in goroutine 14\n\tgithub.com/influxdata/telegraf/agent/agent.go:588 +0xf7\n\ngoroutine 1 [sync.WaitGroup.Wait]:\nsync.runtime_SemacquireWaitGroup(0x87d927680?)\n\truntime/sema.go:110 +0x25\nsync.(*WaitGroup).Wait(0x87d906010?)\n\tsync/\n```\n\n### System info\n\nTelegraf 1.35.4, FreeBSD 14.2\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1. Try to run Telegraf with the config above on FreeBSD\n\n### Expected behavior\n\nNo panics\n\n### Actual behavior\n\nTelegraf panics on start\n\n### Additional info\n\nIt looks like https://github.com/shirou/gopsutil/issues/1898 is probably related to this. Also, Telegraf 1.35.3 doesn't have this bug.",
      "solution": "FYI: the upstream fixed it in https://github.com/shirou/gopsutil/releases/tag/v4.25.8\n\n---\n\nFreeBSD PR \nhttps://bugs.freebsd.org/bugzilla/show_bug.cgi?id=289207\nOfficial port has been fixed by the maintainer as well \nhttps://cgit.freebsd.org/ports/commit/?id=326381de144dfea777d50a6483191fa691b58d08\n\n---\n\n@sergeymakinen and @mdtancsa please test the binary in PR #17567, available as soon as CI finished the tests, and let me know if this fixes the issue!",
      "labels": [
        "bug"
      ],
      "created_at": "2025-08-23T11:18:27Z",
      "closed_at": "2025-09-11T08:29:25Z",
      "url": "https://github.com/influxdata/telegraf/issues/17508",
      "comments_count": 4
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17606,
      "title": "panic: runtime error: invalid memory address or nil pointer dereference [signal SIGSEGV: segmentation violation code=0x1 addr=0x8 pc=0x8d3597]",
      "problem": "Hello, since today's update of the Telegraf Docker container installed on an Unraid server version 7.1.4, it is no longer running. What has changed to cause this error?\nHere is the error:\n\n`panic: runtime error: invalid memory address or nil pointer dereference\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x8 pc=0x8d3597]\n\ngoroutine 1 [running]:\ngithub.com/influxdata/telegraf/selfstat.(*Collector).Register(0x0, {0xa6bfff8, 0x5}, {0xa6fc5b8, 0xd}, 0x0)\n        /go/src/github.com/influxdata/telegraf/selfstat/collector.go:36 +0x77\ngithub.com/influxdata/telegraf/plugins/outputs/influxdb.(*InfluxDB).Init(0xc00114bb08)\n        /go/src/github.com/influxdata/telegraf/plugins/outputs/influxdb/influxdb.go:96 +0x22b\ngithub.com/influxdata/telegraf/models.(*RunningOutput).Init(0x11c9fb30?)\n        /go/src/github.com/influxdata/telegraf/models/running_output.go:165 +0xd6\ngithub.com/influxdata/telegraf/agent.(*Agent).InitPlugins(0xc00248c010)\n        /go/src/github.com/influxdata/telegraf/agent/agent.go:254 +0x5e5\ngithub.com/influxdata/telegraf/agent.(*Agent).Run(0xc00248c010, {0xb721170, 0xc0025986e0})\n        /go/src/github.com/influxdata/telegraf/agent/agent.go:121 +0x357\nmain.(*Telegraf).runAgent(0xc00133c6e0, {0xb721170, 0xc0025986e0}, 0x0?)\n        /go/src/github.com/influxdata/telegraf/cmd/telegraf/telegraf.go:565 +0x19e5\nmain.(*Telegraf).reloadLoop(0xc00133c6e0)\n        /go/src/github.com/influxdata/telegraf/cmd/telegraf/telegraf.go:207 +0x26b\nmain.(*Telegraf).Run(0xc00133c6e0)\n        /go/src/github.com/influxdata/telegraf/cmd/telegraf/telegraf_posix.go:20 +0xb8\nmain.runApp.func1(0xc00137dd40)\n        /go/src/github.com/influxdata/telegraf/cmd/telegraf/main.go:261 +0xdc9\ngithub.com/urfave/cli/v2.(*Command).Run(0xc0013ca2c0, 0xc00137dd40, {0xc0000b20b0, 0x1, 0x1})\n        /go/pkg/mod/github.com/urfave/cli/v2@v2.27.7/command.go:276 +0x7c2\ngithub.com/urfave/cli/v2.(*App).RunContext(0xc000b7fc00, {0xb720e80, 0x124353a0}, {0xc0000b20b0, 0x1, 0x1})\n        /go/pkg/mod/github.com/urfave/cli/v2@v2.27.7/app.go:333 +0x5a5\ngithub.com/urfave/cli/v2.(*App).Run(...)\n        /go/pkg/mod/github.com/urfave/cli/v2@v2.27.7/app.go:307\nmain.runApp({0xc0000b20b0, 0x1, 0x1}, {0xb66e100, 0xc0000f2070}, {0xb6a8260, 0xc000f85800}, {0xb6a8288, 0xc00122f500}, {0xb720d30, ...})\n        /go/src/github.com/influxdata/telegraf/cmd/telegraf/main.go:419 +0x11ff\nmain.main()\n        /go/src/github.com/influxdata/telegraf/cmd/telegraf/main.go:433 +0xe8`",
      "solution": "Sounds like this was fixed with https://github.com/influxdata/telegraf/pull/17605 in v1.36.1?",
      "labels": [],
      "created_at": "2025-09-09T15:18:08Z",
      "closed_at": "2025-09-10T14:21:36Z",
      "url": "https://github.com/influxdata/telegraf/issues/17606",
      "comments_count": 5
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17603,
      "title": "1.36.0 hard crash",
      "problem": "### Relevant telegraf.conf\n\n```toml\n[agent]\ncollection_jitter = \"0s\"\ndebug = false\nflush_interval = \"10s\"\nflush_jitter = \"0s\"\nhostname = \"cloud.aws.MYHOST.com\"\ninterval = \"10s\"\nlogfile = \"\"\nlogfile_rotation_interval = \"0h\"\nlogfile_rotation_max_archives = 5\nlogfile_rotation_max_size = \"0MB\"\nmetric_batch_size = 1000\nmetric_buffer_limit = 15680\nomit_hostname = false\nprecision = \"\"\nquiet = false\nround_interval = true\n[global_tags]\nenv = \"prod\"\nhost = \"ip-10-2-39-20\"\nname = \"cloud\"\nprovider = \"aws\"\nrole = \"aws\"\nserver = \"cloud\"\n[[inputs.cpu]]\npercpu = true\ntotalcpu = true\n[[inputs.disk]]\nignore_fs = [\"tmpfs\", \"devtmpfs\", \"devfs\", \"iso9660\", \"overlay\", \"aufs\", \"squashfs\", \"hugetlbfs\", \"mqueue\", \"cgroup\", \"cgroup2\", \"pstore\", \"autofs\", \"debugfs\", \"configfs\", \"fusectl\", \"rpc_pipefs\", \"binfmt_misc\", \"nsfs\"]\n[[inputs.diskio]]\n[[inputs.internal]]\ncollect_memstats = true\n[[inputs.interrupts]]\ncpu_as_tag = true\n[[inputs.kernel]]\n[[inputs.linux_sysctl_fs]]\n[[inputs.mem]]\n[[inputs.net]]\n[[inputs.netstat]]\n[[inputs.nstat]]\n[[inputs.processes]]\n[[inputs.statsd]]\n[[inputs.swap]]\n[[inputs.system]]\n[[outputs.influxdb]]\ndatabase = \"telegraf\"\nskip_database_creation = false\nurls = [\"http://grafana-i.aws.MYHOST.com:8086\"]\n[outputs.influxdb.tagdrop]\ninfluxdb_database = [\"*\"]\n```\n\n### Logs from Telegraf\n\n```text\ntelegraf[2464431]: panic: runtime error: invalid memory address or nil pointer dereference\ntelegraf[2464431]: [signal SIGSEGV: segmentation violation code=0x1 addr=0x8 pc=0x8d3597]\ntelegraf[2464431]: goroutine 1 [running]:\ntelegraf[2464431]: github.com/influxdata/telegraf/selfstat.(*Collector).Register(0x0, {0xa6bfff8, 0x5}, {0xa6fc5b8, 0xd}, 0x0)\ntelegraf[2464431]: #011/go/src/github.com/influxdata/telegraf/selfstat/collector.go:36 +0x77\ntelegraf[2464431]: github.com/influxdata/telegraf/plugins/outputs/influxdb.(*InfluxDB).Init(0xc00109efc8)\ntelegraf[2464431]: #011/go/src/github.com/influxdata/telegraf/plugins/outputs/influxdb/influxdb.go:96 +0x22b\ntelegraf[2464431]: github.com/influxdata/telegraf/models.(*RunningOutput).Init(0x11c9fb30?)\ntelegraf[2464431]: panic: runtime error: invalid memory address or nil pointer dereference\ntelegraf[2464431]: [signal SIGSEGV: segmentation violation code=0x1 addr=0x8 pc=0x8d3597]\ntelegraf[2464431]: goroutine 1 [running]:\ntelegraf[2464431]: github.com/influxdata/telegraf/selfstat.(*Collector).Register(0x0, {0xa6bfff8, 0x5}, {0xa6fc5b8, 0xd}, 0x0)\ntelegraf[2464431]: #011/go/src/github.com/influxdata/telegraf/selfstat/collector.go:36 +0x77\ntelegraf[2464431]: github.com/influxdata/telegraf/plugins/outputs/influxdb.(*InfluxDB).Init(0xc00109efc8)\ntelegraf[2464431]: #011/go/src/github.com/influxdata/telegraf/plugins/outputs/influxdb/influxdb.go:96 +0x22b\ntelegraf[2464431]: github.com/influxdata/telegraf/models.(*RunningOutput).Init(0x11c9fb30?)\ntelegraf[2464431]: #011/go/src/github.com/influxdata/telegraf/models/running_output.go:165 +0xd6\ntelegraf[2464431]: github.com/influxdata/telegraf/agent.(*Agent).InitPlugins(0xc00102e3e0)\ntelegraf[2464431]: #011/go/src/github.com/influxdata/telegraf/agent/agent.go:254 +0x5e5\ntelegraf[2464431]: github.com/influxdata/telegraf/agent.(*Agent).Run(0xc00102e3e0, {0xb721170, 0xc0010c1130})\ntelegraf[2464431]: #011/go/src/github.com/influxdata/telegraf/agent/agent.go:121 +0x357\ntelegraf[2464431]: main.(*Telegraf).runAgent(0xc001098420, {0xb721170, 0xc0010c1130}, 0x20?)\ntelegraf[2464431]: #011/go/src/github.com/influxdata/telegraf/models/running_output.go:165 +0xd6\ntelegraf[2464431]: github.com/influxdata/telegraf/agent.(*Agent).InitPlugins(0xc00102e3e0)\ntelegraf[2464431]: #011/go/src/github.com/influxdata/telegraf/agent/agent.go:254 +0x5e5\ntelegraf[2464431]: github.com/influxdata/telegraf/agent.(*Agent).Run(0xc00102e3e0, {0xb721170, 0xc0010c1130})\ntelegraf[2464431]: #011/go/src/github.com/influxdata/telegraf/agent/agent.go:121 +0x357\ntelegraf[2464431]: main.(*Telegraf).runAgent(0xc001098420, {0xb721170, 0xc0010c1130}, 0x20?)\ntelegraf[2464431]: #011/go/src/github.com/influxdata/telegraf/cmd/telegraf/telegraf.go:565 +0x19e5\ntelegraf[2464431]: #011/go/src/github.com/influxdata/telegraf/cmd/telegraf/telegraf.go:565 +0x19e5\ntelegraf[2464431]: main.(*Telegraf).reloadLoop(0xc001098420)\ntelegraf[2464431]: main.(*Telegraf).reloadLoop(0xc001098420)\ntelegraf[2464431]: #011/go/src/github.com/influxdata/telegraf/cmd/telegraf/telegraf.go:207 +0x26b\ntelegraf[2464431]: #011/go/src/github.com/influxdata/telegraf/cmd/telegraf/telegraf.go:207 +0x26b\ntelegraf[2464431]: main.(*Telegraf).Run(0xc001098420)\ntelegraf[2464431]: main.(*Telegraf).Run(0xc001098420)\ntelegraf[2464431]: #011/go/src/github.com/influxdata/telegraf/cmd/telegraf/telegraf_posix.go:20 +0xb8\ntelegraf[2464431]: #011/go/src/github.com/influxdata/telegraf/cmd/telegraf/telegraf_posix.go:20 +0xb8\ntelegraf[2464431]: main.runApp.func1(0xc0010be4c0)\ntelegraf[2464431]: main.runApp.func1(0xc0010be4c0)\ntelegraf[2464431]: #011/go/src/github.com/influxdata/telegraf/cmd/telegraf/main.go:261 +0xdc9\ntelegraf[2464431]: #011/go/src/github.com/influxdata/telegraf/cmd/telegraf/main.go:261 +0xdc9\ntelegraf[2464431]: github.com/urfave/cli/v2.(*Command).Run(0xc0010c8000, 0xc0010be4c0, {0xc0000b2050, 0x5, 0x5})\ntelegraf[2464431]: github.com/urfave/cli/v2.(*Command).Run(0xc0010c8000, 0xc0010be4c0, {0xc0000b2050, 0x5, 0x5})\ntelegraf[2464431]: #011/go/pkg/mod/github.com/urfave/cli/v2@v2.27.7/command.go:276 +0x7c2\ntelegraf[2464431]: #011/go/pkg/mod/github.com/urfave/cli/v2@v2.27.7/command.go:276 +0x7c2\ntelegraf[2464431]: github.com/urfave/cli/v2.(*App).RunContext(0xc0005ff000, {0xb720e80, 0x124353a0}, {0xc0000b2050, 0x5, 0x5})\ntelegraf[2464431]: github.com/urfave/cli/v2.(*App).RunContext(0xc0005ff000, {0xb720e80, 0x124353a0}, {0xc0000b2050, 0x5, 0x5})\ntelegraf[2464431]: #011/go/pkg/mod/github.com/urfave/cli/v2@v2.27.7/app.go:333 +0x5a5\ntelegraf[2464431]: github.com/urfave/cli/v2.(*App).Run(...)\ntelegraf[2464431]: #011/go/pkg/mod/github.com/urfave/cli/v2@v2.27.7/app.go:307\ntelegraf[2464431]: #011/go/pkg/mod/github.com/urfave/cli/v2@v2.27.7/app.go:333 +0x5a5\ntelegraf[2464431]: github.com/urfave/cli/v2.(*App).Run(...)\ntelegraf[2464431]: #011/go/pkg/mod/github.com/urfave/cli/v2@v2.27.7/app.go:307\ntelegraf[2464431]: main.runApp({0xc0000b2050, 0x5, 0x5}, {0xb66e100, 0xc0000e0070}, {0xb6a8260, 0xc00102e1e0}, {0xb6a8288, 0xc0010c6000}, {0xb720d30, ...})\ntelegraf[2464431]: #011/go/src/github.com/influxdata/telegraf/cmd/telegraf/main.go:419 +0x11ff\ntelegraf[2464431]: main.runApp({0xc0000b2050, 0x5, 0x5}, {0xb66e100, 0xc0000e0070}, {0xb6a8260, 0xc00102e1e0}, {0xb6a8288, 0xc0010c6000}, {0xb720d30, ...})\ntelegraf[2464431]: #011/go/src/github.com/influxdata/telegraf/cmd/telegraf/main.go:419 +0x11ff\ntelegraf[2464431]: main.main()\ntelegraf[2464431]: #011/go/src/github.com/influxdata/telegraf/cmd/telegraf/main.go:433 +0xe8\ntelegraf[2464431]: main.main()\ntelegraf[2464431]: #011/go/src/github.com/influxdata/telegraf/cmd/telegraf/main.go:433 +0xe8\n```\n\n### System info\n\nDebian 12\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1. run 1.35.4 with the above config. expect it to work\n2. upgrade to 1.36.0\n3. have the mentioned failure happen immediately and consistently\n...\n\n\n### Expected behavior\n\nNo crashes\n\n### Actual behavior\n\nHard crash, will not start.\n\nDowngrading to 1.35.4, everything works again.\n\n### Additional info\n\nOuch.",
      "solution": "Can reproduce this bug and the workaround (downgrade version) on a two Debian 12 installations (no Docker; one is a VM and the other is running on bare metal).\n\nMy config file looks very similar and I can redact and share it if it would help, but I expect this one will be very easy to reproduce. I also expect to see this happen on 40+ hosts that are going to auto-update within the next 24 hours...\n\nThis command will be a life saver for many people: `apt install telegraf=1.35.4-1`\n\n---\n\nThank you for opening this issue, I've reproduced it locally as well. I see the issue and will make a PR shortly, we will make a new release as soon as possible to make sure this is fixed\n\n---\n\n~~1.36.0 is still the latest version available in the apt repos. Any ETA for getting a new release out?~~\n\nI have confirmed that 1.36.1-1 is now available in the apt repos and it fixed this issue. Thank you for the fast response, @mstrandboge ",
      "labels": [
        "bug"
      ],
      "created_at": "2025-09-08T22:15:58Z",
      "closed_at": "2025-09-09T00:09:16Z",
      "url": "https://github.com/influxdata/telegraf/issues/17603",
      "comments_count": 5
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17452,
      "title": "Elasticsearch custom Headers cannot contain coma \",\"",
      "problem": "### Relevant telegraf.conf\n\n```toml\n[[outputs.elasticsearch]]\n  urls = [\"#REDACTED#\"]\n  timeout = \"1m\"\n  flush_interval = \"30s\"\n  enable_sniffer = false\n  health_check_interval = \"0s\"\n  index_name = \"winlog-%Y.%m.%d\"\n  manage_template = false\n  template_name = \"telegraf\"\n  overwrite_template = false\n  [outputs.elasticsearch.headers]\n    \"VL-Msg-Field\" = \"win_eventlog.Message\"\n    \"VL-Time-Field\" = \"@timestamp\"\n    \"VL-Stream-Fields\" = \"tag.Source,tag.Channel,tag.EventID\"\n    \"AccountID\" = \"#REDACTED#\"\n    \"ProjectID\" = \"#REDACTED#\"\n```\n\n### Logs from Telegraf\n\n```text\n2025-08-11T13:38:27Z I! Loading config: C:\\ProgramData\\Telegraf\\telegraf.conf\n2025-08-11T13:38:27Z I! Starting Telegraf 1.35.3 brought to you by InfluxData the makers of InfluxDB\n2025-08-11T13:38:27Z I! Available plugins: 237 inputs, 9 aggregators, 34 processors, 26 parsers, 65 outputs, 5 secret-stores\n2025-08-11T13:38:27Z I! Loaded inputs: win_eventlog\n2025-08-11T13:38:27Z I! Loaded aggregators:\n2025-08-11T13:38:27Z I! Loaded processors:\n2025-08-11T13:38:27Z I! Loaded secretstores:\n2025-08-11T13:38:27Z I! Loaded outputs: elasticsearch\n2025-08-11T13:38:27Z I! Tags enabled: host=#REDACTED# type=WorkStation\n2025-08-11T13:38:27Z I! [agent] Config: Interval:10s, Quiet:false, Hostname:\"#REDACTED#\", Flush Interval:10s\n2025-08-11T13:38:27Z W! [agent] The default value of 'skip_processors_after_aggregators' will change to 'true' with Telegraf v1.40.0! If you need the current default behavior, please explicitly set the option to 'false'!\n2025-08-11T13:38:27Z D! [agent] Initializing plugins\n2025-08-11T13:38:27Z D! [agent] Connecting outputs\n2025-08-11T13:38:27Z D! [agent] Attempting connection to [outputs.elasticsearch]\n2025-08-11T13:38:27Z D! [outputs.elasticsearch] Disabling health check\n2025-08-11T13:38:27Z I! [outputs.elasticsearch] Elasticsearch version: \"8.9.0\"\n2025-08-11T13:38:27Z D! [agent] Successfully connected to outputs.elasticsearch\n2025-08-11T13:38:27Z D! [agent] Starting service inputs\n2025-08-11T13:38:27Z D! [inputs.win_eventlog] Subscription handle id:2\n2025-08-11T13:38:57Z D! [outputs.elasticsearch] Wrote batch of 3 metrics in 4.2567ms\n2025-08-11T13:38:57Z D! [outputs.elasticsearch] Buffer fullness: 0 / 10000 metrics\n2025-08-11T13:39:13Z D! [agent] Stopping service inputs\n2025-08-11T13:39:13Z D! [agent] Input channel closed\n2025-08-11T13:39:13Z I! [agent] Hang on, flushing any cached metrics before shutdown\n2025-08-11T13:39:13Z D! [outputs.elasticsearch] Wrote batch of 4 metrics in 2.1904ms\n2025-08-11T13:39:13Z D! [outputs.elasticsearch] Buffer fullness: 0 / 10000 metrics\n2025-08-11T13:39:13Z I! [agent] Stopping running outputs\n2025-08-11T13:39:13Z D! [agent] Stopped Successfully\n```\n\n### System info\n\nTelegraf 1.35.3, Windows 10-11 2016-2019-2022\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1- Create an ElasticSearch ouput, including [outputs.elasticsearch.headers]\n2- Add a header to your ouput, with value containing one or more coma \",\"\n\n### Expected behavior\n\nThe ElasticSearch url should receive the events and the custom headers with the whole value specified in the config, including comas \",\"\nIn my case, the header \"VL-Stream-Fields\" should have the value \"tag.Source,tag.Channel,tag.EventID\"\n\n### Actual behavior\n\nThe ElasticSearch url receives the events but only the first part of the custom header value : whatever is in front of the first coma \",\"\nIn my case, the header \"VL-Stream-Fields\" have the value \"tag.Source\"\n\n### Additional info\n\nHere is the code responsible for this behaviour (I can create a PR if you want)\nhttps://github.com/influxdata/telegraf/blob/4c8055c08a9acdcb18e49fe3a0679b0fb7f5dcd0/plugins/outputs/elasticsearch/elasticsearch.go#L197C1-L197C48\n\n```\n\t\tfor k, vals := range a.Headers {\n\t\t\tfor _, v := range strings.Split(vals, \",\") { // why is the header value split ?\n\t\t\t\theaders.Add(k, v) // The same key is added multiple times, only the first is taken into account\n\t\t\t}\n\t\t}\n```",
      "solution": "The comma-splitting logic added in PR #15477 assumed that commas in HTTP header values indicate multiple entries. While this worked for some cases, it breaks valid scenarios where commas are part of a single value (per RFC 7230), for example:\n\n* `VL-Stream-Fields = \"tag.Source,tag.Channel,tag.EventID\"`\n* `Content-Disposition = \"attachment; filename=\\\"file,name.csv\\\"\"`\n\nTo fix this, we should stop guessing and let users explicitly define intent:\n\n* **Single values (commas preserved):**\n\n```toml\n\"VL-Stream-Fields\" = \"tag.Source,tag.Channel,tag.EventID\"\n```\n\n* **Multiple values:**\n\n```toml\n\"Accept\" = [\"application/json\", \"text/plain\"]\n```\n\nThis approach is backward compatible, unambiguous, standards-compliant, and future-proof. Existing single-string configs will still work, while array syntax will allow proper multiple header support when needed. Implementation impact is minimal, but it solves the issue cleanly and avoids breaking valid header values.\n",
      "labels": [
        "bug"
      ],
      "created_at": "2025-08-11T13:52:48Z",
      "closed_at": "2025-09-05T17:43:31Z",
      "url": "https://github.com/influxdata/telegraf/issues/17452",
      "comments_count": 2
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17238,
      "title": "[inputs.diskio] Unable to gather disk name",
      "problem": "### Relevant telegraf.conf\n\n```toml\n[global_tags]\n[agent]\n  interval = \"10s\"\n  round_interval = true\n  metric_batch_size = 1000\n  metric_buffer_limit = 10000\n  collection_jitter = \"0s\"\n  flush_interval = \"10s\"\n  flush_jitter = \"0s\"\n  precision = \"0s\"\n  hostname = \"some-host\"\n  omit_hostname = false\n[[outputs.influxdb]]\n  password = \"sdrhsdh\"\n  username = \"etsdjhs\"\n  urls = [\"http://1.2.3.4:8086\"]\n  database = \"db\"\n[[inputs.cpu]]\n  percpu = true\n  totalcpu = true\n  collect_cpu_time = false\n  report_active = false\n[[inputs.disk]]\n  mount_points = [\"/\",\"/home\",\"/disk2\",\"/disk3\",\"/disk4\",\"/disk5\",\"/disk6\"]\n  ignore_fs = [\"tmpfs\", \"devtmpfs\", \"devfs\", \"iso9660\", \"overlay\", \"aufs\", \"squashfs\"]\n\n[[inputs.mem]]\n\n[[inputs.diskio]]\n[[inputs.kernel]]\n[[inputs.mem]]\n[[inputs.processes]]\n[[inputs.swap]]\n[[inputs.system]]\n[[inputs.net]]\n[[inputs.netstat]]\n```\n\n### Logs from Telegraf\n\n```text\nW! [inputs.diskio] Unable to gather disk name for \"nvme13c13n1\": error reading /dev/nvme13c13n1: no such file or directory\nW! [inputs.diskio] Unable to gather disk name for \"nvme6c6n1\": error reading /dev/nvme6c6n1: no such file or directory\nW! [inputs.diskio] Unable to gather disk name for \"nvme8c8n1\": error reading /dev/nvme8c8n1: no such file or directory\nW! [inputs.diskio] Unable to gather disk name for \"nvme11c11n1\": error reading /dev/nvme11c11n1: no such file or directory\nW! [inputs.diskio] Unable to gather disk name for \"nvme16c16n1\": error reading /dev/nvme16c16n1: no such file or directory\nW! [inputs.diskio] Unable to gather disk name for \"nvme2c2n1\": error reading /dev/nvme2c2n1: no such file or directory\nW! [inputs.diskio] Unable to gather disk name for \"nvme22c22n1\": error reading /dev/nvme22c22n1: no such file or directory\nW! [inputs.diskio] Unable to gather disk name for \"nvme18c18n1\": error reading /dev/nvme18c18n1: no such file or directory\nW! [inputs.diskio] Unable to gather disk name for \"nvme20c20n1\": error reading /dev/nvme20c20n1: no such file or directory\nW! [inputs.diskio] Unable to gather disk name for \"nvme21c21n1\": error reading /dev/nvme21c21n1: no such file or directory\n```\n\n### System info\n\nTelegraf 1.35.0 (git: HEAD@e7f8d676)\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\nroot@chi01-bms-qukw-3:~# cat /proc/diskstats | grep nvme\n   0       0 nvme2c2n1 211480339 2100538 36623617203 52368199 83929525 9286848 53237304875 87811320 0 109531684 13758980 0 0 0 0\n   0       0 nvme4c4n1 230261939 1426800 41118199407 63093151 76654943 7520280 51831208970 78411738 1 117075052 11314032 0 0 0 0\n   0       0 nvme1c1n1 335828847 7706884 76961672863 137614266 108313571 10537900 56944210657 101873652 0 129931148 20430516 0 0 0 0\n   0       0 nvme6c6n1 203085634 5014797 34394403639 50355516 78156350 5048957 54555549359 96743435 0 107788812 18262732 0 0 0 0\n   0       0 nvme5c5n1 188041882 5379863 43434660173 65128406 75093667 8211106 54578851007 110768072 0 106809008 23314584 0 0 0 0\n   0       0 nvme0c0n1 288921874 8526254 45375091216 63827916 99493040 10070904 68334617597 100057065 3 139526176 12171156 0 0 0 0\n 259       6 nvme0n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 259       8 nvme2n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 259       7 nvme4n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 259       9 nvme1n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 259      10 nvme6n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 259      11 nvme5n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n   0       0 nvme7c7n1 209298649 6910865 33493981312 50469776 84436860 5970085 53167046359 108551161 0 112738496 20098368 0 0 0 0\n   0       0 nvme8c8n1 222305465 5335803 38997137325 59059283 99138760 76 71402297107 103809896 0 109687520 1372044 0 0 0 0\n   0       0 nvme9c9n1 167607917 4590017 24205294396 28699274 62552196 4141375 52926617696 51002613 2 91556764 275768 0 0 0 0\n   0       0 nvme3c3n1 294327258 10279237 46108849869 75141979 98429109 9622506 51109385112 89930582 2 127018244 12658996 0 0 0 0\n 259      16 nvme7n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 259      17 nvme8n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 259      19 nvme9n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 259      18 nvme3n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n   0       0 nvme12c12n1 200143553 1166378 37031246026 40483929 76144674 11693487 51630571493 57023620 0 105626620 329340 0 0 0 0\n   0       0 nvme11c11n1 533901386 3142936 101888585933 183286716 172221387 921 68114399459 115951880 4 150449744 1913632 0 0 0 0\n   0       0 nvme19c19n1 133991011 205315 25852191086 36009400 65317037 0 54672316367 90746756 2 94206676 6031028 0 0 0 0\n   0       0 nvme22c22n1 173319343 840 24776778776 28322479 64521969 10380241 55823547834 57515551 0 106410104 282828 0 0 0 0\n   0       0 nvme17c17n1 171577746 288075 28194337639 44826516 67611842 9660276 51306427122 75551250 0 100854980 3553892 0 0 0 0\n   0       0 nvme18c18n1 150742980 730609 34131784597 49441422 71600181 4582982 54787042864 97622626 0 97385932 6677036 0 0 0 0\n   0       0 nvme20c20n1 159615934 531059 24184865523 36838859 76888806 2818581 58454951249 79731527 0 93582548 242036 0 0 0 0\n   0       0 nvme16c16n1 219882661 2365104 33341147260 45267980 74936663 5068263 51778153290 59808223 0 99806224 1777816 0 0 0 0\n   0       0 nvme14c14n1 176509444 546126 43271710413 54933129 77261805 4209487 52494678381 74588877 0 96049584 401632 0 0 0 0\n   0       0 nvme10c10n1 205144824 4624103 27149304156 34354409 79196702 7769367 52152172293 63405095 0 108113420 287912 0 0 0 0\n   0       0 nvme21c21n1 129464865 211133 24748171479 27857618 73040629 5613277 52992392062 75400586 0 84329264 214876 0 0 0 0\n   0       0 nvme13c13n1 209506352 4489130 50283261488 51171745 66246838 2317741 55179687977 53325522 0 102718256 407448 0 0 0 0\n   0       0 nvme15c15n1 201862774 553882 40885269498 47909956 86227596 12836985 53849075542 69954999 0 99064108 639032 0 0 0 0\n   0       0 nvme23c23n1 180108054 3423462 34109102435 35587438 80506923 428007 53516485045 69214035 0 97827832 299088 0 0 0 0\n 259      34 nvme12n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 259      36 nvme14n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 259      35 nvme11n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 259      37 nvme23n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 259      38 nvme21n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 259      40 nvme18n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 259      41 nvme19n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 259      42 nvme17n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 259      43 nvme16n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 259      39 nvme22n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 259      45 nvme10n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 259      44 nvme15n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 259      47 nvme20n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 259      46 nvme13n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nroot@chi01-bms-qukw-3:~# lsblk\nNAME     MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\nloop0      7:0    0    64M  1 loop /snap/core20/2318\nloop1      7:1    0  91.9M  1 loop /snap/lxd/24061\nloop2      7:2    0  38.8M  1 loop /snap/snapd/21759\nloop3      7:3    0  50.9M  1 loop /snap/snapd/24718\nloop4      7:4    0  63.8M  1 loop /snap/core20/2599\nloop5      7:5    0  91.9M  1 loop /snap/lxd/32662\nsda        8:0    1 894.3G  0 disk \nsdb        8:16   1 894.3G  0 disk \n\u251c\u2500sdb1     8:17   1     8M  0 part \n\u251c\u2500sdb2     8:18   1     1G  0 part /boot\n\u251c\u2500sdb3     8:19   1 893.2G  0 part /\n\u2514\u2500sdb4     8:20   1  64.3M  0 part \nnvme0n1  259:6    0   500G  0 disk \nnvme4n1  259:7    0   500G  0 disk \nnvme2n1  259:8    0   500G  0 disk \nnvme1n1  259:9    0   500G  0 disk \nnvme6n1  259:10   0   500G  0 disk \nnvme5n1  259:11   0   500G  0 disk \nnvme7n1  259:16   0   500G  0 disk \nnvme8n1  259:17   0   500G  0 disk \nnvme3n1  259:18   0   500G  0 disk \nnvme9n1  259:19   0   500G  0 disk \nnvme12n1 259:34   0   500G  0 disk \nnvme11n1 259:35   0   500G  0 disk \nnvme14n1 259:36   0   500G  0 disk \nnvme23n1 259:37   0   500G  0 disk \nnvme21n1 259:38   0   500G  0 disk \nnvme22n1 259:39   0   500G  0 disk \nnvme18n1 259:40   0   500G  0 disk \nnvme19n1 259:41   0   500G  0 disk \nnvme17n1 259:42   0   500G  0 disk \nnvme16n1 259:43   0   500G  0 disk \nnvme15n1 259:44   0   500G  0 disk \nnvme10n1 259:45   0   500G  0 disk \nnvme13n1 259:46   0   500G  0 disk \nnvme20n1 259:47   0   500G  0 disk \n\n\n### Expected behavior\n\nNo warn msg\n\n### Actual behavior\n\nno\n\n### Additional info\n\nno this problem at Telegraf 1.34.4 (git: HEAD@e7ce1e1e) or blow",
      "solution": "@seecsea Looking at your Telegraf logs, the issue is that `diskio` is trying to read disk information for NVMe devices with a specific naming pattern (`nvmeXcXn1`) that don't actually exist as device files in `/dev/`.\n\nFrom your `lsblk` output, I can see your NVMe devices are named simply as `nvmeXn1` (like `nvme0n1`, `nvme1n1`, etc.), but `/proc/diskstats` is reporting them with the controller notation `nvmeXcXn1` (like `nvme0c0n1`, `nvme1c1n1`, etc.).\n\nThis is a known issue where the kernel reports NVMe devices in `/proc/diskstats` with controller notation, but the actual device files in `/dev/` don't include the controller part.\n\nThe warnings themselves are not critical - Telegraf is still collecting stats from the devices it can access (the `nvmeXn1` devices). The warnings are just informing you that it cannot read the controller-notation devices that appear in `/proc/diskstats`.\n\nThe regression between 1.34.4 and 1.35.0 might be due to changes in the gopsutil library or how Telegraf processes the device names from `/proc/diskstats`.\nLet me do some investigation on my end will get back to you. \n\n---\n\n@seecsea This is fixed in the latest Telegraf release: [v1.35.4](https://github.com/influxdata/telegraf/releases/tag/v1.35.4)",
      "labels": [
        "bug",
        "upstream"
      ],
      "created_at": "2025-06-25T03:45:31Z",
      "closed_at": "2025-09-05T15:54:55Z",
      "url": "https://github.com/influxdata/telegraf/issues/17238",
      "comments_count": 2
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17415,
      "title": "[influxdb_v2] flush interval when connection interrupted with batch size exceeded",
      "problem": "### Relevant telegraf.conf\n\n```toml\n[[outputs.influxdb_v2]]\n  alias = \"test influx\"\n  \n  urls = [\"http://localhost:8086\"]\n  token = \"xxx\"\n  organization = \"Development\"\n  bucket_tag = \"bucket\"\n  exclude_bucket_tag  = true\n  content_encoding = \"gzip\"\n  \n  flush_interval = \"10s\"\n  metric_batch_size = 2\n  metric_buffer_limit = 100000000\n```\n\n### Logs from Telegraf\n\n```text\nWith completely disconnected server\n\n2025-08-01T13:56:56Z E! [outputs.influxdb_v2::test influx] Post \"http://10.0.0.2:8086/api/v2/write?bucket=test&org=Development\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\n2025-08-01T13:56:56Z E! [outputs.influxdb_v2::test influx] When writing to [http://10.0.0.2:8086/api/v2/write]: Post \"http://10.0.0.2:8086/api/v2/write?bucket=test&org=Development\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\n2025-08-01T13:56:56Z D! [outputs.influxdb_v2::test influx] Buffer fullness: 10 metrics\n2025-08-01T13:56:56Z E! [agent] Error writing to outputs.influxdb_v2::test influx: Post \"http://10.0.0.2:8086/api/v2/write?bucket=test&org=Development\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\n\nWith connected server without influxdb running\n2025-08-01T14:01:43Z E! [outputs.influxdb_v2::test influx] Post \"http://localhost:8086/api/v2/write?bucket=test&org=Development\": dial tcp [::1]:8086: connectex: No connection could be made because the target machine actively refused it.\n2025-08-01T14:01:43Z E! [outputs.influxdb_v2::test influx] When writing to [http://localhost:8086/api/v2/write]: Post \"http://localhost:8086/api/v2/write?bucket=test&org=Development\": dial tcp [::1]:8086: connectex: No connection could be made because the target machine actively refused it.\n2025-08-01T14:01:43Z D! [outputs.influxdb_v2::test influx] Buffer fullness: 10 metrics\n2025-08-01T14:01:43Z E! [agent] Error writing to outputs.influxdb_v2::test influx: Post \"http://localhost:8086/api/v2/write?bucket=test&org=Development\": dial tcp [::1]:8086: connectex: No connection could be made because the target machine actively refused it.\n```\n\n### System info\n\n Telegraf 1.35.3 WinServer2022\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1. Create a config that has a small output batch size\n2. Overflow the batchsize\n3. Direct output to an unreachable server/reachable server without influx running.\n4. \n...\n\n### Expected behavior\n\nOnly retry to flush the buffer on the flush intervals. If the connection is down just wait until the next interval.\n\nOr make this configurable with `retries` and `retries_wait`, this way you can for example configure the output to retry 5 times with an interval of 1000ms and then try again the next flush interval.\n\n### Actual behavior\n\nWhen the database is completely disconnected the output tries to flush its full buffer at a rate of 2 times (could be coincidence) the flush interval.\nWhen the database is not running but the server is reachable, the output retries to flush its buffer as fast as it can.\n\nThis only happens when the batchsize is exceeded. So it seems like it wants to flush instantly when the batchsize is exceeded.\n\n### Additional info\n\nI am testing out the capability of telegraf to store and forward metrics. That's why I exceed the batchsize when my database is down and noticed this behavior.\n\nOr am I missing some existing configuration?\n\nSeems like the same issue as #17378 ",
      "solution": "@JeroenVanHoye please test the binary in PR #17566, available as soon as CI finished the tests, and let me know if this fixes the issue!",
      "labels": [
        "bug"
      ],
      "created_at": "2025-08-01T14:20:46Z",
      "closed_at": "2025-09-05T15:04:16Z",
      "url": "https://github.com/influxdata/telegraf/issues/17415",
      "comments_count": 2
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17378,
      "title": "influxDb spamming retries",
      "problem": "When influxDb is having connection problems (for example the host if the output doesnt exist, or network is down) and the metrics request with errors surpasses the metric_batch_size it creates a new batch that spams retries in the machine.\n\nwe got to a point where it was spaming more than 1 log per ms and surpassed the memory of our machine, shutting down the whole system \n\n<img width=\"1691\" height=\"972\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/288c2be5-3c4d-4405-b009-3ca1beac7ebe\" />\n\n_Originally posted by @andrev10 in \nhttps://github.com/influxdata/telegraf/issues/17246#issuecomment-3114508981_\n            ",
      "solution": "@andrev10, @chennin, @JeroenVanHoye and @jmickey please test the binary in PR #17566, available as soon as CI finished the tests, and let me know if this fixes the issue!",
      "labels": [],
      "created_at": "2025-07-25T07:34:48Z",
      "closed_at": "2025-09-05T15:04:16Z",
      "url": "https://github.com/influxdata/telegraf/issues/17378",
      "comments_count": 6
    },
    {
      "tech": "influxdb",
      "repo": "influxdata/telegraf",
      "issue_number": 17453,
      "title": "Panic in chrony input plugin",
      "problem": "### Relevant telegraf.conf\n\n```toml\n[[inputs.chrony]]\n  server = \"unixgram:///var/run/chrony/chronyd.sock\"\n  dns_lookup = true\n  metrics = [\"activity\", \"tracking\", \"serverstats\", \"sources\", \"sourcestats\"]\n  socket_group = \"wheel\"\n  socket_perms = \"0660\"\n```\n\n### Logs from Telegraf\n\n```text\n2025-08-10T22:07:10Z E! FATAL: [inputs.^@hrony] panicked: runtime error: index out of range [256] with length 256, Stack:\ngoroutine 483307 [running]:\ngithub.com/influxdata/telegraf/agent.panicRecover(0x140014dd200)\n    github.com/influxdata/telegraf/agent/agent.go:1202 +0x60\npanic({0x10b46d940?, 0x14000f0d8f0?})\n    runtime/panic.go:792 +0x124\nencoding/binary.(*encoder).uint8(...)\n    encoding/binary/binary.go:792\nencoding/binary.(*encoder).value(0x1400161fcf0, {0x10a352000?, 0x140017e6800?, 0x109ef4769?})\n    encoding/binary/binary.go:953 +0x920\nencoding/binary.(*encoder).value(0x1400161fcf0, {0x10a4c23c0?, 0x140017e671c?, 0x140000d6508?})\n    encoding/binary/binary.go:919 +0x880\nencoding/binary.(*encoder).value(0x1400161fcf0, {0x10b284c20?, 0x140017e6700?, 0x1010f6100?})\n    encoding/binary/binary.go:928 +0x788\nencoding/binary.Write({0x138947fc8, 0x1400151c3f8}, {0x10bc85480, 0x1117d6800}, {0x10a972500, 0x140017e6700})\n    encoding/binary/binary.go:431 +0x260\ngithub.com/facebook/time/ntp/chrony.(*Client).Communicate(0x14001245758, {0x10bbe36c0, 0x140017e6700})\n    github.com/facebook/time@v0.0.0-20240626113945-18207c5d8ddc/ntp/chrony/client.go:37 +0xa0\ngithub.com/influxdata/telegraf/plugins/inputs/chrony.(*Chrony).gatherSourceStats(0x1400149d680, {0x10bc9e260, 0x1400065dbe0})\n    github.com/influxdata/telegraf/plugins/inputs/chrony/chrony.go:474 +0x1a8\ngithub.com/influxdata/telegraf/plugins/inputs/chrony.(*Chrony).Gather(0x1400149d680, {0x10bc9e260, 0x1400065dbe0})\n    github.com/influxdata/telegraf/plugins/inputs/chrony/chrony.go:160 +0x1d0\ngithub.com/influxdata/telegraf/models.(*RunningInput).Gather(0x140014dd200, {0x10bc9e260, 0x1400065dbe0})\n    github.com/influxdata/telegraf/models/running_input.go:260 +0x23c\ngithub.com/influxdata/telegraf/agent.(*Agent).gatherOnce.func1()\n    github.com/influxdata/telegraf/agent/agent.go:590 +0x58\ncreated by github.com/influxdata/telegraf/agent.(*Agent).gatherOnce in goroutine 70\n    github.com/influxdata/telegraf/agent/agent.go:588 +0xc0\n```\n\n### System info\n\nTelegraf 1.35.3, chrony 4.7, macOS 15.6\n\n### Docker\n\n_No response_\n\n### Steps to reproduce\n\n1. add the inputs.chrony snippet above to telegraf.conf\n2. let it run for a few days\n3. eventually observe the panic\n\n### Expected behavior\n\nTelegraf should not panic\n\n### Actual behavior\n\nThe process panics with some data from chrony.\n\n### Additional info\n\n_No response_",
      "solution": "@IngmarStein Thanks for reporting this issue! I've identified that the panic is actually occurring in the underlying `facebook/time` library that Telegraf uses for chrony communication, not in Telegraf itself.\n\nI've opened a PR to fix this upstream: https://github.com/facebook/time/pull/474\n\nThe fix addresses the root cause where `binary.Write` operations were failing during packet encoding, particularly when collecting sourcestats with DNS lookup enabled (which matches your configuration).\n\nOnce that PR is merged and Telegraf updates to the newer version of the `facebook/time` dependency, this panic should be resolved.\n\n---\n\n@skartikey I saw the upstream fix - great job! Your explanation also provides a good workaround for me until the fix makes it into a Telegraf release (disabling sourcestats or DNS lookups).\n\n---\n\n@skartikey Understood! My main point about the second crash was to show that the issue manifests both when getting _source_ stats as well as _server_ stats.",
      "labels": [
        "bug",
        "upstream"
      ],
      "created_at": "2025-08-11T17:51:10Z",
      "closed_at": "2025-09-04T10:43:52Z",
      "url": "https://github.com/influxdata/telegraf/issues/17453",
      "comments_count": 6
    }
  ]
}