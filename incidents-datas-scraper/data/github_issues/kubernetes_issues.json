{
  "tech": "kubernetes",
  "count": 204,
  "examples": [
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 136677,
      "title": "CVE-2026-1580: ingress-nginx auth-method nginx configuration injection",
      "problem": "CVSS Rating: [CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H)\n\nA security issue was discovered in [ingress-nginx](https://github.com/kubernetes/ingress-nginx) where the `nginx.ingress.kubernetes.io/auth-method` Ingress annotation can be used to inject configuration into nginx. This can lead to arbitrary code execution in the context of the ingress-nginx controller, and disclosure of Secrets accessible to the controller. (Note that in the default installation, the controller can access all Secrets cluster-wide.)\n\n### Am I vulnerable?\n\nThis issue affects ingress-nginx. If you do not have ingress-nginx installed on your cluster, you are not affected. You can check this by running \\`kubectl get pods \\--all-namespaces \\--selector app.kubernetes.io/name=ingress-nginx\\`.\n\n#### Affected Versions\n\n- ingress-nginx: < v1.13.7\n- ingress-nginx: < v1.14.3\n\n### How do I mitigate this vulnerability?\n\nACTION REQUIRED: The following steps must be taken to mitigate this vulnerability: Upgrade ingress-nginx to v1.13.7, v1.14.3, or any later version.\n\nPrior to upgrading, this vulnerability can be mitigated by using a validating admission controller to reject Ingress resources with the `nginx.ingress.kubernetes.io/auth-method` annotation.\n\n#### How to upgrade?\n\nTo upgrade, refer to the documentation: [Upgrading Ingress-nginx](https://kubernetes.github.io/ingress-nginx/deploy/upgrade/)\n\n### Detection\n\nSuspicious data within the `nginx.ingress.kubernetes.io/auth-method` annotation of an Ingress resource could indicate an attempt to exploit this vulnerability.\n\n### Acknowledgements\nThis issue was discovered by Volcengine Security Team.\nThe issue was fixed and coordinated by Steven Jin, Marco Ebert, and Tabitha Sable.\n\nIf you find evidence that this vulnerability has been exploited, please contact **security@kubernetes.io**\n\n/area security\n/kind bug\n/committee security-response\n/label official-cve-feed\n/sig network\n<details>\n<summary>OSV format</summary>\n\n```json osv\n{\n  \"schema_version\": \"1.6.0\",\n  \"id\": \"CVE-2026-1580\",\n  \"modified\": \"2026-02-02T15:59:49Z\",\n  \"summary\": \"ingress-nginx auth-method nginx configuration injection\",\n  \"details\": \"A security issue was discovered in [ingress-nginx](https://github.com/kubernetes/ingress-nginx) where the `nginx.ingress.kubernetes.io/auth-method` Ingress annotation can be used to inject configuration into nginx. This can lead to arbitrary code execution in the context of the ingress-nginx controller, and disclosure of Secrets accessible to the controller. (Note that in the default installation, the controller can access all Secrets cluster-wide.)\",\n  \"severity\": [\n    {\n      \"type\": \"CVSS_V3\",\n      \"score\": \"CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H\"\n    }\n  ],\n  \"affected\": [\n    {\n      \"package\": {\n        \"ecosystem\": \"Kubernetes\",\n        \"name\": \"ingress-nginx\"\n      },\n      \"ranges\": [\n        {\n          \"type\": \"SEMVER\",\n          \"events\": [\n            {\n              \"introduced\": \"0\"\n            },\n            {\n              \"fixed\": \"v1.13.7\"\n            },\n            {\n              \"introduced\": \"0\"\n            },\n            {\n              \"fixed\": \"v1.14.3\"\n            }\n          ]\n        }\n      ]\n    }\n  ],\n  \"references\": [\n    {\n      \"type\": \"WEB\",\n      \"url\": \"https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H\"\n    }\n  ]\n}\n```\n\n</details>\n\n<!-- generated by srctl v1.0.0 (2665728667a5-dirty, 2026-01-31T19:18:07Z, go1.25.6) -->\n",
      "solution": "The chart has now been published to GitHub Pages; all release artifacts are now in place for the fixed ingress-nginx versions 1.13.7 and 1.14.3.",
      "labels": [
        "kind/bug",
        "sig/network",
        "area/security",
        "lifecycle/frozen",
        "committee/security-response",
        "triage/accepted",
        "official-cve-feed"
      ],
      "created_at": "2026-02-02T03:05:43Z",
      "closed_at": "2026-02-06T03:27:41Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/136677",
      "comments_count": 10
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 136678,
      "title": "CVE-2026-24512: ingress-nginx rules.http.paths.path nginx configuration injection",
      "problem": "CVSS Rating: [CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H)\n\nA security issue was discovered in [ingress-nginx](https://github.com/kubernetes/ingress-nginx) where the `rules.http.paths.path` Ingress field can be used to inject configuration into nginx. This can lead to arbitrary code execution in the context of the ingress-nginx controller, and disclosure of Secrets accessible to the controller. (Note that in the default installation, the controller can access all Secrets cluster-wide.)\n\n### Am I vulnerable?\n\nThis issue affects ingress-nginx. If you do not have ingress-nginx installed on your cluster, you are not affected. You can check this by running \\`kubectl get pods \\--all-namespaces \\--selector app.kubernetes.io/name=ingress-nginx\\`.\n\n#### Affected Versions\n\n- ingress-nginx: < v1.13.7\n- ingress-nginx: < v1.14.3\n\n### How do I mitigate this vulnerability?\n\nACTION REQUIRED: The following steps must be taken to mitigate this vulnerability: Upgrade ingress-nginx to v1.13.7, v1.14.3, or any later version.\n\nPrior to upgrading, this vulnerability can be mitigated by using a validating admission controller to reject Ingress resources with the `ImplementationSpecific` path type.\n\n#### How to upgrade?\n\nTo upgrade, refer to the documentation: [Upgrading Ingress-nginx](https://kubernetes.github.io/ingress-nginx/deploy/upgrade/)\n\n### Detection\n\nSuspicious data within the rules.http.paths.path field of an Ingress resource could indicate an attempt to exploit this vulnerability.\n\n### Acknowledgements\nThis issue was discovered by Maxime Escourbiac and Yassine Bengana (Michelin CERT).\nThe issue was fixed and coordinated by Steven Jin, Tabitha Sable, and Marco Ebert.\n\nIf you find evidence that this vulnerability has been exploited, please contact **security@kubernetes.io**\n\n/area security\n/kind bug\n/committee security-response\n/label official-cve-feed\n/sig network\n\n<details>\n<summary>OSV format</summary>\n\n```json osv\n{\n  \"schema_version\": \"1.6.0\",\n  \"id\": \"CVE-2026-24512\",\n  \"modified\": \"2026-02-02T16:00:01Z\",\n  \"summary\": \"ingress-nginx rules.http.paths.path nginx configuration injection\",\n  \"details\": \"A security issue was discovered in [ingress-nginx](https://github.com/kubernetes/ingress-nginx) where the `rules.http.paths.path` Ingress field can be used to inject configuration into nginx. This can lead to arbitrary code execution in the context of the ingress-nginx controller, and disclosure of Secrets accessible to the controller. (Note that in the default installation, the controller can access all Secrets cluster-wide.)\",\n  \"severity\": [\n    {\n      \"type\": \"CVSS_V3\",\n      \"score\": \"CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H\"\n    }\n  ],\n  \"affected\": [\n    {\n      \"package\": {\n        \"ecosystem\": \"Kubernetes\",\n        \"name\": \"ingress-nginx\"\n      },\n      \"ranges\": [\n        {\n          \"type\": \"SEMVER\",\n          \"events\": [\n            {\n              \"introduced\": \"0\"\n            },\n            {\n              \"fixed\": \"v1.13.7\"\n            },\n            {\n              \"introduced\": \"0\"\n            },\n            {\n              \"fixed\": \"v1.14.3\"\n            }\n          ]\n        }\n      ]\n    }\n  ],\n  \"references\": [\n    {\n      \"type\": \"WEB\",\n      \"url\": \"https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H\"\n    }\n  ]\n}\n```\n\n</details>\n\n<!-- generated by srctl v1.0.0 (2665728667a5-dirty, 2026-01-31T19:18:07Z, go1.25.6) -->\n",
      "solution": "The chart has now been published to GitHub Pages; all release artifacts are now in place for the fixed ingress-nginx versions 1.13.7 and 1.14.3.",
      "labels": [
        "kind/bug",
        "sig/network",
        "area/security",
        "lifecycle/frozen",
        "committee/security-response",
        "triage/accepted",
        "official-cve-feed"
      ],
      "created_at": "2026-02-02T03:05:54Z",
      "closed_at": "2026-02-06T03:27:33Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/136678",
      "comments_count": 8
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 136679,
      "title": "CVE-2026-24513: ingress-nginx auth-url protection bypass",
      "problem": "CVSS Rating: [CVSS:3.1/AV:N/AC:H/PR:L/UI:N/S:U/C:L/I:N/A:N](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:H/PR:L/UI:N/S:U/C:L/I:N/A:N)\n\nA security issue was discovered in [ingress-nginx](https://github.com/kubernetes/ingress-nginx) where the protection afforded by the `auth-url` Ingress annotation may not be effective in the presence of a specific misconfiguration.\n\nIf the ingress-nginx controller is configured with a default custom-errors configuration that includes HTTP errors 401 or 403, and if the configured default custom-errors backend is defective and fails to respect the X-Code HTTP header, then an Ingress with the `auth-url` annotation may be accessed even when authentication fails.\n\nNote that the built-in custom-errors backend works correctly. To trigger this issue requires an administrator to specifically configure ingress-nginx with a broken external component.\n\n### Am I vulnerable?\n\n* This issue affects ingress-nginx. If you do not have ingress-nginx installed on your cluster, you are not affected. You can check this by running \\`kubectl get pods \\--all-namespaces \\--selector [app.kubernetes.io/name=ingress-nginx](http://app.kubernetes.io/name=ingress-nginx)\\`.\n* This issue only affects Ingresses using the `auth-url` annotation. If you are not adding authentication to your Ingresses using the `auth-url` annotation, then you are not at risk.\n* This issue only affects ingress-nginx controllers that have been configured with default custom-errors settings via editing the controller command-line arguments and configuration map. If you are using per-Ingress custom-errors set via Ingress annotations, then you are not at risk.\n\n#### Affected Versions\n\n- ingress-nginx: < 1.13.7\n- ingress-nginx: < 1.14.3\n\n### How do I mitigate this vulnerability?\n\nACTION REQUIRED: The following steps must be taken to mitigate this vulnerability: Upgrade ingress-nginx to v1.13.7, v1.14.3, or any later version.\n\nPrior to upgrading, this vulnerability can be mitigated by checking to confirm your custom errors backend correctly respects the `X-Code` HTTP header.\n\n#### How to upgrade?\n\nTo upgrade, refer to the documentation: [Upgrading Ingress-nginx](https://kubernetes.github.io/ingress-nginx/deploy/upgrade/)\n\n### Detection\n\nIf you are able to load a URL protected by the `auth-url` annotation despite failing authentication, then you are affected by this vulnerability.\n\n### Acknowledgements\nThis issue was discovered by Aurelia Schittler.\nThe issue was fixed and coordinated by Tabitha Sable and Marco Ebert.\n\nIf you find evidence that this vulnerability has been exploited, please contact **security@kubernetes.io**\n\n/area security\n/kind bug\n/committee security-response\n/label official-cve-feed\n/sig network\n\n<details>\n<summary>OSV format</summary>\n\n```json osv\n{\n  \"schema_version\": \"1.6.0\",\n  \"id\": \"CVE-2026-24513\",\n  \"modified\": \"2026-02-02T16:00:06Z\",\n  \"summary\": \"ingress-nginx auth-url protection bypass\",\n  \"details\": \"A security issue was discovered in [ingress-nginx](https://github.com/kubernetes/ingress-nginx) where the protection afforded by the `auth-url` Ingress annotation may not be effective in the presence of a specific misconfiguration.\\n\\nIf the ingress-nginx controller is configured with a default custom-errors configuration that includes HTTP errors 401 or 403, and if the configured default custom-errors backend is defective and fails to respect the X-Code HTTP header, then an Ingress with the `auth-url` annotation may be accessed even when authentication fails.\\n\\nNote that the built-in custom-errors backend works correctly. To trigger this issue requires an administrator to specifically configure ingress-nginx with a broken external component.\",\n  \"severity\": [\n    {\n      \"type\": \"CVSS_V3\",\n      \"score\": \"CVSS:3.1/AV:N/AC:H/PR:L/UI:N/S:U/C:L/I:N/A:N\"\n    }\n  ],\n  \"affected\": [\n    {\n      \"package\": {\n        \"ecosystem\": \"Kubernetes\",\n        \"name\": \"ingress-nginx\"\n      },\n      \"ranges\": [\n        {\n          \"type\": \"SEMVER\",\n          \"events\": [\n            {\n              \"introduced\": \"0\"\n            },\n            {\n              \"fixed\": \"1.13.7\"\n            },\n            {\n              \"introduced\": \"0\"\n            },\n            {\n              \"fixed\": \"1.14.3\"\n            }\n          ]\n        }\n      ]\n    }\n  ],\n  \"references\": [\n    {\n      \"type\": \"WEB\",\n      \"url\": \"https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:H/PR:L/UI:N/S:U/C:L/I:N/A:N\"\n    }\n  ]\n}\n```\n\n</details>\n\n<!-- generated by srctl v1.0.0 (2665728667a5-dirty, 2026-01-31T19:18:07Z, go1.25.6) -->\n",
      "solution": "The chart has now been published to GitHub Pages; all release artifacts are now in place for the fixed ingress-nginx versions 1.13.7 and 1.14.3.",
      "labels": [
        "kind/bug",
        "sig/network",
        "area/security",
        "lifecycle/frozen",
        "committee/security-response",
        "triage/accepted",
        "official-cve-feed"
      ],
      "created_at": "2026-02-02T03:06:04Z",
      "closed_at": "2026-02-06T03:27:21Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/136679",
      "comments_count": 8
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 136680,
      "title": "CVE-2026-24514: ingress-nginx Admission Controller denial of service",
      "problem": "CVSS Rating: [CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H)\n\nA security issue was discovered in [ingress-nginx](https://github.com/kubernetes/ingress-nginx) where the validating admission controller feature is subject to a denial of service condition. By sending large requests to the validating admission controller, an attacker can cause memory consumption, which may result in the ingress-nginx controller pod being killed or the node running out of memory.\n\n### Am I vulnerable?\n\n* This issue affects ingress-nginx. If you do not have ingress-nginx installed on your cluster, you are not affected. You can check this by running \\`kubectl get pods \\--all-namespaces \\--selector [app.kubernetes.io/name=ingress-nginx](http://app.kubernetes.io/name=ingress-nginx)\\`.\n\n#### Affected Versions\n\n- ingress-nginx: < 1.13.7\n- ingress-nginx: < 1.14.3\n\n### How do I mitigate this vulnerability?\n\nACTION REQUIRED: The following steps must be taken to mitigate this vulnerability: Upgrade ingress-nginx to v1.13.7, v1.14.3, or any later version.\n\n#### How to upgrade?\n\nTo upgrade, refer to the documentation: [Upgrading Ingress-nginx](https://kubernetes.github.io/ingress-nginx/deploy/upgrade/)\n\n### Detection\n\nRequests larger than a few megabytes being sent to the ingress-nginx admission controller may indicate an attempt to exploit this vulnerability.\n\n### Acknowledgements\n\nThis issue was discovered by Matan Shabtay.\nThe issue was fixed and coordinated by Steven Jin, Marco Ebert, and Tabitha Sable\n\nIf you find evidence that this vulnerability has been exploited, please contact **security@kubernetes.io**\n\n/area security\n/kind bug\n/committee security-response\n/label official-cve-feed\n/sig network\n\n<details>\n<summary>OSV format</summary>\n\n```json osv\n{\n  \"schema_version\": \"1.6.0\",\n  \"id\": \"CVE-2026-24514\",\n  \"modified\": \"2026-02-02T16:00:10Z\",\n  \"summary\": \"ingress-nginx Admission Controller denial of service\",\n  \"details\": \"A security issue was discovered in [ingress-nginx](https://github.com/kubernetes/ingress-nginx) where the validating admission controller feature is subject to a denial of service condition. By sending large requests to the validating admission controller, an attacker can cause memory consumption, which may result in the ingress-nginx controller pod being killed or the node running out of memory.\",\n  \"severity\": [\n    {\n      \"type\": \"CVSS_V3\",\n      \"score\": \"CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H\"\n    }\n  ],\n  \"affected\": [\n    {\n      \"package\": {\n        \"ecosystem\": \"Kubernetes\",\n        \"name\": \"ingress-nginx\"\n      },\n      \"ranges\": [\n        {\n          \"type\": \"SEMVER\",\n          \"events\": [\n            {\n              \"introduced\": \"0\"\n            },\n            {\n              \"fixed\": \"1.13.7\"\n            },\n            {\n              \"introduced\": \"0\"\n            },\n            {\n              \"fixed\": \"1.14.3\"\n            }\n          ]\n        }\n      ]\n    }\n  ],\n  \"references\": [\n    {\n      \"type\": \"WEB\",\n      \"url\": \"https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H\"\n    }\n  ]\n}\n```\n\n</details>\n\n<!-- generated by srctl v1.0.0 (2665728667a5-dirty, 2026-01-31T19:18:07Z, go1.25.6) -->\n",
      "solution": "The chart has now been published to GitHub Pages; all release artifacts are now in place for the fixed ingress-nginx versions 1.13.7 and 1.14.3.",
      "labels": [
        "kind/bug",
        "sig/network",
        "area/security",
        "lifecycle/frozen",
        "committee/security-response",
        "triage/accepted",
        "official-cve-feed"
      ],
      "created_at": "2026-02-02T03:06:14Z",
      "closed_at": "2026-02-06T03:27:07Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/136680",
      "comments_count": 8
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 136789,
      "title": "CVE-2025-15566: ingress-nginx auth-proxy-set-headers nginx configuration injection",
      "problem": "CVSS Rating: [CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H)\n\nA security issue was discovered in ingress-nginx where the nginx.ingress.kubernetes.io/auth-proxy-set-headers Ingress annotation can be used to inject configuration into nginx. This can lead to arbitrary code execution in the context of the ingress-nginx controller, and disclosure of Secrets accessible to the controller. (Note that in the default installation, the controller can access all Secrets cluster-wide.)\n\n### Am I vulnerable?\n\nThis issue affects ingress-nginx. If you do not have ingress-nginx installed on your cluster, you are not affected. You can check this by running `kubectl get pods --all-namespaces --selector app.kubernetes.io/name=ingress-nginx`.\n\n#### Affected Versions\n\n- ingress-nginx: v1.12.5\n- ingress-nginx: v1.13.1\n\n### How do I mitigate this vulnerability?\n\nACTION REQUIRED: The following steps must be taken to mitigate this vulnerability: Upgrade ingress-nginx to v1.12.5, v1.13.1, or any later version.\n\n#### How to upgrade?\n\nTo upgrade, refer to the documentation: [Upgrading Ingress-nginx](https://kubernetes.github.io/ingress-nginx/deploy/upgrade/)\n\n### Detection\n\nSuspicious data within a configmap passed to the `nginx.ingress.kubernetes.io/auth-proxy-set-headers` annotation of an Ingress resource could indicate an attempt to exploit this vulnerability.\n\nIf you find evidence that this vulnerability has been exploited, please contact **security@kubernetes.io**\n\n#### Acknowledgements\n\nThis issue was discovered **and patched** by Jan-Otto Kr\u00f6pke.\n\n/area security\n/kind bug\n/committee security-response\n/label official-cve-feed\n/sig network\n\n<details>\n<summary>OSV format</summary>\n\n```json osv\n{\n  \"schema_version\": \"1.6.0\",\n  \"id\": \"CVE-2025-15566\",\n  \"modified\": \"2026-02-06T02:46:50Z\",\n  \"summary\": \"ingress-nginx auth-proxy-set-headers nginx configuration injection\",\n  \"details\": \"A security issue was discovered in ingress-nginx where the nginx.ingress.kubernetes.io/auth-proxy-set-headers Ingress annotation can be used to inject configuration into nginx. This can lead to arbitrary code execution in the context of the ingress-nginx controller, and disclosure of Secrets accessible to the controller. (Note that in the default installation, the controller can access all Secrets cluster-wide.)\",\n  \"severity\": [\n    {\n      \"type\": \"CVSS_V3\",\n      \"score\": \"CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H\"\n    }\n  ],\n  \"affected\": [\n    {\n      \"package\": {\n        \"ecosystem\": \"Kubernetes\",\n        \"name\": \"ingress-nginx\"\n      },\n      \"ranges\": [\n        {\n          \"type\": \"SEMVER\",\n          \"events\": [\n            {\n              \"introduced\": \"0\"\n            },\n            {\n              \"fixed\": \"v1.12.5\"\n            },\n            {\n              \"introduced\": \"0\"\n            },\n            {\n              \"fixed\": \"v1.13.1\"\n            }\n          ]\n        }\n      ]\n    }\n  ],\n  \"references\": [\n    {\n      \"type\": \"WEB\",\n      \"url\": \"https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H\"\n    }\n  ],\n  \"credits\": [\n    {\n      \"name\": \"This issue was discoverd **and patched** by Jan-Otto Kr\u00f6pke.\",\n      \"type\": \"FINDER\"\n    }\n  ]\n}\n```\n\n</details>\n\n<!-- generated by srctl v1.0.0 (2665728667a5-dirty, 2026-01-31T19:18:07Z, go1.25.6) -->\n",
      "solution": "Note that this issue is documenting a vulnerability that was fixed in previous versions of ingress-nginx, released in August 2025.",
      "labels": [
        "kind/bug",
        "sig/network",
        "area/security",
        "committee/security-response",
        "triage/accepted",
        "official-cve-feed"
      ],
      "created_at": "2026-02-06T02:54:24Z",
      "closed_at": "2026-02-06T03:26:54Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/136789",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 128571,
      "title": "\ud83d\udc18 Drop x/exp from k/k dependencies",
      "problem": "- We started the process in `cilium/ebpf` here : https://github.com/cilium/ebpf/issues/1095\r\n- A PR has landed here : https://github.com/cilium/ebpf/pull/1557\r\n- Wait for a new [new release](https://github.com/cilium/ebpf/releases) of `cilium/ebpf` \r\n- Update `opencontainers/runc` to new version of cilium/ebpf, similar to this [PR](https://github.com/opencontainers/runc/pull/4397)\r\n- Wait for a new release/tag of `opencontainers/runc`\r\n- Update k/k",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "sig/architecture",
        "area/code-organization",
        "needs-triage"
      ],
      "created_at": "2024-11-05T12:57:56Z",
      "closed_at": "2026-02-05T21:51:08Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/128571",
      "comments_count": 9
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 123946,
      "title": "[Flaking Test] ci-kubernetes-integration-master TestSelectableFields",
      "problem": "### Which jobs are flaking?\n\nhttps://prow.k8s.io/view/gs/kubernetes-jenkins/logs/ci-kubernetes-integration-master/1768366435643428864\n\n### Which tests are flaking?\n\nk8s.io/apiextensions-apiserver/test: integration\r\n\r\nTestSelectableFields\n\n### Since when has it been flaking?\n\n15/3/2024\n\n### Testgrid link\n\nhttps://testgrid.k8s.io/sig-release-master-blocking#integration-master\n\n### Reason for failure (if possible)\n\nhttps://prow.k8s.io/view/gs/kubernetes-jenkins/logs/ci-kubernetes-integration-master/1768366435643428864#1:build-log.txt%3A27112\r\n\r\n```\r\npanic: conversion webhook for tests.example.com/v1, Kind=Shirt failed: Post \"https://127.0.0.1:45105/convert?timeout=30s\": EOF\r\ngoroutine 57030 [running]:\r\nk8s.io/apiserver/pkg/storage/etcd3.decodeObj({0x2cd4dc0?, 0xc0005d7360?}, {0x2ce45c0, 0x43049a0}, {0xc000727680?, 0x1e0?, 0xc000951f30?}, 0x503c)\r\n\t/home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/storage/etcd3/watcher.go:612 +0x1c6\r\nk8s.io/apiserver/pkg/storage/etcd3.(*watchChan).prepareObjs(0xc0007ede10, 0xc0012ddef0)\r\n\t/home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/storage/etcd3/watcher.go:580 +0xe5\r\nk8s.io/apiserver/pkg/storage/etcd3.(*watchChan).transform(0xc0007ede10, 0xc0012ddef0)\r\n\t/home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/storage/etcd3/watcher.go:469 +0x3b\r\nk8s.io/apiserver/pkg/storage/etcd3.(*watchChan).processEvent(0xc0007ede10, 0x2c7d7b3a22737574?)\r\n\t/home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/storage/etcd3/watcher.go:434 +0x125\r\ncreated by k8s.io/apiserver/pkg/storage/etcd3.(*watchChan).run in goroutine 57028\r\n\t/home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/storage/etcd3/watcher.go:234 +0x129\r\n```\n\n### Anything else we need to know?\n\nTest was introduced in https://github.com/kubernetes/kubernetes/pull/122717\n\n### Relevant SIG(s)\n\n/sig api-machinery",
      "solution": "This seems to be fixed. \r\nhttps://storage.googleapis.com/k8s-triage/index.html?text=TestSelectableFields\r\n\r\n/close \n\n---\n\n@pacoxu: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/123946#issuecomment-2068703690):\n\n>This seems to be fixed. \r\n>https://storage.googleapis.com/k8s-triage/index.html?text=TestSelectableFields\r\n>\r\n>/close \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "sig/api-machinery",
        "kind/flake",
        "lifecycle/stale",
        "needs-triage"
      ],
      "created_at": "2024-03-15T06:43:37Z",
      "closed_at": "2026-02-05T21:49:19Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/123946",
      "comments_count": 9
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 135344,
      "title": "Batch update node status in cloud node controller",
      "problem": "### What would you like to be added?\n\nCloud node controller reconciles on all nodes periodically by quering and updating nodes address. It processes each node one by one. Supporting batch update on nodes can improve the throuput.\n\nhttps://github.com/kubernetes/cloud-provider/blob/f5511889f9d28a97c8244a8c59bc37856e55715b/controllers/node/node_controller.go#L270-L294\n\n### Why is this needed?\n\nCurrent behavior is to parallelize the node update across workers, and each request processes single node. Supporting batch node update can improve the throughput of the cloud node controller and reduce the volume of API requests issued to cloud provider.",
      "solution": "We had added batching EC2 api calls in cloud provider aws recently https://github.com/kubernetes/cloud-provider-aws/pull/1152. However we haven't back ported it for older version and if the issue is specific to that we can evaluate on back porting to older releases.",
      "labels": [
        "kind/feature",
        "sig/cloud-provider",
        "needs-triage"
      ],
      "created_at": "2025-11-19T00:02:04Z",
      "closed_at": "2026-02-05T20:58:31Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/135344",
      "comments_count": 16
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 119248,
      "title": "expect kubelet retry to alloc resource when device plugin restart",
      "problem": "### What happened?\n\nwhen device plugin restart. pod create failed with reason UnexpectedAdmissionError\n\n### What did you expect to happen?\n\nkubelet retry to wait device plugin restart, to let the pod create success\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n1. prepare device plugin report resource.\r\n2. create pod, at the same time, device plugin restart.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\n# paste output here\r\n```\r\n\r\n</details>\r\n\n\n### Cloud provider\n\n<details>\r\n\r\n</details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n</details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n</details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n</details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n</details>\r\n",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/bug",
        "sig/node",
        "priority/important-longterm",
        "needs-triage"
      ],
      "created_at": "2023-07-12T08:31:22Z",
      "closed_at": "2026-02-05T14:22:37Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/119248",
      "comments_count": 11
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 109936,
      "title": "Kubelet removes non-Kubernetes images.",
      "problem": "### What happened?\n\nWe also hit by this issue. We're using Flatcar Linux which is quite barebone so we are using a few containers to augment it. Unfortunately kubelet removes the images when they are not in active use. For example we a have a cronjob which uses an image we pulled and specially tagged. That images isn't used by any pods at all but kubelet choose to remove it under disk pressure. It is very annoying.\r\n\r\nIt would be nice to have a setting which says that \"you can touch images only if the tag is starting with this prefix\" or similar.\r\n\r\nEven if you are using another Linux distribution then you might want to install something which isn't available in the distribution itself. Using a container is a very likely way to go and you don't expect that something which never belonged to the Kubernetes cluster will be removed by kubelet. It's very unintuitive.\n\n### What did you expect to happen?\n\nKubelet won't remove images which aren't belongs to it.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n* Pull some images from the command line.\r\n* Fill up the local disk to create a DiskPressure \"situation\".\r\n* Wait until kubelet removes the images - by the documentation it does in every five minutes.\n\n### Anything else we need to know?\n\nI created this new ticket because the old one (#103255) I found gets closed by k8s-ci-robot.\r\n\r\nWe are still using Kubernetes 1.21 as because of the faulty etcd-client lib used in 1.22 and 1.23 we cannot upgrade our clusters. The fix appeared in 1.24 but never backported to 1.22 or 1.23 which left us no possible upgrade paths.\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nClient Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.2\", GitCommit:\"092fbfbf53427de67cac1e9fa54aaa09a28371d7\", GitTreeState:\"clean\", BuildDate:\"2021-06-16T12:59:11Z\", GoVersion:\"go1.16.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\nServer Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.2\", GitCommit:\"092fbfbf53427de67cac1e9fa54aaa09a28371d7\", GitTreeState:\"clean\", BuildDate:\"2021-06-16T12:53:14Z\", GoVersion:\"go1.16.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n```\r\n\r\n</details>\r\n\n\n### Cloud provider\n\n<details>\r\nKubernetes cluster running on EC2 instances in AWS.\r\n</details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n$ cat /etc/os-release\r\nNAME=\"Flatcar Container Linux by Kinvolk\"\r\nID=flatcar\r\nID_LIKE=coreos\r\nVERSION=3033.2.3\r\nVERSION_ID=3033.2.3\r\nBUILD_ID=2022-03-02-2032\r\nPRETTY_NAME=\"Flatcar Container Linux by Kinvolk 3033.2.3 (Oklo)\"\r\nANSI_COLOR=\"38;5;75\"\r\nHOME_URL=\"https://flatcar-linux.org/\"\r\nBUG_REPORT_URL=\"https://issues.flatcar-linux.org\"\r\nFLATCAR_BOARD=\"amd64-usr\"\r\n$ uname -a\r\nLinux ip-10-8-74-48.ec2.internal 5.10.102-flatcar #1 SMP Wed Mar 2 19:40:13 -00 2022 x86_64 AMD EPYC 7571 AuthenticAMD GNU/Linux\r\n```\r\n\r\n</details>\r\n\n\n### Install tools\n\n<details>\r\nNone.\r\n</details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\nN/A\r\n</details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\nN/A\r\n</details>\r\n",
      "solution": "@BenTheElder  @dims  I would be happy to help in any way I can with this if that is all right with you. In our product we have a local docker registry that runs as a Pod in the local k8s cluster itself. Which like you can already see is a cyclic dependency. Though it is not the best thing, works great for us in AirGap solutions. We were bit by this uncontrolled GC in production a bunch of times. \r\n\r\nThe GC manager ended up cleaning up the images required to run the registry itself as part of the GC and as a side effect it took part of the cluster down with it. \r\n\r\nSince we are managing a custom k8s ourself internally, we went in and put an exception list that we let the end user configure via the kubelet config. The exception file is a JSON file you can specify with support to excluding images via tag/image ID. We customised the Image GC manager to honour this and it has been helping us well so far. Would be more than happy to help get that in the upstream if the community agrees to it. \r\n\r\n```yaml\r\nimageGCConfig: /tmp/gc-excemption.yaml\r\n```\r\n\r\n```yaml\r\nexemptByID:\r\n- id1\r\n- id2\r\nexceptByTag:\r\n- image:tag1\r\n- image:tag*\r\n```\r\nAbove is how the config looks like for our use-case. \n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/bug",
        "sig/node",
        "priority/important-longterm",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2022-05-10T10:01:21Z",
      "closed_at": "2026-02-04T18:50:35Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/109936",
      "comments_count": 26
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 136006,
      "title": "Failure cluster Lifecycle sleep action zero value when create a pod with lifecycle hook using sleep action with a duration of zero seconds prestop hook using sleep action with zero duration",
      "problem": "### Failure cluster [227e0df417839dc5d5ed](https://go.k8s.io/triage#227e0df417839dc5d5ed)\n\n##### Error text:\n```\n[FAILED] unexpected delay duration before killing the pod, cost = 30.207689386s\nIn [It] at: k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:741 @ 12/22/25 10:31:04.983\n\n```\n#### Recent failures:\n[1/1/2026, 11:26:21 AM e2e-kops-gce-cni-calico](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/e2e-kops-gce-cni-calico/2006763877311713280)\n[12/30/2025, 5:21:24 PM e2e-kops-aws-distro-u2510](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/e2e-kops-aws-distro-u2510/2006128433188311040)\n[12/29/2025, 10:56:59 PM e2e-kops-grid-gce-ipalias-deb13-k35](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/e2e-kops-grid-gce-ipalias-deb13-k35/2005850514016702464)\n[12/29/2025, 11:23:58 AM e2e-kops-grid-gce-kubenet-rhel10-k34](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/e2e-kops-grid-gce-kubenet-rhel10-k34/2005676112801173504)\n[12/25/2025, 11:23:20 AM e2e-kops-grid-gce-kubenet-rhel10-k34](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/e2e-kops-grid-gce-kubenet-rhel10-k34/2004226404379529216)\n\n\n/kind failing-test\n<!-- If this is a flake, please add: /kind flake -->\n\n/sig node",
      "solution": "/close\n\nI would think it is resolved by the bump of systemd\n\n---\n\n@SergeyKanzhelev: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/136006#issuecomment-3848862294):\n\n>/close\n>\n>I would think it is resolved by the bump of systemd\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>",
      "labels": [
        "sig/node",
        "kind/flake",
        "triage/accepted"
      ],
      "created_at": "2026-01-02T20:37:42Z",
      "closed_at": "2026-02-04T17:48:04Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/136006",
      "comments_count": 9
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 109229,
      "title": "go vet lostcancel errors in legacy-cloud-providers files",
      "problem": "### What happened?\r\n\r\nWe found go vet issues of `lostcancel` in #109184. (It seems that go vet checks are not running under `/staging` dir.)\r\n\r\n```\r\nstaging/src/k8s.io/legacy-cloud-providers/vsphere/nodemanager.go:190:5: lostcancel: the cancel function is not used on all paths (possible context leak) (govet)\r\n\t\t\t\tctx, cancel := context.WithCancel(context.Background())\r\n\t\t\t\t^\r\nstaging/src/k8s.io/legacy-cloud-providers/vsphere/nodemanager.go:236:3: lostcancel: this return statement may be reached without using the cancel var defined on line 190 (govet)\r\n\t\t}()\r\n```\r\n\r\nWe should fix them to ensure we don't introduce context leak as the message said.\r\n\r\n### What did you expect to happen?\r\n\r\nThese issues from go vet are fixed.\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\nRunning go vet under `staging/src/k8s.io/legacy-cloud-providers/`\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_\r\n\r\n### Kubernetes version\r\n\r\nlatest master\r\n\r\n### Cloud provider\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n",
      "solution": "@liggitt so is this should be resolved before v1.24?\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/bug",
        "good first issue",
        "sig/cloud-provider",
        "needs-triage"
      ],
      "created_at": "2022-04-01T10:44:47Z",
      "closed_at": "2026-02-03T16:10:30Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/109229",
      "comments_count": 19
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 135802,
      "title": "HNS loadbalancers for services are not recreated",
      "problem": "### What happened?\n\nHNS loadbalancers appear to persist in cache, if removed these are never created throughout the lifecycle of kube-proxy. This out-of-band cleanup of loadbalancers could occur during service restarts of kubelet a CNI or race condition with another service.\n\nSimilar to https://github.com/kubernetes/kubernetes/issues/133928 however this issue applies to all service types. Reproduction has occurred using the default `internalTrafficPolicy: Cluster`\n\n\n\n### What did you expect to happen?\n\nkube-proxy invalidates cache and recreates missing HNS loadbalancers to avoid service disruption\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nThis has been known to occur during rke2 service restarts, the simplest reproduction is a manual removal of a loadbalancer:\n\nOn a Windows worker node:\n- List the load balancers: `hnsdiag list loadbalancers`\n  - Example:\n  ```\n  Load Balancer    : 3599612c-54b4-4b9d-9258-4fc5e2100549\n    Virtual IPs      : 172.30.0.1\n    Direct IP IDs    : 78c19c4d-7edf-43bd-ab2a-5ccf97f214fe\n  ```\n\n- Select an LB to remove: `hnsdiag delete loadbalancers <ID>`\n  - Example:\n  ```\n  > hnsdiag delete loadbalancers 3599612c-54b4-4b9d-9258-4fc5e2100549\n  ```\n- Monitor the LB list, and kube-proxy logs:\n  - `hnsdiag list loadbalancers`\n  - In kube-proxy logs the LB is persisted in a cache:\n  ```\n  I1217 23:36:36.438104    7712 proxier.go:1225] \"Policy already applied\" serviceInfo=\"172.30.0.1:443/TCP\"\n  ```\n\nThe kube-proxy sync interval continues to log that the policy is applied, yet the loadbalancers list has no matching LB. Restarting kube-proxy will rebuild the loadbalancers.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\nRKE2 v1.31.14 (however other versions have been tested, v1.33.6 and v1.34.2)\n\n```console\n# kubectl version\nClient Version: v1.31.14+rke2r1\nKustomize Version: v5.4.2\nServer Version: v1.31.14+rke2r1\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n# cat /etc/os-release\nNAME=\"SLES\"\nVERSION=\"15-SP6\"\nVERSION_ID=\"15.6\"\nPRETTY_NAME=\"SUSE Linux Enterprise Server 15 SP6\"\nID=\"sles\"\nID_LIKE=\"suse\"\nANSI_COLOR=\"0;32\"\nCPE_NAME=\"cpe:/o:suse:sles:15:sp6\"\nDOCUMENTATION_URL=\"https://documentation.suse.com/\"\n# uname -a\nLinux ip-172-31-7-247 6.4.0-150600.23.73-default #1 SMP PREEMPT_DYNAMIC Tue Oct  7 08:43:02 UTC 2025 (46f6a23) x86_64 x86_64 x86_64 GNU/Linux\n\n# On Windows:\nC:\\>  Get-CimInstance Win32_OperatingSystem | Select-Object Caption, Version, BuildNumber, OSArchitecture\n\nCaption                                  Version    BuildNumber OSArchitecture\n-------                                  -------    ----------- --------------\nMicrosoft Windows Server 2022 Datacenter 10.0.20348 20348       64-bit\n```\n\n</details>\n\n\n### Install tools\n\n<details>\nInstalled using a custom cluster in Rancher v2.11.2. Configured with `cni: calico` and BGP\n  * https://docs.rke2.io/install/quickstart#windows-agent-worker-node-installation\n  * https://docs.rke2.io/networking/windows_bgp\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\nCalico with BGP enabled\n\n```yaml\n    chartValues:\n      rke2-calico:\n        installation:\n          calicoNetwork:\n            bgp: Enabled\n            ipPools:\n              - blockSize: 24\n                cidr: 172.29.0.0/16\n                encapsulation: None\n                natOutgoing: Enabled\n```\n</details>\n",
      "solution": "Thanks @princepereira, with the serial vs idempotent nature of the kube-proxy LB management being expected in Windows, I've been testing again and found a simpler reproducer of only stopping the kubelet process started by RKE2, in the [most recent releases](https://github.com/rancher/rke2/pull/9339) the kubelet restart handling was revised and avoided the HNS network management from occurring in this scenario\n\nClosing this as the original issue is resolved, feel free to reopen if needed\n",
      "labels": [
        "kind/bug",
        "sig/network",
        "sig/windows",
        "needs-triage"
      ],
      "created_at": "2025-12-18T00:01:24Z",
      "closed_at": "2026-02-03T01:57:44Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/135802",
      "comments_count": 13
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 132947,
      "title": "Auto-approved but unissued kubelet client CSRs persist indefinitely",
      "problem": "### What happened?\n\nAn incident was triggered by unexpectedly high etcd storage usage on production Kubernetes clusters. Initial analysis revealed that the majority of the space was consumed by a large number of Certificate Signing Requests (CSRs).\n\nUsing `etcdctl`, the top resource consumers in etcd were identified:\n\n| Resource Type | Count |\n|---------------|-------|\n| **CertificateSigningRequests** | **520,858** |\n| Events | 1,395 |\n| CRDs (Calico) | 1,005 |\n| ReplicaSets | 864 |\n| RoleBindings | 473 |\n| Services | 428 |\n| Pods | 416 |\n\nThis overwhelming number of CSRs was far beyond what could be explained by normal node provisioning or rotation activity. A sample of 1,000 CSRs showed an average size of **1,192 bytes**, meaning the CSRs alone were consuming **~592 MiB** of etcd space\u2014nearly a third of the 2 GiB etcd limit.\n\nThis dates on the resources showed the build up had been happening for years, with one being generated for each node every 15 minutes.\n\nOn examination they were all of the type:\n\n```\napiVersion: certificates.k8s.io/v1\nkind: CertificateSigningRequest\nmetadata:\n  creationTimestamp: \"2025-07-09T06:51:01Z\"\n  generateName: csr-\n  name: csr-jllx7\n  resourceVersion: \"296695671\"\n  uid: 0f6d31fd-c8ed-465e-ac45-e283f453f56c\nspec:\n  groups:\n  - system:nodes\n  - system:authenticated\n  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlIeE1JR1hBZ0VBTURVeEZUQVRCZ05WQkFvVERITjVjM1JsYlRwdWIyUmxjekVjTUJvR0ExVUVBeE1UYzNsegpkR1Z0T201dlpHVTZiMk4wWVhacFlUQlpNQk1HQnlxR1NNNDlBZ0VHQ0NxR1NNNDlBd0VIQTBJQUJNZTZkT0dGCkQ4U2lmbWtJOURTYWZnMEtKKytPaFpHU3hTUlYxODRDNXdkaDloczgyWnh0SHJ5VlRpRHNzejdxUzlwanVhajYKbHRobHJibjc3L21ReUZLZ0FEQUtCZ2dxaGtqT1BRUURBZ05KQURCR0FpRUF1bWk3KzE0RHlqdFBkdWlUTDNMTQpXVjZDOUs5MWRZeXo3TzN0YzFrdnhzSUNJUURaa1o5UXo3RVlsN3M3M1FDTG1KOFNnSkNTVmNNMCtwUEJ3aFRzCmUrejYyQT09Ci0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=\n  signerName: kubernetes.io/kube-apiserver-client-kubelet\n  usages:\n  - digital signature\n  - client auth\n  username: system:node:octavia\nstatus:\n  conditions:\n  - lastTransitionTime: \"2025-07-09T06:51:01Z\"\n    lastUpdateTime: \"2025-07-09T06:51:01Z\"\n    message: Auto approving self kubelet client certificate after SubjectAccessReview.\n    reason: AutoApproved\n    status: \"True\"\n    type: Approved\n```\n\nTLS Bootsrapping was disabled in our kubeadm config, and setting `rotateCertificates: false` had no effect.\n\nBecause we don't rely on the kubelet + controller-manager auto certificate handling mechanism for our nodes, the kubelets continued to issue client CSRs and the controller kept approving them.\n\nWe narrowed down the root cause to some cluster role bindings that were created automatically by kubeadm.\n\nCRBs in question:\n```\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: kubeadm:node-autoapprove-certificate-rotation\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient\nsubjects:\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes \n```\nand\n```\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  creationTimestamp: \"2023-10-09T23:54:31Z\"\n  name: kubeadm:node-autoapprove-bootstrap\n  resourceVersion: \"90650499\"\n  uid: 7d2c0095-b8c1-4b7d-baac-0f7b8edd0ec5\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:certificates.k8s.io:certificatesigningrequests:nodeclient\nsubjects:\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:bootstrappers:kubeadm:default-node-token\n```\n\nCombined with [no logic in the CSR cleaner controller code accounting for the scenario of kubelet CSR's getting approved with certificates not issued](https://github.com/kubernetes/kubernetes/blob/release-1.30/pkg/controller/certificates/cleaner/cleaner.go#L53-L61) means no GC and a build up over time of approved CSRs. \n\nMitigation is to manually remove these cluster role bindings and to delete all old approved CSRs... all half million of them.\n\nIs there is an alternative way to deal with the problem we're missing? If so the documentation could use an update.\n\nOtherwise we propose the cleaner be updated with a condition to also remove **Approved** but not Issued CSRs in line with the documentation.\nIf there is a reason why we would not want to remove them in such a short period of time as the **Approved** GC timeline in the documentation, at least treating them the same as **Pending** i.e. 24hrs.\n\n\n### What did you expect to happen?\n\n- We expected these would get garbage collected in line with the GC policy where [\"Approved requests: automatically deleted after 1 hour](https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#request-signing-process)\" \n \n- if this condition is not included by design, documentation should be updated to explain it.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n- kubeadm configured without TLS bootstrapping and with self managed certificate issuing\n- Assert these [these cluster role bindings](https://kubernetes.io/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/#approval) allowing the kube controller to auto approve the certificates.\n  - `kubeadm:node-autoapprove-certificate-rotation`\n  - `kubeadm:node-autoapprove-bootstrap`\n- Wait 15 minutes and run check that kubelet CSRs that get created are now moving into an `Approved` state but never get issued certificates `kubectl get csr --sort-by=.metadata.creationTimestamp -o json | jq -r '.items[] | select(.status.conditions[]?.type==\"Approved\" and (.status.certificate|length==0)) | .metadata.name'`\n- After exceeding 24 hours later they should still persist and be growing in count.\n\n### Anything else we need to know?\n\nHashiVault used for certificate signing. Not so much relevant but does gives context in our use case.\n\n### Kubernetes version\n\n<details>\n```console\nClient Version: v1.30.8\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\nServer Version: v1.30.8\n```\n</details>\n\n\n### Cloud provider\n\nN\\A - Bare metal.\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\nPRETTY_NAME=\"Ubuntu 22.04.5 LTS\"\nNAME=\"Ubuntu\"\nVERSION_ID=\"22.04\"\nVERSION=\"22.04.5 LTS (Jammy Jellyfish)\"\nVERSION_CODENAME=jammy\nID=ubuntu\nID_LIKE=debian\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nUBUNTU_CODENAME=jammy\n\n$ uname -a\nLinux broken 5.15.0-139-generic #149-Ubuntu SMP Fri Apr 11 22:06:13 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux\n```\n</details>\n\n\n### Install tools\n\nNot really relevant IMO. Ask if you have any specific information you need.\n\n### Container runtime (CRI) and version (if applicable)\n\n\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n_No response_",
      "solution": "@264nm \nSorry, I didn\u2019t realize you had already prepared a patch.\nSince I\u2019ve worked on `csr-approver` a few times before, I submitted the PR before the triage was accepted to help resolve the issue quickly.\nBut since you\u2019re the one who raised the issue, if your patch is similar to mine, I\u2019d be happy to close my PR and let you open a new one with your version instead.\nTake a look and let me know what you think.\nthanks!\n\n---\n\n@choejwoo  I appreciate you jumping on it. If it's okay I would still like to raise it is really basically identical. The only thing preventing me from raising immediately is I have to jump through a few hoops to contribute via my work as the organisation previously made it very difficult to contribute to open source projects on behalf of the company and we're trying to change that, and one of the criteria is that the issue is accepted hence why I was waiting first for triage. Maybe I attached the wrong SIG though - was hard to tell because API machinery says it handles GC whereas obviously there is relevance for SIG Auth as well.\n\n---\n\nSorry for leaving this open so long I thought I closed the issue last year.",
      "labels": [
        "kind/bug",
        "sig/api-machinery",
        "sig/auth",
        "triage/accepted"
      ],
      "created_at": "2025-07-15T06:16:26Z",
      "closed_at": "2026-02-02T23:51:50Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/132947",
      "comments_count": 12
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 119766,
      "title": "OpenAPI Defaults not accurate",
      "problem": "### What happened?\r\n\r\nThe defaulting listed in OpenAPI spec is terribly incomplete, or incorrect. The following diffs were generated by comparing defaults as applying by the OpenAPI schema to the handwritten defaults applied by runtime.Scheme:\r\n\r\n(Handwritten defaults in RED, Schema in GREEN)\r\n\r\n- [x] /v1.Binding\r\n- [x] /v1.ComponentStatus\r\n- [ ] /v1.ConfigMap\r\n    ```diff\r\n      &v1.ConfigMap{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tImmutable:  nil,\r\n    - \tData:       map[string]string{},\r\n    + \tData:       nil,\r\n      \tBinaryData: nil,\r\n      }\r\n    \r\n    ```\r\n- [x] /v1.DeleteOptions\r\n- [ ] /v1.Endpoints\r\n    ```diff\r\n      &v1.Endpoints{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {OwnerReferences: {{}}, ManagedFields: {{}}},\r\n      \tSubsets: []v1.EndpointSubset{\r\n      \t\t{\r\n      \t\t\tAddresses:         {{}},\r\n      \t\t\tNotReadyAddresses: {{}},\r\n      \t\t\tPorts: []v1.EndpointPort{\r\n      \t\t\t\t{\r\n      \t\t\t\t\tName:        \"\",\r\n      \t\t\t\t\tPort:        0,\r\n    - \t\t\t\t\tProtocol:    \"TCP\",\r\n    + \t\t\t\t\tProtocol:    \"\",\r\n      \t\t\t\t\tAppProtocol: nil,\r\n      \t\t\t\t},\r\n      \t\t\t},\r\n      \t\t},\r\n      \t},\r\n      }\r\n    \r\n    ```\r\n- [x] /v1.Event\r\n- [x] /v1.LimitRange\r\n- [ ] /v1.Namespace\r\n    ```diff\r\n      &v1.Namespace{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tSpec:       {},\r\n      \tStatus: v1.NamespaceStatus{\r\n    - \t\tPhase:      \"Active\",\r\n    + \t\tPhase:      \"\",\r\n      \t\tConditions: nil,\r\n      \t},\r\n      }\r\n    \r\n    ```\r\n- [x] /v1.Node\r\n- [ ] /v1.PersistentVolume\r\n    ```diff\r\n      &v1.PersistentVolume{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tSpec: v1.PersistentVolumeSpec{\r\n      \t\t... // 2 identical fields\r\n      \t\tAccessModes:                   nil,\r\n      \t\tClaimRef:                      nil,\r\n    - \t\tPersistentVolumeReclaimPolicy: \"Retain\",\r\n    + \t\tPersistentVolumeReclaimPolicy: \"\",\r\n      \t\tStorageClassName:              \"\",\r\n      \t\tMountOptions:                  nil,\r\n    - \t\tVolumeMode:                    &\"Filesystem\",\r\n    + \t\tVolumeMode:                    nil,\r\n      \t\tNodeAffinity:                  nil,\r\n      \t},\r\n      \tStatus: v1.PersistentVolumeStatus{\r\n    - \t\tPhase:                   \"Pending\",\r\n    + \t\tPhase:                   \"\",\r\n      \t\tMessage:                 \"\",\r\n      \t\tReason:                  \"\",\r\n      \t\tLastPhaseTransitionTime: nil,\r\n      \t},\r\n      }\r\n    \r\n    ```\r\n- [ ] /v1.PersistentVolumeClaim\r\n    ```diff\r\n      &v1.PersistentVolumeClaim{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tSpec: v1.PersistentVolumeClaimSpec{\r\n      \t\t... // 3 identical fields\r\n      \t\tVolumeName:       \"\",\r\n      \t\tStorageClassName: nil,\r\n    - \t\tVolumeMode:       &\"Filesystem\",\r\n    + \t\tVolumeMode:       nil,\r\n      \t\tDataSource:       nil,\r\n      \t\tDataSourceRef:    nil,\r\n      \t},\r\n      \tStatus: v1.PersistentVolumeClaimStatus{\r\n    - \t\tPhase:       \"Pending\",\r\n    + \t\tPhase:       \"\",\r\n      \t\tAccessModes: nil,\r\n      \t\tCapacity:    nil,\r\n      \t\t... // 3 identical fields\r\n      \t},\r\n      }\r\n    \r\n    ```\r\n- [ ] /v1.Pod\r\n    ```diff\r\n      &v1.Pod{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tSpec: v1.PodSpec{\r\n      \t\t... // 2 identical fields\r\n      \t\tContainers:                    nil,\r\n      \t\tEphemeralContainers:           nil,\r\n    - \t\tRestartPolicy:                 \"Always\",\r\n    + \t\tRestartPolicy:                 \"\",\r\n    - \t\tTerminationGracePeriodSeconds: &30,\r\n    + \t\tTerminationGracePeriodSeconds: nil,\r\n      \t\tActiveDeadlineSeconds:         nil,\r\n    - \t\tDNSPolicy:                     \"ClusterFirst\",\r\n    + \t\tDNSPolicy:                     \"\",\r\n      \t\tNodeSelector:                  nil,\r\n      \t\tServiceAccountName:            \"\",\r\n      \t\t... // 5 identical fields\r\n      \t\tHostIPC:               false,\r\n      \t\tShareProcessNamespace: nil,\r\n    - \t\tSecurityContext:       s\"&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,}\",\r\n    + \t\tSecurityContext:       nil,\r\n      \t\tImagePullSecrets:      nil,\r\n      \t\tHostname:              \"\",\r\n      \t\tSubdomain:             \"\",\r\n      \t\tAffinity:              nil,\r\n    - \t\tSchedulerName:         \"default-scheduler\",\r\n    + \t\tSchedulerName:         \"\",\r\n      \t\tTolerations:           nil,\r\n      \t\tHostAliases:           nil,\r\n      \t\t... // 3 identical fields\r\n      \t\tReadinessGates:     nil,\r\n      \t\tRuntimeClassName:   nil,\r\n    - \t\tEnableServiceLinks: &true,\r\n    + \t\tEnableServiceLinks: nil,\r\n      \t\tPreemptionPolicy:   nil,\r\n      \t\tOverhead:           nil,\r\n      \t\t... // 6 identical fields\r\n      \t},\r\n      \tStatus: {},\r\n      }\r\n    \r\n    ```\r\n- [ ] /v1.PodTemplate\r\n    ```diff\r\n      &v1.PodTemplate{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tTemplate: v1.PodTemplateSpec{\r\n      \t\tObjectMeta: {},\r\n      \t\tSpec: v1.PodSpec{\r\n      \t\t\t... // 2 identical fields\r\n      \t\t\tContainers:                    nil,\r\n      \t\t\tEphemeralContainers:           nil,\r\n    - \t\t\tRestartPolicy:                 \"Always\",\r\n    + \t\t\tRestartPolicy:                 \"\",\r\n    - \t\t\tTerminationGracePeriodSeconds: &30,\r\n    + \t\t\tTerminationGracePeriodSeconds: nil,\r\n      \t\t\tActiveDeadlineSeconds:         nil,\r\n    - \t\t\tDNSPolicy:                     \"ClusterFirst\",\r\n    + \t\t\tDNSPolicy:                     \"\",\r\n      \t\t\tNodeSelector:                  nil,\r\n      \t\t\tServiceAccountName:            \"\",\r\n      \t\t\t... // 5 identical fields\r\n      \t\t\tHostIPC:               false,\r\n      \t\t\tShareProcessNamespace: nil,\r\n    - \t\t\tSecurityContext:       s\"&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,}\",\r\n    + \t\t\tSecurityContext:       nil,\r\n      \t\t\tImagePullSecrets:      nil,\r\n      \t\t\tHostname:              \"\",\r\n      \t\t\tSubdomain:             \"\",\r\n      \t\t\tAffinity:              nil,\r\n    - \t\t\tSchedulerName:         \"default-scheduler\",\r\n    + \t\t\tSchedulerName:         \"\",\r\n      \t\t\tTolerations:           nil,\r\n      \t\t\tHostAliases:           nil,\r\n      \t\t\t... // 14 identical fields\r\n      \t\t},\r\n      \t},\r\n      }\r\n    \r\n    ```\r\n- [ ] /v1.ReplicationController\r\n    ```diff\r\n      &v1.ReplicationController{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tSpec: v1.ReplicationControllerSpec{\r\n    - \t\tReplicas:        &1,\r\n    + \t\tReplicas:        nil,\r\n      \t\tMinReadySeconds: 0,\r\n      \t\tSelector:        nil,\r\n      \t\tTemplate:        nil,\r\n      \t},\r\n      \tStatus: {},\r\n      }\r\n    \r\n    ```\r\n- [x] /v1.ResourceQuota\r\n- [ ] /v1.Secret\r\n    ```diff\r\n      &v1.Secret{\r\n      \t... // 3 identical fields\r\n      \tData:       nil,\r\n      \tStringData: nil,\r\n    - \tType:       \"Opaque\",\r\n    + \tType:       \"\",\r\n      }\r\n    \r\n    ```\r\n- [ ] /v1.Service\r\n    ```diff\r\n      &v1.Service{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tSpec: v1.ServiceSpec{\r\n      \t\t... // 2 identical fields\r\n      \t\tClusterIP:                \"\",\r\n      \t\tClusterIPs:               nil,\r\n    - \t\tType:                     \"ClusterIP\",\r\n    + \t\tType:                     \"\",\r\n      \t\tExternalIPs:              nil,\r\n    - \t\tSessionAffinity:          \"None\",\r\n    + \t\tSessionAffinity:          \"\",\r\n      \t\tLoadBalancerIP:           \"\",\r\n      \t\tLoadBalancerSourceRanges: nil,\r\n      \t\t... // 7 identical fields\r\n      \t\tAllocateLoadBalancerNodePorts: nil,\r\n      \t\tLoadBalancerClass:             nil,\r\n    - \t\tInternalTrafficPolicy:         &\"Cluster\",\r\n    + \t\tInternalTrafficPolicy:         nil,\r\n      \t},\r\n      \tStatus: {},\r\n      }\r\n    \r\n    ```\r\n- [x] /v1.ServiceAccount\r\n- [x] /v1.Status\r\n- [ ] /v1.WatchEvent\r\n    ```diff\r\n      &v1.WatchEvent{\r\n      \tType: \"\",\r\n      \tObject: runtime.RawExtension{\r\n    - \t\tRaw:    nil,\r\n    + \t\tRaw:    []uint8(\"{}\"),\r\n      \t\tObject: nil,\r\n      \t},\r\n      }\r\n    \r\n    ```\r\n- [ ] admissionregistration.k8s.io/v1.MutatingWebhookConfiguration\r\n    ```diff\r\n      &v1.MutatingWebhookConfiguration{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tWebhooks: []v1.MutatingWebhook{\r\n      \t\t{\r\n      \t\t\tName:                    \"\",\r\n      \t\t\tClientConfig:            {},\r\n      \t\t\tRules:                   nil,\r\n    - \t\t\tFailurePolicy:           &\"Fail\",\r\n    + \t\t\tFailurePolicy:           nil,\r\n    - \t\t\tMatchPolicy:             &\"Equivalent\",\r\n    + \t\t\tMatchPolicy:             nil,\r\n    - \t\t\tNamespaceSelector:       s\"&LabelSelector{MatchLabels:map[string]string{},MatchExpressions:[]LabelSelectorRequirement{},}\",\r\n    + \t\t\tNamespaceSelector:       nil,\r\n    - \t\t\tObjectSelector:          s\"&LabelSelector{MatchLabels:map[string]string{},MatchExpressions:[]LabelSelectorRequirement{},}\",\r\n    + \t\t\tObjectSelector:          nil,\r\n      \t\t\tSideEffects:             nil,\r\n    - \t\t\tTimeoutSeconds:          &10,\r\n    + \t\t\tTimeoutSeconds:          nil,\r\n      \t\t\tAdmissionReviewVersions: nil,\r\n    - \t\t\tReinvocationPolicy:      &\"Never\",\r\n    + \t\t\tReinvocationPolicy:      nil,\r\n      \t\t\tMatchConditions:         nil,\r\n      \t\t},\r\n      \t},\r\n      }\r\n    \r\n    ```\r\n- [ ] admissionregistration.k8s.io/v1.ValidatingWebhookConfiguration\r\n    ```diff\r\n      &v1.ValidatingWebhookConfiguration{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tWebhooks: []v1.ValidatingWebhook{\r\n      \t\t{\r\n      \t\t\tName:                    \"\",\r\n      \t\t\tClientConfig:            {},\r\n      \t\t\tRules:                   nil,\r\n    - \t\t\tFailurePolicy:           &\"Fail\",\r\n    + \t\t\tFailurePolicy:           nil,\r\n    - \t\t\tMatchPolicy:             &\"Equivalent\",\r\n    + \t\t\tMatchPolicy:             nil,\r\n    - \t\t\tNamespaceSelector:       s\"&LabelSelector{MatchLabels:map[string]string{},MatchExpressions:[]LabelSelectorRequirement{},}\",\r\n    + \t\t\tNamespaceSelector:       nil,\r\n    - \t\t\tObjectSelector:          s\"&LabelSelector{MatchLabels:map[string]string{},MatchExpressions:[]LabelSelectorRequirement{},}\",\r\n    + \t\t\tObjectSelector:          nil,\r\n      \t\t\tSideEffects:             nil,\r\n    - \t\t\tTimeoutSeconds:          &10,\r\n    + \t\t\tTimeoutSeconds:          nil,\r\n      \t\t\tAdmissionReviewVersions: nil,\r\n      \t\t\tMatchConditions:         nil,\r\n      \t\t},\r\n      \t},\r\n      }\r\n    \r\n    ```\r\n- [ ] admissionregistration.k8s.io/v1alpha1.ValidatingAdmissionPolicy\r\n    ```diff\r\n      &v1alpha1.ValidatingAdmissionPolicy{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tSpec: v1alpha1.ValidatingAdmissionPolicySpec{\r\n      \t\tParamKind:        nil,\r\n      \t\tMatchConstraints: nil,\r\n      \t\tValidations:      nil,\r\n    - \t\tFailurePolicy:    &\"Fail\",\r\n    + \t\tFailurePolicy:    nil,\r\n      \t\tAuditAnnotations: nil,\r\n      \t\tMatchConditions:  nil,\r\n      \t\tVariables:        nil,\r\n      \t},\r\n      \tStatus: {},\r\n      }\r\n    \r\n    ```\r\n- [ ] admissionregistration.k8s.io/v1alpha1.ValidatingAdmissionPolicyBinding\r\n    ```diff\r\n      &v1alpha1.ValidatingAdmissionPolicyBinding{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {OwnerReferences: {{}}, ManagedFields: {{}}},\r\n      \tSpec: v1alpha1.ValidatingAdmissionPolicyBindingSpec{\r\n      \t\tPolicyName: \"\",\r\n      \t\tParamRef: &v1alpha1.ParamRef{\r\n      \t\t\tName:                    \"\",\r\n      \t\t\tNamespace:               \"\",\r\n      \t\t\tSelector:                nil,\r\n    - \t\t\tParameterNotFoundAction: &\"Deny\",\r\n    + \t\t\tParameterNotFoundAction: nil,\r\n      \t\t},\r\n      \t\tMatchResources: &v1alpha1.MatchResources{\r\n    - \t\t\tNamespaceSelector:    s\"&LabelSelector{MatchLabels:map[string]string{},MatchExpressions:[]LabelSelectorRequirement{},}\",\r\n    + \t\t\tNamespaceSelector:    nil,\r\n    - \t\t\tObjectSelector:       s\"&LabelSelector{MatchLabels:map[string]string{},MatchExpressions:[]LabelSelectorRequirement{},}\",\r\n    + \t\t\tObjectSelector:       nil,\r\n      \t\t\tResourceRules:        nil,\r\n      \t\t\tExcludeResourceRules: nil,\r\n    - \t\t\tMatchPolicy:          &\"Equivalent\",\r\n    + \t\t\tMatchPolicy:          nil,\r\n      \t\t},\r\n      \t\tValidationActions: nil,\r\n      \t},\r\n      }\r\n    \r\n    ```\r\n- [ ] admissionregistration.k8s.io/v1beta1.ValidatingAdmissionPolicy\r\n    ```diff\r\n      &v1beta1.ValidatingAdmissionPolicy{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tSpec: v1beta1.ValidatingAdmissionPolicySpec{\r\n      \t\tParamKind:        nil,\r\n      \t\tMatchConstraints: nil,\r\n      \t\tValidations:      nil,\r\n    - \t\tFailurePolicy:    &\"Fail\",\r\n    + \t\tFailurePolicy:    nil,\r\n      \t\tAuditAnnotations: nil,\r\n      \t\tMatchConditions:  nil,\r\n      \t\tVariables:        nil,\r\n      \t},\r\n      \tStatus: {},\r\n      }\r\n    \r\n    ```\r\n- [ ] admissionregistration.k8s.io/v1beta1.ValidatingAdmissionPolicyBinding\r\n    ```diff\r\n      &v1beta1.ValidatingAdmissionPolicyBinding{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {OwnerReferences: {{}}, ManagedFields: {{}}},\r\n      \tSpec: v1beta1.ValidatingAdmissionPolicyBindingSpec{\r\n      \t\tPolicyName: \"\",\r\n      \t\tParamRef:   &{},\r\n      \t\tMatchResources: &v1beta1.MatchResources{\r\n    - \t\t\tNamespaceSelector:    s\"&LabelSelector{MatchLabels:map[string]string{},MatchExpressions:[]LabelSelectorRequirement{},}\",\r\n    + \t\t\tNamespaceSelector:    nil,\r\n    - \t\t\tObjectSelector:       s\"&LabelSelector{MatchLabels:map[string]string{},MatchExpressions:[]LabelSelectorRequirement{},}\",\r\n    + \t\t\tObjectSelector:       nil,\r\n      \t\t\tResourceRules:        nil,\r\n      \t\t\tExcludeResourceRules: nil,\r\n    - \t\t\tMatchPolicy:          &\"Equivalent\",\r\n    + \t\t\tMatchPolicy:          nil,\r\n      \t\t},\r\n      \t\tValidationActions: nil,\r\n      \t},\r\n      }\r\n    \r\n    ```\r\n- [ ] apiextensions.k8s.io/v1.CustomResourceDefinition\r\n    ```diff\r\n      &v1.CustomResourceDefinition{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tSpec: v1.CustomResourceDefinitionSpec{\r\n      \t\t... // 2 identical fields\r\n      \t\tScope:                 \"\",\r\n      \t\tVersions:              nil,\r\n    - \t\tConversion:            s\"&CustomResourceConversion{Strategy:None,Webhook:nil,}\",\r\n    + \t\tConversion:            nil,\r\n      \t\tPreserveUnknownFields: false,\r\n      \t},\r\n      \tStatus: {},\r\n      }\r\n    \r\n    ```\r\n- [ ] apiregistration.k8s.io/v1.APIService\r\n    ```diff\r\n      &v1.APIService{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {OwnerReferences: {{}}, ManagedFields: {{}}},\r\n      \tSpec: v1.APIServiceSpec{\r\n      \t\tService: &v1.ServiceReference{\r\n      \t\t\tNamespace: \"\",\r\n      \t\t\tName:      \"\",\r\n    - \t\t\tPort:      &443,\r\n    + \t\t\tPort:      nil,\r\n      \t\t},\r\n      \t\tGroup:   \"\",\r\n      \t\tVersion: \"\",\r\n      \t\t... // 4 identical fields\r\n      \t},\r\n      \tStatus: {Conditions: {{}}},\r\n      }\r\n    \r\n    ```\r\n- [ ] apps/v1.ControllerRevision\r\n    ```diff\r\n      &v1.ControllerRevision{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tData: runtime.RawExtension{\r\n    - \t\tRaw:    nil,\r\n    + \t\tRaw:    []uint8(\"{}\"),\r\n      \t\tObject: nil,\r\n      \t},\r\n      \tRevision: 0,\r\n      }\r\n    \r\n    ```\r\n- [ ] apps/v1.DaemonSet\r\n    ```diff\r\n      &v1.DaemonSet{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tSpec: v1.DaemonSetSpec{\r\n      \t\tSelector: nil,\r\n      \t\tTemplate: v1.PodTemplateSpec{\r\n      \t\t\tObjectMeta: {},\r\n      \t\t\tSpec: v1.PodSpec{\r\n      \t\t\t\t... // 2 identical fields\r\n      \t\t\t\tContainers:                    nil,\r\n      \t\t\t\tEphemeralContainers:           nil,\r\n    - \t\t\t\tRestartPolicy:                 \"Always\",\r\n    + \t\t\t\tRestartPolicy:                 \"\",\r\n    - \t\t\t\tTerminationGracePeriodSeconds: &30,\r\n    + \t\t\t\tTerminationGracePeriodSeconds: nil,\r\n      \t\t\t\tActiveDeadlineSeconds:         nil,\r\n    - \t\t\t\tDNSPolicy:                     \"ClusterFirst\",\r\n    + \t\t\t\tDNSPolicy:                     \"\",\r\n      \t\t\t\tNodeSelector:                  nil,\r\n      \t\t\t\tServiceAccountName:            \"\",\r\n      \t\t\t\t... // 5 identical fields\r\n      \t\t\t\tHostIPC:               false,\r\n      \t\t\t\tShareProcessNamespace: nil,\r\n    - \t\t\t\tSecurityContext:       s\"&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,}\",\r\n    + \t\t\t\tSecurityContext:       nil,\r\n      \t\t\t\tImagePullSecrets:      nil,\r\n      \t\t\t\tHostname:              \"\",\r\n      \t\t\t\tSubdomain:             \"\",\r\n      \t\t\t\tAffinity:              nil,\r\n    - \t\t\t\tSchedulerName:         \"default-scheduler\",\r\n    + \t\t\t\tSchedulerName:         \"\",\r\n      \t\t\t\tTolerations:           nil,\r\n      \t\t\t\tHostAliases:           nil,\r\n      \t\t\t\t... // 14 identical fields\r\n      \t\t\t},\r\n      \t\t},\r\n      \t\tUpdateStrategy: v1.DaemonSetUpdateStrategy{\r\n    - \t\t\tType:          \"RollingUpdate\",\r\n    + \t\t\tType:          \"\",\r\n    - \t\t\tRollingUpdate: s\"&RollingUpdateDaemonSet{MaxUnavailable:1,MaxSurge:0,}\",\r\n    + \t\t\tRollingUpdate: nil,\r\n      \t\t},\r\n      \t\tMinReadySeconds:      0,\r\n    - \t\tRevisionHistoryLimit: &10,\r\n    + \t\tRevisionHistoryLimit: nil,\r\n      \t},\r\n      \tStatus: {},\r\n      }\r\n    \r\n    ```\r\n- [ ] apps/v1.Deployment\r\n    ```diff\r\n      &v1.Deployment{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tSpec: v1.DeploymentSpec{\r\n    - \t\tReplicas: &1,\r\n    + \t\tReplicas: nil,\r\n      \t\tSelector: nil,\r\n      \t\tTemplate: v1.PodTemplateSpec{\r\n      \t\t\tObjectMeta: {},\r\n      \t\t\tSpec: v1.PodSpec{\r\n      \t\t\t\t... // 2 identical fields\r\n      \t\t\t\tContainers:                    nil,\r\n      \t\t\t\tEphemeralContainers:           nil,\r\n    - \t\t\t\tRestartPolicy:                 \"Always\",\r\n    + \t\t\t\tRestartPolicy:                 \"\",\r\n    - \t\t\t\tTerminationGracePeriodSeconds: &30,\r\n    + \t\t\t\tTerminationGracePeriodSeconds: nil,\r\n      \t\t\t\tActiveDeadlineSeconds:         nil,\r\n    - \t\t\t\tDNSPolicy:                     \"ClusterFirst\",\r\n    + \t\t\t\tDNSPolicy:                     \"\",\r\n      \t\t\t\tNodeSelector:                  nil,\r\n      \t\t\t\tServiceAccountName:            \"\",\r\n      \t\t\t\t... // 5 identical fields\r\n      \t\t\t\tHostIPC:               false,\r\n      \t\t\t\tShareProcessNamespace: nil,\r\n    - \t\t\t\tSecurityContext:       s\"&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,}\",\r\n    + \t\t\t\tSecurityContext:       nil,\r\n      \t\t\t\tImagePullSecrets:      nil,\r\n      \t\t\t\tHostname:              \"\",\r\n      \t\t\t\tSubdomain:             \"\",\r\n      \t\t\t\tAffinity:              nil,\r\n    - \t\t\t\tSchedulerName:         \"default-scheduler\",\r\n    + \t\t\t\tSchedulerName:         \"\",\r\n      \t\t\t\tTolerations:           nil,\r\n      \t\t\t\tHostAliases:           nil,\r\n      \t\t\t\t... // 14 identical fields\r\n      \t\t\t},\r\n      \t\t},\r\n      \t\tStrategy: v1.DeploymentStrategy{\r\n    - \t\t\tType:          \"RollingUpdate\",\r\n    + \t\t\tType:          \"\",\r\n    - \t\t\tRollingUpdate: s\"&RollingUpdateDeployment{MaxUnavailable:25%,MaxSurge:25%,}\",\r\n    + \t\t\tRollingUpdate: nil,\r\n      \t\t},\r\n      \t\tMinReadySeconds:         0,\r\n    - \t\tRevisionHistoryLimit:    &10,\r\n    + \t\tRevisionHistoryLimit:    nil,\r\n      \t\tPaused:                  false,\r\n    - \t\tProgressDeadlineSeconds: &600,\r\n    + \t\tProgressDeadlineSeconds: nil,\r\n      \t},\r\n      \tStatus: {},\r\n      }\r\n    \r\n    ```\r\n- [ ] apps/v1.ReplicaSet\r\n    ```diff\r\n      &v1.ReplicaSet{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tSpec: v1.ReplicaSetSpec{\r\n    - \t\tReplicas:        &1,\r\n    + \t\tReplicas:        nil,\r\n      \t\tMinReadySeconds: 0,\r\n      \t\tSelector:        nil,\r\n      \t\tTemplate: v1.PodTemplateSpec{\r\n      \t\t\tObjectMeta: {},\r\n      \t\t\tSpec: v1.PodSpec{\r\n      \t\t\t\t... // 2 identical fields\r\n      \t\t\t\tContainers:                    nil,\r\n      \t\t\t\tEphemeralContainers:           nil,\r\n    - \t\t\t\tRestartPolicy:                 \"Always\",\r\n    + \t\t\t\tRestartPolicy:                 \"\",\r\n    - \t\t\t\tTerminationGracePeriodSeconds: &30,\r\n    + \t\t\t\tTerminationGracePeriodSeconds: nil,\r\n      \t\t\t\tActiveDeadlineSeconds:         nil,\r\n    - \t\t\t\tDNSPolicy:                     \"ClusterFirst\",\r\n    + \t\t\t\tDNSPolicy:                     \"\",\r\n      \t\t\t\tNodeSelector:                  nil,\r\n      \t\t\t\tServiceAccountName:            \"\",\r\n      \t\t\t\t... // 5 identical fields\r\n      \t\t\t\tHostIPC:               false,\r\n      \t\t\t\tShareProcessNamespace: nil,\r\n    - \t\t\t\tSecurityContext:       s\"&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,}\",\r\n    + \t\t\t\tSecurityContext:       nil,\r\n      \t\t\t\tImagePullSecrets:      nil,\r\n      \t\t\t\tHostname:              \"\",\r\n      \t\t\t\tSubdomain:             \"\",\r\n      \t\t\t\tAffinity:              nil,\r\n    - \t\t\t\tSchedulerName:         \"default-scheduler\",\r\n    + \t\t\t\tSchedulerName:         \"\",\r\n      \t\t\t\tTolerations:           nil,\r\n      \t\t\t\tHostAliases:           nil,\r\n      \t\t\t\t... // 14 identical fields\r\n      \t\t\t},\r\n      \t\t},\r\n      \t},\r\n      \tStatus: {},\r\n      }\r\n    \r\n    ```\r\n- [ ] apps/v1.StatefulSet\r\n    ```diff\r\n      &v1.StatefulSet{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tSpec: v1.StatefulSetSpec{\r\n    - \t\tReplicas: &1,\r\n    + \t\tReplicas: nil,\r\n      \t\tSelector: nil,\r\n      \t\tTemplate: v1.PodTemplateSpec{\r\n      \t\t\tObjectMeta: {},\r\n      \t\t\tSpec: v1.PodSpec{\r\n      \t\t\t\t... // 2 identical fields\r\n      \t\t\t\tContainers:                    nil,\r\n      \t\t\t\tEphemeralContainers:           nil,\r\n    - \t\t\t\tRestartPolicy:                 \"Always\",\r\n    + \t\t\t\tRestartPolicy:                 \"\",\r\n    - \t\t\t\tTerminationGracePeriodSeconds: &30,\r\n    + \t\t\t\tTerminationGracePeriodSeconds: nil,\r\n      \t\t\t\tActiveDeadlineSeconds:         nil,\r\n    - \t\t\t\tDNSPolicy:                     \"ClusterFirst\",\r\n    + \t\t\t\tDNSPolicy:                     \"\",\r\n      \t\t\t\tNodeSelector:                  nil,\r\n      \t\t\t\tServiceAccountName:            \"\",\r\n      \t\t\t\t... // 5 identical fields\r\n      \t\t\t\tHostIPC:               false,\r\n      \t\t\t\tShareProcessNamespace: nil,\r\n    - \t\t\t\tSecurityContext:       s\"&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,}\",\r\n    + \t\t\t\tSecurityContext:       nil,\r\n      \t\t\t\tImagePullSecrets:      nil,\r\n      \t\t\t\tHostname:              \"\",\r\n      \t\t\t\tSubdomain:             \"\",\r\n      \t\t\t\tAffinity:              nil,\r\n    - \t\t\t\tSchedulerName:         \"default-scheduler\",\r\n    + \t\t\t\tSchedulerName:         \"\",\r\n      \t\t\t\tTolerations:           nil,\r\n      \t\t\t\tHostAliases:           nil,\r\n      \t\t\t\t... // 14 identical fields\r\n      \t\t\t},\r\n      \t\t},\r\n      \t\tVolumeClaimTemplates: nil,\r\n      \t\tServiceName:          \"\",\r\n    - \t\tPodManagementPolicy:  \"OrderedReady\",\r\n    + \t\tPodManagementPolicy:  \"\",\r\n      \t\tUpdateStrategy: v1.StatefulSetUpdateStrategy{\r\n    - \t\t\tType:          \"RollingUpdate\",\r\n    + \t\t\tType:          \"\",\r\n    - \t\t\tRollingUpdate: s\"&RollingUpdateStatefulSetStrategy{Partition:*0,MaxUnavailable:<nil>,}\",\r\n    + \t\t\tRollingUpdate: nil,\r\n      \t\t},\r\n    - \t\tRevisionHistoryLimit:                 &10,\r\n    + \t\tRevisionHistoryLimit:                 nil,\r\n      \t\tMinReadySeconds:                      0,\r\n    - \t\tPersistentVolumeClaimRetentionPolicy: s\"&StatefulSetPersistentVolumeClaimRetentionPolicy{WhenDeleted:Retain,WhenScaled:Retain,}\",\r\n    + \t\tPersistentVolumeClaimRetentionPolicy: nil,\r\n      \t\tOrdinals:                             nil,\r\n      \t},\r\n      \tStatus: {},\r\n      }\r\n    \r\n    ```\r\n- [x] authentication.k8s.io/v1.SelfSubjectReview\r\n- [ ] authentication.k8s.io/v1.TokenRequest\r\n    ```diff\r\n      &v1.TokenRequest{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tSpec: v1.TokenRequestSpec{\r\n      \t\tAudiences:         nil,\r\n    - \t\tExpirationSeconds: &3600,\r\n    + \t\tExpirationSeconds: nil,\r\n      \t\tBoundObjectRef:    nil,\r\n      \t},\r\n      \tStatus: {},\r\n      }\r\n    \r\n    ```\r\n- [x] authentication.k8s.io/v1.TokenReview\r\n- [x] authentication.k8s.io/v1alpha1.SelfSubjectReview\r\n- [x] authentication.k8s.io/v1beta1.SelfSubjectReview\r\n- [x] authorization.k8s.io/v1.LocalSubjectAccessReview\r\n- [x] authorization.k8s.io/v1.SelfSubjectAccessReview\r\n- [x] authorization.k8s.io/v1.SelfSubjectRulesReview\r\n- [x] authorization.k8s.io/v1.SubjectAccessReview\r\n- [ ] autoscaling/v1.HorizontalPodAutoscaler\r\n    ```diff\r\n      &v1.HorizontalPodAutoscaler{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tSpec: v1.HorizontalPodAutoscalerSpec{\r\n      \t\tScaleTargetRef:                 {},\r\n    - \t\tMinReplicas:                    &1,\r\n    + \t\tMinReplicas:                    nil,\r\n      \t\tMaxReplicas:                    0,\r\n      \t\tTargetCPUUtilizationPercentage: nil,\r\n      \t},\r\n      \tStatus: {},\r\n      }\r\n    \r\n    ```\r\n- [x] autoscaling/v1.Scale\r\n- [ ] autoscaling/v2.HorizontalPodAutoscaler\r\n    ```diff\r\n      &v2.HorizontalPodAutoscaler{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tSpec: v2.HorizontalPodAutoscalerSpec{\r\n      \t\tScaleTargetRef: {},\r\n    - \t\tMinReplicas:    &1,\r\n    + \t\tMinReplicas:    nil,\r\n      \t\tMaxReplicas:    0,\r\n    - \t\tMetrics: []v2.MetricSpec{\r\n    - \t\t\t{\r\n    - \t\t\t\tType:     \"Resource\",\r\n    - \t\t\t\tResource: s\"&ResourceMetricSource{Name:cpu,Target:MetricTarget{Type:Utilizat\"...,\r\n    - \t\t\t},\r\n    - \t\t},\r\n    + \t\tMetrics:  nil,\r\n      \t\tBehavior: nil,\r\n      \t},\r\n      \tStatus: {},\r\n      }\r\n    \r\n    ```\r\n- [ ] batch/v1.CronJob\r\n    ```diff\r\n      &v1.CronJob{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tSpec: v1.CronJobSpec{\r\n      \t\tSchedule:                \"\",\r\n      \t\tTimeZone:                nil,\r\n      \t\tStartingDeadlineSeconds: nil,\r\n    - \t\tConcurrencyPolicy:       \"Allow\",\r\n    + \t\tConcurrencyPolicy:       \"\",\r\n    - \t\tSuspend:                 &false,\r\n    + \t\tSuspend:                 nil,\r\n      \t\tJobTemplate: v1.JobTemplateSpec{\r\n      \t\t\tObjectMeta: {},\r\n      \t\t\tSpec: v1.JobSpec{\r\n      \t\t\t\t... // 7 identical fields\r\n      \t\t\t\tSelector:       nil,\r\n      \t\t\t\tManualSelector: nil,\r\n      \t\t\t\tTemplate: v1.PodTemplateSpec{\r\n      \t\t\t\t\tObjectMeta: {},\r\n      \t\t\t\t\tSpec: v1.PodSpec{\r\n      \t\t\t\t\t\t... // 2 identical fields\r\n      \t\t\t\t\t\tContainers:                    nil,\r\n      \t\t\t\t\t\tEphemeralContainers:           nil,\r\n    - \t\t\t\t\t\tRestartPolicy:                 \"Always\",\r\n    + \t\t\t\t\t\tRestartPolicy:                 \"\",\r\n    - \t\t\t\t\t\tTerminationGracePeriodSeconds: &30,\r\n    + \t\t\t\t\t\tTerminationGracePeriodSeconds: nil,\r\n      \t\t\t\t\t\tActiveDeadlineSeconds:         nil,\r\n    - \t\t\t\t\t\tDNSPolicy:                     \"ClusterFirst\",\r\n    + \t\t\t\t\t\tDNSPolicy:                     \"\",\r\n      \t\t\t\t\t\tNodeSelector:                  nil,\r\n      \t\t\t\t\t\tServiceAccountName:            \"\",\r\n      \t\t\t\t\t\t... // 5 identical fields\r\n      \t\t\t\t\t\tHostIPC:               false,\r\n      \t\t\t\t\t\tShareProcessNamespace: nil,\r\n    - \t\t\t\t\t\tSecurityContext:       s\"&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,}\",\r\n    + \t\t\t\t\t\tSecurityContext:       nil,\r\n      \t\t\t\t\t\tImagePullSecrets:      nil,\r\n      \t\t\t\t\t\tHostname:              \"\",\r\n      \t\t\t\t\t\tSubdomain:             \"\",\r\n      \t\t\t\t\t\tAffinity:              nil,\r\n    - \t\t\t\t\t\tSchedulerName:         \"default-scheduler\",\r\n    + \t\t\t\t\t\tSchedulerName:         \"\",\r\n      \t\t\t\t\t\tTolerations:           nil,\r\n      \t\t\t\t\t\tHostAliases:           nil,\r\n      \t\t\t\t\t\t... // 14 identical fields\r\n      \t\t\t\t\t},\r\n      \t\t\t\t},\r\n      \t\t\t\tTTLSecondsAfterFinished: nil,\r\n      \t\t\t\tCompletionMode:          nil,\r\n      \t\t\t\t... // 2 identical fields\r\n      \t\t\t},\r\n      \t\t},\r\n    - \t\tSuccessfulJobsHistoryLimit: &3,\r\n    + \t\tSuccessfulJobsHistoryLimit: nil,\r\n    - \t\tFailedJobsHistoryLimit:     &1,\r\n    + \t\tFailedJobsHistoryLimit:     nil,\r\n      \t},\r\n      \tStatus: {},\r\n      }\r\n    \r\n    ```\r\n- [ ] batch/v1.Job\r\n    ```diff\r\n      &v1.Job{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tSpec: v1.JobSpec{\r\n    - \t\tParallelism:           &1,\r\n    + \t\tParallelism:           nil,\r\n    - \t\tCompletions:           &1,\r\n    + \t\tCompletions:           nil,\r\n      \t\tActiveDeadlineSeconds: nil,\r\n      \t\tPodFailurePolicy:      nil,\r\n    - \t\tBackoffLimit:          &6,\r\n    + \t\tBackoffLimit:          nil,\r\n      \t\tBackoffLimitPerIndex:  nil,\r\n      \t\tMaxFailedIndexes:      nil,\r\n      \t\tSelector:              nil,\r\n      \t\tManualSelector:        nil,\r\n      \t\tTemplate: v1.PodTemplateSpec{\r\n      \t\t\tObjectMeta: {},\r\n      \t\t\tSpec: v1.PodSpec{\r\n      \t\t\t\t... // 2 identical fields\r\n      \t\t\t\tContainers:                    nil,\r\n      \t\t\t\tEphemeralContainers:           nil,\r\n    - \t\t\t\tRestartPolicy:                 \"Always\",\r\n    + \t\t\t\tRestartPolicy:                 \"\",\r\n    - \t\t\t\tTerminationGracePeriodSeconds: &30,\r\n    + \t\t\t\tTerminationGracePeriodSeconds: nil,\r\n      \t\t\t\tActiveDeadlineSeconds:         nil,\r\n    - \t\t\t\tDNSPolicy:                     \"ClusterFirst\",\r\n    + \t\t\t\tDNSPolicy:                     \"\",\r\n      \t\t\t\tNodeSelector:                  nil,\r\n      \t\t\t\tServiceAccountName:            \"\",\r\n      \t\t\t\t... // 5 identical fields\r\n      \t\t\t\tHostIPC:               false,\r\n      \t\t\t\tShareProcessNamespace: nil,\r\n    - \t\t\t\tSecurityContext:       s\"&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,}\",\r\n    + \t\t\t\tSecurityContext:       nil,\r\n      \t\t\t\tImagePullSecrets:      nil,\r\n      \t\t\t\tHostname:              \"\",\r\n      \t\t\t\tSubdomain:             \"\",\r\n      \t\t\t\tAffinity:              nil,\r\n    - \t\t\t\tSchedulerName:         \"default-scheduler\",\r\n    + \t\t\t\tSchedulerName:         \"\",\r\n      \t\t\t\tTolerations:           nil,\r\n      \t\t\t\tHostAliases:           nil,\r\n      \t\t\t\t... // 14 identical fields\r\n      \t\t\t},\r\n      \t\t},\r\n      \t\tTTLSecondsAfterFinished: nil,\r\n    - \t\tCompletionMode:          &\"NonIndexed\",\r\n    + \t\tCompletionMode:          nil,\r\n    - \t\tSuspend:                 &false,\r\n    + \t\tSuspend:                 nil,\r\n      \t\tPodReplacementPolicy:    nil,\r\n      \t},\r\n      \tStatus: {},\r\n      }\r\n    \r\n    ```\r\n- [x] certificates.k8s.io/v1.CertificateSigningRequest\r\n- [x] certificates.k8s.io/v1alpha1.ClusterTrustBundle\r\n- [x] coordination.k8s.io/v1.Lease\r\n- [ ] discovery.k8s.io/v1.EndpointSlice\r\n    ```diff\r\n      &v1.EndpointSlice{\r\n      \t... // 2 identical fields\r\n      \tAddressType: \"\",\r\n      \tEndpoints:   {{}},\r\n      \tPorts: []v1.EndpointPort{\r\n      \t\t{\r\n    - \t\t\tName:        &\"\",\r\n    + \t\t\tName:        nil,\r\n    - \t\t\tProtocol:    &\"TCP\",\r\n    + \t\t\tProtocol:    nil,\r\n      \t\t\tPort:        nil,\r\n      \t\t\tAppProtocol: nil,\r\n      \t\t},\r\n      \t},\r\n      }\r\n    \r\n    ```\r\n- [x] events.k8s.io/v1.Event\r\n- [ ] flowcontrol.apiserver.k8s.io/v1beta2.FlowSchema\r\n    ```diff\r\n      &v1beta2.FlowSchema{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tSpec: v1beta2.FlowSchemaSpec{\r\n      \t\tPriorityLevelConfiguration: {},\r\n    - \t\tMatchingPrecedence:         1000,\r\n    + \t\tMatchingPrecedence:         0,\r\n      \t\tDistinguisherMethod:        nil,\r\n      \t\tRules:                      nil,\r\n      \t},\r\n      \tStatus: {},\r\n      }\r\n    \r\n    ```\r\n- [ ] flowcontrol.apiserver.k8s.io/v1beta2.PriorityLevelConfiguration\r\n    ```diff\r\n      &v1beta2.PriorityLevelConfiguration{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {OwnerReferences: {{}}, ManagedFields: {{}}},\r\n      \tSpec: v1beta2.PriorityLevelConfigurationSpec{\r\n      \t\tType: \"\",\r\n      \t\tLimited: &v1beta2.LimitedPriorityLevelConfiguration{\r\n    - \t\t\tAssuredConcurrencyShares: 30,\r\n    + \t\t\tAssuredConcurrencyShares: 0,\r\n      \t\t\tLimitResponse:            {},\r\n    - \t\t\tLendablePercent:          &0,\r\n    + \t\t\tLendablePercent:          nil,\r\n      \t\t\tBorrowingLimitPercent:    nil,\r\n      \t\t},\r\n      \t\tExempt: &v1beta2.ExemptPriorityLevelConfiguration{\r\n    - \t\t\tNominalConcurrencyShares: &0,\r\n    + \t\t\tNominalConcurrencyShares: nil,\r\n    - \t\t\tLendablePercent:          &0,\r\n    + \t\t\tLendablePercent:          nil,\r\n      \t\t},\r\n      \t},\r\n      \tStatus: {Conditions: {{}}},\r\n      }\r\n    \r\n    ```\r\n- [ ] flowcontrol.apiserver.k8s.io/v1beta3.FlowSchema\r\n    ```diff\r\n      &v1beta3.FlowSchema{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tSpec: v1beta3.FlowSchemaSpec{\r\n      \t\tPriorityLevelConfiguration: {},\r\n    - \t\tMatchingPrecedence:         1000,\r\n    + \t\tMatchingPrecedence:         0,\r\n      \t\tDistinguisherMethod:        nil,\r\n      \t\tRules:                      nil,\r\n      \t},\r\n      \tStatus: {},\r\n      }\r\n    \r\n    ```\r\n- [ ] flowcontrol.apiserver.k8s.io/v1beta3.PriorityLevelConfiguration\r\n    ```diff\r\n      &v1beta3.PriorityLevelConfiguration{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {OwnerReferences: {{}}, ManagedFields: {{}}},\r\n      \tSpec: v1beta3.PriorityLevelConfigurationSpec{\r\n      \t\tType: \"\",\r\n      \t\tLimited: &v1beta3.LimitedPriorityLevelConfiguration{\r\n    - \t\t\tNominalConcurrencyShares: 30,\r\n    + \t\t\tNominalConcurrencyShares: 0,\r\n      \t\t\tLimitResponse:            {},\r\n    - \t\t\tLendablePercent:          &0,\r\n    + \t\t\tLendablePercent:          nil,\r\n      \t\t\tBorrowingLimitPercent:    nil,\r\n      \t\t},\r\n      \t\tExempt: &v1beta3.ExemptPriorityLevelConfiguration{\r\n    - \t\t\tNominalConcurrencyShares: &0,\r\n    + \t\t\tNominalConcurrencyShares: nil,\r\n    - \t\t\tLendablePercent:          &0,\r\n    + \t\t\tLendablePercent:          nil,\r\n      \t\t},\r\n      \t},\r\n      \tStatus: {Conditions: {{}}},\r\n      }\r\n    \r\n    ```\r\n- [x] internal.apiserver.k8s.io/v1alpha1.StorageVersion\r\n- [x] networking.k8s.io/v1.Ingress\r\n- [ ] networking.k8s.io/v1.IngressClass\r\n    ```diff\r\n      &v1.IngressClass{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {OwnerReferences: {{}}, ManagedFields: {{}}},\r\n      \tSpec: v1.IngressClassSpec{\r\n      \t\tController: \"\",\r\n      \t\tParameters: &v1.IngressClassParametersReference{\r\n      \t\t\tAPIGroup:  nil,\r\n      \t\t\tKind:      \"\",\r\n      \t\t\tName:      \"\",\r\n    - \t\t\tScope:     &\"Cluster\",\r\n    + \t\t\tScope:     nil,\r\n      \t\t\tNamespace: nil,\r\n      \t\t},\r\n      \t},\r\n      }\r\n    \r\n    ```\r\n- [ ] networking.k8s.io/v1.NetworkPolicy\r\n    ```diff\r\n      &v1.NetworkPolicy{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tSpec: v1.NetworkPolicySpec{\r\n      \t\tPodSelector: {},\r\n      \t\tIngress:     nil,\r\n      \t\tEgress:      nil,\r\n    - \t\tPolicyTypes: []v1.PolicyType{\"Ingress\"},\r\n    + \t\tPolicyTypes: nil,\r\n      \t},\r\n      }\r\n    \r\n    ```\r\n- [x] networking.k8s.io/v1alpha1.ClusterCIDR\r\n- [x] networking.k8s.io/v1alpha1.IPAddress\r\n- [x] node.k8s.io/v1.RuntimeClass\r\n- [x] policy/v1.Eviction\r\n- [x] policy/v1.PodDisruptionBudget\r\n- [x] rbac.authorization.k8s.io/v1.ClusterRole\r\n- [ ] rbac.authorization.k8s.io/v1.ClusterRoleBinding\r\n    ```diff\r\n      &v1.ClusterRoleBinding{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tSubjects:   nil,\r\n      \tRoleRef: v1.RoleRef{\r\n    - \t\tAPIGroup: \"rbac.authorization.k8s.io\",\r\n    + \t\tAPIGroup: \"\",\r\n      \t\tKind:     \"\",\r\n      \t\tName:     \"\",\r\n      \t},\r\n      }\r\n    \r\n    ```\r\n- [x] rbac.authorization.k8s.io/v1.Role\r\n- [ ] rbac.authorization.k8s.io/v1.RoleBinding\r\n    ```diff\r\n      &v1.RoleBinding{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tSubjects:   nil,\r\n      \tRoleRef: v1.RoleRef{\r\n    - \t\tAPIGroup: \"rbac.authorization.k8s.io\",\r\n    + \t\tAPIGroup: \"\",\r\n      \t\tKind:     \"\",\r\n      \t\tName:     \"\",\r\n      \t},\r\n      }\r\n    \r\n    ```\r\n- [x] resource.k8s.io/v1alpha2.PodSchedulingContext\r\n- [ ] resource.k8s.io/v1alpha2.ResourceClaim\r\n    ```diff\r\n      &v1alpha2.ResourceClaim{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tSpec: v1alpha2.ResourceClaimSpec{\r\n      \t\tResourceClassName: \"\",\r\n      \t\tParametersRef:     nil,\r\n    - \t\tAllocationMode:    \"WaitForFirstConsumer\",\r\n    + \t\tAllocationMode:    \"\",\r\n      \t},\r\n      \tStatus: {},\r\n      }\r\n    \r\n    ```\r\n- [ ] resource.k8s.io/v1alpha2.ResourceClaimTemplate\r\n    ```diff\r\n      &v1alpha2.ResourceClaimTemplate{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tSpec: v1alpha2.ResourceClaimTemplateSpec{\r\n      \t\tObjectMeta: {},\r\n      \t\tSpec: v1alpha2.ResourceClaimSpec{\r\n      \t\t\tResourceClassName: \"\",\r\n      \t\t\tParametersRef:     nil,\r\n    - \t\t\tAllocationMode:    \"WaitForFirstConsumer\",\r\n    + \t\t\tAllocationMode:    \"\",\r\n      \t\t},\r\n      \t},\r\n      }\r\n    \r\n    ```\r\n- [x] resource.k8s.io/v1alpha2.ResourceClass\r\n- [ ] scheduling.k8s.io/v1.PriorityClass\r\n    ```diff\r\n      &v1.PriorityClass{\r\n      \t... // 3 identical fields\r\n      \tGlobalDefault:    false,\r\n      \tDescription:      \"\",\r\n    - \tPreemptionPolicy: &\"PreemptLowerPriority\",\r\n    + \tPreemptionPolicy: nil,\r\n      }\r\n    \r\n    ```\r\n- [ ] storage.k8s.io/v1.CSIDriver\r\n    ```diff\r\n      &v1.CSIDriver{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {},\r\n      \tSpec: v1.CSIDriverSpec{\r\n    - \t\tAttachRequired:       &true,\r\n    + \t\tAttachRequired:       nil,\r\n    - \t\tPodInfoOnMount:       &false,\r\n    + \t\tPodInfoOnMount:       nil,\r\n    - \t\tVolumeLifecycleModes: []v1.VolumeLifecycleMode{\"Persistent\"},\r\n    + \t\tVolumeLifecycleModes: nil,\r\n    - \t\tStorageCapacity:      &false,\r\n    + \t\tStorageCapacity:      nil,\r\n    - \t\tFSGroupPolicy:        &\"ReadWriteOnceWithFSType\",\r\n    + \t\tFSGroupPolicy:        nil,\r\n      \t\tTokenRequests:        nil,\r\n    - \t\tRequiresRepublish:    &false,\r\n    + \t\tRequiresRepublish:    nil,\r\n    - \t\tSELinuxMount:         &false,\r\n    + \t\tSELinuxMount:         nil,\r\n      \t},\r\n      }\r\n    \r\n    ```\r\n- [x] storage.k8s.io/v1.CSINode\r\n- [x] storage.k8s.io/v1.CSIStorageCapacity\r\n- [ ] storage.k8s.io/v1.StorageClass\r\n    ```diff\r\n      &v1.StorageClass{\r\n      \t... // 2 identical fields\r\n      \tProvisioner:          \"\",\r\n      \tParameters:           nil,\r\n    - \tReclaimPolicy:        &\"Delete\",\r\n    + \tReclaimPolicy:        nil,\r\n      \tMountOptions:         nil,\r\n      \tAllowVolumeExpansion: nil,\r\n    - \tVolumeBindingMode:    &\"Immediate\",\r\n    + \tVolumeBindingMode:    nil,\r\n      \tAllowedTopologies:    nil,\r\n      }\r\n    \r\n    ```\r\n- [ ] storage.k8s.io/v1.VolumeAttachment\r\n    ```diff\r\n      &v1.VolumeAttachment{\r\n      \tTypeMeta:   {},\r\n      \tObjectMeta: {OwnerReferences: {{}}, ManagedFields: {{}}},\r\n      \tSpec: v1.VolumeAttachmentSpec{\r\n      \t\tAttacher: \"\",\r\n      \t\tSource: v1.VolumeAttachmentSource{\r\n      \t\t\tPersistentVolumeName: nil,\r\n      \t\t\tInlineVolumeSpec: &v1.PersistentVolumeSpec{\r\n      \t\t\t\tCapacity: {s\"\": {s: \"0\", Format: \"DecimalSI\"}},\r\n      \t\t\t\tPersistentVolumeSource: v1.PersistentVolumeSource{\r\n      \t\t\t\t\tGCEPersistentDisk:    &{},\r\n      \t\t\t\t\tAWSElasticBlockStore: &{},\r\n      \t\t\t\t\tHostPath: &v1.HostPathVolumeSource{\r\n      \t\t\t\t\t\tPath: \"\",\r\n    - \t\t\t\t\t\tType: &\"\",\r\n    + \t\t\t\t\t\tType: nil,\r\n      \t\t\t\t\t},\r\n      \t\t\t\t\tGlusterfs: &{},\r\n      \t\t\t\t\tNFS:       &{},\r\n      \t\t\t\t\tRBD: &v1.RBDPersistentVolumeSource{\r\n      \t\t\t\t\t\tCephMonitors: nil,\r\n      \t\t\t\t\t\tRBDImage:     \"\",\r\n      \t\t\t\t\t\tFSType:       \"\",\r\n    - \t\t\t\t\t\tRBDPool:      \"rbd\",\r\n    + \t\t\t\t\t\tRBDPool:      \"\",\r\n    - \t\t\t\t\t\tRadosUser:    \"admin\",\r\n    + \t\t\t\t\t\tRadosUser:    \"\",\r\n    - \t\t\t\t\t\tKeyring:      \"/etc/ceph/keyring\",\r\n    + \t\t\t\t\t\tKeyring:      \"\",\r\n      \t\t\t\t\t\tSecretRef:    nil,\r\n      \t\t\t\t\t\tReadOnly:     false,\r\n      \t\t\t\t\t},\r\n      \t\t\t\t\tISCSI: &v1.ISCSIPersistentVolumeSource{\r\n      \t\t\t\t\t\tTargetPortal:   \"\",\r\n      \t\t\t\t\t\tIQN:            \"\",\r\n      \t\t\t\t\t\tLun:            0,\r\n    - \t\t\t\t\t\tISCSIInterface: \"default\",\r\n    + \t\t\t\t\t\tISCSIInterface: \"\",\r\n      \t\t\t\t\t\tFSType:         \"\",\r\n      \t\t\t\t\t\tReadOnly:       false,\r\n      \t\t\t\t\t\t... // 5 identical fields\r\n      \t\t\t\t\t},\r\n      \t\t\t\t\tCinder: &{},\r\n      \t\t\t\t\tCephFS: &{},\r\n      \t\t\t\t\t... // 4 identical fields\r\n      \t\t\t\t\tVsphereVolume: &{},\r\n      \t\t\t\t\tQuobyte:       &{},\r\n      \t\t\t\t\tAzureDisk: &v1.AzureDiskVolumeSource{\r\n      \t\t\t\t\t\tDiskName:    \"\",\r\n      \t\t\t\t\t\tDataDiskURI: \"\",\r\n    - \t\t\t\t\t\tCachingMode: &\"ReadWrite\",\r\n    + \t\t\t\t\t\tCachingMode: nil,\r\n    - \t\t\t\t\t\tFSType:      &\"ext4\",\r\n    + \t\t\t\t\t\tFSType:      nil,\r\n    - \t\t\t\t\t\tReadOnly:    &false,\r\n    + \t\t\t\t\t\tReadOnly:    nil,\r\n    - \t\t\t\t\t\tKind:        &\"Shared\",\r\n    + \t\t\t\t\t\tKind:        nil,\r\n      \t\t\t\t\t},\r\n      \t\t\t\t\tPhotonPersistentDisk: &{},\r\n      \t\t\t\t\tPortworxVolume:       &{},\r\n      \t\t\t\t\tScaleIO: &v1.ScaleIOPersistentVolumeSource{\r\n      \t\t\t\t\t\t... // 4 identical fields\r\n      \t\t\t\t\t\tProtectionDomain: \"\",\r\n      \t\t\t\t\t\tStoragePool:      \"\",\r\n    - \t\t\t\t\t\tStorageMode:      \"ThinProvisioned\",\r\n    + \t\t\t\t\t\tStorageMode:      \"\",\r\n      \t\t\t\t\t\tVolumeName:       \"\",\r\n    - \t\t\t\t\t\tFSType:           \"xfs\",\r\n    + \t\t\t\t\t\tFSType:           \"\",\r\n      \t\t\t\t\t\tReadOnly:         false,\r\n      \t\t\t\t\t},\r\n      \t\t\t\t\tLocal:     &{},\r\n      \t\t\t\t\tStorageOS: &{},\r\n      \t\t\t\t\tCSI:       &{},\r\n      \t\t\t\t},\r\n      \t\t\t\tAccessModes: nil,\r\n      \t\t\t\tClaimRef:    &{},\r\n      \t\t\t\t... // 5 identical fields\r\n      \t\t\t},\r\n      \t\t},\r\n      \t\tNodeName: \"\",\r\n      \t},\r\n      \tStatus: {AttachError: &{}, DetachError: &{}},\r\n      }\r\n    \r\n    ```\r\n\r\n\r\n\r\n### What did you expect to happen?\r\n\r\nThe diff between OpenAPI defaulting and native defaulting to be minimal/empty.\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\nLook at OpenAPI spec and look at corresponding `defaults.go`\r\n\r\n/sig api-machinery\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_\r\n\r\n### Kubernetes version\r\n\r\n<details>\r\n\r\n```console\r\n$ kubectl version\r\n# paste output here\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Cloud provider\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n",
      "solution": "Looks like most of these can be solved by instrumenting declarative defaulting for constants into defaulter-gen and openapi-gen\n\n---\n\n> Looks like most of these can be solved by instrumenting declarative defaulting for constants into defaulter-gen and openapi-gen\r\n\r\nYeah, most but not all. There are defaults that are going to be wrong, and I don't think there's much we can do? How bad is it?\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/bug",
        "sig/api-machinery",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2023-08-04T21:49:22Z",
      "closed_at": "2026-02-02T22:45:44Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/119766",
      "comments_count": 12
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 124646,
      "title": "Improve the jitter value for kubelet's serviceAccountToken projected volume source",
      "problem": "The current jittering logic in [kubelet token_manager](https://github.com/kubernetes/kubernetes/blob/dd68c5f2409fec7176e6172d6f9d97bd6447c4da/pkg/kubelet/token/token_manager.go#L42) refreshes the token once it has 20% of its lifetime remaining, with 10 seconds of jitter.\r\n\r\nExperimentally, this doesn't seem to be enough to really blunt spikes of refresh requests that come from many workloads having a synced refresh.  This can occur when:\r\n\r\n* A large deployment is scaled up.  All the new replicas will have synced-up refresh traffic.\r\n* A kubelet restarts.  Because kubelet only holds the tokens in an in-memory cache, it will need to acquire new tokens for all pods on the node after it restarts.  After that, all of the pods will have synced-up refresh traffic.\r\n\r\nAs an example, here's serviceaccount/token API traffic from a real cluster.  Note how there is a repeating spike every 48 minutes, which is what we would expect from the \"refresh after 80%\" logic, and the default setting of token lifetime for 1 hour.  Effective jittering logic would be noticeably smoothing these peaks.\r\n\r\nWe should increase the `maxJitter` value.  Perhaps 5 minutes would be a good value.\r\n\r\n![image](https://github.com/kubernetes/kubernetes/assets/1017202/133a6c5c-5a6b-4ad1-a1e8-ffb29fae3059)\r\n\r\n/sig auth\r\n/kind bug",
      "solution": "Need to change the jitter value to take in the token expiration into consideration. It can't be a fixed 5 mins because user can request a token with 10mins expiry. \r\n\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/bug",
        "sig/auth",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2024-04-30T21:52:32Z",
      "closed_at": "2026-02-02T18:44:45Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/124646",
      "comments_count": 8
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 136382,
      "title": "[Flaking Test] [sig-scheduling] k8s.io/kubernetes/test/integration/scheduler/plugins.plugins",
      "problem": "### Which jobs are flaking?\n\n* sig-release-master-blocking\n* integration-arm64-master\n\n### Which tests are flaking?\n\n* [k8s.io/kubernetes/test/integration/scheduler/plugins.plugins](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-integration-arm64-master/2012720398210174976)\n\n### Since when has it been flaking?\n\n* First flaky: Tue, 06 Jan 2026 10:01:24 UTC\n* Latest flaky: Wed, 21 Jan 2026 08:26:56 UTC\n\n### Testgrid link\n\n* [https://testgrid.k8s.io/sig-release-master-blocking#integration-arm64-master&exclude-non-failed-tests=](https://testgrid.k8s.io/sig-release-master-blocking#integration-arm64-master&exclude-non-failed-tests=)\n* [https://storage.googleapis.com/k8s-triage/index.html?job=ci-kubernetes-integration-arm64-master$&test=k8s.io/kubernetes/test/integration/scheduler/plugins.plugins](https://storage.googleapis.com/k8s-triage/index.html?job=ci-kubernetes-integration-arm64-master$&test=k8s.io/kubernetes/test/integration/scheduler/plugins.plugins)\n\n### Reason for failure (if possible)\n\n```\n\tF 2026-01-18 08:25:39 +0530 IST Failed\n=== RUN   TestUnReservePreBindPlugins/All_Reserve_plugins_passe...UnReservePreBindPlugins\n--- FAIL: TestUnReservePreBindPlugins (3.89s)\n\n\tF 2026-01-17 12:19:39 +0530 IST Failed\n=== RUN   TestUnReservePreBindPlugins/All_Reserve_plugins_passe...UnReservePreBindPlugins\n--- FAIL: TestUnReservePreBindPlugins (3.73s)\n\n\tF 2026-01-16 11:06:38 +0530 IST Failed\n=== RUN   TestUnReservePreBindPlugins/All_Reserve_plugins_passe...UnReservePreBindPlugins\n--- FAIL: TestUnReservePreBindPlugins (3.87s)\n\n\tF 2026-01-14 16:44:06 +0530 IST Failed\n=== RUN   TestUnReservePreBindPlugins/All_Reserve_plugins_passe...UnReservePreBindPlugins\n--- FAIL: TestUnReservePreBindPlugins (4.09s)\n\n\tF 2026-01-12 10:31:46 +0530 IST Failed\n=== RUN   TestUnReservePreBindPlugins/All_Reserve_plugins_passe...UnReservePreBindPlugins\n--- FAIL: TestUnReservePreBindPlugins (3.84s)\n\n\tF 2026-01-11 20:20:46 +0530 IST Failed\n=== RUN   TestUnReservePreBindPlugins/All_Reserve_plugins_passe...UnReservePreBindPlugins\n--- FAIL: TestUnReservePreBindPlugins (3.77s)\n\n\tF 2026-01-11 08:20:41 +0530 IST Failed\n=== RUN   TestUnReservePreBindPlugins/All_Reserve_plugins_passe...UnReservePreBindPlugins\n--- FAIL: TestUnReservePreBindPlugins (3.66s)\n\n\tF 2026-01-11 07:20:41 +0530 IST Failed\n=== RUN   TestUnReservePreBindPlugins/All_Reserve_plugins_passe...UnReservePreBindPlugins\n--- FAIL: TestUnReservePreBindPlugins (3.80s)\n\n\tF 2026-01-10 16:13:26 +0530 IST Failed\n=== RUN   TestUnReservePreBindPlugins/All_Reserve_plugins_passe...UnReservePreBindPlugins\n--- FAIL: TestUnReservePreBindPlugins (3.69s)\n\n\tF 2026-01-10 13:13:26 +0530 IST Failed\n=== RUN   TestUnReservePreBindPlugins/All_Reserve_plugins_passe...UnReservePreBindPlugins\n--- FAIL: TestUnReservePreBindPlugins (3.71s)\n\n\n```\n\n### Anything else we need to know?\n\n_No response_\n\n### Relevant SIG(s)\n\n/sig scheduling\n/kind flake\ncc @kubernetes/release-team-release-signal\n",
      "solution": "This was fixed by the https://github.com/kubernetes/kubernetes/pull/136303\n/close\n\n---\n\n@macsko: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/136382#issuecomment-3835146212):\n\n>This was fixed by the https://github.com/kubernetes/kubernetes/pull/136303\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>",
      "labels": [
        "sig/scheduling",
        "kind/flake",
        "needs-triage"
      ],
      "created_at": "2026-01-21T09:40:50Z",
      "closed_at": "2026-02-02T13:32:08Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/136382",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 135384,
      "title": "DATA RACE: test/integration/volumescheduling TestVolumeBindingRescheduling",
      "problem": "### Which jobs are flaking?\n\nhttps://testgrid.k8s.io/sig-testing-canaries#integration-race-master\n\n### Which tests are flaking?\n\nTestVolumeBindingRescheduling\n\n### Since when has it been flaking?\n\nunknown\n\n### Testgrid link\n\n_No response_\n\n### Reason for failure (if possible)\n\n```\nWARNING: DATA RACE\nRead at 0x00c005f55a78 by goroutine 24415:\n  sigs.k8s.io/structured-merge-diff/v6/schema.(*Map).CopyInto()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/schema/elements.go:178 +0x253\n  sigs.k8s.io/structured-merge-diff/v6/schema.(*Schema).Resolve()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/schema/elements.go:332 +0x527\n  sigs.k8s.io/structured-merge-diff/v6/typed.resolveSchema()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/helpers.go:106 +0xcb\n  sigs.k8s.io/structured-merge-diff/v6/typed.(*validatingObjectWalker).validate()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/validate.go:85 +0xf0\n  sigs.k8s.io/structured-merge-diff/v6/typed.(*validatingObjectWalker).doMap.(*validatingObjectWalker).visitMapItems.func1()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/validate.go:186 +0x30f\n  sigs.k8s.io/structured-merge-diff/v6/value.structReflect.IterateUsing.func1()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/value/structreflect.go:112 +0xb3\n  sigs.k8s.io/structured-merge-diff/v6/value.eachStructField()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/value/structreflect.go:123 +0x1f5\n  sigs.k8s.io/structured-merge-diff/v6/value.structReflect.IterateUsing()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/value/structreflect.go:111 +0x1a7\n  sigs.k8s.io/structured-merge-diff/v6/value.(*structReflect).IterateUsing()\n      <autogenerated>:1 +0xcb\n  sigs.k8s.io/structured-merge-diff/v6/typed.(*validatingObjectWalker).visitMapItems()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/validate.go:174 +0x4b5\n  sigs.k8s.io/structured-merge-diff/v6/typed.(*validatingObjectWalker).doMap()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/validate.go:202 +0x326\n  sigs.k8s.io/structured-merge-diff/v6/typed.handleAtom()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/helpers.go:145 +0x254\n  sigs.k8s.io/structured-merge-diff/v6/typed.resolveSchema()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/helpers.go:116 +0x27a\n  sigs.k8s.io/structured-merge-diff/v6/typed.(*validatingObjectWalker).validate()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/validate.go:85 +0xf0\n  sigs.k8s.io/structured-merge-diff/v6/typed.(*validatingObjectWalker).doMap.(*validatingObjectWalker).visitMapItems.func1()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/validate.go:186 +0x30f\n  sigs.k8s.io/structured-merge-diff/v6/value.structReflect.IterateUsing.func1()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/value/structreflect.go:112 +0xb3\n  sigs.k8s.io/structured-merge-diff/v6/value.eachStructField()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/value/structreflect.go:123 +0x1f5\n  sigs.k8s.io/structured-merge-diff/v6/value.structReflect.IterateUsing()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/value/structreflect.go:111 +0x1a7\n  sigs.k8s.io/structured-merge-diff/v6/value.(*structReflect).IterateUsing()\n      <autogenerated>:1 +0xcb\n  sigs.k8s.io/structured-merge-diff/v6/typed.(*validatingObjectWalker).visitMapItems()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/validate.go:174 +0x4b5\n  sigs.k8s.io/structured-merge-diff/v6/typed.(*validatingObjectWalker).doMap()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/validate.go:202 +0x326\n  sigs.k8s.io/structured-merge-diff/v6/typed.handleAtom()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/helpers.go:145 +0x254\n  sigs.k8s.io/structured-merge-diff/v6/typed.resolveSchema()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/helpers.go:116 +0x27a\n  sigs.k8s.io/structured-merge-diff/v6/typed.(*validatingObjectWalker).validate()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/validate.go:85 +0xf0\n  sigs.k8s.io/structured-merge-diff/v6/typed.TypedValue.Validate()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/typed.go:114 +0x1db\n  sigs.k8s.io/structured-merge-diff/v6/typed.AsTyped()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/typed.go:59 +0x204\n  sigs.k8s.io/structured-merge-diff/v6/typed.ParseableType.FromStructured()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/parser.go:125 +0x1b3\n  k8s.io/apimachinery/pkg/util/managedfields/internal.(*typeConverter).ObjectToTyped()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/managedfields/internal/typeconverter.go:67 +0x33d\n  k8s.io/apimachinery/pkg/util/managedfields/internal.(*structuredMergeManager).Update()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/managedfields/internal/structuredmerge.go:100 +0x465\n  k8s.io/apimachinery/pkg/util/managedfields/internal.(*stripMetaManager).Update()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/managedfields/internal/stripmeta.go:58 +0xcd\n  k8s.io/apimachinery/pkg/util/managedfields/internal.(*managedFieldsUpdater).Update()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/managedfields/internal/managedfieldsupdater.go:47 +0xd9\n  k8s.io/apimachinery/pkg/util/managedfields/internal.(*buildManagerInfoManager).Update()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/managedfields/internal/buildmanagerinfo.go:51 +0x1c7\n  k8s.io/apimachinery/pkg/util/managedfields/internal.(*capManagersManager).Update()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/managedfields/internal/capmanagers.go:48 +0xd5\n  k8s.io/apimachinery/pkg/util/managedfields/internal.(*skipNonAppliedManager).Update()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/managedfields/internal/skipnonapplied.go:72 +0x1bc\n  k8s.io/apimachinery/pkg/util/managedfields/internal.(*lastAppliedManager).Update()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/managedfields/internal/lastappliedmanager.go:53 +0xcd\n  k8s.io/apimachinery/pkg/util/managedfields/internal.(*lastAppliedUpdater).Update()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/managedfields/internal/lastappliedupdater.go:43 +0xcd\n  k8s.io/apimachinery/pkg/util/managedfields/internal.(*versionCheckManager).Update()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/managedfields/internal/versioncheck.go:43 +0xcd\n  k8s.io/apimachinery/pkg/util/managedfields/internal.(*FieldManager).Update()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/managedfields/internal/fieldmanager.go:131 +0x12e\n  k8s.io/apimachinery/pkg/util/managedfields/internal.(*FieldManager).UpdateNoErrors()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/managedfields/internal/fieldmanager.go:146 +0x98\n  k8s.io/apiserver/pkg/endpoints/handlers.UpdateResource.func1.1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/update.go:164 +0x14f\n  k8s.io/apiserver/pkg/registry/rest.(*defaultUpdatedObjectInfo).UpdatedObject()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/registry/rest/update.go:221 +0x129\n  k8s.io/apiserver/pkg/registry/generic/registry.(*Store).Update.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.go:653 +0x23a\n  k8s.io/apiserver/pkg/storage/etcd3.(*store).updateState()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/storage/etcd3/store.go:1077 +0xb6\n  k8s.io/apiserver/pkg/storage/etcd3.(*store).GuaranteedUpdate()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/storage/etcd3/store.go:518 +0xeca\n  k8s.io/apiserver/pkg/storage/cacher.(*CacheDelegator).GuaranteedUpdate()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/storage/cacher/delegator.go:258 +0x1ce\n  k8s.io/apiserver/pkg/registry/generic/registry.(*DryRunnableStorage).GuaranteedUpdate()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/registry/generic/registry/dryrun.go:107 +0x230\n  k8s.io/apiserver/pkg/registry/generic/registry.(*Store).Update()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.go:641 +0x851\n  k8s.io/apiserver/pkg/endpoints/handlers.UpdateResource.func1.4()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/update.go:208 +0x136a\n  k8s.io/apiserver/pkg/endpoints/handlers.UpdateResource.func1.4()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/update.go:208 +0x136a\n  k8s.io/apiserver/pkg/endpoints/handlers.UpdateResource.func1.5()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/update.go:228 +0x4e\n  k8s.io/apiserver/pkg/endpoints/handlers/finisher.finishRequest.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:117 +0xfa\n\nPrevious write at 0x00c005f55a78 by goroutine 24405:\n  sigs.k8s.io/structured-merge-diff/v6/schema.(*Map).FindField.func1()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/schema/elements.go:154 +0x73\n  sync.(*Once).doSlow()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/sync/once.go:78 +0xd1\n  sync.(*Once).Do()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/sync/once.go:69 +0x44\n  sigs.k8s.io/structured-merge-diff/v6/schema.(*Map).FindField()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/schema/elements.go:153 +0xb7\n  sigs.k8s.io/structured-merge-diff/v6/typed.(*validatingObjectWalker).doMap.(*validatingObjectWalker).visitMapItems.func1()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/validate.go:177 +0xc4\n  sigs.k8s.io/structured-merge-diff/v6/value.structReflect.IterateUsing.func1()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/value/structreflect.go:112 +0xb3\n  sigs.k8s.io/structured-merge-diff/v6/value.eachStructField()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/value/structreflect.go:123 +0x1f5\n  sigs.k8s.io/structured-merge-diff/v6/value.structReflect.IterateUsing()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/value/structreflect.go:111 +0x1a7\n  sigs.k8s.io/structured-merge-diff/v6/value.(*structReflect).IterateUsing()\n      <autogenerated>:1 +0xcb\n  sigs.k8s.io/structured-merge-diff/v6/typed.(*validatingObjectWalker).visitMapItems()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/validate.go:174 +0x4b5\n  sigs.k8s.io/structured-merge-diff/v6/typed.(*validatingObjectWalker).doMap()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/validate.go:202 +0x326\n  sigs.k8s.io/structured-merge-diff/v6/typed.handleAtom()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/helpers.go:145 +0x254\n  sigs.k8s.io/structured-merge-diff/v6/typed.resolveSchema()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/helpers.go:116 +0x27a\n  sigs.k8s.io/structured-merge-diff/v6/typed.(*validatingObjectWalker).validate()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/validate.go:85 +0xf0\n  sigs.k8s.io/structured-merge-diff/v6/typed.(*validatingObjectWalker).doMap.(*validatingObjectWalker).visitMapItems.func1()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/validate.go:186 +0x30f\n  sigs.k8s.io/structured-merge-diff/v6/value.structReflect.IterateUsing.func1()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/value/structreflect.go:112 +0xb3\n  sigs.k8s.io/structured-merge-diff/v6/value.eachStructField()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/value/structreflect.go:123 +0x1f5\n  sigs.k8s.io/structured-merge-diff/v6/value.structReflect.IterateUsing()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/value/structreflect.go:111 +0x1a7\n  sigs.k8s.io/structured-merge-diff/v6/value.(*structReflect).IterateUsing()\n      <autogenerated>:1 +0xcb\n  sigs.k8s.io/structured-merge-diff/v6/typed.(*validatingObjectWalker).visitMapItems()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/validate.go:174 +0x4b5\n  sigs.k8s.io/structured-merge-diff/v6/typed.(*validatingObjectWalker).doMap()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/validate.go:202 +0x326\n  sigs.k8s.io/structured-merge-diff/v6/typed.handleAtom()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/helpers.go:145 +0x254\n  sigs.k8s.io/structured-merge-diff/v6/typed.resolveSchema()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/helpers.go:116 +0x27a\n  sigs.k8s.io/structured-merge-diff/v6/typed.(*validatingObjectWalker).validate()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/validate.go:85 +0xf0\n  sigs.k8s.io/structured-merge-diff/v6/typed.TypedValue.Validate()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/typed.go:114 +0x1db\n  sigs.k8s.io/structured-merge-diff/v6/typed.AsTyped()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/typed.go:59 +0x204\n  sigs.k8s.io/structured-merge-diff/v6/typed.ParseableType.FromStructured()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/sigs.k8s.io/structured-merge-diff/v6/typed/parser.go:125 +0x1b3\n  k8s.io/apimachinery/pkg/util/managedfields/internal.(*typeConverter).ObjectToTyped()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/managedfields/internal/typeconverter.go:67 +0x33d\n  k8s.io/apimachinery/pkg/util/managedfields/internal.(*structuredMergeManager).Update()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/managedfields/internal/structuredmerge.go:100 +0x465\n  k8s.io/apimachinery/pkg/util/managedfields/internal.(*stripMetaManager).Update()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/managedfields/internal/stripmeta.go:58 +0xcd\n  k8s.io/apimachinery/pkg/util/managedfields/internal.(*managedFieldsUpdater).Update()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/managedfields/internal/managedfieldsupdater.go:47 +0xd9\n  k8s.io/apimachinery/pkg/util/managedfields/internal.(*buildManagerInfoManager).Update()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/managedfields/internal/buildmanagerinfo.go:51 +0x1c7\n  k8s.io/apimachinery/pkg/util/managedfields/internal.(*capManagersManager).Update()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/managedfields/internal/capmanagers.go:48 +0xd5\n  k8s.io/apimachinery/pkg/util/managedfields/internal.(*skipNonAppliedManager).Update()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/managedfields/internal/skipnonapplied.go:72 +0x1bc\n  k8s.io/apimachinery/pkg/util/managedfields/internal.(*lastAppliedManager).Update()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/managedfields/internal/lastappliedmanager.go:53 +0xcd\n  k8s.io/apimachinery/pkg/util/managedfields/internal.(*lastAppliedUpdater).Update()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/managedfields/internal/lastappliedupdater.go:43 +0xcd\n  k8s.io/apimachinery/pkg/util/managedfields/internal.(*versionCheckManager).Update()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/managedfields/internal/versioncheck.go:43 +0xcd\n  k8s.io/apimachinery/pkg/util/managedfields/internal.(*FieldManager).Update()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/managedfields/internal/fieldmanager.go:131 +0x12e\n  k8s.io/apimachinery/pkg/util/managedfields/internal.(*FieldManager).UpdateNoErrors()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/managedfields/internal/fieldmanager.go:146 +0x98\n  k8s.io/apiserver/pkg/endpoints/handlers.CreateResource.createHandler.func1.2()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/create.go:199 +0x251\n  k8s.io/apiserver/pkg/endpoints/handlers/finisher.finishRequest.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:117 +0xfa\n\nGoroutine 24415 (running) created at:\n  k8s.io/apiserver/pkg/endpoints/handlers/finisher.finishRequest()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:92 +0x13c\n  k8s.io/apiserver/pkg/endpoints/handlers/finisher.FinishRequest()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:84 +0x36cb\n  k8s.io/apiserver/pkg/endpoints/handlers.UpdateResource.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/update.go:227 +0x3598\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).registerResourceHandlers.restfulUpdateResource.func9()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:1326 +0xd1\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).registerResourceHandlers.InstrumentRouteFunc.func10()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/metrics/metrics.go:645 +0x2bc\n  github.com/emicklei/go-restful/v3.(*Container).dispatch()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/github.com/emicklei/go-restful/v3/container.go:299 +0xfb5\n  github.com/emicklei/go-restful/v3.(*Container).Dispatch()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/github.com/emicklei/go-restful/v3/container.go:204 +0x827\n  k8s.io/apiserver/pkg/server.director.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/handler.go:145 +0x7f9\n  k8s.io/apiserver/pkg/server.(*director).ServeHTTP()\n      <autogenerated>:1 +0x7b\n  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func22.deferwrap1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:102 +0x6f\n  runtime.deferreturn()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/runtime/panic.go:589 +0x5d\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/endpoints/filters.withAuthorization.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/authorization.go:84 +0x859\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:93 +0x5fa\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func23.deferwrap1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:102 +0x6f\n  runtime.deferreturn()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/runtime/panic.go:589 +0x5d\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle.func10()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:298 +0x134\n  k8s.io/apiserver/pkg/util/flowcontrol.(*configController).Handle.func2()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/apf_filter.go:192 +0x3d2\n  k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset.(*request).Finish.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset/queueset.go:391 +0x96\n  k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset.(*request).Finish()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset/queueset.go:392 +0x4a\n  k8s.io/apiserver/pkg/util/flowcontrol.(*configController).Handle()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/apf_filter.go:179 +0xd61\n  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle.func11()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:304 +0x144\n  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:305 +0x12d3\n  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle-fm()\n      <autogenerated>:1 +0x51\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:93 +0x5fa\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func24.deferwrap1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:102 +0x6f\n  runtime.deferreturn()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/runtime/panic.go:589 +0x5d\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithImpersonation.func4()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/impersonation/impersonation.go:51 +0x214\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:93 +0x5fa\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func25.deferwrap1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:102 +0x6f\n  runtime.deferreturn()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/runtime/panic.go:589 +0x5d\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:93 +0x5fa\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/endpoints/filters.WithTracing.func2()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/traces.go:62 +0x766\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*middleware).serveHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/handler.go:180 +0x1ac1\n  go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.NewMiddleware.func1.1()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/handler.go:67 +0x67\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func27.deferwrap1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:102 +0x6f\n  runtime.deferreturn()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/runtime/panic.go:589 +0x5d\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/endpoints/filters.withAuthentication.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/authentication.go:123 +0xd7c\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:93 +0x5fa\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithWarningRecorder.func11()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/warning.go:35 +0x124\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/timeout.go:115 +0xce\n\nGoroutine 24405 (running) created at:\n  k8s.io/apiserver/pkg/endpoints/handlers/finisher.finishRequest()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:92 +0x13c\n  k8s.io/apiserver/pkg/endpoints/handlers/finisher.FinishRequest()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:84 +0x4b\n  k8s.io/apiserver/pkg/endpoints/handlers.CreateResource.createHandler.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/create.go:194 +0x20a4\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).registerResourceHandlers.restfulCreateResource.func16()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:1308 +0xd1\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).registerResourceHandlers.InstrumentRouteFunc.func17()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/metrics/metrics.go:645 +0x2bc\n  github.com/emicklei/go-restful/v3.(*Container).dispatch()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/github.com/emicklei/go-restful/v3/container.go:299 +0xfb5\n  github.com/emicklei/go-restful/v3.(*Container).Dispatch()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/github.com/emicklei/go-restful/v3/container.go:204 +0x827\n  k8s.io/apiserver/pkg/server.director.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/handler.go:145 +0x7f9\n  k8s.io/apiserver/pkg/server.(*director).ServeHTTP()\n      <autogenerated>:1 +0x7b\n  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func22.deferwrap1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:102 +0x6f\n  runtime.deferreturn()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/runtime/panic.go:589 +0x5d\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/endpoints/filters.withAuthorization.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/authorization.go:84 +0x859\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:93 +0x5fa\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func23.deferwrap1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:102 +0x6f\n  runtime.deferreturn()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/runtime/panic.go:589 +0x5d\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle.func10()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:298 +0x134\n  k8s.io/apiserver/pkg/util/flowcontrol.(*configController).Handle.func2()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/apf_filter.go:192 +0x3d2\n  k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset.(*request).Finish.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset/queueset.go:391 +0x96\n  k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset.(*request).Finish()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset/queueset.go:392 +0x4a\n  k8s.io/apiserver/pkg/util/flowcontrol.(*configController).Handle()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/apf_filter.go:179 +0xd61\n  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle.func11()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:304 +0x144\n  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:305 +0x12d3\n  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle-fm()\n      <autogenerated>:1 +0x51\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:93 +0x5fa\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func24.deferwrap1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:102 +0x6f\n  runtime.deferreturn()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/runtime/panic.go:589 +0x5d\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithImpersonation.func4()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/impersonation/impersonation.go:51 +0x214\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:93 +0x5fa\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func25.deferwrap1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:102 +0x6f\n  runtime.deferreturn()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/runtime/panic.go:589 +0x5d\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:93 +0x5fa\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/endpoints/filters.WithTracing.func2()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/traces.go:62 +0x766\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*middleware).serveHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/handler.go:180 +0x1ac1\n  go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.NewMiddleware.func1.1()\n      /home/prow/go/src/k8s.io/kubernetes/vendor/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/handler.go:67 +0x67\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func27.deferwrap1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:102 +0x6f\n  runtime.deferreturn()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/runtime/panic.go:589 +0x5d\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/endpoints/filters.withAuthentication.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/authentication.go:123 +0xd7c\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:93 +0x5fa\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithWarningRecorder.func11()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/warning.go:35 +0x124\n  net/http.HandlerFunc.ServeHTTP()\n      /home/prow/go/src/k8s.io/kubernetes/_output/local/go/cache/mod/golang.org/toolchain@v0.0.1-go1.25.4.linux-amd64/src/net/http/server.go:2322 +0x47\n  k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/timeout.go:115 +0xce\n==================\n```\n\nThere are some more, probably with the same root cause.\n\n### Anything else we need to know?\n\n_No response_\n\n### Relevant SIG(s)\n\n/sig api-machinery\n\nWhile this is a storage test, the data race occurs in the apiserver side, more specifically structured-merge-diff.\n\n/cc @liggitt @jpbetz \n",
      "solution": "This bug was fixed in https://github.com/kubernetes-sigs/structured-merge-diff/commit/40cac0c9a457d3fa2048901fa291d474cd8aed80 = https://github.com/kubernetes-sigs/structured-merge-diff/pull/307 which still needs to be vendored in Kubernetes.\n\n\n\n\n\n---\n\n@pohly: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/135384#issuecomment-3833696615):\n\n>I think we fixed this.\n>\n>/close\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>",
      "labels": [
        "sig/api-machinery",
        "kind/flake",
        "triage/accepted"
      ],
      "created_at": "2025-11-21T08:28:53Z",
      "closed_at": "2026-02-02T08:28:57Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/135384",
      "comments_count": 9
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 133812,
      "title": "Version 1.34 changelog added an incorrect PR link",
      "problem": "### What happened?\n\nWhile analyzing the 1.34 changelog, I discovered that the following pull request doesn't address the issue described.\n\n```\nEliminated work when creating services or understanding ports, especially for external resources deployed via Helm charts. (#133018, @rushmash91)\n```\n\n### What did you expect to happen?\n\nPlease update to the correct pr link\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nVisit the following link:\nhttps://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.34.md#dependencies\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# paste output here\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/bug",
        "sig/docs",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2025-09-01T06:58:42Z",
      "closed_at": "2026-01-30T18:10:43Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/133812",
      "comments_count": 10
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 132528,
      "title": "Improve error messages for CEL validation",
      "problem": "### What would you like to be added?\n\nWhen I use CEL validation as of today I get messages that not easy to parse for users, e.g.\n\n```\n* spec.name: Invalid value: \"string\": name must consist of lower case alphanumeric characters\n* spec.pods: Invalid value: \"array\": entries in pods must be unique\n```\n\nMore specifically, looks \u201cstring\u201d or \"array\" is not reporting the actual value of the field but the field type.\n\nThis can be reproduced by creating a kind cluster (I used K8s v1.32.0), applying the following CRD:\n\n```\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: v0.18.0\n  name: testresources.test.cluster.x-k8s.io\nspec:\n  group: test.cluster.x-k8s.io\n  names:\n    kind: TestResource\n    listKind: TestResourceList\n    plural: testresources\n    singular: testresource\n  scope: Namespaced\n  versions:\n  - name: v1beta1\n    schema:\n      openAPIV3Schema:\n        description: TestResource defines a test resource.\n        properties:\n          apiVersion:\n            description: |-\n              APIVersion defines the versioned schema of this representation of an object.\n              Servers should convert recognized schemas to the latest internal value, and\n              may reject unrecognized values.\n              More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n            type: string\n          kind:\n            description: |-\n              Kind is a string value representing the REST resource this object represents.\n              Servers may infer this from the endpoint the client submits requests to.\n              Cannot be updated.\n              In CamelCase.\n              More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: TestResourceSpec defines the resource spec.\n            properties:\n              name:\n                maxLength: 253\n                minLength: 1\n                type: string\n                x-kubernetes-validations:\n                - message: name must consist of lower case alphanumeric characters\n                  rule: self.matches('^[a-z0-9]*$')\n              pods:\n                items:\n                  maxLength: 256\n                  minLength: 1\n                  type: string\n                maxItems: 32\n                minItems: 1\n                type: array\n                x-kubernetes-list-type: atomic\n                x-kubernetes-validations:\n                - message: entries in pods must be unique\n                  rule: self.all(x, self.exists_one(y, x == y))\n            required:\n            - name\n            type: object\n          status:\n            description: TestResourceStatus defines the status of a TestResource.\n            properties:\n              availableReplicas:\n                format: int32\n                type: integer\n              readyReplicas:\n                format: int32\n                type: integer\n              replicas:\n                format: int32\n                type: integer\n              upToDateReplicas:\n                format: int32\n                type: integer\n              version:\n                maxLength: 256\n                minLength: 1\n                type: string\n            type: object\n        type: object\n    served: true\n    storage: true\n    subresources:\n      status: {}\n```\n\nAnd then the following yaml to create an invalid resource\n\n```yml\napiVersion: test.cluster.x-k8s.io/v1beta1\nkind: TestResource\nmetadata:\n  name: invalid\nspec:\n  name: $__\n  pods:\n    - foo\n    - foo\n```\n\n\nFYI there was some discussion in https://kubernetes.slack.com/archives/C02TTBG6LF4/p1750091061651119 about this issue, capturing here some relevant points:\n\n<details>\n@sbueringer \nI would probably expect from a user pov that no message prefix is added if message or messageExpression is set\n\n@erikgb \nFrom a developer POV I think it is nice to get some context automatically injected into the error message. :wink: But I would prefer the invalid value to be echoed, and not the type. So I agree there is a bug somewhere.\n\n@JoelSpeed \nWhen you add an XValidation, you can specify the type of the error, whether it's Invalid, Forbidden etc, that is where this context is coming from, I wonder if this is specifically an issue with Invalid, or whether the same also applies to Forbidden or other error types\n\n@liggitt \nmaybe something like this would work\n```diff\ndiff --git a/staging/src/k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel/validation.go b/staging/src/k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel/validation.go\nindex 575fd5e2e9a..2a3f7ca29df 100644\n--- a/staging/src/k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel/validation.go\n+++ b/staging/src/k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel/validation.go\n@@ -476,15 +476,16 @@ func (s *Validator) validateExpressions(ctx context.Context, fldPath *field.Path\n \t\t\t\t\t\treturn errs, -1\n \t\t\t\t\t} else {\n \t\t\t\t\t\tklog.V(2).ErrorS(msgErr, \"messageExpression evaluation failed\")\n-\t\t\t\t\t\taddErr(fieldErrorForReason(currentFldPath, sts.Type, ruleMessageOrDefault(rule), rule.Reason))\n+\t\t\t\t\t\taddErr(fieldErrorForReason(currentFldPath, obj, ruleMessageOrDefault(rule), rule.Reason))\n \t\t\t\t\t\tremainingBudget = newRemainingBudget\n \t\t\t\t\t}\n \t\t\t\t} else {\n-\t\t\t\t\taddErr(fieldErrorForReason(currentFldPath, sts.Type, messageExpression, rule.Reason))\n+\t\t\t\t\t// messageExpression is expected to embed the value if desired\n+\t\t\t\t\taddErr(fieldErrorForReason(currentFldPath, field.OmitValueType{}, messageExpression, rule.Reason))\n \t\t\t\t\tremainingBudget = newRemainingBudget\n \t\t\t\t}\n \t\t\t} else {\n-\t\t\t\taddErr(fieldErrorForReason(currentFldPath, sts.Type, ruleMessageOrDefault(rule), rule.Reason))\n+\t\t\t\taddErr(fieldErrorForReason(currentFldPath, obj, ruleMessageOrDefault(rule), rule.Reason))\n \t\t\t}\n \t\t}\n \t}\n@@ -675,7 +676,13 @@ func fieldErrorForReason(fldPath *field.Path, value interface{}, detail string,\n \tcase apiextensions.FieldValueDuplicate:\n \t\treturn field.Duplicate(fldPath, value)\n \tdefault:\n-\t\treturn field.Invalid(fldPath, value, detail)\n+\t\tdisplayValue := value\n+\t\tswitch value.(type) {\n+\t\tcase map[string]any, []any:\n+\t\t\t// avoid outputting complex structured field values\n+\t\t\tdisplayValue = field.OmitValueType{}\n+\t\t}\n+\t\treturn field.Invalid(fldPath, displayValue, detail)\n \t}\n }\n```\nneeds good tests and such\n</details>\n/sig api-machinery\n/cc @liggitt \n\n### Why is this needed?\n\nImprove UX when CRD implements validation rules using CEL",
      "solution": "I looked into this code a few months ago and had most of a fix ready. One solution is in https://github.com/kubernetes/kubernetes/pull/132798.\n\n---\n\n@cbandy Thank you! #132798 fixed part of this issue (simple values), but not the entire issue, right?\n\n> This improves the messages returned to the user when CEL validation rules fail. Simple values (numbers, booleans, and strings) are included in the message rather than the field type.\n\n---\n\nIncorrectly outputting the type for cel validation is fixed. \"possibly omit the value when outputting errors from custom messageExpressions\" is not yet done but needs agreement before doing that",
      "labels": [
        "sig/api-machinery",
        "kind/feature",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2025-06-25T11:22:38Z",
      "closed_at": "2026-01-30T16:08:43Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/132528",
      "comments_count": 16
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 110172,
      "title": "Better HPA unit and E2E test coverage",
      "problem": "### What would you like to be added?\n\nThe HPA needs better unit and E2E test coverage.\r\n\r\n**E2E TESTS**\r\n\r\nMissing E2E test coverage by API / feature\r\n    \r\nSpec.ScaleTargetRef\r\n- [x] scaling a CRD (not built-in kind)\r\n\r\nSpec.MinReplicas\r\n- [x] scale-to-zero (minReplicas=0)\r\n\r\nSpec.Metrics\r\n- [ ] scale on cpu average value\r\n- [ ] scale on memory average value\r\n- [ ] scale on memory utilization\r\n- [ ] scale on multiple metric types (resource, container resource, pods, object, external)\r\n  - [ ] scale up when one metric missing\r\n  - [ ] no scale down when one metric missing\r\n\r\nSpec.Behavior\r\n- [x] scale up stabilization window\r\n- [x] scale and and down controls (see #97348)\r\n- [x] scaling disabled up / down (select policy disabled)\r\n- [x] scale up / down rate / pods\r\n\r\n**UNIT TESTS**\r\n\r\nMissing unit test coverage by branch / case.\r\n\r\n**horizontal.go**\r\n\r\ndeleteHPA()\r\n- [ ] deleted hpa\r\n\r\ncomputeReplicasForMetric()\r\n- [ ] failed to get object metric value\r\n- [ ] failed to get pods metriv value\r\n- [ ] err on get container resource metric status\r\n\r\nreconcileKey()\r\n- [ ] hpa not found (deleted)\r\n\r\ncomputeStatusForObjectMetric()\r\n- [ ] error getting average value object metric\r\n\r\ncomputeStatusForResourceMetricGeneric()\r\n- [ ] average value\r\n- [ ] neither utilization or average value\r\n\r\ncomputeStatusForContainerResourceMetric()\r\n- [ ] FailedGetContainerResourceMetric\r\n\r\nFailedGetContainerResourceMetric()   \r\n- [ ] FailedGetExternalMetric\r\n\r\nreconcileAutoscaler()\r\n- [ ] default min replica = 1      \r\n- [ ] error updating status after error getting recommendation\r\n- [ ] error updating status after error updates scale subresource  \r\n\r\nnormalizeDesiredReplicasWithBehaviors()\r\n- [ ] stabilizedRecommendation != prenormalizedDesiredReplicas scale up/down stabilized\r\n\r\ncalculateScaleUpLimitWithScalingRules()\r\n- [ ] scaling disabled      \r\n\r\ncalculateScaleDownLimitWithBehaviors()\r\n- [ ] scaling disabled\r\n\r\nscaleForResourceMappings()\r\n- [ ] unrecognized resource\r\n\r\nupdateStatus()\r\n- [ ] error updating status      \r\n  \r\n**metrics/client.go**\r\n\r\nGetResourceMetric()\r\n- [ ] error fetching metrics\r\n\r\ngetPodMetrics()\r\n- [ ] missing resource metrics\r\n\r\nGetRawMetric()\r\n- [ ] unable to fetch metrics\r\n- [ ] metric window seconds != nil\r\n\r\nGetObjectMetric()\r\n- [ ] kind=namespace object metric (root scoped)\r\n- [ ] error getting metrics\r\n\r\nGetExternalMetric()\r\n- [ ] error getting metrics      \r\n\r\n**replica_calculator.go**\r\n\r\nGetResourceReplicas()\r\n- [ ] error listing pods\r\n- [ ] empty list of metrics\r\n- [ ] second run of get resource utilization ratio\r\n\r\nGetRawResourceReplicas()\r\n- [ ] whole function uncovered\r\n\r\ncalcPlainMetricReplicas()\r\n- [ ] error list pods\r\n- [ ] empty list list of pods\r\n- [ ] empty list of metrics\r\n- [ ] missing metrics and wanting to scale up (treat missing as 0%)\r\n- [ ] rebalance ignored (treat unready as 0%)\r\n- [ ] no scale because within tolerance\r\n- [ ] normal return of new replicas (last line)\r\n\r\ngetUsageRatioReplicaCount()\r\n- [ ] error calling get ready pods count\r\n\r\nGetObjectPerPodMetricReplicas()\r\n- [ ] error calling get object metric\r\n\r\ngetReadyPodsCount()\r\n- [ ] error calling list pods\r\n- [ ] pod list empty\r\n\r\nGetExternalMetricReplicas()\r\n- [ ] error parsing metric label selector\r\n\r\nGetExternalPerPodMetricReplicas()\r\n- [ ] error parsing metric label selector\r\n- [ ] error calling get external metric\n\n### Why is this needed?\n\nThe more coverage to less likely we are to break something when refactoring / adding features / fixing bugs.",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "sig/autoscaling",
        "kind/feature",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2022-05-23T13:49:29Z",
      "closed_at": "2026-01-30T16:08:42Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/110172",
      "comments_count": 22
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 104171,
      "title": "List watch request for Events with stale resourceVersion responds immediately without error",
      "problem": "<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\r\n\r\nIf the matter is security related, please disclose it privately via https://kubernetes.io/security/\r\n-->\r\n\r\n\r\n#### What happened:\r\nPer the [documentation on using watches](https://kubernetes.io/docs/reference/using-api/api-concepts/#efficient-detection-of-changes) I attempted to resume a timed out watch for `events.k8s.io/v1` `Event` objects by issuing a new List request with the `watch` parameter set and the `resourceVersion` set to the last `Event` object I'd received on the watch.\r\n\r\nHowever, by the time I issued the new request that `resourceVersion` was stale.  The response I received from the K8S API was an immediate HTTP 200 with an empty request body.  (This confused the K8S library I was using which was expecting a 410 Gone error in that case, and resulted in a busy-loop of retrying over and over which put strain on the K8S API server.)\r\n\r\n#### What you expected to happen:\r\nI expected the watch request to receive a response with some form of `410 Gone` error (either in the HTTP status code or as some form of JSON error object in the response body).  This is the documented behavior, and I was able to verify that this is what happens for other resource types like Secrets.\r\n\r\n#### How to reproduce it (as minimally and precisely as possible):\r\n\r\n##### How to reproduce the failure for Events\r\nEnsure there are recorded events in the Kubernetes cluster, e.g., `kubectl -n YOUR-NS-HERE get events.events.k8s.io` yields a non-empty output.\r\n\r\nThen request a watch for events with a known stale `resourceVersion`.\r\n```\r\n$ curl -i -kE k3s.client.pem 'https://localhost:6443/apis/events.k8s.io/v1/namespaces/YOUR-NS-HERE/events?&watch=1&resourceVersion=1'      \r\nHTTP/1.1 200 OK\r\nCache-Control: no-cache, private\r\nContent-Type: application/json\r\nDate: Thu, 05 Aug 2021 15:58:57 GMT\r\nTransfer-Encoding: chunked\r\n\r\n```\r\n\r\nNote that even though the `Transfer-Encoding: chunked` header is given, the K8S API server closes the connection immediately after serving an empty response.\r\n\r\n##### How to reproduce correct behavior for an alternate Resource type (e.g., a Secret)\r\n```\r\n$ curl -i -kE k3s.client.pem 'https://localhost:6443/api/v1/namespaces/YOUR-NS-HERE/secrets?&watch=true&resourceVersion=1'                                                                                                          HTTP/1.1 200 OK\r\nCache-Control: no-cache, private\r\nContent-Type: application/json\r\nDate: Thu, 05 Aug 2021 15:58:40 GMT\r\nTransfer-Encoding: chunked\r\n\r\n{\"type\":\"ERROR\",\"object\":{\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"too old resource version: 1 (1126176)\",\"reason\":\"Expired\",\"code\":410}}\r\n```\r\n\r\n#### Anything else we need to know?:\r\n\r\n#### Environment:\r\n- Kubernetes version (use `kubectl version`):\r\n```\r\nClient Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.3\", GitCommit:\"ca643a4d1f7bfe34773c74f79527be4afd95bf39\", GitTreeState:\"archive\", BuildDate:\"2021-07-16T17:16:46Z\", GoVersion:\"go1.16.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\nServer Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.2+k3s1\", GitCommit:\"5a67e8dc473f8945e8e181f6f0b0dbbc387f6fca\", GitTreeState:\"clean\", BuildDate:\"2021-06-21T20:52:44Z\", GoVersion:\"go1.16.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n```\r\nI have also tested and verified this bug exists in Kubernetes 1.19.12.\r\n- Cloud provider or hardware configuration: MacBook Pro running Kubernetes locally on Linux\r\n- OS (e.g: `cat /etc/os-release`):\r\n```\r\nNAME=\"Arch Linux\"\r\nPRETTY_NAME=\"Arch Linux\"\r\nID=arch\r\nBUILD_ID=rolling\r\nANSI_COLOR=\"38;2;23;147;209\"\r\nHOME_URL=\"https://archlinux.org/\"\r\nDOCUMENTATION_URL=\"https://wiki.archlinux.org/\"\r\nSUPPORT_URL=\"https://bbs.archlinux.org/\"\r\nBUG_REPORT_URL=\"https://bugs.archlinux.org/\"\r\nLOGO=archlinux\r\n```\r\n- Kernel (e.g. `uname -a`): `Linux redacted 5.10.55-1-lts #1 SMP Sat, 31 Jul 2021 08:12:13 +0000 x86_64 GNU/Linux`\r\n- Install tools: [Arch Linux AUR package for K3S 1.21](https://aur.archlinux.org/packages/k3s-1.21-bin/)\r\n- Network plugin and version (if this is a network-related bug):\r\n- Others:\r\n",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/bug",
        "sig/api-machinery",
        "needs-triage"
      ],
      "created_at": "2021-08-05T16:19:44Z",
      "closed_at": "2025-03-31T14:40:58Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/104171",
      "comments_count": 27
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 133761,
      "title": "Inconsistent fieldPath in CRD validation rules",
      "problem": "### What happened?\n\nWhile validating the contents of an additionalProperties object (a map), I noticed that a rule's fieldPath renders differently depending on where the rule is placed.\n\nRule `size(self.pairs.bar) > 1` is reported as `spec.pairs[bar]`, while\nrule `size(self.bar) > 1` is reported as `spec.pairs.[bar]`. Notice the dot between `pairs` and `bar`.\n\n### What did you expect to happen?\n\nI expected a rule's fieldPath to render consistently at any depth of the CRD schema.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nThis CRD has two rules, one on `spec` and one on `spec.pairs`. Both have a fieldPath that intends to reference `spec.pairs[bar]`.\n\n```yaml\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: foos.example.com\nspec:\n  group: example.com\n  names:\n    kind: Foo\n    listKind: FooList\n    singular: foo\n    plural: foos\n  scope: Namespaced\n  versions:\n  - name: v1\n    served: true\n    storage: true\n    schema:\n      openAPIV3Schema:\n        type: object\n        properties:\n          apiVersion:\n            type: string\n          kind:\n            type: string\n          metadata:\n            type: object\n          spec:\n            type: object\n            x-kubernetes-validations:\n            - rule: 'size(self.pairs.bar) > 5'\n              fieldPath: .pairs.bar\n            properties:\n              pairs:\n                type: object\n                additionalProperties: { type: string }\n                maxProperties: 10\n                x-kubernetes-map-type: granular\n                x-kubernetes-validations:\n                - rule: 'size(self.bar) > 1'\n                  fieldPath: .bar\n```\n\nThis CR fails those validation rules causing `kubectl apply --server-side` to return the following:\n\n```\nThe Foo \"example\" is invalid: \n* spec.pairs[bar]: Invalid value: \"object\": failed rule: size(self.pairs.bar) > 5\n* spec.pairs.[bar]: Invalid value: \"object\": failed rule: size(self.bar) > 1\n```\n\n```yaml\n---\napiVersion: example.com/v1\nkind: Foo\nmetadata:\n  name: example\nspec:\n  pairs:\n    bar: x\n```\n\n### Anything else we need to know?\n\nIt is also possible to spell the fieldPaths with brackets, but the result is the same:\n\n```yaml\n- rule: 'size(self.pairs.bar) > 5'\n  fieldPath: .pairs['bar']\n\n- rule: 'size(self.bar) > 1'\n  fieldPath: \"['bar']\"\n```\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nClient Version: v1.33.1\nKustomize Version: v5.6.0\nServer Version: v1.33.1\n```\n\n</details>\n\n\n### Cloud provider\n\nn/a\n\n\n### OS version\n\nn/a\n\n\n### Install tools\n\n<details>\nminikube\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\nn/a\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\nn/a",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/bug",
        "sig/api-machinery",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2025-08-28T19:28:44Z",
      "closed_at": "2026-01-30T11:04:43Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/133761",
      "comments_count": 7
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 136027,
      "title": "Error-level logs incorrectly use verbosity levels",
      "problem": "Currently, some components in Kubernetes have been migrated (or partially migrated) to [contextual logging](https://github.com/kubernetes/enhancements/blob/master/keps/sig-instrumentation/3077-contextual-logging/README.md). However, an incorrect pattern was introduced during the migration: Error-level logs are being called with verbosity methods (e.g., .V()).\n\nThe underlying package go-logr/logr, which contextual logging relies on, ignores the verbosity level when printing Error-level logs. For example, in the statement `logger.V(8).Error(nil, \"Readiness probe already exists for container\", \"pod\", klog.KObj(pod), \"containerName\", c.Name)`, the error log will be printed regardless of the verbosity level set.\n\nThis can lead to error log flooding, which may hinder users' ability to review normal logs for troubleshooting. More critically, according to the [Kubernetes logging policy](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/logging.md#what-method-to-use), logs should only be recorded when the information requires action from an administrator. This directly contradicts the practice of setting verbosity levels for Error logs, as administrators using the default logging level might miss these actionable errors. Therefore, these instances need to be fixed.\n\nUsing grep, I identified the following instances in the k/k repository where verbosity levels are incorrectly set for Error logs.\n- [ ] /pkg/controller/garbagecollector https://github.com/kubernetes/kubernetes/pull/136040\n```\n./pkg/controller/garbagecollector/garbagecollector.go:363:\t\t\tlogger.V(5).Error(err, \"error syncing item\", \"item\", n.identity)\n./pkg/controller/garbagecollector/graph_builder.go:225:\t\tlogger.V(4).Error(err, \"unable to use a shared informer\", \"resource\", resource, \"kind\", kind)\n```\n- [ ] /pkg/controller/resourcequota https://github.com/kubernetes/kubernetes/pull/136040\n```\n./pkg/controller/resourcequota/resource_quota_monitor.go:178:\tlogger.V(4).Error(err, \"QuotaMonitor unable to use a shared informer\", \"resource\", resource.String())\n```\n- [x] /pkg/controller/storageversionmigrator/ https://github.com/kubernetes/kubernetes/pull/136046\n```\n./pkg/controller/storageversionmigrator/storageversionmigrator.go:227:\t\tlogger.V(4).Error(err, \"resource does not exist in our rest mapper\", \"gvr\", gvr.String())\n./pkg/controller/storageversionmigrator/storageversionmigrator.go:244:\t\tlogger.V(4).Error(errMonitor, \"resource does not exist in GC\", \"gvr\", gvr.String())\n./pkg/controller/storageversionmigrator/storageversionmigrator.go:311:\t\t\tlogger.V(4).Error(err, \"Unable to compare the resource version of the resource\", \"namespace\", accessor.GetNamespace(), \"name\", accessor.GetName(), \"gvr\", gvr.String(), \"accessorRV\", accessor.GetResourceVersion(), \"listResourceVersion\", listResourceVersion, \"error\", err.Error())\n./pkg/controller/storageversionmigrator/storageversionmigrator.go:361:\t\t\tlogger.V(4).Error(errPatch, \"Failed to migrate the resource\", \"namespace\", accessor.GetNamespace(), \"name\", accessor.GetName(), \"gvr\", gvr.String(), \"reason\", apierrors.ReasonForError(errPatch))\n```\n- [ ] ./pkg/controller/volume/ https://github.com/kubernetes/kubernetes/pull/136050\n```\n./pkg/controller/volume/selinuxwarning/selinux_warning_controller.go:458:\t\t\tlogger.V(4).Error(err, \"failed to get SELinux label\", \"pod\", klog.KObj(pod), \"volume\", mount)\n./pkg/controller/volume/selinuxwarning/selinux_warning_controller.go:514:\t\t\tlogger.V(2).Error(err, \"failed to get first pod for event\", \"pod\", conflict.Pod)\n```\n- [ ] ./pkg/kubelet/volumemanager/  https://github.com/kubernetes/kubernetes/pull/136033\n```\n./pkg/kubelet/volumemanager/cache/desired_state_of_world.go:673:\tlogger.V(4).Error(err, \"Please report this error in https://github.com/kubernetes/enhancements/issues/1710, together with full Pod yaml file\")\n./pkg/kubelet/volumemanager/cache/desired_state_of_world.go:688:\t\tlogger.V(4).Error(err, \"failed to get CSI driver name from volume spec\")\n./pkg/kubelet/volumemanager/reconciler/reconstruct.go:188:\t\tlogger.V(4).Error(fetchErr, \"Failed to get Node status to reconstruct device paths\")\n```\n\n\n- [x] ./pkg/kubelet https://github.com/kubernetes/kubernetes/pull/136028\n```\n./pkg/kubelet/stats/helper.go:502:\t\tlogger.V(6).Error(err, \"Unable to fetch pod log stats\", \"pod\", klog.KRef(podNs, podName))\n./pkg/kubelet/stats/helper.go:510:\t\tlogger.V(6).Error(err, \"Unable to fetch pod etc hosts stats\", \"pod\", klog.KRef(podNs, podName))\n./pkg/kubelet/stats/cri_stats_provider.go:168:\t\tlogger.V(5).Error(err,\n./pkg/kubelet/cm/devicemanager/plugin/v1beta1/client.go:107:\t\t\tlogger.V(2).Error(err, \"Failed to close grpc connection\", \"resource\", c.Resource())\n./pkg/kubelet/cm/dra/plugin/dra_plugin_manager.go:398:\t\t\t\t\tlogger.V(3).Error(err, \"Failed to establish WatchResources stream, will retry\")\n./pkg/kubelet/kuberuntime/kuberuntime_container_linux.go:431:\t\t\tlogger.V(5).Error(fmt.Errorf(\"failed to parse /proc/self/cgroup: %w\", err), warn)\n./pkg/kubelet/kuberuntime/kuberuntime_container_linux.go:442:\t\t\tlogger.V(5).Error(err, warn)\n./pkg/kubelet/lifecycle/handlers.go:85:\t\t\tlogger.V(1).Error(err, \"Exec lifecycle hook for Container in Pod failed\", \"execCommand\", handler.Exec.Command, \"containerName\", container.Name, \"pod\", klog.KObj(pod), \"message\", string(output))\n./pkg/kubelet/lifecycle/handlers.go:93:\t\t\tlogger.V(1).Error(err, \"HTTP lifecycle hook for Container in Pod failed\", \"path\", handler.HTTPGet.Path, \"containerName\", container.Name, \"pod\", klog.KObj(pod))\n./pkg/kubelet/lifecycle/handlers.go:101:\t\t\tlogger.V(1).Error(err, \"Sleep lifecycle hook for Container in Pod failed\", \"sleepSeconds\", handler.Sleep.Seconds, \"containerName\", container.Name, \"pod\", klog.KObj(pod))\n./pkg/kubelet/lifecycle/handlers.go:152:\t\tlogger.V(1).Error(err, \"HTTPS request to lifecycle hook got HTTP response, retrying with HTTP.\", \"pod\", klog.KObj(pod), \"host\", req.URL.Host)\n./pkg/kubelet/prober/prober_manager.go:197:\t\t\t\tlogger.V(8).Error(nil, \"Startup probe already exists for container\",\n./pkg/kubelet/prober/prober_manager.go:209:\t\t\t\tlogger.V(8).Error(nil, \"Readiness probe already exists for container\",\n./pkg/kubelet/prober/prober_manager.go:221:\t\t\t\tlogger.V(8).Error(nil, \"Liveness probe already exists for container\",\n./pkg/kubelet/prober/prober.go:106:\t\tlogger.V(1).Error(err, \"Probe errored\", \"probeType\", probeType, \"pod\", klog.KObj(pod), \"podUID\", pod.UID, \"containerName\", container.Name, \"probeResult\", result)\n./pkg/kubelet/metrics/collectors/cri_metrics.go:110:\t\tlogger.V(5).Error(err, \"Descriptor not present in pre-populated list of descriptors\", \"name\", m.Name)\n./pkg/kubelet/pleg/generic.go:300:\t\t\tg.logger.V(4).Error(err, \"PLEG: Ignoring events for pod\", \"pod\", klog.KRef(pod.Namespace, pod.Name))\n./pkg/kubelet/allocation/allocation_manager.go:576:\t\tlogger.V(3).Error(err, \"Failed to delete pod allocation\", \"podUID\", uid)\n```",
      "solution": "I will mark this as a good\u2011first\u2011issue.\n\nIf anyone wants to work on fixing this, please follow the suggestions below:\n\n   1. Submit PRs organized by component or directory. This will make the review process smoother and facilitate better communication with reviewers, since these fixes are relatively small, it's acceptable to fix multiple task in a single PR. However, it's best to avoid having one PR span across different components.\n\n   2. Confirm the appropriate log level for each case. Some logs may actually be Info\u2011level and still need a verbosity setting, while others should remain Error\u2011level. Please discuss with the relevant SIG owners to verify the correct level for each instance.\n  \n   3. Since some incorrectly defined Error-level logs have actually impacted user experience, they will need to be cherry-picked into older release branches after the PR is merged, depending on when the problematic code was introduced.\n\n   4. Since multiple contributors may be interested in resolving this issue, please kindly specify in your PR which parts you are fixing when you work on it, to help avoid duplicate efforts.\n\n/good-first-issue\n\n\n\n---\n\n@HirazawaUi: \n\tThis request has been marked as suitable for new contributors.\n\n### Guidelines\nPlease ensure that the issue body includes answers to the following questions:\n- Why are we solving this issue?\n- To address this issue, are there any code changes? If there are code changes, what needs to be done in the code and what places can the assignee treat as reference points?\n- How can the assignee reach out to you for help?\n\n\nFor more details on the requirements of such an issue, please see [here](https://www.kubernetes.dev/docs/guide/help-wanted/#good-first-issue) and ensure that they are met.\n\nIf this request no longer meets these requirements, the label can be removed\nby commenting with the `/remove-good-first-issue` command.\n\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/136027):\n\n>I will mark this as a good\u2011first\u2011issue.\n>\n>If anyone wants to work on fixing this, please follow the suggestions below:\n>\n>   1. Submit PRs organized by component or directory. This will make the review process smoother and facilitate better communication with reviewers.\n>\n>   2. Confirm the appropriate log level for each case. Some logs may actually be Info\u2011level and still need a verbosity setting, while others should remain Error\u2011level. Please discuss with the relevant SIG owners to verify the correct level for each instance.\n>\n>/good-first-issue\n>\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n\n---\n\nHi @HirazawaUi , thank you for reporting this. I would like to take this issue. Do you believe that your grep result is comprehensive of all log locations that need fixed, or do I need to search for more of them?",
      "labels": [
        "priority/important-soon",
        "sig/node",
        "sig/apps",
        "help wanted",
        "good first issue",
        "triage/accepted"
      ],
      "created_at": "2026-01-05T11:02:56Z",
      "closed_at": "2026-01-30T10:55:02Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/136027",
      "comments_count": 30
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 134831,
      "title": "sendInitialEvents returns events other than ADDED prior to initial BOOKMARK",
      "problem": "### What happened?\n\nI am adding support for sendInitialEvents to our kubernetes client so I am very closely observing the data to verify the reliability of the feature before moving to it. I was recording the raw data into a file while testing so that I could confirm my expectations. What I found is that when starting these watches, I receive a large number of ADDED events in a row, then just before the BOOKMARK is sent, some MODIFIED or DELETED events may get sent unexpectedly.\n\n\n\n### What did you expect to happen?\n\nOn the [api concepts](https://kubernetes.io/docs/reference/using-api/api-concepts/) page, it says:\n> On the client-side the initial state can be requested by specifying sendInitialEvents=true as query string parameter in a watch request. If set, the API server starts the watch stream with synthetic init events (of type `ADDED`) to build the whole state of all existing objects followed by a [`BOOKMARK` event](https://kubernetes.io/docs/reference/using-api/api-concepts/#watch-bookmarks) (if requested via `allowWatchBookmarks=true` option). The bookmark event includes the resource version to which is synced. After sending the bookmark event, the API server continues as for any other watch request.\n\nExample of this from the output of the curl/jq command below:\n```\n\"ADDED\"\n\"ADDED\"\n\"ADDED\"\n\"ADDED\"\n\"ADDED\"\n\"ADDED\"\n\"ADDED\"\n\"ADDED\"\n\"ADDED\"\n\"ADDED\"\n\"ADDED\"\n\"ADDED\"\n\"ADDED\"\n\"ADDED\"\n\"ADDED\"\n\"ADDED\"\n\"ADDED\"\n\"ADDED\"\n\"ADDED\"\n\"ADDED\"\n\"ADDED\"\n\"MODIFIED\"\n\"MODIFIED\"\n\"MODIFIED\"\n\"MODIFIED\"\n\"ADDED\"\n\"MODIFIED\"\n\"MODIFIED\"\n\"MODIFIED\"\n\"MODIFIED\"\n\"MODIFIED\"\n\"MODIFIED\"\n\"MODIFIED\"\n\"MODIFIED\"\n\"MODIFIED\"\n\"MODIFIED\"\n\"MODIFIED\"\n\"MODIFIED\"\n\"MODIFIED\"\n\"MODIFIED\"\n\"MODIFIED\"\n\"MODIFIED\"\n\"MODIFIED\"\n\"BOOKMARK: {\\\"k8s.io/initial-events-end\\\":\\\"true\\\"}\"\n```\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nYou can easily reproduce using curl against the API like:\n```sh\ncurl -s -H \"Authorization: Bearer YOUR_TOKEN\" -H \"Accept: application/json\" 'https://YOUR_API_SERVER/api/v1/pods?allowWatchBookmarks=true&watch=true&timeoutseconds=3600&sendInitialEvents=true&resourceVersionMatch=NotOlderThan' | jq '(.type) + (if .type == \"BOOKMARK\" then \": \\(.object.metadata.annotations)\" else \"\" end)' | grep -m1 -B1000 BOOKMARK\n```\nThat command will watch until it receives the first bookmark event, then log the types of the 1000 events preceding it, and log the bookmark's annotations. Here is a \n\nI have tested this in several clusters. In our smaller clusters, this doesn't seem to happen. In larger clusters with 60,000-100,000 pods, this happens every single time.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nClient Version: v1.31.13\nKustomize Version: v5.4.2\nServer Version: v1.31.13\n\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nEC2\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nI believe this was fixed in 1.32 by https://github.com/kubernetes/kubernetes/pull/127012. [This line](https://github.com/kubernetes/kubernetes/pull/127012/files#diff-5349be781ed7c22c4a2a7ca17e58e8142ed1aaffd6c09586c932abc64d4474a2R507) guarantees that the initial-events-end bookmark is sent right after all the added events. If processInterval doesn't immediately send the bookmark, it gets sent [here in convertToWatchEvent](https://github.com/kubernetes/kubernetes/blame/release-1.33/staging/src/k8s.io/apiserver/pkg/storage/cacher/cache_watcher.go#L366), which will happen in [process](https://github.com/kubernetes/kubernetes/blame/release-1.33/staging/src/k8s.io/apiserver/pkg/storage/cacher/cache_watcher.go#L520), and the bookmark itself is generated from [dispatchEvents](https://github.com/kubernetes/kubernetes/blob/release-1.31/staging/src/k8s.io/apiserver/pkg/storage/cacher/cacher.go#L1022), which is generated on ~1s timer, so all of the non-initial added events are any events dispatched before that bookmark\n\n---\n\n/close\n\nBased on comments before the behavior is expected in 1.31 please upgrade to 1.32 and let us know if you still see it. \n\nClosing as there is no bug, feel free to open if the problem doesn't disappear after upgrade.",
      "labels": [
        "kind/bug",
        "sig/api-machinery",
        "lifecycle/stale",
        "needs-triage"
      ],
      "created_at": "2025-10-23T21:58:59Z",
      "closed_at": "2026-01-30T10:08:36Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/134831",
      "comments_count": 9
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 135825,
      "title": "Kubelet SyncTerminatedPod times out after 30s due to race with cleanupOrphanedPodCgroups",
      "problem": "### What happened?\n\nIn this [OpenShift CI run](https://prow.ci.openshift.org/view/gs/test-platform-results/logs/periodic-ci-openshift-multiarch-master-nightly-4.21-ocp-e2e-ovn-remote-s2s-libvirt-ppc64le/1999572809340162048), the upstream `[sig-node] Lifecycle sleep action zero value when create a pod with lifecycle hook using sleep action with a duration of zero seconds prestop hook using sleep action with zero duration` test failed because SyncTerminatedPod timed out. The kubelet logs shows:\n\n```\nDec 12 21:19:08.719197 compute-1 kubenswrapper[7200]: E1212 21:19:08.719175    7200 pod_workers.go:1324] \"Error syncing pod, skipping\" err=\"failed to delete cgroup paths for [kubepods besteffort poda4418b44-5c5a-4bd5-b7b3-42860ff00483] : unable to destroy cgroup paths for cgroup [kubepods besteffort poda4418b44-5c5a-4bd5-b7b3-42860ff00483] : Timed out while waiting for systemd to remove kubepods-besteffort-poda4418b44_5c5a_4bd5_b7b3_42860ff00483.slice\" pod=\"e2e-pod-lifecycle-sleep-action-allow-zero-761/pod-with-prestop-sleep-hook-zero-duration\" podUID=\"a4418b44-5c5a-4bd5-b7b3-42860ff00483\"\n```\n\nThe systemd journal reveals that the slice was actually removed successfully ~30 seconds before the kubelet timeout:\n\n```\nDec 12 21:18:39.127974 compute-1 systemd[1]: Removed slice libcontainer container kubepods-besteffort-poda4418b44_5c5a_4bd5_b7b3_42860ff00483.slice.\n```\n\nThis shows that systemd completed the slice removal at 21:18:39, but kubelet still timed out at 21:19:08 (exactly ~30 seconds later). The timeout occurred because the kubelet goroutine that initiated the removal never received the D-Bus completion signal.\n\n### What did you expect to happen?\n\nPod deletion should complete promptly after containers terminate, without a 30-second timeout.\n\n### How can we reproduce it? (as minimally and precisely as possible)\n\n1. Create a cluster with cgroupsPerQOS enabled (default)\n2. Run a workload that creates and deletes many pods rapidly\n3. Observe that some pods take 30+ seconds to fully terminate\n\nThe race is more likely to occur when:\n- Multiple pods are being terminated simultaneously\n- The orphaned pod cgroup cleanup housekeeping runs during pod termination\n\n### Anything else we need to know?\n\n#### Root Cause\n\nThere is a race condition between two code paths that call `pcm.Destroy()` on the same pod cgroup:\n\n1. **`SyncTerminatedPod`** - [pkg/kubelet/kubelet.go](https://github.com/kubernetes/kubernetes/blob/c34c5a5426aeb48c55c40af5c4b016a71c24d2d1/pkg/kubelet/kubelet.go#L2398-L2402):\n```go\npcm := kl.containerManager.NewPodContainerManager()\nif err := pcm.Destroy(name); err != nil {\n    return err\n}\n```\n\n2. **`cleanupOrphanedPodCgroups`** - [pkg/kubelet/kubelet_pods.go](https://github.com/kubernetes/kubernetes/blob/c34c5a5426aeb48c55c40af5c4b016a71c24d2d1/pkg/kubelet/kubelet_pods.go#L2835-L2836):\n```go\ngo pcm.Destroy(val)\n```\n\nBoth paths can run concurrently for the same pod cgroup. The `Destroy()` method calls systemd's `StopUnit` via D-Bus using `go-systemd`.\n\n#### Why the 30-second timeout occurs\n\nWhen `StopUnitContext` is called, the `go-systemd` library:\n\n1. Sends a D-Bus method call to systemd's `StopUnit`\n2. Systemd returns a **job path** (e.g., `/org/freedesktop/systemd1/job/12345`)\n3. The library registers the caller's `statusChan` in an internal map keyed by that job path\n4. When systemd emits a `JobRemoved` D-Bus signal with that job path, the library sends the result to the registered channel\n\n**The problem:** When two goroutines call `StopUnit` on the same unit concurrently, systemd may return the **same job path** for both requests (since it's already processing a stop for that unit). The `go-systemd` library overwrites the first channel registration:\n\n[go-systemd/dbus/methods.go](https://github.com/coreos/go-systemd/blob/f3c9410fa503128ff5a025265768843eb09815ca/dbus/methods.go#L55-L75\n)\n```go\n// In go-systemd's dbus/methods.go (simplified)\nfunc (c *Conn) startJob(ch chan<- string, job dbus.ObjectPath) {\n    c.jobListener.Lock()\n    c.jobListener.jobs[job] = ch  // OVERWRITES any existing channel for this job path!\n    c.jobListener.Unlock()\n}\n```\n\n**Timeline of the race:**\n\n1. **T=0ms:** Goroutine A calls `StopUnit`, gets job `/job/100`, registers `chanA`\n2. **T=1ms:** Goroutine B calls `StopUnit`, gets job `/job/100`, registers `chanB` (overwrites `chanA`!)\n3. **T=50ms:** Systemd finishes, emits `JobRemoved` for `/job/100`\n4. **T=50ms:** go-systemd sends result to `chanB` (the only one in the map now)\n5. **T=50ms:** Goroutine B completes successfully\n6. **T=30s:** Goroutine A times out (`chanA` was orphaned, never receives signal)\n\n#### Relevant code\n\nThe 30-second timeout is in [opencontainers/cgroups/systemd/common.go](https://github.com/opencontainers/cgroups/blob/main/systemd/common.go#L184-L210):\n\n```go\nfunc stopUnit(cm *dbusConnManager, unitName string) error {\n    statusChan := make(chan string, 1)\n    err := cm.retryOnDisconnect(func(c *systemdDbus.Conn) error {\n        _, err := c.StopUnitContext(context.TODO(), unitName, \"replace\", statusChan)\n        return err\n    })\n    if err == nil {\n        timeout := time.NewTimer(30 * time.Second)\n        defer timeout.Stop()\n\n        select {\n        case s := <-statusChan:\n            // ...\n        case <-timeout.C:\n            return errors.New(\"Timed out while waiting for systemd to remove \" + unitName)\n        }\n    }\n    // ...\n}\n```\n\n#### Proposed Fix\n\nUse `singleflight.Group` in `podContainerManagerImpl.Destroy()` to deduplicate concurrent calls for the same cgroup. This ensures only one D-Bus `StopUnit` call proceeds per cgroup while other callers wait and receive the same result, avoiding the channel overwrite race entirely.\n\n### Environment\n\n- Kubernetes version: v1.34+\n- CRI: CRI-O (crun/runc)\n- cgroup driver: systemd\n\n/kind bug\n/sig node\n/area kubelet",
      "solution": "Good work on diagnosing the issue. We should fix go-systemd to allow for a slice of channels\n```\nif ch != nil {\n  c.jobListener.jobs[p] = append(c.jobListener.jobs[p], ch)\n}\n```\nWe would need to make sure ch isn't in the list, but that should be trivial to check for.",
      "labels": [
        "kind/bug",
        "area/kubelet",
        "sig/node",
        "priority/important-longterm",
        "triage/accepted"
      ],
      "created_at": "2025-12-18T12:43:52Z",
      "closed_at": "2026-01-29T21:37:57Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/135825",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 136049,
      "title": "Potential goroutines leak in staging/src/k8s.io/client-go/tools/cache/shared_informer.go",
      "problem": "### What happened?\n\nGoroutine 1:If the method distribute() is invoked first, a goroutine holds the **RLock()** and calls listener.add(obj).\nhttps://github.com/kubernetes/kubernetes/blob/f36be36c76192c9ccaf8c22d61ec9ed3c0a2f978/staging/src/k8s.io/client-go/tools/cache/shared_informer.go#L863-L874\nThe goroutine get blocked at the `p.addCh <- notification `\nhttps://github.com/kubernetes/kubernetes/blob/f36be36c76192c9ccaf8c22d61ec9ed3c0a2f978/staging/src/k8s.io/client-go/tools/cache/shared_informer.go#L1015-L1020\n\nGoroutine 2:After that, The method shouldResync() is invoked, and the goroutine get blocked at `p.resyncLock.Lock()` , waiting for the **RLock()** to be released.\nhttps://github.com/kubernetes/kubernetes/blob/f36be36c76192c9ccaf8c22d61ec9ed3c0a2f978/staging/src/k8s.io/client-go/tools/cache/shared_informer.go#L1090-L1099\n\nGoroutine 3: Because the goroutine 2 tries to get Lock() and Write lock has a higher priority. So p.listenersLock.RLock() get blocked, which results in p.wg.Start(listener.pop) can't get executed.\nhttps://github.com/kubernetes/kubernetes/blob/f36be36c76192c9ccaf8c22d61ec9ed3c0a2f978/staging/src/k8s.io/client-go/tools/cache/shared_informer.go#L881-L890\n\nAnd the `<-p.addCh` in listener.pop() can't be executed.\n\nhttps://github.com/kubernetes/kubernetes/blob/f36be36c76192c9ccaf8c22d61ec9ed3c0a2f978/staging/src/k8s.io/client-go/tools/cache/shared_informer.go#L1037\n\nIn the situation, these three goroutines are all blocked. The interleaving sequence that triggers the bug is as follows:\n```\nG1                                            G2                             G3\np.resyncLock.RLock()\np.addCh <- notification \n                                            p.resyncLock.Lock()\n                                                                               p.resyncLock.RLock()                                 \n                                                                                <-p.addCh                  \n```\n\n### What did you expect to happen?\n\nNo goroutines leak.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n1. Adding time.Sleep(time.Second * 7) before `p.listenersLock.RLock()`. This step is to ensure that Lock is executed before RLock.\nhttps://github.com/kubernetes/kubernetes/blob/f36be36c76192c9ccaf8c22d61ec9ed3c0a2f978/staging/src/k8s.io/client-go/tools/cache/shared_informer.go#L881-L883\n\n```\ntime.Sleep(time.Second * 7)\np.listenersLock.RLock()\n```\n2. Using goleak to detect the bug in the test function.\nhttps://github.com/kubernetes/kubernetes/blob/f36be36c76192c9ccaf8c22d61ec9ed3c0a2f978/staging/src/k8s.io/client-go/tools/cache/shared_informer_test.go#L210\n\n```\nfunc TestListenerResyncPeriods(t *testing.T) {\n\tdefer goleak.VerifyNone(t)\n```\n\nThe bug report is as follows,\n\n```\ngoroutine 20 [chan send]:\nk8s.io/client-go/tools/cache.(*processorListener).add(0x24c00e0, {0xdb2360, 0x2409480})\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:1022 +0x6e\nk8s.io/client-go/tools/cache.(*sharedProcessor).distribute(0x2498280, {0xdb2360, 0x2409480}, 0x0)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:873 +0xed\nk8s.io/client-go/tools/cache.(*sharedIndexInformer).OnAdd(0x24c0000, {0xe4d1c0, 0x2632008}, 0x1)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:739 +0x82\nk8s.io/client-go/tools/cache.processDeltas({0xfdc284, 0x24c0000}, {0xfe010c, 0x24900a0}, {0x2409460, 0x1, 0x1}, 0x1)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/controller.go:576 +0x1b3\nk8s.io/client-go/tools/cache.(*sharedIndexInformer).HandleDeltas(0x24c0000, {0xda8220, 0x2409470}, 0x1)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:729 +0x116\nk8s.io/client-go/tools/cache.(*RealFIFO).Pop(0x2614000, 0x2602058)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/the_real_fifo.go:234 +0x464\nk8s.io/client-go/tools/cache.(*controller).processLoop(0x2616000, {0xfdeb24, 0x249a2c0})\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/controller.go:211 +0x50\nk8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext.func1({0xfdeb24, 0x249a2c0}, 0x24b5ee8)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:255 +0x57\nk8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext({0xfdeb24, 0x249a2c0}, 0x24b5ee8, {0xfd9650, 0x26000c0}, 0x1)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xa5\nk8s.io/apimachinery/pkg/util/wait.JitterUntilWithContext({0xfdeb24, 0x249a2c0}, 0x24b5ee8, 0x3b9aca00, 0x0, 0x1)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:223 +0x94\nk8s.io/apimachinery/pkg/util/wait.UntilWithContext(...)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:172\nk8s.io/client-go/tools/cache.(*controller).RunWithContext(0x2616000, {0xfdeb24, 0x249a2c0})\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/controller.go:183 +0x3c4\nk8s.io/client-go/tools/cache.(*sharedIndexInformer).RunWithContext(0x24c0000, {0xfdeb24, 0x249a2c0})\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:587 +0x354\nk8s.io/client-go/tools/cache.(*sharedIndexInformer).Run(...)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:526\nk8s.io/client-go/tools/cache.TestListenerResyncPeriods.(*Group).StartWithChannel.func2()\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:55 +0x20\nk8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x4a\ncreated by k8s.io/apimachinery/pkg/util/wait.(*Group).Start in goroutine 18\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:70 +0x77\n\ngoroutine 35 [sync.RWMutex.RLock]:\nsync.runtime_SemacquireRWMutexR(0x2498290, 0x0, 0x0)\n\tD:/Program Files (x86)/Go/src/runtime/sema.go:100 +0x3d\nsync.(*RWMutex).RLock(0x2498284)\n\tD:/Program Files (x86)/Go/src/sync/rwmutex.go:74 +0x50\nk8s.io/client-go/tools/cache.(*sharedProcessor).run.func1(0x2498280)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:886 +0x4e\nk8s.io/client-go/tools/cache.(*sharedProcessor).run(0x2498280, {0xfdeb98, 0x2618000})\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:893 +0x31\nk8s.io/client-go/tools/cache.(*sharedIndexInformer).RunWithContext.(*Group).StartWithContext.func4()\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:63 +0x27\nk8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x4a\ncreated by k8s.io/apimachinery/pkg/util/wait.(*Group).Start in goroutine 20\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:70 +0x77\n\ngoroutine 36 [sync.Mutex.Lock]:\ninternal/sync.runtime_SemacquireMutex(0x2614004, 0x0, 0x2)\n\tD:/Program Files (x86)/Go/src/runtime/sema.go:95 +0x3d\ninternal/sync.(*Mutex).lockSlow(0x2614000)\n\tD:/Program Files (x86)/Go/src/internal/sync/mutex.go:149 +0x245\ninternal/sync.(*Mutex).Lock(0x2614000)\n\tD:/Program Files (x86)/Go/src/internal/sync/mutex.go:70 +0x4a\nsync.(*Mutex).Lock(...)\n\tD:/Program Files (x86)/Go/src/sync/mutex.go:46\nsync.(*RWMutex).Lock(0x2614000)\n\tD:/Program Files (x86)/Go/src/sync/rwmutex.go:150 +0x28\nk8s.io/client-go/tools/cache.(*RealFIFO).Close(0x2614000)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/the_real_fifo.go:69 +0x2f\nk8s.io/client-go/tools/cache.(*controller).RunWithContext.func1()\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/controller.go:152 +0x4d\ncreated by k8s.io/client-go/tools/cache.(*controller).RunWithContext in goroutine 20\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/controller.go:150 +0xcf\n\ngoroutine 37 [sync.WaitGroup.Wait]:\nsync.runtime_SemacquireWaitGroup(0x2606068, 0x0)\n\tD:/Program Files (x86)/Go/src/runtime/sema.go:114 +0x4d\nsync.(*WaitGroup).Wait(0x2606060)\n\tD:/Program Files (x86)/Go/src/sync/waitgroup.go:206 +0xa1\nk8s.io/apimachinery/pkg/util/wait.(*Group).Wait(...)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:48\nk8s.io/client-go/tools/cache.(*Reflector).watchWithResync.func1()\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/reflector.go:473 +0x30\nk8s.io/client-go/tools/cache.(*Reflector).watchWithResync(0x26040a8, {0xfdeb24, 0x249a2c0}, {0x0, 0x0})\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/reflector.go:478 +0x174\nk8s.io/client-go/tools/cache.(*Reflector).ListAndWatchWithContext(0x26040a8, {0xfdeb24, 0x249a2c0})\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/reflector.go:433 +0x3eb\nk8s.io/client-go/tools/cache.(*Reflector).RunWithContext.func1()\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/reflector.go:358 +0x38\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1({0xfdeb24, 0x249a2c0})\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:233 +0x17\nk8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext.func1({0xfdeb24, 0x249a2c0}, 0x2621f68)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:255 +0x57\nk8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext({0xfdeb24, 0x249a2c0}, 0x2621f68, {0xfd9660, 0x2626000}, 0x1)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xa5\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil(0x2621fa4, {0xfd9660, 0x2626000}, 0x1, 0x249a2c0)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:233 +0x5c\nk8s.io/client-go/tools/cache.(*Reflector).RunWithContext(0x26040a8, {0xfdeb24, 0x249a2c0})\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/reflector.go:357 +0x200\nk8s.io/client-go/tools/cache.(*controller).RunWithContext.(*Group).StartWithContext.func3()\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:63 +0x27\nk8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x4a\ncreated by k8s.io/apimachinery/pkg/util/wait.(*Group).Start in goroutine 20\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:70 +0x77\n\ngoroutine 40 [sync.RWMutex.Lock]:\nsync.runtime_SemacquireRWMutex(0x249828c, 0x0, 0x0)\n\tD:/Program Files (x86)/Go/src/runtime/sema.go:105 +0x3d\nsync.(*RWMutex).Lock(0x2498284)\n\tD:/Program Files (x86)/Go/src/sync/rwmutex.go:155 +0x83\nk8s.io/client-go/tools/cache.(*sharedProcessor).shouldResync(0x2498280)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:915 +0x4f\nk8s.io/client-go/tools/cache.(*Reflector).startResync(0x26040a8, {0xfdeb98, 0x26181e0}, 0x26261c0)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/reflector.go:451 +0x144\nk8s.io/client-go/tools/cache.(*Reflector).watchWithResync.func2()\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/reflector.go:476 +0x33\nk8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x4a\ncreated by k8s.io/apimachinery/pkg/util/wait.(*Group).Start in goroutine 37\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:70 +0x77\n```\nFAIL\tk8s.io/client-go/tools/cache\t32.469s\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\nLatest\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n",
      "solution": "Hmm, this still shows at least a concurrency issue in our tests that could be triggered. I think that warrants a fix at minimum. \n\n> Without this artificial delay, the initialization completes before any other code can interact with the locks.\n\nAre we sure about this? While it seems unlikely that this can happen, I looked through the traces and spotted some concerning sections with the sleep that @user12031 showed.\n\nThe run lock is trying to be acquired by the `run` function and the distribute function is being called by `processReplacedListInfo` in `controller.go`. This does seem like it can happen in a real scenario, however unlikely it is.\n\n```\n\tdefer wg.Wait()                                         // Wait for Processor to stop\n\tdefer stopProcessor(errors.New(\"informer is stopping\")) // Tell Processor to stop\n\t// TODO: extend the MutationDetector interface so that it optionally\n\t// has a RunWithContext method that we can use here.\n\twg.StartWithChannel(processorStopCtx.Done(), s.cacheMutationDetector.Run)\n\twg.StartWithContext(processorStopCtx, s.processor.run)\n\n\tdefer func() {\n\t\ts.startedLock.Lock()\n\t\tdefer s.startedLock.Unlock()\n\t\ts.stopped = true // Don't want any new listeners\n\t}()\n\ts.controller.RunWithContext(ctx)\n```\n\nDue to the fact that wg.Wait only pauses after `s.controller.RunWithContext(ctx)` starts up is a bit of a concern. This seems like the root cause of the issue, since the controller will eventually trigger a replace which adds elements to the informer before it is started, strange...\n\nSince this hasn't been reported on a live env, it seems likely that the informer starts up far quicker than the controller starts pushing events, but I agree that this looks like a real deadlock, possibly worse than just a goroutine leak. I think the main reason we don't see this is that the controller starting a watch is a far greater magnitude of time than the startup time of an informer.\n\ncc @liggitt @pohly \n\n---\n\n@user12031: how is this related to https://github.com/kubernetes/kubernetes/issues/134565? Different root cause?\n\n\n\n---\n\n> [@user12031](https://github.com/user12031): how is this related to [#134565](https://github.com/kubernetes/kubernetes/issues/134565)? Different root cause?\n\nYes, the root casuse is different. I think this bug is caused due to incorrect synchronization. As @michaelasp said above. That's why there is such a local deadlock when adding the delay. ",
      "labels": [
        "kind/bug",
        "sig/api-machinery",
        "needs-triage"
      ],
      "created_at": "2026-01-06T07:26:57Z",
      "closed_at": "2026-01-29T19:52:16Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/136049",
      "comments_count": 20
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 136340,
      "title": "Duplicate ports with different protocols in pod spec get merged after pod spec update",
      "problem": "### What happened?\n\nWhen I add a port with a duplicate port number but different protocol (UDP instead of TCP) to an existing Deployment, one of these two ports get removed.\n\n### What did you expect to happen?\n\nI would expect both to be present.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nApply this:\n\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: test\n  template:\n    metadata:\n      labels:\n        app: test\n    spec:\n      containers:\n      - name: test\n        image: busybox\n        command:\n        - sleep\n        - infinity\n        ports:\n        - containerPort: 1000\n          protocol: TCP\n```\n\nThen `kubectl edit` the `Deployment`, duplicating the containerport, but with UDP, like this:\n\n```\n        ports:\n        - containerPort: 1000\n          protocol: TCP\n        - containerPort: 1000\n          protocol: UDP\n```\n\nAnd you'll observe that one of the two ports is gone in the `Deployment`.\n\n### Anything else we need to know?\n\nThere are no mutating webhooks involved. Even with all hooks removed, I still observ this.\n\nWhen you `get deploy -o yaml`, edit the yaml, `apply --server-side -f /the/yaml`, it does seem to get applied.\n\nMy guess is it is due to (only) `containerPort` being the merge key in types.go.\n\nThis is very similar to #105610, but that issue is about `Services`.\n\n### Kubernetes version\n\nI've observed this in 1.34.2, 1.33.6 and 1.32.4\n\n### Cloud provider\n\nself hosted Talos\n\n### OS version\n\nTalos Linux, various versions\n\n### Install tools\n\nTalos Linux\n\n### Container runtime (CRI) and version (if applicable)\n\ncontainerd\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\nn/a",
      "solution": "Dup of https://github.com/kubernetes/kubernetes/issues/113482\n\nThe problem is that `edit` (and `apply` and maybe more) use client-side apply (CSA).  It being \"client side\" means that the logic is baked into clients.  Unfortunately, `Pod.spec.containers[].ports[]` has an isufficiently unique CSA key declared:\n\n```\n     // +optional                                                                                                                                              \n      // +patchMergeKey=containerPort   \n      // +patchStrategy=merge   \n      // +listType=map   \n      // +listMapKey=containerPort   \n      // +listMapKey=protocol   \n      Ports []ContainerPort `json:\"ports,omitempty\" patchStrategy:\"merge\" patchMergeKey:\"containerPort\" protobuf:\"bytes,6,rep,name=ports\"`   \n```\n\nCSA is `+patchMergeKey=containerPort`.  Server-side apply (SSA) uses `+listMapKey` (plural).\n\nIn this PARTICULAR case, you can make the change you want through SSA, but `edit` does not use SSA. (https://github.com/kubernetes/kubectl/issues/1350)\n\nYou can also use `replace` instead of `edit` or `apply`, but we do not have an automatic fallback for those. (https://github.com/kubernetes/kubectl/issues/1351)\n\nTo api-machinery folks -- could we (in theory?) fix CSA so that new clients (compiled with updated keys) do the right thing and old clients don't break any worse than they are?\n \nI still think we should consider https://github.com/kubernetes/kubernetes/issues/118775\n\n",
      "labels": [
        "kind/bug",
        "sig/network",
        "needs-triage"
      ],
      "created_at": "2026-01-20T11:19:56Z",
      "closed_at": "2026-01-29T18:37:24Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/136340",
      "comments_count": 10
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 132364,
      "title": "kubelet: standardize plugin support",
      "problem": "### What would you like to be added?\n\nHere's a straw-man proposal for a future KEP:\n\n* A node is required to support registering a plugin deployed in a pod.\n* A new, builtin volume source kubeletRegistrationDir with no parameters mounts the kubelet's registration directory such that the pod has write permission there.\n * A new, builtin volume source pluginDataDir with a plugin name as parameter acts like emptyDir, except that it creates a unique path based on that plugin name.\n\n/sig node\n\n### Why is this needed?\n\nAt the moment, device plugins, CSI storage drivers, and DRA drivers cannot be deployed in a portable manner across different Kubernetes distros in a way which is guaranteed to work.\n\nIt's quite possible that this is not worth standardizing. In that case we can use this issue to collect arguments against it and then close it.",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "sig/node",
        "kind/feature",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2025-06-17T18:39:24Z",
      "closed_at": "2026-01-29T18:00:07Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/132364",
      "comments_count": 10
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 129645,
      "title": "Update secret and then upgrade the pod, Sometimes pod will get the old value of secret",
      "problem": "### What happened?\n\nMount the secret to the specified directory in the pod. The startup script of pod will read the value of secret. Our program will update the secret and then upgrade the pod. Sometimes the pod read the old value of secret, after container restart it will read the new value of secret. We use WatchChangeDetectionStrategy, Looks like there's a problem with the kubelet cache update.\n\n### What did you expect to happen?\n\nThe newly created pod immediately detects the secret cache update in kubelet.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nThe probability of the problem is very low, we only encountered it twice in total. I suspect that limiting the CPU resources of the apiserver process and triggering a large number of pods(pods in same node and use same secret) to rebuild may increase the probability of this problem. I am trying to reproduce this problem in this way.\n\n### Anything else we need to know?\n\nI'm having problems probably due to pkg/kubelet/util/manager/watch_based_manager.go method _AddReference_ and _DeleteReference_. I think all the secrets used by new pods should be created with a new list-watch listener instead of reusing the ones already created. From the perspective of method implementation, if multiple pods use the same secret and are on the same node, this situation may occur.\n\n### Kubernetes version\n\n<details>\n\n1.25.3\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n",
      "solution": "@cheese39 @thisisharrsh do either of you have a fix in mind/would you like to take this?\n\n@cheese39 It seems you're on a pretty old version, I am not sure it's fixed now but it could be good to check if a newer, in support version has a fix already.\n\n/triage accepted\n/priority backlog",
      "labels": [
        "kind/bug",
        "priority/backlog",
        "area/kubelet",
        "sig/node",
        "needs-triage"
      ],
      "created_at": "2025-01-15T16:10:44Z",
      "closed_at": "2026-01-28T20:55:34Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/129645",
      "comments_count": 10
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 132978,
      "title": "[KEP-4680] DRA: Health status is not applied to pods that have already terminated",
      "problem": "### What happened?\n\nThe DRA health status feature introduced by KEP-4680 does not currently work for pods that have already reached a terminal state (Failed or Succeeded).\nE2E testing has revealed a race condition in the Kubelet's current implementation. The sequence is as follows:\n\n1.   A pod using a DRA resource runs and its container terminates.\n2.  The Kubelet immediately calls NodeUnprepareResources to begin garbage collection.\n3.  The DRA manager (pkg/kubelet/cm/dra/manager.go) deletes the ClaimInfo from its cache, severing the link between the pod and the allocated device.\n4.  A health update from the DRA plugin arrives after this cleanup.\n5.  The Kubelet's health manager receives the update but can no longer find which pod was using the device, so the update is discarded and the PodStatus is never updated.\n\nThis prevents the \"post-mortem\" troubleshooting use case for batch jobs, which was an intended goal of the KEP.\n\n\n\n### What did you expect to happen?\n\nThe `pod.status.allocatedResourcesStatus` field on the terminated pod object should be updated to reflect the health of the device at the time of failure, even if the health report arrives after the pod has stopped running.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nThis was discovered via an e2e test, which can be reproduced with the following logic:\n\n1.  Create a pod with `RestartPolicy: Never` and a command that runs for a short time before exiting, (e.g. sleep 10 && exit 1). Ensure it uses a DRA resource.\n2.  Wait for the pod to enter the Running phase to confirm the resource was successfully prepared.\n3.  Wait for the pod to enter the Failed phase.\n4. After the pod is Failed, send an Unhealthy status update for its device from the DRA plugin.\n5. Observe the pod's status. The allocatedResourcesStatus field is never populated with the unhealthy status.\n\n### Anything else we need to know?\n\nThis issue is being filed to track a known limitation for [KEP-4680: Add Resource Health Status to the Pod Status](https://github.com/kubernetes/enhancements/issues/4680).\n\nFor the initial Alpha release of the DRA portion of the feature (targeting v1.34), this behavior will be documented as a known limitation. **The core value for long-running services (`RestartPolicy: Always`) is unaffected and provides a solid foundation.**\n\nThis issue will be used to track the future work required to solve this garbage collection race condition. Fixing this will be a requirement for promoting the feature to Beta or GA.\n\nThis will likely require modifying the DRA manager's state handling (`pkg/kubelet/cm/dra/manager.go`) to \"tombstone\" terminated ClaimInfo entries for a grace period instead of deleting them immediately upon un-preparation.\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# paste output here\n```\n\n</details>\nv1.34.0-alpha\n\n### Cloud provider\n\n<details>\n\n\n</details>\nN/A\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n",
      "solution": "this use case of indicating that the Pod has failed because the device went unhealthy is quite important. I do not think we should block alpha of KEP 4680 on this, but we should explore if it can be fixed as a follow up.\n\n---\n\nIs the conclusion that this issue won't be solved?\n\nhttps://github.com/kubernetes/kubernetes/pull/135202#discussion_r2533812734\n\n/wg device-management\n\n\n---\n\n> Is the conclusion that this issue won't be solved?\n> \n> [#135202 (comment)](https://github.com/kubernetes/kubernetes/pull/135202#discussion_r2533812734)\n> \n> /wg device-management\n\n@pohly @Jpsassine based on the [feedback](https://github.com/kubernetes/kubernetes/pull/135202#pullrequestreview-3472383893) from @bart0sh I am working on the approach where we [add health status during unprepare](https://github.com/kubernetes/kubernetes/pull/135202#discussion_r2671403293) \n\nI just opened WIP PR - https://github.com/kubernetes/kubernetes/pull/136573 \n\ncc @haircommander ",
      "labels": [
        "kind/bug",
        "sig/node",
        "priority/important-longterm",
        "triage/accepted",
        "wg/device-management"
      ],
      "created_at": "2025-07-16T00:57:57Z",
      "closed_at": "2026-01-28T17:26:56Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/132978",
      "comments_count": 13
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 135980,
      "title": "kube-aggregator fake clientset deprecated all constructors",
      "problem": "### What happened?\n\nI'm using https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kube-aggregator/pkg/client/clientset_generated/clientset/fake/clientset_generated.go in my tests and I now get the following deprecation warning:\n```\npkg/server/plugin/notifier/k8sbundle/k8sbundle_test.go:906:12: SA1019: fakeaggregator.NewSimpleClientset is deprecated: NewClientset replaces this with support for field management, which significantly improves server side apply testing. NewClientset is only available when apply configurations are generated (e.g. via --with-applyconfig). (staticcheck)\n        client := fakeaggregator.NewSimpleClientset()\n```\n\n### What did you expect to happen?\n\nI'd have expected the package to provide a `NewClientset` function, as the deprecation warning specifies. No such functions is available.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nGrep for `NewClientset` in https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kube-aggregator/pkg/client/clientset_generated/clientset/fake/clientset_generated.go.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\nWas not a problem with k8s.io/kube-aggregator v0.34.2 but it's a problem with v0.35.0\n\n### Cloud provider\n\nN/A\n\n### OS version\n\nN/A\n\n### Install tools\n\nN/A\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n",
      "solution": "https://github.com/kubernetes/kubernetes/pull/136537 should resolve the problem for kube-aggregator and sample-apiserver as well. ",
      "labels": [
        "kind/bug",
        "needs-sig",
        "needs-triage"
      ],
      "created_at": "2025-12-30T13:38:31Z",
      "closed_at": "2026-01-28T16:48:07Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/135980",
      "comments_count": 8
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 130554,
      "title": "Duplicate keys in MatchLabelKeys of PodSpec should be explicitly validated",
      "problem": "#### Summary\nIf `MatchLabelKeys` of `PodSpec` contains duplicate keys, the validation should fail with an explicit error.\n\n#### Details\nIf `MatchLabelKeys` of `PodSpec` contains duplicate keys, validation will fail as follows.\n\n`sample-affinity.yaml`\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-affinity\n  labels:\n    app: sample-affinity\nspec:\n  containers:\n  - image: nginx\n    name: nginx\n  affinity:\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector: {}\n        topologyKey: kubernetes.io/hostname\n        matchLabelKeys:\n        - app\n        - app\n```\n \n```console\n$ kubectl apply -f sample-affinity.yaml\nThe Pod \"sample-affinity\" is invalid: spec.affinity.podAffinity.requiredDuringSchedulingIgnoredDuringExecution[0].matchLabelKeys[1]: Invalid value: \"app\": exists in both matchLabelKeys and labelSelector\n```\n\nIt is caused by duplication between `matchLabelKeys[1]` and the key added to the `labelSelector` corresponding to `matchLabelKeys[0]`, so strictly speaking, key duplication in `matchLabelKeys` is not validated.\nI think it should be explicitly validated.\n\n#### Special notes:\n#130534, which adds `matchLabelKeys` validation for `PodTemplateSpec`, is in progress.\nIn the case of `PodTemplateSpec`, the duplication of keys in `matchLabelKeys` is explicitly validated because the key-values corresponding to `matchLabelKeys` will not be added to the `labelSelector`.\nIf #130534 is merged and this issue is not addressed, the error message will differ between `PodTemplateSpec` and `PodSpec`.\n\nhttps://github.com/kubernetes/kubernetes/blob/0ab306a08655c72fe26c01090a154264648fd2ba/pkg/apis/core/validation/validation.go#L8032-L8040\n\n\n/kind feature \n/sig scheduling\n",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "sig/scheduling",
        "kind/feature",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2025-03-04T07:51:14Z",
      "closed_at": "2026-01-28T15:42:49Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/130554",
      "comments_count": 11
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 136549,
      "title": "Failure cluster [e38cb0c2...] `[sig-node] Ensure Credential Pulled Images pulling images with credentials [FeatureGate:KubeletEnsureSecretPulledImages] `",
      "problem": "<img width=\"1167\" height=\"462\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/77b6eaa7-83f6-42a0-8838-72fcc98fc30f\" />\n\n### Failure cluster [e38cb0c2711f512c7120](https://go.k8s.io/triage#e38cb0c2711f512c7120)\n\n##### Error text:\n```\n[FAILED] Failed to read container status: context deadline exceeded; last observed error from wait loop: failed to get container status: client rate limiter Wait returned an error: context deadline exceeded\nIn [It] at: k8s.io/kubernetes/test/e2e/common/node/runtime.go:386 @ 01/24/26 07:09:41.546\n\n```\n#### Recent failures:\n[1/26/2026, 1:31:12 PM ci-kubernetes-node-swap-ubuntu-serial](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-node-swap-ubuntu-serial/2015779405367873536)\n[1/26/2026, 12:20:03 PM ci-kubernetes-node-kubelet-serial-containerd](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-node-kubelet-serial-containerd/2015761533950234624)\n[1/26/2026, 11:25:59 AM ci-containerd-node-arm64-e2e-serial-ec2](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-containerd-node-arm64-e2e-serial-ec2/2015747692281466880)\n[1/26/2026, 10:19:18 AM ci-cos-containerd-node-e2e-serial](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-cos-containerd-node-e2e-serial/2015731082724380672)\n[1/26/2026, 10:05:54 AM ci-containerd-node-e2e-serial-ec2](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-containerd-node-e2e-serial-ec2/2015727559945228288)\n\n\n/kind failing-test\n<!-- If this is a flake, please add: /kind flake -->\n\n/sig node",
      "solution": "This was hiding behind other things that were broken, which was fixed in:\n- https://github.com/kubernetes/kubernetes/pull/136484\n- https://github.com/kubernetes/kubernetes/pull/136486\n\n---\n\nIt looks like a revert landed and fixed this test for now.\n\n/close\n\n---\n\n@kannon92: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/136549#issuecomment-3811050175):\n\n>It looks like a revert landed and fixed this test for now.\n>\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>",
      "labels": [
        "priority/critical-urgent",
        "sig/node",
        "kind/failing-test",
        "needs-triage"
      ],
      "created_at": "2026-01-26T20:34:02Z",
      "closed_at": "2026-01-28T12:32:12Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/136549",
      "comments_count": 5
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 133474,
      "title": "Frequent churn between EndpointSlice objects",
      "problem": "### What happened?\n\nWe are running a single control-plane k3s cluster in our lab. The cluster has about 56 high-power bare metal nodes. The cluster is used primarily by our developers and our CICD. We create kubevirt VMs for dev and testing. \n\nRecently as our cluster grew to serve a 100+ kubevirt VMs (running in pods), we noticed significant uptick in the number of ssh and nslookup failures by our CICD system when trying to communicate with the VMs. On average, we would see 1 in 100 or so attempts fail to resolve the VMs IP address. The lookups happened based on the VM name. We use a single headless Service for all of our VMs. \n\nWe wrote a small script to see if we could reproduce the issue. The script would send DNS lookups (dig) to all running healthy VMs (VM status = ready). We noticed that coredns would occasionally return NXDOMAIN for perfectly healthy VMs. Importantly, those VMs were not going through any state transition. The NXDOMIANs would be short-lived, usually < 10 seconds and then it go back to resolving correctly. Again, it's important to note that the VMs(and their pods) for which the NXDOMAIN was received were not transitioning states during this period. \n\nTo see why coredns would behave this way, we wrote another program to watch for updates on the EndpointSlice object. What we noticed was that when the number of endpoints per slice grew greater than 100 (the default value), then, as expected, a new slice would be created. However, what would happen next was surprising. The two endpointslices would start to exhibit significant churn, where endpoints would be frequently be removed from one slice and added to the other. Here is a snippet for the two slices: tortuga-4frcm and tortuga-sxqh9. The program logs the type of event (e.g. MODIFIED) and what changed (i.e. it does a diff between the old and new object). Note that this is not a onetime thing, the churn continues for prolonged periods. The vast majority of VMs being shuffled are not going through any state transition. They are just sitting idle. \n\n```\n2025/07/24 22:34:09 EVENT [endpointslices]: Type: MODIFIED | Name: **tortuga-4frcm** | Change: endpoint count changed from 35 to 34, added IP: 10.42.59.53 (ready: true), added IP: 10.42.1.234 (ready: true), added IP: 10.42.4.164 (ready: true), added IP: 10.42.12.42 (ready: true), added IP: 10.42.61.70 (ready: true), added IP: 10.42.3.147 (ready: true), added IP: 10.42.3.145 (ready: true), added IP: 10.42.19.136 (ready: true), added IP: 10.42.31.23 (ready: true), added IP: 10.42.57.87 (ready: true), added IP: 10.42.57.84 (ready: true), added IP: 10.42.32.34 (ready: true), added IP: 10.42.8.181 (ready: true), added IP: 10.42.17.106 (ready: true), added IP: 10.42.7.57 (ready: true), added IP: 10.42.34.117 (ready: true), added IP: 10.42.56.229 (ready: true), added IP: 10.42.61.66 (ready: true), added IP: 10.42.9.155 (ready: true), added IP: 10.42.28.211 (ready: true), added IP: 10.42.10.143 (ready: true), added IP: 10.42.34.113 (ready: true), added IP: 10.42.27.21 (ready: true), added IP: 10.42.19.131 (ready: true), added IP: 10.42.44.221 (ready: true), added IP: 10.42.11.121 (ready: true), added IP: 10.42.15.113 (ready: true), added IP: 10.42.11.119 (ready: true), added IP: 10.42.11.115 (ready: true), added IP: 10.42.17.110 (ready: true), added IP: 10.42.60.77 (ready: true), removed IP: 10.42.3.148, removed IP: 10.42.58.38, removed IP: 10.42.27.22, removed IP: 10.42.60.71, removed IP: 10.42.59.46, removed IP: 10.42.44.225, removed IP: 10.42.12.44, removed IP: 10.42.61.71, removed IP: 10.42.43.235, removed IP: 10.42.4.160, removed IP: 10.42.9.157, removed IP: 10.42.60.74, removed IP: 10.42.40.131, removed IP: 10.42.8.178, removed IP: 10.42.35.149, removed IP: 10.42.32.31, removed IP: 10.42.10.142, removed IP: 10.42.60.76, removed IP: 10.42.7.61, removed IP: 10.42.61.68, removed IP: 10.42.6.208, removed IP: 10.42.60.75, removed IP: 10.42.16.91, removed IP: 10.42.12.45, removed IP: 10.42.44.203, removed IP: 10.42.31.28, removed IP: 10.42.2.56, removed IP: 10.42.6.207, removed IP: 10.42.27.28, removed IP: 10.42.28.210, removed IP: 10.42.3.143, removed IP: 10.42.35.147\n\n2025/07/24 22:34:09 EVENT [endpointslices]: Type: MODIFIED | Name: **tortuga-sxqh9** | Change: added IP: 10.42.59.49 (ready: true), added IP: 10.42.35.146 (ready: true), added IP: 10.42.28.206 (ready: true), added IP: 10.42.57.88 (ready: true), added IP: 10.42.40.131 (ready: true), added IP: 10.42.4.159 (ready: true), added IP: 10.42.6.212 (ready: true), added IP: 10.42.44.224 (ready: true), added IP: 10.42.28.208 (ready: true), added IP: 10.42.2.28 (ready: true), added IP: 10.42.15.110 (ready: true), added IP: 10.42.35.147 (ready: true), added IP: 10.42.35.149 (ready: true), added IP: 10.42.31.22 (ready: true), added IP: 10.42.7.59 (ready: true), added IP: 10.42.60.72 (ready: true), added IP: 10.42.60.76 (ready: true), added IP: 10.42.56.232 (ready: true), added IP: 10.42.32.30 (ready: true), added IP: 10.42.16.91 (ready: true), added IP: 10.42.10.142 (ready: true), added IP: 10.42.19.132 (ready: true), added IP: 10.42.16.81 (ready: true), added IP: 10.42.9.132 (ready: true), added IP: 10.42.30.175 (ready: true), added IP: 10.42.60.80 (ready: true), added IP: 10.42.2.52 (ready: true), added IP: 10.42.56.237 (ready: true), added IP: 10.42.56.236 (ready: true), added IP: 10.42.28.210 (ready: true), added IP: 10.42.40.129 (ready: true), added IP: 10.42.34.114 (ready: true), added IP: 10.42.27.26 (ready: true), added IP: 10.42.61.71 (ready: true), added IP: 10.42.27.27 (ready: true), added IP: 10.42.35.151 (ready: true), added IP: 10.42.12.41 (ready: true), added IP: 10.42.44.203 (ready: true), added IP: 10.42.9.159 (ready: true), added IP: 10.42.8.178 (ready: true), added IP: 10.42.32.38 (ready: true), added IP: 10.42.59.46 (ready: true), added IP: 10.42.6.208 (ready: true), added IP: 10.42.7.60 (ready: true), added IP: 10.42.43.235 (ready: true), added IP: 10.42.7.61 (ready: true), added IP: 10.42.16.89 (ready: true), added IP: 10.42.60.74 (ready: true), added IP: 10.42.3.144 (ready: true), added IP: 10.42.30.174 (ready: true), added IP: 10.42.3.141 (ready: true), added IP: 10.42.59.51 (ready: true), added IP: 10.42.27.22 (ready: true), added IP: 10.42.34.118 (ready: true), added IP: 10.42.19.139 (ready: true), added IP: 10.42.32.31 (ready: true), added IP: 10.42.56.239 (ready: true), added IP: 10.42.32.35 (ready: true), added IP: 10.42.58.38 (ready: true), added IP: 10.42.17.112 (ready: true), added IP: 10.42.61.69 (ready: true), added IP: 10.42.2.56 (ready: true), removed IP: 10.42.11.121, removed IP: 10.42.59.53, removed IP: 10.42.7.56, removed IP: 10.42.19.141, removed IP: 10.42.4.166, removed IP: 10.42.35.150, removed IP: 10.42.28.213, removed IP: 10.42.19.140, removed IP: 10.42.11.117, removed IP: 10.42.12.39, removed IP: 10.42.10.141, removed IP: 10.42.34.109, removed IP: 10.42.61.64, removed IP: 10.42.27.29, removed IP: 10.42.56.235, removed IP: 10.42.16.82, removed IP: 10.42.7.58, removed IP: 10.42.16.86, removed IP: 10.42.17.110, removed IP: 10.42.34.108, removed IP: 10.42.10.143, removed IP: 10.42.12.43, removed IP: 10.42.59.47, removed IP: 10.42.15.111, removed IP: 10.42.27.23, removed IP: 10.42.34.110, removed IP: 10.42.19.136, removed IP: 10.42.27.25, removed IP: 10.42.8.180, removed IP: 10.42.16.76, removed IP: 10.42.6.204, removed IP: 10.42.8.177, removed IP: 10.42.40.132, removed IP: 10.42.60.79, removed IP: 10.42.59.52, removed IP: 10.42.43.238, removed IP: 10.42.44.221, removed IP: 10.42.7.54, removed IP: 10.42.12.38, removed IP: 10.42.16.90, removed IP: 10.42.9.155, removed IP: 10.42.28.211, removed IP: 10.42.7.57, removed IP: 10.42.4.165, removed IP: 10.42.8.182, removed IP: 10.42.6.209, removed IP: 10.42.9.160, removed IP: 10.42.34.113, removed IP: 10.42.4.164, removed IP: 10.42.7.62, removed IP: 10.42.1.231, removed IP: 10.42.11.116, removed IP: 10.42.12.40, removed IP: 10.42.43.236, removed IP: 10.42.40.133, removed IP: 10.42.15.115, removed IP: 10.42.27.21, removed IP: 10.42.2.55, removed IP: 10.42.17.105, removed IP: 10.42.56.240, removed IP: 10.42.32.34, removed IP: 10.42.44.222\n```\n\nSome additional information that may be helpful:\n1. We noticed that the control plane was complaining with these errors:\n\n```\nAug 06 22:20:32 node-name-foo k3s[2663]: time=\"2025-08-06T22:20:32Z\" level=warning msg=\"Proxy error: write failed: io: read/write on closed pipe\"\nAug 06 22:20:42node-name-foo k3s[2663]: I0806 22:20:42.363009    2663 endpointslice_controller.go:337] \"Error syncing endpoint slices for service, retrying\" key=\"jenkins/tortuga\" err=\"EndpointSlice informer cache is out of date\"\n```\n\n2. We are running a single control-plane k3s cluster in our lab. It's certainly possible that this node when under load cannot send out events fast enough to all services (like coredns). However, we would expect that the system should continue to resolve valid VMs to their IPs. \n\nHow does this tie into the coredns issue of returning NXDOMAIN for valid VMs?\n\nCoredns also uses endpointslice objects. One theory would be that there is a significant time gap between the slice update events. Using the example above, an update where one slice removes a (valid) VM is received and then after a delay of several seconds, the update for the second slice, which includes the previously removed VM, is received. Between these two updates, coredns returns NXDOMAIN for the valid VM. \n\nOther thoughts:\n1. The moving of endpoints between slices seems racy to me. If there is significant delay between propagating updates of the two slices, then services like coredns will have temporary inconsistencies. \n2. Is there a corner case where the endpointslice controller frequently balances endpoints across slices? Can this balancing be disabled?\n3. Should coredns (and other consumers of endpointslice) be enhanced to detect stale cache entries?  \n4. Is this expected behavior? I hope not. \n\n### What did you expect to happen?\n\nWe expect that coredns should reliably resolve IPs for VMs/Pods that are healthy.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nUnfortunately, this is not easy to produce, primarily because the issue seems to arise when the control-plane is under load and is having a hard time sending updates to all informers. \n\n### Anything else we need to know?\n\nOur current workaround has been to increase endpoints pers slice from 100 (default) to 200. This has mitigated the issue for the time being, but we would like to scale to 500+ endpoints. Single slice approach may not be appropriate at that point. \n\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nClient Version: v1.33.2\nKustomize Version: v5.6.0\nServer Version: v1.30.5+k3s1\nWARNING: version difference between client (1.33) and server (1.30) exceeds the supported minor version skew of +/-1\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nBare metal.\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\nPRETTY_NAME=\"Ubuntu 24.04 LTS\"\nNAME=\"Ubuntu\"\nVERSION_ID=\"24.04\"\nVERSION=\"24.04 LTS (Noble Numbat)\"\nVERSION_CODENAME=noble\nID=ubuntu\nID_LIKE=debian\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nUBUNTU_CODENAME=noble\nLOGO=ubuntu-logo\n\n$ uname -a\nLinux node-name-foo 6.8.0-45-generic #45-Ubuntu SMP PREEMPT_DYNAMIC Fri Aug 30 12:02:04 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux\n```\n\n</details>\n\n\n### Install tools\n\n<details>\nk3s\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\ncontainerd\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\nCNI: flannel\n</details>\n",
      "solution": "1.30 is out of support upstream: https://kubernetes.io/releases/\n\nIs this reproducible on a supported version? We may have already fixed this.\n\n> Unfortunately, this is not easy to produce, primarily because the issue seems to arise when the control-plane is under load and is having a hard time sending updates to all informers.\n\nMay be possible to replicate under synthetic load with a toy cluster?",
      "labels": [
        "kind/bug",
        "sig/network",
        "triage/needs-information"
      ],
      "created_at": "2025-08-11T21:43:19Z",
      "closed_at": "2026-01-27T18:35:50Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/133474",
      "comments_count": 12
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 136338,
      "title": "The externalTrafficPolicy configuration is invalid.",
      "problem": "### What happened?\n\nk8s version: 1.30.7-aliyun.1\nk8s service yaml: \n``` apiVersion: v1\nkind: Service\nmetadata:\n  name: xxxx-test\n  namespace: default\nspec:\n  ....\n  type: NodePort\n  externalTrafficPolicy: Local\n  internalTrafficPolicy: Cluster\n  ....\n  selector:\n    app: xxx-prod\n   ```\nMy pod count is 1. When I access a node(node IP+ node Port)that doesn't have this pod deployed, it works fine.\n\n### What did you expect to happen?\n\nAs described on the official website, this access should have failed.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: xxxx-test\n  namespace: default\nspec:\n  externalTrafficPolicy: Local\n  ipFamilies:\n  - IPv4\n  ipFamilyPolicy: SingleStack\n  ports:\n  - port: 80\n  selector:\n    app: fool\n  type: NodePort\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.15\", GitCommit:\"1d79bc3bcccfba7466c44cc2055d6e7442e140ea\", GitTreeState:\"clean\", BuildDate:\"2022-09-21T12:18:10Z\", GoVersion:\"go1.16.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"30+\", GitVersion:\"v1.30.7-aliyun.1\", GitCommit:\"cd351e3d740834f7170f62f6e96160f92f74f19d\", GitTreeState:\"clean\", BuildDate:\"2025-10-24T10:30:21Z\", GoVersion:\"go1.22.2\", Compiler:\"gc\", Platform:\"linux/amd64\"\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\naliyun\n</details>\n\n\n### OS version\n\n<details>\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n",
      "solution": "No, no updates. The `ipvs` proxier is unmaintained, so if the CNI causes a failing behaviour, it will not be fixed.\n\nPlease switch to the `nftables` proxier. It is the maintained one, and it does hashing on connections, so it doesn't suffer from long probability chains as the old iptables proxier, which is a reason to use `ipvs`.",
      "labels": [
        "kind/bug",
        "sig/network",
        "area/ipvs",
        "needs-triage"
      ],
      "created_at": "2026-01-20T10:25:33Z",
      "closed_at": "2026-01-27T07:47:28Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/136338",
      "comments_count": 19
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 126972,
      "title": "Watches are not drained during graceful termination when feature gate APIServingWithRoutine is on",
      "problem": "### What happened?\n\nI tried to observe the graceful termination of kube-apiserver, in particular, how watches are drained.\r\n1) I created a cluster in 1.30 with kube-apiserver flags\r\n```\r\n--shutdown-delay-duration=10s --shutdown-send-retry-after=true --shutdown-watch-termination-grace-period=60s \r\n```\r\nand with feature gate `APIServingWithRoutine` on.\r\nAlso, I added to kube-apiserver's manifest the following line\r\n```\r\n\"terminationGracePeriodSeconds\"=60,\r\n```\r\n2) I killed the kube-apiserver, and found the following log during graceful termination:\r\n```\r\n\"[graceful-termination] active watch request(s) have drained\" duration=\"1m0s\" activeWatchesBefore=0 activeWatchesAfter=0 error=null\r\n```\r\n`activeWatchesBefore=0` is not the expected behavior.\r\nAlso, I do not observe any logs of watches being closed.\n\n### What did you expect to happen?\n\nI expected a non-zero number of watches to be drained during graceful termination.\r\nWhen I do the same procedure as above but for cluster 1.29.6 (or a cluster with feature gate `APIServingWithRoutine` off) I see a log similar to this one\r\n```\r\n\"[graceful-termination] active watch request(s) have drained\" duration=\"1m0s\" activeWatchesBefore=623 activeWatchesAfter=0 error=null\"\r\n```\r\nand, preceding it, there are logs of watches being closed (with latency in ~minutes)\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nCreate clusters in 1.30 with provided kube-apiserver flags and with feature gate `APIServingWithRoutine` on and off, respectively.\n\n### Anything else we need to know?\n\nThis issue seems to be related to:\r\n- https://github.com/kubernetes/kubernetes/issues/125614\r\n\r\nIn particular, disabling the feature gate `APIServingWithRoutine` on the kube-apiserver leads to the correct behavior (watches are being drained as in 1.29 version)\n\n### Kubernetes version\n\nObserved in 1.30.2+\n\n### Cloud provider\n\nN/A\r\n\n\n### OS version\n\nN/A\n\n### Install tools\n\nN/A\n\n### Container runtime (CRI) and version (if applicable)\n\nN/A\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\nN/A",
      "solution": "this will be resolved either by https://github.com/kubernetes/kubernetes/pull/126766 or when the feature is removed entirely as [suggested](https://github.com/kubernetes/kubernetes/pull/126470#issuecomment-2259199221)\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/bug",
        "sig/api-machinery",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2024-08-28T13:20:02Z",
      "closed_at": "2026-01-26T23:20:46Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/126972",
      "comments_count": 8
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 135640,
      "title": "--feature-gates=InPlacePodVerticalScaling=true\uff1bIt is not possible to dynamically adjust CPU and memory without restarting the pod.",
      "problem": "### What happened?\n\nAfter enabling the feature --feature-gates=InPlacePodVerticalScaling=true, I tried to dynamically patch the Pod\u2019s CPU and memory with\n```\n> kubectl patch pod operator-0 -n wx \\\n>   --type='json' \\\n>   -p='[{\"op\":\"replace\",\"path\":\"/spec/containers/0/resources\",\"value\":{\"requests\":{\"cpu\":\"4\",\"memory\":\"4096Mi\"},\"limits\":{\"cpu\":\"4\",\"memory\":\"4096Mi\"}}}]'\n```\n\nThe Pod then failed and showed:\n```\nstatus:\n  conditions:\n  - lastProbeTime: null\n    lastTransitionTime: \"2025-12-05T08:06:23Z\"\n    status: \"True\"\n    type: Initialized\n  - lastProbeTime: null\n    lastTransitionTime: \"2025-12-05T08:45:10Z\"\n    message: 'containers with unready status: [goldendb-operator]'\n    reason: ContainersNotReady\n    status: \"False\"\n    type: Ready\n  - lastProbeTime: null\n    lastTransitionTime: \"2025-12-05T08:45:10Z\"\n    message: 'containers with unready status: [goldendb-operator]'\n    reason: ContainersNotReady\n    status: \"False\"\n    type: ContainersReady\n  - lastProbeTime: null\n    lastTransitionTime: \"2025-12-05T08:25:45Z\"\n    status: \"True\"\n    type: PodScheduled\n  containerStatuses:\n  - containerID: docker://cfcbb509420672e03a62431af9e35a4f5b9a3292aac2287cd2e46368c5911eba\n    image: goldendb-release-docker.artnj.zte.com.cn/build/product/x86/kylin/system/v10sword:os-operator-go1.23-20250408\n    imageID: docker-pullable://goldendb-release-docker.artnj.zte.com.cn/build/product/x86/kylin/system/v10sword@sha256:bcd8b38b8b8f65c61456371993a04a001cee18e0df38f69b2c8b4c8d2a0ec61b\n    lastState:\n      terminated:\n        containerID: docker://cfcbb509420672e03a62431af9e35a4f5b9a3292aac2287cd2e46368c5911eba\n        exitCode: 128\n        finishedAt: \"2025-12-06T02:46:03Z\"\n        message: |\n          failed to create task for container: failed to create shim task:\n          OCI runtime create failed: runc create failed: unable to start container\n          process: error during container init: error setting cgroup config for\n          procHooks process: failed to write \"400000\": write /sys/fs/cgroup/cpu,cpuacct/kubepods.slice/kubepods-pod3df68847_8a5e_41b7_bc5c_5370af6aaf95.slice/docker-cfcbb509420672e03a62431af9e35a4f5b9a3292aac2287cd2e46368c5911eba.scope/cpu.cfs_quota_us:\n          invalid argument: unknown\n        reason: ContainerCannotRun\n        startedAt: \"2025-12-06T02:46:03Z\"\n    name: goldendb-operator\n    ready: false\n    restartCount: 217\n    started: false\n    state:\n      waiting:\n        message: back-off 5m0s restarting failed container=goldendb-operator pod=operator-0_wx(3df68847-8a5e-41b7-bc5c-5370af6aaf95)\n        reason: CrashLoopBackOff\n  hostIP: 10.229.43.24\n  phase: Running\n  podIP: 192.168.139.12\n  podIPs:\n  - ip: 192.168.139.12\n  qosClass: Guaranteed\n  startTime: \"2025-12-05T08:06:23Z\"\n```\nI found that when the Pod was first scheduled, the parent cgroup directory\n/sys/fs/cgroup/cpu,cpuacct/kubepods.slice/kubepods-pod3df68847_8a5e_41b7_bc5c_5370af6aaf95.slice\nwas already limited to the original CPU quota.\nDuring the vertical scaling, Kubernetes only updated the container-level cgroup\ndocker-cfcbb509420672e03a62431af9e35a4f5b9a3292aac2287cd2e46368c5911eba.scope,\nbut left the parent directory unchanged, so the new quota could not be applied.\nWhy does Kubernetes adjust only the leaf cgroup and not its parent slice when doing in-place vertical scaling, and how can this be fixed?\ncgroups version:\n```\n> stat -fc %T /sys/fs/cgroup\n> tmpfs\n```\n\n### What did you expect to happen?\n\nI want the Pod\u2019s resources to be scaled up or down normally when the cluster has enough resources.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nInstall Kubernetes 1.28.9 on a Linux system using Docker and cri-dockerd as the container runtime.\nAfter the installation, enable the feature gate:\n--feature-gates=InPlacePodVerticalScaling=true\nThen create a Pod whose requests and limits are identical.\nAttempt to resize the Pod while keeping the new requests and limits equal, and after a while check its status.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\nClient Version: v1.28.9\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\nServer Version: v1.28.9\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\nNAME=\"NewStart CGS Linux\"\nVERSION=\"6.06 (SP)\"\nID=\"ZTEOS\"\nID_LIKE=\"anolis\"\nVERSION_ID=\"6.06\"\nPLATFORM_ID=\"platform:an8\"\nPRETTY_NAME=\"NewStart Carrier Grade Server Linux 6.06 (SP)\"\nANSI_COLOR=\"0;31\"\nHOME_URL=\"http://www.gd-linux.com/\"\nBUG_REPORT_URL=\"http://www.gd-linux.com/\n$ uname -a\nLinux k8sworker 5.10.134-14.zncgsl6.x86_64 #1 SMP Sat Feb 15 18:12:26 CST 2025 x86_64 x86_64 x86_64 GNU/Linux\n\n</details>\n\n\n### Install tools\n\n**<details>**\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n**docker version**\nClient:\n Version:           27.1.1\n API version:       1.46\n Go version:        go1.21.12\n Git commit:        6312585\n Built:             Tue Jul 23 19:55:52 2024\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          27.1.1\n  API version:      1.46 (minimum version 1.24)\n  Go version:       go1.21.12\n  Git commit:       cc13f95\n  Built:            Tue Jul 23 19:57:10 2024\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v1.7.20\n  GitCommit:        8fc6bcff51318944179630522a095cc9dbf9f353\n runc:\n  Version:          1.1.13\n  GitCommit:        v1.1.13-0-g58aa920\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n\n**cri-dockerd version**\ncri-dockerd 0.3.4 (e88b1605)\n</details>\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\nCNI:calico\n</details>\n",
      "solution": "> The version you are currently using is too outdated. The InPlacePodVerticalScaling feature underwent a series of reconstructions between v1.32 and v1.35. Are you willing to try a version above v1.34 to test whether this issue persists?\n\nThis version is the one running in my customer's environment, with other workloads active, so upgrading Kubernetes is not an option at the moment. I'm doing a technical spike on this feature and need to figure out whether the issue is a Kubernetes version-specific bug or an environmental problem under this release.\n\n---\n\n> The version you are currently using is too outdated. The InPlacePodVerticalScaling feature underwent a series of reconstructions between v1.32 and v1.35. Are you willing to try a version above v1.34 to test whether this issue persists?\n\nI understand that after 1.33 this feature has graduated to beta and is enabled by default, so I believe it should work fine; unfortunately, I cannot upgrade the environment right now. Could you please help identify what in v1.28.9 is causing the problem\u2014kernel version, cgroups version, or perhaps cri-docker and Docker? Many thanks in advance!\n\n\n---\n\n> The version you are currently using is too outdated. The InPlacePodVerticalScaling feature underwent a series of reconstructions between v1.32 and v1.35. Are you willing to try a version above v1.34 to test whether this issue persists?\n\nWhile triaging a Kubernetes bug, I came across this issue[https://github.com/kubernetes/kubernetes/issues/118371], which looks similar, but I\u2019m not sure it is the root cause of my current problem.\nAfter the bug was closed, the release notes and version-specific docs were never updated to say that cri-dockerd is unsupported for this feature.\n\nCould you analyze my situation together with this issue and tell me:\n\n1. Whether the closed bug (cri-dockerd missing the Resources field) is indeed the only reason my resize-up requests are failing.\n2. If that bug has been fixed in later Kubernetes versions, or if it still exists and simply isn\u2019t documented.\n3. What definitive check (e.g., kubelet logs, CRI RPC traces, cgroup events) I can run on my Kubernetes 1.28.9 cluster to confirm whether my runtime is failing to return the Resources field\u2014so I can decide whether to migrate to containerd or continue investigating elsewhere.",
      "labels": [
        "kind/bug",
        "sig/node",
        "triage/needs-information"
      ],
      "created_at": "2025-12-06T03:33:20Z",
      "closed_at": "2026-01-26T16:54:03Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/135640",
      "comments_count": 8
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 136331,
      "title": "[failing-test] containerd serial tests failing on reboot",
      "problem": "### Which jobs are failing?\n\nhttps://testgrid.k8s.io/sig-node-release-blocking#node-kubelet-serial-containerd\n\n### Which tests are failing?\n\nRight now the restart is causing all tests to fail.\n\nIt seems to only occur on cos as far as I could tell.\n\n### Since when has it been failing?\n\nAt least for test grid its been failing like this for a few weeks now (Jan 04 2026)\n\n### Testgrid link\n\nhttps://testgrid.k8s.io/sig-node-release-blocking#node-kubelet-serial-containerd\n\n### Reason for failure (if possible)\n\nIts starting to look like the job is failing to restart kubelet causing the full test to fail.\n\nhttps://storage.googleapis.com/kubernetes-ci-logs/logs/ci-kubernetes-node-kubelet-serial-containerd/2013329922633764864/artifacts/e2-standard-2-cos-125-19216-104-95-334854fa/kubelet.log\n\n```\nE0119 19:53:13.251607  206171 run.go:72] \"command failed\" err=\"failed to run Kubelet: failed to create kubelet: create user namespace manager: record pod mappings for existing pod \\\"4cfeffa4-cc3a-4165-b92e-b48f481a0492\\\": wrong user namespace length 131072\"\n````\n\n### Anything else we need to know?\n\nLooks like Test started failing like this since January 14th. Before that we had failures related to credential provider. And then we are going complete failures since.\n\n### Relevant SIG(s)\n\n/sig node",
      "solution": "I tried mitigating this by logging the error instead of failing, but that seems unsafe. If `recordPodMappings` fails and we ignore it, the existing pod's ID range won't be marked in the bitmap, allowing Kubelet to re-allocate those IDs to other pods, which would break isolation as far as I understand.\n\nI think the right fix might be to modify `record()` to handle length mismatches. It would need to calculate which bitmap slots are covered by the existing pod's range and mark them all as used. This ensures we don't crash on startup but also don't risk collisions.\n\nReaching out here before writing anything to gut check if this solution makes sense, as it's a bit more complex than just relaxing the check.\n\n---\n\nLooks like @dims found/fixed some of the bigger problems with this job.\n\nhttps://github.com/kubernetes/kubernetes/pull/136486\nhttps://github.com/kubernetes/kubernetes/pull/136484\n\nI'm not sure if the kubelet namespace errors are still a legit problem. But the test is now showing which tests actually fail.",
      "labels": [
        "priority/critical-urgent",
        "sig/node",
        "kind/failing-test",
        "triage/accepted"
      ],
      "created_at": "2026-01-20T01:39:36Z",
      "closed_at": "2026-01-26T16:49:06Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/136331",
      "comments_count": 12
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 121329,
      "title": "[StructuredAuthnConfig] Make username required",
      "problem": "              I think we want to go ahead and start failing if username is the empty string.\r\n\r\n_Originally posted by @enj in https://github.com/kubernetes/kubernetes/pull/121078#discussion_r1362658684_\r\n           ",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "sig/auth",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2023-10-18T19:56:28Z",
      "closed_at": "2026-01-26T14:19:49Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/121329",
      "comments_count": 12
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 125882,
      "title": "How to reduce the time consumption of apiserver.latency.k8s.io/etcd",
      "problem": "### What happened?\r\n\r\nIn a three node environment, powering off one of the nodes triggers a controller manager master-slave switch. When starting the controller manager, one controller starts for 10 seconds\u3002Found a get request exceeding 10 seconds through audit log localization\uff1a mainly taking time at \"apiserver. latency. k8s. io/etcd\"\r\n```\r\n{\r\n\t\"kind\": \"Event\",\r\n\t\"apiVersion\": \"audit.k8s.io/v1\",\r\n\t\"level\": \"Metadata\",\r\n\t\"auditID\": \"bc5c73d9-f6a5-4182-8980-2557fe668491\",\r\n\t\"stage\": \"ResponseComplete\",\r\n\t\"requestURI\": \"/api/v1/namespaces/kube-system/serviceaccounts/disruption-controller\",\r\n\t\"verb\": \"get\",\r\n\t\"user\": {\r\n\t\t\"username\": \"system:kube-controller-manager\",\r\n\t\t\"groups\": [\r\n\t\t\t\"system:kube-controller-managers\",\r\n\t\t\t\"system:authenticated\"\r\n\t\t]\r\n\t},\r\n\t\"sourceIPs\": [\r\n\t\t\"192.168.139.26\"\r\n\t],\r\n\t\"userAgent\": \"kube-controller-manager1/v1.28.1 (linux/amd64) kubernetes/d5c1d7d/kube-controller-manager\",\r\n\t\"objectRef\": {\r\n\t\t\"resource\": \"serviceaccounts\",\r\n\t\t\"namespace\": \"kube-system\",\r\n\t\t\"name\": \"disruption-controller\",\r\n\t\t\"apiVersion\": \"v1\"\r\n\t},\r\n\t\"responseStatus\": {\r\n\t\t\"metadata\": {},\r\n\t\t\"code\": 200\r\n\t},\r\n\t\"requestReceivedTimestamp\": \"2024-07-03T12:10:30.755816Z\",\r\n\t\"stageTimestamp\": \"2024-07-03T12:10:40.821090Z\",\r\n\t\"annotations\": {\r\n\t\t\"apiserver.latency.k8s.io/etcd\": \"10.061571043s\",\r\n\t\t\"apiserver.latency.k8s.io/response-write\": \"1.93\u00b5s\",\r\n\t\t\"apiserver.latency.k8s.io/serialize-response-object\": \"31.099\u00b5s\",\r\n\t\t\"apiserver.latency.k8s.io/total\": \"10.065274195s\",\r\n\t\t\"apiserver.latency.k8s.io/transform-response-object\": \"1.774\u00b5s\",\r\n\t\t\"authorization.k8s.io/decision\": \"allow\",\r\n\t\t\"authorization.k8s.io/reason\": \"RBAC: allowed by ClusterRoleBinding \\\"system:kube-controller-manager\\\" of ClusterRole \\\"system:kube-controller-manager\\\" to User \\\"system:kube-controller-manager\\\"\"\r\n\t}\r\n}\r\n```\r\napiserver log:\r\n\r\n```\r\nI0703 12:10:40.820718      11 trace.go:236] Trace[245231940]: \"Get\" accept:application/vnd.kubernetes.protobuf, */*,audit-id:bc5c73d9-f6a5-4182-8980-2557fe668491,client:192.168.139.26,protocol:HTTP/2.0,resource:serviceaccounts,scope:resource,url:/api/v1/namespaces/kube-system/serviceaccounts/disruption-controller,user-agent:kube-controller-manager1/v1.28.1 (linux/amd64) kubernetes/d5c1d7d/kube-controller-manager,verb:GET (03-Jul-2024 12:10:30.758) (total time: 10061ms):\r\nTrace[245231940]: ---\"About to Get from storage\" 0ms (12:10:30.758)\r\nTrace[245231940]: ---\"About to write a response\" 10061ms (12:10:40.820)\r\nTrace[245231940]: ---\"Writing http response done\" 0ms (12:10:40.820)\r\nTrace[245231940]: [10.061771961s] [10.061771961s] END\r\nI0703 12:10:40.820770      11 handler.go:153] kube-aggregator: GET \"/apis/coordination.k8s.io/v1/namespaces/manager/leases/appcontroller.leader.lease\" satisfied by nonGoRestful\r\n```\r\n\r\n\r\n\r\n### What did you expect to happen?\r\n\r\nController get requests respond quickly, so that kube-controller-manager switches quickly\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\nOn a three node environment, the node where the down powered main kube controller manager is located.  There is a certain probability that this problem will occur\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_\r\n\r\n### Kubernetes version\r\n\r\n<details>\r\n\r\n```console\r\n$ kubectl version\r\n# paste output here\r\n```\r\nClient Version: v1.28.1\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.28.1\r\n\r\n\r\n</details>\r\n\r\n\r\n### Cloud provider\r\n\r\n<details>\r\nnone\r\n</details>\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\nNAME=\"EulerOS\"\r\nVERSION=\"2.0 (SP12x86_64)\"\r\nID=\"euleros\"\r\nVERSION_ID=\"2.0\"\r\nPRETTY_NAME=\"EulerOS 2.0 (SP12x86_64)\"\r\nANSI_COLOR=\"0;31\"\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n",
      "solution": "/remove-kind bug\r\n/kind support\r\nHi @bigfan6 . Thanks for the issue. Would you mind sharing more on if it causes any issue and serious concern? Thank you\r\n\n\n---\n\nI have found a strong correlation between this time and keepalivetimeout, but when I adjusted keepalivetime and keepalivetimeout to 10 and 5 respectively, the problem still occurred 15 seconds later\r\n![image](https://github.com/user-attachments/assets/40a83652-98bc-41bc-91c5-dd5b7ec04a44)\r\n\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/support",
        "sig/api-machinery",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2024-07-04T09:01:08Z",
      "closed_at": "2026-01-26T14:19:47Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/125882",
      "comments_count": 9
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 126887,
      "title": "lifecycle hooks forbidden from specifying more than 1 handler",
      "problem": "### What happened?\n\nWe are running an application deployment with `PreStop` hooks and  has two handlers `httpGet `and `sleep`.\r\n\r\nThis is our lifecycle section in the deployment:\r\n\r\n```\r\nlifecycle:\r\n  preStop:\r\n    httpGet:\r\n      port: 8080\r\n      path: /stop\r\n    sleep:\r\n      seconds: 5\r\n```\r\n\r\nGetting this error while creating a deployment.\r\n`\r\n[spec.template.spec.containers[0].lifecycle.preStop.httpGet: Forbidden: may not specify more than 1 handler type, spec.template.spec.containers[0].lifecycle.preStop.sleep: Forbidden: may not specify more than 1 handler type]`\r\n\r\nDeployment suceeds only when one handler is specified.\r\n\r\nFYI - We are running on `1.30` version of kubernetes and can confirm the feature gate `PodLifecycleSleepAction` is enabled by default.\n\n### What did you expect to happen?\n\nShould accept in two handlers as part of `kind: deployment`.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nAdding the above mentioned `lifecycle` section to the `deployment` spec.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nClient Version: v1.31.0\r\nKustomize Version: v5.4.2\r\nServer Version: v1.30.2-eks-db838b0\r\n```\r\n\r\n</details>\r\n\n\n### Cloud provider\n\n<details>\r\nAWS\r\n</details>\r\n\n\n### OS version\n\n<details>\r\n\r\nBottlerocket OS 1.20.5 (aws-k8s-1.30)\r\n\r\n</details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n</details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\ncontainerd://1.6.34+bottlerocket\r\n</details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n</details>\r\n",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "priority/backlog",
        "kind/documentation",
        "sig/docs",
        "lifecycle/rotten",
        "needs-kind",
        "needs-triage"
      ],
      "created_at": "2024-08-23T14:44:52Z",
      "closed_at": "2026-01-26T13:18:46Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/126887",
      "comments_count": 17
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 133737,
      "title": "ResourceQuota not updating due to the quota monitor not synced",
      "problem": "### What happened?\n\nWe found that the resource quota resources are not updated after the kcm instancesa were restarted. \nResource quota controller logs resource_quota_controller.go:\n```\nI0826 01:47:28.976532        7 resource_quota_controller.go:307] \"Starting resource quota controller\" logger=\"resourcequota-controller\"\nI0826 01:47:28.985151       7 resource_quota_controller.go:484] \"syncing resource quota controller with updated resources from discovery\" logger=\"resourcequota-controller\" diff=\"added: [/v1, Resource=configmaps /v1, Resource=endpoints /v1, Resource=events /v1, Resource=limitranges /v1, Resource=persistentvolumeclaims /v1, Resource=pods /v1, Resource=podtemplates /v1, Resource=replicationcontrollers /v1, Resource=resourcequotas /v1, Resource=secrets /v1, Resource=serviceaccounts /v1, Resource=services app.tess.io/v1alpha1, ......., Resource=changeobservers spdx.softwarecomposition.kubescape.io/v1beta1, Resource=applicationprofiles spdx.softwarecomposition.kubescape.io/v1beta1, Resource=networkneighborhoods spdx.softwarecomposition.kubescape.io/v1beta1, Resource=openvulnerabilityexchangecontainers spdx.softwarecomposition.kubescape.io/v1beta1, Resource=sbomsyftfiltereds spdx.softwarecomposition.kubescape.io/v1beta1, Resource=sbomsyfts spdx.softwarecomposition.kubescape.io/v1beta1, Resource=seccompprofiles spdx.softwarecomposition.kubescape.io/v1beta1, Resource=vulnerabilitymanifests spdx.softwarecomposition.kubescape.io/v1beta1, Resource=vulnerabilitymanifestsummaries spdx.softwarecomposition.kubescape.io/v1beta1, Resource=workloadconfigurationscans spdx.softwarecomposition.kubescape.io/v1beta1], removed: []\"\nE0826 01:47:59.071869       7 resource_quota_controller.go:506] \"Unhandled Error\" err=\"timed out waiting for quota monitor sync\" logger=\"UnhandledError\"\nI0826 01:48:29.083175       7 resource_quota_controller.go:473] \"no resource updates from discovery, skipping resource quota sync\" logger=\"resourcequota-controller\"\nI0826 01:48:59.094518        7 resource_quota_controller.go:473] \"no resource updates from discovery, skipping resource quota sync\" logger=\"resourcequota-controller\"\nI0826 01:49:29.104691       7 resource_quota_controller.go:473] \"no resource updates from discovery, skipping resource quota sync\" logger=\"resourcequota-controller\"\n```\nThe resource quota controller is waiting for the quota monitor sync and stuck there. The resource quota will not get updated.\n\nThe quota_resource_monitor.go logs\n```\nI0826 01:47:30.277583       7 resource_quota_monitor.go:294] \"quota monitor not synced\" logger=\"resourcequota-controller\" resource=\"spdx.softwarecomposition.kubescape.io/v1beta1, Resource=openvulnerabilityexchangecontainers\" \nI0826 01:47:30.274343       7 resource_quota_monitor.go:294] \"quota monitor not synced\" logger=\"resourcequota-controller\" resource=\"spdx.softwarecomposition.kubescape.io/v1beta1, Resource=vulnerabilitymanifestsummaries\" \nI0826 01:47:30.176685       7 resource_quota_monitor.go:294] \"quota monitor not synced\" logger=\"resourcequota-controller\" resource=\"spdx.softwarecomposition.kubescape.io/v1beta1, Resource=workloadconfigurationscansummaries\" \nI0826 01:47:30.173689       7 resource_quota_monitor.go:294] \"quota monitor not synced\" logger=\"resourcequota-controller\" resource=\"spdx.softwarecomposition.kubescape.io/v1beta1, Resource=sbomsyftfiltereds\"\nI0826 01:47:30.076812       7 resource_quota_monitor.go:294] \"quota monitor not synced\" logger=\"resourcequota-controller\" resource=\"spdx.softwarecomposition.kubescape.io/v1beta1, Resource=vulnerabilitymanifestsummaries\"\nI0826 01:47:30.074502       7 resource_quota_monitor.go:294] \"quota monitor not synced\" logger=\"resourcequota-controller\" resource=\"spdx.softwarecomposition.kubescape.io/v1beta1, Resource=sbomsyftfiltereds\" \nI0826 01:47:29.977541       7 resource_quota_monitor.go:294] \"quota monitor not synced\" logger=\"resourcequota-controller\" resource=\"spdx.softwarecomposition.kubescape.io/v1beta1, Resource=workloadconfigurationscansummaries\"\n```\n\nreflector.go logs\n```\nE0826 01:47:39.481572       7 reflector.go:166] \"Unhandled Error\" err=\"k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: StorageError: invalid object, Code: 4, Key: /spdx.softwarecomposition.kubescape.io/vulnerabilitysummaries, ResourceVersion: 0, AdditionalErrorMsg: operation not supported\" logger=\"UnhandledError\"\nE0826 01:47:38.775972       7 reflector.go:166] \"Unhandled Error\" err=\"k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: StorageError: invalid object, Code: 4, Key: /spdx.softwarecomposition.kubescape.io/configurationscansummaries, ResourceVersion: 0, AdditionalErrorMsg: operation not supported\" logger=\"UnhandledError\"\n```\n\n\nThe new introduced aggregated apis group spdx.softwarecomposition.kubescape.io/v1beta1 may not support PartialObjectMetadata List/Watch, or it has issues when List/Watch operations.\n\nSince the new introduced aggregated apis, the resourcequota usage was no longer updated.\n\n\n\n### What did you expect to happen?\n\nThe issue of one aggregated api/CRD should not block the functionality of the resource quota controller. It should tolerate the misconfigurations of aggregated APIs and CRD resource. \n\n\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nAdd the aggregated api resources with https://github.com/kubescape/storage?tab=readme-ov-file#deploy-into-a-kubernetes-cluster with release version: https://github.com/kubescape/storage/releases/tag/v0.0.174\n\n### Anything else we need to know?\n\nRelated discussion:\nsimilar issues: https://github.com/kubernetes/kubernetes/issues/90597\ncomment: https://github.com/kubernetes/kubernetes/issues/90597#issuecomment-621512319\nsimilar fix for garbage collector: https://github.com/kubernetes/kubernetes/pull/125796\n\n### Kubernetes version\n\n<details>\n\n```console\nKustomize Version: v5.6.0\nServer Version: v1.32.7-62+b241b77573b331\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n",
      "solution": "Thanks @matthyx @xigang. Confirmed it is already fixed by the latest kubescape storage version [v0.0.210](https://github.com/kubescape/storage/releases/tag/v0.0.210).\nI believe the key issue is that the resource quota controller should not be blocked by CRD resources. The resource quota controller should have the tolerations. Similar to the enhacement in garbage collector cache sync. https://github.com/kubernetes/kubernetes/pull/125796\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/bug",
        "sig/api-machinery",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2025-08-28T01:21:02Z",
      "closed_at": "2026-01-26T02:08:45Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/133737",
      "comments_count": 11
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 123280,
      "title": "[APF] Separate Events requests from non-Events",
      "problem": "### What would you like to be added?\n\nAdding separate `prioritylevel`s for Events to avoid resource competitions to non-event requests.\r\n\r\nI currently think two ways of separation:\r\n\r\nOption 1:\r\n\r\nCreate a new FS and PLC for events. The new FS will catch all events regardless of requestors. PLC will have a minimum `nominalConcurrencyShares`\r\n\r\n| Name | Nominal Shares | Lendable | Borrowing Limit | Min Shares |\r\n| ---- | -------------: | -------: | -----------------------: | --: |\r\n| event |  5 |   100% | none | 0 |\r\n\r\nRules\r\n```\r\n  rules:\r\n  - nonResourceRules:\r\n    resourceRules:\r\n    - apiGroups:\r\n      - '*'\r\n      clusterScope: true\r\n      namespaces:\r\n      - '*'\r\n      resources:\r\n      - events\r\n      verbs:\r\n      - '*'\r\n    subjects:\r\n    - group:\r\n        name: '*'\r\n      kind: Group\r\n    - user:\r\n        name: '*'\r\n      kind: User\r\n```\r\n\r\nOption 2:\r\n\r\nCreate a mirror of existing FSs and PLCs for event counterparts\r\n\r\n| Name | Nominal Shares | Lendable | Borrowing Limit | Min Shares |\r\n| ---- | -------------: | -------: | -----------------------: | --: |\r\n| node-high-events       |  5 |  100% | none | 0 |\r\n| system-events          |  2 |  100% | none | 0 |\r\n| workload-high-events   |  2 |  100% | none | 0 |\r\n| workload-low-events    | 5 |  100% | none | 0 |\r\n| global-default-events  |  2 |  100% | none | 0 |\r\n\r\n\r\nThe numbers of \"Nominal Shares\" need more thoughts to be more reasonable but they should be minimum with the hope to use borrowed concurrent shares. I am inclined to Option 1 for its simplicity. \n\n### Why is this needed?\n\nIn my tests that add a few hundreds of nodes to the cluster, I noticed if the machine that hosts `apiserver` & `etcd` is too small, `apiserver` can be easily stressed out and starts responding 429s and 504s (also, nodes became NotReady due to missing lease renews). However, in fact, the majority of the resources were consumed to serve event requests. If I removed events entirely, the cluster was able to survive the whole test more healthily. \r\n\r\nAIUI, event requests are less critical but voluminous so they likely compete currency shares with non-event requests when apiserver is under stress. It's more ideal to process Events when apiserver has free capacity.\r\n\r\nTo give a sense of competition, I observed POST requests of 3k non-event and 9.3k event from kubelet (also a few thousands from scheduler and controller-manager etc) in a test adding 200 nodes to a k8s cluster. Worth mentioning POST requests have to hit the ETCD so they are pretty expensive in terms of the duration occupying the concurrency shares as well as the machine resources.\r\n\r\nMost importantly, event requests are resilient to temporary failures. For example, events are stored in client side up to [1000](https://github.com/kubernetes/client-go/blob/v0.29.1/tools/record/event.go#L43) before dropping. So it seems relatively safe to reject or throttle event requests.\r\n\r\n",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "sig/api-machinery",
        "kind/feature",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2024-02-13T20:34:23Z",
      "closed_at": "2026-01-25T18:06:45Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/123280",
      "comments_count": 15
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 99953,
      "title": "client-go fake client difficult to use with apply",
      "problem": "#### What happened:\r\n\r\nIn PR https://github.com/kubernetes/kubernetes/pull/99462 we migrated the cluster-role-aggregator controller to apply. The controller's unit tests use the client-go fake, which works great for update, but for apply, required adding a custom reactor to handle the apply request because, without any reactors, the fake client returns a \"PatchType not supported\" error (https://github.com/kubernetes/kubernetes/blob/71764b1309348f7e0a6d16f736834197b8a25d44/staging/src/k8s.io/client-go/testing/fixture.go#L182)[.\r\n\r\n#### What you expected to happen:\r\n\r\nThere are a couple possible things we could do:\r\n\r\n- Return a better error that explains that apply testing using the fake client requires registering a reactor (see the example in https://github.com/kubernetes/kubernetes/pull/99462)\r\n- Add better `reactor` support so it's easier to mock out the apply request handling.\r\n- Find a way to add support apply requests to the fake client. Is this feasible? The logic implementing server-side apply is in the apiserver (see https://github.com/kubernetes/kubernetes/tree/master/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/fieldmanager), and if we're going to have the fake client execute apply logic, it should be the correct logic (a fake client that approximates the server-side apply logic would be counterproductive).\r\n\r\n#### How to reproduce it (as minimally and precisely as possible):\r\n\r\nCall apply using a client-go fake client.\r\n",
      "solution": "Issues go stale after 90d of inactivity.\nMark the issue as fresh with `/remove-lifecycle stale`.\nStale issues rot after an additional 30d of inactivity and eventually close.\n\nIf this issue is safe to close now please do so with `/close`.\n\nSend feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n/lifecycle stale\n\n---\n\nStale issues rot after 30d of inactivity.\nMark the issue as fresh with `/remove-lifecycle rotten`.\nRotten issues close after an additional 30d of inactivity.\n\nIf this issue is safe to close now please do so with `/close`.\n\nSend feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n/lifecycle rotten\n\n---\n\nRotten issues close after 30d of inactivity.\nReopen the issue with `/reopen`.\nMark the issue as fresh with `/remove-lifecycle rotten`.\n\nSend feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n/close",
      "labels": [
        "kind/bug",
        "sig/api-machinery",
        "lifecycle/rotten",
        "wg/api-expression",
        "needs-triage"
      ],
      "created_at": "2021-03-08T18:05:51Z",
      "closed_at": "2025-02-07T19:51:25Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/99953",
      "comments_count": 28
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 115598,
      "title": "Server-Side Apply (and fieldmanagement) is missing in client-go/fake",
      "problem": "Since we already have a few issues about this, I decided to create one more:\r\nhttps://github.com/kubernetes/client-go/issues/970\r\nhttps://github.com/kubernetes/client-go/issues/1184\r\nInteresting one: https://github.com/grafana/xk6-kubernetes/pull/81\r\nhttps://github.com/kubernetes/kubernetes/issues/103816\r\nhttps://github.com/kubernetes/kubernetes/pull/103821\r\n\r\nThere are already a few pull-requests that have been pushed to make that happen, current one is waiting on another related pull-request: #115065 \r\nOne trick will be to insert the openapi inside the fake test, we can possibly load from file to make it easier.",
      "solution": "@apelisse Would you be able to provide an update as to what the current status is? The original PR linked in the issue got merged. I realize this is quite an involved change, but as of today it isn't possible to unittest code that uses SSA and that IMHO makes it hard to adopt it",
      "labels": [
        "sig/api-machinery",
        "triage/accepted"
      ],
      "created_at": "2023-02-08T00:20:28Z",
      "closed_at": "2024-06-24T23:48:47Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/115598",
      "comments_count": 10
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 118669,
      "title": "Allow nodes to declare failure probability/SLA",
      "problem": "### What would you like to be added?\r\n\r\n- Add a configuration option to the nodes to declare a failure probability (SLA).\r\n- Allow the workloads to configure the minimum failure probability they need to be able to run. \r\n\r\n/sig scheduling\r\n\r\n### Why is this needed?\r\n\r\nThe idea started to form while implementing the placement-policy schedule plugin ([repo](https://github.com/Azure/placement-policy-scheduler-plugins)) and the need to run on mixed nodes with different availability. Introducing the ability to configure the SLA on nodes and some workloads APIs can lead to better resource management and stabilization.\r\n\r\n// @khenidak ",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "sig/scheduling",
        "sig/node",
        "sig/autoscaling",
        "kind/feature",
        "sig/apps",
        "needs-triage"
      ],
      "created_at": "2023-06-14T20:47:14Z",
      "closed_at": "2025-11-10T20:42:55Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/118669",
      "comments_count": 27
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 118703,
      "title": "Using Dial function with client-go kubernetes.NewForConfig triggers a memory leak",
      "problem": "### What happened?\n\nAfter we upgraded k8s.io/client-go package to v0.26.0, we noticed memory leaks in our components that access the Kubernetes API.  For some components we managed to remove the leak by rolling back client-go to 0.25.4, but for other components it creates a dependency hell.\r\n\r\nAfter some profiling and debugging I managed to write a mockup code that is roughly based on our production code and reliably triggers the leak in client-go 0.26.0 to 0.27.3.  The code is uploaded to https://github.com/kublr/memory_leak_demo .  It includes the pprof endpoint so you can verify the leak.  The memory allocations happen in transport.(*tlsTransportCache)get method.\r\n\r\nThe leak is triggered by the presence of a custom Dial function in rest.Config object.  As I infer from the commits like https://github.com/kubernetes/kubernetes/pull/117311 , this usage is now considered incorrect, but we need some guidance or advice how to adapt our production code.\r\n\n\n### What did you expect to happen?\n\nWe expected to use public interfaces without triggering memory leaks in the package. \n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nCompile and run the sample code in https://github.com/kublr/memory_leak_demo .  Using pprof you can observe growing amount of memory allocated in transport(*tlsTransportCache)get(), and using the debugger you can observe that every call of kubernetes.NewForConfig() creates a new entry in TLS cache.\n\n### Anything else we need to know?\n\nAs I understand, the real culprit is line https://github.com/kubernetes/kubernetes/blob/fa78f28f0a011e9922cf12145dd602c5a63981e8/staging/src/k8s.io/client-go/rest/transport.go#L115 , where client-go creates a DialHolder structure for configs that do not have one, but have a Dial hook.\r\n\r\nBecause DialHolder pointer is added to the **copy** of the original config (at least in the code path originating from kubernetes.NewForConfig), the next calls to kubernetes.NewForConfig will result in the new DialHolder instances.\r\n\r\nBecause pointer to the DialHolder instance is a part of TLS cache key, this creates a sequence of unique map keys that produce useless cache entries (that will never be a cache hit), plus a memory leak.  \r\n\r\nI reiterate that the problem is not a memory leak itself, but a broken cache with unique cache keys.  I think we would be better to mark configs with custom Dial hook as non-cacheable.\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nClient Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.10\", GitCommit:\"e770bdbb87cccdc2daa790ecd69f40cf4df3cc9d\", GitTreeState:\"archive\", BuildDate:\"2023-05-19T08:42:33Z\", GoVersion:\"go1.19.9\", Compiler:\"gc\", Platform:\"darwin/arm64\"}\r\nKustomize Version: v4.5.7\r\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.9\", GitCommit:\"a1a87a0a2bcd605820920c6b0e618a8ab7d117d4\", GitTreeState:\"clean\", BuildDate:\"2023-04-12T12:08:36Z\", GoVersion:\"go1.19.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n```\r\nWe also observed the leaks with k8s versions from 1.22 to 1.26\r\n</details>\r\n\n\n### Cloud provider\n\n<details>\r\nWe observed the leak on vSphere, Azure and onprem installations.  Most likely all clouds are affected.\r\n</details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\nNAME=\"Ubuntu\"\r\nVERSION=\"20.04.2 LTS (Focal Fossa)\"\r\nID=ubuntu\r\nID_LIKE=debian\r\nPRETTY_NAME=\"Ubuntu 20.04.2 LTS\"\r\nVERSION_ID=\"20.04\"\r\nHOME_URL=\"https://www.ubuntu.com/\"\r\nSUPPORT_URL=\"https://help.ubuntu.com/\"\r\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\r\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\r\nVERSION_CODENAME=focal\r\nUBUNTU_CODENAME=focal\r\n$ uname -a\r\nLinux dvi-7696-1686743539-vsp1-master-0 5.4.0-80-generic #90-Ubuntu SMP Fri Jul 9 22:49:44 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n\r\n</details>\r\n\n\n### Install tools\n\n<details>\r\ncustom\r\n</details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n</details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n</details>\r\n",
      "solution": "> To avoid leak, transport.Config and DialHolder instances must be tied to MyK8sApiWrapper instance.\r\n\r\nThis is what I would expect you to do (though not exactly as mentioned below).\r\n\r\n> Also, most of our components are long running services so they will recycle \"MyK8sApiWrapper\" instances from time to time. Because every instance will have its own cache entry, the cache entries still would leak, just not so fast as now.\r\n\r\nThis is true of the API server code in #117258 as well (basically, a known limitation).  You would solve this by holding onto the `*DialHolder` for a given cluster even if you recycle the `MyK8sApiWrapper`.\r\n\r\n> It is only the TLS cache that is leaking. And this leak does not break the connection pooling it supposed to enhance. Which makes me wonder, do we ever need TLS cache in the existing form.\r\n\r\nIt does leak, as I had to fix them: https://github.com/kubernetes/kubectl/issues/1128#issuecomment-1226280651\r\n\r\n---\r\n\r\nFor now, I would probably just set the `Proxy` func to `utilnet.NewProxierWithNoProxyCIDR(http.ProxyFromEnvironment)` so that the TLS cache is skipped for your code.  This isn't guaranteed behavior and may stop working at any time, but it will at least solve the issue for you for now (by restoring the old behavior).\r\n\r\nFor long term improvements to client-go, some options I can see are:\r\n\r\n1. Giving `rest.Config` more control over the the `transport.Config`'s `DialHolder`\r\n2. Giving `rest.Config` the ability to opt-out of TLS caching is a dedicated way\r\n3. Giving users the ability to inform client-go that they are done with a particular client set, so that the cache can be cleaned up correctly\r\n4. Using a `runtime.Finalizer` to automatically clean up the cache when all clientset references have been GC'd\n\n---\n\n@dmitry-irtegov Thank you for this, we're hitting the exact same issue in [hive](https://github.com/openshift/hive/blob/8917099aa9d374aef5e478bde94c1f09ed41090f/pkg/remoteclient/remoteclient.go#L298). Any chance you can share your implementation of the workaround so I can follow your example?\n\n---\n\n> even if one is not using DialHolder, a long-running service that works with multiple k8s API endpoints may still run into a memory leak.\r\n\r\nI can corroborate this. When we deployed code with the [workaround](https://github.com/kubernetes/kubernetes/issues/118703#issuecomment-1595072383), it fixed envs using DialContext -- but we also saw a long-standing \"sawtooth\" memory pattern disappear in the envs that weren't.",
      "labels": [
        "kind/bug",
        "sig/api-machinery",
        "triage/accepted"
      ],
      "created_at": "2023-06-16T07:32:48Z",
      "closed_at": "2026-01-23T20:43:21Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/118703",
      "comments_count": 27
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 121787,
      "title": "Set enableServiceLinks to false as default",
      "problem": "### What would you like to be added?\r\n\r\nI want to make enableServiceLinks disabled by default.\r\n\r\n### Why is this needed?\r\n\r\nService link environment variables are injected into every pod. It creates three environment variables for every service in a respective pod's namespace, regardless of how they are related. This behavior was observed in EKS 1.24.\r\n\r\nIn larger namespaces, this can cause pods to crash due to too many environmental variables. I assume this is due to Container runtime or Operating System Limitations.\r\n\r\nI assume this was needed for legacy network reasons or installs that do not run cluster DNS. Regardless, Cluster DNS is added by default for most Kubernetes installers (excluding mircok8s) out there today.\r\n\r\nI am willing to contribute if this is the direction we want to go.",
      "solution": "So, I have seen a solution to use [Kyverno](https://kyverno.io/).\r\n\r\nExample:\r\nhttps://stackoverflow.com/a/69555759/3263650\r\n\r\nWith this proposal, I am trying to accomplish this:\r\n\r\n- Remove situations where troubleshooting is needed to figure out this problem\r\n  - ex. There is no logging that suggests enableServiceLinks causes this problem; there are only errors when there are too many environment variables for a pod, making it not easy to troubleshoot. \r\n- Not having third-party dependencies to fix this.\r\n\r\n\r\nFWIW, I believe this is the code that performs this action.\r\nhttps://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/kubelet_pods.go#L554\n\n---\n\n@uablrek \r\n\r\nHere is an example of an error we ran into where too many environment variables were set in Java, causing the container to crash. Setting `enableServiceLinks: false` fixed this.\r\n\r\nhttps://github.com/quarkusio/quarkus/issues/36394\n\n---\n\nI don't question the problem, and I agree that the environment variables are basically obsolete, and the default should be enableServiceLinks=false, but...\r\n\r\nWith a so large customer base as K8s has, changing defaults is very risky. According to [Hyrum's law](https://www.hyrumslaw.com/) it _will_ ruin someones workflow.\r\n\r\nThen again, this change may be acceptable. I will bring it up if I attend the next [sig/network bi-weekly meeting](https://www.if-sakerhet.se/brandvarnare-deltronic-x10-med-inbyggt-10-ars-batteri.html).",
      "labels": [
        "sig/network",
        "kind/feature",
        "help wanted",
        "triage/accepted"
      ],
      "created_at": "2023-11-08T05:28:15Z",
      "closed_at": "2025-06-28T19:54:21Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/121787",
      "comments_count": 30
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 134878,
      "title": "Failure cluster [17c9ec19...]: DRA control plane supports reusing resources",
      "problem": "### Failure cluster [17c9ec1980e3fb5b18bd](https://go.k8s.io/triage#17c9ec1980e3fb5b18bd)\n\n##### Error text:\n```\n[FAILED] schedule pod: Timed out after 80.000s.\nexpected pod to be pod is scheduled, got instead:\n    <*v1.Pod | 0xc003aa2908>: \n        metadata:\n          creationTimestamp: \"2025-10-12T09:36:00Z\"\n          generation: 1\n          managedFields:\n          - apiVersion: v1\n            fieldsType: FieldsV1\n            fieldsV1:\n              f:status:\n                f:resourceClaimStatuses:\n                  k:{\"name\":\"my-inline-claim\"}:\n                    .: {}\n                    f:name: {}\n                    f:resourceClaimName: {}\n            manager: ResourceClaimController\n            operation: Apply\n            subresource: status\n            time: \"2025-10-12T09:36:01Z\"\n          - apiVersion: v1\n            fieldsType: FieldsV1\n            fieldsV1:\n              f:spec:\n                f:containers:\n                  k:{\"name\":\"with-resource\"}:\n                    .: {}\n                    f:command: {}\n                    f:image: {}\n                    f:imagePullPolicy: {}\n                    f:name: {}\n                    f:resources:\n                      .: {}\n                      f:claims:\n                        .: {}\n                        k:{\"name\":\"my-inline-claim\"}:\n                          .: {}\n                          f:name: {}\n                    f:securityContext:\n                      .: {}\n                      f:allowPrivilegeEscalation: {}\n                      f:capabilities:\n                        .: {}\n                        f:drop\n```\n#### Recent failures:\n[10/23/2025, 5:44:27 PM ci-kubernetes-e2e-gci-gce-alpha-enabled-default](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-e2e-gci-gce-alpha-enabled-default/1981386239470735360)\n[10/22/2025, 7:32:52 PM ci-kubernetes-e2e-gci-gce-alpha-enabled-default](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-e2e-gci-gce-alpha-enabled-default/1981051139184922624)\n[10/20/2025, 2:45:50 PM ci-kubernetes-e2e-gci-gce](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-e2e-gci-gce/1980254128609169408)\n[10/19/2025, 8:23:52 PM ci-kubernetes-e2e-gci-gce-alpha-enabled-default](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-e2e-gci-gce-alpha-enabled-default/1979976798280093696)\n[10/15/2025, 4:57:43 PM ci-kubernetes-e2e-gci-gce-ip-alias](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-e2e-gci-gce-ip-alias/1978475377672065024)\n\n\n/kind failing-test\n/sig scheduling\n\nLooks like a genuine timeout due to load, not something related to the implementation. Probably need to bump up the timeout.",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "sig/scheduling",
        "kind/failing-test",
        "lifecycle/stale",
        "needs-triage"
      ],
      "created_at": "2025-10-25T08:27:44Z",
      "closed_at": "2026-01-23T10:10:40Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/134878",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 136260,
      "title": "e2e test consistently throwing errors: `[sig-node] SSH should SSH to all nodes and run commands [sig-node]`",
      "problem": "Check any recent run of `pull-kubernetes-e2e-gce`, such as https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/136257/pull-kubernetes-e2e-gce/2011882431119888384\n\nexpand the `build-log.txt`, notice that most tests pass silently, but this test always throws an error like:\n\n```\n\u2022 [31.512 seconds]\n[sig-node] SSH should SSH to all nodes and run commands [sig-node]\nk8s.io/kubernetes/test/e2e/node/ssh.go:48\n  Captured StdOut/StdErr Output >>\n  error dialing prow@i.do.not.exist: 'dial tcp: address i.do.not.exist: missing port in address', retrying\n  error dialing prow@i.do.not.exist: 'dial tcp: address i.do.not.exist: missing port in address', retrying\n  error dialing prow@i.do.not.exist: 'dial tcp: address i.do.not.exist: missing port in address', retrying\n  error dialing prow@i.do.not.exist: 'dial tcp: address i.do.not.exist: missing port in address', retrying\n  << Captured StdOut/StdErr Output\n```\n\n/sig node\n/kind bug",
      "solution": "/cc @ajaysundark \n\n/triage accepted\n/priority backlog\n/sig testing\n\nIs the issue a noisy log or how test failures picked up by UI?\n\n---\n\n> Is the issue a noisy log or how test failures picked up by UI?\n\nI wasn't sure if this was just noisy log (it's the only test logging \"error\" when it passes), or something otherwise wrong with the test.\n\nIf it's just a noisy log, I think we should fix that, a typical test run doesn't have comparable noise from other tests and our logs are already confusing.\n\ncc @pohly @aojea \n\n---\n\nFrom the output it looks like this is a poll loop which logs something on each iteration (\"retrying\"). That's bad practice exactly because it spams log output.\n\nA better solution that can present all relevant information only in case of a real failure is to use gomega.Eventually (https://github.com/kubernetes/community/blob/master/contributors/devel/sig-testing/writing-good-e2e-tests.md#polling-and-timeouts).\n",
      "labels": [
        "kind/bug",
        "priority/backlog",
        "sig/node",
        "sig/testing",
        "triage/accepted"
      ],
      "created_at": "2026-01-15T20:02:19Z",
      "closed_at": "2026-01-22T11:01:34Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/136260",
      "comments_count": 12
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 135865,
      "title": "ci-kubernetes-unit-dependencies failing due to otelgrpc v0.64.0 nil pointer panic",
      "problem": "### What happened?\n\nThe `ci-kubernetes-unit-dependencies` CI job is failing after updating `go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc` from v0.60.0 to v0.64.0.\n\n**Failed job**: https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-unit-dependencies/2002001097211777024\n\n### Root Cause\n\nCommit [07504e453](https://github.com/open-telemetry/opentelemetry-go-contrib/commit/07504e45307f41b8b4e609324a3b62374ad6008f) in otelgrpc removed nil checks from `WithTracerProvider` and `WithMeterProvider` options. When Kubernetes passes a nil `TracerProvider` in `staging/src/k8s.io/apiserver/pkg/storage/storagebackend/factory/etcd3.go:326`:\n\n```go\notelgrpc.WithTracerProvider(c.TracerProvider),  // c.TracerProvider can be nil\n```\n\nThe nil now overwrites the default global provider, causing a panic in `NewClientHandler()` when it calls `c.TracerProvider.Tracer(...)`.\n\n/kind bug\n/priority important-soon",
      "solution": "CI job is back on its feet and we are tracking the problem here:\nhttps://github.com/kubernetes/kubernetes/blob/master/hack/unwanted-dependencies.json#L8-L11\n\nso let's code this out.",
      "labels": [
        "kind/bug",
        "priority/important-soon",
        "sig/architecture",
        "area/code-generation",
        "needs-triage",
        "area/code-organization/future-dependencies"
      ],
      "created_at": "2025-12-20T14:21:25Z",
      "closed_at": "2026-01-22T20:01:28Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/135865",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 136357,
      "title": "Image build is failing - post-kubernetes-push-e2e-jessie-dnsutils-test-images",
      "problem": "Please see:\nhttps://prow.k8s.io/job-history/gs/kubernetes-ci-logs/logs/post-kubernetes-push-e2e-jessie-dnsutils-test-images\n\nLast run:\nhttps://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/post-kubernetes-push-e2e-jessie-dnsutils-test-images/2011957377254821888\n\n```\nBUILD FAILURE: Build step failure: build step 0 \"gcr.io/k8s-staging-test-infra/gcb-docker-gcloud:v20240523-a15ad90fc9@sha256:bb04162508c2c61637eae700a0d8e8c8be8f2d4c831d2b75e59db2d4dd6cf75d\" failed: step exited with non-zero status: 2\nERROR: (gcloud.builds.submit) build c5579767-ea08-4d89-a130-586a365cba30 completed with status \"FAILURE\"\nBuilding image for jessie-dnsutils OS/ARCH: linux/amd64...\n/workspace/_tmp/test-images-build.nCcPLA /workspace/test/images\n#0 building with \"img-builder\" instance using docker-container driver\n\n#1 [internal] load build definition from Dockerfile\n#1 transferring dockerfile: 1.26kB done\n#1 WARN: InvalidDefaultArgInFrom: Default value for ARG $BASEIMAGE results in empty or invalid base image name (line 16)\n#1 DONE 0.0s\n\n#2 [internal] load metadata for docker.io/library/debian:jessie\n#2 DONE 0.4s\n\n#3 [internal] load .dockerignore\n#3 transferring context: 2B done\n#3 DONE 0.0s\n\n#4 [internal] load build context\n#4 transferring context: 1.38kB done\n#4 DONE 0.0s\n\n#5 [1/6] FROM docker.io/library/debian:jessie@sha256:32ad5050caffb2c7e969dac873bce2c370015c2256ff984b70c1c08b3a2816a0\n#5 resolve docker.io/library/debian:jessie@sha256:32ad5050caffb2c7e969dac873bce2c370015c2256ff984b70c1c08b3a2816a0 0.0s done\n#5 sha256:b82b9923b08dfd4c2a83d1669b67a3a0c12f2d17fc989315c05f201eabc33b52 2.10MB / 54.39MB 0.2s\n#5 sha256:b82b9923b08dfd4c2a83d1669b67a3a0c12f2d17fc989315c05f201eabc33b52 8.39MB / 54.39MB 0.3s\n#5 ...\n\n#6 [5/6] ADD https://github.com/coredns/coredns/releases/download/v1.5.0/coredns_1.5.0_linux_amd64.tgz /coredns.tgz\n#6 DONE 0.4s\n\n#5 [1/6] FROM docker.io/library/debian:jessie@sha256:32ad5050caffb2c7e969dac873bce2c370015c2256ff984b70c1c08b3a2816a0\n#5 sha256:b82b9923b08dfd4c2a83d1669b67a3a0c12f2d17fc989315c05f201eabc33b52 18.87MB / 54.39MB 0.5s\n#5 sha256:b82b9923b08dfd4c2a83d1669b67a3a0c12f2d17fc989315c05f201eabc33b52 26.21MB / 54.39MB 0.6s\n#5 sha256:b82b9923b08dfd4c2a83d1669b67a3a0c12f2d17fc989315c05f201eabc33b52 34.60MB / 54.39MB 0.8s\n#5 sha256:b82b9923b08dfd4c2a83d1669b67a3a0c12f2d17fc989315c05f201eabc33b52 42.99MB / 54.39MB 0.9s\n#5 sha256:b82b9923b08dfd4c2a83d1669b67a3a0c12f2d17fc989315c05f201eabc33b52 51.38MB / 54.39MB 1.1s\n#5 sha256:b82b9923b08dfd4c2a83d1669b67a3a0c12f2d17fc989315c05f201eabc33b52 54.39MB / 54.39MB 1.2s\n#5 sha256:b82b9923b08dfd4c2a83d1669b67a3a0c12f2d17fc989315c05f201eabc33b52 54.39MB / 54.39MB 1.2s done\n#5 extracting sha256:b82b9923b08dfd4c2a83d1669b67a3a0c12f2d17fc989315c05f201eabc33b52\n#5 extracting sha256:b82b9923b08dfd4c2a83d1669b67a3a0c12f2d17fc989315c05f201eabc33b52 1.9s done\n#5 DONE 3.2s\n\n#7 [2/6] COPY fixup-apt-list.sh /\n#7 DONE 0.5s\n\n#8 [3/6] RUN [\"/fixup-apt-list.sh\"]\n#8 DONE 0.1s\n\n#9 [4/6] RUN apt-get -q update &&     apt-get install -y --force-yes dnsutils &&     apt-get clean\n#9 0.133 Ign http://security.debian.org jessie/updates InRelease\n#9 0.138 Ign http://security.debian.org jessie/updates Release.gpg\n#9 0.142 Ign http://security.debian.org jessie/updates Release\n#9 0.146 Err http://security.debian.org jessie/updates/main amd64 Packages\n#9 0.146   \n#9 0.151 Err http://security.debian.org jessie/updates/main amd64 Packages\n#9 0.151   \n#9 0.158 Err http://security.debian.org jessie/updates/main amd64 Packages\n#9 0.158   \n#9 0.171 Err http://security.debian.org jessie/updates/main amd64 Packages\n#9 0.171   \n#9 0.175 Err http://security.debian.org jessie/updates/main amd64 Packages\n#9 0.175   404  Not Found [IP: 151.101.194.132 80]\n#9 0.213 Ign http://deb.debian.org jessie InRelease\n#9 0.298 Ign http://deb.debian.org jessie-updates InRelease\n#9 0.386 Ign http://deb.debian.org jessie Release.gpg\n#9 0.472 Ign http://deb.debian.org jessie-updates Release.gpg\n#9 0.554 Ign http://deb.debian.org jessie Release\n#9 0.633 Ign http://deb.debian.org jessie-updates Release\n#9 1.369 Err http://deb.debian.org jessie/main amd64 Packages\n#9 1.369   404  Not Found [IP: 151.101.2.132 80]\n#9 1.451 Err http://deb.debian.org jessie-updates/main amd64 Packages\n#9 1.451   404  Not Found [IP: 151.101.2.132 80]\n#9 1.452 W: Failed to fetch http://security.debian.org/debian-security/dists/jessie/updates/main/binary-amd64/Packages  404  Not Found [IP: 151.101.194.132 80]\n#9 1.452 \n#9 1.452 W: Failed to fetch http://deb.debian.org/debian/dists/jessie/main/binary-amd64/Packages  404  Not Found [IP: 151.101.2.132 80]\n#9 1.452 \n#9 1.452 W: Failed to fetch http://deb.debian.org/debian/dists/jessie-updates/main/binary-amd64/Packages  404  Not Found [IP: 151.101.2.132 80]\n#9 1.452 \n#9 1.452 E: Some index files failed to download. They have been ignored, or old ones used instead.\n#9 ERROR: process \"/bin/sh -c apt-get -q update &&     apt-get install -y --force-yes dnsutils &&     apt-get clean\" did not complete successfully: exit code: 100\n------\n > [4/6] RUN apt-get -q update &&     apt-get install -y --force-yes dnsutils &&     apt-get clean:\n1.369   404  Not Found [IP: 151.101.2.132 80]\n1.451 Err http://deb.debian.org jessie-updates/main amd64 Packages\n1.451   404  Not Found [IP: 151.101.2.132 80]\n1.452 W: Failed to fetch http://security.debian.org/debian-security/dists/jessie/updates/main/binary-amd64/Packages  404  Not Found [IP: 151.101.194.132 80]\n1.452 \n1.452 W: Failed to fetch http://deb.debian.org/debian/dists/jessie/main/binary-amd64/Packages  404  Not Found [IP: 151.101.2.132 80]\n1.452 \n1.452 W: Failed to fetch http://deb.debian.org/debian/dists/jessie-updates/main/binary-amd64/Packages  404  Not Found [IP: 151.101.2.132 80]\n1.452 \n1.452 E: Some index files failed to download. They have been ignored, or old ones used instead.\n------\n\n \u001b[33m1 warning found (use --debug to expand):\n\u001b[0m - InvalidDefaultArgInFrom: Default value for ARG $BASEIMAGE results in empty or invalid base image name (line 16)\nDockerfile:26\n--------------------\n  25 |     \n  26 | >>> RUN apt-get -q update && \\\n  27 | >>>     apt-get install -y --force-yes dnsutils && \\\n  28 | >>>     apt-get clean\n  29 |     \n--------------------\nERROR: failed to solve: process \"/bin/sh -c apt-get -q update &&     apt-get install -y --force-yes dnsutils &&     apt-get clean\" did not complete successfully: exit code: 100\nmake: *** [Makefile:44: all-build-and-push] Error 1\nERROR\nERROR: build step 0 \"gcr.io/k8s-staging-test-infra/gcb-docker-gcloud:v20240523-a15ad90fc9@sha256:bb04162508c2c61637eae700a0d8e8c8be8f2d4c831d2b75e59db2d4dd6cf75d\" failed: step exited with non-zero status: 2\n```\n\n",
      "solution": "\n**Report generated by [stackseek.io](url)**\n\n\n=== ERROR ANALYSIS REPORT ===\n\nREPOSITORY DETAILS:\n- Repository: kubernetes\n\nCUSTOMER EMAIL RESPONSE:\nHi [Customer Name],\n\nThanks for reporting the build failure. We identified that the issue is caused by an outdated configuration in the build environment referencing an unsupported software version.\n\nOur team has updated the configuration to use a supported version, and the build service is being restored. No action is required on your side.\n\nIf you continue to see issues, please let us know.\n\nBest regards,\nThe Support Team.\n\nQUICK SUMMARY:\nThe build failed because it relies on an obsolete operating system version (Debian Jessie) whose update servers have been shut down. It's like trying to call a disconnected phone number. We need to update the configuration to use a modern, supported version.\n\nROOT CAUSE ANALYSIS:\nThe build fails because the Dockerfile uses 'debian:jessie' as its base image. Debian Jessie (8.0) is End-of-Life (EOL), and its package repositories have been moved from the main mirrors to 'archive.debian.org', causing '404 Not Found' errors during 'apt-get update'.\n\nERROR LOCATION:\ntest/images/Dockerfile:26\n\nEXECUTION PATH:\ndocker build -> FROM debian:jessie -> RUN apt-get update -> 404 Error\n\nRESOLUTION STEPS:\nNo action is required on your side. Our engineering team has been notified and is working on a fix. We will update you once resolved.\n\nREPLICATION STEPS:\nCreate a Dockerfile with 'FROM debian:jessie'\nAdd 'RUN apt-get update'\nRun 'docker build .'\n\nSUGGESTED FIX:\nUpdate the base image to a supported version (e.g., 'debian:bookworm-slim') in the Dockerfile. Alternatively, if Jessie is required, update '/etc/apt/sources.list' to point to 'archive.debian.org' before running update.\n\nGENERATED TEST CODE (C):\n#include <stdio.h>\n#include <string.h>\n#include <stdlib.h>\n\n/*\n * SIMULATION OF ERROR CONDITION\n * \n * The error is caused by using 'debian:jessie' (Debian 8) which is End-of-Life.\n * The standard repositories (deb.debian.org, security.debian.org) no longer host \n * packages for Jessie, returning 404 Not Found.\n * \n * This test simulates the package manager's behavior when encountering these 404s.\n */\n\n// Mock function to simulate `apt-get update` behavior\nint mock_apt_get_update(const char* distro_codename, const char* repository_url) {\n    // Logic: If the distro is \"jessie\" and using standard mirrors, simulate 404\n    int is_eol_distro = (strcmp(distro_codename, \"jessie\") == 0);\n    int is_standard_mirror = (strstr(repository_url, \"deb.debian.org\") != NULL || \n                              strstr(repository_url, \"security.debian.org\") != NULL);\n\n    if (is_eol_distro && is_standard_mirror) {\n        // Simulate the log output\n        printf(\"Err %s %s/main amd64 Packages\\n\", repository_url, distro_codename);\n        printf(\"  404  Not Found\\n\");\n        printf(\"W: Failed to fetch %s/dists/%s/main/binary-amd64/Packages  404  Not Found\\n\", repository_url, distro_codename);\n        printf(\"E: Some index files failed to download.\\n\");\n        \n        // Return exit code 100 as seen in the stack trace\n        return 100;\n    }\n\n    return 0; // Success\n}\n\nint main() {\n    // Test Configuration\n    const char* target_distro = \"jessie\";\n    const char* target_repo = \"http://deb.debian.org/debian\";\n\n    printf(\"Running test: Reproduce apt-get update failure for EOL distro %s...\\n\", target_distro);\n\n    // Execute the mock function\n    int exit_code = mock_apt_get_update(target_distro, target_repo);\n\n    // Verification\n    if (exit_code == 100) {\n        printf(\"\\nSUCCESS: Reproduced expected failure (Exit Code 100) for EOL distro.\\n\");\n        return 0; // Test passed\n    } else {\n        printf(\"\\nFAILURE: Expected exit code 100, but got %d.\\n\", exit_code);\n        return 1; // Test failed\n    }\n}\n",
      "labels": [
        "priority/important-soon",
        "sig/network",
        "sig/testing",
        "triage/accepted"
      ],
      "created_at": "2026-01-20T20:28:10Z",
      "closed_at": "2026-01-22T19:58:12Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/136357",
      "comments_count": 5
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 133274,
      "title": "When there are two resource quotas under the same namespace, the used resource values of the two resource quotas are inconsistent.",
      "problem": "### What happened?\n\nUnder the same namespace, there are two resource quotas. I set the deployment hard values for both quotas to 3000, and then created deployments one by one. During the creation process, the quota might fail to update due to the error \"**Operation cannot be fulfilled on resourcequotas 'aaa-rq': the object has been modified; please apply your changes to the latest version and try again**\" leading to the failure of the deployment creation. The last one quota reached its limit, but the other quota did not reach its limit.\n\n### What did you expect to happen?\n\nThese two quotas should have the same usage value.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n1. create a namespace\n2. crate two resourcequota under the namespace, and the two quota have the same hard value, may be 3000 deployments\n3. quickly,create deployments one by one\n4. the problem is present that the actual deployments have not reached 3000, but the usage of one quota has already hit 3000. Any further creation attempts will be rejected.\n\n### Anything else we need to know?\n\nI believe the issue is related to this piece of code.\n```\nfunc (e *quotaEvaluator) checkQuotas(quotas []corev1.ResourceQuota, admissionAttributes []*admissionWaiter, remainingRetries int) {\n...\n\t\tif err := e.quotaAccessor.UpdateQuotaStatus(&newQuota); err != nil {\n\t\t\tupdatedFailedQuotas = append(updatedFailedQuotas, newQuota)\n\t\t\tlastErr = err\n\t\t}\n...\n\tquotasToCheck := []corev1.ResourceQuota{}\n\tfor _, newQuota := range newQuotas {\n\t\tfor _, oldQuota := range updatedFailedQuotas {\n\t\t\tif newQuota.Name == oldQuota.Name {\n\t\t\t\tquotasToCheck = append(quotasToCheck, newQuota)\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\te.checkQuotas(quotasToCheck, admissionAttributes, remainingRetries-1)\n```\n\nquota A and quota B, A was successfully updated, but B never managed to update successfully, ultimately causing the deployments to be forbidden. However, in reality, A had already been updated, while B had not yet been updated. This eventually led to A quickly reaching its limit.\n\n### Kubernetes version\n\n<details>\n1.32\n```console\n$ kubectl version\n# paste output here\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n",
      "solution": "@BenTheElder hi, this issue can ultimately be resolved through kcm, so I'm not sure if it can be considered a \"bug\". Do we intend to fix it?\n\n---\n\n> hi, this issue can ultimately be resolved through kcm, so I'm not sure if it can be considered a \"bug\". Do we intend to fix it?\n\nIt's not clear to me what you mean by \"can be ultimately resolved through kcm\", but bug resolution is entirely volunteers submitting fixes.\n\nI don't personally plan to work on this one at the moment.\n\n---\n\n>  hi, this issue can ultimately be resolved through kcm, so I'm not sure if it can be considered a \"bug\". Do we intend to fix it?\n> It's not clear to me what you mean by \"can be ultimately resolved through kcm\", but bug resolution is entirely volunteers submitting fixes.\n> I don't personally plan to work on this one at the moment.\n\nIn the KCM, the resourcequota controller synchronizes and recalculates the quantity of objects in the environment, then refreshes the resourcequota. Therefore, even if the quota is initially inaccurate, it will eventually be refreshed to the accurate state. I'm curious whether the community considers such logic - which guarantees eventual consistency but cannot ensure intermediate states - as a bug that needs to be fixed?",
      "labels": [
        "kind/bug",
        "sig/api-machinery",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2025-07-29T12:06:59Z",
      "closed_at": "2026-01-22T02:23:39Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/133274",
      "comments_count": 14
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 129802,
      "title": "[Flaking Test] [sig-api-machinery] k8s.io/kubernetes/test/integration/apiserver/coordinatedleaderelection.coordinatedleaderelection",
      "problem": "### Which jobs are flaking?\n\nmaster-blocking\n- integration-master\n\n### Which tests are flaking?\n\nk8s.io/kubernetes/test/integration/apiserver/coordinatedleaderelection.coordinatedleaderelection\n[Prow](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-integration-master/1881787128904421376)\n[Triage](https://storage.googleapis.com/k8s-triage/index.html?test=k8s.io%2Fkubernetes%2Ftest%2Fintegration%2Fapiserver%2Fcoordinatedleaderelection.coordinatedleaderelection&xjob=e2e-kops)\n\n### Since when has it been flaking?\n\n[1/14/2025, 12:41:26 AM](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-integration-master/1878882224963588096)\n[1/18/2025, 1:29:23 AM](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-integration-master/1880343855438499840)\n[1/20/2025, 1:20:29 PM](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-integration-master/1881247568734720000)\n[1/20/2025, 1:37:27 PM](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-integration-1-32/1881251846668947456)\n[1/22/2025, 1:04:24 AM](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-integration-master/1881787128904421376)\n[1/23/2025, 1:45:57 PM](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-integration-1-32/1882341172085526528)\n\n### Testgrid link\n\nhttps://testgrid.k8s.io/sig-release-master-blocking#integration-master\n\n### Reason for failure (if possible)\n\n```\n{Failed  === RUN   TestCoordinatedLeaderElectionLeaseTransfer\n    testserver.go:582: Resolved testserver package path to: \"/home/prow/go/src/k8s.io/kubernetes/cmd/kube-apiserver/app/testing\"\n    testserver.go:402: runtime-config=map[api/all:true]\n    testserver.go:403: Starting kube-apiserver on port 43809...\n    testserver.go:438: Waiting for /healthz to be ok...\n[-]poststarthook/start-apiextensions-controllers failed: not finished\n[-]poststarthook/crd-informer-synced failed: not finished\n[-]poststarthook/start-service-ip-repair-controllers failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/priority-and-fairness-config-producer failed: not finished\n[-]poststarthook/bootstrap-controller failed: not finished\n[-]poststarthook/apiservice-registration-controller failed: not finished\n[-]poststarthook/apiservice-discovery-controller failed: not finished\n[-]autoregister-completion failed: missing APIService: [v1. v1.admissionregistration.k8s.io v1.apiextensions.k8s.io v1.apps v1.authentication.k8s.io v1.authorization.k8s.io v1.autoscaling v1.batch v1.certificates.k8s.io v1.coordination.k8s.io v1.discovery.k8s.io v1.events.k8s.io v1.flowcontrol.apiserver.k8s.io v1.networking.k8s.io v1.node.k8s.io v1.policy v1.rbac.authorization.k8s.io v1.scheduling.k8s.io v1.storage.k8s.io v1alpha1.admissionregistration.k8s.io v1alpha1.internal.apiserver.k8s.io v1alpha1.storage.k8s.io v1alpha2.coordination.k8s.io v1alpha3.resource.k8s.io v1beta1.admissionregistration.k8s.io v1beta1.networking.k8s.io v1beta1.resource.k8s.io v1beta1.storage.k8s.io v2.autoscaling]\n[-]poststarthook/start-apiextensions-controllers failed: not finished\n[-]poststarthook/crd-informer-synced failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/priority-and-fairness-config-producer failed: not finished\n[-]poststarthook/bootstrap-controller failed: not finished\n[-]poststarthook/apiservice-registration-controller failed: not finished\n[-]poststarthook/apiservice-discovery-controller failed: not finished\n[-]autoregister-completion failed: missing APIService: [v1. v1.admissionregistration.k8s.io v1.apiextensions.k8s.io v1.apps v1.authentication.k8s.io v1.authorization.k8s.io v1.autoscaling v1.batch v1.certificates.k8s.io v1.coordination.k8s.io v1.discovery.k8s.io v1.events.k8s.io v1.flowcontrol.apiserver.k8s.io v1.networking.k8s.io v1.node.k8s.io v1.policy v1.rbac.authorization.k8s.io v1.scheduling.k8s.io v1.storage.k8s.io v1alpha1.admissionregistration.k8s.io v1alpha1.internal.apiserver.k8s.io v1alpha1.storage.k8s.io v1alpha2.coordination.k8s.io v1alpha3.resource.k8s.io v1beta1.admissionregistration.k8s.io v1beta1.networking.k8s.io v1beta1.resource.k8s.io v1beta1.storage.k8s.io v2.autoscaling]\n[-]poststarthook/start-apiextensions-controllers failed: not finished\n[-]poststarthook/crd-informer-synced failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/priority-and-fairness-config-producer failed: not finished\n[-]autoregister-completion failed: missing APIService: [v1. v1.admissionregistration.k8s.io v1.apiextensions.k8s.io v1.apps v1.authentication.k8s.io v1.authorization.k8s.io v1.autoscaling v1.batch v1.certificates.k8s.io v1.coordination.k8s.io v1.discovery.k8s.io v1.events.k8s.io v1.flowcontrol.apiserver.k8s.io v1.networking.k8s.io v1.node.k8s.io v1.policy v1.rbac.authorization.k8s.io v1.scheduling.k8s.io v1.storage.k8s.io v1alpha1.admissionregistration.k8s.io v1alpha1.internal.apiserver.k8s.io v1alpha1.storage.k8s.io v1alpha2.coordination.k8s.io v1alpha3.resource.k8s.io v1beta1.admissionregistration.k8s.io v1beta1.networking.k8s.io v1beta1.resource.k8s.io v1beta1.storage.k8s.io v2.autoscaling]\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/priority-and-fairness-config-producer failed: not finished\n[-]autoregister-completion failed: missing APIService: [v1. v1.admissionregistration.k8s.io v1.apiextensions.k8s.io v1.apps v1.authentication.k8s.io v1.authorization.k8s.io v1.autoscaling v1.batch v1.certificates.k8s.io v1.coordination.k8s.io v1.discovery.k8s.io v1.events.k8s.io v1.flowcontrol.apiserver.k8s.io v1.networking.k8s.io v1.node.k8s.io v1.policy v1.rbac.authorization.k8s.io v1.scheduling.k8s.io v1.storage.k8s.io v1alpha1.admissionregistration.k8s.io v1alpha1.internal.apiserver.k8s.io v1alpha1.storage.k8s.io v1alpha2.coordination.k8s.io v1alpha3.resource.k8s.io v1beta1.admissionregistration.k8s.io v1beta1.networking.k8s.io v1beta1.resource.k8s.io v1beta1.storage.k8s.io v2.autoscaling]\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/priority-and-fairness-config-producer failed: not finished\n[-]autoregister-completion failed: missing APIService: [v1. v1.admissionregistration.k8s.io v1.apiextensions.k8s.io v1.apps v1.authentication.k8s.io v1.authorization.k8s.io v1.autoscaling v1.batch v1.certificates.k8s.io v1.coordination.k8s.io v1.discovery.k8s.io v1.events.k8s.io v1.flowcontrol.apiserver.k8s.io v1.networking.k8s.io v1.node.k8s.io v1.policy v1.rbac.authorization.k8s.io v1.scheduling.k8s.io v1.storage.k8s.io v1alpha1.admissionregistration.k8s.io v1alpha1.internal.apiserver.k8s.io v1alpha1.storage.k8s.io v1alpha2.coordination.k8s.io v1alpha3.resource.k8s.io v1beta1.admissionregistration.k8s.io v1beta1.networking.k8s.io v1beta1.resource.k8s.io v1beta1.storage.k8s.io v2.autoscaling]\n  \tType:               \"Dangling\",\n- \tStatus:             \"\",\n+ \tStatus:             \"False\",\n- \tLastTransitionTime: v1.Time{},\n+ \tLastTransitionTime: v1.Time{Time: s\"2025-01-21 19:59:28.22697537 +0000 UTC m=+2.405278625\"},\n- \tReason:             \"\",\n+ \tReason:             \"Found\",\n- \tMessage:            \"\",\n+ \tMessage:            `This FlowSchema references the PriorityLevelConfiguration object named \"system\" and it exists`,\n  }\n  \tType:               \"Dangling\",\n- \tStatus:             \"\",\n+ \tStatus:             \"False\",\n- \tLastTransitionTime: v1.Time{},\n+ \tLastTransitionTime: v1.Time{Time: s\"2025-01-21 19:59:28.232527805 +0000 UTC m=+2.410831090\"},\n- \tReason:             \"\",\n+ \tReason:             \"Found\",\n- \tMessage:            \"\",\n+ \tMessage:            `This FlowSchema references the PriorityLevelConfiguration object named \"node-high\" and it exists`,\n  }\n  \tType:               \"Dangling\",\n- \tStatus:             \"\",\n+ \tStatus:             \"True\",\n- \tLastTransitionTime: v1.Time{},\n+ \tLastTransitionTime: v1.Time{Time: s\"2025-01-21 19:59:28.236808066 +0000 UTC m=+2.415111321\"},\n- \tReason:             \"\",\n+ \tReason:             \"NotFound\",\n- \tMessage:            \"\",\n+ \tMessage:            `This FlowSchema references the PriorityLevelConfiguration object named \"exempt\" but there is no such object`,\n  }\n  \tType:               \"Dangling\",\n- \tStatus:             \"\",\n+ \tStatus:             \"False\",\n- \tLastTransitionTime: v1.Time{},\n+ \tLastTransitionTime: v1.Time{Time: s\"2025-01-21 19:59:28.236810496 +0000 UTC m=+2.415113761\"},\n- \tReason:             \"\",\n+ \tReason:             \"Found\",\n- \tMessage:            \"\",\n+ \tMessage:            `This FlowSchema references the PriorityLevelConfiguration object named \"leader-election\" and it exists`,\n  }\n  \tType:               \"Dangling\",\n- \tStatus:             \"\",\n+ \tStatus:             \"False\",\n- \tLastTransitionTime: v1.Time{},\n+ \tLastTransitionTime: v1.Time{Time: s\"2025-01-21 19:59:28.244349304 +0000 UTC m=+2.422652559\"},\n- \tReason:             \"\",\n+ \tReason:             \"Found\",\n- \tMessage:            \"\",\n+ \tMessage:            `This FlowSchema references the PriorityLevelConfiguration object named \"leader-election\" and it exists`,\n  }\n  \tType:               \"Dangling\",\n- \tStatus:             \"\",\n+ \tStatus:             \"False\",\n- \tLastTransitionTime: v1.Time{},\n+ \tLastTransitionTime: v1.Time{Time: s\"2025-01-21 19:59:28.244351044 +0000 UTC m=+2.422654299\"},\n- \tReason:             \"\",\n+ \tReason:             \"Found\",\n- \tMessage:            \"\",\n+ \tMessage:            `This FlowSchema references the PriorityLevelConfiguration object named \"workload-high\" and it exists`,\n  }\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/priority-and-fairness-config-producer failed: not finished\n  \tType:               \"Dangling\",\n- \tStatus:             \"\",\n+ \tStatus:             \"False\",\n- \tLastTransitionTime: v1.Time{},\n+ \tLastTransitionTime: v1.Time{Time: s\"2025-01-21 19:59:28.253019695 +0000 UTC m=+2.431322950\"},\n- \tReason:             \"\",\n+ \tReason:             \"Found\",\n- \tMessage:            \"\",\n+ \tMessage:            `This FlowSchema references the PriorityLevelConfiguration object named \"workload-high\" and it exists`,\n  }\n  \tType:               \"Dangling\",\n- \tStatus:             \"\",\n+ \tStatus:             \"False\",\n- \tLastTransitionTime: v1.Time{},\n+ \tLastTransitionTime: v1.Time{Time: s\"2025-01-21 19:59:28.253021215 +0000 UTC m=+2.431324470\"},\n- \tReason:             \"\",\n+ \tReason:             \"Found\",\n- \tMessage:            \"\",\n+ \tMessage:            `This FlowSchema references the PriorityLevelConfiguration object named \"workload-high\" and it exists`,\n  }\n  \tType:               \"Dangling\",\n- \tStatus:             \"\",\n+ \tStatus:             \"False\",\n- \tLastTransitionTime: v1.Time{},\n+ \tLastTransitionTime: v1.Time{Time: s\"2025-01-21 19:59:28.253025715 +0000 UTC m=+2.431328970\"},\n- \tReason:             \"\",\n+ \tReason:             \"Found\",\n- \tMessage:            \"\",\n+ \tMessage:            `This FlowSchema references the PriorityLevelConfiguration object named \"workload-high\" and it exists`,\n  }\n  \tType:               \"Dangling\",\n- \tStatus:             \"\",\n+ \tStatus:             \"False\",\n- \tLastTransitionTime: v1.Time{},\n+ \tLastTransitionTime: v1.Time{Time: s\"2025-01-21 19:59:28.266023747 +0000 UTC m=+2.444327022\"},\n- \tReason:             \"\",\n+ \tReason:             \"Found\",\n- \tMessage:            \"\",\n+ \tMessage:            `This FlowSchema references the PriorityLevelConfiguration object named \"exempt\" and it exists`,\n  }\n  \tType:               \"Dangling\",\n- \tStatus:             \"\",\n+ \tStatus:             \"False\",\n- \tLastTransitionTime: v1.Time{},\n+ \tLastTransitionTime: v1.Time{Time: s\"2025-01-21 19:59:28.266032617 +0000 UTC m=+2.444335882\"},\n- \tReason:             \"\",\n+ \tReason:             \"Found\",\n- \tMessage:            \"\",\n+ \tMessage:            `This FlowSchema references the PriorityLevelConfiguration object named \"global-default\" and it exists`,\n  }\n  \tType:               \"Dangling\",\n- \tStatus:             \"\",\n+ \tStatus:             \"False\",\n- \tLastTransitionTime: v1.Time{},\n+ \tLastTransitionTime: v1.Time{Time: s\"2025-01-21 19:59:28.266040247 +0000 UTC m=+2.444343502\"},\n- \tReason:             \"\",\n+ \tReason:             \"Found\",\n- \tMessage:            \"\",\n+ \tMessage:            `This FlowSchema references the PriorityLevelConfiguration object named \"workload-low\" and it exists`,\n  }\n  \tType:               \"Dangling\",\n- \tStatus:             \"True\",\n+ \tStatus:             \"False\",\n- \tLastTransitionTime: v1.Time{Time: s\"2025-01-21 19:59:28 +0000 UTC\"},\n+ \tLastTransitionTime: v1.Time{Time: s\"2025-01-21 19:59:28.266050447 +0000 UTC m=+2.444353702\"},\n- \tReason:             \"NotFound\",\n+ \tReason:             \"Found\",\n  \tMessage: strings.Join({\n  \t\t\"This FlowSchema references the PriorityLevelConfiguration object\",\n  \t\t` named \"exempt\" `,\n- \t\t\"but there is no such object\",\n+ \t\t\"and it exists\",\n  \t}, \"\"),\n  }\n  \tType:               \"Dangling\",\n- \tStatus:             \"\",\n+ \tStatus:             \"False\",\n- \tLastTransitionTime: v1.Time{},\n+ \tLastTransitionTime: v1.Time{Time: s\"2025-01-21 19:59:28.284227301 +0000 UTC m=+2.462530576\"},\n- \tReason:             \"\",\n+ \tReason:             \"Found\",\n- \tMessage:            \"\",\n+ \tMessage:            `This FlowSchema references the PriorityLevelConfiguration object named \"catch-all\" and it exists`,\n  }\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\nleases.coordination.k8s.io \"leader-election-controller\" not found\nleases.coordination.k8s.io \"leader-election-controller\" not found\n    testserver.go:582: Resolved testserver package path to: \"/home/prow/go/src/k8s.io/kubernetes/cmd/kube-apiserver/app/testing\"\n    testserver.go:402: runtime-config=map[api/all:true]\n    testserver.go:403: Starting kube-apiserver on port 42539...\n    testserver.go:438: Waiting for /healthz to be ok...\n[-]poststarthook/start-apiextensions-controllers failed: not finished\n[-]poststarthook/crd-informer-synced failed: not finished\n[-]poststarthook/start-service-ip-repair-controllers failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/priority-and-fairness-config-producer failed: not finished\n[-]poststarthook/bootstrap-controller failed: not finished\n[-]poststarthook/apiservice-registration-controller failed: not finished\n[-]poststarthook/apiservice-discovery-controller failed: not finished\n[-]autoregister-completion failed: missing APIService: [v1. v1.admissionregistration.k8s.io v1.apiextensions.k8s.io v1.apps v1.authentication.k8s.io v1.authorization.k8s.io v1.autoscaling v1.batch v1.certificates.k8s.io v1.coordination.k8s.io v1.discovery.k8s.io v1.events.k8s.io v1.flowcontrol.apiserver.k8s.io v1.networking.k8s.io v1.node.k8s.io v1.policy v1.rbac.authorization.k8s.io v1.scheduling.k8s.io v1.storage.k8s.io v1alpha1.admissionregistration.k8s.io v1alpha1.internal.apiserver.k8s.io v1alpha1.storage.k8s.io v1alpha2.coordination.k8s.io v1alpha3.resource.k8s.io v1beta1.admissionregistration.k8s.io v1beta1.networking.k8s.io v1beta1.resource.k8s.io v1beta1.storage.k8s.io v2.autoscaling]\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/priority-and-fairness-config-producer failed: not finished\n[-]poststarthook/bootstrap-controller failed: not finished\n[-]poststarthook/apiservice-registration-controller failed: not finished\n[-]poststarthook/apiservice-discovery-controller failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/priority-and-fairness-config-producer failed: not finished\n[-]poststarthook/apiservice-registration-controller failed: not finished\n[-]poststarthook/apiservice-discovery-controller failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: not finished\nclient rate limiter Wait returned an error: context deadline exceeded\n    leaderelection_test.go:162: Expected the cle lease lock to transition to the first apiserver\n--- FAIL: TestCoordinatedLeaderElectionLeaseTransfer (25.47s)\n}\n```\n\n### Anything else we need to know?\n\nN/A\n\n### Relevant SIG(s)\n\n/sig api-machinery",
      "solution": "No failures in testgrid since Feb 11. Appears to be fixed, will wait to close for confirmation.",
      "labels": [
        "sig/api-machinery",
        "kind/flake",
        "needs-triage"
      ],
      "created_at": "2025-01-24T11:55:09Z",
      "closed_at": "2025-07-08T21:33:28Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/129802",
      "comments_count": 13
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 136335,
      "title": "[Scheduler] Add a Pod Condition when PreEnqueue fails",
      "problem": "### What would you like to be added?\n\nWhen a Pod fails PreEnqueue, kube-scheduler should (optionally / gated) write a Pod status.conditions[] entry indicating:\n\n- the Pod is currently not schedulable / not eligible (as seen by the scheduler at PreEnqueue stage)\n- a short reason (plugin-reported reason or aggregated reason)\n- a message with human-readable details\n- timestamps for last transition / probe\n\n\n### Why is this needed?\n\nToday, when a Pod is rejected in the scheduler PreEnqueue stage (i.e., the scheduling cycle does not start / Pod is not enqueued), users often get little or no direct, Pod-level feedback explaining why the Pod is not being considered for scheduling. This can lead to \u201csilent\u201d scheduling stalls where the Pod stays Pending without an actionable reason.\n\nIt would be helpful if kube-scheduler could surface a standardized Pod condition when PreEnqueue fails, so users and controllers can quickly understand that the Pod is currently not eligible to be queued for scheduling, and why.",
      "solution": "/close\n\n@KunWuLuan Let me close the issue as duplicated with -\nhttps://github.com/kubernetes/kubernetes/issues/130668\nThis change requires KEP anyway. So, we cannot merge your PR too.\n\n---\n\n@sanposhiho: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/136335#issuecomment-3772102571):\n\n>/close\n>\n>@KunWuLuan Let me close the issue as duplicated with -\n>https://github.com/kubernetes/kubernetes/issues/130668\n>This change requires KEP anyway. So, we cannot merge your PR too.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>",
      "labels": [
        "kind/feature",
        "needs-sig",
        "needs-triage"
      ],
      "created_at": "2026-01-20T09:27:39Z",
      "closed_at": "2026-01-20T10:17:26Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/136335",
      "comments_count": 6
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 106361,
      "title": "Scheduler will run into race conditions on large scale clusters",
      "problem": "### What happened?\n\nThe scheduler has a [30s timeout](https://github.com/kubernetes/kubernetes/blob/0fefe4e605ba05793e919598c09390769c127b3c/pkg/scheduler/scheduler.go#L231) for the bind operation to succeed; if we don't get a response within 30s, the in-memory assignment of pod to node in the [scheduler cache expires](https://github.com/kubernetes/kubernetes/blob/0fefe4e605ba05793e919598c09390769c127b3c/pkg/scheduler/internal/cache/cache.go#L720).\r\n\r\nA race condition will happen in the follow case:\r\n\r\n1. pod1 is assigned to a node, scheduler cache is updated with the assignment, bind operation issued to apiserver.\r\n\r\n1. if the apiserver is under huge pressure, bind takes more than 30s, scheduler expires the cached pod-to-node assignment.\r\n\r\n1. bind eventually succeeds, but because the apiserver is under huge pressure, the pod update with the node name takes a long time to propagate to the scheduler.\r\n\r\n1. because the pod update took a long time to propagate and the cache entry expired, the scheduler is not aware that the assignment actually happened, and so it had no problem assigning a second pod to the same node that would otherwise not fit if the scheduler was aware that the first pod was eventually assigned to the node.\r\n\r\nOn the scheduler side, what we need to do is make the 30s longer for large clusters, and ideally adaptable to cluster state.\r\n\r\n/sig scheduling\n\n### What did you expect to happen?\n\nNo race conditions.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nCreate a large scale cluster.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\n# paste output here\r\n```\r\n\r\n</details>\r\n\n\n### Cloud provider\n\n<details>\r\n\r\n</details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n</details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n</details>\r\n\n\n### Container runtime (CRI) and and version (if applicable)\n\n<details>\r\n\r\n</details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n</details>\r\n",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/bug",
        "sig/scheduling",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2021-11-11T18:09:08Z",
      "closed_at": "2024-01-20T04:11:51Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/106361",
      "comments_count": 30
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 133662,
      "title": "CronJob status should keep track of lastFailureTime",
      "problem": "### What would you like to be added?\n\nThe suggestion is to add a `lastFailureTime` to mimics the `lastSuccessfulTime`, that is updated when a job has the JobFailure condition.\n\n#### Alternative\n\nAn alternative would be to have Conditions for the CronJob that gives more information on the last job result, and for which generation the job was created, but this seems like a bigger refactor.\n\n### Why is this needed?\n\nIt is currently hard to know if a CronJob last execution was successful or a failure. We can usually check for `if status.lastSuccessfulTime < status.lastScheduledTime`, then assume it is a failure, but as soon as a new Job is scheduled, we lose the information to know if the last one failed or not.",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/feature",
        "sig/apps",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2025-08-22T20:02:35Z",
      "closed_at": "2026-01-19T20:58:37Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/133662",
      "comments_count": 6
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 134141,
      "title": "Sig-node-cri-o Serial PSI metrics tests failing for Cgroupv2 on Fedora Coreos",
      "problem": "### Which jobs are failing?\n\npr-node-kubelet-serial-crio-cgroupv2: [testgrid](https://testgrid.k8s.io/sig-node-cri-o#pr-node-kubelet-serial-crio-cgroupv2)\nprowjob_name: `pull-kubernetes-node-kubelet-serial-crio-cgroupv2` \nprowjob_config_url: https://git.k8s.io/test-infra/config/jobs/kubernetes/sig-node/sig-node-presubmit.yaml\n\n### Which tests are failing?\n\nPSI Summary API test\npull-kubernetes-node-kubelet-serial-crio-cgroupv2 Overall\n\n### Since when has it been failing?\n\nsince 9/05/2025 from what the testgrid shows, not sure how far back past that date.\n\n### Testgrid link\n\nhttps://testgrid.k8s.io/sig-node-cri-o#pr-node-kubelet-serial-crio-cgroupv2\n\n### Reason for failure (if possible)\n\nFrom my conversation with @kannon92 : \n- this test is only failing on Fedora OS; it does not fail on Ubuntu or Containerd\n- this could be a kernel or OS issue we are running into\n- kubernetes-ci-logs artifacts: https://gcsweb.k8s.io/gcs/kubernetes-ci-logs/logs/ci-kubernetes-node-kubelet-serial-containerd/1968708022570061824/artifacts/\n\n### Anything else we need to know?\n\nMe and RedHat folks are looking into this! \n\n### Relevant SIG(s)\n\n/sig node",
      "solution": "> [@pacoxu](https://github.com/pacoxu) This test was run in https://testgrid.k8s.io/sig-node-kubelet#kubelet-gce-e2e-swap-ubuntu-serial\n> Also how does the above error message impact the test?\n\nThe PSI info is provided by cadvisor and here are the only logs that are related to cadvisor. And I think the log seems to be OK and no evidence of the root cause found yet.\n\n\n```\nI0922 02:32:28.883253   37048 httplog.go:134] \"HTTP\" verb=\"GET\" URI=\"/pods\" latency=\"357.121\u00b5s\" userAgent=\"e2e_node.test/v1.35.0 (linux/amd64) kubernetes/0b6dba8 -- [sig-node] Summary API [NodeConformance] when querying /stats/summary under pressure [Feature:KubeletPSI] [Serial] should report Memory pressure in PSI metrics\" audit-ID=\"\" srcIP=\"10.128.0.90:57540\" resp=200\nI0922 02:32:28.902355   37048 httplog.go:134] \"HTTP\" verb=\"GET\" URI=\"/metrics\" latency=\"11.54577ms\" userAgent=\"e2e_node.test/v1.35.0 (linux/amd64) kubernetes/0b6dba8 -- [sig-node] Summary API [NodeConformance] when querying /stats/summary under pressure [Feature:KubeletPSI] [Serial] should report Memory pressure in PSI metrics\" audit-ID=\"\" srcIP=\"10.128.0.90:57540\" resp=200\n``` \n\nhttps://github.com/kubernetes/kubernetes/blob/4e9059fecaa2f47bf0f5b0fdac94cdfb69f5629e/vendor/github.com/opencontainers/cgroups/fs2/fs2.go#L116-L126\n\nI think the current log may be not enough.\n\n---\n\n@SergeyKanzhelev Sorry for the confusion, I thought https://github.com/kubernetes/kubernetes/pull/135488 fixed the issue. I'll discuss with @saschagrunert once he's back from holidays and update here.\n\n---\n\nThere is now also a workaround in CRI-O: https://github.com/cri-o/cri-o/pull/9714\nLooks like that there needs to be clarification on the Fedora CoreOS side: https://github.com/coreos/fedora-coreos-tracker/issues/2094",
      "labels": [
        "sig/node",
        "kind/failing-test",
        "triage/accepted"
      ],
      "created_at": "2025-09-18T21:45:17Z",
      "closed_at": "2026-01-17T03:57:17Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/134141",
      "comments_count": 22
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 134939,
      "title": "A potential goroutine leak in pkg/kubelet/pluginmanager/operationexecutor/operation_executor_test.go",
      "problem": "### What happened?\n\nIf the `time.After` case is selected before the `case <-ch:`\nhttps://github.com/kubernetes/kubernetes/blob/b6e7ee359d0e13f81cd8594820e6ec71bec3c78a/pkg/kubelet/pluginmanager/operationexecutor/operation_executor_test.go#L164-L175\nwhich leads to the ch <- nil below get blocked, results in a goroutine leak.\nhttps://github.com/kubernetes/kubernetes/blob/b6e7ee359d0e13f81cd8594820e6ec71bec3c78a/pkg/kubelet/pluginmanager/operationexecutor/operation_executor_test.go#L188\n\n\n### What did you expect to happen?\n\nIf the `time.After` case is selected before the `case <-ch:`, the ch <- nil doesn't get blocked either.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n1.Set time.After(5 * time.Second) to time.After(time.Millisecond * 100)\n2.Add time.Sleep(time.Millisecond * 200) before the `ch <- nil`\nhttps://github.com/kubernetes/kubernetes/blob/b6e7ee359d0e13f81cd8594820e6ec71bec3c78a/pkg/kubelet/pluginmanager/operationexecutor/operation_executor_test.go#L188\nThe two steps make the time.After happen before the `case <-ch:`.\n3.Using goleak to detect the goroutine leak in the test function\nhttps://github.com/kubernetes/kubernetes/blob/b6e7ee359d0e13f81cd8594820e6ec71bec3c78a/pkg/kubelet/pluginmanager/operationexecutor/operation_executor_test.go#L50\n\n```\nfunc TestOperationExecutor_UnregisterPlugin_ConcurrentUnregisterPlugin(t *testing.T) {\n\tdefer goleak.VerifyNone(t)\n```\n\nThe error report is as follows,\n```\nfound unexpected goroutines:\n        [Goroutine 7 in state chan send, with k8s.io/kubernetes/pkg/kubelet/pluginmanager/operationexecutor.startOperationAndBlock on top of the stack:\n        goroutine 7 [chan send]:\n        k8s.io/kubernetes/pkg/kubelet/pluginmanager/operationexecutor.startOperationAndBlock(0x517d34?, 0x0?)\n        \t/home/song2048/\u684c\u9762/goProject/src/GoPie/testdata/realProjects/k8s.io/kubernetes/kubernetes/pkg/kubelet/pluginmanager/operationexecutor/operation_executor_test.go:183 +0x4c\n        k8s.io/kubernetes/pkg/kubelet/pluginmanager/operationexecutor.(*fakeOperationGenerator).GenerateUnregisterPluginFunc.func1()\n        \t/home/song2048/\u684c\u9762/goProject/src/GoPie/testdata/realProjects/k8s.io/kubernetes/kubernetes/pkg/kubelet/pluginmanager/operationexecutor/operation_executor_test.go:124 +0x25\n        k8s.io/kubernetes/pkg/util/goroutinemap.(*goRoutineMap).Run.func1()\n        \t/home/song2048/\u684c\u9762/goProject/src/GoPie/testdata/realProjects/k8s.io/kubernetes/kubernetes/pkg/util/goroutinemap/goroutinemap.go:119 +0xc4\n        created by k8s.io/kubernetes/pkg/util/goroutinemap.(*goRoutineMap).Run\n        \t/home/song2048/\u684c\u9762/goProject/src/GoPie/testdata/realProjects/k8s.io/kubernetes/kubernetes/pkg/util/goroutinemap/goroutinemap.go:112 +0x4d8\n        \n         Goroutine 8 in state chan send, with k8s.io/kubernetes/pkg/kubelet/pluginmanager/operationexecutor.startOperationAndBlock on top of the stack:\n        goroutine 8 [chan send]:\n        k8s.io/kubernetes/pkg/kubelet/pluginmanager/operationexecutor.startOperationAndBlock(0x0?, 0x0?)\n        \t/home/song2048/\u684c\u9762/goProject/src/GoPie/testdata/realProjects/k8s.io/kubernetes/kubernetes/pkg/kubelet/pluginmanager/operationexecutor/operation_executor_test.go:183 +0x4c\n        k8s.io/kubernetes/pkg/kubelet/pluginmanager/operationexecutor.(*fakeOperationGenerator).GenerateUnregisterPluginFunc.func1()\n        \t/home/song2048/\u684c\u9762/goProject/src/GoPie/testdata/realProjects/k8s.io/kubernetes/kubernetes/pkg/kubelet/pluginmanager/operationexecutor/operation_executor_test.go:124 +0x25\n        k8s.io/kubernetes/pkg/util/goroutinemap.(*goRoutineMap).Run.func1()\n        \t/home/song2048/\u684c\u9762/goProject/src/GoPie/testdata/realProjects/k8s.io/kubernetes/kubernetes/pkg/util/goroutinemap/goroutinemap.go:119 +0xc4\n        created by k8s.io/kubernetes/pkg/util/goroutinemap.(*goRoutineMap).Run\n        \t/home/song2048/\u684c\u9762/goProject/src/GoPie/testdata/realProjects/k8s.io/kubernetes/kubernetes/pkg/util/goroutinemap/goroutinemap.go:112 +0x4d8\n        ]\n--- FAIL: TestOperationExecutor_UnregisterPlugin_ConcurrentUnregisterPlugin\n```\n\n\n\n### Anything else we need to know?\n\nHow to fix the bug:\nhttps://github.com/kubernetes/kubernetes/blob/b6e7ee359d0e13f81cd8594820e6ec71bec3c78a/pkg/kubelet/pluginmanager/operationexecutor/operation_executor_test.go#L181\nChange the unbufferd channel to a buffered channel. So that the `ch <- nil` can't get blocked in any case.\n`make(chan interface{}, 1)`\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# paste output here\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n",
      "solution": "@SergeyKanzhelev: \n\tThis request has been marked as needing help from a contributor.\n\n### Guidelines\nPlease ensure that the issue body includes answers to the following questions:\n- Why are we solving this issue?\n- To address this issue, are there any code changes? If there are code changes, what needs to be done in the code and what places can the assignee treat as reference points?\n- How can the assignee reach out to you for help?\n\n\nFor more details on the requirements of such an issue, please see [here](https://www.kubernetes.dev/docs/guide/help-wanted/) and ensure that they are met.\n\nIf this request no longer meets these requirements, the label can be removed\nby commenting with the `/remove-help` command.\n\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/134939):\n\n>/help\n>\n>bug description is easy enough.\n>\n>/area test\n>\n>seems like more of a test issue than real product issue\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>",
      "labels": [
        "kind/bug",
        "area/test",
        "priority/backlog",
        "sig/node",
        "help wanted",
        "triage/accepted"
      ],
      "created_at": "2025-10-29T07:18:55Z",
      "closed_at": "2026-01-17T01:29:17Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/134939",
      "comments_count": 10
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 136222,
      "title": "kubectl v1.35.0 with attached tty and stdin panics while redirecting to stdout",
      "problem": "### What happened?\n\nWith kubectl v1.32.5 one can redirect the kubectl output to file even though tty and stdin is attached.\nI think the program should detect if interactive terminal is present and not fail terribly.\n\nCurrently the kubectl fails on SIGSEV and also breaks terminal.\n\n\n\n### What did you expect to happen?\n\nCorrectly store the stdout into file or fail with reasonable error message.\n\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nOn any running pod containing `echo` run\n\n```\nkubectl exec -ti pod-name -- echo \"test\" > /tmp/test\n```\n\n\n\n\n### Anything else we need to know?\n\nThe command fails with SIGSEV, breaks terminal with scattered output like\n\n```\nE0114 09:48:11.073044   12479 runtime.go:142] \"Observed a panic\" panic=\"runtime error: invalid memory address or nil pointer dereference\" panicGoValue=\"\\\"invalid memory address or nil pointer dereference\\\"\" stacktrace=<\n         \tgoroutine 48 [running]:\n                                       \tk8s.io/apimachinery/pkg/util/runtime.logPanic({0x56001fec3068, 0x5600211350e0}, {0x56001faeb780, 0x56002100d1f0})\n                                                                                                                                                         \t\t/build/kubernetes/src/kubernetes-1.35.0/staging/src/k8s.io/apimachinery/pkg/util/runtime/runtime.go:132 +0x12b\n                                                                    \tk8s.io/apimachinery/pkg/util/runtime.handleCrash({0x56001fec3068, 0x5600211350e0}, {0x56001faeb780, 0x56002100d1f0}, {0x5600211350e0, 0x0, 0x0})\n      \t\t/build/kubernetes/src/kubernetes-1.35.0/staging/src/k8s.io/apimachinery/pkg/util/runtime/runtime.go:107 +0x13d\n                                                                                                                              \tk8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0x0})\n                                                                                                                                                                                                 \t\t/build/kubernetes/src/kubernetes-1.35.0/staging/src/k8s.io/apimachinery/pkg/util/runtime/runtime.go:64 +0x1ff\n                                                                                                           \tpanic({0x56001faeb780?, 0x56002100d1f0?})\n                                                                                                                                                         \t\t/usr/lib/go/src/runtime/panic.go:783 +0x136\n \tk8s.io/kubectl/pkg/cmd/exec.(*terminalSizeQueueAdapter).Next(0xc0009a4fa0)\n                                                                                  \t\t/build/kubernetes/src/kubernetes-1.35.0/staging/src/k8s.io/kubectl/pkg/cmd/exec/exec.go:414 +0x2a\n                                                                                                                                                                                                 \tk8s.io/client-go/tools/remotecommand.(*streamProtocolV3).handleResizes.func1()\n                                                                    \t\t/build/kubernetes/src/kubernetes-1.35.0/staging/src/k8s.io/client-go/tools/remotecommand/v3.go:74 +0xa2\n                                                                                                                                                                                       \tcreated by k8s.io/client-go/tools/remotecommand.(*streamProtocolV3).handleResizes in goroutine 45\n                                                                       \t\t/build/kubernetes/src/kubernetes-1.35.0/staging/src/k8s.io/client-go/tools/remotecommand/v3.go:69 +0x9c\n                                                                                                                                                                                        >\n                                                                                                                                                                                         panic: runtime error: invalid memory address or nil pointer dereference [recovered, repanicked]\n                                                                      [signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x56001f0f7eaa]\n\n                                                                                                                                                  goroutine 48 [running]:\n                                                                                                                                                                         k8s.io/apimachinery/pkg/util/runtime.handleCrash({0x56001fec3068, 0x5600211350e0}, {0x56001faeb780, 0x56002100d1f0}, {0x5600211350e0, 0x0, 0x0})\n                                                                                                       \t/build/kubernetes/src/kubernetes-1.35.0/staging/src/k8s.io/apimachinery/pkg/util/runtime/runtime.go:114 +0x1fd\n    k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0x0})\n                                                                     \t/build/kubernetes/src/kubernetes-1.35.0/staging/src/k8s.io/apimachinery/pkg/util/runtime/runtime.go:64 +0x1ff\n                                                                                                                                                                                     panic({0x56001faeb780?, 0x56002100d1f0?})\n            \t/usr/lib/go/src/runtime/panic.go:783 +0x136\n                                                           k8s.io/kubectl/pkg/cmd/exec.(*terminalSizeQueueAdapter).Next(0xc0009a4fa0)\n                                                                                                                                     \t/build/kubernetes/src/kubernetes-1.35.0/staging/src/k8s.io/kubectl/pkg/cmd/exec/exec.go:414 +0x2a\n                       k8s.io/client-go/tools/remotecommand.(*streamProtocolV3).handleResizes.func1()\n                                                                                                     \t/build/kubernetes/src/kubernetes-1.35.0/staging/src/k8s.io/client-go/tools/remotecommand/v3.go:74 +0xa2\n                                                                                                                                                                                                               created by k8s.io/client-go/tools/remotecommand.(*streamProtocolV3).handleResizes in goroutine 45\n                                                                                              \t/build/kubernetes/src/kubernetes-1.35.0/staging/src/k8s.io/client-go/tools/remotecommand/v3.go:69 +0x9c\n```\n\nTested in kitty terminal.\n\n### Kubernetes version\n\n<details>\n\n```console\nClient Version: v1.35.0\nKustomize Version: v5.7.1\nServer Version: v1.34.1\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nAKS\n</details>\n\n\n### OS version\n\nArchlinux but also tested on latest Fedora\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\nNAME=\"Arch Linux\"\nPRETTY_NAME=\"Arch Linux\"\nID=arch\nBUILD_ID=rolling\nANSI_COLOR=\"38;2;23;147;209\"\nHOME_URL=\"https://archlinux.org/\"\nDOCUMENTATION_URL=\"https://wiki.archlinux.org/\"\nSUPPORT_URL=\"https://bbs.archlinux.org/\"\nBUG_REPORT_URL=\"https://gitlab.archlinux.org/groups/archlinux/-/issues\"\nPRIVACY_POLICY_URL=\"https://terms.archlinux.org/docs/privacy-policy/\"\nLOGO=archlinux-logo\n$ uname -a\nLinux studeny-arch 6.18.5-arch1-1 #1 SMP PREEMPT_DYNAMIC Sun, 11 Jan 2026 17:10:53 +0000 x86_64 GNU/Linux\n```\n\n</details>\n\n\n\n\n### Install tools\n\n<details>\npacman\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n",
      "solution": "This was fixed in https://github.com/kubernetes/kubernetes/pull/135918 and cherry-pick will be opened, it's not https://github.com/kubernetes/kubernetes/pull/136223 because that PR didn't follow [the process](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-release/cherry-picks.md). Given that master is fixed, I'm going to close this issue. \n\n/close \n\n---\n\n@soltysh: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/136222#issuecomment-3759702886):\n\n>This was fixed in https://github.com/kubernetes/kubernetes/pull/135918 and cherry-pick will be opened, it's not https://github.com/kubernetes/kubernetes/pull/136223 because that PR didn't follow [the process](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-release/cherry-picks.md). Given that master is fixed, I'm going to close this issue. \n>\n>/close \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>",
      "labels": [
        "kind/bug",
        "sig/cli",
        "triage/accepted"
      ],
      "created_at": "2026-01-14T09:26:31Z",
      "closed_at": "2026-01-16T11:55:21Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/136222",
      "comments_count": 6
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 135430,
      "title": "Random timeout using Tailscale VPN",
      "problem": "<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\n\nIf the matter is security related, please disclose it privately via https://kubernetes.io/security/\n-->\n\n**What happened**:\n\nHello, I'm having random timeout when using kubectl through my local machine e.g. when doing kubectl get pods. Without switching of configuration or network configuration, sometimes the command outputs stuff, sometimes juste timeout like the node isn't existing.\nMy guess is it could come from a misconfiguration of kubectl since I'm using now Tailscale VPN. It means, in order to reach the master node, my machine has to go through the Tailscale subnet router I configured on an EC2 instance in the same VPC than my EKS cluster. Which means, from the moment I'm inside the subnet router, I should be able to access the EKS cluster without further configurations.\nI would like to have your input on this one, in order to know if you think it could come from kubectl or rather AWS or Tailscale.\n\n**What you expected to happen**:\n\nHave a rapid and always valid answer from kubectl. Not having random downtimes with my node.\n\n**How to reproduce it (as minimally and precisely as possible)**:\n\n- Install kubectl v1.30.2\n- Install aws-cli/2.27.63 Python/3.13.5 Darwin/24.6.0 source/arm64\n- On AWS create a subnet router on a VPC VPC_A\n- On AWS create an EKS cluster on the same VPC VPC_A\n- Connect the subnet router to tailscale and advertise the subnet of the VPC_A. \n- Now on your local machine, connect to Tailscale in an organization containing the subnet router freshly setup\n- Connect to aws cli with 'aws sso login' (the needed setup to connect to EKS). \n- Follow this tuto https://docs.aws.amazon.com/eks/latest/userguide/create-kubeconfig.html in order to connect kubectl to the cluster.\n- Try to do 'kubectl get pods'.\n\n<!-- Please make sure you are able to reproduce the bug using a supported version and version skew of Kubernetes. See https://kubernetes.io/docs/setup/release/version-skew-policy for which versions and are currently supported.\n-->\n\n**Anything else we need to know?**:\n\nIt works sometimes so I guess it could come from an \"oscilating\" DNS configuration in my local machine. Trying to know if it could come from kubectl since I already searched for basic ~/.kube/config and .ssh/known_hosts .\n\n**Environment**:\n- Kubernetes client and server versions (use `kubectl version`): v1.30.2\n- Cloud provider or hardware configuration: EKS on AWS\n- OS (e.g: `cat /etc/os-release`): MacOs Sequoia 15.6.1\n\n",
      "solution": "Hello again, I resolved the problem by deactivating public access endpoint in EKS. It looks like they is a conflict between the two access point name resolution when both private and public access are activated.",
      "labels": [
        "kind/support",
        "needs-sig",
        "needs-triage"
      ],
      "created_at": "2025-11-24T13:53:51Z",
      "closed_at": "2026-01-15T18:52:07Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/135430",
      "comments_count": 8
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 108537,
      "title": "Static pod status is always Init:0/1 when init container GC'd before kubelet restart.",
      "problem": "### What happened?\r\n\r\nWhen init container of static pod was GC'd and kubelet was restarted. The status of the static pod is alway Init:0/1 and never come back to Running.\r\n\r\n\r\n```shell\r\n\r\nkube-controller-manager-paas-192-168-16-112     0/1     Init:0/1                0          11d     192.168.16.112   paas-192-168-16-112   <none>           <none>\r\nkube-controller-manager-paas-192-168-16-113     1/1     Running                 1          11d     192.168.16.113   paas-192-168-16-113   <none>           <none>\r\n\r\n```\r\n\r\n### What did you expect to happen?\r\n\r\nThe pod status is Running\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\n1. remove exited init contianer of one static pod.\r\n2. restart kubelet\r\n\r\n### Anything else we need to know?\r\n\r\nI noticed #96572 makes containers not restarting after init container GC'd. Non-static pod has no problem, but static seems to be affected by it.\r\n\r\n### Kubernetes version\r\n\r\n<details>\r\n\r\nv1.22.1\r\n\r\n</details>\r\n\r\n\r\n### Cloud provider\r\n\r\n<details>\r\nLocal Cluster\r\n</details>\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\n</details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/bug",
        "sig/node",
        "needs-triage"
      ],
      "created_at": "2022-03-05T07:52:45Z",
      "closed_at": "2026-01-14T18:57:40Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/108537",
      "comments_count": 20
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 135554,
      "title": "Kubelet fails to start on WSL2: \"wrong number of fields\" parsing /proc/mounts with Docker Desktop",
      "problem": "### What happened?\n\nKubelet fails to start on WSL2 when Docker Desktop is installed, with the error:\n\n\"Failed to start ContainerManager: system validation failed - wrong number of fields (expected at least 6, got 7)\"\n\nThe root cause is in the parseProcMounts() function in staging/src/k8s.io/mount-utils/mount_linux.go. Docker Desktop on WSL2 creates mount entries with Windows paths containing unescaped spaces in the mount options field.\n\n**Additional Info:**\nThe problematic mount entry from /proc/mounts:\n\nC:\\134Program\\040Files\\134Docker\\134Docker\\134resources /Docker/host 9p rw,noatime,aname=drvfs;path=C:\\Program Files\\Docker\\Docker\\resources;symlinkroot=/mnt/,cache=5,access=client,msize=65536,trans=fd,rfd=3,wfd=3 0 0\n\nThe unescaped space in \"path=C:\\Program Files\\Docker\\Docker\\resources\" causes strings.Fields() to split this into 7 fields instead of 6.\n\nVerification:\n$ cat /proc/mounts | awk '{print NF}' | sort -u\n6\n7\n\n### What did you expect to happen?\n\nKubelet should start successfully on WSL2 with Docker Desktop installed. The /proc/mounts parser should handle mount entries where the options field contains unescaped spaces.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nHow can we reproduce it (as minimally and precisely as possible)?\n\n1. Install Windows with WSL2 and Docker Desktop for Windows with WSL2 backend enabled\n2. Inside WSL2 Ubuntu, install Kubernetes components (kubeadm, kubelet, kubectl v1.34.2)\n3. Join the WSL2 node to an existing Kubernetes cluster running on a separate Linux control-plane using kubeadm join\n4. Observe kubelet fails to start with error: \"Failed to start ContainerManager: system validation failed - wrong number of fields (expected 6, got 7)\"\n\nRoot Cause:\n\nDocker Desktop mounts Windows paths in WSL2's [mounts](vscode-file://vscode-app/c:/Users/mithu/AppData/Local/Programs/Microsoft%20VS%20Code/resources/app/out/vs/code/electron-browser/workbench/workbench.html) with unescaped spaces in the options field\n\nExample problematic entry:\nC:\\134Program\\040Files\\134Docker\\134Docker\\134resources /Docker/host 9p rw,noatime,aname=drvfs;path=C:\\Program Files\\Docker\\Docker\\resources;symlinkroot=/mnt/,...\n\n\nThe options field contains path=C:\\Program Files\\Docker\\Docker\\resources with unescaped spaces\nWhen strings.Fields() parses this line, it splits on whitespace, creating 7-8 fields instead of the expected 6\n\nTo verify the problematic mount entry:\n$ cat /proc/mounts | grep \"Program Files\"\n Check field count (should be 7-8 instead of 6)\n$ cat /proc/mounts | grep \"Program Files\" | awk '{print NF}'\n\nKey Point: This issue only occurs when WSL2 with Docker Desktop is used as a Kubernetes worker node joining a cluster. Running containers directly in WSL2 works fine - the issue is specific to kubelet's mounts parsing during node initialization.\n\n### Anything else we need to know?\n\nThis affects WSL2 users who have Docker Desktop installed. The /proc/mounts format defines 6 fields (device, mountpoint, type, options, dump, pass), but the options field can contain unescaped spaces in 9p/drvfs filesystems used by WSL.\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nClient Version: v1.34.1\nKustomize Version: v5.7.1\nServer Version: v1.34.2\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nN/A - Local WSL2 environment\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\nPRETTY_NAME=\"Ubuntu 24.04.3 LTS\"\nNAME=\"Ubuntu\"\nVERSION_ID=\"24.04\"\nVERSION=\"24.04.3 LTS (Noble Numbat)\"\nVERSION_CODENAME=noble\nID=ubuntu\nID_LIKE=debian\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nUBUNTU_CODENAME=noble\nLOGO=ubuntu-logo\n$ uname -a\nLinux Workstation 6.6.87.2-microsoft-standard-WSL2 #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux\n\n\n### Install tools\n\n<details>\nkubectl v1.34.1\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\ncontainerd 1.7.28\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\nN/A - Issue occurs during kubelet's initial system validation when parsing /proc/mounts, before any plugins are loaded.\n</details>\n",
      "solution": "Thanks for the detailed analysis! You're absolutely right - according to the fstab(5) spec, spaces should be escaped as \\040 in mounts:\n\n\"The line is split into fields before being parsed. This means that any spaces or tabs within the fields must be escaped using \\040 or \\011\"\n\nSo the path path=C:\\Program Files\\Docker\\Docker\\resources should appear as path=C:\\134Program\\040Files\\134Docker\\134Docker\\134resources.\n\nThis confirms the bug is in Docker Desktop/WSL2, not kubelet.\n\nWhy users can't easily work around this:\n\nThe malformed entries come from Docker Desktop's 9p filesystem mounts that are written directly to mounts by the kernel based on how Docker sets up the mount. Users cannot:\n\n1. Edit mounts - it's a read-only virtual filesystem generated by the kernel\n2. Fix Docker's mount options - Docker Desktop internally manages the WSL2 integration mounts; there's no user-facing config to change how paths are escaped\n3. Unmount the problematic entry - these are Docker Desktop's internal mounts required for its operation\n\nThe only workarounds available are:\n\n- Don't install Docker Desktop to a path with spaces (not always possible, especially on managed corporate machines)\n- Don't use Docker Desktop's WSL2 integration (defeats the purpose)\n- Wait for Docker Desktop to fix it upstream\n\nMy suggestion:\nI believe the right path forward is:\n\n1. I'll file a bug with Docker Desktop about the improper escaping of Windows paths in their WSL2 9p mounts\n2. Close this PR since the kubelet code is technically correct per spec\n\nHowever, I still think there's value in defensive parsing (similar to how moby/sys/mountinfo handles edge cases), but I understand if maintainers prefer strict spec compliance. Happy to help either way !\n\nWould you like me to proceed with filing the Docker Desktop issue?\n\n---\n\n+1 to file the issue to the component where the bug is.\n\nadding a special parser that handles that in the kubelet is possible, but i don't think the maintainers will agree to it.",
      "labels": [
        "kind/bug",
        "sig/node",
        "sig/windows",
        "needs-triage"
      ],
      "created_at": "2025-12-02T17:24:20Z",
      "closed_at": "2026-01-14T18:54:38Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/135554",
      "comments_count": 8
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 135375,
      "title": "[Failing CI Test] node-kubelet-serial-crio",
      "problem": "### Which jobs are failing?\n\nhttps://testgrid.k8s.io/sig-node-cri-o#node-kubelet-serial-crio\n\nLogs like \"failed to reset injector because the CRI Proxy is undefined\", \"failed to run ssh command\" etc.\n\n### Which tests are failing?\n\n- ci-kubernetes-node-kubelet-serial-cri-o.Overall\n- kubetest.Node Tests\n- kubetest.Timeout[Changes (https://github.com/kubernetes/kubernetes/compare/b6e83d29f...c245b40b8?)\n\n### Since when has it been failing?\n\n11/05 is as far back as the testgrid shows\n\n### Testgrid link\n\nhttps://testgrid.k8s.io/sig-node-cri-o#node-kubelet-serial-crio\n\n### Reason for failure (if possible)\n\nFrom weekly ci meeting, @haircommander suggested it could be a Service Account issue\n\n### Anything else we need to know?\n\n_No response_\n\n### Relevant SIG(s)\n\n/sig node",
      "solution": "Could be resolved by https://github.com/kubernetes/kubernetes/pull/135369 and https://github.com/kubernetes/test-infra/pull/35946.\n\n---\n\nhttps://github.com/kubernetes/kubernetes/pull/135369 leaves two failing tests open which are WIP. Infra issues have been fixed.\n\n---\n\nThe device plugin test should get fixed with https://github.com/kubernetes/kubernetes/pull/135485",
      "labels": [
        "sig/node",
        "kind/failing-test",
        "needs-triage"
      ],
      "created_at": "2025-11-20T16:31:12Z",
      "closed_at": "2026-01-12T16:54:15Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/135375",
      "comments_count": 8
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 136210,
      "title": "Ubuntu-based pods fail to mount service account volume with runc 1.3.3 when /var/run is a symlink",
      "problem": "### What happened?\n\nWe are encountering an issue where Ubuntu-based container images fail to start because the Kubernetes service account volume cannot be mounted at:\n/var/run/secrets/kubernetes.io/serviceaccount\n\nThis issue occurs when using runc 1.3.3-0ubuntu1~22.04.3. \n\n\nIn Ubuntu images, /var/run is a symbolic link to /run by default. The pod startup fails during container initialization due to a mount error related to this symlinked path.\n\n\n> Error: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error mounting \"/var/lib/kubelet/pods/4fd0a9c2-86a9-4cfe-80ae-187e52dace2a/volumes/kubernetes.io~projected/kube-api-access-bspzq\" to rootfs at \"/var/run/secrets/kubernetes.io/serviceaccount\": create mountpoint for /var/run/secrets/kubernetes.io/serviceaccount mount: make mountpoint \"/var/run/secrets/kubernetes.io/serviceaccount\": openat2 /run/containerd/io.containerd.runtime.v2.task/k8s.io/notebook/rootfs/var/run: not a directory: unknown\n\nI guess this issue is possibly related to https://security.snyk.io/vuln/SNYK-ALPINE322-RUNC-13842568\n\n\n\n\n### What did you expect to happen?\n\nThe service account volume should be mounted successfully, as it worked with earlier versions of runc, and as /var/run being a symlink to /run is standard behavior in Ubuntu.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n1. Prepare a Kubernetes node running Ubuntu 22.04 with:\n\nrunc version 1.3.3-0ubuntu1~22.04.3\n\nA standard container runtime setup (e.g. containerd)\n\n2. Deploy a pod using an Ubuntu 20.04 container image with a service account enabled\n\n\n\nrunc --version\nrunc version 1.3.3-0ubuntu1~22.04.3\n\nkubelet --version\nKubernetes v1.35.0\n\n\n### Anything else we need to know?\n\nhttps://security.snyk.io/vuln/SNYK-ALPINE322-RUNC-13842568\n\n \n\n### Kubernetes version\n\nkubelet --version\nKubernetes v1.35.0\n\n</details>\n\n\n### Cloud provider\n\nself hosted k8s.\n\n\n### OS version\n\nPRETTY_NAME=\"Ubuntu 22.04.5 LTS\"\nNAME=\"Ubuntu\"\nVERSION_ID=\"22.04\"\nVERSION=\"22.04.5 LTS (Jammy Jellyfish)\"\nVERSION_CODENAME=jammy\nID=ubuntu\nID_LIKE=debian\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nUBUNTU_CODENAME=jammy\n\n \n\n### Container runtime (CRI) and version (if applicable)\n\nrunc --version\nrunc version 1.3.3-0ubuntu1~22.04.3\nspec: 1.2.1\ngo: go1.23.1\nlibseccomp: 2.5.3\n\ncontainerd --version\ncontainerd github.com/containerd/containerd v1.7.4 488cd77cf2010e68e043ad4f9e8835822574f67a\n\n\n\n \n",
      "solution": "This appears to be a known regression in `runc` (introduced in 1.3.3) related to recent security hardening for CVE-2024-45310. The runtime is strictly blocking symlinks like `/var/run` -> `/run`, which causes the `openat2` failure seen in the logs.\n\nSince this is an issue with the underlying container runtime binary and not Kubernetes itself, this likely needs to be resolved by an OS-level package update for `runc`.",
      "labels": [
        "kind/bug",
        "sig/node",
        "needs-triage"
      ],
      "created_at": "2026-01-14T02:40:19Z",
      "closed_at": "2026-01-14T06:10:57Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/136210",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 120063,
      "title": "CSINode object is not recreated after Node object is recreated",
      "problem": "### What happened?\r\n\r\nA preemptible Node got deleted and recreated with the same name, however during csi driver registration the CSINode object was updated while it still had an ownerReference to the old Node uid. Then the CSINode object got garbage collected and we're left with no CSINode object, causing attach/mount operations to fail.\r\n\r\nSome audit logs with a timeline of events:\r\n\r\nOld node uuid is `ef26e636-556b-4af7-ba9e-fecece154997`\r\n\r\nThe replacement Node object was created after preemption:\r\n```\r\nDEFAULT 2023-07-28T07:59:59.649269Z [protoPayload.methodName: io.k8s.core.v1.nodes.create] \r\n        \"metadata\": {\r\n          \"uid\": \"bec5b072-7eb8-4ab9-a5c7-e4242414e3e0\"\r\n        },\r\n```\r\n\r\nThere was an update to CSINode to register the driver, however the CSINode object updated has an owner reference to the old Node uid.\r\n```\r\nDEFAULT 2023-07-28T08:00:33.437252Z [protoPayload.methodName: io.k8s.storage.v1.csinodes.update] \r\n          \"ownerReferences\": [\r\n            {\r\n              \"apiVersion\": \"v1\",\r\n              \"kind\": \"Node\",\r\n              \"uid\": \"ef26e636-556b-4af7-ba9e-fecece154997\"\r\n            }\r\n        },\r\n        \"spec\": {\r\n          \"drivers\":  ....\r\n       }\r\n```\r\n\r\nThe CSINode was then garbage collected because the old Node object was deleted.\r\n```\r\nDEFAULT 2023-07-28T08:01:09.931137Z [protoPayload.methodName: io.k8s.storage.v1.csinodes.delete] [protoPayload.authenticationInfo.principalEmail: system:serviceaccount:kube-system:generic-garbage-collector] \r\n          \"ownerReferences\": [\r\n            {\r\n              \"apiVersion\": \"v1\",\r\n              \"kind\": \"Node\",\r\n              \"uid\": \"ef26e636-556b-4af7-ba9e-fecece154997\"\r\n            }\r\n```\r\n\r\nThen CSINode was not recreated until some time later, after the driver was manually restarted.\r\n\r\n### What did you expect to happen?\r\n\r\nCSI driver registration should update/recreate the correct CSINode object.\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\nIt's a race condition between the garbage collector and csi driver registration.\r\n\r\nThe csi driver needs to start up and register before the old CSINode object gets garbage collected.\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_\r\n\r\n### Kubernetes version\r\n\r\n<details>\r\n\r\n```console\r\n$ kubectl version\r\n1.25\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Cloud provider\r\n\r\n<details>\r\nGKE\r\n</details>\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n",
      "solution": "Discussed two approaches:\r\n\r\n1. Make the default deletion to do foreground deletion instead of the background deletion right now. The problem is that there are a few different controllers have already been deleting the node, there will be a lot of work to change all of them to foreground deletion. At the same time, it is difficult to stop new controllers to use foreground deletion by default.\r\n\r\n2. [Preferred] Since the Kubelet creates a node object at start up time, and csi node manager will create a csi node object accordingly, we should make the [csinode manager component initialization](https://github.com/kubernetes/kubernetes/blob/master/pkg/volume/csi/nodeinfomanager/nodeinfomanager.go#L392) blocking by the CSI node object creation.\n\n---\n\nOkay so currently we do call [func InstallCSIDriver in the nodeinfomanager](https://github.com/kubernetes/kubernetes/blob/master/pkg/volume/csi/nodeinfomanager/nodeinfomanager.go#L109), and then we call [func updateCSINode](https://github.com/kubernetes/kubernetes/blob/master/pkg/volume/csi/nodeinfomanager/nodeinfomanager.go#L349), the problem is that we only recreate the CSINode object when it is [not nil](https://github.com/kubernetes/kubernetes/blob/master/pkg/volume/csi/nodeinfomanager/nodeinfomanager.go#L382). \r\n\r\nWe can:\r\n\r\n1. remove the nil check and recreate each time if it is nil, and update each time if it is not nil.\r\n2. if csi node exists do a compare with\r\n```\r\nnode, err := csiKubeClient.CoreV1().Nodes().Get(context.TODO(), string(nim.nodeName), metav1.GetOptions{})\r\nif err != nil {\r\n  return nil, err\r\n}\r\n\r\nnodeInfo := &storagev1.CSINode{\r\n  ObjectMeta: metav1.ObjectMeta{\r\n    Name: string(nim.nodeName),\r\n    OwnerReferences: []metav1.OwnerReference{\r\n      {\r\n        APIVersion: nodeKind.Version,\r\n        Kind:       nodeKind.Kind,\r\n        Name:       node.Name,\r\n        UID:        node.UID,\r\n      },\r\n    },\r\n  },\r\n  Spec: storagev1.CSINodeSpec{\r\n    Drivers: []storagev1.CSINodeDriver{},\r\n  },\r\n}\r\n```\r\neach time and if there is a diff we call csiKubeClient.StorageV1().CSINodes().Update(). \r\n\r\nThe 2nd option saves us the csiKubeClient.StorageV1().CSINodes().Update() call if there is no diff.\r\n\n\n---\n\nI think this is the path that needs to be fixed:\r\n\r\nBefore any CSI driver is registered, we initially create an empty CSINode object with csi migration annotations (and it looks like it does block Node from being Ready): https://github.com/kubernetes/kubernetes/blob/c034f321f93b1be8dc34e3401ca245959e64f0dc/pkg/volume/csi/csi_plugin.go#L250\r\n\r\nThen I think this call needs to be updated to also check that the Node UID matches: https://github.com/kubernetes/kubernetes/blob/c034f321f93b1be8dc34e3401ca245959e64f0dc/pkg/volume/csi/nodeinfomanager/nodeinfomanager.go#L392",
      "labels": [
        "kind/bug",
        "sig/storage",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2023-08-19T03:07:15Z",
      "closed_at": "2026-01-13T20:48:43Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/120063",
      "comments_count": 30
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 115325,
      "title": "Kubelet accepting pod, setting OutOfCpu on scheduled pods",
      "problem": "### What happened?\r\n\r\nI'm using Knative to run services that scale up and down with load. Knative creates Deployments, which create ReplicaSets, and then calls the Kubernetes API to scale up and scale down those Deployments based on metrics. \r\n\r\nWe've been able to identify pods being scheduled onto nodes that do not have enough resources when the pod is accepted. This has been reported and supposedly fixed several times in 1.22.9 and 1.23.6, however we've run into it here on 1.22.15. The oddest part is these are normal Deployments / ReplicaSets that are causing the problem.\r\n\r\n### What did you expect to happen?\r\n\r\nPods should not schedule on nodes where there is not enough CPU to fulfill the resource. This will trigger scheduling elsewhere or node-pool scale up.\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\nThings I think contribute and help to cause this:\r\n\r\n- Have a `nodeSelector` or `nodeAffinity` rules\r\n- Have a pod requests to node resource ratio of at least 50 - where 50+ pods of a service would fit on a node given the resource requests\r\n- Scale the deployment / replicaset up and down rapidly with a number of nodes that is close to the max scale of the required or preferred node pools. At max scale or a required node pool, pods should just NOT schedule.\r\n\r\n### Anything else we need to know?\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/v1.22.17/CHANGELOG/CHANGELOG-1.22.md#bug-or-regression-6\r\n\r\n#106884\r\n\r\n#108366\r\n\r\n#107679\r\n\r\n\r\n\r\n### Kubernetes version\r\n\r\n<details>\r\n\r\n```console\r\nClient Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.3\", GitCommit:\"aef86a93758dc3cb2c658dd9657ab4ad4afc21cb\", GitTreeState:\"clean\", BuildDate:\"2022-07-13T14:30:46Z\", GoVersion:\"go1.18.3\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\r\nKustomize Version: v4.5.4\r\nServer Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.15-gke.1000\", GitCommit:\"fbf2f97a417bed01195ce67ab3c222d68f06f8b7\", GitTreeState:\"clean\", BuildDate:\"2022-10-05T09:28:06Z\", GoVersion:\"go1.16.15b7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Cloud provider\r\n\r\n<details>\r\nRunning on GKE v1.22.15-gke.1000\r\n</details>\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\nGoogle's COS 93 - https://cloud.google.com/container-optimized-os/docs/release-notes/m93\r\n\r\n</details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\nContainerd v1.5.13\r\n</details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n",
      "solution": "/sig node\r\n\r\ncc @ruiwen-zhao \r\n\r\n> Should the scheduler avoid placing a pod on a node that doesn't have much CPU or memory available?\r\n\r\nIt does.\r\nThe issues you refer to in older patches of 1.22 were additional circumstances that were fixed, to the best of my knowledge.\r\nHowever, it is possible that kubelet didn't have a chance to report the creation of `kube-proxy`, which is a static pod in GKE. This is still true in 1.27.\r\n\r\nIn general, Pods are ephemeral and can fail for multiple reasons. You shouldn't use plain Pods to perform critical actions. You should back them with a Deployment, StatefulSet or Job. Job probably fits better for the case of `actions-runner-controller`. I suggest you open an issue in the repo so they make use of Jobs instead of plain Pods.\r\n\n\n---\n\n> update: As far as I understand at the moment, actions-runner-controller is purposely trying to place a pod onto the same node as another pod, so they can share a directory, or a similar idea.\r\n\r\nwait... what? yeah, a pod-placing controller that is ignoring available resources is definitely likely to hit this, and it's an expected outcome (a controller told the node to run more things than it has capacity for)\r\n\r\nI'm more interested in the issue reported by @mbrancato using pods scheduled by the kube-scheduler\n\n---\n\n@alculquicondor I think closing this an ignoring it leaves things in a poor quality state. In 1.22.8 the code was updated to specifically prevent `OutOfcpu` state so that:\r\n```\r\nThe Kubelet now waits to report the phase of a pod as terminal in the API until all running containers are guaranteed to have stopped and no new containers can be started\r\n```\r\nbut that obviously that isn't solving the issue here.\r\n\r\n@liggitt I have a mutating webhooks, however, there are no secrets injected. I also do not see any `Failed creating a mirror pod for` errors.\r\n\r\nIt does look like it happens (at least most recently) after a node pool triggered scale up:\r\n```\r\nEvents:\r\n  Type     Reason            Age   From                                   Message\r\n  ----     ------            ----  ----                                   -------\r\n  Warning  FailedScheduling  28m   gke.io/optimize-utilization-scheduler  0/97 nodes are available: 10 node(s) had untolerated taint {app-1: true}, 3 node(s) had untolerated taint {app-2: true}, 30 Insufficient memory, 5 Too many pods, 58 Insufficient cpu. preemption: 0/97 nodes are available: 13 Preemption is not helpful for scheduling, 84 No preemption victims found for incoming pod.\r\n  Normal   TriggeredScaleUp  27m   cluster-autoscaler                     pod triggered scale-up: [{https://www.googleapis.com/compute/v1/projects/my-project/zones/us-central1-c/instanceGroups/gke-*nodes1*-v-4f0efd05-grp 40->41 (max: 400)}]\r\n  Warning  FailedScheduling  27m   gke.io/optimize-utilization-scheduler  0/97 nodes are available: 10 node(s) had untolerated taint {app-1: true}, 3 node(s) had untolerated taint {app-2: true}, 30 Insufficient memory, 5 Too many pods, 57 Insufficient cpu. preemption: 0/97 nodes are available: 13 Preemption is not helpful for scheduling, 84 No preemption victims found for incoming pod.\r\n  Normal   Scheduled         27m   gke.io/optimize-utilization-scheduler  Successfully assigned detection/my-app-00115-deployment-77dfbd76d-bjh7r to gke-*nodes1*-v-4f0efd05-pd5m\r\n  Warning  OutOfcpu          27m   kubelet                                Node didn't have enough resource: cpu, requested: 275, used: 7678, capacity: 7910\r\n```",
      "labels": [
        "kind/bug",
        "sig/scheduling",
        "sig/node",
        "triage/accepted"
      ],
      "created_at": "2023-01-25T22:23:04Z",
      "closed_at": "2025-07-18T14:58:49Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/115325",
      "comments_count": 30
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 119534,
      "title": "The leaderElection health check pass even when apiserver is unreachable",
      "problem": "### What happened?\r\n\r\nThe `healthz` endpoint only includes `leaderElection` checker when the component, e.g. controller-manager or scheduler, is not a leader. When the component can't reach apiserver, the `healthz` endpoint still returns `ok`.\r\n\r\n### What did you expect to happen?\r\n\r\nI understand in the kubelet liveness probe scenario that restarting the controller-manager or scheduler can not help in this case. But technically speaking the `leaderElection` health check should fail in such case as the component actually is unable to perform leader election at all. And in our case, we don't rely on kubelet to run the control plane.\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\nSimple. Setup a HA Kubernetes, then stop all apiservers (or one apiserver if the controller-manager or the scheduler only talks to local apiserver) and curl the health check endpoint on either the controller-manager or the scheduler.\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_\r\n\r\n### Kubernetes version\r\n\r\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nClient Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.4\", GitCommit:\"fa3d7990104d7c1f16943a67f11b154b71f6a132\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:20:54Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\nKustomize Version: v5.0.1\r\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.4\", GitCommit:\"fa3d7990104d7c1f16943a67f11b154b71f6a132\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:14:49Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Cloud provider\r\n\r\n<details>\r\nnone\r\n</details>\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\nNAME=\"Ubuntu\"\r\nVERSION=\"18.04.5 LTS (Bionic Beaver)\"\r\nID=ubuntu\r\nID_LIKE=debian\r\nPRETTY_NAME=\"Ubuntu 18.04.5 LTS\"\r\nVERSION_ID=\"18.04\"\r\nHOME_URL=\"https://www.ubuntu.com/\"\r\nSUPPORT_URL=\"https://help.ubuntu.com/\"\r\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\r\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\r\nVERSION_CODENAME=bionic\r\nUBUNTU_CODENAME=bionic\r\n$ uname -a\r\nLinux node-10-158-32-31 5.4.0-70-generic #78~18.04.1-Ubuntu SMP Sat Mar 20 14:10:07 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/bug",
        "sig/api-machinery",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2023-07-24T07:21:47Z",
      "closed_at": "2026-01-12T21:29:47Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/119534",
      "comments_count": 10
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 131765,
      "title": "kube-proxy: externalTrafficPolicy:Local and proxy-mode=nftables blackholes pods traffic to external IPs",
      "problem": "### What happened?\n\nWhen using kube-proxy on nftables mode and specify a `LoadBalancer` Service with `externalTrafficPolicy:Local` that has an ExternalIP assigned, kube-proxy will create an entry in the kube-proxy ip nftable that will drop traffic to that external IP, like:\n\n```\n        map no-endpoint-services {\n                type ipv4_addr . inet_proto . inet_service : verdict\n                comment \"vmap to drop or reject packets to services with no endpoints\"\n                elements = { \n                             10.88.1.2 . tcp . 80 comment \"sys-ingress-priv/internal-ingress-controller-v2:web\" : drop,\n```\n\nAs a result, any pod on the host cannot send traffic to the external IP of the LoadBalancer.\n\n### What did you expect to happen?\n\nOn nodes where no workload is present, traffic should not be dropped but load balanced by kube proxy to the target Service endpoints.\nThis will also bring consistency with how other modes work, as this issue was addressed both for ipvs (https://github.com/kubernetes/kubernetes/issues/93456)\nand iptables (https://github.com/kubernetes/kubernetes/pull/77523) modes.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nRun kube-proxy on nftables mode and try hitting a `LoadBalancer` Service `ExternalIP` from within a pod running in a node that does not have ready endpoints of the target service\n\n### Anything else we need to know?\n\n@kubernetes/sig-network-bugs\nSimmilar to https://github.com/kubernetes/kubernetes/issues/75262 but for `nftables` mode\n\n### Kubernetes version\n\n<details>\n\n```console\nServer Version: v1.33.0\n```\n\n</details>\n\nkube-proxy: v1.33.0\n\n### Cloud provider\n\naws, gcp and bare metal\n\n### OS version\n\n_No response_\n\n### Install tools\n\n_No response_\n\n### Container runtime (CRI) and version (if applicable)\n\n_No response_\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n_No response_",
      "solution": "I do not think this issue is actually resolved. We have re-enabled `externalTrafficPolicy: Local` in one of our Services and I am trying to hit it's external IP (10.87.1.2) from inside a pod:\n\n```\n$ kubectl --context exp-1-merit --namespace sys-ingress-priv get svc internal-ingress-controller-v2\nNAME                             TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE\ninternal-ingress-controller-v2   LoadBalancer   10.7.113.128   10.87.1.2     80:32634/TCP,443:32008/TCP   4y243d\n```\n\nIn nftables, I can see that the externalIP is in both `no-endpoint-services` and `service-ips` maps and has ready/live endpoints in our cluster. As the fix (https://github.com/kubernetes/kubernetes/pull/132456) suggests, I can see the `filter-prerouting-pre-dnat` chain:\n```\n        chain filter-prerouting-pre-dnat {\n                type filter hook prerouting priority dstnat - 10; policy accept;\n                ct state new jump firewall-check\n        }\n```\nthat is supposed to bypass the chains that drop packets to external IP. The rest of chains are as follows:\n```\n        chain nodeport-endpoints-check {\n                ip daddr != 127.0.0.0/8 meta l4proto . th dport vmap @no-endpoint-nodeports\n        }\n\n        chain service-endpoints-check {\n                ip daddr . meta l4proto . th dport vmap @no-endpoint-services\n        }\n\n        chain firewall-check {\n                ip daddr . meta l4proto . th dport vmap @firewall-ips\n        }\n\n        chain services {\n                ip daddr . meta l4proto . th dport vmap @service-ips\n                fib daddr type local ip daddr != 127.0.0.0/8 meta l4proto . th dport vmap @service-nodeports\n        }\n\n        chain masquerading {\n                meta mark & 0x00004000 == 0x00000000 return\n                meta mark set meta mark ^ 0x00004000\n                masquerade fully-random\n        }\n\n        chain cluster-ips-check {\n                ip daddr @cluster-ips reject comment \"Reject traffic to invalid ports of ClusterIPs\"\n                ip daddr 10.7.0.0/16 drop comment \"Drop traffic to unallocated ClusterIPs\"\n        }\n\n        chain mark-for-masquerade {\n                meta mark set meta mark | 0x00004000\n        }\n\n        chain reject-chain {\n                comment \"helper for @no-endpoint-services / @no-endpoint-nodeports\"\n                reject\n        }\n\n        chain service-66AAUTXI-sys-argocd/argocd-server/tcp/http {\n...\n```\nto give you a glimpse of where this is jumping to.\n\n\nBut trying to hit this from inside a Pod gives me a timeout:\n```\n$ nc -v -w2 -z 10.87.1.2 443\nnc: 10.87.1.2 (10.87.1.2:443): Operation timed out\ncommand terminated with exit code 1\n```\nwhile using the Service IP (or a Pod IP directly works):\n```\n$ nc -v -w2 -z 10.7.113.128  443\n10.7.113.128 (10.7.113.128:443) open\n```\n\n`tcpdump -i any` on the caller pod host shows that when trying the ExternalIP packets never jump off the calico pod interface:\n```\n11:25:39.411133 calia30537cfb69 In  IP 10.6.12.6.46149 > 10.87.1.2.443: Flags [S], seq 987021165, win 64800, options [mss 1440,sackOK,TS val 3972518332 ecr 0,nop,wscale 7], length 0\n11:25:40.446536 calia30537cfb69 In  IP 10.6.12.6.46149 > 10.87.1.2.443: Flags [S], seq 987021165, win 64800, options [mss 1440,sackOK,TS val 3972519368 ecr 0,nop,wscale 7], length 0\n```\nwhile using a target Pod IP directly is able to be routed out:\n```\n11:25:21.467278 calia30537cfb69 In  IP 10.6.12.6.40527 > 10.6.9.24.443: Flags [S], seq 979113911, win 64800, options [mss 1440,sackOK,TS val 3337680190 ecr 0,nop,wscale 7], length 0         \n11:25:21.467335 wireguard.cali Out IP 10.6.12.6.40527 > 10.6.9.24.443: Flags [S], seq 979113911, win 64800, options [mss 1440,sackOK,TS val 3337680190 ecr 0,nop,wscale 7], length 0          \n11:25:21.468273 calia30537cfb69 In  IP 10.6.12.6.40527 > 10.6.9.24.443: Flags [.], ack 3906855355, win 507, options [nop,nop,TS val 3337680191 ecr 2850828976], length 0                      \n11:25:21.468284 wireguard.cali Out IP 10.6.12.6.40527 > 10.6.9.24.443: Flags [.], ack 1, win 507, options [nop,nop,TS val 3337680191 ecr 2850828976], length 0\n```\n\nAtm we are using `registry.k8s.io/kube-proxy:v1.34.2` image. I think that this issue should be re-opened as the original problem described does not look solved. ",
      "labels": [
        "kind/bug",
        "sig/network",
        "triage/accepted"
      ],
      "created_at": "2025-05-14T12:45:21Z",
      "closed_at": "2025-07-10T00:53:29Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/131765",
      "comments_count": 19
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubernetes",
      "issue_number": 124332,
      "title": "failed to assign quota after kubelet was restarted",
      "problem": "### What happened?\r\n\r\nAfter kubelet was restarted, kubelet starts to output quota related errors and the total count of entries in `/etc/projects` and `/etc/projid` decreased.\r\n\r\n```console\r\nI0416 17:38:13.086019  157568 empty_dir.go:306] Set quota on /var/lib/kubelet/pods/e5149231-3a81-41e0-87be-579846f6caea/volumes/kubernetes.io~configmap/config failed assign quota FAILED exit status 1\r\nI0416 17:38:13.086051  157568 operation_generator.go:838] \"MountVolume.markVolumeErrorState leaving volume uncertain\" volumeName=kubernetes.io/configmap/e5149231-3a81-41e0-87be-579846f6caea-config\r\nE0416 17:38:13.086130  157568 nestedpendingoperations.go:348] Operation for \"{volumeName:kubernetes.io/configmap/e5149231-3a81-41e0-87be-579846f6caea-config podName:e5149231-3a81-41e0-87be-579846f6caea nodeName:}\" failed. No retries permitted until 2024-04-16 17:40:15.086106798 +0800 CST m=+376.261198810 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume \"config\" (UniqueName: \"kubernetes.io/configmap/e5149231-3a81-41e0-87be-579846f6caea-config\") pod \"xxx-yyy\" (UID: \"e5149231-3a81-41e0-87be-579846f6caea\") : assign quota FAILED exit status 1\r\nI0416 17:38:13.086156  157568 event.go:294] \"Event occurred\" object=\"xxx/yyy\" fieldPath=\"\" kind=\"Pod\" apiVersion=\"v1\" type=\"Warning\" reason=\"FailedMount\" message=\"MountVolume.SetUp failed for volume \\\"config\\\" : assign quota FAILED exit status 1\"\r\n```\r\n\r\nSimilar issues: #115309\r\n\r\n### What did you expect to happen?\r\n\r\n-\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\n1. ext4 fs with quota enabled and enforced `tune2fs -O project -Q prjquota /dev/xxx; mount -o prjquota ...`\r\n2. enable featuregate `LocalStorageCapacityIsolationFSQuotaMonitoring` in kubelet config\r\n3. register this node to k8s (start kubelet): works well\r\n4. restart kubelet: reproduced\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_\r\n\r\n### Kubernetes version\r\n\r\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nWARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\r\nClient Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.7\", GitCommit:\"84e1fc493a47446df2e155e70fca768d2653a398\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:23:27Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\nKustomize Version: v4.5.7\r\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.7\", GitCommit:\"84e1fc493a47446df2e155e70fca768d2653a398\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:16:45Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Cloud provider\r\n\r\n<details>\r\nIDC\r\n</details>\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\nNAME=\"AlmaLinux\"\r\nVERSION=\"9.3 (Shamrock Pampas Cat)\"\r\nID=\"almalinux\"\r\nID_LIKE=\"rhel centos fedora\"\r\nVERSION_ID=\"9.3\"\r\nPLATFORM_ID=\"platform:el9\"\r\nPRETTY_NAME=\"AlmaLinux 9.3 (Shamrock Pampas Cat)\"\r\nANSI_COLOR=\"0;34\"\r\nLOGO=\"fedora-logo-icon\"\r\nCPE_NAME=\"cpe:/o:almalinux:almalinux:9::baseos\"\r\nHOME_URL=\"https://almalinux.org/\"\r\nDOCUMENTATION_URL=\"https://wiki.almalinux.org/\"\r\nBUG_REPORT_URL=\"https://bugs.almalinux.org/\"\r\n\r\nALMALINUX_MANTISBT_PROJECT=\"AlmaLinux-9\"\r\nALMALINUX_MANTISBT_PROJECT_VERSION=\"9.3\"\r\nREDHAT_SUPPORT_PRODUCT=\"AlmaLinux\"\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"9.3\"\r\n\r\n$ uname -a\r\nLinux xxx 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\ncontainerd 1.6.31\r\n</details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/bug",
        "sig/storage",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2024-04-16T09:57:10Z",
      "closed_at": "2026-01-12T07:26:26Z",
      "url": "https://github.com/kubernetes/kubernetes/issues/124332",
      "comments_count": 21
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 17700,
      "title": "Starting Minikube messes up binfmt config for Docker, making it unable to run cross-architecture images",
      "problem": "### What Happened?\n\nI was planning using Minikube with `docker` driver, on an Apple M1 chip.\r\n\r\nMy Docker Engine, when freshly started supports the following emulators:\r\n```\r\n% docker run --privileged --rm tonistiigi/binfmt\r\n{\r\n  \"supported\": [\r\n    \"linux/arm64\",\r\n    \"linux/amd64\",\r\n    \"linux/386\",\r\n    \"linux/arm/v7\",\r\n    \"linux/arm/v6\"\r\n  ],\r\n  \"emulators\": [\r\n    \"mac-macho-arm64\",\r\n    \"mac-macho-x86_64\",\r\n    \"mac-universal-arm64\",\r\n    \"mac-universal-x86_64\",\r\n    \"qemu-arm\",\r\n    \"qemu-i386\",\r\n    \"qemu-x86_64\",\r\n    \"rosetta\",\r\n    \"rosetta\u200b\",\r\n    \"rosetta\u200b\u200b\"\r\n  ]\r\n}\r\n% minikube start\r\n\ud83d\ude04  minikube v1.32.0 on Darwin 14.1.1 (arm64)\r\n\u2728  Automatically selected the docker driver. Other choices: qemu2, virtualbox, ssh\r\n\ud83d\udccc  Using Docker Desktop driver with root privileges\r\n\ud83d\udc4d  Starting control plane node minikube in cluster minikube\r\n\ud83d\ude9c  Pulling base image ...\r\n\ud83d\udd25  Creating docker container (CPUs=2, Memory=8100MB) ...\r\n\ud83d\udc33  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...\r\n    \u25aa Generating certificates and keys ...\r\n    \u25aa Booting up control plane ...\r\n    \u25aa Configuring RBAC rules ...\r\n\ud83d\udd17  Configuring bridge CNI (Container Networking Interface) ...\r\n    \u25aa Using image gcr.io/k8s-minikube/storage-provisioner:v5\r\n\ud83d\udd0e  Verifying Kubernetes components...\r\n\ud83c\udf1f  Enabled addons: storage-provisioner, default-storageclass\r\n\ud83c\udfc4  Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\r\n% docker run --privileged --rm tonistiigi/binfmt\r\n{\r\n  \"supported\": [\r\n    \"linux/arm64\"\r\n  ],\r\n  \"emulators\": [\r\n    \"python3.10\"\r\n  ]\r\n}\r\n```\r\n\r\nMinikube messes up the binfmt config, so from that point on, it's only possible to run images of the same architecture as minikube itself (in my case linux/arm64).\r\n\r\nThis invasive behaviour is a showstopper for me to use Minikube, despite it being lovely and I'd really like to switch my K3S based stack to Minikube.\r\n\r\nMy attempts to find a workaround, or try to restore the Binfmt settings manually failed, so reaching out with an issue.\n\n### Attach the log file\n\n[log.txt](https://github.com/kubernetes/minikube/files/13514085/log.txt)\r\n\n\n### Operating System\n\nmacOS (Default)\n\n### Driver\n\nDocker",
      "solution": "**Correction**\r\n_My issue might be misplaced here_\r\n\r\nThe problem is happening with `OrbStack`, but cannot reproduce with Docker Desktop. So the issue might be specific to Minikube on Orbstack with `docker` driver.\n\n---\n\nCurious, is it fixed in https://github.com/kubernetes/minikube/pull/17719 @fbuetler?\r\n\r\nWithout a patch release it seems that that current options are to use v1.31.2 or you should be able to specify to use v1.31.2's version of the base image during start with v1.32.0, I don't think there were any changes that would make the two incompatible.\r\n\r\n`minikube start --base-image gcr.io/k8s-minikube/kicbase:v0.0.40`\n\n---\n\n> Curious, is it fixed in https://github.com/kubernetes/minikube/pull/17719\r\n\r\n@spowelljr yes, the problem still persists.\r\n\r\n```bash\r\n\u276f ./minikube-darwin-arm64 version\r\nminikube version: v1.32.0\r\ncommit: e08a2828f2be3e524baaf41342316dad88935561\r\n\r\n\u276f docker run --privileged --rm tonistiigi/binfmt\r\n{\r\n  \"supported\": [\r\n    \"linux/arm64\",\r\n    \"linux/amd64\",\r\n    \"linux/386\",\r\n    \"linux/arm/v7\",\r\n    \"linux/arm/v6\"\r\n  ],\r\n  \"emulators\": [\r\n    \"mac-macho-arm64\",\r\n    \"mac-macho-x86_64\",\r\n    \"mac-universal-arm64\",\r\n    \"mac-universal-x86_64\",\r\n    \"qemu-arm\",\r\n    \"qemu-i386\",\r\n    \"qemu-x86_64\",\r\n    \"rosetta\",\r\n    \"rosetta\u200b\",\r\n    \"rosetta\u200b\u200b\"\r\n  ]\r\n}\r\n\r\n~\r\n\u276f ./minikube-darwin-arm64 start\r\n\ud83d\ude04  minikube v1.32.0 on Darwin 14.1.1 (arm64)\r\n\u2728  Automatically selected the docker driver\r\n\ud83d\udccc  Using Docker Desktop driver with root privileges\r\n\ud83d\udc4d  Starting control plane node minikube in cluster minikube\r\n\ud83d\ude9c  Pulling base image ...\r\n\ud83d\udcbe  Downloading Kubernetes v1.28.4 preload ...\r\n    > preloaded-images-k8s-v18-v1...:  341.36 MiB / 341.36 MiB  100.00% 21.02 M\r\n    > gcr.io/k8s-minikube/kicbase...:  410.64 MiB / 410.64 MiB  100.00% 15.74 M\r\n\ud83d\udd25  Creating docker container (CPUs=6, Memory=24576MB) ...\r\n\ud83d\udc33  Preparing Kubernetes v1.28.4 on Docker 24.0.7 ...\r\n    \u25aa Generating certificates and keys ...\r\n    \u25aa Booting up control plane ...\r\n    \u25aa Configuring RBAC rules ...\r\n\ud83d\udd17  Configuring bridge CNI (Container Networking Interface) ...\r\n\ud83d\udd0e  Verifying Kubernetes components...\r\n    \u25aa Using image gcr.io/k8s-minikube/storage-provisioner:v5\r\n\ud83c\udf1f  Enabled addons: storage-provisioner, default-storageclass\r\n\ud83c\udfc4  Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\r\n\r\n~\r\n\u276f docker run --privileged --rm tonistiigi/binfmt\r\n{\r\n  \"supported\": [\r\n    \"linux/arm64\"\r\n  ],\r\n  \"emulators\": [\r\n    \"python3.10\"\r\n  ]\r\n}\r\n```",
      "labels": [
        "kind/bug",
        "priority/important-soon",
        "co/docker-driver"
      ],
      "created_at": "2023-11-30T14:40:42Z",
      "closed_at": "2024-01-09T00:06:57Z",
      "url": "https://github.com/kubernetes/minikube/issues/17700",
      "comments_count": 22
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21302,
      "title": "\"TestFunctional/parallel/MountCmd/VerifyCleanup\" on Qemu Macos",
      "problem": "this Test Flakes highly on github actions on Macos qemu\n\nhttps://github.com/kubernetes/minikube/actions/runs/16868077851/job/47778010499?pr=21277\nhere is a full log of one test\n\n```\n=== RUN   TestFunctional/parallel/MountCmd/VerifyCleanup\nfunctional_test_mount_test.go:298: (dbg) daemon: [./minikube-darwin-amd64 mount -p functional-326000 /var/folders/vk/nx37ffx50hv5djclhltc26vw0000gn/T/TestFunctionalparallelMountCmdVerifyCleanup2981785364/001:/mount1 --alsologtostderr -v=1]\nfunctional_test_mount_test.go:298: (dbg) daemon: [./minikube-darwin-amd64 mount -p functional-326000 /var/folders/vk/nx37ffx50hv5djclhltc26vw0000gn/T/TestFunctionalparallelMountCmdVerifyCleanup2981785364/001:/mount2 --alsologtostderr -v=1]\nfunctional_test_mount_test.go:298: (dbg) daemon: [./minikube-darwin-amd64 mount -p functional-326000 /var/folders/vk/nx37ffx50hv5djclhltc26vw0000gn/T/TestFunctionalparallelMountCmdVerifyCleanup2981785364/001:/mount3 --alsologtostderr -v=1]\nfunctional_test_mount_test.go:325: (dbg) Run:  ./minikube-darwin-amd64 -p functional-326000 ssh \"findmnt -T\" /mount1\nfunctional_test_mount_test.go:325: (dbg) Non-zero exit: ./minikube-darwin-amd64 -p functional-326000 ssh \"findmnt -T\" /mount1: exit status 1 (2.875892565s)\n** stderr ** \n\tssh: Process exited with status 1\n** /stderr **\nI0812 09:21:53.001160    7355 retry.go:31] will retry after 266.05868ms: exit status 1\nfunctional_test_mount_test.go:325: (dbg) Run:  ./minikube-darwin-amd64 -p functional-326000 ssh \"findmnt -T\" /mount1\nfunctional_test_mount_test.go:325: (dbg) Done: ./minikube-darwin-amd64 -p functional-326000 ssh \"findmnt -T\" /mount1: (1.671441685s)\nfunctional_test_mount_test.go:325: (dbg) Run:  ./minikube-darwin-amd64 -p functional-326000 ssh \"findmnt -T\" /mount2\nfunctional_test_mount_test.go:325: (dbg) Run:  ./minikube-darwin-amd64 -p functional-326000 ssh \"findmnt -T\" /mount3\nfunctional_test_mount_test.go:370: (dbg) Run:  ./minikube-darwin-amd64 mount -p functional-326000 --kill=true\nfunctional_test_mount_test.go:370: (dbg) Non-zero exit: ./minikube-darwin-amd64 mount -p functional-326000 --kill=true: exit status 30 (188.111739ms)\n-- stdout --\n\t\n\t\n-- /stdout --\n** stderr ** \n\tX Exiting due to HOST_KILL_MOUNT_PROC: Error killing mount process: stale pid: 32993\n\t* \n\t\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\t\u2502                                                                                                                          \u2502\n\t\u2502    * If the above advice does not help, please let us know:                                                              \u2502\n\t\u2502      https://github.com/kubernetes/minikube/issues/new/choose                                                            \u2502\n\t\u2502                                                                                                                          \u2502\n\t\u2502    * Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.                                 \u2502\n\t\u2502    * Please also attach the following file to the GitHub issue:                                                          \u2502\n\t\u2502    * - /var/folders/vk/nx37ffx50hv5djclhltc26vw0000gn/T/minikube_mount_d8d72b61fb2cdb7df8260ab4cdb9662b86544357_0.log    \u2502\n\t\u2502                                                                                                                          \u2502\n\t\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n** /stderr **\nfunctional_test_mount_test.go:372: failed while trying to kill mounts\nfunctional_test_mount_test.go:313: (dbg) stopping [./minikube-darwin-amd64 mount -p functional-326000 /var/folders/vk/nx37ffx50hv5djclhltc26vw0000gn/T/TestFunctionalparallelMountCmdVerifyCleanup2981785364/001:/mount1 --alsologtostderr -v=1] ...\nfunctional_test_mount_test.go:313: (dbg) [./minikube-darwin-amd64 mount -p functional-326000 /var/folders/vk/nx37ffx50hv5djclhltc26vw0000gn/T/TestFunctionalparallelMountCmdVerifyCleanup2981785364/001:/mount1 --alsologtostderr -v=1] stdout:\n* Mounting host path /var/folders/vk/nx37ffx50hv5djclhltc26vw0000gn/T/TestFunctionalparallelMountCmdVerifyCleanup2981785364/001 into VM as /mount1 ...\n- Mount type:   9p\n- User ID:      docker\n- Group ID:     docker\n- Version:      9p2000.L\n- Message Size: 262144\n- Options:      map[]\n- Bind Address: 192.168.105.1:49610\n* Userspace file server: \nufs starting\n* Successfully mounted /var/folders/vk/nx37ffx50hv5djclhltc26vw0000gn/T/TestFunctionalparallelMountCmdVerifyCleanup2981785364/001 to /mount1\n* NOTE: This process must stay alive for the mount to be accessible ...\nfunctional_test_mount_test.go:313: (dbg) [./minikube-darwin-amd64 mount -p functional-326000 /var/folders/vk/nx37ffx50hv5djclhltc26vw0000gn/T/TestFunctionalparallelMountCmdVerifyCleanup2981785364/001:/mount1 --alsologtostderr -v=1] stderr:\nI0812 09:21:50.331520   33627 out.go:360] Setting OutFile to fd 1 ...\nI0812 09:21:50.332856   33627 out.go:408] TERM=,COLORTERM=, which probably does not support color\nI0812 09:21:50.332884   33627 out.go:374] Setting ErrFile to fd 2...\nI0812 09:21:50.332894   33627 out.go:408] TERM=,COLORTERM=, which probably does not support color\nI0812 09:21:50.334282   33627 root.go:338] Updating PATH: /Users/runner/.minikube/bin\nI0812 09:21:50.345761   33627 mustload.go:65] Loading cluster: functional-326000\nI0812 09:21:50.346506   33627 config.go:182] Loaded profile config \"functional-326000\": Driver=qemu2, ContainerRuntime=docker, KubernetesVersion=v1.33.2\nI0812 09:21:50.348902   33627 host.go:66] Checking if \"functional-326000\" exists ...\nI0812 09:21:50.355268   33627 out.go:179] * Mounting host path /var/folders/vk/nx37ffx50hv5djclhltc26vw0000gn/T/TestFunctionalparallelMountCmdVerifyCleanup2981785364/001 into VM as /mount1 ...\nI0812 09:21:50.358786   33627 out.go:179]   - Mount type:   9p\nI0812 09:21:50.362817   33627 out.go:179]   - User ID:      docker\nI0812 09:21:50.366604   33627 out.go:179]   - Group ID:     docker\nI0812 09:21:50.370066   33627 out.go:179]   - Version:      9p2000.L\nI0812 09:21:50.373271   33627 out.go:179]   - Message Size: 262144\nI0812 09:21:50.379707   33627 out.go:179]   - Options:      map[]\nI0812 09:21:50.383699   33627 out.go:179]   - Bind Address: 192.168.105.1:49610\nI0812 09:21:50.387113   33627 out.go:179] * Userspace file server: \nI0812 09:21:50.390477   33627 ssh_runner.go:195] Run: /bin/bash -c \"[ \"x$(findmnt -T /mount1 | grep /mount1)\" != \"x\" ] && sudo umount -f -l /mount1 || echo \"\nI0812 09:21:50.390525   33627 sshutil.go:53] new ssh client: &{IP:192.168.105.2 Port:22 SSHKeyPath:/Users/runner/.minikube/machines/functional-326000/id_rsa Username:docker}\nI0812 09:21:52.316777   33627 ssh_runner.go:235] Completed: /bin/bash -c \"[ \"x$(findmnt -T /mount1 | grep /mount1)\" != \"x\" ] && sudo umount -f -l /mount1 || echo \": (1.92623556s)\nI0812 09:21:52.316852   33627 mount.go:180] unmount for /mount1 ran successfully\nI0812 09:21:52.316872   33627 ssh_runner.go:195] Run: /bin/bash -c \"sudo mkdir -p /mount1\"\nI0812 09:21:53.272974   33627 ssh_runner.go:195] Run: /bin/bash -c \"sudo mount -t 9p -o dfltgid=$(grep ^docker: /etc/group | cut -d: -f3),dfltuid=$(id -u docker),msize=262144,port=49610,trans=tcp,version=9p2000.L 192.168.105.1 /mount1\"\nI0812 09:21:54.426683   33627 main.go:125] stdlog: ufs.go:141 connected\nI0812 09:21:54.428351   33627 main.go:125] stdlog: srv_conn.go:133 >>> 192.168.105.2:43496 Tversion tag 65535 msize 262144 version '9P2000.L'\nI0812 09:21:54.428446   33627 main.go:125] stdlog: srv_conn.go:190 <<< 192.168.105.2:43496 Rversion tag 65535 msize 262144 version '9P2000'\nI0812 09:21:54.432151   33627 main.go:125] stdlog: srv_conn.go:133 >>> 192.168.105.2:43496 Tattach tag 0 fid 0 afid 4294967295 uname 'nobody' nuname 0 aname ''\nI0812 09:21:54.432319   33627 main.go:125] stdlog: srv_conn.go:190 <<< 192.168.105.2:43496 Rattach tag 0 aqid (30036ba8e 9d95ffef 'd')\nI0812 09:21:54.435654   33627 main.go:125] stdlog: srv_conn.go:133 >>> 192.168.105.2:43496 Tstat tag 0 fid 0\nI0812 09:21:54.436761   33627 main.go:125] stdlog: srv_conn.go:190 <<< 192.168.105.2:43496 Rstat tag 0 st ('001' 'runner' '20' '' q (30036ba8e 9d95ffef 'd') m d755 at 0 mt 1754990510 l 64 t 0 d 0 ext )\nI0812 09:21:54.442291   33627 main.go:125] stdlog: srv_conn.go:133 >>> 192.168.105.2:43496 Tstat tag 0 fid 0\nI0812 09:21:54.443292   33627 main.go:125] stdlog: srv_conn.go:190 <<< 192.168.105.2:43496 Rstat tag 0 st ('001' 'runner' '20' '' q (30036ba8e 9d95ffef 'd') m d755 at 0 mt 1754990510 l 64 t 0 d 0 ext )\nI0812 09:21:54.448656   33627 ssh_runner.go:235] Completed: /bin/bash -c \"sudo mount -t 9p -o dfltgid=$(grep ^docker: /etc/group | cut -d: -f3),dfltuid=$(id -u docker),msize=262144,port=49610,trans=tcp,version=9p2000.L 192.168.105.1 /mount1\": (1.175640572s)\nI0812 09:21:54.448715   33627 lock.go:50] WriteFile acquiring /Users/runner/.minikube/profiles/functional-326000/.mount-process: {Name:mk7b2cf112c806c17ecbd5d30b971d4be944648f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}\nI0812 09:21:54.449327   33627 mount.go:105] mount successful: \"\"\nI0812 09:21:54.495146   33627 out.go:179] * Successfully mounted /var/folders/vk/nx37ffx50hv5djclhltc26vw0000gn/T/TestFunctionalparallelMountCmdVerifyCleanup2981785364/001 to /mount1\nI0812 09:21:54.498531   33627 out.go:203] \nI0812 09:21:54.501987   33627 out.go:179] * NOTE: This process must stay alive for the mount to be accessible ...\nfunctional_test_mount_test.go:313: (dbg) stopping [./minikube-darwin-amd64 mount -p functional-326000 /var/folders/vk/nx37ffx50hv5djclhltc26vw0000gn/T/TestFunctionalparallelMountCmdVerifyCleanup2981785364/001:/mount2 --alsologtostderr -v=1] ...\nfunctional_test_mount_test.go:313: (dbg) [./minikube-darwin-amd64 mount -p functional-326000 /var/folders/vk/nx37ffx50hv5djclhltc26vw0000gn/T/TestFunctionalparallelMountCmdVerifyCleanup2981785364/001:/mount2 --alsologtostderr -v=1] stdout:\n* Mounting host path /var/folders/vk/nx37ffx50hv5djclhltc26vw0000gn/T/TestFunctionalparallelMountCmdVerifyCleanup2981785364/001 into VM as /mount2 ...\n- Mount type:   9p\n- User ID:      docker\n- Group ID:     docker\n- Version:      9p2000.L\n- Message Size: 262144\n- Options:      map[]\n- Bind Address: 192.168.105.1:49609\n* Userspace file server: \nufs starting\n* Successfully mounted /var/folders/vk/nx37ffx50hv5djclhltc26vw0000gn/T/TestFunctionalparallelMountCmdVerifyCleanup2981785364/001 to /mount2\n* NOTE: This process must stay alive for the mount to be accessible ...\nfunctional_test_mount_test.go:313: (dbg) [./minikube-darwin-amd64 mount -p functional-326000 /var/folders/vk/nx37ffx50hv5djclhltc26vw0000gn/T/TestFunctionalparallelMountCmdVerifyCleanup2981785364/001:/mount2 --alsologtostderr -v=1] stderr:\nI0812 09:21:50.276942   33628 out.go:360] Setting OutFile to fd 1 ...\nI0812 09:21:50.280394   33628 out.go:408] TERM=,COLORTERM=, which probably does not support color\nI0812 09:21:50.280432   33628 out.go:374] Setting ErrFile to fd 2...\nI0812 09:21:50.280444   33628 out.go:408] TERM=,COLORTERM=, which probably does not support color\nI0812 09:21:50.281859   33628 root.go:338] Updating PATH: /Users/runner/.minikube/bin\nI0812 09:21:50.282735   33628 mustload.go:65] Loading cluster: functional-326000\nI0812 09:21:50.283350   33628 config.go:182] Loaded profile config \"functional-326000\": Driver=qemu2, ContainerRuntime=docker, KubernetesVersion=v1.33.2\nI0812 09:21:50.287887   33628 host.go:66] Checking if \"functional-326000\" exists ...\nI0812 09:21:50.320071   33628 out.go:179] * Mounting host path /var/folders/vk/nx37ffx50hv5djclhltc26vw0000gn/T/TestFunctionalparallelMountCmdVerifyCleanup2981785364/001 into VM as /mount2 ...\nI0812 09:21:50.326121   33628 out.go:179]   - Mount type:   9p\nI0812 09:21:50.330188   33628 out.go:179]   - User ID:      docker\nI0812 09:21:50.374809   33628 out.go:179]   - Group ID:     docker\nI0812 09:21:50.393658   33628 out.go:179]   - Version:      9p2000.L\nI0812 09:21:50.397870   33628 out.go:179]   - Message Size: 262144\nI0812 09:21:50.404013   33628 out.go:179]   - Options:      map[]\nI0812 09:21:50.412228   33628 out.go:179]   - Bind Address: 192.168.105.1:49609\nI0812 09:21:50.417249   33628 out.go:179] * Userspace file server: \nI0812 09:21:50.417851   33628 ssh_runner.go:195] Run: /bin/bash -c \"[ \"x$(findmnt -T /mount2 | grep /mount2)\" != \"x\" ] && sudo umount -f -l /mount2 || echo \"\nI0812 09:21:50.417893   33628 sshutil.go:53] new ssh client: &{IP:192.168.105.2 Port:22 SSHKeyPath:/Users/runner/.minikube/machines/functional-326000/id_rsa Username:docker}\nI0812 09:21:52.893115   33628 ssh_runner.go:235] Completed: /bin/bash -c \"[ \"x$(findmnt -T /mount2 | grep /mount2)\" != \"x\" ] && sudo umount -f -l /mount2 || echo \": (2.475233446s)\nI0812 09:21:52.893181   33628 mount.go:180] unmount for /mount2 ran successfully\nI0812 09:21:52.893205   33628 ssh_runner.go:195] Run: /bin/bash -c \"sudo mkdir -p /mount2\"\nI0812 09:21:53.542263   33628 ssh_runner.go:195] Run: /bin/bash -c \"sudo mount -t 9p -o dfltgid=$(grep ^docker: /etc/group | cut -d: -f3),dfltuid=$(id -u docker),msize=262144,port=49609,trans=tcp,version=9p2000.L 192.168.105.1 /mount2\"\nI0812 09:21:54.707373   33628 main.go:125] stdlog: ufs.go:141 connected\nI0812 09:21:54.727479   33628 main.go:125] stdlog: srv_conn.go:133 >>> 192.168.105.2:47314 Tversion tag 65535 msize 262144 version '9P2000.L'\nI0812 09:21:54.727604   33628 main.go:125] stdlog: srv_conn.go:190 <<< 192.168.105.2:47314 Rversion tag 65535 msize 262144 version '9P2000'\nI0812 09:21:54.753888   33628 main.go:125] stdlog: srv_conn.go:133 >>> 192.168.105.2:47314 Tattach tag 0 fid 0 afid 4294967295 uname 'nobody' nuname 0 aname ''\nI0812 09:21:54.754102   33628 main.go:125] stdlog: srv_conn.go:190 <<< 192.168.105.2:47314 Rattach tag 0 aqid (30036ba8e 9d95ffef 'd')\nI0812 09:21:54.763085   33628 main.go:125] stdlog: srv_conn.go:133 >>> 192.168.105.2:47314 Tstat tag 0 fid 0\nI0812 09:21:54.764713   33628 main.go:125] stdlog: srv_conn.go:190 <<< 192.168.105.2:47314 Rstat tag 0 st ('001' 'runner' '20' '' q (30036ba8e 9d95ffef 'd') m d755 at 0 mt 1754990510 l 64 t 0 d 0 ext )\nI0812 09:21:54.784625   33628 main.go:125] stdlog: srv_conn.go:133 >>> 192.168.105.2:47314 Tstat tag 0 fid 0\nI0812 09:21:54.785832   33628 main.go:125] stdlog: srv_conn.go:190 <<< 192.168.105.2:47314 Rstat tag 0 st ('001' 'runner' '20' '' q (30036ba8e 9d95ffef 'd') m d755 at 0 mt 1754990510 l 64 t 0 d 0 ext )\nI0812 09:21:54.803032   33628 ssh_runner.go:235] Completed: /bin/bash -c \"sudo mount -t 9p -o dfltgid=$(grep ^docker: /etc/group | cut -d: -f3),dfltuid=$(id -u docker),msize=262144,port=49609,trans=tcp,version=9p2000.L 192.168.105.1 /mount2\": (1.260701635s)\nI0812 09:21:54.803091   33628 lock.go:50] WriteFile acquiring /Users/runner/.minikube/profiles/functional-326000/.mount-process: {Name:mk7b2cf112c806c17ecbd5d30b971d4be944648f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}\nI0812 09:21:54.803315   33628 mount.go:105] mount successful: \"\"\nI0812 09:21:54.812880   33628 out.go:179] * Successfully mounted /var/folders/vk/nx37ffx50hv5djclhltc26vw0000gn/T/TestFunctionalparallelMountCmdVerifyCleanup2981785364/001 to /mount2\nI0812 09:21:54.816581   33628 out.go:203] \nI0812 09:21:54.821974   33628 out.go:179] * NOTE: This process must stay alive for the mount to be accessible ...\nfunctional_test_mount_test.go:313: (dbg) stopping [./minikube-darwin-amd64 mount -p functional-326000 /var/folders/vk/nx37ffx50hv5djclhltc26vw0000gn/T/TestFunctionalparallelMountCmdVerifyCleanup2981785364/001:/mount3 --alsologtostderr -v=1] ...\nfunctional_test_mount_test.go:313: (dbg) [./minikube-darwin-amd64 mount -p functional-326000 /var/folders/vk/nx37ffx50hv5djclhltc26vw0000gn/T/TestFunctionalparallelMountCmdVerifyCleanup2981785364/001:/mount3 --alsologtostderr -v=1] stdout:\n* Mounting host path /var/folders/vk/nx37ffx50hv5djclhltc26vw0000gn/T/TestFunctionalparallelMountCmdVerifyCleanup2981785364/001 into VM as /mount3 ...\n- Mount type:   9p\n- User ID:      docker\n- Group ID:     docker\n- Version:      9p2000.L\n- Message Size: 262144\n- Options:      map[]\n- Bind Address: 192.168.105.1:49613\n* Userspace file server: \nufs starting\n* Successfully mounted /var/folders/vk/nx37ffx50hv5djclhltc26vw0000gn/T/TestFunctionalparallelMountCmdVerifyCleanup2981785364/001 to /mount3\n* NOTE: This process must stay alive for the mount to be accessible ...\nfunctional_test_mount_test.go:313: (dbg) [./minikube-darwin-amd64 mount -p functional-326000 /var/folders/vk/nx37ffx50hv5djclhltc26vw0000gn/T/TestFunctionalparallelMountCmdVerifyCleanup2981785364/001:/mount3 --alsologtostderr -v=1] stderr:\nI0812 09:21:50.422318   33629 out.go:360] Setting OutFile to fd 1 ...\nI0812 09:21:50.423822   33629 out.go:408] TERM=,COLORTERM=, which probably does not support color\nI0812 09:21:50.423853   33629 out.go:374] Setting ErrFile to fd 2...\nI0812 09:21:50.423868   33629 out.go:408] TERM=,COLORTERM=, which probably does not support color\nI0812 09:21:50.424266   33629 root.go:338] Updating PATH: /Users/runner/.minikube/bin\nI0812 09:21:50.425445   33629 mustload.go:65] Loading cluster: functional-326000\nI0812 09:21:50.425833   33629 config.go:182] Loaded profile config \"functional-326000\": Driver=qemu2, ContainerRuntime=docker, KubernetesVersion=v1.33.2\nI0812 09:21:50.428366   33629 host.go:66] Checking if \"functional-326000\" exists ...\nI0812 09:21:50.436976   33629 out.go:179] * Mounting host path /var/folders/vk/nx37ffx50hv5djclhltc26vw0000gn/T/TestFunctionalparallelMountCmdVerifyCleanup2981785364/001 into VM as /mount3 ...\nI0812 09:21:50.440470   33629 out.go:179]   - Mount type:   9p\nI0812 09:21:50.444543   33629 out.go:179]   - User ID:      docker\nI0812 09:21:50.448024   33629 out.go:179]   - Group ID:     docker\nI0812 09:21:50.451587   33629 out.go:179]   - Version:      9p2000.L\nI0812 09:21:50.455331   33629 out.go:179]   - Message Size: 262144\nI0812 09:21:50.458989   33629 out.go:179]   - Options:      map[]\nI0812 09:21:50.462501   33629 out.go:179]   - Bind Address: 192.168.105.1:49613\nI0812 09:21:50.466610   33629 out.go:179] * Userspace file server: \nI0812 09:21:50.467516   33629 ssh_runner.go:195] Run: /bin/bash -c \"[ \"x$(findmnt -T /mount3 | grep /mount3)\" != \"x\" ] && sudo umount -f -l /mount3 || echo \"\nI0812 09:21:50.467586   33629 sshutil.go:53] new ssh client: &{IP:192.168.105.2 Port:22 SSHKeyPath:/Users/runner/.minikube/machines/functional-326000/id_rsa Username:docker}\nI0812 09:21:52.808789   33629 ssh_runner.go:235] Completed: /bin/bash -c \"[ \"x$(findmnt -T /mount3 | grep /mount3)\" != \"x\" ] && sudo umount -f -l /mount3 || echo \": (2.34123276s)\nI0812 09:21:52.808862   33629 mount.go:180] unmount for /mount3 ran successfully\nI0812 09:21:52.808883   33629 ssh_runner.go:195] Run: /bin/bash -c \"sudo mkdir -p /mount3\"\nI0812 09:21:53.739165   33629 ssh_runner.go:195] Run: /bin/bash -c \"sudo mount -t 9p -o dfltgid=$(grep ^docker: /etc/group | cut -d: -f3),dfltuid=$(id -u docker),msize=262144,port=49613,trans=tcp,version=9p2000.L 192.168.105.1 /mount3\"\nI0812 09:21:55.202792   33629 main.go:125] stdlog: ufs.go:141 connected\nI0812 09:21:55.218551   33629 main.go:125] stdlog: srv_conn.go:133 >>> 192.168.105.2:59518 Tversion tag 65535 msize 262144 version '9P2000.L'\nI0812 09:21:55.218651   33629 main.go:125] stdlog: srv_conn.go:190 <<< 192.168.105.2:59518 Rversion tag 65535 msize 262144 version '9P2000'\nI0812 09:21:55.228055   33629 main.go:125] stdlog: srv_conn.go:133 >>> 192.168.105.2:59518 Tattach tag 0 fid 0 afid 4294967295 uname 'nobody' nuname 0 aname ''\nI0812 09:21:55.228191   33629 main.go:125] stdlog: srv_conn.go:190 <<< 192.168.105.2:59518 Rattach tag 0 aqid (30036ba8e 9d95ffef 'd')\nI0812 09:21:55.231478   33629 main.go:125] stdlog: srv_conn.go:133 >>> 192.168.105.2:59518 Tstat tag 0 fid 0\nI0812 09:21:55.232778   33629 main.go:125] stdlog: srv_conn.go:190 <<< 192.168.105.2:59518 Rstat tag 0 st ('001' 'runner' '20' '' q (30036ba8e 9d95ffef 'd') m d755 at 0 mt 1754990510 l 64 t 0 d 0 ext )\nI0812 09:21:55.235854   33629 main.go:125] stdlog: srv_conn.go:133 >>> 192.168.105.2:59518 Tstat tag 0 fid 0\nI0812 09:21:55.236764   33629 main.go:125] stdlog: srv_conn.go:190 <<< 192.168.105.2:59518 Rstat tag 0 st ('001' 'runner' '20' '' q (30036ba8e 9d95ffef 'd') m d755 at 0 mt 1754990510 l 64 t 0 d 0 ext )\nI0812 09:21:55.244381   33629 ssh_runner.go:235] Completed: /bin/bash -c \"sudo mount -t 9p -o dfltgid=$(grep ^docker: /etc/group | cut -d: -f3),dfltuid=$(id -u docker),msize=262144,port=49613,trans=tcp,version=9p2000.L 192.168.105.1 /mount3\": (1.505164891s)\nI0812 09:21:55.244469   33629 lock.go:50] WriteFile acquiring /Users/runner/.minikube/profiles/functional-326000/.mount-process: {Name:mk7b2cf112c806c17ecbd5d30b971d4be944648f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}\nI0812 09:21:55.244771   33629 mount.go:105] mount successful: \"\"\nI0812 09:21:55.252996   33629 out.go:179] * Successfully mounted /var/folders/vk/nx37ffx50hv5djclhltc26vw0000gn/T/TestFunctionalparallelMountCmdVerifyCleanup2981785364/001 to /mount3\nI0812 09:21:55.257047   33629 out.go:203] \nI0812 09:21:55.260533   33629 out.go:179] * NOTE: This process must stay alive for the mount to be accessible ...\n--- FAIL: TestFunctional/parallel/MountCmd/VerifyCleanup (6.83s)\n```",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-08-12T08:50:29Z",
      "closed_at": "2026-02-05T20:22:45Z",
      "url": "https://github.com/kubernetes/minikube/issues/21302",
      "comments_count": 6
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21494,
      "title": "Cannot get started with Minikube",
      "problem": "### What Happened?\n\nI installed minikube via winget and tried running \n``` minikube start ```\n\n<img width=\"866\" height=\"290\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/dbf3e716-30e5-4ed3-a694-87eb10425c56\" />\n\n### Attach the log file\n\nFailed to start, no log files generated\n\n### Operating System\n\nWindows\n\n### Driver\n\nHyper-V",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-09-05T17:18:44Z",
      "closed_at": "2026-02-02T19:44:46Z",
      "url": "https://github.com/kubernetes/minikube/issues/21494",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21434,
      "title": "Error when running minikube service python-svc or browsing to the external ip",
      "problem": "### What Happened?\n\nminikube service python-svc\n|-----------|------------|-------------|---------------------------|\n| NAMESPACE |    NAME    | TARGET PORT |            URL            |\n|-----------|------------|-------------|---------------------------|\n| default   | python-svc |          80 | http://192.168.49.2:30148 |\n|-----------|------------|-------------|---------------------------|\n\n\u274c  Exiting due to SVC_UNREACHABLE: service not available: no running pod for service python-svc found\n\n\n### Attach the log file\n\n[logs.txt](https://github.com/user-attachments/files/22009028/logs.txt)\n\n[minikube_service_9fc38eb4b40b4a122f9b78594aaac60650608423_0.log](https://github.com/user-attachments/files/22009058/minikube_service_9fc38eb4b40b4a122f9b78594aaac60650608423_0.log)\n\n### Operating System\n\nUbuntu\n\n### Driver\n\nDocker",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-08-27T15:04:58Z",
      "closed_at": "2026-01-31T14:19:42Z",
      "url": "https://github.com/kubernetes/minikube/issues/21434",
      "comments_count": 5
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 22573,
      "title": "`podman ps` error after `minikube start && eval $(minikube -p minikube podman-env)`",
      "problem": "### What Happened?\n\nI'm using the podman driver like so:\n```\n$ minikube config view\n- driver: podman\n- rootless: false\n- container-runtime: crio\n```\n(The the CRIO container runtime is needed to run ```eval $(minikube -p minikube podman-env)```)\n\nAfter I start minikube (like so: ```minikube start && eval $(minikube -p minikube podman-env)```), I get the following error when I run ```podman ps```:\n```\n$ podman ps\nCannot connect to Podman. Please verify your connection to the Linux system using `podman system connection list`, or try `podman machine init` and `podman machine start` to manage a new Linux VM\nError: unable to connect to Podman socket: server API version is too old. Client \"4.0.0\" server \"3.4.4\": ssh://docker@127.0.0.1:36991/run/podman/podman.sock\n```\n\nminikube start:\n```\n$ minikube start\n\ud83d\ude04  minikube v1.37.0 on Lfs \n\u2728  Automatically selected the podman driver. Other choices: qemu2, ssh\n\ud83d\udccc  Using Podman driver with root privileges\n\ud83d\udc4d  Starting \"minikube\" primary control-plane node in \"minikube\" cluster\n\ud83d\ude9c  Pulling base image v0.0.48 ...\nE0127 21:10:36.628340    3883 cache.go:227] Error downloading kic artifacts:  not yet implemented, see issue #8426\n\ud83d\udd25  Creating podman container (CPUs=2, Memory=7900MB) ...\n\ud83c\udf81  Preparing Kubernetes v1.34.0 on CRI-O 1.24.6 ...\n\ud83d\udd17  Configuring CNI (Container Networking Interface) ...\n\ud83d\udd0e  Verifying Kubernetes components...\n    \u25aa Using image gcr.io/k8s-minikube/storage-provisioner:v5\n\ud83c\udf1f  Enabled addons: storage-provisioner, default-storageclass\n\ud83c\udfc4  Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\n```\n\npodman version (latest for Ubuntu):\n```\n$ podman --version\npodman version 5.4.2\n```\nAs a side note - All the minikube tutorials seems to work fine, but I can't deploy a Quarkus project into minikube because the podman command returns this error for almost every sub command.\n\n### Attach the log file\n\n[log.txt](https://github.com/user-attachments/files/24893073/log.txt)\n\n### Operating System\n\nNone\n\n### Driver\n\nNone",
      "solution": "You need to install an older `podman-remote`, matching the podman-legacy package in the ISO:\n\n`Client \"4.0.0\" server \"3.4.4\"`\n\nhttps://github.com/containers/podman/releases/tag/v3.4.4 (podman-remote-static, or source)\n\nYou could also use a Docker client, since that is more backwards compatible (until Docker v29...).\n\nOr `minikube ssh sudo podman`\n\nThere's also `minikube image`",
      "labels": [
        "kind/support",
        "co/runtime/crio"
      ],
      "created_at": "2026-01-27T19:21:33Z",
      "closed_at": "2026-01-27T22:37:38Z",
      "url": "https://github.com/kubernetes/minikube/issues/22573",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 11705,
      "title": "multi-nodes-minikube loss node label after restart",
      "problem": "<!--- Please include the \"minikube start\" command you used in your reproduction steps --->\r\n**Steps to reproduce the issue:** \r\n\r\n1. minikube start -n 2\r\n2. kubectl label node minikube-m02 nsid_public=open --overwrite\r\n3. minikube stop\r\n4. minikube start\r\n5. kubectl get node -A -o wide --show-labels\r\n\r\n```\r\nkubectl get node -A -o wide --show-labels\r\nNAME           STATUS   ROLES                  AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION        CONTAINER-RUNTIME   LABELS\r\nminikube       Ready    control-plane,master   29d   v1.20.2   192.168.49.2   <none>        Ubuntu 20.04.2 LTS   4.14.81.bm.26-amd64   docker://20.10.6    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=minikube,kubernetes.io/os=linux,minikube.k8s.io/commit=c61663e942ec43b20e8e70839dcca52e44cd85ae,minikube.k8s.io/name=minikube,minikube.k8s.io/updated_at=2021_05_20T13_05_08_0700,minikube.k8s.io/version=v1.20.0,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=\r\nminikube-m02   Ready    <none>                 9d    v1.20.2   192.168.49.3   <none>        Ubuntu 20.04.2 LTS   4.14.81.bm.26-amd64   docker://20.10.6    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=minikube-m02,kubernetes.io/os=linux\r\n```",
      "solution": "@pythonwood  we fixed this issue in this PR https://github.com/kubernetes/minikube/pull/11731\r\nit should be fixed in the beta release\r\n\r\nmind trying the beta release https://github.com/kubernetes/minikube/releases/tag/v1.22.0-beta.0\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/bug",
        "priority/important-longterm",
        "lifecycle/rotten",
        "co/multinode"
      ],
      "created_at": "2021-06-19T09:11:30Z",
      "closed_at": "2021-11-03T18:54:47Z",
      "url": "https://github.com/kubernetes/minikube/issues/11705",
      "comments_count": 10
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 15918,
      "title": "KubeVirt Addon broken due to missing curl in Pod",
      "problem": "### What Happened?\n\nTried `minikube start --addons=kubevirt`\r\n\r\nNo kubevirt resources are created.\r\n\r\nI will attempt a fix at https://github.com/kubernetes/minikube/blob/master/deploy/addons/kubevirt/pod.yaml.tmpl\n\n### Attach the log file\n\nNot using the standard logging, found problem in:\r\n\r\nkubectl -n kube-system logs kubevirt-install-manager\r\n\r\n/kubevirt-scripts/install.sh: line 3: curl: command not found\r\nInstalling KubeVirt version: \r\n/kubevirt-scripts/install.sh: line 6: curl: command not found\r\nerror: the path \"/manifests/kubevirt-operator.yaml\" does not exist\r\nerror: resource mapping not found for name: \"kubevirt\" namespace: \"kubevirt\" from \"STDIN\": no matches for kind \"KubeVirt\" in version \"kubevirt.io/v1\"\r\nensure CRDs are installed first\r\n\n\n### Operating System\n\nRedhat/Fedora\n\n### Driver\n\nKVM2",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe `curl` command was in the prior container image as seen:\r\n\r\n```bash\r\n$ podman run --rm -it --entrypoint sh docker.io/bitnami/kubectl:1.17@sha256:de642e973d3d0ef60e4d0a1f92286a9fdae245535c5990d4762bbe86fcf95887\r\n$ curl --version\r\ncurl 7.64.0 (x86_64-pc-linux-gnu) libcurl/7.64.0 OpenSSL/1.1.1d zlib/1.2.11 libidn2/2.0.5 libpsl/0.20.2 (+libidn2/2.0.5) libssh2/1.8.0 nghttp2/1.36.0 librtmp/2.3\r\nRelease-Date: 2019-02-06\r\nProtocols: dict file ftp ftps gopher http https imap imaps ldap ldaps pop3 pop3s rtmp rtsp scp sftp smb smbs smtp smtps telnet tftp \r\nFeatures: AsynchDNS IDN IPv6 Largefile GSS-API Kerberos SPNEGO NTLM NTLM_WB SSL libz TLS-SRP HTTP2 UnixSockets HTTPS-proxy PSL \r\n```\r\n\r\nThe `curl` command **is not** in the prior container image as seen:\r\n\r\n```bash\r\n$ podman run --rm -it --entrypoint sh docker.io/bitnami/kubectl:1.24.7@sha256:195f5a7a40cfb06e308701ae850abfa436d23baf9d39c0282298e540c9d07863\r\n$ curl --version\r\nsh: 1: curl: not found\r\n```\r\n\r\nThis is caused by #15310 but it may be resolved elsewhere to keep the newer image. The default user is UID 1001 and the image does not contain `sudo` so it does not seem adjusting the `install.sh` blurb initially linked would allow us to get `curl`. I'd appreciate some advice on the best place to get `curl` in this image in a sustainable way.\r\n\r\nI was impacted by this and did some of this sleuthing trying to use KubeVirt on Minikube on KVM. In the meantime I'm running:\r\n\r\n```bash\r\n$ minikube version\r\nminikube version: v1.28.0\r\ncommit: 986b1ebd987211ed16f8cc10aed7d2c42fc8392f\r\n```\r\n\r\nThis enables me to get some bits moving with KubeVirt but I'd like to be tracking the latest release.\r\n\r\n^^ @spowelljr",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2023-02-23T19:16:32Z",
      "closed_at": "2024-03-25T08:55:57Z",
      "url": "https://github.com/kubernetes/minikube/issues/15918",
      "comments_count": 9
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21452,
      "title": "Minikube v1.36.0: Registry Addon Fails with ImagePullBackOff Despite Image Existing Locally",
      "problem": "### What Happened?\n\n**What happened?**\nAfter successfully starting a Minikube cluster and enabling the registry addon, the kube-system/registry-XXXX pod fails to start. It enters a continuous ImagePullBackOff loop.\n\nThe error message from kubectl describe pod shows that Kubelet is failing to pull the image docker.io/registry:3.0.0@sha256:1fc7de... due to a network timeout (Client.Timeout exceeded while awaiting headers).\n\nHowever, upon SSHing into the Minikube node (minikube ssh), running docker image ls confirms that an image named registry with the tag 3.0.0 is already present in the node's local Docker image cache.\n\nIt appears the addon's configuration is forcing Kubelet to pull a specific digest from docker.io, which fails in my network environment, instead of using the locally available image.\n\n**What I expected to happen?**\nThe registry pod should start successfully by using the registry:3.0.0 image that is already present within the Minikube node's Docker environment, avoiding the need to contact the external docker.io registry.\n\n**Environment:**\n- Minikube version: v1.36.0\n- OS: CentOS Linux 7.9.2009\n- Driver: docker\n\n**Evidence and Logs:**\n1. kubectl describe pod output for the failing registry pod:\nThe events clearly show a Failed pull attempt due to a network timeout, leading to ErrImagePull and ImagePullBackOff.\n\n``` bash\nEvents:\n  Type     Reason          Age                  From               Message\n  ----     ------          ----                 ----               -------\n  Normal   Scheduled       117s                 default-scheduler  Successfully assigned kube-system/registry-694bd45846-pdvh7 to minikube\n  Warning  Failed          70s (x2 over 108s)   kubelet            Failed to pull image \"docker.io/registry:3.0.0@sha256:1fc7de554f2ac1247f0b7e0a450e273b0993be7d2beda1f3f5d6f1f001e0de7\": Error response from daemon: Get \"https://registry-1.docker.io/v2/\": context deadline exceeded\n  Normal   Pulling         44s (x3 over 115s)   kubelet            Pulling image \"docker.io/registry:3.0.0@sha256:1fc7de554f2ac1247f0b7e0a450e273b0993be7d2beda1f3f5d6f1f001e0de7\"\n  Warning  Failed          29s (x3 over 108s)   kubelet            Error: ErrImagePull\n  Warning  Failed          29s                  kubelet            Failed to pull image \"docker.io/registry:3.0.0@sha256:1fc7de554f2ac1247f0b7e0a450e273b0993be7d2beda1f3f5d6f1f001e0de7\": Error response from daemon: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\n  Normal   BackOff         4s (x5 over 99s)     kubelet            Back-off pulling image \"docker.io/registry:3.0.0@sha256:1fc7de554f2ac1247f0b7e0a450e273b0993be7d2beda1f3f5d6f1f001e0de7\"\n  Warning  Failed          4s (x5 over 99s)     kubelet            Error: ImagePullBackOff\n```\n\n2. Output of minikube ssh -- docker image ls:\nThis output confirms the registry:3.0.0 image is already available inside the Minikube node.\n\n```bash\ndocker@minikube:~$ docker pull docker.io/registry:3.0.0\nError response from daemon: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while...\ndocker@minikube:~$ docker image ls\nREPOSITORY                                                 TAG       IMAGE ID       CREATED        SIZE\n...\nregistry                                                   3.0.0     3c52eedee88c   4 months ago   57.7MB\n...\n```\n\n**Question for the community**\n1. How can I configure the registry addon to use the locally available registry:3.0.0 image and force it to bypass the pull attempt for the specific @sha256 digest from docker.io?\n2. Is there a command similar to minikube addons configure metrics-server.image=... that can be used to override the default image for the registry addon? Any guidance on how to resolve this would be greatly appreciated.\n\n### Attach the log file\n\nAug 29 07:12:48 minikube dockerd[1076]: time=\"2025-08-29T07:12:48.477069806Z\" level=warning msg=\"Error getting v2 registry: Get \\\"https://gcr.io/v2/\\\": context deadline exceeded\"\nAug 29 07:12:48 minikube dockerd[1076]: time=\"2025-08-29T07:12:48.477105315Z\" level=info msg=\"Attempting next endpoint for pull after error: Get \\\"https://gcr.io/v2/\\\": context deadline exceeded\"\nAug 29 07:12:48 minikube dockerd[1076]: time=\"2025-08-29T07:12:48.497321248Z\" level=error msg=\"Handler for POST /v1.49/images/create returned error: Get \\\"https://gcr.io/v2/\\\": context deadline exceeded\"\nAug 29 07:12:55 minikube dockerd[1076]: time=\"2025-08-29T07:12:55.440206575Z\" level=warning msg=\"Error getting v2 registry: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request canceled while waiting\n for connection (Client.Timeout exceeded while awaiting headers)\"\nAug 29 07:12:55 minikube dockerd[1076]: time=\"2025-08-29T07:12:55.440494758Z\" level=info msg=\"Attempting next endpoint for pull after error: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request cance\nled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"\nAug 29 07:12:55 minikube dockerd[1076]: time=\"2025-08-29T07:12:55.449733401Z\" level=error msg=\"Handler for POST /v1.46/images/create returned error: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: reque\nst canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"\nAug 29 07:13:30 minikube dockerd[1076]: time=\"2025-08-29T07:13:30.474164313Z\" level=error msg=\"Not continuing with pull after error\" error=\"manifest unknown: manifest unknown\"\nAug 29 07:14:03 minikube dockerd[1076]: time=\"2025-08-29T07:14:03.387442071Z\" level=warning msg=\"Error getting v2 registry: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request canceled while waiting\n for connection (Client.Timeout exceeded while awaiting headers)\"\nAug 29 07:14:03 minikube dockerd[1076]: time=\"2025-08-29T07:14:03.387484706Z\" level=info msg=\"Attempting next endpoint for pull after error: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request cance\nled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"\nAug 29 07:14:03 minikube dockerd[1076]: time=\"2025-08-29T07:14:03.395592007Z\" level=error msg=\"Handler for POST /v1.46/images/create returned error: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: reque\nst canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"\nAug 29 07:14:05 minikube dockerd[1076]: time=\"2025-08-29T07:14:05.114911959Z\" level=error msg=\"Not continuing with pull after error\" error=\"errors:\\ndenied: requested access to the resource is denied\\nunaut\nhorized: authentication required\\n\"\nAug 29 07:14:05 minikube dockerd[1076]: time=\"2025-08-29T07:14:05.115010294Z\" level=info msg=\"Ignoring extra error returned from registry\" error=\"unauthorized: authentication required\"\nAug 29 07:15:38 minikube dockerd[1076]: time=\"2025-08-29T07:15:38.858752090Z\" level=warning msg=\"Error getting v2 registry: Get \\\"https://gcr.io/v2/\\\": net/http: request canceled while waiting for connectio\nn (Client.Timeout exceeded while awaiting headers)\"\nAug 29 07:15:38 minikube dockerd[1076]: time=\"2025-08-29T07:15:38.859260023Z\" level=info msg=\"Attempting next endpoint for pull after error: Get \\\"https://gcr.io/v2/\\\": net/http: request canceled while wait\ning for connection (Client.Timeout exceeded while awaiting headers)\"\nAug 29 07:15:38 minikube dockerd[1076]: time=\"2025-08-29T07:15:38.865917924Z\" level=error msg=\"Handler for POST /v1.46/images/create returned error: Get \\\"https://gcr.io/v2/\\\": net/http: request canceled wh\nile waiting for connection (Client.Timeout exceeded while awaiting headers)\"\nAug 29 07:15:53 minikube dockerd[1076]: time=\"2025-08-29T07:15:53.882138972Z\" level=warning msg=\"Error getting v2 registry: Get \\\"https://registry-1.docker.io/v2/\\\": context deadline exceeded\"\nAug 29 07:15:53 minikube dockerd[1076]: time=\"2025-08-29T07:15:53.882186442Z\" level=info msg=\"Attempting next endpoint for pull after error: Get \\\"https://registry-1.docker.io/v2/\\\": context deadline exceed\ned\"\n\n\n### Operating System\n\nOther\n\n### Driver\n\nDocker",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-08-29T08:00:12Z",
      "closed_at": "2026-01-26T09:14:45Z",
      "url": "https://github.com/kubernetes/minikube/issues/21452",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21752,
      "title": "move kube-registry-proxy image to K8s-infra's GCR",
      "problem": "\ncurrently this image is built from this code\nhttps://github.com/spowelljr/kube-registry-proxy and it is used by Registry addon\nthis image used to be maintained by upstream kubernetes but they abandoned it, so minikube continued publishing it for Registry addon (example https://github.com/kubernetes/minikube/issues/20838)\n\nit is currently hosted in internal minikube gcp project but since older images of kube-registry-proxy gets CVEs they will be removed from internal google gcp projects.\n\nwe will need to publish this image somewhere else, either publish it in github container registry or preferably in K8s-infra",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "priority/important-soon",
        "lifecycle/stale",
        "kind/process"
      ],
      "created_at": "2025-10-15T18:05:18Z",
      "closed_at": "2026-01-25T10:47:44Z",
      "url": "https://github.com/kubernetes/minikube/issues/21752",
      "comments_count": 2
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21432,
      "title": "Minikube v1.36.0 fails to pull kicbase and Kubernetes images on Ubuntu 24.04 (VirtualBox driver)",
      "problem": "### What Happened?\n\nWhen running\n\n```bash\nminikube start --kubernetes-version=v1.32.0\n```\n\non **Ubuntu 24.04** with **Minikube v1.36.0** (VirtualBox/Docker driver), the cluster fails to start.\n\nMinikube is unable to download required Kubernetes binaries and images. The error shows:\n\n```\nconnection reset by peer\ninvalid checksum: Error downloading checksum file\n```\n\nand multiple messages like:\n\n```\n\u2757 The image 'registry.k8s.io/kube-apiserver:v1.32.0' was not found; unable to add it to cache.\n\u2757 minikube cannot pull kicbase image from any docker registry\n```\n\nEven though `curl -I https://dl.k8s.io/release/stable.txt` works fine on the same machine, `minikube` itself cannot fetch the binaries.\n\n\n\n### Attach the log file\n\n[logs.txt](https://github.com/user-attachments/files/22004971/logs.txt)\n\n### Operating System\n\nUbuntu\n\n### Driver\n\nDocker",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-08-27T10:09:48Z",
      "closed_at": "2026-01-24T11:52:45Z",
      "url": "https://github.com/kubernetes/minikube/issues/21432",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21429,
      "title": "Service not available",
      "problem": "### What Happened?\n\nI tried running a service, but an error message gets displayed mentioning that there is no pod available for running.\n\n### Attach the log file\n\n[logs.txt](https://github.com/user-attachments/files/22001076/logs.txt)\n\n### Operating System\n\nNone\n\n### Driver\n\nNone",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-08-27T04:49:48Z",
      "closed_at": "2026-01-24T06:52:43Z",
      "url": "https://github.com/kubernetes/minikube/issues/21429",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21427,
      "title": "Fix failing windows unit tests",
      "problem": "### What Happened?\n\nwhen you run `make test` on windows you end up with a fail on windows unit tests\n\n\n\n### Attach the log file\n\n<img width=\"411\" height=\"141\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/ea9b2559-8495-4508-870f-9c7dd445621b\" />\n\n### Operating System\n\nWindows\n\n### Driver\n\nN/A",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-08-26T17:51:26Z",
      "closed_at": "2026-01-23T19:41:43Z",
      "url": "https://github.com/kubernetes/minikube/issues/21427",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 22390,
      "title": "Can't minikube mount a folder to a VM on macos using podman as a driver",
      "problem": "### What Happened?\n\nI followed through the initial setup page with\n```bash\npodman machine init --cpus 2 --memory 2048 --disk-size 20\npodman machine set --rootful\npodman machine start\n\nminikube start --driver=podman --container-runtime=cri-o\n```\n\nThen tried to mount a folder with\n```bash\nminikube mount ./:/app\n```\nand got \n```\n\u274c  Exiting due to IF_HOST_IP: Error getting the host IP address to use from within the VM: RoutableHostIPFromInside is currently only implemented for linux\n```\nThe error is pretty much self-explanatory, however I didn't find similar opened issues/feature requests of people using podman.\n\n### Attach the log file\n\n[logs.txt](https://github.com/user-attachments/files/24455913/logs.txt)\n\n### Operating System\n\nmacOS (Default)\n\n### Driver\n\nPodman",
      "solution": "Thanks for the context \u2014 this is helpful.\n\nYou\u2019re right that KIC drivers can bind-mount directories via the `--mount-string` flag, and that Podman Machine mounts `$HOME` by default via `containers.conf`. However, this issue specifically affects the `minikube mount` command, which relies on resolving the host IP from inside the container.\n\nCurrently on macOS with the Podman driver, `minikube mount` fails early because `RoutableHostIPFromInside` is not implemented for Podman on darwin, resulting in the error:\n> RoutableHostIPFromInside is currently only implemented for linux\n\nThe proposed fix does not change mount mechanisms or Podman Machine defaults \u2014 it only enables correct host IP resolution on macOS using `host.containers.internal`, mirroring Docker\u2019s macOS behavior. This allows `minikube mount` to function as intended when users choose to use it.\n\nRegarding Fedora / 9p: agreed that filesystem support differs across platforms, but this fix is scoped specifically to macOS (darwin) and does not affect Linux or Fedora-based environments.\n\nI also agree that documenting alternative mount approaches (KIC `--mount-string`, Podman Machine defaults) in the Minikube handbook could be useful \u2014 that would be a good follow-up documentation improvement.\n\nI\u2019ll be opening a PR shortly with the scoped darwin-only fix.\n",
      "labels": [
        "os/macos",
        "co/podman-driver"
      ],
      "created_at": "2026-01-06T16:15:34Z",
      "closed_at": "2026-01-23T04:41:07Z",
      "url": "https://github.com/kubernetes/minikube/issues/22390",
      "comments_count": 3
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21408,
      "title": "containerd: preloaded images fail to save",
      "problem": "### What Happened?\n\nSaving preloaded images (e.g. registry.k8s.io/pause:3.10) with containerd runtime is broken, creating empty tar. Saving images pulled into minikube works.\n\nI suspect that the preloaded images were not prepared with contained, and their internal structure is not the same as the image pulled into the cluster by containerd.\n\n## How to reproduce:\n\n### macOS/vkfit\n\n```console\n% minikube start --driver vfkit --container-runtime containerd\n\ud83d\ude04  minikube v1.36.0 on Darwin 15.6.1 (arm64)\n\u2728  Using the vfkit driver based on user configuration\n\ud83d\udc4d  Starting \"minikube\" primary control-plane node in \"minikube\" cluster\n\ud83d\udcbe  Downloading Kubernetes v1.33.2 preload ...\n    > preloaded-images-k8s-v18-v1...:  376.62 MiB / 376.62 MiB  100.00% 18.46 M\n\ud83d\udd25  Creating vfkit VM (CPUs=2, Memory=6144MB, Disk=20000MB) ...\n\ud83d\udce6  Preparing Kubernetes v1.33.2 on containerd 1.7.23 ...\n\ud83d\udd17  Configuring bridge CNI (Container Networking Interface) ...\n\ud83d\udd0e  Verifying Kubernetes components...\n    \u25aa Using image gcr.io/k8s-minikube/storage-provisioner:v5\n\ud83c\udf1f  Enabled addons: default-storageclass, storage-provisioner\n\ud83c\udfc4  Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\n\n\n% minikube image ls\nregistry.k8s.io/pause:3.10\nregistry.k8s.io/kube-scheduler:v1.33.2\nregistry.k8s.io/kube-proxy:v1.33.2\nregistry.k8s.io/kube-controller-manager:v1.33.2\nregistry.k8s.io/kube-apiserver:v1.33.2\nregistry.k8s.io/etcd:3.5.21-0\nregistry.k8s.io/coredns/coredns:v1.12.0\ngcr.io/k8s-minikube/storage-provisioner:v5\ndocker.io/kindest/kindnetd:v20250512-df8de77b\n\n% minikube image save registry.k8s.io/pause:3.10 test.tar; echo $?\n0\n\n% file test.tar \ntest.tar: empty\n```\n\n### Linux/kvm\n\n```console\n$ minikube start --driver kvm --network default --container-runtime containerd\n\ud83d\ude04  minikube v1.36.0 on Fedora 42 (kvm/amd64)\n\u2728  Using the kvm2 driver based on user configuration\n\ud83d\udc4d  Starting \"minikube\" primary control-plane node in \"minikube\" cluster\n\ud83d\udcbe  Downloading Kubernetes v1.33.2 preload ...\n    > preloaded-images-k8s-v18-v1...:  401.08 MiB / 401.08 MiB  100.00% 15.15 M\n\ud83d\udd25  Creating kvm2 VM (CPUs=2, Memory=6144MB, Disk=20000MB) ...\n\ud83d\udce6  Preparing Kubernetes v1.33.2 on containerd 1.7.23 ...\n\ud83d\udd17  Configuring bridge CNI (Container Networking Interface) ...\n\ud83d\udd0e  Verifying Kubernetes components...\n    \u25aa Using image gcr.io/k8s-minikube/storage-provisioner:v5\n\ud83c\udf1f  Enabled addons: storage-provisioner, default-storageclass\n\ud83c\udfc4  Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\n\n$ minikube image ls\nregistry.k8s.io/pause:3.10\nregistry.k8s.io/kube-scheduler:v1.33.2\nregistry.k8s.io/kube-proxy:v1.33.2\nregistry.k8s.io/kube-controller-manager:v1.33.2\nregistry.k8s.io/kube-apiserver:v1.33.2\nregistry.k8s.io/etcd:3.5.21-0\nregistry.k8s.io/coredns/coredns:v1.12.0\ngcr.io/k8s-minikube/storage-provisioner:v5\ndocker.io/kindest/kindnetd:v20250512-df8de77b\n\n$ minikube image save registry.k8s.io/pause:3.10 test.tar; echo $?\n0\n\n$ file test.tar \ntest.tar: empty\n\n$ minikube image save registry.k8s.io/pause:3.10 - >test.tar\n\n$ file test.tar \ntest.tar: empty\n```\n\n### Attach the log file\n\n[log.txt](https://github.com/user-attachments/files/21950876/log.txt)\n\n\n## Debugging\n\nThe issue is related to the images that come from minikube preloads. If we pull new images into minikube we can save correctly.\n\n### containerd 2.1.4\n\nUsing #21409 \n\n#### Testing inside the guest\n\nSaving registry.k8s.io/pause:3.10 fails:\n\n```console\n$ sudo ctr -n k8s.io image export - registry.k8s.io/pause:3.10 >test.tar\nctr: failed to get reader: content digest sha256:75e060e453aa927883755f715daa02fb335ea7f148a8ab249f779be796d4bb7e: not found\n\n\nPulling echo server:\n\n```console\n$ sudo ctr -n k8s.io image pull docker.io/kicbase/echo-server:1.0\ndocker.io/kicbase/echo server:1.0       \tsaved\t\n\u2514\u2500\u2500index (127ac38a2bb9)                 \talready exists\t\n   \u251c\u2500\u2500manifest (42a89d9b22e5)           \talready exists\t\n   \u2502  \u2514\u2500\u2500config (ce2d2cda2d85)          \talready exists\t\n   \u2514\u2500\u2500manifest (a82eba7887a4)           \tcomplete   \t|++++++++++++++++++++++++++++++++++++++|\t\n      \u2514\u2500\u2500config (9056ab77afb8)          \tcomplete   \t|++++++++++++++++++++++++++++++++++++++|\t\napplication/vnd.docker.distribution.manifest.list.v2+json sha256:127ac38a2bb9537b7f252addff209ea6801edcac8a92c8b1104dacd66a583ed6\nCompleted pull from OCI Registry (docker.io/kicbase/echo-server:1.0)\telapsed: 2.9 s\ttotal:  4.1 Ki\t(1.4 KiB/s)\n```\n\nSaving echo server\n\t\n```console\n$ sudo ctr -n k8s.io image export echo-server-1.0.tar docker.io/kicbase/echo-server:1.0\n\n$ tar tf echo-server-1.0.tar \nblobs/\nblobs/sha256/\nblobs/sha256/127ac38a2bb9537b7f252addff209ea6801edcac8a92c8b1104dacd66a583ed6\nblobs/sha256/42a89d9b22e5307cb88494990d5d929c401339f508c0a7e98a4d8ac52623fc5b\nblobs/sha256/ac2c07efdce850736ed8e7b9c3bc15e251d808fc43b9eeeb88797be0def23730\nblobs/sha256/ce2d2cda2d858fdaea84129deb86d18e5dbf1c548f230b79fdca74cc91729d17\nindex.json\nmanifest.json\noci-layout\n```\n\n#### Saving from host\n\n```console\n% minikube image ls | grep docker.io/kicbase/echo-server:1.0\ndocker.io/kicbase/echo-server:1.0\n\n% minikube image save docker.io/kicbase/echo-server:1.0 echo-server-1.0.tar\n\n% tar tf echo-server-1.0.tar \nblobs/\nblobs/sha256/\nblobs/sha256/127ac38a2bb9537b7f252addff209ea6801edcac8a92c8b1104dacd66a583ed6\nblobs/sha256/42a89d9b22e5307cb88494990d5d929c401339f508c0a7e98a4d8ac52623fc5b\nblobs/sha256/ac2c07efdce850736ed8e7b9c3bc15e251d808fc43b9eeeb88797be0def23730\nblobs/sha256/ce2d2cda2d858fdaea84129deb86d18e5dbf1c548f230b79fdca74cc91729d17\nindex.json\nmanifest.json\noci-layout\n```\n\n### containerd 1.17.23\n\nUsing #21405 \n\n#### Saving inside the guest\n\nSaving pause image fails\n\n```console\n$ sudo ctr -n k8s.io image export pause.tar registry.k8s.io/pause:3.10\nctr: failed to get reader: content digest sha256:75e060e453aa927883755f715daa02fb335ea7f148a8ab249f779be796d4bb7e: not found\n```\n\nPulling echo server:\n\n```console\n$ sudo ctr -n k8s.io image pull docker.io/kicbase/echo-server:1.0\ndocker.io/kicbase/echo-server:1.0:                                                resolved       |++++++++++++++++++++++++++++++++++++++| \nindex-sha256:127ac38a2bb9537b7f252addff209ea6801edcac8a92c8b1104dacd66a583ed6:    done           |++++++++++++++++++++++++++++++++++++++| \nmanifest-sha256:42a89d9b22e5307cb88494990d5d929c401339f508c0a7e98a4d8ac52623fc5b: done           |++++++++++++++++++++++++++++++++++++++| \nconfig-sha256:ce2d2cda2d858fdaea84129deb86d18e5dbf1c548f230b79fdca74cc91729d17:   done           |++++++++++++++++++++++++++++++++++++++| \nlayer-sha256:ac2c07efdce850736ed8e7b9c3bc15e251d808fc43b9eeeb88797be0def23730:    done           |++++++++++++++++++++++++++++++++++++++| \nelapsed: 3.4 s                                                                    total:  2.4 Ki (725.0 B/s)                                       \nunpacking linux/arm64/v8 sha256:127ac38a2bb9537b7f252addff209ea6801edcac8a92c8b1104dacd66a583ed6...\ndone: 33.646725ms\t\n```\n\nSaving echo server:\n\n```console\n$ sudo ctr -n k8s.io image export echo-server-1.0.tar docker.io/kicbase/echo-server:1.0\n\n$ tar tf echo-server-1.0.tar \nblobs/\nblobs/sha256/\nblobs/sha256/127ac38a2bb9537b7f252addff209ea6801edcac8a92c8b1104dacd66a583ed6\nblobs/sha256/42a89d9b22e5307cb88494990d5d929c401339f508c0a7e98a4d8ac52623fc5b\nblobs/sha256/ac2c07efdce850736ed8e7b9c3bc15e251d808fc43b9eeeb88797be0def23730\nblobs/sha256/ce2d2cda2d858fdaea84129deb86d18e5dbf1c548f230b79fdca74cc91729d17\nindex.json\nmanifest.json\noci-layout\n```\n\n#### Saving in the host\n\n```console\n% minikube image ls | grep docker.io/kicbase/echo-server:1.0               \ndocker.io/kicbase/echo-server:1.0\n\n% minikube image save docker.io/kicbase/echo-server:1.0 echo-server-1.0.tar\n\n% tar tf echo-server-1.0.tar\nblobs/\nblobs/sha256/\nblobs/sha256/127ac38a2bb9537b7f252addff209ea6801edcac8a92c8b1104dacd66a583ed6\nblobs/sha256/42a89d9b22e5307cb88494990d5d929c401339f508c0a7e98a4d8ac52623fc5b\nblobs/sha256/ac2c07efdce850736ed8e7b9c3bc15e251d808fc43b9eeeb88797be0def23730\nblobs/sha256/ce2d2cda2d858fdaea84129deb86d18e5dbf1c548f230b79fdca74cc91729d17\nindex.json\nmanifest.json\noci-layout\n```",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/bug",
        "lifecycle/rotten"
      ],
      "created_at": "2025-08-23T19:12:58Z",
      "closed_at": "2026-01-21T23:20:39Z",
      "url": "https://github.com/kubernetes/minikube/issues/21408",
      "comments_count": 10
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21128,
      "title": "remove dependency on installing minikube-maintained binaries for cri-dockerd",
      "problem": "in our Kicbase https://github.com/kubernetes/minikube/blob/4ed65a25ed332e70f1cd5688267448ef8fe4a054/deploy/kicbase/Dockerfile#L129\nwe install cri-docker from binaries that  built and hosted by minikube (manually)\nthe reason is building it form source takes a long time, and they used not have all ARCHS in their release \n\n\nwe need to verify if their releases binaries can be used in our DockerFile\nhttps://github.com/kubernetes/minikube/blob/4ed65a25ed332e70f1cd5688267448ef8fe4a054/deploy/kicbase/Dockerfile#L129\n\nhttps://github.com/Mirantis/cri-dockerd/releases\n\n```\n# Install cri-dockerd from pre-compiled binaries stored in GCS, this is way faster than building from source in multi-arch\nRUN echo \"Installing cri-dockerd ${CRI_DOCKERD_VERSION}\" && \\\n\tcurl -L \"https://storage.googleapis.com/kicbase-artifacts/cri-dockerd/${CRI_DOCKERD_COMMIT}/${TARGETARCH}/cri-dockerd\" -o /usr/bin/cri-dockerd && chmod +x /usr/bin/cri-dockerd && \\\n\tcurl -L \"https://storage.googleapis.com/kicbase-artifacts/cri-dockerd/${CRI_DOCKERD_COMMIT}/cri-docker.socket\" -o /usr/lib/systemd/system/cri-docker.socket && \\\n\tcurl -L \"https://storage.googleapis.com/kicbase-artifacts/cri-dockerd/${CRI_DOCKERD_COMMIT}/cri-docker.service\" -o /usr/lib/systemd/system/cri-docker.service\n\n```\n\nrelated https://github.com/Mirantis/cri-dockerd/issues/523",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-07-23T01:03:32Z",
      "closed_at": "2026-01-21T10:15:40Z",
      "url": "https://github.com/kubernetes/minikube/issues/21128",
      "comments_count": 7
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 17648,
      "title": "Ingress-dns addon now working as expected for multinode clusters",
      "problem": "### What Happened?\n\nWhen in multi-node cluster mode the ingress-dns pod is not scheduled on the primary node leading to the errors setting up on the get started guide.\r\n\r\nSteps to reproduce:\r\n1. `minikube start -n 3 --addons=ingress,ingress-dns`\r\n2. follow the steps in the ingress-dns guide\r\n3. do a dns lookup on the running service  ` nslookup hello-jane.test $(minikube ip)` \r\n\r\nExpected: the nslookup succeeds\r\nActual: the dns lookup fails with communication error: connection refused.\n\n### Attach the log file\n\nN/A\n\n### Operating System\n\nUbuntu\n\n### Driver\n\nDocker",
      "solution": "I can confirm that behavior. A quick fix was to modify the ingress-dns template (see the last to lines of the following snippet):\r\n\r\n```\r\nspec:\r\n  serviceAccountName: minikube-ingress-dns\r\n  hostNetwork: true\r\n  containers:\r\n    - name: minikube-ingress-dns\r\n      image: {{.CustomRegistries.IngressDNS | default .ImageRepository | default .Registries.IngressDNS }}{{.Images.IngressDNS}}\r\n      imagePullPolicy: IfNotPresent\r\n      ports:\r\n        - containerPort: 53\r\n          protocol: UDP\r\n      env:\r\n        - name: DNS_PORT\r\n          value: \"53\"\r\n        - name: POD_IP\r\n          valueFrom:\r\n            fieldRef:\r\n              fieldPath: status.podIP\r\n  nodeSelector:\r\n    minikube.k8s.io/primary: \"true\"\r\n```\r\nWith these modification the ingress-dns addon works on a multi-node cluster too. I am not certian if this is a good solution and should become an PR. Any thought on this?\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten",
        "addon/ingress",
        "co/multinode"
      ],
      "created_at": "2023-11-19T15:01:58Z",
      "closed_at": "2026-01-19T16:58:24Z",
      "url": "https://github.com/kubernetes/minikube/issues/17648",
      "comments_count": 27
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 10874,
      "title": "Using a proxy with containerd runtime",
      "problem": "Now I use the default container runtime (`docker`) in minikube. And at the start, I specify the option to use a proxy:\r\n```\r\nminikube start --docker-env HTTP_PROXY=<my proxy>\r\n```\r\nBut what about using `containerd`? How can I specify the environment setting in this case?\r\n```\r\nminikube start --container-runtime=containerd\r\n```\r\nIn production we are already using a containerd. Therefore, I need to use it in minikube too.\r\nI only found [solution](https://github.com/containerd/cri/issues/834#issuecomment-403417158) when it is used without minikube.",
      "solution": "Issues go stale after 90d of inactivity.\nMark the issue as fresh with `/remove-lifecycle stale`.\nStale issues rot after an additional 30d of inactivity and eventually close.\n\nIf this issue is safe to close now please do so with `/close`.\n\nSend feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n/lifecycle stale\n\n---\n\nStale issues rot after 30d of inactivity.\nMark the issue as fresh with `/remove-lifecycle rotten`.\nRotten issues close after an additional 30d of inactivity.\n\nIf this issue is safe to close now please do so with `/close`.\n\nSend feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n/lifecycle rotten",
      "labels": [
        "kind/feature",
        "priority/backlog",
        "lifecycle/frozen",
        "co/runtime/crio",
        "co/runtime/containerd"
      ],
      "created_at": "2021-03-19T11:47:54Z",
      "closed_at": "2026-01-18T14:41:10Z",
      "url": "https://github.com/kubernetes/minikube/issues/10874",
      "comments_count": 3
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21373,
      "title": "Minikube unusable for multiple users on Ubuntu with docker driver",
      "problem": "### What Happened?\n\nThe main problem I'm facing right now is that Minikube refuses to use docker without root.  \nIf I use a regular `minikube start` run with `--driver docker` it gets the error in the logs below. For some reason it constantly tries to force itself to use docker with root privileges:\n`\ud83d\udccc  Using Docker driver with root privileges`\n\nEither way, once minikube has been created, anyone aside from the main user who created the profile can't seem to access it. Even if the user has access to the same docker group, and updates their MINIKUBE_HOME environment variable, they still get the \"no server found for cluster\" error.\n\nIs there a way to make the server fully shareable between all users?\n\n### Attach the log file\n\n`error: no server found for cluster \"minikube\"`\n\n### Operating System\n\nUbuntu\n\n### Driver\n\nDocker",
      "solution": "Update: After adding some more permissions to the folders and files, I got it to work. The problem now is that the Juju files created after running kubectl commands cause locking to other users on the machine. Even when using umask 0002 and changing the permissions on the tmp/ folder, the juju lock files do not follow the umask rules. They simply get created with very restricted permissions.  \nIs there a fix to make sure the juju-files actually follow the umask rules?\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-08-19T13:19:52Z",
      "closed_at": "2026-01-18T09:45:23Z",
      "url": "https://github.com/kubernetes/minikube/issues/21373",
      "comments_count": 5
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21390,
      "title": "minikube start\u95ee\u9898",
      "problem": "<!-- \u8bf7\u5728\u62a5\u544a\u95ee\u9898\u65f6\u4f7f\u7528\u6b64\u6a21\u677f\uff0c\u5e76\u63d0\u4f9b\u5c3d\u53ef\u80fd\u8be6\u7ec6\u7684\u4fe1\u606f\u3002\u5426\u5219\u53ef\u80fd\u5bfc\u81f4\u54cd\u5e94\u5ef6\u8fdf\u3002\u8c22\u8c22\uff01-->\n\n**\u91cd\u73b0\u95ee\u9898\u6240\u9700\u7684\u547d\u4ee4**\uff1a\nminikube start\n\n**\u5931\u8d25\u7684\u547d\u4ee4\u7684\u5b8c\u6574\u8f93\u51fa**\uff1a<details>\n\n* Ubuntu 24.04 \u4e0a\u7684 minikube v1.36.0\n* \u81ea\u52a8\u9009\u62e9 docker \u9a71\u52a8\u3002\u5176\u4ed6\u9009\u9879\uff1assh, none\n* \u4f7f\u7528\u5177\u6709 root \u6743\u9650\u7684 Docker \u9a71\u52a8\u7a0b\u5e8f\n* \u5728\u96c6\u7fa4\u4e2d \"minikube\" \u542f\u52a8\u8282\u70b9 \"minikube\" primary control-plane\n* \u6b63\u5728\u62c9\u53d6\u57fa\u7840\u955c\u50cf v0.0.47 ...\n* \u6b63\u5728\u4e0b\u8f7d Kubernetes v1.33.1 \u7684\u9884\u52a0\u8f7d\u6587\u4ef6...\n    > preloaded-images-k8s-v18-v1...:  23.19 MiB / 347.04 MiB  6.68% 304.65 KiB! minikube cannot pull kicbase image from any docker registry, and is trying to download kicbase tarball from github release page via HTTP.\n! \u5f88\u53ef\u80fd\u60a8\u9047\u5230\u4e86\u7f51\u7edc\u95ee\u9898\u3002\u8bf7\u786e\u4fdd\u60a8\u53ef\u4ee5\u901a\u8fc7 HTTP \u8bbf\u95ee\u4e92\u8054\u7f51\uff0c\u76f4\u63a5\u8fde\u63a5\u6216\u4f7f\u7528\u4ee3\u7406\u3002\u5f53\u524d\u60a8\u7684\u4ee3\u7406\u914d\u7f6e\u4e3a\uff1a\n\n    > preloaded-images-k8s-v18-v1...:  195.28 MiB / 347.04 MiB  56.27% 98.81 Ki\nE0821 11:53:38.694873   84917 cache.go:225] Error downloading kic artifacts:  failed to download kic base image or any fallback image\n* \u521b\u5efa docker container\uff08CPU=2\uff0c\u5185\u5b58=2200MB\uff09...\n! StartHost \u5931\u8d25\uff0c\u5c06\u8981\u91cd\u8bd5: creating host: create: creating: setting up container node: preparing volume for minikube container: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -d /var/lib: exit status 125\nstdout:\n\nstderr:\nUnable to find image 'gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b' locally\ndocker: Error response from daemon: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\n\nRun 'docker run --help' for more information\n\n* docker \"minikube\" \u7f3a\u5931 container\uff0c\u5c06\u91cd\u65b0\u521b\u5efa\u3002\n! The image 'gcr.io/k8s-minikube/storage-provisioner:v5' was not found; unable to add it to cache.\n! The image 'registry.k8s.io/pause:3.10' was not found; unable to add it to cache.\n! The image 'registry.k8s.io/coredns/coredns:v1.12.0' was not found; unable to add it to cache.\n! The image 'registry.k8s.io/kube-scheduler:v1.33.1' was not found; unable to add it to cache.\n! The image 'registry.k8s.io/kube-proxy:v1.33.1' was not found; unable to add it to cache.\n! The image 'registry.k8s.io/etcd:3.5.21-0' was not found; unable to add it to cache.\n! The image 'registry.k8s.io/kube-apiserver:v1.33.1' was not found; unable to add it to cache.\n! The image 'registry.k8s.io/kube-controller-manager:v1.33.1' was not found; unable to add it to cache.\n* \u521b\u5efa docker container\uff08CPU=2\uff0c\u5185\u5b58=2200MB\uff09...\n* \u542f\u52a8 docker container \u5931\u8d25\u3002\u8fd0\u884c \"minikube delete\" \u53ef\u80fd\u9700\u8981\u4fee\u590d\u5b83\uff1a recreate: creating host: create: creating: setting up container node: preparing volume for minikube container: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -d /var/lib: exit status 125\nstdout:\n\nstderr:\nUnable to find image 'gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b' locally\ndocker: Error response from daemon: Get \"https://gcr.io/v2/\": context deadline exceeded\n\nRun 'docker run --help' for more information\n \n\nX \u56e0 GUEST_PROVISION \u9519\u8bef\u800c\u9000\u51fa\uff1aerror provisioning guest: Failed to start host: recreate: creating host: create: creating: setting up container node: preparing volume for minikube container: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -d /var/lib: exit status 125\nstdout:\n\nstderr:\nUnable to find image 'gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b' locally\ndocker: Error response from daemon: Get \"https://gcr.io/v2/\": context deadline exceeded\n\nRun 'docker run --help' for more information\n\n* \n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                   \u2502\n\u2502    * \u5982\u679c\u4e0a\u8ff0\u5efa\u8bae\u65e0\u6cd5\u5e2e\u52a9\u89e3\u51b3\u95ee\u9898\uff0c\u8bf7\u544a\u77e5\u6211\u4eec\uff1a                                                   \u2502\n\u2502      https://github.com/kubernetes/minikube/issues/new/choose                                     \u2502\n\u2502                                                                                                   \u2502\n\u2502    * \u8bf7\u8fd0\u884c minikube logs --file=logs.txt \u547d\u4ee4\uff0c\u5e76\u5c06\u751f\u6210\u7684 logs.txt \u6587\u4ef6\u9644\u52a0\u5230 GitHub \u95ee\u9898\u4e2d\u3002    \u2502\n\u2502                                                                                                   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n</details>\n\n\n**`minikube logs`\u547d\u4ee4\u7684\u8f93\u51fa**\uff1a <details>\n\nX \u56e0 GUEST_STATUS \u9519\u8bef\u800c\u9000\u51fa\uff1a\u65e0\u6cd5\u83b7\u53d6\u63a7\u5236\u5e73\u9762\u8282\u70b9 minikube \u4e3b\u673a\u72b6\u6001\uff1astate: unknown state \"minikube\": docker container inspect minikube --format={{.State.Status}}: exit status 1\nstdout:\n\n\nstderr:\nError response from daemon: No such container: minikube\n\n</details>\n\n\n**\u4f7f\u7528\u7684\u64cd\u4f5c\u7cfb\u7edf\u7248\u672c**\uff1a\nUbuntu 24.04 ",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten",
        "l/zh-CN"
      ],
      "created_at": "2025-08-21T05:06:11Z",
      "closed_at": "2026-01-18T06:43:22Z",
      "url": "https://github.com/kubernetes/minikube/issues/21390",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21362,
      "title": "Detect Heavy Loads on the CPU and warn the user",
      "problem": "in the CI machines for macos I saw really high load averages Before minikube starts on the Macos\nhttps://github.com/kubernetes/minikube/actions/runs/17037435838/job/48293447533?pr=21353\n\n\n=== Uptime and Load ===\n10:08  up 2 mins, 1 user, load averages: 26.71 14.97 6.23\n\nin those cases it would be nice minikube let the user know in the UI that there their system is under heavy load and suggest them to run with --wait-timeout and even it was the Default Wait time out we can maybe auto add to the wait time out to be more than default 6 miniutes.",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-08-18T19:36:44Z",
      "closed_at": "2026-01-17T04:26:21Z",
      "url": "https://github.com/kubernetes/minikube/issues/21362",
      "comments_count": 5
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 22133,
      "title": "CLI Formatting: Missing newline and stray characters in \"minikube service\" output",
      "problem": "### What Happened?\n\nWhen running minikube service <name>, the output formatting is broken. The status message \"Starting tunnel for service...\" does not include a newline at the end. As a result, the table header (NAMESPACE, NAME, etc.) is printed on the same line, making it hard to read. There is also a stray / character (likely from the spinner animation) stuck at the end of the message.\n\nminikube version: v1.37.0\nShell: zsh\n\n### Attach the log file\n\n<img width=\"835\" height=\"204\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/5f08fac5-e7c6-4959-a321-50a5cad875ef\" />\n\n### Operating System\n\nmacOS (Default)\n\n### Driver\n\nNone",
      "solution": "Hi @Lzzz666 ,\n\nI've noticed this issue and I'm interested in working on a fix. \n\n**Observations:**\n1. The output `Starting tunnel for service hello-node./\u250c\u2500\u2500\u2500` appears to be missing a line break between the status message and the following table.\n2. After a quick look at the code, the issue likely originates in the `startKicServiceTunnel` function within `cmd/minikube/cmd/service.go`, possibly around the call to `service.PrintServiceList` where the formatting might be off.\n\n**Next Step:**\nI'm currently setting up my local build environment to reproduce and debug the issue. If no one else is actively working on this, I plan to submit a PR to fix the formatting.\n\nI'll provide updates here. Any guidance or pointers would be appreciated.",
      "labels": [],
      "created_at": "2025-12-13T08:17:10Z",
      "closed_at": "2026-01-17T01:57:03Z",
      "url": "https://github.com/kubernetes/minikube/issues/22133",
      "comments_count": 1
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 14500,
      "title": "Building an ISO under linux/arm64 does not work",
      "problem": "### What Happened?\r\n\r\nWhen trying to build the minikube ISO under linux/arm64 (either with or without docker), the build script will fail due to the lack of gcc-multilib. gcc-multilib is not available for arm64, see https://packages.ubuntu.com/bionic/gcc-multilib and https://packages.ubuntu.com/focal/gcc-multilib\r\n\r\n### Attach the log file\r\n\r\nlog file not relevant (\"package gcc-multilib has no installation candidate\" is the only message worthy of note)\r\n\r\n### Operating System\r\n\r\nUbuntu focal arm64\r\n\r\n### Driver\r\n\r\nN/A",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/feature",
        "lifecycle/rotten",
        "area/guest-vm",
        "arch/arm64"
      ],
      "created_at": "2022-07-03T14:55:43Z",
      "closed_at": "2026-01-15T20:06:20Z",
      "url": "https://github.com/kubernetes/minikube/issues/14500",
      "comments_count": 15
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21360,
      "title": "SVC_UNREACHABLE: service not available: no running pod for service web-website found",
      "problem": "### What Happened?\n\nwhenever am trying to run **minikube service --all** am receiving an error. \n\n<img width=\"799\" height=\"436\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e5ec9752-b266-4b6f-b702-1aca79288d06\" />\n\nMy pods are not expose to the container jsut because of this, How can I fix this \n\n[Minikube service logs.txt](https://github.com/user-attachments/files/21838092/Minikube.service.logs.txt)\n\n[logs.txt](https://github.com/user-attachments/files/21838114/logs.txt)\n\n\n\n\n### Attach the log file\n\nI cretaed miniube using docker driver and created 2 node cluster, kindly help me with that\n\n### Operating System\n\nWindows\n\n### Driver\n\nNone",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-08-18T14:58:37Z",
      "closed_at": "2026-01-15T16:02:51Z",
      "url": "https://github.com/kubernetes/minikube/issues/21360",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21641,
      "title": "replace go mod depdenncy https://github.com/juju/fslock",
      "problem": "we should explore alternatives,\nmaybe github.com/gofrs/flock ?\n\n\nthis is not a beginner friendly task, plz do not take as a first issue.",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/stale"
      ],
      "created_at": "2025-09-26T19:27:07Z",
      "closed_at": "2026-01-15T09:26:07Z",
      "url": "https://github.com/kubernetes/minikube/issues/21641",
      "comments_count": 1
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21340,
      "title": "ImagePullBackOff when minikube stopped and started.",
      "problem": "### What Happened?\n\nWhen I newly create minikube cluster everything works fine. Images are getting pulled normally.\n\nBut when minikube is restarted due to system crash or intentional start and stop, it is failing to pull  the images. \n\nIf I delete and create the cluster again, images are pulled normally.\n\n### Attach the log file\n\n<img width=\"1460\" height=\"313\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e9d18162-082a-481a-9983-7391e800e828\" />\n\n<img width=\"2495\" height=\"1224\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/30c84452-9aa4-4b78-a7b1-64d55691afae\" />\n\n### Operating System\n\nUbuntu\n\n### Driver\n\nDocker",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-08-15T15:32:06Z",
      "closed_at": "2026-01-12T16:28:26Z",
      "url": "https://github.com/kubernetes/minikube/issues/21340",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 20382,
      "title": "Error while building the minikube ISO",
      "problem": "### What Happened?\n\nHi, \n\nI was trying to build the ISO since I would like to test a new feature the is on cri-o runtime 1.31. \n\n```\ngo version go1.23.6 linux/amd64\n\nGNU Make 4.4.1\nBuilt for x86_64-redhat-linux-gnu\nCopyright (C) 1988-2023 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later <https://gnu.org/licenses/gpl.html>\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\n\n```\n\nSo I changed the version in `deploy/iso/minikube-iso/package/crio-bin/crio-bin.mk` and the \n\n```bash\nmake buildroot-image\n```\n\nthat ends correctly but then the following\n\n```bash\nmake out/minikube-amd64.iso\n```\n\nends with this error\n\n```\ngo: downloading github.com/josharian/intern v1.0.0\necho \"{\\\"iso_version\\\": \\\"v1.35.0\\\", \\\"kicbase_version\\\": \\\"v0.0.46\\\", \\\"minikube_version\\\": \\\"v1.35.0\\\", \\\"commit\\\": \\\"cf3ab2afdf2ac4e1237a74407360187452b2cb6c-dirty\\\"}\" > deploy/iso/minikube-iso/board/minikube/x86_64/rootfs-overlay/version.json\necho v1.35.0 > deploy/iso/minikube-iso/board/minikube/x86_64/rootfs-overlay/etc/VERSION\ncp deploy/iso/minikube-iso/arch/x86_64/Config.in.tmpl deploy/iso/minikube-iso/Config.in\nif [ ! -d /mnt/out/buildroot ]; then \\\n\tmkdir -p /mnt/out; \\\n\tgit clone --depth=1 --branch=2023.02.9 https://github.com/buildroot/buildroot /mnt/out/buildroot; \\\n\tperl -pi -e 's@\\s+source \"package/sysdig/Config\\.in\"\\n@@;' /mnt/out/buildroot/package/Config.in; \\\n\trm -r /mnt/out/buildroot/package/sysdig; \\\n\tcp deploy/iso/minikube-iso/go.hash /mnt/out/buildroot/package/go/go.hash; \\\n\tgit --git-dir=/mnt/out/buildroot/.git config user.email \"dev@random.com\"; \\\n\tgit --git-dir=/mnt/out/buildroot/.git config user.name \"Random developer\"; \\\nfi;\n/usr/bin/make -C /mnt/out/buildroot BR2_EXTERNAL=../../deploy/iso/minikube-iso GO_VERSION=1.21.6 GO_HASH_FILE=/mnt/deploy/iso/minikube-iso/go.hash O=/mnt/out/buildroot/output-x86_64 minikube_x86_64_defconfig\nmake[1]: Entering directory '/mnt/out/buildroot'\n  GEN     /mnt/out/buildroot/output-x86_64/Makefile\n#\n# configuration written to /mnt/out/buildroot/output-x86_64/.config\n#\nmake[1]: Leaving directory '/mnt/out/buildroot'\n/usr/bin/make -C /mnt/out/buildroot BR2_EXTERNAL=../../deploy/iso/minikube-iso GO_VERSION=1.21.6 GO_HASH_FILE=/mnt/deploy/iso/minikube-iso/go.hash O=/mnt/out/buildroot/output-x86_64 host-python3\nmake[1]: Entering directory '/mnt/out/buildroot'\n/usr/bin/make -j1  O=/mnt/out/buildroot/output-x86_64 HOSTCC=\"/usr/bin/gcc\" HOSTCXX=\"/usr/bin/g++\" syncconfig\nmake[2]: Entering directory '/mnt/out/buildroot'\n  GEN     /mnt/out/buildroot/output-x86_64/Makefile\nmake[2]: Leaving directory '/mnt/out/buildroot'\nmake[1]: Leaving directory '/mnt/out/buildroot'\n/usr/bin/make -C /mnt/out/buildroot BR2_EXTERNAL=../../deploy/iso/minikube-iso GO_VERSION=1.21.6 GO_HASH_FILE=/mnt/deploy/iso/minikube-iso/go.hash O=/mnt/out/buildroot/output-x86_64\nmake[1]: Entering directory '/mnt/out/buildroot'\n>>> host-go 1.21.6 Building\ncd /mnt/out/buildroot/output-x86_64/build/host-go-1.21.6/src && GO111MODULE=off GOCACHE=/mnt/out/buildroot/output-x86_64/host/share/host-go-cache GOROOT_BOOTSTRAP=/mnt/out/buildroot/output-x86_64/host/lib/go-1.19.11 GOROOT_FINAL=/mnt/out/buildroot/output-x86_64/host/lib/go GOROOT=\"/mnt/out/buildroot/output-x86_64/build/host-go-1.21.6\" GOBIN=\"/mnt/out/buildroot/output-x86_64/build/host-go-1.21.6/bin\" GOOS=linux CC=/usr/bin/gcc CXX=/usr/bin/g++ CGO_ENABLED=1 CC_FOR_TARGET=\"/mnt/out/buildroot/output-x86_64/host/bin/x86_64-minikube-linux-gnu-gcc\" CXX_FOR_TARGET=\"/mnt/out/buildroot/output-x86_64/host/bin/x86_64-minikube-linux-gnu-g++\" GOOS=\"linux\" GOARCH=amd64   GO_ASSUME_CROSSCOMPILING=1 ./make.bash \nBuilding Go cmd/dist using /mnt/out/buildroot/output-x86_64/host/lib/go-1.19.11. (go1.19.11 linux/amd64)\nBuilding Go toolchain1 using /mnt/out/buildroot/output-x86_64/host/lib/go-1.19.11.\nBuilding Go bootstrap cmd/go (go_bootstrap) using Go toolchain1.\nBuilding Go toolchain2 using go_bootstrap and Go toolchain1.\ngo: downloading go1.23.4 (linux/amd64)\ngo: download go1.23.4: golang.org/toolchain@v0.0.1-go1.23.4.linux-amd64: no http in bootstrap go command\ngo tool dist: FAILED: /mnt/out/buildroot/output-x86_64/build/host-go-1.21.6/pkg/tool/linux_amd64/go_bootstrap install -pgo=off cmd/asm cmd/cgo cmd/compile cmd/link: exit status 1\nmake[1]: *** [package/pkg-generic.mk:283: /mnt/out/buildroot/output-x86_64/build/host-go-1.21.6/.stamp_built] Error 2\nmake[1]: Leaving directory '/mnt/out/buildroot'\nmake: *** [Makefile:313: minikube-iso-x86_64] Error 2\nrm deploy/iso/minikube-iso/board/minikube/x86_64/rootfs-overlay/usr/bin/auto-pause\nmake: *** [Makefile:345: out/minikube-amd64.iso] Error 2\n\n```\n\nCan someone help with it?\n\n### Attach the log file\n\nThe issue refers to the image build\n\n### Operating System\n\nRedhat/Fedora\n\n### Driver\n\nNone",
      "solution": "The error \n\n> go: downloading go1.23.4 (linux/amd64)\n> go: download go1.23.4: golang.org/toolchain@v0.0.1-go1.23.4.linux-amd64: no http in bootstrap go command\n\nsuggests that Go 1.23.4 is being fetched, but the actual setup uses Go 1.21.6. There might be a misalignment between the expected and actual Go version, which is creating the problem.\n\n\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/support",
        "lifecycle/rotten"
      ],
      "created_at": "2025-02-10T09:43:57Z",
      "closed_at": "2025-08-24T01:35:03Z",
      "url": "https://github.com/kubernetes/minikube/issues/20382",
      "comments_count": 14
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 20596,
      "title": "Minikube Start Problem",
      "problem": "### What Happened?\n\nHi,\n\nWhen I'm running % minikube start\nI got error message this below;\n\nstderr:\nUnable to find image 'gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279' locally\ngcr.io/k8s-minikube/kicbase@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279: Pulling from k8s-minikube/kicbase\nDigest: sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279\ndocker: cannot overwrite digest sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279\n\nPlease solve ASAP, thanks\n\n### Attach the log file\n\n[logs.txt](https://github.com/user-attachments/files/19627892/logs.txt)\n\n### Operating System\n\nmacOS (Default)\n\n### Driver\n\nDocker",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-04-07T07:42:24Z",
      "closed_at": "2025-09-16T12:54:45Z",
      "url": "https://github.com/kubernetes/minikube/issues/20596",
      "comments_count": 6
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21264,
      "title": "minikube kubectl should always use the profile's minikube context",
      "problem": "### What Happened?\n\nIt's reasonable to expect that when using `minikube kubectl`, it will only mess with minikube.\n\nI can't think of a reasonable use case where someone would want to use the minikube version of kubectl to mess with a context other than minikube and would have no alternatives.\n\nBut I can think of cases where testing scripts run in the wrong context can mess with unintended contexts.\n\nUsing `minikube kubectl` would guarantee against such problems it it was always using the `minikube` context of the selected profile.\n\n\n### Attach the log file\n\nN/A\n\n### Operating System\n\nNone\n\n### Driver\n\nNone",
      "solution": "@didier-viboo What do you mean by \"using the minikube context?\n\nMinikube supports multiple profiles of arbitrary names, so it cannot limit the `minikube kubectl` to context \"minikube\". It also supports multi-nodes clusters that have multiple contexts (name, name-m02, ...).\n\nMaybe you mean that the command should work only if we have a minikube profile with the same name as the --context argument?\n\nThe problem is that kubectl cannot be limited to certain minikube profile, some commands like `kubectl config` do not work on a single context.\n\nAlso why there is a need to limit the scope of the minikube kubectl command? this command is not installed in the PATH so it does not affect usage of standard kubectl command installed in the PATH (e.g /usr/local/bin).\n\n---\n\n> [@didier-viboo](https://github.com/didier-viboo) What do you mean by \"using the minikube context?\n> \n> Minikube supports multiple profiles of arbitrary names, so it cannot limit the `minikube kubectl` to context \"minikube\". It also supports multi-nodes clusters that have multiple contexts (name, name-m02, ...).\n> \n> Maybe you mean that the command should work only if we have a minikube profile with the same name as the --context argument?\n> \n> The problem is that kubectl cannot be limited to certain minikube profile, some commands like `kubectl config` do not work on a single context.\n> \n> Also why there is a need to limit the scope of the minikube kubectl command? this command is not installed in the PATH so it does not affect usage of standard kubectl command installed in the PATH (e.g /usr/local/bin).\n\nThere are contexts which point at minikube instances and contexts which don't.\n\nMy point is that when using `minikube kubectl` to access a context not pointing at a minikube instance it should fail.\n\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-08-08T09:10:41Z",
      "closed_at": "2026-01-08T07:34:09Z",
      "url": "https://github.com/kubernetes/minikube/issues/21264",
      "comments_count": 7
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21477,
      "title": "UI: Add more Korean Translations",
      "problem": "## Issue Contents\nadd and fix korean translations(`ko.json`)\nit's too many context(L1~L1100).\n\n\n## \ud83e\udd5d Comment\nThis issue is so large, I'd like to divide it into 100-line chunks and work on them. I'll post a PR once the work is complete, and the progress is shown below.\n\n## progress - `ko.json`\n\n### L1~100\n- \ud83d\udd17 PR: https://github.com/kubernetes/minikube/pull/21465 (merged)\n\n### L101~200\n- \ud83d\udd17 PR: https://github.com/kubernetes/minikube/pull/21467 (merged)\n\n### L201~300\n- \ud83d\udd17 PR: (in-progress)\n\n### L301~400\n\n### L401~500\n\n### L501~600\n\n### L601~700\n\n### L701~800\n\n### L801~900\n\n### L901~1000\n\n### L1001~end\n\n### convention unification\n\n\n",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-09-03T05:04:00Z",
      "closed_at": "2026-01-07T01:20:36Z",
      "url": "https://github.com/kubernetes/minikube/issues/21477",
      "comments_count": 3
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21052,
      "title": "podman-env support needs to update or deprecate",
      "problem": "### What Happened?\n\nMinikube continues to embed podman 3.4.4, my system is podman 4.9.2 (ships standard with ubuntu noble 24.04) & my machine-default-image is podman 5.9.2\n\nI can't reopen #15036 but here we are 3 years later.   No progress.   Same issue. \n\nThe discussion in issue/15036 speaks of a future to come re `podman-env`, and some other nonsense that failed to materialize.  Minikube podman-env is effectively incompatible with all modern LTS versions of podman.  \n\nI want to understand why, I think minikube should *at least* output an explicit warning saying \"hey, podman-env is irreprarably broked and will never be fixed, and we'll remove it someday\" if that's true, if it's just docker forever, \nOR I want to at least (for now) link an issue .. maybe this one to the warning & workarounds so somebody else doesn't speak the ~6 hours I did going through upgrades, and ultimately being told \"why don't you just use actual docker derp derp\".    What a waste of time, the docker mafia strikes again. \n\nYes, I can work around this with a local insecure registry or several other ways.  \n\nThe specific error to reproduce:\n```\nminikube start --driver=podman --container-runtime=cri-o --kubernetes-version=v1.33.1\n\neval $(minikube -p minikube podman-env)\n\npodman build -t middleware:latest -f ./middleware/Dockerfile ./middleware\n\nCannot connect to Podman. Please verify your connection to the Linux system using `podman system connection list`, or try `podman machine init` and `podman machine start` to manage a new Linux VM\nError: unable to connect to Podman socket: server API version is too old. Client \"4.0.0\" server \"3.4.4\": ssh://docker@127.0.0.1:44503/run/podman/podman.sock\n\n```\n\n\n\n### Attach the log file\n\n\n```\n lsb_release -a\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 24.04.2 LTS\nRelease:        24.04\nCodename:       noble\n```\n\n### Operating System\n\nUbuntu\n\n### Driver\n\nQEMU",
      "solution": "Okay \u2014 @LJTian has closed #20884 (WIP) without progress. \ud83d\udc4b\ud83c\udffb @afbjorklund, any thoughts on the status of #15461?\n\nI'm happy to investigate and submit a fresh solution.\nFor those interested: I\u2019ve successfully run Podman 4.9.2 with Minikube 1.36.0, but only by setting up an insecure registry. I couldn\u2019t get it working with Podman + containerd \u2014 only with CRI-O.\n\nFWIW, I had a discussion with the VSCode team about Podman support \u2014 the consensus was that there are numerous subtle but significant differences between Docker and Podman (too many to list). Trying to support both with a single module introduces complexity and risk. I believe maintaining a separate podman-env module is the better path \u2014 while recommending Docker as the default.\n\nTL;DR: Separation of concerns.\nDocker users generally don\u2019t want to read through Podman-specific errata, and Podman users deserve more detailed, focused documentation without breaking Docker flows or tests. A clean separation avoids cross-contamination and frustration.\n\nRemoving podman-env would be a step backward. Instead, it should be modernized:\n\n* Keep it lean and focused, Reuse patterns from docker-env where feasible\n* Support rootless and advanced Podman features\n* Avoid muddying docker-env with Podman-specific edge cases\n* Ensure test failures are isolated (Docker \u2260 Podman and vice versa)\n\nPodman aims for Docker compatibility, but the reality is messier \u2014 many projects and distros have different assumptions, timelines, and packaging quirks. Without someone from the Podman/CRI-O teams explicitly working to improve Docker compatibility in Minikube, I think the best approach is pragmatic: a dedicated, well-scoped podman-env.\n\nLeaving this here for anyone who wants to weigh in or kick off a feasibility investigation. Happy to help move it forward.\n\n---\n\nAs far as I know, it is the same socket*. So even if there is only docker-env, we should be able to use it with podman too?\n\\* what I mean is that both Docker API (compat) and Podman API (libpod) is on the same sock, but with different paths\n\nLike: `CONTAINER_HOST=$DOCKER_HOST`\n\nDocker has no support for ssh keys though, so the workaround is to feed them to the ssh agent directly (no variable).\nBut that workaround should also work with podman, you don't have to provide an identity if one is already available.\n\nhttps://github.com/kubernetes/minikube/issues/15461#issuecomment-1336362019\n\n---\n\n## \ud83d\udd0d Related Issue Found\n\nDiscovered a separate CRI-O bug while investigating podman-env: **#21251**\n\n**TL;DR**: CRI-O incorrectly resolves  \u2192 , causing  errors.\n\nThis is **separate** from podman-env connectivity - it's a runtime image resolution bug. Both impact podman users but through different mechanisms.\n\nOriginal report: https://github.com/elasticdotventures/minikube/issues/1",
      "labels": [
        "priority/awaiting-more-evidence",
        "lifecycle/rotten",
        "co/runtime/crio"
      ],
      "created_at": "2025-07-09T23:45:09Z",
      "closed_at": "2026-01-04T00:51:14Z",
      "url": "https://github.com/kubernetes/minikube/issues/21052",
      "comments_count": 16
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21256,
      "title": "I can't start my minikube on my VPS based on ubuntu 23.04",
      "problem": "### What Happened?\n\nminikube star --driver=docker\n\nFailed to enable unit, unit **containerd-fuse-overlayfs.service does not exist**.: container exited unexpectedly\n\n### Attach the log file\n\n~$ minikube start --driver=docker\n\n* minikube v1.36.0 sur Ubuntu 23.04\n* Utilisation du pilote docker bas\u00e9 sur la configuration de l'utilisateur\n! docker utilise actuellement le pilote de stockage fuse-overlayfs, d\u00e9finition de preload=false\n* Utilisation du pilote Docker avec le privil\u00e8ge root\n* D\u00e9marrage du n\u0153ud \"minikube\" primary control-plane dans le cluster \"minikube\"\n* Extraction de l'image de base v0.0.47...\n* Cr\u00e9ation de docker container (CPU=2, Memory=2200Mo) ...\n* N\u0153ud d'arr\u00eat \"minikube\" ...\n* Suppression de \"minikube\" dans docker...\n! StartHost a \u00e9chou\u00e9, mais va r\u00e9essayer\u00a0: creating host: create: creating: create kic node: container name \"minikube\": log: 2025-08-06T18:14:42.154726988Z  + echo 'INFO: enabling containerd-fuse-overlayfs service'\n2025-08-06T18:14:42.154731466Z  INFO: enabling containerd-fuse-overlayfs service\n2025-08-06T18:14:42.154734852Z  + systemctl enable containerd-fuse-overlayfs\n2025-08-06T18:14:42.191245421Z  Failed to enable unit, unit containerd-fuse-overlayfs.service does not exist.: container exited unexpectedly\n* Cr\u00e9ation de docker container (CPU=2, Memory=2200Mo) ...\n* \u00c9chec du d\u00e9marrage de docker container. L'ex\u00e9cution de \"minikube delete\" peut r\u00e9soudre le probl\u00e8me : creating host: create: creating: create kic node: container name \"minikube\": log: 2025-08-06T18:15:17.611275314Z  + echo 'INFO: enabling containerd-fuse-overlayfs service'\n2025-08-06T18:15:17.611284687Z  INFO: enabling containerd-fuse-overlayfs service\n2025-08-06T18:15:17.611290274Z  + systemctl enable containerd-fuse-overlayfs\n2025-08-06T18:15:17.622410583Z  Failed to enable unit, unit containerd-fuse-overlayfs.service does not exist.: container exited unexpectedly\n\n! Le conteneur minikube docker s'est ferm\u00e9 de mani\u00e8re inattendue.\n* Si vous \u00eates toujours int\u00e9ress\u00e9 \u00e0 faire fonctionner le pilote docker. Les suggestions suivantes pourraient vous aider \u00e0 surmonter ce probl\u00e8me :\n\n        * - Nettoyer les images docker non utilis\u00e9es, les volumes, les r\u00e9seaux et les conteneurs abandonn\u00e9es.\n\n                                docker system prune --volumes\n\n        * - Red\u00e9marrer votre service docker\n\n        * - Supprimer et recr\u00e9er le cluster de minikube\n                minikube delete\n                minikube start --driver=docker\n\nX Fermeture en raison de GUEST_PROVISION_EXIT_UNEXPECTED : Failed to start host: creating host: create: creating: create kic node: container name \"minikube\": log: 2025-08-06T18:15:17.611275314Z  + echo 'INFO: enabling containerd-fuse-overlayfs service'\n2025-08-06T18:15:17.611284687Z  INFO: enabling containerd-fuse-overlayfs service\n2025-08-06T18:15:17.611290274Z  + systemctl enable containerd-fuse-overlayfs\n2025-08-06T18:15:17.622410583Z  Failed to enable unit, unit containerd-fuse-overlayfs.service does not exist.: container exited unexpectedly\n\n\n### Operating System\n\nUbuntu\n\n### Driver\n\nDocker",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-08-06T18:17:44Z",
      "closed_at": "2026-01-03T19:46:15Z",
      "url": "https://github.com/kubernetes/minikube/issues/21256",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 20340,
      "title": "Not able to run Minikube using Podman",
      "problem": "### What Happened?\n\nI was trying to install Minikube in Mac M3 arm, when trying to install it using \"Minikube start --driver Podman\", I was not able to install it due the below error\n\n### Attach the log file\n\n\n\u274c  Exiting due to GUEST_STATUS: Unable to get control-plane node minikube host status: state: unknown state \"minikube\": podman container inspect minikube --format={{.State.Status}}: exit status 125\nstdout:\n\nstderr:\nError: no such container \"minikube\"\n\n### Operating System\n\nmacOS (Default)\n\n### Driver\n\nPodman",
      "solution": "If (in case) you want to run minikube using `docker`, you can follow the steps mentioned below, which worked for me: \n1. If you executed these commands: `minikube config set driver podman` and `minikube config set rootless true` in past while setting up podman then you need to unset it using `minikube config unset driver` and `minikube config unset rootless`.\n2. Install docker (if not installed) and ensure docker engine is in running state.\n3. Then start minikube using `minikube start --driver=docker`\n4. If you still get error then run `minikube delete` and again start minikube.\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/support",
        "lifecycle/rotten"
      ],
      "created_at": "2025-01-30T18:48:38Z",
      "closed_at": "2025-08-21T11:14:02Z",
      "url": "https://github.com/kubernetes/minikube/issues/20340",
      "comments_count": 11
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21219,
      "title": "Unable to install minikube in root user",
      "problem": "### What Happened?\n\nI have run the following commands to install minikube but it is throwing error\n\nSetup Kubernetes (through Minikube, t2.medium i.e 2 CPU's)\n-------------------------\nInstall Docker\n$ sudo apt update && sudo apt -y install docker.io\n\n Install kubectl\n$ curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.23.7/bin/linux/amd64/kubectl && chmod +x ./kubectl && sudo mv ./kubectl /usr/local/bin/kubectl\n\n Install Minikube\n$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v1.23.2/minikube-linux-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/\n\n Start Minikube\n$  sudo apt install conntrack\n$  minikube start --vm-driver=none\n$  minikube status\n\nAttached the error details\nplease assist.\n\n### Attach the log file\n\n\nBelow are the errors i got while installling minikube\n\n* Preparing Kubernetes v1.22.2 on Docker 27.5.1 ...\n  - kubelet.resolv-conf=/run/systemd/resolve/resolv.conf\n    > kubelet.sha256: 64 B / 64 B [--------------------------] 100.00% ? p/s 0s\n    > kubeadm.sha256: 64 B / 64 B [--------------------------] 100.00% ? p/s 0s\n    > kubectl.sha256: 64 B / 64 B [--------------------------] 100.00% ? p/s 0s\n    > kubeadm: 43.71 MiB / 43.71 MiB [-------------] 100.00% 10.42 MiB p/s 4.4s\n    > kubectl: 44.73 MiB / 44.73 MiB [-------------] 100.00% 10.31 MiB p/s 4.5s\n    > kubelet: 146.25 MiB / 146.25 MiB [------------] 100.00% 13.62 MiB p/s 11s\n! initialization failed, will try again: wait: /bin/bash -c \"sudo env PATH=/var/lib/minikube/binaries/v1.22.2:$PATH kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,Mem\": exit status 1\nstdout:\n[init] Using Kubernetes version: v1.22.2\n[preflight] Running pre-flight checks\n\nstderr:\n        [WARNING FileExisting-socat]: socat not found in system path\n        [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 27.5.1. Latest validated version: 20.10\n        [WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'\nerror execution phase preflight: [preflight] Some fatal errors occurred:\n        [ERROR FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables does not exist\n[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`\nTo see the stack trace of this error execute with --v=5 or higher\n\n*\nX Error starting cluster: wait: /bin/bash -c \"sudo env PATH=/var/lib/minikube/binaries/v1.22.2:$PATH kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,Mem\": exit status 1\nstdout:\n[init] Using Kubernetes version: v1.22.2\n[preflight] Running pre-flight checks\n\nstderr:\n        [WARNING FileExisting-socat]: socat not found in system path\n        [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 27.5.1. Latest validated version: 20.10\n        [WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'\nerror execution phase preflight: [preflight] Some fatal errors occurred:\n        [ERROR FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables does not exist\n[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`\nTo see the stack trace of this error execute with --v=5 or higher\n\n*\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                             \u2502\n\u2502    * If the above advice does not help, please let us know:                                 \u2502\n\u2502      https://github.com/kubernetes/minikube/issues/new/choose                               \u2502\n\u2502                                                                                             \u2502\n\u2502    * Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    \u2502\n\u2502                                                                                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\nX Exiting due to GUEST_START: wait: /bin/bash -c \"sudo env PATH=/var/lib/minikube/binaries/v1.22.2:$PATH kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,Mem\": exit status 1\nstdout:\n[init] Using Kubernetes version: v1.22.2\n[preflight] Running pre-flight checks\n\nstderr:\n        [WARNING FileExisting-socat]: socat not found in system path\n        [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 27.5.1. Latest validated version: 20.10\n        [WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'\nerror execution phase preflight: [preflight] Some fatal errors occurred:\n        [ERROR FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables does not exist\n[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`\nTo see the stack trace of this error execute with --v=5 or higher\n\n*\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                             \u2502\n\u2502    * If the above advice does not help, please let us know:                                 \u2502\n\u2502      https://github.com/kubernetes/minikube/issues/new/choose                               \u2502\n\u2502                                                                                             \u2502\n\u2502    * Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    \u2502\n\n\n### Operating System\n\nUbuntu\n\n### Driver\n\nNone",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-08-02T21:25:27Z",
      "closed_at": "2025-12-30T23:06:13Z",
      "url": "https://github.com/kubernetes/minikube/issues/21219",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21217,
      "title": "Add Sub Steps for Creating VMs",
      "problem": "After this PR is merged\n\nhttps://github.com/kubernetes/minikube/pull/21215#issue-3285138715\n\nnow that we have a Hiding Sub Step Mechanisim, we can break down the \"\ud83d\udd25 Creating krunkit VM \" Step to sub steps and Show and Hide while it is creaiting\n\n```\n\ud83d\ude04  minikube v1.36.0 on Darwin 15.5 (arm64)\n    \u25aa MINIKUBE_ACTIVE_DOCKERD=minikube\n\u2728  Using the krunkit (experimental) driver based on user configuration\n\ud83d\udc4d  Starting \"minikube\" primary control-plane node in \"minikube\" cluster\n\ud83d\udd25  Creating krunkit VM (CPUs=2, Memory=6144MB, Disk=20000MB) ...\n----> TODO add sub steps here too <---\n    \u25aa Setup SSH keys\n    \u25aa Booting the VM\n    \u25aa Configure CRuntime\n...\n\n\ud83d\udc33  Preparing Kubernetes v1.33.2 on Docker 28.3.2 ...\n---> THESE 3 STEPS Show and HIDE< -----\n    \u25aa Generating certificates and keys ...(hide after showing)\n    \u25aa Booting up control plane ...(hide after showing)\n    \u25aa Configuring RBAC rules ...(hide after showing)\n---> END < -----\n\ud83d\udd17  Configuring bridge CNI (Container Networking Interface) ...\n\ud83d\udd0e  Verifying Kubernetes components...\n    \u25aa Using image gcr.io/k8s-minikube/storage-provisioner:v5\n\ud83c\udf1f  Enabled addons: default-storageclass, storage-provisioner\n\ud83c\udfc4  Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\n```",
      "solution": "Showing message for short time and hiding them makes it harder to understand what's going on, and what went wrong on errors. I would keep the current behavior - anything we show remains.\n\nIf our issue is that start creates too many messages we can try to minimize the number of messages.\n\nIf the issue is the that the spinner clears the output when done, we need to fix it or disable the spinner. The spinner is little nicer than static \"...\" but it is not critical.\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-08-02T04:22:57Z",
      "closed_at": "2025-12-30T22:05:13Z",
      "url": "https://github.com/kubernetes/minikube/issues/21217",
      "comments_count": 5
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 22104,
      "title": "Upstream GO_HASH_FILE variable for buildroot go package",
      "problem": "We define GO_HASH_FILE= to override buildroot go.hash file:\n\n```Makefile\n# the go version on the line below is for the ISO\nGOLANG_OPTIONS = GOWORK=off GO_VERSION=1.23.4 GO_HASH_FILE=$(PWD)/deploy/iso/minikube-iso/go.hash\nBUILDROOT_OPTIONS = BR2_EXTERNAL=../../deploy/iso/minikube-iso $(GOLANG_OPTIONS)\n```\n\nbut there is no such variable in the buildroot go package, \nSo we just replace the existing one:\n\n```Makeile\ncp deploy/iso/minikube-iso/go.hash $(BUILD_DIR)/buildroot/package/go/go.hash\n```\n\nProbably should be upstreamed properly...\n\nThe real files for the \"go\" package are in https://github.com/buildroot/buildroot/tree/2025.02.x/package/go\n\nWe can upstream this change to the buildroot package so we don't need to replace the go.hash file.\n\n_Originally posted by @afbjorklund in https://github.com/kubernetes/minikube/issues/22101#issuecomment-3641539986_\n            ",
      "solution": "You could override the VERSION variable in Buildroot, but not the HASH_FILE which was static:\n\nhttps://github.com/buildroot/buildroot/blob/2022.02.x/package/pkg-generic.mk#L522\n\nBut it seems that it has been fixed since, and that you can now affect the HASH_FILE**S** variable:\n\nhttps://github.com/buildroot/buildroot/blob/2024.02.x/package/pkg-generic.mk#L512\n\n----\n\nhttps://github.com/buildroot/buildroot/commit/5d36710e36fc4698c8fae71675bcff7395246006\n\nIt now goes in the patch directory.\n\n`GO_VERSION=1.25.5`\n\n`patches/go/go.hash`\n\n```\n# sha256 checksum from https://go.dev/dl/\nsha256  e1cce9379a24e895714a412c7ddd157d2614d9edbe83a84449b6e1840b4f1226  go1.23.12.src.tar.gz\nsha256  3b2fd446e26642555d1446a38ccbefb2a30bba3179d3ef132ed64d3c63b0c42a  go1.23.12.linux-386.tar.gz\nsha256  d3847fef834e9db11bf64e3fb34db9c04db14e068eeb064f49af747010454f90  go1.23.12.linux-amd64.tar.gz\nsha256  52ce172f96e21da53b1ae9079808560d49b02ac86cecfa457217597f9bc28ab3  go1.23.12.linux-arm64.tar.gz\nsha256  9704eba01401a3793f54fac162164b9c5d8cc6f3cab5cee72684bb72294d9f41  go1.23.12.linux-armv6l.tar.gz\nsha256  1a7cc5f7baeaf39125dce5d660a39438e7f0e04d13d3498590d240aae976b565  go1.23.12.linux-ppc64le.tar.gz\nsha256  2f43708aa0922d692da0a1fc775475c343907610bec77002de1bbe37601ea338  go1.23.12.linux-s390x.tar.gz\nsha256  2d36597f7117c38b006835ae7f537487207d8ec407aa9d9980794b2030cbc067  LICENSE\nsha256  22a5fd0a91efcd28a1b0537106b9959b2804b61f59c3758b51e8e5429c1a954f  go1.25.5.src.tar.gz\nsha256  9e9b755d63b36acf30c12a9a3fc379243714c1c6d3dd72861da637f336ebb35b  go1.25.5.linux-amd64.tar.gz\nsha256  b00b694903d126c588c378e72d3545549935d3982635ba3f7a964c9fa23fe3b9  go1.25.5.linux-arm64.tar.gz\n```\n\nhttps://github.com/buildroot/buildroot/blob/2025.02.x/package/go/go.hash\n\nThere is also go-bin package now, if you want to use the upstream binary and not build it.\n\nhttps://github.com/buildroot/buildroot/tree/2025.02.x/package/go/go-bin\n\n```\n                  (X) host go (source)\n                  ( ) host go (pre-built)\n```\n\n```\nBR2_PACKAGE_HOST_GO=y\nBR2_PACKAGE_HOST_GO_SRC=y\n# BR2_PACKAGE_HOST_GO_BIN is not set\nBR2_PACKAGE_PROVIDES_HOST_GO=\"host-go-src\"\n```\n\nEDIT: It is not really a make variable to be defined",
      "labels": [
        "kind/cleanup",
        "area/guest-vm"
      ],
      "created_at": "2025-12-11T14:14:23Z",
      "closed_at": "2025-12-29T00:31:06Z",
      "url": "https://github.com/kubernetes/minikube/issues/22104",
      "comments_count": 6
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21211,
      "title": "[registry-creds] Underlying docker image not available for arm64",
      "problem": "### What Happened?\n\nThank you for Minikube, usage thus far has been good using the qemu2 driver on macOS.\n\nOne issue is that https://hub.docker.com/r/upmcenterprises/registry-creds/tags has no multiarch images published, so attempting to `minikube addons enable registry-creds` ends in `CrashLoopBackOff` with the logs `exec /registry-creds: exec format error`.\n\nImage: `docker.io/upmcenterprises/registry-creds:1.10@sha256:93a633d4f2b76a1c66bf19c664dbddc56093a543de6d54320f19f585ccd7d605`\n\n### Attach the log file\n\n[mylog.txt](https://github.com/user-attachments/files/21535126/mylog.txt)\n\n### Operating System\n\nmacOS (Default)\n\n### Driver\n\nQEMU",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-07-31T17:34:53Z",
      "closed_at": "2025-12-28T18:42:51Z",
      "url": "https://github.com/kubernetes/minikube/issues/21211",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21441,
      "title": "Default BR2_PACKAGE_GLUSTER to n (no) for Buildroot Minikube iso build or build newer gluster version",
      "problem": "People following https://minikube.sigs.k8s.io/docs/contrib/building/iso/ on newer distros (even using docker) will hit a python2 dependecy.  At first I thought this was a minkube build process dependency , but eventually became clear it's the version of Gluster we attempt to build in which is so old it depends on python2.\n\n<strike>Looking round corners- when the runner in [update-iso-image-versions.yml](https://github.com/kubernetes/minikube/blob/e19892000cb42d6e31b9c35009e58ed6920e1460/.github/workflows/update-iso-image-versions.yml#L19) gets bumped from `ubuntu-22.04` to [`ubuntu-24.04` or `ubuntu-latest`](https://docs.github.com/en/actions/reference/runners/github-hosted-runners#standard-github-hosted-runners-for-public-repositories) </strike>\n\n*\n```\nchecking whether /usr/bin/python2 version >= 2.6... configure: error: too old\nmake[2]: *** [package/pkg-generic.mk:263: /home/minikub-build/minikube/out/buildroot/output-x86_64/build/gluster-4.1.5/.stamp_configured] Error 127\nmake[1]: *** [Makefile:83: _all] Error 2\nmake[1]: Leaving directory '/home/minikub-build/minikube/out/buildroot'\nmake: *** [Makefile:306: minikube-iso-x86_64] Error 2\n```\n\n\nhttps://github.com/kubernetes/minikube/blob/e19892000cb42d6e31b9c35009e58ed6920e1460/.github/workflows/update-iso-image-versions.yml#L19\n\nRelated: https://github.com/kubernetes/minikube/pull/20830\n\n\n\ud83e\udd14*  this is more/also a gluster issue than anything else. Should we instead change `BR2_PACKAGE_GLUSTER` default to `n` since it appears to require python2 ([package](https://github.com/kubernetes/minikube/blob/8fe657329366881e06d04803df8df4e01071e874/deploy/iso/minikube-iso/package/gluster/gluster.mk#L7) [newer version of gluster](https://docs.gluster.org/en/main/release-notes/#glusterfs-5-release-notes))? minikube iso currently [instructs Buildroot to build a very old 4.1.5 version](https://github.com/kubernetes/minikube/blob/8fe657329366881e06d04803df8df4e01071e874/deploy/iso/minikube-iso/package/gluster/gluster.mk#L7) the next version up is a [major though (5)[(https://docs.gluster.org/en/main/release-notes/) which may or may not be compoatible with CSI\n\n```\ngit diff deploy/iso/minikube-iso/package/gluster/Config.in \ndiff --git a/deploy/iso/minikube-iso/package/gluster/Config.in b/deploy/iso/minikube-iso/package/gluster/Config.in\nindex cc5f3b540..124d9bd16 100644\n--- a/deploy/iso/minikube-iso/package/gluster/Config.in\n+++ b/deploy/iso/minikube-iso/package/gluster/Config.in\n@@ -1,6 +1,6 @@\n config BR2_PACKAGE_GLUSTER\n        bool \"gluster\"\n-       default y\n+       default n\n        select BR2_PACKAGE_LIBURCU\n        select BR2_PACKAGE_LIBXML2\n        help\n```\n\nWith the above (disabling gluster from being built-in), one gets a sucessful build of minikube iso:\n\n```\n# x86_64 ISO is still BIOS rather than EFI because of AppArmor issues for KVM, and Gen 2 issues for Hyper-V\nif [ \"x86_64\" = \"aarch64\" ]; then \\\n                mv /home/minikub-build/minikube/out/buildroot/output-aarch64/images/boot.iso /home/minikub-build/minikube/out/minikube-arm64.iso; \\\n        else \\\n                mv /home/minikub-build/minikube/out/buildroot/output-x86_64/images/rootfs.iso9660 /home/minikub-build/minikube/out/minikube-amd64.iso; \\\n        fi;\nrm deploy/iso/minikube-iso/board/minikube/x86_64/rootfs-overlay/usr/bin/auto-pause\nminikub-build@minikube-build:~/minikube$ echo $?\n0\nminikub-build@minikube-build:~/minikube$ cat /etc/os-release \nPRETTY_NAME=\"Ubuntu 24.04.3 LTS\"\n...\nfile out/minikube-amd64.iso \nout/minikube-amd64.iso: ISO 9660 CD-ROM filesystem data 'ISOIMAGE' (bootable)\n```",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/stale"
      ],
      "created_at": "2025-08-27T22:27:01Z",
      "closed_at": "2025-12-27T22:20:35Z",
      "url": "https://github.com/kubernetes/minikube/issues/21441",
      "comments_count": 2
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21129,
      "title": "Disk space leak caused by \"minikube image load\"",
      "problem": "### What Happened?\n\nDisk space leak caused by \"minikube image load\". Reference: https://github.com/kubernetes/minikube/pull/21103#issuecomment-3097245997\n\nThe images are stored in ~/.minikube/cache/images. It appears the space is not being cleaned up after executing the minikube image load command.\n\nThis may lead to disk space exhaustion.\n\n### Attach the log file\n\nNone\n\n### Operating System\n\nNone\n\n### Driver\n\nNone",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-07-23T02:07:46Z",
      "closed_at": "2025-12-27T21:34:52Z",
      "url": "https://github.com/kubernetes/minikube/issues/21129",
      "comments_count": 8
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21201,
      "title": "kvm2 - instructions error",
      "problem": "In the documentation there's a slight error\n\n\n```\n\nRun sudo virsh net-define /usr/share/libvirt/networks/default.xml to recreate the default libvirt network.\n\n    Note: repeat above steps b. and c. and then Run sudo virsh net-define default.xml to restore the original default libvirt network config, in case of any issue.\n\n\n```\n\nBy 'b.' and 'c.' I think you mean items '2.' and '3'.\n\n",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-07-30T15:32:57Z",
      "closed_at": "2025-12-27T21:34:50Z",
      "url": "https://github.com/kubernetes/minikube/issues/21201",
      "comments_count": 5
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21197,
      "title": "Minikube v1.36.0 Fails to Start on Windows 11 with VirtualBox: VERR_CPUM_INVALID_XCR0",
      "problem": "### What Happened?\n\n minikube v1.36.0 on Microsoft Windows 11 Home Single Language 10.0.26100.4652 Build 26100.4652\n* Using the virtualbox driver based on existing profile\n* Starting \"minikube\" primary control-plane node in \"minikube\" cluster\n* Restarting existing virtualbox VM for \"minikube\" ...\n! StartHost failed, but will try again: driver start: Unable to start the VM: C:\\Program Files\\Oracle\\VirtualBox\\VBoxManage.exe startvm minikube --type headless failed:\nVBoxManage.exe: error: Failed to load unit 'cpum' (VERR_CPUM_INVALID_XCR0)\nVBoxManage.exe: error: Details: code E_FAIL (0x80004005), component ConsoleWrap, interface IConsole\n\nDetails: 00:00:04.261437 End of log file - Log started 2025-07-30T08:56:22.969432700Z\n* virtualbox \"minikube\" VM is missing, will recreate.\n* Creating virtualbox VM (CPUs=2, Memory=4000MB, Disk=20000MB) ...\n* Failed to start virtualbox VM. Running \"minikube delete\" may fix it: recreate: creating host: create: creating: write |1: The pipe has been ended.\n\nX Exiting due to GUEST_PROVISION: error provisioning guest: Failed to start host: recreate: creating host: create: creating: write |1: The pipe has been ended.\n\n### Attach the log file\n\n minikube v1.36.0 on Microsoft Windows 11 Home Single Language 10.0.26100.4652 Build 26100.4652\n* Using the virtualbox driver based on existing profile\n* Starting \"minikube\" primary control-plane node in \"minikube\" cluster\n* Restarting existing virtualbox VM for \"minikube\" ...\n! StartHost failed, but will try again: driver start: Unable to start the VM: C:\\Program Files\\Oracle\\VirtualBox\\VBoxManage.exe startvm minikube --type headless failed:\nVBoxManage.exe: error: Failed to load unit 'cpum' (VERR_CPUM_INVALID_XCR0)\nVBoxManage.exe: error: Details: code E_FAIL (0x80004005), component ConsoleWrap, interface IConsole\n\nDetails: 00:00:04.261437 End of log file - Log started 2025-07-30T08:56:22.969432700Z\n* virtualbox \"minikube\" VM is missing, will recreate.\n* Creating virtualbox VM (CPUs=2, Memory=4000MB, Disk=20000MB) ...\n* Failed to start virtualbox VM. Running \"minikube delete\" may fix it: recreate: creating host: create: creating: write |1: The pipe has been ended.\n\nX Exiting due to GUEST_PROVISION: error provisioning guest: Failed to start host: recreate: creating host: create: creating: write |1: The pipe has been ended.\n\n### Operating System\n\nWindows\n\n### Driver\n\nVirtualBox",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/bug",
        "lifecycle/rotten"
      ],
      "created_at": "2025-07-30T08:59:32Z",
      "closed_at": "2025-12-27T21:34:51Z",
      "url": "https://github.com/kubernetes/minikube/issues/21197",
      "comments_count": 6
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 20977,
      "title": "kvm2 driver cannot connect to Docker registry on Fedora 41/42, while qemu2 works",
      "problem": "### What Happened?\n\nOn Fedora 42, Minikube using the kvm2 driver fails to connect to external image registries such as Docker Hub. When switching to the qemu2 driver, the issue is resolved.\n \nMinikube with kvm2:\n```\n$ minikube start --disk-size=10g --extra-disks=1 --driver=kvm2\n\ud83d\ude04  minikube v1.34.0 on Fedora 42\n    \u25aa KUBECONFIG=/home/oviner/ClusterPath/auth/kubeconfig\n\u2728  Using the kvm2 driver based on user configuration\n\ud83d\udc4d  Starting \"minikube\" primary control-plane node in \"minikube\" cluster\n\ud83d\udd25  Creating kvm2 VM (CPUs=2, Memory=6000MB, Disk=10240MB) ...\n\u2757  Failing to connect to https://registry.k8s.io/ from inside the minikube VM\n\ud83d\udca1  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/\n\ud83d\udc33  Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...\n    \u25aa Generating certificates and keys ...\n    \u25aa Booting up control plane ...\n    \u25aa Configuring RBAC rules ...\n\ud83d\udd17  Configuring bridge CNI (Container Networking Interface) ...\n\ud83d\udd0e  Verifying Kubernetes components...\n    \u25aa Using image gcr.io/k8s-minikube/storage-provisioner:v5\n\ud83c\udf1f  Enabled addons: storage-provisioner, default-storageclass\n\ud83c\udfc4  Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\n\n[curl failed]\n$ minikube ssh\n$  curl -I https://registry-1.docker.io/v2/ \n^C\n```\n\nMinikube with qemu2:\n```\n$ minikube start --disk-size=10g --extra-disks=1 --driver=qemu2\n\ud83d\ude04  minikube v1.34.0 on Fedora 42\n    \u25aa KUBECONFIG=/home/oviner/ClusterPath/auth/kubeconfig\n\u2728  Using the qemu2 driver based on user configuration\n\ud83c\udf10  Automatically selected the builtin network\n\u2757  You are using the QEMU driver without a dedicated network, which doesn't support `minikube service` & `minikube tunnel` commands.\n\ud83d\udc4d  Starting \"minikube\" primary control-plane node in \"minikube\" cluster\n\ud83d\udd25  Creating qemu2 VM (CPUs=2, Memory=6000MB, Disk=10240MB) ...\n\ud83d\udc33  Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...\n    \u25aa Generating certificates and keys ...\n    \u25aa Booting up control plane ...\n    \u25aa Configuring RBAC rules ...\n\ud83d\udd17  Configuring bridge CNI (Container Networking Interface) ...\n\ud83d\udd0e  Verifying Kubernetes components...\n    \u25aa Using image gcr.io/k8s-minikube/storage-provisioner:v5\n\ud83c\udf1f  Enabled addons: storage-provisioner, default-storageclass\n\ud83c\udfc4  Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\n\n$ minikube ssh\n$ curl -I https://registry-1.docker.io/v2/ \nHTTP/1.1 401 Unauthorized\ncontent-type: application/json\ndocker-distribution-api-version: registry/2.0\nwww-authenticate: Bearer realm=\"https://auth.docker.io/token\",service=\"registry.docker.io\"\ndate: Mon, 23 Jun 2025 15:06:15 GMT\ncontent-length: 87\nstrict-transport-security: max-age=31536000\n\n```\n\n**Additional Info**\n\n- Minikube version: v1.34.0\n- Fedora version: 42\n- Kubernetes version: v1.31.0\n- Docker inside Minikube: 27.2.0\n- No custom proxy configuration\n- qemu2 works fine, while kvm2 fails consistently with networking timeout\n\n**Expected Behavior**\n`kvm2` driver should be able to access external registries without networking failures.\n\n**Workaround**\nSwitching to the `--driver=qemu2`\n\n[log.txt](https://github.com/user-attachments/files/20867918/log.txt)\n\n flag resolves the issue.\n\n\n\n### Attach the log file\n\n**Workaround**\nSwitching to the `--driver=qemu2`\n\n### Operating System\n\nRedhat/Fedora\n\n### Driver\n\nKVM2",
      "solution": "I have the same issue with minikube 1.36.0. It seems there is no outbound traffic from the VM.\n\nI can confirm that the issue does no exist with the qemu2 driver.\nAlso, this is not an issue with the virtualbox or docker drivers.\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-06-23T15:15:46Z",
      "closed_at": "2025-12-27T01:27:49Z",
      "url": "https://github.com/kubernetes/minikube/issues/20977",
      "comments_count": 8
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21193,
      "title": "minikube addons enable kubevirt - for running helm charts",
      "problem": "### What Happened?\n\nPS C:\\> minikube addons enable kubevirt\n! kubevirt is a 3rd party addon and is not maintained or verified by minikube maintainers, enable at your own risk.\n! kubevirt does not currently have an associated maintainer.\n  - Using image docker.io/bitnami/kubectl:1.31.3\n\nX Exiting due to MK_ADDON_ENABLE: enable failed: run callbacks: running callbacks: [sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/pod.yaml: Process exited with status 1\nstdout:\n\nstderr:\nerror: error validating \"/etc/kubernetes/addons/pod.yaml\": error validating data: failed to download openapi: Get \"https://localhost:8443/openapi/v2?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false\n]\n*\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                            \u2502\n\u2502    * If the above advice does not help, please let us know:                                                \u2502\n\u2502      https://github.com/kubernetes/minikube/issues/new/choose                                              \u2502\n\u2502                                                                                                            \u2502\n\u2502    * Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.                   \u2502\n\u2502    * Please also attach the following file to the GitHub issue:                                            \u2502\n\u2502    * - C:\\Users\\Admin\\AppData\\Local\\Temp\\minikube_addons_7ae018969117df12727339d29a17b6982065c9d8_0.log    \u2502\n\u2502                                                                                                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\n### Attach the log file\n\nPS C:\\> minikube addons enable kubevirt\n! kubevirt is a 3rd party addon and is not maintained or verified by minikube maintainers, enable at your own risk.\n! kubevirt does not currently have an associated maintainer.\n  - Using image docker.io/bitnami/kubectl:1.31.3\n\nX Exiting due to MK_ADDON_ENABLE: enable failed: run callbacks: running callbacks: [sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/pod.yaml: Process exited with status 1\nstdout:\n\nstderr:\nerror: error validating \"/etc/kubernetes/addons/pod.yaml\": error validating data: failed to download openapi: Get \"https://localhost:8443/openapi/v2?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false\n]\n*\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                            \u2502\n\u2502    * If the above advice does not help, please let us know:                                                \u2502\n\u2502      https://github.com/kubernetes/minikube/issues/new/choose                                              \u2502\n\u2502                                                                                                            \u2502\n\u2502    * Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.                   \u2502\n\u2502    * Please also attach the following file to the GitHub issue:                                            \u2502\n\u2502    * - C:\\Users\\Admin\\AppData\\Local\\Temp\\minikube_addons_7ae018969117df12727339d29a17b6982065c9d8_0.log    \u2502\n\u2502                                                                                                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\n### Operating System\n\nWindows\n\n### Driver\n\nDocker",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-07-29T05:05:10Z",
      "closed_at": "2025-12-26T07:17:15Z",
      "url": "https://github.com/kubernetes/minikube/issues/21193",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21192,
      "title": "minikube addons enable kubevirt",
      "problem": "### What Happened?\n\nPS C:\\> minikube addons enable kubevirt\n! kubevirt is a 3rd party addon and is not maintained or verified by minikube maintainers, enable at your own risk.\n! kubevirt does not currently have an associated maintainer.\n  - Using image docker.io/bitnami/kubectl:1.31.3\n\nX Exiting due to MK_ADDON_ENABLE: enable failed: run callbacks: running callbacks: [sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/pod.yaml: Process exited with status 1\nstdout:\n\nstderr:\nerror: error validating \"/etc/kubernetes/addons/pod.yaml\": error validating data: failed to download openapi: Get \"https://localhost:8443/openapi/v2?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false\n]\n*\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                            \u2502\n\u2502    * If the above advice does not help, please let us know:                                                \u2502\n\u2502      https://github.com/kubernetes/minikube/issues/new/choose                                              \u2502\n\u2502                                                                                                            \u2502\n\u2502    * Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.                   \u2502\n\u2502    * Please also attach the following file to the GitHub issue:                                            \u2502\n\u2502    * - C:\\Users\\Admin\\AppData\\Local\\Temp\\minikube_addons_7ae018969117df12727339d29a17b6982065c9d8_0.log    \u2502\n\u2502                                                                                                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\n### Attach the log file\n\nPS C:\\> minikube addons enable kubevirt\n! kubevirt is a 3rd party addon and is not maintained or verified by minikube maintainers, enable at your own risk.\n! kubevirt does not currently have an associated maintainer.\n  - Using image docker.io/bitnami/kubectl:1.31.3\n\nX Exiting due to MK_ADDON_ENABLE: enable failed: run callbacks: running callbacks: [sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/pod.yaml: Process exited with status 1\nstdout:\n\nstderr:\nerror: error validating \"/etc/kubernetes/addons/pod.yaml\": error validating data: failed to download openapi: Get \"https://localhost:8443/openapi/v2?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false\n]\n*\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                            \u2502\n\u2502    * If the above advice does not help, please let us know:                                                \u2502\n\u2502      https://github.com/kubernetes/minikube/issues/new/choose                                              \u2502\n\u2502                                                                                                            \u2502\n\u2502    * Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.                   \u2502\n\u2502    * Please also attach the following file to the GitHub issue:                                            \u2502\n\u2502    * - C:\\Users\\Admin\\AppData\\Local\\Temp\\minikube_addons_7ae018969117df12727339d29a17b6982065c9d8_0.log    \u2502\n\u2502                                                                                                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\n### Operating System\n\nWindows\n\n### Driver\n\nDocker",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-07-29T05:04:27Z",
      "closed_at": "2025-12-26T07:17:15Z",
      "url": "https://github.com/kubernetes/minikube/issues/21192",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21191,
      "title": "minikube addons  enable cubevirt",
      "problem": "### What Happened?\n\nPS C:\\> minikube addons enable kubevirt\n! kubevirt is a 3rd party addon and is not maintained or verified by minikube maintainers, enable at your own risk.\n! kubevirt does not currently have an associated maintainer.\n  - Using image docker.io/bitnami/kubectl:1.31.3\n\nX Exiting due to MK_ADDON_ENABLE: enable failed: run callbacks: running callbacks: [sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/pod.yaml: Process exited with status 1\nstdout:\n\nstderr:\nerror: error validating \"/etc/kubernetes/addons/pod.yaml\": error validating data: failed to download openapi: Get \"https://localhost:8443/openapi/v2?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false\n]\n*\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                            \u2502\n\u2502    * If the above advice does not help, please let us know:                                                \u2502\n\u2502      https://github.com/kubernetes/minikube/issues/new/choose                                              \u2502\n\u2502                                                                                                            \u2502\n\u2502    * Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.                   \u2502\n\u2502    * Please also attach the following file to the GitHub issue:                                            \u2502\n\u2502    * - C:\\Users\\Admin\\AppData\\Local\\Temp\\minikube_addons_7ae018969117df12727339d29a17b6982065c9d8_0.log    \u2502\n\u2502                                                                                                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\n### Attach the log file\n\nPS C:\\> minikube addons enable kubevirt\n! kubevirt is a 3rd party addon and is not maintained or verified by minikube maintainers, enable at your own risk.\n! kubevirt does not currently have an associated maintainer.\n  - Using image docker.io/bitnami/kubectl:1.31.3\n\nX Exiting due to MK_ADDON_ENABLE: enable failed: run callbacks: running callbacks: [sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/pod.yaml: Process exited with status 1\nstdout:\n\nstderr:\nerror: error validating \"/etc/kubernetes/addons/pod.yaml\": error validating data: failed to download openapi: Get \"https://localhost:8443/openapi/v2?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false\n]\n*\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                            \u2502\n\u2502    * If the above advice does not help, please let us know:                                                \u2502\n\u2502      https://github.com/kubernetes/minikube/issues/new/choose                                              \u2502\n\u2502                                                                                                            \u2502\n\u2502    * Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.                   \u2502\n\u2502    * Please also attach the following file to the GitHub issue:                                            \u2502\n\u2502    * - C:\\Users\\Admin\\AppData\\Local\\Temp\\minikube_addons_7ae018969117df12727339d29a17b6982065c9d8_0.log    \u2502\n\u2502                                                                                                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\n### Operating System\n\nWindows\n\n### Driver\n\nHyper-V",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-07-29T05:03:53Z",
      "closed_at": "2025-12-26T07:17:15Z",
      "url": "https://github.com/kubernetes/minikube/issues/21191",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21190,
      "title": "minikube addons enable dashboard command error  in log path",
      "problem": "### What Happened?\n\nPS C:\\> minikube addons enable dashboard\n* dashboard is an addon maintained by Kubernetes. For any concerns contact minikube on GitHub.\nYou can view the list of minikube maintainers at: https://github.com/kubernetes/minikube/blob/master/OWNERS\n  - Using image docker.io/kubernetesui/metrics-scraper:v1.0.8\n  - Using image docker.io/kubernetesui/dashboard:v2.7.0\n\nX Exiting due to MK_ADDON_ENABLE: enable failed: run callbacks: running callbacks: [sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1\nstdout:\n\nstderr:\nerror validating \"/etc/kubernetes/addons/dashboard-ns.yaml\": error validating data: failed to download openapi: Get \"https://localhost:8443/openapi/v2?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false\nerror validating \"/etc/kubernetes/addons/dashboard-clusterrole.yaml\": error validating data: failed to download openapi: Get \"https://localhost:8443/openapi/v2?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false\nerror validating \"/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml\": error validating data: failed to download openapi: Get \"https://localhost:8443/openapi/v2?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false\nerror validating \"/etc/kubernetes/addons/dashboard-configmap.yaml\": error validating data: failed to download openapi: Get \"https://localhost:8443/openapi/v2?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false\nerror validating \"/etc/kubernetes/addons/dashboard-dp.yaml\": error validating data: failed to download openapi: Get \"https://localhost:8443/openapi/v2?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false\nerror validating \"/etc/kubernetes/addons/dashboard-role.yaml\": error validating data: failed to download openapi: Get \"https://localhost:8443/openapi/v2?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false\nerror validating \"/etc/kubernetes/addons/dashboard-rolebinding.yaml\": error validating data: failed to download openapi: Get \"https://localhost:8443/openapi/v2?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false\nerror validating \"/etc/kubernetes/addons/dashboard-sa.yaml\": error validating data: failed to download openapi: Get \"https://localhost:8443/openapi/v2?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false\nerror validating \"/etc/kubernetes/addons/dashboard-secret.yaml\": error validating data: failed to download openapi: Get \"https://localhost:8443/openapi/v2?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false\nerror validating \"/etc/kubernetes/addons/dashboard-svc.yaml\": error validating data: failed to download openapi: Get \"https://localhost:8443/openapi/v2?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false\n]\n*\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                            \u2502\n\u2502    * If the above advice does not help, please let us know:                                                \u2502\n\u2502      https://github.com/kubernetes/minikube/issues/new/choose                                              \u2502\n\u2502                                                                                                            \u2502\n\u2502    * Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.                   \u2502\n\u2502    * Please also attach the following file to the GitHub issue:                                            \u2502\n\u2502    * - C:\\Users\\Admin\\AppData\\Local\\Temp\\minikube_addons_69c921fce5e080e8c2890514714900bca8f5306b_0.log    \u2502\n\u2502                                                                                                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n[minikube_addons_69c921fce5e080e8c2890514714900bca8f5306b_0.log](https://github.com/user-attachments/files/21481048/minikube_addons_69c921fce5e080e8c2890514714900bca8f5306b_0.log)\n\n[minikube_addons_2ec281bcba8283e564e4238bc5f460794da3ebf3_0.log](https://github.com/user-attachments/files/21481063/minikube_addons_2ec281bcba8283e564e4238bc5f460794da3ebf3_0.log)\n\n### Attach the log file\n\nPS C:\\> minikube addons enable dashboard\n* dashboard is an addon maintained by Kubernetes. For any concerns contact minikube on GitHub.\nYou can view the list of minikube maintainers at: https://github.com/kubernetes/minikube/blob/master/OWNERS\n  - Using image docker.io/kubernetesui/metrics-scraper:v1.0.8\n  - Using image docker.io/kubernetesui/dashboard:v2.7.0\n\nX Exiting due to MK_ADDON_ENABLE: enable failed: run callbacks: running callbacks: [sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1\nstdout:\n\nstderr:\nerror validating \"/etc/kubernetes/addons/dashboard-ns.yaml\": error validating data: failed to download openapi: Get \"https://localhost:8443/openapi/v2?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false\nerror validating \"/etc/kubernetes/addons/dashboard-clusterrole.yaml\": error validating data: failed to download openapi: Get \"https://localhost:8443/openapi/v2?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false\nerror validating \"/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml\": error validating data: failed to download openapi: Get \"https://localhost:8443/openapi/v2?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false\nerror validating \"/etc/kubernetes/addons/dashboard-configmap.yaml\": error validating data: failed to download openapi: Get \"https://localhost:8443/openapi/v2?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false\nerror validating \"/etc/kubernetes/addons/dashboard-dp.yaml\": error validating data: failed to download openapi: Get \"https://localhost:8443/openapi/v2?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false\nerror validating \"/etc/kubernetes/addons/dashboard-role.yaml\": error validating data: failed to download openapi: Get \"https://localhost:8443/openapi/v2?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false\nerror validating \"/etc/kubernetes/addons/dashboard-rolebinding.yaml\": error validating data: failed to download openapi: Get \"https://localhost:8443/openapi/v2?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false\nerror validating \"/etc/kubernetes/addons/dashboard-sa.yaml\": error validating data: failed to download openapi: Get \"https://localhost:8443/openapi/v2?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false\nerror validating \"/etc/kubernetes/addons/dashboard-secret.yaml\": error validating data: failed to download openapi: Get \"https://localhost:8443/openapi/v2?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false\nerror validating \"/etc/kubernetes/addons/dashboard-svc.yaml\": error validating data: failed to download openapi: Get \"https://localhost:8443/openapi/v2?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false\n]\n*\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                            \u2502\n\u2502    * If the above advice does not help, please let us know:                                                \u2502\n\u2502      https://github.com/kubernetes/minikube/issues/new/choose                                              \u2502\n\u2502                                                                                                            \u2502\n\u2502    * Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.                   \u2502\n\u2502    * Please also attach the following file to the GitHub issue:                                            \u2502\n\u2502    * - C:\\Users\\Admin\\AppData\\Local\\Temp\\minikube_addons_69c921fce5e080e8c2890514714900bca8f5306b_0.log    \u2502\n\u2502                                                                                                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n[minikube_addons_69c921fce5e080e8c2890514714900bca8f5306b_0.log](https://github.com/user-attachments/files/21481064/minikube_addons_69c921fce5e080e8c2890514714900bca8f5306b_0.log)\n\n### Operating System\n\nWindows\n\n### Driver\n\nHyper-V",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-07-29T05:01:11Z",
      "closed_at": "2025-12-26T07:17:14Z",
      "url": "https://github.com/kubernetes/minikube/issues/21190",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21153,
      "title": "kubectl always fails with \"Unable to connect to the server: net/http: TLS handshake timeout\"",
      "problem": "### What Happened?\n\n## Minikube TLS Handshake Timeout Issue on Windows\n\nHello team,\n\nI'm experiencing a persistent issue when using **Minikube on Windows**. Every time I run any `kubectl` command (such as `kubectl get namespaces`, `kubectl get pods`, etc.), I receive the following error:\n` Unable to connect to the server: net/http: TLS handshake timeout ` .\nEven basic commands like `kubectl version` or `kubectl cluster-info` fail with the same error. This issue occurs consistently, even immediately after starting the cluster using: ` minikube start `\n\n\n\n\n\n\n\n\n### Attach the log file\n\nAny insights or suggestions to resolve this would be greatly appreciated.\n\n### Operating System\n\nNone\n\n### Driver\n\nNone",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-07-27T20:06:10Z",
      "closed_at": "2025-12-24T22:05:14Z",
      "url": "https://github.com/kubernetes/minikube/issues/21153",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21138,
      "title": "minikube -p agones start\u91cd\u542f\u540e\u518d\u5f00\u542f\u5931\u8d25",
      "problem": "<!-- \u8bf7\u5728\u62a5\u544a\u95ee\u9898\u65f6\u4f7f\u7528\u6b64\u6a21\u677f\uff0c\u5e76\u63d0\u4f9b\u5c3d\u53ef\u80fd\u8be6\u7ec6\u7684\u4fe1\u606f\u3002\u5426\u5219\u53ef\u80fd\u5bfc\u81f4\u54cd\u5e94\u5ef6\u8fdf\u3002\u8c22\u8c22\uff01-->\n\n**\u91cd\u73b0\u95ee\u9898\u6240\u9700\u7684\u547d\u4ee4**\uff1a\nminikube -p agones start\n**\u5931\u8d25\u7684\u547d\u4ee4\u7684\u5b8c\u6574\u8f93\u51fa**\uff1a<details>\noceanwsl@oceanfly:/mnt/c/Users/zoceanfly$ minikube -p agones start\n\ud83d\ude04  [agones] minikube v1.36.0 on Ubuntu 24.04 (amd64)\n\u2728  Using the docker driver based on existing profile\n\ud83d\udc4d  Starting \"agones\" primary control-plane node in \"agones\" cluster\n\ud83d\ude9c  Pulling base image v0.0.47 ...\n\ud83d\udd04  Restarting existing docker container for \"agones\" ...\n\ud83c\udf10  Found network options:\n    \u25aa NO_PROXY=localhost,127.0.0.1,.svc,.cluster.local,.minikube.internal,10.96.0.0/12,192.168.49.0/24,192.168.59.0/24,192.168.39.0/24\n\n\n\n\n\n\n\n\u274c  Exiting due to RUNTIME_ENABLE: Failed to start container runtime: Temporary Error: sudo /usr/bin/crictl version: Process exited with status 1\nstdout:\n\nstderr:\nE0725 06:23:51.064961     624 remote_runtime.go:189] \"Version from runtime service failed\" err=\"rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.RuntimeService\"\ntime=\"2025-07-25T06:23:51Z\" level=fatal msg=\"getting the runtime version: rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.RuntimeService\"\n\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                           \u2502\n\u2502    \ud83d\ude3f  If the above advice does not help, please let us know:                             \u2502\n\u2502    \ud83d\udc49  https://github.com/kubernetes/minikube/issues/new/choose                           \u2502\n\u2502                                                                                           \u2502\n\u2502    Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.\n\n</details>\n\n\n**`minikube logs`\u547d\u4ee4\u7684\u8f93\u51fa**\uff1a <details>\n\n\n</details>\n\n\n**\u4f7f\u7528\u7684\u64cd\u4f5c\u7cfb\u7edf\u7248\u672c**\uff1a\n",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten",
        "l/zh-CN"
      ],
      "created_at": "2025-07-25T06:24:53Z",
      "closed_at": "2025-12-22T08:36:12Z",
      "url": "https://github.com/kubernetes/minikube/issues/21138",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 20559,
      "title": "vfkit: Rosetta support - run amd64 containers on Apple silicon macs",
      "problem": "### What Happened?\n\nSome projects do not build container images for arm64 (aarch64 on linux). When using minikube on Apple silicon macs, the container will fail to start.\n\nvfkit already support Rosetta:\nhttps://github.com/crc-org/vfkit/blob/main/doc/usage.md#rosetta\n\nEnabling the feature requires:\n\n1. Adding --device rosetta:\n\n```\n--device rosetta,mountTag=minikube-rosetta\n```\n\n2. Mounting the roseta share in the guest:\n\n```\nmount -t virtiofs minikube-rosetta /mnt/minikube-rosetta\n```\n\n3. Configure [binfmt](https://docs.kernel.org/admin-guide/binfmt-misc.html) to use the rosetta binary for x86_64 executables \n\n```\nsudo /usr/sbin/update-binfmts --install rosetta /mnt/minikube-rosetta/rosetta \\\n    --magic \"\\x7fELF\\x02\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x3e\\x00\" \\\n    --mask \"\\xff\\xff\\xff\\xff\\xff\\xfe\\xfe\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\" \\\n    --credentials yes --preserve yes --fix-binary yes\n```\n\nIssues:\n- `update-binfmts` is not installed in our iso, we need to add it.\n\n## See also\n\n- [vfkit rosetta docs](https://github.com/crc-org/vfkit/blob/main/doc/usage.md#rosetta)\n- [lima](https://lima-vm.io/docs/config/multi-arch/#fast-mode-2-rosetta-intel-containers-on-arm-vm-on-arm-hostfast-mode-2) have full support for rosetta. You only need to add the --rosetta flag and it handles the rest.",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/feature",
        "co/vfkit"
      ],
      "created_at": "2025-03-22T21:34:22Z",
      "closed_at": "2025-12-22T00:45:40Z",
      "url": "https://github.com/kubernetes/minikube/issues/20559",
      "comments_count": 12
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21137,
      "title": "WHILE RUNNING MAKE TEST IT IS THROW AN ERR",
      "problem": "### What Happened?\n\nTHIS IS THE ERROR I AM GETTING WHEN I AM RUNNING MAKE TEST IN WSL in ubuntu distro\n\n### Attach the log file\n\nut.go:358] Setting ErrFile to fd 0...\nI0724 05:24:29.503769 1363790 out.go:373] MINIKUBE_IN_STYLE=\"0\"\nW0724 05:24:29.503772 1363790 out.go:293] xyz123 %s%%%d\nI0724 05:24:29.503775 1363790 out.go:201] unrelated message\nI0724 05:24:29.504028 1363790 out.go:358] Setting ErrFile to fd 0...\nW0724 05:24:29.512390 1363790 out.go:293] \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                           \u2502\n\u2502    :crying_cat_face:  If the above advice does not help, please let us know:                             \u2502\n\u2502    :point_right:  https://github.com/kubernetes/minikube/issues/new/choose                           \u2502\n\u2502                                                                                           \u2502\n\u2502    Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    \u2502\n\u2502                                                                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nI0724 05:24:29.512450 1363790 out.go:358] Setting ErrFile to fd 0...\nW0724 05:24:29.519972 1363790 out.go:293] \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                           \u2502\n\u2502    :crying_cat_face:  If the above advice does not help, please let us know:                             \u2502\n\u2502    :point_right:  https://github.com/kubernetes/minikube/issues/new/choose                           \u2502\n\u2502                                                                                           \u2502\n\u2502    Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    \u2502\n\u2502    Please also attach the following file to the GitHub issue:                             \u2502\n\u2502    - /tmp                                                                                 \u2502\n\u2502                                                                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nI0724 05:24:29.520123 1363790 out.go:345] Setting OutFile to fd 0 ...\nI0724 05:24:29.520414 1363790 out.go:201] \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                \u2502\n\u2502    Running with docker driver and port 8000    \u2502\n\u2502                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n--- FAIL: TestBoxed (0.00s)\n    out_test.go:256: Boxed() = \"\\x1b[31m\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\\x1b[0m\\n\\x1b[31m\u2502\\x1b[0m                                                \\x1b[31m\u2502\\x1b[0m\\n\\x1b[31m\u2502\\x1b[0m    Running with docker driver and port 8000    \\x1b[31m\u2502\\x1b[0m\\n\\x1b[31m\u2502\\x1b[0m                                                \\x1b[31m\u2502\\x1b[0m\\n\\x1b[31m\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\\x1b[0m\\n\", want \"\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\\n\u2502                                                \u2502\\n\u2502    Running with docker driver and port 8000    \u2502\\n\u2502                                                \u2502\\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\\n\"\nI0724 05:24:29.520525 1363790 out.go:358] Setting ErrFile to fd 0...\nW0724 05:24:29.520848 1363790 out.go:293] \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                \u2502\n\u2502    Running with docker driver and port 8000    \u2502\n\u2502                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n--- FAIL: TestBoxedErr (0.00s)\n    out_test.go:273: Boxed() = \"\\x1b[31m\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\\x1b[0m\\n\\x1b[31m\u2502\\x1b[0m                                                \\x1b[31m\u2502\\x1b[0m\\n\\x1b[31m\u2502\\x1b[0m    Running with docker driver and port 8000    \\x1b[31m\u2502\\x1b[0m\\n\\x1b[31m\u2502\\x1b[0m                                                \\x1b[31m\u2502\\x1b[0m\\n\\x1b[31m\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\\x1b[0m\\n\", want \"\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\\n\u2502                                                \u2502\\n\u2502    Running with docker driver and port 8000    \u2502\\n\u2502                                                \u2502\\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\\n\"\nI0724 05:24:29.520938 1363790 out.go:345] Setting OutFile to fd 0 ...\nI0724 05:24:29.521028 1363790 out.go:201] \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 \u2502\n\u2502                 \u2502\n\u2502  Boxed content  \u2502\n\u2502                 \u2502\n\u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nI0724 05:24:29.521035 1363790 out.go:345] Setting OutFile to fd 0 ...\nI0724 05:24:29.521188 1363790 out.go:201] \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502Boxed content with 0 padding\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nI0724 05:24:29.521195 1363790 out.go:345] Setting OutFile to fd 0 ...\nI0724 05:24:29.521423 1363790 out.go:201] \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                 \u2502\n\u2502           Hello World           \u2502\n\u2502                                 \u2502\n\u2502 Boxed content with title inside \u2502\n\u2502                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nI0724 05:24:29.521429 1363790 out.go:345] Setting OutFile to fd 0 ...\nI0724 05:24:29.521585 1363790 out.go:201] \u250c Hello World \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                 \u2502\n\u2502 Boxed content with title inside \u2502\n\u2502                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nI0724 05:24:29.521593 1363790 out.go:345] Setting OutFile to fd 0 ...\nI0724 05:24:29.521755 1363790 out.go:201] \u250c :bulb:  Hello World \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                 \u2502\n\u2502 Boxed content with title inside \u2502\n\u2502                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nI0724 05:24:29.521833 1363790 out.go:345] Setting OutFile to fd 0 ...\nI0724 05:24:29.522009 1363790 out.go:201] \u250c :bulb:  Hello World \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                 \u2502\n\u2502 Boxed content with title inside \u2502\n\u2502                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n--- FAIL: TestBoxedWithConfig (0.00s)\n    out_test.go:377: Expecting BoxedWithConfig({1 1   Top <nil> <nil> <nil> false 0}, 69, Hello World, Boxed content with title inside, []) =\n        \u250c :bulb:  Hello World \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                                 \u2502\n        \u2502 Boxed content with title inside \u2502\n        \u2502                                 \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        , want\n        \u250c * Hello World \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                                 \u2502\n        \u2502 Boxed content with title inside \u2502\n        \u2502                                 \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    out_test.go:377: Expecting BoxedWithConfig({1 1   Top <nil> <nil> <nil> false 0}, 69, Hello\n        World, Boxed content with title inside, []) =\n        \u250c :bulb:  Hello World \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                                 \u2502\n        \u2502 Boxed content with title inside \u2502\n        \u2502                                 \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        , want\n        \u250c * Hello World \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                                 \u2502\n        \u2502 Boxed content with title inside \u2502\n        \u2502                                 \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nFAIL\ncoverage: 63.5% of statements\nFAIL    [k8s.io/minikube/pkg/minikube/out](http://k8s.io/minikube/pkg/minikube/out)        0.073s\nok      [k8s.io/minikube/pkg/minikube/out/register](http://k8s.io/minikube/pkg/minikube/out/register)       0.036s  coverage: 55.4% of statements\nok      [k8s.io/minikube/pkg/minikube/perf](http://k8s.io/minikube/pkg/minikube/perf)       4.216s  coverage: 20.7% of statements\nok      [k8s.io/minikube/pkg/minikube/process](http://k8s.io/minikube/pkg/minikube/process)    3.547s  coverage: 83.0% of statements\nok      [k8s.io/minikube/pkg/minikube/proxy](http://k8s.io/minikube/pkg/minikube/proxy)      0.054s  coverage: 78.1% of statements\nok      [k8s.io/minikube/pkg/minikube/reason](http://k8s.io/minikube/pkg/minikube/reason)     0.171s  coverage: 70.0% of statements\nok      [k8s.io/minikube/pkg/minikube/registry](http://k8s.io/minikube/pkg/minikube/registry)   0.047s  coverage: 78.8% of statements\nok      [k8s.io/minikube/pkg/minikube/registry/drvs/docker](http://k8s.io/minikube/pkg/minikube/registry/drvs/docker)       0.180s  coverage: 48.8% of statements\nok      [k8s.io/minikube/pkg/minikube/service](http://k8s.io/minikube/pkg/minikube/service)    3.371s  coverage: 74.7% of statements\nok      [k8s.io/minikube/pkg/minikube/shell](http://k8s.io/minikube/pkg/minikube/shell)      0.009s  coverage: 94.4% of statements\nok      [k8s.io/minikube/pkg/minikube/storageclass](http://k8s.io/minikube/pkg/minikube/storageclass)       0.728s  coverage: 100.0% of statements\nok      [k8s.io/minikube/pkg/minikube/style](http://k8s.io/minikube/pkg/minikube/style)      0.008s  coverage: 100.0% of statements\nok      [k8s.io/minikube/pkg/minikube/sysinit](http://k8s.io/minikube/pkg/minikube/sysinit)    0.392s  coverage: 5.8% of statements\nok      [k8s.io/minikube/pkg/minikube/translate](http://k8s.io/minikube/pkg/minikube/translate)  12.388s coverage: 27.6% of statements\nok      [k8s.io/minikube/pkg/minikube/tunnel](http://k8s.io/minikube/pkg/minikube/tunnel)     4.943s  coverage: 64.3% of statements\nok      [k8s.io/minikube/pkg/network](http://k8s.io/minikube/pkg/network)     1.607s  coverage: 75.5% of statements\nok      [k8s.io/minikube/pkg/util](http://k8s.io/minikube/pkg/util)        0.708s  coverage: 77.4% of statements\nok      [k8s.io/minikube/pkg/util/lock](http://k8s.io/minikube/pkg/util/lock)   0.189s  coverage: 10.0% of statements\nok      [k8s.io/minikube/pkg/util/retry](http://k8s.io/minikube/pkg/util/retry)  0.003s  coverage: 0.0% of statements\nFAIL\n\n### Operating System\n\nUbuntu\n\n### Driver\n\nNone",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-07-24T17:54:14Z",
      "closed_at": "2025-12-21T19:26:12Z",
      "url": "https://github.com/kubernetes/minikube/issues/21137",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 15354,
      "title": "Can't access internet from inside pods in `docker` and `virtualbox` driver (coredns prints `[ERROR] plugin/errors: 2 ... HINFO: read udp 172.17.0.2:40058->192.168.58.1:53: i/o timeout`)",
      "problem": "### What Happened?\n\nWhen I start a minikube cluster with `docker` or `virtualbox` driver, minikube is able to pull images from the internet but the pods are not able to access the internet. \r\nI ran `dnsutils` pod [from K8s docs](https://k8s.io/examples/admin/dns/dnsutils.yaml) \r\nWhen I try `apt install curl` in dnsutils pod I get the following error in the pod\r\n![image](https://user-images.githubusercontent.com/34534103/201593317-63ca20ea-7663-4b16-ba71-31762d456e18.png)\r\n```\r\nsuraj@suraj:~/sandbox$ k get po \r\nNAME       READY   STATUS    RESTARTS   AGE\r\ndnsutils   1/1     Running   0          17m\r\nsuraj@suraj:~/sandbox$ k exec -it dnsutils -- apt install curl\r\nReading package lists... Done\r\nBuilding dependency tree       \r\nReading state information... Done\r\nThe following extra packages will be installed:\r\n  ca-certificates libcurl3 libidn11 libldap-2.4-2 librtmp1 libsasl2-2\r\n  libsasl2-modules libsasl2-modules-db libssh2-1 openssl\r\nSuggested packages:\r\n  libsasl2-modules-otp libsasl2-modules-ldap libsasl2-modules-sql\r\n  libsasl2-modules-gssapi-mit libsasl2-modules-gssapi-heimdal\r\nThe following NEW packages will be installed:\r\n  ca-certificates curl libcurl3 libidn11 libldap-2.4-2 librtmp1 libsasl2-2\r\n  libsasl2-modules libsasl2-modules-db libssh2-1 openssl\r\n0 upgraded, 11 newly installed, 0 to remove and 0 not upgraded.\r\nNeed to get 2131 kB of archives.\r\nAfter this operation, 4430 kB of additional disk space will be used.\r\nDo you want to continue? [Y/n] y\r\nErr http://security.debian.org/debian-security/ jessie/updates/main libidn11 amd64 1.29-1+deb8u3\r\n  Temporary failure resolving 'security.debian.org'\r\nErr http://security.debian.org/debian-security/ jessie/updates/main libsasl2-modules-db amd64 2.1.26.dfsg1-13+deb8u2\r\n  Temporary failure resolving 'security.debian.org'\r\n\r\n```\r\nWhen I look at CoreDNS logs I see:\r\n```\r\n[ERROR] plugin/errors: 2 security.debian.org. A: read udp 172.17.0.2:36309->192.168.58.1:53: i/o timeout\r\n[ERROR] plugin/errors: 2 security.debian.org.domain.name. A: read udp 172.17.0.2:44695->192.168.58.1:53: i/o timeout\r\n[ERROR] plugin/errors: 2 security.debian.org.domain.name. A: read udp 172.17.0.2:34011->192.168.58.1:53: i/o timeout\r\n[ERROR] plugin/errors: 2 security.debian.org. A: read udp 172.17.0.2:59136->192.168.58.1:53: i/o timeout\r\n[ERROR] plugin/errors: 2 security.debian.org. A: read udp 172.17.0.2:39131->192.168.58.1:53: i/o timeout\r\n[ERROR] plugin/errors: 2 security.debian.org.domain.name. A: read udp 172.17.0.2:46414->192.168.58.1:53: i/o timeout\r\n```\r\nMy CoreFile looks like this:\r\n\r\n![image](https://user-images.githubusercontent.com/34534103/201593558-a0a998e6-c3df-4b95-9db5-c02d3ed931ac.png)\r\n```\r\nsuraj@suraj:~/sandbox$ k get cm  coredns -oyaml -nkube-system\r\napiVersion: v1\r\ndata:\r\n  Corefile: |\r\n    .:53 {\r\n        errors\r\n        debug\r\n        health {\r\n           lameduck 5s\r\n        }\r\n        ready\r\n        kubernetes cluster.local in-addr.arpa ip6.arpa {\r\n           pods insecure\r\n           fallthrough in-addr.arpa ip6.arpa\r\n           ttl 30\r\n        }\r\n        prometheus :9153\r\n        hosts {\r\n           192.168.58.1 host.minikube.internal\r\n           fallthrough\r\n        }\r\n        forward . /etc/resolv.conf {\r\n           max_concurrent 1000\r\n        }\r\n        cache 30\r\n        loop\r\n        reload\r\n        loadbalance\r\n    }\r\nkind: ConfigMap\r\nmetadata:\r\n  creationTimestamp: \"2022-11-14T06:03:14Z\"\r\n  name: coredns\r\n  namespace: kube-system\r\n  resourceVersion: \"1245\"\r\n  uid: 5810b9e1-4e8d-4827-9771-2c74b8eb45f2\r\n\r\n\r\n```\r\n\r\n`nslookup` can't resolve `host.minikube.internal`\r\n```\r\nsuraj@suraj:~/sandbox$ k exec -it dnsutils -- nslookup host.minikube.internal\r\nServer:\t\t10.96.0.10\r\nAddress:\t10.96.0.10#53\r\n\r\n** server can't find host.minikube.internal.domain.name: SERVFAIL\r\n\r\n```\r\n```\r\nsuraj@suraj:~/sandbox$ k exec -it dnsutils -- ping -c 3 host.minikube.internal \r\nPING host.minikube.internal (192.168.58.1) 56(84) bytes of data.\r\n64 bytes from host.minikube.internal (192.168.58.1): icmp_seq=1 ttl=63 time=0.164 ms\r\n64 bytes from host.minikube.internal (192.168.58.1): icmp_seq=2 ttl=63 time=0.030 ms\r\n64 bytes from host.minikube.internal (192.168.58.1): icmp_seq=3 ttl=63 time=0.029 ms\r\n\r\n--- host.minikube.internal ping statistics ---\r\n3 packets transmitted, 3 received, 0% packet loss, time 2025ms\r\nrtt min/avg/max/mdev = 0.029/0.074/0.164/0.063 ms\r\n```\r\nping works but coredns prints errors\r\n```\r\n[ERROR] plugin/errors: 2 host.minikube.internal.domain.name. A: read udp 172.17.0.2:44665->192.168.58.1:53: i/o timeout\r\n[ERROR] plugin/errors: 2 host.minikube.internal.domain.name. A: read udp 172.17.0.2:51268->192.168.58.1:53: i/o timeout\r\n[ERROR] plugin/errors: 2 host.minikube.internal.domain.name. A: read udp 172.17.0.2:59899->192.168.58.1:53: i/o timeout\r\n\r\n```\r\n### minikube info\r\n```\r\nsuraj@suraj:~/sandbox$ minikube version\r\nminikube version: v1.28.0\r\ncommit: 986b1ebd987211ed16f8cc10aed7d2c42fc8392f\r\n\r\n```\r\nI started the cluster with the following command:\r\n```\r\nsuraj@suraj:~$ minikube start\r\n\r\n```\n\n### Attach the log file\n\n[minikube_networking_broken_log.txt](https://github.com/kubernetes/minikube/files/10000330/minikube_networking_broken_log.txt)\r\n\n\n### Operating System\n\nUbuntu\n\n### Driver\n\nDocker",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2022-11-14T06:55:15Z",
      "closed_at": "2023-04-13T08:28:26Z",
      "url": "https://github.com/kubernetes/minikube/issues/15354",
      "comments_count": 7
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21131,
      "title": "Windows mount: inode is determined by file size leading to \"Unknown error 526\"",
      "problem": "### What Happened?\n\nThis issue is afaik first identified in https://github.com/kubernetes/minikube/issues/3046 back in 2018, but it still persists to this day.\n\nWhen mounting the host's files on Windows with both Docker as well as Hyper-V as driver (haven't tested others) has a problem determining the inode of files which leads to all sorts of weird behavior and errors but often `Unknown error 526` when trying to delete or move files or directories.\n\nSimplest repro:\n```ps1\n$> Minikube start --driver=docker\n$> $MOUNTDIR = \"C:\\temp\\minikube\"\n$> $DESTDIR = \"/run/desktop/mnt/host/c/temp/minikube\"\n$> Start-Process minikube -ArgumentList \"mount $MOUNTDIR`:$DESTDIR\"\n$> ssh minikube\n```\n\n```sh\n$> cd /run/desktop/mnt/host/c/temp/minikube\n$> mkdir test\n$> mkdir test/foo\n$> mkdir test/bar\n$> rm -r test\nrm: WARNING: Circular directory structure.\nThis almost certainly means that you have a corrupted file system.\nNOTIFY YOUR SYSTEM MANAGER.\nThe following directory is part of the cycle:\n  test/bar\n\nrm: cannot remove 'test': Unknown error 526\n```\n\nRunning this in home works fine, so it has definitely something to do with the mount:\n```sh\n$> cd ~\n$> mkdir test\n$> mkdir test/foo\n$> mkdir test/bar\n$> rm -r test\n$> ls\n$>\n```\n\nSo let's dive deeper into the inode problem. Back in the mounted directory:\n```sh\n$> cd /run/desktop/mnt/host/c/temp/minikube/test\n$> stat bar\n  File: bar\n  Size: 0               Blocks: 0          IO Block: 262144 directory\nDevice: 8ch/140d        Inode: 2           Links: 1\nAccess: (0777/drwxrwxrwx)  Uid: ( 1000/  docker)   Gid: (  999/  docker)\nAccess: 2025-07-23 08:49:41.000000000 +0000\nModify: 2025-07-23 08:49:41.000000000 +0000\nChange: 2025-07-23 08:49:41.000000000 +0000\n Birth: -\n$> stat foo\n  File: foo\n  Size: 0               Blocks: 0          IO Block: 262144 directory\nDevice: 8ch/140d        Inode: 2           Links: 1\nAccess: (0777/drwxrwxrwx)  Uid: ( 1000/  docker)   Gid: (  999/  docker)\nAccess: 2025-07-23 08:49:39.000000000 +0000\nModify: 2025-07-23 08:49:39.000000000 +0000\nChange: 2025-07-23 08:49:39.000000000 +0000\n Birth: -\n```\n\nNotice how the field Inode for both files is `2`. Now if we check for files:\n\n```sh\n$> echo \"123\" > afile\n$> stat afile\n  File: afile\n  Size: 4               Blocks: 1          IO Block: 262144 regular file\nDevice: 8ch/140d        Inode: 6           Links: 1\nAccess: (0666/-rw-rw-rw-)  Uid: ( 1000/  docker)   Gid: (  999/  docker)\nAccess: 2025-07-23 08:58:38.000000000 +0000\nModify: 2025-07-23 08:58:38.000000000 +0000\nChange: 2025-07-23 08:58:38.000000000 +0000\n Birth: -\n$> echo \"456\" > file2\n$> stat file2\n  File: file2\n  Size: 4               Blocks: 1          IO Block: 262144 regular file\nDevice: 8ch/140d        Inode: 6           Links: 1\nAccess: (0666/-rw-rw-rw-)  Uid: ( 1000/  docker)   Gid: (  999/  docker)\nAccess: 2025-07-23 08:59:33.000000000 +0000\nModify: 2025-07-23 08:59:33.000000000 +0000\nChange: 2025-07-23 08:59:33.000000000 +0000\n Birth: -\n```\n\nBoth files have the inode `6`. Which is coincidentally...\n```sh\n$> ls -l\ntotal 1\n-rw-rw-rw- 1 docker docker 4 Jul 23 08:58 afile\ndrwxrwxrwx 1 docker docker 0 Jul 23 08:49 bar\n-rw-rw-rw- 1 docker docker 4 Jul 23 08:59 file2\ndrwxrwxrwx 1 docker docker 0 Jul 23 08:49 foo\n```\n... all equal to their file size + 2.\n\nAnd again, this behavior is only in the mounted directory:\n```sh\n$> cd ~\n$> echo \"123\" > file1\n$> stat file1\n  File: file1\n  Size: 4               Blocks: 8          IO Block: 4096   regular file\nDevice: 67h/103d        Inode: 60714390    Links: 1\nAccess: (0644/-rw-r--r--)  Uid: (    0/    root)   Gid: (    0/    root)\nAccess: 2025-07-23 09:03:14.207590046 +0000\nModify: 2025-07-23 09:03:14.207590046 +0000\nChange: 2025-07-23 09:03:14.207590046 +0000\n Birth: 2025-07-23 09:03:14.207590046 +0000\n```\n\nBack in the original issue, @frickenate posits it might be due to [this field in a struct](https://github.com/kubernetes/minikube/blob/7498245a96aa3e9be815f5f676aad34e9171f6f7/third_party/go9p/p9.go#L129), but this is beyond my knowledge so I'm just passing it on.\n\n### Attach the log file\n\n[log.txt](https://github.com/user-attachments/files/21384618/log.txt)\n\n### Operating System\n\nWindows\n\n### Driver\n\nDocker",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-07-23T09:17:37Z",
      "closed_at": "2025-12-20T11:09:10Z",
      "url": "https://github.com/kubernetes/minikube/issues/21131",
      "comments_count": 5
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21130,
      "title": "Update minikube ingress dns addon to respond to AAAA request correctly",
      "problem": "### What Happened?\n\nWhen using the minikube-ingress-dns addon on Windows, ingress name resolution fails under certain conditions due to improper handling of DNS query types.\n\n**Root Cause**\n**Windows DNS Resolver Behavior:**\n\n1. When resolving a hostname, Windows sends both:\n      - A query (IPv4)\n      - AAAA query (IPv6)\n\n2. The minikube-ingress-dns Node.js server (at [index.js](https://github.com/sharifelgamal/minikube-ingress-dns/blob/main/app/dns-server/nodejs/src/index.js)):\n     - Accepts all queries on port 53/5353.\n     - Does not check query type (q.type) (see L69\u2013L77).\n     - Always responds with an empty response if no A record exists, even for AAAA queries.\n\n**Result:**\n- Windows send AAAA request first as it prioritizes ipv6 routing.\n- Windows receives a valid but empty AAAA response (no IPv6 address).\n- Windows does NOT retry A query immediately (because the AAAA response is considered valid).\n- Connection fails \u2192 timeout \u2192 ingress hostname unusable.\n\nA PR already exists in the original repo that fixes this:\nPR: [Respond only to IPv4/ANY requests, others with NoData response](https://github.com/sharifelgamal/minikube-ingress-dns/pull/4)\n\nChange: Adds logic to respond only to A/ANY and return NoData for AAAA.\n\n**Note:** Update ingress-dns image tag when above PR is merged.\n\n### Attach the log file\n\n**Current Behavior**\n- AAAA query \u2192 DNS server responds with empty answer (no IPv6 record).\n- Windows stops retrying for A record \u2192 ingress name cannot resolve.\n\n### Operating System\n\nWindows\n\n### Driver\n\nN/A",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-07-23T08:07:17Z",
      "closed_at": "2025-12-20T10:09:10Z",
      "url": "https://github.com/kubernetes/minikube/issues/21130",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21106,
      "title": "Problemas al hacer minikube start",
      "problem": "<!-- Utiliza esta plantilla para informar de incidencias y proporciona tanta informaci\u00f3n como sea posible. Si no lo haces, es posible que la respuesta se retrase. Muchas gracias. -->\n\n**Los comandos necesarios para reproducir la incidencia**:\n\n**El resultado completo del comando que ha fallado**: <details>\n\n\n\n</details>\n\n**El resultado del comando `minikube logs`**: <details>\n\n\n\n</details>\n\n**La versi\u00f3n del sistema operativo que utilizaste**:\n\n---\n\n[logs.txt](https://github.com/user-attachments/files/21352436/logs.txt)\n\nIf the above advice does not help, please let us know:                             \u2502\n\u2502    \ud83d\udc49  https://github.com/kubernetes/minikube/issues/new/choose                           \u2502\n\u2502                                                                                           \u2502\n\u2502    Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.  \n\n---\n\nArchivo logs.txt cargado",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten",
        "l/es"
      ],
      "created_at": "2025-07-21T18:26:47Z",
      "closed_at": "2025-12-18T20:46:10Z",
      "url": "https://github.com/kubernetes/minikube/issues/21106",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 15460,
      "title": "Install cri-tools/cni-plugins with bootstrapper, and not with provisioner",
      "problem": "\r\nCurrently, cri-tools and cni-plugins are installed together with the container runtime and re-used for all k8s versions.\r\n\r\nBut they are supposed to be installed together with the kubernetes installation, which could require specific versions.\r\n\r\nActually it is only `crictl` (not `critest`), and the `flannel` plugin has been scoped out (installed from external project)\r\n\r\nThe versions to use are included with the rpm/deb packaging, and mentioned in the documentation for the binaries:\r\n\r\nhttps://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl\r\n\r\n`CRICTL_VERSION=\"v1.25.0\"`\r\nhttps://github.com/kubernetes-sigs/cri-tools\r\n\r\n`CNI_PLUGINS_VERSION=\"v1.1.1\"`\r\nhttps://github.com/containernetworking/plugins\r\n\r\nThe packages from kubernetes are just re-packaged from the binaries that are available on dl.k8s.io and github.com\r\n\r\n----\r\n\r\nHopefully this feature will make it easier to keep them up-to-date, since they are now hopelessly out of date:\r\n\r\n* https://github.com/kubernetes/minikube/issues/14830\r\n\r\nThe configuration of CRI and CNI is still needed, only the binaries are included in the upstream packaging.\r\n\r\nThey should probably just overwrite any existing `/usr/bin/crictl` and `/opt/cni/bin`, during the installation ?",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/feature",
        "area/build-release",
        "lifecycle/rotten"
      ],
      "created_at": "2022-12-03T13:50:39Z",
      "closed_at": "2025-01-07T09:16:07Z",
      "url": "https://github.com/kubernetes/minikube/issues/15460",
      "comments_count": 17
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21104,
      "title": "Deploy error",
      "problem": "### What Happened?\n\nWhen I try to deploy nodejs app\nwrite minikube service my-service\ndisplay an error, this is the first time I have faced this problem\n\n### Attach the log file\n\n==> Audit <==\n|-----------|--------------------------------|----------|-------------------|---------|----------------------|----------------------|\n|  Command  |              Args              | Profile  |       User        | Version |      Start Time      |       End Time       |\n|-----------|--------------------------------|----------|-------------------|---------|----------------------|----------------------|\n| service   | -n monitoring                  | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 10 May 25 23:54 EEST | 10 May 25 23:54 EEST |\n|           | prometheus-server              |          |                   |         |                      |                      |\n| service   | -n monitoring grafana          | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 10 May 25 23:56 EEST | 10 May 25 23:57 EEST |\n| service   | -n monitoring grafana          | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 10 May 25 23:59 EEST | 10 May 25 23:59 EEST |\n| service   | -n monitoring grafana          | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 11 May 25 00:01 EEST | 11 May 25 00:02 EEST |\n| service   | -n monitoring grafana          | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 11 May 25 00:03 EEST | 11 May 25 00:04 EEST |\n| service   | -n monitoring grafana          | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 11 May 25 00:06 EEST | 11 May 25 00:06 EEST |\n| service   | -n monitoring grafana          | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 11 May 25 00:08 EEST | 11 May 25 00:08 EEST |\n| service   | -n monitoring grafana          | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 11 May 25 00:09 EEST | 11 May 25 00:10 EEST |\n| service   | -n monitoring grafana          | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 11 May 25 00:11 EEST | 11 May 25 00:12 EEST |\n| service   | -n monitoring grafana          | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 11 May 25 00:13 EEST | 11 May 25 00:14 EEST |\n| service   | -n monitoring grafana          | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 11 May 25 00:15 EEST | 11 May 25 00:16 EEST |\n| service   | -n monitoring grafana          | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 11 May 25 00:17 EEST | 11 May 25 00:18 EEST |\n| service   | -n monitoring grafana          | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 11 May 25 00:19 EEST | 11 May 25 00:31 EEST |\n| service   | -n monitoring grafana          | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 11 May 25 00:33 EEST | 11 May 25 00:33 EEST |\n| service   | -n monitoring grafana          | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 11 May 25 00:35 EEST | 11 May 25 00:36 EEST |\n| service   | -n monitoring grafana          | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 11 May 25 00:38 EEST | 11 May 25 00:38 EEST |\n| service   | -n monitoring grafana          | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 11 May 25 00:40 EEST | 11 May 25 00:40 EEST |\n| service   | -n monitoring grafana          | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 11 May 25 00:42 EEST | 11 May 25 00:42 EEST |\n| service   | -n monitoring                  | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 11 May 25 00:44 EEST | 11 May 25 00:46 EEST |\n|           | prometheus-server              |          |                   |         |                      |                      |\n| service   | -n jenkins                     | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 11 May 25 01:56 EEST |                      |\n| start     |                                | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 11 May 25 03:18 EEST |                      |\n| service   | jenkins                        | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 11 May 25 03:39 EEST | 11 May 25 03:54 EEST |\n| start     |                                | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 11 May 25 04:34 EEST |                      |\n| start     |                                | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 11 May 25 05:47 EEST |                      |\n| start     | --driver docker                | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 11 May 25 19:34 EEST | 11 May 25 19:36 EEST |\n| ip        |                                | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 12 May 25 17:36 EEST | 12 May 25 17:36 EEST |\n| addons    | list                           | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 14 May 25 01:38 EEST | 14 May 25 01:38 EEST |\n| dashboard |                                | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 14 May 25 01:40 EEST |                      |\n| start     |                                | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 14 May 25 14:46 EEST | 14 May 25 14:47 EEST |\n| start     |                                | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 15 May 25 17:31 EEST | 15 May 25 17:32 EEST |\n| start     | --driver docker                | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 15 May 25 21:34 EEST | 15 May 25 21:36 EEST |\n| addons    | list                           | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 16 May 25 01:50 EEST | 16 May 25 01:50 EEST |\n| start     |                                | minikube | ABD_ELHAMID\\ahmed | v1.35.0 | 18 May 25 14:53 EEST | 18 May 25 14:54 EEST |\n| start     | --driver=docker                | minikube | ABD_ELHAMID\\ahmed | v1.36.0 | 02 Jun 25 15:05 EEST | 02 Jun 25 15:12 EEST |\n| addons    | list                           | minikube | ABD_ELHAMID\\ahmed | v1.36.0 | 02 Jun 25 15:12 EEST | 02 Jun 25 15:12 EEST |\n| addons    | enable ingress                 | minikube | ABD_ELHAMID\\ahmed | v1.36.0 | 02 Jun 25 15:12 EEST | 02 Jun 25 15:13 EEST |\n| addons    | list                           | minikube | ABD_ELHAMID\\ahmed | v1.36.0 | 02 Jun 25 15:13 EEST | 02 Jun 25 15:13 EEST |\n| addons    | metrics-server                 | minikube | ABD_ELHAMID\\ahmed | v1.36.0 | 02 Jun 25 15:13 EEST | 02 Jun 25 15:13 EEST |\n| addons    | enable metrics-server          | minikube | ABD_ELHAMID\\ahmed | v1.36.0 | 02 Jun 25 15:13 EEST | 02 Jun 25 15:14 EEST |\n| addons    | list                           | minikube | ABD_ELHAMID\\ahmed | v1.36.0 | 02 Jun 25 15:14 EEST | 02 Jun 25 15:14 EEST |\n| stop      |                                | minikube | ABD_ELHAMID\\ahmed | v1.36.0 | 02 Jun 25 16:51 EEST | 02 Jun 25 16:51 EEST |\n| start     | --driver=docker                | minikube | ABD_ELHAMID\\ahmed | v1.36.0 | 28 Jun 25 21:18 EEST | 28 Jun 25 21:19 EEST |\n| stop      |                                | minikube | ABD_ELHAMID\\ahmed | v1.36.0 | 28 Jun 25 22:43 EEST | 28 Jun 25 22:43 EEST |\n| start     | --driver=docker                | minikube | ABD_ELHAMID\\ahmed | v1.36.0 | 28 Jun 25 22:44 EEST | 28 Jun 25 22:44 EEST |\n| stop      |                                | minikube | ABD_ELHAMID\\ahmed | v1.36.0 | 29 Jun 25 01:00 EEST | 29 Jun 25 01:00 EEST |\n| start     | --driver=docker                | minikube | ABD_ELHAMID\\ahmed | v1.36.0 | 21 Jul 25 13:51 EEST | 21 Jul 25 13:51 EEST |\n| service   | my-service                     | minikube | ABD_ELHAMID\\ahmed | v1.36.0 | 21 Jul 25 15:03 EEST | 21 Jul 25 15:07 EEST |\n| dashboard |                                | minikube | ABD_ELHAMID\\ahmed | v1.36.0 | 21 Jul 25 15:04 EEST |                      |\n| tunnel    |                                | minikube | ABD_ELHAMID\\ahmed | v1.36.0 | 21 Jul 25 15:07 EEST | 21 Jul 25 15:13 EEST |\n| service   | my-service                     | minikube | ABD_ELHAMID\\ahmed | v1.36.0 | 21 Jul 25 15:07 EEST |                      |\n| service   | my-service --url               | minikube | ABD_ELHAMID\\ahmed | v1.36.0 | 21 Jul 25 15:08 EEST |                      |\n| service   | my-service                     | minikube | ABD_ELHAMID\\ahmed | v1.36.0 | 21 Jul 25 15:11 EEST |                      |\n| service   | my-service                     | minikube | ABD_ELHAMID\\ahmed | v1.36.0 | 21 Jul 25 15:16 EEST |                      |\n| tunnel    |                                | minikube | ABD_ELHAMID\\ahmed | v1.36.0 | 21 Jul 25 15:16 EEST | 21 Jul 25 15:27 EEST |\n| service   | my-service                     | minikube | ABD_ELHAMID\\ahmed | v1.36.0 | 21 Jul 25 15:16 EEST |                      |\n| dashboard |                                | minikube | ABD_ELHAMID\\ahmed | v1.36.0 | 21 Jul 25 15:27 EEST |                      |\n| tunnel    |                                | minikube | ABD_ELHAMID\\ahmed | v1.36.0 | 21 Jul 25 15:27 EEST | 21 Jul 25 15:38 EEST |\n| service   | my-service                     | minikube | ABD_ELHAMID\\ahmed | v1.36.0 | 21 Jul 25 15:28 EEST |                      |\n| service   | my-service                     | minikube | ABD_ELHAMID\\ahmed | v1.36.0 | 21 Jul 25 15:35 EEST |                      |\n| service   | my-service                     | minikube | ABD_ELHAMID\\ahmed | v1.36.0 | 21 Jul 25 15:39 EEST |                      |\n|-----------|--------------------------------|----------|-------------------|---------|----------------------|----------------------|\n\n\n==> Last Start <==\nLog file created at: 2025/07/21 13:51:07\nRunning on machine: Abd_Elhamid\nBinary: Built with gc go1.24.0 for windows/amd64\nLog line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg\nI0721 13:51:07.841609    7612 out.go:345] Setting OutFile to fd 660 ...\nI0721 13:51:07.841609    7612 out.go:392] TERM=,COLORTERM=, which probably does not support color\nI0721 13:51:07.841609    7612 out.go:358] Setting ErrFile to fd 2036...\nI0721 13:51:07.841609    7612 out.go:392] TERM=,COLORTERM=, which probably does not support color\nW0721 13:51:07.852140    7612 root.go:314] Error reading config file at C:\\Users\\ahmed\\.minikube\\config\\config.json: open C:\\Users\\ahmed\\.minikube\\config\\config.json: The system cannot find the file specified.\nI0721 13:51:07.877470    7612 out.go:352] Setting JSON to false\nI0721 13:51:07.881206    7612 start.go:130] hostinfo: {\"hostname\":\"Abd_Elhamid\",\"uptime\":71999,\"bootTime\":1753023068,\"procs\":310,\"os\":\"windows\",\"platform\":\"Microsoft Windows 11 Pro\",\"platformFamily\":\"Standalone Workstation\",\"platformVersion\":\"10.0.26100.4652 Build 26100.4652\",\"kernelVersion\":\"10.0.26100.4652 Build 26100.4652\",\"kernelArch\":\"x86_64\",\"virtualizationSystem\":\"\",\"virtualizationRole\":\"\",\"hostId\":\"68f96d9b-67cc-4fdb-ad6f-5e053df715f9\"}\nW0721 13:51:07.881206    7612 start.go:138] gopshost.Virtualization returned error: not implemented yet\nI0721 13:51:07.883109    7612 out.go:177] * minikube v1.36.0 on Microsoft Windows 11 Pro 10.0.26100.4652 Build 26100.4652\nI0721 13:51:07.885406    7612 notify.go:220] Checking for updates...\nI0721 13:51:07.885971    7612 config.go:182] Loaded profile config \"minikube\": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0\nI0721 13:51:07.887632    7612 out.go:177] * Kubernetes 1.33.1 is now available. If you would like to upgrade, specify: --kubernetes-version=v1.33.1\nI0721 13:51:07.889892    7612 driver.go:404] Setting default libvirt URI to qemu:///system\nI0721 13:51:07.968843    7612 docker.go:123] docker version: linux-28.1.1:Docker Desktop 4.41.2 (191736)\nI0721 13:51:07.978327    7612 cli_runner.go:164] Run: docker system info --format \"{{json .}}\"\nI0721 13:51:08.238600    7612 info.go:266] docker info: {ID:e6305218-cd57-4a86-a722-1d45d3695a92 Containers:2 ContainersRunning:0 ContainersPaused:0 ContainersStopped:2 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:72 OomKillDisable:true NGoroutines:92 SystemTime:2025-07-21 10:43:03.950116854 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:18 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:8180121600 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\\\.\\pipe\\docker_cli] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:05044ec0a9a75232cad458027ca83437aae3f4da} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:v1.2.5-0-g59923ef} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\\Program Files\\Docker\\cli-plugins\\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.7] map[Name:buildx Path:C:\\Program Files\\Docker\\cli-plugins\\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0-desktop.1] map[Name:cloud Path:C:\\Program Files\\Docker\\cli-plugins\\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.3.0] map[Name:compose Path:C:\\Program Files\\Docker\\cli-plugins\\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1-desktop.1] map[Name:debug Path:C:\\Program Files\\Docker\\cli-plugins\\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\\Program Files\\Docker\\cli-plugins\\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.8] map[Name:dev Path:C:\\Program Files\\Docker\\cli-plugins\\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\\Program Files\\Docker\\cli-plugins\\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:C:\\Program Files\\Docker\\cli-plugins\\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\\Program Files\\Docker\\cli-plugins\\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:dev] map[Name:model Path:C:\\Program Files\\Docker\\cli-plugins\\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.11] map[Name:sbom Path:C:\\Program Files\\Docker\\cli-plugins\\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\\Program Files\\Docker\\cli-plugins\\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.1]] Warnings:<nil>}}\nI0721 13:51:08.242837    7612 out.go:177] * Using the docker driver based on existing profile\nI0721 13:51:08.244412    7612 start.go:304] selected driver: docker\nI0721 13:51:08.244412    7612 start.go:908] validating driver \"docker\" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\\Users\\ahmed:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}\nI0721 13:51:08.244412    7612 start.go:919] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}\nI0721 13:51:08.262713    7612 cli_runner.go:164] Run: docker system info --format \"{{json .}}\"\nI0721 13:51:08.528368    7612 info.go:266] docker info: {ID:e6305218-cd57-4a86-a722-1d45d3695a92 Containers:2 ContainersRunning:0 ContainersPaused:0 ContainersStopped:2 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:72 OomKillDisable:true NGoroutines:92 SystemTime:2025-07-21 10:43:03.950116854 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:18 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:8180121600 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\\\.\\pipe\\docker_cli] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:05044ec0a9a75232cad458027ca83437aae3f4da} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:v1.2.5-0-g59923ef} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\\Program Files\\Docker\\cli-plugins\\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.7] map[Name:buildx Path:C:\\Program Files\\Docker\\cli-plugins\\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0-desktop.1] map[Name:cloud Path:C:\\Program Files\\Docker\\cli-plugins\\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.3.0] map[Name:compose Path:C:\\Program Files\\Docker\\cli-plugins\\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1-desktop.1] map[Name:debug Path:C:\\Program Files\\Docker\\cli-plugins\\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\\Program Files\\Docker\\cli-plugins\\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.8] map[Name:dev Path:C:\\Program Files\\Docker\\cli-plugins\\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\\Program Files\\Docker\\cli-plugins\\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:C:\\Program Files\\Docker\\cli-plugins\\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\\Program Files\\Docker\\cli-plugins\\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:dev] map[Name:model Path:C:\\Program Files\\Docker\\cli-plugins\\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.11] map[Name:sbom Path:C:\\Program Files\\Docker\\cli-plugins\\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\\Program Files\\Docker\\cli-plugins\\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.1]] Warnings:<nil>}}\nI0721 13:51:08.573278    7612 cni.go:84] Creating CNI manager for \"\"\nI0721 13:51:08.573278    7612 cni.go:158] \"docker\" driver + \"docker\" container runtime found on kubernetes v1.24+, recommending bridge\nI0721 13:51:08.573278    7612 start.go:347] cluster config:\n{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\\Users\\ahmed:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}\nI0721 13:51:08.575438    7612 out.go:177] * Starting \"minikube\" primary control-plane node in \"minikube\" cluster\nI0721 13:51:08.577103    7612 cache.go:121] Beginning downloading kic base image for docker with docker\nI0721 13:51:08.578257    7612 out.go:177] * Pulling base image v0.0.47 ...\nI0721 13:51:08.580506    7612 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker\nI0721 13:51:08.580506    7612 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon\nI0721 13:51:08.581564    7612 preload.go:146] Found local preload: C:\\Users\\ahmed\\.minikube\\cache\\preloaded-tarball\\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4\nI0721 13:51:08.581564    7612 cache.go:56] Caching tarball of preloaded images\nI0721 13:51:08.581564    7612 preload.go:172] Found C:\\Users\\ahmed\\.minikube\\cache\\preloaded-tarball\\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download\nI0721 13:51:08.581564    7612 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker\nI0721 13:51:08.582096    7612 profile.go:143] Saving config to C:\\Users\\ahmed\\.minikube\\profiles\\minikube\\config.json ...\nI0721 13:51:08.663018    7612 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon, skipping pull\nI0721 13:51:08.663018    7612 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in daemon, skipping load\nI0721 13:51:08.663018    7612 cache.go:230] Successfully downloaded all kic artifacts\nI0721 13:51:08.663550    7612 start.go:360] acquireMachinesLock for minikube: {Name:mk27b9d09f3e4a2eea93194ca50bdc6f4359e36f Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}\nI0721 13:51:08.663550    7612 start.go:364] duration metric: took 0s to acquireMachinesLock for \"minikube\"\nI0721 13:51:08.663550    7612 start.go:96] Skipping create...Using existing machine configuration\nI0721 13:51:08.663550    7612 fix.go:54] fixHost starting: \nI0721 13:51:08.682976    7612 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}\nI0721 13:51:08.741745    7612 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>\nW0721 13:51:08.741745    7612 fix.go:138] unexpected machine state, will restart: <nil>\nI0721 13:51:08.746591    7612 out.go:177] * Restarting existing docker container for \"minikube\" ...\nI0721 13:51:08.758817    7612 cli_runner.go:164] Run: docker start minikube\nI0721 13:51:09.182459    7612 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}\nI0721 13:51:09.280563    7612 kic.go:430] container \"minikube\" state is running.\nI0721 13:51:09.295199    7612 cli_runner.go:164] Run: docker container inspect -f \"{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}\" minikube\nI0721 13:51:09.391622    7612 profile.go:143] Saving config to C:\\Users\\ahmed\\.minikube\\profiles\\minikube\\config.json ...\nI0721 13:51:09.393752    7612 machine.go:93] provisionDockerMachine start ...\nI0721 13:51:09.406027    7612 cli_runner.go:164] Run: docker container inspect -f \"'{{(index (index .NetworkSettings.Ports \"22/tcp\") 0).HostPort}}'\" minikube\nI0721 13:51:09.479300    7612 main.go:141] libmachine: Using SSH client type: native\nI0721 13:51:09.488926    7612 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x126a9e0] 0x126d520 <nil>  [] 0s} 127.0.0.1 58158 <nil> <nil>}\nI0721 13:51:09.488926    7612 main.go:141] libmachine: About to run SSH command:\nhostname\nI0721 13:51:09.491057    7612 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF\nI0721 13:51:12.660708    7612 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube\n\nI0721 13:51:12.660708    7612 ubuntu.go:169] provisioning hostname \"minikube\"\nI0721 13:51:12.671907    7612 cli_runner.go:164] Run: docker container inspect -f \"'{{(index (index .NetworkSettings.Ports \"22/tcp\") 0).HostPort}}'\" minikube\nI0721 13:51:12.736750    7612 main.go:141] libmachine: Using SSH client type: native\nI0721 13:51:12.736750    7612 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x126a9e0] 0x126d520 <nil>  [] 0s} 127.0.0.1 58158 <nil> <nil>}\nI0721 13:51:12.736750    7612 main.go:141] libmachine: About to run SSH command:\nsudo hostname minikube && echo \"minikube\" | sudo tee /etc/hostname\nI0721 13:51:12.957823    7612 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube\n\nI0721 13:51:12.967875    7612 cli_runner.go:164] Run: docker container inspect -f \"'{{(index (index .NetworkSettings.Ports \"22/tcp\") 0).HostPort}}'\" minikube\nI0721 13:51:13.032288    7612 main.go:141] libmachine: Using SSH client type: native\nI0721 13:51:13.032288    7612 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x126a9e0] 0x126d520 <nil>  [] 0s} 127.0.0.1 58158 <nil> <nil>}\nI0721 13:51:13.032288    7612 main.go:141] libmachine: About to run SSH command:\n\n\t\tif ! grep -xq '.*\\sminikube' /etc/hosts; then\n\t\t\tif grep -xq '127.0.1.1\\s.*' /etc/hosts; then\n\t\t\t\tsudo sed -i 's/^127.0.1.1\\s.*/127.0.1.1 minikube/g' /etc/hosts;\n\t\t\telse \n\t\t\t\techo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; \n\t\t\tfi\n\t\tfi\nI0721 13:51:13.175247    7612 main.go:141] libmachine: SSH cmd err, output: <nil>: \nI0721 13:51:13.175247    7612 ubuntu.go:175] set auth options {CertDir:C:\\Users\\ahmed\\.minikube CaCertPath:C:\\Users\\ahmed\\.minikube\\certs\\ca.pem CaPrivateKeyPath:C:\\Users\\ahmed\\.minikube\\certs\\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\\Users\\ahmed\\.minikube\\machines\\server.pem ServerKeyPath:C:\\Users\\ahmed\\.minikube\\machines\\server-key.pem ClientKeyPath:C:\\Users\\ahmed\\.minikube\\certs\\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\\Users\\ahmed\\.minikube\\certs\\cert.pem ServerCertSANs:[] StorePath:C:\\Users\\ahmed\\.minikube}\nI0721 13:51:13.175247    7612 ubuntu.go:177] setting up certificates\nI0721 13:51:13.175247    7612 provision.go:84] configureAuth start\nI0721 13:51:13.184827    7612 cli_runner.go:164] Run: docker container inspect -f \"{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}\" minikube\nI0721 13:51:13.247007    7612 provision.go:143] copyHostCerts\nI0721 13:51:13.259893    7612 exec_runner.go:144] found C:\\Users\\ahmed\\.minikube/ca.pem, removing ...\nI0721 13:51:13.260253    7612 exec_runner.go:203] rm: C:\\Users\\ahmed\\.minikube\\ca.pem\nI0721 13:51:13.260253    7612 exec_runner.go:151] cp: C:\\Users\\ahmed\\.minikube\\certs\\ca.pem --> C:\\Users\\ahmed\\.minikube/ca.pem (1074 bytes)\nI0721 13:51:13.271721    7612 exec_runner.go:144] found C:\\Users\\ahmed\\.minikube/cert.pem, removing ...\nI0721 13:51:13.271721    7612 exec_runner.go:203] rm: C:\\Users\\ahmed\\.minikube\\cert.pem\nI0721 13:51:13.271721    7612 exec_runner.go:151] cp: C:\\Users\\ahmed\\.minikube\\certs\\cert.pem --> C:\\Users\\ahmed\\.minikube/cert.pem (1119 bytes)\nI0721 13:51:13.280302    7612 exec_runner.go:144] found C:\\Users\\ahmed\\.minikube/key.pem, removing ...\nI0721 13:51:13.280302    7612 exec_runner.go:203] rm: C:\\Users\\ahmed\\.minikube\\key.pem\nI0721 13:51:13.280814    7612 exec_runner.go:151] cp: C:\\Users\\ahmed\\.minikube\\certs\\key.pem --> C:\\Users\\ahmed\\.minikube/key.pem (1679 bytes)\nI0721 13:51:13.280814    7612 provision.go:117] generating server cert: C:\\Users\\ahmed\\.minikube\\machines\\server.pem ca-key=C:\\Users\\ahmed\\.minikube\\certs\\ca.pem private-key=C:\\Users\\ahmed\\.minikube\\certs\\ca-key.pem org=ahmed.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]\nI0721 13:51:13.600300    7612 provision.go:177] copyRemoteCerts\nI0721 13:51:13.612875    7612 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker\nI0721 13:51:13.623358    7612 cli_runner.go:164] Run: docker container inspect -f \"'{{(index (index .NetworkSettings.Ports \"22/tcp\") 0).HostPort}}'\" minikube\nI0721 13:51:13.689308    7612 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58158 SSHKeyPath:C:\\Users\\ahmed\\.minikube\\machines\\minikube\\id_rsa Username:docker}\nI0721 13:51:13.798858    7612 ssh_runner.go:362] scp C:\\Users\\ahmed\\.minikube\\machines\\server.pem --> /etc/docker/server.pem (1176 bytes)\nI0721 13:51:13.826189    7612 ssh_runner.go:362] scp C:\\Users\\ahmed\\.minikube\\machines\\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)\nI0721 13:51:13.852553    7612 ssh_runner.go:362] scp C:\\Users\\ahmed\\.minikube\\certs\\ca.pem --> /etc/docker/ca.pem (1074 bytes)\nI0721 13:51:13.877708    7612 provision.go:87] duration metric: took 702.4602ms to configureAuth\nI0721 13:51:13.877708    7612 ubuntu.go:193] setting minikube options for container-runtime\nI0721 13:51:13.878301    7612 config.go:182] Loaded profile config \"minikube\": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0\nI0721 13:51:13.888472    7612 cli_runner.go:164] Run: docker container inspect -f \"'{{(index (index .NetworkSettings.Ports \"22/tcp\") 0).HostPort}}'\" minikube\nI0721 13:51:13.954396    7612 main.go:141] libmachine: Using SSH client type: native\nI0721 13:51:13.954942    7612 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x126a9e0] 0x126d520 <nil>  [] 0s} 127.0.0.1 58158 <nil> <nil>}\nI0721 13:51:13.954942    7612 main.go:141] libmachine: About to run SSH command:\ndf --output=fstype / | tail -n 1\nI0721 13:51:14.096249    7612 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay\n\nI0721 13:51:14.096249    7612 ubuntu.go:71] root file system type: overlay\nI0721 13:51:14.096249    7612 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...\nI0721 13:51:14.107848    7612 cli_runner.go:164] Run: docker container inspect -f \"'{{(index (index .NetworkSettings.Ports \"22/tcp\") 0).HostPort}}'\" minikube\nI0721 13:51:14.176984    7612 main.go:141] libmachine: Using SSH client type: native\nI0721 13:51:14.177510    7612 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x126a9e0] 0x126d520 <nil>  [] 0s} 127.0.0.1 58158 <nil> <nil>}\nI0721 13:51:14.177510    7612 main.go:141] libmachine: About to run SSH command:\nsudo mkdir -p /lib/systemd/system && printf %s \"[Unit]\nDescription=Docker Application Container Engine\nDocumentation=https://docs.docker.com\nBindsTo=containerd.service\nAfter=network-online.target firewalld.service containerd.service\nWants=network-online.target\nRequires=docker.socket\nStartLimitBurst=3\nStartLimitIntervalSec=60\n\n[Service]\nType=notify\nRestart=on-failure\n\n### Operating System\n\nNone\n\n### Driver\n\nNone",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-07-21T12:50:47Z",
      "closed_at": "2025-12-18T16:46:11Z",
      "url": "https://github.com/kubernetes/minikube/issues/21104",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21099,
      "title": "no running pod for service",
      "problem": "### What Happened?\n\nI'm having issues running k8s fils with docker. Attaching logs.\n\n### Attach the log file\n\n[minikube_service_335debe4d4d5c6e740c13cd65c9b08327a3bacb8_0.log](https://github.com/user-attachments/files/21334349/minikube_service_335debe4d4d5c6e740c13cd65c9b08327a3bacb8_0.log)\n\n[logs.txt](https://github.com/user-attachments/files/21334350/logs.txt)\n\n### Operating System\n\nWindows\n\n### Driver\n\nNone",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-07-20T10:41:51Z",
      "closed_at": "2025-12-17T13:33:11Z",
      "url": "https://github.com/kubernetes/minikube/issues/21099",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 10085,
      "title": "istio-ingressgateway: tunnel doesn't start",
      "problem": "While minikube seems to be starting and running effectively with `minikube start`, I am unable to successfully execute the command `minikube tunnel`. After entering the password and waiting a significant amount of time, no output similar to what is show on [Accessing apps](https://minikube.sigs.k8s.io/docs/handbook/accessing/) displays nor does the minikube ip seem to be responsive. The last message to display with logging verbosity level 2 is `Patched istio-ingressgateway with IP 127.0.0.1`. I've tried running with `sudo` as well, but same hanging.\r\n\r\nThis is using the default kube config generated from `minikube start`. Minikube was installed via homebrew.\r\n\r\n**System:**\r\nMacOS - 11.1\r\nDocker - 20.10.0\r\nKubernetes - 1.19.3\r\nMinikube - 1.16.0\r\n\r\n<!--- Please include the \"minikube start\" command you used in your reproduction steps --->\r\n**Steps to reproduce the issue:** \r\n\r\n1. `minikube start`\r\n2. `minikube tunnel --alsologtostderr --v=2`\r\n\r\n<!--- TIP: Add the \"--alsologtostderr\" flag to the command-line for more logs --->\r\n**Full output of failed command:** \r\n\r\n```\r\nminikube tunnel --alsologtostderr --v=2\r\nI0102 13:56:53.026587    5794 out.go:221] Setting OutFile to fd 1 ...\r\nI0102 13:56:53.027277    5794 out.go:273] isatty.IsTerminal(1) = true\r\nI0102 13:56:53.027295    5794 out.go:234] Setting ErrFile to fd 2...\r\nI0102 13:56:53.027303    5794 out.go:273] isatty.IsTerminal(2) = true\r\nI0102 13:56:53.027417    5794 root.go:280] Updating PATH: /Users/someuser/.minikube/bin\r\nW0102 13:56:53.027576    5794 root.go:255] Error reading config file at /Users/someuser/.minikube/config/config.json: open /Users/someuser/.minikube/config/config.json: no such file or directory\r\nI0102 13:56:53.028066    5794 mustload.go:66] Loading cluster: minikube\r\nI0102 13:56:53.029093    5794 cli_runner.go:111] Run: docker container inspect minikube --format={{.State.Status}}\r\nI0102 13:56:53.184465    5794 host.go:66] Checking if \"minikube\" exists ...\r\nI0102 13:56:53.184936    5794 cli_runner.go:111] Run: docker container inspect -f \"'{{(index (index .NetworkSettings.Ports \"8443/tcp\") 0).HostPort}}'\" minikube\r\nI0102 13:56:53.329575    5794 api_server.go:146] Checking apiserver status ...\r\nI0102 13:56:53.329732    5794 ssh_runner.go:149] Run: sudo pgrep -xnf kube-apiserver.*minikube.*\r\nI0102 13:56:53.329817    5794 cli_runner.go:111] Run: docker container inspect -f \"'{{(index (index .NetworkSettings.Ports \"22/tcp\") 0).HostPort}}'\" minikube\r\nI0102 13:56:53.480793    5794 sshutil.go:48] new ssh client: &{IP:127.0.0.1 Port:55007 SSHKeyPath:/Users/someuser/.minikube/machines/minikube/id_rsa Username:docker}\r\nI0102 13:56:53.621072    5794 ssh_runner.go:149] Run: sudo egrep ^[0-9]+:freezer: /proc/1884/cgroup\r\nI0102 13:56:53.632939    5794 api_server.go:162] apiserver freezer: \"7:freezer:/docker/f65f71a326b1bc0138a18b4f832afb887fd58b3e919089379f915cb88d2f67ae/kubepods/burstable/pod524cecac593a7ad14f29307cb61f56b8/7f39232f1fc0ca71da44a5579f60e7d6b0839e7717a4bafd3470a7ef23ba5eee\"\r\nI0102 13:56:53.633091    5794 ssh_runner.go:149] Run: sudo cat /sys/fs/cgroup/freezer/docker/f65f71a326b1bc0138a18b4f832afb887fd58b3e919089379f915cb88d2f67ae/kubepods/burstable/pod524cecac593a7ad14f29307cb61f56b8/7f39232f1fc0ca71da44a5579f60e7d6b0839e7717a4bafd3470a7ef23ba5eee/freezer.state\r\nI0102 13:56:53.650094    5794 api_server.go:184] freezer state: \"THAWED\"\r\nI0102 13:56:53.650147    5794 api_server.go:221] Checking apiserver healthz at https://127.0.0.1:55004/healthz ...\r\nI0102 13:56:53.663613    5794 api_server.go:241] https://127.0.0.1:55004/healthz returned 200:\r\nok\r\nI0102 13:56:53.663646    5794 tunnel.go:57] Checking for tunnels to cleanup...\r\nI0102 13:56:53.665014    5794 kapi.go:59] client config for minikube: &rest.Config{Host:\"https://127.0.0.1:55004\", APIPath:\"\", ContentConfig:rest.ContentConfig{AcceptContentTypes:\"\", ContentType:\"\", GroupVersion:(*schema.GroupVersion)(nil), NegotiatedSerializer:runtime.NegotiatedSerializer(nil)}, Username:\"\", Password:\"\", BearerToken:\"\", BearerTokenFile:\"\", Impersonate:rest.ImpersonationConfig{UserName:\"\", Groups:[]string(nil), Extra:map[string][]string(nil)}, AuthProvider:<nil>, AuthConfigPersister:rest.AuthProviderConfigPersister(nil), ExecProvider:<nil>, TLSClientConfig:rest.sanitizedTLSClientConfig{Insecure:false, ServerName:\"\", CertFile:\"/Users/someuser/.minikube/profiles/minikube/client.crt\", KeyFile:\"/Users/someuser/.minikube/profiles/minikube/client.key\", CAFile:\"/Users/someuser/.minikube/ca.crt\", CertData:[]uint8(nil), KeyData:[]uint8(nil), CAData:[]uint8(nil), NextProtos:[]string(nil)}, UserAgent:\"\", DisableCompression:false, Transport:http.RoundTripper(nil), WrapTransport:(transport.WrapperFunc)(0x541a300), QPS:0, Burst:0, RateLimiter:flowcontrol.RateLimiter(nil), Timeout:0, Dial:(func(context.Context, string, string) (net.Conn, error))(nil)}\r\nI0102 13:56:53.669189    5794 cli_runner.go:111] Run: docker container inspect -f \"'{{(index (index .NetworkSettings.Ports \"22/tcp\") 0).HostPort}}'\" minikube\r\nI0102 13:56:53.838208    5794 out.go:119] \u2757  The service istio-ingressgateway requires privileged ports to be exposed: [80 443]\r\n\u2757  The service istio-ingressgateway requires privileged ports to be exposed: [80 443]\r\nI0102 13:56:53.843739    5794 out.go:119] \ud83d\udd11  sudo permission will be asked for it.\r\n\ud83d\udd11  sudo permission will be asked for it.\r\nI0102 13:56:53.851142    5794 out.go:119] \ud83c\udfc3  Starting tunnel for service istio-ingressgateway.\r\n\ud83c\udfc3  Starting tunnel for service istio-ingressgateway.\r\nI0102 13:56:53.854697    5794 loadbalancer_patcher.go:121] Patched istio-ingressgateway with IP 127.0.0.1\r\n```\r\n\r\n**Full output of `minikube start` command used, if not already included:**\r\n\r\n```\r\n\ud83d\ude04  minikube v1.16.0 on Darwin 11.1\r\n\u2728  Using the docker driver based on existing profile\r\n\ud83d\udc4d  Starting control plane node minikube in cluster minikube\r\n\ud83d\udd04  Restarting existing docker container for \"minikube\" ...\r\n\ud83d\udc33  Preparing Kubernetes v1.20.0 on Docker 20.10.0 ...\r\n\ud83d\udd0e  Verifying Kubernetes components...\r\n\ud83c\udf1f  Enabled addons: default-storageclass, storage-provisioner, dashboard\r\n\ud83c\udfc4  Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\r\n```\r\n\r\n**Optional: Full output of `minikube logs` command:**\r\n<details>\r\n\r\n\r\n</details>\r\n",
      "solution": "I found a solution\r\n\r\nI had to expose a \"LoadBalancer\" in order for me to reach the app. This was mentioned nowhere on the docs.\r\n\r\nHere's what I had to do.\r\n\r\n`my-ingress.yml`\r\n```yml\r\napiVersion: networking.k8s.io/v1\r\nkind: Ingress\r\nmetadata:\r\n  name: my-ingress\r\nspec:\r\n  rules:\r\n  - http:\r\n      paths:\r\n      - pathType: Prefix\r\n        path: /\r\n        backend:\r\n          service:\r\n            name: hello-nodejs-service\r\n            port:\r\n              number: 80\r\n```\r\n`my-service.yml`\r\n```yml\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: hello-nodejs-service\r\nspec:\r\n  type: NodePort\r\n  ports:\r\n    - port: 80\r\n      targetPort: 80\r\n      protocol: TCP\r\n  selector:\r\n    app: hello-nodejs\r\n```\r\n\r\n`my-deployment.yml`\r\n```yml\r\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: hello-nodejs-deployment\r\nspec:\r\n  replicas: 2\r\n  selector:\r\n    matchLabels:\r\n      app: hello-nodejs\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: hello-nodejs\r\n    spec:\r\n      containers:\r\n      - image: hello-nodejs:latest #you need to switch this with your own container image / or use a public docker image\r\n        imagePullPolicy: IfNotPresent\r\n        name: hello-nodejs\r\n        resources:\r\n          limits:\r\n            cpu: \"500m\"\r\n            memory: \"256Mi\"\r\n        ports:\r\n        - containerPort: 80\r\n```\r\n\r\nApply the configs with `kubectl apply -f filename.yml\r\n\r\nThen I had to run the following:\r\n`kubectl expose deployment my-deployment --type=LoadBalancer --port=80`\r\nafter that was done\r\n`minikube tunnel` would start and output a message. It wasn't \"hanging\" It just had no deployment running\r\n\r\n**edit**\r\nstill having issues with this now\n\n---\n\n> > With Hyperkit (v0.20210107-2-g2f061e) instead of Docker, it is running fine.\r\n> \r\n> How did you fix it? @martinknechtel\r\n\r\n@AlbertMarashi The only pitfall I had on starting up minikube is broken DNS connection, but thats another problem ;-) Observation:\r\n```\r\n\u276f minikube start\r\n[...]\r\n\u2757  This VM is having trouble accessing https://k8s.gcr.io\r\n```\r\n\r\nSolution:\r\n```\r\nminikube ssh\r\nrm -f /etc/resolv.conf && echo nameserver 192.168.178.1 > /etc/resolv.conf #replace with your nameserver IP\r\n```\n\n---\n\nI don't know why this wasn't mentioned in the docs anywhere, but you need to run the following before your ingress works\r\n`kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.41.2/deploy/static/provider/cloud/deploy.yaml`\r\n\r\nAfter I ran this command, my endpoints were available on 127.0.0.1\r\n\r\nIf you are using hosts, don't forget to put them in your `/etc/hosts` ",
      "labels": [
        "kind/bug",
        "help wanted",
        "priority/backlog",
        "lifecycle/frozen",
        "area/tunnel"
      ],
      "created_at": "2021-01-02T21:07:47Z",
      "closed_at": "2025-12-14T22:38:07Z",
      "url": "https://github.com/kubernetes/minikube/issues/10085",
      "comments_count": 26
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21079,
      "title": "minikube logs --file=logs.txt",
      "problem": "### What Happened?\n\nCreating docker container (CPUs=2, Memory=2200MB) ...\n\ud83e\udd26  StartHost failed, but will try again: creating host: create: creating: setting up container node: preparing volume for minikube container: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -d /var/lib: exit status 125\nstdout:\n\nstderr:\ndocker: Error response from daemon: failed to create endpoint minikube-preload-sidecar on network bridge: failed to add the host (vethd193d56) <=> sandbox (vethe43f3d5) pair interfaces: operation not supported.\n\n\ud83e\udd37  docker \"minikube\" container is missing, will recreate.\n\ud83d\udd25  Creating docker container (CPUs=2, Memory=2200MB) ...\n\ud83d\ude3f  Failed to start docker container. Running \"minikube delete\" may fix it: recreate: creating host: create: creating: setting up container node: preparing volume for minikube container: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -d /var/lib: exit status 125\nstdout:\n\nstderr:\ndocker: Error response from daemon: failed to create endpoint minikube-preload-sidecar on network bridge: failed to add the host (veth08244f6) <=> sandbox (veth9e95aae) pair interfaces: operation not supported.\n\n\n\u274c  Exiting due to GUEST_PROVISION: error provisioning guest: Failed to start host: recreate: creating host: create: creating: setting up container node: preparing volume for minikube container: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -d /var/lib: exit status 125\nstdout:\n\nstderr:\ndocker: Error response from daemon: failed to create endpoint minikube-preload-sidecar on network bridge: failed to add the host (veth08244f6) <=> sandbox (veth9e95aae) pair interfaces: operation not supported.\n\n\n\n### Attach the log file\n\nminikube is not starting\n\n\n### Operating System\n\nNone\n\n### Driver\n\nNone",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-07-17T08:40:07Z",
      "closed_at": "2025-12-14T09:59:13Z",
      "url": "https://github.com/kubernetes/minikube/issues/21079",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21078,
      "title": "Minikube image issue",
      "problem": "### What Happened?\n\n\u274c  Exiting due to GUEST_STATUS: Unable to get control-plane node minikube host status: state: unknown state \"minikube\": docker container inspect minikube --format={{.State.Status}}: exit status 1\nstdout:\n\n\nstderr:\nError response from daemon: No such container: minikube\n\n### Attach the log file\n\n\u274c  Exiting due to GUEST_STATUS: Unable to get control-plane node minikube host status: state: unknown state \"minikube\": docker container inspect minikube --format={{.State.Status}}: exit status 1\nstdout:\n\n\nstderr:\nError response from daemon: No such container: minikube\n\n### Operating System\n\nmacOS (Default)\n\n### Driver\n\nDocker",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-07-17T07:55:38Z",
      "closed_at": "2025-12-14T08:59:13Z",
      "url": "https://github.com/kubernetes/minikube/issues/21078",
      "comments_count": 5
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21074,
      "title": "Kubernetes",
      "problem": "Existing due to GUEST_PROVISION: error provisioning guest:  Failed to start host\n\n\n\n<!-- Utiliza esta plantilla para informar de incidencias y proporciona tanta informaci\u00f3n como sea posible. Si no lo haces, es posible que la respuesta se retrase. Muchas gracias. -->\n\n**Los comandos necesarios para reproducir la incidencia**:\n\n**El resultado completo del comando que ha fallado**: <details>\n\n\n\n</details>\n\n**El resultado del comando `minikube logs`**: <details>\n\n\n\n</details>\n\n**La versi\u00f3n del sistema operativo que utilizaste**:\n",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten",
        "l/es"
      ],
      "created_at": "2025-07-16T02:19:18Z",
      "closed_at": "2025-12-13T03:44:28Z",
      "url": "https://github.com/kubernetes/minikube/issues/21074",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 14932,
      "title": "Ingress not working on fedora/podman/containerd",
      "problem": "### What Happened?\r\n\r\nTo boil down another issue I literally copy'n'pasted the [tutorial](https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/). Everything looks fine, but I do not get a response from the ingress, it's just timing out.\r\n\r\n### 0. reset everything\r\n`$ minikube delete`\r\n```\r\n\ud83d\udd25  Deleting \"minikube\" in podman ...\r\n\ud83d\udd25  Deleting container \"minikube\" ...\r\n\ud83d\udd25  Removing /home/marcwittke/.minikube/machines/minikube ...\r\n\ud83d\udc80  Removed all traces of the \"minikube\" cluster.\r\n```\r\n\r\n### 1. create cluster\r\n`$ minikube start --driver=podman --container-runtime=containerd`\r\n```\r\n\ud83d\ude04  minikube v1.26.1 on Fedora 36\r\n    \u25aa MINIKUBE_ROOTLESS=true\r\n\u2728  Using the podman driver based on user configuration\r\n\ud83d\udccc  Using rootless Podman driver\r\n\ud83d\udc4d  Starting control plane node minikube in cluster minikube\r\n\ud83d\ude9c  Pulling base image ...\r\nE0909 13:10:32.349701   99470 cache.go:203] Error downloading kic artifacts:  not yet implemented, see issue #8426\r\n\ud83d\udd25  Creating podman container (CPUs=2, Memory=7900MB) ...\r\n\ud83d\udce6  Preparing Kubernetes v1.24.3 on containerd 1.6.6 ...\r\n    \u25aa Generating certificates and keys ...\r\n    \u25aa Booting up control plane ...\r\n    \u25aa Configuring RBAC rules ...\r\n\ud83d\udd17  Configuring CNI (Container Networking Interface) ...\r\n\ud83d\udd0e  Verifying Kubernetes components...\r\n    \u25aa Using image gcr.io/k8s-minikube/storage-provisioner:v5\r\n\ud83c\udf1f  Enabled addons: storage-provisioner, default-storageclass\r\n\ud83c\udfc4  Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\r\n```\r\n\r\n### 2. enable ingress plugin\r\n`$ minikube addons enable ingress`\r\n```\r\n\ud83d\udca1  ingress is an addon maintained by Kubernetes. For any concerns contact minikube on GitHub.\r\nYou can view the list of minikube maintainers at: https://github.com/kubernetes/minikube/blob/master/OWNERS\r\n    \u25aa Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1\r\n    \u25aa Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1\r\n    \u25aa Using image k8s.gcr.io/ingress-nginx/controller:v1.2.1\r\n\ud83d\udd0e  Verifying ingress addon...\r\n\ud83c\udf1f  The 'ingress' addon is enabled\r\n```\r\nverifying...\r\n`$ kubectl get pods -n ingress-nginx`\r\n```\r\nNAME                                        READY   STATUS      RESTARTS   AGE\r\ningress-nginx-admission-create-s5hqn        0/1     Completed   0          59s\r\ningress-nginx-admission-patch-s22w4         0/1     Completed   2          59s\r\ningress-nginx-controller-755dfbfc65-pjz4z   1/1     Running     0          59s\r\n```\r\n\r\n### 3. create a sample service and expose it\r\n`$ kubectl create deployment web --image=gcr.io/google-samples/hello-app:1.0`\r\n```\r\ndeployment.apps/web created\r\n```\r\n\r\n`$ kubectl expose deployment web --type=NodePort --port=8080`\r\n```\r\nservice/web exposed\r\n```\r\n\r\n`$ kubectl get service web`\r\n\r\n```\r\nNAME   TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\r\nweb    NodePort   10.104.219.120   <none>        8080:31280/TCP   9s\r\n```\r\n\r\n`$ minikube service web --url`\r\n\r\n```\r\nhttp://127.0.0.1:39717\r\n\u2757  Because you are using a Docker driver on linux, the terminal needs to be open to run it.\r\n```\r\n\r\n`$ curl http://127.0.0.1:39717`\r\n```\r\nHello, world!\r\nVersion: 1.0.0\r\nHostname: web-6bf786c76b-j8s9q\r\n```\r\n\r\n### 4. fire up the ingress\r\n`$ kubectl apply -f https://k8s.io/examples/service/networking/example-ingress.yaml`\r\n```\r\ningress.networking.k8s.io/example-ingress created\r\n```\r\n\r\n`$ kubectl get ingress`\r\n\r\n```\r\nNAME              CLASS   HOSTS              ADDRESS   PORTS   AGE\r\nexample-ingress   nginx   hello-world.info   192.168.49.2   80      36s\r\n```\r\n\r\n### 5. fix /etc/hosts\r\n`$ cat /etc/hosts`\r\n```\r\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4 mw-ryzen5900x \r\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6 mw-ryzen5900x \r\n192.168.49.2 hello-world.info\r\n```\r\n\r\n### 6. try it\r\n`$ curl hello-world.info` \r\n\r\n```\r\ncurl: (28) Failed to connect to hello-world.info port 80 after 132325 ms: Connection timed out\r\n```\r\n\r\n### 7. be sad :disappointed: \r\ntried half a day now to find out whats happening. On my notebook (Upgraded a few times from Fedora 32 and also had dockerd some time) it works. The issue is happening on a rather clean machine I am using since two months or so.\r\n\r\nTried without luck:\r\n- Disabling the firewall completely\r\n- Disarming SELinux with `setenforce 0`\r\n- `sysctl net.ipv4.ip_unprivileged_port_start=80`\r\n\r\nTried also, with success:\r\n- doing everything in a root shell (had to provide `--force` to convince podman to run as root)\r\nso I am suspecting some kind of rootless issue\r\n\r\n\r\n### Attach the log file\r\n\r\n[log.txt](https://github.com/kubernetes/minikube/files/9537229/log.txt)\r\n\r\n\r\n### Operating System\r\n\r\nRedhat/Fedora\r\n\r\n### Driver\r\n\r\nPodman",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nI have the same issue!\r\nUpvoting the issue\r\nHow to replicate\r\n```\r\nminikube start --driver=podman\r\nminikube addons enable ingress\r\nminikube ip\r\n```\r\nconnect your browser to the ip shown by minikube, browser is stuck while connecting\r\n\r\nIf I do the same things with docker\r\n```\r\nminikube start --driver=docker\r\nminikube addons enable ingress\r\nminikube ip\r\n```\r\ni can connect to ingress and get 404 from it.\r\nEverything works then :c\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/bug",
        "os/linux",
        "priority/awaiting-more-evidence",
        "lifecycle/rotten",
        "addon/ingress",
        "co/podman-driver"
      ],
      "created_at": "2022-09-09T16:43:20Z",
      "closed_at": "2024-03-26T10:10:41Z",
      "url": "https://github.com/kubernetes/minikube/issues/14932",
      "comments_count": 14
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 22067,
      "title": "tests: Too long timeout - test fails after 6+ minutes",
      "problem": "I'm not sure what is the purpose of `TestFunctionalNewestKubernetes/Versionv1.35.0-beta.0/parallel/PersistentVolumeClaim` but there is no reason why the test should fail after 6 minutes.\n\nChecking the logs shows that it failed because the known rate-limit issue, but it is not clear why we need to pull nginx image for testing a PVC.\n\nWe need to minimize the test timeout so it fails quickly.\n\n```\n\tEvents:\n\t  Type     Reason     Age                  From               Message\n\t  ----     ------     ----                 ----               -------\n\t  Normal   Scheduled  6m3s                 default-scheduler  Successfully assigned default/sp-pod to functional-940895\n\t  Warning  Failed     5m16s                kubelet            Failed to pull image \"docker.io/nginx\": copying system image from manifest list: determining manifest MIME type for docker://nginx:latest: reading manifest sha256:5c733364e9a8f7e6d7289ceaad623c6600479fe95c3ab5534f07bfd7416d9541 in docker.io/library/nginx: toomanyrequests: You have reached your unauthenticated pull rate limit. https://www.docker.com/increase-rate-limit\n\t  Normal   Pulling    47s (x5 over 6m)     kubelet            Pulling image \"docker.io/nginx\"\n\t  Warning  Failed     15s (x5 over 5m16s)  kubelet            Error: ErrImagePull\n\t  Warning  Failed     15s (x4 over 4m28s)  kubelet            Failed to pull image \"docker.io/nginx\": reading manifest latest in docker.io/library/nginx: toomanyrequests: You have reached your unauthenticated pull rate limit. https://www.docker.com/increase-rate-limit\n\t  Normal   BackOff    1s (x11 over 5m16s)  kubelet            Back-off pulling image \"docker.io/nginx\"\n\t  Warning  Failed     1s (x11 over 5m16s)  kubelet            Error: ImagePullBackOff\n-- /stdout --\nhelpers_test.go:293: <<< TestFunctionalNewestKubernetes/Versionv1.35.0-beta.0/parallel/PersistentVolumeClaim FAILED: end of post-mortem logs <<<\nhelpers_test.go:294: ---------------------/post-mortem---------------------------------\n--- FAIL: TestFunctionalNewestKubernetes/Versionv1.35.0-beta.0/parallel/PersistentVolumeClaim (369.63s)\n```\n\nFull test log:\nhttps://storage.googleapis.com/minikube-builds/logs/21409/42683/KVM_Linux_crio.html",
      "solution": "@medyagh #22075 fixed this by replacing the image, right?\n\n#22081 does not fix the timeout - it handles only the rate limit issue. Every test needs a timeout to cover all possible failures.",
      "labels": [],
      "created_at": "2025-12-08T10:18:50Z",
      "closed_at": "2025-12-09T22:19:37Z",
      "url": "https://github.com/kubernetes/minikube/issues/22067",
      "comments_count": 2
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21025,
      "title": "Tell macOS users to start minikube with podman driver only (no CRI-O)",
      "problem": "### What Happened?\n\nThe instructions for using podman with minikube at https://minikube.sigs.k8s.io/docs/drivers/podman/ state the following:\n\n> It\u2019s recommended to run minikube with the podman driver and [CRI-O container runtime](https://cri-o.io/) (except when using Rootless Podman):\n> \n> minikube start --driver=podman --container-runtime=cri-o\n> \n> Alternatively, start minikube with the podman driver only:\n> \n> minikube start --driver=podman\n\nHowever, CRI-O does not appear to be available for macOS. When I tried `minikube start --driver=podman --container-runtime=cri-o`, I kept getting `ImageInspectError`, but running just `minikube start --driver=podman` worked.\n\nI suspect the above section should say something like:\n\n> If you do not have CRI-O installed or if you are running macOS, start minikube with the podman driver only:\n>\n> minikube start --driver=podman\n\n### Attach the log file\n\nOutput of `kubectl describe pod foo`:\n\n```\nEvents:\n  Type     Reason         Age                 From               Message\n  ----     ------         ----                ----               -------\n  Normal   Scheduled      15m                 default-scheduler  Successfully assigned default/my-release-cockroachdb-0 to minikube\n  Warning  Failed         13m (x12 over 15m)  kubelet            Error: ImageInspectError\n  Warning  InspectFailed  25s (x71 over 15m)  kubelet            Failed to inspect image \"\": rpc error: code = Unknown desc = short-name \"foo:v25.2.2\" did not resolve to an alias and no unqualified-search registries are defined in \"/etc/containers/registries.conf\"\n```\n\n### Operating System\n\nmacOS (Default)\n\n### Driver\n\nPodman",
      "solution": "It's a bug in the crio configuration, it is supposed to work:\n\n* https://github.com/kubernetes/minikube/issues/19396\n\nThe workaround is to use fully qualified names, i.e. docker.io\n\n`docker.io/library/foo:v25.2.2`\n\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten",
        "co/runtime/crio"
      ],
      "created_at": "2025-07-03T15:01:39Z",
      "closed_at": "2025-12-09T07:52:37Z",
      "url": "https://github.com/kubernetes/minikube/issues/21025",
      "comments_count": 8
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 20497,
      "title": "Bump to Containerd 2",
      "problem": "https://github.com/kubernetes/minikube/pull/20485\n\ncurrently the ISO Fails to build\n\n<!-- Failed to upload \"#915.txt\" -->\n\n\nwe might need to bump some Version of dependecies in the ISO and Kic Base\nin that case we need to make sure it is PreBaked for the Default Kubernetes version and other kubernetes versions can Download on the Fly or Get a user warnining",
      "solution": "As I have described in the previous triage meeting, the problem is this:\n\n- we never automatically update the version of buildroot in makefile, and it is locked on version 2023.02.9 , which is too old\n- at the meantime, this version build both go compiler from source. It use a 2-step-leap to build go(first build go 1.4 as a bootstrap, and then use go 1.4 to build go 1.19, and eventually use go 1.19 build the go compiler you want)\n- However, in containerd 2.0 it specifies that it requires golang 1.23. golang 1.23 requires at least golang>=1.21 as a bootstrap. using golang 1.19 to build it will cause a failure\n\nThis is the reason why previous CI fails.\n\nThe best solution seems to be: update buildroot to the newest version, but it will cause some other problems:\n\n - It seems that new version of buildroot has given up the 2-step-leap build for go bootstrap. Now in go.hash it requires the binary tarball instead of the sourcecode tarball. I added the hash of golang binary tarball file into go.hash\n - It seems that conmon is now a part of buildroot, and redeclare it in minikube's .mk file will cause an error. i removed it from our iso package and use the built-in conmon instead\n - falco cannot build. I haven't figured out why yet.\n \nFor now I am trying if we can build an iso without falco. \n\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-03-05T19:35:00Z",
      "closed_at": "2025-09-06T19:35:58Z",
      "url": "https://github.com/kubernetes/minikube/issues/20497",
      "comments_count": 9
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21055,
      "title": "Issue starting minikube on Mac M2",
      "problem": "### What Happened?\n\nError while starting minikube on a mcbook M2 with a podman driver\n\n### Attach the log file\n\n[logs.txt](https://github.com/user-attachments/files/21183868/logs.txt)\n\n### Operating System\n\nmacOS (Default)\n\n### Driver\n\nPodman",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-07-11T13:21:07Z",
      "closed_at": "2025-12-08T16:48:37Z",
      "url": "https://github.com/kubernetes/minikube/issues/21055",
      "comments_count": 5
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 20518,
      "title": "Error message in K8S_APISERVER_MISSING",
      "problem": "### What Happened?\n\nHi, after update of the Docker Desktop for MacOS (Apple Silicon) I came across following issue (img at the bottom). \nEven after multiple restarts (PC, Docker and minikube) error still occur. \n\nIn the suggestions at the end of the error message I found information about probable problems with SELinux but that was not a issue. \nI tried many things and only one solved my issue, I had to delete cluster `minikube delete`. \n\nMaybe it would be a good idea to link this issue and add this solution to list at the bottom of the error message.  \n\n### Attach the log file\n\nI already solved this problem so every thing that I got is this error message from the terminal \n\n<img width=\"1440\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/35b72d96-cba0-40ca-9307-f2c30a2cdafd\" />\n\n### Operating System\n\nmacOS (Default)\n\n### Driver\n\nDocker",
      "solution": "Hi @Ret2Me, minikube error logs mainly provide suggestions about what went wrong so that we can do troubleshooting (based on that) to resolve the error. Including the suggested solution in the error log might be a good way.\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-03-10T20:49:45Z",
      "closed_at": "2025-12-07T19:41:35Z",
      "url": "https://github.com/kubernetes/minikube/issues/20518",
      "comments_count": 11
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 13872,
      "title": "Exiting due to MK_ADDON_ENABLE: run callbacks: running callbacks: [waiting for app.kubernetes.io/name=ingress-nginx pods: timed out waiting for the condition]",
      "problem": "### What Happened?\r\n\r\nHello!\r\nI'm tried to enable ingress addon, but takes error such as from title\r\npartial reference for issue #13841\r\n_Originally posted by @spowelljr in https://github.com/kubernetes/minikube/issues/13841#issuecomment-1081007149_\r\n\r\n```\r\nroot@orion:~# minikube addons enable ingress --alsologtostderr\r\nI0328 21:23:12.286390 1954403 out.go:297] Setting OutFile to fd 1 ...\r\nI0328 21:23:12.286699 1954403 out.go:349] isatty.IsTerminal(1) = true\r\nI0328 21:23:12.286760 1954403 out.go:310] Setting ErrFile to fd 2...\r\nI0328 21:23:12.286819 1954403 out.go:349] isatty.IsTerminal(2) = true\r\nI0328 21:23:12.287050 1954403 root.go:315] Updating PATH: /root/.minikube/bin\r\nW0328 21:23:12.287255 1954403 root.go:293] Error reading config file at /root/.minikube/config/config.json: open /root/.minikube/config/config.json: no such file or directory\r\nI0328 21:23:12.288046 1954403 config.go:176] Loaded profile config \"minikube\": Driver=none, ContainerRuntime=docker, KubernetesVersion=v1.23.4-rc.0\r\nI0328 21:23:12.288111 1954403 addons.go:65] Setting ingress=true in profile \"minikube\"\r\nI0328 21:23:12.288159 1954403 addons.go:153] Setting addon ingress=true in \"minikube\"\r\nI0328 21:23:12.288490 1954403 host.go:66] Checking if \"minikube\" exists ...\r\nI0328 21:23:12.289084 1954403 exec_runner.go:51] Run: systemctl --version\r\nI0328 21:23:12.294233 1954403 kubeconfig.go:92] found \"minikube\" server: \"https://192.168.188.160:8443\"\r\nI0328 21:23:12.294262 1954403 api_server.go:165] Checking apiserver status ...\r\nI0328 21:23:12.294304 1954403 exec_runner.go:51] Run: sudo pgrep -xnf kube-apiserver.*minikube.*\r\nI0328 21:23:12.305745 1954403 exec_runner.go:51] Run: sudo egrep ^[0-9]+:freezer: /proc/1951927/cgroup\r\nW0328 21:23:12.314202 1954403 api_server.go:176] unable to find freezer cgroup: sudo egrep ^[0-9]+:freezer: /proc/1951927/cgroup: exit status 1\r\nstdout:\r\n\r\nstderr:\r\nI0328 21:23:12.314232 1954403 api_server.go:240] Checking apiserver healthz at https://192.168.188.160:8443/healthz ...\r\nI0328 21:23:12.319308 1954403 api_server.go:266] https://192.168.188.160:8443/healthz returned 200:\r\nok\r\nI0328 21:23:12.334410 1954403 out.go:176]     \u25aa Using image k8s.gcr.io/ingress-nginx/controller:v1.1.1\r\n    \u25aa Using image k8s.gcr.io/ingress-nginx/controller:v1.1.1\r\nI0328 21:23:12.344282 1954403 out.go:176]     \u25aa Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1\r\n    \u25aa Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1\r\nI0328 21:23:12.354291 1954403 out.go:176]     \u25aa Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1\r\n    \u25aa Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1\r\nI0328 21:23:12.354596 1954403 addons.go:348] installing /etc/kubernetes/addons/ingress-deploy.yaml\r\nI0328 21:23:12.354654 1954403 exec_runner.go:144] found /etc/kubernetes/addons/ingress-deploy.yaml, removing ...\r\nI0328 21:23:12.354670 1954403 exec_runner.go:207] rm: /etc/kubernetes/addons/ingress-deploy.yaml\r\nI0328 21:23:12.354731 1954403 exec_runner.go:151] cp: memory --> /etc/kubernetes/addons/ingress-deploy.yaml (15567 bytes)\r\nI0328 21:23:12.354865 1954403 exec_runner.go:51] Run: sudo cp -a /tmp/minikube3233579437 /etc/kubernetes/addons/ingress-deploy.yaml\r\nI0328 21:23:12.363407 1954403 exec_runner.go:51] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.23.4-rc.0/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml\r\nI0328 21:23:12.936710 1954403 addons.go:386] Verifying addon ingress=true in \"minikube\"\r\nI0328 21:23:12.999722 1954403 out.go:176] \r\n\ud83d\udd0e  Verifying ingress addon...\r\nI0328 21:23:13.000806 1954403 kapi.go:75] Waiting for pod with label \"app.kubernetes.io/name=ingress-nginx\" in ns \"ingress-nginx\" ...\r\nI0328 21:23:13.044867 1954403 kapi.go:86] Found 3 Pods for label selector app.kubernetes.io/name=ingress-nginx\r\nI0328 21:23:13.044897 1954403 kapi.go:96] waiting for pod \"app.kubernetes.io/name=ingress-nginx\", current state: Pending: [<nil>]\r\nI0328 21:23:13.548198 1954403 kapi.go:96] waiting for pod \"app.kubernetes.io/name=ingress-nginx\", current state: Pending: [<nil>]\r\nI0328 21:23:14.048591 1954403 kapi.go:96] waiting for pod \"app.kubernetes.io/name=ingress-nginx\", current state: Pending: [<nil>]\r\nI0328 21:23:14.547859 1954403 kapi.go:96] waiting for pod \"app.kubernetes.io/name=ingress-nginx\", current state: Pending: [<nil>]\r\n... cut ...\r\nI0328 21:29:13.050823 1954403 kapi.go:96] waiting for pod \"app.kubernetes.io/name=ingress-nginx\", current state: Pending: [<nil>]\r\nI0328 21:29:13.050850 1954403 kapi.go:108] duration metric: took 6m0.050047714s to wait for app.kubernetes.io/name=ingress-nginx ...\r\nI0328 21:29:13.063110 1954403 out.go:176]\r\n\r\nW0328 21:29:13.063269 1954403 out.go:241] \u274c  Exiting due to MK_ADDON_ENABLE: run callbacks: running callbacks: [waiting for app.kubernetes.io/name=ingress-nginx pods: timed out waiting for the condition]\r\n\u274c  Exiting due to MK_ADDON_ENABLE: run callbacks: running callbacks: [waiting for app.kubernetes.io/name=ingress-nginx pods: timed out waiting for the condition]\r\nW0328 21:29:13.063341 1954403 out.go:241]\r\n\r\nW0328 21:29:13.072474 1954403 out.go:241] \r\n````\r\n\r\n### Attach the log file\r\n\r\n[logs.txt](https://github.com/kubernetes/minikube/files/8366237/logs.txt)\r\n\r\n\r\n### Operating System\r\n\r\nOther (Debian 11.3)\r\n\r\n### Driver\r\n\r\nnone",
      "solution": "> do you solve this problem , I got the same issue. oops....\r\n\r\n@16871560 no, the problem still persisted\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\n@Orygeunik I also encountered this problem, and then check the pod error log, and prompt \"0 / 1 nodes are available: 1 node (s) didn't match pod's node affinity / selector Preemption: 0 / 1 nodes are available: 1 preemption is not helpful for scheduling \",  i find pod's  label \"minikube.k8s.io/primary=true\" does not exist in the minikube node. later, the problem is solved by running the command:\"minikube label node minikube minikube.k8s.io/primary=true\"",
      "labels": [
        "kind/support",
        "lifecycle/rotten",
        "addon/ingress"
      ],
      "created_at": "2022-03-28T21:00:15Z",
      "closed_at": "2022-12-23T05:28:23Z",
      "url": "https://github.com/kubernetes/minikube/issues/13872",
      "comments_count": 30
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 22041,
      "title": "Preload jobs failing \"docker storage driver overlayfs does not match requested overlay2 called from:\"",
      "problem": "since nov 20, preload jobs been failing\n\n```\ndocker storage driver overlayfs does not match requested overlay2 called from:\n```\n\nif true that means docker has changed its storage driver to overlayfs, and would need minikube to change as well\n\n\nI created a minikube from HEAD and it shows overlay2\n\n```\ndocker@minikube:~$ docker info -f {{.Info.Driver}}\noverlay2\n```\n\n\nhere is a full log\n\n```\n10:39:47 I1120 18:39:47.265659    2943 oci.go:103] Successfully created a docker volume generate-preloaded-images-tar\n10:39:47 I1120 18:39:47.265745    2943 cli_runner.go:164] Run: docker run --rm --name generate-preloaded-images-tar-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=generate-preloaded-images-tar --entrypoint /usr/bin/test -v generate-preloaded-images-tar:/var gcr.io/k8s-minikube/kicbase-builds:v0.0.48-1763507788-21924@sha256:1e20c07242571f3eb6bbb213b88269c923b5578034662e07409047e7102bdd1a -d /var/lib\n10:40:20 I1120 18:40:20.467308    2943 cli_runner.go:217] Completed: docker run --rm --name generate-preloaded-images-tar-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=generate-preloaded-images-tar --entrypoint /usr/bin/test -v generate-preloaded-images-tar:/var gcr.io/k8s-minikube/kicbase-builds:v0.0.48-1763507788-21924@sha256:1e20c07242571f3eb6bbb213b88269c923b5578034662e07409047e7102bdd1a -d /var/lib: (33.201507631s)\n10:40:20 I1120 18:40:20.467361    2943 oci.go:107] Successfully prepared a docker volume generate-preloaded-images-tar\n10:40:20 I1120 18:40:20.467419    2943 preload.go:188] Checking if preload exists for k8s version v1.35.0-beta.0 and runtime docker\n10:40:20 W1120 18:40:20.467529    2943 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.\n10:40:20 W1120 18:40:20.467581    2943 oci.go:252] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.\n10:40:20 I1120 18:40:20.467636    2943 cli_runner.go:164] Run: docker info --format \"'{{json .SecurityOptions}}'\"\n10:40:23 I1120 18:40:23.401306    2943 cli_runner.go:217] Completed: docker info --format \"'{{json .SecurityOptions}}'\": (2.933630266s)\n10:40:23 I1120 18:40:23.401431    2943 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname generate-preloaded-images-tar --name generate-preloaded-images-tar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=generate-preloaded-images-tar --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=generate-preloaded-images-tar --network generate-preloaded-images-tar --ip 192.168.49.2 --volume generate-preloaded-images-tar:/var --security-opt apparmor=unconfined --memory=4000mb -e container=docker --expose 8080 --publish=127.0.0.1::8080 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase-builds:v0.0.48-1763507788-21924@sha256:1e20c07242571f3eb6bbb213b88269c923b5578034662e07409047e7102bdd1a\n10:40:24 I1120 18:40:24.267962    2943 cli_runner.go:164] Run: docker container inspect generate-preloaded-images-tar --format={{.State.Running}}\n10:40:24 I1120 18:40:24.292769    2943 cli_runner.go:164] Run: docker container inspect generate-preloaded-images-tar --format={{.State.Status}}\n10:40:24 I1120 18:40:24.319481    2943 cli_runner.go:164] Run: docker exec generate-preloaded-images-tar stat /var/lib/dpkg/alternatives/iptables\n10:40:24 I1120 18:40:24.416144    2943 oci.go:144] the created container \"generate-preloaded-images-tar\" has a running status.\n10:40:24 I1120 18:40:24.416177    2943 kic.go:225] Creating ssh key for kic: /home/jenkins/.minikube/machines/generate-preloaded-images-tar/id_rsa...\n10:40:24 I1120 18:40:24.497133    2943 kic_runner.go:191] docker (temp): /home/jenkins/.minikube/machines/generate-preloaded-images-tar/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)\n10:40:24 I1120 18:40:24.522417    2943 cli_runner.go:164] Run: docker container inspect generate-preloaded-images-tar --format={{.State.Status}}\n10:40:24 I1120 18:40:24.543758    2943 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys\n10:40:24 I1120 18:40:24.543783    2943 kic_runner.go:114] Args: [docker exec --privileged generate-preloaded-images-tar chown docker:docker /home/docker/.ssh/authorized_keys]\n10:40:24 I1120 18:40:24.734736    2943 retry.go:31] will retry after 124.631\u00b5s: [docker exec generate-preloaded-images-tar docker info -f {{.Info.Driver}}]: exit status 1:\n10:40:24 failed to connect to the docker API at unix:///var/run/docker.sock; check if the path is correct and if the daemon is running: dial unix /var/run/docker.sock: connect: no such file or directory\n10:40:24 I1120 18:40:24.842598    2943 retry.go:31] will retry after 181.678\u00b5s: [docker exec generate-preloaded-images-tar docker info -f {{.Info.Driver}}]: exit status 1:\n10:40:24 failed to connect to the docker API at unix:///var/run/docker.sock; check if the path is correct and if the daemon is running: dial unix /var/run/docker.sock: connect: no such file or directory\n10:40:24 I1120 18:40:24.971889    2943 retry.go:31] will retry after 147.385\u00b5s: [docker exec generate-preloaded-images-tar docker info -f {{.Info.Driver}}]: exit status 1:\n10:40:24 failed to connect to the docker API at unix:///var/run/docker.sock; check if the path is correct and if the daemon is running: dial unix /var/run/docker.sock: connect: no such file or directory\n10:40:25 I1120 18:40:25.074567    2943 retry.go:31] will retry after 292.239\u00b5s: [docker exec generate-preloaded-images-tar docker info -f {{.Info.Driver}}]: exit status 1:\n10:40:25 failed to connect to the docker API at unix:///var/run/docker.sock; check if the path is correct and if the daemon is running: dial unix /var/run/docker.sock: connect: no such file or directory\n10:40:25 I1120 18:40:25.178677    2943 retry.go:31] will retry after 560.547\u00b5s: [docker exec generate-preloaded-images-tar docker info -f {{.Info.Driver}}]: exit status 1:\n10:40:25 failed to connect to the docker API at unix:///var/run/docker.sock; check if the path is correct and if the daemon is running: dial unix /var/run/docker.sock: connect: no such file or directory\n10:40:25 I1120 18:40:25.288394    2943 retry.go:31] will retry after 648.64\u00b5s: [docker exec generate-preloaded-images-tar docker info -f {{.Info.Driver}}]: exit status 1:\n10:40:25 failed to connect to the docker API at unix:///var/run/docker.sock; check if the path is correct and if the daemon is running: dial unix /var/run/docker.sock: connect: no such file or directory\n10:40:25 I1120 18:40:25.387113    2943 retry.go:31] will retry after 1.520773ms: [docker exec generate-preloaded-images-tar docker info -f {{.Info.Driver}}]: exit status 1:\n10:40:25 failed to connect to the docker API at unix:///var/run/docker.sock; check if the path is correct and if the daemon is running: dial unix /var/run/docker.sock: connect: no such file or directory\n10:40:25 I1120 18:40:25.493632    2943 retry.go:31] will retry after 2.189262ms: [docker exec generate-preloaded-images-tar docker info -f {{.Info.Driver}}]: exit status 1:\n10:40:25 failed to connect to the docker API at unix:///var/run/docker.sock; check if the path is correct and if the daemon is running: dial unix /var/run/docker.sock: connect: no such file or directory\n10:40:25 I1120 18:40:25.595367    2943 retry.go:31] will retry after 1.529226ms: [docker exec generate-preloaded-images-tar docker info -f {{.Info.Driver}}]: exit status 1:\n10:40:25 failed to connect to the docker API at unix:///var/run/docker.sock; check if the path is correct and if the daemon is running: dial unix /var/run/docker.sock: connect: no such file or directory\n10:40:26 I1120 18:40:26.262287    2943 retry.go:31] will retry after 5.082739ms: docker storage driver overlayfs does not match requested overlay2\n10:40:26 I1120 18:40:26.376172    2943 retry.go:31] will retry after 5.649221ms: docker storage driver overlayfs does not match requested overlay2\n10:40:26 I1120 18:40:26.492315    2943 retry.go:31] will retry after 9.136636ms: docker storage driver overlayfs does not match requested overlay2\n10:40:26 I1120 18:40:26.611145    2943 retry.go:31] will retry after 9.73171ms: docker storage driver overlayfs does not match requested overlay2\n10:40:26 I1120 18:40:26.728757    2943 retry.go:31] will retry after 24.197422ms: docker storage driver overlayfs does not match requested overlay2\n10:40:26 I1120 18:40:26.861672    2943 retry.go:31] will retry after 38.515059ms: docker storage driver overlayfs does not match requested overlay2\n10:40:27 I1120 18:40:27.007299    2943 retry.go:31] will retry after 58.577096ms: docker storage driver overlayfs does not match requested overlay2\n10:40:27 I1120 18:40:27.174019    2943 retry.go:31] will retry after 97.236679ms: docker storage driver overlayfs does not match requested overlay2\n10:40:27 I1120 18:40:27.378249    2943 retry.go:31] will retry after 128.162567ms: docker storage driver overlayfs does not match requested overlay2\n10:40:27 I1120 18:40:27.615612    2943 retry.go:31] will retry after 131.740866ms: docker storage driver overlayfs does not match requested overlay2\n10:40:27 I1120 18:40:27.857800    2943 retry.go:31] will retry after 271.408721ms: docker storage driver overlayfs does not match requested overlay2\n10:40:28 I1120 18:40:28.237750    2943 retry.go:31] will retry after 441.251524ms: docker storage driver overlayfs does not match requested overlay2\n10:40:28 I1120 18:40:28.788468    2943 retry.go:31] will retry after 514.585157ms: docker storage driver overlayfs does not match requested overlay2\n10:40:29 I1120 18:40:29.415126    2943 retry.go:31] will retry after 416.820148ms: docker storage driver overlayfs does not match requested overlay2\n10:40:29 I1120 18:40:29.947671    2943 retry.go:31] will retry after 702.210848ms: docker storage driver overlayfs does not match requested overlay2\n10:40:30 I1120 18:40:30.762921    2943 retry.go:31] will retry after 2.023189473s: docker storage driver overlayfs does not match requested overlay2\n10:40:32 I1120 18:40:32.900220    2943 retry.go:31] will retry after 2.538733888s: docker storage driver overlayfs does not match requested overlay2\n10:40:35 I1120 18:40:35.548432    2943 retry.go:31] will retry after 4.532598021s: docker storage driver overlayfs does not match requested overlay2\n10:40:40 I1120 18:40:40.193176    2943 retry.go:31] will retry after 6.962167079s: docker storage driver overlayfs does not match requested overlay2\n10:40:47 I1120 18:40:47.274489    2943 retry.go:31] will retry after 6.655054746s: docker storage driver overlayfs does not match requested overlay2\n10:40:54 I1120 18:40:54.047789    2943 retry.go:31] will retry after 13.907408368s: docker storage driver overlayfs does not match requested overlay2\n10:41:08 I1120 18:41:08.082032    2943 retry.go:31] will retry after 14.881609238s: docker storage driver overlayfs does not match requested overlay2\n10:41:23 I1120 18:41:23.071816    2943 retry.go:31] will retry after 41.787100268s: docker storage driver overlayfs does not match requested overlay2\n10:42:05 * Removing /home/jenkins/.minikube/machines/generate-preloaded-images-tar ...\n10:42:05 * Removed all traces of the \"generate-preloaded-images-tar\" cluster.\n10:42:05 * Deleting container \"generate-preloaded-images-tar\" ...\n10:42:06 WithError(generating tarball for k8s version v1.35.0-beta.0 with docker: verifying storage: Docker storage type is incompatible: docker storage driver overlayfs does not match requested overlay2)=generating tarball for k8s version v1.35.0-beta.0 with docker: verifying storage: Docker storage type is incompatible: docker storage driver overlayfs does not match requested overlay2 called from:\n10:42:06 goroutine 1 [running]:\n10:42:06 runtime/debug.Stack()\n10:42:06 \t/usr/local/go/src/runtime/debug/stack.go:26 +0x5e\n10:42:06 main.exit({0xc0001cc180, 0xb8}, {0x3732460, 0xc001c6a0d8})\n10:42:06 \t/home/jenkins/workspace/Preload Generation/hack/preload-images/preload_images.go:226 +0x3f\n10:42:06 main.main()\n10:42:06 \t/home/jenkins/workspace/Preload Generation/hack/preload-images/preload_images.go:121 +0x32d\n10:42:06 * Removed all traces of the \"generate-preloaded-images-tar\" cluster.\n10:42:06 make: *** [Makefile:771: upload-preloaded-images-tar] Error 60\n10:42:06 Build step 'Execute shell' marked build as failure\n10:42:06 [WS-CLEANUP] Deleting project workspace...\n10:42:06 [WS-CLEANUP] Deferred wipeout is used...\n10:42:06 [WS-CLEANUP] done\n10:42:06 Finished: FAILURE\n\n```",
      "solution": "ok I I think I found out the root cause,\nin the preload geneation, we create minikube using Kic Library (not actual minikube) and we try to verify stroage type right after Kic container is created\nThe preload tool checks the storage driver too early. In hack/preload-images/generate.go (line 65) we call verifyStorage right after the kic container is created, before cr.Enable is run. cr.Enable (see pkg/minikube/cruntime/docker.go in configureDocker) is what writes /etc/docker/daemon.json and restarts dockerd with storage-driver: \"overlay2\".\nhe kicbase image was bumped around Nov 19 (see commit 4cf3e568b \u201cUpdating kicbase image to v0.0.48-1764169655-21974\u201d). That image boots dockerd with its default overlayfs until Minikube rewrites /etc/docker/daemon.json inside the node.\n",
      "labels": [],
      "created_at": "2025-12-06T00:27:13Z",
      "closed_at": "2025-12-06T01:03:17Z",
      "url": "https://github.com/kubernetes/minikube/issues/22041",
      "comments_count": 3
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21047,
      "title": "update python on jenkins agents",
      "problem": "13:57:47 New run name is '#21336 - PR #21042 @ 40370'\n13:57:47 [Docker_Linux_containerd_arm64] $ /bin/bash -xe /tmp/jenkins11663950536367727647.sh\n13:57:47 + set -e\n13:57:47 + gsutil -m cp -r gs://minikube-builds/21042/installers .\n13:57:48 Error: gsutil requires Python version 3.9-3.13, but a different version is installed.\n13:57:48 You are currently running Python 3.8\n13:57:48 Follow the steps below to resolve this issue:\n13:57:48 \t1. Switch to Python 3.9-3.13 using your Python version manager or install an appropriate version.\n13:57:48 \t2. If you are unsure how to manage Python versions, visit [https://cloud.google.com/storage/docs/gsutil_install#specifications] for detailed instructions.\n13:57:48 WARNING:  Python 3.8 will be deprecated on July 15th, 2025. Please use Python version 3.9 and up.\n13:57:48 \n13:57:48 If you have a compatible Python interpreter installed, you can use it by setting\n13:57:48 the CLOUDSDK_PYTHON environment variable to point to it.\n13:57:48 \n13:57:48 Build step 'Execute shell' marked build as failure",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-07-08T17:06:43Z",
      "closed_at": "2025-12-05T19:15:35Z",
      "url": "https://github.com/kubernetes/minikube/issues/21047",
      "comments_count": 6
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 17880,
      "title": "Access Minikube from outside server",
      "problem": "### What Happened?\n\nI have a remote physical server with public IP.\r\n\r\nI installed minikube and started the server with docker\r\n\r\n```\r\nminikube start --memory=16384 --cpus=8 --nodes=6 --driver=docker  --force-systemd=true --extra-config kubeadm.ignore-preflight-errors=SystemVerification\r\n```\r\n\r\nThen installed istio ingress and load balancer and mysql using helm.\r\n\r\nI am able to `kubectl exec -it mysql-primary-0` -- /bin/sh \r\n\r\nI ran minikube tunnel and it shows external IP as `127.0.0.1` \r\n\r\nThen i did IP routing using iptables to route all 3306 traffic on public IP to `127.0.0.1:3306`, \r\n\r\n```\r\nsudo iptables -t nat -A PREROUTING -p tcp -d x.x.x.x  --dport 3306 -j DNAT --to-destination 127.0.0.1:3306\r\nsudo iptables -A FORWARD -p tcp -d 127.0.0.1 --dport 3306 -m state --state NEW,ESTABLISHED,RELATED -j ACCEPT\r\nsudo iptables -t nat -A POSTROUTING -d 127.0.0.1 -p tcp --dport 3306 -j SNAT --to-source x.x.x.x\r\nsudo iptables -t nat -I OUTPUT -p tcp -d x.x.x.x --dport 3306 -j DNAT --to-destination 127.0.0.1:3306\r\n```\r\n\r\nthis same setup works that i did for port 80 and 443 and it worked but is not working for mysql. \r\n\r\nI have tried to updated istio gateway to accept 3306 traffic and enabled tcp services too. This exact setup was working in previous minikube version few months back and started failing, don't have old version details but current I am on latest version. The only new thing in new setup is IP is 127.0.0.1 while in old used to get LB IP as 192xxx. Also no FW or blocking traffic on port 3306.\r\n\r\nPlease help.\r\n\n\n### Attach the log file\n\nNA\n\n### Operating System\n\nUbuntu\n\n### Driver\n\nDocker",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2024-01-02T07:31:50Z",
      "closed_at": "2024-05-31T08:49:18Z",
      "url": "https://github.com/kubernetes/minikube/issues/17880",
      "comments_count": 5
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21970,
      "title": "Update Automation PR:The \"after\" version is wrong in PR title",
      "problem": "This Automation PR \nhttps://github.com/kubernetes/minikube/pull/21950/files\n\nhttps://github.com/kubernetes/minikube/tree/master/.github/workflows/update-go-github-version.yml\n\n\nThe PR Title is:\nCI: update go-github from v74.0.0 to v74.0.0\n\nWhile the PR itself is changing it to v79.0,0\n\n\n\nanyone taking this issue I suggest looking at the yaml file above and also the hack/update folder\n\n\nI noticed a couple other PRs titles were wrong and I fixed manually",
      "solution": "also happend to  https://github.com/kubernetes/minikube/pull/21975/files (fixed it manually)\n\nmake update-cni-plugins-version using [update-cni-plugins-version.yml](https://github.com/kubernetes/minikube/tree/master/.github/workflows/update-cni-plugins-version.yml) CI Workflow.\n\n---\n\n> also happend to https://github.com/kubernetes/minikube/pull/21975/files (fixed it manually)\n> \n> make update-cni-plugins-version using [update-cni-plugins-version.yml](https://github.com/kubernetes/minikube/tree/master/.github/workflows/update-cni-plugins-version.yml) CI Workflow.\n\nLooking at the (https://github.com/kubernetes/minikube/tree/master/.github/workflows/update-cni-plugins-version.yml) workflow, it uses the same pattern as the go-github workflow:\n\n```\necho \"OLD_VERSION=$(DEP=cni-plugins make get-dependency-version)\" >> \"$GITHUB_OUTPUT\"\nmake update-cni-plugins-version\necho \"NEW_VERSION=$(DEP=cni-plugins make get-dependency-version)\" >> \"$GITHUB_OUTPUT\"\n```\n\nBoth workflows rely on hack/update/get_version/get_version.go to capture versions,\n\nThe cni-plugins dependency reads its version from:\n\n\"cni-plugins\": {\"deploy/iso/minikube-iso/arch/x86_64/package/cni-plugins-latest/cni-plugins-latest.mk\", \n                `CNI_PLUGINS_LATEST_VERSION = (.*)`} \n\nwhich has one version (unlike go.mod which can have multiple), i found that there was a bump that update this version and file on september 8\n\n```\ncommit 18eea256f692fe5260f34a13de0cc56053c756fc\nAuthor: minikube-bot <minikube-bot@google.com>\nDate:   Mon Sep 8 17:52:16 2025 +0000\n\n    Kicbase/ISO: Update cni-plugins from v1.7.1 to v1.8.0\n\ncommit 9171dbbc18499d074fb1347bbe5819e022c13243\n```\n\nThis means when PR #21975 ran later, the file already contained `v1.8.0`. The workflow would have captured:\n- **OLD_VERSION**: v1.8.0 (current version in file)\n- **NEW_VERSION**: v1.8.0 (no new version available)\n- **Result**: PR title showing \"v1.8.0 to v1.8.0\" instead of the intended range\n\n",
      "labels": [
        "good first issue"
      ],
      "created_at": "2025-11-22T18:13:32Z",
      "closed_at": "2025-11-30T19:30:34Z",
      "url": "https://github.com/kubernetes/minikube/issues/21970",
      "comments_count": 3
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21028,
      "title": "Document Cross Build Process and requirement for moving to github action",
      "problem": "This Issue documents the cross build process\n\nok-to-test command triggers \"minikube\" upstream job in jenkins that runs Cross Build\nhttps://github.com/kubernetes/minikube/blob/master/hack/jenkins/minikube_cross_build_and_upload.sh\n\nafter that it triggers all downstream jobs\nlike kvm_linux, none_linux,...\n\nafter that the Post Jobs will run",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-07-03T17:13:23Z",
      "closed_at": "2025-11-30T19:26:29Z",
      "url": "https://github.com/kubernetes/minikube/issues/21028",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21026,
      "title": "Network does not exist starting minikube but network exists",
      "problem": "### What Happened?\n\nOS is Almalinux 9.6\n\nAs per subject.\nStarting minikube fails:\nError response from daemon: failed to set up container networking: failed to create endpoint minikube on network minikube: network 7ee15742102f2016ea64d8487264bb8cb81e455225c16733bf3584509b65b5c4 does not exist\n\nminikube user is in docker group\nhowever docker is running and network is present:\n\ndocker network inspect minikube\n[\n    {\n        \"Name\": \"minikube\",\n        \"Id\": \"7ee15742102f2016ea64d8487264bb8cb81e455225c16733bf3584509b65b5c4\",\n        \"Created\": \"2025-06-12T12:57:00.640929393+02:00\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"EnableIPv4\": true,\n        \"EnableIPv6\": false,\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Options\": {},\n            \"Config\": [\n                {\n                    \"Subnet\": \"192.168.49.0/24\",\n                    \"Gateway\": \"192.168.49.1\"\n                }\n            ]\n        },\n        \"Internal\": false,\n        \"Attachable\": false,\n        \"Ingress\": false,\n        \"ConfigFrom\": {\n            \"Network\": \"\"\n        },\n        \"ConfigOnly\": false,\n        \"Containers\": {},\n        \"Options\": {\n            \"--icc\": \"\",\n            \"--ip-masq\": \"\",\n            \"com.docker.network.driver.mtu\": \"1500\"\n        },\n        \"Labels\": {\n            \"created_by.minikube.sigs.k8s.io\": \"true\",\n            \"name.minikube.sigs.k8s.io\": \"minikube\"\n        }\n    }\n]\n\n\n### Attach the log file\n\n[logs.txt](https://github.com/user-attachments/files/21040686/logs.txt)\n\n### Operating System\n\nRedhat/Fedora\n\n### Driver\n\nDocker",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-07-03T15:43:14Z",
      "closed_at": "2025-11-30T17:26:30Z",
      "url": "https://github.com/kubernetes/minikube/issues/21026",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21023,
      "title": "Cannot host a dashboard for my cluster locally",
      "problem": "### What Happened?\n\nI got this error:\n\n\u279c  learn_kubernetes minikube dashboard --port=63840                                                                                                                      (minikube/default)\n\ud83d\udd0c  Enabling dashboard ...\n    \u25aa Using image docker.io/kubernetesui/dashboard:v2.7.0\n    \u25aa Using image docker.io/kubernetesui/metrics-scraper:v1.0.8\n\ud83d\udca1  Some dashboard features require the metrics-server addon. To enable all features please run:\n\n        minikube addons enable metrics-server\n\n\ud83e\udd14  Verifying dashboard health ...\n\ud83d\ude80  Launching proxy ...\n\n\u274c  Exiting due to HOST_KUBECTL_PROXY: kubectl proxy: readByteWithTimeout: EOF\n\n### Attach the log file\n\n[logs.txt](https://github.com/user-attachments/files/21035379/logs.txt)\n\n[minikube_dashboard_bc1d981f579bb28e84ebd7e6e0696e70cda0764d_0.log](https://github.com/user-attachments/files/21035456/minikube_dashboard_bc1d981f579bb28e84ebd7e6e0696e70cda0764d_0.log)\n\n### Operating System\n\nUbuntu\n\n### Driver\n\nNone",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-07-03T11:04:55Z",
      "closed_at": "2025-11-30T13:24:30Z",
      "url": "https://github.com/kubernetes/minikube/issues/21023",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 20724,
      "title": "`minikube addons enable registry` fails on Podman (rootless)",
      "problem": "### What happened\n\nI tried to enable the `registry` addon using:\n\n```bash\nminikube addons enable registry\n```\n\nIt failed with:\n\n```\n\u274c  ... enable failed: run callbacks: running callbacks: [waiting for kubernetes.io/minikube-addons=registry pods: context deadline exceeded]\n```\n\n#### Additional context\n\nIn another terminal, I checked the pod status:\n\n```bash\nkubectl describe pod registry-proxy-xxxxx -n kube-system\n```\n\nThe pod was stuck in `ContainerCreating`, with repeated sandbox errors like:\n\n```\nFailed to create pod sandbox: ... failed to setup network for sandbox ... could not insert 'ip_tables': Operation not permitted\n```\n\nFull message:\n\n```\nmodprobe: ERROR: could not insert 'ip_tables': Operation not permitted\niptables v1.8.7 (legacy): can't initialize iptables table `nat': Table does not exist (do you need to insmod?)\n```\n\n#### Environment\n\n* **Minikube version**:\n```\nminikube version: v1.35.0\ncommit: dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty\n```\n* **OS**: Fedora 42\n* **Driver**: Podman (rootless)\n* **Container runtime**: containerd\n\n### Attach the log file\n\n[log.txt](https://github.com/user-attachments/files/20146703/log.txt)\n\n[minikube_addons_a4fe86e99845e102d4e73c94aae831dab434b0c1_0.log](https://github.com/user-attachments/files/20146705/minikube_addons_a4fe86e99845e102d4e73c94aae831dab434b0c1_0.log)\n\n### Operating System\n\nRedhat/Fedora\n\n### Driver\n\nPodman",
      "solution": "Had the same problem on a very similar environment (minikube 1.36, fedora 42, rootless podman, containerd), managed to solve the problem via: \n```\nsudo modprobe ip_tables\nsudo modprobe iptable_nat\n```\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-05-11T02:40:17Z",
      "closed_at": "2025-11-30T09:20:29Z",
      "url": "https://github.com/kubernetes/minikube/issues/20724",
      "comments_count": 6
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 21008,
      "title": "vulnerability in minikube project",
      "problem": "### What Happened?\n\nWhile working on the minikube project, I found a vulnerability in the mapstructure package [GHSA-fv92-fjc5-jj9h](https://vulert.com/vuln-db/go-github-com-go-viper-mapstructure-v2-194208). This issue could lead to sensitive information being leaked in logs when using WeakDecode. The vulnerability is fixed in version 2.3.0, and it\u2019s recommended to upgrade.  \n\n[CVE Link](https://vulert.com/vuln-db/go-github-com-go-viper-mapstructure-v2-194208)\n[CVE Report](https://vulert.com/vuln-scan/list/08471bff-6760-4a78-abc9-b2cd982cf890?sort_order=desc&sort_by=created_at)\n\n### Attach the log file\n\nI uploaded your project's go.mod to [Vulert](https://vulert.com/), and it flagged a security issue \n\n### Operating System\n\nNone\n\n### Driver\n\nNone",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-07-01T07:15:05Z",
      "closed_at": "2025-11-29T19:17:28Z",
      "url": "https://github.com/kubernetes/minikube/issues/21008",
      "comments_count": 5
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/minikube",
      "issue_number": 20463,
      "title": "Failing to connect to https://registry.k8s.io/ from inside the minikube container -  MINIKUBE_ROOTLESS=true",
      "problem": "### What Happened?\n\nFollowing the docs on [using podman in rootless mode](https://minikube.sigs.k8s.io/docs/drivers/podman/), I've execute:\n\n```\ngilberto.andrade@C430760:~$ minikube config set rootless true\ngilberto.andrade@C430760:~$ minikube start --driver=podman --container-runtime=containerd\n\ud83d\ude04  minikube v1.35.0 on Opensuse-Leap 15.6\n    \u25aa MINIKUBE_ROOTLESS=true\n\u2728  Using the podman driver based on user configuration\n\ud83d\udccc  Using rootless Podman driver\n\ud83d\udc4d  Starting \"minikube\" primary control-plane node in \"minikube\" cluster\n\ud83d\ude9c  Pulling base image v0.0.46 ...\n\ud83d\udcbe  Downloading Kubernetes v1.32.0 preload ...\n    > preloaded-images-k8s-v18-v1...:  379.64 MiB / 379.64 MiB  100.00% 29.61 M\n    > gcr.io/k8s-minikube/kicbase...:  500.31 MiB / 500.31 MiB  100.00% 28.55 M\nE0225 13:45:30.254467   25543 cache.go:222] Error downloading kic artifacts:  not yet implemented, see issue #8426\n\ud83d\udd25  Creating podman container (CPUs=2, Memory=3900MB) ...\n\u2757  Failing to connect to https://registry.k8s.io/ from inside the minikube container\n\ud83d\udca1  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/\n\ud83d\udce6  Preparing Kubernetes v1.32.0 on containerd 1.7.24 ...\n    \u25aa Generating certificates and keys ...\n    \u25aa Booting up control plane ...\n    \u25aa Configuring RBAC rules ...\n\ud83d\udd17  Configuring CNI (Container Networking Interface) ...\n\ud83d\udd0e  Verifying Kubernetes components...\n    \u25aa Using image gcr.io/k8s-minikube/storage-provisioner:v5\n\ud83c\udf1f  Enabled addons: storage-provisioner, default-storageclass\n\ud83c\udfc4  Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\n\n```\nChecking:\n\n```\ngilberto.andrade@C430760:~$ kubectl get po -A\nNAMESPACE     NAME                               READY   STATUS    RESTARTS      AGE\nkube-system   coredns-668d6bf9bc-zcdck           1/1     Running   0             33m\nkube-system   etcd-minikube                      1/1     Running   0             33m\nkube-system   kindnet-ljs2g                      1/1     Running   0             33m\nkube-system   kube-apiserver-minikube            1/1     Running   0             33m\nkube-system   kube-controller-manager-minikube   1/1     Running   0             33m\nkube-system   kube-proxy-bj5j9                   1/1     Running   0             33m\nkube-system   kube-scheduler-minikube            1/1     Running   0             33m\nkube-system   storage-provisioner                1/1     Running   1 (33m ago)   33m\n\ngilberto.andrade@C430760:~$ cat /etc/os-release \nNAME=\"openSUSE Leap\"\nVERSION=\"15.6\"\n\ngilberto.andrade@C430760:~$ minikube version\nminikube version: v1.35.0\ncommit: dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty\n\n```\nI've noted that it complained **\"Failing to connect to https://registry.k8s.io/ from inside the minikube container\"** and so I've tried to pull an image from inside:\n\n```\ngilberto.andrade@C430760:~$ minikube ssh\ndocker@minikube:~$ podman version\nWARN[0000] The cgroupv2 manager is set to systemd but there is no systemd user session available \nWARN[0000] For using systemd, you may need to login using an user session \nWARN[0000] Alternatively, you can enable lingering with: `loginctl enable-linger 1000` (possibly as root) \nWARN[0000] Falling back to --cgroup-manager=cgroupfs    \nError: command required for rootless mode with multiple IDs: exec: \"newuidmap\": executable file not found in $PATH\n\ndocker@minikube:~$ sudo podman version\nVersion:      3.4.4\nAPI Version:  3.4.4\nGo Version:   go1.18.1\nBuilt:        Thu Jan  1 00:00:00 1970\nOS/Arch:      linux/amd64\n\ndocker@minikube:~$ sudo podman pull registry.access.redhat.com/ubi8/ubi\nTrying to pull registry.access.redhat.com/ubi8/ubi:latest...\nError: initializing source docker://registry.access.redhat.com/ubi8/ubi:latest: pinging container registry registry.access.redhat.com: Get \"https://registry.access.redhat.com/v2/\": dial tcp: lookup registry.access.redhat.com on 192.168.49.1:53: read udp 192.168.49.2:59184->192.168.49.1:53: i/o timeout\n\ndocker@minikube:~$ exit\nlogout\nssh: Process exited with status 125\n\ngilberto.andrade@C430760:~$ podman pull registry.access.redhat.com/ubi8/ubi\nTrying to pull registry.access.redhat.com/ubi8/ubi:latest...\nGetting image source signatures\nCopying blob f8750fc49bf2 done   | \nCopying config dc564c6cc0 done   | \nWriting manifest to image destination\ndc564c6cc036db0628baa2d1dd623054d115a2528c57b1cf5324ee9dedb019cb\n\ngilberto.andrade@C430760:~$ \n\n```\n\nIs it poss\u00edvel to update podman in minukube image? Maybe it is the problem?\n\n### Attach the log file\n\n[log.txt](https://github.com/user-attachments/files/18969865/log.txt)\n\n[minikube-linux-amd64-v1.36.0-alsologtostderr.log](https://github.com/user-attachments/files/22124309/minikube-linux-amd64-v1.36.0-alsologtostderr.log)\n\n[minikube-linux-amd64-v1.33.1-alsologtostderr.log](https://github.com/user-attachments/files/22124315/minikube-linux-amd64-v1.33.1-alsologtostderr.log)\n\n### Operating System\n\nOther\n\n### Driver\n\nPodman",
      "solution": "> is this problem solved? i am having same issue\n\nMe too!\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nI've tested with the current version (v1.36), but unfortunately the dns issue persists. \nFirst:\n```\n~/Downloads/KDE\nminikube stop\nminikube delete --all\npodman system prune -a\n\n```\n\n Then:\n\n```\n~/Downloads/KDE\n$ minikube config set rootless true\n~/Downloads/KDE\n$ minikube start --driver=podman --container-runtime=containerd alsologtostderr=true\n\ud83d\ude04  minikube v1.36.0 on Opensuse-Leap 15.6\n    \u25aa MINIKUBE_ROOTLESS=true\n\u2728  Using the podman driver based on user configuration\n\ud83d\udccc  Using rootless Podman driver\n\ud83d\udc4d  Starting \"minikube\" primary control-plane node in \"minikube\" cluster\n\ud83d\ude9c  Pulling base image v0.0.47 ...\nE0901 14:27:15.978336   13022 cache.go:225] Error downloading kic artifacts:  not yet implemented, see issue #8426\n\ud83d\udd25  Creating podman container (CPUs=2, Memory=3900MB) ...\n\u2757  Failing to connect to https://registry.k8s.io/ from inside the minikube container\n\ud83d\udca1  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/\n\ud83d\udce6  Preparing Kubernetes v1.33.1 on containerd 1.7.27 ...\n    \u25aa Generating certificates and keys ...\n    \u25aa Booting up control plane ...\n    \u25aa Configuring RBAC rules ...\n\ud83d\udd17  Configuring CNI (Container Networking Interface) ...\n\ud83d\udd0e  Verifying Kubernetes components...\n    \u25aa Using image gcr.io/k8s-minikube/storage-provisioner:v5\n\ud83c\udf1f  Enabled addons: storage-provisioner, default-storageclass\n\ud83c\udfc4  Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\n~/Downloads/KDE\n$ kubectl get po -A\nNAMESPACE     NAME                               READY   STATUS              RESTARTS      AGE\nkube-system   coredns-674b8bbfcf-449lw           0/1     ContainerCreating   0             55s\nkube-system   etcd-minikube                      1/1     Running             0             60s\nkube-system   kindnet-bpk9t                      0/1     ErrImagePull        0             55s\nkube-system   kube-apiserver-minikube            1/1     Running             0             61s\nkube-system   kube-controller-manager-minikube   1/1     Running             0             60s\nkube-system   kube-proxy-5xnk5                   1/1     Running             0             55s\nkube-system   kube-scheduler-minikube            1/1     Running             0             61s\nkube-system   storage-provisioner                1/1     Running             1 (24s ago)   59s\n~/Downloads/KDE\n\n``` \nInconsistent dns resolution:\n\n```\n~/Downloads/KDE\n$ minikube ssh\ndocker@minikube:~$ nslookup registry-1.docker.io\n;; communications error to 192.168.49.1#53: timed out\nServer:         192.168.49.1\nAddress:        192.168.49.1#53\n\nNon-authoritative answer:\nName:   registry-1.docker.io\nAddress: 54.166.25.11\nName:   registry-1.docker.io\nAddress: 3.226.118.171\nName:   registry-1.docker.io\nAddress: 52.71.177.86\nName:   registry-1.docker.io\nAddress: 44.215.205.88\nName:   registry-1.docker.io\nAddress: 52.201.102.193\nName:   registry-1.docker.io\nAddress: 54.161.123.11\nName:   registry-1.docker.io\nAddress: 54.82.138.1\nName:   registry-1.docker.io\nAddress: 34.197.219.69\n;; communications error to 192.168.49.1#53: timed out\nName:   registry-1.docker.io\nAddress: 2600:1f18:2148:bc01:9efe:a1b5:4630:f569\nName:   registry-1.docker.io\nAddress: 2600:1f18:2148:bc02:1430:16f3:3693:e14d\nName:   registry-1.docker.io\nAddress: 2600:1f18:2148:bc01:eef9:a328:d72a:b25a\nName:   registry-1.docker.io\nAddress: 2600:1f18:2148:bc01:1aa0:a12a:be32:e315\nName:   registry-1.docker.io\nAddress: 2600:1f18:2148:bc02:c06e:e9b4:c773:69eb\nName:   registry-1.docker.io\nAddress: 2600:1f18:2148:bc00:2cf7:95b1:68e3:6f67\nName:   registry-1.docker.io\nAddress: 2600:1f18:2148:bc00:b979:555:b50c:b1bf\nName:   registry-1.docker.io\nAddress: 2600:1f18:2148:bc00:ae04:1f6e:9dad:9ab7\ndocker@minikube:~$ \ndocker@minikube:~$ cat /etc/resolv.conf\nsearch dns.podman\nnameserver 192.168.49.1\ndocker@minikube:~$ \n\n```\nCant pull image from inside:\n\n```\ndocker@minikube:~$ sudo podman pull registry.access.redhat.com/ubi8/ubi\nTrying to pull registry.access.redhat.com/ubi8/ubi:latest...\nError: initializing source docker://registry.access.redhat.com/ubi8/ubi:latest: pinging container registry registry.access.redhat.com: Get \"https://registry.access.redhat.com/v2/\": dial tcp: lookup registry.access.redhat.com on 192.168.49.1:53: read udp 192.168.49.2:40714->192.168.49.1:53: i/o timeout\ndocker@minikube:~$ \ndocker@minikube:~$ sudo podman version\nVersion:      3.4.4\nAPI Version:  3.4.4\nGo Version:   go1.18.1\nBuilt:        Thu Jan  1 00:00:00 1970\nOS/Arch:      linux/amd64\ndocker@minikube:~$ \n\n```",
      "labels": [
        "lifecycle/rotten"
      ],
      "created_at": "2025-02-25T17:48:26Z",
      "closed_at": "2025-11-25T00:31:25Z",
      "url": "https://github.com/kubernetes/minikube/issues/20463",
      "comments_count": 12
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1809,
      "title": "Observed panic with nil pointer dereference",
      "problem": "**What happened**:\n\nI wanted to run the following:\n\n`kubectl exec -it pqs-postgres-0 -n catalyst-canton -- bash env | grep POSTGRES`\n\nbut I got this:\n\n`E1219 16:07:37.556979   22674 runtime.go:142] \"Observed a panic\" panic=\"runtime error: invalid memory address or nil pointer dereference\" panicGoValue=\"\\\"invalid memory address or nil pointer dereference\\\"\" stacktrace=<`\n\n**What you expected to happen**:\n\nGet the filtered envs\n\n**Anything else we need to know?**:\n\nI checked the stacktrace and it showed me this:\n\n```\n[k8s.io/kubectl/pkg/cmd/exec.(*terminalSizeQueueAdapter).Next(0x0?)](http://k8s.io/kubectl/pkg/cmd/exec.(*terminalSizeQueueAdapter).Next(0x0?))\n[k8s.io/kubectl/pkg/cmd/exec/exec.go:414](http://k8s.io/kubectl/pkg/cmd/exec/exec.go:414) +0x1c\n```\n\nSo I checked what is it [there](https://github.com/kubernetes/kubectl/blob/master/pkg/cmd/exec/exec.go#L414). After I saw that the initialization of the delegate field happening [here](https://github.com/kubernetes/kubectl/blob/master/pkg/cmd/exec/exec.go#L370). This can [return a nil](https://github.com/kubernetes/kubectl/blob/master/pkg/util/term/resize.go#L69), which is not checked at the initialization. \n\n**Environment**:\n```\nkubectl version\nClient Version: v1.35.0\nKustomize Version: v5.7.1\n```\n\nOS: MacOS (one of my colleague so version unclear)\n\n",
      "solution": "As a workaround, this appears to work, at least for me in testing with the version reported\n\n```\nkubectl exec -n my-namespace \\\n  -it deployments/my-deployment \\\n  --tty=false \\\n  -- bash -c \\\n  'echo \"this is the command to run\"'\n```\n\n",
      "labels": [
        "kind/bug",
        "priority/backlog",
        "triage/accepted"
      ],
      "created_at": "2025-12-22T12:44:57Z",
      "closed_at": "2025-12-24T07:22:33Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1809",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1527,
      "title": "Updating with Non Existing Resource Name Gets Success Response",
      "problem": "<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\r\n\r\nIf the matter is security related, please disclose it privately via https://kubernetes.io/security/\r\n-->\r\n\r\n**What happened**: I updated rbac.authorization.k8s.io/v1 role binding for non-existent resources using the python kubernetes-asyncio library, but I get a 200 normal response and create a new resource. It is said that the same result occurred when running with kubectl, and the same bug occurs when runtime class and limit range update.\r\nhttps://github.com/tomplus/kubernetes_asyncio/issues/294\r\n\r\n**What you expected to happen**: I think 404 not found error should occur when updating for all nonexistent role binding, runtime class, and limit range, lease.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**: \r\n```yaml\r\n# role-binding.yaml\r\napiVersion: rbac.authorization.k8s.io/v1\r\nkind: RoleBinding\r\nmetadata:\r\n  name: non-existing-resource\r\n  namespace: default\r\nsubjects:\r\n- kind: User\r\n  name: jane\r\n  apiGroup: rbac.authorization.k8s.io\r\nroleRef:\r\n  kind: Role\r\n  name: pod-reader\r\n  apiGroup: rbac.authorization.k8s.io\r\n```\r\n```bash\r\nkubectl replace -f role-binding.yaml\r\n```\r\n```yaml\r\n# runtime-class.yaml\r\napiVersion: node.k8s.io/v1\r\nkind: RuntimeClass\r\nmetadata:\r\n  name: non-existing-resource\r\nhandler: myconfiguration\r\n```\r\n```bash\r\nkubectl replace -f runtime-class.yaml\r\n```\r\n```yaml\r\n# limit-range.yaml\r\napiVersion: v1\r\nkind: LimitRange\r\nmetadata:\r\n  name: non-existing-resource\r\nspec:\r\n  limits:\r\n  - type: Container\r\n    default:\r\n      memory: 512Mi\r\n      cpu: 500m\r\n    defaultRequest:\r\n      memory: 256Mi\r\n      cpu: 250m\r\n    max:\r\n      memory: 1024Mi\r\n      cpu: 1000m\r\n    min:\r\n      memory: 128Mi\r\n      cpu: 125m\r\n    maxLimitRequestRatio:\r\n      memory: 2\r\n      cpu: 2\r\n```\r\n```bash\r\nkubectl replace -f limit-range.yaml\r\n```\r\n```yaml\r\n# lease.yaml\r\napiVersion: coordination.k8s.io/v1\r\nkind: Lease\r\nmetadata:\r\n  labels:\r\n    apiserver.kubernetes.io/identity: kube-apiserver\r\n    kubernetes.io/hostname: master-1\r\n  name: non-exist-resource\r\n  namespace: default\r\nspec:\r\n  holderIdentity: apiserver-07a5ea9b9b072c4a5f3d1c3702_0c8914f7-0f35-440e-8676-7844977d3a05\r\n  leaseDurationSeconds: 3500\r\n```\r\n```bash\r\nkubectl replace -f lease.yaml\r\n```\r\n<!-- Please make sure you are able to reproduce the bug using a supported version and version skew of Kubernetes. See https://kubernetes.io/docs/setup/release/version-skew-policy for which versions and are currently supported.\r\n-->\r\n\r\n**Anything else we need to know?**:\r\n\r\n**Environment**:\r\n- Kubernetes client and server versions (use `kubectl version`): \r\n```pre\r\nClient Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.5\", GitCommit:\"93e0d7146fb9c3e9f68aa41b2b4265b2fcdb0a4c\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:48:26Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\nKustomize Version: v5.0.1\r\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.5\", GitCommit:\"93e0d7146fb9c3e9f68aa41b2b4265b2fcdb0a4c\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:42:11Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n```\r\n- Cloud provider or hardware configuration: vmware-vsphere 7.0.3.00700\r\n- OS (e.g: `cat /etc/os-release`): \r\n```pre\r\nNAME=\"Ubuntu\"\r\nVERSION=\"20.04.6 LTS (Focal Fossa)\"\r\nID=ubuntu\r\nID_LIKE=debian\r\nPRETTY_NAME=\"Ubuntu 20.04.6 LTS\"\r\nVERSION_ID=\"20.04\"\r\nHOME_URL=\"https://www.ubuntu.com/\"\r\nSUPPORT_URL=\"https://help.ubuntu.com/\"\r\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\r\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\r\nVERSION_CODENAME=focal\r\nUBUNTU_CODENAME=focal\r\n```\r\n\r\n",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/support",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2023-12-06T04:16:45Z",
      "closed_at": "2026-01-28T17:49:21Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1527",
      "comments_count": 8
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1531,
      "title": "kubectl diff command output diff informations when patch is failed for statefulset",
      "problem": "### What would you like to be added?\r\n\r\nwhen statefulset patch is failed, kubectl output the the patch informations, not just output something like\r\n```\r\nupdates to statefulset spec for fields other than 'replicas', 'template', 'updateStrategy' and 'minReadySeconds' are forbidden\r\n```\r\n\r\n\r\n### Why is this needed?\r\n\r\nsome  patch information may help user to understand why patch is failed, it is useful when the actual diff is not so obvious",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\n/close\nThe error message is pretty clear, I'm unsure what exactly the ask is from this issue, so I'm going to close this. If a script can be provided to demonstrate exactly what the problem is we can investigate. I suspect that the best answer is to use apply, however.",
      "labels": [
        "kind/feature",
        "sig/cli",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2023-12-09T07:01:56Z",
      "closed_at": "2026-01-28T17:41:16Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1531",
      "comments_count": 10
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1544,
      "title": "interrupting kubectl watch with Ctrl-C breaks readline",
      "problem": "<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\r\n\r\nIf the matter is security related, please disclose it privately via https://kubernetes.io/security/\r\n-->\r\n\r\n**What happened**:\r\nrunning kubectl get pod -w, then interrupting with Ctrl-C, will leave \"^C\" written on the terminal without a new line after. The terminal prompt is drawn right after the ^C\r\n\r\nHere is why it's an issue: this causes weird issues with the terminal, if you for example, try to reexecute the previous statement with pressing the up arrow then go to the beg of the line with Ctrl-A, and travel through the line with Alt-F and edit that statement, you realize that readline does not really know where the line starts, and you're going to mess up the terminal input.\r\n\r\nPractically, this means the user has to run \"Ctrl-C\" a second time to end up on a new line:\r\n```\r\n$ oc get pod -w\r\n^C$ ^C\r\n$\r\n```\r\ncompare to :\r\n```\r\n$ sleep 10\r\n^C\r\n$\r\n```\r\n\r\n**What you expected to happen**:\r\nThe ^C should be followed by a new line.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n```kubectl get pod -w```\r\nthen type Ctrl-C to interrupt it.\r\n\r\n**Anything else we need to know?**:\r\nfor the readline behavior\r\nhttps://askubuntu.com/a/861621\r\n\r\n**Environment**:\r\n- Kubernetes client and server versions (use `kubectl version`): v1.29.0\r\n- Cloud provider or hardware configuration: any\r\n- OS (e.g: `cat /etc/os-release`): centos stream9/ubuntu\r\n\r\nbash           5.1-6ubuntu1 / bash-5.1.8-6.el9.x86_64\r\n",
      "solution": " Hello @mpuckett159, @ardaguclu\r\nI have investigate on this issue and there is common issue between multiple resources not just `get`, currently i've tested `debug`. But `events`, `rollout_status`, `run` should have the same issue.\r\n\r\nI have fixed this issue on my local, can i assign myself? or should i provide @ardaguclu with my findings?\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/bug",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2024-01-12T10:25:08Z",
      "closed_at": "2026-01-28T17:36:17Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1544",
      "comments_count": 10
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1546,
      "title": "kubectl completion, config current-context and version commands are too slow",
      "problem": "### What happened?\r\n\r\nCommands `kubectl completion zsh`, `kubectl config current-context`, `kubectl version` and many others are too slow because they are trying to connect to the server.\r\nThis is very annoying behaviour which causes slow opening of new terminal (~0.5s with VPN and ~5s without VPN, when connection isn't possible).\r\n\r\nI don't see any reason for those commands to connect to any server, they should just return output locally.\r\n\r\n\r\n### What did you expect to happen?\r\n\r\nAll the commands run ~0.05s, like they do without any network connection:\r\n```console\r\n$ time kubectl completion zsh > /dev/null                                                                                                                                                                                                          \r\nkubectl completion zsh > /dev/null  0.05s user 0.01s system 126% cpu 0.044 total\r\n```\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\nWithout VPN:\r\n```console\r\n$ time kubectl version                                                                                                                                                                                                                             \r\nWARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\r\nClient Version: version.Info{Major:\"1\", Minor:\"27+\", GitVersion:\"v1.27.9-dispatcher\", GitCommit:\"8b508a33aafcd3ba51641b6b2ef203adbdd9de1e\", GitTreeState:\"clean\", BuildDate:\"2023-12-21T23:22:51Z\", GoVersion:\"go1.20.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\nKustomize Version: v5.0.1\r\nUnable to connect to the server: dial tcp 10.xxx.xxx.xxx:443: i/o timeout\r\nkubectl version  0.05s user 0.03s system 0% cpu 35.058 total\r\n\r\n$ time kubectl completion zsh > /dev/null                                                                                                                                                                                                          \r\nkubectl completion zsh > /dev/null  0.05s user 0.05s system 1% cpu 5.075 total\r\n\r\n$ time kubectl config current-context                                                                                                                                                                                                              \r\nXXXXXXXXX\r\nkubectl config current-context  0.07s user 0.03s system 1% cpu 5.078 total\r\n```\r\n\r\nWith VPN connected (still annoyingly slow):\r\n```console\r\n$ time kubectl version                                                                                                                                                                                                                             \r\nWARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\r\nClient Version: version.Info{Major:\"1\", Minor:\"27+\", GitVersion:\"v1.27.9-dispatcher\", GitCommit:\"8b508a33aafcd3ba51641b6b2ef203adbdd9de1e\", GitTreeState:\"clean\", BuildDate:\"2023-12-21T23:22:51Z\", GoVersion:\"go1.20.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\nKustomize Version: v5.0.1\r\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.8-gke.1067000\", GitCommit:\"3936242da351c64ea89912f00faa0b28fb7eab76\", GitTreeState:\"clean\", BuildDate:\"2023-12-06T20:41:34Z\", GoVersion:\"go1.20.11 X:boringcrypto\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\nkubectl version  0.07s user 0.03s system 9% cpu 1.087 total\r\n\r\n$ time kubectl completion zsh > /dev/null                                                                                                                                                                                                          \r\nkubectl completion zsh > /dev/null  0.04s user 0.03s system 16% cpu 0.414 total\r\n\r\n$ time kubectl config current-context                                                                                                                                                                                                              \r\nXXXXXXXXX\r\nkubectl config current-context  0.06s user 0.03s system 20% cpu 0.444 total\r\n```\r\n\r\n### Anything else we need to know?\r\n\r\nThere was previous issue https://github.com/kubernetes/kubernetes/issues/82883, which was closed without resolution, also please do not mark this as feature: it's a bug because `kubecli` can and should run the commands for ~0.05s but run for 0.5s or even 5s.\r\n\r\n### Kubernetes version\r\n\r\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nWARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\r\nClient Version: version.Info{Major:\"1\", Minor:\"27+\", GitVersion:\"v1.27.9-dispatcher\", GitCommit:\"8b508a33aafcd3ba51641b6b2ef203adbdd9de1e\", GitTreeState:\"clean\", BuildDate:\"2023-12-21T23:22:51Z\", GoVersion:\"go1.20.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\nKustomize Version: v5.0.1\r\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.8-gke.1067000\", GitCommit:\"3936242da351c64ea89912f00faa0b28fb7eab76\", GitTreeState:\"clean\", BuildDate:\"2023-12-06T20:41:34Z\", GoVersion:\"go1.20.11 X:boringcrypto\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Cloud provider\r\n\r\n<details>\r\nGCP\r\n</details>\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\nPRETTY_NAME=\"Debian GNU/Linux trixie/sid\"\r\nNAME=\"Debian GNU/Linux\"\r\nVERSION_CODENAME=trixie\r\nID=debian\r\nHOME_URL=\"https://www.debian.org/\"\r\nSUPPORT_URL=\"https://www.debian.org/support\"\r\nBUG_REPORT_URL=\"https://bugs.debian.org/\"\r\n\r\n$ uname -a\r\nLinux xxxxx 6.6.9-amd64 kubernetes/kubernetes#1 SMP PREEMPT_DYNAMIC Debian 6.6.9-1 (2024-01-01) x86_64 GNU/Linux\r\n\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n```console\r\n$ apt policy kubectl                                                                                                                                             \r\nkubectl:\r\n  Installed: 1:460.0.0-0\r\n  Candidate: 1:460.0.0-0\r\n  Version table:\r\n *** 1:460.0.0-0 500\r\n        500 https://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages\r\n        100 /var/lib/dpkg/status\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n",
      "solution": "@yorik \r\nI was experiencing this exact issue with the kubectl version provided by gcloud cli. And this was something relatively new.\r\nIn fact when using that version, when the current context was an AWS EKS cluster, even when running `kubelet --help`, it would try to connect using AWS SSO.\r\n\r\nI compiled kubelet from source, on `master`, and the issue did not replicate.\r\nI think this is something that was solved.\r\n\r\nCan you try compiling from source or downgrading?\r\nYou can also try running something like:\r\n```\r\nKUBECONFIG=/dev/null kubelet version\r\n```\r\n\r\nP.S I agree that this has nothing to do with https://github.com/kubernetes/kubernetes/issues/82883\n\n---\n\nI've tested it with `kubectl` built from https://github.com/kubernetes/kubernetes at 2d4100335e4c4ccc28f96fac78153f378212da4c and I wasn't able to reproduce the problem.\r\n\r\nAlso I wasn't able to find git commit 8b508a33aafcd3ba51641b6b2ef203adbdd9de1e in the repo from kubectl version installed from https://packages.cloud.google.com/apt, so maybe there are some google's patches added.\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/bug",
        "sig/cli",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2024-01-18T17:54:53Z",
      "closed_at": "2026-01-28T17:34:12Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1546",
      "comments_count": 30
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1548,
      "title": "kubectl follow logs stops after few seconds if there is a lot data to stdout",
      "problem": "### What happened?\n\nA lot of running jobs generates a massive amount of log to stdout.\r\nSteps:\r\n1. Running K8s job that invokes a lot of lines to stdout.\r\n2. kubectl logs -f <podName> -n <nameSpace>\r\nStarts tailing pod output, till it stuck randomly. \r\n**Note**, it stuck till pod exit with a success or failure. \r\n \n\n### What did you expect to happen?\n\nExpected result is to see all pod stdout on screen. This is not happening. \n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n1. invokes a pod that generates a massive amount of stdout.\r\n2. run:\r\n   # kubectl logs -f <pod-name> -n <namespace>\r\n3. after a while, this is randomly duration, it stops showing output \n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nClient Version: v1.28.1\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.27.7\r\n```\r\n\r\n</details>\r\n\n\n### Cloud provider\n\n<details>\r\n\r\n</details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\n# NAME=\"CentOS Linux\"\r\nVERSION=\"7 (Core)\"\r\nID=\"centos\"\r\nID_LIKE=\"rhel fedora\"\r\nVERSION_ID=\"7\"\r\nPRETTY_NAME=\"CentOS Linux 7 (Core)\"\r\nANSI_COLOR=\"0;31\"\r\nCPE_NAME=\"cpe:/o:centos:centos:7\"\r\nHOME_URL=\"https://www.centos.org/\"\r\nBUG_REPORT_URL=\"https://bugs.centos.org/\"\r\n\r\nCENTOS_MANTISBT_PROJECT=\"CentOS-7\"\r\nCENTOS_MANTISBT_PROJECT_VERSION=\"7\"\r\nREDHAT_SUPPORT_PRODUCT=\"centos\"\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\r\n$ uname -a\r\n# Linux <hostname> 3.10.0-1160.99.1.el7.x86_64 kubernetes/kubernetes#1 SMP Wed Sep 13 14:19:20 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n\r\n\r\n</details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n</details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n</details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n</details>\r\n",
      "solution": "When logs are tailed kubectl uses the watcher utility, which relies on go channels, in this case a buffered go channel. We can see in the below link that the default size for the buffer is 100, and this is not currently configurable. Now, normally this would be fine because the logs coming in would be sent out, but because you are generating so much data, the logs are unable to exit the channel buffer at a faster rate than the logs are flowing in to the channel buffer, resulting in negative pressure and the channel to hang.\r\n\r\n[How the log function spawns multiple watchers](https://github.com/kubernetes/kubernetes/blob/00c30941260a27e6929aef84c7fdbc8f1508518c/staging/src/k8s.io/kubectl/pkg/cmd/logs/logs.go#L346-L345)\r\n\r\n[The default buffer size of the channel used by the watchers](https://github.com/kubernetes/kubernetes/blob/7e434682e450e28d36f0ee4787e7b4672e8eb255/staging/src/k8s.io/apimachinery/pkg/watch/watch.go#L51-L53)\r\n\r\nIf you run the command causing the issue with `-v=9` flag that may give us a clearer indication that this is what is happening. Could you do that and post the resulting logs for that message (not the logs that are being output but the ones being returned as a result of the verbosity flag)?\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nI spent some time on this today and it was hard to reproduce. I think, checking the kubelet and api server logs if that happens can enlighten the issue. I'm not sure this issue is definitely happening on kubectl and we need to eliminate the other components first. ",
      "labels": [
        "kind/bug",
        "needs-sig",
        "sig/cli",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2024-01-23T11:28:01Z",
      "closed_at": "2026-01-28T17:29:51Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1548",
      "comments_count": 30
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1582,
      "title": "pkg/drain: Provide hooks for intermediate eviction errors",
      "problem": "## What would you like to be added\r\n\r\nCurrently https://pkg.go.dev/k8s.io/kubectl/pkg/drain offers hooks that callers can use to get notified when a Pod eviction is starting or finished. [For example](https://github.com/kubernetes/kubectl/blob/v0.29.3/pkg/drain/drain.go#L94-L95):\r\n\r\n```go\r\n// OnPodDeletionOrEvictionFinished is called when a pod is eviction/deletetion is failed;\r\n// for printing progress output\r\nOnPodDeletionOrEvictionFinished func(pod *corev1.Pod, usingEviction bool, err error)\r\n```\r\n\r\nThis method signature (and Godoc) made me think my code will get intermediate errors in the `err` variable that I can use to make decisions, or print progress.\r\n\r\nHowever, that did not turn out to be the case.\r\n\r\nCurrently, the  `OnPodDeletionOrEvictionFinished()` **is only called when**:\r\n\r\n1. `Evict` or `Delete Pod` call succeeded + polling for Pod to be deleted succeeds.\r\n2. `Evict` or `Delete Pod` call succeeded + there was an error in polling for waiting Pod to be fully deleted.\r\n\r\n## Proposal\r\n\r\nExtend the semantics around `OnPodDeletionOrEvictionFinished()` hook to include intermediate errors.\r\n\r\n## Why is this needed\r\n\r\n1. If my code gets an `err` that is of `code=429` (indicating PDB disallowing eviction), I can `cancel()` the `ctx` I passed to `RunNodeDrain(ctx)`, because waiting for many more minutes [and retrying in 5s](https://github.com/kubernetes/kubectl/blob/v0.29.3/pkg/drain/drain.go#L320) likely won't fix the issue.\r\n\r\n2. The documentation of `OnPodDeletionOrEvictionFinished()` hook says `for printing progress output`. However, if the drain operation is currently failing due to a single Pod refusing to Evict (say, due to PDB), this can provide the callers with an option to print progress\r\n\r\n   Currently, this hook only works for printing progress if the eviction is successful, which makes it basically the same as <nobr>`OnPodDeletedOrEvicted func(pod *corev1.Pod, usingEviction bool)`</nobr> hook.\r\n\r\n/kind feature\r\n/sig cli\r\n/cc @adilGhaffarDev\r\n/cc soltysh",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/feature",
        "sig/cli",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2024-04-05T04:15:21Z",
      "closed_at": "2026-01-28T17:25:36Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1582",
      "comments_count": 11
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1586,
      "title": "WarningHandler cannot be configured when applying",
      "problem": "<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\r\n\r\nIf the matter is security related, please disclose it privately via https://kubernetes.io/security/\r\n-->\r\n\r\n**What happened**:\r\nIn Argo project, kubectl client is used to apply the resources and WarningHandler need to be configured in order to expose warning when applying the resources. However, I think WarningHandler cannot be configured when using kubctl client.\r\n\r\n**What you expected to happen**:\r\nWarningHandler should be configured when using kubectl client.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\n**Anything else we need to know?**:\r\nCustomized WarningHandler will be configured in ApplyOptions and\r\nin this [line](https://github.com/kubernetes/kubectl/blob/master/pkg/cmd/apply/apply.go#L511), it generate the objects but it does not contains the WarningHandler in the ApplyOptions. And the client in generated object is used to communicate with API server in this [line](https://github.com/kubernetes/kubectl/blob/master/pkg/cmd/apply/apply.go#L527), this [line](https://github.com/kubernetes/kubectl/blob/master/pkg/cmd/apply/apply.go#L565) and this [line](https://github.com/kubernetes/kubectl/blob/master/pkg/cmd/apply/apply.go#L580). So it means, whatever the user configure the WarningHandler in ApplyOptions, it is not used.\r\n\r\n**Environment**:\r\n- Kubernetes client and server versions (use `kubectl version`):\r\n- Cloud provider or hardware configuration:\r\n- OS (e.g: `cat /etc/os-release`):\r\n\r\n",
      "solution": "Thanks for raising the issue.\r\nI'm not sure if this is a bug, did you read this article? https://kubernetes.io/blog/2020/09/03/warnings/ .\r\nMaybe you can propose a feature to maybe customize the behavior based on user input.\n\n---\n\nHi @ah8ad3 and @brianpursley, thank you for your reply and I am sorry if I did not make myself clear. In [rest.config](https://github.com/kubernetes/client-go/blob/master/rest/config.go#L128), `warningHandler` could be config and a [dynamicClient](https://github.com/kubernetes/client-go/blob/master/dynamic/simple.go#L71) could be generated by this config. And, In kubectl side, `dynamicClient` is one of the fields in [ApplyOptions](https://github.com/kubernetes/kubectl/blob/master/pkg/cmd/apply/apply.go#L105), which will be used for [Run](https://github.com/kubernetes/kubectl/blob/master/pkg/cmd/apply/apply.go#L495) command. So the problem is, a `warningWriter` has been set as `warningHandler` in rest.config and used to generate the `dynamicClient`, and the client has been passed in `applyoptions` and calling the `Run` function. But based on my debug and research as I described previously, the `warningWriter` is not used when actually communicate with kubeAPI so I cannot use the warning message. Also, @brianpursley I did not look very deep on the code, not too sure the private `warningWriter` is the root cause for this. Hope it makes sense and look forward to your reply. Thank you.\n\n---\n\nHi @mpuckett159, thank you for looking at it. To be honest, currently I do not know the root cause and how to fix it. The simple way to describe this issue is that I believe [this](https://github.com/kubernetes/kubectl/blob/master/pkg/cmd/apply/apply.go#L580) is the real place to apply the resources but it does not use the `warningHandler`/`warningWriter` which exists in `DynamicClient` in the `o *ApplyOptions`. Hope it makes sense.",
      "labels": [
        "kind/feature",
        "priority/backlog",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2024-04-17T10:27:45Z",
      "closed_at": "2026-01-28T17:23:22Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1586",
      "comments_count": 20
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1811,
      "title": "Wait for Debug Container to be Running but don't attach",
      "problem": "**What would you like to be added**:\nWhen running `kubectl debug`, it would be useful to create a debug pod and wait for it to be running, but not attach.\n\nCurrently you can use `--attach=true` to wait for the container to be running and attach, or use `--attach=false` to return immediately. As far as I can tell there's no midpoint where you wait for the container to be running before returning but don't attach.\n\nSome other `kubectl` commands have a `--wait` for this kind of functionality.\n\n**Why is this needed**:\nWhen automating against `kubectl debug`, it's much easier to deploy the container using one command, then execute subsequent commands with multiple `kubectl exec`s rather than trying to do the whole process in a single command.\n\nFor example, one of our runbooks takes the following steps:\n1) Deploy JDK container with `kubectl debug` attaching to a container running the JRE.\n2) Collect various pieces of troubleshooting information.\n    1) These are mostly but not always written to the filesystem of the JRE container.\n3) Use `kubectl cp` to copy artifacts down from the containers.\n\nWe have worked around this limitations using the below script, but it would be nice if this functionality was built in to `kubectl`.\n\n```bash\nNAMESPACE=\"$1\"\nPOD=\"$2\"\n\nJDK_IMAGE=\"public.ecr.aws/docker/library/eclipse-temurin:25_36-jdk\"\nTARGET_CONTAINER=\"$(kubectl get pods -n \"$NAMESPACE\" \"$POD\" -o=jsonpath='{.spec.containers[0].name}')\"\nDEBUG_CONTAINER=\"debug-$(date '+%s')\"\n\nkubectl debug -n \"$NAMESPACE\" --profile=general --image=\"$JDK_IMAGE\" --target \"$TARGET_CONTAINER\" -c \"$DEBUG_CONTAINER\" \"$POD\" -- sleep infinity\n\nkubectl wait --for=jsonpath=\"{.status.ephemeralContainerStatuses[?(@.name==\\\"$DEBUG_CONTAINER\\\")].state.running}\" -n \"$NAMESPACE\" pod \"$POD\"\n\nkubectl exec -itn \"$NAMESPACE\" \"$POD\" -c \"$DEBUG_CONTAINER\" -- <debug command>\n```",
      "solution": "Attaching to a pod definitely requires the pod to be running. On the other hand, this is not mandatory when `--attach` is false. That would mean introducing a new flag to customize the behavior. However, we would be reluctant to introduce a new flag unless it is unavoidable. In my opinion, scripting this with a series of kubectl commands (as you did in your workaround) is the best choice.\n\n---\n\n/close\nAgree with Arda that this is an acceptable solution to this problem.\n\n---\n\n@mpuckett159: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubectl/issues/1811#issuecomment-3812678585):\n\n>/close\n>Agree with Arda that this is an acceptable solution to this problem.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>",
      "labels": [
        "kind/feature",
        "needs-triage"
      ],
      "created_at": "2026-01-01T15:43:50Z",
      "closed_at": "2026-01-28T17:16:40Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1811",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1437,
      "title": "Fix or reduce frequency or switch off perma-failing jobs",
      "problem": "Data as of June 3rd, 9:00 PM Eastern. Latest data can be found here:\r\nhttp://storage.googleapis.com/k8s-metrics/failures-latest.json\r\n\r\nAdditional Context:\r\nhttps://github.com/kubernetes/test-infra/issues/18600\r\n\r\n| CI Job | Days Failed |\r\n| --- | --- |\r\n| ci-kubernetes-e2e-gce-new-master-gci-kubectl-skew  | 66  |                                            \r\n| ci-kubernetes-e2e-gce-stable1-latest-gci-kubectl-skew  | 66  |                                            \r\n| ci-kubernetes-e2e-gce-stable2-stable1-gci-kubectl-skew  |  36  |                                            \r\n",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "priority/important-soon",
        "needs-triage"
      ],
      "created_at": "2023-06-05T02:05:54Z",
      "closed_at": "2026-01-22T20:01:59Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1437",
      "comments_count": 15
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1780,
      "title": "kubectl debug: change default profile and remove legacy profile",
      "problem": "At v1.32, the `legacy` profile is the default profile for `kubectl debug`.\nHowever, a deprecation warning message is displayed after kubernetes/kubernetes#127230 has been merged and the [user document](https://kubernetes.io/docs/tasks/debug/debug-application/debug-running-pod/#debugging-profiles) says that `legacy` profile is deprecated as follows:\n\n> If you don't specify --profile, the legacy profile is used by default, but it is planned to be deprecated in the near future. So it is recommended to use other profiles such as general.\n\nI think we should make some profile(e.g. `general`) default and remove the `legacy` profile in the near future.\n\nAccording to [Deprecating a feature or behavior](https://kubernetes.io/docs/reference/using-api/deprecation-policy/#deprecating-a-feature-or-behavior):\n\n> Rule kubernetes/kubernetes#7: Deprecated behaviors must function for no less than 1 year after their announced deprecation.\n\nTherefore the path seems to be as follows:\n- Make `general` profile default: v1.33 or v1.34\n- Remove `legacy` profile: At the earliest v1.35, or later\n\n/sig cli",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "priority/important-longterm",
        "sig/cli",
        "triage/accepted"
      ],
      "created_at": "2025-03-10T08:49:32Z",
      "closed_at": "2026-01-16T21:01:18Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1780",
      "comments_count": 22
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1800,
      "title": "Unexpected EOF error when trying to create a job from a cronjob in kubectl 1.32+",
      "problem": "**What happened**: With kubectl client 1.32+ and server 1.33.5, running `kubectl create job test --from=cronjob/any-cronjob` fails with an error such as \n```\nerror: failed to create job: the object provided is unrecognized (must be of type Job): unexpected EOF (6b3873000a0f0a0862617463682f763112034a6f6212efbfbd0e0aefbfbd ...)\n```\n\n**What you expected to happen**: This command should create a `Job` resource.\n\n**How to reproduce it (as minimally and precisely as possible)**: Attempt to create a `Job` from a `CronJob` via cli with the specified kube client and server versions.\n\n**Anything else we need to know?**:\nMy AI assistant thinks it is due to protobuf serialization failure and may be related to the Go version used to build these. 1.22.x is used for 1.31.13, where this works, and 1.23.x is used for 1.32.\n\n**Environment**:\n- Kubernetes client and server versions (use `kubectl version`): Client: 1.32+ (tested on 1.32, 1.33, 1.34). Server: v1.33.5-eks-3cfe0ce\n- Cloud provider or hardware configuration: AWS EKS on EC2\n- OS (e.g: `cat /etc/os-release`): MacOS Sequoia 15.7.2 on M1 darwin/arm64\n\n",
      "solution": "> **What happened**: With kubectl client 1.32+ and server 1.33.5, running `kubectl create job test --from=cronjob/any-cronjob` fails with an error such as\n> \n> ```\n> error: failed to create job: the object provided is unrecognized (must be of type Job): unexpected EOF (6b3873000a0f0a0862617463682f763112034a6f6212efbfbd0e0aefbfbd ...)\n> ```\n\nCan you try re-running your command with increased verbosity, like `-v=9` and provide it here for further debugging? \n\n> **How to reproduce it (as minimally and precisely as possible)**: Attempt to create a `Job` from a `CronJob` via cli with the specified kube client and server versions.\n> \n> **Anything else we need to know?**: My AI assistant thinks it is due to protobuf serialization failure and may be related to the Go version used to build these. 1.22.x is used for 1.31.13, where this works, and 1.23.x is used for 1.32.\n\nI've personally tried with kubectl 1.32 (latest available in `release-1.32` branch) compiled with go1.25.5 and go1.23.12 against 1.33.4 and 1.35.0-rc.0 clusters. In all cases it seems to be working just fine. Protobuf does not change between golang versions, so I don't think that might be the problem here. \n\n> * Kubernetes client and server versions (use `kubectl version`): Client: 1.32+ (tested on 1.32, 1.33, 1.34). Server: v1.33.5-eks-3cfe0ce\n> \n> * Cloud provider or hardware configuration: AWS EKS on EC2\n\nSince your particular problem seems to be related with AWS EKS I'd consider opening an issue with them. It's possible their binary might be different that what we provide. Thus might be causing the above problems. \n\n---\n\nThanks for looking into it and getting back to me!\n\nI fed these logs to my AI assistant and this is what it said:\n```\nRoot Cause Identified:\n\nThe hex dump shows kubectl is sending a Protobuf-encoded request (Content-Type: application/vnd.kubernetes.protobuf) but the Job manifest is truncated or malformed during serialization. The server responds with:\n\n\"the object provided is unrecognized (must be of type Job): unexpected EOF\"\n\nThe binary payload ends abruptly at offset 0x4bc (line 000004b0), indicating the Protobuf message is incomplete.\n\nWhy This Happens:\n\nkubectl 1.32.10 + Protobuf encoding bug \u2013 Newer kubectl uses Protobuf by default for efficiency, but certain field combinations (especially large annotations or labels from ArgoCD/Kyverno policies) can trigger serialization bugs.\n\nLarge annotation set \u2013 Your CronJob has massive annotations from:\n\nArgoCD tracking (kubectl.kubernetes.io/last-applied-configuration contains full YAML)\nKyverno policies (policies.kyverno.io/last-applied-patches)\nCustom bs/* tags (13+ annotations)\nWhen kubectl copies these into the Job template, the Protobuf encoder may hit an internal buffer limit or encoding error.\n```\n\nPerhaps it's an issue with the annotations on my cronjob.\n\nHere are the verbose logs:\n\n```\n> kubectl create job --from=cronjob/my-cronjob test -v9\nI1211 14:54:10.693819   74321 loader.go:402] Config loaded from file:  /Users/bscaleb/.kube/config\nI1211 14:54:10.694417   74321 envvar.go:172] \"Feature gate default state\" feature=\"ClientsAllowCBOR\" enabled=false\nI1211 14:54:10.694425   74321 envvar.go:172] \"Feature gate default state\" feature=\"ClientsPreferCBOR\" enabled=false\nI1211 14:54:10.694435   74321 envvar.go:172] \"Feature gate default state\" feature=\"InformerResourceVersion\" enabled=false\nI1211 14:54:10.694438   74321 envvar.go:172] \"Feature gate default state\" feature=\"WatchListClient\" enabled=false\nI1211 14:54:10.696854   74321 discovery_client.go:253] \"Request Body\" body=\"\"\nI1211 14:54:10.696928   74321 round_trippers.go:473] curl -v -XGET  -H \"Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json\" -H \"User-Agent: kubectl/v1.32.10 (darwin/arm64) kubernetes/d26b18b\" -H \"Authorization: Bearer <masked>\" 'https://localhost:62780/api?timeout=32s'\nI1211 14:54:10.698742   74321 round_trippers.go:502] HTTP Trace: DNS Lookup for localhost resolved to [{::1 } {127.0.0.1 }]\nI1211 14:54:10.699089   74321 round_trippers.go:515] HTTP Trace: Dial to tcp:[::1]:62780 failed: dial tcp [::1]:62780: connect: connection refused\nI1211 14:54:10.699390   74321 round_trippers.go:517] HTTP Trace: Dial to tcp:127.0.0.1:62780 succeed\nI1211 14:54:10.791978   74321 round_trippers.go:560] GET https://localhost:62780/api?timeout=32s 200 OK in 95 milliseconds\nI1211 14:54:10.792002   74321 round_trippers.go:577] HTTP Statistics: DNSLookup 0 ms Dial 0 ms TLSHandshake 2 ms ServerProcessing 89 ms Duration 95 ms\nI1211 14:54:10.792008   74321 round_trippers.go:584] Response Headers:\nI1211 14:54:10.792013   74321 round_trippers.go:587]     Etag: \"B71838287F84440751EF3E5744A7306A573352C87E530D4A937F4A2BCBB23B71C20457F1B57490979D43B5FBD3071512F6607C7FED9E4AC951494FE8201B8C95\"\nI1211 14:54:10.792016   74321 round_trippers.go:587]     X-Kubernetes-Pf-Prioritylevel-Uid: 44f4a016-cddf-476d-a69a-7364c3ffb845\nI1211 14:54:10.792020   74321 round_trippers.go:587]     X-Varied-Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json\nI1211 14:54:10.792023   74321 round_trippers.go:587]     Audit-Id: 8d014e43-3fc0-4841-93ba-ce217afe3fd9\nI1211 14:54:10.792026   74321 round_trippers.go:587]     Cache-Control: public\nI1211 14:54:10.792028   74321 round_trippers.go:587]     Content-Type: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList\nI1211 14:54:10.792031   74321 round_trippers.go:587]     Date: Thu, 11 Dec 2025 19:54:10 GMT\nI1211 14:54:10.792033   74321 round_trippers.go:587]     Vary: Accept\nI1211 14:54:10.792036   74321 round_trippers.go:587]     X-From-Cache: 1\nI1211 14:54:10.792043   74321 round_trippers.go:587]     X-Kubernetes-Pf-Flowschema-Uid: 1b99ce95-1c19-4f69-aa96-2f2f2808daf3\nI1211 14:54:10.793020   74321 discovery_client.go:253] \"Response Body\" body=<\n\t{\"kind\":\"APIGroupDiscoveryList\",\"apiVersion\":\"apidiscovery.k8s.io/v2\",\"metadata\":{},\"items\":[{\"metadata\":{\"creationTimestamp\":null},\"versions\":[{\"version\":\"v1\",\"resources\":[{\"resource\":\"bindings\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"Binding\"},\"scope\":\"Namespaced\",\"singularResource\":\"binding\",\"verbs\":[\"create\"]},{\"resource\":\"componentstatuses\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"ComponentStatus\"},\"scope\":\"Cluster\",\"singularResource\":\"componentstatus\",\"verbs\":[\"get\",\"list\"],\"shortNames\":[\"cs\"]},{\"resource\":\"configmaps\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"ConfigMap\"},\"scope\":\"Namespaced\",\"singularResource\":\"configmap\",\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"get\",\"list\",\"patch\",\"update\",\"watch\"],\"shortNames\":[\"cm\"]},{\"resource\":\"endpoints\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"Endpoints\"},\"scope\":\"Namespaced\",\"singularResource\":\"endpoints\",\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"get\",\"list\",\"patch\",\"update\",\"watch\"],\"shortNames\":[\"ep\"]},{\"resource\":\"events\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"Event\"},\"scope\":\"Namespaced\",\"singularResource\":\"event\",\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"get\",\"list\",\"patch\",\"update\",\"watch\"],\"shortNames\":[\"ev\"]},{\"resource\":\"limitranges\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"LimitRange\"},\"scope\":\"Namespaced\",\"singularResource\":\"limitrange\",\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"get\",\"list\",\"patch\",\"update\",\"watch\"],\"shortNames\":[\"limits\"]},{\"resource\":\"namespaces\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"Namespace\"},\"scope\":\"Cluster\",\"singularResource\":\"namespace\",\"verbs\":[\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"],\"shortNames\":[\"ns\"],\"subresources\":[{\"subresource\":\"finalize\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"Namespace\"},\"verbs\":[\"update\"]},{\"subresource\":\"status\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"Namespace\"},\"verbs\":[\"get\",\"patch\",\"update\"]}]},{\"resource\":\"nodes\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"Node\"},\"scope\":\"Cluster\",\"singularResource\":\"node\",\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"get\",\"list\",\"patch\",\"update\",\"watch\"],\"shortNames\":[\"no\"],\"subresources\":[{\"subresource\":\"proxy\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"NodeProxyOptions\"},\"verbs\":[\"create\",\"delete\",\"get\",\"patch\",\"update\"]},{\"subresource\":\"status\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"Node\"},\"verbs\":[\"get\",\"patch\",\"update\"]}]},{\"resource\":\"persistentvolumeclaims\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"PersistentVolumeClaim\"},\"scope\":\"Namespaced\",\"singularResource\":\"persistentvolumeclaim\",\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"get\",\"list\",\"patch\",\"update\",\"watch\"],\"shortNames\":[\"pvc\"],\"subresources\":[{\"subresource\":\"status\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"PersistentVolumeClaim\"},\"verbs\":[\"get\",\"patch\",\"update\"]}]},{\"resource\":\"persistentvolumes\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"PersistentVolume\"},\"scope\":\"Cluster\",\"singularResource\":\"persistentvolume\",\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"get\",\"list\",\"patch\",\"update\",\"watch\"],\"shortNames\":[\"pv\"],\"subresources\":[{\"subresource\":\"status\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"PersistentVolume\"},\"verbs\":[\"get\",\"patch\",\"update\"]}]},{\"resource\":\"pods\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"Pod\"},\"scope\":\"Namespaced\",\"singularResource\":\"pod\",\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"get\",\"list\",\"patch\",\"update\",\"watch\"],\"shortNames\":[\"po\"],\"categories\":[\"all\"],\"subresources\":[{\"subresource\":\"attach\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"PodAttachOptions\"},\"verbs\":[\"create\",\"get\"]},{\"subresource\":\"binding\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"Binding\"},\"verbs\":[\"create\"]},{\"subresource\":\"ephemeralcontainers\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"Pod\"},\"verbs\":[\"get\",\"patch\",\"update\"]},{\"subresource\":\"eviction\",\"responseKind\":{\"group\":\"policy\",\"version\":\"v1\",\"kind\":\"Eviction\"},\"verbs\":[\"create\"]},{\"subresource\":\"exec\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"PodExecOptions\"},\"verbs\":[\"create\",\"get\"]},{\"subresource\":\"log\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"Pod\"},\"verbs\":[\"get\"]},{\"subresource\":\"portforward\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"PodPortForwardOptions\"},\"verbs\":[\"create\",\"get\"]},{\"subresource\":\"proxy\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"PodProxyOptions\"},\"verbs\":[\"create\",\"delete\",\"get\",\"patch\",\"update\"]},{\"subresource\":\"resize\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"Pod\"},\"verbs\":[\"get\",\"patch\",\"update\"]},{\"subresource\":\"status\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"Pod\"},\"verbs\":[\"get\",\"patch\",\"update\"]}]},{\"resource\":\"podtemplates\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"PodTemplate\"},\"scope\":\"Namespaced\",\"singularResource\":\"podtemplate\",\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]},{\"resource\":\"replicationcontrollers\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"ReplicationController\"},\"scope\":\"Namespaced\",\"singularResource\":\"replicationcontroller\",\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"get\",\"list\",\"patch\",\"update\",\"watch\"],\"shortNames\":[\"rc\"],\"categories\":[\"all\"],\"subresources\":[{\"subresource\":\"scale\",\"responseKind\":{\"group\":\"autoscaling\",\"version\":\"v1\",\"kind\":\"Scale\"},\"verbs\":[\"get\",\"patch\",\"update\"]},{\"subresource\":\"status\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"ReplicationController\"},\"verbs\":[\"get\",\"patch\",\"update\"]}]},{\"resource\":\"resourcequotas\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"ResourceQuota\"},\"scope\":\"Namespaced\",\"singularResource\":\"resourcequota\",\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"get\",\"list\",\"patch\",\"update\",\"watch\"],\"shortNames\":[\"quota\"],\"subresources\":[{\"subresource\":\"status\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"ResourceQuota\"},\"verbs\":[\"get\",\"patch\",\"update\"]}]},{\"resource\":\"secrets\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"Secret\"},\"scope\":\"Namespaced\",\"singularResource\":\"secret\",\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]},{\"resource\":\"serviceaccounts\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"ServiceAccount\"},\"scope\":\"Namespaced\",\"singularResource\":\"serviceaccount\",\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"get\",\"list\",\"patch\",\"update\",\"watch\"],\"shortNames\":[\"sa\"],\"subresources\":[{\"subresource\":\"token\",\"responseKind\":{\"group\":\"authentication.k8s.io\",\"version\":\"v1\",\"kind\":\"TokenRequest\"},\"verbs\":[\"create\"]}]},{\"resource\":\"services\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"Service\"},\"scope\":\"Namespaced\",\"singularResource\":\"service\",\"verbs\":[\"create\",\"delete\",\"deletecollection\",\"get\",\"list\",\"patch\",\"update\",\"watch\"],\"shortNames\":[\"svc\"],\"categories\":[\"all\"],\"subresources\":[{\"subresource\":\"proxy\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"ServiceProxyOptions\"},\"verbs\":[\"create\",\"delete\",\"get\",\"patch\",\"update\"]},{\"subresource\":\"status\",\"responseKind\":{\"group\":\"\",\"version\":\"\",\"kind\":\"Service\"},\"verbs\":[\"get\",\"patch\",\"update\"]}]}],\"freshness\":\"Current\"}]}]}\n >\nI1211 14:54:10.794125   74321 discovery_client.go:319] \"Request Body\" body=\"\"\nI1211 14:54:10.794176   74321 round_trippers.go:473] curl -v -XGET  -H \"Authorization: Bearer <masked>\" -H \"Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json\" -H \"User-Agent: kubectl/v1.32.10 (darwin/arm64) kubernetes/d26b18b\" 'https://localhost:62780/apis?timeout=32s'\nI1211 14:54:10.890288   74321 round_trippers.go:560] GET https://localhost:62780/apis?timeout=32s 200 OK in 96 milliseconds\nI1211 14:54:10.890313   74321 round_trippers.go:577] HTTP Statistics: GetConnection 0 ms ServerProcessing 95 ms Duration 96 ms\nI1211 14:54:10.890320   74321 round_trippers.go:584] Response Headers:\nI1211 14:54:10.890327   74321 round_trippers.go:587]     Audit-Id: 4a048f5d-66d5-4d6c-b814-4e2bf7e1b66d\nI1211 14:54:10.890330   74321 round_trippers.go:587]     Cache-Control: public\nI1211 14:54:10.890334   74321 round_trippers.go:587]     Content-Type: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList\nI1211 14:54:10.890338   74321 round_trippers.go:587]     Vary: Accept\nI1211 14:54:10.890341   74321 round_trippers.go:587]     X-From-Cache: 1\nI1211 14:54:10.890343   74321 round_trippers.go:587]     Date: Thu, 11 Dec 2025 19:54:11 GMT\nI1211 14:54:10.890346   74321 round_trippers.go:587]     Etag: \"D27DE2C4956DF0CE2DE5A6DB3DB9F17C2843B55BFC35DD8980D05C6D0B0BB2B1E15F3105F88DF0460169F02938C997DC0B83240A3FE6A870304D5DE63881F9F5\"\nI1211 14:54:10.890349   74321 round_trippers.go:587]     X-Kubernetes-Pf-Flowschema-Uid: 1b99ce95-1c19-4f69-aa96-2f2f2808daf3\nI1211 14:54:10.890351   74321 round_trippers.go:587]     X-Kubernetes-Pf-Prioritylevel-Uid: 44f4a016-cddf-476d-a69a-7364c3ffb845\nI1211 14:54:10.890354   74321 round_trippers.go:587]     X-Varied-Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json\nI1211 14:54:10.893967   74321 discovery_client.go:319] \"Response Body\" body=\"{\\\"kind\\\":\\\"APIGroupDiscoveryList\\\",\\\"apiVersion\\\":\\\"apidiscovery.k8s.io/v2\\\",\\\"metadata\\\":{},\\\"items\\\":[{\\\"metadata\\\":{\\\"name\\\":\\\"apiregistration.k8s.io\\\",\\\"creationTimestamp\\\":null},\\\"versions\\\":[{\\\"version\\\":\\\"v1\\\",\\\"resources\\\":[{\\\"resource\\\":\\\"apiservices\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"APIService\\\"},\\\"scope\\\":\\\"Cluster\\\",\\\"singularResource\\\":\\\"apiservice\\\",\\\"verbs\\\":[\\\"create\\\",\\\"delete\\\",\\\"deletecollection\\\",\\\"get\\\",\\\"list\\\",\\\"patch\\\",\\\"update\\\",\\\"watch\\\"],\\\"categories\\\":[\\\"api-extensions\\\"],\\\"subresources\\\":[{\\\"subresource\\\":\\\"status\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"APIService\\\"},\\\"verbs\\\":[\\\"get\\\",\\\"patch\\\",\\\"update\\\"]}]}],\\\"freshness\\\":\\\"Current\\\"}]},{\\\"metadata\\\":{\\\"name\\\":\\\"apps\\\",\\\"creationTimestamp\\\":null},\\\"versions\\\":[{\\\"version\\\":\\\"v1\\\",\\\"resources\\\":[{\\\"resource\\\":\\\"controllerrevisions\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"ControllerRevision\\\"},\\\"scope\\\":\\\"Namespaced\\\",\\\"singularResource\\\":\\\"controllerrevision\\\",\\\"verbs\\\":[\\\"create\\\",\\\"delete\\\",\\\"deletecollection\\\",\\\"get\\\",\\\"list\\\",\\\"patch\\\",\\\"update\\\",\\\"watch\\\"]},{\\\"resource\\\":\\\"daemonsets\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"DaemonSet\\\"},\\\"scope\\\":\\\"Namespaced\\\",\\\"singularResource\\\":\\\"daemonset\\\",\\\"verbs\\\":[\\\"create\\\",\\\"delete\\\",\\\"deletecollection\\\",\\\"get\\\",\\\"list\\\",\\\"patch\\\",\\\"update\\\",\\\"watch\\\"],\\\"shortNames\\\":[\\\"ds\\\"],\\\"categories\\\":[\\\"all\\\"],\\\"subresources\\\":[{\\\"subresource\\\":\\\"status\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"DaemonSet\\\"},\\\"verbs\\\":[\\\"get\\\",\\\"patch\\\",\\\"update\\\"]}]},{\\\"resource\\\":\\\"deployments\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"Deployment\\\"},\\\"scope\\\":\\\"Namespaced\\\",\\\"singularResource\\\":\\\"deployment\\\",\\\"verbs\\\":[\\\"create\\\",\\\"delete\\\",\\\"deletecollection\\\",\\\"get\\\",\\\"list\\\",\\\"patch\\\",\\\"update\\\",\\\"watch\\\"],\\\"shortNames\\\":[\\\"deploy\\\"],\\\"categories\\\":[\\\"all\\\"],\\\"subresources\\\":[{\\\"subresource\\\":\\\"scale\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"autoscaling\\\",\\\"version\\\":\\\"v1\\\",\\\"kind\\\":\\\"Scale\\\"},\\\"verbs\\\":[\\\"get\\\",\\\"patch\\\",\\\"update\\\"]},{\\\"subresource\\\":\\\"status\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"Deployment\\\"},\\\"verbs\\\":[\\\"get\\\",\\\"patch\\\",\\\"update\\\"]}]},{\\\"resource\\\":\\\"replicasets\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"ReplicaSet\\\"},\\\"scope\\\":\\\"Namespaced\\\",\\\"singularResource\\\":\\\"replicaset\\\",\\\"verbs\\\":[\\\"create\\\",\\\"delete\\\",\\\"deletecollection\\\",\\\"get\\\",\\\"list\\\",\\\"patch\\\",\\\"update\\\",\\\"watch\\\"],\\\"shortNames\\\":[\\\"rs\\\"],\\\"categories\\\":[\\\"all\\\"],\\\"subresources\\\":[{\\\"subresource\\\":\\\"scale\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"autoscaling\\\",\\\"version\\\":\\\"v1\\\",\\\"kind\\\":\\\"Scale\\\"},\\\"verbs\\\":[\\\"get\\\",\\\"patch\\\",\\\"update\\\"]},{\\\"subresource\\\":\\\"status\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"ReplicaSet\\\"},\\\"verbs\\\":[\\\"get\\\",\\\"patch\\\",\\\"update\\\"]}]},{\\\"resource\\\":\\\"statefulsets\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"StatefulSet\\\"},\\\"scope\\\":\\\"Namespaced\\\",\\\"singularResource\\\":\\\"statefulset\\\",\\\"verbs\\\":[\\\"create\\\",\\\"delete\\\",\\\"deletecollection\\\",\\\"get\\\",\\\"list\\\",\\\"patch\\\",\\\"update\\\",\\\"watch\\\"],\\\"shortNames\\\":[\\\"sts\\\"],\\\"categories\\\":[\\\"all\\\"],\\\"subresources\\\":[{\\\"subresource\\\":\\\"scale\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"autoscaling\\\",\\\"version\\\":\\\"v1\\\",\\\"kind\\\":\\\"Scale\\\"},\\\"verbs\\\":[\\\"get\\\",\\\"patch\\\",\\\"update\\\"]},{\\\"subresource\\\":\\\"status\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"StatefulSet\\\"},\\\"verbs\\\":[\\\"get\\\",\\\"patch\\\",\\\"update\\\"]}]}],\\\"freshness\\\":\\\"Current\\\"}]},{\\\"metadata\\\":{\\\"name\\\":\\\"events.k8s.io\\\",\\\"creationTimestamp\\\":null},\\\"versions\\\":[{\\\"version\\\":\\\"v1\\\",\\\"resources\\\":[{\\\"resource\\\":\\\"events\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"Event\\\"},\\\"scope\\\":\\\"Namespaced\\\",\\\"singularResource\\\":\\\"event\\\",\\\"verbs\\\":[\\\"create\\\",\\\"delete\\\",\\\"deletecollection\\\",\\\"get\\\",\\\"list\\\",\\\"patch\\\",\\\"update\\\",\\\"watch\\\"],\\\"shortNames\\\":[\\\"ev\\\"]}],\\\"freshness\\\":\\\"Current\\\"}]},{\\\"metadata\\\":{\\\"name\\\":\\\"authentication.k8s.io\\\",\\\"creationTimestamp\\\":null},\\\"versions\\\":[{\\\"version\\\":\\\"v1\\\",\\\"resources\\\":[{\\\"resource\\\":\\\"selfsubjectreviews\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"SelfSubjectReview\\\"},\\\"scope\\\":\\\"Cluster\\\",\\\"singularResource\\\":\\\"selfsubjectreview\\\",\\\"verbs\\\":[\\\"create\\\"]},{\\\"resource\\\":\\\"tokenreviews\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"TokenReview\\\"},\\\"scope\\\":\\\"Cluster\\\",\\\"singularResource\\\":\\\"tokenreview\\\",\\\"verbs\\\":[\\\"create\\\"]}],\\\"freshness\\\":\\\"Current\\\"}]},{\\\"metadata\\\":{\\\"name\\\":\\\"authorization.k8s.io\\\",\\\"creationTimestamp\\\":null},\\\"versions\\\":[{\\\"version\\\":\\\"v1\\\",\\\"resources\\\":[{\\\"resource\\\":\\\"localsubjectaccessreviews\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"LocalSubjectAccessReview\\\"},\\\"scope\\\":\\\"Namespaced\\\",\\\"singularResource\\\":\\\"localsubjectaccessreview\\\",\\\"verbs\\\":[\\\"create\\\"]},{\\\"resource\\\":\\\"selfsubjectaccessreviews\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"SelfSubjectAccessReview\\\"},\\\"scope\\\":\\\"Cluster\\\",\\\"singularResource\\\":\\\"selfsubjectaccessreview\\\",\\\"verbs\\\":[\\\"create\\\"]},{\\\"resource\\\":\\\"selfsubjectrulesreviews\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"SelfSubjectRulesReview\\\"},\\\"scope\\\":\\\"Cluster\\\",\\\"singularResource\\\":\\\"selfsubjectrulesreview\\\",\\\"verbs\\\":[\\\"create\\\"]},{\\\"resource\\\":\\\"subjectaccessreviews\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"SubjectAccessReview\\\"},\\\"scope\\\":\\\"Cluster\\\",\\\"singularResource\\\":\\\"subjectaccessreview\\\",\\\"verbs\\\":[\\\"create\\\"]}],\\\"freshness\\\":\\\"Current\\\"}]},{\\\"metadata\\\":{\\\"name\\\":\\\"autoscaling\\\",\\\"creationTimestamp\\\":null},\\\"versions\\\":[{\\\"version\\\":\\\"v2\\\",\\\"resources\\\":[{\\\"resource\\\":\\\"horizontalpodautoscalers\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"HorizontalPodAutoscaler\\\"},\\\"scope\\\":\\\"Namespaced\\\",\\\"singularResource\\\":\\\"horizontalpodautoscaler\\\",\\\"verbs\\\":[\\\"create\\\",\\\"delete\\\",\\\"deletecollection\\\",\\\"get\\\",\\\"list\\\",\\\"patch\\\",\\\"update\\\",\\\"watch\\\"],\\\"shortNames\\\":[\\\"hpa\\\"],\\\"categories\\\":[\\\"all\\\"],\\\"subresources\\\":[{\\\"subresource\\\":\\\"status\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"HorizontalPodAutoscaler\\\"},\\\"verbs\\\":[\\\"get\\\",\\\"patch\\\",\\\"update\\\"]}]}],\\\"freshness\\\":\\\"Current\\\"},{\\\"version\\\":\\\"v1\\\",\\\"resources\\\":[{\\\"resource\\\":\\\"horizontalpodautoscalers\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"HorizontalPodAutoscaler\\\"},\\\"scope\\\":\\\"Namespaced\\\",\\\"singularResource\\\":\\\"horizontalpodautoscaler\\\",\\\"verbs\\\":[\\\"create\\\",\\\"delete\\\",\\\"deletecollection\\\",\\\"get\\\",\\\"list\\\",\\\"patch\\\",\\\"update\\\",\\\"watch\\\"],\\\"shortNames\\\":[\\\"hpa\\\"],\\\"categories\\\":[\\\"all\\\"],\\\"subresources\\\":[{\\\"subresource\\\":\\\"status\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"HorizontalPodAutoscaler\\\"},\\\"verbs\\\":[\\\"get\\\",\\\"patch\\\",\\\"update\\\"]}]}],\\\"freshness\\\":\\\"Current\\\"}]},{\\\"metadata\\\":{\\\"name\\\":\\\"batch\\\",\\\"creationTimestamp\\\":null},\\\"versions\\\":[{\\\"version\\\":\\\"v1\\\",\\\"resources\\\":[{\\\"resource\\\":\\\"cronjobs\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"CronJob\\\"},\\\"scope\\\":\\\"Namespaced\\\",\\\"singularResource\\\":\\\"cronjob\\\",\\\"verbs\\\":[\\\"create\\\",\\\"delete\\\",\\\"deletecollection\\\",\\\"get\\\",\\\"list\\\",\\\"patch\\\",\\\"update\\\",\\\"watch\\\"],\\\"shortNames\\\":[\\\"cj\\\"],\\\"categories\\\":[\\\"all\\\"],\\\"subresources\\\":[{\\\"subresource\\\":\\\"status\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"CronJob\\\"},\\\"verbs\\\":[\\\"get\\\",\\\"patch\\\",\\\"update\\\"]}]},{\\\"resource\\\":\\\"jobs\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"Job\\\"},\\\"scope\\\":\\\"Namespaced\\\",\\\"singularResource\\\":\\\"job\\\",\\\"verbs\\\":[\\\"create\\\",\\\"delete\\\",\\\"deletecollection\\\",\\\"get\\\",\\\"list\\\",\\\"patch\\\",\\\"update\\\",\\\"watch\\\"],\\\"categories\\\":[\\\"all\\\"],\\\"subresources\\\":[{\\\"subresource\\\":\\\"status\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"Job\\\"},\\\"verbs\\\":[\\\"get\\\",\\\"patch\\\",\\\"update\\\"]}]}],\\\"freshness\\\":\\\"Current\\\"}]},{\\\"metadata\\\":{\\\"name\\\":\\\"certificates.k8s.io\\\",\\\"creationTimestamp\\\":null},\\\"versions\\\":[{\\\"version\\\":\\\"v1\\\",\\\"resources\\\":[{\\\"resource\\\":\\\"certificatesigningrequests\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"CertificateSigningRequest\\\"},\\\"scope\\\":\\\"Cluster\\\",\\\"singularResource\\\":\\\"certificatesigningrequest\\\",\\\"verbs\\\":[\\\"create\\\",\\\"delete\\\",\\\"deletecollection\\\",\\\"get\\\",\\\"list\\\",\\\"patch\\\",\\\"update\\\",\\\"watch\\\"],\\\"shortNames\\\":[\\\"csr\\\"],\\\"subresources\\\":[{\\\"subresource\\\":\\\"approval\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"CertificateSigningRequest\\\"},\\\"verbs\\\":[\\\"get\\\",\\\"patch\\\",\\\"update\\\"]},{\\\"subresource\\\":\\\"status\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"CertificateSigningRequest\\\"},\\\"verbs\\\":[\\\"get\\\",\\\"patch\\\",\\\"update\\\"]}]}],\\\"freshness\\\":\\\"Current\\\"}]},{\\\"metadata\\\":{\\\"name\\\":\\\"networking.k8s.io\\\",\\\"creationTimestamp\\\":null},\\\"versions\\\":[{\\\"version\\\":\\\"v1\\\",\\\"resources\\\":[{\\\"resource\\\":\\\"ingressclasses\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"IngressClass\\\"},\\\"scope\\\":\\\"Cluster\\\",\\\"singularResource\\\":\\\"ingressclass\\\",\\\"verbs\\\":[\\\"create\\\",\\\"delete\\\",\\\"deletecollection\\\",\\\"get\\\",\\\"list\\\",\\\"patch\\\",\\\"update\\\",\\\"watch\\\"]},{\\\"resource\\\":\\\"ingresses\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"Ingress\\\"},\\\"scope\\\":\\\"Namespaced\\\",\\\"singularResource\\\":\\\"ingress\\\",\\\"verbs\\\":[\\\"create\\\",\\\"delete\\\",\\\"deletecollection\\\",\\\"get\\\",\\\"list\\\",\\\"patch\\\",\\\"update\\\",\\\"watch\\\"],\\\"shortNames\\\":[\\\"ing\\\"],\\\"subresources\\\":[{\\\"subresource\\\":\\\"status\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"Ingress\\\"},\\\"verbs\\\":[\\\"get\\\",\\\"patch\\\",\\\"update\\\"]}]},{\\\"resource\\\":\\\"ipaddresses\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"IPAddress\\\"},\\\"scope\\\":\\\"Cluster\\\",\\\"singularResource\\\":\\\"ipaddress\\\",\\\"verbs\\\":[\\\"create\\\",\\\"delete\\\",\\\"deletecollection\\\",\\\"get\\\",\\\"list\\\",\\\"patch\\\",\\\"update\\\",\\\"watch\\\"],\\\"shortNames\\\":[\\\"ip\\\"]},{\\\"resource\\\":\\\"networkpolicies\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"NetworkPolicy\\\"},\\\"scope\\\":\\\"Namespaced\\\",\\\"singularResource\\\":\\\"networkpolicy\\\",\\\"verbs\\\":[\\\"create\\\",\\\"delete\\\",\\\"deletecollection\\\",\\\"get\\\",\\\"list\\\",\\\"patch\\\",\\\"update\\\",\\\"watch\\\"],\\\"shortNames\\\":[\\\"netpol\\\"]},{\\\"resource\\\":\\\"servicecidrs\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"ServiceCIDR\\\"},\\\"scope\\\":\\\"Cluster\\\",\\\"singularResource\\\":\\\"servicecidr\\\",\\\"verbs\\\":[\\\"create\\\",\\\"delete\\\",\\\"deletecollection\\\",\\\"get\\\",\\\"list\\\",\\\"patch\\\",\\\"update\\\",\\\"watch\\\"],\\\"subresources\\\":[{\\\"subresource\\\":\\\"status\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"ServiceCIDR\\\"},\\\"verbs\\\":[\\\"get\\\",\\\"patch\\\",\\\"update\\\"]}]}],\\\"freshness\\\":\\\"Current\\\"}]},{\\\"metadata\\\":{\\\"name\\\":\\\"policy\\\",\\\"creationTimestamp\\\":null},\\\"versions\\\":[{\\\"version\\\":\\\"v1\\\",\\\"resources\\\":[{\\\"resource\\\":\\\"poddisruptionbudgets\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"PodDisruptionBudget\\\"},\\\"scope\\\":\\\"Namespaced\\\",\\\"singularResource\\\":\\\"poddisruptionbudget\\\",\\\"verbs\\\":[\\\"create\\\",\\\"delete\\\",\\\"deletecollection\\\",\\\"get\\\",\\\"list\\\",\\\"patch\\\",\\\"update\\\",\\\"watch\\\"],\\\"shortNames\\\":[\\\"pdb\\\"],\\\"subresources\\\":[{\\\"subresource\\\":\\\"status\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"PodDisruptionBudget\\\"},\\\"verbs\\\":[\\\"get\\\",\\\"patch\\\",\\\"update\\\"]}]}],\\\"freshness\\\":\\\"Current\\\"}]},{\\\"metadata\\\":{\\\"name\\\":\\\"rbac.authorization.k8s.io\\\",\\\"creationTimestamp\\\":null},\\\"versions\\\":[{\\\"version\\\":\\\"v1\\\",\\\"resources\\\":[{\\\"resource\\\":\\\"clusterrolebindings\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"ClusterRoleBinding\\\"},\\\"scope\\\":\\\"Cluster\\\",\\\"singularResource\\\":\\\"clusterrolebinding\\\",\\\"verbs\\\":[\\\"create\\\",\\\"delete\\\",\\\"deletecollection\\\",\\\"get\\\",\\\"list\\\",\\\"patch\\\",\\\"update\\\",\\\"watch\\\"]},{\\\"resource\\\":\\\"clusterroles\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"ClusterRole\\\"},\\\"scope\\\":\\\"Cluster\\\",\\\"singularResource\\\":\\\"clusterrole\\\",\\\"verbs\\\":[\\\"create\\\",\\\"delete\\\",\\\"deletecollection\\\",\\\"get\\\",\\\"list\\\",\\\"patch\\\",\\\"update\\\",\\\"watch\\\"]},{\\\"resource\\\":\\\"rolebindings\\\",\\\"responseKind\\\":{\\\"group\\\":\\\"\\\",\\\"version\\\":\\\"\\\",\\\"kind\\\":\\\"RoleBinding\\\"},\\\"scope\\\":\\\"Namespaced\\\" [truncated 216910 chars]\"\nI1211 14:54:10.900117   74321 cached_discovery.go:88] skipped caching discovery info, no resources found\nI1211 14:54:10.902290   74321 cached_discovery.go:88] skipped caching discovery info, no resources found\nI1211 14:54:10.906734   74321 helper.go:105] \"Request Body\" body=\"\"\nI1211 14:54:10.906774   74321 round_trippers.go:473] curl -v -XGET  -H \"Accept: application/json, */*\" -H \"User-Agent: kubectl/v1.32.10 (darwin/arm64) kubernetes/d26b18b\" -H \"Authorization: Bearer <masked>\" 'https://localhost:62780/apis/batch/v1/namespaces/my-namespace/cronjobs/my-cronjob'\nI1211 14:54:11.007258   74321 round_trippers.go:560] GET https://localhost:62780/apis/batch/v1/namespaces/my-namespace/cronjobs/my-cronjob 200 OK in 100 milliseconds\nI1211 14:54:11.007295   74321 round_trippers.go:577] HTTP Statistics: GetConnection 0 ms ServerProcessing 100 ms Duration 100 ms\nI1211 14:54:11.007304   74321 round_trippers.go:584] Response Headers:\nI1211 14:54:11.007313   74321 round_trippers.go:587]     Audit-Id: 9a9de406-d587-46a3-8189-ea2904556651\nI1211 14:54:11.007318   74321 round_trippers.go:587]     Cache-Control: no-cache, private\nI1211 14:54:11.007321   74321 round_trippers.go:587]     Content-Type: application/json\nI1211 14:54:11.007324   74321 round_trippers.go:587]     Date: Thu, 11 Dec 2025 19:54:11 GMT\nI1211 14:54:11.007327   74321 round_trippers.go:587]     X-Kubernetes-Pf-Flowschema-Uid: 1b99ce95-1c19-4f69-aa96-2f2f2808daf3\nI1211 14:54:11.007330   74321 round_trippers.go:587]     X-Kubernetes-Pf-Prioritylevel-Uid: 44f4a016-cddf-476d-a69a-7364c3ffb845\nI1211 14:54:11.007468   74321 helper.go:105] \"Response Body\" body=<\n\t{\"kind\":\"CronJob\",\"apiVersion\":\"batch/v1\",\"metadata\":{\"name\":\"my-cronjob\",\"namespace\":\"my-namespace\",\"uid\":\"55490122-047e-4006-b449-2ed16a8411d2\",\"resourceVersion\":\"7881219092\",\"generation\":7,\"creationTimestamp\":\"2025-01-16T15:56:32Z\",\"labels\":{\"app.kubernetes.io/instance\":\"my-namespace-dev\",\"app.kubernetes.io/version\":\"0.0.1\",\"argocd.argoproj.io/instance\":\"my-namespace-dev\"},\"annotations\":{\"argocd.argoproj.io/tracking-id\":\"my-namespace-dev:batch/CronJob:my-namespace/my-cronjob\",\"bs/context\":\"non-production\",\"bs/created-by\":\"arn:aws:sts::123456789:assumed-role/My-Role/12345\",\"bs/department\":\"engineering\",\"bs/environment\":\"development\",\"bs/infra-app\":\"true\",\"bs/kubernetes-cluster\":\"development\",\"bs/kubernetes-namespace\":\"my-namespace\",\"bs/lob\":\"mylob\",\"bs/monitoring_enabled\":\"true\",\"bs/source-repo-name\":\"my-repo\",\"bs/team\":\"mylob\",\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"batch/v1\\\",\\\"kind\\\":\\\"CronJob\\\",\\\"metadata\\\":{\\\"annotations\\\":{\\\"argocd.argoproj.io/tracking-id\\\":\\\"my-namespace-dev:batch/CronJob:my-namespace/my-cronjob\\\",\\\"bs/department\\\":\\\"engineering\\\",\\\"bs/lob\\\":\\\"mylob\\\",\\\"bs/source-repo-name\\\":\\\"my-repo\\\",\\\"bs/team\\\":\\\"mylob\\\"},\\\"labels\\\":{\\\"app.kubernetes.io/instance\\\":\\\"my-namespace-dev\\\",\\\"app.kubernetes.io/version\\\":\\\"0.0.1\\\",\\\"argocd.argoproj.io/instance\\\":\\\"my-namespace-dev\\\"},\\\"name\\\":\\\"my-cronjob\\\",\\\"namespace\\\":\\\"my-namespace\\\"},\\\"spec\\\":{\\\"concurrencyPolicy\\\":\\\"Forbid\\\",\\\"failedJobsHistoryLimit\\\":3,\\\"jobTemplate\\\":{\\\"spec\\\":{\\\"backoffLimit\\\":4,\\\"template\\\":{\\\"metadata\\\":{\\\"annotations\\\":{\\\"bs/department\\\":\\\"engineering\\\",\\\"bs/lob\\\":\\\"mylob\\\",\\\"bs/source-repo-name\\\":\\\"my-repo\\\",\\\"bs/team\\\":\\\"mylob\\\"},\\\"labels\\\":{\\\"app\\\":\\\"my-namespace\\\",\\\"app.kubernetes.io/instance\\\":\\\"my-namespace-dev\\\",\\\"app.kubernetes.io/version\\\":\\\"0.0.1\\\",\\\"cronjob\\\":\\\"my-cronjob\\\"}},\\\"spec\\\":{\\\"containers\\\":[{\\\"command\\\":[\\\"sh\\\",\\\"-c\\\",\\\"pip3 install -r /app/requirements.txt \\\\u0026\\\\u0026 python /app/run.py\\\"],\\\"env\\\":[{\\\"name\\\":\\\"AWS_CLUSTER_NAME\\\",\\\"value\\\":\\\"development\\\"},{\\\"name\\\":\\\"KUBE_NAMESPACE\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"metadata.namespace\\\"}}},{\\\"name\\\":\\\"PYTHONUNBUFFERED\\\",\\\"value\\\":\\\"1\\\"}],\\\"image\\\":\\\"123456789.dkr.ecr.us-east-1.amazonaws.com/my-namespace:latest\\\",\\\"imagePullPolicy\\\":\\\"Always\\\",\\\"name\\\":\\\"my-namespace\\\",\\\"volumeMounts\\\":[{\\\"mountPath\\\":\\\"/app\\\",\\\"name\\\":\\\"scripts\\\"}]}],\\\"restartPolicy\\\":\\\"Never\\\",\\\"serviceAccountName\\\":\\\"my-cronjob\\\",\\\"volumes\\\":[{\\\"configMap\\\":{\\\"defaultMode\\\":511,\\\"name\\\":\\\"my-cronjob\\\"},\\\"name\\\":\\\"scripts\\\"}]}},\\\"ttlSecondsAfterFinished\\\":604800}},\\\"schedule\\\":\\\"*/30 * * * *\\\",\\\"successfulJobsHistoryLimit\\\":7}}\\n\",\"policies.kyverno.io/last-applied-patches\":\"add-cronjobs-other-job.mutate-workloads-other-job.kyverno.io: added /spec/jobTemplate/spec/template/metadata/annotations/bs~1context\\n\"},\"managedFields\":[{\"manager\":\"argocd-controller\",\"operation\":\"Update\",\"apiVersion\":\"batch/v1\",\"time\":\"2025-10-09T20:10:43Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:annotations\":{\".\":{},\"f:argocd.argoproj.io/tracking-id\":{},\"f:bs/department\":{},\"f:bs/lob\":{},\"f:bs/source-repo-name\":{},\"f:bs/team\":{},\"f:kubectl.kubernetes.io/last-applied-configuration\":{}},\"f:labels\":{\".\":{},\"f:app.kubernetes.io/instance\":{},\"f:app.kubernetes.io/version\":{},\"f:argocd.argoproj.io/instance\":{}}},\"f:spec\":{\"f:concurrencyPolicy\":{},\"f:failedJobsHistoryLimit\":{},\"f:jobTemplate\":{\"f:spec\":{\"f:backoffLimit\":{},\"f:template\":{\"f:metadata\":{\"f:annotations\":{\".\":{},\"f:bs/department\":{},\"f:bs/lob\":{},\"f:bs/source-repo-name\":{},\"f:bs/team\":{}},\"f:labels\":{\".\":{},\"f:app\":{},\"f:app.kubernetes.io/instance\":{},\"f:app.kubernetes.io/version\":{},\"f:cronjob\":{}}},\"f:spec\":{\"f:containers\":{\"k:{\\\"name\\\":\\\"my-namespace\\\"}\":{\".\":{},\"f:command\":{},\"f:env\":{\".\":{},\"k:{\\\"name\\\":\\\"AWS_CLUSTER_NAME\\\"}\":{\".\":{},\"f:name\":{},\"f:value\":{}},\"k:{\\\"name\\\":\\\"KUBE_NAMESPACE\\\"}\":{\".\":{},\"f:name\":{},\"f:valueFrom\":{\".\":{},\"f:fieldRef\":{}}},\"k:{\\\"name\\\":\\\"PYTHONUNBUFFERED\\\"}\":{\".\":{},\"f:name\":{},\"f:value\":{}}},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{},\"f:volumeMounts\":{\".\":{},\"k:{\\\"mountPath\\\":\\\"/app\\\"}\":{\".\":{},\"f:mountPath\":{},\"f:name\":{}}}}},\"f:dnsPolicy\":{},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:serviceAccount\":{},\"f:serviceAccountName\":{},\"f:terminationGracePeriodSeconds\":{},\"f:volumes\":{\".\":{},\"k:{\\\"name\\\":\\\"scripts\\\"}\":{\".\":{},\"f:configMap\":{\".\":{},\"f:defaultMode\":{},\"f:name\":{}},\"f:name\":{}}}}},\"f:ttlSecondsAfterFinished\":{}}},\"f:schedule\":{},\"f:successfulJobsHistoryLimit\":{},\"f:suspend\":{}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"batch/v1\",\"time\":\"2025-12-11T19:31:04Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:lastScheduleTime\":{},\"f:lastSuccessfulTime\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"schedule\":\"*/30 * * * *\",\"concurrencyPolicy\":\"Forbid\",\"suspend\":false,\"jobTemplate\":{\"metadata\":{\"creationTimestamp\":null},\"spec\":{\"backoffLimit\":4,\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"app\":\"my-namespace\",\"app.kubernetes.io/instance\":\"my-namespace-dev\",\"app.kubernetes.io/version\":\"0.0.1\",\"cronjob\":\"my-cronjob\"},\"annotations\":{\"bs/context\":\"non-production\",\"bs/created-by\":\"arn:aws:sts::123456789:assumed-role/My-AWS-Role/123412341234\",\"bs/department\":\"engineering\",\"bs/environment\":\"development\",\"bs/infra-app\":\"true\",\"bs/kubernetes-cluster\":\"development\",\"bs/kubernetes-namespace\":\"my-namespace\",\"bs/lob\":\"mylob\",\"bs/monitoring_enabled\":\"true\",\"bs/source-repo-name\":\"my-repo\",\"bs/team\":\"mylob\"}},\"spec\":{\"volumes\":[{\"name\":\"scripts\",\"configMap\":{\"name\":\"my-cronjob\",\"defaultMode\":511}}],\"containers\":[{\"name\":\"my-namespace\",\"image\":\"123456789.dkr.ecr.us-east-1.amazonaws.com/my-namespace:latest\",\"command\":[\"sh\",\"-c\",\"pip3 install -r /app/requirements.txt \\u0026\\u0026 python /app/run.py\"],\"env\":[{\"name\":\"AWS_CLUSTER_NAME\",\"value\":\"development\"},{\"name\":\"KUBE_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"apiVersion\":\"v1\",\"fieldPath\":\"metadata.namespace\"}}},{\"name\":\"PYTHONUNBUFFERED\",\"value\":\"1\"}],\"resources\":{},\"volumeMounts\":[{\"name\":\"scripts\",\"mountPath\":\"/app\"}],\"terminationMessagePath\":\"/dev/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"Always\"}],\"restartPolicy\":\"Never\",\"terminationGracePeriodSeconds\":30,\"dnsPolicy\":\"ClusterFirst\",\"serviceAccountName\":\"my-cronjob\",\"serviceAccount\":\"my-cronjob\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}},\"ttlSecondsAfterFinished\":604800}},\"successfulJobsHistoryLimit\":7,\"failedJobsHistoryLimit\":3},\"status\":{\"lastScheduleTime\":\"2025-12-11T19:30:00Z\",\"lastSuccessfulTime\":\"2025-12-11T19:31:04Z\"}}\n >\nI1211 14:54:11.010896   74321 type.go:252] \"Request Body\" body=<\n\t00000000  6b 38 73 00 0a 0f 0a 08  62 61 74 63 68 2f 76 31  |k8s.....batch/v1|\n\t00000010  12 03 4a 6f 62 12 9f 09  0a 8f 01 0a 04 74 65 73  |..Job........tes|\n\t00000020  74 12 00 1a 00 22 00 2a  00 32 00 38 00 42 00 62  |t....\".*.2.8.B.b|\n\t00000030  2b 0a 21 63 72 6f 6e 6a  6f 62 2e 6b 75 62 65 72  |+.!cronjob.kuber|\n\t00000040  6e 65 74 65 73 2e 69 6f  2f 69 6e 73 74 61 6e 74  |netes.io/instant|\n\t00000050  69 61 74 65 12 06 6d 61  6e 75 61 6c 6a 4c 0a 07  |iate..manualjL..|\n\t00000060  43 72 6f 6e 4a 6f 62 1a  0f 74 61 67 2d 65 62 73  |CronJob..my-cron|\n\t00000070  2d 76 6f 6c 75 6d 65 73  22 24 35 35 34 39 30 31  |job12345\"$554901|\n\t00000080  32 32 2d 30 34 37 65 2d  34 30 30 36 2d 62 34 34  |22-047e-4006-b44|\n\t00000090  39 2d 32 65 64 31 36 61  38 34 31 31 64 32 2a 08  |9-2ed16a8411d2*.|\n\t000000a0  62 61 74 63 68 2f 76 31  30 01 12 80 08 32 f7 07  |batch/v10....2..|\n\t000000b0  0a a2 04 0a 00 12 00 1a  00 22 00 2a 00 32 00 38  |.........\".*.2.8|\n\t000000c0  00 42 00 5a 13 0a 03 61  70 70 12 0c 62 73 2d 61  |.B.Z...app..my-n|\n\t000000d0  77 73 2d 75 74 69 6c 73  5a 2e 0a 1a 61 70 70 2e  |ws-utilsZ...app.|\n\t000000e0  6b 75 62 65 72 6e 65 74  65 73 2e 69 6f 2f 69 6e  |kubernetes.io/in|\n\t000000f0  73 74 61 6e 63 65 12 10  62 73 2d 61 77 73 2d 75  |stance..my-names|\n\t00000100  74 69 6c 73 2d 64 65 76  5a 22 0a 19 61 70 70 2e  |tils-devZ\"..app.|\n\t00000110  6b 75 62 65 72 6e 65 74  65 73 2e 69 6f 2f 76 65  |kubernetes.io/ve|\n\t00000120  72 73 69 6f 6e 12 05 30  2e 30 2e 31 5a 1a 0a 07  |rsion..0.0.1Z...|\n\t00000130  63 72 6f 6e 6a 6f 62 12  0f 74 61 67 2d 65 62 73  |cronjob..my-cron|\n\t00000140  2d 76 6f 6c 75 6d 65 73  62 1c 0a 0a 62 73 2f 63  |job12345b...bs/c|\n\t00000150  6f 6e 74 65 78 74 12 0e  6e 6f 6e 2d 70 72 6f 64  |ontext..non-prod|\n\t00000160  75 63 74 69 6f 6e 62 61  0a 0d 62 73 2f 63 72 65  |uctionba..bs/cre|\n\t00000170  61 74 65 64 2d 62 79 12  50 61 72 6e 3a 61 77 73  |ated-by.Parn:aws|\n\t00000180  3a 73 74 73 3a 3a 36 39  32 36 37 34 30 34 36 35  |:sts::123456789|\n\t00000190  38 31 3a 61 73 73 75 6d  65 64 2d 72 6f 6c 65 2f  |81:assumed-role/|\n\t000001a0  49 6e 66 72 61 2d 41 72  67 6f 43 64 2d 44 65 70  |My-IAM-Role-AAAA|\n\t000001b0  6c 6f 79 65 72 2f 31 37  33 37 30 34 32 32 38 39  |AAAAA/1082470187|\n\t000001c0  31 37 35 35 31 30 39 37  33 62 1c 0a 0d 62 73 2f  |123123333b...bs/|\n\t000001d0  64 65 70 61 72 74 6d 65  6e 74 12 0b 65 6e 67 69  |department..engi|\n\t000001e0  6e 65 65 72 69 6e 67 62  1d 0a 0e 62 73 2f 65 6e  |neeringb...bs/en|\n\t000001f0  76 69 72 6f 6e 6d 65 6e  74 12 0b 64 65 76 65 6c  |vironment..devel|\n\t00000200  6f 70 6d 65 6e 74 62 14  0a 0c 62 73 2f 69 6e 66  |opmentb...bs/inf|\n\t00000210  72 61 2d 61 70 70 12 04  74 72 75 65 62 24 0a 15  |ra-app..trueb$..|\n\t00000220  62 73 2f 6b 75 62 65 72  6e 65 74 65 73 2d 63 6c  |bs/kubernetes-cl|\n\t00000230  75 73 74 65 72 12 0b 64  65 76 65 6c 6f 70 6d 65  |uster..developme|\n\t00000240  6e 74 62 27 0a 17 62 73  2f 6b 75 62 65 72 6e 65  |ntb'..bs/kuberne|\n\t00000250  74 65 73 2d 6e 61 6d 65  73 70 61 63 65 12 0c 62  |tes-namespace..m|\n\t00000260  73 2d 61 77 73 2d 75 74  69 6c 73 62 0e 0a 06 62  |sy-namespacb...b|\n\t00000270  73 2f 6c 6f 62 12 04 62  6f 74 73 62 1d 0a 15 62  |s/lob..mylobb...b|\n\t00000280  73 2f 6d 6f 6e 69 74 6f  72 69 6e 67 5f 65 6e 61  |s/monitoring_ena|\n\t00000290  62 6c 65 64 12 04 74 72  75 65 62 28 0a 13 62 73  |bled..trueb(..bs|\n\t000002a0  2f 73 6f 75 72 63 65 2d  72 65 70 6f 2d 6e 61 6d  |/source-repo-nam|\n\t000002b0  65 12 11 62 73 2d 69 6e  66 72 61 2d 6b 38 73 2d  |e..my-repo-1234-|\n\t000002c0  61 70 70 73 62 0f 0a 07  62 73 2f 74 65 61 6d 12  |appsb...bs/team.|\n\t000002d0  04 62 6f 74 73 12 cf 03  0a 24 0a 07 73 63 72 69  |.mylob....$..scri|\n\t000002e0  70 74 73 12 19 9a 01 16  0a 11 0a 0f 74 61 67 2d  |pts.........my-c|\n\t000002f0  65 62 73 2d 76 6f 6c 75  6d 65 73 18 ff 03 12 c6  |ronjob..........|\n\t00000300  02 0a 0c 62 73 2d 61 77  73 2d 75 74 69 6c 73 12  |...my-namespace.|\n\t00000310  40 36 39 32 36 37 34 30  34 36 35 38 31 2e 64 6b  |@123456789.dk|\n\t00000320  72 2e 65 63 72 2e 75 73  2d 65 61 73 74 2d 31 2e  |r.ecr.us-east-1.|\n\t00000330  61 6d 61 7a 6f 6e 61 77  73 2e 63 6f 6d 2f 62 73  |amazonaws.com/my|\n\t00000340  2d 61 77 73 2d 75 74 69  6c 73 3a 6c 61 74 65 73  |-dockerimg:lates|\n\t00000350  74 1a 02 73 68 1a 02 2d  63 1a 3b 70 69 70 33 20  |t..sh..-c.;pip3 |\n\t00000360  69 6e 73 74 61 6c 6c 20  2d 72 20 2f 61 70 70 2f  |install -r /app/|\n\t00000370  72 65 71 75 69 72 65 6d  65 6e 74 73 2e 74 78 74  |requirements.txt|\n\t00000380  20 26 26 20 70 79 74 68  6f 6e 20 2f 61 70 70 2f  | && python /app/|\n\t00000390  72 75 6e 2e 70 79 2a 00  3a 1f 0a 10 41 57 53 5f  |run.py*.:...AWS_|\n\t000003a0  43 4c 55 53 54 45 52 5f  4e 41 4d 45 12 0b 64 65  |CLUSTER_NAME..de|\n\t000003b0  76 65 6c 6f 70 6d 65 6e  74 3a 2e 0a 0e 4b 55 42  |velopment:...KUB|\n\t000003c0  45 5f 4e 41 4d 45 53 50  41 43 45 12 00 1a 1a 0a  |E_NAMESPACE.....|\n\t000003d0  18 0a 02 76 31 12 12 6d  65 74 61 64 61 74 61 2e  |...v1..metadata.|\n\t000003e0  6e 61 6d 65 73 70 61 63  65 3a 15 0a 10 50 59 54  |namespace:...PYT|\n\t000003f0  48 4f 4e 55 4e 42 55 46  46 45 52 45 44 12 01 31  |HONUNBUFFERED..1|\n\t00000400  42 00 4a 15 0a 07 73 63  72 69 70 74 73 10 00 1a  |B.J...scripts...|\n\t00000410  04 2f 61 70 70 22 00 32  00 6a 14 2f 64 65 76 2f  |./app\".2.j./dev/|\n\t00000420  74 65 72 6d 69 6e 61 74  69 6f 6e 2d 6c 6f 67 72  |termination-logr|\n\t00000430  06 41 6c 77 61 79 73 80  01 00 88 01 00 90 01 00  |.Always.........|\n\t00000440  a2 01 04 46 69 6c 65 1a  05 4e 65 76 65 72 20 1e  |...File..Never .|\n\t00000450  32 0c 43 6c 75 73 74 65  72 46 69 72 73 74 42 0f  |2.ClusterFirstB.|\n\t00000460  74 61 67 2d 65 62 73 2d  76 6f 6c 75 6d 65 73 4a  |my-cronjobJ|\n\t00000470  0f 74 61 67 2d 65 62 73  2d 76 6f 6c 75 6d 65 73  |.my-cronjob|\n\t00000480  52 00 58 00 60 00 68 00  72 00 82 01 00 8a 01 00  |R.X.`.h.r.......|\n\t00000490  9a 01 11 64 65 66 61 75  6c 74 2d 73 63 68 65 64  |...default-sched|\n\t000004a0  75 6c 65 72 c2 01 00 38  04 40 80 f5 24 1a 08 20  |uler...8.@..$.. |\n\t000004b0  00 28 00 30 00 3a 00 1a  00 22 00                 |.(.0.:...\".|\n >\nI1211 14:54:11.011182   74321 round_trippers.go:473] curl -v -XPOST  -H \"Accept: application/vnd.kubernetes.protobuf,application/json\" -H \"Content-Type: application/vnd.kubernetes.protobuf\" -H \"User-Agent: kubectl/v1.32.10 (darwin/arm64) kubernetes/d26b18b\" -H \"Authorization: Bearer <masked>\" 'https://localhost:62780/apis/batch/v1/namespaces/my-namespace/jobs?fieldManager=kubectl-create&fieldValidation=Strict'\nI1211 14:54:11.104230   74321 round_trippers.go:560] POST https://localhost:62780/apis/batch/v1/namespaces/my-namespace/jobs?fieldManager=kubectl-create&fieldValidation=Strict 400 Bad Request in 93 milliseconds\nI1211 14:54:11.104275   74321 round_trippers.go:577] HTTP Statistics: GetConnection 0 ms ServerProcessing 92 ms Duration 93 ms\nI1211 14:54:11.104285   74321 round_trippers.go:584] Response Headers:\nI1211 14:54:11.104297   74321 round_trippers.go:587]     X-Kubernetes-Pf-Flowschema-Uid: 1b99ce95-1c19-4f69-aa96-2f2f2808daf3\nI1211 14:54:11.104304   74321 round_trippers.go:587]     X-Kubernetes-Pf-Prioritylevel-Uid: 44f4a016-cddf-476d-a69a-7364c3ffb845\nI1211 14:54:11.104309   74321 round_trippers.go:587]     Content-Length: 200\nI1211 14:54:11.104313   74321 round_trippers.go:587]     Audit-Id: 19ba52cb-9afb-4939-982e-9f619802d025\nI1211 14:54:11.104317   74321 round_trippers.go:587]     Cache-Control: no-cache, private\nI1211 14:54:11.104323   74321 round_trippers.go:587]     Content-Type: application/vnd.kubernetes.protobuf\nI1211 14:54:11.104329   74321 round_trippers.go:587]     Date: Thu, 11 Dec 2025 19:54:11 GMT\nI1211 14:54:11.104390   74321 type.go:252] \"Response Body\" body=<\n\t00000000  6b 38 73 00 0a 0c 0a 02  76 31 12 06 53 74 61 74  |k8s.....v1..Stat|\n\t00000010  75 73 12 af 01 0a 06 0a  00 12 00 1a 00 12 07 46  |us.............F|\n\t00000020  61 69 6c 75 72 65 1a 8c  01 74 68 65 20 6f 62 6a  |ailure...the obj|\n\t00000030  65 63 74 20 70 72 6f 76  69 64 65 64 20 69 73 20  |ect provided is |\n\t00000040  75 6e 72 65 63 6f 67 6e  69 7a 65 64 20 28 6d 75  |unrecognized (mu|\n\t00000050  73 74 20 62 65 20 6f 66  20 74 79 70 65 20 4a 6f  |st be of type Jo|\n\t00000060  62 29 3a 20 75 6e 65 78  70 65 63 74 65 64 20 45  |b): unexpected E|\n\t00000070  4f 46 20 28 36 62 33 38  37 33 30 30 30 61 30 66  |OF (6b3873000a0f|\n\t00000080  30 61 30 38 36 32 36 31  37 34 36 33 36 38 32 66  |0a0862617463682f|\n\t00000090  37 36 33 31 31 32 30 33  34 61 36 66 36 32 31 32  |763112034a6f6212|\n\t000000a0  65 66 62 66 62 64 30 39  30 61 65 66 62 66 62 64  |efbfbd090aefbfbd|\n\t000000b0  20 2e 2e 2e 29 22 0a 42  61 64 52 65 71 75 65 73  | ...)\".BadReques|\n\t000000c0  74 30 90 03 1a 00 22 00                           |t0....\".|\n >\nerror: failed to create job: the object provided is unrecognized (must be of type Job): unexpected EOF (6b3873000a0f0a0862617463682f763112034a6f6212efbfbd090aefbfbd ...)\n```\n\n---\n\nThanks! According to that, the annotation size limit is 256kb. The largest annotation on my resource is last-applied-configuration, which is 1.5kb, so it seems like this is not the issue. Additionally, this works with kubectl <1.32 and fails with kubectl 1.32+. I would expect that a too-large annotation would fail in any version of kubectl.\n\nThis is over my head, but my AI tool has doubled down on protobuf. This is what it says:\n\n```\nRoot Cause: kubectl 1.32.10 Protobuf Encoder Bug\n\n1. Incomplete Serialization\nThe hex shows kubectl successfully:\n- Fetches CronJob as JSON (580ms, 200 OK)\n- Converts to Protobuf \n- Truncates during Pod spec serialization (notice the pattern stops mid-spec)\n\n2. Field Ordering Issue\n- The Protobuf wire format uses field tags. At offset 0x4a0-0x4bb, you see:\n\n9a 01 11 = field tag 19, length 17 (\"default-scheduler\")\nc2 01 00 = field tag 24, length 0 (expected securityContext, but empty)\n38 04 = field tag 7, varint 4 (backoffLimit)\n40 80 f5 24 = field tag 8, varint 604800 (ttlSecondsAfterFinished)\n1a 08 20 00 28 00 30 00 3a 00 = incomplete message (should be status or finalizers)\nThe encoder skips required nested fields after securityContext, causing the server to reject it with \"unexpected EOF\".\n\n3. Why This Happens\nkubectl 1.32+ changed Protobuf encoding to use lazy field evaluation for performance. When it encounters:\n\n- Empty maps (like resources: {}, securityContext: {})\n- Defaulted fields (your Pod has 30+ fields with zero/empty values)\n- Nested ConfigMap volumes with non-standard defaultMode: 511 (0x1FF)\nThe encoder may skip finalizer bytes if it incorrectly calculates the message length upfront.\n```",
      "labels": [
        "kind/support",
        "needs-triage"
      ],
      "created_at": "2025-11-20T20:41:08Z",
      "closed_at": "2025-12-12T11:58:37Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1800",
      "comments_count": 16
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1144,
      "title": "`kubectl apply -k` with `helmCharts` cannot run",
      "problem": "<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\r\n\r\nIf the matter is security related, please disclose it privately via https://kubernetes.io/security/\r\n-->\r\n\r\n**What happened**:\r\nCan't pass the `kustomize` flag `--enable-helm` to `kubectl apply -k`\r\n\r\n**What you expected to happen**:\r\nHelm chart gets built and deployed.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n1. create this kustomize file: `test/kustomization.yaml`\r\n```yaml\r\n---\r\napiVersion: kustomize.config.k8s.io/v1beta1\r\nkind: Kustomization\r\nhelmCharts:\r\n  - name: minecraft\r\n    releaseName: test\r\n    version: 3.1.3\r\n    repo: https://itzg.github.io/minecraft-server-charts\r\n```\r\n\r\n2. Try to deploy it with  `kubectl.exe apply -k test`\r\n3. This happens:\r\n```\r\nerror: trouble configuring builtin HelmChartInflationGenerator with config: `\r\nname: minecraft\r\nreleaseName: test\r\nrepo: https://itzg.github.io/minecraft-server-charts\r\nversion: 3.1.3\r\n`: must specify --enable-helm\r\n```\r\n\r\nHowever, I found no way to pass `--enable-helm`\r\n\r\n**Anything else we need to know?**:\r\n\r\nusing `kustomize build --enable-helm test` (version `v4.1.3`) works as expected.\r\n\r\nThe doc about `helmCharts` is very minimal. is there some way to enable `kustomize` flags globally from `kubectl`?\r\n\r\n**Environment**:\r\n- Kubernetes client and server versions (use `kubectl version`):\r\n`Client Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.0\", GitCommit:\"c2b5237ccd9c0f1d600d3072634ca66cefdf272f\", GitTreeState:\"clean\", BuildDate:\"2021-08-04T18:03:20Z\", GoVersion:\"go1.16.6\", Compiler:\"gc\", Platform:\"windows/amd64\"}\r\nServer Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.4\", GitCommit:\"3cce4a82b44f032d0cd1a1790e6d2f5a55d20aae\", GitTreeState:\"clean\", BuildDate:\"2021-08-11T18:10:22Z\", GoVersion:\"go1.16.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}`\r\n- Cloud provider or hardware configuration: `docker desktop k8s on WSL`\r\n- OS (e.g: `cat /etc/os-release`): `windows 10`\r\n",
      "solution": "You're right that `kubectl apply -k` does not expose additional Kustomize-specific flags. I believe it intentionally only supports the simple case, for the sake of the simplicity of `apply` options. If you need to configure Kustomize behavior with additional flags, you can use `kubectl kustomize` and standalone `kustomize` instead.\r\n\r\n/close\n\n---\n\n@KnVerey: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubectl/issues/1144#issuecomment-965568358):\n\n>You're right that `kubectl apply -k` does not expose additional Kustomize-specific flags. I believe it intentionally only supports the simple case, for the sake of the simplicity of `apply` options. If you need to configure Kustomize behavior with additional flags, you can use `kubectl kustomize` and standalone `kustomize` instead.\r\n>\r\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>",
      "labels": [
        "kind/bug",
        "needs-triage"
      ],
      "created_at": "2021-11-09T16:36:09Z",
      "closed_at": "2021-11-10T17:24:42Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1144",
      "comments_count": 10
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1323,
      "title": "`kubectl label`: Inconsistent messaging when both adding and removing labels",
      "problem": "**What happened**:\r\n\r\n**What you expected to happen**: `kubectl` label should produce a consistent, accurate message when labels are both removed and added at the same time (maybe just \"labels modified\" for simplicity?).\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n<!-- Please make sure you are able to reproduce the bug using a supported version and version skew of Kubernetes. See https://kubernetes.io/docs/setup/release/version-skew-policy for which versions and are currently supported.\r\n-->\r\n\r\n```bash\r\n$ kubectl label pod mypod a=b aa=bb\r\npod/mypod labeled\r\n\r\n$ kubectl label pod mypod a- ee=ff\r\npod/mypod labeled # because the new label is textually longer\r\n\r\n$ kubectl label pod mypod aa- e=f # same operations as above\r\npod/mypod unlabeled # different message, because the old label is textually longer\r\n```\r\n\r\n**Anything else we need to know?**: The problem seems to be the use of a length comparison in this function: https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubectl/pkg/cmd/label/label.go#L399-L408. That might be reliable if labels could only be added OR removed by an operation, but as in the examples above, both are possible at once.\r\n\r\n**Environment**:\r\n- Kubernetes client and server versions (use `kubectl version`): `Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.2\", GitCommit:\"5835544ca568b757a8ecae5c153f317e5736700e\", GitTreeState:\"clean\", BuildDate:\"2022-09-21T14:25:45Z\", GoVersion:\"go1.19.1\", Compiler:\"gc\", Platform:\"darwin/arm64\"}`\r\n- Cloud provider or hardware configuration: n/a\r\n- OS (e.g: `cat /etc/os-release`): n/a\r\n\r\n/sig cli\r\n\r\nFixed by https://github.com/kubernetes/kubernetes/pull/110124",
      "solution": "Although there is no direct relation but to distinguish `label/unlabel` message also causing this issue https://github.com/kubernetes/kubectl/issues/1265 as a symptom. If we decide to just say `modified`, this issue will also be fixed.\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/bug",
        "priority/backlog",
        "sig/cli",
        "triage/accepted"
      ],
      "created_at": "2022-11-07T22:05:34Z",
      "closed_at": "2025-12-18T00:27:03Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1323",
      "comments_count": 20
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1071,
      "title": "debug: ability to replicate volume mounts ",
      "problem": "<!-- Please only use this template for submitting enhancement requests -->\r\n\r\n**What would you like to be added**:\r\n\r\nWhen debugging by adding a container to a pod, having the ability to match volume mounts with the target container. \r\n\r\nSpecifically:\r\n* [Adding a container to a running pod](https://kubernetes.io/docs/tasks/debug-application-cluster/debug-running-pod/#ephemeral-container)\r\n* [Copying a pod while adding a new container](https://kubernetes.io/docs/tasks/debug-application-cluster/debug-running-pod/#copying-a-pod-while-adding-a-new-container).\r\n\r\n**Why is this needed**:\r\n\r\nFrom a new debug container added to a running pod, I can't access data other containers can see on volume mounts. Obvious candidates: emptyDir, NFS mounts, configmap/secret mounts, EBS volumes, etc\r\n\r\n**Ideas**:\r\n\r\n1. Replicate some/all volume mounts from one other container\r\n2. Expose all the volumes available to the pod so they can be mounted by name: `--volume-mount=name:/path` There are a lot of options exposed (subPaths, read-only, etc) though to try and cram into a CLI option.\r\n\r\nIMO (1) is probably easiest and solves most use cases. Maybe something like:\r\n\r\n* `--with-volume-mounts[=<container>]` Container name to attempt to replicate volume mounts from. Without a container name, uses the container from `--target`. ",
      "solution": "> for your use case does accessing the target container's root filesystem via `/proc/$PID/root` work?\r\n\r\nI get `ls: cannot access '/proc/7/root/': Permission denied`, so I guess I need https://github.com/kubernetes/kubernetes/issues/97103 resolved first? (or add caps to / make my original pod privileged?)\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/feature",
        "triage/accepted"
      ],
      "created_at": "2021-06-08T10:02:10Z",
      "closed_at": "2024-02-16T09:06:53Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1071",
      "comments_count": 30
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1802,
      "title": "Can't provide filepath via KUBERC environment variable",
      "problem": "### What happened?\n\nAccording to the [docs](https://kubernetes.io/docs/reference/kubectl/kuberc/), one can use the environment variable `KUBERC` to provide a path to a specific kuberc file that should get loaded. For me and my colleagues this is not working.\n\nI can use the default file (`~/.kube/kuberc`) and I can provide a path via `--kuberc` to every command, but not via the env variable.\n\nWhen setting `KUBERC=off` and providing `--kuberc` I get the warning that these two are mutually exclusive, so I know that the value of the env variable gets read.\n\nBut providing a path in `KUBERC` just has no effect and kubectl will apply the default config.\n\n### What did you expect to happen?\n\nThe kuberc which I provided via the env variable, should have been loaded and applied ( and overwrite the default config if there is one)\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n1. Remove your .kube/kuberc if you have one\n2. Take the example kuberc from the docs which will add interactive delete\n3. Save it somewhere which is not the default kuberc path\n4. export KUBERC=<path from step 3>\n5. run `kubectl delete ...`\n6. Your object got deleted without an interactive question\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n% kubectl version                                                                                                                                                                                                                       infra-stage/default\nClient Version: v1.34.2\nKustomize Version: v5.7.1\nServer Version: v1.34.2\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nOn-Prem\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n$ cat /etc/os-release\nNAME=\"Arch Linux\"\nPRETTY_NAME=\"Arch Linux\"\nID=arch\nBUILD_ID=rolling\nANSI_COLOR=\"38;2;23;147;209\"\nHOME_URL=\"https://archlinux.org/\"\nDOCUMENTATION_URL=\"https://wiki.archlinux.org/\"\nSUPPORT_URL=\"https://bbs.archlinux.org/\"\nBUG_REPORT_URL=\"https://gitlab.archlinux.org/groups/archlinux/-/issues\"\nPRIVACY_POLICY_URL=\"https://terms.archlinux.org/docs/privacy-policy/\"\nLOGO=archlinux-logo\n$ uname -a\nLinux felixb-xm2307 6.17.9-arch1-1 kubernetes/kubernetes#1 SMP PREEMPT_DYNAMIC Mon, 24 Nov 2025 15:21:09 +0000 x86_64 GNU/Linux\n\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n",
      "solution": "@fleaz thank you for filing this issue. Indeed there was a bug related to this and it has been fixed in 1.35 by https://github.com/kubernetes/kubernetes/pull/135003.\n\nSo I believe that once you use 1.35 kubectl, it will work. \n\n---\n\nSince this has been fixed in 1.35, I'm closing as done.",
      "labels": [
        "kind/bug",
        "sig/cli",
        "needs-triage"
      ],
      "created_at": "2025-12-01T12:54:39Z",
      "closed_at": "2025-12-02T04:44:15Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1802",
      "comments_count": 6
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1799,
      "title": "kubectl fails to authenticate with AWS EKS despite valid credentials and working token generation",
      "problem": "### What happened?\n\nkubectl fails to authenticate with AWS EKS cluster with error \"the server has asked for the client to provide credentials\" even though:\n1. AWS SSO authentication is valid and working\n2. Manual token generation via `aws eks get-token` succeeds\n3. Using the generated token directly with curl successfully accesses the API\n4. The exec credential plugin configuration appears correct in kubeconfig\n\n### What did you expect to happen?\n\nkubectl should successfully authenticate using the exec credential plugin to call `aws eks get-token` and access the EKS cluster.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n1. Configure AWS SSO and authenticate:\n```bash\naws sso login --profile staging\naws --profile staging sts get-caller-identity  # Works correctly\n```\n\n2. Update kubeconfig for EKS cluster:\n```bash\naws eks update-kubeconfig --region us-east-2 --name <cluster-name> --profile staging\n```\n\n3. Verify kubeconfig has correct exec configuration:\n```yaml\nusers:\n- name: arn:aws:eks:us-east-2:XXXXXXXXXXXX:cluster/<cluster-name>\n  user:\n    exec:\n      apiVersion: client.authentication.k8s.io/v1beta1\n      args:\n      - --region\n      - us-east-2\n      - eks\n      - get-token\n      - --cluster-name\n      - <cluster-name>\n      - --output\n      - json\n      command: aws\n      env:\n      - name: AWS_PROFILE\n        value: staging\n```\n\n4. Test kubectl:\n```bash\nkubectl get namespaces\n# Error: You must be logged in to the server (the server has asked for the client to provide credentials)\n```\n\n5. Verify token generation works manually:\n```bash\naws --profile staging --region us-east-2 eks get-token --cluster-name <cluster-name> --output json\n# Successfully returns a valid token\n```\n\n6. Verify the token works with curl:\n```bash\nTOKEN=$(aws --profile staging --region us-east-2 eks get-token --cluster-name <cluster-name> --output json | jq -r \".status.token\")\ncurl -k -H \"Authorization: Bearer $TOKEN\" https://<eks-endpoint>/api/v1/namespaces\n# Successfully returns namespace list\n```\n\n### Anything else we need to know?\n\n**Workaround**: Created a wrapper script that manually gets the token and passes it to kubectl:\n```bash\n#!/bin/bash\nTOKEN=$(aws --profile staging --region us-east-2 eks get-token --cluster-name <cluster-name> --output json | jq -r \".status.token\")\nkubectl --token=\"$TOKEN\" \"$@\"\n```\n\nThis workaround confirms that:\n- The AWS credentials are valid\n- The token generation works\n- kubectl can authenticate when provided the token directly\n- The issue is specifically with kubectl's exec credential plugin execution\n\n**Additional observations:**\n- No error output from the exec plugin when running kubectl with `-v=9`\n- The exec plugin appears to run but doesn't provide credentials\n- Issue persists across kubectl restarts and kubeconfig regeneration\n- AWS CLI version: aws-cli/2.28.21 Python/3.13.7 Darwin/25.1.0 source/arm64\n- SSO session is valid throughout testing\n\n### Environment\n\n```bash\n# Kubernetes client and server versions\nClient Version: v1.34.2\nKustomize Version: v5.7.1\nServer Version: v1.34.1-eks-3cfe0ce\n\n# Operating system\nmacOS (Darwin 25.1.0)\n\n# AWS CLI version\naws-cli/2.28.21 Python/3.13.7 Darwin/25.1.0 source/arm64\n\n# Authentication method\nAWS SSO with assumed role: AWSReservedSSO_RestrictedDeveloper_<hash>\n\n# EKS cluster configuration\n- Private endpoint only (no public access)\n- Accessed via SSM port forwarding through bastion host\n```\n\n### Related Issues\n\n- #747 - Similar exec credential failures but with exit status 255\n- #1210 - AWS authentication broke in v1.24 (resolved)\n\n### Possible Root Causes\n\n1. kubectl's exec plugin may not be properly inheriting or passing environment variables (AWS_PROFILE)\n2. The exec plugin execution context might differ from shell execution\n3. Potential issue with how kubectl handles the exec credential response format\n4. SSO token refresh might not be triggered properly within the exec plugin context\n\n### Impact\n\nThis issue affects users who:\n- Use AWS SSO for authentication\n- Access private EKS clusters via port forwarding\n- Rely on exec credential plugins for token generation\n\nThe issue forces users to implement workarounds that bypass kubectl's intended authentication flow, reducing security and complicating cluster access.",
      "solution": "Please let us know about the issue. It is very low probably that this occurs due to a bug in kubernetes.\n/close\n\n---\n\n@ardaguclu: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubectl/issues/1799#issuecomment-3570917141):\n\n>Please let us know about the issue. It is very low probably that this occurs due to a bug in kubernetes.\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>",
      "labels": [
        "kind/bug",
        "needs-triage"
      ],
      "created_at": "2025-11-14T07:32:07Z",
      "closed_at": "2025-11-24T13:57:53Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1799",
      "comments_count": 5
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1798,
      "title": "kubectl leaks memory on macOS when authentication fails (kalloc.1024 keeps growing)",
      "problem": "### Problem description\nOn macOS, `kubectl` leaks memory in the `kalloc.1024` zone whenever the authentication session has expired.  \nIf kubectl is logged in \u2192 **no leak**.  \nIf the session expires and kubectl starts returning `the server has asked for the client to provide credentials` \u2192 **every call allocates memory that is never released**.  \nIf I refresh or replace the token (valid login) \u2192 the leak instantly stops.  \nIt looks like this may be related to TLS session or credential plugin handling on failed authentication, because it only happens after token expiration and never during normal operation.\n\n### Investigation Summary\nI have been investigating this memory leak on macOS for over two months.  \nIf multiple `kubectl` processes are left running in the background (I often have ~8 `watch kubectl` loops), they all start leaking as soon as the token expires.  \nWhen this happens, memory grows roughly **100 KB per second** in the `kalloc.1024` zone, continuously.  \nThe **shared memory (data_shared.kalloc.1024)** keeps growing without limit \u2014 it can reach **24\u201328 GB** and never goes down.  \nI also tested without `watch` (just calling `kubectl` repeatedly manually), and the leak still occurs, so it is not caused by `watch`.\n\n### Impact\nWithin many hours, the `kalloc.1024` zone starts consuming huge amounts of memory \u2014 it can easily eat **24\u201328 GB of RAM on macOS**.  \nAfter roughly one day the system becomes unstable and macOS always crashes.  \nThe memory is held in `data_shared.kalloc.1024` and **cannot be freed without a full system reboot** \u2014 killing the kubectl processes does not release it. Only restarting the laptop clears it.\n\n### Environment\nmacOS **Sonoma (Tahoe) 26.1 \u2013 25B78**  \nAs of **11 November**, this is the latest released version, and the issue is still present.  \nPrevious macOS updates also had the same problem.\n\nThe leak happens on multiple Kubernetes clusters \u2014 both Google Kubernetes Engine (GCP) and on-prem vanilla Kubernetes.  \nIt is not cloud-specific or provider-specific \u2014 the leak appears entirely on the kubectl/client side on macOS.\n\n### Versions tested\nI tested kubectl versions from **1.24.x up to 1.34.0**, and the leak occurs on every version.\n\n\n**How to reproduce**\n1. Log in normally (no leak)\n2. Let the session expire\n3. Run: `watch -n 1 'kubectl get nodes -o wide'`\n4. Monitor `watch -n 1 'zprint  -t | grep -Ei \"data_shared.kalloc.1024|zone name\"`\n5. same when tried `kubectl get --raw=/api --request-timeout=1s >/dev/null 2>&1` this also leaking\n\nThe `inuse` value in `data_shared.kalloc.1024` keeps increasing with every failed `kubectl` request.  \nIt never goes down \u2014 memory stays allocated even after kubectl exits \u2014 and continues growing until it reaches tens of gigabytes.\n\n<img width=\"1290\" height=\"175\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/0dc308c1-88a9-464e-96a6-0e85071c0426\" />\n\n",
      "solution": "@ardaguclu after updating to the latest gcloud build the memory leaks stopped completely. It looks like something was indeed fixed on their side, although it\u2019s hard to tell what exactly \u2014 gcloud release notes notoriously omit a lot of important internal changes, so I\u2019ll probably never know what was actually broken.",
      "labels": [
        "kind/bug",
        "needs-triage"
      ],
      "created_at": "2025-11-11T16:09:48Z",
      "closed_at": "2025-11-22T18:54:16Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1798",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1713,
      "title": "kubectl logs command doesn't show logs from all the pods when using label selector",
      "problem": "<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\n\nIf the matter is security related, please disclose it privately via https://kubernetes.io/security/\n-->\n\n**What happened**: I'm using kubectl logs command to get the logs of multiple pods of same application using labels. However, it doesn't give logs from all the pods. \n\n**What you expected to happen**: I should be able to use the -f/--follow option to get the logs from all the pods and use grep/awk with it. \n\n**How to reproduce it (as minimally and precisely as possible)**:\n<!-- Please make sure you are able to reproduce the bug using a supported version and version skew of Kubernetes. See https://kubernetes.io/docs/setup/release/version-skew-policy for which versions and are currently supported.\n--> Running multiple replicas of a deployment, try to get the logs using label selector. \n\n**Anything else we need to know?**: NA\n\n**Environment**:\n- Kubernetes client and server versions (use `kubectl version`): v1.29.0\n- Cloud provider or hardware configuration: GCP\n- OS (e.g: `cat /etc/os-release`):  \n\n```\nNAME=\"Ubuntu\"\nVERSION=\"20.04.6 LTS (Focal Fossa)\"\nID=ubuntu\nID_LIKE=debian\nPRETTY_NAME=\"Ubuntu 20.04.6 LTS\"\nVERSION_ID=\"20.04\"\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nVERSION_CODENAME=focal\nUBUNTU_CODENAME=focal\n```\n\n",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/bug",
        "lifecycle/rotten",
        "triage/needs-information"
      ],
      "created_at": "2025-02-10T11:39:45Z",
      "closed_at": "2025-11-22T18:05:32Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1713",
      "comments_count": 7
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1169,
      "title": "Port-forward drops connection to pod after first connection",
      "problem": "**What happened**:\r\n\r\nWhen running Kubernetes `v1.23.1` on Minikube with kubectl `v1.23.2` I experienced the following unexpected behaviour when trying to create a port-forward to a pod running an arbitrary service.\r\n\r\n`kubectl version`:\r\n```\r\nClient Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.2\", GitCommit:\"9d142434e3af351a628bffee3939e64c681afa4d\", GitTreeState:\"clean\", BuildDate:\"2022-01-19T17:27:51Z\", GoVersion:\"go1.17.6\", Compiler:\"gc\", Platform:\"darwin/amd64\"}                                                        \r\nServer Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.1\", GitCommit:\"86ec240af8cbd1b60bcc4c03c20da9b98005b92e\", GitTreeState:\"clean\", BuildDate:\"2021-12-16T11:34:54Z\", GoVersion:\"go1.17.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n```\r\n\r\nWhat we see is that after the first netcat connection successfully closes we get a lost connection to the pod and the port-forward closes:\r\n\r\n`kubectl port-forward` to pod output:\r\n```\r\nForwarding from 127.0.0.1:5432 -> 5432\r\nForwarding from [::1]:5432 -> 5432\r\nHandling connection for 5432\r\nE0125 16:43:20.470080   17437 portforward.go:406] an error occurred forwarding 5432 -> 5432: error forwarding port 5432 to pod 55b25aeaae996c672f7eb762ce083e9b9666acabe96946d47790c167f1949d64, uid : exit status 1: 2022/01/25 15:43:20 socat[5831] E connect(5, AF=2 127.0.0.1:5432, 16): Connection refused\r\nE0125 16:43:20.470389   17437 portforward.go:234] lost connection to pod\r\n```\r\n\r\nWe would expect the the connection to stay open as is the case with Kubernetes before `v1.23.0`. \r\n\r\n**What you expected to happen**:\r\nWhen running the test against EKS running Kubernetes version `v1.21.5-eks-bc4871b` we get the port-forward behavior we are use to. The port-forward remains open after the first successful netcat connection.\r\n\r\n`kubectl version`:\r\n```\r\nClient Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.2\", GitCommit:\"9d142434e3af351a628bffee3939e64c681afa4d\", GitTreeState:\"clean\", BuildDate:\"2022-01-19T17:27:51Z\", GoVersion:\"go1.17.6\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\r\nServer Version: version.Info{Major:\"1\", Minor:\"21+\", GitVersion:\"v1.21.5-eks-bc4871b\", GitCommit:\"5236faf39f1b7a7dabea8df12726f25608131aa9\", GitTreeState:\"clean\", BuildDate:\"2021-10-29T23:32:16Z\", GoVersion:\"go1.16.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\nWARNING: version difference between client (1.23) and server (1.21) exceeds the supported minor version skew of +/-1\r\n```\r\nNotice how the kubectl version is v1.23.2 and the server version is v1.21.5-eks-bc4871b. EKS seems to manage version skew on its own somehow.\r\n\r\nThe output we get after opening multiple connections is what we expect. The connection is not closed after subsequent nc commands (don\u2019t be alarmed by the connection refusal by PostgreSQL, we are not using the right protocol or credentials. We are just trying to test the port-forward behavior and this is a simple approach to express the issue).\r\n\r\n`kubectl port-forward` to pod output:\r\n```\r\nForwarding from 127.0.0.1:5432 -> 5432\r\nForwarding from [::1]:5432 -> 5432\r\nHandling connection for 5432\r\nE0125 16:35:32.441184   17073 portforward.go:400] an error occurred forwarding 5432 -> 5432: error forwarding port 5432 to pod b4b99448ef949d8f4a2f7960edf5d25eaf0e3c7b82bb1fcd525c7f30ad2830d7, uid : exit status 1: 2022/01/25 15:35:32 socat[45088] E connect(5, AF=2 127.0.0.1:5432, 16): Connection refused\r\nHandling connection for 5432\r\nE0125 16:35:35.765744   17073 portforward.go:400] an error occurred forwarding 5432 -> 5432: error forwarding port 5432 to pod b4b99448ef949d8f4a2f7960edf5d25eaf0e3c7b82bb1fcd525c7f30ad2830d7, uid : exit status 1: 2022/01/25 15:35:35 socat[45202] E connect(5, AF=2 127.0.0.1:5432, 16): Connection refused\r\nHandling connection for 5432\r\nE0125 16:35:37.129167   17073 portforward.go:400] an error occurred forwarding 5432 -> 5432: error forwarding port 5432 to pod b4b99448ef949d8f4a2f7960edf5d25eaf0e3c7b82bb1fcd525c7f30ad2830d7, uid : exit status 1: 2022/01/25 15:35:37 socat[45243] E connect(5, AF=2 127.0.0.1:5432, 16): Connection refused\r\nHandling connection for 5432\r\n```\r\nAs we can see the port-forward connection lasts for many netcat connections. This is the behavior we expect.\r\n\r\nFor completeness this was tested using Minikube running `v1.21.5` Kubernetes. The problem still exists if we don't take into account version skew. But if we match the kubectl and Minikube Kubernetes version to `v1.21.5` then we get the expected behavior again of port-forwards remaining open past the first connection.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\nMy test is as follows: \r\n\r\n1. Open port forward to pod with a running service like PostgreSQL (`kubectl port-forward $POD_WITH_SERVICE  5432:5432`)\r\n2. Try and open a nc connections on localhost to the localport (`nc -v localhost 5432`)\r\n3. We should be able to open nc connection multiple times without the port-forward breaking (behaviour on Kubernetes before v1.23.0)\r\n\r\nTests were conducted against Kubernetes versions (v1.21.5, v1.22.1 and v1.23.1) on Minikube using `minikube start --kubernetes-version=v1.21.5`. Using `minikube kubectl -- ` we can match the kubectl version to the Kubernetes version Minikube is using to avoid version skew. The problem I describe only appears when running Kubernetes above v1.23.0.\r\n\r\n\r\n**Anything else we need to know?**:\r\nBased on the above testing it would seem that there is a bug introduced in kubectl  >`v1.23.0` which causes port-forwards to close immediately after a successful connection. This is a problem given the above test expects the old behaviour of long lasting kubectl port-forwards. My assumption is that this is a bug based on there being no mention of this behavior explicitly in [CHANGELOG-1.23.](http://changelog-1.23.md/) so it may be a regression. Could someone please shed light on whether this is a regression or expected future behavior now for reasons unbeknown to me?\r\n\r\n**Environment**:\r\n- Kubernetes client and server versions (use `kubectl version`): Listed above based on my expectations\r\n- Cloud provider or hardware configuration: minikube v1.25.1 on Darwin 12.1 using Docker Desktop 4.4.2 (73305) and EKS `v1.21.5-eks-bc4871b` to verify behavior.\r\n- OS (e.g: `cat /etc/os-release`): When testing locally on a Docker node:\r\n```\r\nID=ubuntu\r\nID_LIKE=debian\r\nPRETTY_NAME=\"Ubuntu 20.04.2 LTS\"\r\nVERSION_ID=\"20.04\"\r\nHOME_URL=\"https://www.ubuntu.com/\"\r\nSUPPORT_URL=\"https://help.ubuntu.com/\"\r\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\r\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\r\nVERSION_CODENAME=focal\r\nUBUNTU_CODENAME=focal\r\n```",
      "solution": "@eddiezane Yes, I think this is probably related to the \"fix\" in https://github.com/kubernetes/kubernetes/pull/103526.  I put \"fix\" in quotes because the fix was to allow port-forward to fail when there is an error instead of getting stuck in an unrecoverable non-failed state that can never process connections again.\r\n\r\n@mkfdoherty You mentioned that this is expected behavior:\r\n```\r\nForwarding from 127.0.0.1:5432 -> 5432\r\nForwarding from [::1]:5432 -> 5432\r\nHandling connection for 5432\r\nE0125 16:35:32.441184   17073 portforward.go:400] an error occurred forwarding 5432 -> 5432: error forwarding port 5432 to pod b4b99448ef949d8f4a2f7960edf5d25eaf0e3c7b82bb1fcd525c7f30ad2830d7, uid : exit status 1: 2022/01/25 15:35:32 socat[45088] E connect(5, AF=2 127.0.0.1:5432, 16): Connection refused\r\nHandling connection for 5432\r\nE0125 16:35:35.765744   17073 portforward.go:400] an error occurred forwarding 5432 -> 5432: error forwarding port 5432 to pod b4b99448ef949d8f4a2f7960edf5d25eaf0e3c7b82bb1fcd525c7f30ad2830d7, uid : exit status 1: 2022/01/25 15:35:35 socat[45202] E connect(5, AF=2 127.0.0.1:5432, 16): Connection refused\r\nHandling connection for 5432\r\nE0125 16:35:37.129167   17073 portforward.go:400] an error occurred forwarding 5432 -> 5432: error forwarding port 5432 to pod b4b99448ef949d8f4a2f7960edf5d25eaf0e3c7b82bb1fcd525c7f30ad2830d7, uid : exit status 1: 2022/01/25 15:35:37 socat[45243] E connect(5, AF=2 127.0.0.1:5432, 16): Connection refused\r\nHandling connection for 5432\r\n```\r\n\r\nBut I'm guessing that you continue to get connection refused even though the pod has failed and restarted.  It says it is handling the connection, but it fails every time, so it's not really forwarding them.  In this case port-forward is still technically running (from a process standpoint on your local machine), but is never able to forward connections again until you stop and restart it.  This was behavior of kubectl prior to 1.23.0.  \r\n\r\n@mkfdoherty  Can you double-check your kubectl version you were using in both cases?  I don't think this problem should be dependent on the cluster version which is why I'm asking.  It would surprise me if the behavior of port-forward using the same kubectl version would be different depending on the cluster version.\r\n\r\nAlso, can you check whether your pod has restarted while port-forward is running?  If that happens, the behavior from kubectl 1.23.0  and later is for the `kubectl port-forward` command to log an error saying \"lost connection to pod\" and exit.\r\n\r\n---\r\n\r\nFor reference, I tried reproducing using kubectl 1.23.1 with a 1.24.0-alpha cluster and also with a 1.21 cluster (this one an EKS cluster).\r\n\r\nI was starting a tcp echo server, like this:\r\n```\r\nkubectl run tcpecho --image=alpine --restart=Never -- /bin/sh -c \"apk add socat && socat -v tcp-listen:8080,fork EXEC:cat\"\r\nkubectl port-forward pod/tcpecho 8080\r\n```\r\n\r\nThen connecting like this:\r\n```\r\nnc -v localhost 8080\r\n```\r\n\r\nAre you able to reproduce the problem using the tcp echo server I mentioned above?\r\n\r\n\n\n---\n\n@eddiezane and @brianpursley I do agree that it does sound like the \"fix\" in https://github.com/kubernetes/kubernetes/pull/103526. Which is general is a fix. But I found it unexpected in this specific scenario that could be generalised to other cases of opening and closing connections to a service that a pod is running:\r\n1. A pod running PostgreSQL is up and running.\r\n2. We create a port-forward to the PostgreSQL pod using kubectl `v1.23.3`.\r\n3. We now open a successful `psql` connection to a database on the PostgreSQL instance via the port-forward.\r\n4. We close the connection to `psql` gracefully.\r\n5. The port-forward is now closed with the following error:\r\n```\r\nE0207 08:03:13.969992   13701 portforward.go:406] an error occurred forwarding 5432 -> 5432: error forwarding port 5432 to pod ae5cb9fc17a1a793190887ac6d87bb3bf12e06df55bb03e370480884d2b4d69f, uid : failed to execute portforward in network namespace \"/var/run/netns/cni-06fe22d7-1b91-ffc9-2c5b-568ff9137a34\": read tcp4 127.0.0.1:59580->127.0.0.1:5432: read: connection reset by peer\r\nE0207 08:03:13.970197   13701 portforward.go:234] lost connection to pod\r\n```\r\nWe expected the port-forward to remain open for subsequent psql connections (that we close after each use). This was the case before v1.23.x. I have tested using kubectl `v1.22.6` and port-forward does continue to remain open and functional even when we close psql connections although the port-forward does complain of errors (these errors are not unrecoverable). \r\n```\r\nHandling connection for 5432\r\nE0207 09:01:20.124576   14998 portforward.go:400] an error occurred forwarding 5432 -> 5432: error forwarding port 5432 to pod ae5cb9fc17a1a793190887ac6d87bb3bf12e06df55bb03e370480884d2b4d69f, uid : failed to execute portforward in network namespace \"/var/run/netns/cni-06fe22d7-1b91-ffc9-2c5b-568ff9137a34\": read tcp4 127.0.0.1:33310->127.0.0.1:5432: read: connection reset by peer\r\n```\r\n\r\nI would not consider this scenario to be an example of a port-forward error occurring that requires the port-forward connection to be closed. Opening a `psql` connection and closing the connection does not indicate that there is anything wrong with the underlying pod or the port-forward connection. So I would say that in this scenario the port-forward is in a recoverable state and can accept new connections unlike many other scenarios in which a port-forward may return error. Can we better distinguish between these different cases of error with port-forwards? \r\n\r\nI have run the echo server which does not cause the port-forward to break when using netcat to connect to it. This does indeed work as you have said. I would hope that this same bahaviour would be the case when opening up connections using `psql`. Opening and closing a netcat connection does not close the port-forward but opening and closing psql connections gracefully does return an error and close the port-forward. The only difference I see is that a netcat connection closing does not cause the port-forward return a recoverable error but closing the psql gracefully does. Are these scenarios so different? Might we not expect the same behaviour? Or might we consider it problematic to use the port-forward in this way?\r\n\r\nTo be clarify an error in my reproduction from the original post:\r\nMy original reproduction method using netcat does not suffice and was the result of a false positive. Minikube has network issues in a recent update causing my pods to fail periodically. I understand this to be the case since my pod is running Patroni which manages PostgreSQL and retries the PostgreSQL process without kubelet being aware of this (A major downside of this design approach). Therefore the port-forward would fail for good reasons without pods being actually being restarted by Kubelet. Which I think is the intended value behind the https://github.com/kubernetes/kubernetes/pull/103526 PR. I am sorry for realising this after the fact. I really appreciate you taking the time to reproduce my issue. I am using [kind](https://github.com/kubernetes-sigs/kind) and EKS now to avoid the current issue I experience with networking on Minikube for my use case. And so the issue appears to be only with kubectl versions as you have proposed.\r\n\n\n---\n\n> This was the case before v1.23.x. I have tested using kubectl v1.22.6 and port-forward does continue to remain open and functional even when we close psql connections although the port-forward does complain of errors (these errors are not unrecoverable).\r\n\r\nHmm, so maybe there are different types of errors, some unrecoverable where it makes sense to stop forwarding, but others which are recoverable like your example, which you mentioned.\r\n\r\nI\u2019ll have to test with psql and see if I can reproduce that way and see what the difference is.  It sounds like you are indeed hitting an issue with the change that was made in kubectl 1.23.0.\r\n\r\nIf that\u2019s the case, using kubectl <1.23 should be a workaround for now until we can figure out what is going on here and fix it.\r\n",
      "labels": [
        "kind/bug",
        "needs-triage"
      ],
      "created_at": "2022-01-26T08:35:34Z",
      "closed_at": "2022-03-02T17:24:20Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1169",
      "comments_count": 30
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1781,
      "title": "Add --run-as-user and --run-as-group flags to kubectl debug",
      "problem": "<!-- Please only use this template for submitting enhancement requests -->\n\n**What would you like to be added**:\n\nAdd `--run-as-user <id>` and `--run-as-group <id>` flags to `kubectl debug`. These would set the UID and GID of the ephemeral container in its SecurityContext to the provided values. If user is provided but group is not, then the user id is used for both. If both unset, the behavior is unchanged.\n\n**Why is this needed**:\n\nAttaching an ephemeral container to a pod is a useful technique to get a shell on a pod that doesn't have a shell, such as those made from distroless or scratch base images. In order to access the filesystem of the target pod, however, the user and group IDs of the ephemeral container must match that of the target container.\n\nCurrently kubectl lacks an easy way to set the UID and GID of the ephemeral container. If the target container uses a UID/GID that does not match the default of the image that they want to use for the ephemeral container then they must either create a custom image with appropriate default UID/GID or prepare a custom profile for use with kubectl. These options are inconvenient when all one wants to do is set the UID/GID of the ephemeral container to match that of the target container.\n\n\nCurrent behavior when the target pod has a different UID/GID than the ephemeral container:\n\n```\n$ _output/bin/kubectl debug -it testpod --target=testcontainer --image=cgr.dev/chainguard/nginx:latest-dev --profile restricted -- /bin/sh\nTargeting container \"testcontainer\". If you don't see processes from this container it may be because the container runtime doesn't support this feature.\nDefaulting debug container name to debugger-94rnp.\nAll commands and output from this session will be recorded in container logs, including credentials and sensitive information passed through the command prompt.\nIf you don't see a command prompt, try pressing enter.\n/ $ ps -ef\nPID   USER     TIME  COMMAND\n    1 1000      0:00 nginx: master process /usr/sbin/nginx -c /etc/nginx/nginx.conf -e /dev/stderr -g daemon off;\n   12 1000      0:00 nginx: worker process\n...\n   29 1000      0:00 nginx: worker process\n   30 nginx     0:00 /bin/sh\n   41 nginx     0:00 ps -ef\n/ $ id\nuid=65532(nginx) gid=65532(nginx) groups=65532(nginx)\n/ $ ls /proc/1/root/\nls: /proc/1/root/: Permission denied\n/ $ exit\n```\n\nProposed flags and result:\n\n```\n$ _output/bin/kubectl debug -it testpod --target=testcontainer --image=cgr.dev/chainguard/nginx:latest-dev --profile restricted --run-as-user 1000 -- /bin/sh\nTargeting container \"testcontainer\". If you don't see processes from this container it may be because the container runtime doesn't support this feature.\nDefaulting debug container name to debugger-vdnm7.\nAll commands and output from this session will be recorded in container logs, including credentials and sensitive information passed through the command prompt.\nIf you don't see a command prompt, try pressing enter.\n~ $ id\nuid=1000 gid=1000 groups=1000\n~ $ ps -ef\nPID   USER     TIME  COMMAND\n    1 1000      0:00 nginx: master process /usr/sbin/nginx -c /etc/nginx/nginx.conf -e /dev/stderr -g daemon off;\n   12 1000      0:00 nginx: worker process\n...\n   29 1000      0:00 nginx: worker process\n   72 1000      0:00 /bin/sh\n   84 1000      0:00 ps -ef\n~ $ ls /proc/1/root/\nbin           etc           lib           opt           product_name  root          sbin          tmp           var\ndev           home          lib64         proc          product_uuid  run           sys           usr\n~ $ exit\n```\n\nFor a target container with UID 1000 and GID 2000:\n\n```\n$ _output/bin/kubectl debug -it testpod --target=testcontainer --image=cgr.dev/chainguard/nginx:latest-dev --profile restricted --run-as-user 1000 --run-as-group 2000 -- /bin/sh\nTargeting container \"testcontainer\". If you don't see processes from this container it may be because the container runtime doesn't support this feature.\nDefaulting debug container name to debugger-mjtll.\nAll commands and output from this session will be recorded in container logs, including credentials and sensitive information passed through the command prompt.\nIf you don't see a command prompt, try pressing enter.\n~ $ id\nuid=1000 gid=2000 groups=2000\n~ $ ps -ef\nPID   USER     TIME  COMMAND\n    1 1000      0:00 nginx: master process /usr/sbin/nginx -c /etc/nginx/nginx.conf -e /dev/stderr -g daemon off;\n   12 1000      0:00 nginx: worker process\n...\n   29 1000      0:00 nginx: worker process\n   44 1000      0:00 /bin/sh\n   56 1000      0:00 ps -ef\n~ $ ls /proc/1/root\nbin           etc           lib           opt           product_name  root          sbin          tmp           var\ndev           home          lib64         proc          product_uuid  run           sys           usr\n~ $\n```\n\nI've prototyped this out (the above output is real) but wanted to get thoughts/feedback here before submitting a PR to kubernetes/kubernetes.",
      "solution": "/close\nI appreciate the effort put in to make these flags work but because we have an existing solution (custom profiling) and we do not want to encourage the further proliferation of flags we won't be accepting this feature request.\n\n---\n\n@mpuckett159: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubectl/issues/1781#issuecomment-3275612181):\n\n>/close\n>I appreciate the effort put in to make these flags work but because we have an existing solution (custom profiling) and we do not want to encourage the further proliferation of flags we won't be accepting this feature request.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>",
      "labels": [
        "kind/feature",
        "needs-triage"
      ],
      "created_at": "2025-09-08T03:56:05Z",
      "closed_at": "2025-09-10T16:08:47Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1781",
      "comments_count": 7
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1775,
      "title": "kubectl 1.34 broken completions",
      "problem": "**What happened**:\n\nSince kubectl 1.34 my tab-completion for some ressource (like deployements) look like this:\n\n```\n> kubectl get deployments\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ deploy\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ apps/v1\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ true\\ \\ \\ \\ Deployment\n```\n\n**What you expected to happen**:\n\nTo actually get the completion correctly like this:\n\n```\n> kubectl get deployments.apps\n```\n\n**How to reproduce it (as minimally and precisely as possible)**:\n\n1. Install kubectl v1.34\n2. type \"kubectl get dep\"\n3. press <TAB>\n\n**Anything else we need to know?**:\n\n**Environment**:\n- macOS\n",
      "solution": "I see the same problem (also on MacOS).\n\nCompletions work as expected with kubectl 1.33.4 but have the problem you describe starting with 1.34.0.\n\n---\n\nYou are right @brianpursley, the problem starts with commit cb33accc8fc (#132604)",
      "labels": [
        "kind/bug",
        "priority/critical-urgent",
        "triage/accepted"
      ],
      "created_at": "2025-08-28T13:58:45Z",
      "closed_at": "2025-08-29T08:03:11Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1775",
      "comments_count": 16
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1786,
      "title": "kubectl: Local node object mutated before server patch/update succeeds",
      "problem": "### What happened?\n\nIn pkg drain's CordonHelper.PatchOrReplaceWithContext\nThe local mutation (c.node.Spec.Unschedulable = c.desired) occurs before the patch/update request is sent and confirmed.\nhttps://github.com/kubernetes/kubernetes/blob/0bdf1f89c3a2eeac2a64ca29b93f552a4b601341/staging/src/k8s.io/kubectl/pkg/drain/cordon.go#L89-L110\n\nIf the Patch/Update fails, c.node now reflects the desired state even though the server-side state did not change.\n\n\n### What did you expect to happen?\n\nIs this mutation-before-commit intentional? Are callers expected to treat c.node as a working copy rather than a reflection of the server state?\nWould you consider avoiding mutation of the shared c.node until after a successful server response?\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nNA\n\n### Anything else we need to know?\n\nPotential solutions:\n- Work on a copy for diffing/patch generation\n- On success, replace c.node with the server response from Patch/Update to ensure local state matches the authoritative object.\n- ...(Open discussion on optimization approach)\n\nI prefer updating the local node' state from the server response after success.\nGiven that this function is a widely used public library utility, this approach aligns better with established behavior and ensures the local node state remains correct and consistent with the server state.\n\nI\u2019m happy to open a PR implementing one of these approaches.\nThanks!\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nClient Version: v1.32.3\nKustomize Version: v5.5.0\nServer Version: v1.32.8-dirty\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nOKD version: 4.19.0-okd-scos.18\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\nNAME=\"CentOS Stream CoreOS\"\nVERSION=\"9.0.20250827-0\"\nID=\"centos\"\nID_LIKE=\"rhel fedora\"\nVERSION_ID=\"9\"\nPLATFORM_ID=\"platform:el9\"\nPRETTY_NAME=\"CentOS Stream CoreOS 9.0.20250827-0\"\nANSI_COLOR=\"0;31\"\nLOGO=\"fedora-logo-icon\"\nCPE_NAME=\"cpe:/o:centos:centos:9\"\nHOME_URL=\"https://centos.org/\"\nBUG_REPORT_URL=\"https://issues.redhat.com/\"\nREDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux 9\"\nREDHAT_SUPPORT_PRODUCT_VERSION=\"CentOS Stream\"\nOSTREE_VERSION='9.0.20250827-0'\nVARIANT=CoreOS\nVARIANT_ID=coreos\nOPENSHIFT_VERSION=\"4.19\"\n$ uname -a\nLinux xxxxxxxxxx 5.14.0-611.el9.x86_64 kubernetes/kubernetes#1 SMP PREEMPT_DYNAMIC Fri Aug 22 16:46:08 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n",
      "solution": "Thanks for the question, let me clarify:\nSending a patch that reflects the desired spec is absolutely valid.\nThe issue is about when we record that desired state locally, we mutate the shared in-memory object (c.node) to the desired state before the server accepts the change.\n\nIf the Patch/Update fails, we end up with:\n- Server: still at the old state (unchanged).\n- c.node: already mutated to the desired state.\n\n---\n\nThis method in a public package that any consumer can call. We didn\u2019t discover the issue via kubectl cordon, but in openshift/machine-config-operator\u2019s drain_controller. In that controller, a pre-existing node reference is used to initialize the CordonHelper. With this usage, if the API request fails, the desired state gets written into the shared local object before the server accepts it, which triggers the inconsistency I described earlier.",
      "labels": [
        "kind/bug",
        "priority/backlog",
        "sig/cli",
        "triage/accepted"
      ],
      "created_at": "2025-09-27T16:17:32Z",
      "closed_at": "2025-10-01T05:56:17Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1786",
      "comments_count": 9
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1784,
      "title": "Error: failed to authenticate: DeviceCodeCredential: Key not valid for use in specified state.",
      "problem": "<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\n\nIf the matter is security related, please disclose it privately via https://kubernetes.io/security/\n-->\n\n**What happened**:\nWhen trying to execute some commands like `kubectl version` or `kubectl cluster-info`, it asks to do a device login by opening a webpage. After inserting credentials in Azure Kubernetes Service AAD Client and hit Confirm, it gets back to the CLI returning a DeviceCodeCredential error.\n\n```\nError: failed to authenticate: DeviceCodeCredential: Key not valid for use in specified state.\nUnable to connect to the server: getting credentials: exec: executable kubelogin failed with exit code 1\n```\n\nI tried to execute after having the cli installed for a while in my computer, but I changed my computer user to be local account instead of Microsoft Account, which I think is responsible for causing it, but I don't know how to fix it.\n\n**What you expected to happen**:\nTo work normally after /devicelogin.\n\n**How to reproduce it (as minimally and precisely as possible)**:\n<!-- Please make sure you are able to reproduce the bug using a supported version and version skew of Kubernetes. See https://kubernetes.io/docs/setup/release/version-skew-policy for which versions and are currently supported.\n-->\n\n```\n> az login --tenant XYZXYZ \u2705\n> az account set --subscription XYZXYZ \u2705\n> az aks get-credentials -n aks-qa-v1 -g aks-qa-v1-rg \u2705\n> kubectl cluster-info dump \u274c\nTo sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code XYZXYZXYZ to authenticate.\nError: failed to authenticate: DeviceCodeCredential: Key not valid for use in specified state.\nUnable to connect to the server: getting credentials: exec: executable kubelogin failed with exit code 1\n```\n\n**Anything else we need to know?**:\n\n**Environment**:\n- Kubernetes client and server versions (use `kubectl version`):\n```\n> kubectl version\nTo sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code XYZXYZXYZ to authenticate.\nError: failed to authenticate: DeviceCodeCredential: Key not valid for use in specified state.\nClient Version: v1.33.1\nKustomize Version: v5.6.0\nUnable to connect to the server: getting credentials: exec: executable kubelogin failed with exit code 1\n```\n- Cloud provider or hardware configuration: Azure Kubernetes\n- OS (e.g: `cat /etc/os-release`):\n  ```\n  OS Name:                   Microsoft Windows 11 Pro\n  OS Version:                10.0.22621 N/A Build 22621\n  Hotfix(s):                 4 Hotfix(s) Installed.\n                             [01]: KB5044033\n                             [02]: KB5012170\n                             [03]: KB5044285\n                             [04]: KB5046247\n  . . .\n  ```\n\n### What have I tried so far\n\n- Removing ~/.kube folder\n- Removing ~/.azure folder\n- Removing `C:\\Users\\$USER\\AppData\\Roaming\\Microsoft\\Crypto\\PSA` folder\n- Removing `C:\\Users\\$USER\\AppData\\Local\\Microsoft\\Credentials` folder\n- Removing `C:\\Users\\$USER\\AppData\\Roaming\\Microsoft\\Credentials` folder\n- `az account clear`\n- `az logout` and then `az login`\n- `az config set core.encrypt_token_cache=false`\n- Removing and adding back the work accounts in Windows Accounts > Access work or school under my user \n",
      "solution": "Thanks @ardaguclu and so sorry for the mistake.\n\nRunning the following line fixed it for me!\n`kubelogin convert-kubeconfig -l azurecli`\n\n---\n\n> Thanks [@ardaguclu](https://github.com/ardaguclu) and so sorry for the mistake.\n> \n> Running the following line fixed it for me! `kubelogin convert-kubeconfig -l azurecli`\n\nNo worries. I'm happy to see that your issue has been resolved. So I'm closing this.\n/close\n\n---\n\n@ardaguclu: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubectl/issues/1784#issuecomment-3314736428):\n\n>> Thanks [@ardaguclu](https://github.com/ardaguclu) and so sorry for the mistake.\n>> \n>> Running the following line fixed it for me! `kubelogin convert-kubeconfig -l azurecli`\n>\n>No worries. I'm happy to see that your issue has been resolved. So I'm closing this.\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>",
      "labels": [
        "kind/bug",
        "needs-triage"
      ],
      "created_at": "2025-09-19T17:18:50Z",
      "closed_at": "2025-09-20T07:34:03Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1784",
      "comments_count": 5
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1770,
      "title": "`kubectl scale` shows confusing/concatenated error vs. clear NotFound from other commands",
      "problem": "<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\n\nIf the matter is security related, please disclose it privately via https://kubernetes.io/security/\n-->\n\n**What happened**:\n\nWhen scaling a non-existent resource, `kubectl scale` prints a confusing message that looks like two errors glued together: `error: no objects passed to scale <api-group> \"<resource-name>\" not found`\n\n**What you expected to happen**:\n\nmessage like `Error from server (NotFound): <api-group> <resource-name> not found` as reported by other commands when resource is not found.\n\n**How to reproduce it (as minimally and precisely as possible)**:\n<!-- Please make sure you are able to reproduce the bug using a supported version and version skew of Kubernetes. See https://kubernetes.io/docs/setup/release/version-skew-policy for which versions and are currently supported.\n-->\n- run `kubectl scale` over a non-existing resource\n\n**Anything else we need to know?**:\n\n**Environment**:\n- Kubernetes client and server versions (use `kubectl version`): \n```\nClient Version: v1.33.4\nServer Version: v1.32.2\n```\n\n",
      "solution": "@mpuckett159: \n\tThis request has been marked as suitable for new contributors.\n\n### Guidelines\nPlease ensure that the issue body includes answers to the following questions:\n- Why are we solving this issue?\n- To address this issue, are there any code changes? If there are code changes, what needs to be done in the code and what places can the assignee treat as reference points?\n- How can the assignee reach out to you for help?\n\n\nFor more details on the requirements of such an issue, please see [here](https://www.kubernetes.dev/docs/guide/help-wanted/#good-first-issue) and ensure that they are met.\n\nIf this request no longer meets these requirements, the label can be removed\nby commenting with the `/remove-good-first-issue` command.\n\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubectl/issues/1770):\n\n>/triage accepted\n>/good-first-issue\n>/kind feature\n>We should definitely standardize this to more closely match other error messages.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>",
      "labels": [
        "help wanted",
        "kind/bug",
        "kind/feature",
        "good first issue",
        "triage/accepted"
      ],
      "created_at": "2025-08-14T15:52:26Z",
      "closed_at": "2025-09-12T05:43:49Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1770",
      "comments_count": 10
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1771,
      "title": "kubectl describe should display the involvedObject Field in Events",
      "problem": "This issue was introduced by: https://github.com/kubernetes/kubernetes/pull/130471\n\nSummary:\nThe kubelet generates events for containers within a Pod, but the Message field of these events does not include the container name.\n\nAs a result, when users run kubectl describe pods, they see events like:\n```\nEvents:\n  Type     Reason               Age               From               Message\n  ----     ------               ----              ----               -------\n  ...\n  Normal   Pulling              9s (x2 over 20s)  kubelet            Pulling image \"nginx\"\n  Normal   Pulling              5s (x2 over 14s)  kubelet            Pulling image \"nginx\"\n  Warning  FailedPostStartHook  5s (x2 over 14s)  kubelet            PostStartHook failed\n  Normal   Killing              5s (x2 over 14s)  kubelet            FailedPostStartHook\n  Warning  FailedPreStopHook    5s (x2 over 14s)  kubelet            PreStopHook failed\n  Warning  FailedPostStartHook  0s (x2 over 10s)  kubelet            PostStartHook failed\n  Normal   Killing              0s (x2 over 10s)  kubelet            FailedPostStartHook\n  Warning  FailedPreStopHook    0s (x2 over 10s)  kubelet            PreStopHook failed\n  ...\n```\nAs we can see, the container name is not included in the Message of these events, which makes it somewhat inconvenient to troubleshoot the status of a multi-container pod based on events.\n\nHowever, considering that the container-related fields are already included in the `involvedObjects` field of the events, having the kubelet redundantly add the container name to the Message seems unnecessary.\n\nTherefore, perhaps this could be better addressed in kubectl. `kubectl describe pods` could either:\n\n* Default to displaying the `involvedObjects.fieldpath` field for events, or\n\n* Add a flag like `--show-fieldpath` to display the `involvedObjects.fieldpath` field for events.\n\nWe would greatly appreciate any suggestions from SIG CLI on this matter.",
      "solution": "I think that displaying the container context is crucial for debugging. Rather than adding a new `--show-fieldpath` flag, I've implemented a solution that includes the `involvedObject.fieldPath` directly in the default `kubectl describe` output.\n\nThis approach is more user-friendly as it surfaces this critical information by default, eliminating the need for users to discover and remember a special flag. The change is non-breaking and provides immediate clarity on which container an event belongs to.",
      "labels": [
        "kind/feature",
        "triage/accepted"
      ],
      "created_at": "2025-08-16T09:26:46Z",
      "closed_at": "2025-09-11T00:05:58Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1771",
      "comments_count": 2
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1766,
      "title": "`kubectl apply`: Deleting one instance of a duplicate environment variable removes all occurrences \u2013 breaking change affecting Helm, Helmfile, ArgoCD, and automation toolchains",
      "problem": "**What happened**:\n\nWhen an environment variable (e.g., `NODE_ENV`) appears multiple times in the `env` array in a Kubernetes manifest, and one duplicate is removed, running `kubectl apply` will remove **all** instances of that variable from the live resource. Only after a subsequent apply (with the deduplicated manifest) will the variable be added again.\n\nThis is a breaking change compared to prior behavior, where the first instance was preserved. In modern deployment pipelines, manifests are often generated automatically, and such duplicities can easily occur.\n\n> [!important] \n> **This change fundamentally alters the behavior of `kubectl apply` regarding duplicate environment variables\u2014a core functionality used daily by thousands of users for over a decade.**\n\n**Why this is a critical problem**:\n\n- **Automated toolchains** (Helm, Helmfile, ArgoCD, Flux, GitHub Actions, CI/CD, GitOps, etc.) often generate or process manifests where accidental duplicates are possible and hard to eliminate completely.\n- **Workaround (\"use server-side apply\") is not acceptable** \u2013 many toolchains, including Helm and ArgoCD, rely on the default client-side apply and cannot easily switch to server-side apply.\n- **This behavior can cause outages or severe misconfiguration in production**, as critical environment variables may be silently removed from running workloads.\n- **Detection is difficult**: The issue is not obvious and may lead to silent failures, security issues, or outages if environment variables disappear unexpectedly.\n- **This is not backwards compatible** with previous `kubectl`/Kubernetes behavior and is a regression for many users.\n\n**How to reproduce it (minimized)**:\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: nginx\n          env:\n            - name: NODE_ENV\n              value: \"qa\"\n            - name: NODE_ENV\n              value: \"qa\"\n```\n\n1. `kubectl apply -f deployment.yaml` # creates deployment with duplicated env\n2. Remove one of the `NODE_ENV` envs from the manifest.\n3. `kubectl apply -f deployment.yaml` # both NODE_ENV are removed from the live object\n4. `kubectl apply -f deployment.yaml` # only now is NODE_ENV re-added\n\n**Expected behavior**:\n\n- Removing one duplicate should only remove that instance; at least one instance of the env var should remain.\n- Ideally, apply should error on duplicate keys in lists, or at minimum, preserve the first occurrence, and never silently remove all.\n\n**Environment:**\n\n- **Kubernetes Client Version:** `v1.30.5`\n- **Kubernetes Server Version:** `v1.30-v1.33`\n- **OS:** Linux\n\n## Why the current workaround is insufficient\n\n- Not all toolchains support server-side apply.\n- **Helm**, **Helmfile**, **ArgoCD** and many others cannot simply be reconfigured.\n- Manual linting or deduplication is not always feasible in complex CI/CD environments.\n\n## Request\n\n- Please consider this a critical regression and re-evaluate the merge/diff algorithm.\n- At minimum, `kubectl apply` should fail with a clear error on duplicated items in lists (such as env).\n- Never silently remove all instances of a variable if just one was removed from the input manifest.\n\n## Related\n\n- Prior closed issue: https://github.com/kubernetes/kubectl/issues/1750 (closed with \"workaround\" that is not suitable for most real-world use cases)\n\n---\n**This bug can cause production outages and is not a corner case. Please re-open or escalate.**",
      "solution": "> Workaround (\"use server-side apply\") is not acceptable \u2013 many toolchains, including Helm and ArgoCD, rely on the default client-side apply and cannot easily switch to server-side apply.\n\n`--server-side` apply is not the workaround. It is a direct replacement of client-side apply. We can't change the default behavior, as it is not backwards compatible. But server-side apply is our solution to fix this problem.\n\nAs a result, **all the tools** should be switched to server-side apply.\n\n---\n\n> This is not backwards compatible with previous kubectl/Kubernetes behavior and is a regression for many users.\n\n@JuryA do you know the version of the latest kubectl that this functionality works. If this was working and suddenly stopped working on newer versions. I believe that this needs to be fixed.\n\nIf this is not a regression, `--server-side` is still the valid recommendation.",
      "labels": [
        "kind/bug",
        "needs-triage"
      ],
      "created_at": "2025-08-04T13:41:46Z",
      "closed_at": "2025-08-26T11:55:40Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1766",
      "comments_count": 8
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1774,
      "title": "Add a safeguard for `kubectl delete <workloads> --all --all-namespaces`.",
      "problem": "<!-- Please only use this template for submitting enhancement requests -->\nCurrently the following command is allowed without any restriction:\n`kubectl delete deployments --all --all-namespaces`\nThis holds potential to delete every resource of that type cluster-wide, including system-critical workloads in kube-system and other namespaces. Although some components may eventually recover, but still could cause harm to the system. Although this behavior is technically correct and consistent with how `kubectl` works; it is extremely easy to run by mistake or misuse. A single command can trigger a cluster-wide outage. (I did, somehow recovered, but it could have been much worse.)\n\n**What would you like to be added**:\nThere should be a safeguard around this, whether it could be `strong warning bar` or `explicit confirmation prompt`. \n\n\n**Why is this needed**:\nTo prevent accidental or unintentional cluster-wide deletions that can wipe out both application workloads and system components, leading to downtime and critical issue.\n",
      "solution": "This was discussed in multiple issues in the past (you can see the historical context from similar issues in https://github.com/kubernetes/kubectl/issues?q=is%3Aissue%20state%3Aclosed%20all-namespaces%20delete). The reason we don't fix this issue is to keep backwards compatibility. Because any fix would mean that a behavioral change which is not ideal for community (considering the number of usages of this command after the long years of existence).\n\nSo the recommended resolution by the sig-cli is using the kuberc (https://github.com/kubernetes/enhancements/tree/master/keps/sig-cli/3104-introduce-kuberc). You can add enable (`--interactive` flag of delete command in your kuberc file). After that kubectl delete will always ask confirmation before proceeding to any deletion.\n\nI think, we'll have to close this as not a bug.",
      "labels": [
        "kind/feature",
        "needs-triage"
      ],
      "created_at": "2025-08-24T12:42:48Z",
      "closed_at": "2025-08-24T15:26:21Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1774",
      "comments_count": 3
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1772,
      "title": "kubectl edit behaves strangely on v1.33 client (editor issue, possibly resource-related)",
      "problem": "I\u2019m experiencing strange behavior when using kubectl edit with Client Version: v1.33.\n\nWhen I run, for example:\n\n`kubectl edit svc <service-name>\n`\nthe file opens, but editing becomes very hard \u2014 the cursor in the editor jumps around / acts unpredictably and makes it difficult to modify the manifest.\n\nEnvironment\n\nkubectl Client Version: v1.33.4\n\nServer Version: v1.33.1\n\nEditor: acting same over SSH(from Mac) or local(Ubuntu24).\n\nQuestions:\nCould it be tied to the number of resources in the cluster (many pods/services/etc.)?\nOr is this more likely an integration problem with the editor when launched via kubectl edit?\n\nSteps to Reproduce:\nRun kubectl edit svc <svc-name>\nTry to make changes in the editor\nNotice cursor behaving strangely (difficult to place text properly)\n\nWhat I Expected\nEditing to behave normally (like when opening the same file directly in the editor).\n\nWhat Happened\nCursor in the editor acts strangely, making edits very hard.\n\n**PS: export KUBE_EDITOR=\"nano\" or \"nvim\" has fixed the issue for me. It exists if I go back to \"vi\" however.** \n",
      "solution": "@ardaguclu after some digging, I can confirm this is related mostly to vi. I am closing the issue. TY!",
      "labels": [
        "sig/cli",
        "needs-triage"
      ],
      "created_at": "2025-08-18T15:38:15Z",
      "closed_at": "2025-08-19T10:30:16Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1772",
      "comments_count": 5
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1744,
      "title": "`kubectl diff` unable to detect changes made by `kubectl edit`",
      "problem": "**What happened**:\nAfter adding some new fields to an existing resource by editing it with `kubectl edit`, following apply/diff commands are unable to detect changes made.\n\n**What you expected to happen**:\n`kubectl diff` to detect changes.\n\n**How to reproduce it (as minimally and precisely as possible)**:\n1. create following configmap:\n```shell\n$ cat cm.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: diff-test\ndata:\n  key1: value1\n  key2: value2\n```\n\n```shell\n$ kubectl apply -f cm.yaml\nconfigmap/diff-test created\n```\n\n2. edit the configmap and add `key3: value3` to data:\n\n```shell\n$ kubectl edit configmap diff-test\nconfigmap/diff-test edited\n```\n\n3. get configmap and see modification is done on live object:\n\n```shell\n$ kubectl get configmap diff-test -oyaml\napiVersion: v1\ndata:\n  key1: value1\n  key2: value2\n  key3: value3\nkind: ConfigMap\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"v1\",\"data\":{\"key1\":\"value1\",\"key2\":\"value2\"},\"kind\":\"ConfigMap\",\"metadata\":{\"annotations\":{},\"name\":\"diff-test\",\"namespace\":\"cloud\"}}\n  creationTimestamp: \"2025-05-15T13:46:29Z\"\n  name: diff-test\n  namespace: cloud\n  resourceVersion: \"1456590571\"\n  uid: dc46a856-f498-46e2-abba-52ab5ce7674d\n```\n\n4. check diff and see it is not detected:\n\n```shell\n$ kubectl diff -f cm.yaml\n```\n\n**Anything else we need to know?**:\n\nI think this is caused by mixing declarative (kubectl apply) and imperative (kubectl edit) approaches. With client side apply three-way merge, and with server-side apply different field managers makes kubectl think extra data is added by some operator/controller and ignored during diff check. \n\nI also tested using server side apply with `--server-side=true`, still the diff is not detected.\n\n**Environment**:\nKubernetes client and server versions (use `kubectl version`):\n\nClient Version: v1.33.0\nKustomize Version: v5.6.0\nServer Version: v1.32.3-eks-bcf3d70\n\nThis might not be a bug, maybe it is intentional. If so, what is the suggested way to make `kubectl diff` detect these manually made changes ?\n\nI am aware doing `kubectl get -oyaml >> live.yaml` and checking diff between local manifest and live manifest is a solution but our CI tools are not able to do it, they use `kubectl diff`\n",
      "solution": "diff command (when it is used for client-side apply) relies on `last-applied-configuration` annotation. It compares the objects by checking from this annotation. diff command works well with apply command because apply (when it is used for client-side apply) manages the `last-applied-configuration` in each client-side execution. \n\nedit command is not like that. As you can see the output after the edit command execution, although you saw `key3`, this wasn't represented in your `last-applied-configuration` annotation. So that diff returns nothing.\n\nThe resolution is you should configure edit command by modifying this annotation after your operation is completed by passing `--save-config` flag.\n\nAs a result, `kubectl edit --save-config` should work.\n\nThis is not a bug and designed that way;\n/close\n\n---\n\n@ardaguclu: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubectl/issues/1744#issuecomment-2885607523):\n\n>diff command (when it is used for client-side apply) relies on `last-applied-configuration` annotation. It compares the objects by checking from this annotation. diff command works well with apply command because apply (when it is used for client-side apply) manages the `last-applied-configuration` in each client-side execution. \n>\n>edit command is not like that. As you can see the output after the edit command execution, although you saw `key3`, this wasn't represented in your `last-applied-configuration` annotation. So that diff returns nothing.\n>\n>The resolution is you should configure edit command by modifying this annotation after your operation is completed by passing `--save-config` flag.\n>\n>As a result, `kubectl edit --save-config` should resolve what you expect. \n>\n>This is not a bug and designed that way;\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>",
      "labels": [
        "kind/bug",
        "needs-triage"
      ],
      "created_at": "2025-05-15T14:35:02Z",
      "closed_at": "2025-05-16T04:33:54Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1744",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1475,
      "title": "Only show what I have permission to see in kubectl get all",
      "problem": "Currently `kubectl get all` is a useful default tool for getting a quick look at what is going on in my context, despite the misleading name I still like it. But if I don't have permission to view some of the resources I see errors, and that's distracting and quite unhelpful. It make an empty namespace look like a train wreck (you only see errors, and many people add CRDs to the \"all\" list but don't give you permission to see them). Would it be really hard to just skip the error messages for permission denied, by default anyway?",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/feature",
        "needs-triage"
      ],
      "created_at": "2023-08-23T13:19:25Z",
      "closed_at": "2025-07-28T02:11:41Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1475",
      "comments_count": 18
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1758,
      "title": "Kubectl fails to connect through proxy",
      "problem": "\n\n**What happened**:\nkubectl fails to connect to cluster when proxy variables are set\n```\nenv|grep -i proxy\nhttps_proxy=https://user:password@myproxy.com:3129\nhttp_proxy=http://myproxy.com:3128\nALL_PROXY=https://user:password@myproxy.com:3129\n\nkubectl get pods\nE0707 18:03:26.967819   63072 memcache.go:265] \"Unhandled Error\" err=\"couldn't get current server API group list: Get \\\"https://CLUSTER_IP:6443/api?timeout=32s\\\": proxyconnect tcp: remote error: tls: handshake failure\"\n```\n\nWhen I run with -v9\n```\nkubectl get pods -v9\nI0707 18:12:43.834329   66034 loader.go:402] Config loaded from file:  /Users/me/.kube/config\nI0707 18:12:43.835806   66034 envvar.go:172] \"Feature gate default state\" feature=\"WatchListClient\" enabled=false\nI0707 18:12:43.835821   66034 envvar.go:172] \"Feature gate default state\" feature=\"ClientsAllowCBOR\" enabled=false\nI0707 18:12:43.835827   66034 envvar.go:172] \"Feature gate default state\" feature=\"ClientsPreferCBOR\" enabled=false\nI0707 18:12:43.835832   66034 envvar.go:172] \"Feature gate default state\" feature=\"InformerResourceVersion\" enabled=false\nI0707 18:12:43.835837   66034 envvar.go:172] \"Feature gate default state\" feature=\"InOrderInformers\" enabled=true\nI0707 18:12:43.835919   66034 discovery_client.go:252] \"Request Body\" body=\"\"\nI0707 18:12:43.836450   66034 round_trippers.go:527] \"Request\" curlCommand=<\n\tcurl -v -XGET  -H \"User-Agent: kubectl/v1.33.2 (darwin/arm64) kubernetes/a57b6f7\" -H \"Authorization: Bearer <masked>\" -H \"Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json\" 'https://CLUSTER_IP:6443/api?timeout=32s'\n >\nI0707 18:12:43.942257   66034 round_trippers.go:547] \"HTTP Trace: DNS Lookup resolved\" host=\"myproxy.com\" address=[{\"IP\":\"x.x.x.x\",\"Zone\":\"\"}]\nI0707 18:12:44.076404   66034 round_trippers.go:562] \"HTTP Trace: Dial succeed\" network=\"tcp\" address=\"x.x.x.x:3129\"\nI0707 18:12:44.222826   66034 round_trippers.go:632] \"Response\" verb=\"GET\" url=\"https://CLUSTER_IP:6443/api?timeout=32s\" status=\"\" headers=\"\" milliseconds=386 dnsLookupMilliseconds=103 dialMilliseconds=134 tlsHandshakeMilliseconds=146\nE0707 18:12:44.223243   66034 memcache.go:265] \"Unhandled Error\" err=\"couldn't get current server API group list: Get \\\"https://CLUSTER_IP:6443/api?timeout=32s\\\": proxyconnect tcp: remote error: tls: handshake failure\"\nI0707 18:12:44.224433   66034 cached_discovery.go:120] skipped caching discovery info due to Get \"https://CLUSTER_IP:6443/api?timeout=32s\": proxyconnect tcp: remote error: tls: handshake failure\n```\n\nHowever I can curl that just fine:\n```\n curl -v -XGET  -H \"Authorization: Bearer KUBE_TOKEN\" -H \"Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json\" -H \"User-Agent: kubectl/v1.33.2 (darwin/arm64) kubernetes/a57b6f7\" 'https://CLUSTER_IP:6443/api?timeout=32s' | jq .\n{\n  \"kind\": \"APIGroupDiscoveryList\",\n  \"apiVersion\": \"apidiscovery.k8s.io/v2\",\n  \"metadata\": {},\n  \"items\": [\n...\n...\n...\n```\n\nIf I run kubectl as a docker container, version 1.29 works, but not 1.30+\n```\nalias k='docker run --rm -v ${KUBECONFIG}:/.kube/config -e https_proxy=$https_proxy bitnami/kubectl:1.29'\nk get pods \nNo resources found in default namespace.\n\nalias k='docker run --rm -v ${KUBECONFIG}:/.kube/config -e https_proxy=$https_proxy bitnami/kubectl:1.30'\nk get pods\nE0707 22:22:55.235839       1 memcache.go:265] couldn't get current server API group list: Get \"https://CLUSTER_IP/k8s/clusters/c-m-g6d8s5mt/api?timeout=32s\": proxyconnect tcp: remote error: tls: handshake failure\n\n\nalias k='docker run --rm -v ${KUBECONFIG}:/.kube/config -e https_proxy=$https_proxy bitnami/kubectl:1.31'\nE0707 22:23:01.488768       1 memcache.go:265] \"Unhandled Error\" err=\"couldn't get current server API group list: Get \\\"https://CLUSTER_IP/k8s/clusters/c-m-g6d8s5mt/api?timeout=32s\\\": proxyconnect tcp: remote error: tls: handshake failure\"\n```\n\n\n**What you expected to happen**:\nI expect to get successful return from kubectl\n\n\n\n**Environment**:\nApple ARM\nkubectl version 1.33.2\ncluster version v1.31.3+rke2r1\n\n\n\n\nProxy is Squid 3.5.20\n",
      "solution": "Hi @chipmanc, \n\nI think this issue is related to the Go version upgrade between kubectl 1.29 and 1.30. Kubernetes 1.29 was built with [Go 1.21](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.29.md#changelog-since-v1280), while Kubernetes 1.30 upgraded to [Go 1.22](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.30.md#changelog-since-v1290).\n\nThe critical change in Go 1.22 is the [removal of RSA key exchange cipher suites from the default TLS configuration](https://go.dev/doc/go1.22#crypto/tls). Your Squid 3.5.20 proxy (from 2015) likely only supports these older RSA-based cipher suites, which would explain why the TLS handshake fails with kubectl 1.30+ but succeeds with curl (which uses OpenSSL and still includes RSA cipher suites).\n\nTo verify this hypothesis, you can try setting the `GODEBUG` environment variable to re-enable RSA key exchange:\n ```bash\n export GODEBUG=tlsrsakex=1\n kubectl get pods\n ```\n\nIf this fixes the issue, it confirms that the removal of RSA cipher suites in Go 1.22 is the root cause. The [Go 1.22 release notes](https://go.dev/doc/go1.22#crypto/tls) specifically mention this environment variable as a compatibility workaround.\n\nFor a permanent solution, I'd recommend upgrading your proxy infrastructure to support modern ECDHE cipher suites, as the RSA key exchange cipher suites were removed for security reasons. \n\ncc @ardaguclu \n\n---\n\n@rushmash91 thank you for investing time on this. Your comment above explains the issue.\n\n@chipmanc please let us know about the results.",
      "labels": [
        "kind/bug",
        "triage/accepted"
      ],
      "created_at": "2025-07-07T22:28:03Z",
      "closed_at": "2025-07-22T12:58:02Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1758",
      "comments_count": 7
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1759,
      "title": "Supplying kubeconfig via process subsitution ( --kubeconfig=<(cat kubeconfig.file) ) doesn't work",
      "problem": "Hey, don't know if bug or not supported but supplying a kubeconfig via process substitution ([1](https://www.gnu.org/software/bash/manual/html_node/Process-Substitution.html), [2](https://tldp.org/LDP/abs/html/process-sub.html)) doesn't seem to work for me. (v1.32.4-dispatcher on Ubuntu 24.04.2 LTS)\n\nIn short:\n```bash\n# this works\nkubectl --kubeconfig=config.test get pods\nNo resources found in default namespace.\n\n# this doesn't\nkubectl --v=6 --kubeconfig=<(cat config.test) get pods\nI0714 08:14:11.674703    7125 loader.go:395] Config loaded from file:  /dev/fd/63\nI0714 08:14:11.675601    7125 round_trippers.go:553] GET http://localhost:8080/api?timeout=32s  in 0 milliseconds\nE0714 08:14:11.675693    7125 memcache.go:265] \"Unhandled Error\" err=\"couldn't get current server API group list: Get \\\"http://localhost:8080/api?timeout=32s\\\": dial tcp 127.0.0.1:8080: connect: connection refused\"\nI0714 08:14:11.676925    7125 cached_discovery.go:120] skipped caching discovery info due to Get \"http://localhost:8080/api?timeout=32s\": dial tcp 127.0.0.1:8080: connect: connection refused\nI0714 08:14:11.677497    7125 round_trippers.go:553] GET http://localhost:8080/api?timeout=32s  in 0 milliseconds\nE0714 08:14:11.677545    7125 memcache.go:265] \"Unhandled Error\" err=\"couldn't get current server API group list: Get \\\"http://localhost:8080/api?timeout=32s\\\": dial tcp 127.0.0.1:8080: connect: connection refused\"\nI0714 08:14:11.678696    7125 cached_discovery.go:120] skipped caching discovery info due to Get \"http://localhost:8080/api?timeout=32s\": dial tcp 127.0.0.1:8080: connect: connection refused\nI0714 08:14:11.678720    7125 shortcut.go:103] Error loading discovery information: Get \"http://localhost:8080/api?timeout=32s\": dial tcp 127.0.0.1:8080: connect: connection refused\nI0714 08:14:11.679191    7125 round_trippers.go:553] GET http://localhost:8080/api?timeout=32s  in 0 milliseconds\nE0714 08:14:11.679232    7125 memcache.go:265] \"Unhandled Error\" err=\"couldn't get current server API group list: Get \\\"http://localhost:8080/api?timeout=32s\\\": dial tcp 127.0.0.1:8080: connect: connection refused\"\nI0714 08:14:11.680464    7125 cached_discovery.go:120] skipped caching discovery info due to Get \"http://localhost:8080/api?timeout=32s\": dial tcp 127.0.0.1:8080: connect: connection refused\nI0714 08:14:11.680885    7125 round_trippers.go:553] GET http://localhost:8080/api?timeout=32s  in 0 milliseconds\nE0714 08:14:11.680926    7125 memcache.go:265] \"Unhandled Error\" err=\"couldn't get current server API group list: Get \\\"http://localhost:8080/api?timeout=32s\\\": dial tcp 127.0.0.1:8080: connect: connection refused\"\nI0714 08:14:11.682078    7125 cached_discovery.go:120] skipped caching discovery info due to Get \"http://localhost:8080/api?timeout=32s\": dial tcp 127.0.0.1:8080: connect: connection refused\nI0714 08:14:11.682628    7125 round_trippers.go:553] GET http://localhost:8080/api?timeout=32s  in 0 milliseconds\nE0714 08:14:11.682670    7125 memcache.go:265] \"Unhandled Error\" err=\"couldn't get current server API group list: Get \\\"http://localhost:8080/api?timeout=32s\\\": dial tcp 127.0.0.1:8080: connect: connection refused\"\nI0714 08:14:11.683770    7125 cached_discovery.go:120] skipped caching discovery info due to Get \"http://localhost:8080/api?timeout=32s\": dial tcp 127.0.0.1:8080: connect: connection refused\nI0714 08:14:11.683811    7125 helpers.go:264] Connection error: Get http://localhost:8080/api?timeout=32s: dial tcp 127.0.0.1:8080: connect: connection refused\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\n\n# cat doesn't destroy the formatting. this also works\ncat config.test > config.test.cat\nkubectl --kubeconfig=config.test.cat get pods\nNo resources found in default namespace.\n\n# config view returns empty config\nkubectl --v=9 --kubeconfig=<(cat config.test) config view\nI0714 08:14:34.521052    7274 loader.go:395] Config loaded from file:  /dev/fd/63\napiVersion: v1\nclusters: null\ncontexts: null\ncurrent-context: \"\"\nkind: Config\npreferences: {}\nusers: null\n```\n\nBackground:\nThis is in the context of CI/CD pipelines. When generating a kubeconfig for the cluster on my cloud provider, I have the option to instead of writing it to disk to just print it to stdout. So I want to capture this output and directly supply it to kubectl via process substitution. I want to avoid ever writing the kubeconfig to disk.\nI want to do smth like this:\n```\nKUBECONFIG_DATA=$(<command to generate and print kubeconfig to stdout> | base64 -w 0)\nkubectl --kubeconfig=<(echo $KUBECONFIG_DATA | base64 -d) get pods \n```\n\nShould this work? If it can't can anybody elaborate why not? I've seen articles suggesting that this worked on older kubectl versions ([1](https://www.ameyalokare.com/technology/kubernetes/ci/cd/github-actions/2021/10/30/exploring-kubernetes-automated-deployment-part-1.html), [2](https://medium.com/synechron/a-more-secure-way-to-call-kubectl-from-terraform-1052adf37af8)). This issue seems to also be present on the helm cli ([1](https://github.com/helm/helm/issues/11419), [2](https://github.com/helm/helm/issues/9183), [3](https://github.com/helm/helm/issues/10376)). I know it's a different tool but maybe the underlying mechanism is the same.\n\nedit: From what I understand, the issue is that the `/dev/fd/63` file disappears after being read one time. So, I guess it is actually read multiple times, which doesn't work and is not caught by the code. I don't know whether the issue is that somehow `o.configAccess.GetExplicitFile()` gets called multiple times in kubectl or this is an issue in client-go and  [LoadFromFile()](https://github.com/kubernetes/client-go/blob/3a4ad9c658b1bd280b1ddf397669c9876b05fdba/tools/clientcmd/loader.go#L393) in [client-go/tools/clientcmd/loader.go](https://github.com/kubernetes/client-go/blob/master/tools/clientcmd/loader.go#L393) gets called multiple times. I'm not familiar with the code so I may not entirely get what's going on here.",
      "solution": "Oh, dispatcher is also a GCP supplied kubectl customized binary so it is not exactly something we here support, and it is possible that this is what is causing the issue, because the kubeconfig read happens really early in the binary processes and dispatcher does some intelligent like version selection based on the server, so that could be what is causing the config to get lost. Could you also try using the open source distribution?",
      "labels": [
        "kind/support",
        "needs-triage"
      ],
      "created_at": "2025-07-11T07:10:34Z",
      "closed_at": "2025-07-21T18:13:31Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1759",
      "comments_count": 9
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1654,
      "title": "Add kubectl create secret --from-base64-literal and --random-value-for-key",
      "problem": "### What would you like to be added?\r\n\r\n`kubectl create secret --from-literal=key=secret-value` is very handy. I have a use case where I'm creating a secret that needs to be binary (an encryption key), be of a certain size, and use all of those bits for entropy. I'm taking randomness from `/dev/urandom` but cannot use it as a literal value since it's binary.\r\n\r\nI could put the secret material into a file and use `kubectl create secret --from-file=./file` but:\r\n\r\n- Secret data hitting the disk is undesirable as it's potentially less secure.\r\n- Secret is potentially exposed on disk, even if briefly.\r\n- It's a hassle to clean up the file in the script, potentially leaving it behind, exposing it later.\r\n\r\nIdeally I'd use something like `kubectl create secret --from-base64-literal=\"key=$(head -c 64 /dev/urandom | base64)\"` to generate a value of 64 bytes, base64 encode it, pass to kubectl. It would decode it, persist into the cluster.\r\n\r\nYet another idea (can open a separate issue) is to have `--random-value-for-key=key=64`. It would generate a random value of length 64 and persist it into the `key` key. Even nicer and more secure (secret is not exposed on disk and via command line flags, where it can be seen by other processes) for bootstrapping a secret (from a script or console).\r\n\r\n/sig cli\r\n\r\n### Why is this needed?\r\n\r\nBetter scripting experience, more secure secret data handling.",
      "solution": "/assign\r\n\r\nmaybe it is festiable by using stdin device, here is my workaround.\r\n```\r\nhead -c 64 /dev/urandom | kubectl create secret genetic my-secret --from-file=/dev/stdin\r\n```\r\n\r\nEDIT:\r\nAlthough there is a workaround, I still vote to adding this flag.\r\n1. This method seems not portiable in Windows(Sorry I'm not familiar with Windows, correct me if I'm wrong). \r\n2. Flag way is simpler and have better experience.\n\n---\n\nThere is a workaround as stated in https://github.com/kubernetes/kubectl/issues/1654#issuecomment-2346265086\r\n\r\n```\r\nhead -c 64 /dev/urandom | kubectl create secret genetic my-secret --from-file=/dev/stdin\r\n```\r\n\r\nand this workaround is also a reasonable and looks not hacky. \r\n\r\nWe usually would consider adding more flags, if there is no other way around and in that case there is.\n\n---\n\n> Added --from-base64-literal feature to secret kubernetes/kubernetes#127609\r\n\r\n@Bhargav-manepalli  \r\nI'm afraid you have to give a good reason to convince sig-cli of the necessity of this parameter. AFAIK, for signle key with random data, reading from stdin is a fine solution. maybe mulitiple key with random value? But I wander is this a real world scenario? \ud83e\udd14 \ud83d\udcad ",
      "labels": [
        "kind/feature",
        "sig/cli",
        "needs-triage"
      ],
      "created_at": "2024-09-05T00:42:17Z",
      "closed_at": "2025-04-23T16:24:29Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1654",
      "comments_count": 30
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1757,
      "title": "Not working tab completion for kubectl get command",
      "problem": "When I am trying to tab complete command:\n```\nk get\n```\nIt produces no output and hangs; typing has no effect, so I need to press Ctrl+C to interrupt it.\n\n\nWhen I do:\n```\nk auth\n```\n\nI get:\n```\ncan-i      (Check whether an action is allowed)\nreconcile  (Reconciles rules for RBAC role, role binding, cluster role, and cluster role binding objects)\nwhoami     (Experimental: Check self subject attributes)\n```\n\nEnvironment:\nShell: Bash\nOS: Fedora 42\nkubectl version --client:\nClient Version: v1.29.15\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\n",
      "solution": "After discussion in discord server, I found out that the problem was that I didnt do `minikube start`.\nBut I would appreciate to get some error message that would tell me that I need to do minikube start first",
      "labels": [
        "kind/bug",
        "needs-triage"
      ],
      "created_at": "2025-07-06T21:48:42Z",
      "closed_at": "2025-07-07T09:36:24Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1757",
      "comments_count": 6
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1682,
      "title": "very slow apply performance due to openapi schema reparsing",
      "problem": "**What happened**:\r\nkubectl apply is very slow in reading manifest files. It appears to be the case that kubectl rereads its configuration several times per manifest it is supposed to apply which gets extremely slow, several minutes when applying large amount of manifests.\r\n\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n```\r\n$ cat kubeconfig\r\napiVersion: v1\r\nclusters:\r\n- cluster:\r\n    certificate-authority: /tmp/cluster.pem\r\n    server: https://cluster.url\r\n  name: cluster\r\ncontexts:\r\n- context:\r\n    cluster: cluster\r\n    user: user\r\n  name: cluster-user\r\ncurrent-context: cluster-user\r\nkind: Config\r\npreferences: {}\r\nusers:\r\n- name: user\r\n  user:\r\n    token: ...\r\n\r\nfor i in {1..10}; do echo '{  \"apiVersion\": \"v1\",  \"kind\": \"Namespace\",   \"metadata\": { \"name\": \"ns'$i'\" }}' ; done > manifests\r\n$ cat manifests\r\n{  \"apiVersion\": \"v1\",  \"kind\": \"Namespace\",   \"metadata\": { \"name\": \"ns1\" }}\r\n{  \"apiVersion\": \"v1\",  \"kind\": \"Namespace\",   \"metadata\": { \"name\": \"ns2\" }}\r\n{  \"apiVersion\": \"v1\",  \"kind\": \"Namespace\",   \"metadata\": { \"name\": \"ns3\" }}\r\n...\r\n```\r\n\r\nNow run following dry-run kubectl and observe how often it opens (and reads) the certificate-authority file cluster.pem:\r\n\r\n```\r\n$ kubectl version\r\nClient Version: v1.31.0\r\nKustomize Version: v5.4.2\r\n\r\n$ for i in {1..10}; do echo '{  \"apiVersion\": \"v1\",  \"kind\": \"Namespace\",   \"metadata\": { \"name\": \"ns'$i'\" }}' ; done > manifests\r\n$ KUBECONFIG=config GOMAXPROCS=2 strace -f -e openat apply --dry-run=client  -f manifests|& grep cluster.pem -c\r\n27\r\n# run in 1 second\r\n$ for i in {1..100}; do echo '{  \"apiVersion\": \"v1\",  \"kind\": \"Namespace\",   \"metadata\": { \"name\": \"ns'$i'\" }}' ; done > manifests\r\n$ KUBECONFIG=config GOMAXPROCS=2 strace -f -e openat apply --dry-run=client  -f manifests|& grep cluster.pem -c\r\n207\r\n# run in 12 seconds\r\n```\r\n\r\nIncreasing the number of objects in the manifests file increase the times it reads the cluster certificate from its configuration which does not seem necessary.\r\nedit: see below, the real problem is repeated json decoding of the the openapi schema documents\r\n\r\nWhen applying more manifests the time before kubectl even contacts the server increases to several minutes which can be very relevant for e.g. CI test runs.\r\n\r\nThe problem should not be the the manifest parsing time, as e.g. python yaml or the golang yq version can parse these manifests in a fraction of the time it takes kubectl.\r\n\r\n",
      "solution": "yes, the problem occurs before it even starts talking to the server regarding the manifests (and after it has setup its resource caches from the servers api-resources)\n\n---\n\nI don't see an obvious way to make kubectl reuse the decoded schemas for multiple resource visits, though the client-go openapi class already does cache the json document in bytes form.\r\nhttps://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/client-go/openapi/cached/groupversion.go#L47\r\n\r\nSo a possible solution would be to adjust client-go to cache the decoded json instead of or in addition and the kubectl issue would automatically be resolved.\r\nI assume that would be an issue to be filed in the main kubernetes project?\n\n---\n\nPersonally, I'd file the issue against kubectl as it's kubectl you want to enhance, but I can see the case for making a generic client-go solution.",
      "labels": [
        "kind/feature",
        "triage/accepted"
      ],
      "created_at": "2024-12-04T10:14:14Z",
      "closed_at": "2025-07-02T11:03:28Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1682",
      "comments_count": 21
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1621,
      "title": "Include Namespace in `kubectl delete` Dry-Run Output",
      "problem": "<!-- Please only use this template for submitting enhancement requests -->\r\n\r\n**What would you like to be added:**\r\n\r\nPrefix namespace to kubectl delete dry-run result.\r\n\r\n#### current:\r\n```\r\n$ kubectl delete -f ... --dry-run=server\r\ndeployment.apps \"myapp\" deleted (server dry run)\r\n$\r\n```\r\n\r\n#### proposed:\r\n```\r\n$ kubectl delete -f ... --dry-run=server\r\nmyapp-namespace deployment.apps \"myapp\" deleted (server dry run)\r\n$\r\n```\r\n\r\n**Why is this needed:**\r\n\r\nThe current output is ambiguous, as the resource name should be identifiable as unique.\r\n\r\nWhen working with multiple namespaces like \"myapp-prod\" and \"myapp-dev\", and intending to tear down some resources in \"myapp-dev\", the command might look like this:\r\n```\r\n$ kustomize build . | kubectl delete -f - --dry-run=server\r\ndeployment.apps \"kube-prometheus-stack-kube-state-metrics\" deleted (server dry run)\r\n$\r\n```\r\nFrom this output, it's unclear whether the manifest targets \"myapp-dev\" or \"myapp-prod\". This ambiguity requires additional checks to ensure the correct namespace is being targeted.\r\n\r\nPrinting the namespace in the dry-run output would enhance clarity and confidence in identifying the targeted resources.\r\n\r\n**Other considerations**\r\nThis change can be applied for other operations like apply and replace. However, non-delete operations can be validate with the \"diff\" command.\r\nTherefore, I think it is acceptable to add this feature only for the delete operation.\r\n\r\nsample implementation would be like this: https://github.com/totegamma/kubernetes/commit/65c18816d3bc8b47810d1230bbf88e8aef219a5e\r\n\r\nIf this issue accepted, I want to get assigned and make a PR.",
      "solution": "@mpuckett159: \n\tThis request has been marked as suitable for new contributors.\n\n### Guidelines\nPlease ensure that the issue body includes answers to the following questions:\n- Why are we solving this issue?\n- To address this issue, are there any code changes? If there are code changes, what needs to be done in the code and what places can the assignee treat as reference points?\n- Does this issue have zero to low barrier of entry?\n- How can the assignee reach out to you for help?\n\n\nFor more details on the requirements of such an issue, please see [here](https://git.k8s.io/community/contributors/guide/help-wanted.md#good-first-issue) and ensure that they are met.\n\nIf this request no longer meets these requirements, the label can be removed\nby commenting with the `/remove-good-first-issue` command.\n\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubectl/issues/1621):\n\n>/triage accepted\r\n>/good-first-issue\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>",
      "labels": [
        "help wanted",
        "kind/feature",
        "good first issue",
        "triage/accepted"
      ],
      "created_at": "2024-07-08T09:46:57Z",
      "closed_at": "2025-06-24T00:00:32Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1621",
      "comments_count": 10
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1232,
      "title": "kubectl edit fails with 'error: there was a problem with the editor \"vi\"' when you search for something that is not in the file",
      "problem": "**What happened**:\r\n\r\nThe bug is very simple. If you use `kubectl edit` to edit a resource and your editor is `vi/vim`, then if you try to search for something in the file, and your search fails because nothing was found in the file, then whenever you save your changes, vim will error out with:\r\n\r\n> error: there was a problem with the editor \"vi\"\r\n\r\nIt doesn't matter what you do before or after the search, this error will always occur. In fact, even if you don't do anything with the file, just search and quit (:q), the error will already appear.\r\n\r\nThe search I am mentioning is done via `/<text>` in vim.\r\n\r\nAlso, if you search for something that actually exists, the error does not occur.\r\n\r\n**What you expected to happen**:\r\n\r\nkubectl edit should succeed.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n1. Run `kubectl edit <any resource>` making sure that your editor is vi/vim.\r\n2. Type `/randomTextThatDoNotExistInTheFile`, which will search for a word that does not exist.\r\n3. Type `:wq` to leave vim\r\n\r\n**Anything else we need to know?**:\r\n\r\n**Environment**:\r\n- Kubernetes client and server versions (use `kubectl version`):\r\n```\r\n$ kubectl version\r\nWARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\r\nClient Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.2\", GitCommit:\"f66044f4361b9f1f96f0053dd46cb7dce5e990a8\", GitTreeState:\"clean\", BuildDate:\"2022-06-15T14:14:10Z\", GoVersion:\"go1.18.3\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\r\nKustomize Version: v4.5.4\r\nServer Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.10\", GitCommit:\"a7a32748b5c60445c4c7ee904caf01b91f2dbb71\", GitTreeState:\"clean\", BuildDate:\"2022-02-16T11:18:16Z\", GoVersion:\"go1.16.14\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\nWARNING: version difference between client (1.24) and server (1.21) exceeds the supported minor version skew of +/-1\r\n```\r\n- Cloud provider or hardware configuration:\r\nI think this happens with all cloud providers, but this was done in AWS.\r\n- OS (e.g: `cat /etc/os-release`):\r\nMAC OS X Monterey 12.4\r\n\r\nAlso,\r\n```\r\nvim --version\r\nVIM - Vi IMproved 8.2 (2019 Dec 12, compiled Apr 19 2022 21:41:43)\r\n```",
      "solution": "@brianpursley Thanks for sharing. I was looking for a solution like this. I had previously tried exporting the values to KUBE_EDITOR and EDITOR, but didn\u2019t get the expected results. However, I\u2019ve now managed to set it up by adding it to my ~/.bashrc, so I no longer need to type it every time.\n\n```shell\necho 'export kubedit=\"KUBE_EDITOR=vim kubectl edit\"' >> ~/.bashrc\n```\n```shell\nsource ~/.bashrc\n```\nthen easy to use\n```shell \nkubedit <resource> <resource_name>\n```",
      "labels": [
        "kind/bug",
        "needs-triage"
      ],
      "created_at": "2022-06-23T21:42:30Z",
      "closed_at": "2022-07-20T16:43:28Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1232",
      "comments_count": 8
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1730,
      "title": "When use kubectl describe hpa get hpa metrics incorrect",
      "problem": "What happened:\nWhen use `kubectl describe hpa` get hpa metrics incorrect. There is an extra `m`  in the unit of `resource memory on pods`, and the value should be divided by 1000 to be correct.\n\nHow to reproduce it (as minimally and precisely as possible):\n```bash\n\n\nkubectl get --raw /apis/metrics.k8s.io/v1beta1/namespaces/prod-app/pods/xxl-7cbf685b5b-7s7pb\n\n{\"kind\":\"PodMetrics\",\"apiVersion\":\"metrics.k8s.io/v1beta1\",\"metadata\":{\"name\":\"xxl-7cbf685b5b-7s7pb\",\"namespace\":\"prod-app\",\"creationTimestamp\":\"2025-03-27T06:20:17Z\",\"labels\":{\"admission.datadoghq.com/enabled\":\"true\",\"app.kubernetes.io/instance\":\"xxl\",\"app.kubernetes.io/name\":\"twwin\",\"pod-template-hash\":\"7cbf685b5b\",\"tags.datadoghq.com/service\":\"xxl\"}},\"timestamp\":\"2025-03-27T06:20:07Z\",\"window\":\"17.104s\",\"containers\":[{\"name\":\"xxl\",\"usage\":{\"cpu\":\"1010123648n\",\"memory\":\"1852124Ki\"}}]}\n\nkubectl get --raw /apis/metrics.k8s.io/v1beta1/namespaces/prod-app/pods/xxl-7cbf685b5b-962gl\n\n{\"kind\":\"PodMetrics\",\"apiVersion\":\"metrics.k8s.io/v1beta1\",\"metadata\":{\"name\":\"xxl-7cbf685b5b-962gl\",\"namespace\":\"prod-app\",\"creationTimestamp\":\"2025-03-27T06:20:18Z\",\"labels\":{\"admission.datadoghq.com/enabled\":\"true\",\"app.kubernetes.io/instance\":\"xxl\",\"app.kubernetes.io/name\":\"twwin\",\"pod-template-hash\":\"7cbf685b5b\",\"tags.datadoghq.com/service\":\"xxl\"}},\"timestamp\":\"2025-03-27T06:20:04Z\",\"window\":\"16.021s\",\"containers\":[{\"name\":\"xxl\",\"usage\":{\"cpu\":\"887341813n\",\"memory\":\"1868996Ki\"}}]}\n\nkubectl get --raw /apis/metrics.k8s.io/v1beta1/namespaces/prod-app/pods/xxl-7cbf685b5b-jsw4l\n\n{\"kind\":\"PodMetrics\",\"apiVersion\":\"metrics.k8s.io/v1beta1\",\"metadata\":{\"name\":\"xxl-7cbf685b5b-jsw4l\",\"namespace\":\"prod-app\",\"creationTimestamp\":\"2025-03-27T06:20:18Z\",\"labels\":{\"admission.datadoghq.com/enabled\":\"true\",\"app.kubernetes.io/instance\":\"xxl\",\"app.kubernetes.io/name\":\"twwin\",\"pod-template-hash\":\"7cbf685b5b\",\"tags.datadoghq.com/service\":\"xxl\"}},\"timestamp\":\"2025-03-27T06:20:08Z\",\"window\":\"14.046s\",\"containers\":[{\"name\":\"xxl\",\"usage\":{\"cpu\":\"931186875n\",\"memory\":\"1839636Ki\"}}]}\n\nkubectl get --raw /apis/metrics.k8s.io/v1beta1/namespaces/prod-app/pods/xxl-7cbf685b5b-qsb7t\n\n{\"kind\":\"PodMetrics\",\"apiVersion\":\"metrics.k8s.io/v1beta1\",\"metadata\":{\"name\":\"xxl-7cbf685b5b-qsb7t\",\"namespace\":\"prod-app\",\"creationTimestamp\":\"2025-03-27T06:20:19Z\",\"labels\":{\"admission.datadoghq.com/enabled\":\"true\",\"app.kubernetes.io/instance\":\"xxl\",\"app.kubernetes.io/name\":\"twwin\",\"pod-template-hash\":\"7cbf685b5b\",\"tags.datadoghq.com/service\":\"xxl\"}},\"timestamp\":\"2025-03-27T06:20:13Z\",\"window\":\"18.11s\",\"containers\":[{\"name\":\"xxl\",\"usage\":{\"cpu\":\"642321645n\",\"memory\":\"1842416Ki\"}}]}\n\nkubectl get --raw /apis/metrics.k8s.io/v1beta1/namespaces/prod-app/pods/xxl-7cbf685b5b-qww4r\n\n{\"kind\":\"PodMetrics\",\"apiVersion\":\"metrics.k8s.io/v1beta1\",\"metadata\":{\"name\":\"xxl-7cbf685b5b-qww4r\",\"namespace\":\"prod-app\",\"creationTimestamp\":\"2025-03-27T06:20:19Z\",\"labels\":{\"admission.datadoghq.com/enabled\":\"true\",\"app.kubernetes.io/instance\":\"xxl\",\"app.kubernetes.io/name\":\"twwin\",\"pod-template-hash\":\"7cbf685b5b\",\"tags.datadoghq.com/service\":\"xxl\"}},\"timestamp\":\"2025-03-27T06:20:07Z\",\"window\":\"18.829s\",\"containers\":[{\"name\":\"xxl\",\"usage\":{\"cpu\":\"572363174n\",\"memory\":\"1857472Ki\"}}]}\n\nkubectl get --raw /apis/metrics.k8s.io/v1beta1/namespaces/prod-app/pods/xxl-7cbf685b5b-s82rz\n\n{\"kind\":\"PodMetrics\",\"apiVersion\":\"metrics.k8s.io/v1beta1\",\"metadata\":{\"name\":\"xxl-7cbf685b5b-s82rz\",\"namespace\":\"prod-app\",\"creationTimestamp\":\"2025-03-27T06:20:20Z\",\"labels\":{\"admission.datadoghq.com/enabled\":\"true\",\"app.kubernetes.io/instance\":\"xxl\",\"app.kubernetes.io/name\":\"twwin\",\"pod-template-hash\":\"7cbf685b5b\",\"tags.datadoghq.com/service\":\"xxl\"}},\"timestamp\":\"2025-03-27T06:20:02Z\",\"window\":\"16.855s\",\"containers\":[{\"name\":\"xxl\",\"usage\":{\"cpu\":\"918341005n\",\"memory\":\"1846032Ki\"}}]}\n\n\nkubectl describe hpa  xxl -n prod-app\nName:                       xxl\nNamespace:                  prod-app\nLabels:                     <none>\nAnnotations:                <none>\nCreationTimestamp:          Wed, 26 Mar 2025 15:08:34 +0800\nReference:                  Deployment/xxl\nMetrics:                    ( current / target )\n  resource cpu on pods:     827m / 1600m\n  resource memory on pods:  1895539370666m / 3277M ### this issue:1895539370666m\nMin replicas:               6\nMax replicas:               8\nBehavior:\n  Scale Up:\n    Stabilization Window: 60 seconds\n    Select Policy: Max\n    Policies:\n      - Type: Percent  Value: 50  Period: 60 seconds\n      - Type: Pods     Value: 1   Period: 60 seconds\n  Scale Down:\n    Stabilization Window: 300 seconds\n    Select Policy: Min\n    Policies:\n      - Type: Percent  Value: 25  Period: 60 seconds\n      - Type: Pods     Value: 1   Period: 60 seconds\nDeployment pods:       6 current / 6 desired\nConditions:\n  Type            Status  Reason            Message\n  ----            ------  ------            -------\n  AbleToScale     True    ReadyForNewScale  recommended size matches current size\n  ScalingActive   True    ValidMetricFound  the HPA was able to successfully calculate a replica count from cpu resource\n  ScalingLimited  True    TooFewReplicas    the desired replica count is less than the minimum replica count\nEvents:           <none>\n```\n\nEnvironment\n```bash\nmacos 14.6 (23G80)\n\nkubectl version\nClient Version: v1.32.0\nKustomize Version: v5.5.0\nServer Version: v1.31.6-eks-bc803b4\n\nKubernetes verion\nKubernetes gitVersion: v1.31.4-eks-2d5f260\nKubernetes buildDate: 2024-12-13 04:56:32\nKubernetes platform: linux/amd64\n```\n\nSame as this situation #1250 ",
      "solution": "@mpuckett159: \n\tThis request has been marked as suitable for new contributors.\n\n### Guidelines\nPlease ensure that the issue body includes answers to the following questions:\n- Why are we solving this issue?\n- To address this issue, are there any code changes? If there are code changes, what needs to be done in the code and what places can the assignee treat as reference points?\n- How can the assignee reach out to you for help?\n\n\nFor more details on the requirements of such an issue, please see [here](https://www.kubernetes.dev/docs/guide/help-wanted/#good-first-issue) and ensure that they are met.\n\nIf this request no longer meets these requirements, the label can be removed\nby commenting with the `/remove-good-first-issue` command.\n\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubectl/issues/1730):\n\n>/triage accepted\n>/good-first-issue\n>Thanks for reporting this.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>",
      "labels": [
        "help wanted",
        "kind/support",
        "kind/bug",
        "good first issue",
        "triage/accepted"
      ],
      "created_at": "2025-03-27T06:40:21Z",
      "closed_at": "2025-06-20T19:27:26Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1730",
      "comments_count": 6
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1648,
      "title": "kubectl version --error-on-version-mismatch",
      "problem": "### What would you like to be added?\n\nadd an flag for ``kubectl version`` which fails if the client version exceeds the supported minor version skew of +/-1\r\n\r\n``--warnings-as-errors`` isn't working as this only treats warnings as errors when received from the server. the ``kubectl version`` warning is a client side warning.\r\n\r\non the other side ``--match-server-version`` is too strict.\r\n\r\nor add ``kubectl check-support`` or something similar\n\n### Why is this needed?\n\navoid running unsupported kubectl versions",
      "solution": "@mpuckett159: \n\tThis request has been marked as suitable for new contributors.\n\n### Guidelines\nPlease ensure that the issue body includes answers to the following questions:\n- Why are we solving this issue?\n- To address this issue, are there any code changes? If there are code changes, what needs to be done in the code and what places can the assignee treat as reference points?\n- Does this issue have zero to low barrier of entry?\n- How can the assignee reach out to you for help?\n\n\nFor more details on the requirements of such an issue, please see [here](https://git.k8s.io/community/contributors/guide/help-wanted.md#good-first-issue) and ensure that they are met.\n\nIf this request no longer meets these requirements, the label can be removed\nby commenting with the `/remove-good-first-issue` command.\n\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubectl/issues/1648):\n\n>/triage accepted\r\n>/good-first-issue\r\n>It seems like there were a few plugins to address this in the past that have gone missing.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>",
      "labels": [
        "help wanted",
        "kind/feature",
        "sig/cli",
        "good first issue",
        "triage/accepted"
      ],
      "created_at": "2024-09-06T12:51:43Z",
      "closed_at": "2025-05-19T17:19:16Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1648",
      "comments_count": 9
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1076,
      "title": "'kubectl exec --tty --stdin' \"leaking\" proccess inside pod when terminated",
      "problem": "## Overview\r\n\r\nWhen stopping a local `kubectl exec` process without explicitly exiting the spawned in-pod process, the latter remains running inside the pod. I am unsure whether the `--tty --stdin` flags are relevant.\r\n\r\n## Reproducing\r\n\r\nExplicitly, the issue I am seeing is a bash process left over inside the pod after going through a procedure like the following:\r\n\r\n1. Launch a bash shell inside some pod\r\n\r\n```\r\nlocal$ kubectl exec -ti some-pod -- bash\r\nsome-pod# echo $$\r\n<pid>\r\n```\r\n\r\n2. Kill above `kubectl` process (*e.g.* by closing terminal, explicit `kill`, *etc.*)\r\n\r\n3. Verify that the above `<pid>` still remains inside the pod:\r\n\r\n```\r\nlocal$ kubectl exec -ti some-pod -- bash\r\nsome-pod# echo /proc/<pid>/*\r\n```\r\n\r\n## Expected Behaviour\r\n\r\nI am quite new to k8s overall, so if this is just a naive user error, please forgive the spam. What I expect to happen is for the in-pod `bash` process to also terminate when `kubectl` terminates.\r\n\r\nThis issue is occurring within the large dev team of my organization. Due to developers running `kubectl exec` under WSL and closing windows instead of properly exiting bash, we are left with pods having hundreds of \"trash\" processes sticking around. Some of these lingering processes end up tying up resources unexpectedly and causing errors in our application.\r\n\r\n## Thoughts\r\n\r\nAbove, the local `kubectl` process receives `SIGTERM` or `SIGSTOP` depending on how it's killed. On a local machine spawning local processes, one would expect the in-pod \"child process\" to recieve SIGCHLD and terminate (or handle) accordingly.\r\n\r\nSetting `trap 'echo CHLD >test.log' CHLD` within the in-pod `bash` process above confirms that `SIGCHLD` fails to propagate. Does this sound correct? Is this intended?",
      "solution": "The problem itself is beyond kubectl itself. I'm suggesting to investigate this and open the issue with appropriate container engine. kubectl relies heavily on that behavior for this one.\r\n\r\n/close\n\n---\n\n@soltysh: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubectl/issues/1076#issuecomment-901253204):\n\n>The problem itself is beyond kubectl itself. I'm suggesting to investigate this and open the issue with appropriate container engine. kubectl relies heavily on that behavior for this one.\r\n>\r\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>",
      "labels": [
        "needs-triage"
      ],
      "created_at": "2021-06-22T00:54:49Z",
      "closed_at": "2021-08-18T16:25:22Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1076",
      "comments_count": 7
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1723,
      "title": "describe resource fail for kubectl v1.32.2",
      "problem": "<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\n\nIf the matter is security related, please disclose it privately via https://kubernetes.io/security/\n-->\n\n**What happened**:\nfail to describe deployment when using the latest kubectl.\ncommond:     \n`./kubectl-1.32.2 describe deployment --kubeconfig ./kubeconfig\n`\nerror:    \n`Error from server (NotAcceptable): object *v1.Deployment does not implement the protobuf marshalling interface and cannot be encoded to a protobuf message`\n\n**What you expected to happen**:\n\n**How to reproduce it (as minimally and precisely as possible)**:\n<!-- Please make sure you are able to reproduce the bug using a supported version and version skew of Kubernetes. See https://kubernetes.io/docs/setup/release/version-skew-policy for which versions and are currently supported.\n-->\n\n**Anything else we need to know?**:\n\n**Environment**:\n- Kubernetes client and server versions (use `kubectl version`): v1.32.2\n- Cloud provider or hardware configuration:\n- OS (e.g: `cat /etc/os-release`):   linux amd64\n\n",
      "solution": "Can you elaborate more on the steps to reproduce the issue?\nI tried running the following commands but didn't encounter any errors:\n```\n\uf8ff ~/ k version\nClient Version: v1.32.2\nKustomize Version: v5.5.0\nServer Version: v1.32.2\n\uf8ff ~/ k describe deployment --kubeconfig $HOME/.kube/config \nName:                   nginx-deployment\nNamespace:              default\nCreationTimestamp:      Fri, 07 Mar 2025 22:56:35 +0200\nLabels:                 app=nginx\nAnnotations:            deployment.kubernetes.io/revision: 1\nSelector:               app=nginx\nReplicas:               3 desired | 3 updated | 3 total | 0 available | 3 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\n  Labels:  app=nginx\n  Containers:\n   nginx:\n    Image:         nginx:1.14.2\n    Port:          80/TCP\n    Host Port:     0/TCP\n    Environment:   <none>\n    Mounts:        <none>\n  Volumes:         <none>\n  Node-Selectors:  <none>\n  Tolerations:     <none>\nConditions:\n  Type           Status  Reason\n  ----           ------  ------\n  Available      False   MinimumReplicasUnavailable\n  Progressing    True    ReplicaSetUpdated\nOldReplicaSets:  <none>\nNewReplicaSet:   nginx-deployment-647677fc66 (3/3 replicas created)\nEvents:\n  Type    Reason             Age   From                   Message\n  ----    ------             ----  ----                   -------\n  Normal  ScalingReplicaSet  9s    deployment-controller  Scaled up replica set nginx-deployment-647677fc66 from 0 to 3\n```",
      "labels": [
        "kind/bug",
        "triage/accepted"
      ],
      "created_at": "2025-02-24T08:13:05Z",
      "closed_at": "2025-05-14T11:29:11Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1723",
      "comments_count": 4
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1743,
      "title": "kubectl apply with \"--selector\" fails if an unrelated label is set to Null",
      "problem": "**What happened**:\n\nIf you have a resource with 2+ labels and one of the labels is set to `Null` or is empty `kubectl apply` works with no issues.\n\nIf you try to apply the resource using a selector `-l` the apply command fails with not so useful error message:\n\n```sh\nerror: no objects passed to apply\n```\n\n**What you expected to happen**:\n\nThe filtering should work. \n\nThe existing error message caused confusion because the label that we are searching for is actually set and we would expect that the filtering would find that resource.\n\n**How to reproduce it (as minimally and precisely as possible)**:\n\nCreate a `test_file.yaml` with a CM resource definition.\n\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: test-config\n  labels:\n    label_a: null\n    label_b: \"bar\"\n    app.kubernetes.io/version: \"v1.0.0\"\ndata:\n  config_key_1: \"value1\"\n```\n\n```sh\n>> kubectl apply -f test_file.yaml\nconfigmap/test-config created\n```\n\nIf you try to apply the resource using a selector `-l` the apply command fails with not so useful error message:\n\n```sh\n>> kubectl apply -f test_file.yaml -l='label_b=bar'\nerror: no objects passed to apply\n```\n\nThe `label_a` being Null/missing is causing issues with the `-l` label selector, even though `label_b` is being filtered.\n\n**Anything else we need to know?**:\n\n**Environment**:\n- Kubernetes client and server versions (use `kubectl version`): Client Version: v1.30.5, Server Version: v1.31.4\n- Cloud provider or hardware configuration: n/a\n- OS (e.g: `cat /etc/os-release`): Darwin 24.4.0 Darwin Kernel Version 24.4.0:\n",
      "solution": "I just tried it with **v1.33.0** and it works. Maybe there was a change in the newer versions of kubectl that fixes the problem!\n\n```sh\nkubectl version --client\nClient Version: v1.33.0\n```\n\nclient-side apply:\n\n```sh\n> kubectl apply -f test_cm.yaml -l='label_b=bar'\nconfigmap/test-config created\n\n> k get cm test-config -oyaml \napiVersion: v1\ndata:\n  config_key_1: value1\nkind: ConfigMap\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"v1\",\"data\":{\"config_key_1\":\"value1\"},\"kind\":\"ConfigMap\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/version\":\"v1.0.0\",\"label_a\":null,\"label_b\":\"bar\"},\"name\":\"test-config\",\"namespace\":\"default\"}}\n  creationTimestamp: \"2025-05-12T10:06:08Z\"\n  labels:\n    app.kubernetes.io/version: v1.0.0\n    label_b: bar\n  name: test-config\n  namespace: default\n  ...\n```\n\nserver-side apply\n\n```\n>kubectl apply -f test_cm.yaml -l='label_b=bar' --server-side\nconfigmap/test-config serverside-applied\n\n>k get cm test-config -oyaml\napiVersion: v1\ndata:\n  config_key_1: value1\nkind: ConfigMap\nmetadata:\n  creationTimestamp: \"2025-05-12T10:07:28Z\"\n  labels:\n    app.kubernetes.io/version: v1.0.0\n    label_a: \"\"\n    label_b: bar\n  name: test-config\n  namespace: default\n  ...\n```\n\nFor server-side the resource contains `label_a` set explicitly to empty string `\"\"` and in the client-side is completely missing.... but the original point was not with regards to differences to client-side vs server-side.\n\nAs far as I am concerned, it seems that the newer versions of kubectl do not show the issue with the label selector anymore and I just need to update my version to the latest.\n\nThank you for checking and your help @ardaguclu \n",
      "labels": [
        "kind/bug",
        "needs-triage"
      ],
      "created_at": "2025-05-08T12:45:00Z",
      "closed_at": "2025-05-12T10:13:55Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1743",
      "comments_count": 11
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1731,
      "title": "It's not possible to get logs during command construction",
      "problem": "**What happened**:\nWhile preparing for KubeCon EU SIG CLI presentation I've wanted to show how users can see the logs when using [kubectl preferences](https://github.com/kubernetes/enhancements/issues/3104). \nSo I've run:\n```\nKUBECTL_KUBERC=true kubectl -v=5 get po -A\n```\nwith an empty `$HOME/.kube/kuberc` file, hoping to see this message\nhttps://github.com/kubernetes/kubernetes/blob/bdda0a530ed26fd5b9f389ae4333a748de861f32/staging/src/k8s.io/kubectl/pkg/kuberc/marshal.go#L99\nbut sadly I didn't get any. I started looking around and due to how we create commands [we only initialize logging when running command](https://github.com/kubernetes/kubernetes/blob/bdda0a530ed26fd5b9f389ae4333a748de861f32/cmd/kubectl/kubectl.go#L30). \n\n**What you expected to happen**:\nI'd like to see the output for all log messages even during command construction. \n\n**How to reproduce it (as minimally and precisely as possible)**:\n```\nKUBECTL_KUBERC=true kubectl -v=5 get po -A\n```\n\n",
      "solution": "Alternate solution in https://github.com/kubernetes/kubernetes/pull/131668",
      "labels": [
        "kind/bug",
        "triage/accepted"
      ],
      "created_at": "2025-03-28T17:04:54Z",
      "closed_at": "2025-05-08T15:53:16Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1731",
      "comments_count": 5
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1571,
      "title": "Fish completion is broken for option flags if/when leading options contain \"special\" shell characters.",
      "problem": "<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\r\n\r\nIf the matter is security related, please disclose it privately via https://kubernetes.io/security/\r\n-->\r\n\r\n**What happened**:\r\nType `kubectl` command and invoke [TAB] completion after the \"-\" prefix .\r\n```\r\nkubectl get pod -l 'app in(test)' -[TAB]\r\n```\r\n\r\n**What you expected to happen**:\r\nExpected to see completion options like the following:\r\n```\r\nkubectl get pod -l 'app in(foo)' -\r\n-A  --all-namespaces                                                                     (If present, list the requested object(s) across all namespaces. Namespace in current context is ignored even if specified with --namespace.)\r\n-f  --filename                                                                                                                                   (Filename, directory, or URL to files identifying the resource to get from a server.)\r\n-h  --help                                                                                                                                                                                                              (help for get)\r\n-k  --kustomize                \r\n...\r\n```\r\n\r\nInstead, kubectl fish completion reports an error:\r\n\r\n```\r\nkubectl get pod -l 'app in(foo)' -fish: Unknown command: foo\r\nin command substitution\r\n        called on line 1 of file /opt/homebrew/share/fish/vendor_completions.d/kubectl.fish\r\nin command substitution\r\n        called on line 42 of file /opt/homebrew/share/fish/vendor_completions.d/kubectl.fish\r\nin function '__kubectl_perform_completion'\r\n        called on line 1 of file /opt/homebrew/share/fish/vendor_completions.d/kubectl.fish\r\nin command substitution\r\n        called on line 84 of file /opt/homebrew/share/fish/vendor_completions.d/kubectl.fish\r\nin function '__kubectl_perform_completion_once'\r\n        called on line 106 of file /opt/homebrew/share/fish/vendor_completions.d/kubectl.fish\r\nin function '__kubectl_requires_order_preservation'\r\nin command substitution\r\n/opt/homebrew/share/fish/vendor_completions.d/kubectl.fish (line 1): Unknown command\r\nKUBECTL_ACTIVE_HELP=0 kubectl __complete get pod -l app in(foo) -\r\n                                                          ^~~~^\r\n...\r\n```\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n<!-- Please make sure you are able to reproduce the bug using a supported version and version skew of Kubernetes. See https://kubernetes.io/docs/setup/release/version-skew-policy for which versions and are currently supported.\r\n-->\r\n```\r\nkubectl get pod -l 'app in (foo)' -[TAB]\r\n```\r\n\r\n**Anything else we need to know?**:\r\nThis issue is due to the BUG in https://github.com/spf13/cobra with pending fix in PR: https://github.com/spf13/cobra/pull/2095\r\n\r\n**Environment**:\r\n- Kubernetes client and server versions (use `kubectl version`): `Client Version: v1.29.2`\r\n- Cloud provider or hardware configuration: N/A\r\n- OS (e.g: `cat /etc/os-release`): Any/All\r\n\r\n",
      "solution": "/close\nWe would accept a PR to fix this behavior, if someone would like to re-open the issue please do.\n\n---\n\n@mpuckett159: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubectl/issues/1571#issuecomment-2824897492):\n\n>/close\n>We would accept a PR to fix this behavior, if someone would like to re-open the issue please do.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>",
      "labels": [
        "kind/bug",
        "needs-triage"
      ],
      "created_at": "2024-03-10T23:06:03Z",
      "closed_at": "2025-04-23T16:38:01Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1571",
      "comments_count": 7
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1117,
      "title": "Wait for a CRD type to deploy before deploying resources that use the type",
      "problem": "Presently, when kubectl is used to apply a large manifest that defines new custom resource definitions (CRDs) as well as resources that use the new resource kind, conditions can cause the deployment to fail. Assuming you're using `kubectl apply -f -` and an external `kustomize` you might see an error like:\r\n\r\n```\r\nunable to recognize \"STDIN\": no matches for kind \"Alertmanager\" in version \"monitoring.coreos.com/v1\"\r\n```\r\n\r\n(the exact resource \"kind\" and api \"version\" will vary depending on what you're deplying).\r\n\r\nThis appears to be a race between the k8s cluster applying the new CRD types and kustomize sending requests that use the new types, but there's no indication of that in the command's output. It's confusing for new users, and it's a hassle operationally since deployments will fail then work when re-tried. This is something the `kubectl` tool could help users with.\r\n\r\nThe `--server-side` option does not help, as the same race occurs then. And `--wait=true` only affects resource removal, not creation.\r\n\r\nThis can often be reproduced with a kind cluster, though it varies since it's a race. For example:\r\n\r\n```\r\nkind create cluster\r\ngit clone -b v0.8.0 https://github.com/prometheus-operator/kube-prometheus\r\nkubectl apply -k kube-prometheus/\r\n```\r\n\r\n... which will often fail with:\r\n\r\n```\r\ndaemonset.apps/node-exporter created\r\nunable to recognize \"kube-prometheus/\": no matches for kind \"Alertmanager\" in version \"monitoring.coreos.com/v1\"\r\nunable to recognize \"kube-prometheus/\": no matches for kind \"Prometheus\" in version \"monitoring.coreos.com/v1\"\r\nunable to recognize \"kube-prometheus/\": no matches for kind \"PrometheusRule\" in version \"monitoring.coreos.com/v1\"\r\nunable to recognize \"kube-prometheus/\": no matches for kind \"PrometheusRule\" in version \"monitoring.coreos.com/v1\"\r\nunable to recognize \"kube-prometheus/\": no matches for kind \"PrometheusRule\" in version \"monitoring.coreos.com/v1\"\r\nunable to recognize \"kube-prometheus/\": no matches for kind \"PrometheusRule\" in version \"monitoring.coreos.com/v1\"\r\nunable to recognize \"kube-prometheus/\": no matches for kind \"PrometheusRule\" in version \"monitoring.coreos.com/v1\"\r\nunable to recognize \"kube-prometheus/\": no matches for kind \"PrometheusRule\" in version \"monitoring.coreos.com/v1\"\r\nunable to recognize \"kube-prometheus/\": no matches for kind \"PrometheusRule\" in version \"monitoring.coreos.com/v1\"\r\nunable to recognize \"kube-prometheus/\": no matches for kind \"ServiceMonitor\" in version \"monitoring.coreos.com/v1\"\r\nunable to recognize \"kube-prometheus/\": no matches for kind \"ServiceMonitor\" in version \"monitoring.coreos.com/v1\"\r\nunable to recognize \"kube-prometheus/\": no matches for kind \"ServiceMonitor\" in version \"monitoring.coreos.com/v1\"\r\nunable to recognize \"kube-prometheus/\": no matches for kind \"ServiceMonitor\" in version \"monitoring.coreos.com/v1\"\r\nunable to recognize \"kube-prometheus/\": no matches for kind \"ServiceMonitor\" in version \"monitoring.coreos.com/v1\"\r\nunable to recognize \"kube-prometheus/\": no matches for kind \"ServiceMonitor\" in version \"monitoring.coreos.com/v1\"\r\nunable to recognize \"kube-prometheus/\": no matches for kind \"ServiceMonitor\" in version \"monitoring.coreos.com/v1\"\r\nunable to recognize \"kube-prometheus/\": no matches for kind \"ServiceMonitor\" in version \"monitoring.coreos.com/v1\"\r\nunable to recognize \"kube-prometheus/\": no matches for kind \"ServiceMonitor\" in version \"monitoring.coreos.com/v1\"\r\nunable to recognize \"kube-prometheus/\": no matches for kind \"ServiceMonitor\" in version \"monitoring.coreos.com/v1\"\r\nunable to recognize \"kube-prometheus/\": no matches for kind \"ServiceMonitor\" in version \"monitoring.coreos.com/v1\"\r\nunable to recognize \"kube-prometheus/\": no matches for kind \"ServiceMonitor\" in version \"monitoring.coreos.com/v1\"\r\nunable to recognize \"kube-prometheus/\": no matches for kind \"ServiceMonitor\" in version \"monitoring.coreos.com/v1\"\r\n```\r\n\r\nbut when the same command is repeated, it will succeed:\r\n\r\n```\r\ndaemonset.apps/node-exporter unchanged\r\nalertmanager.monitoring.coreos.com/main created\r\nprometheus.monitoring.coreos.com/k8s created\r\nprometheusrule.monitoring.coreos.com/alertmanager-main-rules created\r\nprometheusrule.monitoring.coreos.com/kube-prometheus-rules created\r\n...\r\n```\r\n\r\nThere doesn't seem to be any (obvious) kubectl flag to impose a delay between requests, wait for a new resource to become visible before continuing, or retry a request if it fails because of a server-side error indicating something was missing.\r\n\r\nThe error message is confusing for new users and definitely does not help. A wording change and some context would help a lot. I raised that separately: https://github.com/kubernetes/kubectl/issues/1118",
      "solution": "A workaround is to use `kfilt` to deploy the CRDs first, then wait for them to become visible, then deploy the rest:\r\n\r\n    kustomize build somedir | kfilt -i kind=CustomResourceDefinition | kubectl apply -f -\r\n    kustomize build somedir | kfilt -i kind=CustomResourceDefinition | kubectl wait --for condition=established --timeout=60s -f -\r\n    kustomize build somedir | kubectl apply -f - \r\n\n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThank you so much for the `kfilt` example above. I am running into this in another way.\r\n\r\nMy vagrant provisioning step fails because [k3s](https//k3s.io/) is installed but did not complete setting up traefik. So sometimes applying my resources failed with\r\n\r\n```\r\ndefault: error: unable to recognize \"/vagrant/traefik_certificate.yaml\": no matches for kind \"TLSStore\" in version \"traefik.containo.us/v1alpha1\"\r\n```\r\nSo I can now fix this with\r\n\r\n```shell\r\n$ kubectl wait --for condition=established crd tlsstores.traefik.containo.us\r\n```\r\nThanks! I agree that a more generic solution in kubectl would be great.\r\n",
      "labels": [
        "kind/feature",
        "lifecycle/stale",
        "needs-triage"
      ],
      "created_at": "2021-09-20T02:20:29Z",
      "closed_at": "2022-01-05T20:05:42Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1117",
      "comments_count": 14
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1250,
      "title": "Memory request/limit not correct when applying with `Gi`",
      "problem": "**What happened**:\r\nI am not getting the correct memory requested in memory request/limits when using `Gi` notation. I get `157286400m` for `0.15Gi` in request and `188743680m` for `0.18Gi` in limits\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\n```YAML\r\napiVersion: batch/v1\r\nkind: Job\r\nmetadata:\r\n  name: job-testing\r\nspec:\r\n  backoffLimit: 0\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: job-testing\r\n    spec:\r\n      containers:\r\n      - command:\r\n        - /bin/bash\r\n        - -c\r\n        - python3 main.py\r\n        env:\r\n        - name: DEPLOY_ENV\r\n          value: prd\r\n        image: <IMAGE>\r\n        imagePullPolicy: Always\r\n        name: job-testing\r\n        resources:\r\n          limits:\r\n            cpu: 0.18\r\n            memory: 0.18Gi\r\n          requests:\r\n            cpu: 0.15\r\n            memory: 0.15Gi\r\n        securityContext:\r\n          allowPrivilegeEscalation: false\r\n      imagePullSecrets:\r\n      - name: <DTR_SERVER>\r\n      restartPolicy: Never\r\n      securityContext:\r\n        fsGroup: 11111\r\n        runAsGroup: 11111\r\n        runAsUser: 11111\r\n```\r\n\r\nMy original YAML has volumes and secrets which i stripped here and using a corporate DTR/docker Image\r\n\r\n`kubectl apply -n my-namespace -f job.yaml`\r\n\r\n**Anything else we need to know?**:\r\nThe YAML file is generated with [kubernetes-client ](https://github.com/kubernetes-client/python/) in python. But when I print it, I get expected values of `0.15Gi` and `0.18Gi`. And the error is the same when the package applies the configuration or that I apply myself the YAML file. I am wondering[ if this could be changed by the server.](https://github.com/kubernetes-client/python/issues/1783)\r\n\r\n**Environment**:\r\nClient Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.5\", GitCommit:\"c285e781331a3785a7f436042c65c5641ce8a9e9\", GitTreeState:\"clean\", BuildDate:\"2022-03-16T15:58:47Z\", GoVersion:\"go1.17.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\nServer Version: version.Info{Major:\"1\", Minor:\"20+\", GitVersion:\"v1.20.11-mirantis-1\", GitCommit:\"c0557aa1a9571c353503500cb7130a2f491d073f\", GitTreeState:\"clean\", BuildDate:\"2021-09-18T04:05:21Z\", GoVersion:\"go1.15.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n\r\n",
      "solution": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/bug",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2022-07-25T14:46:25Z",
      "closed_at": "2023-01-14T20:59:43Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1250",
      "comments_count": 8
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1668,
      "title": "kubectl using 1200% CPU on MacOS 14.4.1",
      "problem": "<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\r\n\r\nIf the matter is security related, please disclose it privately via https://kubernetes.io/security/\r\n-->\r\n\r\n**What happened**:\r\n\r\nI always keep a terminal open with `watch \"kubectl get pods\"` while I work, so that if I can at a glance see the status of my remote cluster.\r\nI noticed today while working that my computer was sluggish. When looking in activity monitor, kubectl was running at 1200% (12 full CPU cores) CPU usage, with low memory usage. At that time, `watch \"kubectl get pods\"` had been running for 5d 14h, polling state every 2s while my laptop is not in sleep mode.\r\nI killed the command `watch \"kubectl get pods\"` and the process successfully exited, releasing the CPU load.\r\n\r\n**What you expected to happen**:\r\n\r\nNot eat 12 full CPU, it's polling once every 2 sec.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n<!-- Please make sure you are able to reproduce the bug using a supported version and version skew of Kubernetes. See https://kubernetes.io/docs/setup/release/version-skew-policy for which versions and are currently supported.\r\n-->\r\n\r\nNo idea really! Anything I can do to help diagnose this?\r\nThe only reason that I'm posting here is that high CPU usage like this can be indicative of an exploited security vulnerability, and thus why I'm taking proactive action to open this issue.\r\n\r\n- Is there any sort of log that is left by kubectl locally or remotely (our cluster is in GCP K8S)?\r\n- How do I check the integrity of the program (checksum perhaps?)?\r\n\r\nI think my kubectl is packaged directly with gcloud. I'm not sure; how do I check?\r\n\r\n**Anything else we need to know?**:\r\n\r\n**Environment**: \r\n- Kubernetes client and server versions (use `kubectl version`):\r\n\r\nClient Version: v1.30.4\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.30.4-gke.1348000\r\n\r\n- Cloud provider or hardware configuration: Google Cloud Platform, K8S; locally, MacBook Pro 2019 Intel Core i9\r\n- OS (e.g: `cat /etc/os-release`): MacOS 14.4.1 Sonoma\r\n\r\n",
      "solution": "The interesting thing about this is that kubectl is not running for 5 days, it is being invoked by watch every 2 seconds for 5 days.\r\n\r\nIn addition to using `-v=9` as @ardaguclu suggested...\r\n\r\nIf it happens again, try doing the following in another terminal, while the problem is occurring, to collect information that might be helpful to diagnose the problem:\r\n\r\n```\r\nps -F $(pgrep kubectl)\r\n```\r\n\r\n```\r\npgrep kubectl | xargs -L1 lsof -p\r\n```\r\n\r\n\n\n---\n\nThat said, I think it would be best to replicate with an official binary, you can grab the official 1.30.9 here: https://kubernetes.io/releases/download/\n\nIt sounds like it might take some time to replicate this. It would also be good to know if we can replicate it on a more recent release since 1.30 is the tail end right now and we may have already fixed it https://kubernetes.io/releases/\n\n---\n\nHey @mpuckett159, thanks for the thread necromancy :P\nUnfortunately, the case support from GCP did not provide any further information. Here was their response:\n\n> For your second questions, you can still assert the integrity of the gcloud sdk with the publicly available checksums that we shared, just note that kubectl has an additional component installed after the Cloud SDK the checksum would not be able to assert its integrity.\n\nSo dead end on that side. I won't be able to provide more information moving forward.\nFurthermore I changed computer recently to a different processor architecture, so I doubt I will be able to ever replicate the issue. Note that the reason I had to change computer was that the previous one had a hardware defect on the motherboard which resulted in locking me out indefinitely from my computer. So maybe this was due to the motherboard issue.\n\nI'm good with closing this issue. I'll reopen it if I end up experiencing a similar issue!",
      "labels": [
        "kind/support",
        "kind/bug",
        "priority/awaiting-more-evidence",
        "needs-triage"
      ],
      "created_at": "2024-10-08T04:18:00Z",
      "closed_at": "2025-03-27T02:05:06Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1668",
      "comments_count": 19
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1687,
      "title": "Bring back `kubectl exec [POD] [COMMAND]`",
      "problem": "### What happened?\n\nhttps://github.com/kubernetes/kubernetes/pull/125437 removed `kubectl exec [POD] [COMMAND]` in favor of only `exec [POD] -- [COMMAND]`.\r\n\r\nThis adds unnecessary extra steps to the command and disparity with familiar tools like `docker`.  \n\n### What did you expect to happen?\n\nCommand stays with a warning to indicate `--` is _preferred_, but not made required\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n`kubectl exec POD ls` will fail\n\n### Anything else we need to know?\n\nI don't feel that removing this adds any value to users. Its not less code to maintain or anything -- all it seems to do is impose individual preferences on everyone in a backwards incompatible way. The fact it was \"deprecated\" for years doesn't really change much\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nClient Version: v1.31.3\r\n```\r\n\r\n</details>\r\n\n\n### Cloud provider\n\n<details>\r\nna\r\n</details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n</details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n</details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n</details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n</details>\r\n",
      "solution": "> @soltysh I'd like to ask opinions as well, as he deprecated this in [kubernetes/kubernetes#88460](https://github.com/kubernetes/kubernetes/pull/88460) due to a solid reason.\r\n\r\nNot sure if there's anything more for me to add here. Arguments like the one @ardaguclu quoted above wrt maintainability, and ease of understanding how each flag is applied, which are also mentioned in the [linked issue](https://github.com/ahmetb/kubectl-aliases/issues/44) (there specifically the problem was `--namespace` positioning were sufficient to allow us move forward. We've put a lot of effort into ensuring the deprecation period was extended more than [official guidelines](https://kubernetes.io/docs/reference/using-api/deprecation-policy/#deprecating-a-flag-or-cli). I understand that this is indeed causing some issues to users, and I apologize for that, but I don't think we're inclined to introduce the removed notion. \r\n",
      "labels": [
        "kind/bug",
        "priority/awaiting-more-evidence",
        "sig/cli",
        "needs-triage"
      ],
      "created_at": "2024-12-18T14:27:14Z",
      "closed_at": "2025-03-26T16:36:33Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1687",
      "comments_count": 11
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1650,
      "title": "kubectl debug: profile \"sysadmin\" does not work as expected when uid != 0 is specified",
      "problem": "<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\r\n\r\nIf the matter is security related, please disclose it privately via https://kubernetes.io/security/\r\n-->\r\n\r\n**What happened**:\r\nI wanted to create an ephemeral container with `sysadmin` (or `netadmin`) profile to be able to capture traffic using `tcpdump` using the following command:\r\n\r\n```\r\nkubectl debug test-pod -it --image nicolaka/netshoot --profile=sysadmin -- zsh\r\n\r\nDefaulting debug container name to debugger-wj6qq.\r\nIf you don't see a command prompt, try pressing enter.\r\n\r\ntest-pod% whoami\r\nwhoami: unknown uid 1000\r\n\r\ntest-pod% tcpdump\r\ntcpdump: eth0: You don't have permission to perform this capture on that device\r\n(socket: Operation not permitted)\r\n```\r\n\r\nThe ephemeral container is set to `privileged: true` as expected, but the Pod level `securityContext` forces the ephemeral container to run as user `1000` which is IMO an unwanted behavior for an ephemeral container with `sysadmin` profile set.\r\n\r\n**What you expected to happen**:\r\nI would expect my ephemeral container with `sysadmin` to be able to capture traffic in any case.\r\n\r\nOn a container level `securityContext` I would not only expect `privileged: true`, but also `runAsUser: 0` to avoid such user override collisions from pod level. Otherwise: a parameter to override the user for the ephemeral container would help in that regard as well.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n1. Create a Pod with the following securityContext\r\n\r\n```yaml\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  labels:\r\n    run: test-pod\r\n  name: test-pod\r\nspec:\r\n  securityContext:\r\n    runAsUser: 1000 # Override user != 0\r\n  containers:\r\n  - image: kennethreitz/httpbin\r\n    name: test-pod\r\n    resources: {}\r\n  dnsPolicy: ClusterFirst\r\n  restartPolicy: Always\r\n\r\n```\r\n\r\n2. Attach ephemeral Container via debug command and profile `sysadmin` set\r\n\r\n```\r\nkubectl debug test-pod -it --image nicolaka/netshoot --profile=sysadmin -- zsh\r\n\r\nDefaulting debug container name to debugger-wj6qq.\r\nIf you don't see a command prompt, try pressing enter.\r\n\r\ntest-pod% whoami\r\nwhoami: unknown uid 1000\r\n\r\ntest-pod% tcpdump\r\ntcpdump: eth0: You don't have permission to perform this capture on that device\r\n(socket: Operation not permitted)\r\ntest-pod% \r\n```\r\n<!-- Please make sure you are able to reproduce the bug using a supported version and version skew of Kubernetes. See https://kubernetes.io/docs/setup/release/version-skew-policy for which versions and are currently supported.\r\n-->\r\n\r\n**Anything else we need to know?**:\r\n\r\n**Environment**:\r\n- Kubernetes client and server versions (use `kubectl version`):\r\n```sh\r\nClient Version: v1.30.3\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.30.3-eks-a18cd3a\r\n```\r\n- Cloud provider or hardware configuration: AWS EKS (see above)\r\n\r\n\r\n",
      "solution": "@ardaguclu \r\nI think the same situation is happening here with ephemeral container.\r\n\r\nWhen `privileged: true` is set for a container running as root user, the following [CapabilitySet](https://man7.org/linux/man-pages/man7/capabilities.7.html) is applied.\r\nThe key point is that the `CapEff` where the capabilities actually used for permission checks are set.\r\n\r\n```yaml\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  labels:\r\n    run: privileged\r\n  name: privileged\r\nspec:\r\n  containers:\r\n  - image: busybox\r\n    command: [\"sh\", \"-c\", \"sleep infinity\"]\r\n    name: privileged\r\n    securityContext:\r\n      privileged: true\r\n  terminationGracePeriodSeconds: 0\r\n```\r\n\r\n```bash\r\n$ kubectl exec -it privileged -- /bin/sh\r\n/ # whoami\r\nroot\r\n/ # grep Cap /proc/1/status\r\nCapInh:\t0000000000000000\r\nCapPrm:\t000001ffffffffff\r\nCapEff:\t000001ffffffffff\r\nCapBnd:\t000001ffffffffff\r\nCapAmb:\t0000000000000000\r\n```\r\n\r\nOn the other hand, when non-root user is specified in `runAsUser`, even if `privileged: true` or specific capabilities are set, the container process does not have the appropriate capability(`CapEff`), and it can not have the required permission.\r\n\r\n```yaml\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  labels:\r\n    run: runasuser-with-privileged\r\n  name: runasuser-with-privileged\r\nspec:\r\n  securityContext:\r\n    runAsUser: 1000\r\n  containers:\r\n  - image: busybox\r\n    command: [\"sh\", \"-c\", \"sleep infinity\"]\r\n    name: runasuser-with-privileged\r\n    securityContext:\r\n      privileged: true\r\n  terminationGracePeriodSeconds: 0\r\n```\r\n\r\n```bash\r\n$ kubectl exec -it runasuser-with-privileged -- /bin/sh\r\n~ $ whoami\r\nwhoami: unknown uid 1000\r\n~ $ grep Cap /proc/1/status\r\nCapInh:\t0000000000000000\r\nCapPrm:\t0000000000000000\r\nCapEff:\t0000000000000000\r\nCapBnd:\t000001ffffffffff\r\nCapAmb:\t0000000000000000\r\n```\r\n\r\nI have not checked the details yet, but this issue has been reported in [#56374](https://github.com/kubernetes/kubernetes/issues/56374), and [KEP #2763](https://github.com/kubernetes/enhancements/issues/2763) has been proposed.\r\nHowever, it seems to not be implemented yet.\r\n\r\n\r\nSo currently, I think the simplest workaround is to define `runAsUser` under the `containers` field.\r\n\r\n```yaml\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  labels:\r\n    run: test-pod\r\n  name: test-pod\r\nspec:\r\n  # securityContext:\r\n  #   runAsUser: 1000 # Override user != 0\r\n  containers:\r\n  - image: kennethreitz/httpbin\r\n    name: test-pod\r\n    securityContext:\r\n      runAsUser: 1000 # Override user != 0\r\n    resources: {}\r\n  dnsPolicy: ClusterFirst\r\n  restartPolicy: Always\r\n```\r\n\r\n```bash\r\n$ kubectl debug test-pod -it --image nicolaka/netshoot --profile=sysadmin -- zsh\r\nDefaulting debug container name to debugger-dwt84.\r\nIf you don't see a command prompt, try pressing enter.\r\ntest-pod \ue0b0 ~ \ue0b0 whoami\r\nroot\r\ntest-pod \ue0b0 ~ \ue0b0 grep Cap /proc/$$/status\r\nCapInh:\t0000000000000000\r\nCapPrm:\t000001ffffffffff\r\nCapEff:\t000001ffffffffff\r\nCapBnd:\t000001ffffffffff\r\nCapAmb:\t0000000000000000\r\n```\r\n\r\nor using [Custom Profile](https://kubernetes.io/blog/2024/08/22/kubernetes-1-31-custom-profiling-kubectl-debug/).\r\n\r\n`profile-runas-root.yaml`\r\n```yaml\r\nsecurityContext:\r\n  runAsUser: 0\r\n  privileged: true\r\n```\r\n\r\n```bash\r\n$ kubectl debug test-pod -it --image=busybox --custom=profile-runas-root.yaml -- /bin/sh\r\nDefaulting debug container name to debugger-mjp6g.\r\nIf you don't see a command prompt, try pressing enter.\r\n/ # grep Cap /proc/$$/status\r\nCapInh:\t0000000000000000\r\nCapPrm:\t000001ffffffffff\r\nCapEff:\t000001ffffffffff\r\nCapBnd:\t000001ffffffffff\r\nCapAmb:\t0000000000000000\r\n\r\n/ # exit\r\n\r\n$ kubectl get pod test-pod -o=jsonpath='{.spec.ephemeralContainers[0].securityContext}' | jq .\r\n{\r\n  \"privileged\": true,\r\n  \"runAsUser\": 0\r\n}\r\n```\r\n\r\n\r\nAnother solution which I come up with is to set `runAsUser: 0` to ephemeral container when `--profile=sysadmin` or `--profile=netadmin` is specified.\r\nHowever, I don't know it's appropriate...\n\n---\n\nThanks a lot for your extensive investigation @mochizuki875 and it is really helpful.\r\n\r\nI think we should wait this KEP https://github.com/kubernetes/enhancements/issues/2763 is revived and the suggested workaround to overcome this issue is using custom profiling ^^. \r\n\r\n/triage accepted\r\n/priority backlog\r\n\n\n---\n\n\r\n> So currently, I think the simplest workaround is to define runAsUser under the containers field.\r\n\r\nThere are workarounds, but surely a user (of kubectl) should not have to do all that to simply achieve a privileged ephemeral container to run as UID 0 (or any other). The running pods and their `runAsUser` might originate from some upstream Helm chart and also a workaround is only good if it can be found easily. Don't want to discourage and scare people away from unprivileged and UID!=0 containers just because it's that one or two steps harder to debug them ;-) \r\n\r\n\r\n> Thank you for your reviewing.\r\n> I agree with that, and I think it's one of the cases where custom profile works well\ud83d\udc4d\r\n\r\n\r\n> Another solution which I come up with is to set `runAsUser: 0` to ephemeral container when `--profile=sysadmin` or `--profile=netadmin` is specified.\r\n> However, I don't know it's appropriate...\r\n\r\nIf the ephemeral container does not have anything else in its spec that is totally reasonable. And talking about `kubectl` patching a pod based on distinct CLI options to dynamically create a debug container it seems the more reasonable to simply add this aspect to spec patch created controlled by kubectl anyways?\r\n\r\n\r\n",
      "labels": [
        "kind/bug",
        "priority/backlog",
        "triage/accepted"
      ],
      "created_at": "2024-09-10T13:22:16Z",
      "closed_at": "2025-03-20T05:22:15Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1650",
      "comments_count": 11
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1689,
      "title": "Prune with namespace requires access to api group \"\"",
      "problem": "### What happened?\n\nOur azure pipeline applies configurations to kubernetes with the `Kubernetes@1` task using the `apply` command. This works fine, until I add `prune -l tier=frontend` as arguments. I see 2 error lines:\r\n\r\n1. Deprecated: kubectl apply will no longer prune non-namespaced resources by default when used with the --namespace flag in a future release. To preserve the current behaviour, list the resources you want to target explicitly in the --prune-allowlist flag.\r\n2. error pruning nonNamespaced object /v1, Kind=Namespace: namespaces is forbidden: User \"<GUID>\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope: User does not have access to the resource in Azure. Update role assignment to allow access.\r\n \r\nThe first one is picked up as an error by Azure, but should only be a warning. The second one is where I'm at a loss: why do I need access to this?\n\n### What did you expect to happen?\n\nGiven an explicit namespace, prune only the resources with matching labels within that namespace, do not require access to anything outside of that\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\ntask: Kubernetes@1\r\ndisplayName: Deploy to kubernetes\r\ninputs:\r\n  command: apply\r\n  arguments: --prune -l tier=frontend\r\n  namespace: test\r\n  (azure & kubernetes details...)\r\n\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nWARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\r\nClient Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.9\", GitCommit:\"d15213f69952c79b317e635abff6ff4ec81475f8\", GitTreeState:\"clean\", BuildDate:\"2023-12-19T13:41:13Z\", GoVersion:\"go1.20.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\nKustomize Version: v5.0.1\r\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.9\", GitCommit:\"1c9860e7360c3f8147ae068e867eaab73b4a6257\", GitTreeState:\"clean\", BuildDate:\"2024-04-12T23:21:51Z\", GoVersion:\"go1.20.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n```\r\n\r\n</details>\r\n\n\n### Cloud provider\n\n<details>\r\nAKS\r\n</details>\r\n\n\n### OS version\n\n_No response_\n\n### Install tools\n\n_No response_\n\n### Container runtime (CRI) and version (if applicable)\n\n_No response_\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n_No response_",
      "solution": "> ### What happened?\r\n> Our azure pipeline applies configurations to kubernetes with the `Kubernetes@1` task using the `apply` command. This works fine, until I add `prune -l tier=frontend` as arguments. I see 2 error lines:\r\n> \r\n> 1. Deprecated: kubectl apply will no longer prune non-namespaced resources by default when used with the --namespace flag in a future release. To preserve the current behaviour, list the resources you want to target explicitly in the --prune-allowlist flag.\r\n> 2. error pruning nonNamespaced object /v1, Kind=Namespace: namespaces is forbidden: User \"\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope: User does not have access to the resource in Azure. Update role assignment to allow access.\r\n> \r\n> The first one is picked up as an error by Azure, but should only be a warning. The second one is where I'm at a loss: why do I need access to this?\r\n> \r\n> ### What did you expect to happen?\r\n> Given an explicit namespace, prune only the resources with matching labels within that namespace, do not require access to anything outside of that\r\n> \r\n> ### How can we reproduce it (as minimally and precisely as possible)?\r\n> task: Kubernetes@1 displayName: Deploy to kubernetes inputs: command: apply arguments: --prune -l tier=frontend namespace: test (azure & kubernetes details...)\r\n> \r\n> ### Anything else we need to know?\r\n> _No response_\r\n> \r\n> ### Kubernetes version\r\n> ### Cloud provider\r\n> ### OS version\r\n> _No response_\r\n> \r\n> ### Install tools\r\n> _No response_\r\n> \r\n> ### Container runtime (CRI) and version (if applicable)\r\n> _No response_\r\n> \r\n> ### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n> _No response_\r\n\r\nThe first error is actually a warning rather than an error. The command will get executed. The warning was added as part of this issue: https://github.com/kubernetes/kubernetes/issues/110905. You can try using --prune-allowlist flag till this issue is resolved. You can refer to the following documentation for the same: https://kubernetes.io/docs/tasks/manage-kubernetes-objects/declarative-config/#alternative-kubectl-apply-f-directory-prune",
      "labels": [
        "kind/bug",
        "sig/cli",
        "needs-triage"
      ],
      "created_at": "2024-09-23T08:27:39Z",
      "closed_at": "2025-02-26T17:45:25Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1689",
      "comments_count": 10
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1667,
      "title": "some kubectl stdOut messages are sent to stdErr",
      "problem": "What happened\r\n\r\nAttempted to use GNU/Bash command substitution to assign the output of a command to a variable:\r\n\r\n```shell\r\n% yo=\"$(kubectl get pods -n kube-public)\"; echo \"$yo\" | cat -vte\r\nNo resources found in kube-public namespace.\r\n$\r\n```\r\n\r\nThis results in the message being printed to `stdErr` and nothing being assigned to the variable `yo`.\r\n\r\n1. the error prints to the terminal, then\r\n2. the empty variable prints to the terminal: `$`\r\n\r\nAfter redirecting `stdErr` back to `stdOut` the issue is resolved:\r\n\r\n```shell\r\n% yo=\"$(kubectl get pods -n kube-public 2>&1)\"; echo \"$yo\" | cat -vte\r\nNo resources found in kube-public namespace.$\r\n```\r\n\r\n**What you expected to happen**:\r\n\r\nThe message to the user \"No resources found in kube-public namespace.\" should be sent through `stdOut`.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\nSee above.\r\n\r\n**Anything else we need to know?**:\r\n\r\nFor reference, even given a similar command with a resulting failure, like searching for namespace `foo` (and there is no namespace `foo`) should even be sent to `stdOut` (if there is a message) and accompanied by exit `status:1` ; `kubectl` is telling the human something important.\r\n\r\nIf there is nothing to share with the user (no messages printed to the terminal) then simple exit with `status:1` and move on.\r\n\r\nFor Reference:\r\n\r\nThe most programmatically friendly thing to do is:\r\n\r\n* Send ALL `kubectl` messages (errors and everything) to `stdOut`, and \r\n* Whenever `kubectl` has nothing to share with the human (rarely) THAT should exit with `status:1`.\r\n\r\n## Example\r\n\r\nGiven these conditions:\r\n\r\n```shell\r\n% cat /tmp/yo\r\na1 b1 c1\r\na2 b2 c2\r\na3 b3 c3\r\n```\r\n\r\n`grep` will indicate success with a message and exit status:\r\n\r\n```shell\r\n% grep b2 /tmp/yo; echo $?\r\na2 b2 c2    # information for user\r\n0           # exit status: success\r\n```\r\n\r\nIn this case the succeeding operation outputs a message to the user: `a2 b2 c2`.\r\n\r\n```shell\r\n% grep foo /tmp/yo; echo $?\r\n1           # exit status: fail (no message to user)\r\n```\r\n\r\nIn this case there is nothing to share with the user; the _purpose_ of the operation failed to return anything. But, given the exit status, we can make subsequent automation decisions on the exit status - simple.\r\n\r\nPlease see excerpts from our bible, [The Linux Programming Interface](https://man7.org/tlpi/download/TLPI-04-File_IO_The_Universal_IO_Model.pdf). While this excerpt only explains the mechanics, the `grep` example supports historic implementation.\r\n\r\nI'd be interested in hearing from those with more gray hairs than me about the `whys` behind the historic bits.\r\n\r\n**Environment**:\r\n- Kubernetes client and server versions (use `kubectl version`): `Client Version: v1.31.0`\r\n- Cloud provider or hardware configuration: NA\r\n- OS (e.g: `cat /etc/os-release`): `macOS Sonoma/14.6.1`",
      "solution": "I think, this KEP https://github.com/kubernetes/enhancements/issues/2551 will resolve the issue explained in here. \n\n---\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n---\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/",
      "labels": [
        "kind/bug",
        "lifecycle/rotten",
        "needs-triage"
      ],
      "created_at": "2024-10-07T23:23:19Z",
      "closed_at": "2025-02-26T17:42:19Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1667",
      "comments_count": 7
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1709,
      "title": "Special characters like & not handled correctly in --docker-password when creating secrets using kubectl create secret",
      "problem": "I\u2019m facing an issue when trying to create a Secret in Kubernetes with a password that contains the special character & using the kubectl create secret command. The password is either being encoded incorrectly or not handled properly when the special character is included.\nHere\u2019s the command I\u2019m trying to execute:\nkubectl create secret docker-registry docker-login-secret10 \\\n  --docker-server=\nregistry.example.com \\\n  --docker-username=mke_prod \\\n  --docker-password=\"5a6g(07hl9gsv&QzF9Fgcrz@6rnHA4h7\" \\\n  --docker-email=seu-email@example.com \\\n  --namespace=default\nThe secret creation seems to work correctly, but when I decode the .dockerconfigjson field, the password appears in the format\n\"password\": \"5a6g(07hl9gsv\\u0026QzF9Fgcrz@6rnHA4h7\"\nExpected result:\n\"password\": \"5a6g(07hl9gsv&QzF9Fgcrz@6rnHA4h7\"\nActual result:\n\"password\": \"5a6g(07hl9gsv\\&QzF9Fgcrz@6rnHA4h7\" \n\n\"&\" character is being encoded as \\u0026 or another incorrect value, which is not the expected behavior.\n\nWhat I\u2019ve tried:\nEscaped the & using \\& in the command, but it didn't work.\nTried using single and double quotes around the password, but the issue persists.\nChecked the base64 encoding, but the special character seems to be handled incorrectly.",
      "solution": "I investigated the issue related to Docker registry passwords containing special characters (like &) being incorrectly encoded when creating a Kubernetes secret using the `kubectl create secret docker-registry command`.\nAs stated, the password value was being encoded incorrectly, with special characters like & being converted to \\u0026. This encoding issue persisted even when retrieving the secret using kubectl get.\n\n![Image](https://github.com/user-attachments/assets/2d00aae3-f53e-4ccb-a28d-5e12f00cf167)",
      "labels": [
        "kind/bug",
        "needs-triage"
      ],
      "created_at": "2025-02-04T23:12:49Z",
      "closed_at": "2025-02-26T17:23:11Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1709",
      "comments_count": 7
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1694,
      "title": "`kubectl describe pod` shows strange info in volumes.projected-volumes.SecretOptionalName",
      "problem": "<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\r\n\r\nIf the matter is security related, please disclose it privately via https://kubernetes.io/security/\r\n-->\r\n\r\n**What happened**:\r\nWhen running `kubectl describe pod` on a pod with an optional secret volume I get the following `Volumes` section:\r\n```\r\nVolumes:\r\n  projected-volume:\r\n    Type:                Projected (a volume that contains injected data from multiple sources)\r\n    SecretName:          secret3\r\n    SecretOptionalName:  0x14000123977\r\n```\r\nThe SecretOptionalName is irrelevant and probably be used to show whether the Secret above it is optional or not (boolean).\r\n\r\n**What you expected to happen**:\r\nShow boolean optional information regarding the above secret.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n<!-- Please make sure you are able to reproduce the bug using a supported version and version skew of Kubernetes. See https://kubernetes.io/docs/setup/release/version-skew-policy for which versions and are currently supported.\r\n-->\r\nApply the following pod:\r\n```\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  name: test-pod\r\n  namespace: default\r\nspec:\r\n  containers:\r\n  - name: test-container\r\n    image: busybox\r\n    command: [\"/bin/sh\", \"-c\", \"while true; do sleep 3600; done\"]\r\n    volumeMounts:\r\n    - name: projected-volume\r\n      mountPath: /etc/projected-volume\r\n  volumes:\r\n  - name: projected-volume\r\n    projected:\r\n      sources:\r\n      - secret:\r\n          name: secret3\r\n          optional: true\r\n```\r\n\r\nThen, run `kubectl describe pod test-pod` and inspect the Volumes section.\r\n\r\n**Anything else we need to know?**:\r\n\r\n**Environment**:\r\n- Kubernetes client and server versions (use `kubectl version`): Client Version: v1.31.3, Kustomize Version: v5.4.2, Server Version: v1.27.3\r\n- Cloud provider or hardware configuration: kind\r\n- OS (e.g: `cat /etc/os-release`):  macOS 15.1 (24B83)\r\n\r\n",
      "solution": "That sounds like a bug and needs to be fixed with a test. Thank you reporting this @gshaibi. Do you want to fix it? \r\n/triage accepted\r\n/priority backlog",
      "labels": [
        "kind/bug",
        "priority/backlog",
        "triage/accepted"
      ],
      "created_at": "2025-01-01T12:48:36Z",
      "closed_at": "2025-01-06T13:50:17Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1694",
      "comments_count": 2
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1620,
      "title": "Port forwarding dies after cancelling a TCP request",
      "problem": "I use `kubectl port-forward` to forward traffic to a service and sometimes the service is not reachable anymore. After some investigation it looks like the port forwarding cannot handle with cancelled TCP requests and the port-forwarding locks up and showed the following error message:\r\n```\r\nE0705 12:50:42.897359   76107 portforward.go:394] error copying from local connection to remote stream: read tcp6 [::1]:8080->[::1]:44368: read: connection reset by peer\r\nE0705 12:50:42.897360   76107 portforward.go:381] error copying from remote stream to local connection: readfrom tcp6 [::1]:8080->[::1]:44368: write tcp6 [::1]:8080->[::1]:44368: write: broken pipe\r\n```\r\nThe `kubectl` process keeps running, but the port-forwarding is dead. Restarting `kubectl port-forward` brings the service back alive.\r\n\r\nI have created a very [basic application (in Go)](https://github.com/ramondeklein/k8s-portforward-bug/blob/master/server.go) that serves a 32MiB file and it can be invoked using `curl -o /dev/null http://localhost:8080` and it will complete the download. When running `curl http://localhost:8080`, it will cancel the download after the request, because it won't display binary data. This application works fine locally.\r\n\r\nWhen run in Kubernetes and using `kubectl port-forward server 8080:8080` it works fine when files are downloaded, but it locks up when a download is cancelled. It looks like `kubectl` is still accepting requests on port 8080, but doesn't seem to be able to contact the pod anymore.\r\n\r\nI tested with both Kubernetes v1.30 (using a single node [Kind](https://kind.sigs.k8s.io/) cluster) and in Kubernetes v1.29 using AKS (MS Azure). A full ready-to-run reproduction is available on https://github.com/ramondeklein/k8s-portforward-bug.",
      "solution": "/close\r\nClosing for now, please let me know if websockets did not solve this for you and we will re-open the issue.",
      "labels": [
        "kind/feature",
        "triage/needs-information"
      ],
      "created_at": "2024-07-05T12:46:21Z",
      "closed_at": "2024-08-14T16:58:17Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1620",
      "comments_count": 8
    },
    {
      "tech": "kubernetes",
      "repo": "kubernetes/kubectl",
      "issue_number": 1482,
      "title": "kubectl wait hangs even after job is deleted",
      "problem": "<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\r\n\r\nIf the matter is security related, please disclose it privately via https://kubernetes.io/security/\r\n-->\r\n\r\n**What happened**:\r\nStarted a job.\r\nWant to wait for end result of job, either complete or failed, but waiting for failed hangs, even after job is deleted by k8s.\r\nFollowing the suggestion in SO post https://stackoverflow.com/a/60286538, I start two kubectl wait's.\r\nThe job has\r\nttlSecondsAfterFinished: 300\r\nbackoffLimit: 0\r\n\r\nThe wait for complete status works and returns, but even though the job is deleted after about 5 minutes, still the wait for Failed process hangs on for 30 minutes: kubectl wait job/my-job --for=condition=Failed --timeout=1800s \r\n\r\n**What you expected to happen**:\r\nIf the job is completed why would it wait for Failed, it should just return some error code to indicate failure to wait for failed state.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n<!-- Please make sure you are able to reproduce the bug using a supported version and version skew of Kubernetes. See https://kubernetes.io/docs/setup/release/version-skew-policy for which versions and are currently supported.\r\n-->\r\n```\r\napiVersion: batch/v1\r\nkind: Job\r\nmetadata:\r\n  name: pi-with-ttl\r\nspec:\r\n  ttlSecondsAfterFinished: 100\r\n  template:\r\n    spec:\r\n      containers:\r\n      - name: pi\r\n        image: perl:5.34.0\r\n        command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\r\n      restartPolicy: Never\r\n```\r\n- apply job\r\n- run wait for failed, it will wait even past job deletion:\r\nkubectl wait job/pi-with-ttl --for=condition=Failed --timeout=300s \r\n\r\n\r\n**Environment**:\r\n- Kubernetes client and server versions (use `kubectl version`):\r\n- 1.26.2 client,  1.25.6 server\r\n- Cloud provider or hardware configuration:\r\n- Azure AKS\r\n\r\n",
      "solution": "@bj8sk I tried reproducing the issue in the following manner and `kubectl wait` didn't hang for me. My kubectl client version is `1.29`. Can you try reproducing the issue with the latest version and check if the issue still happens? I had initially tried using your `pi-with-ttl` Job, in which case the Job was getting `Completed` instead of `Failed`. Please let me know if the issue persists. Here are the details of how I tried reproducing the issue. \r\n\r\nkubectl version\r\n```bash\r\n\u276f kubectl version\r\nClient Version: v1.29.0\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.28.4-eks-8cb36c9\r\n```\r\n\r\nThe YAML for the Job I used to meet the `Failed` condition\r\n```yaml\r\napiVersion: batch/v1\r\nkind: Job\r\nmetadata:\r\n  name: sreerams-failing-job\r\n  namespace: sreeram-dev\r\nspec:\r\n  ttlSecondsAfterFinished: 100\r\n  template:\r\n    spec:\r\n      containers:\r\n      - name: fail-container\r\n        image: busybox\r\n        command: [\"/bin/sh\", \"-c\"]\r\n        args: [\"exit 1\"]\r\n      restartPolicy: Never\r\n  backoffLimit: 0\r\n```\r\n\r\nRunning `kubectl get` to check on the Job and the Pod:\r\n```bash\r\n\u276f k get jobs -n sreeram-dev\r\nNAME                   COMPLETIONS   DURATION   AGE\r\nsreerams-failing-job   0/1           81s        81s\r\n\r\n\u276f k get pods -n sreeram-dev\r\nNAME                         READY   STATUS   RESTARTS   AGE\r\nsreerams-failing-job-wj57m   0/1     Error    0          87s\r\n```\r\n\r\nHere's what I used to wait for the job's failure\r\n```bash\r\n\u276f k wait --for=condition=Failed job/sreerams-failing-job -n sreeram-dev --timeout=300s\r\njob.batch/sreerams-failing-job condition met\r\n```",
      "labels": [
        "kind/bug",
        "triage/accepted"
      ],
      "created_at": "2023-09-11T07:56:21Z",
      "closed_at": "2024-11-21T07:01:51Z",
      "url": "https://github.com/kubernetes/kubectl/issues/1482",
      "comments_count": 7
    }
  ]
}