{
  "tech": "docker",
  "count": 210,
  "examples": [
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 66146088,
      "title": "Docker gets error &quot;failed to compute cache key: not found&quot; - runs fine in Visual Studio",
      "problem": "I've generated a Dockerfile with Visual\u00a0Studio. It runs in Visual\u00a0Studio just fine and now I'm trying to build it from Windows itself (`docker build .`, and I tried many combinations). Yet I get the following error:\n```\n`> [build 3/7] COPY [client/client.csproj, client/]:\n------\nfailed to compute cache key: \"/client/client.csproj\" not found: not found\n`\n```\nWhen I change copy to `./client.csproj` it does continue and then I get:\n```\n` => ERROR [build 7/7] RUN dotnet build \"client.csproj\" -c Release -o /app/build                3.3s\n------\n> [build 7/7] RUN dotnet build \"client.csproj\" -c Release -o /app/build:\n#15 0.652 Microsoft (R) Build Engine version 16.8.3+39993d9d for .NET\n#15 0.652 Copyright (C) Microsoft Corporation. All rights reserved.\n#15 0.652\n#15 1.169   Determining projects to restore...\n#15 1.483   All projects are up-to-date for restore.\n#15 3.231 CSC : error CS5001: Program does not contain a static 'Main' method suitable for an entry point [/src/client/client.csproj]\n#15 3.240\n#15 3.240 Build FAILED.\n#15 3.240\n#15 3.240 CSC : error CS5001: Program does not contain a static 'Main' method suitable for an entry point [/src/client/client.csproj]\n#15 3.240     0 Warning (5)\n#15 3.240     1 Error (5)\n#15 3.240\n#15 3.240 Time Elapsed 00:00:02.51\n-----\nexecutor failed running [/bin/sh -c dotnet build \"client.csproj\" -c Release -o /app/build]: exit code: 1\n`\n```\nWhat am I doing wrong? I changed Docker Linux to Windows, changed WSL, and restarted everything.\n```\n`#See https://aka.ms/containerfastmode to understand how Visua...\n\nFROM mcr.microsoft.com/dotnet/aspnet:5.0-buster-slim AS base\nWORKDIR /app\nEXPOSE 80\nEXPOSE 443\n\nFROM mcr.microsoft.com/dotnet/sdk:5.0-buster-slim AS build\nWORKDIR /src\nCOPY [\"client/client.csproj\", \"client/\"]\nRUN dotnet restore \"client/client.csproj\"\nCOPY . .\nWORKDIR \"/src/client\"\nRUN dotnet build \"client.csproj\" -c Release -o /app/build\n\nFROM build AS publish\nRUN dotnet publish \"client.csproj\" -c Release -o /app/publish\n\nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app/publish .\nENTRYPOINT [\"dotnet', \"client.dll\"]\n`\n```",
      "solution": "The way Visual Studio does it is a little bit odd.\nInstead of launching `docker build` in the folder with the Dockerfile, it launches in the parent folder and specifies the Dockerfile with the `-f` option.\nI was using the demo project (trying to create a minimal solution for another question) and struck the same situation.\nSetup for my demo project is\n```\n`\\WorkerService2  (\"solution\" folder)\n   +- WorkerService2.sln\n   +- WorkserService2  (\"project\" folder)\n       +- DockerFile\n       +- WorkerService2.csproj\n       +- ... other program files\n`\n```\nSo I would expect to go\n```\n`cd \\Workerservice2\\WorkerService2\ndocker build .\n`\n```\nBut I get your error message.\n```\n` => ERROR [build 3/7] COPY [WorkerService2/WorkerService2.csproj, WorkerService2/]                                                                                                                        0.0s\n------\n > [build 3/7] COPY [WorkerService2/WorkerService2.csproj, WorkerService2/]:\n------\nfailed to compute cache key: \"/WorkerService2/WorkerService2.csproj\" not found: not found\n`\n```\nInstead, go to the parent directory, with the `.sln` file and use the docker `-f` option to specify the Dockerfile to use in the subfolder:\n```\n`cd \\Workerservice2\ndocker build -f WorkerService2\\Dockerfile --force-rm -t worker2/try7 .\n\ndocker run -it worker2/try7    \n`\n```\n\nNote the final dot on the `docker build` command.\nFor docker the final part of the command is the location of the files that Docker will work with. Usually this is the folder with the Dockerfile in, but that's what's different about how VS does it. In this case the dockerfile is specified with the `-f`. Any paths (such as with the `COPY` instruction in the dockerfile) are relative to the location specified. The `.` means \"current directory\", which in my example is `\\WorkerService2`.\nI got to this stage by inspecting the output of the build process, with verbosity set to Detailed.\nIf you choose Tools / Options / Projects and Solutions / Build and Run you can adjust the build output verbosity, I made mine Detailed.\n\nI think I've worked out why Visual Studio does it this way.\nIt allows the project references in the same solution to be copied in.\nIf it was set up to do `docker build` from the project folder, docker would not be able to `COPY` any of the other projects in the solution in. But the way this is set up, with current directory being the solution folder, you can copy referenced projects (subfolders) into your docker build process.",
      "question_score": 373,
      "answer_score": 285,
      "created_at": "2021-02-10T23:32:45",
      "url": "https://stackoverflow.com/questions/66146088/docker-gets-error-failed-to-compute-cache-key-not-found-runs-fine-in-visual"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 73285601,
      "title": "Docker : exec /usr/bin/sh: exec format error",
      "problem": "I created a custom docker image and push it to docker hub but when I run it in CI/CD it gives me this error.\n`exec /usr/bin/sh: exec format error`\nWhere :\nDockerfile\n```\n`FROM ubuntu:20.04\nRUN apt-get update\nRUN apt-get install -y software-properties-common\nRUN apt-get install -y python3-pip\nRUN pip3 install robotframework\n`\n```\n.gitlab-ci.yml\n```\n`robot-framework:\n  image: rethkevin/rf:v1\n  allow_failure: true\n  script:\n    - ls\n    - pip3 --version\n`\n```\nOutput\n```\n`Running with gitlab-runner 15.1.0 (76984217)\n  on runner zgjy8gPC\nPreparing the \"docker\" executor\nUsing Docker executor with image rethkevin/rf:v1 ...\nPulling docker image rethkevin/rf:v1 ...\nUsing docker image sha256:d2db066f04bd0c04f69db1622cd73b2fc2e78a5d95a68445618fe54b87f1d31f for rethkevin/rf:v1 with digest rethkevin/rf@sha256:58a500afcbd75ba477aa3076955967cebf66e2f69d4a5c1cca23d69f6775bf6a ...\nPreparing environment\n00:01\nRunning on runner-zgjy8gpc-project-1049-concurrent-0 via 1c8189df1d47...\nGetting source from Git repository\n00:01\nFetching changes with git depth set to 20...\nReinitialized existing Git repository in /builds/reth.bagares/test-rf/.git/\nChecking out 339458a3 as main...\nSkipping Git submodules setup\nExecuting \"step_script\" stage of the job script\n00:00\nUsing docker image sha256:d2db066f04bd0c04f69db1622cd73b2fc2e78a5d95a68445618fe54b87f1d31f for rethkevin/rf:v1 with digest rethkevin/rf@sha256:58a500afcbd75ba477aa3076955967cebf66e2f69d4a5c1cca23d69f6775bf6a ...\nexec /usr/bin/sh: exec format error\nCleaning up project directory and file based variables\n00:01\nERROR: Job failed: exit code 1\n`\n```\nany thoughts on this to resolve the error?",
      "solution": "The problem is that you built this image for arm64/v8 -- but your runner is using a different architecture.\nIf you run:\n```\n`docker image inspect rethkevin/rf:v1\n`\n```\nYou will see this in the output:\n```\n`...\n        \"Architecture\": \"arm64\",\n        \"Variant\": \"v8\",\n        \"Os\": \"linux\",\n...\n`\n```\nTry building and pushing your image from your GitLab CI runner so the architecture of the image will match your runner's architecture.\nAlternatively, you can build for multiple architectures using `docker buildx` . Alternatively still, you could also run a GitLab runner on ARM architecture so that it can run the image for the architecture you built it on.\n\nWith modern versions of docker, you may also explicitly control the platform docker uses. Docker will use platform emulation if the specified platform is different from your native platform.\nFor example:\nUsing the `DOCKER_DEFAULT_PLATFORM` environment variable:\n`DOCKER_DEFAULT_PLATFORM=\"linux/amd64\" docker build -t test .\n`\nUsing the `--platform` argument, either in the CLI or in your dockerfile:\n`docker build --platform=\"linux/amd64\" -t test .\n`\n```\n`FROM --platform=linux/amd64 ubuntu:jammy\n`\n```\nSystems with docker desktop installed should already be able to do this. If your system is using docker without docker desktop, you may need to install the docker-buildx plugins explicitly.",
      "question_score": 203,
      "answer_score": 255,
      "created_at": "2022-08-09T03:26:08",
      "url": "https://stackoverflow.com/questions/73285601/docker-exec-usr-bin-sh-exec-format-error"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 70452836,
      "title": "Docker push to AWS ECR hangs immediately and times out",
      "problem": "I'm trying to push my first docker image to ECR.  I've followed the steps provided by AWS and things seem to be going smoothly until the final push which immediately times out.  Specifically, I pass my aws ecr credentials to docker and get a \"login succeeded\" message.  I then tag the image which also works.  pushing to the ecr repo I get no error message, just the following:\n```\n`The push refers to repository [xxxxxxxxxxx.dkr.ecr.ca-central-1.amazonaws.com/reponame]\n714c1b96dd83: Retrying in 1 second \nd2cdc77dd068: Retrying in 1 second \n30aad807caf5: Retrying in 1 second \n0559774c4ea2: Retrying in 1 second \n285b8616682f: Retrying in 1 second \n4aeea0ec2b15: Waiting \n1b1312f842d8: Waiting \nc310009e0ef3: Waiting \na48777e566d3: Waiting \n2a0c9f28029a: Waiting \nEOF\n`\n```\nIt tries a bunch of times and then exits with no message.  Any idea what's wrong?",
      "solution": "I figured out my issue.  I wasn't using the correct credentials.  I had a personal AWS account as my default credentials and needed to add my work profile to my credentials.\nEDIT\nIf you have multiple aws profiles, you can mention the profile name at the docker login as below (assuming you have done `aws configure --profile someprofile` at earlier day),\n```\n`aws ecr get-login-password --region us-east-1 --profile someprofile | docker login ....\n`\n```",
      "question_score": 197,
      "answer_score": 172,
      "created_at": "2021-12-22T18:23:56",
      "url": "https://stackoverflow.com/questions/70452836/docker-push-to-aws-ecr-hangs-immediately-and-times-out"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 65896681,
      "title": "exec: &quot;docker-credential-desktop.exe&quot;: executable file not found in $PATH",
      "problem": "I got this error during `docker build`:\n```\n` => ERROR [internal] load metadata for docker.io/library/ubuntu:18.04                                                                                                                                                                   2.1s\n------\n > [internal] load metadata for docker.io/library/ubuntu:18.04:\n------\nfailed to solve with frontend dockerfile.v0: failed to create LLB definition: rpc error: code = Unknown desc = error getting credentials - err: exec: \"docker-credential-desktop.exe\": executable file not found in $PATH, out: ``\nMakefile:26: recipe for target 'build-local' failed\nmake: *** [build-local] Error 1\n`\n```",
      "solution": "I found a docker forum discussion that solved my error.\nEdit `~/.docker/config.json` and either delete or rename the `credsStore` entry:\nBroken:\n`\"credsStore\": ...\n`\nWorks:\n`\"credStore\": ...\n`\nExplanation:\nThe property `credsStore` specifies an external binary to serve as the default credential store. When this property is set, `docker login` will attempt to store credentials in the binary specified by `docker-credential-` which is visible on `$PATH`. If this property is not set, credentials will be stored in the auths property of the config.\nMore information on credStore can be found in the Docker reference.\nThanks goes to @Javier Buzzi and @valk for the explanation.",
      "question_score": 170,
      "answer_score": 687,
      "created_at": "2021-01-26T07:16:33",
      "url": "https://stackoverflow.com/questions/65896681/exec-docker-credential-desktop-exe-executable-file-not-found-in-path"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 67642620,
      "title": "docker-credential-desktop not installed or not available in PATH",
      "problem": "I might have a bit of a messed Docker installation on my Mac..\nAt first I installed Docker desktop but then running it I learned that as I'm on an older Mac I had to install VirtualBox so I did following these steps:\n\nenable writing on the `/usr/local/bin` folder for user\n`sudo chown -R $(whoami) /usr/local/bin`\n\ninstall Docker-Machine\n\n`base=https://github.com/docker/machine/releases/download/v0.16.0 &&\n  curl -L $base/docker-machine-$(uname -s)-$(uname -m) >/usr/local/bin/docker-machine &&\n  chmod +x /usr/local/bin/docker-machine\n`\n\ninstall Xcode CLI..manually from dev account\n\nInstall Home Brew\n\n`/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n`\n\nInstall  Docker  + wget ( Using Brew)\n`brew install docker`\n`brew install wget`\n\nInstall bash completion scripts\n\n`base=https://raw.githubusercontent.com/docker/machine/v0.16.0\nfor i in docker-machine-prompt.bash docker-machine-wrapper.bash docker-machine.bash\ndo\n    sudo wget \"$base/contrib/completion/bash/${i}\" -P /etc/bash_completion.d\ndone\n`\n\nenable the docker-machine shell prompt\n`echo 'PS1='[\\u@\\h \\W$(__docker_machine_ps1)]\\$ '' >> ~/.bashrc`\n\nInstall VirtualBox, ExtensionPack and SDK: https://www.virtualbox.org/wiki/Downloads\n\nI now installed docker-compose (docker-compose version 1.29.2, build unknown) with home-brew but when running `docker-compose up` I get the following error:\n\n`docker.credentials.errors.InitializationError: docker-credential-desktop not installed or not available in PATH`\n\n`which docker` prints /usr/local/bin/docker.\nBrew installations are in `/usr/local/Cellar/docker/20.10.6` and `/usr/local/Cellar/docker-compose/1.29.2`.\nAs I see there is also a home-brew for docker-machine should I install docker-machine via home-brew instead?\nWhat can I check to make sure that I use the docker installations from home-brew and wipe/correct the installations made from steps above?",
      "solution": "After a long googling I found out that the problem is with the `config.json` file.\nThe `\"credsStore\" : \"docker-credential-desktop\"` is wrong one in :\n```\n`{\n  \"credsStore\" : \"docker-credential-desktop\",\n  \"stackOrchestrator\" : \"swarm\",\n  \"experimental\" : \"disabled\"\n} \n`\n```\nchanged the `\"credsStore\"` key value to `\"desktop\"` and compose now works as expected. Some pointed out that `credsDstore` typo was the problem and fixed it with `credDstore`, but in my case the value was the problem, it works both with `\"credsStore\" : \"desktop\"` and `\"credStore\" : \"desktop\"`.\nHope it'll help others starting out with Docker.\nCheers.",
      "question_score": 143,
      "answer_score": 32,
      "created_at": "2021-05-21T21:03:01",
      "url": "https://stackoverflow.com/questions/67642620/docker-credential-desktop-not-installed-or-not-available-in-path"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 70256928,
      "title": "Docker Compose prompted error: no configuration file provided: not found",
      "problem": "I had setup Docker Desktop with Windows WSL integration version 2 and I run into issue when execute certain `docker compose` command with following errors\n```\n`docker compose logs\nno configuration file provided: not found\n`\n```\nHowever, there were no problem found when executing the following\n```\n`docker compose up\n`\n```\nand image built and fired up successfully.\nIs there anyone who can help with this?\n\nOutput of `docker info`\n```\n`Client:\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc., v0.7.1)\n  compose: Docker Compose (Docker Inc., v2.2.1)\n  scan: Docker Scan (Docker Inc., 0.9.0)\n\nServer:\n Containers: 3\n  Running: 3\n  Paused: 0\n  Stopped: 0\n Images: 4\n Server Version: 20.10.11\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: cgroupfs\n Cgroup Version: 1\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 7b11cfaabd73bb80907dd23182b9347b4245eb5d\n runc version: v1.0.2-0-g52b36a2\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: default\n Kernel Version: 5.10.60.1-microsoft-standard-WSL2\n Operating System: Docker Desktop\n OSType: linux\n Architecture: x86_64\n CPUs: 8\n Total Memory: 24.95GiB\n Name: docker-desktop\n ID: FUMA:ZOXR:BA4L:YSOZ:4NQT:HHIZ:ASAD:EJGA:NJRG:SO4S:GXN3:JG5H\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Registry: https://index.docker.io/v1/\n Labels:\n Experimental: false\n Insecure Registries:\n  127.0.0.0/8\n Live Restore Enabled: false\n\nWARNING: No blkio throttle.read_bps_device support\nWARNING: No blkio throttle.write_bps_device support\nWARNING: No blkio throttle.read_iops_device support\nWARNING: No blkio throttle.write_iops_device support\n`\n```",
      "solution": "execute docker-compose command where docker-compose.yml file located at should resolved it.\nor specify the docker-compose.yml file as bellow\n```\n`docker-compose -f  logs \n`\n```\nas suggested",
      "question_score": 141,
      "answer_score": 109,
      "created_at": "2021-12-07T09:24:21",
      "url": "https://stackoverflow.com/questions/70256928/docker-compose-prompted-error-no-configuration-file-provided-not-found"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 66912085,
      "title": "Why is docker-compose failing with ERROR internal load metadata suddenly?",
      "problem": "I've been running docker-compose build for days, many times per day, and haven't changed my DOCKERFILEs or docker-compose.yml. Suddenly an hour ago I started getting this:\n```\n`Building frontdesk-api\nfailed to get console mode for stdout: The handle is invalid.\n[+] Building 10.0s (3/4)\n => [internal] load build definition from Dockerfile                       0.0s\n[+] Building 10.1s (4/4) FINISHED\n => [internal] load build definition from Dockerfile                       0.0s\n => => transferring dockerfile: 32B                                        0.0s\n => [internal] load .dockerignore                                          0.0s\n => => transferring context: 2B                                            0.0s\n => ERROR [internal] load metadata for mcr.microsoft.com/dotnet/sdk:5.0.  10.0sailed to do request: Head https://mcr.microsoft.com/v2/dotnet/sdk/manifests/5.0.201-buster-slim: dial tcp: lookup mcr.microsoft.com on 192.168.65.5:53:\n => [internal] load metadata for mcr.microsoft.com/dotnet/aspnet:5.0-bust  0.0s\n------\n > [internal] load metadata for mcr.microsoft.com/dotnet/sdk:5.0.201-buster-slim:\n------\nERROR: Service 'frontdesk-api' failed to build\n`\n```\nThings I've tried:\n\nRunning it again\n` docker rm -f $(docker ps -a -q)`\n`docker login`\nDifferent SDK image\n\nHere is the DOCKERFILE:\n```\n`FROM mcr.microsoft.com/dotnet/aspnet:5.0-buster-slim AS base\nWORKDIR /app\nEXPOSE 80\nEXPOSE 443\n\nFROM mcr.microsoft.com/dotnet/sdk:5.0.201-buster-slim AS build\nWORKDIR /app\n# run this from repository root\nCOPY ./ ./ \n#RUN ls -lha .\n\nRUN echo 'Building FrontDesk container'\n\nWORKDIR /app/FrontDesk/src/FrontDesk.Api\n#RUN ls -lha .\nRUN dotnet restore\n\nRUN dotnet build \"FrontDesk.Api.csproj\" -c Release -o /app/build\n\nFROM build AS publish\nRUN dotnet publish \"FrontDesk.Api.csproj\" -c Release -o /app/publish\n\nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app/publish .\nENTRYPOINT [\"dotnet\", \"FrontDesk.Api.dll\"]\n`\n```\nHow can I get my build working again?",
      "solution": "mcr.microsoft.com is down at the moment\nI'm receiving several different errors when pulling:\n```\n`% docker pull mcr.microsoft.com/dotnet/sdk:5.0\nError response from daemon: Get https://mcr.microsoft.com/v2/: Service Unavailable\n\n% docker pull mcr.microsoft.com/dotnet/sdk:5.0\n5.0: Pulling from dotnet/sdk\nreceived unexpected HTTP status: 500 Internal Server Error\n`\n```",
      "question_score": 137,
      "answer_score": 13,
      "created_at": "2021-04-02T00:00:08",
      "url": "https://stackoverflow.com/questions/66912085/why-is-docker-compose-failing-with-error-internal-load-metadata-suddenly"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 67458621,
      "title": "How to run amd64 docker image on arm64 host platform?",
      "problem": "I have an m1 mac and I am trying to run a amd64 based docker image on my arm64 based host platform. However, when I try to do so (with docker run) I get the following error:\n```\n`WARNING: The requested image's platform (linux/amd64) does not\n match the detected host platform (linux/arm64/v8)\n and no specific platform was requested. \n`\n```\nWhen I try adding the tag `--platform linux/amd64` the error message doesn't appear, but I can't seem to go into the relevant shell and `docker ps -a` shows that the container is immediately exited upon starting. Would anyone know how I can run this exact image on my machine given the circumstances/how to make the `--platform` tag work?",
      "solution": "Using `--platform` is correct. On my M1 Mac I'm able to run both arm64 and amd64 versions of the Ubuntu image from Docker Hub. The machine hardware name provided by uname proves it.\n```\n`# docker run --rm -ti --platform linux/arm/v7 ubuntu:latest uname -m\narmv7l\n\n# docker run --rm -ti --platform linux/amd64 ubuntu:latest uname -m\nx86_64\n`\n```\nRunning amd64 images is enabled by Rosetta2 emulation, as indicated here.\n\nNot all images are available for ARM64 architecture. You can add `--platform linux/amd64` to run an Intel image under emulation.\n\nIf the container is exiting immediately, that's a problem with the specific container you're using.",
      "question_score": 128,
      "answer_score": 182,
      "created_at": "2021-05-09T16:06:32",
      "url": "https://stackoverflow.com/questions/67458621/how-to-run-amd64-docker-image-on-arm64-host-platform"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 68673221,
      "title": "Why do I still get a warning about &quot;Running pip as the &#39;root&#39; user&quot; inside a Docker container?",
      "problem": "I made a simple image of my python Django app in Docker, using this Dockerfile:\n`FROM python:3.8-slim-buster\n\nWORKDIR /app\nCOPY requirements.txt requirements.txt\nRUN pip install -r requirements.txt\nCOPY . .\nCMD [\"python\", \"manage.py\", \"runserver\", \"0.0.0.0:8000\"]\n`\nAnd building it using:\n`sudo docker build -t my_app:1 .\n`\nBut after building the container (on Ubuntu 20.04) I get this warning:\n```\n`WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead\n`\n```\nWhy does it throw this warning if I am installing Python requirements inside my image? Can I actually break my system this way?",
      "solution": "The way your container is built doesn't add a user, so everything is done as root.\nYou could create a user and install to that users's home directory by doing something like this;\n```\n`FROM python:3.8.3-alpine\n\nRUN pip install --upgrade pip\n\nRUN adduser -D myuser\nUSER myuser\nWORKDIR /home/myuser\n\nCOPY --chown=myuser:myuser requirements.txt requirements.txt\nRUN pip install --user -r requirements.txt\n\nENV PATH=\"/home/myuser/.local/bin:${PATH}\"\n\nCOPY --chown=myuser:myuser . .\n\nCMD [\"python\", \"manage.py\", \"runserver\", \"0.0.0.0:8000\"]\n`\n```",
      "question_score": 126,
      "answer_score": 70,
      "created_at": "2021-08-05T22:37:54",
      "url": "https://stackoverflow.com/questions/68673221/why-do-i-still-get-a-warning-about-running-pip-as-the-root-user-inside-a-doc"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 79340672,
      "title": "\u201ccom.docker.socket\u201d was not opened because it contains malware",
      "problem": "I noticed this issue on a Mac running Docker Desktop. I've been running Docker on this Mac for a while with no problems, and this cropped up apropos of nothing, after a reboot:\n\n\"com.docker.socket\" was not opened because it contains malware\n\nAnyone have an idea why, and what can be done to work around it? Simply dragging the app icon to the trash and re-installing didn't seem to help.\nNote: I re-created this question because it was asked then deleted by its author, as I was in the act of answering it. I pretty much expected it would be closed for reasons discussed below, so it's since been re-asked and answered on apple.stackexchange.com.",
      "solution": "Update: The official solution for this problem is documented here; thanks to Adjmed Omar for pointing that out. See this blog post from Docker for context.\nWhat follows is what worked for me, retained here for posterity only. Use the official solution linked above instead. I appreciate the suggestions to use Colima; that's simply beyond the scope of the initial problem.\n\nSee the GitHub issue docker/for-mac #7520 for context. Quoting from that issue:\n\nDocker rotated an old signing certificate for macOS and this unexpectedly broke users who currently have Docker Desktop installed.\n\nI opted to simply uninstall Docker and reinstall it, rather than muck about with `launchctl` commands in the terminal. As it turned out, this step was probably unnecessary, and you should note that the `uninstall` binary in the Docker.app bundle \"destroys Docker containers, images, volumes, and other Docker-related data local to the machine.\"\nAs I recall, though, the uninstaller seemed to have overlooked the file(s) that macOS identified as \"malware\" in the first place: `/Library/PrivilegedHelperTools/com.docker.socket`, and `com.docker.vmnetd`. I removed both of those files manually and rebooted for good measure\u2014just like Windows!\nAfterwards, I re-installed Docker by dragging the app icon from the latest `.dmg` image to the \"Applications\" folder, and things appear to be fine so far.",
      "question_score": 110,
      "answer_score": 83,
      "created_at": "2025-01-08T21:27:25",
      "url": "https://stackoverflow.com/questions/79340672/com-docker-socket-was-not-opened-because-it-contains-malware"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 69054921,
      "title": "Docker on Mac M1 gives: &quot;The requested image&#39;s platform (linux/amd64) does not match the detected host platform&quot;",
      "problem": "I want to run a docker container for `Ganache` on my MacBook M1, but get the following error:\n```\n`The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\n`\n```\nAfter this line nothing else will happen anymore and the whole process is stuck, although the qemu-system-aarch64 is running on 100% CPU according to Activity Monitor until I press CTRL+C.\nMy docker-files come from this repository. After running into the same issues there I tried to isolate the root cause and came up with the smallest setup that will run into the same error.\nThis is the output of `docker-compose up --build`:\n```\n`Building ganache\nSending build context to Docker daemon  196.6kB\nStep 1/17 : FROM trufflesuite/ganache-cli:v6.9.1\n ---> 40b011a5f8e5\nStep 2/17 : LABEL Unlock \n ---> Using cache\n ---> aad8a72dac4e\nStep 3/17 : RUN apk add --no-cache git openssh bash\n ---> Using cache\n ---> 4ca6312438bd\nStep 4/17 : RUN apk add --no-cache   python   python-dev   py-pip   build-base   && pip install virtualenv\n ---> Using cache\n ---> 0be290f541ed\nStep 5/17 : RUN npm install -g npm@6.4.1\n ---> Using cache\n ---> d906d229a768\nStep 6/17 : RUN npm install -g yarn\n ---> [Warning] The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\n ---> Running in 991c1d804fdf\n`\n```\ndocker-compose.yml:\n```\n`version: '3.2'\nservices:\n  ganache:\n    restart: always\n    build:\n      context: ./development\n      dockerfile: ganache.dockerfile\n    env_file: ../.env.dev.local\n    ports:\n      - 8545:8545\n\n  ganache-standup:\n    image: ganache-standup\n    build:\n      context: ./development\n      dockerfile: ganache.dockerfile\n    env_file: ../.env.dev.local\n    entrypoint: ['node', '/standup/prepare-ganache-for-unlock.js']\n    depends_on:\n      - ganache\n`\n```\nganache.dockerfile:\nThe ganache.dockerfile can be found here.\nRunning the whole project on an older iMac with Intel-processor works fine.",
      "solution": "With docker-compose you also have the `platform` option.\n```\n`version: \"2.4\"\nservices:\n  zookeeper:\n    image: confluentinc/cp-zookeeper:7.1.1\n    hostname: zookeeper\n    container_name: zookeeper\n    platform: linux/amd64\n    ports:\n      - \"2181:2181\"\n`\n```",
      "question_score": 108,
      "answer_score": 36,
      "created_at": "2021-09-04T13:37:58",
      "url": "https://stackoverflow.com/questions/69054921/docker-on-mac-m1-gives-the-requested-images-platform-linux-amd64-does-not-m"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 65806330,
      "title": "toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading",
      "problem": "Why does this happen, when I want to build an image from a Dockerfile in CodeCommit with CodeBuild?\nI get this Error:\n\ntoomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit",
      "solution": "Try not to pull the images from the docker hub because docker has throttling for pulling the images.\nUse ECR(Elastic Container Registry)  for private images and  Amazon ECR Public Gallery for public docker images.\nAdvice for customers dealing with Docker Hub rate limits, and a Coming Soon announcement for the advice from AWS for handling this.\nUpdate: Docker Hub will only allow an unauthenticated 10/pulls per hour starting March 1st",
      "question_score": 107,
      "answer_score": 107,
      "created_at": "2021-01-20T10:08:18",
      "url": "https://stackoverflow.com/questions/65806330/toomanyrequests-you-have-reached-your-pull-rate-limit-you-may-increase-the-lim"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 67231714,
      "title": "How to add trusted root CA to Docker alpine",
      "problem": "Suppose I am at network where there is MITM SSL swaping firewall (google.com is not issued by Google, but reissued by custom CA root authority) some more details here https://security.stackexchange.com/questions/107542/is-it-common-practice-for-companies-to-mitm-https-traffic .\nI have simple Dockerfile:\n```\n`FROM alpine:latest\nRUN apk --no-cache add curl\n`\n```\nIt fails badly with error with SSL errors\n```\n` => ERROR [2/2] RUN apk --no-cache add curl                                                                                                                                    1.0s\n------\n > [2/2] RUN apk --no-cache add curl:\n#5 0.265 fetch https://dl-cdn.alpinelinux.org/alpine/v3.13/main/x86_64/APKINDEX.tar.gz\n#5 0.647 140037857143624:error:1416F086:SSL routines:tls_process_server_certificate:certificate verify failed:ssl/statem/statem_clnt.c:1913:\n#5 0.649 WARNING: Ignoring https://dl-cdn.alpinelinux.org/alpine/v3.13/main: Permission denied\n#5 0.649 fetch https://dl-cdn.alpinelinux.org/alpine/v3.13/community/x86_64/APKINDEX.tar.gz\n#5 0.938 140037857143624:error:1416F086:SSL routines:tls_process_server_certificate:certificate verify failed:ssl/statem/statem_clnt.c:1913:\n#5 0.940 WARNING: Ignoring https://dl-cdn.alpinelinux.org/alpine/v3.13/community: Permission denied\n#5 0.941 ERROR: unable to select packages:\n#5 0.942   curl (no such package):\n#5 0.942     required by: world[curl]\n------\nexecutor failed running [/bin/sh -c apk --no-cache add curl]: exit code: 1\n`\n```\nEvery tutorial at Internet says that I can add own \"trusted\" root certificate and run `update-ca-certificates`.\nBut it can be added by \"apt add\" only. This situation seems to me as \"chicken-egg\" problem.\n```\n`FROM alpine:latest\nUSER root\nRUN apk --no-cache add ca-certificates \\\n  && update-ca-certificates\n`\n```\nError is similar\n```\n`=> ERROR [2/2] RUN apk --no-cache add ca-certificates   && update-ca-certificates                                                                                             1.0s\n------\n > [2/2] RUN apk --no-cache add ca-certificates   && update-ca-certificates:\n#5 0.269 fetch https://dl-cdn.alpinelinux.org/alpine/v3.13/main/x86_64/APKINDEX.tar.gz\n#5 0.662 140490932583240:error:1416F086:SSL routines:tls_process_server_certificate:certificate verify failed:ssl/statem/statem_clnt.c:1913:\n#5 0.663 fetch https://dl-cdn.alpinelinux.org/alpine/v3.13/community/x86_64/APKINDEX.tar.gz\n#5 0.663 WARNING: Ignoring https://dl-cdn.alpinelinux.org/alpine/v3.13/main: Permission denied\n#5 0.929 140490932583240:error:1416F086:SSL routines:tls_process_server_certificate:certificate verify failed:ssl/statem/statem_clnt.c:1913:\n#5 0.931 WARNING: Ignoring https://dl-cdn.alpinelinux.org/alpine/v3.13/community: Permission denied\n#5 0.932 ERROR: unable to select packages:\n#5 0.933   ca-certificates (no such package):\n#5 0.933     required by: world[ca-certificates]\n------\nexecutor failed running [/bin/sh -c apk --no-cache add ca-certificates   && update-ca-certificates]: exit code: 1\n`\n```\nIs there some other solution how to install `update-ca-certificates` tool? Or am I missing something?\nThx\nSee @kthompso answer for working solution.\nWorking solution (with `update-ca-certificates` commnad) based on @kthompso answer and info from unable to add certificates to alpine linux container\n```\n`FROM alpine:latest\n\nUSER root\n\n# To be able to download `ca-certificates` with `apk add` command\nCOPY my-root-ca.crt /root/my-root-ca.crt\nRUN cat /root/my-root-ca.crt >> /etc/ssl/certs/ca-certificates.crt\n\n# Add again root CA with `update-ca-certificates` tool\nRUN apk --no-cache add ca-certificates \\\n    && rm -rf /var/cache/apk/*\nCOPY my-root-ca.crt /usr/local/share/ca-certificates\nRUN update-ca-certificates\n\nRUN apk --no-cache add curl\n`\n```\nEdit: One solution I have in my mind is to use curl docker image with `-k` option and download `.apk` with those certificates and tools. Install it as local file. Add my root CA certificate and run `update-ca-certificates`. It sounds super crazy, so I think that have to be better solution :)",
      "solution": "Append your self-signed cert to `/etc/ssl/certs/ca-certificates.crt` manually.\nAssuming you have the self-signed certificate in a file in your build directory called `my-cert.pem`:\n```\n`FROM alpine:latest\n  \nCOPY my-cert.pem /usr/local/share/ca-certificates/my-cert.crt\n\nRUN cat /usr/local/share/ca-certificates/my-cert.crt >> /etc/ssl/certs/ca-certificates.crt && \\\n    apk --no-cache add \\\n        curl\n`\n```\nNote: When you're using `update-ca-certificates`, you need to place your cert file into `/usr/local/share/ca-certificates/` first.  Otherwise it will be removed from `/etc/ssl/certs/ca-certificates.crt` the first time you run `update-ca-certificates`.",
      "question_score": 104,
      "answer_score": 143,
      "created_at": "2021-04-23T16:22:37",
      "url": "https://stackoverflow.com/questions/67231714/how-to-add-trusted-root-ca-to-docker-alpine"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 65919460,
      "title": "The author primary signature&#39;s timestamp found a chain building issue: UntrustedRoot: self signed certificate in certificate chain",
      "problem": "While doing a docker build on my .NET Core project, I got the following error on all my NuGets:\n\n80.19 /app/GradingTool.Tests/GradingTool.Tests.csproj : error NU3028: Package 'Microsoft.EntityFrameworkCore 5.0.0' from source 'https://api.nuget.org/v3/index.json': The author primary signature's timestamp found a chain building issue: UntrustedRoot: self signed certificate in certificate chain [/app/GradingTool.sln]\n#12 80.20 /app/GradingTool.Tests/GradingTool.Tests.csproj : error NU3037: Package 'Microsoft.EntityFrameworkCore 5.0.0' from source 'https://api.nuget.org/v3/index.json': The author primary signature validity period has expired. [/app/GradingTool.sln]\n#12 80.20 /app/GradingTool.Tests/GradingTool.Tests.csproj : error NU3028: Package 'Microsoft.EntityFrameworkCore 5.0.0' from source 'https://api.nuget.org/v3/index.json': The repository countersignature's timestamp found a chain building issue: UntrustedRoot: self signed certificate in certificate chain [/app/GradingTool.sln]\n\nI never had this error before,\nCan someone help me figure out what the problem is?\nDockerfile:\n```\n`FROM mcr.microsoft.com/dotnet/sdk:latest AS build-env\nWORKDIR /app\nRUN apt-get update -yq \\\n    && apt-get install curl gnupg -yq \\\n    && curl -sL https://deb.nodesource.com/setup_10.x | bash \\\n    && apt-get install nodejs -yq\n# Copy csproj and restore as distinct layers\nCOPY . ./\nRUN dotnet restore\nRUN dotnet publish -c Release -o out\n\n# Build runtime image\nFROM mcr.microsoft.com/dotnet/aspnet:latest\nRUN apt-get update \\\n    && apt-get install -y --no-install-recommends libgdiplus libc6-dev \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\nWORKDIR /app\nCOPY --from=build-env /app/out .\nENV ASPNETCORE_URLS=\"http://+:4200\"\nENV ASPNETCORE_ENVIRONMENT=\"Production\"\nENV GOOGLE_APPLICATION_CREDENTIALS=\"Credentials/SchoolTools-e9f260bdf56e.json\"\nENV VIRTUAL_HOST=\"eva.schooltools.lu,www.eva.schooltools.lu,schooltools.lu,www.schooltools.lu\"\nENV LETSENCRYPT_HOST=\"eva.schooltools.lu,www.eva.schooltools.lu,schooltools.lu,www.schooltools.lu\"\nENV LETSENCRYPT_EMAIL=\"wilson.silva@edutec.lu\"\nEXPOSE 4200\nENTRYPOINT [\"dotnet\", \"GradingTool.dll\"]\n`\n```",
      "solution": "In the Dockerfile file, I changed from\n```\n`FROM mcr.microsoft.com/dotnet/aspnet:5.0-buster-slim\n`\n```\nto\n```\n`FROM mcr.microsoft.com/dotnet/sdk:5.0-alpine\n`\n```\nThis worked for me!",
      "question_score": 97,
      "answer_score": 39,
      "created_at": "2021-01-27T13:44:37",
      "url": "https://stackoverflow.com/questions/65919460/the-author-primary-signatures-timestamp-found-a-chain-building-issue-untrusted"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 70434777,
      "title": "Cannot kill container: &lt;container-id&gt;: tried to kill container, but did not receive an exit event",
      "problem": "I cannot stop, remove or kill my docker container. Commands given below with their respective error messages:\n```\n`1. docker stop \n2. docker kill \n3. docker rm \n`\n```\nI get\n```\n`1. Error response from daemon: cannot stop container: : tried to kill container, but did not receive an exit event\n2. Error response from daemon: cannot kill container: : tried to kill container, but did not receive an exit event\n3. Error response from daemon: You cannot remove a running container . Stop the container before attempting removal or force remove\n`\n```\nSame error messages if i prefix everything with sudo and also the same messages if I run all of commands above with --force. How do I solve this? It seems like I can't stop, kill or remove the container because it does not \"receive an exit event\". Nothing here helps: Error response from daemon: cannot stop container - signaling init process caused \"permission denied\" .",
      "solution": "I used wilon's answer from https://forums.docker.com/t/restart-docker-from-command-line/9420/2\nI ran `killall Docker && open /Applications/Docker.app`\nOnce that was done, I ran `docker-compose down` and all containers stopped as expected.",
      "question_score": 96,
      "answer_score": 114,
      "created_at": "2021-12-21T12:34:59",
      "url": "https://stackoverflow.com/questions/70434777/cannot-kill-container-container-id-tried-to-kill-container-but-did-not-rece"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 66963068,
      "title": "Docker Alpine executable binary not found even if in PATH",
      "problem": "I have an alpine running container which contains some binaries in `usr/local/bin`\nWhen I `ls` the content of `usr/local/bin` I got this output :\n`/usr/local/bin # ls\ndwg2SVG     dwg2dxf     dwgadd      dwgbmp      dwgfilter   dwggrep     dwglayers   dwgread     dwgrewrite  dwgwrite    dxf2dwg     dxfwrite\n`\nWhich is what I expected.\nHowever if I execute one of these binaries by calling it I got a `not found` error from the shell :\n`/usr/local/bin # dwg2dxf\nsh: dwgread: not found\n/usr/local/bin # ./dwg2dxf\nsh: ./dwgread: not found\n`\nI tested my `$PATH` which seems to be correct :\n`/usr/local/bin # echo $PATH\n/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n`\nHow can I make those binaries callable or \"foundable\" ? Did I miss something in my Dockerfile build ?\nI suppose that there is something with the `ldconfig` command in alpine that went wrong but I'm not sure.\nEDIT\nAs suggested in one answer here I executed the `file` command and here is the output :\n`/usr/local/bin # file dwg2dxf\ndwgread: ELF 64-bit LSB pie executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, for GNU/Linux 3.2.0, BuildID[sha1]=7835d4a42651a5fb7bdfa2bd8a76e40096bacb07, with debug_info, not stripped\n`\nThoses binaries are from the LibreDWG Official repository as well as the first part of my Dockerfile. Here is the complete Dockerfile :\n```\n`# podman/docker build -t libredwg .\n############################\n# STEP 1 build package from latest tar.xz\n############################\n\nFROM python:3.7.7-buster AS extracting\n# libxml2-dev is broken so we need to compile it by our own\nARG LIBXML2VER=2.9.9\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends autoconf libtool swig texinfo \\\n            build-essential gcc libxml2 python3-libxml2 libpcre2-dev libpcre2-32-0 curl \\\n            libperl-dev libxml2-dev && \\\n    mkdir libxmlInstall && cd libxmlInstall && \\\n    wget ftp://xmlsoft.org/libxml2/libxml2-$LIBXML2VER.tar.gz && \\\n    tar xf libxml2-$LIBXML2VER.tar.gz && \\\n    cd libxml2-$LIBXML2VER/ && \\\n    ./configure && \\\n    make && \\\n    make install && \\\n    cd /libxmlInstall && \\\n    rm -rf gg libxml2-$LIBXML2VER.tar.gz libxml2-$LIBXML2VER\nWORKDIR /app\nRUN tarxz=`curl --silent 'https://ftp.gnu.org/gnu/libredwg/?C=M;O=D' | grep '.tar.xz<' | \\\n         head -n1|sed -E 's/.*href=\"([^\"]+)\".*/\\1/'`; \\\n    echo \"latest release $tarxz\"; \\\n    curl --silent --output \"$tarxz\" https://ftp.gnu.org/gnu/libredwg/$tarxz && \\\n    mkdir libredwg && \\\n    tar -C libredwg --xz --strip-components 1 -xf \"$tarxz\" && \\\n    rm \"$tarxz\" && \\\n    cd libredwg && \\\n    ./configure --disable-bindings --enable-release && \\\n    make -j `nproc` && \\\n    mkdir install && \\\n    make install DESTDIR=\"$PWD/install\" && \\\n    make check DOCKER=1 DESTDIR=\"$PWD/install\"\n\n############################\n# STEP 2 install into stable-slim\n############################\n\n# pull official base image\nFROM osgeo/gdal:alpine-normal-latest\n\n# set work directory\nWORKDIR /usr/src/app\n\n# set environment variables\nENV PYTHONDONTWRITEBYTECODE 1\nENV PYTHONUNBUFFERED 1\n\n# copy requirements file\nCOPY ./requirements.txt /usr/src/app/requirements.txt\n\n# install dependencies\nRUN set -eux \\\n    && apk add --no-cache --virtual .build-deps build-base \\\n        py3-pip libressl-dev libffi-dev gcc musl-dev python3-dev postgresql-dev\\\n    && pip3 install --upgrade pip setuptools wheel \\\n    && pip3 install -r /usr/src/app/requirements.txt \\\n    && rm -rf /root/.cache/pip\n\n# Install libredwg binaries\nCOPY --from=extracting /app/libredwg/install/usr/local/bin/* /usr/local/bin/\nCOPY --from=extracting /app/libredwg/install/usr/local/include/* /usr/local/include/\nCOPY --from=extracting /app/libredwg/install/usr/local/lib/* /usr/local/lib/\nCOPY --from=extracting /app/libredwg/install/usr/local/share/* /usr/local/share/\nRUN ldconfig /usr/local/bin/\nRUN ldconfig /usr/local/include/\nRUN ldconfig /usr/local/lib/\nRUN ldconfig /usr/local/share/\n\n# copy project\nCOPY . /usr/src/app/\n`\n```",
      "solution": "On Alpine Linux, the `not found` error is a typical symptom of dynamic link failure. It is indeed a rather confusing error by musl's `ldd` linker.\nMost of the world Linux software is linked against glibc, the GNU libc library (libc provides the standard C library and POSIX API). Most Linux distributions are based on glibc. OTOH, Alpine Linux is based on the musl libc library, which is a minimal implementation and strictly POSIX compliant. Executables built on glibc distributions depend on `/lib/x86_64-linux-gnu/libc.so.6`, for example, which is not available on Alpine (unless, they are statically linked).\nExcept for this dependency, it's important to note that while musl attempts to maintain glibc compatibility to some extent, it is far from being fully compatible, and complex software that's built against glibc won't work with musl-libc, so simply symlinking `/lib/ld-musl-x86_64.so.1` to the glibc path isn't likely going to work.\nGenerally, there are several ways for running glibc binaries on Alpine:\n\nInstall one the glibc compatibility packages, libc6-compat or gcompat:\n\n```\n`# apk add gcompat\napk add libc6-compat\n`\n```\nBoth packages provide a light weight glibc compatibility layer which may be suitable for running simple glibc applications.\n`libc6-compat` implements glibc compatibility APIs and provides symlinks to glibc shared libraries such as `libm.so`, `libpthread.so` and `libcrypt.so`. The `gcompat` package is based on Adelie Linux gcompat project and does the same but provides a single library `libgcompat.so`. Both libraries install loader stubs. Depdending on the application, one of them may work while the other won't, so it's good to try both.\n\nInstall proper glibc on Alpine, for providing all glibc methods and functionalities. There are glibc builds available for Alpine, which should be installed in the following procedure (example):\n\n```\n`# Source: https://github.com/anapsix/docker-alpine-java\n\nENV GLIBC_REPO=https://github.com/sgerrand/alpine-pkg-glibc\nENV GLIBC_VERSION=2.30-r0\n\nRUN set -ex && \\\n    apk --update add libstdc++ curl ca-certificates && \\\n    for pkg in glibc-${GLIBC_VERSION} glibc-bin-${GLIBC_VERSION}; \\\n        do curl -sSL ${GLIBC_REPO}/releases/download/${GLIBC_VERSION}/${pkg}.apk -o /tmp/${pkg}.apk; done && \\\n    apk add --allow-untrusted /tmp/*.apk && \\\n    rm -v /tmp/*.apk && \\\n    /usr/glibc-compat/sbin/ldconfig /lib /usr/glibc-compat/lib\n`\n```\n\nUse statically linked executables. Static executables don't carry dynamic dependencies and could run on any Linux.\n\nAlternatively, the software may be built from source on Alpine.\n\nFor LibreDWG, let's first verify the issue:\n```\n`/usr/local/bin # ./dwg2dxf\n/bin/sh: ./dwg2dxf: not found\n/usr/local/bin\n/usr/local/bin # ldd ./dwg2dxf\n    /lib64/ld-linux-x86-64.so.2 (0x7fd375538000)\n    libredwg.so.0 => /usr/local/lib/libredwg.so.0 (0x7fd3744db000)\n    libm.so.6 => /lib64/ld-linux-x86-64.so.2 (0x7fd375538000)\n    libc.so.6 => /lib64/ld-linux-x86-64.so.2 (0x7fd375538000)\nError relocating /usr/local/lib/libredwg.so.0: __strcat_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __snprintf_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __memcpy_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __stpcpy_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __strcpy_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __printf_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __fprintf_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __strncat_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __sprintf_chk: symbol not found\nError relocating ./dwg2dxf: __snprintf_chk: symbol not found\nError relocating ./dwg2dxf: __printf_chk: symbol not found\nError relocating ./dwg2dxf: __fprintf_chk: symbol not found\n`\n```\nYou can see that `dwg2dxf` depends on several glibc symbols.\nNow, let's follow option 2 for installing glibc:\n```\n`/usr/src/app # cd /usr/local/bin\n/usr/local/bin # ls\ndwg2SVG     dwg2dxf     dwgadd      dwgbmp      dwgfilter   dwggrep     dwglayers   dwgread     dwgrewrite  dwgwrite    dxf2dwg     dxfwrite\n/usr/local/bin # ./dwg2dxf\n/bin/sh: ./dwg2dxf: not found\n/usr/local/bin # export GLIBC_REPO=https://github.com/sgerrand/alpine-pkg-glibc && \\\n> export GLIBC_VERSION=2.30-r0 && \\\n> apk --update add libstdc++ curl ca-certificates && \\\n> for pkg in glibc-${GLIBC_VERSION} glibc-bin-${GLIBC_VERSION}; \\\n>    do curl -sSL ${GLIBC_REPO}/releases/download/${GLIBC_VERSION}/${pkg}.apk -o /tmp/${pkg}.apk; done && \\\n> apk add --allow-untrusted /tmp/*.apk && \\\n> rm -v /tmp/*.apk && \\\n> /usr/glibc-compat/sbin/ldconfig /lib /usr/glibc-compat/lib\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.13/main/x86_64/APKINDEX.tar.gz\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.13/community/x86_64/APKINDEX.tar.gz\n(1/1) Installing curl (7.74.0-r1)\nExecuting busybox-1.32.1-r3.trigger\nOK: 629 MiB in 126 packages\n(1/2) Installing glibc (2.30-r0)\n(2/2) Installing glibc-bin (2.30-r0)\nExecuting glibc-bin-2.30-r0.trigger\n/usr/glibc-compat/sbin/ldconfig: /usr/local/lib/libredwg.so.0 is not a symbolic link\n/usr/glibc-compat/sbin/ldconfig: /usr/glibc-compat/lib/ld-linux-x86-64.so.2 is not a symbolic link\nOK: 640 MiB in 128 packages\nremoved '/tmp/glibc-2.30-r0.apk'\nremoved '/tmp/glibc-bin-2.30-r0.apk'\n/usr/glibc-compat/sbin/ldconfig: /usr/glibc-compat/lib/ld-linux-x86-64.so.2 is not a symbolic link\n\n/usr/glibc-compat/sbin/ldconfig: /usr/local/lib/libredwg.so.0 is not a symbolic link\n`\n```\nVoila:\n```\n`/usr/local/bin # ./dwg2dxf\n\nUsage: dwg2dxf [-v[N]] [--as rNNNN] [-m|--minimal] [-b|--binary] DWGFILES...\n`\n```",
      "question_score": 91,
      "answer_score": 224,
      "created_at": "2021-04-06T07:18:18",
      "url": "https://stackoverflow.com/questions/66963068/docker-alpine-executable-binary-not-found-even-if-in-path"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 65960211,
      "title": "Docker build pull access denied, repository does not exist or may require",
      "problem": "I am trying to generate a Docker Image without using Visual Studio.  I am in the project folder and I execute from windows 10 admin command line  `docker build .`  I can't figure out how to make this work.\n```\n`[+] Building 1.3s (8/9)\n => [internal] load build definition from Dockerfile                                              0.0s\n => => transferring dockerfile: 753B                                                              0.0s\n => [internal] load .dockerignore                                                                 0.0s\n => => transferring context: 34B                                                                  0.0s\n => [internal] load metadata for mcr.microsoft.com/dotnet/core/aspnet:3.1-buster-slim             0.0s\n => [base 1/2] FROM mcr.microsoft.com/dotnet/core/aspnet:3.1-buster-slim                          0.0s\n => ERROR FROM docker.io/publish/app:latest                                                       1.2s\n => => resolve docker.io/publish/app:latest                                                       1.2s\n => CACHED [base 2/2] WORKDIR /app                                                                0.0s\n => CACHED [final 1/2] WORKDIR /app                                                               0.0s\n => [auth] publish/app:pull token for registry-1.docker.io                                        0.0s\n------\n > FROM docker.io/publish/app:latest:\n------\nfailed to load cache key: pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed\n`\n```\nThis is my dockerfile:\n```\n`#See https://aka.ms/containerfastmode to understand how Visual Studio uses this Dockerfile to build your images for faster debugging.\nFROM mcr.microsoft.com/dotnet/core/aspnet:3.1-buster-slim AS base\nWORKDIR /app\nEXPOSE 80\nEXPOSE 443\n\nFROM mcr.microsoft.com/dotnet/core/sdk:3.1-buster AS build\nWORKDIR /src\nCOPY [\"src/App.Web/App.Web.csproj\", \"src/App.Web/\"]\nRUN dotnet restore \"App.Web.csproj\"\nCOPY . .\nWORKDIR \"/src/App.Web\"\nRUN dotnet build App.Web.csproj -c Debug -o /app\n\nFROM build as debug\nRUN dotnet publish \"App.Web.csproj\" -c Debug -o /app\n\nFROM base as final\nWORKDIR /app\nCOPY --from=publish/app /app .\nENTRYPOINT [\"dotnet\",\"App.Web.dll\"]\n`\n```",
      "solution": "You have stages called base, build, and debug. Then in the final stage you have:\n```\n`COPY --from=publish/app /app .\n`\n```\nWhen docker can't find the stage with that name, `publish/app`, it tries to find that image, which doesn't exist. I'm guessing you want to copy from the build stage, e.g.\n```\n`COPY --from=build /app .\n`\n```",
      "question_score": 90,
      "answer_score": 184,
      "created_at": "2021-01-29T19:54:25",
      "url": "https://stackoverflow.com/questions/65960211/docker-build-pull-access-denied-repository-does-not-exist-or-may-require"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 72695311,
      "title": "Failure starting Docker container. &quot;failed to create shim task: OCI runtime create failed: runc create failed&quot;",
      "problem": "I am new to Ubuntu and new to Docker. I am running a command that was given to me in an explanation of how to start the project. I want to start my Docker containers and they fail with an error.\nSome notes:\n\nIt is a new Ubuntu laptop.\nI added Docker to have `sudo` privileges. `groups` yields `docker` among the list it responds with.\n\nHere's the command I use to start it: `docker-compose -f docker-compose.yml -f docker-compose.dev.yml up -d --build`\nAnd it gives:\n`Step 11/12 : EXPOSE $PORT\n ---> Using cache\n ---> 7620427ebfe9\nStep 12/12 : CMD [\"ts-node\", \"./src/server.ts\"]\n ---> Using cache\n ---> 00a32820e6e2\nSuccessfully built 00a32820e6e2\nSuccessfully tagged backend-marketplace_backend:latest\nbackend-marketplace_database_1 is up-to-date\nStarting backend-marketplace_backend_1 ...\nStarting backend-marketplace_backend_1 ... error\n\nERROR: for backend-marketplace_backend_1  Cannot start service backend: failed to create shim task:\nOCI runtime create failed: runc create failed: unable to start container process: error during container init:\nerror mounting \"/var/lib/docker/volumes/3ceff6572cda1981f7d29faf09f888cb9a8c0c5ac41b10bb323eb5d14e7e1d35/_data\"\nto rootfs at \"/app/node_modules\": mkdir /var/lib/docker/overlay2/c0a5b761bb9a94bb9a4dd3c21a862968dbbabe87698c0f744569ea56e323ea0e/merged/app/node_modules:\nread-only file system: unknown\n\nERROR: for backend  Cannot start service backend: failed to create shim task:\nOCI runtime create failed: runc create failed: unable to start container process: error during container init:\nerror mounting \"/var/lib/docker/volumes/3ceff6572cda1981f7d29faf09f888cb9a8c0c5ac41b10bb323eb5d14e7e1d35/_data\" to rootfs at\n\"/app/node_modules\": mkdir /var/lib/docker/overlay2/c0a5b761bb9a94bb9a4dd3c21a862968dbbabe87698c0f744569ea56e323ea0e/merged/app/node_modules:\nread-only file system: unknown\nERROR: Encountered errors while bringing up the project.\n`\nI see `docker-compose.yml` and `docker-compose.dev.yml` mentioned so here they are:\ndocker-compse.yml:\n```\n`version: \"3\"\nservices:\n  backend:\n    build: .\n    ports:\n      - \"8000:8000\"\n    env_file:\n      - ./.env\n\n`\n```\nand docker-compose.dev.yml:\n```\n`version: \"3\"\nservices:\n  backend:\n    build:\n      context: .\n      args:\n        NODE_ENV: development\n    volumes:\n      - ./:/app:ro\n      - /app/node_modules\n    links:\n      - database\n    env_file:\n      - ./.env\n    command: npm run dev\n  database:\n    image: \"postgres:latest\"\n    volumes:\n      - pgdata:/var/lib/postgresql/data\n    expose:\n      - \"5432\"\n    ports:\n      - \"5432:5432\"\n    env_file:\n      - ./.env.database\n  pgadmin:\n    image: dpage/pgadmin4:latest\n    ports:\n      - 5454:5454/tcp\n    environment:\n      - PGADMIN_DEFAULT_EMAIL=\n      - PGADMIN_DEFAULT_PASSWORD=\n      - PGADMIN_LISTEN_PORT=5454\n    depends_on:\n      - database\nvolumes:\n  pgdata:\n`\n```\nI would love to say \"I found a few threads and tried what they recommend\", but to be honest I don't really understand them when I read them yet. The following threads might be related but they read like Latin to me.\n\"Error response from daemon: failed to create shim: OCI runtime create failed\" error on Windows machine\nCannot start service api: OCI runtime create failed: container_linux.go:380: starting container process caused: exec: \"python manage.py runserver\nCannot start service app: OCI runtime create failed: container_linux.go:349\nLike, my guess from reading the error message is that there's some sort of write permission I need to turn on, because the error message ends in \"read-only file system: unknown\". Sadly, that's all I can contribute.",
      "solution": "A coworker solved my issue.\n`FROM node:16-alpine\nENV NODE_ENV=\"development\"\nWORKDIR /app\nCOPY package.json .\nCOPY package-lock.json .\nARG NODE_ENV\nRUN apk add g++ make py3-pip\nRUN npm install\nRUN chown -R node /app/node_modules\nRUN npm install -g ts-node nodemon\nCOPY . ./\nENV PORT 8000\nEXPOSE $PORT\nCMD [\"ts-node\", \"./src/server.ts\"]\n`\nI added `RUN chown -R node /app/node_modules` and it worked. He said the issue was Linux specific.",
      "question_score": 90,
      "answer_score": 19,
      "created_at": "2022-06-21T06:22:46",
      "url": "https://stackoverflow.com/questions/72695311/failure-starting-docker-container-failed-to-create-shim-task-oci-runtime-crea"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 74804296,
      "title": "Docker endpoint for &quot;default&quot; not found",
      "problem": "I did clone a project and in the first step when I tried to start the container, I did run windows CMD in my project root and I type this command:\n```\n`docker-compose up --build\n`\n```\nand this message is shown to me:\n\ndocker endpoint for \"default\" not found.\n\nI'll be more than happy if somebody helps me. When I write this command for the first time I had an internet problem it got paused, in second time started to download something then this message printed.\nI tried to delete my old Containers, also I try with my VPN on and off, and restart docker in PowerShell.",
      "solution": "For Windows Users\n\nDelete `.docker` directory. Which exists on PATH `C:\\Users\\your-username\\.docker`\nRestart docker.",
      "question_score": 87,
      "answer_score": 188,
      "created_at": "2022-12-14T22:21:00",
      "url": "https://stackoverflow.com/questions/74804296/docker-endpoint-for-default-not-found"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 72784094,
      "title": "Homebrew - Error: Permission denied @ apply2files - /usr/local/lib/docker/cli-plugins",
      "problem": "Homebrew gives me this error on macOS Monterey while using it:\n```\n`Error: Permission denied @ apply2files - /usr/local/lib/docker/cli-plugins\n`\n```\nHow can I fix it?",
      "solution": "EDIT: Permanent Solution from the JW's comment below:\n```\n`sudo rm -r /usr/local/lib/docker/cli-plugins\n`\n```\n\nPrevious Solution\nIf you had recently deleted Docker, you have to manually recreate the Docker folder:\n```\n`mkdir -p /Applications/Docker.app/Contents/Resources/cli-plugins\n`\n```\nThen run\n```\n`brew cleanup\n`\n```\nYou should have fixed the issue!\nAfter that, you can delete the Docker application inside the Application folder\nSource: https://flaviocopes.com/homebrew-fix-permission-denied-apply2files/",
      "question_score": 82,
      "answer_score": 214,
      "created_at": "2022-06-28T11:38:16",
      "url": "https://stackoverflow.com/questions/72784094/homebrew-error-permission-denied-apply2files-usr-local-lib-docker-cli-pl"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 67904609,
      "title": "How do you perform a HEALTHCHECK in the Redis Docker image?",
      "problem": "Recently, we had an outage due to Redis being unable to write to a file system (not sure why it's Amazon EFS) anyway I noted that there was no actual HEALTHCHECK set up for the Docker service to make sure it is running correctly, Redis is up so I can't simply use `nc -z` to check if the port is open.\nIs there a command I can execute in the `redis:6-alpine` (or non-alpine) image that I can put in the `healthcheck` block of the `docker-compose.yml` file.\nNote I am looking for command that is available internally in the image.  Not an external healthcheck.",
      "solution": "Although the `ping` operation from @nitrin0 answer generally works.  It does not handle the case where the write operation will actually fail.  So instead I perform a change that will just increment a value to a key I don't plan to use.\n```\n`image: redis:6\nhealthcheck:\n  test: [ \"CMD\", \"redis-cli\", \"--raw\", \"incr\", \"ping\" ]\n`\n```\nNote this MUST NOT be performed on a cluster that is initialized by Docker.  Since this health check will prevent the cluster from being formed as the Redis are not empty.",
      "question_score": 81,
      "answer_score": 79,
      "created_at": "2021-06-09T14:56:41",
      "url": "https://stackoverflow.com/questions/67904609/how-do-you-perform-a-healthcheck-in-the-redis-docker-image"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 67161579,
      "title": "Cannot find module &#39;sass&#39;",
      "problem": "I wrote a React app and tried to dockerize it.\nAfter I do this it doesn't compile correctly, it doesn't find the `sass` module and the error is:\n\nFailed to compile.\n./src/index.scss\n(./node_modules/css-loader/dist/cjs.js??ref--5-oneOf-6-\n1!./node_modules/postcss-loader/src??postcss!./node_modules/resolve-url-loader??ref--5-\noneOf-6-3!./node_modules/sass-loader/dist/cjs.js??ref--5-oneOf-6-4!./src/index.scss)\nCannot find module 'sass' Require stack:\n\n/app/node_modules/sass-loader/dist/utils.js\n/app/node_modules/sass-loader/dist/index.js\n/app/node_modules/sass-loader/dist/cjs.js\n/app/node_modules/loader-runner/lib/loadLoader.js\n/app/node_modules/loader-runner/lib/LoaderRunner.js\n/app/node_modules/webpack/lib/NormalModule.js\n/app/node_modules/webpack/lib/NormalModuleFactory.js\n/app/node_modules/webpack/lib/Compiler.js\n/app/node_modules/webpack/lib/webpack.js\n/app/node_modules/react-scripts/scripts/start.js\n\nand this is my dockerfile:\n```\n`From node:14.16.1-alpine\n\nWORKDIR /app\n\nENV PATH /app/node_modules/.bin:$PATH\n\nCOPY package.json ./\nCOPY package-lock.json ./\nRUN npm install --silent\nRUN npm install react-scripts@4.0.3 -g --silent\n\nCOPY . ./\n\nCMD [\"npm\", \"start\"]\n`\n```\nand I don't have docker-compose.\nany solution?\nI add this line to my docker file but it doesn't work and gets me the same Error:\n```\n`RUN npm install -g sass\n`\n```",
      "solution": "To note! `node-sass` is deprecated as by now!\n\nWarning: LibSass and Node Sass are deprecated. While they will continue to receive maintenance releases indefinitely, there are no plans to add additional features or compatibility with any new CSS or Sass features. Projects that still use it should move onto Dart Sass.\n\nInstead you can see that Sass is followed on the Dart sass project!\nreact-scripts already moved that direction!\n\nThe used package now is sass! `npm i -g sass` or `npm i sass --save-dev`\nIf you go to npm `sass` page\n\nThis package is a distribution of Dart Sass, compiled to pure JavaScript with no native code or external dependencies. It provides a command-line sass executable and a Node.js API.\n\nYou can install Sass globally using `npm install -g sass` which will provide access to the sass executable. You can also add it to your project using `npm install --save-dev sass`. This provides the executable as well as a library.\n\nWhat should be done\nInstall `sass`\nGlobally\n```\n`npm i -g sass\n`\n```\nor\nLocally\n```\n`npm i sass --save-dev\n`\n```\nI personally prefer to always go with local installs! So that `npm install` will add it automatically! Sometimes too to maintain the versions per project!\nAnd\n\nApp compilling and running after install!\nold version of react scripts\nIf you are running on an old version that require node-sass!\nThen you can Update to the latest version! And before that! You may like to remove `node_modules` and `package-lock.json`.\n```\n`npm i react-scripts --save\n`\n```\nAfter that `npm install` to install again the project dependencies\nAnd you can go for installing `sass` step",
      "question_score": 78,
      "answer_score": 134,
      "created_at": "2021-04-19T13:58:26",
      "url": "https://stackoverflow.com/questions/67161579/cannot-find-module-sass"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 70307460,
      "title": "Error in docker: network &quot;path&quot; declared as external, but could not be found",
      "problem": "I am new to docker. I have been assigned with a task that uses Docker container for development. I followed the tutorial for installing the Docker and the containers on Windows 10, but I have the following error: `network remaxmdcrm_remaxmd-network declared as external, but could not be found`\nThe steps I've done so far are:\n\nCloned the repository from GitHub.\nInstalled Docker on my laptop.\nOnce I installed Docker, I went in the root of my project and ran the following command. `docker-compose build -d -t docker-compose.yml` - docker-compose.yml being the file in the root dir.\nI opened Docker app and I ran the images created.\nI ran the command `docker-compose up`. When I ran this command, the error I specified at the beginning appears. `network remaxmdcrm_remaxmd-network declared as external, but could not be found`\n\ndocker-compose.yml\n```\n`services:\n    ui:\n        build:\n            context: .\n            dockerfile: Dockerfile.development\n        volumes:\n            - .:/app\n        ports:\n            - \"5000:5000\"\n        restart: unless-stopped\n        networks:\n            - remaxmdcrm_remaxmd-network\n\n    redis:\n        image: 'redis:alpine'\n        networks:\n            - remaxmdcrm_remaxmd-network\nnetworks:\n    remaxmdcrm_remaxmd-network:\n        external: true\n`\n```\nRan: `docker ps -a`\n```\n`ID              IMAGE\n5e6cf997487c   remaxmd-site_ui:latest      \n451009e0a2a6   redis:alpine                \n85e7cde67d05   docmer-compose.yml:latest \n\n`\n```\nI might do something wrong here. Can somebody help me? I much appreciate your time!",
      "solution": "I solved the issue, finally. The issue came from the fact that I had in docker-compose.yml `remaxmdcrm_remaxmd-network` declared as external. The external network was not created during installation, thus I needed to create a bridging network.\nI ran the command `docker network create \"name_of_network\"`\nFor further details, here is the full documentation this",
      "question_score": 75,
      "answer_score": 158,
      "created_at": "2021-12-10T17:17:17",
      "url": "https://stackoverflow.com/questions/70307460/error-in-docker-network-path-declared-as-external-but-could-not-be-found"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 70828205,
      "title": "Pushing an image to ECR, getting &quot;Retrying in ... seconds&quot;",
      "problem": "I recently created a new repository in AWS ECR, and I'm attempting to push an image. I'm copy/pasting the directions provided via the \"View push commands\" button on the repository page. I'll copy those here for reference:\n\n`aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 123456789.dkr.ecr.us-west-2.amazonaws.com`\n\n(\"Login succeeded\")\n\n`docker build -t myorg/myapp .`\n\n`docker tag myorg/myapp:latest 123456789.dkr.ecr.us-west-2.amazonaws.com/myorg/myapp:latest`\n\n`docker push 123456789.dkr.ecr.us-west-2.amazonaws.com/myorg/myapp:latest`\n\nHowever, when I get to the `docker push` step, I see:\n```\n`> docker push 123456789.dkr.ecr.us-west-2.amazonaws.com/myorg/myapp:latest\nThe push refers to repository [123456789.dkr.ecr.us-west-2.amazonaws.com/myorg/myapp]\n\na53c8ed5f326: Retrying in 1 second \n78e16537476e: Retrying in 1 second \nb7e38d172e62: Retrying in 1 second \nf1ff72b2b1ca: Retrying in 1 second \n33b67aceeff0: Retrying in 1 second \nc3a550784113: Waiting \n83fc4b4db427: Waiting \ne8ade0d39f19: Waiting \n487d5f9ec63f: Waiting \nb24e42eb9639: Waiting \n9262398ff7bf: Waiting \n804aae047b71: Waiting \n5d33f5d87bf5: Waiting \n4e38024e7e09: Waiting\nEOF\n`\n```\nI'm wondering if this has something to do with the permissions/policies associated with this repository. Right now there are no statements attached to this repository. Is that the missing part? If so, what would that statement look like? I've tried this, but it had no effect:\n```\n`{\n  \"Version\": \"2008-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"AllowPutImage\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::123456789:root\"\n      },\n      \"Action\": \"ecr:PutImage\"\n    }\n  ]\n}\n`\n```\nBonus Points:\nI eventually want to use this in a CDK CodeBuildAction. I was getting the same error as above, so I check to see if I was getting the same result in my local terminal, which I am. So if the policy statement needs to be different for use in the CDK CodeBuildAction those details would be appreciated as well.\nThank you in advance for and advice.",
      "solution": "I was having the same problem when trying to upload the image manually using the AWS and Docker CLI. I was able to fix it by going into ECR -> Repositories -> Permissions then adding a new policy statement with `principal:*` and the following actions:\n```\n`\"ecr:BatchGetImage\",\n\"ecr:BatchCheckLayerAvailability\",\n\"ecr:CompleteLayerUpload\",\n\"ecr:GetDownloadUrlForLayer\",\n\"ecr:InitiateLayerUpload\",\n\"ecr:PutImage\",\n\"ecr:UploadLayerPart\"\n`\n```\nBe sure to add more restrictive principals. I was just trying to see if permissions were the problem in this case and sure enough they were.",
      "question_score": 75,
      "answer_score": 52,
      "created_at": "2022-01-24T02:55:26",
      "url": "https://stackoverflow.com/questions/70828205/pushing-an-image-to-ecr-getting-retrying-in-seconds"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 79817033,
      "title": "Sudden Docker error about &quot;client API version&quot;",
      "problem": "I've been successfully using TestContainers with Docker for quite a while now.  All of a sudden today, I started getting this error:\n```\n`UnixSocketClientProviderStrategy: failed with exception BadRequestException (Status 400:\n{\"message\":\"client version 1.32 is too old. Minimum supported API version is 1.44, please upgrade your client to a newer version\"}\n`\n```\nWhat could explain such a sudden breakage?",
      "solution": "There is an issue raised with Testcontainers about this.\nIf you can run Testcontainers 2.x, update to 2.0.2 or later. If not, update to 1.21.4.\nIf you cannot update at all for now, the following hotfix can be used as a workaround:\n`echo api.version=1.44 >> ~/.docker-java.properties\n`",
      "question_score": 73,
      "answer_score": 88,
      "created_at": "2025-11-11T20:00:14",
      "url": "https://stackoverflow.com/questions/79817033/sudden-docker-error-about-client-api-version"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 73398714,
      "title": "Docker Fails When Building on M1 Macs (exec /usr/local/bin/docker-entrypoint.sh: exec format error)",
      "problem": "I got a legacy project running with `docker-compose`. A year ago it was starting with the configuration below.\nNow it's throwing an error:\n```\n`exec /usr/local/bin/docker-entrypoint.sh: exec format error\n`\n```\nI would like to run the container with the CMD configuration. I found in the web to add `#!/bin/bash` is required to avoid this error, which I added to the Dockerfile.\nThere is no custom `docker-entrypoint.sh` defined. As far as I understand the docs there needs to be either an entrypoint or a command.\n\nThe main purpose of a CMD is to provide defaults for an executing\ncontainer. These defaults can include an executable, or they can omit\nthe executable, in which case you must specify an ENTRYPOINT\ninstruction as well.\n\nDockerfile\n```\n`#!/bin/bash\n\n#nodejs\nFROM node:11.15\nENV NODE_VERSION 11.15\n\n#app directory\nWORKDIR ./\n\n#mongodb tools\nRUN wget -qO - https://www.mongodb.org/static/pgp/server-5.0.asc | apt-key add -\nRUN echo \"deb http://repo.mongodb.org/apt/debian buster/mongodb-org/5.0 main\" | tee /etc/apt/sources.list.d/mongodb-org-5.0.list\nRUN apt-get update\nRUN apt-get install -y mongodb\n\nRUN apt-get install nano\n\n#nodejs packages\n# A wildcard is used to ensure both package.json AND package-lock.json are copied\n# where available (npm@5+)\nCOPY package*.json ./\n\nRUN npm install --ignore-scripts sharp\nRUN npm install --only=production\n\nCOPY . .\n\nRUN mkdir -p /logs/\n\n# wait for mongoDB launch\nADD https://github.com/ufoscout/docker-compose-wait/releases/download/2.5.1/wait /wait\nRUN chmod +x /wait\n\n#port of the app\nEXPOSE 8080\n\nCMD /wait && npm run dockerServer\n`\n```\nDocker Compose\n```\n`version: \"3\"\nservices:\n    watchtower:\n        container_name: watchtower\n        image: v2tec/watchtower\n        env_file:\n             - watchtower.env\n        volumes:\n            - /var/run/docker.sock:/var/run/docker.sock\n            - /root/.docker/config.json:/config.json\n        command: --interval 30\n        restart: always\n    mongo:\n        container_name: mongo\n        ports:\n            - '27017:27017'\n        volumes:\n            - '/temp/im/docker/mongo/data:/data/db'\n            - '/temp/im/docker/backup:/data/backup'\n        image: mongo\n        restart: always\n    core:\n        container_name: core\n        ports:\n            - '8080:8080'\n        env_file:\n            - core.env\n        depends_on:\n            - \"mongo\"\n        volumes:\n            - '/temp/im/docker/logs:/data/logs'\n            - '/temp/im/docker/backup:/data/backup'\n        image: index.docker.io/regname/core:beta\n        logging:\n            driver: \"json-file\"\n            options:\n                max-file: '5'\n                max-size: '10m'\n        restart: always\n`\n```\nEDIT: I changed the title to make it better discoverable.",
      "solution": "Try to change `FROM`:\n```\n`FROM --platform=linux/amd64 node:11.15\n`\n```\nThere is a good explanation.",
      "question_score": 72,
      "answer_score": 167,
      "created_at": "2022-08-18T09:05:30",
      "url": "https://stackoverflow.com/questions/73398714/docker-fails-when-building-on-m1-macs-exec-usr-local-bin-docker-entrypoint-sh"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 71189819,
      "title": "ImportError: cannot import name &#39;json&#39; from itsdangerous",
      "problem": "I am trying to get a Flask and Docker application to work but when I try and run it using my `docker-compose up` command in my Visual Studio terminal, it gives me an ImportError called `ImportError: cannot import name 'json' from itsdangerous`. I have tried to look for possible solutions to this problem but as of right now there are not many on here or anywhere else. The only two solutions I could find are to change the current installation of MarkupSafe and itsdangerous to a higher version: https://serverfault.com/questions/1094062/from-itsdangerous-import-json-as-json-importerror-cannot-import-name-json-fr and another one on GitHub that tells me to essentially change the MarkUpSafe and itsdangerous installation again https://github.com/aws/aws-sam-cli/issues/3661, I have also tried to make a virtual environment named `veganetworkscriptenv` to install the packages but that has also failed as well. I am currently using Flask 2.0.0 and Docker 5.0.0 and the error occurs on line eight in vegamain.py.\nHere is the full ImportError that I get when I try and run the program:\n```\n`veganetworkscript-backend-1  | Traceback (most recent call last):\nveganetworkscript-backend-1  |   File \"/app/vegamain.py\", line 8, in \nveganetworkscript-backend-1  |     from flask import Flask\nveganetworkscript-backend-1  |   File \"/usr/local/lib/python3.9/site-packages/flask/__init__.py\", line 19, in \nveganetworkscript-backend-1  |     from . import json\nveganetworkscript-backend-1  |   File \"/usr/local/lib/python3.9/site-packages/flask/json/__init__.py\", line 15, in \nveganetworkscript-backend-1  |     from itsdangerous import json as _json\nveganetworkscript-backend-1  | ImportError: cannot import name 'json' from 'itsdangerous' (/usr/local/lib/python3.9/site-packages/itsdangerous/__init__.py)\nveganetworkscript-backend-1 exited with code 1\n`\n```\nHere are my requirements.txt, vegamain.py, Dockerfile, and docker-compose.yml files:\nrequirements.txt:\n```\n`Flask==2.0.0\nFlask-SQLAlchemy==2.4.4\nSQLAlchemy==1.3.20\nFlask-Migrate==2.5.3\nFlask-Script==2.0.6\nFlask-Cors==3.0.9\nrequests==2.25.0\nmysqlclient==2.0.1\npika==1.1.0\nwolframalpha==4.3.0\n`\n```\nvegamain.py:\n```\n`# Veganetwork (C) TetraSystemSolutions 2022\n# all rights are reserved.  \n# \n# Author: Trevor R. Blanchard Feb-19-2022-Jul-30-2022\n#\n\n# get our imports in order first\nfrom flask import Flask # \nif __name__ == '__main__':\n    app.run(debug=True, host='0.0.0.0') \n`\n```\nDockerfile:\n```\n`FROM python:3.9\nENV PYTHONUNBUFFERED 1\nWORKDIR /app\nCOPY requirements.txt /app/requirements.txt\nRUN pip install -r requirements.txt\nCOPY . /app\n`\n```\ndocker-compose.yml:\n```\n`version: '3.8'\nservices:\n  backend:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    command: 'python vegamain.py'\n    ports:\n      - 8004:5000\n    volumes:\n      - .:/app\n    depends_on:\n      - db\n\n#  queue:\n#    build:\n#      context: .\n#      dockerfile: Dockerfile\n#    command: 'python -u consumer.py'\n#    depends_on:\n#      - db\n\n  db:\n    image: mysql:5.7.22\n    restart: always\n    environment:\n      MYSQL_DATABASE: admin\n      MYSQL_USER: root\n      MYSQL_PASSWORD: root\n      MYSQL_ROOT_PASSWORD: root\n    volumes:\n      - .dbdata:/var/lib/mysql\n    ports:\n      - 33069:3306\n`\n```\nHow exactly can I fix this code? thank you!",
      "solution": "I just put `itsdangerous==2.0.1` in my requirements.txt .Then updated my virtualenv using `pip install -r requirements.txt` and then `docker-compose up --build` . Now everything fine for me. Didnot upgrade the flask version.",
      "question_score": 72,
      "answer_score": 34,
      "created_at": "2022-02-19T23:41:20",
      "url": "https://stackoverflow.com/questions/71189819/importerror-cannot-import-name-json-from-itsdangerous"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 76708329,
      "title": "Docker-compose no longer building image (AttributeError: cython_sources)",
      "problem": "I am building a django-react site and suddenly my docker-compose no longer builds despite any changes to requirements or image versions.\nMy requirements.txt looks as follows:\n`Django>=3.2.4,=3.12.4,=4.8.0,=2.8.6,=0.15.1,0.5,=3.0,\nThe error output:\n` => [backend internal] load .dockerignore                                                                                                                                                                  0.0s \n => => transferring context: 234B                                                                                                                                                                          0.0s \n => [backend internal] load build definition from Dockerfile                                                                                                                                               0.0s \n => => transferring dockerfile: 933B                                                                                                                                                                       0.0s \n => [backend internal] load metadata for docker.io/library/python:3.9-alpine3.13                                                                                                                           0.5s \n => [backend 1/6] FROM docker.io/library/python:3.9-alpine3.13@sha256:a7cbd1e7784a35a098cedbc8681b790d35ff6030a5e13f043185e2465003a040                                                                     0.0s \n => [backend internal] load build context                                                                                                                                                                  0.0s \n => => transferring context: 2.53kB                                                                                                                                                                        0.0s \n => CACHED [backend 2/6] WORKDIR /app/backend                                                                                                                                                              0.0s \n => CACHED [backend 3/6] COPY ./requirements.txt /tmp/requirements.txt                                                                                                                                     0.0s \n => CACHED [backend 4/6] COPY ./requirements.dev.txt /tmp/requirements.dev.txt                                                                                                                             0.0s \n => CACHED [backend 5/6] COPY . /app/backend                                                                                                                                                               0.0s \n => ERROR [backend 6/6] RUN python -m venv /py &&     /py/bin/pip install --upgrade pip &&     apk add --update --no-cache postgresql-client jpeg-dev &&     apk add --update --no-cache --virtual .tmp-  18.9s \n------\n > [backend 6/6] RUN python -m venv /py &&     /py/bin/pip install --upgrade pip &&     apk add --update --no-cache postgresql-client jpeg-dev &&     apk add --update --no-cache --virtual .tmp-build-deps     \n    build-base postgresql-dev musl-dev zlib zlib-dev &&     /py/bin/pip install -r /tmp/requirements.txt &&     if [ true = \"true\" ];         then /py/bin/pip install -r /tmp/requirements.dev.txt ;     fi && \n    rm -rf /tmp &&     apk del .tmp-build-deps &&     adduser         --disabled-password         --no-create-home         django-user:\n2.951 Requirement already satisfied: pip in /py/lib/python3.9/site-packages (21.2.4)\n3.090 Collecting pip\n3.202   Downloading pip-23.2-py3-none-any.whl (2.1 MB)\n3.389 Installing collected packages: pip\n3.389   Attempting uninstall: pip\n3.389     Found existing installation: pip 21.2.4\n3.454     Uninstalling pip-21.2.4:\n3.458       Successfully uninstalled pip-21.2.4\n4.339 Successfully installed pip-23.2\n4.506 fetch https://dl-cdn.alpinelinux.org/alpine/v3.13/main/x86_64/APKINDEX.tar.gz\n4.832 fetch https://dl-cdn.alpinelinux.org/alpine/v3.13/community/x86_64/APKINDEX.tar.gz\n5.062 (1/9) Installing libjpeg-turbo (2.1.0-r0)\n5.092 (2/9) Installing pkgconf (1.7.3-r0)\n5.116 (3/9) Installing libjpeg-turbo-dev (2.1.0-r0)\n5.137 (4/9) Installing jpeg-dev (9d-r1)\n5.156 (5/9) Installing libedit (20191231.3.1-r1)\n5.179 (6/9) Installing libsasl (2.1.28-r0)\n5.204 (7/9) Installing libldap (2.4.57-r1)\n5.263 (8/9) Installing libpq (13.11-r0)\n5.287 (9/9) Installing postgresql-client (13.11-r0)\n5.358 Executing busybox-1.32.1-r7.trigger\n5.362 OK: 20 MiB in 45 packages\n5.383 fetch https://dl-cdn.alpinelinux.org/alpine/v3.13/main/x86_64/APKINDEX.tar.gz\n5.520 fetch https://dl-cdn.alpinelinux.org/alpine/v3.13/community/x86_64/APKINDEX.tar.gz\n5.644 (1/37) Upgrading musl (1.2.2-r1 -> 1.2.2-r2)\n5.676 (2/37) Upgrading libcrypto1.1 (1.1.1l-r0 -> 1.1.1s-r0)\n5.755 (3/37) Upgrading libssl1.1 (1.1.1l-r0 -> 1.1.1s-r0)\n5.806 (4/37) Upgrading zlib (1.2.11-r3 -> 1.2.12-r3)\n5.830 (5/37) Installing libgcc (10.2.1_pre1-r3)\n5.860 (6/37) Installing libstdc++ (10.2.1_pre1-r3)\n5.923 (7/37) Installing binutils (2.35.2-r1)\n6.069 (8/37) Installing libmagic (5.39-r0)\n6.137 (9/37) Installing file (5.39-r0)\n6.156 (10/37) Installing libgomp (10.2.1_pre1-r3)\n6.191 (11/37) Installing libatomic (10.2.1_pre1-r3)\n6.208 (12/37) Installing libgphobos (10.2.1_pre1-r3)\n6.307 (13/37) Installing gmp (6.2.1-r1)\n6.342 (14/37) Installing isl22 (0.22-r0)\n6.403 (15/37) Installing mpfr4 (4.1.0-r0)\n6.501 (16/37) Installing mpc1 (1.2.0-r0)\n6.528 (17/37) Installing gcc (10.2.1_pre1-r3)\n7.430 (18/37) Installing musl-dev (1.2.2-r2)\n7.544 (19/37) Installing libc-dev (0.7.2-r3)\n7.574 (20/37) Installing g++ (10.2.1_pre1-r3)\n8.135 (21/37) Installing make (4.3-r0)\n8.157 (22/37) Installing fortify-headers (1.1-r0)\n8.177 (23/37) Installing patch (2.7.6-r7)\n8.209 (24/37) Installing build-base (0.5-r3)\n8.229 (25/37) Installing libxml2 (2.9.14-r2)\n8.293 (26/37) Installing llvm10-libs (10.0.1-r1)\n8.719 (27/37) Installing clang-libs (10.0.1-r0)\n9.345 (28/37) Installing clang (10.0.1-r0)\n9.587 (29/37) Installing llvm10 (10.0.1-r1)\n9.695 (30/37) Installing openssl-dev (1.1.1s-r0)\n9.724 (31/37) Installing icu-libs (67.1-r2)\n9.985 (32/37) Installing icu (67.1-r2)\n10.01 (33/37) Installing icu-dev (67.1-r2)\n10.07 (34/37) Installing postgresql-libs (13.11-r0)\n10.10 (35/37) Installing postgresql-dev (13.11-r0)\n10.22 (36/37) Installing zlib-dev (1.2.12-r3)\n10.25 (37/37) Installing .tmp-build-deps (20230717.214226)\n10.25 Executing busybox-1.32.1-r7.trigger\n10.25 Executing ca-certificates-20191127-r5.trigger\n10.31 OK: 414 MiB in 78 packages\n10.78 Collecting Django=3.2.4 (from -r /tmp/requirements.txt (line 1))\n10.78   Obtaining dependency information for Django=3.2.4 from https://files.pythonhosted.org/packages/84/eb/5329ae72bf26b91844985d0de74e4edf876e3ca409d085820f230eea2eba/Django-3.2.20-py3-none-any.whl.metadata\n10.84   Downloading Django-3.2.20-py3-none-any.whl.metadata (4.1 kB)\n10.91 Collecting djangorestframework=3.12.4 (from -r /tmp/requirements.txt (line 2))\n10.93   Downloading djangorestframework-3.13.1-py3-none-any.whl (958 kB)\n11.04      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 958.3/958.3 kB 9.6 MB/s eta 0:00:00\n11.07 Collecting djangorestframework-simplejwt=4.8.0 (from -r /tmp/requirements.txt (line 3))\n11.09   Downloading djangorestframework_simplejwt-5.2.2-py3-none-any.whl (85 kB)\n11.11      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 85.4/85.4 kB 7.8 MB/s eta 0:00:00\n11.17 Collecting psycopg2=2.8.6 (from -r /tmp/requirements.txt (line 4))\n11.19   Downloading psycopg2-2.8.6.tar.gz (383 kB)\n11.21      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 383.8/383.8 kB 30.7 MB/s eta 0:00:00\n11.24   Installing build dependencies: started\n12.43   Installing build dependencies: finished with status 'done'\n12.43   Getting requirements to build wheel: started\n12.57   Getting requirements to build wheel: finished with status 'done'\n12.57   Preparing metadata (pyproject.toml): started\n12.69   Preparing metadata (pyproject.toml): finished with status 'done'\n12.74 Collecting drf-spectacular=0.15.1 (from -r /tmp/requirements.txt (line 5))\n12.74   Obtaining dependency information for drf-spectacular=0.15.1 from https://files.pythonhosted.org/packages/e2/3b/29189bbfb2443335ed1d6f0750d14f7ba5e5b89699a58cc87fa82fdb5f49/drf_spectacular-0.26.3-py3-none-any.whl.metadata\n12.76   Downloading drf_spectacular-0.26.3-py3-none-any.whl.metadata (13 kB)\n12.79 Collecting django-allauth0.5 (from -r /tmp/requirements.txt (line 6))\n12.81   Downloading django-allauth-0.54.0.tar.gz (737 kB)\n12.85      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 737.1/737.1 kB 18.6 MB/s eta 0:00:00\n13.04   Installing build dependencies: started\n14.05   Installing build dependencies: finished with status 'done'\n14.05   Getting requirements to build wheel: started\n14.24   Getting requirements to build wheel: finished with status 'done'\n14.25   Preparing metadata (pyproject.toml): started\n14.44   Preparing metadata (pyproject.toml): finished with status 'done'\n14.48 Collecting dj-rest-auth=3.0 (from -r /tmp/requirements.txt (line 7))\n14.50   Downloading dj-rest-auth-3.0.0.tar.gz (100 kB)\n14.51      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100.2/100.2 kB 10.6 MB/s eta 0:00:00\n14.54   Installing build dependencies: started\n15.52   Installing build dependencies: finished with status 'done'\n15.52   Getting requirements to build wheel: started\n15.63   Getting requirements to build wheel: finished with status 'done'\n15.63   Preparing metadata (pyproject.toml): started\n15.75   Preparing metadata (pyproject.toml): finished with status 'done'\n15.79 Collecting asgiref=3.3.2 (from Django=3.2.4->-r /tmp/requirements.txt (line 1))\n15.79   Obtaining dependency information for asgiref=3.3.2 from https://files.pythonhosted.org/packages/9b/80/b9051a4a07ad231558fcd8ffc89232711b4e618c15cb7a392a17384bbeef/asgiref-3.7.2-py3-none-any.whl.metadata\n15.80   Downloading asgiref-3.7.2-py3-none-any.whl.metadata (9.2 kB)\n15.87 Collecting pytz (from Django=3.2.4->-r /tmp/requirements.txt (line 1))\n15.90   Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n15.93      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 502.3/502.3 kB 20.1 MB/s eta 0:00:00\n15.96 Collecting sqlparse>=0.2.2 (from Django=3.2.4->-r /tmp/requirements.txt (line 1))\n15.98   Downloading sqlparse-0.4.4-py3-none-any.whl (41 kB)\n15.99      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 41.2/41.2 kB 11.2 MB/s eta 0:00:00\n16.05 Collecting pyjwt=1.7.1 (from djangorestframework-simplejwt=4.8.0->-r /tmp/requirements.txt (line 3))\n16.07   Downloading PyJWT-2.7.0-py3-none-any.whl (22 kB)\n16.11 Collecting uritemplate>=2.0.0 (from drf-spectacular=0.15.1->-r /tmp/requirements.txt (line 5))\n16.13   Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n16.18 Collecting PyYAML>=5.1 (from drf-spectacular=0.15.1->-r /tmp/requirements.txt (line 5))\n16.20   Downloading PyYAML-6.0.tar.gz (124 kB)\n16.21      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 125.0/125.0 kB 16.8 MB/s eta 0:00:00\n16.31   Installing build dependencies: started\n18.33   Installing build dependencies: finished with status 'done'\n18.33   Getting requirements to build wheel: started\n18.48   Getting requirements to build wheel: finished with status 'error'\n18.48   error: subprocess-exited-with-error\n18.48\n18.48   \u00d7 Getting requirements to build wheel did not run successfully.\n18.48   \u2502 exit code: 1\n18.48   \u2570\u2500> [48 lines of output]\n18.48       running egg_info\n18.48       writing lib/PyYAML.egg-info/PKG-INFO\n18.48       writing dependency_links to lib/PyYAML.egg-info/dependency_links.txt\n18.48       writing top-level names to lib/PyYAML.egg-info/top_level.txt\n18.48       Traceback (most recent call last):\n18.48         File \"/py/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in \n18.48           main()\n18.48         File \"/py/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n18.48           json_out['return_val'] = hook(**hook_input['kwargs'])\n18.48         File \"/py/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 118, in get_requires_for_build_wheel\n18.48           return hook(config_settings)\n18.48         File \"/tmp/pip-build-env-a_iltcrr/overlay/lib/python3.9/site-packages/setuptools/build_meta.py\", line 341, in get_requires_for_build_wheel\n18.48           return self._get_build_requires(config_settings, requirements=['wheel'])\n18.48         File \"/tmp/pip-build-env-a_iltcrr/overlay/lib/python3.9/site-packages/setuptools/build_meta.py\", line 323, in _get_build_requires\n18.48           self.run_setup()\n18.48         File \"/tmp/pip-build-env-a_iltcrr/overlay/lib/python3.9/site-packages/setuptools/build_meta.py\", line 338, in run_setup\n18.48           exec(code, locals())\n18.48         File \"\", line 288, in \n18.48         File \"/tmp/pip-build-env-a_iltcrr/overlay/lib/python3.9/site-packages/setuptools/__init__.py\", line 107, in setup\n18.48           return distutils.core.setup(**attrs)\n18.48         File \"/tmp/pip-build-env-a_iltcrr/overlay/lib/python3.9/site-packages/setuptools/_distutils/core.py\", line 185, in setup\n18.48           return run_commands(dist)\n18.48         File \"/tmp/pip-build-env-a_iltcrr/overlay/lib/python3.9/site-packages/setuptools/_distutils/core.py\", line 201, in run_commands\n18.48           dist.run_commands()\n18.48         File \"/tmp/pip-build-env-a_iltcrr/overlay/lib/python3.9/site-packages/setuptools/_distutils/dist.py\", line 969, in run_commands\n18.48           self.run_command(cmd)\n18.48         File \"/tmp/pip-build-env-a_iltcrr/overlay/lib/python3.9/site-packages/setuptools/dist.py\", line 1234, in run_command\n18.48           super().run_command(command)\n18.48         File \"/tmp/pip-build-env-a_iltcrr/overlay/lib/python3.9/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n18.48           cmd_obj.run()\n18.48         File \"/tmp/pip-build-env-a_iltcrr/overlay/lib/python3.9/site-packages/setuptools/command/egg_info.py\", line 314, in run\n18.48           self.find_sources()\n18.48         File \"/tmp/pip-build-env-a_iltcrr/overlay/lib/python3.9/site-packages/setuptools/command/egg_info.py\", line 322, in find_sources\n18.48           mm.run()\n18.48         File \"/tmp/pip-build-env-a_iltcrr/overlay/lib/python3.9/site-packages/setuptools/command/egg_info.py\", line 551, in run\n18.48           self.add_defaults()\n18.48         File \"/tmp/pip-build-env-a_iltcrr/overlay/lib/python3.9/site-packages/setuptools/command/egg_info.py\", line 589, in add_defaults\n18.48           sdist.add_defaults(self)\n18.48         File \"/tmp/pip-build-env-a_iltcrr/overlay/lib/python3.9/site-packages/setuptools/command/sdist.py\", line 104, in add_defaults\n18.48           super().add_defaults()\n18.48         File \"/tmp/pip-build-env-a_iltcrr/overlay/lib/python3.9/site-packages/setuptools/_distutils/command/sdist.py\", line 251, in add_defaults\n18.48           self._add_defaults_ext()\n18.48         File \"/tmp/pip-build-env-a_iltcrr/overlay/lib/python3.9/site-packages/setuptools/_distutils/command/sdist.py\", line 336, in _add_defaults_ext\n18.48           self.filelist.extend(build_ext.get_source_files())\n18.48         File \"\", line 204, in get_source_files\n18.48         File \"/tmp/pip-build-env-a_iltcrr/overlay/lib/python3.9/site-packages/setuptools/_distutils/cmd.py\", line 107, in __getattr__\n18.48           raise AttributeError(attr)\n18.48       AttributeError: cython_sources\n18.48       [end of output]\n18.48\n18.48   note: This error originates from a subprocess, and is likely not a problem with pip.\n18.49 error: subprocess-exited-with-error\n18.49\n18.49 \u00d7 Getting requirements to build wheel did not run successfully.\n18.49 \u2502 exit code: 1\n18.49 \u2570\u2500> See above for output.\n18.49\n18.49 note: This error originates from a subprocess, and is likely not a problem with pip.\n`\nI build this exact image with these dependencies yesterday so I know it's compatible. Yet tried to build today and no longer works.\nWith the answer from Simeon Borisov, the problem seems to be drf-spectacular in my requirements folder, as removing this fixes the build.\nHowever, I'm still struggling.\n\nAdding `pyyaml==5.4.1` above it does not fix the problem.\nThis is curious, as before these exact versions worked together, but now they do not. Furthermore, older versions I've seem compiled in tutorials in which I first learned docker, which I have used myself, also no longer work for me. What exactly changed?\n\nAfter adding `pyyaml` dependency, my error log is as follows:\n`Collecting Django=3.2.4 (from -r /tmp/requirements.txt (line 1))\n23.17   Obtaining dependency information for Django=3.2.4 from https://files.pythonhosted.org/packages/84/eb/5329ae72bf26b91844985d0de74e4edf876e3ca409d085820f230eea2eba/Django-3.2.20-py3-none-any.whl.metadata\n23.32   Downloading Django-3.2.20-py3-none-any.whl.metadata (4.1 kB)\n23.37 Collecting djangorestframework=3.12.4 (from -r /tmp/requirements.txt (line 2))\n23.39   Downloading djangorestframework-3.13.1-py3-none-any.whl (958 kB)\n23.54      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 958.3/958.3 kB 6.8 MB/s eta 0:00:00\n23.57 Collecting djangorestframework-simplejwt=4.8.0 (from -r /tmp/requirements.txt (line 3))\n23.60   Downloading djangorestframework_simplejwt-5.2.2-py3-none-any.whl (85 kB)\n23.61      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 85.4/85.4 kB 9.6 MB/s eta 0:00:00\n23.68 Collecting psycopg2=2.8.6 (from -r /tmp/requirements.txt (line 4))\n23.71   Downloading psycopg2-2.8.6.tar.gz (383 kB)\n23.76      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 383.8/383.8 kB 7.9 MB/s eta 0:00:00\n23.80   Installing build dependencies: started\n25.10   Installing build dependencies: finished with status 'done'\n25.10   Getting requirements to build wheel: started\n25.23   Getting requirements to build wheel: finished with status 'done'\n25.23   Preparing metadata (pyproject.toml): started\n25.36   Preparing metadata (pyproject.toml): finished with status 'done'\n25.46 Collecting pyyaml==5.4.1 (from -r /tmp/requirements.txt (line 5))\n25.48   Downloading PyYAML-5.4.1.tar.gz (175 kB)\n25.50      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 175.1/175.1 kB 7.7 MB/s eta 0:00:00\n25.60   Installing build dependencies: started\n27.95   Installing build dependencies: finished with status 'done'\n27.95   Getting requirements to build wheel: started\n28.12   Getting requirements to build wheel: finished with status 'error'\n28.12   error: subprocess-exited-with-error\n28.12\n28.12   \u00d7 Getting requirements to build wheel did not run successfully.\n28.12   \u2502 exit code: 1\n28.12   \u2570\u2500> [62 lines of output]\n28.12       /tmp/pip-build-env-m9dhhrb5/overlay/lib/python3.9/site-packages/setuptools/config/setupcfg.py:293: _DeprecatedConfig: Deprecated config in `setup.cfg`\n28.12       !!\n28.12\n28.12               ********************************************************************************\n28.12               The license_file parameter is deprecated, use license_files instead.\n28.12\n28.12               By 2023-Oct-30, you need to update your project and remove deprecated calls\n28.12               or your builds will no longer be supported.\n28.12\n28.12               See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n28.12               ********************************************************************************\n28.12\n28.12       !!\n28.12         parsed = self.parsers.get(option_name, lambda x: x)(value)\n28.12       running egg_info\n28.12       writing lib3/PyYAML.egg-info/PKG-INFO\n28.12       writing dependency_links to lib3/PyYAML.egg-info/dependency_links.txt\n28.12       writing top-level names to lib3/PyYAML.egg-info/top_level.txt\n28.12       Traceback (most recent call last):\n28.12         File \"/py/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in \n28.12           main()\n28.12         File \"/py/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n28.12           json_out['return_val'] = hook(**hook_input['kwargs'])\n28.12         File \"/py/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 118, in get_requires_for_build_wheel\n28.12           return hook(config_settings)\n28.12         File \"/tmp/pip-build-env-m9dhhrb5/overlay/lib/python3.9/site-packages/setuptools/build_meta.py\", line 341, in get_requires_for_build_wheel\n28.12           return self._get_build_requires(config_settings, requirements=['wheel'])\n28.12         File \"/tmp/pip-build-env-m9dhhrb5/overlay/lib/python3.9/site-packages/setuptools/build_meta.py\", line 323, in _get_build_requires\n28.12           self.run_setup()\n28.12         File \"/tmp/pip-build-env-m9dhhrb5/overlay/lib/python3.9/site-packages/setuptools/build_meta.py\", line 338, in run_setup\n28.12           exec(code, locals())\n28.12         File \"\", line 271, in \n28.12         File \"/tmp/pip-build-env-m9dhhrb5/overlay/lib/python3.9/site-packages/setuptools/__init__.py\", line 107, in setup\n28.12           return distutils.core.setup(**attrs)\n28.12         File \"/tmp/pip-build-env-m9dhhrb5/overlay/lib/python3.9/site-packages/setuptools/_distutils/core.py\", line 185, in setup\n28.12           return run_commands(dist)\n28.12         File \"/tmp/pip-build-env-m9dhhrb5/overlay/lib/python3.9/site-packages/setuptools/_distutils/core.py\", line 201, in run_commands\n28.12           dist.run_commands()\n28.12         File \"/tmp/pip-build-env-m9dhhrb5/overlay/lib/python3.9/site-packages/setuptools/_distutils/dist.py\", line 969, in run_commands\n28.12           self.run_command(cmd)\n28.12         File \"/tmp/pip-build-env-m9dhhrb5/overlay/lib/python3.9/site-packages/setuptools/dist.py\", line 1234, in run_command\n28.12           super().run_command(command)\n28.12         File \"/tmp/pip-build-env-m9dhhrb5/overlay/lib/python3.9/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n28.12           cmd_obj.run()\n28.12         File \"/tmp/pip-build-env-m9dhhrb5/overlay/lib/python3.9/site-packages/setuptools/command/egg_info.py\", line 314, in run\n28.12           self.find_sources()\n28.12         File \"/tmp/pip-build-env-m9dhhrb5/overlay/lib/python3.9/site-packages/setuptools/command/egg_info.py\", line 322, in find_sources\n28.12           mm.run()\n28.12         File \"/tmp/pip-build-env-m9dhhrb5/overlay/lib/python3.9/site-packages/setuptools/command/egg_info.py\", line 551, in run\n28.12           self.add_defaults()\n28.12         File \"/tmp/pip-build-env-m9dhhrb5/overlay/lib/python3.9/site-packages/setuptools/command/egg_info.py\", line 589, in add_defaults\n28.12           sdist.add_defaults(self)\n28.12         File \"/tmp/pip-build-env-m9dhhrb5/overlay/lib/python3.9/site-packages/setuptools/command/sdist.py\", line 104, in add_defaults\n28.12           super().add_defaults()\n28.12         File \"/tmp/pip-build-env-m9dhhrb5/overlay/lib/python3.9/site-packages/setuptools/_distutils/command/sdist.py\", line 251, in add_defaults\n28.12           self._add_defaults_ext()\n28.12         File \"/tmp/pip-build-env-m9dhhrb5/overlay/lib/python3.9/site-packages/setuptools/_distutils/command/sdist.py\", line 336, in _add_defaults_ext\n28.12           self.filelist.extend(build_ext.get_source_files())\n28.12         File \"\", line 201, in get_source_files\n28.12         File \"/tmp/pip-build-env-m9dhhrb5/overlay/lib/python3.9/site-packages/setuptools/_distutils/cmd.py\", line 107, in __getattr__\n28.12           raise AttributeError(attr)\n28.12       AttributeError: cython_sources\n28.12       [end of output]\n28.12\n28.12   note: This error originates from a subprocess, and is likely not a problem with pip.\n28.13 error: subprocess-exited-with-error\n28.13\n28.13 \u00d7 Getting requirements to build wheel did not run successfully.\n28.13 \u2502 exit code: 1\n28.13 \u2570\u2500> See above for output.\n`",
      "solution": "It seems like a regression according to this thread. The PyYAML package is probably a dependency of one of your dependencies and autoupdated to a broken version (most likely because of `drf-spectacular`). You should be able to add the pyyaml package into your reuqirements.txt and specify the version of it. Perhaps you should also be more specific with the versioning of `drf-spectacular`, in case you get dependency compatibility issues.\n```\n`pyyaml==5.4.1\ndrf-spectacular=={whatever the version was yesterday}\n`\n```\nIf you'd like to avoid this in the future, consider using pipenv and it's Pipfile and Pipfile.lock versioning system, which gives you control over which versions you use in your build.\nEdit: I see from other threads the issue has been resolved and it's probably a good idea to update `pyyaml` to the latest version.",
      "question_score": 72,
      "answer_score": 5,
      "created_at": "2023-07-17T23:57:25",
      "url": "https://stackoverflow.com/questions/76708329/docker-compose-no-longer-building-image-attributeerror-cython-sources"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 71941032,
      "title": "Why I cannot run `apt update` inside a fresh ubuntu:22.04?",
      "problem": "I currently don't manage to run `apt update` inside a fresh `ubuntu:22.04` (codename `jammy`).\nProtocol\n`$ docker --version\nDocker version 20.10.2, build 2291f61\n\n$ docker run --init --rm -it ubuntu:22.04\nroot@123456789:/# apt update\n`\nObserved\n`$ docker run --init --rm -it ubuntu:22.04\nroot@6444bf2cb8b4:/# apt update\nGet:1 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [90.7 kB]            \nGet:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [90.7 kB] \nGet:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [90.7 kB]             \nGet:5 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\nGet:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]       \nGet:7 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]           \nGet:8 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB]             \nFetched 20.2 MB in 1s (17.6 MB/s)                                                           \nReading package lists... Done\nE: Problem executing scripts APT::Update::Post-Invoke 'rm -f /var/cache/apt/archives/*.deb /var/cache/apt/archives/partial/*.deb /var/cache/apt/*.bin || true'\nE: Sub-process returned an error code\nroot@6444bf2cb8b4:/# \n`\nExpected\n`apt update` pass like on a `ubuntu:20.04` base image...\nnote: same issue with `apt-get install` ...\nref: https://hub.docker.com/_/ubuntu",
      "solution": "Seems this is related to the use of the syscall `clone3` by `Glibc >= 2.34`...\nSo you need `Docker >= 20.10.9` to fix it.\nref: https://github.com/moby/moby/pull/42681\nref: https://pascalroeleven.nl/2021/09/09/ubuntu-21-10-and-fedora-35-in-docker/",
      "question_score": 71,
      "answer_score": 74,
      "created_at": "2022-04-20T15:58:00",
      "url": "https://stackoverflow.com/questions/71941032/why-i-cannot-run-apt-update-inside-a-fresh-ubuntu22-04"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 68984133,
      "title": "Error: &quot;Failed to solve with frontend dockerfile.v0: failed to create LLB definition: no match for platform in manifest&quot; when building a Docker image",
      "problem": "I get the error:\n\nfailed to solve with frontend dockerfile.v0: failed to create LLB definition: no match for platform in manifest\n\nwhen building the following Dockerfile:\n```\n`FROM mcr.microsoft.com/dotnet/framework/aspnet:4.8\nCOPY . /inetpub/wwwroot\n`\n```",
      "solution": "The cause was simple. I had my Docker desktop running on Linux containers and the image was build from a Windows image.\nSimply switching to Windows containers solved the problem.\nThe message is clueless, so I hope this save some time for others.",
      "question_score": 69,
      "answer_score": 86,
      "created_at": "2021-08-30T14:24:29",
      "url": "https://stackoverflow.com/questions/68984133/error-failed-to-solve-with-frontend-dockerfile-v0-failed-to-create-llb-defini"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 70955307,
      "title": "How to install Google chrome in a docker container",
      "problem": "I'm trying to install chrome in a docker container. I execute:\n```\n`RUN apt-get install -y wget\nRUN wget -q https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\nRUN dpkg -i google-chrome-stable_current_amd64.deb  # problem here\nRUN apt -f install -y\n`\n```\nThe problem is that `dpkg -i` fails because of missing dependencies. In principle this is not a big problem, as the next command should fix this, and indeed it does it when run interactively from within the container. But the problem is that when building a docker container this error makes the build process to stop:\n```\n`dpkg: error processing package google-chrome-stable (--install):\n dependency problems - leaving unconfigured\nErrors were encountered while processing:\n google-chrome-stable\nroot@78b45ab9aa33:/# \nexit\n`\n```\nHow can I overcome this problem? Isn't there a simpler way to install chrome without provoking the dependence problem? I can't find the repository to add so I can run a regular `apg-get install google-chrome`, that is what I'd like to do. In the google linux repository they just mention that the \"the packages will automatically configure the repository settings necessary\". Which is not exactly what I get...",
      "solution": "After the comment by @Facty and some more search, I found two solutions to install Google Chrome without raising this error. I'll post it below for future references or people having the same issue.\nThere are actually two ways to install Chrome on a docker container:\nIf you download the deb file manually, you can install it with `apt-get` instead of `dpkg`. This will automatically install the dependencies without having to call `apt -f install -y` later :\n```\n`RUN apt-get install -y wget\nRUN wget -q https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\nRUN apt-get install -y ./google-chrome-stable_current_amd64.deb\n`\n```\nThe other solution is to add the repositories (installing the gpg key) and install from them directly, skipping the manual download:\n```\n`RUN apt-get install -y wget\nRUN wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - \\ \n    && echo \"deb http://dl.google.com/linux/chrome/deb/ stable main\" >> /etc/apt/sources.list.d/google.list\nRUN apt-get update && apt-get -y install google-chrome-stable\n`\n```",
      "question_score": 68,
      "answer_score": 79,
      "created_at": "2022-02-02T13:04:32",
      "url": "https://stackoverflow.com/questions/70955307/how-to-install-google-chrome-in-a-docker-container"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 67361936,
      "title": "&#39;exec user process caused: exec format error&#39; in AWS Fargate Service",
      "problem": "I am totally new to AWS but I've been running my dockerized application locally for months now with no issues. Now that I am trying to deploy this app via AWS ECS/Fargate, my containers are stopped repeatedly with this linux error: `standard_init_linux.go:219: exec user process caused: exec format error`. This error seems to suggest that the architecture in Fargate does not recognize one of the Linux commands I'm running but I can't find a good answer anywhere for how to find the architecture that's running or how to track down the specific command that's causing the issue.\nThese are my Dockerfiles for the frontend and backend. The project is built in the MERN stack and is split into the frontend (React) and the backend (MongoDB/Express)\nFrontend:\n```\n`FROM alpine:3.10\n\nENV NODE_VERSION 15.9.0\n\nWORKDIR /frontend\n\nCOPY package*.json ./\n\nRUN apk add --no-cache nodejs npm\n\n# some packages rely on gyp so we need this\n# pulled from https://github.com/nodejs/docker-node/issues/282\nRUN apk add --no-cache --virtual .gyp \\\n        python \\\n        make \\\n        g++ \\\n    && npm install\n\nCOPY . .\n\nEXPOSE 3000\n\nCMD [\"npm\", \"start\"]\n`\n```\nBackend:\n```\n`FROM alpine:3.10\n\nENV NODE_VERSION 15.9.0\n\nWORKDIR /backend\n\nCOPY package*.json ./\n\nRUN apk add --no-cache nodejs npm\n\n# some packages rely on gyp so we need this\n# pulled from https://github.com/nodejs/docker-node/issues/282\nRUN apk add --no-cache --virtual .gyp \\\n        python \\\n        make \\\n        g++ \\\n    && npm install --silent\\\n    && apk del .gyp\n\nCOPY ./ ./\n\nEXPOSE 8080\n\nCMD [\"npm\", \"start\"]\n`\n```\nAny help would be greatly appreciated!",
      "solution": "I think you've identified your problem.\nYou're building your images on Apple's M1 chip, which is an ARM architecture. Fargate is probably running on the more common Intel x86-64 architecture. Images you build locally on your Mac aren't going to be able to run there.\nThe easiest solution is probably to have your images build automatically in Docker Hub (or use a Github Action to build them in Github).\nI don't have all the details about how you're building and deploying your images, so it's possible I'm missing some details.",
      "question_score": 67,
      "answer_score": 65,
      "created_at": "2021-05-03T01:21:44",
      "url": "https://stackoverflow.com/questions/67361936/exec-user-process-caused-exec-format-error-in-aws-fargate-service"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 65614378,
      "title": "Getting docker build to show IDs of intermediate containers",
      "problem": "I'm trying to learn docker but experience some discrepancies to what I've read in each tutorial:\nWhen using the following command `docker build -t my-app:1.0 .` my build fails due to some error in my Dockerfile. Following this answer, I wanted to run the last intermediate container ID. Anyhow, in contrast to all tutorials I've seen so far, my console is not showing any intermediate container IDs:\n\nI'm running Docker 19 on Windows 10.\nHow do I get the intermediate container IDs?",
      "solution": "I had the same problem. Setting \"buildkit\" to false in `~/.docker/daemon.json` (In Windows you should find daemon.json in C:\\ProgramData\\Docker\\config) solved this for me:\n```\n`    {\n      \"experimental\": true,\n      \"features\": {\n        \"buildkit\": false\n      }\n    }\n`\n```",
      "question_score": 65,
      "answer_score": 29,
      "created_at": "2021-01-07T15:36:30",
      "url": "https://stackoverflow.com/questions/65614378/getting-docker-build-to-show-ids-of-intermediate-containers"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 72152446,
      "title": "WARNING: The requested image&#39;s platform (linux/amd64) does not match the detected host platform (linux/arm64/v8)",
      "problem": "WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\ndocker: Error response from daemon: could not select device driver \"\" with capabilities: [[gpu]].\n\nI am facing this error on mac while trying to run this command `docker run --rm --gpus all -v static_volume:/home/app/staticfiles/ -v media_volume:/app/uploaded_videos/ --name=deepfakeapplication abhijitjadhav1998/deefake-detection-20framemodel`\nHow to solve this error?",
      "solution": "Try changing the command as\n```\n`docker run --rm --gpus all --platform linux/amd64 -v static_volume:/home/app/staticfiles/ -v media_volume:/app/uploaded_videos/ --name=deepfakeapplication abhijitjadhav1998/deefake-detection-20framemodel\n`\n```\nPlease ensure that you have compatible Nvidia Drivers available as this application uses Nvidia CUDA.",
      "question_score": 65,
      "answer_score": 8,
      "created_at": "2022-05-07T14:22:17",
      "url": "https://stackoverflow.com/questions/72152446/warning-the-requested-images-platform-linux-amd64-does-not-match-the-detecte"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 66317275,
      "title": "What is difference between Docker and Docker Desktop?",
      "problem": "I am using a mac and apparently installing `Docker Desktop` is mandatory to use Docker in macOS. Why Docker Desktop is required in Mac and Windows OS, and not in Linux? What purpose Docker Desktop serves? I have tried googling it but can't find appropriate answers.",
      "solution": "docker desktop for both mac and windows is using a Linux virtual machine behind the scenes for running regular docker daemon.\nReference\nThat state docker for windows is using WSL2 which is running a VM.\nHere there is a mention of backing up docker VM on docker for mac.\n\nDocker Desktop handles the setup and teardown of lightweight VMs on both Windows and macOS, using Hyper-V on Windows desktops and Hyperkit on macOS.\n\nFrom docker blog .",
      "question_score": 65,
      "answer_score": 34,
      "created_at": "2021-02-22T15:09:58",
      "url": "https://stackoverflow.com/questions/66317275/what-is-difference-between-docker-and-docker-desktop"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 69867166,
      "title": "Undefined volume with Docker Compose",
      "problem": "I wanted to translate this docker CLI command (from smallstep/step-ca) into a docker-compose.yml file to run with `docker compose` (version 2):\n`docker run -d -v step:/home/step \\\n    -p 9000:9000 \\\n    -e \"DOCKER_STEPCA_INIT_NAME=Smallstep\" \\\n    -e \"DOCKER_STEPCA_INIT_DNS_NAMES=localhost,$(hostname -f)\" \\\n    smallstep/step-ca\n`\nThis command successfully starts the container.\nHere is the compose file I \"composed\":\n`version: \"3.9\"\nservices:\n  ca:\n    image: smallstep/step-ca\n    volumes:\n      - \"step:/home/step\"\n    environment:\n      - DOCKER_STEPCA_INIT_NAME=Smallstep\n      - DOCKER_STEPCA_INIT_DNS_NAMES=localhost,ubuntu\n    ports:\n      - \"9000:9000\"\n`\nWhen I run `docker compose up` (again, using v2 here), I get this error:\n\nservice \"ca\" refers to undefined volume step: invalid compose project\n\nIs this the right way to go about this? I'm thinking I missed an extra step with volume creation in docker compose projects, but I am not sure what that would be, or if this is even a valid use case.",
      "solution": "The Compose file also has a top-level `volumes:` block and you need to declare volumes there.\n`version: '3.8'\nservices:\n  ca:\n    volumes:\n      - \"step:/home/step\"\n    et: cetera\nvolumes:   # add this section\n  step:    # does not need anything underneath this\n`\nThere are additional options possible, but you do not usually need to specify these unless you need to reuse a preexisting Docker named volume or you need non-standard Linux mount options (the linked documentation gives an example of an NFS-mount volume, for example).",
      "question_score": 63,
      "answer_score": 80,
      "created_at": "2021-11-06T20:18:32",
      "url": "https://stackoverflow.com/questions/69867166/undefined-volume-with-docker-compose"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 74656167,
      "title": "Unable to pull image from GitHub Container Registry (GHCR)",
      "problem": "For the past two days, I haven't been able to pull any images from the GitHub Container Registry (GHCR). This issue occurs with both public and private images. I have tried using both CMD and Windows Terminal, but without success. However, I am able to pull images normally from the Docker Hub.\nThe command I use is this:\n```\n`docker pull ghcr.io/someorg/someimage:sometag\n`\n```\nand the error I get is this:\n\nError response from daemon: Head \"https://ghcr.io/v2/someorg/someimage/manifests/sometag\": denied: denied\n\nIt only states \"denied\" without offering any explanation regarding the reason. After extensive searching, all I found was an issue on GitHub which stated that it was a platform issue that had been resolved.",
      "solution": "Since the images you are trying to pull are public and you get that error, it is safe to assume that you are logged in with an access token that no longer exists (because you probably deleted it or it expired).\nGiven this, you have to remove login credentials for `ghcr.io` using the following command:\n```\n`docker logout ghcr.io\n`\n```\nand either log in again with an existing access token or don't log in at all.\nThe `pull` command should then work.",
      "question_score": 60,
      "answer_score": 142,
      "created_at": "2022-12-02T13:57:32",
      "url": "https://stackoverflow.com/questions/74656167/unable-to-pull-image-from-github-container-registry-ghcr"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 69464001,
      "title": "docker-compose container name use dash (-) instead of underscore (_)",
      "problem": "I always used docker-compose on Ubuntu, in this environment containers are named with underscore:\n\n__\n\nBut now, I switched to Windows 10 (using Docker Desktop) and naming convention has changed:\n\n--\n\nI don't know if this is OS dependent but it's a problem. My scripts are failing because they rely on containers named with underscores.\nIs there a way to customize this and use underscore instead of dashes?",
      "solution": "This naming convention difference appears to be a difference between Docker Compose versions v1 (Python) and v2 (Go). The latest docker/compose repo that is packaged with Docker Desktop is the golang version in the docker/compose v2 branch. Looking at the source code here in this branch:\n`// Separator is used for naming components\nvar Separator = \"-\"\n`\nThe python branch source code is using the `_` naming convention for components, here for example:\n`    def rename_to_tmp_name(self):\n        \"\"\"Rename the container to a hopefully unique temporary container name\n        by prepending the short id.\n        \"\"\"\n        if not self.name.startswith(self.short_id):\n            self.client.rename(\n                self.id, '{}_{}'.format(self.short_id, self.name)\n            )\n`\nAs to solving this, you may want to uninstall the compose included with Docker Desktop and revert to a 1.28.x version. The compose readme  says you can use `pip install docker-compose` to install. The compose docs have a section about upgrading this and commands to migrate to v2: https://docs.docker.com/compose/install/#upgrading but your question suggests you want to stay with the `_` v1 naming convention.\nAs mentioned in comments, the following options retain the compose compatibility:\n\nuse `--compatibility` flag with `docker-compose` commands\nset `COMPOSE_COMPATIBILITY=true` environment variable\n\nOther doc links:\n\nBackward compatibility is discussed here: https://github.com/docker/compose#about-update-and-backward-compatibility\nDocs/Readmes suggest installing compose-switch for translating docker-compose commands\nIf you instead want to upgrade to v2 on Ubuntu and modify scripts: https://docs.docker.com/compose/cli-command/#install-on-linux",
      "question_score": 60,
      "answer_score": 80,
      "created_at": "2021-10-06T12:28:03",
      "url": "https://stackoverflow.com/questions/69464001/docker-compose-container-name-use-dash-instead-of-underscore"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 70322031,
      "title": "Does docker-compose support init container?",
      "problem": "`init container` is a great feature in Kubernetes and I wonder whether docker-compose supports it? it allows me to run some command before launch the main application.\nI come cross this PR https://github.com/docker/compose-cli/issues/1499 which mentions to support init container. But I can't find related doc in their reference.",
      "solution": "This was a discovery for me but yes, it is now possible to use init containers with `docker-compose` since version 1.29 as can be seen in the PR you linked in your question.\nMeanwhile, while I write those lines, it seems that this feature has not yet found its way to the documentation\nYou can define a dependency on an other container with a condition being basically \"when that other container has successfully finished its job\". This leaves the room to define containers running any kind of script and exit when they are done before an other dependent container is launched.\nTo illustrate, I crafted an example with a pretty common scenario: spin up a db container, make sure the db is up and initialize its data prior to launching the application container.\nNote: initializing the db (at least as far as the official mysql image is concerned) does not require an init container so this example is more an illustration than a rock solid typical workflow.\nThe complete example is available in a public github repo so I will only show the key points in this answer.\nLet's start with the compose file\n`---\nx-common-env: &cenv\n    MYSQL_ROOT_PASSWORD: totopipobingo\n\nservices:\n    db:\n        image: mysql:8.0\n        command: --default-authentication-plugin=mysql_native_password\n        environment:\n            \nYou can see I define 3 services:\n\nThe database which is the first to start\nThe init container which starts only once db is started. This one only runs a script (see below) that will exit once everything is initialized\nThe application container which will only start once the init container has successfuly done its job.\n\nThe `initproject.sh` script run by the `db-init` container is very basic for this demo and simply retries to connect to the db every 2 seconds until it succeeds or reaches a limit of 50 tries, then creates a db/table and insert some data:\n`#! /usr/bin/env bash\n\n# Test we can access the db container allowing for start\nfor i in {1..50}; do mysql -u root -p${MYSQL_ROOT_PASSWORD} -h db -e \"show databases\" && s=0 && break || s=$? && sleep 2; done\nif [ ! $s -eq 0 ]; then exit $s; fi\n\n# Init some stuff in db before leaving the floor to the application\nmysql -u root -p${MYSQL_ROOT_PASSWORD} -h db -e \"create database my_app\"\nmysql -u root -p${MYSQL_ROOT_PASSWORD} -h db -e \"create table my_app.test (id int unsigned not null auto_increment primary key, myval varchar(255) not null)\"\nmysql -u root -p${MYSQL_ROOT_PASSWORD} -h db -e \"insert into my_app.test (myval) values ('toto'), ('pipo'), ('bingo')\"\n`\nThe Dockerfile for the app container is trivial (adding a mysqli driver for php) and can be found in the example repo as well as the php script to test the init was succesful by calling `http://localhost:9999` in your browser.\nThe interesting part is to observe what's going on when launching the service with `docker-compose up -d`.\nThe only limit to what can be done with such a feature is probably your imagination ;) Thanks for making me discovering this.",
      "question_score": 60,
      "answer_score": 84,
      "created_at": "2021-12-12T09:25:39",
      "url": "https://stackoverflow.com/questions/70322031/does-docker-compose-support-init-container"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 68096476,
      "title": "Docker Failed to Initialize on Windows",
      "problem": "I have problem regarding pulling docker-dev in a docker image for making my development environment. When I tried to pull docker-dev, I got an error like \"docker manifest not found.\"\nCan anyone help me out with this error?\nI want to know about the docker failed to initialize error which I'm getting right now.\nThe error is like this:\n\nI tried so many things like re-installing the docker desktop or WSL updates, but none of those worked.\nThe error in the command is like this:",
      "solution": "Got the same issue and fixed it by deleting `%appdata%\\Docker` as mentioned by Github User \"tocklime\"\n(Original Source : https://github.com/docker/for-win/issues/3088)",
      "question_score": 58,
      "answer_score": 172,
      "created_at": "2021-06-23T10:44:00",
      "url": "https://stackoverflow.com/questions/68096476/docker-failed-to-initialize-on-windows"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 66839443,
      "title": "How to enable/disable buildkit in docker?",
      "problem": "I got this command from the documentation, but i really have no idea how can I use it or where should I start to move, I'm new to docker, and concepts are still hard to me to digest:\n```\n`$ DOCKER_BUILDKIT=1 docker build .\n`\n```\nHow can I use this command to enable/disable buildkit in docker engine??\nI want to disable it, because as i knew it is enabled by default and i suspect it as i can't build anything by docker since i get always this error\n```\n`failed to solve with frontend dockerfile.v0: failed to read dockerfile: open /var/lib/docker/tmp/buildkit-mount847288160/Dockerfile: no such file or directory\n`\n```",
      "solution": "You must adjust the Docker Engine's daemon settings, stored in the `daemon.json`, and restart the engine. As @Zeitounator suggests, you should be able to temporarily disable the buildkit with `DOCKER_BUILDKIT=0 docker build .`. Docker CLI will parse that environment variable and should honor it as that checking is done here in the docker/cli source code.\nTo adjust the Docker daemon's buildkit settings, you can follow the instructions below.\nFrom these docs. Partially on the command line, you can do that this way in Powershell:\n\nOpen the file, on the command line the easiest way to do this is:\n\n```\n`notepad \"$env:USERPROFILE\\.docker\\daemon.json\"\n`\n```\n\nChange the value of `\"buildkit\"` to `false` so it looks like this:\n\n```\n`{\n  \"registry-mirrors\": [],\n  \"insecure-registries\": [],\n  \"debug\": true,\n  \"experimental\": false,\n  \"features\": {\n    \"buildkit\": false\n  }\n}\n`\n```\n\nRestart the Docker service:\n\n```\n`Restart-Service *docker*\n`\n```\n\nAlternatively, on Docker Desktop for Windows app:\nOpen the Dashboard > Settings:\nSelect Docker Engine and edit the json `\"features\"` field to read `false` if it's not already:",
      "question_score": 58,
      "answer_score": 55,
      "created_at": "2021-03-28T09:59:02",
      "url": "https://stackoverflow.com/questions/66839443/how-to-enable-disable-buildkit-in-docker"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 74173489,
      "title": "Docker socket is not found while using Intellij IDEA and Docker desktop on MacOS",
      "problem": "I downloaded Docker using Docker Desktop for Apple M1 chips. I can run containers, the integration with VsCode works okay but I can't integrate it with Intellij IDEA Ultimate. It keeps giving this error.\n\nBut I can run my containers and create images from the terminal, I can also see the containers and images in Docker Desktop too. What could be the reason behind this? I also tried to check whether var/run/docker.sock is existing and it really isn't, there is no such file as that.\nI also tried the same steps on my second computer and the exact same thing happened.\nSteps to reproduce:\n\nDownload Intellij IDEA Ultimate, open a repo that uses docker\nDownload Docker Desktop for Mac M1\nTry to add Docker service to Intellij\n\nI didn't do anything else because I think Docker Desktop is enough to configure everything on Mac. I am trying to run an FT on intellij and I get the error\n```\n`[main] ERROR o.t.d.DockerClientProviderStrategy - Could not find a valid Docker environment. Please check configuration. Attempted configurations were:\n[main] ERROR o.t.d.DockerClientProviderStrategy -     UnixSocketClientProviderStrategy: failed with exception InvalidConfigurationException (Could not find unix domain socket). Root cause NoSuchFileException (/var/run/docker.sock)\n[main] ERROR o.t.d.DockerClientProviderStrategy -     DockerMachineClientProviderStrategy: failed with exception ShellCommandException (Exception when executing docker-machine status ). Root cause InvalidExitValueException (Unexpected exit value: 1, allowed exit values: [0], executed command [docker-machine, status, ], output was 122 bytes:\nDocker machine \"\" does not exist. Use \"docker-machine ls\" to list machines. Use \"docker-machine create\" to add a new one.)\n[main] ERROR o.t.d.DockerClientProviderStrategy - As no valid configuration was found, execution cannot continue\n`\n```\nI've been trying everything for the last 2 days but I can't seem to find a solution.",
      "solution": "EDITED 2022-10-31\nRelease notes for Docker Desktop (4.13.1) (and following versions), states that, there is no need to create the symlink anymore, citing notes:\n\nAdded back the `/var/run/docker.sock` symlink on Mac by default, to increase compatibility with tooling like `tilt` and `docker-py`. Fixes docker/for-mac#6529.\n\nThe official fix now is to UPGRADE your Docker Desktop installation.\nFor the Docker Desktop (4.13.0) version:\nBy default Docker will not create the /var/run/docker.sock symlink on the host and use the docker-desktop CLI context instead. (see: https://docs.docker.com/desktop/release-notes/)\nThat will prevent IntelliJ from finding Docker using the default context.\nYou can see the current contexts in your machine by running `docker context ls`, which should produce an output like:\n```\n`NAME                TYPE    DESCRIPTION                               DOCKER ENDPOINT                                KUBERNETES ENDPOINT                                 ORCHESTRATOR\ndefault             moby    Current DOCKER_HOST based configuration   unix:///var/run/docker.sock                    https://kubernetes.docker.internal:6443 (default)   swarm\ndesktop-linux *     moby                                              unix:///Users//.docker/run/docker.sock\n\n`\n```\nAs a workaround that will allow IntelliJ to connect to Docker you can use the TCP Socket checkbox and put in the Engine API URL the value that appears under `DOCKER ENDPOINT` in the active context.\nThe case for this example will be: `unix:///Users//.docker/run/docker.sock`\nThen IntelliJ will be able to connect to Docker Desktop.\nEDITED 2023-07-17\nIf the option in \"Settings > Advanced > Allow the default Docker socket to be used\" is already enabled and the socket is not available try disabling it and re-enabling it.\n\n**Hacky option**\nAnother way to make IntelliJ (and other components that rely on the default config) to find Docker will be to manually create a symlink to the new `DOCKER ENDPOINT` by running:\n```\n`sudo ln -svf /Users//.docker/run/docker.sock /var/run/docker.sock\n`\n```\nIn that way all the components looking for Docker under `/var/run/docker.sock` will find it.",
      "question_score": 54,
      "answer_score": 88,
      "created_at": "2022-10-23T19:55:53",
      "url": "https://stackoverflow.com/questions/74173489/docker-socket-is-not-found-while-using-intellij-idea-and-docker-desktop-on-macos"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 69852628,
      "title": "Docker: COPY failed: file not found in build context (Dockerfile)",
      "problem": "I'd like to instruct Docker to `COPY` my certificates from the local `/etc/` folder on my Ubuntu machine.\nI get the error:\n\nCOPY failed: file not found in build context or excluded by\n.dockerignore: stat etc/.auth_keys/fullchain.pem: file does not exist\n\nI have not excluded in `.dockerignore`\nHow can I do it?\nDockerfile:\n```\n`FROM nginx:1.21.3-alpine\n\nRUN rm /etc/nginx/conf.d/default.conf\nRUN mkdir /etc/nginx/ssl\nCOPY nginx.conf /etc/nginx/conf.d\nCOPY ./etc/.auth_keys/fullchain.pem /etc/nginx/ssl/\nCOPY ./etc/.auth_keys/privkey.pem /etc/nginx/ssl/\n\nWORKDIR /usr/src/app\n`\n```\nI have also tried without the `dot` --> same error\n```\n`COPY /etc/.auth_keys/fullchain.pem /etc/nginx/ssl/\nCOPY /etc/.auth_keys/privkey.pem /etc/nginx/ssl/\n`\n```\nBy placing the folder `.auth_keys` next to the Dockerfile --> works, but not desireable\n```\n`COPY /.auth_keys/fullchain.pem /etc/nginx/ssl/\nCOPY /.auth_keys/privkey.pem /etc/nginx/ssl/\n`\n```",
      "solution": "The docker context is the directory the Dockerfile is located in. If you want to build an image that is one of the restrictions you have to face.\nIn this documentation you can see how contexts can be switched, but to keep it simple just consider the same directory to be the context. Note; this also doesn't work with symbolic links.\nSo your observation was correct and you need to place the files you need to copy in the same directory.\nAlternatively, if you don't need to copy them but still have them available at runtime you could opt for a mount. I can imagine this not working in your case because you likely need the files at startup of the container.",
      "question_score": 53,
      "answer_score": 46,
      "created_at": "2021-11-05T12:35:20",
      "url": "https://stackoverflow.com/questions/69852628/docker-copy-failed-file-not-found-in-build-context-dockerfile"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 72414481,
      "title": "Error in anyjson setup command: use_2to3 is invalid",
      "problem": "```\n`#25 3.990   \u00d7 python setup.py egg_info did not run successfully.\n#25 3.990   \u2502 exit code: 1\n#25 3.990   \u2570\u2500> [1 lines of output]\n#25 3.990       error in anyjson setup command: use_2to3 is invalid.\n#25 3.990       [end of output]\n`\n```\nThis is a common error which the most common solution to is to downgrade setuptools to below version 58. This was not working for me. I tried installing python3-anyjson but this didn't work either. I'm at a complete loss.. any advice or help is much appreciated.\nIf it matters: this application is legacy spaghetti and I am trying to polish it up for a migration. There's no documentation of any kind.\nThe requirements.txt is as follows:\n```\n`cachetools>=2.0.0,=1.17.2\ngoogle-api-python-client==1.12.1\ngunicorn==20.1.0\nhttplib2.system-ca-certs-locater\nhttplib2==0.9.2\noauth2client==2.0.1\npyasn1-modules==0.2.1\nredis\nrequests==2.18.0\nwerkzeug==2.1.2\nsix==1.13.0\npyasn1==0.4.1\nJinja2==3.1.1\nitsdangerous==2.0.1\n\nFlask-Celery-Helper\nFlask-JWT==0.2.0\nFlask-Limiter\nFlask-Mail\nFlask-Migrate\nFlask-Restless==0.16.0\nFlask-SQLAlchemy\nFlask-Script\nFlask-Testing\nFlask==2.0.3\nPillow<=6.2.2\nShapely\nbeautifulsoup4\nboto\ncelery==3.1.23\ngeopy\ngevent==21.12.0\nnumpy<1.17\noauth2client==2.0.1\npasslib\npsycopg2\npyproj<2\npython-dateutil==2.4.1\nscipy\n`\n```",
      "solution": "Downgrading setuptools worked for me\n```\n`pip install \"setuptoolsAnd then\n```\n`pip install django-celery\n`\n```",
      "question_score": 51,
      "answer_score": 110,
      "created_at": "2022-05-28T11:29:23",
      "url": "https://stackoverflow.com/questions/72414481/error-in-anyjson-setup-command-use-2to3-is-invalid"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 70601439,
      "title": "&quot;organization has enabled or enforced SAML SSO. To access remote: this repository&quot; Error on Docker Build",
      "problem": "```\n`FROM golang:1.17-alpine as builder\nENV GOPRIVATE=github.com/XXXXX/\nARG GITHUB_TOKEN=$GITHUB_TOKEN\nRUN apk update && apk add git gcc g++ libc-dev librdkafka-dev pkgconf && mkdir /app && git config --global url.\"https://someusername:$GITHUB_TOKEN@github.com\".insteadOf \"https://github.com\"\nWORKDIR /app\nCOPY go.mod go.sum ./\nRUN go mod download\nCOPY ./ /app\nRUN go build -tags dynamic\n\nFROM alpine:3.1\nRUN addgroup -S app && \\\n    apk add --no-cache librdkafka-dev\nRUN mkdir /logs && mkdir /app\nWORKDIR /app\nUSER app\nEXPOSE 8000 8001 8002\nCMD [\"./main\"]\n`\n```\nfatal: could not read Username for 'https://github.com': terminal prompts disabled Confirm the import path was entered correctly.\nIf this is a private repository, see https://golang.org/doc/faq#git_https for additional information.\nAny idea how can I fix it inside docker",
      "solution": "When you're in an organization that uses SAML SSO, every personal access token that's used to access that organization's resources has to be specifically enabled for that organization.\nYou can go into the Personal Access Token settings and choose \"Configure SSO\".  From there, you can enable the token for the specific organization that you're using.  After that point, using it as normal should work.",
      "question_score": 51,
      "answer_score": 80,
      "created_at": "2022-01-06T02:17:49",
      "url": "https://stackoverflow.com/questions/70601439/organization-has-enabled-or-enforced-saml-sso-to-access-remote-this-repositor"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 68155641,
      "title": "Should I run things inside a docker container as non root for safety?",
      "problem": "I already run my `docker build` and `docker run` without sudo. However, when I launch a process inside a docker container, it appears as a root process on `top` on the host (not inside the container).\nWhile it cannot access the host filesystem because of namespacing and cgroups from docker, is it still more dangerous than running as a simple user?\nIf so, how is the right way of running things inside docker as non root?\nShould I just do `USER nonroot` at the end of the Dockerfile?\nUPDATE:\nroot it also needed for building some things. Should I put `USER` on the very top of the Dockerfile and then install `sudo` together with other dependencies, and then use `sudo` only when needed in the build?\nCan someone give a simple Dockerfile example with USER in the beggining and installing and using `sudo`?",
      "solution": "Running the container as `root` brings a lot of risks. Although being `root` inside the container is not the same as `root` on the host machine (some more details here) and you're able to deny a lot of capabilities during container startup, it is still the recommended approach to avoid being `root`.\nUsually it is a good idea to use the `USER` directive in your Dockerfile after you install some general packages/libraries. In other words - after the operations that require `root` privileges. Installing `sudo` in a production service image is a mistake, unless you have a really good reason for it. In most cases - you don't need it and it is more of a security issue. If you need permissions to access some particular files or directories in the image, then make sure that the user you specified in the Dockerfile can really access them (setting proper `uid`, `gid` and other options, depending on where you deploy your container). Usually you don't need to create the user beforehand, but if you need something custom, you can always do that.\nHere's an example Dockerfile for a Java application that runs under user `my-service`:\n```\n`FROM alpine:latest\nRUN apk add openjdk8-jre\nCOPY  ./some.jar /app/\nENV SERVICE_NAME=\"my-service\"\n\nRUN addgroup --gid 1001 -S $SERVICE_NAME && \\\n    adduser -G $SERVICE_NAME --shell /bin/false --disabled-password -H --uid 1001 $SERVICE_NAME && \\\n    mkdir -p /var/log/$SERVICE_NAME && \\\n    chown $SERVICE_NAME:$SERVICE_NAME /var/log/$SERVICE_NAME\n\nEXPOSE 8080\nUSER $SERVICE_NAME\nCMD [\"java\", \"-jar\", \"/app/some.jar\"]\n`\n```\nAs you can see, I create the user beforehand and set its `gid`, disable its shell and password login, as it is going to be a 'service' user. The user also becomes owner of `/var/log/$SERVICE_NAME`, assuming it will write to some files there. Now we have a lot smaller attack surface.",
      "question_score": 51,
      "answer_score": 43,
      "created_at": "2021-06-28T00:05:34",
      "url": "https://stackoverflow.com/questions/68155641/should-i-run-things-inside-a-docker-container-as-non-root-for-safety"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 66118337,
      "title": "How to get rid of cryptography build error?",
      "problem": "I am trying to build a dockerfile but the problem is when it trying to build specifically cryptography is not building.\nMY Dockerfile\n```\n`FROM python:3.7-alpine\n\nENV PYTHONUNBUFFERED 1\n\nRUN apk update \\\n  # psycopg2 dependencies\n  && apk add --virtual build-deps gcc python3-dev musl-dev\\\n  && apk add postgresql-dev \\\n  && apk add build-base \\\n  # Pillow dependencies\n  && apk add jpeg-dev zlib-dev freetype-dev lcms2-dev openjpeg-dev tiff-dev tk-dev tcl-dev \\\n  # CFFI dependencies\n  && apk add libffi-dev py-cffi \\\n  # Translations dependencies\n  && apk add gettext \\\n  # https://docs.djangoproject.com/en/dev/ref/django-admin/#dbshell\n  && apk add postgresql-client \\\n  # cairo\n  && apk add cairo cairo-dev pango-dev gdk-pixbuf-dev poppler-utils\n\n# fonts for weasyprint\nRUN mkdir ~/.fonts\nCOPY ./fonts/* /root/.fonts/\n\n# secret key (should be in docker-secrets, or we need to run minikube locally\nRUN mkdir /etc/secrets\nCOPY secret.readme proxy_rsa_key* /etc/secrets/\n\n# Requirements are installed here to ensure they will be cached.\nCOPY ./requirements /requirements\nRUN pip install -r /requirements/local.txt\n\nCOPY ./compose/local/django/entrypoint /entrypoint\nRUN sed -i 's/\\r//' /entrypoint\nRUN chmod +x /entrypoint\n\nCOPY ./compose/local/django/start /start\nRUN sed -i 's/\\r//' /start\nRUN chmod +x /start\n\nCOPY ./compose/local/django/celery/worker/start /start-celeryworker\nRUN sed -i 's/\\r//' /start-celeryworker\nRUN chmod +x /start-celeryworker\n\nCOPY ./compose/local/django/celery/beat/start /start-celerybeat\nRUN sed -i 's/\\r//' /start-celerybeat\nRUN chmod +x /start-celerybeat\n\nCOPY ./compose/local/django/celery/flower/start /start-flower\nRUN sed -i 's/\\r//' /start-flower\nRUN chmod +x /start-flower\n\nWORKDIR /app\n\nENTRYPOINT [\"/entrypoint\"]\n`\n```\nwhen I try to build my dockerfile it shows:\n```\n`Building wheel for cryptography (PEP 517): finished with status 'error'\n  ERROR: Command errored out with exit status 1:\n  \n  error: Can not find Rust compiler\n  ----------------------------------------\n  ERROR: Failed building wheel for cryptography\n\n`\n```\nI tried to solve but i couldn't. I am newbie in docker.Please help how to get rid of this problem.",
      "solution": "Since the error is...\n```\n`error: Can not find Rust compiler\n`\n```\n...the solution is to install the rust compiler. You'll also need\n`cargo`, the Rust package manager, and it looks like your `Dockerfile`\nis missing `openssl-dev`.\nThe following builds successfully for me:\n```\n`FROM python:3.7-alpine\n\nENV PYTHONUNBUFFERED 1\n\nRUN apk add --update \\\n  build-base \\\n  cairo \\\n  cairo-dev \\\n  cargo \\\n  freetype-dev \\\n  gcc \\\n  gdk-pixbuf-dev \\\n  gettext \\\n  jpeg-dev \\\n  lcms2-dev \\\n  libffi-dev \\\n  musl-dev \\\n  openjpeg-dev \\\n  openssl-dev \\\n  pango-dev \\\n  poppler-utils \\\n  postgresql-client \\\n  postgresql-dev \\\n  py-cffi \\\n  python3-dev \\\n  rust \\\n  tcl-dev \\\n  tiff-dev \\\n  tk-dev \\\n  zlib-dev\n\nRUN pip install cryptography\n`\n```\nNote that the above `apk add ...` command line is largely the same as\nwhat you've got; I've just simplified the multiple `apk add ...`\nstatements into a single `apk add` execution.",
      "question_score": 50,
      "answer_score": 38,
      "created_at": "2021-02-09T12:43:29",
      "url": "https://stackoverflow.com/questions/66118337/how-to-get-rid-of-cryptography-build-error"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 70608086,
      "title": "I am getting error while converting my next js project to docker",
      "problem": "I'm trying to convert my Next js project to Docker. The Dockerfile I got from the next js github page worked fine for me and I got a build successfully.\n```\n`# Install dependencies only when needed\nFROM node:16-alpine AS deps\n# Check https://github.com/nodejs/docker-node/tree/b4117f9333da4138b03a546ec926ef50a31506c3#nodealpine to understand why libc6-compat might be needed.\nRUN apk add --no-cache libc6-compat\nWORKDIR /app\nCOPY package.json yarn.lock ./\nRUN yarn install --frozen-lockfile\n\n# Rebuild the source code only when needed\nFROM node:16-alpine AS builder\nWORKDIR /app\nCOPY --from=deps /app/node_modules ./node_modules\nCOPY . .\nRUN yarn build\n\n# Production image, copy all the files and run next\nFROM node:16-alpine AS runner\nWORKDIR /app\n\nENV NODE_ENV production\n\nRUN addgroup -g 1001 -S nodejs\nRUN adduser -S nextjs -u 1001\n\n# You only need to copy next.config.js if you are NOT using the default configuration\n# COPY --from=builder /app/next.config.js ./\nCOPY --from=builder /app/public ./public\nCOPY --from=builder /app/package.json ./package.json\n\n# Automatically leverage output traces to reduce image size\n# https://nextjs.org/docs/advanced-features/output-file-tracing\n# COPY --from=builder --chown=nextjs:nodejs /app/.next/standalone ./\nCOPY --from=builder --chown=nextjs:nodejs /app/.next/static ./.next/static\n\nUSER nextjs\n\nEXPOSE 3000\n\nENV PORT 3000\n\n# Next.js collects completely anonymous telemetry data about general usage.\n# Learn more here: https://nextjs.org/telemetry\n# Uncomment the following line in case you want to disable telemetry.\n# ENV NEXT_TELEMETRY_DISABLED 1\n\nCMD [\"node\", \"server.js\"]\n`\n```\nI wrote this later.\n```\n`docker run -p 3000:3000 imagename\n`\n```\nThen I faced such error and I can't solve it.\n```\n`node:internal/modules/cjs/loader:936\n  throw err;\n  ^\n\nError: Cannot find module '/app/server.js'\n    at Function.Module._resolveFilename (node:internal/modules/cjs/loader:933:15)\n    at Function.Module._load (node:internal/modules/cjs/loader:778:27)\n    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:81:12)\n    at node:internal/main/run_main_module:17:47 {\n  code: 'MODULE_NOT_FOUND',\n  requireStack: []\n}\n`\n```\nI searched a lot on the internet but couldn't find much. What do you think I should do?",
      "solution": "Ensure that you're copying all files from the example. In this case you need to ensure you have added or customized `next.config.js` with this:\n`module.exports = {\n  output: 'standalone'\n}\n`\nYou'll notice the file is also defined in the examples: https://github.com/vercel/next.js/blob/canary/examples/with-docker/next.config.js",
      "question_score": 48,
      "answer_score": 100,
      "created_at": "2022-01-06T14:47:14",
      "url": "https://stackoverflow.com/questions/70608086/i-am-getting-error-while-converting-my-next-js-project-to-docker"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 68996420,
      "title": "How to set timezone inside alpine base docker image?",
      "problem": "I want to set time zone in my docker container. I follow this article but can't find working solution for alpine base image.\nCould you please guide me?\nhttps://dev.to/0xbf/set-timezone-in-your-docker-image-d22",
      "solution": "You need to install the `tzdata` package and then set the enviroment variable `TZ` to a timezone. (List with all the timezones)\n```\n`FROM alpine:latest\nRUN apk add --no-cache tzdata\nENV TZ=Europe/Copenhagen\n`\n```\nOutput\n```\n`$ docker run --rm alpine date\nTue Aug 31 09:52:08 UTC 2021\n\n$ docker run --rm myimage date\nTue Aug 31 11:52:13 CEST 2021\n`\n```",
      "question_score": 48,
      "answer_score": 137,
      "created_at": "2021-08-31T11:41:23",
      "url": "https://stackoverflow.com/questions/68996420/how-to-set-timezone-inside-alpine-base-docker-image"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker",
      "question_id": 66091744,
      "title": "Docker Failed to Start",
      "problem": "I have installed the Docker and then I have started the Docker. Docker says that \"Docker is starting.\" Then,Docker says \"Docker Failed to Start\". Please, Could you help me?\nDocker Error Detail:\n```\n`System.InvalidOperationException:\nFailed to deploy distro docker-desktop to C:\\Users\\---\\AppData\\Local\\Docker\\wsl\\distro: exit code: -1\n stdout: The operation could not be started because a required feature is not installed.\n\n stderr: \n   at Docker.ApiServices.WSL2.WslShortLivedCommandResult.LogAndThrowIfUnexpectedExitCode(String prefix, ILogger log, Int32 expectedExitCode) in C:\\workspaces\\PR-15138\\src\\github.com\\docker\\pinata\\win\\src\\Docker.ApiServices\\WSL2\\WslCommand.cs:line 146\n   at Docker.Engines.WSL2.WSL2Provisioning.d__17.MoveNext() in C:\\workspaces\\PR-15138\\src\\github.com\\docker\\pinata\\win\\src\\Docker.Desktop\\Engines\\WSL2\\WSL2Provisioning.cs:line 169\n--- End of stack trace from previous location where exception was thrown ---\n   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()\n   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)\n   at Docker.Engines.WSL2.WSL2Provisioning.d__8.MoveNext() in C:\\workspaces\\PR-15138\\src\\github.com\\docker\\pinata\\win\\src\\Docker.Desktop\\Engines\\WSL2\\WSL2Provisioning.cs:line 78\n--- End of stack trace from previous location where exception was thrown ---\n   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()\n   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)\n   at Docker.Engines.WSL2.LinuxWSL2Engine.d__25.MoveNext() in C:\\workspaces\\PR-15138\\src\\github.com\\docker\\pinata\\win\\src\\Docker.Desktop\\Engines\\WSL2\\LinuxWSL2Engine.cs:line 99\n--- End of stack trace from previous location where exception was thrown ---\n   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()\n   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)\n   at Docker.ApiServices.StateMachines.TaskExtensions.d__0.MoveNext() in C:\\workspaces\\PR-15138\\src\\github.com\\docker\\pinata\\win\\src\\Docker.ApiServices\\StateMachines\\TaskExtensions.cs:line 29\n--- End of stack trace from previous location where exception was thrown ---\n   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()\n   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)\n   at Docker.ApiServices.StateMachines.StartTransition.d__5.MoveNext() in C:\\workspaces\\PR-15138\\src\\github.com\\docker\\pinata\\win\\src\\Docker.ApiServices\\StateMachines\\StartTransition.cs:line 67\n--- End of stack trace from previous location where exception was thrown ---\n   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()\n   at Docker.ApiServices.StateMachines.StartTransition.d__5.MoveNext() in C:\\workspaces\\PR-15138\\src\\github.com\\docker\\pinata\\win\\src\\Docker.ApiServices\\StateMachines\\StartTransition.cs:line 92\n--- End of stack trace from previous location where exception was thrown ---\n   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()\n   at Docker.Engines.Engines.d__30.MoveNext() in C:\\workspaces\\PR-15138\\src\\github.com\\docker\\pinata\\win\\src\\Docker.Desktop\\Engines\\Engines.cs:line 358\n--- End of stack trace from previous location where exception was thrown ---\n   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()\n   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)\n   at Docker.Engines.Engines.d__26.MoveNext() in C:\\workspaces\\PR-15138\\src\\github.com\\docker\\pinata\\win\\src\\Docker.Desktop\\Engines\\Engines.cs:line 250\n`\n```",
      "solution": "My Solution:\nI have checked the Prerequisites for Docker. I have applied the second step in prerequisites. (Windows Subsystem for Linux Installation with Manual Installation Steps) It has fixed the error for me.\n\nIf you get an error like this (during this installation process):\n`WslRegisterDistribution failed with error: 0x80370114 Error: 0x80370114 The operation could not be started because a required feature is not installed.` (I encountered this error.) Please, apply this.\n```\n` These steps are quoted from the \"this\" named hyperlink above. \n1. Open \"Windows Security\"\n2. Open \"App & Browser control\"\n3. Click \"Exploit protection settings\" at the bottom\n4. Switch to \"Program settings\" tab\n5. Locate \"C:\\WINDOWS\\System32\\vmcompute.exe\" in the list and expand it\n6. Click \"Edit\"\n7. Scroll down to \"Code flow guard (CFG)\" and uncheck \"Override system settings\"\n8. Start vmcompute from powershell \"net start vmcompute\"\n9. Then go back and command wsl --set-default-version 2\n`\n```\n\nI have fixed my problems like that. Good luck :)",
      "question_score": 47,
      "answer_score": 30,
      "created_at": "2021-02-07T19:42:50",
      "url": "https://stackoverflow.com/questions/66091744/docker-failed-to-start"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 68818400,
      "title": "Docker compose with .NET Core, SQL Server, Elasticsearch, and cerebro services",
      "problem": "I'm trying to run a number of services using a docker-compose file.\nFirst of all let's say that Docker, version 20.10.3, is running on a Red Hat Enterprise Linux release 8.3.\nThis is the docker-compose file:\n```\n`version: \"3.8\"\nservices:\n  projmssql:\n    image: \"mcr.microsoft.com/mssql/server:2019-latest\"\n    container_name: proj-core-sqlserver\n    environment:\n        SA_PASSWORD: \"mypassword\"\n        ACCEPT_EULA: \"Y\"\n    ports:\n      - \"1401:1433\"\n    volumes:\n      - type: bind\n        source: ./data-mssql\n        target: /var/opt/mssql/data\n      - type : bind\n        source: ./log-mssql\n        target: /var/opt/mssql/log\n    restart: always\n  projelastic:\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.10.1\n    container_name: proj-core-elastic\n    environment:\n      - node.name=es01\n      - cluster.name=proj-docker-cluster\n      - discovery.type=single-node\n      - bootstrap.memory_lock=false\n      - \"ES_JAVA_OPTS=-Xms1g -Xmx1g\"\n      - path.repo=/usr/share/elasticsearch/backup\n      - path.logs=/usr/share/elasticsearch/logs\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    volumes:\n      - type: bind\n        source: ./data-es01\n        target: /usr/share/elasticsearch/data\n      - type : bind\n        source: ./_backup-es01\n        target: /usr/share/elasticsearch/backup\n      - type : bind\n        source: ./logs-es01\n        target: /usr/share/elasticsearch/logs\n    ports:\n      - 9220:9200\n      - 9320:9300\n  projcerebro:\n    image: lmenezes/cerebro\n    container_name: proj-core-cerebro\n    ports:\n      - \"9020:9000\"\n    command:\n      - -Dhosts.0.host=http://projelastic:9200\n  projapi:\n    image: proj/projcoreapp \n    depends_on:\n       - projmssql\n    container_name: proj-core-api\n    ports:\n      - \"8080:80\"\n    restart: always\n    #Specify Environment Variables for the Api Service\n    environment: \n      - ASPNETCORE_ENVIRONMENT=Docker\n      \n`\n```\nThe projApi service comes from my local image (proj/projcoreapp) built with the following Dockerfile:\n```\n`FROM mcr.microsoft.com/dotnet/sdk:5.0 AS build-env\nWORKDIR /app\n\nCOPY . ./\nRUN dotnet publish proj.api.net -c Release -o out\n\nFROM mcr.microsoft.com/dotnet/aspnet:5.0\nWORKDIR /app\nCOPY --from=build-env /app/out .\n\nENTRYPOINT [\"dotnet\", \"Proj.Api.dll\"]\n`\n```\nIn my .net core app I have a dedicated appsettings.Docker.json file\n```\n`{\n  \"Logging\": {\n    \"LogLevel\": {\n      \"Default\": \"Information\",\n      \"Microsoft\": \"Warning\",\n      \"Microsoft.Hosting.Lifetime\": \"Information\"\n    }\n  },\n  \"AllowedHosts\": \"*\",\n  \"Elasticsearch\": {\n    \"Server\": \"http://projelastic:9200\",\n    \"DebugMode\": true,\n    \"UpdateMapping\": false\n  },\n  \"ConnectionStrings\": {\n    \"DatabaseConnection\": \"Server=projmssql;Database=PROJ-NET;Persist Security Info=False;User ID=sa;Password=mypassword;MultipleActiveResultSets=True;TrustServerCertificate=False;Connection Timeout=30;\"\n  }\n}\n`\n```\nThe `.csproj` of the API .net core project\n`\n\n  \n    net5.0\n  \n\n  \n    \n  \n\n  \n    \n    \n    \n    \n    \n    \n  \n\n  \n    \n  \n\n`\nThe `.csproj` of the Domain library\n`\n\n  \n    net5.0\n  \n\n  \n    \n    \n    \n  \n\n  \n    \n    \n    \n    \n    \n  \n\n  \n    \n    \n  \n\n  \n    \n  \n\n`\nThe `.csproj` of the Filter library\n`\n\n  \n    net5.0\n  \n\n  \n    \n    \n    \n    \n  \n\n  \n    \n    \n    \n  \n\n`\nIn my .net core startup I have the following configuration\n```\n`public void ConfigureServices(IServiceCollection services)\n        {\n            services.AddDbContext(options =>\n                options.UseSqlServer(Configuration.GetConnectionString(\"DatabaseConnection\"))\n            );\n            services.AddScoped();\n            services.AddSingleton(s =>\n                new ProjEntitiesService\n                (\n                    Configuration.GetValue(\"Elasticsearch:Server\"),\n                    Configuration.GetValue(\"Elasticsearch:DebugMode\"),\n                    Configuration.GetValue(\"Elasticsearch:UpdateMapping\")\n                ));\n\n            services.AddControllers();\n            services.AddSwaggerGen(c =>\n            {\n`\n```\nNow let's talk about the issue:\nonce I run the docker-compose file, all the services run correctly and from the host I can interact with all of them. The API service is able to connect internally to the database but once I run some APIs to interact with the elasticsearch service I get the following error:\n```\n`Elasticsearch.Net.UnexpectedElasticsearchClientException: The information requested is unavailable on the current platform.\n ---> System.PlatformNotSupportedException: The information requested is unavailable on the current platform.\n   at System.Net.NetworkInformation.StringParsingHelpers.ParseActiveTcpConnectionsFromFiles(String tcp4ConnectionsFile, String tcp6ConnectionsFile)\n   at System.Net.NetworkInformation.LinuxIPGlobalProperties.GetActiveTcpConnections()\n   at Elasticsearch.Net.Diagnostics.TcpStats.GetStates()\n   at Elasticsearch.Net.HttpConnection.Request[TResponse](RequestData requestData)\n   at Elasticsearch.Net.RequestPipeline.CallElasticsearch[TResponse](RequestData requestData)\n   at Elasticsearch.Net.Transport`1.Request[TResponse](HttpMethod method, String path, PostData data, IRequestParameters requestParameters)\n   --- End of inner exception stack trace ---\n`\n```\nAs I said all the services are working properly and even the Cerebro service is able to connect internally to the elasticsearch service, so it looks like an issue regarding the .net core service only.\nI couldn't find any topic regarding the above issue so I hope someone here can help me to figure it out.\nPS: the .NET Core API, deployed locally on a Windows machine (Kestrel web server), runs properly connecting to the database and elasticsearch with both of them running on Docker.\nUpdate:\nI forgot to mention that the machine is running behind a corporate proxy. Keeping that in mind, I wanted to check if my API service was able to reach the Elasticsearch service endpoint: `http://projelastic:9200`.\nTo do that I executed an interactive bash shell on my API service container:\n`docker exec -it proj-core-api /bin/bash`. Here I noticed that my containers do not have access to internet\n```\n`root@8e7c81f98455:/app# apt-get update\nErr:1 http://security.debian.org/debian-security buster/updates InRelease\n  Could not connect to security.debian.org:80 (111: Connection refused) \n`\n```\nSo I added to the docker-compose file the variables for the proxy server including the no_proxy one for the Elasticsearch service domain.\n```\n`projapi:\n    image: proj/projcoreapp \n    depends_on:\n       - projmssql\n    container_name: proj-core-api\n    ports:\n      - \"8080:80\"\n    restart: always\n    #Specify Environment Variables for the Api Service\n    environment: \n      - ASPNETCORE_ENVIRONMENT=Docker\n      - http_proxy=http://user:pwd@address      \n      - https_proxy=http://user:pwd@address\n      - no_proxy=projelastic\n\n`\n```\nWith the above change I was able to install curl in my API container and perform my initial test:\n```\n`root@0573a44a6836:/app# curl -X GET http://projelastic:9200\n{\n  \"name\" : \"es01\",\n  \"cluster_name\" : \"proj-docker-cluster\",\n  \"cluster_uuid\" : \"j7mdpxkrSRKxpHnktvnZIw\",\n  \"version\" : {\n    \"number\" : \"7.10.1\",\n    \"build_flavor\" : \"default\",\n    \"build_type\" : \"docker\",\n    \"build_hash\" : \"1c34507e66d7db1211f66f3513706fdf548736aa\",\n    \"build_date\" : \"2020-12-05T01:00:33.671820Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"8.7.0\",\n    \"minimum_wire_compatibility_version\" : \"6.8.0\",\n    \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n\n`\n```\nAnyway adding the proxy variables didn't fix the initial error, so as a final test I brought my source code on a `Windows 10 machine` where it's running a `Docker version 20.10.7` and no proxy. I built the API image and run the docker-compose. All services work properly, so I still think that on the Linux machine the proxy is playing an important role on my current issue.\nUpdate 2:\nI was able to fix that issue thanks to the @AndrewSilver suggestion.\nMany thanks also to @kha for providing more details.",
      "solution": "just to make sure the above is a useful question, I'm writing here the solution that helped me and others solve the problem and that is to disable TcpStats.\n@kha also added: The exact command is `connectionSettings.EnableTcpStats(false);`. Remember that TcpStats are enabled if you use DebugMode so make sure to add this as the very last parameter to the connection settings before establishing the connection.",
      "question_score": 43,
      "answer_score": 1,
      "created_at": "2021-08-17T15:38:45",
      "url": "https://stackoverflow.com/questions/68818400/docker-compose-with-net-core-sql-server-elasticsearch-and-cerebro-services"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 66831863,
      "title": "Mysql docker container keeps restarting",
      "problem": "The Container keeps restarting.\nI tried\n\ndocker-compose down -v\ndocker volume rm \n\nThe container was working fine earlier.\n\nLogs\n```\n`2021-03-27 13:16:08+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.23-1debian10 started.\n\n2021-03-27 13:16:08+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'\n\n2021-03-27 13:16:08+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.23-1debian10 started.\n\n2021-03-27 13:16:08+00:00 [ERROR] [Entrypoint]: MYSQL_USER=\"root\", MYSQL_USER and MYSQL_PASSWORD are for configuring a regular user and cannot be used for the root user\n\nRemove MYSQL_USER=\"root\" and use one of the following to control the root user password:\n\n- MYSQL_ROOT_PASSWORD\n\n- MYSQL_ALLOW_EMPTY_PASSWORD\n\n- MYSQL_RANDOM_ROOT_PASSWORD\n`\n```\nDocker-compose.yml\n \n```\n` mysql:\n    image: mysql:8.0\n    ports:\n      - 3306:3306\n    expose:\n      - \"3306\"\n    cap_add:\n      - SYS_NICE # CAP_SYS_NICE\n    volumes:\n      - ./cache/mysql:/var/lib/mysql\n      - ./conf-mysql.cnf:/etc/mysql/conf.d/mysql.cnf\n    environment:\n      - MYSQL_ROOT_PASSWORD=root\n      - MYSQL_PASSWORD=root\n      - MYSQL_USER=root\n      - MYSQL_DATABASE=mydb\n    restart: unless-stopped\n`\n```",
      "solution": "Simply remove the `MYSQL_USER` and it will work fine because the `root` user gets created automatically.\nPS. This seems to be a problem with a newer docker version because this used to work before and not throw an error.",
      "question_score": 36,
      "answer_score": 86,
      "created_at": "2021-03-27T14:29:09",
      "url": "https://stackoverflow.com/questions/66831863/mysql-docker-container-keeps-restarting"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 71187944,
      "title": "dlopen: libcrypt.so.1: cannot open shared object file: No such file or directory",
      "problem": "I use EndeavourOS and have updated my system on February 17 2022 using\n```\n`sudo pacman -Syu\n`\n```\nEversince, when I run `docker-compose`, I get this error message:\n\n[4221] Error loading Python lib '/tmp/_MEIgGJQGW/libpython3.7m.so.1.0': dlopen: libcrypt.so.1: cannot open shared object file: No such file or directory\n\nSome forum threads suggested to reinstall docker-compose, which I did. I tried the following solution, but both without success:\nPython3.7: error while loading shared libraries: libpython3.7m.so.1.0\nHow can I resolve this issue?",
      "solution": "The underlying issue here is that you use docker-compose instead of docker compose, which are two different binaries. docker-compose is also known as V1, and is deprecated since April 26, 2022. Since then, it does not receive updates or patches, other than high-severity security patches.\nSo, to fix your issue, use `docker compose` instead of `docker-compose`. If you compare `docker compose version` and `docker-compose version`, you will see that this uses the newer docker compose and runs without an issue.",
      "question_score": 35,
      "answer_score": 3,
      "created_at": "2022-02-19T19:31:40",
      "url": "https://stackoverflow.com/questions/71187944/dlopen-libcrypt-so-1-cannot-open-shared-object-file-no-such-file-or-directory"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 69941444,
      "title": "How to have docker compose init a SQL Server database",
      "problem": "I have a docker-compose file that creates a starts SQL Server. This is working fine. I can connect to the database and see the master database.\nWhat I am trying to do is create a new database, and add a table and some data to that table.  I have been unable to find an example of doing this using SQL Server. All the examples I have seen are either using PostgreSQL or Mysql.\nI have tried to adapt this example Docker Compose MySQL Multiple Database\nI have created an init directory with a file called 01.sql and the only thing in it is\n```\n`CREATE DATABASE `test`;\n`\n```\nMy docker-compose.yml looks like this\n```\n`services:\n    db:\n        image: \"mcr.microsoft.com/mssql/server\"\n        ports:\n            - 1433:1433\n        volumes:             \n            - ./init:/docker-entrypoint-initdb.d\n        environment:\n            SA_PASSWORD: \"password123!\"\n            ACCEPT_EULA: \"Y\"\n`\n```\nWhen I run docker-compose up\nI'm not seeing anything in the logs that implies it's even trying to load this file.  When I check the database I do not see any new database either.\nI am at a loss to understand why this isn't working for SQL Server but the tutorial implies that it works for MySql. Is there a different command for SQL Server?",
      "solution": "After quite a bit of Googling and combining four or five very old tutorials, I got this working. Ensuring that you are using Linux line endings is critical with these scripts.\nDocker-compose.yml\n```\n`version: '3'\n\nservices:\n  db:\n    build: ./Db\n    ports:\n        - 1433:1433\n`\n```\nDb/DockerFile\n```\n`# Choose ubuntu version\nFROM mcr.microsoft.com/mssql/server:2019-CU13-ubuntu-20.04\n\n# Create app directory\nWORKDIR /usr/src/app\n\n# Copy initialization scripts\nCOPY . /usr/src/app\n             \n# Set environment variables, not have to write them with the docker run command\n# Note: make sure that your password matches what is in the run-initialization script \nENV SA_PASSWORD password123!\nENV ACCEPT_EULA Y\nENV MSSQL_PID Express\n\n# Expose port 1433 in case accessing from other container\n# Expose port externally from docker-compose.yml\nEXPOSE 1433\n\n# Run Microsoft SQL Server and initialization script (at the same time)\nCMD /bin/bash ./entrypoint.sh\n`\n```\nDb/entrypoint.sh\n```\n`# Run Microsoft SQl Server and initialization script (at the same time)\n/usr/src/app/run-initialization.sh & /opt/mssql/bin/sqlservr\n`\n```\nDb/run-initialization.sh\n```\n`# Wait to be sure that SQL Server came up\nsleep 90s\n\n# Run the setup script to create the DB and the schema in the DB\n# Note: make sure that your password matches what is in the Dockerfile\n/opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P password123! -d master -i create-database.sql\n`\n```\nDb/create-database.sql\n```\n`CREATE DATABASE [product-db]\nGO\n\nUSE [product-db];\nGO\n\nCREATE TABLE product (\n    Id INT NOT NULL IDENTITY,\n    Name TEXT NOT NULL,\n    Description TEXT NOT NULL,\n    PRIMARY KEY (Id)\n);\nGO\n\nINSERT INTO [product] (Name, Description)\nVALUES \n('T-Shirt Blue', 'Its blue'),\n('T-Shirt Black', 'Its black'); \nGO\n`\n```\nTip: If you change any of the scripts after running it the first time you need to do a `docker-compose up --build` to ensure that the container is built again or it will just be using your old scripts.\nConnect:\n```\n`host:  127.0.0.1\nUsername: SA\nPassword:  password123!\n`\n```",
      "question_score": 31,
      "answer_score": 36,
      "created_at": "2021-11-12T11:31:14",
      "url": "https://stackoverflow.com/questions/69941444/how-to-have-docker-compose-init-a-sql-server-database"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 67773973,
      "title": "How to run docker-compose inside VS Code devcontainer",
      "problem": "I have this multi-container project comprised of 3 NestJS and 1 dotnet5.0 application. Besides the applications, the project depends on a RabbitMQ and an InfluxDB services (running as pure docker images)\nThe `docker-compose` file looks like this:\n`version: '3.8'\n\nservices:\n  influxdb:\n    image: influxdb:2.0\n    container_name: influxdb\n    ports:\n      - '8086:8086'\n    expose:\n      - '8086'\n    volumes:\n      - ./data/influxdb2/data:/var/lib/influxdb2\n      - ./data/influxdb2/config:/etc/influxdb2\n\n  rabbitmq:\n    hostname: 'rabbitmq'\n    image: rabbitmq:3-management\n    container_name: rabbitmq\n    ports:\n      - '15672:15672'\n      - '5672:5672'\n\n  microservice1:\n    image: microservice1\n    container_name: microservice1\n    depends_on: [rabbitmq, influxdb]\n    build:\n      context: .\n      dockerfile: ./apps/microservice1/Dockerfile\n\n  microservice2:\n    image: microservice2\n    container_name: microservice2\n    depends_on: [rabbitmq, influxdb]\n    build:\n      context: .\n      dockerfile: ./apps/microservice2/Dockerfile\n\n  microservice3:\n    image: microservice3\n    container_name: microservice3\n    depends_on: [rabbitmq, influxdb]\n    build:\n      context: .\n      dockerfile: ./apps/microservice3/Dockerfile\n\n  microservice4:\n    image: microservice4\n    container_name: microservice4\n    depends_on: [rabbitmq, influxdb]\n    build:\n      context: .\n      dockerfile: ./apps/microservice4/Dockerfile\n\n`\nI want to move the whole dev. environment to the new VS Code devcontainers but I'm not quite getting how to work with dependencies (like rabbitmq and influxdb here).\nIdeally, I'd open the repo in a devcontainer with both nodejs and dotnet SDKs to be able to run the microservices during development. But, I don't want to also install influxdb and rabbitmq into the devcontainer as I want to leverage the existing (and convenient) docker images.\nProblem is, once I open the repo inside the devcontainer there's no way to interact with docker-compose from the inside (as docker/docker-compose is not available inside the devcontainer).\nIs it possible to interact with Docker engine on the host from inside the container? So I can simply have a `dev.sh` script that can simply `up` the rabbitmq and influxdb dependencies and then launch whatever microservice I want to run?\nMaybe I'm getting it all wrong but I couldn't find a clear explanation on how to mix VS Code devcontainers and docker-compose files (with image-based dependencies).",
      "solution": "vscode can only use one service as \"the workspace\" where the IDE runs. Just like when working locally you are on the IDE and the other services run in other containers.\nNone of your current services seem \"good\" for being the IDE workspace, so you would have to add that one. That would be \"just like\" your host machine, but in the container.\nYou can use multiple compose files so you can avoid changing your current `docker-compose.yml` while being able to add your new service.\nSo, part I:\n\nCreate a second `docker-compose.yml` file (maybe: `docker-compose.workspace.yml`)\nAdd one single service to that file, maybe call it \"workspace\". vscode Remote part will run there. What image? You may use one vscode's precooked ones.\nOn `.devcontainer.json` point to both files and define the workspace service:\n\n`...\n  \"dockerComposeFile\": [\n    \"docker-compose.yaml\",\n    \"docker-compose.workspace.yaml\"\n  ],\n  \"service\": \"workspace\",\n...\n`\nOk. So that gives you the workspace container and everything else on the side. That gets us to the other part of the question:\n\nIs it possible to interact with Docker engine on the host from inside the container? So I can simply have a dev.sh script that can simply up the rabbitmq and influxdb dependencies and then launch whatever microservice I want to run?\n\nFirst, if you want the docker & docker-compose commands you have to install the packages. Some images have them builtin, others not. You may use your own image, etc.\nBut that is not enough. You workspace container is ignorant of the host's docker. But it is easy enough to fix. Just add a volume mount:\n```\n`/var/run/docker.sock:/var/run/docker.sock\n`\n```\nOn the workspace service. That way vscode will \"see\" your host's docker and operate with it.\nBeware that it is still the host's docker, so you may get into trouble with paths, etc depending on what you do.",
      "question_score": 31,
      "answer_score": 26,
      "created_at": "2021-05-31T14:42:40",
      "url": "https://stackoverflow.com/questions/67773973/how-to-run-docker-compose-inside-vs-code-devcontainer"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 78380867,
      "title": "docker-compose run issue 2024: Error: &#39;ContainerConfig&#39;",
      "problem": "I have what seems like a very strange issue that I hope someone has hit before.\nI have a docker-compose file that houses a service for redis. Nothing special, I just grab the latest redis from docker hub. I went in to redeploy today and I normally run `--force-recreate` to down/up the containers, but when I attempt to run `--force-recreate` today, I am getting weird errors I have not seen before (and this worked fine yesterday).\nStrangely enough though, running normal down/up commands works and there is no issue. Am I missing something?\nHere are the commands that work to down/up my system without errors:\n`docker-compose -f docker-compose.prod.yml down \ndocker-compose -f docker-compose.prod.yml up -d\n`\nHere is the command that should be fine, but it fails with 'ContainerConfig' errors for redis' docker-compose:\n`docker-compose -f docker-compose.prod.yml up -d --force-recreate\n`\nOutput...\n`docker-compose -f docker-compose.prod.yml up -d --force-recreate\nRecreating app_redis_1 ...\n\nERROR: for app_redis_1  'ContainerConfig'\n\nTraceback (most recent call last):\n  File \"docker-compose\", line 3, in \n  File \"compose/cli/main.py\", line 81, in main\n  File \"compose/cli/main.py\", line 203, in perform_command\n  File \"compose/metrics/decorator.py\", line 18, in wrapper\n  File \"compose/cli/main.py\", line 1186, in up\n  File \"compose/cli/main.py\", line 1182, in up\n  File \"compose/project.py\", line 702, in up\n  File \"compose/parallel.py\", line 108, in parallel_execute\n  File \"compose/parallel.py\", line 206, in producer\n  File \"compose/project.py\", line 688, in do\n  File \"compose/service.py\", line 581, in execute_convergence_plan\n  File \"compose/service.py\", line 503, in _execute_convergence_recreate\n  File \"compose/parallel.py\", line 108, in parallel_execute\n  File \"compose/parallel.py\", line 206, in producer\n  File \"compose/service.py\", line 496, in recreate\n  File \"compose/service.py\", line 615, in recreate_container\n  File \"compose/service.py\", line 334, in create_container\n  File \"compose/service.py\", line 922, in _get_container_create_options\n  File \"compose/service.py\", line 962, in _build_container_volume_options\n  File \"compose/service.py\", line 1549, in merge_volume_bindings\n  File \"compose/service.py\", line 1579, in get_container_data_volumes\nKeyError: 'ContainerConfig'\n[88001] Failed to execute script docker-compose\n`\nHere is the simple `docker-compose` config for the redis service:\n```\n`version: '3.8'\n\nservices:\n\n  redis:\n    image: redis:latest\n    restart: always\n    ports:\n      - \"6379\"\n`\n```",
      "solution": "As noted in countless posts here, and as noted in the comments by @Chris Becke this was a cause of depreciated commands in Docker. It is now 2024 and things have updated.\nFor whatever reason, `--force-recreate` with `docker-compose` now fails on my production system after system updates, while `docker compose up -d --force-recreate` works as expected. (Notice the removal of `-`)\nWeird thing is I host on DigitalOcean and did run updates the other day via `apt-get...` and I did notice docker being updated...but this error was not easy to figure out the cause which is why I asked here. It also hasn't effected my staging env so not sure what the cause for the original commands not working is...",
      "question_score": 30,
      "answer_score": 32,
      "created_at": "2024-04-24T21:41:59",
      "url": "https://stackoverflow.com/questions/78380867/docker-compose-run-issue-2024-error-containerconfig"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 68647242,
      "title": "define volumes in docker-compose.yaml",
      "problem": "I am writing a `docker-compose.yaml` file for my project. I have checked the volumes documentation here .\nI also understand the concept of `volume` in docker that I can mount a volume e.g. `-v my-data/:/var/lib/db` where `my-data/` is a directory on my host machine while  `/var/lib/db` is the path inside database container.\nMy confuse is with the link I put above. There it has the following sample:\n```\n`version: \"3.9\"\n\nservices:\n  db:\n    image: db\n    volumes:\n      - data-volume:/var/lib/db\n  backup:\n    image: backup-service\n    volumes:\n      - data-volume:/var/lib/backup/data\n\nvolumes:\n  data-volume:\n`\n```\nI wonder does it mean that I have to create a directory named `data-volume` on my host machine?  What if I have a directory on my machine with path `temp/my-data/` and I want to mount that path to the database container  `/var/lib/db` ? Should I do something like below?\n```\n`version: \"3.9\"\n\nservices:\n  db:\n    image: db\n    volumes:\n      - temp/my-data/:/var/lib/db\n\nvolumes:\n  temp/my-data/:\n`\n```\nMy main confusion is the `volumes:` section at the bottom, I am not sure whether the volume name should be the path of my directory or should be just literally a name I give & if it is the latter case then how could the given name be mapped with `temp/my-data/` on my machine? The sample doesn't indicate that & is ambiguous to clarify that.\nCould someone please clarify it for me?\nP.S. I tried with above docker-compose I guessed, ended up with the error:\n```\n`ERROR: The Compose file './docker-compose.yaml' is invalid because:\nvolumes value 'temp/my-data/' does not match any of the regexes: '^[a-zA-Z0-9._-]+$'\n`\n```",
      "solution": "Mapped volumes can either be files/directories on the host machine (sometimes called `bind mounts` in the documentation) or they can be docker volumes that can be managed using `docker volume` commands.\nThe `volumes:` section in a docker-compose file specify docker volumes, i.e. not files/directories. The first docker-compose in your post uses such a volume.\nIf you want to map a file or directory (like in your last docker-compose file), you don't need to specify anything in the `volumes:` section.\nDocker volumes (the ones specified in the `volumes:` section or created using `docker volume create`) are of course also stored somewhere on your host computer, but docker manages that and you shouldn't normally need to know where or what the format is.\nThis part of the documentation is pretty good about explaining it, I think https://docs.docker.com/storage/volumes/",
      "question_score": 30,
      "answer_score": 23,
      "created_at": "2021-08-04T09:39:55",
      "url": "https://stackoverflow.com/questions/68647242/define-volumes-in-docker-compose-yaml"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 67199539,
      "title": "TSC not found in Docker build",
      "problem": "When building an image that needs to be compiled from typescript, I get this error.\n\nsh: 1: tsc: not found\n\nThe command '/bin/sh -c npm run tsc' returned a non-zero code: 127\n\nHere is the relevant code:\ndocker-compose.yaml\n```\n`version: '3.1'\n\nservices:\n  nodeserver:\n    build:\n      context: .\n      target: prod\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - ./src:/app/src\n      - ./public:/app/public\n      - ./templates:/app/templates\n`\n```\nDockerfile\n```\n`FROM node:15.11.0 AS base\nEXPOSE 3000\nENV NODE_ENV=production\nWORKDIR /app\nCOPY package*.json ./\n\nRUN npm install --only=production && npm cache clean --force\n\n##########################################################################################\n\nFROM base AS dev\n\nENV NODE_ENV=development\n\nRUN npm install --only=development\n\nCMD npm run dev\n\n##########################################################################################\n\nFROM dev AS source\n\nCOPY dist dist\nCOPY templates templates\nCOPY public public\n\nRUN npm run tsc\n\n##########################################################################################\n\nFROM base AS test\n\nCOPY --from=source /app/node_modules /app/node_modules\nCOPY --from=source /app/templates /app/templates\nCOPY --from=source /app/public /app/public\nCOPY --from=source /app/dist /app/dist\n\nCMD npm run test\n\n##########################################################################################\n\nFROM test AS prod\n\nCMD npm start\n`\n```\npackage.json\n```\n`{\n  \"name\": \"nodeserver\",\n  \"version\": \"1.0.0\",\n  \"description\": \"\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"node ./dist/app.js\",\n    \"deploy\": \"git add . && git commit -m Heroku && git push heroku main\",\n    \"tsc\": \"tsc --outDir ./dist\",\n    \"dev\": \"npm run ts-watch\",\n    \"test\": \"npm run jest --runInBand\",\n    \"ts-watch\": \"tsc-watch --project . --outDir ./dist --onSuccess \\\"nodemon ./dist/app.js\\\"\"\n  },\n  \"jest\": {\n    \"testEnvironment\": \"node\"\n  },\n  \"repository\": {\n    \"type\": \"git\",\n    \"url\": \"git+https://github.com/MiquelPiza/nodeserver.git\"\n  },\n  \"author\": \"\",\n  \"license\": \"ISC\",\n  \"bugs\": {\n    \"url\": \"https://github.com/MiquelPiza/nodeserver/issues\"\n  },\n  \"homepage\": \"https://github.com/MiquelPiza/nodeserver#readme\",\n  \"dependencies\": {\n    \"@sendgrid/mail\": \"^7.4.2\",\n    \"bcryptjs\": \"^2.4.3\",\n    \"express\": \"^4.17.1\",\n    \"handlebars\": \"^4.7.7\",\n    \"jsonwebtoken\": \"^8.5.1\",\n    \"lodash\": \"^4.17.20\",\n    \"mongodb\": \"^3.6.4\",\n    \"mongoose\": \"^5.11.19\",\n    \"multer\": \"^1.4.2\",\n    \"socket.io\": \"^4.0.0\",\n    \"validator\": \"^13.5.2\"\n  },\n  \"devDependencies\": {\n    \"@types/bcryptjs\": \"^2.4.2\",\n    \"@types/express\": \"^4.17.11\",\n    \"@types/jsonwebtoken\": \"^8.5.0\",\n    \"@types/lodash\": \"^4.14.168\",\n    \"@types/mongoose\": \"^5.10.3\",\n    \"@types/multer\": \"^1.4.5\",\n    \"@types/node\": \"^14.14.33\",\n    \"@types/sendgrid\": \"^4.3.0\",\n    \"@types/validator\": \"^13.1.3\",\n    \"env-cmd\": \"^10.1.0\",\n    \"jest\": \"^26.6.3\",\n    \"nodemon\": \"^2.0.7\",\n    \"supertest\": \"^6.1.3\",\n    \"tsc-watch\": \"^4.2.9\",\n    \"typescript\": \"^4.2.3\"\n  },\n  \"engines\": {\n    \"node\": \"15.11.0\"\n  }\n}\n`\n```\ntsconfig.json\n```\n`{\n  \"compilerOptions\": {\n\n    \"target\": \"es5\", \n    \"module\": \"commonjs\",\n    \"strict\": true,\n    \"strictNullChecks\": false,\n    \"esModuleInterop\": true,\n    \"skipLibCheck\": true,\n    \"forceConsistentCasingInFileNames\": true,\n  },\n  \"include\": [\"src\"]\n}\n`\n```\nThis dockerfile works:\n```\n`FROM node:15.11.0 AS build\n\nWORKDIR /app\nCOPY package.json .\nRUN npm install\nADD . .\nRUN npm run tsc\n\nFROM node:15.11.0\nWORKDIR /app\n\nCOPY package.json .\nRUN npm install --production\n\nADD public ./public\nADD templates ./templates\nCOPY --from=build /app/dist dist\n\nEXPOSE 3000\nCMD npm start\n`\n```\nI'm using this dockerfile for reference, from a Docker course: https://github.com/BretFisher/docker-mastery-for-nodejs/blob/master/typescript/Dockerfile\nI don't see what I'm doing wrong, the source stage should have the dev dependencies, among them typescript, so it should be able to run tsc.\nAny help appreciated. Thanks.\nEDIT:\nIn addition to using npm ci instead of npm install, I had to copy tsconfig.json to the working directory (and copy src directory instead of dist, which is created by tsc) for tsc to work properly. This is the modified source stage in the Dockerfile:\n```\n`FROM dev AS source\n\nCOPY src src\nCOPY templates templates\nCOPY public public\nCOPY tsconfig.json tsconfig.json\n\nRUN npm run tsc\n`\n```",
      "solution": "EDIT 2:\nThis issue has been resolved.  If your package-lock.json file is corrupted, you may be able to fix it with the utility fix-has-install-script.\nOriginal Answer:\nUse `npm ci` (or add `package-lock.json` to your `.dockerignore` file, or delete `package-lock.json` in your local environment before building).  The why is answered here.\nEDIT 1:\nHere's what I believe is going on.  Disclaimer, I'm not an expert on `nodejs` or `npm` -- in fact I'm something of a novice.  And all of this is conjecture based on some experiments.\nWhat's going wrong?\n`npm` is not linking the binaries for the dev dependencies via sym links in `node_modules/.bin` because the `package-lock.json` file has gotten into a corrupted state where the (prod) dependencies are in lockfileVersion 2 format, and the dev dependencies are still in lockfileVersion 1 format.\nWhy is this happening?\nNote: Making a bunch of assumptions here.\n\nYour local host using using npm 6, and the docker container is using npm 7.\nBecause of this, the existing `package-lock.json` is in `lockfileVersion: 1` which doesn't include a `bin` section for dependencies that have binaries.  Version 2 does save the `bin:` section, which `npm` must use to determine what binaries to install/link.\n\nWhen you run the production dependency install (e.g. `NODE_ENV=production npm install`), using `npm` version 7, `npm` is upgrading the version of your `package-lock.json` to `lockfileVersion: 2`, part of this includes saving `bin:` sections for the dependencies that install binaries.  Importantly, it updates only the production dependencies.  Now the `package-lock.json` file is corrupted because it claims to be in version 2 format, but all the dev dependencies are either still in version 1 or at least don't have the `bin:` section correctly applied.\n\nWhen you now try to install your dev dependencies, `npm` sees that the `package-lock.json` is in `lockfileVersion: 2`, so it assumes that the dev dependencies have been upgraded as well (but they haven't been, or at least not correctly).  It doesn't find the `bin:` sections because they don't exist and so it doesn't link the binaries to the `node_modules/.bin/` directory.\n\nYou can perform a minimum reproduction of it using this `Dockerfile`:\n```\n`FROM node:14 as npm6\nWORKDIR /app\n# Create a node project using npm 6 and install a dev dependency\n# that contains a binary.\nRUN npm init --yes && \\\n    npm install --save-dev typescript\n\nFROM node:15 as npm7\nCOPY --from=npm6 /app/package*.json /app/\nWORKDIR /app\n# Install production dependencies, then all dependencies. This should\n# link the binaries for typescript in (e.g. tsc) under node_modules/.bin.\nRUN npm install -g npm@7.10.0 && \\\n    npm install --production && \\\n    npm install\n\n# Causes error, tsc not found.\nCMD [\"npx\", \"-c\", \"tsc --version\"]\n`\n```\nI couldn't find an existing bug ticket so I created one here.  Perhaps it will get fixed.",
      "question_score": 29,
      "answer_score": 11,
      "created_at": "2021-04-21T18:10:05",
      "url": "https://stackoverflow.com/questions/67199539/tsc-not-found-in-docker-build"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 72384329,
      "title": "Using SSH agent with Docker Compose and Dockerfile",
      "problem": "I am having issues using a private github repo in one of my NestJS apps. When I create the docker image using the `docker build` command, the image is successfully created and everything works fine. However I can't use the Dockerfile with `docker-compose`.\nHere's the part of `Dockerfile` where I use the `BuildKit` mount feature:\n```\n`RUN mkdir -p -m 0600 ~/.ssh && ssh-keyscan github.com >> ~/.ssh/known_hosts\n\nRUN --mount=type=ssh npm install\n\n`\n```\nWhen building the image with `Dockerfile` alone I pass the `--ssh default` argument, like this and it successfully installs the private repo:\n```\n`docker build --ssh default -t CONTAINER_NAME .\n`\n```\nFollowing this article, inside the `docker-compose.yml` file I have included the `$SSH_AUTH_SOCK` like this:\n```\n`environment:\n      - NODE_ENV:${NODE_ENV}\n      - SSH_AUTH_SOCK:${SSH_AUTH_SOCK}\nvolumes:\n      - $SSH_AUTH_SOCK:${SSH_AUTH_SOCK}\n`\n```\nHowever I get this error whenever I try to run `docker-compose up`\n```\n`#11 44.97 npm ERR! code 128\n#11 44.97 npm ERR! An unknown git error occurred\n#11 44.97 npm ERR! command git --no-replace-objects ls-remote ssh://git@github.com/organization/repo.git\n#11 44.97 npm ERR! git@github.com: Permission denied (publickey).\n#11 44.97 npm ERR! fatal: Could not read from remote repository.\n#11 44.97 npm ERR! \n#11 44.97 npm ERR! Please make sure you have the correct access rights\n#11 44.97 npm ERR! and the repository exists.\n`\n```\nAny idea what I am doing wrong?",
      "solution": "They have added the ssh flag as option to the build key in compose: https://github.com/compose-spec/compose-spec/pull/234\n`services:\n  sample:\n    build:\n      context: .\n      ssh:\n        - default\n`",
      "question_score": 29,
      "answer_score": 41,
      "created_at": "2022-05-25T23:22:52",
      "url": "https://stackoverflow.com/questions/72384329/using-ssh-agent-with-docker-compose-and-dockerfile"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 67087735,
      "title": "(EACCES: permission denied, mkdir &#39;/usr/app/node_modules/.cache) How can I create a docker-compose file to make node_modules a non-root folder?",
      "problem": "I'm trying to dockerize a simple create-react-app project. (Is the initial project after running `npx create-react-app test`, no files were changed).\nThe problem seems to be that in newer versions of React, they moved the annoying `.eslintcache` from the root folder to `/node_modules/.cache`, causing problems when the container is trying to run the application via `docker-compose`.\nDockerfile\n```\n`FROM node:alpine\n\nWORKDIR /usr/app\n\nCOPY package*.json ./\n\nRUN npm install\n\nRUN chown -R node.node /usr/app/node_modules\n\nCOPY . ./\n\nCMD [\"npm\", \"start\"]\n`\n```\ndocker-compose\n`version: '3'\nservices: \n  test:\n    stdin_open: true\n    build:\n      context: .\n      dockerfile: Dockerfile\n    environment:\n      - CHOKIDAR_USEPOLLING=true\n    volumes:\n      - /usr/app/node_modules\n      - .:/usr/app\n    ports:\n      - '3000:3000'\n`\nThe container is logging this error message:\n```\n`test_1   | Failed to compile.\ntest_1   | \ntest_1   | EACCES: permission denied, mkdir '/usr/app/node_modules/.cache\n`\n```\nAs you can notice, I tried to set the `node_modules` folder owner to the node user (the default user for `node:alpine`), but it is not working; exploring the container, you can see that the `node_modules` folder is still owned by `root`:\n```\n`drwxrwxr-x    5 node     node          4096 Apr 14 07:04 .\ndrwxr-xr-x    1 root     root          4096 Apr 14 07:08 ..\n-rw-rw-r--    1 node     node           310 Apr 14 06:56 .gitignore\n-rw-rw-r--    1 node     node           192 Apr 14 07:30 Dockerfile\n-rw-rw-r--    1 node     node          3369 Apr 14 06:56 README.md\ndrwxrwxr-x 1061 root     root         36864 Apr 14 07:12 node_modules\n-rw-rw-r--    1 node     node        692936 Apr 14 06:56 package-lock.json\n-rw-rw-r--    1 node     node           808 Apr 14 06:56 package.json\ndrwxrwxr-x    2 node     node          4096 Apr 14 06:56 public\ndrwxrwxr-x    2 node     node          4096 Apr 14 06:56 src\n`\n```\nI also tried to create the folder `RUN mkdir -p /usr/app` and use `USER node` but that end up in an issue where npm wasn't able to create the `node_modules` folder.\nIs there any workaround where either `.eslintcache` is disabled or `node_modules` is owned by the `node` user?\nUpdate\nApparently, this is occurring because I'm using Ubuntu and docker mounts volumes as root on Linux systems.",
      "solution": "Adding this line just after `RUN npm install` in your Dockerfile would solve the issue:\n```\n`RUN mkdir -p node_modules/.cache && chmod -R 777 node_modules/.cache\n`\n```\nFinal Dockerfile\n```\n`FROM node:alpine\n\nWORKDIR /usr/app\n\nCOPY package.json .\nRUN npm install\n\nRUN mkdir node_modules/.cache && chmod -R 777 node_modules/.cache\n\nCOPY . .\n\nCMD [\"npm\", \"run\", \"start\"]\n`\n```\nThen you don't need to copy the `node_modules` folder from your local dir to the container. You can safely bookmark it.",
      "question_score": 28,
      "answer_score": 50,
      "created_at": "2021-04-14T09:59:47",
      "url": "https://stackoverflow.com/questions/67087735/eacces-permission-denied-mkdir-usr-app-node-modules-cache-how-can-i-creat"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 67769806,
      "title": "docker: Error response from daemon: manifest for jenkins:latest not found: manifest unknown: manifest unknown",
      "problem": "I want run this line:\n```\n`docker run --name myjenkins1 -v myvoll:/var/jenkins_home -p 8080:8080 -p 50000:50000 jenkins\n`\n```\nBut result is:\n\nUnable to find image 'jenkins:latest' locally\ndocker: Error response from daemon: manifest for jenkins:latest not found: manifest unknown: manifest unknown.\nSee 'docker run --help'.\n\nHow can i solve this ...",
      "solution": "The jenkins image has been deprecated for over 2 years in favor of the jenkins/jenkins:lts image provided and maintained by the Jenkins Community as part of the project's release process.\nUse below image:\n`docker pull jenkins/jenkins`\n`docker run -p 8080:8080 --name=jenkins-master -d jenkins/jenkins`\nFor more info:\nhttps://hub.docker.com/r/jenkins/jenkins",
      "question_score": 27,
      "answer_score": 70,
      "created_at": "2021-05-31T09:51:36",
      "url": "https://stackoverflow.com/questions/67769806/docker-error-response-from-daemon-manifest-for-jenkinslatest-not-found-manif"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 70400523,
      "title": "error while removing network: &lt;network&gt; id has active endpoints",
      "problem": "I am trying run docker compose down using jenkins job.\n\"sudo docker-compose down --remove-orphans\"\nI have used --remove-orphans command while using the docker-compose down.\nStill it gives below error.\nRemoving network. abc\nerror while removing network: network  id ************ has active endpoints\nFailed command  with status 1: sudo docker-compose down --remove-orphans\nBelow is my docker compose:\n```\n`version: \"3.9\"\nservices:\n  abc:\n    image: \n    container_name: 'abc'\n    hostname: abc\n    ports:\n      - \"5****:5****\"\n      - \"1****:1***\"\n    volumes:\n      - ~/.docker-conf/\n    networks:\n      - \n\n      \n  container-app-1:\n    image: \n    container_name: 'container-app-1'\n    hostname: 'container-app-1'\n    depends_on:\n      - abc\n    ports:\n      - \"8085:8085\"\n    env_file: ./.env\n    networks:\n      - \n\nnetworks:\n  :\n    driver: bridge\n    name: \n`\n```",
      "solution": "To list your networks, run `docker network ls`. You should see your `` there. Then get the containers still attached to that network with (replacing your network name at the end of the command):\n```\n`docker network inspect \\\n  --format '{{range $cid,$v := .Containers}}{{printf \"%s: %s\\n\" $cid $v.Name}}{{end}}' \\\n  \"\"\n`\n```\nFor the various returned container id's, you can check why they haven't stopped (inspecting the logs, making sure they are part of the compose project, etc), or manually stop them if they aren't needed anymore with (replacing the `` with your container id):\n```\n`docker container stop \"\"\n`\n```\nThen you should be able to stop the compose project.",
      "question_score": 27,
      "answer_score": 26,
      "created_at": "2021-12-18T03:11:47",
      "url": "https://stackoverflow.com/questions/70400523/error-while-removing-network-network-id-has-active-endpoints"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 71297042,
      "title": "React hot reload doesn&#39;t work in docker container",
      "problem": "I am trying to set up React with docker, but for some reason I cannot get the hot reload to work. Currently, if I create a file it recompiles, but if I change something in a file it does not. Also, I didn't change any packages or configuration files, the project was generated with `npx create-react-app projectname --template typescript`.\nFrom researching this online I found out that I needed to add `CHOKIDAR_USEPOLLING=true` to a .env file, I tried this but it didn't work, I tried placing the .env in all directories in case I placed it in the wrong one. I also added it to the docker-compose.yml environment.\nIn addition to this, I also tried downgrading react-scripts to 4.0.3 because I found this, that also didn't work.\nI also tried changing a file locally and then checking if it also changes inside the docker container, it does, so I'm pretty sure my docker related files are correct.\nVersions\n\nNode 16.14\nDocker Desktop 4.5.1 (Windows)\nreact 17.0.2\nreact-scripts 5.0.0\n\nDirectory structure\n```\n`project/\n\u2502   README.md\n\u2502   docker-compose.yml    \n\u2502\n\u2514\u2500\u2500\u2500frontend/\n    \u2502   Dockerfile\n    \u2502   package.json\n    \u2502   src/\n    \u2502   ...\n\n`\n```\nDockerfile\n```\n`FROM node:16.14-alpine3.14\n\nWORKDIR /app\n\nCOPY package.json .\nCOPY package-lock.json .\nRUN npm install \n\nCMD [\"npm\", \"start\"]\n`\n```\ndocker-compose.yml\n```\n`services:\n  frontend:\n    build: ./frontend\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - \"./frontend:/app\"\n      - \"/app/node_modules\"\n    environment:\n      CHOKIDAR_USEPOLLING: \"true\"\n`\n```",
      "solution": "The Dockerfile you have is great for when you want to package your app into a container, ready for deployment. It's not so good for development where you want to have the source outside the container and have the running container react to changes in the source.\nWhat I do is keep the Dockerfile for packaging the app and only build that, when I'm done.\nWhen developing, I can often do that without a dockerfile at all, just by running a container and mapping my source code into it.\nFor instance, here's a command I use to run a node app\n```\n`docker run -u=1000:1000 -v $(pwd):/app -w=/app -d -p 3000:3000 --rm --name=nodedev node bash -c \"npm install && npm run dev\"\n`\n```\nAs you can see, it just runs a standard node image. Let me go through the different parts of the command:\n`-u 1000:1000` 1000 is my UID and GID on the host. By running the container using the same ids, any files created by the container will be owned by me on the host.\n`-v $(pwd):/app` map the current directory into the /app directory in the container\n`-w /app` set the working directory in the container to /app\n`-d` run detached\n`-p 3000:3000` map port 3000 in the container to 3000 on the host\n`--rm` remove the container when it exits\n`-name=nodedev` give it a name, so    I can kill it without looking up the name\nat the end there's a command for the container `bash -c \"npm install && npm run dev\"` which starts by installing any dependencies and then runs the dev script in the package.json file. That script starts up node in a mode, where it hot reloads.",
      "question_score": 26,
      "answer_score": 11,
      "created_at": "2022-02-28T16:09:22",
      "url": "https://stackoverflow.com/questions/71297042/react-hot-reload-doesnt-work-in-docker-container"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 73427249,
      "title": "&#39;name&#39; does not match any of the regexes: &#39;^x-&#39; while trying to set the project name in a docker-compose.yml",
      "problem": "I put the following line at the top of my docker-compose.yml, to get a pre-set names for my containers spun by that particular docker-compose:\n```\n`name: my_project_name\n`\n```\nThe goal is, for example, to have my database service container named \"my_project_name_database\".\nHowever, after trying to start it with docker-compose up, I get the following error:\n\nERROR: The Compose file './docker-compose.yml' is invalid because:\n'name' does not match any of the regexes: '^x-'\nYou might be seeing this error because you're using the wrong Compose file version. Either specify a supported version (e.g \"2.2\" or \"3.3\") and place your service definitions under the `services` key, or omit the `version` key and place your service definitions at the root of the file to use version 1.\nFor more on the Compose file format versions, see https://docs.docker.com/compose/compose-file/\n\nThis keeps happening no matter what version of docker-compose I specify.\nWhat am I doing wrong in here? Is this the right way to set the project name at the docker-compose.yml level?",
      "solution": "I'd recommend not setting the top-level `name:` key, but do set the `COMPOSE_PROJECT_NAME` environment variable if you need it.\n`export COMPOSE_PROJECT_NAME=my_project_name\ndocker-compose up -d\n`\nThe project name also defaults to the base name of the current directory, so renaming it is potentially an option.\n`cd ..\nmv project my_project_name\ncd my_project_name\ndocker-compose up -d\n`\n\nDocker has done some confusing things with the Compose file format.  They've introduced a Compose specification which has some incompatibilities with the existing tooling, and then labeled the existing versions that are well-supported by all reasonably-current versions of the Compose tool as \"legacy\".  The top-level `name:` key is new in the \"specification\" version, but not present in the \"legacy\" versions.\nNeither Compose file version 2 nor 3 supports a top-level `name:` key.  If you're running the standalone `docker-compose` tool, there just isn't an option to set the project name in the Compose file.  But `$COMPOSE_PROJECT_NAME` works with all reasonably current versions of Compose.\nIf you run `docker-compose version` and it prints\n`$ docker-compose version\ndocker-compose version 1.29.2, build 5becea4c\ndocker-py version: 5.0.0\nCPython version: 3.9.0\nOpenSSL version: OpenSSL 1.1.1h  22 Sep 2020\n`\nthen you can't use the Compose specification, but you can use the well-supported \"legacy\" versions.  But, if you run `docker compose version` (note, three words, no hyphen) and it prints\n`$ docker compose version\nDocker Compose version v2.6.0\n`\nthen you can use either the \"legacy\" versions or the \"specification\" version.\nThat is, if you use the \"specification\" extensions, then you must be using the most-recent version of Docker with the Compose extension installed, but if you use `version: '2.4'` or `version: '3.8'` then it will work with any more-or-less-current version of Compose.\n\nThe usual use case I see for setting `$COMPOSE_PROJECT_NAME` is to run multiple copies of the same Compose file at the same time.  For this you'll need to set the environment variable (and also to use environment variables for published `ports:`, and avoid manually setting `container_name:` or volume or network names); if it was a constant in the file then you'd have to edit it when working with one instance or the other of the application.",
      "question_score": 23,
      "answer_score": 36,
      "created_at": "2022-08-20T15:53:21",
      "url": "https://stackoverflow.com/questions/73427249/name-does-not-match-any-of-the-regexes-x-while-trying-to-set-the-project"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 77452312,
      "title": "Docker &quot;Failed to solve: Canceled: context canceled&quot; when loading build context",
      "problem": "When running `docker-compose up --build` I'm constantly getting this error.\n```\n` => [web internal] load build definition from Dockerfile                                                           0.0s\n => => transferring dockerfile: 1.58kB                                                                             0.0s\n => [web internal] load .dockerignore                                                                              0.0s\n => => transferring context: 279B                                                                                  0.0s\n => [web internal] load metadata for docker.io/library/python:3.8                                                  0.8s\n => CANCELED [web  1/14] FROM docker.io/library/python:3.8@sha256:7a82536f5a2895b70416ccaffc49e6469d11ed8d9bf6bcf  0.3s\n => => resolve docker.io/library/python:3.8@sha256:7a82536f5a2895b70416ccaffc49e6469d11ed8d9bf6bcfc52328faeae7c77  0.2s\n => => sha256:795c73a8d985b6d1b7e5730dd2eece7f316ee2607544b0f91841d4c4142d9448 7.56kB / 7.56kB                     0.0s\n => => sha256:7a82536f5a2895b70416ccaffc49e6469d11ed8d9bf6bcfc52328faeae7c7710 1.86kB / 1.86kB                     0.0s\n => => sha256:129534c722d189b3baf69f6e3289b799caf45f75da37035c854100852edcbd7d 2.01kB / 2.01kB                     0.0s\n => CANCELED [web internal] load build context                                                                     0.1s\n => => transferring context: 26.00kB                                                                               0.0s\nfailed to solve: Canceled: context canceled\n`\n```\nThis has been happening since yesterday, before my images would build and my application would run just fine.\n\nTried identifying any processes/apps that interfere with the docker build process\nReinstalled Docker Desktop completely\nTried changing Python versions inside my Dockerfile\n\nI'm quite lost as to what is going wrong, I've used `python3.8` and `python3.9` without issues before. Below is my dockerfile and docker-compose.\n```\n`# Use an official Python runtime as the base image\nFROM python:3.8\n\n# Set environment variables\nENV PYTHONDONTWRITEBYTECODE 1\nENV PYTHONUNBUFFERED 1\nENV NODE_ENV production\n\n# Create and set the working directory\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n        default-mysql-client \\\n        software-properties-common \\\n        curl \\\n        gnupg \\\n    && curl -fsSL https://deb.nodesource.com/gpgkey/nodesource.gpg.key | gpg --dearmor -o /usr/share/keyrings/nodesource.gpg \\\n    && echo \"deb [signed-by=/usr/share/keyrings/nodesource.gpg] https://deb.nodesource.com/node_20.x bullseye main\" | tee /etc/apt/sources.list.d/nodesource.list \\\n    && apt-get update && apt-get install -y nodejs \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Verify that Node.js and npm are installed\nRUN node --version\nRUN npm --version\n\n# Install Python dependencies\nCOPY requirements.txt .\nRUN pip install --upgrade pip && \\\n    pip install -r requirements.txt\n\n# Install Tailwind CSS and its peer dependencies\nCOPY package.json package-lock.json ./\nRUN npm install\n\n# Copy the current directory contents into the container\nCOPY . .\n\n# Build the Tailwind CSS\nRUN npm run build:css\n\nRUN npm run watch:css\n\nCOPY entrypoint.sh ./entrypoint.sh\nRUN chmod +x ./entrypoint.sh\n\nENTRYPOINT [\"./entrypoint.sh\"]\n\n# The main command to run when the container starts\nCMD [\"gunicorn\", \"--bind\", \":8000\", \"bmlabs.wsgi:application\"]\n`\n```\nDocker-compose\n```\n`\nservices:\n  db:\n    image: mysql:5.7\n    volumes:\n      - db_data:/var/lib/mysql\n    restart: always\n    environment:\n      MYSQL_ROOT_PASSWORD: \n      MYSQL_DATABASE:\n      MYSQL_USER: \n      MYSQL_PASSWORD:\n    networks:\n      - backend\n\n  web:\n    build: .\n    volumes:\n      - .:/app\n      - static_volume:/app/staticfiles\n      - media_volume:/app/mediafiles\n    depends_on:\n      - db\n    networks:\n      - backend\n    expose:\n      - \"8000\"\n\n  nginx:\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n    volumes:\n      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro\n      - ./nginx/bmlabs.conf:/etc/nginx/conf.d/bmlabs.conf:ro\n      - static_volume:/app/staticfiles\n      - media_volume:/app/mediafiles\n    depends_on:\n      - web\n    networks:\n      - backend\n\nnetworks:\n  backend:\n\nvolumes:\n  db_data:\n  cache_data:\n  static_volume:\n  media_volume:\n`\n```\nI am on Windows 10.",
      "solution": "Altough downgrading docker worked, the actual problem was that I didn't exclude node_modules in Dockerignore. I had been running the containers fine for quite a long time.\nAfter purging all containers and images and adding the `node_modules/` line to my `.dockerignore` it fixed it.\nI'm guessing the error had to do with the amount of files inside certain directories.",
      "question_score": 23,
      "answer_score": 26,
      "created_at": "2023-11-09T11:20:09",
      "url": "https://stackoverflow.com/questions/77452312/docker-failed-to-solve-canceled-context-canceled-when-loading-build-context"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 68626017,
      "title": "docker compose psql: error: FATAL: role &quot;postgres&quot; does not exist",
      "problem": "I faced a problem when I try to use psql command with my docker-compose file on my local Ubuntu machine:\n`psql: error: FATAL:  role \"postgres\" does not exist`\nI tried to use others solution like removing docker image, volume. `psql -U postgres` doesn't work for me either.\nI try to use first `docker-compose up`, then `docker exec -it database bash`\nThere's my docker-compose file\n```\n`services:\n  db:\n    container_name: postgres\n    image: postgres:13.3-alpine\n    restart: always\n    user: postgres\n    environment:\n      - POSTGRES_DB=postgres\n      - POSTGRES_PASSWORD=postgres\n      - POSTGRES_USER=root\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - ./data/db:/var/lib/postgresql/data\n`\n```\nMaybe this string tells something?\n`postgres | PostgreSQL Database directory appears to contain a database; Skipping initialization`\nOUTPUT:\n```\n`Attaching to postgres\npostgres | \npostgres | PostgreSQL Database directory appears to contain a database; Skipping initialization\npostgres | \npostgres | 2021-08-02 17:29:10.426 UTC [1] LOG:  starting PostgreSQL 13.3 on x86_64-pc-linux-musl, compiled by gcc (Alpine 10.3.1_git20210424) 10.3.1 20210424, 64-bit\npostgres | 2021-08-02 17:29:10.426 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\npostgres | 2021-08-02 17:29:10.426 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\npostgres | 2021-08-02 17:29:10.429 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\npostgres | 2021-08-02 17:29:10.433 UTC [12] LOG:  database system was shut down at 2021-08-02 17:22:17 UTC\npostgres | 2021-08-02 17:29:10.438 UTC [1] LOG:  database system is ready to accept connections\npostgres | 2021-08-02 17:37:53.452 UTC [33] FATAL:  role \"postgres\" does not exist\npostgres | 2021-08-02 17:37:56.958 UTC [35] FATAL:  role \"user\" does not exist\npostgres | 2021-08-02 17:41:54.294 UTC [45] FATAL:  role \"postgres\" does not exist```\n`\n```",
      "solution": "First, you've set `POSTGRES_USER` to `root`, so you're going to have a `root` user instead of `postgres` user.\nSecond, if a database already exists, it doesn't matter what you set for `POSTGRES_USER` and `POSTGRES_PASSWORD` -- postgres will use whatever is in the database.\nSo you can either:\n\nDelete the database (`rm -rf data/db`) and start over, or\nEdit your `pg_hba.conf` so that you don't need a password",
      "question_score": 23,
      "answer_score": 16,
      "created_at": "2021-08-02T19:53:26",
      "url": "https://stackoverflow.com/questions/68626017/docker-compose-psql-error-fatal-role-postgres-does-not-exist"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 67851351,
      "title": "Cannot install additional requirements to apache airflow",
      "problem": "I am using the following docker-compose image, I got this image from: https://github.com/apache/airflow/blob/main/docs/apache-airflow/start/docker-compose.yaml\n`version: \"3\"\nx-airflow-common: &airflow-common\n  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.0.0-python3.7}\n  environment: &airflow-common-env\n    AIRFLOW__CORE__EXECUTOR: CeleryExecutor\n    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow\n    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0\n    AIRFLOW__CORE__FERNET_KEY: \"\"\n    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: \"true\"\n    AIRFLOW__CORE__LOAD_EXAMPLES: \"false\"\n    AIRFLOW__API__AUTH_BACKEND: \"airflow.api.auth.backend.basic_auth\"\n    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-apache-airflow-providers-apache-spark}\n  volumes:\n    - ./dags:/opt/airflow/dags\n    - ./logs:/opt/airflow/logs\n    - ./plugins:/opt/airflow/plugins\n  user: \"${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-50000}\"\n  depends_on: &airflow-common-depends-on\n    redis:\n      condition: service_healthy\n    postgres:\n      condition: service_healthy\n\nservices:\n  postgres:\n    image: postgres:13\n    environment:\n      POSTGRES_USER: airflow\n      POSTGRES_PASSWORD: airflow\n      POSTGRES_DB: airflow\n    volumes:\n      - postgres-db-volume:/var/lib/postgresql/data\n    healthcheck:\n      test: [\"CMD\", \"pg_isready\", \"-U\", \"airflow\"]\n      interval: 5s\n      retries: 5\n    restart: always\n\n  redis:\n    image: redis:latest\n    ports:\n      - 6379:6379\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n      interval: 5s\n      timeout: 30s\n      retries: 50\n    restart: always\n\n  airflow-webserver:\n    \nI am trying to run the following DAG:\n`from airflow import DAG\nfrom airflow.providers.http.sensors.http import HttpSensor\nfrom airflow.sensors.filesystem import FileSensor\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\n\nfrom airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\nfrom airflow.operators.email import EmailOperator\n\nfrom datetime import datetime, timedelta\nimport csv\nimport requests\nimport json\n\ndefault_args = {\n    \"owner\": \"airflow\",\n    \"email_on_failure\": False,\n    \"email_on_retry\": False,\n    \"email\": \"admin@localhost.com\",\n    \"retries\": 1,\n    \"retry_delay\": timedelta(minutes=5),\n}\n\ndef printar():\n    print(\"success!\")\n\nwith DAG(\n    \"forex_data_pipeline\",\n    start_date=datetime(2021, 1, 1),\n    schedule_interval=\"@daily\",\n    default_args=default_args,\n    catchup=False,\n) as dag:\n\n    downloading_rates = PythonOperator(task_id=\"test1\", python_callable=printar)\n\n    forex_processing = SparkSubmitOperator(\n        task_id=\"spark1\",\n        application=\"/opt/airflow/dags/test.py\",\n        conn_id=\"spark_conn\",\n        verbose=False,\n    )\n\n    downloading_rates  >> forex_processing\n`\nBut I see this error in the airflow ui:\n`Broken DAG: [/opt/airflow/dags/dag_spark.py] Traceback (most recent call last):\n  File \"\", line 219, in _call_with_frames_removed\n  File \"/opt/airflow/dags/dag_spark.py\", line 7, in \n    from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\nModuleNotFoundError: No module named 'airflow.providers.apache'\n`\nI have specified to install additional requirements in the docker-compose file:\n```\n`_PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-apache-airflow-providers-apache-spark}\n`\n```\nI am writing it wrong? how I should specify the additional requirements I want to install in airflow? can I pass a requirements.txt? if so, how I specify the path?",
      "solution": "Support for `_PIP_ADDITIONAL_REQUIREMENTS` environment variable has not been released yet. It is only supported by the developer/unreleased version of the docker image. It is planned that this feature will be available in Airflow 2.1.1. For more information, see: Adding extra requirements for build and runtime of the PROD image.\nFor the older version, you should build a new image and set this image in the `docker-compose.yaml`. To do this, you need to follow a few steps.\n\nCreate a new `Dockerfile` with the following content:\n```\n`FROM apache/airflow:2.0.0\nRUN pip install --no-cache-dir apache-airflow-providers\n`\n```\n\nBuild a new image:\n`docker build . --tag my-company-airflow:2.0.0\n`\n\nSet this image in `docker-compose.yaml` file:\n`echo \"AIRFLOW_IMAGE_NAME=my-company-airflow:2.0.0\" >> .env\n`\n\nFor more information, see:\nOfficial guide about running Airflow in docker-compose environment\nIn particular, I recommend this fragment which describes what to do as you need to install a new pip package.\n\nModuleNotFoundError: No module named 'XYZ'\nThe Docker Compose file uses the latest Airflow image (apache/airflow). If you need to install a new Python library or system library, you can customize and extend it.\n\nI recommend you check out the guide about building Docker Image. This explains how to install even more complex dependencies.\nI also recommend only using Docker-compose files from the official website and intended for a specific version. Docker-compose files from newer versions may not work with older versions of Airflow, because we are making many improvements to these files all the time to improve stability reliability, and user experience.",
      "question_score": 21,
      "answer_score": 12,
      "created_at": "2021-06-05T17:53:00",
      "url": "https://stackoverflow.com/questions/67851351/cannot-install-additional-requirements-to-apache-airflow"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 71503871,
      "title": "Laravel error: Laravel Sail no such File or directory found",
      "problem": "I am following this tutorial,\nI installed Docker and WSL2(Ubuntu 20.04.4 LTS) on my windows system, as shown in image below,\n\nWhen i am trying to run Laravel project using command,\n```\n`./vendor/bin/sail up\n`\n```\nWhy i am getting error no such file or directory found?",
      "solution": "Instead of:\n`./vendor/bin/sail up \u274c\n`\nUse this:\n`bash ./vendor/laravel/sail/bin/sail up \u2705\n`",
      "question_score": 20,
      "answer_score": 55,
      "created_at": "2022-03-16T21:27:53",
      "url": "https://stackoverflow.com/questions/71503871/laravel-error-laravel-sail-no-such-file-or-directory-found"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 66195113,
      "title": "How to add phpmyadmin to laravel 8 sail docker-compose.yml",
      "problem": "I'm trying to add `phpmyadmin` to laravel 8 `docker-compose.yml` file.\nNow, I can access `phpmyadmin` on \"http://localhost:8080\" but the user is not got Cannot log:\n```\n`Cannot log in to the MySQL server\n\nmysqli::real_connect(): php_network_getaddresses: getaddrinfo failed: Name or service not known\n\nmysqli::real_connect(): (HY000/2002): php_network_getaddresses: getaddrinfo failed: Name or service not known\n`\n```\n\nMy `docker-compose.yml` file looks like this:\n```\n`# For more information: https://laravel.com/docs/sail\nversion: \"3\"\nservices:\n  laravel.test:\n    build:\n      context: ./vendor/laravel/sail/runtimes/8.0\n      dockerfile: Dockerfile\n      args:\n        WWWGROUP: \"${WWWGROUP}\"\n    image: sail-8.0/app\n    ports:\n      - \"${APP_PORT:-80}:80\"\n    environment:\n      WWWUSER: \"${WWWUSER}\"\n      LARAVEL_SAIL: 1\n    volumes:\n      - \".:/var/www/html\"\n    networks:\n      - sail\n    depends_on:\n      - mysql\n      # - pgsql\n      - redis\n      # - selenium\n  # selenium:\n  #     image: 'selenium/standalone-chrome'\n  #     volumes:\n  #         - '/dev/shm:/dev/shm'\n  #     networks:\n  #         - sail\n  mysql:\n    image: \"mysql:8.0\"\n    ports:\n      - \"${FORWARD_DB_PORT:-3306}:3306\"\n    environment:\n      MYSQL_ROOT_PASSWORD: \"${DB_PASSWORD}\"\n      MYSQL_DATABASE: \"${DB_DATABASE}\"\n      MYSQL_USER: \"${DB_USERNAME}\"\n      MYSQL_PASSWORD: \"${DB_PASSWORD}\"\n      MYSQL_ALLOW_EMPTY_PASSWORD: \"yes\"\n    volumes:\n      - \"sailmysql:/var/lib/mysql\"\n    networks:\n      - sail\n    healthcheck:\n      test: [\"CMD\", \"mysqladmin\", \"ping\"]\n  #    pgsql:\n  #        image: postgres:13\n  #        ports:\n  #            - '${FORWARD_DB_PORT:-5432}:5432'\n  #        environment:\n  #            PGPASSWORD: '${DB_PASSWORD:-secret}'\n  #            POSTGRES_DB: '${DB_DATABASE}'\n  #            POSTGRES_USER: '${DB_USERNAME}'\n  #            POSTGRES_PASSWORD: '${DB_PASSWORD:-secret}'\n  #        volumes:\n  #            - 'sailpostgresql:/var/lib/postgresql/data'\n  #        networks:\n  #            - sail\n  redis:\n    image: \"redis:alpine\"\n    ports:\n      - \"${FORWARD_REDIS_PORT:-6379}:6379\"\n    volumes:\n      - \"sailredis:/data\"\n    networks:\n      - sail\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n  # memcached:\n  #     image: 'memcached:alpine'\n  #     ports:\n  #         - '11211:11211'\n  #     networks:\n  #         - sail\n  mailhog:\n    image: \"mailhog/mailhog:latest\"\n    ports:\n      - \"${FORWARD_MAILHOG_PORT:-1025}:1025\"\n      - \"${FORWARD_MAILHOG_DASHBOARD_PORT:-8025}:8025\"\n    networks:\n      - sail\n\n  phpmyadmin:\n    image: phpmyadmin/phpmyadmin\n    links:\n      - mysql:mysql\n    ports:\n      - 8080:80\n    environment:\n      MYSQL_USERNAME: \"${DB_USERNAME}\"\n      MYSQL_ROOT_PASSWORD: \"${DB_PASSWORD}\"\n      PMA_HOST: mysql\n\nnetworks:\n  sail:\n    driver: bridge\nvolumes:\n  sailmysql:\n    driver: local\n  #    sailpostgresql:\n  #        driver: local\n  sailredis:\n    driver: local\n\n`\n```",
      "solution": "For containers to communicate with each other, they have to be on the same Docker network.  You've explicitly assigned the `mysql` container to `networks: [sail]`, but the `phpmyadmin` container isn't on that network.  You can add\n`services:\n  phpmyadmin:\n    networks:\n      - sail\n`\nCompose also provides a network named `default` for you (see Networking in Compose for more details).  If you don't explicitly specify `networks:` for a service then it will be on the `default` network.  Another solution could be to just delete all of the `networks:` blocks everywhere in the file to let every container be on the `default` network.\n`links:` only are used with an obsolete form of Docker networking, and you can delete that part of the file as well.",
      "question_score": 20,
      "answer_score": 31,
      "created_at": "2021-02-14T12:48:29",
      "url": "https://stackoverflow.com/questions/66195113/how-to-add-phpmyadmin-to-laravel-8-sail-docker-compose-yml"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 71946026,
      "title": "Docker: Error response from daemon: unauthorized: The client does not have permission for manifest",
      "problem": "My company uses Artifactory to store it's artifacts and I was getting this error when I tried to pull down the image.\nWhen I run `docker compose up -d` I got the error, `Error response from daemon: unauthorized: The client does not have permission for manifest`\nI have no idea what to do with this. I was directed to this article but it didn't do anything: K8s Image Pull from Private Artifactory\nand\nhttps://github.com/kubernetes-sigs/kustomize/issues/1420\nBut still didn't work",
      "solution": "It turned out I needed to run the command `docker login -u your-username@your-domain.com your-company-or-project-docker.jfrog.io`\nThen you put in your Artifactory apikey as the password.",
      "question_score": 20,
      "answer_score": 18,
      "created_at": "2022-04-20T22:47:04",
      "url": "https://stackoverflow.com/questions/71946026/docker-error-response-from-daemon-unauthorized-the-client-does-not-have-permi"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 68308209,
      "title": "docker-compose, failed to solve: rpc error: code = Unknown desc = failed to compute cache key: &quot;/app/package.json&quot; not found: not found",
      "problem": "I have a problem with pathways `docker-compose`, when I try build project with only `docker build`, it works great, but I mustn't use `docker build`, I have to use `docker-compose`. When I use `docker-compose` it returns 2 ERRORS at step 3/5 `=> ERROR [3/5] COPY /app/package.json .`  and at step 5/5 `=> ERROR [5/5] COPY /app .`:\n```\n`PS C:\\Users\\mamba\\Desktop\\project-practice> docker-compose -f docker/docker-compose.yml up -d\n[+] Building 1.4s (9/9) FINISHED\n => [internal] load build definition from Dockerfile                                                                                                          0.1s \n => => transferring dockerfile: 31B                                                                                                                           0.0s \n => [internal] load .dockerignore                                                                                                                             0.1s \n => => transferring context: 34B                                                                                                                              0.0s \n => [internal] load metadata for docker.io/library/node:latest                                                                                                1.0s \n => [1/5] FROM docker.io/library/node@sha256:c3356b2b11ad643852a321308c15d70ca2bc106e40d3ffe7a4879d3588a9d479                                                 0.0s \n => [internal] load build context                                                                                                                             0.1s \n => => transferring context: 2B                                                                                                                               0.0s \n => CACHED [2/5] WORKDIR /app                                                                                                                                 0.0s \n => ERROR [3/5] COPY /app/package.json .                                                                                                                      0.0s \n => CACHED [4/5] RUN npm install                                                                                                                              0.0s \n => ERROR [5/5] COPY /app .                                                                                                                                   0.0s \n------\n > [3/5] COPY /app/package.json .:\n------\n------\n > [5/5] COPY /app .:\n------\nfailed to solve: rpc error: code = Unknown desc = failed to compute cache key: \"/app/package.json\" not found: not found\n`\n```\nthis is my project structure\nhttp://skrinshoter.ru/s/080721/upY64zwf\nthis is my Dockerfile\n```\n`FROM node\nWORKDIR /app\nCOPY /app/package.json .\nRUN npm install\nCOPY /app .\nEXPOSE 3000\nCMD [\"npm\", \"start\"]\n`\n```\nthis is my docker-compose.yml\n```\n`version: \"3.8\"\nservices: \n    react-app:\n        working_dir: /app\n        build: \n            dockerfile: Dockerfile\n        ports: \n            - \"3000:3000\"\n        volumes: \n            - ./app/src:/app/src\n        environment: \n            - CHOKIDAR_USEPOLLING=true\n        # env_file: \n        #     - ./docker/.env\n`\n```\nIf I move docker-compose.yml upper in structure of files to `project-practice`, it works great, it builds and server starts, but I have to keep structure of folders and files like this.\n```\n`|-project-practice\n|-app\n|  |...\n|-docker\n   |...\n`\n```",
      "solution": "Looks like you mounting volumes wrong.\nChange your docker-compose configuration from:\n```\n`build: \n     dockerfile: Dockerfile\nvolumes:\n     - ./app/src:/app/src     \n`\n```\nYou mounting only the SRC folder, but you need files outside of it. Also you need to add context to your docker file\nto:\n```\n`build: \n     context: ../\n     dockerfile: /docker/Dockerfile\nvolumes:\n     - ../app:/app\n`\n```\nThe path should be relative to a docker-compose file location.\nAlso you need to modify Dockerfile:\n```\n`FROM node\nWORKDIR /app\nCOPY /app/package.json .\nRUN npm install\nCOPY /app .\nEXPOSE 3000\nCMD [\"npm\", \"start\"]\n`\n```",
      "question_score": 20,
      "answer_score": 9,
      "created_at": "2021-07-08T22:23:04",
      "url": "https://stackoverflow.com/questions/68308209/docker-compose-failed-to-solve-rpc-error-code-unknown-desc-failed-to-comp"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 68776387,
      "title": "Docker - library initialization failed - unable to allocate file descriptor table - out of memory",
      "problem": "I have been try to run Zookeeper and Kafka on Docker container.\nI got a lot of errors `[error occurred during error reporting , id 0xb]` and `[Too many errors, abort]` in my terminal. And then `library initialization failed - unable to allocate file descriptor table - out of memory`.\nI use the following docker-compose.yml file\n`version: \"3.8\"\nservices:\n  zookeeper:\n    image: confluentinc/cp-zookeeper:5.5.4\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n      ZOOKEEPER_TICK_TIME: 2000\n  kafka:\n    image: confluentinc/cp-kafka:5.5.4\n    # If you want to expose these ports outside your dev PC,\n    # remove the \"127.0.0.1:\" prefix\n    ports:\n      - 127.0.0.1:9092:9092\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n`",
      "solution": "This problem is caused by having too many file handles available to the program.  It increases its soft limit on files to the maximum available, then attempts to allocate memory for each of those file handles.  For some systems, with high limits, this causes insufficent memory.\nIt can be fixed by overriding the command arguments of `ExecStart` in docker.service\n`sudo systemctl edit docker\n`\nand then enter\n```\n`[Service]\nExecStart=\nExecStart=/usr/bin/dockerd --default-ulimit nofile=65536:65536 -H fd://\n`\n```\nSave and run `sudo systemctl daemon-reload` and `sudo systemctl restart docker`.",
      "question_score": 19,
      "answer_score": 31,
      "created_at": "2021-08-13T19:42:56",
      "url": "https://stackoverflow.com/questions/68776387/docker-library-initialization-failed-unable-to-allocate-file-descriptor-tabl"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 72446973,
      "title": "unknown shorthand flag: &#39;d&#39; in -d docker compose",
      "problem": "I am working with a docker compose. when a trying to run docker compose in background, but it shows error unknown shorthand flag: 'd' in -d\nI am tried in this way\n```\n`docker compose -d up\n`\n```\ndocker-compose.yml\n```\n`version: '3'\n\nnetworks:\n  loki:\n\nservices:\n  loki:\n    image: grafana/loki:2.5.0\n    # volumes:\n    #   - ./loki:/loki\n    ports:\n      - 3100:3100\n    networks:\n      - loki\n  \n  promtail:\n    image: grafana/promtail\n    volumes:\n      - ./promtail:/etc/promtail\n      - /var/log/nginx/:/var/log/nginx/\n    command: -config.file=/etc/promtail/promtail-config.yml\n    ports:\n      - 9080:9080\n    networks:\n      - loki\n  \n  grafana:\n    image: grafana/grafana\n    ports:\n      - 3000:3000\n    networks:\n      - loki\n    \n  \n`\n```",
      "solution": "`-d` is an option of subcommand `up`.\nif you run `docker compose up --help` you will have more information.\nTo solve the problem run `docker compose up -d`",
      "question_score": 19,
      "answer_score": 18,
      "created_at": "2022-05-31T13:27:16",
      "url": "https://stackoverflow.com/questions/72446973/unknown-shorthand-flag-d-in-d-docker-compose"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 78769459,
      "title": "Command not found: docker-compose after docker 4.32.0 update",
      "problem": "After updating Docker on my mac to 4.32.0, `docker-compose` seems to not work anymore. Everything with docker works fine, all the docker commands work fine. I can also use `docker run` to start any container. However, `docker-compsoe` and all its commands don't seem to work.\nWhat could be issue? Could it be that `docker-compose` needs to be separately installed, or any configuration that needs to be done? Or is it deprecated?",
      "solution": "Docker version: `4.32.0` need to remove the dash from `docker-compose` to `docker compose`\n```\n`$ docker compose up --build\n`\n```\nor\n```\n`$ docker compose up -d\n`\n```\nIf not work, you can try this: \nDocker > Settings > Advanced > `select` User `then` Apply and Restart\n\nand use `docker compose`\n - Hope it will work.\nOn my system `macOS 14.5` & `Docker 4.32.0` \nit's work on default docker settings (Docker > Settings > Advanced > System)\n - using `docker compose`\n \nAlso in the compose file `docker-compose.yml` if you are using `version: 'x.y'`\n you will get this warning `version is obsolete`\n need to `remove` the first line (`version: 'x.y'`) of the file, it\u2019s now `deprecated`.",
      "question_score": 19,
      "answer_score": 33,
      "created_at": "2024-07-19T14:48:33",
      "url": "https://stackoverflow.com/questions/78769459/command-not-found-docker-compose-after-docker-4-32-0-update"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 68653051,
      "title": "using docker-compose without sudo doesn&#39;t work",
      "problem": "I was recently told that running `docker` or `docker-compose` with sudo is a big nono, and that I had to create/add my user to the `docker` group in order to run `docker` and  `docker-compose` commands without `sudo`. Which I did, as per the documentation here\nNow, `docker` runs normally via my user. e.g. :\n```\n`~$ docker run hello-world\n\nUnable to find image 'hello-world:latest' locally\nlatest: Pulling from library/hello-world\nb8dfde127a29: Pull complete\nDigest: sha256:df5f5184104426b65967e016ff2ac0bfcd44ad7899ca3bbcf8e44e4461491a9e\nStatus: Downloaded newer image for hello-world:latest\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n 1. The Docker client contacted the Docker daemon.\n 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (amd64)\n 3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n 4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n $ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\n https://hub.docker.com/\n\nFor more examples and ideas, visit:\n https://docs.docker.com/get-started/\n\n`\n```\nBut when I try to run docker-compose, I get a `Permission Denied`\n```\n`~$ docker-compose --help\n\n-bash: /usr/local/bin/docker-compose: Permission denied\n`\n```\nCould you please explain how this works ? I thought having a `docker` group enabled the usage of these commands because the binaries belong to this group, but actually they don't, they only belong to `root`...\n```\n`~$ ls -al /usr/bin/docker*\n-rwxr-xr-x 1 root root  71706288 Jul 23 19:36 /usr/bin/docker\n-rwxr-xr-x 1 root root    804408 Jul 23 19:36 /usr/bin/docker-init\n-rwxr-xr-x 1 root root   2944247 Jul 23 19:36 /usr/bin/docker-proxy\n-rwxr-xr-x 1 root root 116375640 Jul 23 19:36 /usr/bin/dockerd\n`\n```\n```\n`~$ ls -al /usr/local/bin/\ntotal 12448\ndrwxr-xr-x  2 root root     4096 May 26 11:08 .\ndrwxr-xr-x 10 root root     4096 May 14 19:36 ..\n-rwxr--r--  1 root root 12737304 May 26 11:08 docker-compose\n`\n```\nSo, how does this work?\nAnd how do I enable `docker-compose` to run for users that belong to the `docker` group?",
      "solution": "`sudo chmod a+x /usr/local/bin/docker-compose\n`\n\nAs of Jun 2023, the `docker-compose` command has been deprecated in favor of the compose plugin.\nhttps://docs.docker.com/compose/install/linux/\n\nInstall the Compose plugin\n\n```\n`sudo apt-get update\nsudo apt-get install docker-compose-plugin\n\n...\n\ndocker compose ps # note that docker and compose are now two words.\n`\n```\n\nWill turn your permissions on.\n`docker-compose` is just a wrapper, and it uses an external docker daemon, the same way the `docker` command doesn't actually run anything but gives an order to a docker daemon.\nYou can change the docker daemon you communicate with using the `DOCKER_HOST` variable. By default, it is empty ; and when it is empty, both `docker` and `docker-compose` assume it is located at `/var/run/docker.sock`\nAccording to the dockerd documentation :\n\nBy default, a unix domain socket (or IPC socket) is created at /var/run/docker.sock, requiring either root permission, or docker group membership.\n\nAnd this is enforced by giving read and write access to the docker group to the socket.\n`$ ls -l /var/run/docker.sock \nsrw-rw---- 1 root docker 0 nov.  15 19:54 /var/run/docker.sock\n`\nAs described in https://docs.docker.com/engine/install/linux-postinstall/, to add an user to the `docker` group, you can do it like that :\n`sudo usermod -aG docker $USER # this adds the permissions\nnewgrp docker # this refreshes the permissions in the current session\n`\n\nThat being said, using `docker` with `sudo` is the same as using it with the `docker` group, because giving acces to the `/var/run/docker.sock` is equivalent to giving full root acces:\nFrom https://docs.docker.com/engine/install/linux-postinstall/\n\nThe docker group grants privileges equivalent to the root user. For details on how this impacts security in your system, see Docker Daemon Attack Surface.\n\nIf root permission is a security issue for your system, another page is mentioned :\n\nTo run Docker without root privileges, see Run the Docker daemon as a non-root user (Rootless mode).\n\ndocker is composed of multiple elements : https://docs.docker.com/get-started/overview/\nFirst, there are clients :\n`$ type docker\ndocker is /usr/bin/docker\n$ dpkg -S /usr/bin/docker\ndocker-ce-cli: /usr/bin/docker\n`\nYou can see that the `docker` command is installed when you install the `docker-ce-cli` package.\nHere, ce stands for community edition.\nThe `docker` cli communicates with the docker daemon, also known as `dockerd`.\n`dockerd` is a daemon (a server) and exposes by default the unix socket `/var/run/docker.sock` ; which default permissions are `root:docker`.\nThere are other components involved, for instance `dockerd` uses `containerd` : https://containerd.io/\n\nThe rest is basic linux permission management :\n\noperating the docker daemon is the same as having root permission on that machine.\nto operate the docker daemon, you need to be able to read and write from and to the socket it listens to ; in your case it is `/var/run/docker.sock`. whether or not you are a sudoer does not change anything to that.\nto be able to read and write to and from `/var/run/docker.sock`, you must either be `root` or being in the `docker` group.\n`docker-compose` is another cli it has the same requirements as `docker`.",
      "question_score": 18,
      "answer_score": 30,
      "created_at": "2021-08-04T16:27:31",
      "url": "https://stackoverflow.com/questions/68653051/using-docker-compose-without-sudo-doesnt-work"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 71060072,
      "title": "Docker Compose: depends_on with condition -&gt; invalid type, should be an array",
      "problem": "I have the following compose file:\n`version: \"3\"\n\nservices:\n\n  zookeeper:\n    image: docker-dev.art.intern/wurstmeister/zookeeper:latest\n    ports:\n      - 2181:2181\n\n  kafka:\n    image: docker-dev.art.intern/wurstmeister/kafka:latest\n    ports:\n      - 9092:9092\n    environment:\n      - KAFKA_LISTENERS=PLAINTEXT://:9092\n      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092\n      - KAFKA_ADVERTISED_HOST_NAME=kafka\n      - KAFKA_ADVERTISED_PORT=9092\n      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181\n    depends_on:\n      - zookeeper\n\n  app:\n    build:\n      context: ./\n      dockerfile: app/Dockerfile\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:4020/actuator/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n    depends_on:\n      - kafka\n      - zookeeper\n\n  app-test:\n    build:\n      context: ./\n      dockerfile: test/Dockerfile\n    depends_on:\n      app:\n        condition: service_healthy\n`\nAs you can see im implementing a healthcheck for the app and I use `service_healthy` condition.\nBut that leads to the error:\n```\n`The Compose file '.\\docker-compose.yml' is invalid because:\nservices.app-test.depends_on contains an invalid type, it should be an array\n`\n```\nIs there a way to fix that issue?\nIf I change to array sanytax:\n`...\n\n  app-test:\n    build:\n      context: ./\n      dockerfile: test/Dockerfile\n    depends_on:\n      - app:\n          condition: service_healthy\n\n`\nThe error changes to:\n```\n`The Compose file '.\\docker-compose.yml' is invalid because:\nservices.app-test.depends_on contains an invalid type, it should be a string\n`\n```",
      "solution": "This appears to have been removed in version 3 of the docker compose specification, but then re-introduced in version 3.9.\nSee https://github.com/compose-spec/compose-spec/blob/master/spec.md#long-syntax-1\nNote that this seems to require Compose V2, which is executed as `docker compose` on the latest docker binary.\nSee https://docs.docker.com/compose/#compose-v2-and-the-new-docker-compose-command",
      "question_score": 18,
      "answer_score": 17,
      "created_at": "2022-02-10T06:08:34",
      "url": "https://stackoverflow.com/questions/71060072/docker-compose-depends-on-with-condition-invalid-type-should-be-an-array"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 73612978,
      "title": "Docker - No matching manifest for linux/arm64/v8 in the manifest list entries - Ubuntu 22.04/arm64/v8",
      "problem": "I have installed Docker and docker compose and tested it and i got the \u201cHello World\u201d message, so everything works fine\nAfter that i tried to install a Node.js backend, but idk why i keep getting this error message\n\n\" no matching manifest for linux/arm64/v8 in the manifest list entries\n\"\n\ni have a VPS server, 4 CPU, 24 RAM, running Ubuntu 22.04, ARM64\nIdk what is the problem and what shall i do to fix it!\nSomeone in the docker community said:\n\nThat image does not have a compatible version with your CPU. You can\ntry to use QEMU to emulate it.\nhttps://www.stereolabs.com/docs/docker/building-arm-container-on-x86/\n\n```\n`sudo apt-get install qemu binfmt-support qemu-user-static\ndocker run --platform linux/amd64 ...\n`\n```\n\nThis way you can use the AMD64 version, but the emulation is not\nalways perfect, and it may be slower then running a container from a\ncompatible image.\n\nI installed the qemu, but still don't know what shall i do to fix the no matching manifest issue!\nnote: i'm not familiar with docker stuff, just trying to install the Node.js backend website because it's requiring docker.\nI hope if someone can help, Thanks!",
      "solution": "Ok .. here is the solution\nopen your docker-compose.yml, if you're using nano then\n```\n`nano docker-compose.yml\n`\n```\nNow add the following:\n```\n`platform: linux/amd64\n`\n```\nfor each MyService\nExample:\n```\n`services:\n   myservice:\n      platform: linux/amd64\n   myotherservice:\n      platform: linux/amd64\n`\n```\nthen you can run:\n```\n`docker compose up -d\n`\n```\nThis works 100% for me, and big thanks to \u00c1kos Tak\u00e1cs for his help.",
      "question_score": 17,
      "answer_score": 46,
      "created_at": "2022-09-05T19:38:39",
      "url": "https://stackoverflow.com/questions/73612978/docker-no-matching-manifest-for-linux-arm64-v8-in-the-manifest-list-entries"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 67819391,
      "title": "Docker Compose network_mode and port_binding compatibility issue",
      "problem": "My docker-compose.yml contains this:\n```\n`version: '3.2'\nservices:\n  mysql:\n    image: mysql:latest\n    container_name: mysql\n    restart: always\n    network_mode: \"host\"\n    hostname: localhost\n    environment:\n      MYSQL_ROOT_PASSWORD: root\n      MYSQL_ALLOW_EMPTY_PASSWORD: \"yes\"\n    volumes:\n      - $HOME/data/datasql:/var/lib/mysql\n    ports:\n      - 3306:3306\n\n  user-management-service:\n    build: user-management-service/\n    container_name: user-management-service\n    restart: always\n    depends_on:\n      - mysql\n      - rabbitmq\n      - eureka\n    network_mode: \"host\"\n    hostname: localhost\n    ports:\n      - 8089:8089\n`\n```\nWhen I try to do docker-compose up, I get the following error:\n```\n`\"host\" network_mode is incompatible with port_bindings\n`\n```\nCan anyone help me with the solution?",
      "solution": "`network_mode: host` is almost never necessary.  For straightforward servers, like the MySQL server you show or what looks like a normal HTTP application, it's enough to use normal (bridged) Docker networking and `ports:`, like you show.\nIf you do set up host networking, it completely disables Docker's networking stack.  You can't call to other containers using their host name, and you can't remap a container's port using `ports:` (or choose to not publish it at all).\nYou should delete the `network_mode:` lines you show in your `docker-compose.yml` file.  The `container_name:` and `hostname:` lines are also unnecessary, and you can delete those too (specific exception: RabbitMQ needs a fixed `hostname:`).\nI feel like the two places I see host networking are endorsed are either to call back to the host machine (see From inside of a Docker container, how do I connect to the localhost of the machine?), or because the application code has hard-coded `localhost` as the host name of the database or other components (in which case Docker and a non-Docker development setup fundamentally act differently, and you should configure these locations using environment variable or another mechanism).",
      "question_score": 17,
      "answer_score": 17,
      "created_at": "2021-06-03T12:02:04",
      "url": "https://stackoverflow.com/questions/67819391/docker-compose-network-mode-and-port-binding-compatibility-issue"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 72909192,
      "title": "docker-compose fork/exec permission denied",
      "problem": "Last night I updated Docker desktop to the latest version 4.10.1 and today when I tried to run my containers, I get this error:\n\n```\n`Cannot start Docker Compose application. Reason: fork/exec [docker-app-path]/bin/docker-compose-v1: permission denied\n`\n```\nI'm on a Mac with M1.\nI checked some issues, where people suggest adding the command `RUN chmod 777 /root` to my Dockerfile, but I've got 8 separate apps running, and it's a team-shared repo, so updating the Dockerfiles for this is not the most viable solution.",
      "solution": "I'm having the same issue in my Mac with M1. It seems like an issue with the latest version of Docker Desktop, which is 4.10.1 (82475).\nDowngrade to Docker Desktop 4.9.1:\nhttps://docs.docker.com/desktop/release-notes/#docker-desktop-491\nOr run your container from Terminal:\n```\n`docker-compose up\n`\n```",
      "question_score": 17,
      "answer_score": 15,
      "created_at": "2022-07-08T11:13:28",
      "url": "https://stackoverflow.com/questions/72909192/docker-compose-fork-exec-permission-denied"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 72280771,
      "title": "How to use secrets when building docker compose locally",
      "problem": "I recently started using Buildkit to hide some env vars, and it worked great in prod by gha!\nMy Dockerfile now is something like this:\n```\n`# syntax=docker/dockerfile:1.2\n...\nRUN --mount=type=secret,id=my_secret,uid=1000 \\\n    MY_SECRET=$(cat /run/secrets/my_secret) \\\n    && export MY_SECRET\n`\n```\nAnd my front was something like this:\n```\n`DOCKER_BUILDKIT=1 docker build \\\n    --secret  id=my_secret,env=\"MY_SECRET\"\n`\n```\nAnd when I run this on my Github actions, it works perfectly.\nBut now, the problem here is when I try to build it locally. When performing a `docker-compose build` it fails. Of course, because I'm not passing in any secret so my backend (Dockerfile) won't be able to read it from `run/secrets/`.\nWhat I've tried to do, so far, to accomplish the local build using `docker-compose build`:\n1. Working with Docker secrets:\nI basically tried doing:\n```\n`$ docker swarm init\n$ echo \"my_secret_value\" docker secret create my_secret -\n`\n```\nI thought that saving a secret would fix the problem but didn't work. I still got the same error message:\n\ncat: can't open '/run/secrets/my_secret': No such file or directory\n\nI also tried passing in the secret on my docker-compose file like the following but didn't work either:\n\n```\n`version: '3'\nservices:\n  app:\n    build:\n      context: \".\"\n      args:\n        - \"MY_SECRET\"\n  secrets:\n      - my_secret\nsecrets:\n  my_secret:\n    external: true\n`\n```\n\nI also tried storing the secret in a local file, but didn't work, the same error:\n\n```\n`version: '3'\nservices:\n  app:\n    build:\n      context: \".\"\n      args:\n        - \"MY_SECRET\"\n  secrets:\n      - my_secret\nsecrets:\n  my_secret:\n    file: ./my_secret.txt\n`\n```\n\nI also tried doing something like this answer something like this:\n\n```\n`args:\n    - secret=id=my_secret,src=./my_secret.txt\n`\n```\nBut still got the same error:\n\ncat: can't open '/run/secrets/my_secret': No such file or directory\n\nWhat am I doing wrong to successfully perform a `docker-compose build`?\nI'm aware that I can easily use two Dockerfiles, a Dockerfile to build in local and a Dockerfile to build in prod but I just want to use Buildkit as it is, by only modifying my `docker-compose.yml` file.\nDoes anyone have an idea about what am I missing to be able to build locally reading from `/run/secrets/`?",
      "solution": "Support for this was recently implemented in v2. See the below pull requests.\n\nhttps://github.com/docker/compose/pull/9386\nhttps://github.com/compose-spec/compose-spec/pull/238\n\nThe provided example looks like this:\n`services:\n  frontend:\n    build: \n      context: .\n      secrets:\n        - server-certificate\nsecrets:\n  server-certificate:\n    file: ./server.cert\n`\nSo you are close, but you have to add the secret key under the build key.\nAlso keep in mind that you have to use `docker compose` instead of `docker-compose`, in order to use v2 which is built into the docker client.",
      "question_score": 16,
      "answer_score": 30,
      "created_at": "2022-05-17T23:18:46",
      "url": "https://stackoverflow.com/questions/72280771/how-to-use-secrets-when-building-docker-compose-locally"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 66197399,
      "title": "Run Redis Insights in Docker Compose",
      "problem": "I'm trying to run Redis Insight in Docker Compose and I always get errors even though the only thing I'm changing from the Docker Run command is the volume. How do I fix this?\ndocker-compose.yml\n```\n`redisinsights:\n  image: redislabs/redisinsight:latest\n  restart: always\n  ports:\n    - '8001:8001'\n  volumes:\n    - ./data/redisinsight:/db\n`\n```\nlogs\n```\n`redisinsights_1  | Process 9 died: No such process; trying to remove PID file. (/run/avahi-daemon//pid)\nredisinsights_1  | Traceback (most recent call last):\nredisinsights_1  |   File \"./entry.py\", line 11, in \nredisinsights_1  |   File \"./startup.py\", line 47, in \nredisinsights_1  |   File \"/usr/local/lib/python3.6/site-packages/django/conf/__init__.py\", line 79, in __getattr__\nredisinsights_1  |     self._setup(name)\nredisinsights_1  |   File \"/usr/local/lib/python3.6/site-packages/django/conf/__init__.py\", line 66, in _setup\nredisinsights_1  |     self._wrapped = Settings(settings_module)\nredisinsights_1  |   File \"/usr/local/lib/python3.6/site-packages/django/conf/__init__.py\", line 157, in __init__\nredisinsights_1  |     mod = importlib.import_module(self.SETTINGS_MODULE)\nredisinsights_1  |   File \"/usr/local/lib/python3.6/importlib/__init__.py\", line 126, in import_module\nredisinsights_1  |     return _bootstrap._gcd_import(name[level:], package, level)\nredisinsights_1  |   File \"./redisinsight/settings/__init__.py\", line 365, in \nredisinsights_1  |   File \"/usr/local/lib/python3.6/os.py\", line 220, in makedirs\nredisinsights_1  |     mkdir(name, mode)\nredisinsights_1  | PermissionError: [Errno 13] Permission denied: '/db/rsnaps'\n`\n```",
      "solution": "Follow the below steps to make it work:\nStep 1. Create a Docker Compose file as shown below:\n```\n`version: '3'\nservices:\n  redis:\n    image: redislabs/redismod\n    ports:\n      - 6379:6379\n  redisinsight:\n    image: redislabs/redisinsight:latest\n    ports:\n      - '5540:5540'\n    volumes:\n      - ./Users/ajeetraina/data/redisinsight:/db \n`\n```\nStep 2. Provide sufficient permission\nGo to Preference under Docker Desktop > File Sharing and add your folder structure which you want to share.\nPlease change the directory structure as per your environment\nStep 3. Execute the Docker compose CLI\n```\n`docker-compose ps\n          Name                        Command               State           Ports         \n------------------------------------------------------------------------------------------\npinegraph_redis_1          redis-server --loadmodule  ...   Up      0.0.0.0:6379->6379/tcp\npinegraph_redisinsight_1   bash ./docker-entry.sh pyt ...   Up      0.0.0.0:5540->5540/tcp\n`\n```\nGo to web browser and open RedisInsight URL.\nEnjoy!",
      "question_score": 16,
      "answer_score": 4,
      "created_at": "2021-02-14T17:21:54",
      "url": "https://stackoverflow.com/questions/66197399/run-redis-insights-in-docker-compose"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 68365844,
      "title": "Docker: Apache in Apple Silicon M1",
      "problem": "I have a `docker-compose.yml` file:\n```\n`version: '1'\nservices:\n  mariadb:\n    image: 'docker.io/bitnami/mariadb:10.3-debian-10'\n    ports:\n      - '3307:3306'\n    volumes:\n      - ./db:/bitnami/mariadb\n    environment:\n      - MARIADB_USER=bn_wordpress\n      - MARIADB_DATABASE=bitnami_wordpress\n      - ALLOW_EMPTY_PASSWORD=yes\n  wordpress:\n    image: 'docker.io/bitnami/wordpress:5-debian-10'\n    ports:\n      - '8081:8080'\n      - '8444:8443'\n    volumes:\n      - ./wp:/bitnami/wordpress\n    depends_on:\n      - mariadb\n    environment:\n      - MARIADB_HOST=mariadb\n      - MARIADB_PORT_NUMBER=3306\n      - WORDPRESS_DATABASE_USER=bn_wordpress\n      - WORDPRESS_DATABASE_NAME=bitnami_wordpress\n      - ALLOW_EMPTY_PASSWORD=yes\n`\n```\nIn Mac (Intel) and Linux, I run `docker-compose up` and it works perfectly.\nBut in Macbook M1, I installed Docker for Apple Silicon chip and updated rosetta, it prompts this at the end:\n```\n`wordpress_1  | wordpress 15:48:36.49 INFO  ==> ** Starting Apache **\nwordpress_1  | [Tue Jul 13 15:48:36.652803 2021] [core:emerg] [pid 1] (95)Operation not supported: AH00023: Couldn't create the mpm-accept mutex \nwordpress_1  | (95)Operation not supported: could not create accept mutex\nwordpress_1  | AH00015: Unable to open logs\n`\n```\nHow can I overcome the issue? Appreciate your help!",
      "solution": "A bit late but have you tried adding `platform: linux/amd64`? Under both `mariadb` and `wordpress`",
      "question_score": 16,
      "answer_score": 4,
      "created_at": "2021-07-13T18:00:10",
      "url": "https://stackoverflow.com/questions/68365844/docker-apache-in-apple-silicon-m1"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 69178211,
      "title": "How do I run multiple sites on the same server using docker and nginx?",
      "problem": "I'm trying to run two sites on django on the same server under different ip, an error occurs that the port is busy, I fixed the ports, but the site does not start. Tell me where is the error please? Ip work, when I go to the second ip I get redirects to the first site. All settings were specified for the second site. At the end, I added the nginx setting of the first site\nThis is the second docker-compose file and its settings. I would be very grateful for your help\n.env\n```\n`#Django\n# Should be one of dev, prod\nMODE=prod\nPORT=8008\n\n#postgres\nDB_NAME=xxx\nDB_USER=xxx\nDB_HOST=xxx\nDB_PASSWORD=xxxx\nDB_PORT=5432\nPOSTGRES_PASSWORD=mysecretpassword\n\n#WSGI\nWSGI_PORT=8008\nWSGI_WORKERS=4\nWSGI_LOG_LEVEL=debug\n\n# Celery\nCELERY_NUM_WORKERS=2\n\n# Email\nEMAIL_HOST_USER=xxxx\nEMAIL_HOST_PASSWORD=xxxx\n`\n```\ndocker-compose.yml\n```\n`version: '3'\n\nservices:\n\n  backend:\n    build: ./\n    container_name: site_container\n    restart: always\n    command: ./commands/start_server.sh\n    ports:\n      - \"${PORT}:${WSGI_PORT}\"\n    volumes:\n      - ./src:/srv/project/src\n      - ./commands:/srv/project/commands\n      - static_content:/var/www/site\n    env_file:\n      - .env\n    depends_on:\n      - postgres\n\n  postgres:\n    image: postgres:12\n    volumes:\n      - pg_data:/var/lib/postgresql/data\n    env_file:\n      - .env\n#    environment:\n#      - DJANGO_SETTINGS_MODULE=app.settings.${MODE}\n\n  nginx:\n    image: nginx:1.19\n    volumes:\n      - ./nginx:/etc/nginx/conf.d\n      - static_content:/var/www/site\n    ports:\n      - 81:80\n      - 444:443\n    env_file:\n      - .env\n    depends_on:\n      - backend\n\nvolumes:\n  pg_data: {}\n  static_content: {}\n`\n```\ndefault.conf\n```\n`server {\n    listen 80 default_server;\n\n    server_name 183.22.332.12;\n\n    location /static/ {\n        root /var/www/site;\n    }\n\n    location /media/ {\n        root /var/www/site;\n    }\n\n    location / {\n        proxy_set_header Host $host;\n        proxy_pass http://backend:8010;\n    }\n}\n`\n```\ndefault.conf for first site\n```\n`server {\n    #listen 80 default_server;\n    listen 443 ssl http2;\n    listen [::]:443 ssl http2;\n\n    server_name site1 ip_site1;\n\n    ssl_certificate /etc/letsencrypt/live/site1/fullchain.pem;\n    ssl_certificate_key /etc/letsencrypt/live/site1/privkey.pem;\n    ssl_trusted_certificate /etc/letsencrypt/live/site1/chain.pem;\n\n    location /static/ {\n        root /var/www/artads;\n    }\n\n    location /media/ {\n        root /var/www/artads;\n    }\n\n    location / {\n        proxy_set_header Host $host;\n        proxy_pass http://backend:8008;\n    }\n}\n\nserver {\n    listen 80 default_server;\n\n    server_name ip_site2 site2;\n\n    location /static/ {\n        root /var/www/gdr_mr;\n    }\n\n    location /media/ {\n        root /var/www/gdr_mr;\n    }\n\n    location / {\n        proxy_set_header Host $host;\n        proxy_pass http://backend:8013;\n    }\n}\n\nserver {\n        listen 80;\n        listen [::]:80;\n\n        server_name www.site1 site1;\n\n        location / {\n                return 301 https://site1$request_uri;\n        }\n}\n`\n```",
      "solution": "Thanks to @Roman Tokaren and @Oleksandr\nHere the english translated version submitted by @Roman Tokaren here\n\nYou can always argue a lot about the \"correct\" launch - after all, how many people, so many opinions, but I will describe an example + - of a \"convenient\" and scalable configuration. For the \"convenience\" of working in such a configuration, I would suggest installing nginxproxymanager as a reverse proxy and combining containers and nginxproxymanager into one network - after which it will be possible to forward container ports via http (s), tcp, udp to an external interface using the GUI as well as a number of other goodies, such as the generation of SSL certificates and their auto renewal\n\nFirst, let's create the network itself\n```\n`docker network create --driver bridge --subnet 172.26.0.0/24 testnet\n`\n```\n\nLet's configure NPM (nginxproxymanager) - by default we will consider the reverse proxy as the last network node, as a result we will get\n```\n`version: \"3\"\nservices:\n  app:\n    image: 'jc21/nginx-proxy-manager:latest'\n    networks:\n      testnet:\n        ipv4_address: 172.26.0.254\n    restart: always\n    ports:\n      # Public HTTP Port:\n      - '80:80'\n      # Public HTTPS Port:\n      - '443:443'\n      # Admin Web Port:\n      - '81:81'\n    environment:\n      # These are the settings to access your db\n      DB_MYSQL_HOST: \"172.26.0.253\"\n      DB_MYSQL_PORT: 3306\n      DB_MYSQL_USER: \"user\"\n      DB_MYSQL_PASSWORD: \"pwd\"\n      DB_MYSQL_NAME: \"npm\"\n    volumes:\n      - ./data/nginx-proxy-manager:/data\n      - ./letsencrypt:/etc/letsencrypt\n    depends_on:\n      - db\n  db:\n    image: yobasystems/alpine-mariadb:latest\n    restart: always\n    networks:\n      testnet:\n        ipv4_address: 172.26.0.253\n    environment:\n      MYSQL_ROOT_PASSWORD: \"pwd\"\n      MYSQL_DATABASE: \"npm\"\n      MYSQL_USER: \"user\"\n      MYSQL_PASSWORD: \"pwd\"\n    volumes:\n      - ./data/mariadb:/var/lib/mysql\nnetworks:\n  testnet:\n    external: true\n`\n```\n\nAnd configure the container itself\n```\n`version: '3'\nservices:\n  backend:\n    build: ./\n    container_name: site_container\n    restart: always\n    command: ./commands/start_server.sh\n    networks:\n      testnet:\n        ipv4_address: 172.26.0.2\n    volumes:\n      - ./src:/srv/project/src\n      - ./commands:/srv/project/commands\n      - static_content:/var/www/site\n    env_file:\n      - .env\n    depends_on:\n      - postgres\n\n  postgres:\n    image: postgres:12\n    volumes:\n      - pg_data:/var/lib/postgresql/data\n    env_file:\n      - .env\n#    environment:\n#      - DJANGO_SETTINGS_MODULE=app.settings.${MODE}\n\nnetworks:\n  testnet:\n    external: true\nvolumes:\n  pg_data: {}\n  static_content: {}\n`\n```\n\nAfter that, we carry out the initial configuration of NPM according to the instructions and add the host",
      "question_score": 16,
      "answer_score": 8,
      "created_at": "2021-09-14T14:56:05",
      "url": "https://stackoverflow.com/questions/69178211/how-do-i-run-multiple-sites-on-the-same-server-using-docker-and-nginx"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 70915151,
      "title": "How to show all running containers created by docker-compose, globally, regardless of docker-compose.yml",
      "problem": "I have several `docker-compose.yml` files, running ok with `docker-compose up`, individually.\nEach docker-compose runs several containers.\nAfter they are up, I can't see what containers are up with `docker ps`.\nI can see something with `docker-compose ps`, but only for a specific docker-compose.yml.\nI want access to global polling of the containers state.\nHow can I list all running containers, no matter their origin?",
      "solution": "Docker compose adds labels to each container that it creates. If you want to get all containers created by compose, you can perform a container ls and apply a filter.\n`docker container ls --filter label=com.docker.compose.project\n`\nThis will show all running container created by compose, regardless of the project name.\nFor example, I created some containers from different compose projects. With the filter, I get only those, but no other container that have not been created by compose and therefore don't have a project label.\n`$ base='{{.Status}}\\t{{.ID}}\\t{{.Names}}\\t{{.Image}}\\t{{.Ports}}\\t{{.Networks}}\\t{{.Mounts}}'\n$ compose='{{.Label \"com.docker.compose.project\"}}\\t{{.Label \"com.docker.compose.service\"}}'\n\n$ docker container ls --all \\\n  --filter label=com.docker.compose.project \\\n  --format \"table $compose\\t$base\"\n\nproject        service     STATUS                      CONTAINER ID   NAMES                IMAGE                   PORTS                                                                     NETWORKS               MOUNTS\nkafka          kafka       Up 5 minutes                3f97a460266e   kafka_kafka_1        bitnami/kafka:3         0.0.0.0:9092->9092/tcp, :::9092->9092/tcp                                 kafka_default          kafka_kafka_da\u2026,kafka_kafa_con\u2026\nkafka          zookeeper   Up 5 minutes                0b6f32ccd196   kafka_zookeeper_1    bitnami/zookeeper:3.7   2888/tcp, 3888/tcp, 0.0.0.0:2181->2181/tcp, :::2181->2181/tcp, 8080/tcp   kafka_default          kafka_zookeepe\u2026\nmanager        db          Up 22 minutes               4f0e799b4fd7   manager_db_1         da2cb49d7a8d            5432/tcp                                                                  manager_default        0d667a0e48a280\u2026\nfoo            db          Exited (0) 37 minutes ago   e106c5cdbf5e   foo_db_1             da2cb49d7a8d                                                                                      foo_default            5a87e93627b8f6\u2026\nfoo            backend     Up 10 minutes               08a0873c0587   foo_backend_2        c316d5a335a5            80/tcp                                                                    foo_default            \nfoo            frontend    Up 10 minutes               be723bf41aeb   foo_frontend_1       c316d5a335a5            80/tcp                                                                    foo_default            \nfoo            backend     Up 10 minutes               5d91d4bcfcb3   foo_backend_1        c316d5a335a5            80/tcp                                                                    foo_default            \nmanager        app         Up 22 minutes               2ca4c0920807   manager_app_1        c316d5a335a5            80/tcp                                                                    manager_default        \nmanager        app         Up 22 minutes               b2fa2b9724b0   manager_app_2        c316d5a335a5            80/tcp                                                                    manager_default        \nloadbalancer   app         Exited (0) 37 minutes ago   791f4059b4af   loadbalancer_app_1   c316d5a335a5                                                                                      loadbalancer_default   \n`\nIf you want to see all container regardless of their state, you can add the `--all` or short `-a` flag to the ls command, like I did in my example. Otherwise, only running containers are shown.",
      "question_score": 15,
      "answer_score": 27,
      "created_at": "2022-01-30T14:36:49",
      "url": "https://stackoverflow.com/questions/70915151/how-to-show-all-running-containers-created-by-docker-compose-globally-regardle"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 68469150,
      "title": "Setting context in docker-compose file for a parent folder",
      "problem": "I got a docker-compose file in which I want to set a context and docker file to look something like this:\n```\n`build:\n  context: \n  dockerfile: \n`\n```\nFor now my file is in the root folder so its simply:\n```\n`build: \n  context: .\n  dockerfile: .\n`\n```\nThis way it does work.\nThe structure of the project is something like this:\n```\n`./\n  - folder1/\n    - folder2/\n         docker-compose.yaml\n         DockerFile\n`\n```\nI want to copy files as part of the commands in the DockerFile and I want the paths to be relative to the root folder of the project.\nHow with this project structure do I set the context to be the root folder of the project? I tried doing `context: ../../` but I then got an error:\n\nError response from daemon: unexpected error reading Dockerfile: read (path): is a directory\n\nHow do I set the context correctly?",
      "solution": "You've set:\n```\n` dockerfile: .\n`\n```\nJust try to use a relative path to you Dockerfile from the set context:\n```\n`context: ../../\ndockerfile: ./folder1/folder2/Dockerfile\n`\n```",
      "question_score": 15,
      "answer_score": 20,
      "created_at": "2021-07-21T14:02:37",
      "url": "https://stackoverflow.com/questions/68469150/setting-context-in-docker-compose-file-for-a-parent-folder"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 71372066,
      "title": "Docker fails to install cffi with python:3.9-alpine in Dockerfile",
      "problem": "Im trying to run the below Dockerfile using docker-compose.\nI searched around but I couldnt find a solution on how to install cffi with python:3.9-alpine.\nI also read this post which states that pip 21.2.4 or greater can be a possible solution but it didn't work out form me\nhttps://www.pythonfixing.com/2021/09/fixed-why-i-getting-this-error-while.html\nDocker file\n```\n`FROM python:3.9-alpine\n\nENV PYTHONDONTWRITEBYTECODE 1\nENV PYTHONUNBUFFERED 1\n\nCOPY ./requirements.txt .\n\nRUN apk add --update --no-cache postgresql-client\n\nRUN apk add --update --no-cache --virtual .tmp-build-deps \\\n    gcc libc-dev linux-headers postgresql-dev\nRUN pip3 install --upgrade pip && pip3 install -r /requirements.txt\n\nRUN apk del .tmp-build-deps\n\nRUN mkdir /app\nWORKDIR /app\nCOPY . /app\n\nRUN adduser -D user\n\nUSER user\n`\n```\nThis is the requirements.txt file.\n```\n`asgiref==3.5.0\nbackports.zoneinfo==0.2.1\ncertifi==2021.10.8\ncffi==1.15.0\ncfgv==3.3.1\n...\n`\n```\nError message:\n```\n`process-exited-with-error\n#9 47.99   \n#9 47.99   \u00d7 Running setup.py install for cffi did not run successfully.\n#9 47.99   \u2502 exit code: 1\n#9 47.99   \u2570\u2500> [58 lines of output]\n#9 47.99       Package libffi was not found in the pkg-config search path.\n#9 47.99       Perhaps you should add the directory containing `libffi.pc'\n#9 47.99       to the PKG_CONFIG_PATH environment variable\n#9 47.99       Package 'libffi', required by 'virtual:world', not found\n#9 47.99       Package libffi was not found in the pkg-config search path.\n#9 47.99       Perhaps you should add the directory containing `libffi.pc'\n#9 47.99       to the PKG_CONFIG_PATH environment variable\n#9 47.99       Package 'libffi', required by 'virtual:world', not found\n#9 47.99       Package libffi was not found in the pkg-config search path.\n#9 47.99       Perhaps you should add the directory containing `libffi.pc'\n#9 47.99       to the PKG_CONFIG_PATH environment variable\n#9 47.99       Package 'libffi', required by 'virtual:world', not found\n#9 47.99       Package libffi was not found in the pkg-config search path.\n#9 47.99       Perhaps you should add the directory containing `libffi.pc'\n#9 47.99       to the PKG_CONFIG_PATH environment variable\n#9 47.99       Package 'libffi', required by 'virtual:world', not found\n#9 47.99       Package libffi was not found in the pkg-config search path.\n#9 47.99       Perhaps you should add the directory containing `libffi.pc'\n#9 47.99       to the PKG_CONFIG_PATH environment variable\n#9 47.99       Package 'libffi', required by 'virtual:world', not found\n#9 47.99       running install\n#9 47.99       running build\n#9 47.99       running build_py\n#9 47.99       creating build\n#9 47.99       creating build/lib.linux-aarch64-3.9\n#9 47.99       creating build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/__init__.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/cffi_opcode.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/commontypes.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/vengine_gen.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/vengine_cpy.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/backend_ctypes.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/api.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/ffiplatform.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/verifier.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/error.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/setuptools_ext.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/lock.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/recompiler.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/pkgconfig.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/cparser.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/model.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/_cffi_include.h -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/parse_c_type.h -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/_embedding.h -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/_cffi_errors.h -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       warning: build_py: byte-compiling is disabled, skipping.\n#9 47.99       \n#9 47.99       running build_ext\n#9 47.99       building '_cffi_backend' extension\n#9 47.99       creating build/temp.linux-aarch64-3.9\n#9 47.99       creating build/temp.linux-aarch64-3.9/c\n#9 47.99       gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -DTHREAD_STACK_SIZE=0x100000 -fPIC -DUSE__THREAD -DHAVE_SYNC_SYNCHRONIZE -I/usr/include/ffi -I/usr/include/libffi -I/usr/local/include/python3.9 -c c/_cffi_backend.c -o build/temp.linux-aarch64-3.9/c/_cffi_backend.o\n#9 47.99       c/_cffi_backend.c:15:10: fatal error: ffi.h: No such file or directory\n#9 47.99          15 | #include \n#9 47.99             |          ^~~~~~~\n#9 47.99       compilation terminated.\n#9 47.99       error: command '/usr/bin/gcc' failed with exit code 1\n#9 47.99       [end of output]\n#9 47.99   \n#9 47.99   note: This error originates from a subprocess, and is likely not a problem with pip.\n#9 47.99 error: legacy-install-failure\n#9 47.99 \n#9 47.99 \u00d7 Encountered error while trying to install package.\n#9 47.99 \u2570\u2500> cffi\n#9 47.99 \n#9 47.99 note: This is an issue with the package mentioned above, not pip.\n#9 47.99 hint: See above for output from the failure.\n`\n```",
      "solution": "@Klaus D.'s comment helped a lot.\nI updated Dockerfile:\n```\n`RUN apk add --update --no-cache --virtual .tmp-build-deps \\\n    gcc libc-dev linux-headers postgresql-dev \\\n    && apk add libffi-dev\n`\n```",
      "question_score": 14,
      "answer_score": 23,
      "created_at": "2022-03-06T17:20:05",
      "url": "https://stackoverflow.com/questions/71372066/docker-fails-to-install-cffi-with-python3-9-alpine-in-dockerfile"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 68206642,
      "title": "How to solve &quot;Can&#39;t separate key from value&quot; in docker",
      "problem": "After upgrading my docker desktop, I get an error when running `docker-compose up`. My usual setup consists of microservices controlled with the `docker-compose` command. When I run `docker-compose up`, all the containers are started. After updating my docker desktop, I get:\n`Can't separate key from value \n\n`\nwhile running `docker-compose up`. How do I fix this?",
      "solution": "Check for the version number of docker and if its 3.4+ then the docker compose v2 is enabled by default. To disable it, go to > docker desktop > preferences > experimental features > un-check \"use Docker Compose V2\" option. This is a move by docker hub to incorporate docker-compose as `docker compose` and may cause problems to your usual workflow. Enjoy :)",
      "question_score": 14,
      "answer_score": 23,
      "created_at": "2021-07-01T10:15:13",
      "url": "https://stackoverflow.com/questions/68206642/how-to-solve-cant-separate-key-from-value-in-docker"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 71863950,
      "title": "Connection Refused from Request Inside Docker Compose",
      "problem": "I have an API running on my host machine on port 8000. Meanwhile, I have a docker compose cluster with one container that's supposed to connect said API. To get the url for the request, I use \"host.docker.internal:8000\" on my windows machine and it works wonderfully. However, I have a linux deployment server and in there, \"host.docker.internal\" doesn't resolve to anything, causing a connection error to the API. I saw on another post on stackoverflow, that you solve this on linux by adding the following on your `docker-compose.yaml`\n`services:\n  service_name:\n    extra_hosts:\n      - host.docker.internal:host-gateway\n`\nThis added the docker0 IP to `/etc/hosts`, but when I try to do a GET request, the resulting message is:\n`Failed to connect to host.docker.internal port 8000: Connection refused`\nI'm really confused right now. I don't know if this is a firewall issue, a docker issue, a docker compose issue, a docker on linux issue. Please help...",
      "solution": "Credit to @Hans Kilian:\n\nAdd `extra_hosts` to docker-compose file\nChange URL to use `host.docker.internal` instead of `localhost`\nChange service to serve on `0.0.0.0` instead of `localhost`",
      "question_score": 14,
      "answer_score": 19,
      "created_at": "2022-04-13T23:20:05",
      "url": "https://stackoverflow.com/questions/71863950/connection-refused-from-request-inside-docker-compose"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 66212390,
      "title": "How can I prevent Docker from removing intermediate containers when executing RUN command?",
      "problem": "The error I'm experiencing is that I want to execute the command \"change directory\" in my Docker machine, but every time I execute RUN instruction in my Dockerfile, it deletes the actual container (intermediate container).\nDOCKERFILE\n\nThis happens when I execute the Dockerfile from above\n\nHow can I prevent Docker from doing that?",
      "solution": "The current paths are different for Dockerfile and RUN (inside container).\nEach RUN command starts from the Dockerfile path (e. g. '/').\nWhen you do `RUN cd /app`, the \"inside path\" changes, but not the \"Dockerfile path\". The next RUN command will again be run at '/'.\nTo change the \"Dockerfile path\", use WORKDIR (see reference), for example `WORKDIR /opt/firefox`.\nThe alternative would be chaining the executed RUN commands, as EvgeniySharapov pointed out: `RUN cd opt; ls; cd firefox; ls`\non multiple lines:\n```\n`RUN cd opt; \\\n    ls; \\\n    cd firefox; \\\n    ls\n`\n```\n(To clarify: It doesn't matter that Docker removes intermediate containers, that is not the problem in this case.)",
      "question_score": 14,
      "answer_score": 9,
      "created_at": "2021-02-15T18:10:41",
      "url": "https://stackoverflow.com/questions/66212390/how-can-i-prevent-docker-from-removing-intermediate-containers-when-executing-ru"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 72020904,
      "title": "Minio console not accessible behind nginx reverse proxy",
      "problem": "I am trying to redirect a `example.com/minio` location to minio console, which is run behind a nginx proxy both run by a docker compose file. My problem is that, when I'm trying to reverse proxy the minio endpoint to a path, like `/minio` it does not work, but when I run the `minio` reverse proxy on root path in the nginx reverse proxy, it works. I seriously cannot findout what the problem might be.\nThis is my compose file:\n`services:\n  nginx:\n    container_name: nginx\n    image: nginx\n    restart: unless-stopped\n    ports:\n      - 80:80\n      - 443:443\n    volumes:\n      - ./nginx.conf:/etc/nginx/conf.d/default.conf\n      - ./log/nginx:/var/log/nginx/\n  minio:\n    image: minio/minio\n    container_name: minio\n    volumes:\n      - ./data/minio/:/data\n    command: server /data --address ':9000' --console-address ':9001'\n    environment:\n      MINIO_ROOT_USER: minio_admin\n      MINIO_ROOT_PASSWORD: minio_123456\n    ports:\n      - 9000\n      - 9001\n    restart: always\n    logging:\n      driver: \"json-file\"\n      options:\n        max-file: \"10\"\n        max-size: 20m\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://127.0.0.1:9000/minio/health/live\"]\n      interval: 30s\n      timeout: 20s\n      retries: 3\n`\nMy nginx configuration is like this:\n```\n`server {\n    listen 80;\n    server_name example.com;\n\n    # To allow special characters in headers\n    ignore_invalid_headers off;\n    # Allow any size file to be uploaded.\n    # Set to a value such as 1000m; to restrict file size to a specific value\n    client_max_body_size 0;\n    # To disable buffering\n    proxy_buffering off;\n\n    access_log /var/log/nginx/service-access.log;\n    error_log /var/log/nginx/service-error.log debug;\n\n    location / {\n        return 200 \"salam\";\n        default_type text/plain;\n    }\n    location /minio {\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n        proxy_set_header Host $http_host;\n\n        proxy_connect_timeout 300;\n        # Default is HTTP/1, keepalive is only enabled in HTTP/1.1\n        proxy_http_version 1.1;\n        proxy_set_header Connection \"\";\n        chunked_transfer_encoding off;\n\n        proxy_pass http://minio:9001;\n    }\n}\n`\n```\nThe picture I'm seeing of minio console at the domain is this:\n\nAnd the response of curling the endpoint (`$ curl -k http://example.com/minio`):\n`\n    \n        \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            MinIO Console\n            \n            \n        \n        \n            You need to enable JavaScript to run this app.\n            \n                \n                    \n                    \n                \n                \n                    \n                \n            \n        \n    \n    %\n`",
      "solution": "I also struggled with this for a long time and was finally able to resolve it.\nAs far as I can tell, the key changes to make this work for me where:\n\nManually specifying a `rewrite` directive (instead of relying on the Nginx proxy_pass+URI behaviour which didn't seem to work for me).\nSetting the `resolver` directive with short timeouts (so that rescheduling of services onto other nodes gets resolved).\nSetting `$upstream` to prevent DNS caching.\n\nI had to change your setup a little bit so that now the Minio S3 API is served behind `minio.example.com` while the UI Web Console is accessible at `minio.example.com/console/`.\nI have edited your config files below:\ndocker-compose.yml:\n```\n`services:\n  nginx:\n    container_name: nginx\n    image: nginx\n    restart: unless-stopped\n    ports:\n      - 80:80\n      - 443:443\n    volumes:\n      - ./nginx.conf:/etc/nginx/conf.d/default.conf\n      - ./log/nginx:/var/log/nginx/\n  minio:\n    image: minio/minio\n    container_name: minio\n    volumes:\n      - ./data/minio/:/data\n    command: server /data --address ':9000' --console-address ':9001'\n    environment:\n      MINIO_SERVER_URL: \"http://minio.example.com/\"\n      MINIO_BROWSER_REDIRECT_URL: \"http://minio.example.com/console/\"    \n      MINIO_ROOT_USER: minio_admin\n      MINIO_ROOT_PASSWORD: minio_123456\n    ports:\n      - 9000\n      - 9001\n    restart: always\n    logging:\n      driver: \"json-file\"\n      options:\n        max-file: \"10\"\n        max-size: 20m\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://127.0.0.1:9000/minio/health/live\"]\n      interval: 30s\n      timeout: 20s\n      retries: 3\n`\n```\nnginx.conf:\n```\n`server {\n    listen 80;\n    server_name minio.example.com;\n\n    # To allow special characters in headers\n    ignore_invalid_headers off;\n    # Allow any size file to be uploaded.\n    # Set to a value such as 1000m; to restrict file size to a specific value\n    client_max_body_size 0;\n    # To disable buffering\n    proxy_buffering off;\n\n    access_log /var/log/nginx/service-access.log;\n    error_log /var/log/nginx/service-error.log debug;\n\n    # Use Docker DNS\n    # You might not need this section but in case you need to resolve\n    # docker service names inside the container then this can be useful.\n    resolver 127.0.0.11 valid=10s;\n    resolver_timeout 5s;\n\n    # Apparently the following line might prevent caching of DNS lookups\n    # and force nginx to resolve the name on each request via the internal\n    # Docker DNS.\n    set $upstream \"minio\";\n\n    # Minio Console (UI)\n    location /console/ {\n\n        # This was really the key for me. Even though the Nginx docs say \n        # that with a URI part in the `proxy_pass` directive, the `/console/`\n        # URI should automatically be rewritten, this wasn't working for me.\n        rewrite ^/console/(.*)$ /$1 break;\n\n        proxy_pass http://$upstream:9001;\n\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n        proxy_set_header Host $http_host;\n\n        proxy_connect_timeout 300;\n\n        # To support websocket\n        # Default is HTTP/1, keepalive is only enabled in HTTP/1.1\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n        chunked_transfer_encoding off;    \n    }\n\n    # Proxy requests to the Minio API on port 9000\n    location / {\n\n        proxy_pass http://$upstream:9000;\n\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n        proxy_set_header Host $http_host;\n\n        proxy_connect_timeout 300;\n\n        # To support websocket\n        # Default is HTTP/1, keepalive is only enabled in HTTP/1.1\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n        chunked_transfer_encoding off;\n    }\n\n}\n`\n```\nHTH!",
      "question_score": 14,
      "answer_score": 11,
      "created_at": "2022-04-26T23:49:31",
      "url": "https://stackoverflow.com/questions/72020904/minio-console-not-accessible-behind-nginx-reverse-proxy"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 67831394,
      "title": "Docker container time zone remains as UTC",
      "problem": "I have been working through setting my ms-sql docker container to use the Linux host date time settings. I'm using a docker-compose file, and for the most part the solution appears to be to map volumes like so (references: Bobcares, SO ):\n```\n`volume:\n  - /etc/timezone:/etc/timezone:ro\n  - /etc/localtime:/etc/localtime:ro\n`\n```\nAt first glance, this looks fine. I can check what date the container thinks it is, and it reflects the host local time zone. The SQL instance, however, is still displaying UTC time for `GetDate()`.\nThere is a symbolic link within the container though, that always looks like this: `localtime -> /usr/share/zoneinfo/Etc/UTC`. This is not what I am expecting based on the volume mapping, as it is different from the host `localtime -> ../usr/share/zoneinfo/Australia/Sydney`. I can manually update this link within the container and `GetDate()` on SQL returns the correct local datetime. All is lost on a `docker-compose down`, as expected.\nI need this to persist across container restarts, and I want to use the volume from the docker-compose file to handle time zones as we deploy to multiple time zones. I'm sure the problem is my noob linux skillset, so happy for advice on how to improve this. How do I fix the symbolic link problem (if that is indeed the root cause of the problem)?\nHost is: Ubuntu 20.04.1 LTS (Focal Fossa)\nms-sql image is: mcr.microsoft.com/mssql/server:2019-latest",
      "solution": "I kept searching and I found the answer in this post.\nFull steps I needed:\n\nSet timezone of host\nadd TZ enviroment variable to docker-compose.yml\nadd volume mapping for host time zone to docker-compose.yml\n\nMy docker-conpose section for ms-sql now looks like this (some parts omitted for brevity):\n```\n`ms-sql-server:\n  image: damo/sqlexpress:1.0.1\n  ports: \n    - \"14333:1433\"\n  volumes: \n    - sqldata:/var/opt/mssql\n    - /etc/timezone:/etc/timezone:ro\n    - /etc/localtime:/etc/localtime:ro\n  restart: unless-stopped\n`\n```\nThe only change I need to make between deployments is the environment variable.",
      "question_score": 14,
      "answer_score": 11,
      "created_at": "2021-06-04T06:14:19",
      "url": "https://stackoverflow.com/questions/67831394/docker-container-time-zone-remains-as-utc"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 72187612,
      "title": "Installing docker-compose-plugin on amazon linux 2",
      "problem": "I'm trying to install docker-compose-plugin onto Amazon Linux 2, so that \"docker compose\" acts more like \"docker-compose\" would. [As I understand, the latter is deprecated] I can't find any instructions and the obvious approach (treat it like Centos 7) does not work at all - the basic way docker is installed on Amazon Linux 2 is very different.\nDoes anybody know?",
      "solution": "Here is a bash script to install latest docker compose plugin for all users on  amazon linux 2:\n`sudo mkdir -p /usr/local/lib/docker/cli-plugins/\nsudo curl -SL https://github.com/docker/compose/releases/latest/download/docker-compose-linux-x86_64 -o /usr/local/lib/docker/cli-plugins/docker-compose\nsudo chmod +x /usr/local/lib/docker/cli-plugins/docker-compose\n`",
      "question_score": 13,
      "answer_score": 41,
      "created_at": "2022-05-10T15:51:32",
      "url": "https://stackoverflow.com/questions/72187612/installing-docker-compose-plugin-on-amazon-linux-2"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 73325805,
      "title": "Check if clickhouse is healthy docker-compose",
      "problem": "I need to wait for Clickhouse to start before I can start my backend server, however the healthcheck does not work.\nHere is my `docker-compose.yml` file:\n`version: '3.9'\nservices:\n  server:\n    build: .\n    depends_on:\n      clickhouse:\n        condition: service_healthy\n\n  clickhouse:\n    image: yandex/clickhouse-server\n    ports:\n      - '8123:8123'\n      - '9000:9000'\n      - '9009:9009'\n    healthcheck:\n      test: ['CMD', 'curl', '-f', 'http://localhost:8123']\n      interval: 5s\n      timeout: 3s\n      retries: 5\n`\nHowever when I run `docker-compose up --build`, then you can see the Clickhouse server starting, everything is fine, however the healthcheck never passes. The command exits preemptively with the error: `container for service \"Clickhouse\" is unhealthy`. However, if, during this time, I go run the command `curl -f http://localhost:8123` on my own computer (outside of the docker container) then it returns `Ok.`\nSo is there a way to wait for Clickhouse to be healthy before starting another service?",
      "solution": "Try this way:\n`    ..\n    healthcheck:\n      test: wget --no-verbose --tries=1 --spider http://localhost:8123/?query=SELECT%201 || exit 1\n    ..\n`\nor\n`    ..\n    healthcheck:\n      test: wget --no-verbose --tries=1 --spider http://localhost:8123/ping || exit 1\n    ..\n`",
      "question_score": 13,
      "answer_score": 16,
      "created_at": "2022-08-11T21:12:36",
      "url": "https://stackoverflow.com/questions/73325805/check-if-clickhouse-is-healthy-docker-compose"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 68548009,
      "title": "Running docker compose up outputs logs even when logging driver is set to none",
      "problem": "I'm using docker-compose to run tests for my app. My docker-compose.yml file has three services, one for mongodb, one for my app, and a third for my tests. I have logging: driver: \"none\" set for the app and mongodb because I only want to see the test logs.\nThis previously worked as expected. Since the last time I worked on this project, I've upgraded docker desktop on my mac to Version 3.5.2 (3.5.2.18). Now, all container logs flood my terminal when running. I'm not sure what has changed.\n```\n`version: '3.1'\n\nservices:\n\n  mongodb:\n    image: mongo\n    expose:\n      - 27017\n    logging:\n      driver: \"none\"\n\n  rsscloud:\n    build: .\n    command: node --use_strict app.js\n    environment:\n      DOMAIN: rsscloud\n      PORT: 5337\n      MONGODB_URI: mongodb://mongodb:27017/rsscloud\n      NODE_TLS_REJECT_UNAUTHORIZED: 0\n    expose:\n      - 5337\n    depends_on:\n      - mongodb\n    logging:\n      driver: \"none\"\n\n  rsscloud-tests:\n    build: .\n    command: dockerize -wait tcp://mongodb:27017 -wait http://rsscloud:5337 -timeout 10s bash -c \"npm test\"\n    environment:\n      APP_URL: http://rsscloud:5337\n      MONGODB_URI: mongodb://mongodb:27017/rsscloud\n      MOCK_SERVER_DOMAIN: rsscloud-tests\n      MOCK_SERVER_PORT: 8002\n      SECURE_MOCK_SERVER_PORT: 8003\n    volumes:\n      - ./xunit:/app/xunit\n    expose:\n      - 8002\n      - 8003\n    depends_on:\n      - mongodb\n      - rsscloud\n`\n```\nI run `docker compose up --build --abort-on-container-exit`\nHere is a sample of the output:\n```\n`Attaching to mongodb_1, rsscloud-tests_1, rsscloud_1\nrsscloud-tests_1  | 2021/07/27 15:43:58 Waiting for: tcp://mongodb:27017\nrsscloud-tests_1  | 2021/07/27 15:43:58 Waiting for: http://rsscloud:5337\nrsscloud-tests_1  | 2021/07/27 15:43:58 Connected to tcp://mongodb:27017\nmongodb_1         | {\"t\":{\"$date\":\"2021-07-27T15:43:58.964+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":22943,   \"ctx\":\"listener\",\"msg\":\"Connection accepted\",\"attr\":{\"remote\":\"172.18.0.4:39890\",\"uuid\":\"d9874a7d-e9c3-4e88-b163-c9b9deb582d1\",\"connectionId\":2,\"connectionCount\":2}}\nrsscloud-tests_1  | 2021/07/27 15:43:58 Received 200 from http://rsscloud:5337\nrsscloud_1        | [15:43:58.993] GET / 200 577 - ::ffff:172.18.0.4 - 20.885 ms\nrsscloud-tests_1  | \nrsscloud-tests_1  | > rsscloud-server@2.0.0 test /app\nrsscloud-tests_1  | > mocha -R mocha-multi --reporter-options spec=-,xunit=xunit/test-results.xml --timeout 10000\nrsscloud-tests_1  | \nrsscloud-tests_1  | \nrsscloud-tests_1  | \nrsscloud-tests_1  |   Ping XML-RPC to http-post returning XML\nmongodb_1         | {\"t\":{\"$date\":\"2021-07-27T15:44:00.100+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":22943,   \"ctx\":\"listener\",\"msg\":\"Connection accepted\",\"attr\":{\"remote\":\"172.18.0.4:39892\",\"uuid\":\"df65fb3a-9913-4075-9671-34d2e9a4d7e8\",\"connectionId\":3,\"connectionCount\":3}}\nmongodb_1         | {\"t\":{\"$date\":\"2021-07-27T15:44:00.105+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":51800,   \"ctx\":\"conn3\",\"msg\":\"client metadata\",\"attr\":{\"remote\":\"172.18.0.4:39892\",\"client\":\"conn3\",\"doc\":{\"driver\":{\"name\":\"nodejs\",\"version\":\"4.0.1\"},\"os\":{\"type\":\"Linux\",\"name\":\"linux\",\"architecture\":\"x64\",\"version\":\"5.10.25-linuxkit\"},\"platform\":\"Node.js v12.22.3, LE (unified)|Node.js v12.22.3, LE (unified)\"}}}\nrsscloud-tests_1  |     \u2192 MongoDB 'rsscloud' Database Connected\nrsscloud-tests_1  |     \u2192 Mock server started on port: 8002\nrsscloud-tests_1  |     \u2192 Mock secure server started on port: 8003\n`\n```",
      "solution": "This is an intentional change with docker compose. You can change it back to using `docker-compose` which is different from `docker compose` and will likely provide your expected previous behavior.\nOr you can run `docker compose up -d` and `docker compose logs rsscloud-tests` but I'm not sure there's an easy way to do that with `--abort-on-container-exit` since that's likely incompatible with the `-d` option.\nI'd recommend following this enhancement request and give it your thumbs up: https://github.com/docker/compose-cli/issues/1615",
      "question_score": 13,
      "answer_score": 14,
      "created_at": "2021-07-27T17:46:46",
      "url": "https://stackoverflow.com/questions/68548009/running-docker-compose-up-outputs-logs-even-when-logging-driver-is-set-to-none"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 67639257,
      "title": "ERROR: TLS configuration is invalid - make sure your DOCKER_TLS_VERIFY and DOCKER_CERT_PATH are set correctly. on windows",
      "problem": "I went through links like https://github.com/docker/compose/issues/3021 and https://github.com/docker/compose/issues/3937, but still I am facing below error:\n```\n`C:\\Users\\pc\\Downloads\\docker-compose-scripts>docker-compose up --d\nERROR: TLS configuration is invalid - make sure your DOCKER_TLS_VERIFY and DOCKER_CERT_PATH are set correctly.\nYou might need to run `eval \"$(docker-machine env default)\"`\n`\n```\nVersions of docker\n```\n`C:\\Users\\pc>docker --version\nDocker version 20.10.6, build 370c289\n\nC:\\Users\\pc>docker-compose --version\ndocker-compose version 1.29.1, build c34c88b2\n`\n```",
      "solution": "For intellij, it can also be resolved by adding `DOCKER_TLS_VERIFY`and `DOCKER_CERT_PATH` to your run/debug configuration as environment variables.\nThe value of each can be empty (depending on your docker setup) so the run/debug configuration shows:\n`DOCKER_TLS_VERIFY=;DOCKER_CERT_PATH=`",
      "question_score": 12,
      "answer_score": 17,
      "created_at": "2021-05-21T16:54:24",
      "url": "https://stackoverflow.com/questions/67639257/error-tls-configuration-is-invalid-make-sure-your-docker-tls-verify-and-docke"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 78500319,
      "title": "how to pull model automatically with container creation?",
      "problem": "When trying to access the ollama container from another (node) service in my docker compose setup, I get the following error:\n\nResponseError: model 'llama3' not found, try pulling it first\n\nI want the setup for the containers to be automatic and don't want to manually connect to the containers and manually pull the models.\nIs there a way to load the model of my choice automatically when I create the ollama docker container?\nHere is my relevant part of the docker-compose.yml\n```\n`ollama:\n        image: ollama/ollama:latest\n        ports:\n            - 11434:11434\n        volumes:\n            - ./ollama/ollama:/root/.ollama\n        container_name: ollama\n        pull_policy: always\n        tty: true\n        restart: always\n`\n```",
      "solution": "Use a custom entrypoint script to download the model when a container is launched. The model will be persisted in the volume mount, so this will go quickly with subsequent starts.\n```\n`version: '3.7'\n\nservices:\n  ollama:\n    image: ollama/ollama:latest\n    ports:\n        - 11434:11434\n    volumes:\n        - ./ollama/ollama:/root/.ollama\n        - ./entrypoint.sh:/entrypoint.sh\n    container_name: ollama\n    pull_policy: always\n    tty: true\n    restart: always\n    entrypoint: [\"/usr/bin/bash\", \"/entrypoint.sh\"]\n`\n```\nKey changes:\n\nAdded an `entrypoint`.\nAdded a volume mount for `entrypoint.sh`.\n\nThis is the content of `entrypoint.sh`:\n```\n`#!/bin/bash\n\n# Start Ollama in the background.\n/bin/ollama serve &\n# Record Process ID.\npid=$!\n\n# Pause for Ollama to start.\nsleep 5\n\necho \"\ud83d\udd34 Retrieve LLAMA3 model...\"\nollama pull llama3\necho \"\ud83d\udfe2 Done!\"\n\n# Wait for Ollama process to finish.\nwait $pid\n`\n```",
      "question_score": 12,
      "answer_score": 27,
      "created_at": "2024-05-18T18:17:43",
      "url": "https://stackoverflow.com/questions/78500319/how-to-pull-model-automatically-with-container-creation"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 72891468,
      "title": "Docker Failed - bcrypt_lib.node: Exec format error",
      "problem": "I have tried to create a docker image of my backend API.\nbut getting errors. I have googled about it, and everyone who has the same issue had to add node_module on the .dockerignore file.\nI already did it, but, still have the same error.\nI am adding my file info here.\nDockerfile\n```\n`FROM node:alpine\nWORKDIR /usr/src/app\nCOPY package*.json .\n#COPY yarn.lock .\nRUN apk add --no-cache yarn --repository=\"http://dl-cdn.alpinelinux.org/alpine/edge/community\"\n#RUN yarn install --frozen-lockfile\nRUN yarn install\nRUN yarn\nCOPY . .\nCMD [\"yarn\", \"dev\"];\n`\n```\n.dockerignore\n```\n`/node_modules\n.env\ndocker-compose.yml\n`\n```\ndocker-compose.yml\n```\n`version: \"3.9\"\n\nservices:\n  mongo_db:\n    container_name: mongodb_container\n    image: mongo:latest\n    restart: always\n    ports:\n      - \"27017:27017\"\n    volumes:\n      - mongo_db:/data/db\n\n  #EET service\n  eetapi:\n    container_name: eetapi_container\n    build: .\n    volumes:\n      - .:/usr/src/app\n    ports:\n      - \"3000:3000\"\n    environment:\n      SITE_URL: http://localhost\n      PORT: 3000\n      MONGO_URL: mongodb://mongodb_container:27017/easyetapi\n      JWT_SECRET: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n      SENTRY_DSN: https://xxxxxxxxxxxxxxx@xxxxxxx.ingest.sentry.io/xxxxxxx\n      MAILGUN_DOMAIN: mg.myeetdomain.tld\n      MAILGUN_API_KEY: xxxxxxxxxxxxxxx-xxxxxxxxxxx-xxxxxxxx\n      NODE_ENV: production\n    depends_on:\n      - mongo_db\nvolumes:\n  mongo_db: {}\n`\n```\nThe Error\nError Screenshot\nPlease help me out.\nThank You",
      "solution": "The `volumes:` block overwrites everything in the image with the current directory on the host.  That includes the `node_modules` tree installed in the Dockerfile.  If you have a MacOS or Windows host but a Linux container, replacing the `node_modules` tree will cause the error you get.\nYou should delete the `volumes:` block so that you run the code and library tree that are built into the image.\nSince the bind-mount overwrites literally everything the Dockerfile does, it negates any benefit you get from building the Docker image.  Effectively you're just running an unmodified `node` image with bind-mounted host content, and you'll get the same effect with a much simpler setup if you Node on the host without involving Docker.  (You could still benefit from running the database in a container.)",
      "question_score": 12,
      "answer_score": 13,
      "created_at": "2022-07-07T03:49:12",
      "url": "https://stackoverflow.com/questions/72891468/docker-failed-bcrypt-lib-node-exec-format-error"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 65953634,
      "title": "How do I inspect the stopped docker container files",
      "problem": "Step 1:\n`docker ps -a`\n`container Id: dd5cf6b519b4`\nI need to inspect inside the stopped docker container which is cannot start.\nI tried with `docker exec -it container-id bin/bash` But this is for running container.",
      "solution": "You can start container with specific entrypoint\n```\n`docker run --entrypoint sleep YOUR_IMAGE 3600\n`\n```\nIt will block current terminal for 3600 seconds. You can open new terminal tab(do not close current one) and you can verify if your container is working with the\n```\n`docker ps\n`\n```\nIf you do not want to block current terminal, you can add `-d` flag to docker run:\n```\n`docker run -d --entrypoint sleep YOUR_IMAGE 3600\n`\n```\nAbove command will start docker which will be doing nothing, then you can ssh into the container when it is working with\n```\n`docker exec -ti CONTAINER HASH sh\n`\n```",
      "question_score": 12,
      "answer_score": 11,
      "created_at": "2021-01-29T12:29:55",
      "url": "https://stackoverflow.com/questions/65953634/how-do-i-inspect-the-stopped-docker-container-files"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 77028925,
      "title": "Docker Compose Fails: error: externally-managed-environment",
      "problem": "I am using a windows machine and have installed wsl to be able to use Docker desktop. Of course the build failed and then I observed python3 and pip3 in the dockerfile. So I installed ubuntu and debian via wsl and then tried to run the app (docker-compose up). It still fails and throws the following error:\n```\n` ERROR [test 3/5] RUN pip3 install daff==1.3.46                                                                                                                           0.9s\n------\n > [test 3/5] RUN pip3 install daff==1.3.46:\n0.812 error: externally-managed-environment\n0.812\n0.812 \u00d7 This environment is externally managed\n0.812 \u2570\u2500> To install Python packages system-wide, try apt install\n0.812     python3-xyz, where xyz is the package you are trying to\n0.812     install.\n0.812\n0.812     If you wish to install a non-Debian-packaged Python package,\n0.812     create a virtual environment using python3 -m venv path/to/venv.\n0.812     Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n0.812     sure you have python3-full installed.\n0.812\n0.812     If you wish to install a non-Debian packaged Python application,\n0.812     it may be easiest to use pipx install xyz, which will manage a\n0.812     virtual environment for you. Make sure you have pipx installed.\n0.812\n0.812     See /usr/share/doc/python3.11/README.venv for more information.\n0.812\n0.812 note: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n0.812 hint: See PEP 668 for the detailed specification.\n------\nfailed to solve: process \"/bin/sh -c pip3 install daff==1.3.46\" did not complete successfully: exit code: 1\n`\n```\nHere's the DockerFile:\n```\n`FROM postgres:12\n\nRUN apt-get update && apt-get install -y \\\n    python3 \\\n    python3-pip\nRUN pip3 install daff==1.3.46\n\n# Copy project files\nCOPY . /app\nWORKDIR /app\nENV PATH=$PATH:/app/bin\n`\n```\nAny idea how to resolve the issue?\nBecause I'm using docker, this How do I solve \"error: externally-managed-environment\" everytime I use pip3? doesn't help me.",
      "solution": "This error occurs due to this: PEP0668.\nI found the resolution for local machines while going through this answer.\nSpecifically for docker, using the `pip install` command with the flag `--break-system-packages` worked for me.\nIn your script update the line:\n`RUN pip3 install daff==1.3.46` \nwith\n`RUN pip3 install daff==1.3.46 --break-system-packages`",
      "question_score": 12,
      "answer_score": 21,
      "created_at": "2023-09-02T17:41:15",
      "url": "https://stackoverflow.com/questions/77028925/docker-compose-fails-error-externally-managed-environment"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 73814619,
      "title": "Permission denied trying to use rootless Podman + docker-compose + Traefik with podman.sock",
      "problem": "TL:DR: Trying to use rootless Podman with docker-compose through podman socket, and use a Traefik container (talking to podman socket) to proxy traffic to other containers, related to https://stackoverflow.com/a/73774327/1469083\nI get permission denied errors, which I can fix with privileged container, which I don't want to use.\nSetup\nI am running on RHEL 8\n```\n`$ cat /etc/redhat-release \nRed Hat Enterprise Linux release 8.6 (Ootpa)\n`\n```\nPodman came preinstalled, I added docker-compose (\"standalone\") and podman-docker:\n```\n`$ curl -SL https://github.com/docker/compose/releases/download/v2.10.2/docker-compose-linux-x86_64 -o /usr/local/bin/docker-compose\n$ chmod a+x /usr/local/bin/docker-compose\n$ sudo yum install podman-docker\n`\n```\nAnd activated rootless podman socket so that podman and docker-compose can talk to each other:\n```\n`$ systemctl --user enable podman.socket\n$ systemctl --user start podman.socket\n$ systemctl --user status podman.socket\n$ export DOCKER_HOST=unix:///run/user/$UID/podman/podman.sock\n$ echo $DOCKER_HOST\nunix:///run/user/1001/podman/podman.sock\n`\n```\nI also switched network backend to netavark, DNS did not work without that change\n```\n`$ podman info |grep -i networkbackend\n  networkBackend: netavark\n`\n```\nProblems\nFirst I tried the compose stack from https://stackoverflow.com/a/73774327/1469083 with small modifications:\n```\n`version: \"3\"\nservices:\n  frontend:\n    image: \"docker.io/traefik:v2.8\"\n    ports:\n      - \"3000:80\"\n      - \"127.0.0.1:3080:8080\"\n    command:\n      - --api.insecure=true\n      - --providers.docker\n    volumes:\n      - /run/user/$UID/podman/podman.sock:/var/run/docker.sock\n\n  backend:\n    labels:\n      traefik.http.routers.backend.rule: Host(`localhost`)\n    image: \"tomcat:latest\"\n    scale: 3\n`\n```\nMy setup did not appreciate the `$UID` variable:\n```\n`WARN[0000] The \"UID\" variable is not set. Defaulting to a blank string. \n...\nError response from daemon: make cli opts(): error making volume mountpoint for volume /run/user//podman/podman.sock: mkdir /run/user//podman: permission denied\n`\n```\nI replaced the volume map with hard-coded UID=1001 (it is the UID of the user running rootless podman, I assumed I should use that one?). Socket looks like this:\n```\n`ls -la /run/user/1001/podman/podman.sock \nsrw-rw----. 1 myrootlessuser myrootlessuser 0 22. 9. 11:28 /run/user/1001/podman/podman.sock\n\nvolumes:\n  - /run/user/1001/podman/podman.sock:/var/run/docker.sock\n`\n```\nBut now I get permission denied errors from Traefik trying to connect to /var/run/docker.sock unsuccessfully:\n```\n`example-docker-compose-frontend-1  | time=\"2022-09-22T12:04:52Z\" level=error msg=\"Provider connection error Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get \\\"http://%2Fvar%2Frun%2Fdocker.sock/v1.24/version\\\": dial unix /var/run/docker.sock: connect: permission denied, retrying in 5.656635591s\" providerName=docker\n`\n```\nIf I change the Traefik-container to `privileged: true`, this fixes the problem. I don't get the errors anymore, and proxying works like it should.\nBut, I would prefer not to use privileged containers for security reasons, or at least understand why it is like this.\nQuestions\n\nHow can I make this work with non-privileged Traefik container?\n\nHow do I verify that rootless docker/podman socket is working correctly? I've seen commands like this for testing rootful podman socket, but haven't had success on rootless\n`$ sudo curl -H \"Content-Type: application/json\" --unix-socket /var/run/docker.sock http://localhost/_ping`\n`curl: (7) Couldn't connect to server`\n\nWhere can I find the documentation for setting up docker socket for rootless Podman? Did I do my setup correctly (`systemctl --user enable podman.socket` etc.)? I've only been able to find some blogs about this, and advice varies and is often for older Podman versions. For example:\n\nhttps://fedoramagazine.org/use-docker-compose-with-podman-to-orchestrate-containers-on-fedora/ tells me to enable `podman.socket`\nhttps://earthly.dev/blog/earthly-podman/ tells me to enable `podman.service`",
      "solution": "Question 1 and 2\nIf you are using\n```\n`export DOCKER_HOST=unix:///run/user/$UID/podman/podman.sock\n`\n```\nyou are using rootless (unprivileged) Podman (even if you specify `privileged: true` in the Compose file).\nTo use the leaked socket in the container, you need to run `podman run` with the command-line option `--security-opt label=disable`.\nExample:\nStart and enable the podman socket\n```\n`$ systemctl --user enable --now podman.socket\nCreated symlink /home/testuser/.config/systemd/user/sockets.target.wants/podman.socket \u2192 /usr/lib/systemd/user/podman.socket.\n`\n```\nTest the Docker API service.\nResult: failure. Curl prints `Couldn't connect to server`\n```\n`$ podman run --rm \\\n  -v $XDG_RUNTIME_DIR/podman/podman.sock:/var/run/docker.sock \\\n  docker.io/library/fedora \\\n    /usr/bin/curl \\\n      -H \"Content-Type: application/json\" \\\n      --unix-socket /var/run/docker.sock \\\n        http://localhost/_ping\ncurl: (7) Couldn't connect to server\n`\n```\nTest the Docker API service again, but now add `--security-opt label=disable`.\nResult: success\n```\n`$ podman run --rm \\\n  --security-opt label=disable \\\n  -v $XDG_RUNTIME_DIR/podman/podman.sock:/var/run/docker.sock \\\n  docker.io/library/fedora \\\n    /usr/bin/curl \\\n      -Hs \"Content-Type: application/json\" \\\n      --unix-socket /var/run/docker.sock \\\n        http://localhost/_ping\nOK$\n`\n```\nThe Docker API service responded with the text string  OK.\n(In the command above I also added the curl option -s so that curl prints less debug output)\nI would guess that adding `privileged: true` to the Compose file has the same effect as providing the --privileged command-line option to `podman run`.\nOne of the effects of using  --privileged is that it implies --security-opt label=disable.\nQuestion 3\nSummary:\nIt is enough to run\n```\n`$ systemctl --user start podman.socket\n`\n```\nto set up the UNIX socket for rootless Podman.\nIt's not necessary but if you in addition to that run\n```\n`$ systemctl --user start podman.service\n`\n```\nthe podman.service will be started right away (even before the first client has connected to the UNIX socket).\nLonger version:\nIf podman.socket is active, then the podman.service will be started when a client connects. (Podman supports socket activation)\nThe podman.service will also be started after a reboot if the podman.service has been enabled (`systemctl --user enable podman.service`) and lingering is enabled (`loginctl enable-linger`).\nThe podman.service will also be started when the user logs in if the podman.service has been enabled (`systemctl --user enable podman.service`).\nThe podman process running in the podman.service will automatically exit after some time of inactivity (by default 5 seconds).\nOn a Fedora 36 computer, the `Restart` directive is set to `no` (the default value):\n```\n`$ grep Restart= /usr/lib/systemd/user/podman.service\n$ \n`\n```\nThis means that it really doesn't matter much whether\n```\n`systemctl --user enable podman.service\n`\n```\nhas been run or not. The service podman.service will anyway be stopped in 5 seconds if no clients access it.",
      "question_score": 12,
      "answer_score": 21,
      "created_at": "2022-09-22T14:26:46",
      "url": "https://stackoverflow.com/questions/73814619/permission-denied-trying-to-use-rootless-podman-docker-compose-traefik-with"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 72222070,
      "title": "Postgres and Docker Compose; password authentication fails and role &#39;postgres&#39; does not exist. Cannot connect from pgAdmin4",
      "problem": "I have a docker-compose that brings up the psql database as below, currently I'm trying to connect to it with pgAdmin4 (not in a docker container) and be able to view it. I've been having trouble authenticating with the DB and I don't understand why.\ndocker-compose\n```\n`version: \"3\"\n\nservices:\n  # nginx and server also have an override, but not important for this q.\n  nginx:\n    ports:\n      - 1234:80\n      - 1235:443\n  server:\n    build: ./server\n    ports:\n      - 3001:3001 # app server port\n      - 9230:9230 # debugging port\n    env_file: .env\n    command: yarn dev\n    volumes:\n      # Mirror local code but not node_modules\n      - /server/node_modules/\n      - ./server:/server\n  \n  database:\n    container_name: column-db\n    image: 'postgres:latest'\n    restart: always\n    ports:\n      - 5432:5432\n    environment:\n      POSTGRES_USER: postgres # The PostgreSQL user (useful to connect to the database)\n      POSTGRES_PASSWORD: root # The PostgreSQL password (useful to connect to the database)\n      POSTGRES_DB: postgres # The PostgreSQL default database (automatically created at first launch)\n    volumes:\n      - ./db-data/:/var/lib/postgresql/data/\n\nnetworks:\n  app-network:\n    driver: bridge\n`\n```\nI do `docker-compose up` then check the logs, and it says that it is ready for connections. I go to pgAdmin and enter the following:\n\nwhere password is `root`. I then get this error:\n```\n`FATAL:  password authentication failed for user \"postgres\"\n`\n```\nI check the docker logs and I see\n```\n`DETAIL:  Role \"postgres\" does not exist.\n`\n```\nI'm not sure what I'm doing wrong, according to the docs the super user should be created with those specifications. Am I missing something? Been banging my head against this for an hour now. Any help is appreciated!",
      "solution": "@jjanes solved it in a comment, I had used a mapped volume and never properly set up the db. Removed the volume and we're good to go.",
      "question_score": 12,
      "answer_score": 19,
      "created_at": "2022-05-12T23:08:19",
      "url": "https://stackoverflow.com/questions/72222070/postgres-and-docker-compose-password-authentication-fails-and-role-postgres-d"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 71522171,
      "title": "How to keep docker image build during job across two stages with Gitlab CI?",
      "problem": "I use Gitlab runner on an EC2 to `build`, `test` and `deploy` docker images on a ECS.\nI start my CI workflow using a \"push/pull\" logic: I build all my docker images during the first stage and push them to my gitlab repository then I pull them during the `test` stage.\nI thought that I could drastically improve the workflow time by keeping the image built during the `build` stage between `build` and `test` stages.\nMy `gitlab-ci.yml` looks like this:\n```\n`stages:\n  - build\n  - test\n  - deploy\n\nbuild_backend:\n  stage: build\n  image: docker\n  services:\n    - docker:dind\n  before_script:\n    - echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER $CI_REGISTRY --password-stdin\n  script:\n    - docker build -t backend:$CI_COMMIT_BRANCH ./backend\n  only:\n    refs:\n      - develop\n      - master\n\nbuild_generator:\n  stage: build\n  image: docker\n  services:\n    - docker:dind\n  before_script:\n    - echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER $CI_REGISTRY --password-stdin\n  script:\n    - docker build -t generator:$CI_COMMIT_BRANCH ./generator\n  only:\n    refs:\n      - develop\n      - master\n\nbuild_frontend:\n  stage: build\n  image: docker\n  services:\n    - docker:dind\n  before_script:\n    - echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER $CI_REGISTRY --password-stdin\n  script:\n    - docker build -t frontend:$CI_COMMIT_BRANCH ./frontend\n  only:\n    refs:\n      - develop\n      - master\n\nbuild_scraping:\n  stage: build\n  image: docker\n  services:\n    - docker:dind\n  before_script:\n    - echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER $CI_REGISTRY --password-stdin\n  script:\n    - docker build -t scraping:$CI_COMMIT_BRANCH ./scraping\n  only:\n    refs:\n      - develop\n      - master\n\ntest_backend:\n  stage: test\n  needs: [\"build_backend\"]\n  image: docker\n  services:\n    - docker:dind\n  before_script:\n    - echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER $CI_REGISTRY --password-stdin\n    - DOCKER_CONFIG=${DOCKER_CONFIG:-$HOME/.docker}\n    - mkdir -p $DOCKER_CONFIG/cli-plugins\n    - apk add curl\n    - curl -SL https://github.com/docker/compose/releases/download/v2.3.2/docker-compose-linux-x86_64 -o $DOCKER_CONFIG/cli-plugins/docker-compose\n    - chmod +x $DOCKER_CONFIG/cli-plugins/docker-compose\n  script:\n    - docker compose -f docker-compose-ci.yml up -d backend\n    - docker exec backend pip3 install --no-cache-dir --upgrade -r requirements-test.txt\n    - docker exec db sh mongo_init.sh\n    - docker exec backend pytest test --junitxml=report.xml -p no:cacheprovider\n  artifacts:\n    when: always\n    reports:\n      junit: backend/report.xml\n  only:\n    refs:\n      - develop\n      - master\n\ntest_generator:\n  stage: test\n  needs: [\"build_generator\"]\n  image: docker\n  services:\n    - docker:dind\n  before_script:\n    - echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER $CI_REGISTRY --password-stdin\n    - DOCKER_CONFIG=${DOCKER_CONFIG:-$HOME/.docker}\n    - mkdir -p $DOCKER_CONFIG/cli-plugins\n    - apk add curl\n    - curl -SL https://github.com/docker/compose/releases/download/v2.3.2/docker-compose-linux-x86_64 -o $DOCKER_CONFIG/cli-plugins/docker-compose\n    - chmod +x $DOCKER_CONFIG/cli-plugins/docker-compose\n  script:\n    - docker compose -f docker-compose-ci.yml up -d generator\n    - docker exec generator pip3 install --no-cache-dir --upgrade -r requirements-test.txt\n    - docker exec generator pip3 install --no-cache-dir --upgrade -r requirements.txt\n    - docker exec db sh mongo_init.sh\n    - docker exec generator pytest test --junitxml=report.xml -p no:cacheprovider\n  artifacts:\n    when: always\n    reports:\n      junit: generator/report.xml\n  only:\n    refs:\n      - develop\n      - master\n   \n[...]\n`\n```\n`gitlab-runner/config.toml`:\n```\n`concurrent = 5\ncheck_interval = 0\n\n[session_server]\n  session_timeout = 1800\n\n[[runners]]\n  name = \"Docker Runner\"\n  url = \"https://gitlab.com/\"\n  token = \"\"\n  executor = \"docker\"\n  [runners.custom_build_dir]\n  [runners.cache]\n    [runners.cache.s3]\n    [runners.cache.gcs]\n    [runners.cache.azure]\n  [runners.docker]\n    tls_verify = false\n    image = \"docker:19.03.12\"\n    privileged = true\n    disable_entrypoint_overwrite = false\n    oom_kill_disable = false\n    disable_cache = false\n    volumes = [\"/certs/client\", \"/cache\"]\n    shm_size = 0\n`\n```\n`docker-compose-ci.yml`:\n```\n`services:\n  backend:\n    container_name: backend\n    image: backend:$CI_COMMIT_BRANCH\n    build:\n      context: backend\n    volumes:\n      - ./backend:/app\n    networks:\n      default:\n    ports:\n      - 8000:8000\n      - 587:587\n      - 443:443\n    environment:\n      - ENVIRONMENT=development\n    depends_on:\n      - db\n\n  generator:\n    container_name: generator\n    image: generator:$CI_COMMIT_BRANCH\n    build:\n      context: generator\n    volumes:\n      - ./generator:/var/task\n    networks:\n      default:\n    ports:\n      - 9000:8080\n    environment:\n      - ENVIRONMENT=development\n    depends_on:\n      - db\n\n  db:\n    container_name: db\n    image: mongo\n    volumes:\n      - ./mongo_init.sh:/mongo_init.sh:ro\n    networks:\n      default:\n    environment:\n      MONGO_INITDB_DATABASE: DB\n      MONGO_INITDB_ROOT_USERNAME: admin\n      MONGO_INITDB_ROOT_PASSWORD: admin\n    ports:\n      - 27017:27017\n\n  frontend:\n    container_name: frontend\n    image: frontend:$CI_COMMIT_BRANCH\n    build:\n      context: frontend\n    volumes:\n      - ./frontend:/app\n    networks:\n      default:\n    ports:\n      - 8080:8080\n    depends_on:\n      - backend\n\nnetworks:\n  default:\n    driver: bridge\n`\n```\nWhen I comment `context:` in my `docker-compose-ci.yml`, Docker can't find my image and indeed it is not keep between jobs.\nWhat is the best Docker approach during CI to `build` -> `test` -> `deploy`?\nShould I zip my docker image and share them between stages using artifacts? It doesn't seem to be the most efficient way to do this.\nI'm a bit lost about which approach I should use to perform a such common workflow in Gitlab CI using Docker.",
      "solution": "The best way to do this is to push the image to the registry and pull it in other stages where it is needed. You appear to be missing the push/pull logic.\nYou also want to make sure you've leveraging docker caching in your docker builds. You'll probably want to specify the `cache_from:` key in your compose file.\nFor example:\n`build:\n  stage: build\n  script:\n    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY\n    # pull latest image to leverage cached layers\n    - docker pull $CI_REGISTRY_IMAGE:latest || true\n\n    # build and push the image to be used in subsequent stages\n    - docker build --cache-from $CI_REGISTRY_IMAGE:latest --tag $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA .\n    - docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA  # push the image\n\ntest:\n  stage: test\n  needs: [build]\n  script:\n    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY\n    # pull the image that was built in the previous stage\n    - docker pull $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA\n    - docker-compose up # or docker run or whatever\n`\n\nEdit:\nIn modern versions of Docker with Docker buildkit/buildx, you can use the buildkit inline caching instead of pulling the image ahead of time. This requires pushing a somewhat larger image to your repo, but makes cache pulls speedier because docker can tell which layers are valid for caching before it pulls them. Normal pull speed is unaffected.\n`export DOCKER_BUILDKIT=1\n\ndocker build --build-arg BUILDKIT_INLINE_CACHE=1 \\\n             --cache-from \"$CI_REGISTRY_IMAGE:latest\"\n             --cache-from \"$CI_REGISTRY_IMAGE:$CI_REF_NAME\" \\\n             --tag \"$CI_REGISTRY_IMAGE:$CI_REF_NAME\" \\\n             --tag \"$CI_REGISTRY_IMAGE:$CI_COMMIT_SHA\" \\ \n             .\n\ndocker push \"$CI_REGISTRY_IMAGE:$CI_REF_NAME\"\ndocker push \"$CI_REGISTRY_IMAGE:$CI_COMMIT_SHA\"\n`",
      "question_score": 12,
      "answer_score": 17,
      "created_at": "2022-03-18T04:31:04",
      "url": "https://stackoverflow.com/questions/71522171/how-to-keep-docker-image-build-during-job-across-two-stages-with-gitlab-ci"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 74419205,
      "title": "&quot;docker: &#39;compose&#39; is not a docker command&quot; when running docker compose",
      "problem": "After installing the latest version of Docker Desktop on my M1 Mac pro, I can't seem to run docker compose despite having all the correct files installed.\nI've tried running `docker-compose` and `docker compose` in my terminal and I get this message: `docker: 'compose' is not a docker command.`\nAfter running `docker --version`, I'm currently on `Docker version 20.10.21, build baeda1f`\nCompose is correctly added to my PATH vars under `/usr/local/bin/`, which has the following files:\n```\n`com.docker.cli          docker-credential-desktop       fuzzy_match         kubectl.docker\ndocker                  docker-credential-ecr-login     httpclient          vpnkit\ndocker-compose          docker-credential-osxkeychain   hub-tool            xcodeproj\ndocker-compose-v1       docker-index                    kubectl\n`\n```\nWhen I run `which docker-compose`, it returns `/usr/local/bin/docker-compose` so it seems like everything is configured correctly.\nAlso running `docker-compose-v1` does seem to work for some weird reason.\nAny ideas?\nEdit: Seems like this was fixed after installing the newest version of Docker (Docker Desktop 4.14.1 (91661), Docker version 20.10.21, build baeda1f).",
      "solution": "Install Rosetta 2 with `softwareupdate --install-rosetta`, then uncheck Use Docker Compose V2 in Docker Dashboard -> Settings -> General.",
      "question_score": 12,
      "answer_score": 6,
      "created_at": "2022-11-13T08:29:27",
      "url": "https://stackoverflow.com/questions/74419205/docker-compose-is-not-a-docker-command-when-running-docker-compose"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 73042460,
      "title": "Kafka There is no leader for this topic-partition as we are in the middle of a leadership election",
      "problem": "just starting learning Kafka. Im trying to setup a small kafka cluster including 2 brokers. I was successfull in sending messages to my topic when both brokers are up. I want to test the behavior of my cluster when one of 2 brokers goes done. I stopped my primary broker (Kafka1) using docker stop kafka1, and i tried then to send a message to my cluster to see if my producer is able to understand that he need to send to kafka2 as kafka1 is down.\nHowever i constantly receiving the below errors:\n{\"level\":\"ERROR\",\"timestamp\":\"2022-07-19T18:59:46.891Z\",\"logger\":\"kafkajs\",\"message\":\"[Connection] Response Metadata(key: 3, version: 6)\",\"broker\":\"localhost:39092\",\"clientId\":\"my-app\",\"error\":\"There is no leader for this topic-partition as we are in the middle of a leadership election\",\"correlationId\":1,\"size\":144}\nbelow is my producer code:\n```\n`const kafka = new Kafka({\nclientId: 'my-app',\nbrokers: ['localhost:29092', 'localhost:39092'],\n})\nconst producer = kafka.producer({ createPartitioner: Partitioners.LegacyPartitioner })\n\nawait producer.connect()\n\nawait producer.send({\n  topic: 'coverageEvolved',\n  messages: [\n    { value: JSON.stringify(bodyActiveMq), key: bodyActiveMq[0].roamPartner},\n  ],\n})\n\nawait producer.disconnect()\n`\n```\nand below is my docker-compose-file:\n```\n`version: '2'\nservices:\n    zookeeper:\n    image: confluentinc/cp-zookeeper:latest\n    restart: unless-stopped\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n      ZOOKEEPER_TICK_TIME: 2000\n    ports:\n      - 22181:2181\n    volumes:\n      - ./zookeeper/data:/var/lib/zookeeper/data\n    kafka-1:\n        image: confluentinc/cp-kafka:latest\n        depends_on:\n          - zookeeper\n        ports:\n            - 29092:29092\n        restart: unless-stopped\n        environment:\n          KAFKA_BROKER_ID: 1\n          KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n          KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka- \n    1:9092,PLAINTEXT_HOST://localhost:29092\n          KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n          KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n          KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2\n        volumes:\n          - ./kafka1/data:/var/lib/kafka/data\n      kafka-2:\n        image: confluentinc/cp-kafka:latest\n        depends_on:\n          - zookeeper\n        ports:\n          - 39092:39092\n        restart: unless-stopped\n        environment:\n          KAFKA_BROKER_ID: 2\n          KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n          KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-2:9092,PLAINTEXT_HOST://localhost:39092\n          KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n          KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n          KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2\n        volumes:\n          - ./kafka2/data:/var/lib/kafka/data\n`\n```",
      "solution": "If you've not created your topic some other way, Kafka will default to create your `coverageEvolved` topic used in the code with only one replica, and only one partition.\nIf you kill the broker hosting that one replica, there will be no in sync replica leader that can be produced to.\nYou can use Kafkajs to create topics.\nAlso worth mentioning, there's a transactions topic that only has one replica (you're missing an environment variable for it). This is mainly only relevant for Java clients since transactional producers are enabled by default as of Kafka 3.0",
      "question_score": 12,
      "answer_score": 5,
      "created_at": "2022-07-19T21:23:03",
      "url": "https://stackoverflow.com/questions/73042460/kafka-there-is-no-leader-for-this-topic-partition-as-we-are-in-the-middle-of-a-l"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 66413237,
      "title": "How to determine highest docker-compose file version for the installed docker-compose",
      "problem": "I have docker-compose 1.25.4 on a system, and docker 19.03. How do I know which version of Compose file format it supports, without trying a bunch or searching through release notes etc?\nI determined by trial and error that the highest Compose file format version that docker-compose 1.25.4 supports is 3.7. Therefore, my docker-compose.yml requires `version: \"3.7\"`.\nBut what if I need to determine this programmatically? Or reliably, without trial and error, and without looking at release notes?\nThe docs at https://docs.docker.com/compose/compose-file/compose-versioning/ suggest that highest version should be 3.8 but this does not work. Indeed the release notes indicate that 3.8 became supported as of docker-compose 1.25.5.\nI looked through various docker-compose commands and could not spot anything that would say \"This version of docker-compose supports Compose file version up to x.y\".\nUpdate July 2025:\nThe compose schema `version` is no longer needed since compose v2. Per the official docs:\n\nVersion two of the Docker Compose command-line binary was announced in 2020, is written in Go, and is invoked with docker compose. Compose v2 ignores the version top-level element in the compose.yaml file.\n\nIn fact, you will get a warning if your compose yaml states version:\n```\n`$ docker compose version \nDocker Compose version v2.37.3\n\n# if yaml has a version attribute:\n$ docker compose ... \n... \nWARN[0000] /path/to/docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion\n`\n```\nTherefore this question only relates to pre-v2 compose (v2 came out in 2020, and v1 became officially deprecated in July 2023).",
      "solution": "I figured out that there is no way to find this out via the cli.\nYou need to check the docker-compose github releases for this.\nFor example on Ubuntu: v1.25.0 version of docker-compose which is currently the latest and gets to be installed by default, but it is compatible with the 3.7 at highest.\nhttps://github.com/docker/compose/releases/tag/1.25.0\nThe Compose file format compatibility matrix part is the interesting here.",
      "question_score": 12,
      "answer_score": 5,
      "created_at": "2021-02-28T20:50:22",
      "url": "https://stackoverflow.com/questions/66413237/how-to-determine-highest-docker-compose-file-version-for-the-installed-docker-co"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 65784039,
      "title": "Why a service configured with only `expose` is able to communicate with internet?",
      "problem": "Something confuse me about docker networking. I've a `docker-compose.yml` file which can be simplified like this:\n```\n`version: '3.8'\nservices:\n    foo:\n        ...\n        networks:\n            - main_network\n        ports:\n            - \"3000:3000\"\n    bar:\n        ...\n        networks:\n            - main_network\n        expose:\n          - \"5000\"\nnetworks:\n    main_network:\n`\n```\nAccording to this answer, `expose`...\n\nExpose ports without publishing them to the host machine - they\u2019ll only be accessible to linked services. Only the internal port can be specified.\n\nIf this is true, `bar` should only expose the 5000 port to `foo` service. And It seems to work as expected. If, I run `bash` into `bar` service and execute:\n```\n`$ ss -lntu \n`\n```\nThe 5000 port is opened correctly:\n```\n`Netid    State     Recv-Q  Send-Q   Local Address:Port    Peer Address:Port\n...\ntcp      LISTEN    0       128      0.0.0.0:5000          0.0.0.0:*\n...\n`\n```\nAs expected, from outside of my container, using a web browser for example, I cannot connect to this host. Also, If I run\n```\n`$ nmap -p1-65535 127.0.0.1\n`\n```\nI can verify that only the 3000/TCP port of `foo` service is opened:\n```\n`PORT     STATE SERVICE\n3000/tcp open  ppp\n`\n```\nSo, what I don't understand is that IRL, my `bar` service is able to connect to Mongo Atlas online or ping internet. How does it get it's answer if the ports aren't Exposed/Opened in order to receive it?",
      "solution": "The answer to which you have linked is old and incorrect.\n\nThe `EXPOSE` keyword is primarily a no-op. It's informative (\"this image will offer services on these ports\"), but it doesn't have any operational impact.\nIn older versions of Docker, the `EXPOSE` keyword could be used for service discovery by linked containers, but (a) it still didn't have any operational impact -- the ports were available whether or not there was a matching `EXPOSE` -- and container \"linking\" has been deprecated for a quite some time.\n\nContainers have outbound internet access by default. Outbound access is managed with a simple NAT rule in the host firewall `nat` table (and corresponding rules in the `filter` `FORWARD` chain).  E.g., on a system where I am running Docker 20.10.2, in the `FORWARD` chain I have:\n```\n`-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT\n-A FORWARD -o docker0 -j DOCKER\n-A FORWARD -i docker0 ! -o docker0 -j ACCEPT\n-A FORWARD -i docker0 -o docker0 -j ACCEPT\n`\n```\nThe first rule passes packets that are part of an existing TCP connection. This permits the return packets for your outbound connections.",
      "question_score": 11,
      "answer_score": 48,
      "created_at": "2021-01-19T01:44:15",
      "url": "https://stackoverflow.com/questions/65784039/why-a-service-configured-with-only-expose-is-able-to-communicate-with-internet"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 70488471,
      "title": "Docker with create react app is not updating changes",
      "problem": "I'm creating a React app with docker in WSL2 and create-react-app and everything seems to be working fine except the app is not updating with changes in the code. I mean, when I make a change in the code, the browser should update the changes automatically, but it doesn't and I have to restart the container to see them. I added `CHOKIDAR_USEPOLLING=true` in `ENV` but it's not working either. These are the configuration files:\ndockerfile\n```\n`# pull official base image\nFROM node:16.13.1\n\n# set working directory\nWORKDIR /app\n\n# add `/app/node_modules/.bin` to $PATH\nENV PATH /app/node_modules/.bin:$PATH\n\n# install app dependencies\n\nCOPY package.json ./\nCOPY package-lock.json ./\nRUN npm install\n\n# add app\nCOPY . ./\n\n# start app\nCMD [\"npm\", \"start\"]\n`\n```\ndocker-compose.yml\n```\n`services:\n  react:\n    build: ./frontend\n    command: npm start\n    ports:\n      - 3000:3000\n    volumes: \n      - ./frontend:/app\n    env_file:\n      - 'env.react'\n`\n```\nenv.react\n```\n`CHOKIDAR_USEPOLLING=true\n`\n```\npackage.json\n```\n`{\n  \"name\": \"app\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"dependencies\": {\n    \"@testing-library/jest-dom\": \"^5.16.1\",\n    \"@testing-library/react\": \"^12.1.2\",\n    \"@testing-library/user-event\": \"^13.5.0\",\n    \"mdbreact\": \"^5.2.0\",\n    \"react\": \"^17.0.2\",\n    \"react-dom\": \"^17.0.2\",\n    \"react-painter\": \"^0.4.0\",\n    \"react-router-dom\": \"^6.2.1\",\n    \"react-scripts\": \"5.0.0\",\n    \"sass\": \"^1.45.1\",\n    \"web-vitals\": \"^2.1.2\"\n  },\n  \"scripts\": {\n    \"start\": \"react-scripts start\",\n    \"build\": \"react-scripts build\",\n    \"test\": \"react-scripts test\",\n    \"eject\": \"react-scripts eject\"\n  },\n  \"eslintConfig\": {\n    \"extends\": [\n      \"react-app\",\n      \"react-app/jest\"\n    ]\n  },\n  \"browserslist\": {\n    \"production\": [\n      \">0.2%\",\n      \"not dead\",\n      \"not op_mini all\"\n    ],\n    \"development\": [\n      \"last 1 chrome version\",\n      \"last 1 firefox version\",\n      \"last 1 safari version\"\n    ]\n  }\n}\n\n`\n```\nCan you see what I'm doing wrong? Thanks!",
      "solution": "`CHOKIDAR_USEPOLLING` will no longer work in `react-scripts` ^5, as Webpack started using its own filesystem watcher (Watchpack) as a replacement for Chokidar, try:\n`environment:\n  - WATCHPACK_POLLING=true\n`",
      "question_score": 11,
      "answer_score": 29,
      "created_at": "2021-12-26T18:26:03",
      "url": "https://stackoverflow.com/questions/70488471/docker-with-create-react-app-is-not-updating-changes"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 79269631,
      "title": "Prisma openssl version issue",
      "problem": "I\u2019m encountering an issue while trying to compose Docker instances in my local environment. When I run the docker-compose up command, I get the following error:\n```\n`> 2024-12-10 12:46:12 > laredo-backend-api@0.0.1 start:docker:dev\n> 2024-12-10 12:46:12 > npm install && npm run start:prisma && npm run\n> start:dev 2024-12-10 12:46:12  2024-12-10 12:46:16  2024-12-10\n> 12:46:16 added 1 package, and audited 1011 packages in 3s 2024-12-10\n> 12:46:16  2024-12-10 12:46:16 134 packages are looking for funding\n> 2024-12-10 12:46:16   run `npm fund` for details 2024-12-10 12:46:16 \n> 2024-12-10 12:46:16 6 vulnerabilities (1 low, 2 moderate, 3 high)\n> 2024-12-10 12:46:16  2024-12-10 12:46:16 To address issues that do not\n> require attention, run: 2024-12-10 12:46:16   npm audit fix 2024-12-10\n> 12:46:16  2024-12-10 12:46:16 To address all issues, run: 2024-12-10\n> 12:46:16   npm audit fix --force 2024-12-10 12:46:16  2024-12-10\n> 12:46:16 Run `npm audit` for details. 2024-12-10 12:46:16  2024-12-10\n> 12:46:16 > laredo-backend-api@0.0.1 start:prisma 2024-12-10 12:46:16 >\n> prisma migrate deploy 2024-12-10 12:46:16  2024-12-10 12:46:22\n> Environment variables loaded from .env 2024-12-10 12:46:22 Prisma\n> schema loaded from prisma/schema.prisma 2024-12-10 12:46:22 Datasource\n> \"db\": PostgreSQL database \"laredo\", schema \"public\" at \"postgres:5432\"\n> 2024-12-10 12:46:22  2024-12-10 12:46:17 prisma:warn Prisma failed to\n> detect the libssl/openssl version to use, and may not work as\n> expected. Defaulting to \"openssl-1.1.x\". 2024-12-10 12:46:17 Please\n> manually install OpenSSL and try installing Prisma again. 2024-12-10\n> 12:46:22 prisma:warn Prisma failed to detect the libssl/openssl\n> version to use, and may not work as expected. Defaulting to\n> \"openssl-1.1.x\". 2024-12-10 12:46:22 Please manually install OpenSSL\n> and try installing Prisma again. 2024-12-10 12:46:22 Error: Could not\n> parse schema engine response: SyntaxError: Unexpected token 'E',\n> \"Error load\"... is not valid JSON 2024-12-10 12:46:22 npm notice\n> 2024-12-10 12:46:22 npm notice New minor version of npm available!\n> 10.8.2 -> 10.9.2 2024-12-10 12:46:22 npm notice Changelog: https://github.com/npm/cli/releases/tag/v10.9.2 2024-12-10 12:46:22\n> npm notice To update run: npm install -g npm@10.9.2 2024-12-10\n> 12:46:22 npm notice\n`\n```\nI've done some googling and ran the following commands in order to fix this:\n\n`brew install openssl@3`\n\n`rm -rf node_modules && npm i`\n\n`npx prisma generate`\n\n`npx prisma migrate --dev`\n\nThe follwoing is my prisma generator\n```\n`generator client {\n  provider      = \"prisma-client-js\"\n  binaryTargets = [\"native\", \"linux-musl\", \"darwin-arm64\", \"linux-musl-openssl-3.0.x\"]\n}\n`\n```\nDocker file\n```\n`FROM node:20-alpine AS builder\n\nUSER root\n\nRUN npm i -g npm@~9.3.1\n\nWORKDIR /usr/app\n\nCOPY . ./\n\nRUN chmod +x ./entrypoint.sh\n\nRUN npm ci\n\nRUN npm run build\n\nFROM node:20-alpine AS deployment\n\n# added this to try to fix but still broken\n\nRUN set -ex; \\\n    apk update; \\\n    apk add --no-cache \\\n    openssl\n\nCOPY --from=builder --chown=node:node /usr/app /usr/app\n\nUSER node\n\nCMD [\"sh\"]\n`\n```\nAny pointers are appreciated thanks!",
      "solution": "Thanks for all the pointers everyone the thing that ended up working however was adding `FROM node:20-alpine3.17` to the docker and docker compose files. As shown in this comment https://github.com/nodejs/docker-node/issues/2175#issuecomment-2530559047",
      "question_score": 11,
      "answer_score": 7,
      "created_at": "2024-12-10T21:02:30",
      "url": "https://stackoverflow.com/questions/79269631/prisma-openssl-version-issue"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 69615253,
      "title": "COPY failed: forbidden path outside the build context docker compose",
      "problem": "THis is the project structure\n```\n`Project\n /deployment\n   /Dockerfile\n   /docker-compose.yml\n /services\n   /ui\n     /widget\n`\n```\nHere is the docker file\n```\n`FROM node:14\n\nWORKDIR /app\n\nUSER root\n\n# create new user (only root can do this) and assign owenership to newly created user\nRUN echo \"$(date '+%Y-%m-%d %H:%M:%S'): ======> Setup Appusr\" \\\n    && groupadd -g 1001 appusr \\\n    && useradd -r -u 1001 -g appusr appusr \\\n    && mkdir /home/appusr/ \\\n    && chown -R appusr:appusr /home/appusr/\\\n    && chown -R appusr:appusr /app\n\n# switch to new created user so that appuser will be responsible for all files and has access\nUSER appusr:appusr\n\nCOPY ../services/ui/widget/ /app/\nCOPY ../.env /app/\n\n# installing deps\nRUN npm install \n`\n```\nand docker-compose\nversion: \"3.4\"\n```\n`x-env: &env\n  HOST: 127.0.0.1\n\nservices:\n  widget:\n    build:\n      dockerfile: Dockerfile\n      context: .\n    ports:\n      - 3002:3002\n    command:\n      npm start\n    environment:\n      and from `project/deplyment/docker-compose up` it shows\n```\n`Step 6/8 : COPY ../services/ui/widget/ /app/\nERROR: Service 'widget' failed to build : COPY failed: forbidden path outside the build context: ../services/ui/widget/ ()\n`\n```\nam i setting the wrong context?",
      "solution": "You cannot `COPY` or `ADD` files outside the current path where `Dockerfile` exists.\nYou should either move these two directories to where `Dockerfile` is and then change your `Dockerfile` to:\n```\n`COPY ./services/ui/widget/ /app/\nCOPY ./.env /app/\n`\n```\nOr use `volumes` in `docker-compose`, and remove the two `COPY` lines.\nSo, your `docker-compose` should look like this:\n```\n`x-env: &env\n  HOST: 127.0.0.1\n\nservices:\n  widget:\n    build:\n      dockerfile: Dockerfile\n      context: .\n    ports:\n      - 3002:3002\n    command:\n      npm start\n    environment:\n      And this should be your `Dockerfile if you use `volumes`in`docker-compose`:\n```\n`FROM node:14\n\nWORKDIR /app\n\nUSER root\n\n# create new user (only root can do this) and assign owenership to newly created user\nRUN echo \"$(date '+%Y-%m-%d %H:%M:%S'): ======> Setup Appusr\" \\\n    && groupadd -g 1001 appusr \\\n    && useradd -r -u 1001 -g appusr appusr \\\n    && mkdir /home/appusr/ \\\n    && chown -R appusr:appusr /home/appusr/\\\n    && chown -R appusr:appusr /app\n\n# switch to new created user so that appuser will be responsible for all files and has access\nUSER appusr:appusr\n\n# installing deps\nRUN npm install \n`\n```",
      "question_score": 11,
      "answer_score": 12,
      "created_at": "2021-10-18T13:28:42",
      "url": "https://stackoverflow.com/questions/69615253/copy-failed-forbidden-path-outside-the-build-context-docker-compose"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 68083050,
      "title": "Problem with connecting mongo express service in docker",
      "problem": "I have a docker configuration where I started nginx server. Now I have to run mongo and mongo-express to be able to connect to my database. When I put my configuration for those two in docker-compose.yml and try docker compose up, mongo service starts but mongo express shows this error\n```\n`Could not connect to database using connectionString: mongodb://root:pass@mongodb:27017/\" mongo-express_1 | (node:8) UnhandledPromiseRejectionWarning: MongoNetworkError: failed to connect to server [mongodb:27017] on first connect\n`\n```\nAny help is appreciated, here are my configurations.\ndocker-compose.yml\n```\n`version: \"3\"\n\nservices:\napp:\n  build:\n    context: .\n    dockerfile: Dockerfile\n  image: cryptoterminal\n  container_name: app\n  restart: unless-stopped\n  volumes:\n    - ./:/var/www\n\nwebserver:\n  build:\n    context: .\n    dockerfile: Dockerfile_Nginx\n  image: nginx\n  container_name: webserver\n  restart: unless-stopped\n  ports:\n    - \"8080:80\"\n  volumes: \n    - ./:/var/www\n    - ./config/nginx/:/etc/nginx/conf.d/\n  depends_on:\n    - app\nmongo:\n  image: mongo\n  environment:\n      - MONGO_INITDB_ROOT_USERNAME=root\n      - MONGO_INITDB_ROOT_PASSWORD=pass\nmongo-express:\n  image: mongo-express\n  environment:\n    - ME_CONFIG_MONGODB_SERVER=mongodb\n    - ME_CONFIG_MONGODB_ENABLE_ADMIN=true\n    - ME_CONFIG_MONGODB_ADMINUSERNAME=root\n    - ME_CONFIG_MONGODB_ADMINPASSWORD=pass\n    - ME_CONFIG_BASICAUTH_USERNAME=admin\n    - ME_CONFIG_BASICAUTH_PASSWORD=admin123\n  depends_on:\n      - mongo\n  ports:\n    - \"8888:8081\"\n`\n```\nDockerfile\n```\n`FROM php:7.4-fpm-alpine\n\nWORKDIR /var/www\n\nRUN apk update && apk add \\\n\nbuild-base \\\nvim\n\nRUN addgroup -g 1000 -S www && \\\nadduser -u 1000 -S www -G www\n\nUSER www\n\nCOPY --chown=www:www . /var/www\n\nEXPOSE 9000\n`\n```\nDockerfile_Nginx\n```\n`FROM nginx:alpine\n\nCOPY ./config/nginx/app.conf /etc/nginx/conf.d/app.conf\n\nCOPY . /var/www\n`\n```\n.env file\n```\n`MONGO_DB_URI=mongodb://root:pass@mongodb:27017/crypto-terminal?authSource=admin\n`\n```\napp.conf file\n```\n`server {\n    listen 80;\n    index index.php index.html;\n    error_log /var/log/nginx/error.log;\n    access_log /var/log/nginx/access.log;\n    root /var/www;\n    location ~ \\.php$ {\n        try_files $uri =404;\n        fastcgi_split_path_info ^(.+\\.php)(/.+)$;\n        fastcgi_pass app:9000;\n        fastcgi_index index.php;\n        include fastcgi_params;\n        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;\n        fastcgi_param PATH_INFO $fastcgi_path_info;\n    }\n    location / {\n        try_files $uri $uri/ /index.php?$query_string;\n    }\n}\n`\n```\nfolder structure\n```\n`project_folder\n    php\n        config\n            nginx\n                app.conf\n    .env\n    Dockerfile\n    Dockerfile_Nginx\n    docker-compose.yml\n    index.php\n`\n```",
      "solution": "In your docker-compose file, you call the mongo service `mongo`. That's the name you can address it as on the docker network. In your connection string, you've said\n```\n`mongodb://root:pass@mongodb:27017/\"\n`\n```\nIt should be\n```\n`mongo://root:pass@mongo:27017/\"\n`\n```\nSince the connection string is built using the environment variables in your docker-compose file, the thing to change is\n```\n`- ME_CONFIG_MONGODB_SERVER=mongodb\n`\n```\nto\n```\n`- ME_CONFIG_MONGODB_SERVER=mongo\n`\n```",
      "question_score": 11,
      "answer_score": 13,
      "created_at": "2021-06-22T13:51:32",
      "url": "https://stackoverflow.com/questions/68083050/problem-with-connecting-mongo-express-service-in-docker"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 70006997,
      "title": "Access host from within a docker container",
      "problem": "I have a dockerized app and I use the following docker-compose.yml to run it:\n```\n`version: '3.1'\n\nservices:\n    db:\n        image: mysql:5.7\n        ports:\n            - \"3306:3306\"\n        env_file:\n            - ./docker/db/.env\n        volumes:\n            - ./docker/db/data:/var/lib/mysql:rw\n            - ./docker/db/config:/etc/mysql/conf.d\n        command: mysqld --sql_mode=\"NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION\"\n\n    php:\n        build: ./docker/php/7.4/\n        volumes:\n            - ./docker/php/app.ini:/usr/local/etc/php/conf.d/docker-php-ext-app.ini:ro\n            - ./docker/logs/app:/var/www/app/var/log:cached\n            - .:/var/www/app:cached\n        working_dir: /var/www/app\n        links:\n            - db\n        env_file:\n            - ./docker/php/.env\n\n    webserver:\n        image: nginx:1\n        depends_on:\n            - php\n        volumes:\n            - ./docker/webserver/app.conf:/etc/nginx/conf.d/default.conf:ro\n            - ./docker/logs/webserver/:/var/log/nginx:cached\n            - .:/var/www/app:ro\n        ports:\n            - \"80:80\"\n`\n```\nI have a server that is not dockerized runing on my machine, I can access it via `localhost:3000`. I would like my `php` service to be able to access it.\nI found people suggesting to add to following to my `php` service configuration:\n```\n`extra_hosts:\n    - \"host.docker.internal:host-gateway\"\n`\n```\nBut when I add this, then `docker-compose up -d` and try `docker exec -ti php_1 curl http://localhost:3000`, I get `curl: (7) Failed to connect to localhost port 3000 after 0 ms: Connection refused`. I have the same error when I try to `curl http://host.docker.internal:3000`.\nI desperatly tried to add a port mapping to the `php` container:\n```\n`ports:\n    - 3000:3000\n`\n```\nBut then when I start the services I have the following error:\n```\n`ERROR: for php_1  Cannot start service php: driver failed programming external connectivity on endpoint php_1 (9dacd567ee97b9a46699969f9704899b04ed0b61b32ff55c67c27cb6867b7cef): Error starting userland proxy: listen tcp4 0.0.0.0:3000: bind: address already in use\n\nERROR: for php  Cannot start service php: driver failed programming external connectivity on endpoint php_1 (9dacd567ee97b9a46699969f9704899b04ed0b61b32ff55c67c27cb6867b7cef): Error starting userland proxy: listen tcp4 0.0.0.0:3000: bind: address already in use\n`\n```\nWhich is obvious since my server is running on that `3000` port.\nI also tried to add\n```\n`network_mode: host\n`\n```\nBut it fails because I already have a `links`. I get the following error:\n```\n`Cannot create container for service php: conflicting options: host type networking can't be used with links.\n`\n```\nI am running docker v20.10.6 on Ubuntu 21.10.\nAny help appreciated, thanks in advance!",
      "solution": "Make sure you are using version of docker that supports host.docker.internal.\nIf you are using linux version, then 20.10+ supports it.\nFor other systems you should probably consult documentation and probably some issues on github of docker-for-linux / other projects OS revelant.\nAfter that...\nMake sure extra_hosts is direct child of php service:\n```\n`php:\n    extra_hosts:\n        host.docker.internal: host-gateway\n    build: ./docker/php/7.4/\n`\n```\nTry using `ping host.docker.internal` first to check whether your host machine responds correctly.\nMake sure that your service on port 3000 is working properly and there is no firewall issue.\nRemember that `localhost` means always local ip from current container point of view. It means that `localhost` inside container maps to local container IP and not your host machine IP. This is a reason for sending extra_hosts section.\nAlso docker.host.internal is not your host loopback interface.\nIf service you are trying to reach listens only on localhost interface then there is no chance to reach it without doing some magic with iptables / firewall.\nYou can check what service is listening on which interface / ip address running following command on your host machine: `netstat -tulpn`\nThis should return something like following output:\n```\n`$ netstat -tulpn\n(Not all processes could be identified, non-owned process info will not be shown, you would have to be root to see it all.)\nActive Internet connections (only servers)\nProto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    \ntcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      -                   \ntcp        0      0 127.0.0.1:631           0.0.0.0:*               LISTEN      -                   \ntcp        0      0 0.0.0.0:39195           0.0.0.0:*               LISTEN      -                   \ntcp        0      0 0.0.0.0:3306            0.0.0.0:*               LISTEN      -                   \ntcp6       0      0 :::22                   :::*                    LISTEN      -                   \ntcp6       0      0 ::1:631                 :::*                    LISTEN      -                   \n`\n```\nFrom docker container I can reach services listening on 0.0.0.0 (all interfaces) but cannot access 631 port as it is only on 127.0.0.1\n```\n`$ docker run --rm -it --add-host=\"host.docker.internal:host-gateway\" busybox\n/ # ping host.docker.internal\nPING host.docker.internal (172.17.0.1): 56 data bytes\n64 bytes from 172.17.0.1: seq=0 ttl=64 time=0.124 ms\n64 bytes from 172.17.0.1: seq=1 ttl=64 time=0.060 ms\n^C\n--- host.docker.internal ping statistics ---\n2 packets transmitted, 2 packets received, 0% packet loss\nround-trip min/avg/max = 0.060/0.092/0.124 ms\n/ # telnet host.docker.internal 631\ntelnet: can't connect to remote host (172.17.0.1): Connection refused\n/ # telnet host.docker.internal 22\nConnected to host.docker.internal\nSSH-2.0-OpenSSH_8.6\n`\n```",
      "question_score": 11,
      "answer_score": 9,
      "created_at": "2021-11-17T16:16:37",
      "url": "https://stackoverflow.com/questions/70006997/access-host-from-within-a-docker-container"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 67879603,
      "title": "Define specific docker-compose file to use for AWS Elastic Beanstalk Deployment",
      "problem": "Before I run `eb create` command, how can I tell Elastic Beanstalk to use a DIFFERENT `docker-compose` file?\nFor example, my project directory:\n```\n`HelloWorldDocker\n\u251c\u2500\u2500.elasticbeanstalk\n\u2502  \u2514\u2500\u2500config.yml\n\u251c\u2500\u2500app/\n\u251c\u2500\u2500proxy/\n\u2514\u2500\u2500docker-compose.prod.yml\n\u2514\u2500\u2500docker-compose.yml\n`\n```\n\nMy `docker-compose.yml` is what I use for local development\nMy `docker-compose.prod.yml` is what I want to use for production\n\nIs there a way to define this configuration before running the `eb create` command from the EB CLI?\nStating the obvious: I realize I could use `docker-compose.yml` for my production file and a `docker-compose.dev.yml` for my local development but then running the `docker-compose up` command becomes more tedious locally (ie: `docker-compose -f docker-compose.dev.yml up --build...`). Further, I'm mainly interested if this is even possible as I'm learning Elastic Beanstalk, and how I could do it if I wanted to.\n\nEDIT / UPDATE: June 11, 2021\nI attempted to rename `docker-compose.prod.yml` to `docker-compose.yml` in `.ebextensions/docker-settings.config` with this:\n```\n`container_commands:\n  rename_docker_compose:\n    command: mv docker-compose.prod.yml docker-compose.yml\n`\n```\n\n>eb deploy:\n\n```\n`2021-06-11 16:44:45    ERROR   Instance deployment failed.\n  For details, see 'eb-engine.log'.\n2021-06-11 16:44:45    ERROR   Instance deployment: Both \n  'Dockerfile' and 'Dockerrun.aws.json' are missing in your\n  source bundle. Include at least one of them. The deployment\n  failed.\n`\n```\nIn eb-engine.log, I see:\n```\n`2021/06/11 16:44:45.818876 [ERROR] An error occurred during \n  execution of command [app-deploy] - [Docker Specific Build\n  Application]. Stop running the command. Error: Dockerfile and\n  Dockerrun.aws.json are both missing, abort deployment\n`\n```\nBased on my testing, this is due to AWS needing to call `/bin/sh -c docker-compose config` before getting to the later steps of `container_commands`.\n\nEdit / Update #2\nIf I use `commands` instead of `container_commands`:\n```\n`commands:\n  rename_docker_compose:\n    command: mv docker-compose.prod.yml docker-compose.yml\n    cwd: /var/app/staging\n`\n```\nit does seem to do the replacement successfully:\n```\n`2021-06-11 21:40:44,809 P1957 [INFO] Command find_docker_compose_file\n2021-06-11 21:40:45,086 P1957 [INFO] -----------------------Command Output-----------------------\n2021-06-11 21:40:45,086 P1957 [INFO]    ./var/app/staging/docker-compose.prod.yml\n2021-06-11 21:40:45,086 P1957 [INFO] ------------------------------------------------------------\n2021-06-11 21:40:45,086 P1957 [INFO] Completed successfully.\n`\n```\nbut I still am hit with:\n```\n`2021/06/11 21:40:45.192780 [ERROR] An error occurred during\n  execution of command [app-deploy] - [Docker Specific Build \n  Application]. Stop running the command. Error: Dockerfile and \n  Dockerrun.aws.json are both missing, abort deployment \n`\n```\n\nEDIT / UPDATE: June 12, 2021\nI'm on a Windows 10 machine. Before running `eb deploy` command locally, I opened up Git Bash which uses MINGW64 terminal. I `cd`d to the `prebuild` directory where `build.sh` exists. I ran:\n```\n`chmod +x build.sh\n`\n```\nIf I do `ls -l`, it returns:\n```\n`-rwxr-xr-x 1 Jarad 197121 58 Jun 12 12:31 build.sh*\n`\n```\nI think this means the file is executable.\nI then committed to git.\nI then ran `eb deploy`.\nI am seeing a `build.sh: permission denied` error in eb-engine.log. Below is an excerpt of the relevant portion.\n```\n`...\n2021/06/12 19:41:38.108528 [INFO] application/zip\n\n2021/06/12 19:41:38.108541 [INFO] app source bundle is zip file ...\n2021/06/12 19:41:38.108547 [INFO] extracting /opt/elasticbeanstalk/deployment/app_source_bundle to /var/app/staging/\n2021/06/12 19:41:38.108556 [INFO] Running command /bin/sh -c /usr/bin/unzip -q -o /opt/elasticbeanstalk/deployment/app_source_bundle -d /var/app/staging/\n2021/06/12 19:41:38.149125 [INFO] finished extracting /opt/elasticbeanstalk/deployment/app_source_bundle to /var/app/staging/ successfully\n2021/06/12 19:41:38.149142 [INFO] Executing instruction: RunAppDeployPreBuildHooks\n2021/06/12 19:41:38.149190 [INFO] Executing platform hooks in .platform/hooks/prebuild/\n2021/06/12 19:41:38.149249 [INFO] Following platform hooks will be executed in order: [build.sh]\n2021/06/12 19:41:38.149255 [INFO] Running platform hook: .platform/hooks/prebuild/build.sh\n2021/06/12 19:41:38.149457 [ERROR] An error occurred during execution of command [app-deploy] - [RunAppDeployPreBuildHooks]. Stop running the command. Error: Command .platform/hooks/prebuild/build.sh failed with error fork/exec .platform/hooks/prebuild/build.sh: permission denied \n\n2021/06/12 19:41:38.149464 [INFO] Executing cleanup logic\n2021/06/12 19:41:38.149572 [INFO] CommandService Response: {\"status\":\"FAILURE\",\"api_version\":\"1.0\",\"results\":[{\"status\":\"FAILURE\",\"msg\":\"Engine execution has encountered an error.\",\"returncode\":1,\"events\":[{\"msg\":\"Instance deployment failed. For details, see 'eb-engine.log'.\",\"timestamp\":1623526898,\"severity\":\"ERROR\"}]}]}\n\n2021/06/12 19:41:38.149706 [INFO] Platform Engine finished execution on command: app-deploy\n...\n`\n```\nAny idea why I am getting a permission denied error?\n\nMy Conclusion From This Madness\nElastic Beanstalk's EB CLI `eb deploy` command does not zip files (the `app_source_bundle` it creates) correctly on Windows machines.\nProof\nI was able to recreate Marcin's example by zipping it locally and manually uploading it through the Elastic Beanstalk online interface. When I do that and check the source bundle, it shows that `build.sh` does have executable permissions (-rwxr-xr-x).\n```\n`[root@ip-172-31-11-170 deployment]# zipinfo app_source_bundle\nArchive:  app_source_bundle\nZip file size: 993 bytes, number of entries: 5\ndrwxr-xr-x  3.0 unx        0 bx stor 21-Jun-13 03:08 .platform/\ndrwxr-xr-x  3.0 unx        0 bx stor 21-Jun-13 03:08 .platform/hooks/\ndrwxr-xr-x  3.0 unx        0 bx stor 21-Jun-13 03:08 .platform/hooks/prebuild/\n-rwxr-xr-x  3.0 unx       58 tx defN 21-Jun-13 03:09 .platform/hooks/prebuild/build.sh\n-rw-r--r--  3.0 unx       98 tx defN 21-Jun-13 03:08 docker-compose.prod.yml\n`\n```\nWhen I initialize and create using the EB CLI and the exact same files, `build.sh` does NOT have executable permissions (-rw-rw-rw-).\n```\n`[ec2-user@ip-172-31-5-39 deployment]$ zipinfo app_source_bundle\nArchive:  app_source_bundle\nZip file size: 1092 bytes, number of entries: 5\ndrwxrwxrwx  2.0 fat        0 b- stor 21-Jun-12 20:32 ./\n-rw-rw-rw-  2.0 fat       98 b- defN 21-Jun-12 20:08 docker-compose.prod.yml\n-rw-rw-rw-  2.0 fat      993 b- defN 21-Jun-12 20:15 myzip.zip\ndrwxrwxrwx  2.0 fat        0 b- stor 21-Jun-12 20:08 .platform/hooks/prebuild/\n-rw-rw-rw-  2.0 fat       58 b- defN 21-Jun-12 20:09 .platform/hooks/prebuild/build.sh\n`\n```\nTherefore, I think this is a bug with AWS EB CLI deploy command in regards to how it zips files for Windows users.",
      "solution": "You can't do this from command level. But I guess you could write container_commands script to rename your `docker-compose` file from `docker-compose.dev.yml` to `docker-compose.yml`:\n\nYou can use the container_commands key to execute commands that affect your application source code. Container commands run after the application and web server have been set up and the application version archive has been extracted, but before the application version is deployed.\n\nUPDATE 12 Jun 2021\nI tried to replicate the issue using simplified setup with just `docker-compose.prod.yml` and `Docker running on 64bit Amazon Linux 2` 3.4.1 EB platform.\ndocker-compose.prod.yml\n```\n`version: \"3\"\n\nservices:\n    client:\n        image: nginx\n        ports:\n            - 80:80\n`\n```\nI can confirm and reproduce the issue with `container_commands`. So in my tests, the solution was to setup prebuild deployment hook.\nSo my deployment zip had the structure:\n```\n`\u251c\u2500\u2500 docker-compose.prod.yml\n\u2514\u2500\u2500 .platform\n    \u2514\u2500\u2500 hooks\n        \u2514\u2500\u2500 prebuild\n            \u2514\u2500\u2500 build.sh\n\n`\n```\nwhere\nbuild.sh\n```\n`#!/bin/bash\n\nmv docker-compose.prod.yml docker-compose.yml\n`\n```\nI also made the `build.sh` executable before creating deployment zip.\napp_source_bundle permissions (`zipinfo -l`)\n```\n`Zip file size: 1008 bytes, number of entries: 5\ndrwxr-xr-x  3.0 unx        0 bx        0 stor 21-Jun-12 07:37 .platform/\ndrwxr-xr-x  3.0 unx        0 bx        0 stor 21-Jun-12 07:37 .platform/hooks/\ndrwxr-xr-x  3.0 unx        0 bx        0 stor 21-Jun-12 07:38 .platform/hooks/prebuild/\n-rwxr-xr-x  3.0 unx       77 tx       64 defN 21-Jun-12 07:24 .platform/hooks/prebuild/build.sh\n-rw-r--r--  3.0 unx       92 tx       68 defN 21-Jun-12 07:01 docker-compose.prod.ym\n`\n```",
      "question_score": 11,
      "answer_score": 3,
      "created_at": "2021-06-08T01:05:20",
      "url": "https://stackoverflow.com/questions/67879603/define-specific-docker-compose-file-to-use-for-aws-elastic-beanstalk-deployment"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 65608258,
      "title": "AWS Parameter Store inside docker-compose.yml",
      "problem": "I save all secret data into AWS Parameter Store, my app run with docker-compose and I try to evaluate `aws ssm get-parameters` just inside `docker-compose.yaml`:\n```\n`version: \"3.7\"\n\nservices:\n  db:\n    image: postgres\n    restart: always\n    volumes:\n      - /home/ec2-user/dbdata:/var/lib/postgresql/data\n    environment:\n      POSTGRES_USER: $$(aws ssm get-parameters --name DB_USERNAME --region eu-north-1 --output text --query Parameters[].Value)\n      POSTGRES_PASSWORD: $$(aws ssm get-parameters --name DB_PASSWORD --region eu-north-1 --output text --with-decryption --query Parameters[].Value)\n      POSTGRES_DB: $$(aws ssm get-parameters --name DB_NAME --region eu-north-1 --output text --query Parameters[].Value)\n    ports:\n      - 5432:5432\n\n  auth:\n    build: .\n    restart: always\n    environment:\n      DB_ENDPOINT: $$(aws ssm get-parameters --name DB_ENDPOINT --region eu-north-1 --output text --query Parameters[].Value)\n      DB_USERNAME: $$(aws ssm get-parameters --name DB_USERNAME --region eu-north-1 --output text --query Parameters[].Value)\n      DB_PASSWORD: $$(aws ssm get-parameters --name DB_PASSWORD --region eu-north-1 --output text --with-decryption --query Parameters[].Value)\n      JWT_KEY: $$(aws ssm get-parameters --name JWT_KEY --region eu-north-1 --output text --query Parameters[].Value)\n      JWT_EXPIRED: $$(aws ssm get-parameters --name JWT_EXPIRED --region eu-north-1 --output text --query Parameters[].Value)\n    depends_on:\n      - db\n    links:\n      - db\n    ports:\n      - 80:80\n`\n```\nBut construction `$$()` doesn't work.\nInstead of expected values from the parameter store, I'm getting\n\n$(get-parameters --name DB_ENDPOINT --region eu-north-1 --output text\n--query Parameters[].Value)\n\nI know how to do this with prepare a script like `export MY_VAR=\"$(get-parameters ...)\"` that is understood.\nBut the question is can I fetch Parameter Store values just in `docker-compose.yaml` if yes, how?",
      "solution": "Unfortunally there's no way to do this.\nAs you said, you have to set the environment variable before running command `docker-compose up`:\n`export MY_KEY=$(aws ssm get-parameter --name \"YOUR-PARAMETER-NAME\" [--with-decryption] --output text --query Parameter.Value)\n`\nand then in your `docker-compose.yml` file, set the environment variable:\n`version: \"3.7\"\n\nservices:\n  ...\n    environment:\n      MY_SSM_KEY: ${MY_KEY}\n  ...\n`\nNow, `docker-compose up` will run as desired.",
      "question_score": 11,
      "answer_score": 2,
      "created_at": "2021-01-07T08:33:53",
      "url": "https://stackoverflow.com/questions/65608258/aws-parameter-store-inside-docker-compose-yml"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 71863593,
      "title": "Docker: PermissionError: [Errno 13] Permission denied",
      "problem": "I got this when I was running docker-compose\n`Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post \"http://%2Fvar%2Frun%2Fdocker.sock/v1.24/build?buildargs=%7B%7D&cachefrom=%5B%5D&cgroupparent=&cpuperiod=0&cpuquota=0&cpusetcpus=&cpusetmems=&cpushares=0&dockerfile=Dockerfile&labels=%7B%7D&memory=0&memswap=0&networkmode=default&rm=1&shmsize=0&t=iramiro-api.jar&target=&ulimits=null&version=1\": dial unix /var/run/docker.sock: connect: permission denied\nRunning docker compose\n[6099] Failed to execute script docker-compose\nTraceback (most recent call last):\n  File \"urllib3/connectionpool.py\", line 677, in urlopen\n  File \"urllib3/connectionpool.py\", line 392, in _make_request\n  File \"http/client.py\", line 1252, in request\n  File \"http/client.py\", line 1298, in _send_request\n  File \"http/client.py\", line 1247, in endheaders\n  File \"http/client.py\", line 1026, in _send_output\n  File \"http/client.py\", line 966, in send\n  File \"docker/transport/unixconn.py\", line 43, in connect\nPermissionError: [Errno 13] Permission denied\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"requests/adapters.py\", line 449, in send\n  File \"urllib3/connectionpool.py\", line 727, in urlopen\n  File \"urllib3/util/retry.py\", line 403, in increment\n  File \"urllib3/packages/six.py\", line 734, in reraise\n  File \"urllib3/connectionpool.py\", line 677, in urlopen\n  File \"urllib3/connectionpool.py\", line 392, in _make_request\n  File \"http/client.py\", line 1252, in request\n  File \"http/client.py\", line 1298, in _send_request\n  File \"http/client.py\", line 1247, in endheaders\n  File \"http/client.py\", line 1026, in _send_output\n  File \"http/client.py\", line 966, in send\n  File \"docker/transport/unixconn.py\", line 43, in connect\nurllib3.exceptions.ProtocolError: ('Connection aborted.', PermissionError(13, 'Permission denied'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"docker/api/client.py\", line 205, in _retrieve_server_version\n  File \"docker/api/daemon.py\", line 181, in version\n  File \"docker/utils/decorators.py\", line 46, in inner\n  File \"docker/api/client.py\", line 228, in _get\n  File \"requests/sessions.py\", line 543, in get\n  File \"requests/sessions.py\", line 530, in request\n  File \"requests/sessions.py\", line 643, in send\n  File \"requests/adapters.py\", line 498, in send\nrequests.exceptions.ConnectionError: ('Connection aborted.', PermissionError(13, 'Permission denied'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"bin/docker-compose\", line 3, in \n  File \"compose/cli/main.py\", line 67, in main\n  File \"compose/cli/main.py\", line 123, in perform_command\n  File \"compose/cli/command.py\", line 69, in project_from_options\n  File \"compose/cli/command.py\", line 132, in get_project\n  File \"compose/cli/docker_client.py\", line 43, in get_client\n  File \"compose/cli/docker_client.py\", line 170, in docker_client\n  File \"docker/api/client.py\", line 188, in __init__\n  File \"docker/api/client.py\", line 213, in _retrieve_server_version\ndocker.errors.DockerException: Error while fetching server API version: ('Connection aborted.', PermissionError(13, 'Permission denied'))\n[Pipeline] }\n[Pipeline] // stage\n[Pipeline] }\n[Pipeline] // withEnv\n[Pipeline] }\n[Pipeline] // withEnv\n[Pipeline] }\n[Pipeline] // node\n[Pipeline] End of Pipeline\nERROR: script returned exit code 255\nFinished: FAILURE\n`",
      "solution": "You  just need to run this command, it saved my day\n`sudo chmod 777 /var/run/docker.sock\n`",
      "question_score": 10,
      "answer_score": 38,
      "created_at": "2022-04-13T22:37:48",
      "url": "https://stackoverflow.com/questions/71863593/docker-permissionerror-errno-13-permission-denied"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 71717395,
      "title": "Getting Error: connect ECONNREFUSED 127.0.0.1:6379 in docker-compose while connecting redis",
      "problem": "I am getting connection  Error: connect ECONNREFUSED 127.0.0.1:6379 while working with docker-compose to use Redis with node js.\nI used the same host name and service name in Redis but still got an error.\nMy node js code is:\n```\n`const express = require('express');\nconst redis = require('redis');\n\nconst client = redis.createClient({\n    port: 6379,\n    host: 'redis'\n});\nclient.connect();\nclient.on('connect', (err)=>{\n    if(err) throw err;\n    else console.log('Redis Connected..!');\n});\n\nconst app = express();\napp.get('/',async (req,res)=>{\n    let key = req.query['name'];\n    if(key){\n        let value = await client.get(key);\n        if(value){\n            value++;\n            client.set(key,value);\n            res.send(`Hello ${key}, ${value}!`);\n            console.log(`${key}, ${value}`);\n        }\n        else{\n            value = 1;\n            client.set(key,value); \n            res.send(`Hello ${key}, ${value}!`);\n            console.log(`${key}, ${value}`);\n        }\n    }\n    else{\n        console.log(\"Name not passed!\");\n        res.send(\"Hello World!\");\n    }\n});\nconst port = 3000;\napp.listen(port,()=>{\n    console.log(`App is listening at http://localhost:${port}`);\n});\n`\n```\nMy docker-compose.yml file is:\n```\n`version: \"3\"\nservices:\n  redis: \n    image: redis:latest\n    container_name: client\n    restart: unless-stopped\n    expose:\n      - 6379\n  app:\n    depends_on:\n      - redis\n    build:\n      context: .\n      dockerfile: Dockerfile\n    container_name: app\n    restart: on-failure\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - .:/app\n`\n```\nand what I am getting on console is\n```\n`C:\\Users\\ashok\\Desktop\\Practice>docker-compose up\n[+] Running 3/3\n - Network practice_default  Created                                                                                                        0.1s\n - Container client          Created                                                                                                        1.1s\n - Container app             Created                                                                                                        0.3s\nAttaching to app, client\nclient  | 1:C 02 Apr 2022 11:08:28.885 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\nclient  | 1:C 02 Apr 2022 11:08:28.886 # Redis version=6.2.6, bits=64, commit=00000000, modified=0, pid=1, just started\nclient  | 1:C 02 Apr 2022 11:08:28.886 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\nclient  | 1:M 02 Apr 2022 11:08:28.889 * monotonic clock: POSIX clock_gettime\nclient  | 1:M 02 Apr 2022 11:08:28.890 * Running mode=standalone, port=6379.\nclient  | 1:M 02 Apr 2022 11:08:28.891 # Server initialized\nclient  | 1:M 02 Apr 2022 11:08:28.891 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.\nclient  | 1:M 02 Apr 2022 11:08:28.893 * Ready to accept connections\napp     |\napp     | > practice@1.0.0 start\napp     | > node app.js\napp     |\napp     | Server is live at port: 3000\napp     | node:internal/process/promises:279\napp exited with code 1\napp     |             triggerUncaughtException(err, true /* fromPromise */);\napp     |             ^\napp     |\napp     | Error: connect ECONNREFUSED 127.0.0.1:6379\napp     |     at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1187:16)\napp     | Emitted 'error' event on Commander instance at:\napp     |     at RedisSocket. (/app/node_modules/@node-redis/client/dist/lib/client/index.js:339:14)\napp     |     at RedisSocket.emit (node:events:527:28)\napp     |     at RedisSocket._RedisSocket_connect (/app/node_modules/@node-redis/client/dist/lib/client/socket.js:117:14)\napp     |     at processTicksAndRejections (node:internal/process/task_queues:96:5)\napp     |     at async Commander.connect (/app/node_modules/@node-redis/client/dist/lib/client/index.js:162:9) {\napp     |   errno: -111,\napp     |   code: 'ECONNREFUSED',\napp     |   syscall: 'connect',\napp     |   address: '127.0.0.1',\napp     |   port: 6379\napp     | }\n`\n```\nwhat can I do to resolve it?",
      "solution": "I had tried specifying host and port in the client creation too, and that didn't work.\nI found that using\n```\n`const client = redis.createClient({\nurl: 'redis://redis:6379'\n});\n`\n```\nfixed the issue for me.",
      "question_score": 10,
      "answer_score": 19,
      "created_at": "2022-04-02T14:06:03",
      "url": "https://stackoverflow.com/questions/71717395/getting-error-connect-econnrefused-127-0-0-16379-in-docker-compose-while-conne"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 76013265,
      "title": "How to do a MongoDB 6 single node replicaset with Docker compose?",
      "problem": "On my local Mac M1 PRO, I use for several month now a Docker compose to mount a single node replicaset based on MongoDB 5.\n```\n`version: \"3.9\"\nservices:\n  mongodb:\n    image: mongo:5\n    command: --replSet rs0\n    ports:\n      - '28017:27017'\n    healthcheck:\n      test: echo 'db.runCommand(\"ping\").ok' | mongo localhost:27017/admin --quiet\n      interval: 2s\n      timeout: 3s\n      retries: 5\n\n  mongo-init:\n    image: mongo:5\n    restart: \"no\"\n    depends_on:\n      mongodb:\n        condition: service_healthy\n    command: >\n      mongo --host mongodb:27017 --eval\n      '\n      rs.initiate( {\n        _id : \"rs0\",\n        members: [\n          { _id: 0, host: \"localhost:27017\" }\n        ]\n      })\n      '\n`\n```\nIt works well and I have a simple MongoDB 5 replicaset. Now, I want the same thing with MongoDB 6. So, I modify the image from mongodb:5 to mongodb:6 but the replicaset didn't mount.\nI have this error:\n```\n`{\"t\":{\"$date\":\"2023-04-14T08:52:52.326+00:00\"},\"s\":\"I\",  \"c\":\"-\", \"id\":4939300, \"ctx\":\"monitoring-keys-for-HMAC\",\"msg\":\"Failed to refresh key cache\",\"attr\":{\"error\":\"NotYetInitialized: Cannot use non-local read concern until replica set is finished initializing.\",\"nextWakeupMillis\":24600}}\n`\n```\nI don't need TLS or encryption fancy feature.\nWhat is wrong with my configuration?",
      "solution": "The mongo6 docker container has changed some internals, here is a working version for me\nPort differences arent relevant except for setting your correct ones in the commands/healthchecks.\n```\n`  mongo:\n    image: mongo:6\n    command: [--replSet, my-replica-set, --bind_ip_all, --port, \"30001\"]\n    ports:\n      - 30001:30001\n    healthcheck:\n      test: test $$(mongosh --port 30001 --quiet --eval \"try {rs.initiate({_id:'my-replica-set',members:[{_id:0,host:\\\"mongo:30001\\\"}]})} catch(e) {rs.status().ok}\") -eq 1\n      interval: 10s\n      start_period: 30s\n`\n```",
      "question_score": 10,
      "answer_score": 17,
      "created_at": "2023-04-14T10:56:52",
      "url": "https://stackoverflow.com/questions/76013265/how-to-do-a-mongodb-6-single-node-replicaset-with-docker-compose"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 68659216,
      "title": "Docker Image &gt; 1GB in size from python:3.8.3-alpine",
      "problem": "I'm pretty new to docker and, although I've read lots of articles, tutorials and watched YouTube videos, I'm still finding that my image size is in excess of 1 GB when the alpine image for Python is only about 25 MB (if I'm reading this correctly!).\nI'm trying to work out how to make it smaller (if in fact it needs to be).\n[Note: I've been following tutorials to create what I have below. Most of it makes sense .. but some of it feels like voodoo]\nHere is my Dockerfile:\n```\n`FROM python:3.8.3-alpine\n\nENV PYTHONDONTWRITEBYTECODE 1\nENV PYTHONUNBUFFERED 1\n\nRUN mkdir -p /home/app\n\nRUN addgroup -S app && adduser -S app -G app\n\nENV HOME=/home/app\nENV APP_HOME=/home/app/web\nRUN mkdir $APP_HOME\nRUN mkdir $APP_HOME/staticfiles\nRUN mkdir $APP_HOME/mediafiles\nWORKDIR $APP_HOME\n\nRUN pip install --upgrade pip\n\nCOPY requirements.txt .\n\nRUN apk update \\\n    && apk add --virtual build-deps gcc python3-dev musl-dev \\\n    && apk add postgresql-dev \\\n    && apk add jpeg-dev zlib-dev libjpeg \\\n    && apk add --update --no-cache postgresql-client\n\nRUN pip install -r requirements.txt\n\nRUN apk del build-deps\n\nCOPY entrypoint.prod.sh $APP_HOME\n\nCOPY . $APP_HOME\n\nRUN chown -R app:app $APP_HOME\n\nUSER app\n\nENTRYPOINT [\"/home/app/web/entrypoint.prod.sh\"]\n`\n```\nUsing `Pillow` and `psycopg2-binary` has caused a world of confusion and hurt. Particularly with the following:\n```\n`RUN apk update \\\n    && apk add --virtual build-deps gcc python3-dev musl-dev \\\n    && apk add postgresql-dev \\\n    && apk add jpeg-dev zlib-dev libjpeg \\\n    && apk add --update --no-cache postgresql-client\n\nRUN pip install -r requirements.txt\n\nRUN apk del build-deps\n`\n```\nThis was originally:\n```\n`RUN apk update \\\n    && apk add --virtual build-deps gcc python3-dev musl-dev \\\n    && apk add postgresql \\\n    && apk add postgresql-dev \\\n    && apk add --update --no-cache postgresql-client \\\n    && pip install psycopg2-binary \\\n    && apk add jpeg-dev zlib-dev libjpeg \\\n    && pip install Pillow \\\n    && apk del build-deps\n`\n```\nI really have no idea how much of the above I need to make it work. I think there might be a way of reducing the build.\nI know there is a way to build the original image and then use that to transfer things over, but the only tutorials are confusing and I am struggling to get my head around this without adding more complexity. I really wish I had someone who could just explain it in person.\nI also don't know if the size of the image is coming from the `requirements.txt` file. I'm using django and there are a number of requirements:\nrequirements.txt\n```\n`asgiref==3.4.1\nBabel==2.9.1\nboto3==1.18.12\nbotocore==1.21.12\ncertifi==2021.5.30\ncharset-normalizer==2.0.4\ncrispy-bootstrap5==0.4\ndefusedxml==0.7.1\ndiff-match-patch==20200713\nDjango==3.2.5\ndjango-anymail==8.4\ndjango-compat==1.0.15\ndjango-crispy-forms==1.12.0\ndjango-environ==0.4.5\ndjango-extensions==3.1.3\ndjango-hijack==2.3.0\ndjango-hijack-admin==2.1.10\ndjango-import-export==2.5.0\ndjango-money==2.0.1\ndjango-recaptcha==2.0.6\ndjango-social-share==2.2.1\ndjango-storages==1.11.1\net-xmlfile==1.1.0\nfontawesomefree==5.15.3\ngunicorn==20.1.0\nidna==3.2\njmespath==0.10.0\nMarkupPy==1.14\nodfpy==1.4.1\nopenpyxl==3.0.7\nPillow==8.3.1\npsycopg2-binary==2.9.1\npy-moneyed==1.2\npython-dateutil==2.8.2\npytz==2021.1\nPyYAML==5.4.1\nrequests==2.26.0\ns3transfer==0.5.0\nsix==1.16.0\nsqlparse==0.4.1\nstripe==2.60.0\ntablib==3.0.0\nurllib3==1.26.6\nxlrd==2.0.1\nxlwt==1.3.0\n`\n```\nThe question I have is, how do I make the image smaller. Does it need to be smaller?\nI'm just trying to find the best way to deploy the Django app to Digitalocean and there is a world of confusion with so many approaches and tutorials etc. I don't know if it makes it easier to use docker. Do I just use their App Platform? Will that provide SSL? What are the advantages to using docker etc?\ndocker-compose file (for reference)\n```\n`version: '3.7'\n\nservices:\n  web:\n    build:\n      context: .\n      dockerfile: Dockerfile.prod\n    command: gunicorn maffsguru.wsgi:application --bind 0.0.0.0:8000\n    volumes:\n      - static_volume:/home/app/web/staticfiles\n      - media_volume:/home/app/web/mediafiles\n    expose:\n      - 8000\n    env_file:\n      - .env.docker\n    depends_on:\n      - db\n  db:\n    image: postgres:12.0-alpine\n    env_file:\n      - .env.docker\n    volumes:\n      - postgres_data:/var/lib/postgresql/data/\n    ports:\n      - 5432:5432\n  nginx:\n    build: ./nginx\n    volumes:\n      - static_volume:/home/app/web/staticfiles\n      - media_volume:/home/app/web/mediafiles\n    ports:\n      - 1337:80\n    depends_on:\n      - web\n\nvolumes:\n  postgres_data:\n  static_volume:\n  media_volume:\n`\n```\nJust to say ... the above all seems to work ... but I don't know if the size of the image etc is going to be a problem?\nI am also confused as to why Nginx seems to need me to do http://0.0.0.0:1337 to view the site. Isn't the whole point to view it by navigating to http://0.0.0.0/\nThanks for any advice or guidance you might be able to give and apologies for the random nature of my questions",
      "solution": "welcome to Docker! It can be quite the thing to wrap one's head around, especially when beginning, but you're asking really valid questions that are all pertinent\nReducing Size\nHow to\nA great place to start is Docker's own Dockerfile best practices page:\nhttps://docs.docker.com/develop/develop-images/dockerfile_best-practices/\nThey explain neatly how your each directve (`COPY`, `RUN`, `ENV`, etc) all create additional layers, increasing your containers size. Importantly, they show how to reduce your image size by minimising the different directives. They key to alot of minimisation is chaining commands in `RUN` statements with the use of `&&`.\nSomething else I note in your Dockerfile is one specific line:\n```\n`COPY . $APP_HOME\n`\n```\nNow, depending on how you build your container (Specifically, what folder you pass to Docker as the context), this will copy EVERYTHING in that it has available to it. Chances are, this will be bringing in your `venv` folder etc if you have one. I feel that this may be the largest perpetrator of size for you. You can mitigate this by adding an explicit `COPY` in, or using a `.dockerignore` file.\nI built your image (Without any source code, and without copying in `entrypoint.sh`), and it came out to 710MB as a base. It could be a good idea to check the size of your source code, and see if anything else is getting in there.\nAfter I re-arranged some of the commands to reuse directives, the image was 484MB, which is considerably smaller!\nIf you get stuck, I can pop it into a gist on Github for you and walk you through it, however, the Docker documentation should hopefully get you going\nWhy?\nWell, larger applications / images aren't inherently bad, but with any increase in data, some operations may be slower.\nWhen I say operations, I tend to mean pulling images from a registry, or pushing them to publish. It will take longer to transfer 1GB than it will 50MB.\nThere's also a consideration to be made when you scale your containers. While the image size does not necessarily correlate directly to how much disk you will use when you start a container, it will certainly increase the requirements for the machine you're running on, and limit others on smaller devices\nDocker\nThe advantages of using Docker are widespread, and I can't cover them all here without submitting my writing for thesis defence ;-)\nBut it mainly boils down to the following points:\n\nAlot of providers support running your applications in docker\nDockerfiles help you to build your application in a consisten environment, meaning you dont have to configure each host your app runs on, or worry about version clashes\nContainers let you develop and run your application in a consistent (And the same) environment\nContainers usually provide really nice networking capabilities. An example you will have encountered is within docker compose, you can reach other containers simply through their hostname\n\nNginx\nYou've set things up well there, from what I can gather! I imagine nginx is 'telling you' (Via the logs?) to navigate to `0.0.0.0` because that is what it will have bound to in the container. Now, you've forwarded traffic from `1337:80`. Docker follows the format of `host:container`, so this means that traffic on `localhost:1337` will be directed to the containers port `80`.\nYou may need to swap this around based on your nginx configuration, but rest assured you will be able to navigate to localhost in your browser and see your website once everything is set up\nLet me know if you need help with any of the above, or want more resources to aid you. Happy to correspond and walk you through anything anytime given we seem to be in the same timezone \ud83e\udd19",
      "question_score": 10,
      "answer_score": 14,
      "created_at": "2021-08-05T02:07:52",
      "url": "https://stackoverflow.com/questions/68659216/docker-image-1gb-in-size-from-python3-8-3-alpine"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 78071458,
      "title": "Keycloak Docker Compose",
      "problem": "I've been attempting to learn Docker and specifically Docker Compose for my home network.  I have a number of other containers I've been able to successfully deploy, but I'm still learning/experimenting.  My recent project is to attempt creating a Keycloak container, but I've been struggling with my database connection.\nI've been adapting the instructions found on Running Keycloak in a Container and found examples of Docker Compose configurations.  Here is what I have so far for my Docker Compose file:\n```\n`version: '3.6'\n\nservices:\n  keycloak_web:\n    image: quay.io/keycloak/keycloak:23.0.2\n    container_name: keycloak_web\n    environment:\n      KC_DB: postgres\n      KC_DB_URL: jdbc:postgresql://keycloakdb:5432/keycloak\n      KC_DB_USERNAME: keycloak\n      KC_DB_PASSWORD: keycloak\n\n      KC_HOSTNAME: localhost\n      KC_HOSTNAME_PORT: 8080\n      KC_HOSTNAME_STRICT: 'false'\n      KC_HOSTNAME_STRICT_HTTPS: 'false'\n\n      KC_LOG_LEVEL: info\n      KC_METRICS_ENABLED: 'true'\n      KC_HEALTH_ENABLED: 'true'\n    command: start-dev\n    depends_on:\n      - keycloakdb\n    ports:\n      - '8080:8080'\n\n  keycloakdb:\n    image: postgres:16\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n    environment:\n      POSTGRES_DB: keycloak\n      POSTGRES_USER: keycloak\n      POSTGRES_PASSWORD: keycloak\n\nvolumes:\n  postgres_data:\n`\n```\nWhen I attempt to bring my container online, I receive the following error (which indicates a database connection error):\n```\n`Starting data_keycloakdb_1 ... done\nRecreating keycloak_web    ... done\nAttaching to keycloak_web\nkeycloak_web     | Updating the configuration and installing your custom providers, if any. Please wait.\nkeycloak_web     | 2024-02-28 01:05:44,993 INFO  [io.quarkus.deployment.QuarkusAugmentor] (main) Quarkus augmentation completed in 11051ms\nkeycloak_web     | 2024-02-28 01:05:47,100 INFO  [org.keycloak.quarkus.runtime.hostname.DefaultHostnameProvider] (main) Hostname settings: Base URL: , Hostname: localhost, Strict HTTPS: false, Path: , Strict BackChannel: false, Admin URL: , Admin: , Port: 8080, Proxied: false\nkeycloak_web     | 2024-02-28 01:05:49,123 WARN  [io.quarkus.agroal.runtime.DataSources] (main) Datasource  enables XA but transaction recovery is not enabled. Please enable transaction recovery by setting quarkus.transaction-manager.enable-recovery=true, otherwise data may be lost if the application is terminated abruptly\nkeycloak_web     | 2024-02-28 01:05:49,293 WARN  [org.infinispan.PERSISTENCE] (keycloak-cache-init) ISPN000554: jboss-marshalling is deprecated and planned for removal\nkeycloak_web     | 2024-02-28 01:05:49,350 WARN  [org.infinispan.CONFIG] (keycloak-cache-init) ISPN000569: Unable to persist Infinispan internal caches as no global state enabled\nkeycloak_web     | 2024-02-28 01:05:49,455 INFO  [org.infinispan.CONTAINER] (keycloak-cache-init) ISPN000556: Starting user marshaller 'org.infinispan.jboss.marshalling.core.JBossUserMarshaller'\nkeycloak_web     | 2024-02-28 01:05:49,470 WARN  [io.agroal.pool] (agroal-11) Datasource '': The connection attempt failed.\nkeycloak_web     | 2024-02-28 01:05:49,472 WARN  [org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator] (JPA Startup Thread) HHH000342: Could not obtain connection to query metadata: java.lang.NullPointerException: Cannot invoke \"org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(java.sql.SQLException, String)\" because the return value of \"org.hibernate.resource.transaction.backend.jta.internal.JtaIsolationDelegate.sqlExceptionHelper()\" is null\nkeycloak_web     |      at org.hibernate.resource.transaction.backend.jta.internal.JtaIsolationDelegate.doTheWork(JtaIsolationDelegate.java:186)\nkeycloak_web     |      at org.hibernate.resource.transaction.backend.jta.internal.JtaIsolationDelegate.lambda$delegateWork$1(JtaIsolationDelegate.java:75)\nkeycloak_web     |      at org.hibernate.resource.transaction.backend.jta.internal.JtaIsolationDelegate.doInSuspendedTransaction(JtaIsolationDelegate.java:107)\nkeycloak_web     |      at org.hibernate.resource.transaction.backend.jta.internal.JtaIsolationDelegate.delegateWork(JtaIsolationDelegate.java:72)\nkeycloak_web     |      at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.getJdbcEnvironmentUsingJdbcMetadata(JdbcEnvironmentInitiator.java:279)\nkeycloak_web     |      at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:193)\nkeycloak_web     |      at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:69)\nkeycloak_web     |      at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:119)\nkeycloak_web     |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:264)\nkeycloak_web     |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:239)\nkeycloak_web     |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:216)\nkeycloak_web     |      at org.hibernate.engine.jdbc.internal.JdbcServicesImpl.configure(JdbcServicesImpl.java:52)\nkeycloak_web     |      at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.configureService(StandardServiceRegistryImpl.java:125)\nkeycloak_web     |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:248)\nkeycloak_web     |      at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:216)\nkeycloak_web     |      at org.hibernate.boot.internal.SessionFactoryOptionsBuilder.(SessionFactoryOptionsBuilder.java:273)\nkeycloak_web     |      at io.quarkus.hibernate.orm.runtime.recording.PrevalidatedQuarkusMetadata.buildSessionFactoryOptionsBuilder(PrevalidatedQuarkusMetadata.java:70)\nkeycloak_web     |      at io.quarkus.hibernate.orm.runtime.boot.FastBootEntityManagerFactoryBuilder.build(FastBootEntityManagerFactoryBuilder.java:81)\nkeycloak_web     |      at io.quarkus.hibernate.orm.runtime.FastBootHibernatePersistenceProvider.createEntityManagerFactory(FastBootHibernatePersistenceProvider.java:74)\nkeycloak_web     |      at jakarta.persistence.Persistence.createEntityManagerFactory(Persistence.java:80)\nkeycloak_web     |      at jakarta.persistence.Persistence.createEntityManagerFactory(Persistence.java:55)\nkeycloak_web     |      at io.quarkus.hibernate.orm.runtime.JPAConfig$LazyPersistenceUnit.get(JPAConfig.java:156)\nkeycloak_web     |      at io.quarkus.hibernate.orm.runtime.JPAConfig$1.run(JPAConfig.java:64)\nkeycloak_web     |      at java.base/java.lang.Thread.run(Thread.java:840)\nkeycloak_web     |\nkeycloak_web     | 2024-02-28 01:05:50,519 INFO  [org.keycloak.connections.infinispan.DefaultInfinispanConnectionProviderFactory] (main) Node name: node_227989, Site name: null\nkeycloak_web     | 2024-02-28 01:05:50,522 INFO  [org.keycloak.broker.provider.AbstractIdentityProviderMapper] (main) Registering class org.keycloak.broker.provider.mappersync.ConfigSyncEventListener\nkeycloak_web     | 2024-02-28 01:05:50,530 WARN  [io.agroal.pool] (agroal-11) Datasource '': The connection attempt failed.\nkeycloak_web     | 2024-02-28 01:05:50,572 ERROR [org.keycloak.quarkus.runtime.cli.ExecutionExceptionHandler] (main) ERROR: Failed to start server in (development) mode\nkeycloak_web     | 2024-02-28 01:05:50,572 ERROR [org.keycloak.quarkus.runtime.cli.ExecutionExceptionHandler] (main) ERROR: Failed to obtain JDBC connection\nkeycloak_web     | 2024-02-28 01:05:50,572 ERROR [org.keycloak.quarkus.runtime.cli.ExecutionExceptionHandler] (main) ERROR: The connection attempt failed.\nkeycloak_web     | 2024-02-28 01:05:50,572 ERROR [org.keycloak.quarkus.runtime.cli.ExecutionExceptionHandler] (main) ERROR: keycloakdb\nkeycloak_web     | 2024-02-28 01:05:50,573 ERROR [org.keycloak.quarkus.runtime.cli.ExecutionExceptionHandler] (main) For more details run the same command passing the '--verbose' option. Also you can use '--help' to see the details about the usage of the particular command.\n`\n```\nAdditionally, I've tried a bunch of different iterations of the Docker networking, database URL settings, and I've changed the KC_DB_URL to breakout KC_DB_HOSTNAME, KC_DB_PORT, etc. explicitly.\nAny help or pointer in the right direction would be greatly appreciated.  Thank you in advance.",
      "solution": "The `postgres` version is matter\n`KC_DB_USERNAME` and`KC_DB_PASSWORD` can't use same string.\nThis `docker-compose.yml` file will work\n`version: '3.6'\n\nservices:\n  keycloak_web:\n    image: quay.io/keycloak/keycloak:23.0.7\n    container_name: keycloak_web\n    environment:\n      KC_DB: postgres\n      KC_DB_URL: jdbc:postgresql://keycloakdb:5432/keycloak\n      KC_DB_USERNAME: keycloak\n      KC_DB_PASSWORD: password\n\n      KC_HOSTNAME: localhost\n      KC_HOSTNAME_PORT: 8080\n      KC_HOSTNAME_STRICT: false\n      KC_HOSTNAME_STRICT_HTTPS: false\n\n      KC_LOG_LEVEL: info\n      KC_METRICS_ENABLED: true\n      KC_HEALTH_ENABLED: true\n      KEYCLOAK_ADMIN: admin\n      KEYCLOAK_ADMIN_PASSWORD: admin\n    command: start-dev\n    depends_on:\n      - keycloakdb\n    ports:\n      - 8080:8080\n\n  keycloakdb:\n    image: postgres:15\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n    environment:\n      POSTGRES_DB: keycloak\n      POSTGRES_USER: keycloak\n      POSTGRES_PASSWORD: password\n\nvolumes:\n  postgres_data:\n\n`\nLogging\n`Attaching to keycloakdb-1, keycloak_web\nkeycloakdb-1  | \nkeycloakdb-1  | PostgreSQL Database directory appears to contain a database; Skipping initialization\nkeycloakdb-1  |\nkeycloakdb-1  | 2024-02-28 03:48:28.026 UTC [1] LOG:  starting PostgreSQL 15.6 (Debian 15.6-1.pgdg120+2) on x86_64-pc-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit\nkeycloakdb-1  | 2024-02-28 03:48:28.027 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\nkeycloakdb-1  | 2024-02-28 03:48:28.027 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\nkeycloakdb-1  | 2024-02-28 03:48:28.042 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\nkeycloakdb-1  | 2024-02-28 03:48:28.058 UTC [29] LOG:  database system was interrupted; last known up at 2024-02-28 03:48:01 UTC\nkeycloak_web  | Updating the configuration and installing your custom providers, if any. Please wait.\nkeycloakdb-1  | 2024-02-28 03:48:29.325 UTC [29] LOG:  database system was not properly shut down; automatic recovery in progress\nkeycloakdb-1  | 2024-02-28 03:48:29.330 UTC [29] LOG:  invalid record length at 0/1D580F0: wanted 24, got 0\nkeycloakdb-1  | 2024-02-28 03:48:29.330 UTC [29] LOG:  redo is not required\nkeycloakdb-1  | 2024-02-28 03:48:29.341 UTC [27] LOG:  checkpoint starting: end-of-recovery immediate wait\nkeycloakdb-1  | 2024-02-28 03:48:29.381 UTC [27] LOG:  checkpoint complete: wrote 3 buffers (0.0%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.010 s, sync=0.011 s, total=0.045 s; sync files=2, longest=0.007 s, average=0.006 s; distance=0 kB, estimate=0 kB\nkeycloakdb-1  | 2024-02-28 03:48:29.388 UTC [1] LOG:  database system is ready to accept connections\nkeycloak_web  | 2024-02-28 03:48:36,877 INFO  [io.quarkus.deployment.QuarkusAugmentor] (main) Quarkus augmentation completed in 7132ms\nkeycloak_web  | 2024-02-28 03:48:38,000 INFO  [org.keycloak.quarkus.runtime.hostname.DefaultHostnameProvider] (main) Hostname settings: Base URL: , Hostname: localhost, Strict HTTPS: false, Path: , Strict BackChannel: false, Admin URL: , Admin: , Port: 8080, Proxied: false\nkeycloak_web  | 2024-02-28 03:48:39,079 WARN  [io.quarkus.agroal.runtime.DataSources] (main) Datasource  enables XA but transaction recovery is not enabled. Please enable transaction recovery by setting quarkus.transaction-manager.enable-recovery=true, otherwise data may be lost if the application is terminated abruptly\nkeycloakdb-1  | 2024-02-28 03:48:39.232 UTC [33] WARNING:  database \"keycloak\" has a collation version mismatch\nkeycloakdb-1  | 2024-02-28 03:48:39.232 UTC [33] DETAIL:  The database was created using collation version 2.31, but the operating system provides version 2.36.\nkeycloakdb-1  | 2024-02-28 03:48:39.232 UTC [33] HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE keycloak REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\nkeycloak_web  | 2024-02-28 03:48:39,423 WARN  [org.infinispan.PERSISTENCE] (keycloak-cache-init) ISPN000554: jboss-marshalling is deprecated and planned for removal\nkeycloak_web  | 2024-02-28 03:48:39,475 WARN  [org.infinispan.CONFIG] (keycloak-cache-init) ISPN000569: Unable to persist Infinispan internal caches as no global state enabled\nkeycloak_web  | 2024-02-28 03:48:39,538 INFO  [org.infinispan.CONTAINER] (keycloak-cache-init) ISPN000556: Starting user marshaller 'org.infinispan.jboss.marshalling.core.JBossUserMarshaller'\nkeycloak_web  | 2024-02-28 03:48:39,922 INFO  [org.keycloak.broker.provider.AbstractIdentityProviderMapper] (main) Registering class org.keycloak.broker.provider.mappersync.ConfigSyncEventListener\nkeycloak_web  | 2024-02-28 03:48:40,184 INFO  [org.keycloak.connections.infinispan.DefaultInfinispanConnectionProviderFactory] (main) Node name: node_838099, Site name: null\nkeycloakdb-1  | 2024-02-28 03:48:40.393 UTC [34] WARNING:  database \"keycloak\" has a collation version mismatch\nkeycloakdb-1  | 2024-02-28 03:48:40.393 UTC [34] DETAIL:  The database was created using collation version 2.31, but the operating system provides version 2.36.\nkeycloakdb-1  | 2024-02-28 03:48:40.393 UTC [34] HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE keycloak REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\nkeycloak_web  | 2024-02-28 03:48:40,813 INFO  [io.quarkus] (main) Keycloak 23.0.7 on JVM (powered by Quarkus 3.2.10.Final) started in 3.856s. Listening on: http://0.0.0.0:8080\nkeycloak_web  | 2024-02-28 03:48:40,813 INFO  [io.quarkus] (main) Profile dev activated.\nkeycloak_web  | 2024-02-28 03:48:40,813 INFO  [io.quarkus] (main) Installed features: [agroal, cdi, hibernate-orm, jdbc-h2, jdbc-mariadb, jdbc-mssql, jdbc-mysql, jdbc-oracle, jdbc-postgresql, keycloak, logging-gelf, micrometer, narayana-jta, reactive-routes, resteasy-reactive, resteasy-reactive-jackson, smallrye-context-propagation, smallrye-health, vertx]\nkeycloak_web  | 2024-02-28 03:48:40,863 WARN  [org.keycloak.quarkus.runtime.KeycloakMain] (main) Running the server in development mode. DO NOT use this configuration in production.\nkeycloakdb-1  | 2024-02-28 03:49:29.450 UTC [35] WARNING:  database \"keycloak\" has a collation version mismatch\nkeycloakdb-1  | 2024-02-28 03:49:29.450 UTC [35] DETAIL:  The database was created using collation version 2.31, but the operating system provides version 2.36.\nkeycloakdb-1  | 2024-02-28 03:49:29.450 UTC [35] HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE keycloak REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\n`\nLogging URL\n```\n`http://localhost:8080/admin/master/console/\n`\n```\nusername and password is `admin`\n\nIn here, more latest version of `docker-compose.yml`",
      "question_score": 10,
      "answer_score": 22,
      "created_at": "2024-02-28T02:29:06",
      "url": "https://stackoverflow.com/questions/78071458/keycloak-docker-compose"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 68010612,
      "title": "Error response from daemon: Unrecognised volume spec: file &#39;\\\\.\\pipe\\docker_engine&#39; cannot be mapped. Only directories can be mapped on this platform",
      "problem": "I'm new to the docker. Any help and tips are welcome.\nEnvironments:\n\nWindows: Windows 10 Pro 21H1\nDocker Desktop: 3.4\n\nI can run hello work example without any issues. But seems like I can't use named piped, can't figure out what is the issue.\nSome people mentioned named piped is only available for Windows server, but this blog (https://www.docker.com/blog/docker-windows-server-1709/)clearly mentioned Windows 10 is supported.\ndocker version output:\n```\n`\u276f docker version\nClient:\n Cloud integration: 1.0.17\n Version:           20.10.7\n API version:       1.41\n Go version:        go1.16.4\n Git commit:        f0df350\n Built:             Wed Jun  2 12:00:56 2021\n OS/Arch:           windows/amd64\n Context:           default\n Experimental:      true\n\nServer: Docker Engine - Community\n Engine:\n  Version:          20.10.7\n  API version:      1.41 (minimum version 1.24)\n  Go version:       go1.13.15\n  Git commit:       b0f5bc3\n  Built:            Wed Jun  2 11:56:41 2021\n  OS/Arch:          windows/amd64\n  Experimental:     false\n`\n```\nYAML file:\n```\n`version: \"2.4\"\nservices:\n  traefik:\n    isolation: ${TRAEFIK_ISOLATION}\n    image: ${TRAEFIK_IMAGE}\n    command:\n      - \"--ping\"\n      - \"--api.insecure=true\"\n      - \"--providers.docker.endpoint=npipe:////./pipe/docker_engine\"\n      - \"--providers.docker.exposedByDefault=false\"\n      - \"--providers.file.directory=C:/etc/traefik/config/dynamic\"\n      - \"--entryPoints.websecure.address=:443\"\n    ports:\n      - \"443:443\"\n      - \"8079:8080\"\n    healthcheck:\n      test: [\"CMD\", \"traefik\", \"healthcheck\", \"--ping\"]\n    volumes:\n      - source: \\\\.\\pipe\\docker_engine\n        target: \\\\.\\pipe\\docker_engine\n        type: npipe\n      - ./traefik:C:/etc/traefik\n    depends_on:\n      id:\n        condition: service_healthy\n      cm:\n        condition: service_healthy\n  .......\n`\n```\nI can create the container if I removed the named pipe in volumes then I got different error:\n```\n`time=\"2021-06-17T06:32:13+08:00\" level=error msg=\"Provider connection error error during connect: This error may indicate that the docker daemon is not running.: Get \\\"http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.24/version\\\": open //./pipe/docker_engine: The system cannot find the file specified., retrying in 7.701954985s\" providerName=docker\n`\n```\nThe rest of containers are running Okay.\ndocker compose up output:\n```\n`\u276f docker compose up\n[+] Running 10/11\n - Network sitecore-xp0_default                     Created                                                        1.1s\n - Container sitecore-xp0_mssql_1                   Created                                                        0.5s\n - Container sitecore-xp0_solr_1                    Created                                                        0.5s\n - Container sitecore-xp0_id_1                      Created                                                        0.4s\n - Container sitecore-xp0_solr-init_1               Created                                                        0.3s\n - Container sitecore-xp0_xconnect_1                Created                                                        0.3s\n - Container sitecore-xp0_cortexprocessingworker_1  Created                                                        0.6s\n - Container sitecore-xp0_xdbautomationworker_1     Created                                                        0.6s\n - Container sitecore-xp0_xdbsearchworker_1         Created                                                        0.9s\n - Container sitecore-xp0_cm_1                      Created                                                        0.9s\n - Container sitecore-xp0_traefik_1                 Creating                                                       0.2s\nError response from daemon: Unrecognised volume spec: file '\\\\.\\pipe\\docker_engine' cannot be mapped. Only directories can be mapped on this platform\n`\n```",
      "solution": "I got the same error and the solutions for me was to add a \\ to the end.\nRef: https://sitecore.stackexchange.com/a/31522/11642",
      "question_score": 10,
      "answer_score": 3,
      "created_at": "2021-06-17T00:13:00",
      "url": "https://stackoverflow.com/questions/68010612/error-response-from-daemon-unrecognised-volume-spec-file-pipe-docker-engi"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 77475771,
      "title": "Error when running OTEL collector with Jaeger in Docker containers",
      "problem": "I'm trying to run jaeger and an OTEL collector as docker containers.\nUnfortunately I run into the following error message:\notel-collector  | Error: failed to get config: cannot unmarshal the configuration: 1 error(s) decoding:\nerror decoding 'exporters': unknown type: \"jaeger\" for id: \"jaeger\" (valid values: [logging otlp otlphttp file kafka prometheus debug opencensus prometheusremotewrite zipkin])\nMy docker compose file looks as follows:\n`\n```\n`version: '3'\nservices:\n  otel-collector:\n    container_name: otel-collector\n    image: otel/opentelemetry-collector\n    command: [\"--config=/etc/otel-collector-config.yaml\"]\n    volumes:\n      - ./Configuration/collector-config.yaml:/etc/otel-collector-config.yaml\n    ports:\n      - \"4317:4317\"\n\n  jaeger:\n    container_name: jaeger\n    image: jaegertracing/all-in-one\n    ports:\n      - \"16686:16686\"\n      - \"14250\"\n`\n```\nThe config file for the collector looks as follows:\n```\n`receivers:\n  otlp:\n    protocols:\n      grpc:\nprocessors:\n  batch:\nexporters:\n  logging:\n    loglevel: debug\n  jaeger:\n    endpoint: jaeger:14250\n    tls:\n      insecure: true\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [logging, jaeger]\n`\n```\nAny thoughts as to what I am doing wrong?",
      "solution": "Jaeger is not a valid exporter for the latest otelm collector. Currently the following exports are allowed:\n\ndebug\nlogging\notlp\nazuredataexplorer\ngooglecloud\ninfluxdb\nprometheusremotewrite\ndatadog\ninstana\nmezmo\nsapm\nsignalfx\notlphttp\nazuremonitor\ncassandra\ndataset\nopencensus\nawscloudwatchlogs\ngooglemanagedprometheus\nlogicmonitor\nloki\ntencentcloud_logservice\nfile\nalibabacloud_logservice\nawsemf\nawskinesis\nawsxray\ncarbon\nclickhouse\ndynatrace\nloadbalancing\nsumologic\nf5cloud\nkafka\nprometheus\nsentry\nskywalking\nzipkin\nawss3\nelasticsearch\nlogzio\npulsar\nsplunk_hec\ncoralogix\ngooglecloudpubsub\ntan\nzuobservability\n\nTo fix, Change to otlp:\n`otlp:\n    endpoint: jaeger:4317\n    tls:\n      insecure: true\n`\nExample in otelm demo chart: https://github.com/open-telemetry/opentelemetry-helm-charts/blob/0def0da3d8beb5e95840e62c614fd616694c287e/charts/opentelemetry-demo/values.yaml#L627",
      "question_score": 10,
      "answer_score": 17,
      "created_at": "2023-11-13T18:34:24",
      "url": "https://stackoverflow.com/questions/77475771/error-when-running-otel-collector-with-jaeger-in-docker-containers"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 73299883,
      "title": "Docker containers terminate on shell logout",
      "problem": "I'm running a debian backed docker container created with a `docker-compose up -d` detached command. It starts without problems and serves requests without issues, but it requires I keep logged-in with an active shell in order to keep containers running.\nMy docker-compose.yml file is as follows:\n```\n`services:\n  db:\n    image: postgres:alpine\n    restart: always\n    ports:\n      - '5432:5432'    \n    volumes:\n      - /data/docker/postgresql/data:/var/lib/postgresql/data:Z\n  pgadmin:\n    image: dpage/pgadmin4:latest\n    restart: always\n    ports:\n      - \"5050:80\"\n    volumes:\n      - /data/docker/postgresql/pgadmin-data:/var/lib/pgadmin:Z\n`\n```\nEverytime I restart the server it doesn't appears to start up until I log in.\nThe status of docker service, as per `systemctl status docker.service` is:\n```\n`\u25cf docker.service - Docker Application Container Engine\n     Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled)\n     Active: active (running) since Tue 2022-08-09 22:37:46 -03; 21min ago\nTriggeredBy: \u25cf docker.socket\n       Docs: https://docs.docker.com\n   Main PID: 4359 (dockerd)\n      Tasks: 11\n        CPU: 1.396s\n     CGroup: /system.slice/docker.service\n             \u2514\u25004359 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock\n`\n```\nEven when all looks good, the moment I logout the shell session, the containers stop running.",
      "solution": "The problem I was having can be searched for:\nRootless containers exit once the user session exits\nI finally could fix it after realizing i was using the rootless configuration for docker, and both containers where started using a normal user.\nLinux stops processes started by a normal user if loginctl is configured to not use lingering, to prevent normal users to keep long-running processes executing in the system.\nI've found this link that helped: 17) rootless containers exit once the user session exits\n\nYou need to set lingering mode through loginctl to prevent user\nprocesses to be killed once the user session completed.\n\nSymptom\nOnce the user logs out all the containers exit.\nSolution\nYou'll need to either:\n```\n`# loginctl enable-linger $UID\n`\n```",
      "question_score": 10,
      "answer_score": 18,
      "created_at": "2022-08-10T04:07:45",
      "url": "https://stackoverflow.com/questions/73299883/docker-containers-terminate-on-shell-logout"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 66067320,
      "title": "Running mongorestore on Docker once the container starts",
      "problem": "I'm trying to set up a container running MongoDB that gets populated with data using mongorestore when it starts up. The idea is to quickly set up a dummy database for testing and mocking.\nMy Dockerfile looks like this:\n```\n`FROM mongo:bionic\nCOPY ./db-dump/mydatabase/* /db-dump/\n`\n```\nand docker-compose.yml looks like this:\n```\n`version: \"3.1\"\n  \nservices:\n  mongo:\n    build: ./mongo\n    command: mongorestore -d mydatabase ./db-dump\n    ports:\n      - \"27017:27017\"\n`\n```\nIf I run this with `docker-compose up`, it pauses for a while and then I get an error saying:\n```\n`error connecting to host: could not connect to server: server selection error: server selection timeout, current topology: { Type: Single, Servers: [{ Addr: localhost:27017, Type: Unknown, State: Connected, Average RTT: 0, Last error: connection() : dial tcp 127.0.0.1:27017: connect: connection refused }, ] }\n`\n```\nOpening a CLI on the container and running the exact same command works without any issues, however. I've tried adding `-h` with the name of the container or 127.0.0.1, and it doesn't make a difference. Why isn't this command able to connect when it works fine once the container is running?",
      "solution": "There is a better way than overriding the default command - using `/docker-entrypoint-initdb.d`:\n\nWhen a container is started for the first time it will execute files with extensions `.sh` and `.js` that are found in `/docker-entrypoint-initdb.d`. Files will be executed in alphabetical order. `.js` files will be executed by mongo using the database specified by the MONGO_INITDB_DATABASE variable, if it is present, or test otherwise. You may also switch databases within the `.js` script.\n\n[Source]\nSo you simply write that command into a file named `mongorestore.sh`:\n```\n`mongorestore -d mydatabase /db-dump\n`\n```\nand then mount it inside along with the dump file:\n`version: \"3.1\"\n  \nservices:\n  mongo:\n    image: mongo:bionic\n    ports:\n      - \"27017:27017\"\n    volumes:\n      - ./mongorestore.sh:/docker-entrypoint-initdb.d/mongorestore.sh\n      - ./db-dump:/db-dump\n`\nYou don't even need a Dockerfile.",
      "question_score": 10,
      "answer_score": 18,
      "created_at": "2021-02-05T17:53:02",
      "url": "https://stackoverflow.com/questions/66067320/running-mongorestore-on-docker-once-the-container-starts"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 67124153,
      "title": "Copy letsencrypt files to docker container",
      "problem": "Im not an expert with docker, I am just getting used to it. I want to copy ssl certificates, which are generated on the host machine to my docker container. I read that it should be able to do with `volumes` argument in the docker-compose file but starting my server it always excites as it cannot find the copied files within the working directory.\nFolderstructure\n```\n`- repo\n   - backend\n      - api\n         - static\n            - ssl\n         - dockerfile\n   - frontend\n   - docker-compose.yml\n`\n```\nDockerfile\n```\n`FROM node:14-alpine\n\nENV NODE_ENV=production SERVER_PORT_HTTP=80 SERVER_PORT_HTTPS=443\n\nWORKDIR /usr/src/app\n\nRUN npm install\n\nCOPY . .\n\nEXPOSE ${SERVER_PORT_HTTP} ${SERVER_PORT_HTTPS}\n\nCMD [ \"npm\", \"run\", \"start\" ]\n`\n```\nDocker-Compose\n```\n`version: \"3\"\n\nservices:\n  api:\n    build:\n      context: ./backend/api\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - /etc/letsencrypt/live/api.example.com:/static/ssl\n    restart: unless-stopped\n`\n```",
      "solution": "You did everything right, the problem is with the files. If you look at them, you'll see that they're not normal files but links:\n```\n`root@fbe56bc38ad6:/# ls /etc/letsencrypt/live/example.com/ -l\ntotal 4\n-rw-r--r-- 1 root root 692 Jul 24  2020 README\nlrwxrwxrwx 1 root root  44 Mar 22 00:03 cert.pem -> ../../archive/example.com/cert5.pem\nlrwxrwxrwx 1 root root  45 Mar 22 00:03 chain.pem -> ../../archive/example.com/chain5.pem\nlrwxrwxrwx 1 root root  49 Mar 22 00:03 fullchain.pem -> ../../archive/example.com/fullchain5.pem\nlrwxrwxrwx 1 root root  47 Mar 22 00:03 privkey.pem -> ../../archive/example.com/privkey5.pem\nlrwxrwxrwx 1 root root  42 Mar  1 12:57 example.com -> /etc/letsencrypt/live/example.com\nlrwxrwxrwx 1 root root  33 Mar  1 12:57 ssl-dhparams.pem -> /etc/letsencrypt/ssl-dhparams.pem\n`\n```\nAnd so you mounted a bunch of relative links that point to a non-existent location.\nI suggest you mount `/etc/letsencrypt` to `/etc/letsencrypt` in container:\n`    volumes:\n      - /etc/letsencrypt:/etc/letsencrypt\n`\nThen make your application to look for files in `/etc/letsencrypt/live/example.com` or make another link at `/static/ssl` that points to `/etc/letsencrypt/live/example.com`:\n```\n`ln -s /etc/letsencrypt/live/example.com /static/ssl\n`\n```",
      "question_score": 10,
      "answer_score": 16,
      "created_at": "2021-04-16T13:11:50",
      "url": "https://stackoverflow.com/questions/67124153/copy-letsencrypt-files-to-docker-container"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 70140111,
      "title": "Mounting a network drive with docker compose on Windows 10",
      "problem": "I've been successfully mounting volumes on Windows 10 in various projects recently using the example docker-compose.yml file below. For a new project today I needed to mount a folder from the Z:/ drive (a network mounted drive which appears as `\\\\IP.IP.IP.IP\\public\\data (Z:)` when I navigate to that area in Windows File Explorer.\nWhen I edit the volumes to point to locations on Z: (e.g. in the second docker-compose.yml below), the volumes are not mounted properly and are empty folders when I connect to the container via the CLI.\nAny advice on getting the Z: drive folders to mount properly would be great, thanks.\nWorking docker-compose.yml file:\n`version: '3.1'\n\nservices:\n  db:\n    image: mysql:8.0.25\n    container_name: db\n    restart: always\n    secrets:\n      - mysql_root\n    environment:\n      MYSQL_ROOT_PASSWORD_FILE: /run/secrets/mysql_root\n      MYSQL_DATABASE: donuts\n      TZ: \"Australia/NSW\"\n    volumes:\n      - mysql-data:/var/lib/mysql\n      - ./mysql-init.sql:/docker-entrypoint-initdb.d/mysql-init.sql\n    network_mode: \"host\"\n  \n  voyager_donuts:\n    container_name: voyager_donuts\n    build:\n      context: .\n      dockerfile: Dockerfile\n    image: voyager_donuts\n    network_mode: \"host\"\n    environment:\n      TZ: \"Australia/NSW\"\n    volumes:\n      - c:/Users/MYUSERNAME/data/DonutsCalibration:/voyager_calibration\n      - c:/Users/MYUSERNAME/data/DonutsLog:/voyager_log\n      - c:/Users/MYUSERNAME/data:/voyager_data\n      - c:/Users/MYUSERNAME/data/DonutsReference:/voyager_reference\n\nvolumes:\n  mysql-data:\n\nsecrets:\n  mysql_root:\n    file: ./secrets/mysql_root\n`\nBroken volumes docker-compose.yml file:\n`version: '3.1'\n\nservices:\n  db:\n    image: mysql:8.0.25\n    container_name: db\n    restart: always\n    secrets:\n      - mysql_root\n    environment:\n      MYSQL_ROOT_PASSWORD_FILE: /run/secrets/mysql_root\n      MYSQL_DATABASE: donuts\n      TZ: \"Australia/NSW\"\n    volumes:\n      - mysql-data:/var/lib/mysql\n      - ./mysql-init.sql:/docker-entrypoint-initdb.d/mysql-init.sql\n    network_mode: \"host\"\n  \n  voyager_donuts:\n    container_name: voyager_donuts\n    build:\n      context: .\n      dockerfile: Dockerfile\n    image: voyager_donuts\n    network_mode: \"host\"\n    environment:\n      TZ: \"Australia/NSW\"\n    volumes:\n      - z:/RAW/DonutsCalibration:/voyager_calibration\n      - z:/RAW/DonutsLog:/voyager_log\n      - z:/RAW:/voyager_data\n      - z:/RAW/DonutsReference:/voyager_reference\n\nvolumes:\n  mysql-data:\n\nsecrets:\n  mysql_root:\n    file: ./secrets/mysql_root\n`",
      "solution": "According to this forum thread you would have to use something like this to be able to mount network shares:\n```\n`volumes:\n  foldermix:\n    driver_opts:\n      type: cifs\n      o: username={smbuser},password={smbpass},uid={UID for mount},gid={gid for mount},vers=3.0\n      device: //Share1/FolderMix\n`\n```\nSee also the docker documentation for Samba/CIFS volumes.\nOf course, if you really need the indirection to mount the network drive instead of the network share, i.e. because the drive can be mounted to different shares or you do not want to put your credentials into the Docker-Compose file, this will not solve the issue.",
      "question_score": 10,
      "answer_score": 11,
      "created_at": "2021-11-28T03:03:56",
      "url": "https://stackoverflow.com/questions/70140111/mounting-a-network-drive-with-docker-compose-on-windows-10"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 66579446,
      "title": "Error executing docker-compose: Building webserver unable to prepare context: unable to &#39;git clone&#39; to temporary context directory: error fetching",
      "problem": "I'm following this Apache Airflow tutorial and I'm failing to execute\n```\n`docker-compose up -d\n`\n```\ncommand.\nI get following error:\n```\n`Building webserver\nunable to prepare context: unable to 'git clone' to temporary context directory: error fetching: /usr/lib/git-core/git-remote-https: /tmp/_MEItH0v3Q/libcrypto.so.1.1: version `OPENSSL_1_1_1' not found (required by /lib/x86_64-linux-gnu/libssh.so.4)\n: exit status 128\nERROR: Service 'webserver' failed to build\n`\n```\nI'm using Ubuntu 20.04 on WSL2.\nI've installed exactly that version of OPEN SSL - OPENSSL_1_1_1 but error remains.\nI've also updated git to 2.30.2 because I've read it could fix it but no luck.",
      "solution": "I had the same issue. Apparently this is a known bug for docker-compose 1.29 and Ubuntu 20 [1]\nMy `docker-compose` was installed using curl.\nWhat worked for me was removing `docker-compose` and install it using pip instead:\n`sudo rm /usr/local/bin/docker-compose\npip3 install docker-compose \n`\nAfter that everything worked as expected.\n\nThis bug has been reported several times:\n\nhttps://github.com/docker/compose/issues/8170\nhttps://github.com/docker/compose/issues/8309\nhttps://github.com/docker/compose/issues/8461\nAt the time of this writing (December 2021) things have not yet been resolved.",
      "question_score": 10,
      "answer_score": 4,
      "created_at": "2021-03-11T10:11:56",
      "url": "https://stackoverflow.com/questions/66579446/error-executing-docker-compose-building-webserver-unable-to-prepare-context-un"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 66259110,
      "title": "testcontainer initializationError while running a test suite",
      "problem": "I have multiple test classes running the same docker-compose with testcontainer.\nThe suite fails with `initializationError` although each test passes when performed separately.\nHere is the relevant part of the stacktrace occuring during the second test.\n`./gradlew e2e:test -i `\n```\n`io.foo.e2e.AuthTest > initializationError FAILED\n    org.testcontainers.containers.ContainerLaunchException: Container startup failed\n        at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:330)\n        at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:311)\n        at org.testcontainers.containers.DockerComposeContainer.startAmbassadorContainers(DockerComposeContainer.java:331)\n        at org.testcontainers.containers.DockerComposeContainer.start(DockerComposeContainer.java:178)\n        at io.foo.e2e.bases.BaseE2eTest$Companion.beforeAll$e2e(BaseE2eTest.kt:62)\n        at io.foo.e2e.bases.BaseE2eTest.beforeAll$e2e(BaseE2eTest.kt)\n       ...\n\n        Caused by:\n        org.rnorth.ducttape.RetryCountExceededException: Retry limit hit with exception\n            at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:88)\n            at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:323)\n            ... 83 more\n\n            Caused by:\n            org.testcontainers.containers.ContainerLaunchException: Could not create/start container\n                at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:497)\n                at org.testcontainers.containers.GenericContainer.lambda$doStart$0(GenericContainer.java:325)\n                at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:81)\n                ... 84 more\n\n                Caused by:\n                org.testcontainers.containers.ContainerLaunchException: Aborting attempt to link to container btraq5fzahac_worker_1 as it is not running\n                    at org.testcontainers.containers.GenericContainer.applyConfiguration(GenericContainer.java:779)\n                    at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:359)\n                    ... 86 more\n`\n```\nIt seems to me that the second test doesn't wait for the first to shutdown previous containers.\nHere the base class that all tests inherit from. It is responsible for spinning up the containers.\n`open class BaseE2eTest {\n\n    ...\n\n    companion object {\n        const val A = \"containera_1\"\n        const val B = \"containerb_1\"\n        const val C = \"containerc_1\"\n\n        val dockerCompose: KDockerComposeContainer by lazy {\n            defineDockerCompose()\n                .withLocalCompose(true)\n                .withExposedService(A, 8080, Wait.forListeningPort())\n                .withExposedService(B, 8081)\n                .withExposedService(C, 5672, Wait.forListeningPort())\n        }\n\n        class KDockerComposeContainer(file: File) : DockerComposeContainer(file)\n\n        private fun defineDockerCompose() = KDockerComposeContainer(File(\"../docker-compose.yml\"))\n\n        @BeforeAll\n        @JvmStatic\n        internal fun beforeAll() {\n            dockerCompose.start()\n        }\n\n        @AfterAll\n        @JvmStatic\n        internal fun afterAll() {\n            dockerCompose.stop()\n        }\n    }\n}\n`\n```\n`docker-compose version 1.27.4, build 40524192\ntestcontainer 1.15.2\ntestcontainers:junit-jupiter:1.15.2\n`\n```",
      "solution": "After watching this talk, I realized that my testcontainers instantiation approach with Junit5 was wrong.\nHere is the working code:\n`@TestInstance(TestInstance.Lifecycle.PER_CLASS)\nopen class BaseE2eTest {\n\n    ...\n\n    val A = \"containera_1\"\n    val B = \"containerb_1\"\n    val C = \"containerc_1\"\n\n    val dockerCompose: KDockerComposeContainer by lazy {\n        defineDockerCompose()\n            .withLocalCompose(true)\n            .withExposedService(A, 8080, Wait.forListeningPort())\n            .withExposedService(B, 8081)\n            .withExposedService(C, 5672, Wait.forListeningPort())\n    }\n\n    class KDockerComposeContainer(file: File) : DockerComposeContainer(file)\n\n    private fun defineDockerCompose() = KDockerComposeContainer(File(\"../docker-compose.yml\"))\n\n    @BeforeAll\n    fun beforeAll() {\n        dockerCompose.start()\n    }\n\n    @AfterAll\n    fun afterAll() {\n        dockerCompose.stop()\n    }\n}\n`\nNow the test suite passes.",
      "question_score": 10,
      "answer_score": 1,
      "created_at": "2021-02-18T12:33:55",
      "url": "https://stackoverflow.com/questions/66259110/testcontainer-initializationerror-while-running-a-test-suite"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 75841925,
      "title": "Why is docker-compose throwing vite not found during the build?",
      "problem": "I have a React frontend app built with Vite. I'm getting the following error when I'm running my Docker\n```\n`[+] Running 1/1\n \u283f Container client-client-1  Recreated                                                                0.2s\nAttaching to client-client-1\nclient-client-1  |\nclient-client-1  | > client@0.0.0 dev\nclient-client-1  | > vite\nclient-client-1  |\nclient-client-1  | sh: 1: vite: not found\nclient-client-1 exited with code 127\n`\n```\nBelow you can see my Dockerfile:\n```\n`FROM node\n\nWORKDIR /usr/src/app\n\nCOPY ./package.json .\n\nRUN npm i\n\nCOPY . .\n\nCMD [\"npm\", \"run\", \"dev\"]\n`\n```\nI tried to add a command to install vite during the build of the docker container but it didn't work.\nYou check my docker-compose file below\n`version: '3.9'\n\nservices:\n  client:\n    build: .\n    ports:\n      - 5173:5173\n    volumes:\n      - .:/usr/src/app\n`\nHere is my package.json\n```\n`{\n  \"name\": \"client\",\n  \"private\": true,\n  \"version\": \"0.0.0\",\n  \"type\": \"module\",\n  \"scripts\": {\n    \"dev\": \"vite\",\n    \"build\": \"tsc && vite build\",\n    \"preview\": \"vite preview\",\n    \"lint\": \"eslint .\",\n    \"lint:fix\": \"eslint --fix .\"\n  },\n  \"dependencies\": {\n    \"eslint\": \"^8.36.0\",\n    \"eslint-config-airbnb\": \"^19.0.4\",\n    \"react\": \"^18.2.0\",\n    \"react-dom\": \"^18.2.0\"\n  },\n  \"devDependencies\": {\n    \"@testing-library/jest-dom\": \"^5.16.5\",\n    \"@testing-library/react\": \"^14.0.0\",\n    \"@types/react\": \"^18.0.28\",\n    \"@types/react-dom\": \"^18.0.11\",\n    \"@typescript-eslint/eslint-plugin\": \"^5.54.1\",\n    \"@typescript-eslint/parser\": \"^5.54.1\",\n    \"@vitejs/plugin-react\": \"^3.1.0\",\n    \"eslint\": \"^8.35.0\",\n    \"eslint-config-airbnb\": \"^19.0.4\",\n    \"eslint-config-airbnb-typescript\": \"^17.0.0\",\n    \"eslint-config-prettier\": \"^8.7.0\",\n    \"eslint-plugin-import\": \"^2.27.5\",\n    \"eslint-plugin-jsx-a11y\": \"^6.7.1\",\n    \"eslint-plugin-prettier\": \"^4.2.1\",\n    \"eslint-plugin-react\": \"^7.32.2\",\n    \"eslint-plugin-react-hooks\": \"^4.6.0\",\n    \"jsdom\": \"^21.1.0\",\n    \"prettier\": \"^2.8.4\",\n    \"typescript\": \"^4.9.3\",\n    \"vite\": \"^4.2.0\"\n  }\n}\n`\n```",
      "solution": "Change your docker-compose to mount an anonymous persistent volume to\nnode_modules to prevent your local overriding it\nsource\n\nThis worked for me\n```\n`version: '3.9'\n\nservices:\n  client:\n    build: .\n    ports:\n      - 5173:5173\n    volumes:\n      - .:/usr/src/app\n      - /usr/src/app/node_modules\n`\n```",
      "question_score": 9,
      "answer_score": 22,
      "created_at": "2023-03-25T14:11:37",
      "url": "https://stackoverflow.com/questions/75841925/why-is-docker-compose-throwing-vite-not-found-during-the-build"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 68242250,
      "title": "Unable to run mongo express with docker compose",
      "problem": "This is my first post in node js and docker so bear with me. I am running a mongo and mongo express container with the docker-compose but mongo express is not running. When I run mongo and mongo express without docker-compose it works perfectly. So, I think I am having some issues in docker-compose or maybe node js code\ndocker-compose.yaml\n```\n`version: '3'\nservices:\n  mongodb:\n    image: mongo\n    ports:\n      - 27017:27017\n    environment:\n      - MONGO_INITDB_ROOT_USERNAME=admin\n      - MONGO_INITDB_ROOT_PASSWORD=password\n  mongo-express:\n    image: mongo-express\n    ports:\n      - 8080:8081\n    environment:\n      - ME_CONFIG_MONGODB_ADMINUSERNAME=admin\n      - ME_CONFIG_MONGODB_ADMINPASSWORD=password\n      - ME_CONFIG_MONGODB_SERVER=mongodb\n`\n```\nserver.js\n```\n`let express = require('express');\nlet path = require('path');\nlet fs = require('fs');\nlet MongoClient = require('mongodb').MongoClient;\nlet bodyParser = require('body-parser');\nlet app = express();\n\napp.use(bodyParser.urlencoded({\n  extended: true\n}));\napp.use(bodyParser.json());\n\napp.get('/', function (req, res) {\n    res.sendFile(path.join(__dirname, \"index.html\"));\n  });\n\napp.get('/profile-picture', function (req, res) {\n  let img = fs.readFileSync(path.join(__dirname, \"images/profile-1.jpg\"));\n  res.writeHead(200, {'Content-Type': 'image/jpg' });\n  res.end(img, 'binary');\n});\n\n// use when starting application locally\nlet mongoUrlLocal = \"mongodb://admin:password@localhost:27017\";\n\n// use when starting application as docker container\nlet mongoUrlDocker = \"mongodb://admin:password@mongodb\";\n\n// pass these options to mongo client connect request to avoid DeprecationWarning for current Server Discovery and Monitoring engine\nlet mongoClientOptions = { useNewUrlParser: true, useUnifiedTopology: true };\n\n// \"user-account\" in demo with docker. \"my-db\" in demo with docker-compose\nlet databaseName = \"my-db\";\n\napp.post('/update-profile', function (req, res) {\n  let userObj = req.body;\n\n  MongoClient.connect(mongoUrlLocal, mongoClientOptions, function (err, client) {\n    if (err) throw err;\n\n    let db = client.db(databaseName);\n    userObj['userid'] = 1;\n\n    let myquery = { userid: 1 };\n    let newvalues = { $set: userObj };\n\n    db.collection(\"users\").updateOne(myquery, newvalues, {upsert: true}, function(err, res) {\n      if (err) throw err;\n      client.close();\n    });\n\n  });\n  // Send response\n  res.send(userObj);\n});\n\napp.get('/get-profile', function (req, res) {\n  let response = {};\n  // Connect to the db\n  MongoClient.connect(mongoUrlLocal, mongoClientOptions, function (err, client) {\n    if (err) throw err;\n\n    let db = client.db(databaseName);\n\n    let myquery = { userid: 1 };\n\n    db.collection(\"users\").findOne(myquery, function (err, result) {\n      if (err) throw err;\n      response = result;\n      client.close();\n\n      // Send response\n      res.send(response ? response : {});\n    });\n  });\n});\n\napp.listen(3000, function () {\n  console.log(\"app listening on port 3000!\");\n});\n`\n```\nIf I run docker ps I can only see mongo is running\n```\n`CONTAINER ID   IMAGE     COMMAND                  CREATED          STATUS          PORTS                                           NAMES\nd20c4784d316   mongo     \"docker-entrypoint.s\u2026\"   43 seconds ago   Up 38 seconds   0.0.0.0:27017->27017/tcp, :::27017->27017/tcp   nodeapplications_mongodb_1\n`\n```\nAnd when I run my docker-compose with the below command I see this log where I suspect an issue. Any help is appreciated\ndocker-compose -f docker-compose.yaml up\nLogs\n```\n`mongo-express_1  | Welcome to mongo-express\nmongo-express_1  | ------------------------\nmongo-express_1  | \nmongo-express_1  | \nmongo-express_1  | (node:7) [MONGODB DRIVER] Warning: Current Server Discovery and Monitoring engine is deprecated, and will be removed in a future version. To use the new Server Discover and Monitoring engine, pass option { useUnifiedTopology: true } to the MongoClient constructor.\nmongo-express_1  | Could not connect to database using connectionString: mongodb://admin:password@mongodb:27017/\"\nmongo-express_1  | (node:7) UnhandledPromiseRejectionWarning: MongoNetworkError: failed to connect to server [mongodb:27017] on first connect [Error: connect ECONNREFUSED 172.19.0.3:27017\nmongo-express_1  |     at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1144:16) {\nmongo-express_1  |   name: 'MongoNetworkError'\nmongo-express_1  | }]\nmongo-express_1  |     at Pool. (/node_modules/mongodb/lib/core/topologies/server.js:438:11)\nmongo-express_1  |     at Pool.emit (events.js:314:20)\nmongo-express_1  |     at /node_modules/mongodb/lib/core/connection/pool.js:562:14\nmongo-express_1  |     at /node_modules/mongodb/lib/core/connection/pool.js:995:11\nmongo-express_1  |     at /node_modules/mongodb/lib/core/connection/connect.js:32:7\nmongo-express_1  |     at callback (/node_modules/mongodb/lib/core/connection/connect.js:280:5)\nmongo-express_1  |     at Socket. (/node_modules/mongodb/lib/core/connection/connect.js:310:7)\nmongo-express_1  |     at Object.onceWrapper (events.js:421:26)\nmongo-express_1  |     at Socket.emit (events.js:314:20)\nmongo-express_1  |     at emitErrorNT (internal/streams/destroy.js:92:8)\nmongo-express_1  |     at emitErrorAndCloseNT (internal/streams/destroy.js:60:3)\nmongo-express_1  |     at processTicksAndRejections (internal/process/task_queues.js:84:21)\nmongo-express_1  | (node:7) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag `--unhandled-rejections=strict` (see https://nodejs.org/api/cli.html#cli_unhandled_rejections_mode). (rejection id: 1)\n`\n```\nAs suggested by @Blunderchips Update 1\nserver.js\n```\n`let express = require('express');\nlet path = require('path');\nlet fs = require('fs');\nlet MongoClient = require('mongodb').MongoClient;\nlet bodyParser = require('body-parser');\nlet app = express();\n\nconst dbServer = process.env.ME_CONFIG_MONGODB_SERVER;\nconst dbPassword = process.env.ME_CONFIG_MONGODB_ADMINPASSWORD;\nconst dbUserName = process.env.ME_CONFIG_MONGODB_ADMINUSERNAME;\nconst dbPort = process.env.ME_CONFIG_MONGODB_PORT;\n\napp.use(bodyParser.urlencoded({\n  extended: true\n}));\napp.use(bodyParser.json());\n\napp.get('/', function (req, res) {\n    res.sendFile(path.join(__dirname, \"index.html\"));\n  });\n\napp.get('/profile-picture', function (req, res) {\n  let img = fs.readFileSync(path.join(__dirname, \"images/profile-1.jpg\"));\n  res.writeHead(200, {'Content-Type': 'image/jpg' });\n  res.end(img, 'binary');\n});\n\n// use when starting application locally\n//let mongoUrlLocal = \"mongodb://admin:password@localhost:27017\";\n\n// use when starting application as docker container\nlet mongoUrlDocker = `mongodb://${dbUserName}:${dbPassword}@${dbServer}:${dbPort}`;//\"mongodb://admin:password@mongodb:27017\";//\"mongodb://admin:password@mongodb\";\n\n// pass these options to mongo client connect request to avoid DeprecationWarning for current Server Discovery and Monitoring engine\nlet mongoClientOptions = { useNewUrlParser: true, useUnifiedTopology: true };\n\n// \"user-account\" in demo with docker. \"my-db\" in demo with docker-compose\nlet databaseName = \"my-db\";\n\napp.post('/update-profile', function (req, res) {\n  let userObj = req.body;\n\n  MongoClient.connect(mongoUrlDocker, mongoClientOptions, function (err, client) {\n    if (err) throw err;\n\n    let db = client.db(databaseName);\n    userObj['userid'] = 1;\n\n    let myquery = { userid: 1 };\n    let newvalues = { $set: userObj };\n\n    db.collection(\"users\").updateOne(myquery, newvalues, {upsert: true}, function(err, res) {\n      if (err) throw err;\n      client.close();\n    });\n\n  });\n  // Send response\n  res.send(userObj);\n});\n\napp.get('/get-profile', function (req, res) {\n  let response = {};\n  // Connect to the db\n  MongoClient.connect(mongoUrlDocker, mongoClientOptions, function (err, client) {\n    if (err) throw err;\n\n    let db = client.db(databaseName);\n\n    let myquery = { userid: 1 };\n\n    db.collection(\"users\").findOne(myquery, function (err, result) {\n      if (err) throw err;\n      response = result;\n      client.close();\n\n      // Send response\n      res.send(response ? response : {});\n    });\n  });\n});\n\napp.listen(3000, function () {\n  console.log(\"app listening on port 3000!\");\n});\n`\n```\ndocker-compose.yaml\n```\n`version: '3'\nservices:\n  mongodb:\n    image: mongo\n    ports:\n      - 27017:27017\n    environment:\n      - MONGO_INITDB_ROOT_USERNAME=admin\n      - MONGO_INITDB_ROOT_PASSWORD=password\n  mongo-express:\n    image: mongo-express\n    ports:\n      - 8080:8081\n    environment:\n      - ME_CONFIG_MONGODB_ADMINUSERNAME=admin\n      - ME_CONFIG_MONGODB_ADMINPASSWORD=password\n      - ME_CONFIG_MONGODB_SERVER=mongodb\n    links: \n        - mongodb:mongodb\n`\n```\nI still can't see mongo express running\n```\n`docker ps\nCONTAINER ID   IMAGE     COMMAND                  CREATED              STATUS              PORTS                                           NAMES\n23428dc0c3a1   mongo     \"docker-entrypoint.s\u2026\"   About a minute ago   Up About a minute   0.0.0.0:27017->27017/tcp, :::27017->27017/tcp   nodeapplications_mongodb_1\n`\n```\nLogs\n```\n`mongo-express_1  | Welcome to mongo-express\nmongo-express_1  | ------------------------\nmongo-express_1  | \nmongo-express_1  | \nmongo-express_1  | (node:7) [MONGODB DRIVER] Warning: Current Server Discovery and Monitoring engine is deprecated, and will be removed in a future version. To use the new Server Discover and Monitoring engine, pass option { useUnifiedTopology: true } to the MongoClient constructor.\nmongo-express_1  | Could not connect to database using connectionString: mongodb://admin:password@mongodb:27017/\"\nmongo-express_1  | (node:7) UnhandledPromiseRejectionWarning: MongoNetworkError: failed to connect to server [mongodb:27017] on first connect [Error: connect ECONNREFUSED 172.23.0.2:27017\nmongo-express_1  |     at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1144:16) {\nmongo-express_1  |   name: 'MongoNetworkError'\nmongo-express_1  | }]\nmongo-express_1  |     at Pool. (/node_modules/mongodb/lib/core/topologies/server.js:438:11)\nmongo-express_1  |     at Pool.emit (events.js:314:20)\nmongo-express_1  |     at /node_modules/mongodb/lib/core/connection/pool.js:562:14\nmongo-express_1  |     at /node_modules/mongodb/lib/core/connection/pool.js:995:11\nmongo-express_1  |     at /node_modules/mongodb/lib/core/connection/connect.js:32:7\nmongo-express_1  |     at callback (/node_modules/mongodb/lib/core/connection/connect.js:280:5)\nmongo-express_1  |     at Socket. (/node_modules/mongodb/lib/core/connection/connect.js:310:7)\nmongo-express_1  |     at Object.onceWrapper (events.js:421:26)\nmongo-express_1  |     at Socket.emit (events.js:314:20)\nmongo-express_1  |     at emitErrorNT (internal/streams/destroy.js:92:8)\nmongo-express_1  |     at emitErrorAndCloseNT (internal/streams/destroy.js:60:3)\nmongo-express_1  |     at processTicksAndRejections (internal/process/task_queues.js:84:21)\nmongo-express_1  | (node:7) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag `--unhandled-rejections=strict` (see https://nodejs.org/api/cli.html#cli_unhandled_rejections_mode). (rejection id: 1)\nmongo-express_1  | (node:7) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.\n`\n```\nUpdate 2\n```\n`let express = require('express');\nlet path = require('path');\nlet fs = require('fs');\nlet MongoClient = require('mongodb').MongoClient;\nlet bodyParser = require('body-parser');\nlet app = express();\n\napp.use(bodyParser.urlencoded({\n  extended: true\n}));\napp.use(bodyParser.json());\n\napp.get('/', function (req, res) {\n    res.sendFile(path.join(__dirname, \"index.html\"));\n  });\n\napp.get('/profile-picture', function (req, res) {\n  let img = fs.readFileSync(path.join(__dirname, \"images/profile-1.jpg\"));\n  res.writeHead(200, {'Content-Type': 'image/jpg' });\n  res.end(img, 'binary');\n});\n\n// use when starting application locally\n//let mongoUrlLocal = \"mongodb://admin:password@localhost:27017\";\n\n// use when starting application as docker container\nlet mongoUrlDocker = \"mongodb://admin:password@mongodb:27017\";\n\n// pass these options to mongo client connect request to avoid DeprecationWarning for current Server Discovery and Monitoring engine\nlet mongoClientOptions = { useNewUrlParser: true, useUnifiedTopology: true };\n\n// \"user-account\" in demo with docker. \"my-db\" in demo with docker-compose\nlet databaseName = \"my-db\";\n\napp.post('/update-profile', function (req, res) {\n  let userObj = req.body;\n\n  MongoClient.connect(mongoUrlDocker, mongoClientOptions, function (err, client) {\n    if (err) throw err;\n\n    let db = client.db(databaseName);\n    userObj['userid'] = 1;\n\n    let myquery = { userid: 1 };\n    let newvalues = { $set: userObj };\n\n    db.collection(\"users\").updateOne(myquery, newvalues, {upsert: true}, function(err, res) {\n      if (err) throw err;\n      client.close();\n    });\n\n  });\n  // Send response\n  res.send(userObj);\n});\n\napp.get('/get-profile', function (req, res) {\n  let response = {};\n  // Connect to the db\n  MongoClient.connect(mongoUrlDocker, mongoClientOptions, function (err, client) {\n    if (err) throw err;\n\n    let db = client.db(databaseName);\n\n    let myquery = { userid: 1 };\n\n    db.collection(\"users\").findOne(myquery, function (err, result) {\n      if (err) throw err;\n      response = result;\n      client.close();\n\n      // Send response\n      res.send(response ? response : {});\n    });\n  });\n});\n\napp.listen(3000, function () {\n  console.log(\"app listening on port 3000!\");\n});\n`\n```\nUpdate 3\ndocker-compose\n```\n`version: '3'\nservices:\n  mongodb:\n    image: mongo\n    ports:\n      - 27017:27017\n    environment:\n      - MONGO_INITDB_ROOT_USERNAME=admin\n      - MONGO_INITDB_ROOT_PASSWORD=password\n  mongo-express:\n    image: mongo-express\n    ports:\n      - 8080:8081\n    environment:\n      - ME_CONFIG_MONGODB_ADMINUSERNAME=admin\n      - ME_CONFIG_MONGODB_ADMINPASSWORD=password\n      - ME_CONFIG_MONGODB_SERVER=mongodb\n    links: \n        - mongodb:mongodb\n    restart: on-failure\n`\n```\nFull Logs\n```\n` mongo-express_1  | Welcome to mongo-express\nmongo-express_1  | ------------------------\nmongo-express_1  | \nmongo-express_1  | \nmongodb_1        | {\"t\":{\"$date\":\"2021-07-04T10:41:58.806+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":22318,   \"ctx\":\"SignalHandler\",\"msg\":\"Shutting down session sweeper thread\"}\nmongodb_1        | {\"t\":{\"$date\":\"2021-07-04T10:41:58.806+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":22319,   \"ctx\":\"SignalHandler\",\"msg\":\"Finished shutting down session sweeper thread\"}\nmongodb_1        | {\"t\":{\"$date\":\"2021-07-04T10:41:58.807+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":22322,   \"ctx\":\"SignalHandler\",\"msg\":\"Shutting down checkpoint thread\"}\nmongodb_1        | {\"t\":{\"$date\":\"2021-07-04T10:41:58.807+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":22323,   \"ctx\":\"SignalHandler\",\"msg\":\"Finished shutting down checkpoint thread\"}\nmongodb_1        | {\"t\":{\"$date\":\"2021-07-04T10:41:58.807+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":4795902, \"ctx\":\"SignalHandler\",\"msg\":\"Closing WiredTiger\",\"attr\":{\"closeConfig\":\"leak_memory=true,\"}}\nmongodb_1        | {\"t\":{\"$date\":\"2021-07-04T10:41:58.810+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":22430,   \"ctx\":\"SignalHandler\",\"msg\":\"WiredTiger message\",\"attr\":{\"message\":\"[1625395318:810568][28:0x7f50eec9b700], close_ckpt: [WT_VERB_CHECKPOINT_PROGRESS] saving checkpoint snapshot min: 48, snapshot max: 48 snapshot count: 0, oldest timestamp: (0, 0) , meta checkpoint timestamp: (0, 0)\"}}\nmongo-express_1  | (node:7) [MONGODB DRIVER] Warning: Current Server Discovery and Monitoring engine is deprecated, and will be removed in a future version. To use the new Server Discover and Monitoring engine, pass option { useUnifiedTopology: true } to the MongoClient constructor.\nmongo-express_1  | Could not connect to database using connectionString: mongodb://admin:password@mongodb:27017/\"\nmongo-express_1  | (node:7) UnhandledPromiseRejectionWarning: MongoNetworkError: failed to connect to server [mongodb:27017] on first connect [Error: connect ECONNREFUSED 172.27.0.2:27017\nmongo-express_1  |     at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1144:16) {\nmongo-express_1  |   name: 'MongoNetworkError'\nmongo-express_1  | }]\nmongo-express_1  |     at Pool. (/node_modules/mongodb/lib/core/topologies/server.js:438:11)\nmongo-express_1  |     at Pool.emit (events.js:314:20)\nmongo-express_1  |     at /node_modules/mongodb/lib/core/connection/pool.js:562:14\nmongo-express_1  |     at /node_modules/mongodb/lib/core/connection/pool.js:995:11\nmongo-express_1  |     at /node_modules/mongodb/lib/core/connection/connect.js:32:7\nmongo-express_1  |     at callback (/node_modules/mongodb/lib/core/connection/connect.js:280:5)\nmongo-express_1  |     at Socket. (/node_modules/mongodb/lib/core/connection/connect.js:310:7)\nmongo-express_1  |     at Object.onceWrapper (events.js:421:26)\nmongo-express_1  |     at Socket.emit (events.js:314:20)\nmongo-express_1  |     at emitErrorNT (internal/streams/destroy.js:92:8)\nmongo-express_1  |     at emitErrorAndCloseNT (internal/streams/destroy.js:60:3)\nmongo-express_1  |     at processTicksAndRejections (internal/process/task_queues.js:84:21)\nmongo-express_1  | (node:7) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag `--unhandled-rejections=strict` (see https://nodejs.org/api/cli.html#cli_unhandled_rejections_mode). (rejection id: 1)\nmongo-express_1  | (node:7) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.\nmongodb_1        | {\"t\":{\"$date\":\"2021-07-04T10:42:00.871+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":4795901, \"ctx\":\"SignalHandler\",\"msg\":\"WiredTiger closed\",\"attr\":{\"durationMillis\":2064}}\nmongodb_1        | {\"t\":{\"$date\":\"2021-07-04T10:42:00.871+00:00\"},\"s\":\"I\",  \"c\":\"STORAGE\",  \"id\":22279,   \"ctx\":\"SignalHandler\",\"msg\":\"shutdown: removing fs lock...\"}\nmongodb_1        | {\"t\":{\"$date\":\"2021-07-04T10:42:00.872+00:00\"},\"s\":\"I\",  \"c\":\"-\",        \"id\":4784931, \"ctx\":\"SignalHandler\",\"msg\":\"Dropping the scope cache for shutdown\"}\nmongodb_1        | {\"t\":{\"$date\":\"2021-07-04T10:42:00.873+00:00\"},\"s\":\"I\",  \"c\":\"FTDC\",     \"id\":4784926, \"ctx\":\"SignalHandler\",\"msg\":\"Shutting down full-time data capture\"}\nmongodb_1        | {\"t\":{\"$date\":\"2021-07-04T10:42:00.873+00:00\"},\"s\":\"I\",  \"c\":\"FTDC\",     \"id\":20626,   \"ctx\":\"SignalHandler\",\"msg\":\"Shutting down full-time diagnostic data capture\"}\nmongodb_1        | {\"t\":{\"$date\":\"2021-07-04T10:42:00.878+00:00\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":20565,   \"ctx\":\"SignalHandler\",\"msg\":\"Now exiting\"}\nmongodb_1        | {\"t\":{\"$date\":\"2021-07-04T10:42:00.879+00:00\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":23138,   \"ctx\":\"SignalHandler\",\"msg\":\"Shutting down\",\"attr\":{\"exitCode\":0}}\nnodeapplications_mongo-express_1 exited with code 0\nmongodb_1        | \nmongodb_1        | MongoDB init process complete; ready for start up.\n`\n```",
      "solution": "As suggested by @David Maze by adding restart: unless-stopped it worked\n```\n`version: '3'\nservices:\n  mongodb:\n    image: mongo\n    ports:\n      - 27017:27017\n    environment:\n      - MONGO_INITDB_ROOT_USERNAME=admin\n      - MONGO_INITDB_ROOT_PASSWORD=password\n  mongo-express:\n    image: mongo-express\n    ports:\n      - 8081:8081\n    environment:\n      - ME_CONFIG_MONGODB_ADMINUSERNAME=admin\n      - ME_CONFIG_MONGODB_ADMINPASSWORD=password\n      - ME_CONFIG_MONGODB_SERVER=mongodb\n    restart: unless-stopped\n`\n```",
      "question_score": 9,
      "answer_score": 19,
      "created_at": "2021-07-04T08:24:11",
      "url": "https://stackoverflow.com/questions/68242250/unable-to-run-mongo-express-with-docker-compose"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 68151318,
      "title": "Make an API call from one container to another",
      "problem": "I have a node application , a next.js app for front end ,Redis and Postgres as databases . I have dockerized Next.js and node.js in different containers .\nDocker-compose.yaml is as follows\n```\n`version: '3'\nservices: \n  redis-server:\n    image: 'redis'\n    restart: always\n  postgres-server:\n    image: 'postgres:latest'\n    environment:\n      - POSTGRES_USER=postgres\n      - POSTGRES_PASSWORD=postgres\n      - POSTGRES_DB=postgres\n    ports: \n      - \"5433:5432\"\n    volumes:\n        - ./docker/postgres/data/data/pg_hba.conf:/var/lib/postgresql/app_data/pg_hba.conf\n        - ./src/db/sql/CREATE_TABLES.sql:/docker-entrypoint-initdb.d/CREATE_TABLES.sql\n        - ./src/db/sql/INSERT_TO_TABLES.sql:/docker-entrypoint-initdb.d/INSERT_TO_TABLES.sql\n        - ./src/db/sql/CREATE_FUNCTIONS.sql:/docker-entrypoint-initdb.d/CREATE_FUNCTIONS.sql\n  node-app:\n    build: .\n    ports: \n      - \"4200:4200\" \n  client:\n    build:\n      context: ./client\n      dockerfile: Dockerfile\n    container_name: client\n    restart: always\n    volumes:\n      - ./:/app\n      - /app/node_modules\n      - /app/.next\n    ports:\n      - 3000:3000\n`\n```\nWhen using SSR , I cannot make a request to localhost:4200 .Now, I get that it is because they are in different containers and if the request is not from client side , then the client container is being checked for server at port 4200. Now , I am not sure how to simply refer to the container using the container name or something to make an API request for the SSR data (like `fetch('node-app/users')`)",
      "solution": "docker-compose sets up a network for all the services in a compose file. To reach another container in the network, you use the name of that container as a hostname, and it will resolve to the right ip.\nSo in your case doing `fetch('http://node-app:4200/users')` should do the trick.",
      "question_score": 9,
      "answer_score": 22,
      "created_at": "2021-06-27T14:48:32",
      "url": "https://stackoverflow.com/questions/68151318/make-an-api-call-from-one-container-to-another"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 68227680,
      "title": "Having permissions issues with Grafana 7.3.0 on Docker",
      "problem": "I'm using docker-compose to create a Docker network  of containers with InfluxDB, a python script and Grafana to harvest and visualize response codes, query times & other stats of different websites.\nI am using Grafana image 7.3.0 with a volume,\nI have modified the paths environment variables so I'll have to use only one volume to save all the data.\nWhen I start the Grafana container it logs:\n```\n`GF_PATHS_CONFIG='/etc/grafana/grafana.ini' is not readable.\nGF_PATHS_DATA='/etc/grafana/data' is not writable.\nGF_PATHS_HOME='/etc/grafana/home' is not readable.\n\nYou may have issues with file permissions, more information here: http://docs.grafana.org/installation/docker/#migration-from-a-previous-version-of-the-\n\ndocker-container-to-5-1-or-later\n\nmkdir: can't create directory '/etc/grafana/plugins': Permission denied\n`\n```\nBut here is the thing, I'm not migrating from below 5.1 I'm not even migrating at all!\nSo I tried to follow their instruction to change permissions of files but it did not worked.\nI tried to set the user id  in the docker-compose but it did not help.\n(as-said in the docs 472 == post 5.1, 104 == pre 5.1 but both did not worked)\nI can't even change permissions manually (which is not a satisfying solution btw) because the container is crashing.\nI normally don't ask questions because they already have answers but I've seen no one with this trouble using 7.3.0 so I guess it's my time to shine Haha.\nHere is my docker-compose.yml (only the grafana part)\n```\n`version: '3.3'\n\nservices:\n  grafana:\n    image: grafana/grafana:7.3.0\n    ports:\n      - '3000:3000'\n    volumes:\n      - './grafana:/etc/grafana'\n    networks:\n      - db-to-grafana\n    depends_on:\n      - db\n      - influxdb_cli\n    environment:\n      - GF_PATHS_CONFIG=/etc/grafana/grafana.ini\n      - GF_PATHS_DATA=/etc/grafana/data\n      - GF_PATHS_HOME=/etc/grafana/home\n      - GF_PATHS_LOGS=/etc/grafana/logs\n      - GF_PATHS_PLUGINS=/etc/grafana/plugins\n      - GF_PATHS_PROVISIONING=/etc/grafana/provisioning\n   user: \"472\"\n`\n```\nThank you very much for your potential help!\nEdit : I've been wondering if there is a grafana user in latest version (8.0), I think that build a home dir for grafana using a Dockerfile could be the solution I just need to find that user.",
      "solution": "I'm here to close this subject.\nSo this was kind of a noob mistake but I could not have known.\nThe problem came from the fact that Grafana won't chown and chmod the volume folder. No error will be raised during install or execution of Grafana, but the software won't work because it cannot save it's data.\nThe solution was to remove the env variables and changing permissions of the local './grafana' folder wich contained the volume.\nSo I did\n```\n`chown -R  /path/to/local/volume/folder && \\\nchmod -R 777 /path/to/local/volume/folder\n`\n```\nAnd now it works normally\nHere is my new docker compose\n```\n`   docker-compose.yml   \n    grafana:\n        image: grafana/grafana\n        ports:\n          - '3000:3000'\n        volumes:\n          - './grafana:/var/lib/grafana'\n        networks:\n          - db-to-grafana\n        depends_on:\n          - db\n          - influxdb_cli\n`\n```\nThanks everybody four your help !",
      "question_score": 9,
      "answer_score": 17,
      "created_at": "2021-07-02T17:19:46",
      "url": "https://stackoverflow.com/questions/68227680/having-permissions-issues-with-grafana-7-3-0-on-docker"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 70454682,
      "title": "why does yarn --watch exit (send SIGTERM)",
      "problem": "I have a Docker installation that I would like to start with `docker compose up` (and not have to run 2 extra ttys ) so I added a Procfile.dev looking like this\n```\n`web: bin/rails server -p 3000 -b '0.0.0.0'\njs: yarn build_js --watch\ncss: yarn build_css --watch\n`\n```\nThe output is, however, less than enjoyable\n```\n`\u221a mindling % docker compose up\n[+] Running 3/0\n \u283f Container mindling_redis       Running                                                                                                                                     0.0s\n \u283f Container mindling_db          Running                                                                                                                                     0.0s\n \u283f Container mindling_mindling_1  Created                                                                                                                                     0.0s\nAttaching to mindling_db, mindling_1, mindling_redis\nmindling_1      | 19:54:04 web.1  | started with pid 16\nmindling_1      | 19:54:04 js.1   | started with pid 19\nmindling_1      | 19:54:04 css.1  | started with pid 22\nmindling_1      | 19:54:06 css.1  | yarn run v1.22.17\nmindling_1      | 19:54:06 js.1   | yarn run v1.22.17\nmindling_1      | 19:54:06 js.1   | $ esbuild app/javascript/*.* --bundle --outdir=app/assets/builds --watch\nmindling_1      | 19:54:06 css.1  | $ tailwindcss -i ./app/assets/stylesheets/application.tailwind.css -o ./app/assets/builds/application.css --watch\nmindling_1      | 19:54:08 js.1   | Done in 2.02s.\nmindling_1      | 19:54:08 js.1   | exited with code 0\nmindling_1      | 19:54:08 system | sending SIGTERM to all processes\nmindling_1      | 19:54:08 web.1  | terminated by SIGTERM\nmindling_1      | 19:54:09 css.1  | terminated by SIGTERM\nmindling_1 exited with code 0\n`\n```\nI've tried running a Bash in the application container - and calling the Procfile in a tty by itself looks more or less like this:\n```\n`root@facfb249dc6b:/app# foreman start -f Procfile.dev\n20:11:45 web.1  | started with pid 12\n20:11:45 js.1   | started with pid 15\n20:11:45 css.1  | started with pid 18\n20:11:48 css.1  | yarn run v1.22.17\n20:11:48 js.1   | yarn run v1.22.17\n20:11:48 css.1  | $ tailwindcss -i ./app/assets/stylesheets/application.tailwind.css -o ./app/assets/builds/application.css --watch\n20:11:49 js.1   | $ esbuild app/javascript/*.* --bundle --outdir=app/assets/builds --watch\n20:11:50 js.1   | [watch] build finished, watching for changes...\n20:11:53 web.1  | => Booting Puma\n20:11:53 web.1  | => Rails 7.0.0 application starting in development \n20:11:53 web.1  | => Run `bin/rails server --help` for more startup options\n20:11:57 web.1  | Puma starting in single mode...\n20:11:57 web.1  | * Puma version: 5.5.2 (ruby 3.0.3-p157) (\"Zawgyi\")\n20:11:57 web.1  | *  Min threads: 5\n20:11:57 web.1  | *  Max threads: 5\n20:11:57 web.1  | *  Environment: development\n20:11:57 web.1  | *          PID: 22\n20:11:57 web.1  | * Listening on http://0.0.0.0:3000\n20:11:57 web.1  | Use Ctrl-C to stop\n20:11:58 css.1  | \n20:11:58 css.1  | Rebuilding...\n20:11:59 css.1  | Done in 1066ms.\n^C20:13:23 system | SIGINT received, starting shutdown\n20:13:23 web.1  | - Gracefully stopping, waiting for requests to finish\n20:13:23 web.1  | === puma shutdown: 2021-12-22 20:13:23 +0000 ===\n20:13:23 web.1  | - Goodbye!\n20:13:23 web.1  | Exiting\n20:13:24 system | sending SIGTERM to all processes\n20:13:25 web.1  | terminated by SIGINT\n20:13:25 js.1   | terminated by SIGINT\n20:13:25 css.1  | terminated by SIGINT\nroot@facfb249dc6b:/app# \n`\n```\nWhat is going on? It works when doing it 'by hand' but if I let docker-compose rip the processes somehow terminates!?!\nI have isolated the issue to the `build_css` script in package.json (or at least it does keep going if I comment that line in the Procfile.dev)\n\nAll the 'dirty linen'\nMy package.json looks like this\n```\n`{\n...8My containers are exceptionally boring, looking like almost everybody else's:\n```\n`FROM ruby:3.0.3\nRUN curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | apt-key add -\nRUN echo \"deb https://dl.yarnpkg.com/debian/ stable main\" | tee /etc/apt/sources.list.d/yarn.list\nRUN apt-get update && apt-get install -y nodejs yarn\nWORKDIR /app\nCOPY src/Gemfile /app/Gemfile\nCOPY src/Gemfile.lock /app/Gemfile.lock\nRUN gem install bundler foreman && bundle install\nEXPOSE 3000\nENTRYPOINT [ \"entrypoint.sh\" ]\n`\n```\n```\n`version: \"3.9\"\n  db:\n    build: mysql\n    image: mindling_db\n    container_name: mindling_db\n    command: [ \"--default-authentication-plugin=mysql_native_password\" ]\n    ports:\n      - \"3306:3306\"\n    volumes:\n      - ~/src/mysql_data:/var/lib/mysql\n    environment:\n      MYSQL_ROOT_PASSWORD: secret\n      MYSQL_DATABASE: mindling_development\n\n  mindling:\n    platform: linux/x86_64\n    build: .\n    volumes:\n      - ./src:/app\n    ports:\n      - \"3000:3000\"\n    depends_on:\n      - db\n\n`\n```\nand finally my entrypoint.sh\n```\n`#!/usr/bin/env bash\nrm -rf /app/tmp/pids/server.pid\nforeman start -f Procfile.dev\n`\n```",
      "solution": "Allow me to give credit to they who deserve it!! The correct answer was provided by earlopain in this issue on rails/rails\nIt's actually an almost embarrassingly easy fix - once you know it :)\nAdd tty: true to your docker-compose.yml - like this\n```\n`  mindling:\n    platform: linux/x86_64\n    build: .\n    tty: true\n    volumes:\n      - ./src:/app\n    ports:\n      - \"3000:3000\"\n    depends_on:\n      - db\n\n`\n```",
      "question_score": 9,
      "answer_score": 16,
      "created_at": "2021-12-22T21:19:37",
      "url": "https://stackoverflow.com/questions/70454682/why-does-yarn-watch-exit-send-sigterm"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 68194327,
      "title": "How to configure celery worker on distributed airflow architecture using docker-compose?",
      "problem": "I\u2019m setting up a distributed Airflow cluster where everything else except the celery workers are run on one host and processing is done on several hosts. The airflow2.0 setup is configured using the yaml file given at the Airflow documentation https://airflow.apache.org/docs/apache-airflow/stable/docker-compose.yaml . In my initial tests I got the architecture to work nicely when I run everything at the same host. The question is, how to start the celery workers at the remote hosts?\nSo far, I tried to create a trimmed version of the above docker-compose where I only start the celery workers at the worker host and nothing else. But I run into some issues with db connection. In the trimmed version I changed the URL so that they point to the host that runs the db and redis.\ndags, logs, plugins and the postgresql db are located on a shared drive that is visible to all hosts.\nHow should I do the configuration? Any ideas what to check? Connections etc.?\nCelery worker docker-compose configuration:\n```\n`---\nversion: '3'\nx-airflow-common:\n  &airflow-common\n  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.1.0}\n  environment:\n    &airflow-common-env\n    AIRFLOW_UID: 50000\n    AIRFLOW_GID: 50000\n    AIRFLOW__CORE__EXECUTOR: CeleryExecutor\n    AIRFLOW__CORE__SQL_ALCHEMY_CONN: \npostgresql+psycopg2://airflow:airflow@airflowhost.example.com:8080/airflow\n    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@airflow@airflowhost.example.com:8080/airflow\n    AIRFLOW__CELERY__BROKER_URL: redis://:@airflow@airflowhost.example.com:6380/0\n    AIRFLOW__CORE__FERNET_KEY: ''\n    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'\n    AIRFLOW__CORE__LOAD_EXAMPLES: 'true'\n    AIRFLOW__API__AUTH_BACKEND: 'airflow.api.auth.backend.basic_auth'\n    REDIS_PORT: 6380\n   volumes:\n    - /airflow/dev/dags:/opt/airflow/dags\n    - /airflow/dev/logs:/opt/airflow/logs\n    - /airflow/dev/plugins:/opt/airflow/plugins\n   user: \"${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-50000}\"\nservices:\n  airflow-remote-worker:\n    EDIT 1:\nI'm Still having some difficulties with the log files. It appears that sharing the log directory doesn't solve the issue of missing log files. I added the extra_host definition on main like suggested and opened the port 8793 on the worker machine.\nThe worker tasks fail with log:\n```\n`*** Log file does not exist: \n/opt/airflow/logs/tutorial/print_date/2021-07- \n01T13:57:11.087882+00:00/1.log\n*** Fetching from: http://:8793/log/tutorial/print_date/2021-07-01T13:57:11.087882+00:00/1.log\n*** Failed to fetch log file from worker. Unsupported URL protocol ''\n`\n```",
      "solution": "Far from being the \"ultimate set-up\", these are some settings that worked for me using the docker-compose from Airflow in the core node and the workers:\nMain node:\n\nThe worker nodes have to be reachable from the main node where the `Webserver` runs. I found this diagram of the `CeleryExecutor` architecture to be very helpful to sort things out.\nWhen trying to read the logs, if they are not found locally, it will try to retrieve them from the remote worker. Thus your main node may not know the hostname of your workers, so you either change how the hostnames are being resolved (`hostname_callable` setting, which defaults to `socket.getfqdn` ) or you just simply add name resolution capability to the `Webserver`. This could be done by adding the `extra_hosts` config key in the `x-airflow-common` definition:\n\n`---\nversion: \"3\"\nx-airflow-common: &airflow-common\n  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.1.0}\n  environment: &airflow-common-env\n    ...# env vars\n  extra_hosts:\n    - \"worker-01-hostname:worker-01-ip-address\" # \"worker-01-hostname:192.168.0.11\"\n    - \"worker-02-hostname:worker-02-ip-address\"\n`\n*Note that in your specific case where you have a shared drive, so I think the logs will be found locally.\n\nDefine parallelism, DAG concurrency, and scheduler parsing processes. Could be done by using env vars:\n\n`x-airflow-common: &airflow-common\n  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.1.0}\n  environment: &airflow-common-env\n    AIRFLOW__CORE__PARALLELISM: 64\n    AIRFLOW__CORE__DAG_CONCURRENCY: 32\n    AIRFLOW__SCHEDULER__PARSING_PROCESSES: 4\n`\nOf course, the values to be set depend on your specific case and available resources. This article has a good overview of the subject. DAG settings could also be overridden at `DAG` definition.\nWorker nodes:\n\nDefine worker `CELERY__WORKER_CONCURRENCY`, default could be the numbers of CPUs available on the machine (docs).\n\nDefine how to reach the services running in the main node. Set an IP or hostname and watch out for matching exposed ports in the main node:\n\n`x-airflow-common: &airflow-common\n  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.1.0}\n  environment: &airflow-common-env\n  AIRFLOW__CORE__EXECUTOR: CeleryExecutor\n  AIRFLOW__CELERY__WORKER_CONCURRENCY: 8\n  AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@main_node_ip_or_hostname:5432/airflow # 5432 is default postgres port\n  AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@main_node_ip_or_hostname:5432/airflow\n  AIRFLOW__CELERY__BROKER_URL: redis://:@main_node_ip_or_hostname:6379/0\n`\n\nShare the same Fernet Key and Secret Key reading them from an \".env\" file:\n\n`  environment: &airflow-common-env\n    AIRFLOW__CORE__FERNET_KEY: ${FERNET_KEY}\n    AIRFLOW__WEBSERVER__SECRET_KEY: ${SECRET_KEY}\n\n  env_file:\n    - .env\n`\n.env file: `FERNET_KEY=jvYUaxxxxxxxxxxxxx=`\n\nIt's critical that every node in the cluster (main and workers) has the same settings applied.\n\nDefine a hostname to the worker service to avoid autogenerated matching the container id.\n\nExpose port 8793, which is the default port used to fetch the logs from the worker (docs):\n\n`services:\n  airflow-worker:\n    \n\nMake sure every worker node host is running with the same time configuration, a few minutes difference could cause serious execution errors which may not be so easy to find. Consider enabling NTP service on host OS.\n\nIf you have heavy workloads and high concurrency in general, you may need to tune Postgres settings such as `max_connections` and `shared_buffers`. The same applies to the host OS network settings such as `ip_local_port_range` or `somaxconn`.\nIn any issues I encountered during the initial cluster setup, `Flower` and the worker execution logs always provided helpful details and error messages, both the task-level logs and the Docker-Compose service log i.e: `docker-compose logs --tail=10000 airflow-worker > worker_logs.log`.\nHope that works for you!",
      "question_score": 9,
      "answer_score": 15,
      "created_at": "2021-06-30T13:59:44",
      "url": "https://stackoverflow.com/questions/68194327/how-to-configure-celery-worker-on-distributed-airflow-architecture-using-docker"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 67641068,
      "title": "Docker CLI failing to compose with &quot;Additional property postgres is not allowed&quot;",
      "problem": "My docker-compose.yml looks like this\n```\n`postgres:\n    container_name: postgres-container-1\n    image: postgres:latest\n    ports:\n        - \"15432:5432\"\n`\n```\nWhen I try to run\n```\n`docker compose up -d\n`\n```\nI get following error\n```\n`(root) Additional property postgres is not allowed\n`\n```\nHowever when I run\n```\n` docker-compose up -d\n`\n```\neverything runs as intended, but I get info message that\n```\n`Docker Compose is now in the Docker CLI, try `docker compose up`\n`\n```\nSo I assume that docker-compose will soon be obsolete... What did I do wrong?",
      "solution": "I had this issue as well. This other StackOverflow post fixed it for me.\nI needed the `services:` key at the top, with `postgres:` nested under it.\nMy docker-compose.yml\n`services:\n  postgres:\n    image: postgres:latest\n    environment:\n      POSTGRES_PASSWORD: password1\n      POSTGRES_USER: postgres\n      POSTGRES_DB: some-db\n    ports:\n      - '5432:5432'\n    volumes:\n      - ./sql/setup.sql:/docker-entrypoint-initdb.d/init.sql\n`",
      "question_score": 9,
      "answer_score": 18,
      "created_at": "2021-05-21T19:00:27",
      "url": "https://stackoverflow.com/questions/67641068/docker-cli-failing-to-compose-with-additional-property-postgres-is-not-allowed"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 67406041,
      "title": "Debug Docker with Rider: Exited with code 244",
      "problem": "Sorry if this question was already asked but I couldn't find any instance of \"Exited with code 244\".\nI'm trying to debug my docker images, we have a few and they work completely fine when you just run, the container is built and you can access them as expected.\nBut if I click on the Debug icon on Rider, it builds everything and then exits with code 244, no idea what to do.\nMy Rider connection to Docker is completely fine (it works when I don't debug)\nTried connecting to the Docker Daemon via TCP, it connects fine but the result is the same.\nTried debugging only one out of our 12 images and the result is the same (exited with code 244)\nhere's the deploy log:\n```\n`\"C:\\Program Files\\Docker\\Docker\\resources\\bin\\docker-compose.exe\" -f E:\\Git\\MYPROJECT\\src\\docker-compose.yml -f C:\\Users\\USER\\AppData\\Local\\JetBrains\\Rider2021.1\\tmp\\docker-compose.override.74.yml up\nDocker Compose is now in the Docker CLI, try `docker compose up`\nStarting src_authentication.internal.dev_1 ...\nStarting src_devredis_1                    ...\nStarting src_transactions.internal.dev_1   ...\nStarting src_userinvitations.internal.dev_1 ...\nStarting src_users.internal.dev_1           ...\nStarting src_devrabbit_1                    ...\nStarting src_debugdata.internal.dev_1       ...\nStarting src_authorization.internal.dev_1   ...\nStarting src_web.bff.agent_1                ...\nStarting src_globalsettings.internal.dev_1  ...\nStarting src_web.bff.admin_1                ...\nStarting src_web.bff.console_1              ...\nAttaching to src_users.internal.dev_1, src_transactions.internal.dev_1, src_debugdata.internal.dev_1, src_authentication\n.internal.dev_1, src_userinvitations.internal.dev_1, src_authorization.internal.dev_1, src_devredis_1, src_devrabbit_1,\nsrc_web.bff.agent_1, src_globalsettings.internal.dev_1, src_web.bff.admin_1, src_web.bff.console_1\nsrc_users.internal.dev_1 exited with code 244\nsrc_transactions.internal.dev_1 exited with code 244\nsrc_debugdata.internal.dev_1 exited with code 244\nsrc_authentication.internal.dev_1 exited with code 244\nsrc_userinvitations.internal.dev_1 exited with code 244\nsrc_authorization.internal.dev_1 exited with code 244\nsrc_devredis_1 exited with code 244\nsrc_devrabbit_1 exited with code 244\nsrc_web.bff.agent_1 exited with code 244\nsrc_globalsettings.internal.dev_1 exited with code 244\nsrc_web.bff.admin_1 exited with code 244\nsrc_web.bff.console_1 exited with code 244\n\n`\n```\nif I try to access the servers, I simply get a \"unable to connect to server\".\nI guess that if I discover what is this code 244 it may give me the answer.\nDoes anyone have any suggestions?\nThanks in advance",
      "solution": "After many tests I think I finally figured the problem.\nOn our `docker-compose` file we have the `web.bff.console`, `web.bff.admin` and `web.bff.agent` as our \"main\" projects that have the others as dependencies, so I did a few things:\n\nOn my Rider Docker configuration, I added back the \"Attach to: none\" (this added a `-d ...` after the `docker compose up` and removed the 244 error)\nInstead of running a docker compose with all the services, I now run with the main ones, that spawn the dependencies and everything still works as expected\nPurged all the data from Docker, cleaned the temp folder for Rider and recompiled everything again.\n\nAlthough it was technically a configuration problem, the error messages could've been clearer",
      "question_score": 9,
      "answer_score": 18,
      "created_at": "2021-05-05T19:26:25",
      "url": "https://stackoverflow.com/questions/67406041/debug-docker-with-rider-exited-with-code-244"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 72092982,
      "title": "Pass variables from .env file to dockerfile through docker-compose",
      "problem": "```\n`project\n\u2514\u2500\u2500\u2500app\n\u2502   \u2502   ...\n\u2502   \u2502   Dockerfile\n\u2502   \u2502\n\u2514\u2500\u2500\u2500prod.env\n\u2514\u2500\u2500\u2500docker-compose.yml\n`\n```\nMy docker-compose looks like this:\n```\n`services:\n   app:\n      build:\n         context: .\\app\n         args:\n            ARG1: val1\n            ARG2: val2\n      env_file:\n         - prod.env\n`\n```\nBut I've tried this too:\n```\n`services:\n   app:\n      build:\n         context: .\\app\n         args:\n            ARG1: ${ARG1}\n            ARG2: ${ARG2}\n      env_file:\n         - prod.env\n`\n```\nMy prod.env file looks like this:\n```\n`ARG1 = 'val1'\nARG2 = 'val2'\n`\n```\nBut I've tried this too:\n```\n`ARG1=val1\nARG2=val2\n`\n```\nI would like for either the values of args or the values from the prod.env file to be passed to the dockerfile.\nThis is what I've tried to get this:\n```\n`ARG ARG1\nARG ARG2\n\nRUN echo ${ARG1}\nRUN echo ${ARG2}\n`\n```\n```\n`ENV ARG1 ${ARG1}\nENV ARG2 ${ARG2}\n\nRUN echo ${ARG1}\nRUN echo ${ARG2}\n`\n```\n```\n`ENV ARG1 \"new val2\"\nENV ARG2 \"new val2\"\n\nRUN echo ${ARG1}\nRUN echo ${ARG2}\n`\n```\nIt always end with blank values.\nAny help would be greatly appreciated. I feel like no answers from other posts have worked when I tried them.\nTo build I use `docker-compose --env-file prod.env build`\nThanks\nUpdate\nSergio Santiago asked if I could run `docker-compose config` and show the results.\nHere are the final files I used for this test.\ndocker-compose:\n```\n`services:\n   app:\n      build:\n         context: .\\app\n         args:\n            ARG1: val1\n            ARG2: val2\n      env_file:\n         - prod.env\n`\n```\nprod.env:\n```\n`ARG3 = 'val3'\nARG4 = 'val4'\n`\n```\nAnd here is the output of `docker-compose --env-file prod.env config`\n```\n`networks:\n  demo-net: {}\nservices:\n  app:\n    build:\n      args:\n        ARG1: val1\n        ARG2: val2\n      context: C:\\project\\app\n    environment:\n      ENV: prod.env\n      ARG3: val3\n      ARG4: val4\n`\n```\nI would like to add that clearly from here getting the variable from the .env file to the docker-compose file is not the issue. I also have a flask app running on the container and through os.environ it is able to use the variables in the .env file. I just can't figure out how to give the same access to the Dockerfile.\nUpdate 2\nMore specific information in relation to ErikMD's answer\nprod.env\n```\n`DOMAIN = 'actualdomain.com'\nENV = 'prod.env'\nENV_NUM = 1\nARG1 = 'value1'\n`\n```\ndev.env\n```\n`DOMAIN = 'localhost'\nENV = 'dev.env'\nENV_NUM = 0\nARG1 = 'value1'\n`\n```\nNotice that the value for ARG1 is the same but the other values are different.\ndocker-compose.yml\n```\n`version: \"3.7\"\nservices:\n  home:\n    image: home-${ENV_NUM}\n    build: \n      context: .\\home\n      args:\n        ARG1: \"${ARG1}\"\n    networks:\n      - demo-net\n    env_file:\n      - ${ENV}\n    labels:\n      - traefik.enable=true\n      - traefik.http.routers.home.rule=Host(`${DOMAIN}`)\n      - traefik.http.routers.home.entrypoints=web\n    volumes:\n      - g:\\:c:\\sharedrive\n...\n...\n  reverse-proxy:\n    restart: always\n    image: traefik:v2.6.1-windowsservercore-1809\n    command:\n      - --api.insecure=true\n      - --providers.docker=true\n      - --entrypoints.web.address=:80\n      - --providers.docker.endpoint=npipe:////./pipe/docker_engine\n    ports:\n      - 80:80\n      - 443:443\n      - 8080:8080\n    networks:\n     - demo-net\n    volumes:\n      - source: \\\\.\\pipe\\docker_engine\\\n        target: \\\\.\\pipe\\docker_engine\\\n        type: npipe\nnetworks:\n  demo-net:\n`\n```\nThe dots represent other apps that would be formatted the same as home.\ndockerfile\n```\n`FROM python:3.10.3\n\nARG ARG1=\"default\"\n\nENV ARG1=\"${ARG1}\"\n\nWORKDIR /app\n\nENV PYTHONDONTWRITEBYTECODE 1\nENV PYTHONUNBUFFERED 1\n\nRUN echo \"This is argument 1 -> ${ARG1}\"\n`\n```\noutput of `docker-compose --env-file prod.env config`\n```\n`networks:\n  demo-net: {}\nservices:\n  home:\n    build:\n      args:\n        ARG1: value1\n      context: C:\\MIS-Web-App\\home\n    environment:\n      DOMAIN: actualdomain.com\n      ENV: prod.env\n      ENV_NUM: '1'\n      ARG1: value1\n    image: home-1\n    labels:\n      traefik.enable: \"true\"\n      traefik.http.routers.home.entrypoints: web\n      traefik.http.routers.home.rule: Host(`mis.canaras.net`)\n    networks:\n      demo-net: null\n    volumes:\n    - g:\\:c:\\sharedrive:rw\n...\n...\n`\n```\nThen I run either `docker-compose --env-file prod.env build` or `docker-compose --env-file dev.env build`\noutput of build\n```\n`Step 9/23 : RUN echo \"This is argument 1 -> ${ARG1}\"\n ---> Running in 5142850de365\nThis\nis\nargument\n1\n->\nRemoving intermediate container 5142850de365\n`\n```\nNow I call pass the env_file in the command as well as in the actual file because there are variables in there that my docker-compose file needs and variables that my flask app needs. And there is definitely overlap.\nGetting the values from the prod.env or dev.env files to docker-compose is not the issue. Neither is getting it to my flask app. The issue is getting those values to the dockerfile.",
      "solution": "My solution was annoying which is why it took me so long to figure it out.\nMy dockerfile was using powershell on a windows server, so I had to do this for every argument:\n```\n`ARG ARG1\nRUN echo $env:ARG1\n`\n```\nThis seems pretty niche especially since using windows containers on a windows server is not my first choice, so check out @ErikMD 's answer if your having issues with env files and whatnot.",
      "question_score": 9,
      "answer_score": 4,
      "created_at": "2022-05-03T00:19:53",
      "url": "https://stackoverflow.com/questions/72092982/pass-variables-from-env-file-to-dockerfile-through-docker-compose"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 78360112,
      "title": "Error while running &quot;docker compose pull&quot; in Apache Superset",
      "problem": "Following the steps provided in this guide while installing Apache Superset on Windows,\nI've had errors while setting up a docker container using Docker Compose (after cloning the project in apache/superset).\n`docker compose pull`\nand\n`docker compose up -d`\ngives me:\n`time=\"2024-04-20T21:18:08-03:00\" level=warning msg=\"The \\\"CYPRESS_CONFIG\\\" variable is not set. Defaulting to a blank string.\"\ntime=\"2024-04-20T21:18:08-03:00\" level=warning msg=\"The \\\"SCARF_ANALYTICS\\\" variable is not set. Defaulting to a blank string.\"\ntime=\"2024-04-20T21:18:08-03:00\" level=warning msg=\"The \\\"CYPRESS_CONFIG\\\" variable is not set. Defaulting to a blank string.\"\nvalidating C:\\superset\\superset\\docker-compose.yml: services.superset.env_file.0 must be a string\n`\nI don't know how to fix this error, since it says it can't find the environment variables and it can't read properly the env file. I have searched in the whole internet, github discussions and stackoverflow but had zero success.",
      "solution": "Update your docker version to latest(on windows update docker desktop), because you are using compose plugin. Compose also will be updated to ~ `v2.26.X`. This is a latest feature of docker compose `2.24`.\nAdded to superset here: https://github.com/apache/superset/pull/28039",
      "question_score": 9,
      "answer_score": 8,
      "created_at": "2024-04-21T02:43:07",
      "url": "https://stackoverflow.com/questions/78360112/error-while-running-docker-compose-pull-in-apache-superset"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 76758501,
      "title": "Set the &#39;start-interval&#39; of a healthcheck in docker-compose.yml",
      "problem": "I am deploying some services using Docker Compose. I want to check that my containers are healthy using `healthcheck` (see Docker Compose documentation here).\nLet's consider the following code. It works fine, except when I uncomment the last line:\n`# docker-compose.yml\n\nversion: \"3.8\"\n\nservices:\n  postgres:    # this is just an example, please don't focus on that particular image\n    image: postgres:15-alpine\n    healthcheck:\n      test: pg_isready -d my_db -U my_user    # again, the command here is not my point, nor the env, feel free to change it\n      interval: 60s\n      start_period: 30s\n      # start_interval: 5s # \u2192 this line is commented because it raises an error! Why?\n`\nThis page of the Dockerfile documentation describes the difference between various settings in the `HEALTHCHECK` instruction. It details the difference between the option `interval` and the option `start-interval`.\nUncommenting the last line would raise the following exception, but why? Admittedly, I haven't seen `start_interval` mentioned in the Docker Compose doc, it just felt intuitive...\n```\n`validating /my_path/docker-compose.yml: services.postgres.healthcheck Additional property start_interval is not allowed\n`\n```\nThe `start_period` option has been added in file format 3.4 (release note). Am I right assuming that `start_interval` hasn't been introduced and cannot be parsed by Docker Compose yet? Am I the only one needing this feature?\nAny alternative solution? My `docker-compose.yml` gathers many services that I want to `healthcheck`. I'd like to avoid having a `Dockerfile` for each of them: wherever possible, I lean on environment variables to custom default images and I'm simply putting the `healthcheck` instructions in my `docker-compose.yml`.",
      "solution": "Answering my own question for future readers.\nThis is a new feature in Docker Compose. It has already been implemented but a few days ago. I was definitely not the only one frustrated by this lack!\nHere is the corresponding pull request which has been merged and released (release note of `compose-go` and release note of `docker compose`).\nThus, updating my version of `Docker Compose` to `v2.20.2` fixed the issue.",
      "question_score": 9,
      "answer_score": 12,
      "created_at": "2023-07-25T01:01:58",
      "url": "https://stackoverflow.com/questions/76758501/set-the-start-interval-of-a-healthcheck-in-docker-compose-yml"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 67331112,
      "title": "What does the Docker sleep command do?",
      "problem": "I can't find a clear explanation as to what the Docker sleep command does so can someone explain please? For example:\n```\n`docker container run -d alpine sleep 1d\n`\n```\nI have had a look online but I can't find a simple explanation.",
      "solution": "```\n`docker container run -d --name mycontainer myimage:mytag sleep infinity\n`\n```\n\nThe last part after the image name  (i.e. `sleep infinity`) is not a docker command but a command sent to the container to override its default command (set in the `Dockerfile`).\nAn extract from the documentation (you can get it typing `man sleep` in your terminal, it may vary depending on the implementation)\n\nPause for NUMBER seconds.  SUFFIX may be 's' for seconds (the default),\n'm' for minutes, 'h' for hours or 'd' for days\n\nTo my surprise, the parameter `infinity` is not documented in my implementation but is still accepted. It's quite easy to understand: it pauses indefinitely (i.e. until the command is stopped/killed). In your own example above, it will pause for one day.\nWhat is the usual reason to use `sleep` as a command to run a docker container?\nA docker container will live until the command it runs finishes. This command is normally set in the `Dockerfile` used to build the image (in a `CMD` stanza) and can be overridden on the command line (as in the above examples).\nA number of base images (like base OS for debian, ubuntu, centos....) will run a shell as the default command (`bash` or `sh` in general). If you try to spawn a container from that image using its default command, it will live until the shell exits.\nWhen running such an image interactively (i.e. with `docker container run -it .....`), it will run until you end your shell session. But if you want to launch it in the background (i.e. with `docker container run -d ...`) it will exit immediately leaving you with a stopped container.\nIn this case, you can \"fake\" a long running service by overriding the default command with a long running command that basically does nothing but wait for the container to stop. Two widely used commands for this are `sleep infinity` (or whatever period suiting your needs) and `tail -f /dev/null`\nAfter you launched a container like this you can use it to test whatever you need. The most common way is to run an interactive shell against it:\n```\n`# command will depend on shells available in your image\ndocker exec -it mycontainer [bash|sh|zsh|ash|...]\n`\n```\nOnce you are done with your experimentation/test, you can stop and recycle your container\n```\n`docker container stop mycontainer\ndocker container rm mycontainer\n`\n```",
      "question_score": 9,
      "answer_score": 14,
      "created_at": "2021-04-30T11:05:20",
      "url": "https://stackoverflow.com/questions/67331112/what-does-the-docker-sleep-command-do"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 70516739,
      "title": "How to configure port mapping for replicated containers in Docker Compose?",
      "problem": "The goal is to run two containers of `publisher-app`. One container should be mapped to port 8080 on the host machine, and the other on 8081. Here is the `docker-compose`:\n`  publisher_app:\n    ports:\n      - \"8080-8081:8080\"\n    environment:\n      server.port: 8080\n    deploy:\n      mode: replicated\n      replicas: 2\n`\nTwo containers are created, but as I understand, both ports are assigned to the first one, and the second one produces this error: `Ports are not available: listen tcp 0.0.0.0:8081: bind: address already in use`.\nHere is the output of `docker ps -a`:\n```\n`6c7067b4ebee   spring-boot-rest-kafka_publisher_app   \"java -jar /app.jar\"     33 seconds ago   Up 28 seconds              0.0.0.0:8080->8080/tcp, 0.0.0.0:8081->8080/tcp   spring-boot-rest-kafka_publisher_app_2\n70828ba8f370   spring-boot-rest-kafka_publisher_app   \"java -jar /app.jar\"     33 seconds ago   Created                                                                     spring-boot-rest-kafka_publisher_app_1\n`\n```\nDocker engine version: 20.10.11\nDocker compose version: 2.2.1\nHow to handle this case? Your help will be very appreciated.\nHere is the source code: https://github.com/aleksei17/springboot-rest-kafka-mysql/blob/master/docker-compose.yml",
      "solution": "tried locally on Windows 10 and failed similarly, both with v2 and with v2 disabled.\nIt seems like a compose issue\nwhen tried on arch: `amd64` fedora based linux distro with package manager installed docker and manually installing docker-compose `1.29.2` (using the official guide for linux) worked:\ncompose file:\n```\n`version : \"3\"\nservices:\n  web:\n    image: \"nginx:latest\"\n    ports:\n      - \"8000-8020:80\"\n`\n```\ndocker command:\n```\n`docker-compose up --scale web=5\n`\n```\n```\n`CONTAINER ID   IMAGE                                     COMMAND                  CREATED          STATUS                    PORTS                                   NAMES\nb304d397b2cd   nginx:latest                              \"/docker-entrypoint.\u2026\"   14 seconds ago   Up 7 seconds              0.0.0.0:8004->80/tcp, :::8004->80/tcp   testdir_web_4\na8c6f177a6e6   nginx:latest                              \"/docker-entrypoint.\u2026\"   14 seconds ago   Up 7 seconds              0.0.0.0:8003->80/tcp, :::8003->80/tcp   testdir_web_3\nb1abe53e7d7d   nginx:latest                              \"/docker-entrypoint.\u2026\"   14 seconds ago   Up 8 seconds              0.0.0.0:8002->80/tcp, :::8002->80/tcp   testdir_web_2\nead91e9df671   nginx:latest                              \"/docker-entrypoint.\u2026\"   14 seconds ago   Up 9 seconds              0.0.0.0:8001->80/tcp, :::8001->80/tcp   testdir_web_5\n65ffd6a87715   nginx:latest                              \"/docker-entrypoint.\u2026\"   24 seconds ago   Up 21 seconds             0.0.0.0:8000->80/tcp, :::8000->80/tcp   testdir_web_1\n`\n```",
      "question_score": 9,
      "answer_score": 11,
      "created_at": "2021-12-29T09:10:34",
      "url": "https://stackoverflow.com/questions/70516739/how-to-configure-port-mapping-for-replicated-containers-in-docker-compose"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 68701716,
      "title": "Docker-compose Postgres connection refused",
      "problem": "I'm running Postgres DB with pg-admin and GO on the docker-compose.\nProblem: I can connect from `pg-admin` to `Postgres`. But cannot establish a connection from `Go`.\nI tried different combinations of authentication string but it does not work. String format same as here https://github.com/karlkeefer/pngr - but different container name - `database`\n(ERROR) Connection URl:\n```\n`backend_1         | 2021/08/08 14:24:40 DB connection: database://main:fugZwypczB94m0LP7CcH@postgres:5432/temp_db?sslmode=disable\nbackend_1         | 2021/08/08 14:24:40 Unalble to open DB connection: dial tcp 127.0.0.1:5432: connect: connection refused\n`\n```\n(URI generation same as here https://github.com/karlkeefer/pngr)\nDocker:\n```\n`version: '3.8'\nservices:\n  backend:\n    restart: always\n    build: \n      context: backend\n      target: dev\n    volumes:\n      - ./backend:/root\n    ports:\n      - \"5000:5000\"\n    env_file: .env\n    depends_on: \n      - database\n  database:\n    build: database\n    restart: always\n    environment:\n      POSTGRES_DB: ${POSTGRES_DB}\n      POSTGRES_USER: ${POSTGRES_USER}\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n      PGDATA: /var/lib/postgresql/data\n    volumes:\n      - ./database/data:/var/lib/postgresql/data\n      - ./logs/databse:/var/log/postgresql\n      - ./database/migrations:/docker-entrypoint-initdb.d/migrations\n    ports:\n      - \"5432:5432\"\n  database-admin:\n    image: dpage/pgadmin4:5.5\n    restart: always\n    environment:\n      PGADMIN_DEFAULT_EMAIL: ${PG_ADMIN_EMAIL}\n      PGADMIN_DEFAULT_PASSWORD: ${PG_ADMIN_PASSWORD}\n      PGADMIN_LISTEN_PORT: 80\n    ports:\n      - \"8080:80\"\n    volumes:\n      - ./database/admin:/var/lib/pgadmin\n    links:\n      - \"database:pgsql-server\"\n    depends_on: \n      - database\nvolumes:\n  database:\n  database-admin:\n`\n```\nEnvironment:\n```\n`POSTGRES_HOST=postgres\nPOSTGRES_PORT=5432\nPOSTGRES_DB=temp_db\nPOSTGRES_USER=main\nPOSTGRES_PASSWORD=fugZwypczB94m0LP7CcH\nPG_ADMIN_EMAIL=admin@temp.com\nPG_ADMIN_PASSWORD=ayzi2ta8f1TnX3vKQSN1\nPG_ADMIN_PORT=80\n`\n```\nGO Code:\n```\n`db, err = sqlx.Open(\"postgres\", str)\n`\n```\nstr\n```\n`func buildConnectionString() string {\n    user := os.Getenv(\"POSTGRES_USER\")\n    pass := os.Getenv(\"POSTGRES_PASSWORD\")\n    if user == \"\" || pass == \"\" {\n        log.Fatalln(\"You must include POSTGRES_USER and POSTGRES_PASSWORD environment variables\")\n    }\n    host := os.Getenv(\"POSTGRES_HOST\")\n    port := os.Getenv(\"POSTGRES_PORT\")\n    dbname := os.Getenv(\"POSTGRES_DB\")\n    if host == \"\" || port == \"\" || dbname == \"\" {\n        log.Fatalln(\"You must include POSTGRES_HOST, POSTGRES_PORT, and POSTGRES_DB environment variables\")\n    }\n\n    str := fmt.Sprintf(\"database://%s:%s@%s:%s/%s?sslmode=disable\", user, pass, host, port, dbname)\n\n    log.Println(\"DB connection: \" + str)\n\n    return str\n}\n`\n```\nThanks in advance!",
      "solution": "You reference the database hostname as `postgres` (`POSTGRES_HOST=postgres`) which is fine, but the container/service name is `database`.\nEither change the name in your `compose.yaml` from `database` to `postgres` or add an explicit `hostname` field:\n```\n`database:\n  build: database\n  restart: always\n  hostname: postgres   # \nYou may also want to add a dedicated network for multiple container services to talk to one another (or prevent others from). To do this, add this to each service your want to use a specific network e.g.\n```\n`database:\n  # ...\n\n  networks:\n    - mynet\n\nbackend:\n  # ...\n\n  networks:\n    - mynet\n`\n```\nand define the network at the end of your `compose.yaml`\n```\n`networks:\n  mynet:\n    name: my-shared-db-network\n`\n```",
      "question_score": 9,
      "answer_score": 11,
      "created_at": "2021-08-08T16:31:25",
      "url": "https://stackoverflow.com/questions/68701716/docker-compose-postgres-connection-refused"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 66676204,
      "title": "Prometheus node_exporter in docker: Host networking vs hostnames",
      "problem": "To monitor a small home server, I run prometheus and node_exporter (and grafana, and several more things) on docker (similar to https://github.com/stefanprodan/dockprom). I run prometheus on a bridged docker network. For node_exporter, I have two options, which affect the `node_network_transmit_bytes_total` metric.\n\nUsing the same bridged docker network as prometheus\n\nPro: nodeexporter can be addressed directly by name, thanks to docker's internal DNS\nCon: The `node_network_transmit_bytes_total` metric only has docker's virtual internal NIC, not the physical NIC of the box being monitored. This is depsite bind-mounting `/proc` from the host into `/host/proc` in the container (specifically my physical interface is `eno0`, visible in `/proc/net/dev` on the host):\n```\n`$ docker exec -it nodeexporter2 cat /host/proc/net/dev | awk '{print $1}'\nInter-|\nface\neth0:\nlo:\n`\n```\n\nUsing host-mode networking for nodeexporter\n\nPro: All NICs, including the physical host NIC, are visible\nCon: There does not appear to be a clean way for prometheus to address nodeexporter:\n\n`localhost` means prometheus itself\nHostname of host seems inaccessible? Running `docker exec -it prometheus wget -O - http://actual-hostname:9100/metrics` works (and uses my host's LAN IP, 192.168.x.x), but configuring `actual-hostname:9100` as a prometheus target gives an error (`Get \"http://actual-hostname:9100/metrics\": dial tcp 127.0.1.1:9100: connect: connection refused`). I'm not sure why they're resolving differently.\nWhat I ended up doing is emulating the `host.docker.internal` feature available for docker-on-windows and docker-on-mac, by adding this to my `docker-compose.yml`:\n`extra_hosts:\n- \"host.docker.internal:172.18.0.1\"\n`\nThat's very brittle, however: That 172.18 was just recently 172.19; I believe it changed on reboot or docker version upgrade. I'd love to do be able to set the `extra_hosts` to a result of running some script on the host to determine the correct network name, but that wouldn't automatically be re-run on boot.\n\nAny advice?",
      "solution": "Late edit: Thanks to thomas, turns out there's a magic host `host-gateway` that does this, so `extra_hosts: [\"host.docker.internal:host-gateway\"]` should do the trick.  undocumented, but apparently it's implemented here. and already live in docker 20.10.6 (and likely earlier).\n\nI ended up solving this by manually configuring the network:\n`networks:\n  monitor-net:\n    driver: bridge\n    ipam:\n      driver: default\n      config:\n        - subnet: 172.23.0.0/16\n          ip_range: 172.23.5.0/24\n          gateway: 172.23.5.254\n\n# ...\n\nservices:\n  nodeexporter:\n    network_mode: host\n    # ...\n  prometheus:\n    networks:\n      - monitor-net\n    extra_hosts:\n      - \"host.docker.internal: 172.23.5.254\"\n`\nThen prometheus has the target of `host.docker.internal` for node_exporter, and the address should be stable.",
      "question_score": 9,
      "answer_score": 9,
      "created_at": "2021-03-17T16:29:39",
      "url": "https://stackoverflow.com/questions/66676204/prometheus-node-exporter-in-docker-host-networking-vs-hostnames"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 75558256,
      "title": "Django &quot;Detected change in ..., reloading&quot; Error in Docker",
      "problem": "I'm having a problem which I don't understand, and therefore can't resolve.\nI have a Dockerised Django project, which I created using Cookiecutter Django months ago. Today, my development environment has started displaying the following error on every request:\n\nI am not currently having this issue in production. I tried rolling back to commits that worked properly before (1 week old commits, for example), and I'm still getting this error.\nThe reloading is causing connections to the database to close and therefore my project isn't working properly at all.\nDoes anyone know what causes this, and how I might fix it? It feels like an issue with my Docker setup, but that hasn't changed in months, so I don't understand why that would change now.\nMany Thanks for any help anyone can offer!",
      "solution": "Solution:\nremove Werkzeug[watchdog] from your requirements and replace it with just Werkzeug, then rebuild your docker container.\nExplanation:\nThere's an issue with the watchdog Werkzeug backend which is included in new cookiecutter-django projects by default. Relevant thread here: https://github.com/cookiecutter/cookiecutter-django/issues/4179.",
      "question_score": 9,
      "answer_score": 8,
      "created_at": "2023-02-24T16:16:48",
      "url": "https://stackoverflow.com/questions/75558256/django-detected-change-in-reloading-error-in-docker"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 74927234,
      "title": "FATAL: data directory &quot;/var/lib/postgresql/data/pgdata&quot; has invalid permissions Docker-compose",
      "problem": "I faced with problem when create postgres container in the Docker on Windows 10 (on the MacOS is OK)\nI get error: FATAL:  data directory \"/var/lib/postgresql/data/pgdata\" has invalid permissions\nfull logs\n```\n`2022-12-27 11:40:08 This user must also own the server process.\n2022-12-27 11:40:08 \n2022-12-27 11:40:08 The database cluster will be initialized with locale \"en_US.utf8\".\n2022-12-27 11:40:08 The default database encoding has accordingly been set to \"UTF8\".\n2022-12-27 11:40:08 The default text search configuration will be set to \"english\".\n2022-12-27 11:40:08 \n2022-12-27 11:40:08 Data page checksums are disabled.\n2022-12-27 11:40:08 \n2022-12-27 11:40:08 fixing permissions on existing directory /var/lib/postgresql/data/pgdata ... ok\n2022-12-27 11:40:08 creating subdirectories ... ok\n2022-12-27 11:40:08 selecting dynamic shared memory implementation ... posix\n2022-12-27 11:40:08 selecting default max_connections ... 20\n2022-12-27 11:40:08 selecting default shared_buffers ... 400kB\n2022-12-27 11:40:08 selecting default time zone ... Etc/UTC\n2022-12-27 11:40:08 creating configuration files ... ok\n2022-12-27 11:40:08 2022-12-27 08:40:08.524 UTC [69] FATAL:  data directory \"/var/lib/postgresql/data/pgdata\" has invalid permissions\n2022-12-27 11:40:08 2022-12-27 08:40:08.524 UTC [69] DETAIL:  Permissions should be u=rwx (0700) or u=rwx,g=rx (0750).\n2022-12-27 11:40:08 child process exited with exit code 1\n2022-12-27 11:40:08 initdb: removing contents of data directory \"/var/lib/postgresql/data/pgdata\"\n`\n```\nI use such docker-compose.yml\n```\n`  postgres_container:\n    container_name: postgres_container\n    image: postgres:13.3\n    environment:\n      - POSTGRES_DB=$POSTGRES_NAME_DB\n      - POSTGRES_USER=$POSTGRES_USER\n      - POSTGRES_PASSWORD=$POSTGRES_PASSWORD\n      - PGDATA=$POSTGRES_PGDATA\n    ports:\n      - $POSTGRES_LOCAL_PORT:$POSTGRES_DOCKER_PORT\n    user: postgres\n    volumes:\n      - ./postgres/scripts/init.sql:/docker-entrypoint-initdb.d/init.sql\n      - .:/var/lib/postgresql/data\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U admin -d pgdb\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n      start_period: 10s\n    restart: unless-stopped\n    deploy:\n      resources:\n        limits:\n          cpus: '1'\n          memory: 4G\n`\n```\nWhere $POSTGRES_PGDATA=/var/lib/postgresql/data/pgdata",
      "solution": "I add pgdata volumes\n`- pgdata:/var/lib/postgresql/data`\nand added\n```\n`volumes:\n  pgdata:\n`\n```\nit works for me",
      "question_score": 9,
      "answer_score": 10,
      "created_at": "2022-12-27T09:42:59",
      "url": "https://stackoverflow.com/questions/74927234/fatal-data-directory-var-lib-postgresql-data-pgdata-has-invalid-permissions"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 73409389,
      "title": "Docker Compose: executable file not found in $PATH: unknown",
      "problem": "My project directory structure:\n```\n`myapp/\n    src/\n    Dockerfile\n    docker-compose.yml\n    docker-deploy.sh\n    wait-for-it.sh\n    .env\n`\n```\nWhere `wait-for-it.sh` is a copy of the famous wait-for-it script.\nMy `Dockerfile`:\n```\n`FROM node:16\n\nWORKDIR /usr/src/app\n\nCOPY package*.json ./\nCOPY wait-for-it.sh ./\nCOPY docker-deploy.sh ./\n\nRUN chmod +x docker-deploy.sh\n\nRUN npm install --legacy-peer-deps\n\nCOPY . .\n\nRUN npm run build\n\nENTRYPOINT [\"docker-deploy.sh\"]\n`\n```\nAnd `docker-deploy.sh` is:\n```\n`#!/bin/bash\n\n# make wait-for-it executable\nchmod +x wait-for-it.sh\n\n# call wait-for-it with passed in args and then start node if it succeeds\nbash wait-for-it.sh -h $1 -p $2 -t 300 -s -- node start\n`\n```\nAnd my `docker-compose.yml`:\n```\n`version: '3.7'\n\nservices:\n  my-service:\n    build: .\n  postgres:\n    container_name: postgres\n    image: postgres:14.3\n    environment:\n      POSTGRES_PASSWORD: ${DB_PASSWORD}\n      POSTGRES_USER: ${DB_USER}\n      POSTGRES_DB: my-service-db\n      PG_DATA: /var/lib/postgresql2/data\n    ports:\n      - ${DB_PORT}:${DB_PORT}\n    volumes:\n      - pgdata:/var/lib/postgresql2/data\nvolumes:\n  pgdata:\n`\n```\nAnd where my `.env` looks like:\n```\n`DB_PASSWORD=1234\nDB_USER=root\nDB_PORT=5432\n`\n```\nWhen I run the following command-line from the project root:\n```\n`docker-compose --env-file .env up --build\n`\n```\nI get:\n```\n`Creating myapp_my-service_1 ... error\n\nCreating postgres                    ... \nCreating postgres                    ... done\n\nERROR: for my-service  Cannot start service my-service: OCI runtime create failed: container_linux.go:380: starting container process caused: exec: \"docker-deploy.sh\": executable file not found in $PATH: unknown\n\nERROR: Encountered errors while bringing up the project.\n`\n```\nWhat is going on? Is the error coming from the `wait-for-it.sh` script itself, from a poorly configured `CMD` directive in the `Dockerfile`, or from the actual Node/JS app running as `my-service`?\nUpdate\nLatest errors after applying @ErikMD's suggested changes:\n```\n`Creating postgres ... done\nCreating myapp_my-service_1 ... error\n\nERROR: for myapp_my-service_1  Cannot start service my-service: OCI runtime create failed: container_linux.go:380: starting container process caused: exec: \"./docker-deploy.sh\": permission denied: unknown\n\nERROR: for my-service  Cannot start service my-service: OCI runtime create failed: container_linux.go:380: starting container process caused: exec: \"./docker-deploy.sh\": permission denied: unknown\nERROR: Encountered errors while bringing up the project.\n`\n```\nSo it is spinning up the DB (`postgres`) no problem but is still for some reason getting permissions-related issues with the `docker-deploy.sh` script.",
      "solution": "As pointed out in @derpirscher's comment and mine, one of the issues was the permission of your script(s) and the way they should be called as the `ENTRYPOINT` (not `CMD`).\nConsider this alternative code for your Dockerfile :\n```\n`FROM node:16\n\nWORKDIR /usr/src/app\n\nCOPY package*.json ./\nCOPY wait-for-it.sh ./\nCOPY docker-deploy.sh ./\n\n# Use a single RUN command to avoid creating multiple RUN layers\nRUN chmod +x wait-for-it.sh \\\n  && chmod +x docker-deploy.sh \\\n  && npm install --legacy-peer-deps\n\nCOPY . .\n\nRUN npm run build\n\nENTRYPOINT [\"./docker-deploy.sh\"]\n`\n```\ndocker-deploy.sh script :\n`#!/bin/sh\n\n# call wait-for-it with args and then start node if it succeeds\nexec ./wait-for-it.sh -h \"${DB_HOST}\" -p \"${DB_PORT}\" -t 300 -s -- node start\n`\nSee this other SO question for more context on the need for the `exec` builtin in a Docker shell entrypoint.\nAlso, note that the fact this `exec ...` command line is written inside a shell script (not directly in an `ENTRYPOINT / CMD` exec form) is a key ingredient for using the parameter expansion.\nIn other words: in the revision 2 of your question, the `\"${DB_HOST}:${DB_PORT}\"` argument was understood literally because no shell interpolation occurs in an `ENTRYPOINT / CMD` exec form.\nRegarding the docker-compose.yml :\n`# version: '3.7'\n# In the Docker Compose specification, \"version:\" is now deprecated.\n\nservices:\n  my-service:\n    build: .\n    # Add \"image:\" for readability\n    image: some-optional-fresh-tag-name\n    # Pass environment values to the entrypoint\n    environment:\n      DB_HOST: postgres\n      DB_PORT: ${DB_PORT}\n      # etc.\n    # Add network spec to make it explicit what services can communicate together\n    networks:\n      - postgres-network\n    # Add \"depends_on:\" to improve \"docker-run scheduling\":\n    depends_on:\n      - postgres\n\n  postgres:\n    # container_name: postgres # unneeded\n    image: postgres:14.3\n    environment:\n      POSTGRES_PASSWORD: ${DB_PASSWORD}\n      POSTGRES_USER: ${DB_USER}\n      POSTGRES_DB: my-service-db\n      PG_DATA: /var/lib/postgresql2/data\n    volumes:\n      - pgdata:/var/lib/postgresql2/data\n    networks:\n      - postgres-network\n    # ports:\n    #   - ${DB_PORT}:${DB_PORT}\n    # Rather remove this line in prod, which is a typical weakness, see (\u00a7)\n\nnetworks:\n  postgres-network:\n    driver: bridge\n\nvolumes:\n  pgdata:\n    # let's be more explicit\n    driver: local\n`\nNote that in this Docker setting, the `wait-for-it` host should be `postgres` (the Docker service name of your database), not `0.0.0.0` nor `localhost`. Because the `wait-for-it` script acts as a client that tries to connect to the specified web service in the ambient `docker-compose` network.\nFor a bit more details on the difference between `0.0.0.0` (a server-side, catch-all special IP) and `localhost` in a Docker context, see e.g. this other SO answer of mine.\n(\u00a7): last but not least, the `ports: [ \"${DB_PORT}:${DB_PORT}\" ]` lines should rather be removed because they are not necessary for the Compose services to communicate (the services just need to belong to a common Compose network and use the other Compose services' hostname), while exposing one such port directly on the host increases the attack surface.\nLast but not least:\nTo follow-up this comment of mine, suggesting to run `ls -l docker-deploy.sh; file docker-deploy.sh` in your `myapp/` directory as a debugging step (BTW: feel free to do this later on then comment for the record):\nAssuming there might be an unexpected bug in Docker similar to this one as pointed by @Lety:\nI'd suggest to just replacing (in the Dockerfile) the line\n`RUN chmod +x wait-for-it.sh \\\n  && chmod +x docker-deploy.sh \\\n  && npm install --legacy-peer-deps\n`\nwith\n`RUN npm install --legacy-peer-deps\n`\nand running directly in a terminal on the host machine:\n`cd myapp/\nchmod -v 755 docker-deploy.sh\nchmod -v 755 wait-for-it.sh\n\ndocker-compose --env-file .env up --build\n`\nIf this does not work, here is another useful information you may want to provide: what is your OS, and what is your Docker package name? (e.g. docker-ce or podman\u2026)",
      "question_score": 9,
      "answer_score": 6,
      "created_at": "2022-08-18T23:01:06",
      "url": "https://stackoverflow.com/questions/73409389/docker-compose-executable-file-not-found-in-path-unknown"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 68636725,
      "title": "Docker Error response from daemon: OCI runtime create failed container_linux.go:380: starting container process caused",
      "problem": "I have this error after running this command in PowerShell\n`docker-compose up`\nMy Docker Version\n`Docker version 20.10.7, build f0df350`\nMy Docker Info\n```\n`Client:\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Build with BuildKit (Docker Inc., v0.5.1-docker)\n  compose: Docker Compose (Docker Inc., v2.0.0-beta.6)\n  scan: Docker Scan (Docker Inc., v0.8.0)\n\nServer:\n Containers: 0\n  Running: 0\n  Paused: 0\n  Stopped: 0\n Images: 0\n Server Version: 20.10.7\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: cgroupfs\n Cgroup Version: 1\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: d71fcd7d8303cbf684402823e425e9dd2e99285d\n runc version: b9ee9c6314599f1b4a7f497e1f1f856fe433d3b7\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: default\n Kernel Version: 5.10.16.3-microsoft-standard-WSL2\n Operating System: Docker Desktop\n OSType: linux\n Architecture: x86_64\n CPUs: 12\n Total Memory: 12.43GiB\n Name: docker-desktop\n ID: 65YS:IH5I:4VI6:ZBXX:SB7J:NAR5:OSHP:OQ3S:ZGHX:653Z:KSFS:3CKX\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Registry: https://index.docker.io/v1/\n Labels:\n Experimental: false\n Insecure Registries:\n  127.0.0.0/8\n Live Restore Enabled: false\n\nWARNING: No blkio throttle.read_bps_device support\nWARNING: No blkio throttle.write_bps_device support\nWARNING: No blkio throttle.read_iops_device support\nWARNING: No blkio throttle.write_iops_device support\n`\n```\nError\n```\n`[+] Running 5/6\n - Network project_default          Created                                                                        0.7s\n - Volume \"project_mongo-db\"        Created                                                                        0.0s\n - Container project_mongo_1        Started                                                                       12.4s\n - Container project_node-app_1     Starting                                                                      12.9s\n - Container project_react-app_1    Created                                                                        8.9s\n - Container project_nginx-proxy_1  Created                                                                        0.1s\nError response from daemon: OCI runtime create failed: container_linux.go:380: starting container process caused: process_linux.go:545: container init caused: rootfs_linux.go:76: mounting \"/var/lib/docker/volumes/4b1671442c7499623e352a827a29d54b514fd4f186b937181a90ab497d12995c/_data\" to rootfs at \"/usr/src/server/node_modules\" caused: mkdir /var/lib/docker/overlay2/c11386ffc7cd58452b395472bb289e20df441f5b59e1082d6d055de466b81a4e/merged/usr/src/server/node_modules: read-only file system: unknown\n`\n```\nThis project I get from my friend. My friend didn't have Dockerfile in project folder but have just:\n\nSo I find so many tutorials and I find how to use docker-compose. It's like going well but I have this error. I find this error in google but no one can help me even in the StackOverflow. Thanks For Helping!!! :)\ndocker-compose.yml:\n```\n`version: '3.4'\n\nservices:\n    nginx-proxy:\n        restart: always\n        build:\n            context: ./\n            dockerfile: ./nginx-proxy/Dockerfile\n        ports:\n            - '${NGINX_PROXY_PORT}:${NGINX_PROXY_PORT}'\n        environment:\n            - NGINX_LE_PLACEHOLDER_1=${PORT}\n            - NGINX_LE_PLACEHOLDER_2=${SERVER_PORT}\n            - NGINX_LE_PLACEHOLDER_3=${NGINX_PROXY_PORT}\n            - NGINX_LE_TZ\n        depends_on:\n            - node-app\n            - react-app\n\n    react-app:\n        build:\n            context: ./\n            dockerfile: ./client/Dockerfile\n        volumes: \n            - ./client:/usr/src/client:ro\n            - /usr/src/client/node_modules\n        depends_on: \n            - node-app\n        environment: \n            - NODE_ENV=development\n            - CHOKIDAR_USEPOLLING=true\n        ports:\n            - '${PORT}:${PORT}'\n  \n    node-app:\n        build: \n            context: ./\n            dockerfile: ./server/Dockerfile\n        volumes:\n            - ./server:/usr/src/server:ro\n            - /usr/src/server/node_modules\n        environment:\n            - NODE_ENV=development\n        env_file:\n            - ./.env\n        ports:\n            - '${SERVER_PORT}:${SERVER_PORT}'\n        depends_on:\n            - mongo\n  \n    mongo:\n        image: mongo:latest\n        volumes:\n            - mongo-db:/data/db\n        ports: \n            - \"27017:27017\"\n        environment: \n            - MONGO_INITDB_ROOT_USERNAME=\n            - MONGO_INITDB_ROOT_PASSWORD=\nvolumes:\n    mongo-db:\n`\n```\nReact Project Dockerfile (client):\n```\n`FROM node:16-alpine3.11\n\nRUN apk add --update git\nWORKDIR /usr/src/client\nENV PATH /usr/src/client/node_modules/.bin:$PATH\n\nCOPY ./client/package.json ./\nRUN npm install --legacy-peer-deps --silent\n\nCOPY ./client ./\n\nCMD [ \"npm\", \"start\" ]\n`\n```\nnginx proxy folder Docker:\n```\n`FROM 0x8861/nginx-le:v2.0.0\nCOPY ./nginx-proxy/templates/no-ssl.service.conf.dev /etc/nginx/no-ssl.service.conf\n`\n```\nNode.js App Dockerfile (server):\n```\n`FROM node:lts-buster\nWORKDIR /usr/src/server\nCOPY ./server/package*.json ./\nRUN npm install\nCOPY ./.env ../.env\nCMD [\"npm\", \"run\", \"dev\"]\n`\n```\nMy Dockerfile Name:\n\nThanks again to those who can help! :)\nand if this is duplicate am so sorry but I can't find how to fix it thanks again :)\nAnother thing I tried doing it by a single project folder it can but have another error. and my friend suggest using docker-compose.",
      "solution": "It looks like you're trying to replace the application code in the image with different code using Docker bind mounts.  Docker's general model is that a container runs a self-contained image; you shouldn't need to separately install the application code on the host.\nIn particular, these two `volumes:` blocks cause the error you're seeing, and can safely be removed:\n`services:\n  react-app:\n    build:             # \nMechanically, the first `volumes:` line replaces the image's code with different code from the host, but with a read-only mount.  The second `volumes:` line then further tries to replace the `node_modules` directory with an old copy from an anonymous volume.  This will create the `node_modules` directory if it doesn't exist yet; but the parent directory is a read-only volume mount, resulting in the error you're seeing.",
      "question_score": 9,
      "answer_score": 7,
      "created_at": "2021-08-03T15:13:09",
      "url": "https://stackoverflow.com/questions/68636725/docker-error-response-from-daemon-oci-runtime-create-failed-container-linux-go"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-compose",
      "question_id": 67352142,
      "title": "WSL File permission problems on docker-compose mounted volumes",
      "problem": "I'm using docker-compose in a `WSL` environment. I noticed some of the files created by the running docker container show up as user=root and group=root. How can I change the docker-compose to create new files under my current UID and GID?\nI noticed that in the `WSL` bash shell I can delete files owned by root:root as a regular user without sudo. Conversely the running docker containers can't delete files, even if the file wasn't owned by root.\nThe files are at `/mnt/c/projects-new/...` or in Windows at `c:\\projects-new`.\n/etc/wsl.conf\n```\n`[network]\ngenerateResolvConf = true\n`\n```\nBefore you ask, metadata for Linux perms is defined in `/etc/fstab`:\n```\n`LABEL=cloudimg-rootfs / ext4 defaults 0 0\nC: /mnt/c drvfs defaults,metadata 0 0\n`\n```\nI'm using `Win 10 20H2 (OS Buidl 19042.928) WSL Version 2`\ncat /etc/*release\n```\n`DISTRIB_ID=Ubuntu\nDISTRIB_RELEASE=20.04\nDISTRIB_CODENAME=focal\nDISTRIB_DESCRIPTION=\"Ubuntu 20.04.1 LTS\"\nNAME=\"Ubuntu\"\nVERSION=\"20.04.1 LTS (Focal Fossa)\"\nID=ubuntu\nID_LIKE=debian\nPRETTY_NAME=\"Ubuntu 20.04.1 LTS\"\nVERSION_ID=\"20.04\"\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nVERSION_CODENAME=focal\nUBUNTU_CODENAME=focal\n`\n```\nMy `docker-compose.yml` file is as below:\n```\n`version: \"3.2\"\nservices:\n  ssc-file-generator-db2-test:\n    container_name: \"ssc-file-generator-db2-test\"\n    image: ibmcom/db2:latest\n    hostname: db2server\n    privileged: true\n    env_file: [\"acceptance-run.environment\"]\n    ports:\n      - 50100:50000\n      - 55100:55000\n    networks:\n      - back-tier\n    restart: \"no\"\n    volumes:\n      - setup-sql:/setup-sql\n      - db2-shell-scripts:/var/custom\n      - host-dirs:/host-dirs\n      - database:/database  \nnetworks:\n  back-tier: \n    external: true\nvolumes:\n  setup-sql:\n    driver: local\n    driver_opts:\n      o: bind\n      type: none\n      device: ./setup-sql  \n  db2-shell-scripts:\n    driver: local\n    driver_opts:\n      o: bind\n      type: none\n      device: ./db2-shell-scripts\n  host-dirs:\n    driver: local\n    driver_opts:\n      o: bind\n      type: none\n      device: ./host-dirs\n  flyway_wait_bin:\n    driver: local\n    driver_opts:\n      o: bind\n      type: none\n      device: ./flyway/wait_bin\n  flyway_conf:\n    driver: local\n    driver_opts:\n      o: bind\n      type: none\n      device: ./flyway/conf\n  flyway_drivers:\n    driver: local\n    driver_opts:\n      o: bind\n      type: none\n      device: ./flyway/drivers\n  flyway_sql:\n    driver: local\n    driver_opts:\n      o: bind\n      type: none\n      device: ./flyway/sql\n  flyway_jars:\n    driver: local\n    driver_opts:\n      o: bind\n      type: none\n      device: ./flyway/jars\n  database:\n    driver: local\n    driver_opts:\n      o: bind\n      type: none\n      device: ./database      \n`\n```",
      "solution": "Not sure that this will work but I don't have enough reputation to comment:\n\nThis is my my `automount` section in `wsl.conf`\n\n```\n`[automount]\nenabled = true\nmountfstab = true\nroot = /mnt/\noptions = metadata,uid=1000,gid=1000,umask=0022,fmask=11,case=off\n`\n```\n\nSet your `uid` and `gid` in options.\nIf this does not work the next step is to add your user to the `docker` group:\n(if you don't have it already you will have to create it with `sudo groupadd docker`)\n\n```\n`sudo usermod -aG docker \n`\n```\n\nAfter this change the owner and group of the docker compose executable to your user and group to `docker`\n\n```\n`sudo chown $(whoami):docker $(which docker-compose) \n`\n```",
      "question_score": 9,
      "answer_score": 6,
      "created_at": "2021-05-02T03:02:58",
      "url": "https://stackoverflow.com/questions/67352142/wsl-file-permission-problems-on-docker-compose-mounted-volumes"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 73101754,
      "title": "Docker change mtu on the fly?",
      "problem": "I have production docker swarm with 9 stacks, most of them have volumes. Currently docker is running with single node.\nI have to add second node and it is the place where problems started. Especially the problem is with portainer - it becomes very laggy, almost unusable. Also when I move some containers to a new node, my project seems to be completely stucked - the problem is in communication between containers on different nodes. Some requests are ok, but most of them seems to be broken.\nAfter some research I found out that problem seems to be with MTU. MTU of eth1 is 1450 and the docker's default is 1500.\nThe question is, is it possible to change docker's MTU to 1450 on the fly?\nI tried:\n\nAdd key to dockerd --mtu=1450 - the docker service didn't start at all\nChange main network's mtu in docker-compose - seems that it didn't updated, I think that the network should be recreated\nAdd mtu option to /etc/docker/daemon.json also seemed to be with no effect\n\nHow to change mtu on working production server? Possible downtime for 10-15 minutes is ok, but I don't want to remove all stacks and recreate them.",
      "solution": "Solved. The long way...\nI had vps named \"m1\" with my docker stacks and finally yesterday I realized that I can not update mtu on working cluster ((\nSo I added \"m2\" (as manager) and \"m3\" (as worker) and created docker swarm cluster on \"m2\"+\"m3\" (not \"m1\").\n\nI modified `/lib/systemd/system/docker.service` on \"m2\" and added `--mtu=1450` here:\n\n```\n`ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --mtu=1450\n`\n```\nI also created new file `/etc/docker/daemon.json` with `{ \"mtu\": 1450 }` (thanks @BMitch)\n\nI removed ingress network on \"m2\" and recreated it with option `\"com.docker.network.driver.mtu\": \"1450\"`\n\nI added mtu to all my overlay networks in project (in docker-compose):\n\n```\n`networks:\n  network1:\n    driver: overlay\n    driver_opts:\n      com.docker.network.driver.mtu: 1450\n`\n```\nUpdate: here is the purpose of \"m2\"+\"m3\" - I was able check that the problem is solved at new cluster, while my production \"m1\" was working. I tried portainer and it was working without any lags I've seen before. Next time I will just remove stacks on laggy vps, update settings (mtu) and redeploy stacks - this would be much more faster!\n\nI removed stacks from \"m1\" and copied volumes to \"m2\" (thanks to How to copy docker volume from one machine to another?)\n\nI deployed services to \"m2\" and updated my domain's DNS to \"m2\" ip\n\nIt works fine with no lags after MTU update!",
      "question_score": 11,
      "answer_score": 10,
      "created_at": "2022-07-24T22:21:41",
      "url": "https://stackoverflow.com/questions/73101754/docker-change-mtu-on-the-fly"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 71065971,
      "title": "Why do you need &quot;traefik.docker.network&quot; for some service?",
      "problem": "I have a docker swarm setup with traefik in it. I added a service (in this case, grafana and prometheus) with this label:\n```\n`  grafana:\n    ...\n    labels:\n      - \"router=inbound\"\n      - \"traefik.http.routers.grafana.entrypoints=http\"\n      - \"traefik.http.routers.grafana.rule=Host(`grafana.localhost`)\"\n      - \"traefik.http.routers.grafana.service=grafana\"\n      - \"traefik.http.services.grafana.loadbalancer.server.port=3000\"\n      - \"traefik.docker.network=inbound\"\n  networks:\n      - inbound\n\n  traefik:\n    ...\n    command:\n      - --providers.docker.constraints=Label(`router`,`inbound`)\n      - --providers.docker\n      - --providers.docker.exposedbydefault=false\n      - --entryPoints.http.address=:80\n    networks:\n      - inbound\n`\n```\nWithout `\"traefik.docker.network=inbound\"` in grafana labels, I can reach the service and access the UI, but it's incredibly unstable and I often got connection error. With that line, everything works smoothly.\nI wonder what exactly that line does. Why is the service having connection issues (but not totally unreachable) without that line, when the container itself is already configured to be inside the same network as traefik router.",
      "solution": "How traefik selects IP address of a service\nYour example declares for the service `grafana` labels:\n```\n`      - \"traefik.http.routers.grafana.service=grafana\"\n      - \"traefik.http.services.grafana.loadbalancer.server.port=3000\"\n`\n```\nThese labels instruct `traefik` to forward HTTP requests to port 3000 of the `grafana` service.\nAs docker allows each service to be connected to more than one network, the service (e.g. grafana) may have multiple network interfaces with different IP addresses.\nYour HTTP service might bind to all network interfaces and others services might bind only to subset of them. The later ones will not serve HTTP service on some interfaces.\nWhen `traefik` does not find the label:\n```\n`      - \"traefik.docker.network=inbound\"\n`\n```\nit asks swarm for an IP address of the `grafana` service and picks whatever comes first regardless of the network. If the address picked is not served by you service, you get timeouts - thus the random connection issues you have experienced.\nOn the other hand - when the label `traefik.docker.network` is present, `traefik` is more picky and takes only the IP address of the service bound to the given network. Assuming `grafana` being always bound to that network interface you get stable responses.\nCure for services such as grafana, virtuoso and others\nMany services work perfectly without that label, e.g. `whoami`, `nginx`, python web applications using `waitress` or `uvicorn` worked to us without any issue.\nThere are other services such as `grafana` or `virtuoso`, which might become unstable or totally unusable in docker swarm with `traefik` if not used with the `traefik.docker.network` label.\nMe and my colleague Milan Holub have spent around 18 hours trying to resolve mysterious issue for `virtuoso` (responding only the first request,  changing it's IP address in the traefik dashbord...) and the `traefik.docker.network` label was the final solution.",
      "question_score": 9,
      "answer_score": 9,
      "created_at": "2022-02-10T14:28:53",
      "url": "https://stackoverflow.com/questions/71065971/why-do-you-need-traefik-docker-network-for-some-service"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 69410569,
      "title": "Docker Error response from daemon: Error processing tar file(exit status 1): no space left on device",
      "problem": "I need to create a docker's image from a virtual box image. The file is 16GB. When I run the command line `# tar -c . | docker import - ` appear this error below.\nP.S. : The container is empty.\nError:\n\nError response from daemon: Error processing tar file(exit status 1):\nwrite\n/var/lib/docker/overlay2/f8bb15057c3b3578c13e1122be1d00ca72b3301b7a0bf27262edc759e9f207fb/diff/var/lib/docker/overlay2/f8bb15057c3b3578c13e1122be1d00ca72b3301b7a0bf27262edc759e9f207fb/diff/var/lib/cassandra/commitlog/CommitLog-6-1633100360364.log:\nno space left on device\n\nDocker info:\n```\n`Client:\n Context:    default\n Debug Mode: false\n Plugins:\n  app: Docker App (Docker Inc., v0.9.1-beta3)\n  buildx: Build with BuildKit (Docker Inc., v0.6.1-docker)\n  scan: Docker Scan (Docker Inc., v0.8.0)\n\nServer:\n Containers: 0\n  Running: 0\n  Paused: 0\n  Stopped: 0\n Images: 1\n Server Version: 20.10.8\n Storage Driver: overlay2\n  Backing Filesystem: xfs\n  Supports d_type: true\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: cgroupfs\n Cgroup Version: 1\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: e25210fe30a0a703442421b0f60afac609f950a3\n runc version: v1.0.1-0-g4144b63\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: default\n Kernel Version: 3.10.0-1127.el7.x86_64\n Operating System: CentOS Linux 7 (Core)\n OSType: linux\n Architecture: x86_64\n CPUs: 1\n Total Memory: 15.51GiB\n Name: GDBDEV03\n ID: UGGJ:AIIA:7C44:DZXR:PNYH:2WQH:GPKI:TFUL:N2WZ:5RRB:X3JK:VU3B\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Registry: https://index.docker.io/v1/\n Labels:\n Experimental: false\n Insecure Registries:\n  127.0.0.0/8\n Live Restore Enabled: false\n`\n```\nEDIT\n```\n`[root@GDBDEV03 chroot0]# df -h\nFilesystem               Size  Used Avail Use% Mounted on\ndevtmpfs                 7.8G     0  7.8G   0% /dev\ntmpfs                    7.8G     0  7.8G   0% /dev/shm\ntmpfs                    7.8G  8.7M  7.8G   1% /run\ntmpfs                    7.8G     0  7.8G   0% /sys/fs/cgroup\n/dev/mapper/centos-root   14G   13G  1.3G  91% /\n/dev/sda1               1014M  152M  863M  15% /boot\ntmpfs                    1.6G     0  1.6G   0% /run/user/0\n`\n```",
      "solution": "It's not clear exactly what you're attempting to do (where is `.`, and is this a local docker engine or are you sending the image to a remote node). And putting a VM image inside of a container image is a significant code smell (containers aren't a VM).\nThat said, from description, you have a 16GB file you're trying to package on a system with 1.3G available to docker (assuming you are running docker from /var/lib/docker). There's just no way that's going to work under any conditions. Typically you want 2-3x the space available to handle the copies needed to package the layers.\nSo for your root partition, you'd want to expand that from 14GB to 45GB (13 current + 3x16). Steps to expand the partition depend on your underlying storage and filesystem type, and would be more appropriate for superuser.SE.",
      "question_score": 6,
      "answer_score": 1,
      "created_at": "2021-10-01T20:34:07",
      "url": "https://stackoverflow.com/questions/69410569/docker-error-response-from-daemon-error-processing-tar-fileexit-status-1-no"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 71557316,
      "title": "Docker Alpine execute command as another user",
      "problem": "I'm having a problem while using an amazoncorretto-alpine image on which I run a Spring boot application.\nTo startup the container I use a specific bash script which (along with other stuff) attempt to run the executable jar for the Spring boot application.\nMy need is to run the executable jar with a different user , so while the bash script runs with root the \"java -jar springBoot.jar\" must be executed as \"spring\" user.\nIn the docker file a user and a group has been created and given permissions for the springBoot.jar like this:\n```\n`...\nRUN addgroup -S spring && adduser -S -D spring -G spring\nRUN chown spring:spring springBoot.jar\n...\nCMD [\"myBash.sh\"]\n`\n```\nThe user and group are present, the permissions on the file are configured correctly and the container starts by executing myBash.sh.\nIn the bash, that runs with \"root\" privileges, I'm using this command line to execute the jar with another user:\n```\n`su - spring -c \"java -jar springBoot.jar\"\n`\n```\nI did some other test by putting the -c \"command\" before the user but the error is always the same:\n\n\"The Account is not available\"\n\nThis message is printed in the Docker console when starting the container.\nAlpine version in the image:\n\n\"Alpine Linux v3.15\"\n\nNote: if I remove the instruction \"su - spring....\" above and just run the java -jar springBoot.jar in the bash script all works fine but the application is started with root (as expected).\nAnyone have any idea what could be the problem?",
      "solution": "instead of create the user directly in docker file try to create it inside the script like this\n```\n`adduser -D spring -g \"test\" -s /bin/sh -D spring\n`\n```\nthen switch the user\n```\n`su -s /bin/bash spring  <<EOF\njava -jar java_file.jar\nEOF\n`\n```",
      "question_score": 6,
      "answer_score": 4,
      "created_at": "2022-03-21T13:02:23",
      "url": "https://stackoverflow.com/questions/71557316/docker-alpine-execute-command-as-another-user"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 74743428,
      "title": "Secrets are not available in Docker container",
      "problem": "I create secret \"databasePassword\" using the below command:\n`echo 123456 | docker secret create databasePassword -`\nBelow is my yml file to create the MySQL and phpMyAdmin where I'm trying to use this secret in the yml file.\n```\n`version: '3.1'\n\nservices:\n\n  db:\n    image: mysql\n    command: --default-authentication-plugin=mysql_native_password\n    restart: unless-stopped\n    container_name: db-mysql\n    environment:\n      MYSQL_ALLOW_EMPTY_PASSWORD: 'false'\n      MYSQL_ROOT_PASSWORD: /run/secrets/databasePassword\n    ports:\n      - 3306:3306\n    healthcheck:\n      test: [\"CMD\", \"mysqladmin\", \"ping\", \"-h\", \"localhost\"]\n      timeout: 20s\n      retries: 10\n    secrets:\n      - databasePassword\n\n  beyond-phpmyadmin:\n    image: phpmyadmin\n    restart: unless-stopped\n    container_name: beyond-phpmyadmin\n    environment:\n      PMA_HOST: db-mysql\n      PMA_PORT: 3306\n      PMA_ARBITRARY: 1\n    links:\n      - db\n    ports:\n      - 8081:80\n`\n```\nBut the databasePassword is not getting set as 123456. It is set as the string \"/run/secrets/databasePassword\" I tried using `docker stack deploy` also, but it also didn't work.\nI tried setting the secrets at the end of the file like below by some web research, but it also didn't work.\n```\n`version: '3.1'\n\nservices:\n\n  db:\n    image: mysql\n    command: --default-authentication-plugin=mysql_native_password\n    restart: unless-stopped\n    container_name: db-mysql\n    environment:\n      MYSQL_ALLOW_EMPTY_PASSWORD: 'false'\n      MYSQL_ROOT_PASSWORD: /run/secrets/databasePassword\n    ports:\n      - 3306:3306\n    healthcheck:\n      test: [\"CMD\", \"mysqladmin\", \"ping\", \"-h\", \"localhost\"]\n      timeout: 20s\n      retries: 10\n    secrets:\n      - databasePassword\n\n  beyond-phpmyadmin:\n    image: phpmyadmin\n    restart: unless-stopped\n    container_name: beyond-phpmyadmin\n    environment:\n      PMA_HOST: db-mysql\n      PMA_PORT: 3306\n      PMA_ARBITRARY: 1\n    links:\n      - db\n    ports:\n      - 8081:80\nsecrets:\n  databasePassword:\n    external: true\n`\n```",
      "solution": "Docker cannot know that `/run/secrets/databasePassword` is not a literal value of the `MYSQL_ROOT_PASSWORD` variable, but a path to a file that you would like to read the secret from. That's not how secrets work. They are simply available in a `/run/secrets/` file inside the container. To use a secret, your container needs to read it from the file.\nFortunatelly for you, the `mysql` image knows how to do it. Simply use `MYSQL_ROOT_PASSWORD_FILE` instead of `MYSQL_ROOT_PASSWORD`:\n```\n`services:\n  db:\n    image: mysql\n    environment:\n      MYSQL_ALLOW_EMPTY_PASSWORD: 'false'\n      MYSQL_ROOT_PASSWORD_FILE: /run/secrets/databasePassword\n    secrets:\n      - databasePassword\n\n...\n\nsecrets:\n  databasePassword:\n    external: true\n`\n```\nSee \"Docker Secrets\" in the mysql image documentation.",
      "question_score": 3,
      "answer_score": 6,
      "created_at": "2022-12-09T13:51:47",
      "url": "https://stackoverflow.com/questions/74743428/secrets-are-not-available-in-docker-container"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 69158446,
      "title": "Docker swarm. Unable to run exec against a container of a docker stack, because &quot;docker container list&quot; does not find the container",
      "problem": "Section 1\nI am trying to execute \"exec\" on one of the containers in a stack or service. I have followed the answers provided here execute a command within docker swarm service as well as on the official Docker documentation here https://docs.docker.com/engine/swarm/secrets/ but it seems \"docker container list\" and its variants (\"docker ps\") seem not to find any container listed in the \"docker service list\".\nSee the example below from https://docs.docker.com/engine/swarm/secrets/ which is meant to use of \"exec\" on a container of a service.\n1)\n```\n`printf \"This is a secret\" | docker secret create my_secret_data -\n`\n```\n\n```\n`docker service  create --name redis --secret my_secret_data redis:alpine\n`\n```\n\n```\n`docker service ps redis\n`\n```\nyields `ID             NAME      IMAGE          NODE      DESIRED STATE   CURRENT STATE            ERROR     PORTS 75olesuh9n4h   redis.1   redis:alpine   virt-05   Running         Running 16 minutes ago`\n\nThe two commands below return zero records for \"redis\"\n\n```\n`docker ps --filter name=redis -q\ndocker container list\n\n`\n```\nThe two commands below return no records for \"redis\" hence the commands below are bound not to work.\n```\n`docker container exec $(docker ps --filter name=redis -q) ls -l /run/secrets\ndocker container exec $(docker ps --filter name=redis -q) cat /run/secrets/my_secret_data\n`\n```\nWhat could be different in my environment to make it not possible to reproduce the results in the example above (https://docs.docker.com/engine/swarm/secrets/)\nMy environment is as follows\n```\n`$ uname -r\n4.18.0-338.el8.x86_64\n`\n```\n```\n`$ docker version\nClient: Docker Engine - Community\n Version:           20.10.8\n API version:       1.41\n Go version:        go1.16.6\n Git commit:        3967b7d\n Built:             Fri Jul 30 19:53:39 2021\n OS/Arch:           linux/amd64\n Context:           default\n Experimental:      true\n\nServer: Docker Engine - Community\n Engine:\n  Version:          20.10.8\n  API version:      1.41 (minimum version 1.12)\n  Go version:       go1.16.6\n  Git commit:       75249d8\n  Built:            Fri Jul 30 19:52:00 2021\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.4.9\n  GitCommit:        e25210fe30a0a703442421b0f60afac609f950a3\n runc:\n  Version:          1.0.1\n  GitCommit:        v1.0.1-0-g4144b63\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n\n`\n```\nSection 2\nI have just initialized a docker swam on another node and I am able to successfully reproduce the behaviour the example https://docs.docker.com/engine/swarm/secrets/.\nIn this second environment docker ps is able to find (list) the redis container started as service.\nThe command below returns a record.\n```\n`docker ps --filter name=redis -q\n3de724329171\n`\n```\nI was able to successfully run \"exec\" on this container by running the commands below\n```\n`docker container exec $(docker ps --filter name=redis -q) ls -l /run/secrets\ndocker container exec $(docker ps --filter name=redis -q) cat /run/secrets/my_secret_data\n`\n```\nThe docker version of if this second environment is given below\n```\n`$ uname -r\n3.10.0-1160.36.2.el7.x86_64\n`\n```\n```\n`docker version\nClient: Docker Engine - Community\n Version:           20.10.8\n API version:       1.41\n Go version:        go1.16.6\n Git commit:        3967b7d\n Built:             Fri Jul 30 19:55:49 2021\n OS/Arch:           linux/amd64\n Context:           default\n Experimental:      true\n\nServer: Docker Engine - Community\n Engine:\n  Version:          20.10.8\n  API version:      1.41 (minimum version 1.12)\n  Go version:       go1.16.6\n  Git commit:       75249d8\n  Built:            Fri Jul 30 19:54:13 2021\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.4.9\n  GitCommit:        e25210fe30a0a703442421b0f60afac609f950a3\n runc:\n  Version:          1.0.1\n  GitCommit:        v1.0.1-0-g4144b63\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n\n`\n```",
      "solution": "To use docker to access containers in multi node swarm, to avoid having to log into each vm / docker host individually you are going to want to ensure your docker swarm nodes are setup for remote access.\nThe docker daemon has parameters to expose a public socket :2375 for insecure, or :2376 for mtls protected comms. mtls is recommended for anything but the most sandboxed lab environments.\nAlternatively, passwordless ssh access (where you register your local .ssh_id on each of the servers) allows you to use ssh urls to access docker remotely and is the recommended option.\nOnce you have enabled either ssh or tcp access to docker then you can pass the remote node url to docker, set it in an env variable, or use it via a docker context:\n`# using DOCKER_HOST\nexport DOCKER_HOST=tcp://lab-node1:2375\ndocker container ls\n# Passing a url directly\ndocker -H ssh://user@lab-node2 container ls\n# Using a docker context\ndocker context create node1 --docker \"host=ssh://user@node1\"\ndocker context use node1\ndocker container ls\n`\nOnce you have multi docker access, ensure that docker is pointing at a manager node and get the relevant details of the service:\nIn this case, we need to know the node that service tasks are running on, and the container name of the tasks, which is a combination of the name and id.\n```\n`docker service ps your-service --format '{{.Node}} {{.Name}}.{{.ID}}' --no-trunc --filter desired-state=running\n`\n```\nNow that you have the remote node, assuming you have created contexts for each node, then a command like this (where $containername is the '{{.Name}}.{{.ID}}' string.\n```\n`docker -c $node exec $containername cmdline\n`\n```",
      "question_score": 3,
      "answer_score": 5,
      "created_at": "2021-09-13T08:51:25",
      "url": "https://stackoverflow.com/questions/69158446/docker-swarm-unable-to-run-exec-against-a-container-of-a-docker-stack-because"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 77181562,
      "title": "Grafana cannot retrieve logs (and labels) from Loki",
      "problem": "I have Docker Swarm environment with several nodes, one of them called `gateway`. On all nodes installed Docker Plugin `grafana/loki-docker-driver:2.9.1` instead of using Promtail. I used Portainer monitoring template to deploy stack which includes `Grafana 9.5.2`. This stack will be `stack A` for simplicity. Also I have `stack B` with `Loki 2.9.1` and `mingrammer/flog` as log producer to check if everything is fine. So, stack A and stack B use same `net` docker network. Constraints are the same, so both stacks are one the same node `gateway`:\n```\n`- node.role == manager\n- node.labels.monitoring == true\n`\n```\nThis is stack B\n```\n`version: '3.8'\n\nx-logging: &logging\n  logging:\n    driver: loki\n    options:\n      loki-url: \"http://host.docker.internal:3100/loki/api/v1/push\"\n\nservices:\n  loki:\n    image: grafana/loki:2.9.1\n    and this is config for Loki\n```\n`auth_enabled: false\n\nserver:\n  http_listen_port: 3100\n  grpc_listen_port: 9096\n\ncommon:\n  path_prefix: /tmp/loki\n  storage:\n    filesystem:\n      chunks_directory: /tmp/loki/chunks\n      rules_directory: /tmp/loki/rules\n  replication_factor: 1\n  ring:\n    instance_addr: 127.0.0.1\n    kvstore:\n      store: inmemory\n\nfrontend:\n    address: 0.0.0.0\n\nschema_config:\n  configs:\n    - from: 2020-10-24\n      store: boltdb\n      object_store: filesystem\n      schema: v11\n      index:\n        prefix: index_\n        period: 48h\n\nstorage_config:\n  boltdb:\n    directory: /loki/index\n  filesystem:\n    directory: /loki/chunks\n`\n```\nThis is stack A:\n```\n`version: \"3.8\"\n\nservices:\n  grafana:\n    image: portainer/template-swarm-monitoring:grafana-9.5.2\n    ports:\n      - target: 3000\n        published: 3000\n        protocol: tcp\n        mode: ingress\n    deploy:\n      replicas: 1\n      restart_policy:\n        condition: on-failure\n      placement:\n        constraints:\n          - node.role == manager\n          - node.labels.monitoring == true\n    volumes:\n      - type: volume\n        source: grafana-data\n        target: /var/lib/grafana\n    environment:\n      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER}\n      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}\n      - GF_USERS_ALLOW_SIGN_UP=false\n    networks:\n      - net    \n...\n\nvolumes:\n  grafana-data:\n  prometheus-data:\n\nnetworks:\n  net:\n    driver: overlay\n\nconfigs:\n  prometheus_conf:\n    external:\n      name: prometheus_config\n`\n```\nIn the Grafana UI I tried to add new connection:\nand before has been constantly saying:\n\nUnable to fetch labels from Loki (Failed to call resource), please check the server logs for more details\n\nThen I realize, that address is wrong and set it to `http://loki:3100` and now it says:\n\nData source connected, but no labels received. Verify that Loki and Promtail is configured properly.\n\nLogs from Grafana on this request:\n```\n`logger=context userId=1 orgId=1 uname=admin t=2023-09-27T09:46:06.21413999Z level=error msg=\"Datasource has already been updated by someone else. Please reload and try again\" error=\"trying to update old version of datasource\" remote_addr=10.0.0.2 traceID=\nlogger=context userId=1 orgId=1 uname=admin t=2023-09-27T09:46:06.21431287Z level=info msg=\"Request Completed\" method=PUT path=/api/datasources/uid/a4c05563-ea62-4f76-b2a7-b6f964314b88 status=409 remote_addr=10.0.0.2 time_ms=1 duration=1.141585ms size=107 referer=http://:3000/connections/your-connections/datasources/edit/a4c05563-ea62-4f76-b2a7-b6f964314b88 handler=/api/datasources/uid/:uid\nlogger=context userId=1 orgId=1 uname=admin t=2023-09-27T09:53:33.342223169Z level=info msg=\"Request Completed\" method=GET path=/api/live/ws status=-1 remote_addr=10.0.0.2 time_ms=0 duration=746.46\u00b5s size=0 referer= handler=/api/live/ws\n`\n```\nLoki in that moment doesn't show any logs which could refer to sent request, only \"save checkpoint wal\".\nI absolutely don't know what's the problem here. I stuck here for around 4 days in a row, Could you provide and solution or ideas to overcome it?",
      "solution": "So, there are could be several problems:\n1. mingrammer/flog generates log to file instead of stdout\nIt means that docker-loki-driver couldn't get any info to pass to Loki. So, in this case just change line\n```\n`- --output=/var/log/generated-logs.txt\n`\n```\nto\n```\n`- --output=/dev/stdout\n`\n```\n2. Docker plugins able to work only(!) in bridge and host network modes\nHence, it simply couldn't find any host with name `loki` (or within any docker network or its alias).\n3. host.docker.internal depends on OS\nAlso, `http://host.docker.internal:3100/loki/api/v1/push` line is wrong in case you would deploy it on Linux, because it's only Windows and Mac specific feature, in case of Linux family you should add it manually, e.g. as mentioned here: https://stackoverflow.com/a/67158212/7502538.\nThus, as workaround you can manage it using static ip address (added manually to host file):\n```\n`loki-url: http://172.0.0.15:3100/loki/api/v1/push\n`\n```\nOr simply deploy any proxy-server like nginx and set up redirection. Finally (and it works for my project) set something like:\n```\n`loki-url: http://loki-example-dns.com/loki/api/v1/push\n`\n```\nMeaning docker-loki-driver should be able to lookup Loki service in the host network as you usually do (e.g. with `ping` command).\n4. Wrong access rights\nFor this configuration it's quite ok to let it be public accessible. But if something goes wrong be sure you didn't enable any type of authentication/authorization, either proceeds with right credentials. I faced it during this project and also they `Basic Auth` was handled by nginx.\nSetting up Grafana\nIn Grafana `Data Sources` if you wish to add Loki and they stay in the same docker network (in swarm mode with respect to question), that and in only that case you could use alias:\n```\n`http://loki:3100/\n`\n```\nAdditions\nSeveral links that helped me out to figure this out:\n\nhttps://github.com/grafana/loki/issues/2537\nhttps://github.com/grafana/loki/issues/1368\nhttps://github.com/grafana/loki/issues/1962\nhttps://github.com/grafana/loki/issues/1051#issuecomment-646923354\nhttps://github.com/grafana/loki/blob/main/production/docker/config/loki.yaml\n\nLastly this command used to be really helpful to analyze logs:\n`sudo journalctl -u docker.service -f`",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2023-09-26T17:50:20",
      "url": "https://stackoverflow.com/questions/77181562/grafana-cannot-retrieve-logs-and-labels-from-loki"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 70134962,
      "title": "Traefik Dashboard returns always 404 in Docker Swarm deployment",
      "problem": "I'm trying my best to get Traefik dashboard available through http://gateway.localhost/dashboard/, but I'm always getting a 404 response* from Traefik. Can s.o. please review my stack file and tell me, why it's not working?\nI tried it on my server with a valid domain, but it's either working there or on localhost with Docker Desktop in Swarm mode. The WhoAmI service can be reached through http://localhost which is correct.\n`docker stack deploy -c traefik.yml traefik`\n*404 is returned for these routes too: http://gateway.localhost, http://gateway.localhost/dashboard\ntraefik.yml:\n```\n`version: '3'\n\nservices:\n  reverse-proxy:\n    image: traefik:v2.5\n    command:\n      - \"--providers.docker.swarmmode=true\"\n      - \"--providers.docker.exposedByDefault=false\"\n      - \"--api.dashboard=true\"\n      - \"--entrypoints.web.address=:80\"\n      # Logging\n      - \"--accesslog\"\n      - \"--log.level=INFO\"\n    ports:\n      - \"80:80\"\n    deploy:\n      labels:\n        #Because Swarm API does not support automatic way\n        - \"traefik.http.services.reverse-proxy.loadbalancer.server.port=80\"\n        #Dashboard\n        - \"traefik.http.routers.dashboard.rule=Host(`gateway.localhost`) && (PathPrefix(`/api`) || PathPrefix(`/dashboard`))\"\n        - \"traefik.http.routers.dashboard.service=api@internal\"\n        - \"traefik.http.routers.dashboard.entrypoints=web\"\n        - \"traefik.http.routers.dashboard.middlewares=auth\"\n        - \"traefik.http.middlewares.auth.basicauth.users=test:$$apr1$$H6uskkkW$$IgXLP6ewTrSuBkTrqE8wj/,test2:$$apr1$$d9hr9HBB$$4HxwgUir3HP4EsggP/QNo0\"\n      placement:\n        constraints:\n          - node.role == manager\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n\n  whoami:\n    image: traefik/whoami\n    deploy:\n      labels:\n        - \"traefik.enable=true\"\n        - \"traefik.http.routers.whoami.rule=Host(`localhost`)\"\n        - \"traefik.http.routers.whoami.entrypoints=web\"\n        - \"traefik.http.services.whoami.loadbalancer.server.port=80\"\n`\n```",
      "solution": "You need to enable traefik for the container with the `traefik.enable=true` label:\n`version: '3'\n\nservices:\n  reverse-proxy:\n    image: traefik:v2.5\n    command:\n      - \"--providers.docker.swarmmode=true\"\n      - \"--providers.docker.exposedByDefault=false\"\n      - \"--api.dashboard=true\"\n      - \"--entrypoints.web.address=:80\"\n      # Logging\n      - \"--accesslog\"\n      - \"--log.level=INFO\"\n    ports:\n      - \"80:80\"\n    deploy:\n      labels:\n        ######## add the following label to enable traefik #######\n        - \"traefik.enable=true\"\n        #Because Swarm API does not support automatic way\n        - \"traefik.http.services.reverse-proxy.loadbalancer.server.port=80\"\n        #Dashboard\n        - \"traefik.http.routers.dashboard.rule=Host(`gateway.localhost`) && (PathPrefix(`/api`) || PathPrefix(`/dashboard`))\"\n        - \"traefik.http.routers.dashboard.service=api@internal\"\n        - \"traefik.http.routers.dashboard.entrypoints=web\"\n        - \"traefik.http.routers.dashboard.middlewares=auth\"\n        - \"traefik.http.middlewares.auth.basicauth.users=test:$$apr1$$H6uskkkW$$IgXLP6ewTrSuBkTrqE8wj/,test2:$$apr1$$d9hr9HBB$$4HxwgUir3HP4EsggP/QNo0\"\n      placement:\n        constraints:\n          - node.role == manager\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n\n  whoami:\n    image: traefik/whoami\n    deploy:\n      labels:\n        - \"traefik.enable=true\"\n        - \"traefik.http.routers.whoami.rule=Host(`localhost`)\"\n        - \"traefik.http.routers.whoami.entrypoints=web\"\n        - \"traefik.http.services.whoami.loadbalancer.server.port=80\"\n`",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2021-11-27T13:52:43",
      "url": "https://stackoverflow.com/questions/70134962/traefik-dashboard-returns-always-404-in-docker-swarm-deployment"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 68332554,
      "title": "Overlay driver not being applied when creating docker swarm network",
      "problem": "I'm working with docker swarm and trying to create a network with the `overlay` driver.\nWhenever I create the network, the driver is not attached.\n\nIf I try and attach a service to the network, the process just hangs infinitely.\nIf I create a service without attaching it to the network, it works right away.\n\n```\n`pi@node3:~ $ docker network ls\nNETWORK ID     NAME              DRIVER    SCOPE\na1cc2e1f4f2b   bridge            bridge    local\n83597f713bcf   docker_gwbridge   bridge    local\n277f1166485e   host              host      local\nfs2vvjeuejxc   ingress           overlay   swarm\n5d0ce08c744c   none              null      local\n\npi@node3:~ $ docker network create --driver overlay test\n4bfkahhkhrblod2t79yd83vws\n\npi@node3:~ $ docker network ls\nNETWORK ID     NAME              DRIVER    SCOPE\na1cc2e1f4f2b   bridge            bridge    local\n83597f713bcf   docker_gwbridge   bridge    local\n277f1166485e   host              host      local\nfs2vvjeuejxc   ingress           overlay   swarm\n5d0ce08c744c   none              null      local\n4bfkahhkhrbl   test                        swarm\n`\n```\nI can't figure out why it's not adding the driver. I have a suspicion it has something to do with the ingress network settings, but I'm stuck as for troubleshooting here.\nRelevant Info\nSwarm:\n```\n`pi@node3:~ $ docker node ls\nID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION\nygcte2diochpbgu7bqtw41k70     node1      Ready     Active                          20.10.7\nxbllxgfa35937rmvdi8mi8dlb     node2      Ready     Active                          20.10.7\ntvw4b53w6g3qv2k3919dg3a81 *   node3      Ready     Active         Leader           20.10.7\n`\n```\nManager node:\n```\n`pi@node3:~ $ docker node inspect node3\n[\n    {\n        \"ID\": \"tvw4b53w6g3qv2k3919dg3a81\",\n        \"Version\": {\n            \"Index\": 165\n        },\n        \"CreatedAt\": \"2021-07-10T16:41:23.043334654Z\",\n        \"UpdatedAt\": \"2021-07-11T00:27:25.807737662Z\",\n        \"Spec\": {\n            \"Labels\": {},\n            \"Role\": \"manager\",\n            \"Availability\": \"active\"\n        },\n        \"Description\": {\n            \"Hostname\": \"node3\",\n            \"Platform\": {\n                \"Architecture\": \"armv7l\",\n                \"OS\": \"linux\"\n            },\n            \"Resources\": {\n                \"NanoCPUs\": 4000000000,\n                \"MemoryBytes\": 969105408\n            },\n            \"Engine\": {\n                \"EngineVersion\": \"20.10.7\",\n                \"Plugins\": [\n                    {\n                        \"Type\": \"Log\",\n                        \"Name\": \"awslogs\"\n                    },\n                    {\n                        \"Type\": \"Log\",\n                        \"Name\": \"fluentd\"\n                    },\n                    {\n                        \"Type\": \"Log\",\n                        \"Name\": \"gcplogs\"\n                    },\n                    {\n                        \"Type\": \"Log\",\n                        \"Name\": \"gelf\"\n                    },\n                    {\n                        \"Type\": \"Log\",\n                        \"Name\": \"journald\"\n                    },\n                    {\n                        \"Type\": \"Log\",\n                        \"Name\": \"json-file\"\n                    },\n                    {\n                        \"Type\": \"Log\",\n                        \"Name\": \"local\"\n                    },\n                    {\n                        \"Type\": \"Log\",\n                        \"Name\": \"logentries\"\n                    },\n                    {\n                        \"Type\": \"Log\",\n                        \"Name\": \"splunk\"\n                    },\n                    {\n                        \"Type\": \"Log\",\n                        \"Name\": \"syslog\"\n                    },\n                    {\n                        \"Type\": \"Network\",\n                        \"Name\": \"bridge\"\n                    },\n                    {\n                        \"Type\": \"Network\",\n                        \"Name\": \"host\"\n                    },\n                    {\n                        \"Type\": \"Network\",\n                        \"Name\": \"ipvlan\"\n                    },\n                    {\n                        \"Type\": \"Network\",\n                        \"Name\": \"macvlan\"\n                    },\n                    {\n                        \"Type\": \"Network\",\n                        \"Name\": \"null\"\n                    },\n                    {\n                        \"Type\": \"Network\",\n                        \"Name\": \"overlay\"\n                    },\n                    {\n                        \"Type\": \"Volume\",\n                        \"Name\": \"local\"\n                    }\n                ]\n            },\n            \"TLSInfo\": {\n                \"TrustRoot\": \"-----BEGIN CERTIFICATE-----\\nMIIBajCCARCgAwIBAgIUFIx3NAw+jgaasNXCoi+QP4GxaOQwCgYIKoZIzj0EAwIw\\nEzERMA8GA1UEAxMIc3dhcm0tY2EwHhcNMjEwNzEwMTYyMjAwWhcNNDEwNzA1MTYy\\nMjAwWjATMREwDwYDVQQDEwhzd2FybS1jYTBZMBMGByqGSM49AgEGCCqGSM49AwEH\\nA0IABKyunnrZtfkOO+Cc/MX/qbyJjG12ee8es0IHB1HXF2MhqSfYOeUuBlTvuHuB\\nxl8s8eQ4IMfjP0w5LYJNqypZp0KjQjBAMA4GA1UdDwEB/wQEAwIBBjAPBgNVHRMB\\nAf8EBTADAQH/MB0GA1UdDgQWBBRq6yBEIFv03tQqBkohCh4A+mIZdTAKBggqhkjO\\nPQQDAgNIADBFAiA5kKgC2WxcOMyfrmFr8fU6w1Mo1mq5GMKA4owTB7pcEQIhALZi\\n9AH0vVyR+7NmmR7LfPO65CIJ9UVuPZBXRZ6pcmzX\\n-----END CERTIFICATE-----\\n\",\n                \"CertIssuerSubject\": \"MBMxETAPBgNVBAMTCHN3YXJtLWNh\",\n                \"CertIssuerPublicKey\": \"MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAErK6eetm1+Q474Jz8xf+pvImMbXZ57x6zQgcHUdcXYyGpJ9g55S4GVO+4e4HGXyzx5Dggx+M/TDktgk2rKlmnQg==\"\n            }\n        },\n        \"Status\": {\n            \"State\": \"ready\",\n            \"Addr\": \"0.0.0.0\"\n        },\n        \"ManagerStatus\": {\n            \"Leader\": true,\n            \"Reachability\": \"reachable\",\n            \"Addr\": \"10.0.0.93:2377\"\n        }\n    }\n`\n```\nIngress network:\n```\n`pi@node3:~ $ docker network inspect ingress\n[\n    {\n        \"Name\": \"ingress\",\n        \"Id\": \"fs2vvjeuejxcjxqivenb76kgj\",\n        \"Created\": \"2021-07-10T17:24:14.228552858-07:00\",\n        \"Scope\": \"swarm\",\n        \"Driver\": \"overlay\",\n        \"EnableIPv6\": false,\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Options\": null,\n            \"Config\": [\n                {\n                    \"Subnet\": \"10.10.0.0/24\",\n                    \"Gateway\": \"10.10.0.1\"\n                }\n            ]\n        },\n        \"Internal\": false,\n        \"Attachable\": false,\n        \"Ingress\": true,\n        \"ConfigFrom\": {\n            \"Network\": \"\"\n        },\n        \"ConfigOnly\": false,\n        \"Containers\": {\n            \"ingress-sbox\": {\n                \"Name\": \"ingress-endpoint\",\n                \"EndpointID\": \"34003d042d395b90328ed90c8133505a6bec6df90065c5b47b47ee3853545c91\",\n                \"MacAddress\": \"02:42:0a:0a:00:02\",\n                \"IPv4Address\": \"10.10.0.2/24\",\n                \"IPv6Address\": \"\"\n            }\n        },\n        \"Options\": {\n            \"com.docker.network.driver.overlay.vxlanid_list\": \"4096\"\n        },\n        \"Labels\": {},\n        \"Peers\": [\n            {\n                \"Name\": \"e2f4d4e6ba20\",\n                \"IP\": \"10.0.0.93\"\n            },\n            {\n                \"Name\": \"de3d98ce0f8d\",\n                \"IP\": \"10.0.0.25\"\n            },\n            {\n                \"Name\": \"b61722e30756\",\n                \"IP\": \"10.0.0.12\"\n            }\n        ]\n    }\n]\n`\n```\nDocker version:\n```\n`pi@node3:~ $ docker --version\nDocker version 20.10.7, build f0df350\n`\n```\nDocker info:\n```\n`pi@node3:~ $ docker info\nClient:\n Context:    default\n Debug Mode: false\n Plugins:\n  app: Docker App (Docker Inc., v0.9.1-beta3)\n  buildx: Build with BuildKit (Docker Inc., v0.5.1-docker)\n\nServer:\n Containers: 0\n  Running: 0\n  Paused: 0\n  Stopped: 0\n Images: 5\n Server Version: 20.10.7\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: cgroupfs\n Cgroup Version: 1\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\n Swarm: active\n  NodeID: tvw4b53w6g3qv2k3919dg3a81\n  Is Manager: true\n  ClusterID: 4vf16jdlegf3ctys5k6wumcfc\n  Managers: 1\n  Nodes: 3\n  Default Address Pool: 10.10.0.0/24  \n  SubnetSize: 24\n  Data Path Port: 4789\n  Orchestration:\n   Task History Retention Limit: 5\n  Raft:\n   Snapshot Interval: 10000\n   Number of Old Snapshots to Retain: 0\n   Heartbeat Tick: 1\n   Election Tick: 10\n  Dispatcher:\n   Heartbeat Period: 5 seconds\n  CA Configuration:\n   Expiry Duration: 3 months\n   Force Rotate: 0\n  Autolock Managers: false\n  Root Rotation In Progress: false\n  Node Address: 10.0.0.93\n  Manager Addresses:\n   10.0.0.93:2377\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: d71fcd7d8303cbf684402823e425e9dd2e99285d\n runc version: b9ee9c6314599f1b4a7f497e1f1f856fe433d3b7\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: default\n Kernel Version: 5.10.17-v7+\n Operating System: Raspbian GNU/Linux 10 (buster)\n OSType: linux\n Architecture: armv7l\n CPUs: 4\n Total Memory: 924.2MiB\n Name: node3\n ID: A67O:SIT4:QOMH:SILY:WHAY:KSGQ:VWMF:QVEJ:VCOZ:KW32:PZRV:ZD4B\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Registry: https://index.docker.io/v1/\n Labels:\n Experimental: false\n Insecure Registries:\n  127.0.0.0/8\n Live Restore Enabled: false\n\nWARNING: No memory limit support\nWARNING: No swap limit support\nWARNING: No kernel memory TCP limit support\nWARNING: No oom kill disable support\nWARNING: No blkio throttle.read_bps_device support\nWARNING: No blkio throttle.write_bps_device support\nWARNING: No blkio throttle.read_iops_device support\nWARNING: No blkio throttle.write_iops_device support\n`\n```\nWhat I've tried:\n\nRemoving all the nodes and creating a new swarm\nRemoving the ingress network and creating a new one following the instructions here\nTried to go through the walkthrough here but can't get past Create the Services 2.\nRebooted all the nodes\n\nAny advice or pointing in the right direction would be much appreciated! I've been stuck here for 48 hours.",
      "solution": "Solved!\nThe issue ended up being:\n\nThe nodes were all on `10.0.0.x`\nI set the `--default-addr-pool 10.10.0.0/24` when initializing the swarm\n\nAny network I tried to create using the `--driver overlay` would end up without any subnet or gateway information.\nHow I resolved the issue:\nI was able to solve it using the `--subnet` flag when creating a custom network.\n```\n`pi@node3:~ $ docker network create --driver overlay --subnet 10.10.10.0/24 test\n`\n```\n```\n`pi@node3:~ $ docker network ls\nNETWORK ID     NAME              DRIVER    SCOPE\n55ab64773261   bridge            bridge    local\nce1a0f497e9d   docker_gwbridge   bridge    local\n7c85cac72cf8   host              host      local\no7iew29j70nl   ingress           overlay   swarm\nca5fc5682911   none              null      local\nplezwc8zahpl   test              overlay   swarm\n`\n```\n```\n`pi@node3:~ $ docker network inspect test\n[\n    {\n        \"Name\": \"test\",\n        \"Id\": \"plezwc8zahpl9gs8hbv64bbo3\",\n        \"Created\": \"2021-07-16T17:30:28.773110478Z\",\n        \"Scope\": \"swarm\",\n        \"Driver\": \"overlay\",\n        \"EnableIPv6\": false,\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Options\": null,\n            \"Config\": [\n                {\n                    \"Subnet\": \"10.10.10.0/24\",\n                    \"Gateway\": \"10.10.10.1\"\n                }\n            ]\n        },\n        \"Internal\": false,\n        \"Attachable\": false,\n        \"Ingress\": false,\n        \"ConfigFrom\": {\n            \"Network\": \"\"\n        },\n        \"ConfigOnly\": false,\n        \"Containers\": null,\n        \"Options\": {\n            \"com.docker.network.driver.overlay.vxlanid_list\": \"4097\"\n        },\n        \"Labels\": null\n    }\n]\n`\n```",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2021-07-11T03:06:13",
      "url": "https://stackoverflow.com/questions/68332554/overlay-driver-not-being-applied-when-creating-docker-swarm-network"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 67707342,
      "title": "Unwanted Warm Shutdown (MainProcess) of node worker in airflow docker swarm",
      "problem": "I am currently setting up remote workers via docker swarm for Apache Airflow on AWS EC2 instances.\nA remote worker shuts down every 60 seconds without an apparent reason with the following error:\n```\n`BACKEND=postgresql+psycopg2\nDB_HOST=postgres\nDB_PORT=5432\n\nBACKEND=postgresql+psycopg2\nDB_HOST=postgres\nDB_PORT=5432\n\n/home/airflow/.local/lib/python3.7/site-packages/airflow/configuration.py:813: DeprecationWarning: Specifying both AIRFLOW_HOME environment variable and airflow_home in the config file is deprecated. Please use only the AIRFLOW_HOME environment variable and remove the config file entry.\n  warnings.warn(msg, category=DeprecationWarning)\nStarting flask\n * Serving Flask app \"airflow.utils.serve_logs\" (lazy loading)\n * Environment: production\n   WARNING: This is a development server. Do not use it in a production deployment.\n   Use a production WSGI server instead.\n * Debug mode: off\n[2021-05-26 08:37:48,027] {_internal.py:113} INFO -  * Running on http://0.0.0.0:8793/ (Press CTRL+C to quit)\n/home/airflow/.local/lib/python3.7/site-packages/celery/platforms.py:801 RuntimeWarning: You're running the worker with superuser privileges: this is\nabsolutely not recommended!\n\nPlease specify a different user using the --uid option.\n\nUser information: uid=1000 euid=1000 gid=0 egid=0\n\n[2021-05-26 08:37:49,557: INFO/MainProcess] Connected to redis://redis:6379/0\n[2021-05-26 08:37:49,567: INFO/MainProcess] mingle: searching for neighbors\n[2021-05-26 08:37:50,587: INFO/MainProcess] mingle: sync with 3 nodes\n[2021-05-26 08:37:50,587: INFO/MainProcess] mingle: sync complete\n[2021-05-26 08:37:50,601: INFO/MainProcess] celery@fcd56490a11f ready.\n[2021-05-26 08:37:55,296: INFO/MainProcess] Events of group {task} enabled by remote.\n\nworker: Warm shutdown (MainProcess)\n\n -------------- celery@fcd56490a11f v4.4.7 (cliffs)\n--- ***** -----\n-- ******* ---- Linux-5.4.0-1045-aws-x86_64-with-debian-10.8 2021-05-26 08:37:48\n- *** --- * ---\n- ** ---------- [config]\n- ** ---------- .> app:         airflow.executors.celery_executor:0x7f951e9d3fd0\n- ** ---------- .> transport:   redis://redis:6379/0\n- ** ---------- .> results:     postgresql://airflow:**@postgres/airflow\n- *** --- * --- .> concurrency: 16 (prefork)\n-- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)\n--- ***** -----\n -------------- [queues]\n                .> default          exchange=default(direct) key=default\n\n[tasks]\n  . airflow.executors.celery_executor.execute_command\n\n`\n```\nAll docker services running in my docker stack on the manager node are doing fine, also the selenium service on the remote node. Following the docker compose setup here for Airflow I developed the docker compose file seen below.\nPostgres, Redis and Selenium are standard Images.\nFor the airflow services there are two images:\n\n`airflow-manager` which is just the name of the image that is locally created when starting the container.\n\n`localhost:5000/myadmin/airflow-remote` is the same image pushed to a local registry such that it can be seen from other machines.\n\ndocker-compose.yaml:\n```\n`version: '3.7'\n\nservices:\n  postgres:\n    image: postgres:13\n    env_file:\n      - ./config/postgres_test.env\n    ports:\n      - 5432:5432\n    volumes:\n      - postgres-db-volume:/var/lib/postgresql/data\n    healthcheck:\n      test: [\"CMD\", \"pg_isready\", \"-d\", \"postgres\", \"-U\", \"airflow\"]\n      interval: 5s\n      retries: 5\n    restart: always\n    depends_on: []\n    deploy:\n      placement:\n        constraints: [ node.role == manager ]\n\n  redis:\n    image: redis:latest\n    env_file:\n      - ./config/postgres_test.env\n    ports:\n      - 6379:6379\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n      interval: 5s\n      timeout: 30s\n      retries: 50\n    restart: always\n    depends_on: []\n    deploy:\n      placement:\n        constraints: [ node.role == manager ]\n\n  airflow-webserver:\n    image: airflow-manager\n    build:\n      context: .\n      dockerfile: Dockerfile\n    env_file:\n      - ./config/airflow.env\n      - ./config/postgres_test.env\n    volumes:\n      - ./:/opt/airflow\n    user: \"${AIRFLOW_UID:-1000}:${AIRFLOW_GID:-0}\"\n    command: webserver\n    ports:\n      - 8080:8080\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8080/health\"]\n      interval: 10s\n      timeout: 10s\n      retries: 5\n    restart: always\n    depends_on:\n      - airflow-init\n    deploy:\n      placement:\n        constraints: [ node.role == manager ]\n\n  airflow-scheduler:\n    image: airflow-manager\n    build:\n      context: .\n      dockerfile: Dockerfile\n    env_file:\n      - ./config/airflow.env\n      - ./config/postgres_test.env\n    volumes:\n      - ./:/opt/airflow\n    user: \"${AIRFLOW_UID:-1000}:${AIRFLOW_GID:-0}\"\n    command: scheduler\n    restart: always\n    depends_on:\n      - airflow-init\n    deploy:\n      placement:\n        constraints: [ node.role == manager ]\n\n  airflow-worker-manager:\n    image: airflow-manager\n    build:\n      context: .\n      dockerfile: Dockerfile\n    env_file:\n      - ./config/airflow.env\n      - ./config/postgres_test.env\n    volumes:\n      - ./:/opt/airflow\n    user: \"${AIRFLOW_UID:-1000}:${AIRFLOW_GID:-0}\"\n    command: celery worker\n    restart: always\n    ports:\n      - 8794:8080\n    depends_on:\n      - airflow-scheduler\n      - airflow-webserver\n      - airflow-init\n    deploy:\n      placement:\n        constraints: [ node.role == manager ]\n\n  airflow-worker-remote:\n    image: localhost:5000/myadmin/airflow-remote\n    build:\n      context: .\n      dockerfile: Dockerfile\n    env_file:\n      - ./config/airflow.env\n      - ./config/postgres_test.env\n    volumes:\n      - ./:/opt/airflow\n    user: \"${AIRFLOW_UID:-1000}:${AIRFLOW_GID:-0}\"\n    command: celery worker\n    restart: always\n    ports:\n      - 8795:8080\n    depends_on:\n      - airflow-scheduler\n      - airflow-webserver\n      - airflow-init\n    deploy:\n      placement:\n        constraints: [ node.role == worker ]\n\n  airflow-init:\n    image: airflow-manager\n    build:\n      context: .\n      dockerfile: Dockerfile\n    env_file:\n      - ./config/airflow.env\n      - ./config/postgres_test.env\n      - ./config/init.env\n    volumes:\n      - ./:/opt/airflow\n    user: \"${AIRFLOW_UID:-1000}:${AIRFLOW_GID:-0}\"\n    command: version\n    depends_on:\n      - postgres\n      - redis\n    deploy:\n      placement:\n        constraints: [ node.role == manager ]\n\n  flower:\n    image: airflow-manager\n    build:\n      context: .\n      dockerfile: Dockerfile\n    env_file:\n      - ./config/airflow.env\n      - ./config/postgres_test.env\n    volumes:\n      - ./:/opt/airflow\n    user: \"${AIRFLOW_UID:-1000}:${AIRFLOW_GID:-0}\"\n    command: celery flower\n    ports:\n      - 5555:5555\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:5555/\"]\n      interval: 10s\n      timeout: 10s\n      retries: 5\n    restart: always\n    depends_on: []\n    deploy:\n      placement:\n        constraints: [ node.role == manager ]\n\n  selenium-chrome:\n    image: selenium/standalone-chrome:latest\n    ports:\n      - 4444:4444\n    deploy:\n      placement:\n        constraints: [ node.role == worker ]\n    depends_on: []\n\nvolumes:\n  postgres-db-volume:\n\n`\n```\nDockerfile:\n```\n`\nFROM apache/airflow:2.0.1-python3.7\nCOPY config/requirements.txt /tmp/\nRUN mkdir -p /home/airflow/.cache/zeep\nRUN chmod -R 777 /home/airflow/.cache/zeep\nRUN chmod -R 777 /opt/airflow/\nRUN mkdir -p /home/airflow/.wdm\nRUN chmod -R 777 /home/airflow/.wdm\nRUN pip install -r /tmp/requirements.txt\n\n`\n```\nEnvironment files:\nairflow_env:\n```\n`\nPYTHONPATH=/opt/airflow/\nAIRFLOW_UID=1000\nAIRFLOW_GID=0\nAIRFLOW_HOME=/opt/airflow/\nAIRFLOW__CORE__AIRFLOW_HOME=/opt/airflow/\nAIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags\nAIRFLOW__CORE__ENABLE_XCOM_PICKLING=true\nAIRFLOW__CORE__EXECUTOR=CeleryExecutor\nAIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow\nAIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres/airflow\nAIRFLOW__CELERY__BROKER_URL=redis://:@redis:6379/0\nAIRFLOW__CORE__FERNET_KEY=****\nAIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true\nAIRFLOW__CORE__LOAD_EXAMPLES=false\nAIRFLOW__CORE__PLUGINS_FOLDER=/plugins/\nAIRFLOW__CORE__PARALLELISM=48\nAIRFLOW__CORE__DAG_CONCURRENCY=8\nAIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG=1\nAIRFLOW__WEBSERVER__DAG_DEFAULT_VIEW=graph\nAIRFLOW__WEBSERVER__LOG_FETCH_TIMEOUT_SEC=30\nAIRFLOW__WEBSERVER__HIDE_PAUSED_DAGS_BY_DEFAULT=true\nAIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT=false\nCELERY_ACKS_LATE=true\n\n`\n```\npostgres_test.env:\n```\n`\nPOSTGRES_USER=airflow\nPOSTGRES_PASSWORD=airflow\nPOSTGRES_HOST=postgres\nPOSTGRES_PORT=5432\nPOSTGRES_DB=airflow\n\n`\n```\ninit.env:\n```\n`_AIRFLOW_DB_UPGRADE=true\n_AIRFLOW_WWW_USER_CREATE=true\n_AIRFLOW_WWW_USER_USERNAME=${_AIRFLOW_WWW_USER_USERNAME:-airflow}\n_AIRFLOW_WWW_USER_PASSWORD=${_AIRFLOW_WWW_USER_PASSWORD:-airflow}\n`\n```\nI saw issues about this being resolved by setting the env `CELERY_ACKS_LATE=true`, but this didn't help in my case.\nThis is a really annoying issue, because it spams my flower worker supervision and I would like to extend to more workers running on other nodes.\nDo you have any idea what this could be? Any help is appreciated!\nThanks in Advance!",
      "solution": "just wanted to let you know that I was able to fix the issue by setting\n`CELERY_WORKER_MAX_TASKS_PER_CHILD=500`, which otherwise defaults to 50. Our Airflow DAG was sending around 85 tasks to this worker, so it was probably overwhelmed.\nApparently celery doesn't accept more incoming messages from redis and redis shuts down the worker if its outgoing message pipeline is full.\nAfter searching for days with two people, we found the answer. Apparently it is still a workaround, but it works as is should now.\nI found the answer in this github issue.\nJust wanted to let you know.\nIf you have further insights please feel free to share.",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2021-05-26T16:47:21",
      "url": "https://stackoverflow.com/questions/67707342/unwanted-warm-shutdown-mainprocess-of-node-worker-in-airflow-docker-swarm"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 69341721,
      "title": "Running a docker container on localhost not working",
      "problem": "I have created a Dockerfile image with an easy react app boilerplate result of `npx create-react-app my-app`\n```\n`FROM node\nWORKDIR /app\nCOPY package.json /app\nRUN npm install\nCOPY . /app\nEXPOSE 4000\nCMD \"npm\" \"start\n`\n```\nEverything worked fine and created the image, started the container, no errors but cannot open on localhost, does anybody know what could be the problem ?\nI have used `docker run -p 4000:4000 -it react-repo` to start the container.",
      "solution": "A sample Docker file for below express.js app.\n`const express = require('express');\n\nconst PORT = 8080;\nconst HOST = '0.0.0.0';\n\nconst app = express();\napp.get('/', (req, res) => {\n  res.send('Hello World');\n});\n\napp.listen(PORT, HOST);\nconsole.log(`Running on http://${HOST}:${PORT}`);\n`\n```\n`FROM node:14\n\nWORKDIR /usr/src/app\n\nCOPY package*.json ./\n\nRUN npm install\n\nCOPY . .\n\nEXPOSE 8080\nCMD [ \"node\", \"server.js\" ]\n`\n```\nTo Run,\n```\n`# docker run -p : imageName\ndocker run -p 8080:8080 imageName\n`\n```",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2021-09-27T07:45:29",
      "url": "https://stackoverflow.com/questions/69341721/running-a-docker-container-on-localhost-not-working"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 66603133,
      "title": "Reliable way of getting the full container name inside a container running in Docker Swarm",
      "problem": "Background: I have a setup where many different scalable services connect to their databases via a connection pool (per instance). These services run within a Docker Swarm.\nIn my current database setup, this ends up looking as follows (using PostgreSQL in this example):\n```\n`PID | Database | User | Application                     | Client | ...\n... | db1      | app  | --standard JDBC driver string-- | x      | ...\n... | db1      | app  | --standard JDBC driver string-- | y      | ... \n... | db1      | app  | --standard JDBC driver string-- | y      | ... \n... | ...      | app  | ...                             | x      | ... \n... | ...      | app  | ...                             | x      | ... \n... | db2      | app  | --standard JDBC driver string-- | y      | ... \n... | db2      | app  | --standard JDBC driver string-- | y      | ... \n... | ...      | app  | ...                             | x      | ... \n... | ...      | app  | ...                             | x      | ... \n`\n```\nWhat I would like to do, effectively, is provide the current Docker Swarm container name, including scaling identifier, to the DBMS to be able to better monitor the database connections, i.e.:\n```\n`PID | Database | User | Application        | Client | ...\n... | db1      | app  | books-service.1    | x      | ...\n... | db1      | app  | books-service.2    | y      | ... \n... | db1      | app  | books-service.3    | y      | ... \n... | ...      | app  | ...                | x      | ... \n... | ...      | app  | ...                | x      | ... \n... | db2      | app  | checkout-service.2 | y      | ... \n... | db2      | app  | checkout-service.2 | y      | ... \n... | ...      | app  | ...                | x      | ... \n... | ...      | app  | ...                | x      | ... \n`\n```\n(obviously, setting the connection string is trivial - it's getting the information to set that is the issue)\nSince my applications are managed by Docker Swarm (and sometime in the future, likely Kubernetes), I cannot manually set this value via environment (as I do not perform a `docker run`).\nWhen running `docker ps` on a given Swarm node, I see the following output:\n```\n`CONTAINER ID        IMAGE                                               COMMAND                  CREATED             STATUS              PORTS                                 NAMES\n59cdbf724091        my-docker-registry.net/books-service:latest         \"/bin/sh -c '/usr/bi\u2026\"   7 minutes ago       Up 7 minutes                                              books-service.1.zabvo1jal0h2xya9qfftnrnej\n0eeeee15e92a        my-docker-registry.net/checkout-service:latest      \"/bin/sh -c 'exec /u\u2026\"   8 minutes ago       Up 8 minutes                                              checkout-service.2.189s7d09m0q86y7fdf3wpy0vc\n`\n```\nOf note is the `NAMES` column, which includes an identifier for the actual instance of the given container (or image, however you'd prefer to look at it). Do note that this name is not the hostname of the container, which by default is the container ID.\nI know there are ways to determine if an application is running inside Docker (e.g. using /proc/1/cgroup), but that doesn't help me either as those also only list the container ID.\nIs there a way to get this value from inside a Docker container that is being run in a swarm?",
      "solution": "What you need (`books-service.1`) is a combination of a swarm service name and a task slot. Both of these can be passed to the container as environment variables, as well as a full task name (`books-service.1.zabvo1jal0h2xya9qfftnrnej`):\n`version: \"3.0\"\nservices:\n  test:\n    image: debian:buster\n    command: cat /dev/stdout\n    environment:\n      SERVICE_NAME: '{{ .Service.Name }}'\n      TASK_SLOT: '{{ .Task.Slot }}'\n      TASK_NAME: \"{{ .Task.Name }}\"\n`\nAfter passing these you can either strip the `TASK_NAME` to get what you need or combine the `SERVICE_NAME` with the `TASK_SLOT`. Come to think of it, you can combine them right in the template:\n`version: \"3.0\"\nservices:\n  test:\n    image: debian:buster\n    command: cat /dev/stdout\n    environment:\n      MY_NAME: '{{ .Service.Name }}.{{.Task.Slot}}'\n`\nOther possible placeholders can be found here.",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2021-03-12T16:53:02",
      "url": "https://stackoverflow.com/questions/66603133/reliable-way-of-getting-the-full-container-name-inside-a-container-running-in-do"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 77724879,
      "title": "Can&#39;t create Docker Swarm because of connection failure",
      "problem": "Target: Creating a Docker Swarm\n\nCondition:\n\nHost A: Ubuntu 23.10\nHost B: Mac Sonoma 14.1.2\n\nOperation:\n\nIn Host A:\n`docker swarm init\n`\n\nIn Host B:\n`docker swarm join --token SWMTKN-1-3o2m78qf57hy2zikfx8p2yc7hrn63edlmlixwrq7bh28xws7zx-9oirn0wh3mbrdui3kcwytl560 192.168.65.9:2377\n`\n\nError:\n`Error response from daemon: rpc error: code = Unavailable desc = connection error:\ndesc = \"transport: Error while dialing dial tcp 192.168.65.9:2377: connect: no route to host\"\n`\n\nWhat I've done so far:\n\nInstalled firewalld on Ubuntu Host A, but for unknown reason it caused system crash. I guess this is because Ubuntu 23.10 doesn't support firewalld anymore.\nUse ufw command to open communication port on Ubuntu Host A, but it doesn't work.\n\nAsking for help: I saw other people can easily run the \"docker swarm join\" command in the tutorial. Why I got this problem? Any one can help me out? Really appreciate your help.",
      "solution": "You need to open several ports for the communication (On `A` as well ass `B` hosts):\nAs Docker documentations says:\n\nPort 2377 TCP for communication with and between manager nodes\nPort 7946 TCP/UDP for overlay network node discovery\nPort 4789 UDP (configurable) for overlay network traffic\n\nFurthermore:\n\nPort 2376 TCP for secure Docker client communication.\n\nSet UFW config (Or disable the UFW as you mentioned in your question):\n`ufw allow 22/tcp\nufw allow 2376/tcp\nufw allow 2377/tcp\nufw allow 7946/tcp\nufw allow 7946/udp\nufw allow 4789/udp\nufw reload\nufw enable\nsystemctl restart docker\n`\nYou can check the IpTables configuration as well based on this documentation: https://www.digitalocean.com/community/tutorials/how-to-configure-the-linux-firewall-for-docker-swarm-on-ubuntu-16-04\nBUT, The \"Docker for Mac\" uses different networking as Linux based and that can cause turbulence in Docker Swarm. Here is a ticket for it: https://github.com/moby/swarmkit/issues/1146#issuecomment-231412874\nBased on the above ticket the Mac can run only single-node Swarm right now. (I didn't find fix for it)",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2023-12-28T04:20:29",
      "url": "https://stackoverflow.com/questions/77724879/cant-create-docker-swarm-because-of-connection-failure"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 72018377,
      "title": "Is there a way to deploy x86 images on a Apple silicon (ARM) using docker stack?",
      "problem": "We have a Docker stack compose file using services built for x86. To get our dev-setup running locally on my Apple Silicon/M1/ARM chip, I tried to deploy the stack using docker stack deploy. The services won't run and fail citing the unsupported architecture on the node. Is there a way to emulate an intel architecture on my docker swarm node? Or to run the x86 services using rosetta?\nThe `--platform` flag mentioned in the docs does not seem to work for the Docker stack.",
      "solution": "We have found a solution: `docker stack` deploys pre-built images. You will get an \"unsupported platform\" error for any service which doesn't have an arm-build.\nUsing the `--platform linux/amd64` trick doesn't work for docker stack because it has no `--platform` option. What you need to do instead is:\n\nmake sure the services of the stack are locally available (for example by executing docker stack deploy with you docker stack yml file, which again will fail to start the services but download them).\n\nadd `--resolve-image \"never\"` to your docker stack deploy, for example:  `docker stack deploy --resolve-image \"never\" --compose-file=./stack.yml`\n\nNow the services will started, using rosetta/qemu emulation in the background for the x86 services. This is not advised and can lead to faulty behavior and bad performance. For example, in my case the x86 keycloak image would start but not work. In those cases, it is necessary to build the service locally for arm.",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2022-04-26T19:49:06",
      "url": "https://stackoverflow.com/questions/72018377/is-there-a-way-to-deploy-x86-images-on-a-apple-silicon-arm-using-docker-stack"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 70121761,
      "title": "Scaling Airflow with a Celery cluster using Docker swarm",
      "problem": "As the title says, i want to setup Airflow that would run on a cluster (1 master, 2 nodes) using Docker swarm.\nCurrent setup:\nRight now i have Airflow setup that uses the CeleryExecutor that is running on single EC2.\nI have a Dockerfile that pulls Airflow's image and `pip install -r requirements.txt`.\nFrom this Dockerfile I'm creating a local image and this image is used in the docker-compose.yml that spins up the different services Airflow need (webserver, scheduler, redis, flower and some worker. metadb is Postgres that is on a separate RDS).\nThe docker-compose is used in docker swarm mode ie. `docker stack deploy . airflow_stack`\nRequired Setup:\nI want to scale the current setup to 3 EC2s (1 master, 2 nodes) that the master would run the webserver, schedule, redis and flower and the workers would run in the nodes.\nAfter searching and web and docs, there are a few things that are still not clear to me that I would love to know\n\nfrom what i understand, in order for the nodes to run the workers, the local image that I'm building from the Dockerfile need to be pushed to some repository (if it's really needed, i would use AWS ECR) for the airflow workers to be able to create the containers from that image. is that correct?\nsyncing volumes and env files, right now, I'm mounting the volumes and insert the envs in the docker-compose file. would these mounts and envs be synced to the nodes (and airflow workers containers)? if not, how can make sure that everything is sync as airflow requires that all the components (apart from redis) would have all the dependencies, etc.\none of the envs that needs to be set when using a CeleryExecuter is the broker_url, how can i make sure that the nodes recognize the redis broker that is on the master\n\nI'm sure that there are a few more things that i forget, but what i wrote is a good start.\nAny help or recommendation would be greatly appreciated\nThanks!\nDockerfile:\n```\n`FROM apache/airflow:2.1.3-python3.9\nUSER root\n\nRUN apt update;\nRUN apt -y install build-essential;\n\nUSER airflow\nCOPY requirements.txt requirements.txt\nCOPY requirements.airflow.txt requirements.airflow.txt\n\nRUN pip install --upgrade pip;\nRUN pip install --upgrade wheel;\n\nRUN pip install -r requirements.airflow.txt\nRUN pip install -r requirements.txt\n\nEXPOSE 8793 8786 8787\n`\n```\ndocker-compose.yml:\n```\n`version: '3.8'\nx-airflow-celery: &airflow-celery\n  image: local_image:latest\n  volumes:\n    -some_volume\n  env_file:\n    -some_env_file\n\nservices:\n  webserver:\n    <<: *airflow-celery\n    command: airflow webserver\n    restart: always\n    ports:\n      - 80:8080\n    healthcheck:\n      test: [ \"CMD-SHELL\", \"[ -f /opt/airflow/airflow-webserver.pid ]\" ]\n      interval: 10s\n      timeout: 30s\n      retries: 3\n\n  scheduler:\n    <<: *airflow-celery\n    command: airflow scheduler\n    restart: always\n    deploy:\n      replicas: 2\n\n  redis:\n    image: redis:6.0\n    command: redis-server --include /redis.conf\n    healthcheck:\n      test: [ \"CMD\", \"redis-cli\", \"ping\" ]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n    ports:\n      - 6379:6379\n    environment:\n      - REDIS_PORT=6379\n\n  worker:\n    <<: *airflow-celery\n    command: airflow celery worker\n    deploy:\n      replicas: 16\n\n  flower:\n    <<: *airflow-celery\n    command: airflow celery flower\n    ports:\n      - 5555:5555\n`\n```",
      "solution": "Sounds like you are heading in the right direction (with one general comment at the end though).\n\nYes, you need to push image to container registry and refer to it via public (or private if you authenticate) tag. The tag in this case is usally the `registry/name:tag`. For example you can see one of the CI images of Airlfow here: https://github.com/apache/airflow/pkgs/container/airflow%2Fmain%2Fci%2Fpython3.9  - the purpose is a bit different (we use it for our CI builds) but the mechanism is the same: you build it locally, tag with the \"registry/image:tag\" `docker build . --tag registry/image:tag` and run `docker push registry/image:tag`.\nThen whenever you refer to it from your docker compose, via `registry/image:tag`, docker compose/swarm will pull the right image. Just make sure you make unique TAGs when you build your images to know which image you push (and account for future images).\n\nEnv files should be fine and they will distribute across the instances, but locally mounted volumes will not. You either need to have some shared filesystem (like NFS, maybe EFS if you use AWS) where the DAGs are stored, or  use some other synchronization method to distribute the DAGs. It can be for example git-sync - which has very nice properties especially if you use Git to store the DAG files, or baking DAGs into the image (which requires to re-push images when they change). You can see different options explained in our Helm Chart https://airflow.apache.org/docs/helm-chart/stable/manage-dags-files.html\n\nYou cannot use `localhost` you need to set it to a specific host and make sure your broker URL is reachable from all instances. This can be done either by assining specific IP address/DNS name to your 'broker' instance and opening up the right ports in firewalls (make sure you control where you can reach thsoe ports from) and maybe even employing some load-balancing.\n\nI do not know DockerSwarm well enough how difficult or easy it is to set it all up, but nonestly, that's kind of a lot of work - it seems - to do it all manually.\nI would strongly, really strongly encourage you to use Kubernetes and the Helm Chart which Airlfow community develops: https://airflow.apache.org/docs/helm-chart/stable/index.html . There a lot of issues and necessary configurations either solved in the K8S (scaling, shared filesystems - PVs, networking and connectiviy, resource management etc. etc.) or by our Helm (Git-Sync side containers, broker configuration etc.)",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2021-11-26T09:54:46",
      "url": "https://stackoverflow.com/questions/70121761/scaling-airflow-with-a-celery-cluster-using-docker-swarm"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 66518524,
      "title": "Error query in Grafana/Prometheus with node-exporter in docker swarm mode",
      "problem": "I am struggling a query problem with Grafana `variable` query in Dashboard configuration. The query variable should return the number of nodes joined the swarm but it did not. In my case, I only have one swarm node but the variable in Grafana returns up to 5 nodes.  I relly don't understand what causes the error.\nHere is the situation: I set up docker swarm in my laptop as a manager, only my laptop with the swarm mode, no other nodes joined.\nI used the source from https://github.com/stefanprodan/swarmprom to monitor the host by `node-exporter`. I kept the `prometheus.yml` as original.\nwhen I executes the metric from `prometheus`, only one host returned. This is correct because I only had one node. You can see the figure below\n\nBut when I did the query in Grafana, Grafana returned 5 hosts. It was really strange here. I dont know why I got 5 hosts because I had only one swarm node.\n\nI did check the git repo again with play-with-docker, configured one manager node and 2 client nodes. Everything worked fine. The query in Grafana returned 3 hosts.\n\nHere is the query formula: `label_values(node_uname_info{job=\"node-exporter\"}, instance)`\nThank you so much for you supporting in advance.",
      "solution": "What you have faced is a consequence of ephemeral container nature, one of the challenges in monitoring container applications. Before we go into any solution options, let us see ...\nHow it did happen that Grafana shows more instances than there is.\nPrometheus is a time-series database. Once in a while it contacts its scraping targets and collects metrics. Those metrics are saved with a time stamp and a set of labels, one of which is the 'instance' label in question.\nThe instance label normally consists of an address (a host/domain name or an IP-address) and a port, that prometheus uses to scrape metrics. In this example instance address is an IP-address, because the list of targets is obtained through a DNS-server (`dns_sd_configs` in job definition).\nWhen you deployed the stack, docker created at least one container for each service, including node-exporter and prometheus. Soon after that prometheus started obtaining metrics from node-exporter instance, however after some time node-exporter container was recreated. Either you updated it, or killed it, or it's crashed - I can't know, but the key is - you had a new container. The new node-exporter container got a different IP-address and because of that metrics from the new instance received a different 'instance' label.\nRemember that prometheus is a time series database? You have not lost metrics from the instance that went offline, they're still in the database. It is just at this point you had started collecting node-exporter metrics with a different label set (new IP-address in the 'instance' label at least). When Grafana queries labels for you, it requests metrics from the period currently set on the dashboard. Since the period was 'today', you've seen instances that were present today. In other words when you request a list of possible instance values, you receive a list of values for the period without any filtering for currently active instances.\nGeneral solution.\nYou need to use some static label(s) for this task. An 'instance' or a 'pod_name' (K8s) labels are a poor choice if you don't like to see dead instances in the list. Pick a label that represents the thing or unit you want to watch and stick to it. Since node-exporter is to monitor node metrics, I think a host name label will do.\nIf you see no way in avoiding use of dynamic labels, you can use a short time range on the dashboard, so that the `label_values()` function does not return long dead labels. You'd like to set variable refresh option to 'On Time Range Change', so that you can use a short dashboard interval to see and pick currently active instances, and a long period for any other case.\nAn option for this particular problem.\nAs I said previously, using a host name label will be better in this case. The problem is - there is no such label in the metric in question. Checking swarmprom repo, I found that this node-exporter was made to expose a host name via `node_meta` label (here). So it is possible to map a host name to an instance(s) using chained variables.\nAnother problem is that this solution may require changes in panel queries. Since one host name can resolve into multiple instances, it is essential that panel queries use regex match for 'instance' label (that is `=~` instead of `=`).\nHere's how to do all this:\n\nCreate a new variable called 'hostname', set refresh option to 'On Time Range Change', and use this for the query field:\n\n```\n`label_values(node_meta, node_name)\n`\n```\nThis one will be used as a selector on the dashboard.\n\nUpdate the 'node' variable: set refresh option to 'On Time Range Change', enable 'Multi-value' and 'Add All option', replace query with this:\n\n```\n`label_values(node_meta{node_name=\"$hostname\"}, instance)\n`\n```\nThis will return a set of 'instance' labels matching the selected 'hostname'. If you select all and update panel queries to support multi-value instance label, you will be able to view metrics from all container instances associated with the selected host name.\n\nOpen dashboard JSON model and copy it in your favourite text editor. Replace all occurrences of `instance=` with `instance=~`, then copy-paste the edited model in Grafana.",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2021-03-07T17:04:28",
      "url": "https://stackoverflow.com/questions/66518524/error-query-in-grafana-prometheus-with-node-exporter-in-docker-swarm-mode"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 70995033,
      "title": "Running Apache (with .htaccess) App Behind Nginx Rerverse Proxy",
      "problem": "I've recently begun trying to Dockerize my services and I'm to the point of Dockerizing everything that already has an image built. Now I'm trying to build an image for facileManager (FM) which doesn't yet have one. I've got it mostly working but I'm having an issue when running it behind Nginx. FM is normally an apache-php app and doesn't include install instructions for Nginx. What I've noticed with my container/image is that it works ok when I connect directly to it through a published port but if I try to connect to it through Nginx it errors out complaining about the .htaccess file not working. I'm not an expert in either Apache or Nginx so I did my Googleing but didn't come up with much beyond Wordpress having a similar issue with it's \"pretty urls\" so I'm hoping someone here can give a hand.\nFirst here is the Github repo for the app: https://github.com/WillyXJ/facileManager/tree/ea159f5f6112727de8422c552aa05b6682aa4d79/server\nThe .htaccess file specifically is:\n```\n`\n    \n        Header set Cache-Control \"max-age=7200\"\n    \n    \n        Header set Cache-Control \"max-age=2592000\"\n    \n\nRewriteEngine On\n\nRewriteCond %{REQUEST_FILENAME} !-f\nRewriteCond %{REQUEST_FILENAME} !-d\nRewriteRule . index.php [L]\n\n`\n```\nThe exact error I'm getting is from here:\nhttps://github.com/WillyXJ/facileManager/blob/ea159f5f6112727de8422c552aa05b6682aa4d79/server/fm-includes/init.php#L153\n```\n`if (!defined('INSTALL')) {\n        if (@dns_get_record($_SERVER['SERVER_NAME'], DNS_A + DNS_AAAA)) {\n            $test_output = getPostData($GLOBALS['FM_URL'] . 'admin-accounts.php?verify', array('module_type' => 'CLIENT'));\n            $test_output = isSerialized($test_output) ? unserialize($test_output) : $test_output;\n            if (strpos($test_output, 'Account is not found.') === false) {\n                $message = sprintf(_('The required .htaccess file appears to not work with your Apache configuration which is required by %1s. '\n                        . 'AllowOverride None in your configuration may be blocking the use of .htaccess or %s is not resolvable.'),\n                        $fm_name, $_SERVER['SERVER_NAME']);\n                if ($single_check) {\n                    bailOut($message);\n                } else {\n                    $requirement_check .= displayProgress(_('Test Rewrites'), false, 'display', $message);\n                    $error = true;\n                }\n            } else {\n                if (!$single_check) $requirement_check .= displayProgress(_('Test Rewrites'), true, 'display');\n            }\n        }\n    }\n`\n```\nNginx config:\n```\n`server {\n        listen 80;\n\n        server_tokens off;\n\n        location /.well-known/acme-challenge/ {\n                root /var/www/certbot;\n        }\n\n        location / {\n                return 301 https://$host$request_uri;\n        }\n}\n\nserver {\n        listen 443 ssl;\n\n        server_name bound.example.com;\n\n        ssl_certificate /etc/nginx/ssl/live/bound.example.com/fullchain.pem;\n        ssl_certificate_key /etc/nginx/ssl/live/bound.example.com/privkey.pem;\n        include /etc/nginx/ssl/options-ssl-nginx.conf;\n        ssl_dhparam /etc/nginx/ssl/ssl-dhparams.pem;\n\n        location / {\n                proxy_pass http://FM.;     My Dockerfile/commands run: https://github.com/MeCJay12/facileManager-docker\n```\n`FROM php:7.4-apache\n\nENV TZ=UTC\nENV Version=4.2.0\nARG DEBIAN_FRONTEND=noninteractive\nWORKDIR /src\n\nRUN apt-get update \\\n    && apt-get -qqy install wget libldb-dev libldap2-dev tzdata \\\n    && wget http://www.facilemanager.com/download/facilemanager-complete-$Version.tar.gz \\\n    && tar -xvf facilemanager-complete-$Version.tar.gz \\\n    && mv facileManager/server/* /var/www/html/\n\nRUN ln -s /usr/lib/x86_64-linux-gnu/libldap.so /usr/lib/libldap.so \\\n    && docker-php-ext-install mysqli ldap \\\n    && a2enmod rewrite dump_io\n\nCOPY php.ini /usr/local/etc/php/php.ini\nRUN rm -r /src\n`\n```\nBonus question: When accessing this container/image directly through a published port, the images are all broken. I assume it's related since the .htaccess file includes references to images files.\nThanks in advance for the help!",
      "solution": "Found solution in https://github.com/WillyXJ/facileManager/issues/491. Needed to remove a check in the webapp that wasn't working quite right.",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2022-02-05T04:20:40",
      "url": "https://stackoverflow.com/questions/70995033/running-apache-with-htaccess-app-behind-nginx-rerverse-proxy"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 71133611,
      "title": "Mount OpenMediaVault NFS in docker-compose.yml volume for Docker Swarm",
      "problem": "I am trying to externalise my runtime data from my applications to be saved in OpenMediaVault shared folder.\nI was able to create shared folder and configure NFS or at least I think so. The config I see in `OMV/Services/NFS/Shares` is:\n```\n`Shared folder: NasFolder[on /dev/sda1, nas/]\nClient: 192.168.50.0/24\nPrivelage: Read/Write\nExtra options: subtree_check,insecure\n`\n```\nNow in that shared folder I have this structure(I checked it using windows SMB/CIFS config)\n```\n`\\\\nfs-ip\\NasFolder\n          |- mysql\n          |   \\- some my sql folders...\n          |- TEST.txt\n`\n```\nI want to use this `mysql` folder to store MariaDB runtime data(I know names are messed up I am in a middle of a migration to Maria...). And meaby create some other folders for other services. This is my config from `docker-compose.yml`:\n```\n`version: '3.2'\nservices:\n  mysqldb:\n    image: arm64v8/mariadb:latest\n    ports:\n      - 3306:3306\n    restart: on-failure:3\n    volumes:\n      - type: volume\n        source: nfs-volume\n        target: /mysql\n        volume:\n          nocopy: true\n    environment:\n      - MYSQL_ROOT_PASSWORD=my-secret-pw\n    command: --character-set-server=utf8 --collation-server=utf8_general_ci\n\nvolumes:\n  nfs-volume:\n    driver: local\n    driver_opts:\n      type: \"nfs\"\n      o: addr=192.168.50.70,nolock,soft,rw\n      device: \":/NasFolder\"\n`\n```\nNow when I run `docker stack deploy -c docker-compose.yml --with-registry-auth maprealm` on my manager node I get error on `maprealm_mysqldb.1` that looks like this:\n```\n`\"Err\": \"starting container failed: error while mounting volume '/var/lib/docker/volumes/maprealm_nfs-volume/_data': failed to mount local volume: mount :/NasFolder:/var/lib/docker/volumes/maprealm_nfs-volume/_data, data: addr=192.168.50.70,nolock,soft: permission denied\",\n`\n```\nI am pretty new to integration stuff. This is my home server and I just can't find good tutorials that 'get through my thick skull' how to configure those NFS paths and permissions or at least how can I debug it beside just getting this error. I know that `volumes.nfs-volume.driver_opts.device` is supposed to be a path but I am not sure what path should that be.\nI was trying to adapt config from here: https://gist.github.com/ruanbekker/4a9c0d250bce9f84482f2a788ce92131\nEdit1) Few additional details:\n\nDocker swarm has 3 nodes and only one node is manager with availability pause.\nOMV is running on a separet machine that is not a part of a cluster",
      "solution": "Ok so if someone would be looking for solution:\n\nOMV by default has `/export/` for NFS so volume needed to be updated. I needed to update volume for mysql and update `volumes.mysql-volume.driver_opts.device` to include that `/export/` prefix and I also added path to `mysql` folder to have volume for `mysqldb` service use only:\n\n```\n`volumes:\n  mysql-volume:\n    driver: local\n    driver_opts:\n      type: \"nfs\"\n      o: addr=192.168.50.70,nolock,soft,rw\n      device: \":/export/NasFolder/mysql\"\n`\n```\n\nAfter those changes there was need to update volume config on mysql/mariadb:\n\n```\n`  mysqldb:\n    image: arm64v8/mariadb:latest\n    ports:\n      - 3306:3306\n    restart: on-failure:3\n    volumes:\n      - type: volume\n        source: mysql-volume\n        target: /var/lib/mysql\n        volume:\n          nocopy: true\n    environment:\n      - MYSQL_ROOT_PASSWORD=my-secret-pw\n    command: --character-set-server=utf8 --collation-server=utf8_general_ci\n`\n```\n`mysqldb.volumes.source` points to name of your volume defined in step 1 - `mysql-volume`\n`mysqldb.volumes.target` is where inside container runtime data is stored. In mysql/mariadb databases runtime data is stored in `/var/lib/mysql` so you want to point to that and you can only use full path.\n\nSince I used default OMV config there were problems with permissions. So I updated `OMV/Services/NFS/Shares` to this:\n\n```\n`Shared folder: NasFolder[on /dev/sda1, nas/]\n#here you can see note 'The location of the files to share. The share will be accessible at /export/.'\nClient: 192.168.50.0/24\nPrivelage: Read/Write\nExtra options: rw,sync,no_root_squash,anonuid=1000,anongid=1000,no_acl\n`\n```",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2022-02-15T22:26:43",
      "url": "https://stackoverflow.com/questions/71133611/mount-openmediavault-nfs-in-docker-compose-yml-volume-for-docker-swarm"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 70982759,
      "title": "Traefik custom error middleware for HTTP status 502 doesn\u2019t work",
      "problem": "I would like to serve custom error pages for common HTTP error codes. I was following this tutorial. For this purpose, I created the following `docker-compose.yml` file to test the functionality. It consists of:\n\n`errorPageHandler` which is a nginx server which is able to return static HTML pages (`404.html` and `502.html`).\n\n`whoami1` which routes the request to port 80 of `traefik/whoami` image\n\n`whoami2` which routes the request to non-existent port 81 of `traefik/whoami` image which results in 502 - Bad Gateway\n\n`version: \"3.7\"\n\nservices:\n  errorPageHandler:\n    image: \"nginx:latest\"\n    networks:\n      - traefik-net\n    volumes:\n      - \"./ErrorPages:/usr/share/nginx/ErrorPages\"\n      - \"./default.conf:/etc/nginx/conf.d/default.conf\"\n    deploy:\n      replicas: 1\n      labels:\n        # enable Traefik for this service\n        - \"traefik.enable=true\"\n        - \"traefik.docker.network=traefik-net\"\n\n        # router (catches all requests with lowest possible priority)\n        - \"traefik.http.routers.error-router.rule=HostRegexp(`{catchall:.*}`)\"\n        - \"traefik.http.routers.error-router.priority=1\"\n        - \"traefik.http.routers.error-router.middlewares=error-pages-middleware\"\n\n        # middleware\n        - \"traefik.http.middlewares.error-pages-middleware.errors.status=400-599\"\n        - \"traefik.http.middlewares.error-pages-middleware.errors.service=error-pages-service\"\n        - \"traefik.http.middlewares.error-pages-middleware.errors.query=/{status}.html\"\n\n        # service\n        - \"traefik.http.services.error-pages-service.loadbalancer.server.port=80\"\n  \n  whoami1:\n    image: \"traefik/whoami\"\n    networks:\n      - traefik-net\n    deploy:\n      replicas: 1\n      labels:\n        # enable Traefik for this service\n        - \"traefik.enable=true\"\n        - \"traefik.docker.network=traefik-net\"\n\n        # router\n        - \"traefik.http.routers.whoami1.rule=HostRegexp(`whoami1.local`)\"\n        - \"traefik.http.routers.whoami1.service=whoami1\"\n        \n        # service\n        - \"traefik.http.services.whoami1.loadbalancer.server.port=80\"\n        - \"traefik.http.services.whoami1.loadbalancer.server.scheme=http\"\n        \n  whoami2:\n    image: \"traefik/whoami\"\n    networks:\n      - traefik-net\n    deploy:\n      replicas: 1\n      labels:\n        # enable Traefik for this service\n        - \"traefik.enable=true\"\n        - \"traefik.docker.network=traefik-net\"\n\n        # router\n        - \"traefik.http.routers.whoami2.rule=HostRegexp(`whoami2.local`)\"\n        - \"traefik.http.routers.whoami2.service=whoami2\"\n        \n        # service\n        - \"traefik.http.services.whoami2.loadbalancer.server.port=81\" # purposely wrong port\n        - \"traefik.http.services.whoami2.loadbalancer.server.scheme=http\"\n\nnetworks:\n  traefik-net:\n    external: true\n    name: traefik-net\n`\nFolder `ErrorPages` contains two static HTML files, `404.html` and `502.html`. Content of `default.conf` is:\n```\n`server {\n    listen       80;\n    server_name  localhost;\n\n    error_page  404    /404.html;\n    error_page  502    /502.html;\n\n    location / {\n        root  /usr/share/nginx/ErrorPages;\n        internal;\n    }\n}\n`\n```\nI added the following entries to my `hosts` file (`172.15.16.17` is IP of the server where Traefik and all of the mentioned services are deployed):\n```\n`172.15.16.17 error404.local\n172.15.16.17 whoami1.local\n172.15.16.17 whoami2.local\n`\n```\nMy observations:\n\nWhen I visit `http://error404.local`, Traefik routes the request to nginx and `404.html` is returned. This is expected behavior because `http://error404.local` doesn't match any of the routes defined in Traefik.\n\nWhen I visit `http://whoami1.local`, Traefik routes request to `whoami1` service and information about container is displayed on the page (expected behavior).\n\nWhen I visit `http://whoami2.local`, Traefik doesn't route request to nginx service, but it displays its default `Bad Gateway` page. Why doesn't it route the request to the nginx?",
      "solution": "The traefik routers (whoami1 and whoami2) needs to use the error middleware that you defined\nhttps://doc.traefik.io/traefik/middlewares/http/errorpages/\nTraefik error middleware docs\nSo add the following labels to their respective containers\n```\n`- \"traefik.http.whoami1.middlewares=error-pages-middleware\"\n\n- \"traefik.http.whoami2.middlewares=error-pages-middleware\"\n`\n```\nthe Traefik dashboard will show that the routes are using the middleware (refer image) One of my routes from my traefik dashboard\nAnother option would be to add the middleware to the entrypoint so that all routes use the error middleware (Add this to traefik container compose)\n```\n`command:\n  - \"--entrypoints.websecure.address=:443\"\n  - \"--entrypoints.websecure.http.middlewares=error-pages-middleware@docker\"\n`\n```",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2022-02-04T08:25:13",
      "url": "https://stackoverflow.com/questions/70982759/traefik-custom-error-middleware-for-http-status-502-doesn-t-work"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 70083932,
      "title": "Get full text for warning in docker service update",
      "problem": "When running the following command:\n```\n`docker service update captain-captain --force\n`\n```\nI am briefly seeing a warning:\n```\n`no suitable node (scheduling constraints not satisfied on 2 nodes; host-mo\u2026\".\n`\n```\n\nBut I can't see the full text to understand this properly.  Nor is there a task ID e.g. `mqo2k39bax94y6fiq7boxxtge` which I've seen in the past for similar warnings/errors, which I can inspect with `docker inspect mqo2k39bax94y6fiq7boxxtge`.\nThe warning does disappear after a short time and the update seems to complete OK, so it's clearly not fatal, but I want to understand a bit more about why it is showing in the first place.",
      "solution": "The key was to add `--detach` to unblock the terminal (so that it doesn't wait for the task to finish):\n```\n`docker service update captain-captain --force --detach \n`\n```\nThen quickly (while the truncated message would still be showing had the terminal no been unblocked:\n```\n`docker service ps captain-captain\n\nID             NAME                    IMAGE                      NODE         DESIRED STATE   CURRENT STATE           ERROR                              PORTS\n1ejhe98ozrdn   captain-captain.1       caprover/caprover:1.10.1                Ready           Pending 2 seconds ago   \"no suitable node (host-mode p\u2026\"   \no9y4cfwlsqy7    \\_ captain-captain.1   caprover/caprover:1.10.1   mercian-31   Shutdown        Running 2 seconds ago\n`\n```\nThen quickly grab the ID of the task showing the error, and inspect it before the error is resolved:\n```\n`docker inspect 1ejhe98ozrdn\n`\n```\nWithin that output you'll find the full error, in this case:\n```\n`\"Err\": \"no suitable node (scheduling constraints not satisfied on 2 nodes; host-mode port already in use on 1 node)\"\n`\n```\nQuite why docker can't just show the full error message in the first place, without having to just through these hoops, I'm not sure.\nAs an aside, docker seems not to be stopping the old instance before it schedules the new one even though we're definitely using `stop-first`.\nCredit for this answer goes here.",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-11-23T16:55:26",
      "url": "https://stackoverflow.com/questions/70083932/get-full-text-for-warning-in-docker-service-update"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 68529616,
      "title": "How to use SignalR on multiple servers?",
      "problem": "I have a chat app that I made with dotnet core, singalR, and react native. My chat is working well when I publish it on a single server. But when I get publish it in multiple servers by docker swarm. I get this error.\nUnable to connect to the server with any of the available transports. WebSockets failed: Error: There was an error with the transport.\nBy this error message, the app is just sometimes working normally. When I leave the page and return back it is not working again.\nI am using ubuntu server. I both aligned the versions of signalR on server and client. They are both using 5.0.3. I don't have proxy server in front of the app and I m using load balancing feature of docker swarm.\nConfigure Service\n```\n`var tokenKey = new SymmetricSecurityKey(Encoding.UTF8.GetBytes(Configuration[\"TokenKey\"]));\n        services.AddAuthentication(JwtBearerDefaults.AuthenticationScheme)\n            .AddJwtBearer(opt =>\n            {\n                opt.TokenValidationParameters = new TokenValidationParameters\n                {\n                    ValidateIssuerSigningKey = true,\n                    IssuerSigningKey = tokenKey,\n                    ValidateAudience = false, \n                    ValidateIssuer = false,\n                    ValidateLifetime = true,\n                    ClockSkew = TimeSpan.Zero\n                };\n                opt.Events = new JwtBearerEvents\n                {\n                    OnMessageReceived = context =>\n                    {\n                        var accessToken = context.Request.Query[\"access_token\"];\n                        var path = context.HttpContext.Request.Path;\n\n                        if (!string.IsNullOrEmpty(accessToken))\n                        {\n                            if (path.StartsWithSegments(\"/chat\")\n                            || path.StartsWithSegments(\"/dialog\"))\n                            {\n                                context.Token = accessToken;\n                            }\n                        }\n\n                        return Task.CompletedTask;\n                    }\n                };\n            });\n`\n```\nConfigure Void\n```\n`app.UseEndpoints(endpoints =>\n        {\n            endpoints.MapControllers();\n            endpoints.MapHub(\"/chat\", opt => { opt.Transports = HttpTransportType.WebSockets; });     \n            endpoints.MapHub(\"/dialog\", opt => { opt.Transports =  HttpTransportType.WebSockets; }); \n\n        });\n`\n```",
      "solution": "When scaling out SignalR to multiple servers, a shared data plane would be needed to manage distributed state, in addition to the network considerations.\nAs noted in the docs, Microsoft suggests either introducing a Redis backplane or delegating to their managed service, Azure SignalR.\n\nAn app that uses SignalR needs to keep track of all its connections,\nwhich creates problems for a server farm. Add a server, and it gets\nnew connections that the other servers don't know about.\n\nHaving used Azure SignalR, it's fairly straightforward to integrate with an ASP.NET Core app. You then have offloaded all the overhead of managing connections from your app.",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-07-26T14:13:25",
      "url": "https://stackoverflow.com/questions/68529616/how-to-use-signalr-on-multiple-servers"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 65745020,
      "title": "Docker container not created after stack deploy. Where can I find error logs?",
      "problem": "I have a single-node swarm. My stack has two services. I deployed like so:\n```\n`$ docker stack deploy -c /tmp/docker-compose.yml -c /tmp/docker-compose-prod.yml ide-controller\"\nCreating network ide-controller_default\nCreating service ide-controller_app\nCreating service ide-controller_traefik\n`\n```\nNo errors. However, according to `docker ps`, only one container is created. The `ide-controller_traefik` container was not created.\nWhen I check `docker stack services`, it says `0/1` for the traefik container:\n```\n`ID             NAME                     MODE         REPLICAS   IMAGE                                       PORTS\naz4n6brex4zi   ide-controller_app       replicated   1/1        boldidea.azurecr.io/ide/controller:latest\n1qp623hi431e   ide-controller_traefik   replicated   0/1        traefik:2.3.6                               *:80->80/tcp, *:443->443/tcp\n`\n```\nDocker service logs has nothing:\n```\n`$ docker service logs ide-controller_traefik -n 1000\n$\n`\n```\nThere are no `traefik` containers in `docker ps -a`, so I can't check logs:\n```\n`$ docker ps -a                                                                                                                                                                                                                                                          \nCONTAINER ID   IMAGE                                       COMMAND                  CREATED         STATUS         PORTS      NAMES                                                                                                                                                         \n922fdff58c25   boldidea.azurecr.io/ide/controller:latest   \"docker-entrypoint.s\u2026\"   3 minutes ago   Up 3 minutes   3000/tcp   ide-controller_app.1.py8jrtmufgsf3inhqxfgkzpep\n`\n```\nHow can I find out what went wrong or what is preventing the container from being created?",
      "solution": "`docker service ps ` has an error column that can expose errors encountered by libswarm trying to create containers, such as bad image names.\nOr, for a more detailed look, `docker service inspect ` has the current, and previous, service spec, as well as some root level nodes that will trace the state of the last operation and its message.",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2021-01-16T00:50:21",
      "url": "https://stackoverflow.com/questions/65745020/docker-container-not-created-after-stack-deploy-where-can-i-find-error-logs"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 77065725,
      "title": "Docker Swarm - Serve different stack service with NGINX",
      "problem": "I use Swarmpit to control Swarm nodes.\nThere are multiple stacks on Swarm (one for each application) and another stack with NGINX. All the stacks are attached to one `external` docker network named \"public\".\nThe web applications is a Laravel application (Octane) serverd on port 9000 (It is reachable from IP:9000).\nNGINX is reachable on port 80\nI need to serve all the apps through NGINX.\nWeb Application docker-compose.yml\n```\n`version: '3.3'\nservices:\n  laravel:\n    image: myimage:latest\n    extra_hosts:\n     - host.docker.internal:host-gateway\n    environment:\n      CHOKIDAR_USEPOLLING: 'true'\n      WWWUSER: '1000'\n      XDEBUG_CONFIG: client_host=host.docker.internal\n      XDEBUG_MODE: 'off'\n    ports:\n     - 5173:5173\n     - 9000:9000\n    networks:\n     - net\n     - public\n    logging:\n      driver: json-file\n  mongo:\n    image: mongo:latest\n    command:\n     - --quiet\n     - --logpath\n     - /dev/null\n    environment:\n      ME_CONFIG_MONGODB_ADMINPASSWORD: ...\n      ME_CONFIG_MONGODB_ADMINUSERNAME: ...\n      MONGO_INITDB_DATABASE: ...\n      MONGO_INITDB_ROOT_PASSWORD: ...\n      MONGO_INITDB_ROOT_USERNAME: ...\n    ports:\n     - 27017:27017\n    volumes:\n     - mongo:/data/db\n    networks:\n     - net\n    logging:\n      driver: json-file\n  mysql:\n    image: mysql:8\n    environment:\n      MYSQL_DATABASE: ...\n      MYSQL_PASSWORD: ...\n      MYSQL_ROOT_PASSWORD: ...\n      MYSQL_USER: ...\n    ports:\n     - 3306:3306\n    volumes:\n     - mysql:/var/lib/mysql\n    networks:\n     - net\n    logging:\n      driver: json-file\nnetworks:\n  net:\n    driver: overlay\n  public:\n    external: true\nvolumes:\n  mongo:\n    driver: local\n  mysql:\n    driver: local\n`\n```\nNGINX docker-compose.yml\n```\n`version: '3.3'\nservices:\n  webserver:\n    image: mycustomnginximage:latest\n    ports:\n     - 80:80\n     - 8080:8080\n     - 443:443\n    networks:\n     - global\n     - public\n    logging:\n      driver: json-file\nnetworks:\n  global:\n    driver: overlay\n  public:\n    external: true\n`\n```\nNGINX server conf\n```\n`map $http_upgrade $connection_upgrade {\n    default upgrade;\n    ''      close;\n}\n\nserver {\n    server_name mywebsite.dev *.mywebsite.dev;\n    server_tokens off;\n    root /home/forge/domain.com/public;\n\n    # SSL configuration\n    listen 443 ssl;\n    listen [::]:443 ssl;\n    ssl_certificate /etc/nginx/certificates/mywebsite.pem;\n    ssl_certificate_key /etc/nginx/certificates/mywebsite.key;\n\n    index index.php;\n\n    charset utf-8;\n\n    location /index.php {\n        try_files /not_exists @octane;\n    }\n\n    location / {\n        try_files $uri $uri/ @octane;\n    }\n\n    location = /favicon.ico { access_log off; log_not_found off; }\n    location = /robots.txt  { access_log off; log_not_found off; }\n\n    access_log off;\n    error_log  /var/log/nginx/domain.com-error.log error;\n\n    error_page 404 /index.php;\n\n    location @octane {\n        set $suffix \"\";\n\n        if ($uri = /index.php) {\n            set $suffix ?$query_string;\n        }\n\n        proxy_http_version 1.1;\n        proxy_set_header Host $http_host;\n        proxy_set_header Scheme $scheme;\n        proxy_set_header SERVER_PORT $server_port;\n        proxy_set_header REMOTE_ADDR $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection $connection_upgrade;\n\n        proxy_pass http://my_swarm_service_name:9000;\n    }\n}\n`\n```\nThe result is that the application is reachable directly through `IP:9000` but not through `https://mywebsite.dev`  As if nginx couldn't divert traffic to the service.\n\nI've tryied with:\n```\n`proxy_pass http://my_swarm_service_name:9000;\nproxy_pass http://localhost:9000;\nproxy_pass http://docker.service.internal.ip:9000;\n`\n```\nHow should i use the `proxy_pass` directive? Or there are something else wrong?",
      "solution": "The service name is `laravel` so, to access the service vip you could use the following hostnames:\n```\n`# Each service is available using its plain service name\nproxy_pass http://laravel:9000\n# If you have multiple laravels in multiple stacks, you can name the specific one you\n# want. Assuming you have deployed a stack called appstack1\nproxy_pass http://appstack1_laravel:9000\n# Docker also allows you to specify which network, this can help resolve ambiguities,\n#i.e. prevent picking up an unwanted other laravel registered on the global network.\nproxy_pass http://laravel.public:9000\n`\n```\nNone of this is going to work out the box however until you add the most crucial piece: Nginx does not do dns resolution at runtime of docker service names unless you tell it to.\nYou need to configure the resolver Nginx uses to point to dockers container dns:\n```\n`server {\n  resolver 127.0.0.11;\n}\n`\n```",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2023-09-08T11:30:55",
      "url": "https://stackoverflow.com/questions/77065725/docker-swarm-serve-different-stack-service-with-nginx"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 73467753,
      "title": "All docker stack are restarting automatically",
      "problem": "I have a multi-services environment that is hosted with docker swarm. There are multiple stacks that are created. All the docker containers which are running have an inbuild Spring Boot application. The issue is coming that all my stacks get restarted on their own. Now I know that in compose file I have mentioned that restart_policy  as on failure. Hence it auto restarted. The issue comes that when services are restarted, I get errors from a particular service and this breaks everything.\nI am not able to figure out what actually happens.\nI did quite a lot of research and found out about these things.\n\nDocker daemon is not restarted. I double-checked this with the uptime of the docker daemon.\nI checked the docker service ps  and there I can see service showing shutdown and starting. No other information.\nI checked the docker service logs  but no error in there too.\nI checked for resource crunch. I can assure you that there was quite a good resource available at the host as well as each container level.\nCan someone help where exactly to find logs for this even? Any other thoughts on this?\n\nMy host is actually a VM hosted on VMWare Vcenter.",
      "solution": "After a lot of research and going through all docker logs, I could not find the solution. Later on, I discovered that there was a memory snapshot taken for backup every 24 hours.\nHere is what I observe:\n\nWhenever we take a snapshot, all docker services running on the host restart automatically. There will be no errors in that but they will just restart gracefully.\nI found some questions already having this problem with VMware snapshots.\nAs far as I know, when we take a snapshot, it points to a different memory location and saves the previous one. I am not able to find why it's happening but yes Root cause of the problem was this. If anyone is a VMWare snapshots expert, please let us know.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2022-08-24T07:17:39",
      "url": "https://stackoverflow.com/questions/73467753/all-docker-stack-are-restarting-automatically"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 71425384,
      "title": "ActiveMQ Artemis cluster on Docker Swarm",
      "problem": "I am trying to setup simple cluster of 2 ActiveMQ Artemis brokers within Docker Swarm environment.\nStack configuration\n```\n`version: \"3.7\"\n\nservices:\n    broker_1:\n        image: broker-local\n        build:\n            context: ./activemq\n            dockerfile: Dockerfile-centos\n        ports:\n            - \"8161:8161\" # Embedded web server\n            - \"61616:61616\" # Main Artemis acceptor\n        networks:\n            artemis-cluster-network:\n        environment:\n            EXTRA_ARGS: \"--http-host 0.0.0.0 --relax-jolokia\"\n            ANONYMOUS_LOGIN: \"true\"\n        configs:\n            -   source: broker_1_config\n                target: /run/config/broker.xml\n\n    broker_2:\n        image: broker-local\n        depends_on:\n            - \"broker_1\"\n        build:\n            context: ./activemq\n            dockerfile: Dockerfile-centos\n        ports:\n            - \"8162:8161\" # Embedded web server\n            - \"61617:61617\" # Main Artemis acceptor\n        networks:\n            artemis-cluster-network:\n        environment:\n            EXTRA_ARGS: \"--http-host 0.0.0.0 --relax-jolokia\"\n        configs:\n            -   source: broker_2_config\n                target: /run/config/broker.xml\n\nnetworks:\n    artemis-cluster-network:\n        driver: overlay\n        attachable: true\n\nconfigs:\n    broker_1_config:\n        file: ./broker_1.xml\n    broker_2_config:\n        file: ./broker_2.xml\n`\n```\nBroker_1 configuration\n`   \n      \n\n         broker_1\n\n         ./data/bindings\n\n         ./data/journal\n\n         ./data/largemessages\n\n         ./data/paging\n\n         \n\n         \n            tcp://0.0.0.0:61616\n            \n            tcp://broker_2:61617\n         \n\n         \n         \n            tcp://0.0.0.0:61616\n         \n\n         \n            \n               netty-connector\n               500\n               true\n               STRICT\n               5\n               \n                  server2-connector\n               \n            \n         \n\n         \n\n         \n            \n            \n               \n               \n               \n               \n               \n               \n            \n         \n\n         \n            \n               \n                  \n               \n            \n         \n\n         \n            \n               \n               \n               \n               \n               \n               \n               \n            \n         \n      \n   \n`\nBroker_2 configuration\n`  \n     \n\n        broker_2\n\n        ./data/bindings\n\n        ./data/journal\n\n        ./data/largemessages\n\n        ./data/paging\n\n        \n        \n           tcp://0.0.0.0:61617\n           \n           tcp://broker_1:61616\n        \n\n        \n        \n           tcp://0.0.0.0:61617\n        \n\n        \n           \n              netty-connector\n              500\n              true\n              STRICT\n              5\n              \n                 server1-connector\n              \n           \n        \n\n        \n\n        \n           \n           \n              \n              \n              \n              \n              \n              \n           \n        \n\n        \n           \n              \n                 \n              \n           \n        \n\n        \n           \n              \n              \n              \n              \n              \n              \n              \n           \n        \n     \n  \n`\nAfter running both services nothing happens. Nothing interesting is logged.\nI tried UDP discovery approach but found out that is not possible on Docker Overlay Network.\nThen I tried JGroups approach which did not work also. There were attempts to establish connection but every attempt timed out. And attempts were made to hosts represented by container ID.\nThis approach could be interesting but shared ping directory which is used by JGRoups is not valid for Swarm approach. On local machine it was not problem to share it between two containers.\nAlso I am using Fiddler so names like broker_1 are translated to localhost.",
      "solution": "The `connector` named \"netty-connector\" shouldn't use `0.0.0.0`. This is the address that will be sent to other nodes in the cluster to tell them how to connect back to the node who sent it. The address `0.0.0.0` will be meaningless in that context. It needs to be the actual IP address or hostname where the broker is listening for network connections.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2022-03-10T14:59:49",
      "url": "https://stackoverflow.com/questions/71425384/activemq-artemis-cluster-on-docker-swarm"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 68475767,
      "title": "Docker Swarm - Requests fail to reach a service on a different node",
      "problem": "I've setup a Docker Swarm with Traefik v2 as the reverse proxy, and have been able to access the dashboard with no issues.\nI am having an issue where I cannot get a response from any service that runs on a different node to the node Traefik is running on. I'm been testing and researching and presuming it's a network issue of some type.\nI've done some quick testing with a empty Nginx image and was able to deploy another stack and get a response if the image was on the same node. Other stacks on the swarm which deploy across multiple nodes (but not including the Traefik node) are able to communicate to each other without issues).\nHere is the test stack to provide some context of what I was using.\n`version: '3.8'\n\nservices:\n    test:\n        image: nginx:latest\n        deploy:\n            replicas: 1\n            placement:\n                constraints:\n                    - node.role==worker\n            labels:\n                - \"traefik.enable=true\"\n                - \"traefik.docker.network=uccser-dev-public\"\n                - \"traefik.http.services.test.loadbalancer.server.port=80\"\n                - \"traefik.http.routers.test.service=test\"\n                - \"traefik.http.routers.test.rule=Host(`TEST DOMAIN`) && PathPrefix(`/test`)\"\n                - \"traefik.http.routers.test.entryPoints=web\"\n        networks:\n            - uccser-dev-public\n\nnetworks:\n  uccser-dev-public:\n    external: true\n`\nThe `uccser-dev-public` network is an overlay network across all nodes, with no encryption.\nIf I added a constraint to specify the Traefik node, then the requests worked with no issues. However, if I switched it to a different node, I get the Traefik 404 page.\nThe Traefik dashboard is showing it sees the service.\nHowever the access logs show the following:\n```\n`proxy_traefik.1.6fbx58k4n3fj@SWARM_NODE    | IP_ADDRESS - - [21/Jul/2021:09:03:02 +0000] \"GET / HTTP/2.0\" - - \"-\" \"-\" 1430 \"-\" \"-\" 0ms\n`\n```\nIt's just blank, and I don't know where to proceed from here. The normal log shows no errors that I can see.\nTraefik stack file:\n`version: '3.8'\n\nx-default-opts:\n  &default-opts\n  logging:\n    options:\n      max-size: '1m'\n      max-file: '3'\n\nservices:\n  # Custom proxy to secure docker socket for Traefik\n  docker-socket:\n    \nAny ideas?",
      "solution": "Further testing showed other services were working on different nodes, so figured it must be an issue with my application. Turns out my Django application still had a bunch of settings configured for it's previous hosting location regarding HTTPS. As it wasn't passing the required settings it had denied the requests before the were processed. I needed to have the logging level for gunicorn (WSGI) lower to see more information too.\nIn summary, Traefik and Swarm were fine.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-07-21T21:58:33",
      "url": "https://stackoverflow.com/questions/68475767/docker-swarm-requests-fail-to-reach-a-service-on-a-different-node"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 66345888,
      "title": "How to create service in Docker Swarm using prebuilt local image",
      "problem": "I have created 3 nodes, 1 as manager and other 2 as workers in `Docker Swarm` as shown below\n\nNow when I try to create service (on manager node) using local image `app1:latest` I get an error (mentioned below).\n```\n`$ sudo docker service create --name app1 -p 5001:5000 app1:latest                                                                                                                         \nimage app1:latest could not be accessed on a registry to record\nits digest. Each node will access app1:latest independently,\npossibly leading to different nodes running different\nversions of the image.\n\ni6s8hyzzuxx35enl2j0xd3ef9\noverall progress: 0 out of 1 tasks \n1/1: No such image: app1:latest \n`\n```\nOn the other hand if I use public images from `Docker Hub` (for example `nginx` image) then I can easily create the service.\nHow can I use prebuilt local image to create the service ?\nBelow is the `Dockerfile` and `app.py` file used to build the image `app1:latest`\nDockerfile\n```\n`FROM ubuntu:18.04\n\nRUN apt-get update \\\n    && apt-get install -y apt-utils \\\n    python3.6 \\\n    python3-pip\n\nWORKDIR /app\nCOPY . /app\n\nRUN pip3 install -r requirements.txt\n\nENTRYPOINT [\"python3\"]\nCMD [\"app.py\"]\n`\n```\napp.py\n```\n`from flask import Flask\nfrom flask import jsonify\n\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    return jsonify('App #1')\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', debug=True)\n`\n```\nI have followed some articles and video tutorials about `Docker Swarm` but they all are using public image from Docker Hub to create the service",
      "solution": "Unless you \"push\" your image to a docker registry (either docker hub or a private one), the image you build only exists on your local machine (and not accessible to the swarm nodes).\nSo, you either need to push to docker hub, or run your own registry. In the latter case, make sure the registry host is reachable from swarm nodes.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-02-24T07:42:09",
      "url": "https://stackoverflow.com/questions/66345888/how-to-create-service-in-docker-swarm-using-prebuilt-local-image"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 75249957,
      "title": "Migrate docker-compose to a single node docker-swarm cluster",
      "problem": "At the moment I have implemented a flask application, connected with mysql database, and the entire implementation is running on a single webserver.\nIn order to avoid exposing my app publicly, I am running it on the localhost interface of the server, and I am only exposing the public interface (port 443), via a haproxy that redirects the traffic to localhost interface.\nThe configuration of docker-compose and haproxy can be found below\ndocker-compose:\n```\n`version: '3.1'\n\nservices:\n\n  db:\n    image: mysql:latest\n    volumes:\n      - mysql-volume:/var/lib/mysql\n    container_name: mysql\n    ports:\n      - 127.0.0.1:3306:3306\n    environment:\n            MYSQL_ROOT_PASSWORD: xxxxxx\n\n  app:\n    #environment:\n    #  - ENVIRONMENT=stage\n    #  - BUILD_DATETIME=$(date +'%Y-%m-%d %H:%M:%S')\n    build:\n      context: .\n      dockerfile: Dockerfile\n      #labels:\n      #  - \"build_datetime=${BUILD_DATETIME}\"\n    container_name: stage_backend\n    ports:\n      - 127.0.0.1:5000:5000\n\nvolumes:\n  mysql-volume:\n    driver: local\n`\n```\nsample haproxy configuration:\n```\n`global\n        log /dev/log    local0\n        log /dev/log    local1 notice\n        # Default SSL material locations\n        ca-base /etc/ssl/certs\n        crt-base /etc/ssl/private\ndefaults\n        log     global\n        mode    http\n        option  httplog\n        option  dontlognull\n        timeout connect 10s\n        timeout client  30s \n        timeout server  30s\n\nfrontend test\n        bind *:80\n        bind *:443 ssl crt /etc/letsencrypt/live/testdomain.com/haproxy.pem alpn h2,http/1.1\n        redirect scheme https if !{ ssl_fc }\n        mode http\n        acl domain_testdomain hdr_beg(host) -i testdomain.com\n        use_backend web_servers if domain_testdomain\n\nbackend web_servers\n        timeout connect 10s\n        timeout server 100s\n        balance roundrobin\n        mode http\n        server test_server 127.0.0.1:5000 check\n\n`\n```\nSo haproxy is running on the public interface as a service via systemd (not containerized) and containers are running on localhost.\nThis is going to become a production setup soon, so I want to deploy a single node docker swarm cluster, within that server only, as docker swarm implementation is more safe on a production environment.\nMy question is how can I deploy that on docker swarm.\n\nDoes it make sense to leave haproxy as a systemd service and somehow to make it forward requests to the docker swarm cluster?\n\nIs it easier/better implementation, to also containerize the haproxy and put it inside the cluster as a docker-compose service?\n\nIf I follow the second approach, how can I make it run on a different interface than the application (`haproxy --> public, flask & db --> localhost`)\nAgain, I am talking about a single server here, so this is why I am trying to separate the network interfaces and only expose haproxy on 443 on the public interface.\nIdeally I didn't want to change from haproxy to nginx reverse proxy, as I am familiar with it and how ssl termination exactly work there, but I am open to hear any other implementation that makes more sense.",
      "solution": "You seem to be overthinking things, and in the process throwing away security features that docker offers.\nfirst off, docker gives you private networking out the box in both compose and swarm modes. an implicit network called `_default` is created and services are attached to it, and DNS resolution is setup in each container to resolve each service name.\nSo, assuming your app and db don't explicitly declare any networks, then the following implicit declarations apply, and your app can connect to the db using the connection string `mysql://db:3306` directly.\nThe db container does not need to either publish, or try and protect, access to this port, only other containers attached to the [stack_]default network will have access.\n```\n`networks:\n  default: # implicit\n\nservices:\n  app:\n    networks:\n      default: # implicit\n    environment:\n      MYSQL: mysql://db:3306 #\n\n  db:\n    networks:\n      default: # implicit\n`\n```\nAt this point, its your choice to run HAProxy as a service or not. Personally I would (do). It is handy in swarm to have a single service that handles :80 and :443 ingress, does offloading, and then uses docker networks to direct traffic to other services on whatever service:port's handle those connections.\nI use Traefik rather than HAProxy as it can use service labels to route traffic dynamically, but either way, having HAProxy as a service means, if you continue to use that, you can more easily deploy HAProxy config updates.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2023-01-26T18:58:07",
      "url": "https://stackoverflow.com/questions/75249957/migrate-docker-compose-to-a-single-node-docker-swarm-cluster"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 69381574,
      "title": "How to get real client ip behind docker load balancer using plain tcp?",
      "problem": "What I want\nI want to get client's IP in my TCP server and use it to whitelist/bind actions, detect regions (for language and currency), etc.\nHow I approached it\nI'm using TornadoWeb framework for python to set up my tcp server.\nIt contains `TCPServer.handler_stream(stream: IOStream, address: tuple[str, int])`\nfrom where i can get a hand on client's IP address. It's all good when running with host network (i.e. exposing my ports directly)\nProblem\nIf launched multiple tasks (containers) in docker swarm, and therefore use docker's load balancer, client's IP address gets replaced with docker's inner one.\nQuestion\nHow do i configure docker swarm or another load balancer to somehow send real client's ip to my server?\nI'm not particularly bound to one piece of software or another, nor do i know which is better, the only thing I wouldn't consider - is using Kubernetes (not now at least), so any suggestion on software rather than configuration are also welcome!\nClient and server can be altered, so other techniques I could use regardless of protocol itself will also be useful. However, something like using HTTPs requests to gather IP address via `X-Forwarded-For` or `X-Real-IP` headers is possible, but i'd like to refrain myself from it.",
      "solution": "How do i configure docker swarm or another load balancer to somehow send real client's ip to my server?\n\nDocker has already an open issue for that, see https://github.com/docker/roadmap/issues/157 . It is currently not possible to do any configuration to do that.\nAs you already mentioned earlier, you will have to use some custom way of handling that until the issue is closed.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-09-29T20:20:46",
      "url": "https://stackoverflow.com/questions/69381574/how-to-get-real-client-ip-behind-docker-load-balancer-using-plain-tcp"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 67160831,
      "title": "Best way to handle Canary deployments with Docker Swarm",
      "problem": "One best-practice for microservice deployments is \"Canary Deployments\" (to deploy a new version of an app alongside an existing one and to get your API gateway/proxy to send a small amount of traffic to the new version until you are happy it is stable, at which point you eventually increase it to 100% and delete the old service version.)\nIn order to do this in Docker Swarm, it seems to me that I would need to use the version number in the name of the service to allow this side-by-side deployment e.g. `myapp_1_0_0` and `myapp_1_0_1`\nThe only problem with this seems to be that this will affect dns lookups both between services and also between proxy/gateway and service, which would mean that there might be a lot of complexity/api calls etc. to ensure settings are updated correctly to avoid downtime. It would be nicer to have a single dns name like `myapp` so that everything always looks for myapp and a proxy routes accordingly.\nIs the only way around this to do a lot of work in the proxy and then get all the services to use the proxy for dns lookups and then stick with the versioned names? I looked at Envoy so would it make most sense to use something like that in front of the Swarm? Is there a document that anyone knows about that describes this setup?",
      "solution": "As there is no native way to achieve canary in docker-swarm, I tried a custom approach with envoy and Envoy-Pilot.\nHere is the step by steps to flow: https://github.com/dineshba/docker-swarm-canary\nThis approach will introduce:\n\n1 envoy proxy container per application (as envoy is lightweight, should not be much overhead)\nVery small latency as all network will go via envoy proxy (delay(approx): 0.004 seconds (4ms))\n\nHope this gives you idea of how canary can be achieved in docker-swarm",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-04-19T13:08:31",
      "url": "https://stackoverflow.com/questions/67160831/best-way-to-handle-canary-deployments-with-docker-swarm"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 73588464,
      "title": "minikube failed to start : Exiting due to HOST_JUJU_LOCK_PERMISSION",
      "problem": "i wanted to install minikube and after the start command a got the following error text :\n```\n`\ud83d\ude04  minikube v1.26.1 on Ubuntu 22.04\n\u2757  minikube skips various validations when --force is supplied; this may lead to unexpected behavior\n\u2728  Using the docker driver based on existing profile\n\ud83d\uded1  The \"docker\" driver should not be used with root privileges. If you wish to continue as root, use --force.\n\ud83d\udca1  If you are running minikube within a VM, consider using --driver=none:\n\ud83d\udcd8    https://minikube.sigs.k8s.io/docs/reference/drivers/none/\n\ud83d\udca1  Tip: To remove this root owned cluster, run: sudo minikube delete\n\ud83d\udc4d  Starting control plane node minikube in cluster minikube\n\ud83d\ude9c  Pulling base image ...\n\u270b  Stopping node \"minikube\"  ...\n\ud83d\uded1  Powering off \"minikube\" via SSH ...\n\ud83d\udd25  Deleting \"minikube\" in docker ...\n\ud83e\udd26  StartHost failed, but will try again: boot lock: unable to open /tmp/juju-mke11f63b5835bf422927bf558fccac7a21a838f: permission denied\n\ud83d\ude3f  Failed to start docker container. Running \"minikube delete\" may fix it: boot lock: unable to open /tmp/juju-mke11f63b5835bf422927bf558fccac7a21a838f: permission denied\n\n\u274c  Exiting due to HOST_JUJU_LOCK_PERMISSION: Failed to start host: boot lock: unable to open /tmp/juju-mke11f63b5835bf422927bf558fccac7a21a838f: permission denied\n\ud83d\udca1  Suggestion: Run 'sudo sysctl fs.protected_regular=0', or try a driver which does not require root, such as '--driver=docker'\n\ud83c\udf7f  Related issue: https://github.com/kubernetes/minikube/issues/6391\n`\n```",
      "solution": "i tried this command and it worked for me :\n```\n`minikube delete && minikube start\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-09-03T00:40:54",
      "url": "https://stackoverflow.com/questions/73588464/minikube-failed-to-start-exiting-due-to-host-juju-lock-permission"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 69811682,
      "title": "How to properly configure HAProxy in Docker Swarm to automatically route traffic to replicated services (via SSL)?",
      "problem": "I'm trying to deploy a Docker Swarm of three host nodes with a single replicated service and put an HAProxy in front of it. I want the clients to be able to connect via SSL.\nMy `docker-compose.yml`:\n`version: '3.9'\n\nservices:\n  proxy:\n    image: haproxy\n    ports:\n      - 443:8080\n    volumes:\n      - haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg\n    deploy:\n      placement:\n        constraints: [node.role == manager]\n    networks:\n      - servers-network\n  node-server:\n    image: glusk/hackathon-2021:latest\n    ports:\n      - 8080:8080\n    command: npm run server\n    deploy:\n      mode: replicated\n      replicas: 2\n    networks:\n      - servers-network\nnetworks:\n  servers-network:\n    driver: overlay\n`\nMy `haproxy.cfg` (based on the official example):\n`# Simple configuration for an HTTP proxy listening on port 80 on all\n# interfaces and forwarding requests to a single backend \"servers\" with a\n# single server \"server1\" listening on 127.0.0.1:8000\nglobal\n    daemon\n    maxconn 256\n\ndefaults\n    mode http\n    timeout connect 5000ms\n    timeout client 50000ms\n    timeout server 50000ms\n\nfrontend http-in\n    bind *:80\n    default_backend servers\n\nbackend servers\n    server server1 127.0.0.1:8000 maxconn 32\n`\nMy hosts are Lightsail VPS Ubuntu instances and share the same private network.\n\n`node-service` runs each https server task inside its own container on: `0.0.0.0:8080`.\nThe way I'm trying to make this work at the moment is to `ssh` into the manager node (which also has a static and public IP), copy over my configuration files from above, and run:\n`docker stack deploy --compose-file=docker-compose.yml hackathon-2021\n`\nbut it doesn't work.",
      "solution": "Well, first of all and regarding SSL (since it's the first thing that you mention) you need to configure it using the certificate and listen on the port `443`, not port `80`.\nWith that modification, your Proxy configuration would already change to:\n```\n`global\n    daemon\n    maxconn 256\n\ndefaults\n    mode http\n    timeout connect 5000ms\n    timeout client 50000ms\n    timeout server 50000ms\n\nfrontend http-in\n    bind *:80\n    default_backend servers\n\nfrontend https-in\n    bind *:443 ssl crt /etc/ssl/certs/hackaton2021.pem\n    default_backend servers\n`\n```\nThat would be a really simplified configuration for allowing SSL connection.\n\nNow, let's go for the access to the different services.\nFirst of all, you cannot access to the service on `localhost`, actually you shouldn't even expose the ports of the services you have to the host. The reason? That you already have those applications in the same network than the `haproxy`, so the ideal would be to take advantage of the Docker DNS to access directly to them\nIn order to do this, first we need to be able to resolve the service names. For that you need to add the following section to your configuration:\n```\n`resolvers docker\n    nameserver dns1 127.0.0.11:53\n    resolve_retries 3\n    timeout resolve 1s\n    timeout retry   1s\n    hold other      10s\n    hold refused    10s\n    hold nx         10s\n    hold timeout    10s\n    hold valid      10s\n    hold obsolete   10s\n`\n```\nThe Docker Swarm DNS service is always available at `127.0.0.11`.\nNow to your previous existent configuration, we would have to add the server but using the service-name discovery:\n```\n`backend servers\n    balance roundrobin\n    server-template node- 2 node-server:8080 check resolvers docker init-addr libc,none\n`\n```\nIf you check what we are doing, we are creating a server for each one of the discovered containers in the Swarm within the `node-server` service (so the replicas) and we will create those adding the prefix `node-` to each one of them.\nBasically, that would be the equivalent to get the actual IPs of each of the replicas and add them stacked as a basic `server` configuration.\n\nFor deployment, you also have some errors, since we aren't interested into actually expose the `node-server` ports to the host, but to create the two replicas and use HAProxy for the networking.\nFor that, we should use the following Docker Compose:\n```\n`version: '3.9'\n\nservices:\n  proxy:\n    image: haproxy\n    ports:\n      - 80:80\n      - 443:443\n    volumes:\n      - hackaton2021.pem:/etc/ssl/certs/hackaton2021.pem\n      - haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg\n    deploy:\n      placement:\n        constraints: [node.role == manager]\n\n  node-server:\n    image: glusk/hackathon-2021:latest\n    command: npm run server\n    deploy:\n      mode: replicated\n      replicas: 2\n`\n```\nRemember to copy your `haproxy.cfg` and the self-signed (or real) certificate for your application to the instance before deploying the Stack.\nAlso, when you create that stack it will automatically create a network with the name `-default`, so you don't need to define a network just for connecting both services.",
      "question_score": 1,
      "answer_score": 6,
      "created_at": "2021-11-02T14:56:29",
      "url": "https://stackoverflow.com/questions/69811682/how-to-properly-configure-haproxy-in-docker-swarm-to-automatically-route-traffic"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 69916390,
      "title": "&#39;docker service logs&#39; overwriting and unstable after &#39;docker service update&#39; in swarm mode",
      "problem": "My docker services logs are usually working fine (printing in chronological order) but,\nafter performing:\n`docker service update --force --image myimageregistry:mytag myservice \n`\nif I request logs with:\n`docker service logs myservice\n`\nthen I get some logs but many lines are missing and last lines don't correspond to the most recent actions. It seems as if older logs got written on top of newer ones and thus the final display is very messy.\nBesides, if I keep an other terminal open following logs during the update, logs in that terminal are fine, but the issue is the same if I request them afterwards.",
      "solution": "When you request service logs `docker service logs $service` docker goes and streams all the logs from all the containers that are still linked to the service via tasks, running or stopped.\nDocker doesn't try to cleanup these logs in any way so multiple tasks logs can all end up interleaved and in the wrong order.\nIf you specifically want the logs for just the current process you need to first enumerate the tasks associated with the service and then list the logs for one of the tasks:\n```\n`$ docker service ps traefik_traefik\nID             NAME                                            IMAGE          NODE      DESIRED STATE   CURRENT STATE           ERROR                              PORTS\npwabbqdyucc3   traefik_traefik.n6fz7h5d122hads1rqwmmhgmw       traefik:v2.3   ...\n\n$ docker service logs pwabb\n...\n`\n```",
      "question_score": 1,
      "answer_score": 4,
      "created_at": "2021-11-10T16:56:38",
      "url": "https://stackoverflow.com/questions/69916390/docker-service-logs-overwriting-and-unstable-after-docker-service-update-in"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 77147077,
      "title": "How many services can I run in Docker swarm?",
      "problem": "I am running very lightweight Docker images in a Docker swarm environment. For this I forward a random port to the destination port inside the container. Here is an example output:\n```\n`y0xxxjj8kvi3   foo-test-1 1/1   strm/helloworld-http:latest  *:30001->80/tcp\n37f4czmipex0   foo-test-2 1/1   strm/helloworld-http:latest  *:30002->80/tcp\nqmzjlxt95n8r   foo-test-3 1/1   strm/helloworld-http:latest  *:30003->80/tcp\n...\n`\n```\nAs you can see, the more services I deploy, the more ports are being used. Does this mean the number of services is limited by the number of ports available to the network stack? Is there another network mode that is more suitable?\nCurrently the routing is done by the Docker swarm routing, but I have a load balancer ready that could forward the calls to the containers directly using their virtual IP address, but I am not sure if that solves the problem. Which network mode would be best fitting for this?\nThanks!\nEdit: I am aware that Kubernetes might be the better fit here given the limitations, but I am still curious to solve this with Docker swarm.",
      "solution": "Realistically\u00a0the exposed ports are not an issue as in any production setup you would run services behind reverse proxies. Traefik, nginx, caddy, haproxy etc. All of these can listen on :80 or :443 and then route traffic to the appropriate service over docker networks.\nThis means, you need to examine the limitations of docker networks. In a default install, docker swarm allocates overlay networks as /24, giving essentially 254 IPs to allocate to services. In endpoint_mode vip, services allocate an IP for a vip AND a IP for each task, in endpoint_mode dnsrr, only an IP per task container. Unless you are careful, this can limit you to 127 service attachments per network.\nYou can make overlay networks larger but the overlay network driver references moby/moby#44973 which limits you to increasing the overlay networks to 1024 (/22) in size. (The Swarm Networking page mentions an older issue moby/moby#30820 which has been fixed)\nSo, your limitation is 500-1000 services per reverse proxy network. However this does not apply any kind of hard limit as you can attach a reverse proxy to multiple networks. As each network is modelled as an interface being attached I can't even hazard a guess as to what that limit is.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2023-09-21T05:48:28",
      "url": "https://stackoverflow.com/questions/77147077/how-many-services-can-i-run-in-docker-swarm"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 69761617,
      "title": "Docker Swarm Access Container to Container on Published Port?",
      "problem": "I've seen at least 10 questions like this, but couldn't find an answer.\nI'm moving from Rancher to Docker Swarm.\nI used docker-compose to deploy a stack.\nI'm using linux hosts.\n```\n`\u279c  swarm git:(master) \u2717 docker --version\nDocker version 20.10.9, build c2ea9bc\n`\n```\n```\n`\u279c  swarm git:(master) \u2717 docker-compose --version\ndocker-compose version 1.29.2, build 5becea4c\n`\n```\nThe issue I'm having is I have a container (service) `smtp` that listens on port `2525`. Inside the swarm from other services I need to reference this service as `smtp:25` not `2525`. Naturally I just did a port mapping. But the other services cannot access the service on port `25` only on `2525`.\nNote that for testing I'm trying the mapping as `2526->2525` to rule out any privilege issues.\nMy docker-compose entry looks like:\n```\n`smtp:\n    image: \n    user: node\n    environment:\n      - NODE_ENV=production\n    privileged: true\n    ports:\n      - published: 2526\n        target: 2525\n        protocol: tcp\n        mode: ingress # I don't need this on the host, just on the swarm vip.\n    deploy:\n      replicas: 2\n      placement:\n        constraints:\n          - node.labels.scheme == task\n      restart_policy:\n        condition: on-failure\n        delay: 5s # how long to wait between retarts\n        window: 20s # How long to determine if startup has succeeded\n    healthcheck:\n      test: netstat -l | grep 2525\n      interval: 10s\n      timeout: 10s\n      retries: 10\n      start_period: 20s\n`\n```\nFrom a busybox container, I can successfully `telnet smtp 2525` but `telnet smtp 2526` gives me `connection refused`. Also, I cannot telnet to smtp:2526 from inside the smtp container either.\nDocker inspect on the service reveals\n```\n` \"Endpoint\": {\n            \"Spec\": {\n                \"Mode\": \"vip\",\n                \"Ports\": [\n                    {\n                        \"Protocol\": \"tcp\",\n                        \"TargetPort\": 2525,\n                        \"PublishedPort\": 2526,\n                        \"PublishMode\": \"ingress\"\n                    }\n                ]\n            },\n            \"Ports\": [\n                {\n                    \"Protocol\": \"tcp\",\n                    \"TargetPort\": 2525,\n                    \"PublishedPort\": 2526,\n                    \"PublishMode\": \"ingress\"\n                }\n            ],\n            \"VirtualIPs\": [\n                {\n                    \"NetworkID\": \"wv8fe0yih5ql9ye09chcl6y1b\",\n                    \"Addr\": \"10.0.0.41/24\"\n                },\n                {\n                    \"NetworkID\": \"5a9tjqidsvphi9lphldit62tm\",\n                    \"Addr\": \"10.0.2.145/24\"\n                }\n            ]\n        }\n`\n```\nNetstat reveals\n```\n`~/app $ netstat -tulpn | grep LISTEN\n(Not all processes could be identified, non-owned process info\n will not be shown, you would have to be root to see it all.)\ntcp        0      0 0.0.0.0:2525            0.0.0.0:*               LISTEN      13/node             \ntcp        0      0 127.0.0.11:42381        0.0.0.0:*               LISTEN      -                   \ntcp        0      0 127.0.0.1:783           0.0.0.0:*               LISTEN      -        \n`\n```\nAny ideas? Am I not allowed to do this?\nEDIT:\nI had to make the container to listen on port 25 to get it to work.",
      "solution": "First, on docker swarm, you use `docker stack deploy` not `docker compose` to deploy services.\nDocker swarm manages service connections with overlay networks: When you deploy a stack (`docker stack deploy`) an overlay network is implicitly created and the services are attached to it. All services attached to an overlay network can discover each other using docker's mesh networking dns discovery.\nSo, in practical terms, you could deploy your smtp service with a stack file modified to look more like this:\n`version: \"3.9\"\n\nnetworks:\n  public:\n    attachable: true\n    driver: overlay\n    name: smtp\n\nservices:\n  smtp:\n    image: ...\n    command: .... --listen :25\n    networks:\n      - public\n    ports:\n      - 2525:25\n    healthcheck:\n      test: netstat -l | grep 25\n    ...\n`\nFurther caveats: there is no port remapping when services connect to each other using dockers bridge or overlay networks. The \"ports\" directive only applies to ingress, that is, packets arriving on the docker nodes from outside the swarm, that need to be routed to services.\nSo, for service-to-service traffic to communicate over :25, then the smtp container needs to be configured to listen on :25.\nOnce that is done you can connect to smtp from containers and services by specifying the target network.\nWith that in mind, and assuming the smtp service can be configured to listen on :25 I attach the service explicitly to a network called \"public\" in the compose file. This network is created with the global name \"smtp\", and it is marked as attachable so ad-hoc containers can be attached to it. Its explicitly declared as \"overlay\" as docker compose would deploy this compose and try and create it as a `bridge` network.\nThen, to verify that this works, the netshoot container can be used. Simply attach it to the now available network and verify that dns resolves the \"smtp\" name on that network, and port 25 is reachable.\n```\n`docker run --network smtp --rm -it nicolaka/netshoot\n$ telnet smtp 25 \n...\n`\n```\nOther services that need to consume smtp would then be attached to the smtp public network:\n`version: \"3.9\"\n\nnetworks:\n  smtp:\n    external: true\n\nservices:\n  some-other-service:\n    image: ...\n    command: .... \n    environment:\n      SMTP_URL: smtp://smtp:25\n    networks:\n    - default\n    - smtp\n`\nHere I am explicitly attaching this service to the implicit default stack network in case there are other services its talking to, in addition to the smtp network.",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2021-10-29T00:23:40",
      "url": "https://stackoverflow.com/questions/69761617/docker-swarm-access-container-to-container-on-published-port"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 68489435,
      "title": "Docker Swarm Failing to Resolve DNS by Service Name With Python Celery Workers Connecting to RabbitMQ Broker Resulting in Connection Timeout",
      "problem": "Setup\nI have Docker installed and connected 9 machines, 1 manager and 8 worker nodes, using Docker swarm. This arrangement has been used in our development servers for ~5 years now.\nI'm using this to launch a task queue that uses Celery for Python. Celery is using RabbitMQ as its broker and Redis for the results backend.\nI have created an overlay network in Docker so that all my Celery workers launched by Docker swarm can reference their broker and results backend by name; i.e., rabbitmq or redis, instead of by IP address. The network was created by running the following command:\n`docker network create -d overlay `\nThe RabbitMQ service and Redis service were launched on the manager node under this overlay network using the following commands:\n`docker service create --network  --name redis --constraint \"node.hostname==manager\" redis`\n`docker service create --network  --name rabbitmq --constraint \"node.hostname==manager\" rabbitmq`\nOnce both of these have been launched, I deploy my Celery workers, one per each Docker swarm worker node, on the same overlay network using the following command:\n`docker service create --network  --name celery-worker --constraint \"node.hostname!=manager\" --replicas 8 --replicas-max-per-node 1 `\nBefore someone suggest it, yes I know I should be using a Docker compose file to launch all of this. I'm currently testing, and I'll write up one after I can get everything working.\nThe Problem\nThe Celery workers are configured to reference their broker and backend by the container name:\n```\n`app = Celery('tasks', backend='redis://redis', broker='pyamqp://guest@rabbitmq//')\n`\n```\nOnce all the services have been launched and verified by Docker, 3 of the 8 start successfully, connect to the broker and backend, and allow me to begin running task on them. The other 5 continuously time out when attempting to connect to RabbitMQ and report the following message:\n`consumer: Cannot connect to amqp://guest:**@rabbitmq:5672//: timed out.`\nI'm at my wits end trying to find out why only 3 of my worker nodes allow the connection to occur while the other 5 cause a continuous timeout. All launched services are connected over the same overlay network.\nThe issue persist when I attempt to use brokers other than RabbitMQ, leading me to think that it's not specific to any one broker. I'd likely have issues connecting to any service by name on the overlay network when on the machines that are reporting the timeout. Stopping the service and launching again always produces the same results - the same 3 nodes work while the other 5 timeout.\nAll nodes are running the same version of Docker (19.03.4, build 9013bf583a), and the machines were created from identical images. They're virtually the same. The only difference among them is their hostnames, e.g., manager, worker1, worker2, and etc.\nI have been able to replicate this setup outside of Docker swarm (all on one machine) by using a bridge network instead of overlay when developing my application on my personal computer without issue. I didn't experience problems until I launched everything on our development server, using the steps detailed above, to test it before pushing it to production.\nAny ideas on why this is occurring and how I can remedy it? Switching form Docker swarm to Kubernetes isn't an option for me currently.",
      "solution": "It's not the answer I wanted, but this appears to be an on-going bug in Docker swarm. For any who are interested, I'll include the issue page.\nhttps://github.com/docker/swarmkit/issues/1429\nThere's a work around listed by one user on there that may wake for some, but your mileage may vary. It didn't work for me. The work around is listed in the bullet below:\n\nDon't try to use docker for Windows to get multi-node mesh network (swarm) running. It's simply not (yet) supported. If you google around, you find some Microsoft blogs telling about it. Also the docker documentation mentions it somewhere. It would be nice, if docker cmd itself would print an error/warning when trying to set something up under Windows - which simply doesn't work. It does work on a single node though.\nDon't try to use a Linux in a Virtualbox under Windows and hoping to workaround with it. It, of course, doesn't work since it has the same limitations as the underlying Windows.\nMake sure you open ports at least 7946 tcp/udp and 4789 udp for worker nodes. For master also 2377 tcp. Use e.g. netcat -vz -u for udp check. Without -u for tcp.\nMake sure to pass --advertise-addr on the docker worker node (!) when executing the join swarm command. Here put the external IP address of the worker node which has the mentioned ports open. Doublecheck that the ports are really open!\nUsing ping to check the DNS resolution for container names works. If you forget the --advertise-addr or not opening port 7946 results in DNS resolution not working on worker nodes!\n\nI suggest attempting all of the above first if you encounter the same issue. To clarify a few things in the above bullet points, the --advertise-addr flag should be used on a worker node when joining it to the swarm. If your worker node doesn't have a static IP address, you can use the interface to connect it. Run ifconfig to view your interfaces. You'll need to use the interface that has your external facing IP address. For most people, this will probably be eth0, but you should still check before running the command. Doing this, the command you would issue on the worker is:\n```\n`docker swarm join --advertise-addr eth0:2377 --token  :2377\n`\n```\nWith 2377 being the port Docker uses. Verify that you joined with your correct IP address by going into your manager node and running the following:\n```\n`docker node inspect \n`\n```\nIf you don't know your node name, it should be the host name of the machine which you joined as a worker node. You can see it by running:\n```\n`docker node ls\n`\n```\nIf you joined on the right interface, you will see this at the bottom of the return when running inspect:\n```\n`{\n    \"Status\": \"ready\",\n    \"Addr\": \n}\n`\n```\nIf you verified that everything has joined correctly, but the issue still persist, you can try launching your services with the additional flag of --dns-option use-vc when running Docker swarm create as such:\n```\n`docker swarm create --dns-option use-vc --network  ...\n`\n```\nLastly, if all the above fails for you as it did for me, then you can expose the port of the running service you wish connect to in the swarm. For me, I wished to connect my services on my worker nodes to RabbitMQ and Redis on my manager node. I did so by exposing the services port. You can do this at creation by running:\n```\n`docker swarm create -p : ...\n`\n```\nOr after the services has been launched by running\n```\n`docker service update --publish-add : \n`\n```\nAfter this, your worker node services can connect to the manager node service by the IP address of the worker node host and the port you exposed. For example, using RabbitMQ, this would be:\n```\n`pyamqp://:@:/\n`\n```\nHopefully this helps someone who stumbles on this post.",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2021-07-22T19:49:45",
      "url": "https://stackoverflow.com/questions/68489435/docker-swarm-failing-to-resolve-dns-by-service-name-with-python-celery-workers-c"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 68083108,
      "title": "How to handle when leader node goes down in docker swarm",
      "problem": "I have two docker nodes running in swarm like below. The second node i promoted to work as manager.\nimb9cmobjk0fp7s6h5zoivfmo *   Node1        Ready               Active              Leader              19.03.11-ol\na9gsb12wqw436zujakdpbqu5p     Node2       Ready               Active              Reachable           19.03.11-ol\nThis works fine when leader node goes to drain/pause. But as part of my test i have stopped the Node1 instance then i got below error when try to see what are the nodes(docker node ls) in the second node and when tried to list the services running(docker service ls).\nError response from daemon: rpc error: code = Unknown desc = The swarm does not have a leader. It's possible that too few managers are online. Make sure more than half of the managers are online\nAlso no docker process coming up in node 2 which were running in node 1 before stopping the instance. Only the existing process are running. My expectation is after stopping the node1 instance, the procees were running in node 1 has to move to node2. This works fine when a node goes to drain status",
      "solution": "The raft consensus algoritm fails when it cant' find a clear majority.\nThis means, never run with 2 manager nodes as one node going down leaves the other with 50% - which is not a majority and quorum cannot be reached.\nGenerally in fact, avoid even numbers, especially when splitting managers between availability zones, as a zone split can leave you with a 50/50 partition - again no majority and no Quorum and a dead swarm.\nSo, valid numbers of swarm managers to try are generally: 1,3,5,7. Going higher than 7 generally reduces performance and doesn't help availability.\n1 should only be used if you are using a 1 or 2 node swarm, and in these cases, loss of the manager node equates to loss of the swarm anyway.\n3 managers is really the minimum you should aim for. If you only have 3 nodes, then prefer to use the managers as workers than run 1 manager and 2 workers.",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2021-06-22T13:55:56",
      "url": "https://stackoverflow.com/questions/68083108/how-to-handle-when-leader-node-goes-down-in-docker-swarm"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 67485881,
      "title": "Docker Swarm global mode issue",
      "problem": "I am trying to create a php: apache service with Docker Swarm. I have 3 nodes, manager, node1, node2.\nI have created the Dockerfile file:\n```\n`FROM php:apache\nCOPY html/ /var/www/html/\n`\n```\nAnd the docker-compose.yml file:\n```\n`version: \"3\"\nnetworks:\n  network1:\n\nservices:\n  php:\n    build:\n      context: ./\n      dockerfile: Dockerfile\n    image: \"php:apache\"\n    ports:\n      - \"80:80\"\n    deploy:\n      mode: global\n    networks:\n      - network1\n`\n```\nWhen I run\n```\n`docker stack deploy --compose-file docker-compose.yml s2\n`\n```\nThe service is created correctly (3 REPLICAS)\n```\n`[vagrant@manager php]$ docker service ls\nID             NAME      MODE      REPLICAS   IMAGE        PORTS\nojoz9i15gud4   s2_php    global    3/3        php:apache   *:80->80/tcp\n`\n```\nbut I cant acces to php content because I get this:\n```\n`\n\n403 Forbidden\n\nForbidden\nYou don't have permission to access this resource.\n\nApache/2.4.38 (Debian) Server at 192.168.100.100 Port 80\n\n`\n```\nSo I added the volume to docker-compose.yml\n```\n`version: \"3\"\nnetworks:\n  network1:\n\nservices:\n  php:\n    build:\n      context: ./\n      dockerfile: Dockerfile\n    image: \"php:apache\"\n    ports:\n      - \"80:80\"\n    volumes:\n      - ./html/:/var/www/html/\n    deploy:\n      mode: global\n    networks:\n      - network1\n`\n```\nBut now when I create the service is only in manager node, 1 REPLICA:\n```\n`[vagrant@manager php]$ docker service ls\nID             NAME      MODE      REPLICAS   IMAGE        PORTS\n5e9ts14itaow   s2_php    global    1/1        php:apache   *:80->80/tcp\n`\n```\nCan someone explain to me why this happens?",
      "solution": "Swarm doesn't do anything special with volumes, so if you have a host volume, mounting a path on that host, you need to prepopulate that directory on each host where the container can run. Because of this, many elect to include static content inside the image to avoid the need to have a volume. Or you can use a shared filesystem like NFS to mount a common location regardless of where the containers are running.",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2021-05-11T13:31:50",
      "url": "https://stackoverflow.com/questions/67485881/docker-swarm-global-mode-issue"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 78234144,
      "title": "One shared named volume with multiple paths",
      "problem": "Is it possible to use one shared named volume with multiple paths ?\nI want to avoid repetition (DRY pattern) and avoid the creation of many named volumes for different paths that must be centralized due to business requirements (backup policies and so on).\nI created the following named volume `vol-web` based on NFS to share with many workers in my swarm cluster. But it can be reproduced locally. Just imagine that the volume points to a directory in your host.\n```\n`docker volume create --driver local \\\n    --opt type=nfs \\\n    --opt o=addr=10.0.0.2,rw \\\n    --opt device=:/var/web \\\n    vol-web\n`\n```\nThe file below is only for test purposes, but I have many compose files, many services that will use this solution in the practice.\n```\n`version: '3.8'\n\nservices:\n  database:\n    image: postgres:16.2\n    container_name: mydb\n    restart: always\n    environment:\n      - POSTGRES_USER=myuser\n      - POSTGRES_PASSWORD=mypassword\n    volumes:\n      - vol-web/pgdata:/var/lib/postgresql/data\n      - vol-web/config/pg/postgres-init.sh:/docker-entrypoint-initdb.d/postgres-init.sh\n    ports:\n      - \"5432:5432\"\n  nginx:\n    image: nginx:1.25.4\n    container_name: my-nginx\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - vol-web/config/nginx/nginx.conf:/etc/nginx/nginx.conf:ro\n      - vol-web/certbot/conf:/etc/letsencrypt\n\nvolumes:\n  vol-web:\n`\n```\nI get actually the following error message:\n```\n`service \"database\" refers to undefined volume vol-web/pgdata: invalid compose project\n`\n```\nI am open for new solutions for this case.",
      "solution": "Support for mounting volume subpaths is only available in Docker 26.0.0 (release March 20, 2024) and later (the pull request merged in janurary). This adds the `subpath` option to volume mounts.\nUsing `docker run`, we can mount the `pgdata` subdirectory of the `vol-web` volume on `/data` like this:\n```\n`docker run -it --rm \\\n  --mount type=volume,source=vol-web,target=/data,volume-subpath=pgdata \\\n  alpine sh\n`\n```\nSupport for this feature in `docker compose` was only merged a few days ago and is only available in the `compose` plugin version 2.26.0 and later; this doesn't appear to be available in a release of Docker at this time.\nPrior to the above versions it was not possible to mount a volume subpath.\n\nUnrelated to your question but important: when you create a volume like this:\n```\n`docker volume create --driver local \\\n    --opt type=nfs \\\n    --opt o=addr=10.0.0.2,rw \\\n    --opt device=:/var/web \\\n    vol-web\n`\n```\nAnd then use a volume entry like this in your compose file:\n```\n`volumes:\n  vol-web:\n`\n```\nThey are not referring to the same volume. Volume, network, and container names in your compose file get prefixed by your compose project name (typically the name of the directory that contains the compose file, but can also be set explicitly). If you want your compose file to reference an existing external volume, you would need to set it as `external`:\n```\n`volumes:\n  vol-web:\n    external: true\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2024-03-27T20:29:53",
      "url": "https://stackoverflow.com/questions/78234144/one-shared-named-volume-with-multiple-paths"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 77631583,
      "title": "elasticsearch cluster using docker swarm with three nodes on three separate servers",
      "problem": "I want to set up an elasticsearch cluster using docker swarm with three nodes on three separate servers. But when I do the docker stack deploy command and the service comes up,\n\nbut I get docker logs from the container, there are errors on all three.\nnode ---> ubuntu1\n```\n`{\"@timestamp\":\"2023-12-09T14:11:54.628Z\", \"log.level\": \"WARN\", \"message\":\"address [10.0.12.8:9300], node [null], requesting [false] discovery result: [es02][10.0.0.29:9300] connect_exception: Failed execution: io.netty.channel.ConnectTimeoutException: connection timed out: 10.0.0.29/10.0.0.29:9300: connection timed out: 10.0.0.29/10.0.0.29:9300\", \"ecs.version\": \"1.2.0\",\"service.name\":\"ES_ECS\",\"event.dataset\":\"elasticsearch.server\",\"process.thread.name\":\"elasticsearch[es03][generic][T#4]\",\"log.logger\":\"org.elasticsearch.discovery.PeerFinder\",\"elasticsearch.node.name\":\"es03\",\"elasticsearch.cluster.name\":\"my-cluster\"}\n{\"@timestamp\":\"2023-12-09T14:11:58.273Z\", \"log.level\": \"WARN\", \"message\":\"master not discovered yet, this node has not previously joined a bootstrapped cluster, and this node must discover master-eligible nodes [es01, es02, es03] to bootstrap a cluster: have discovered [{es03}{58joxi_jQPqr8cd0eHxySg}{uIrh9xE3SNGyLtZc6DAUUA}{es03}{10.0.0.25}{10.0.0.25:9300}{cdfhilmrstw}{8.10.4}{7000099-8100499}]; discovery will continue using [10.0.12.5:9300, 10.0.12.8:9300] from hosts providers and [{es03}{58joxi_jQPqr8cd0eHxySg}{uIrh9xE3SNGyLtZc6DAUUA}{es03}{10.0.0.25}{10.0.0.25:9300}{cdfhilmrstw}{8.10.4}{7000099-8100499}] from last-known cluster state; node term 0, last-accepted version 0 in term 0; for troubleshooting guidance, see https://www.elastic.co/guide/en/elasticsearch/reference/8.10/discovery-troubleshooting.html\", \"ecs.version\": \"1.2.0\",\"service.name\":\"ES_ECS\",\"event.dataset\":\"elasticsearch.server\",\"process.thread.name\":\"elasticsearch[es03][cluster_coordination][T#1]\",\"log.logger\":\"org.elasticsearch.cluster.coordination.ClusterFormationFailureHelper\",\"elasticsearch.node.name\":\"es03\",\"elasticsearch.cluster.name\":\"my-cluster\"}\n`\n```\nnode ---> ubuntu2\n```\n`{\"@timestamp\":\"2023-12-09T14:09:23.515Z\", \"log.level\": \"WARN\", \"message\":\"address [10.0.12.8:9300], node [null], requesting [false] discovery result: [es02][10.0.0.29:9300] connect_exception: Failed execution: io.netty.channel.ConnectTimeoutException: connection timed out: 10.0.0.29/10.0.0.29:9300: connection timed out: 10.0.0.29/10.0.0.29:9300\", \"ecs.version\": \"1.2.0\",\"service.name\":\"ES_ECS\",\"event.dataset\":\"elasticsearch.server\",\"process.thread.name\":\"elasticsearch[es01][generic][T#3]\",\"log.logger\":\"org.elasticsearch.discovery.PeerFinder\",\"elasticsearch.node.name\":\"es01\",\"elasticsearch.cluster.name\":\"my-cluster\"}\n{\"@timestamp\":\"2023-12-09T14:09:26.306Z\", \"log.level\": \"WARN\", \"message\":\"master not discovered yet, this node has not previously joined a bootstrapped cluster, and this node must discover master-eligible nodes [es01, es02, es03] to bootstrap a cluster: have discovered [{es01}{hlJovsOSSKW7_XcF5Wcw1A}{93k4zmohQEaW1WfH7UjoRA}{es01}{10.0.0.27}{10.0.0.27:9300}{cdfhilmrstw}{8.10.4}{7000099-8100499}]; discovery will continue using [10.0.12.8:9300, 10.0.12.2:9300] from hosts providers and [{es01}{hlJovsOSSKW7_XcF5Wcw1A}{93k4zmohQEaW1WfH7UjoRA}{es01}{10.0.0.27}{10.0.0.27:9300}{cdfhilmrstw}{8.10.4}{7000099-8100499}] from last-known cluster state; node term 0, last-accepted version 0 in term 0; for troubleshooting guidance, see https://www.elastic.co/guide/en/elasticsearch/reference/8.10/discovery-troubleshooting.html\", \"ecs.version\": \"1.2.0\",\"service.name\":\"ES_ECS\",\"event.dataset\":\"elasticsearch.server\",\"process.thread.name\":\"elasticsearch[es01][cluster_coordination][T#1]\",\"log.logger\":\"org.elasticsearch.cluster.coordination.ClusterFormationFailureHelper\",\"elasticsearch.node.name\":\"es01\",\"elasticsearch.cluster.name\":\"my-cluster\"}\n`\n```\nnode ---> ubuntu3\n```\n`{\"@timestamp\":\"2023-12-09T14:10:26.640Z\", \"log.level\": \"WARN\", \"message\":\"address [10.0.12.2:9300], node [null], requesting [false] discovery result: [es03][10.0.0.25:9300] connect_timeout[30s]\", \"ecs.version\": \"1.2.0\",\"service.name\":\"ES_ECS\",\"event.dataset\":\"elasticsearch.server\",\"process.thread.name\":\"elasticsearch[es02][generic][T#3]\",\"log.logger\":\"org.elasticsearch.discovery.PeerFinder\",\"elasticsearch.node.name\":\"es02\",\"elasticsearch.cluster.name\":\"my-cluster\"}\n{\"@timestamp\":\"2023-12-09T14:10:30.387Z\", \"log.level\": \"WARN\", \"message\":\"master not discovered yet, this node has not previously joined a bootstrapped cluster, and this node must discover master-eligible nodes [es01, es02, es03] to bootstrap a cluster: have discovered [{es02}{SdcA-EyrRyujky7n4MliiA}{cwcUzo9WQOGq1so0di2lvQ}{es02}{10.0.0.29}{10.0.0.29:9300}{cdfhilmrstw}{8.10.4}{7000099-8100499}]; discovery will continue using [10.0.12.5:9300, 10.0.12.2:9300] from hosts providers and [{es02}{SdcA-EyrRyujky7n4MliiA}{cwcUzo9WQOGq1so0di2lvQ}{es02}{10.0.0.29}{10.0.0.29:9300}{cdfhilmrstw}{8.10.4}{7000099-8100499}] from last-known cluster state; node term 0, last-accepted version 0 in term 0; for troubleshooting guidance, see https://www.elastic.co/guide/en/elasticsearch/reference/8.10/discovery-troubleshooting.html\", \"ecs.version\": \"1.2.0\",\"service.name\":\"ES_ECS\",\"event.dataset\":\"elasticsearch.server\",\"process.thread.name\":\"elasticsearch[es02][cluster_coordination][T#1]\",\"log.logger\":\"org.elasticsearch.cluster.coordination.ClusterFormationFailureHelper\",\"elasticsearch.node.name\":\"es02\",\"elasticsearch.cluster.name\":\"my-cluster\"}\n\n`\n```\nThe configuration file is as follows\n```\n`version: '3.8'\n\nservices:\n  es01:\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.10.4\n    container_name: es01\n    environment:\n      - node.name=es01\n      - discovery.seed_hosts=es02,es03\n      - cluster.initial_master_nodes=es01,es02,es03\n      - cluster.name=my-cluster\n      - bootstrap.memory_lock=true\n      - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\n      - xpack.security.enabled=false\n      - network.host=0.0.0.0\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    volumes:\n      - esdata01:/usr/share/elasticsearch/data\n    ports:\n      - 9201:9200\n    networks:\n      - esnet\n\n  es02:\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.10.4\n    container_name: es02\n    environment:\n      - node.name=es02\n      - discovery.seed_hosts=es01,es03\n      - cluster.initial_master_nodes=es01,es02,es03\n      - cluster.name=my-cluster\n      - bootstrap.memory_lock=true\n      - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\n      - xpack.security.enabled=false\n      - network.host=0.0.0.0\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    volumes:\n      - esdata02:/usr/share/elasticsearch/data\n    ports:\n      - 9202:9200\n    networks:\n      - esnet\n\n  es03:\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.10.4\n    container_name: es03\n    environment:\n      - node.name=es03\n      - discovery.seed_hosts=es01,es02\n      - cluster.initial_master_nodes=es01,es02,es03\n      - cluster.name=my-cluster\n      - bootstrap.memory_lock=true\n      - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\n      - xpack.security.enabled=false\n      - network.host=0.0.0.0\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    volumes:\n      - esdata03:/usr/share/elasticsearch/data\n    ports:\n      - 9203:9200\n    networks:\n      - esnet\n\nvolumes:\n  esdata01:\n    driver: local\n  esdata02:\n    driver: local\n  esdata03:\n    driver: local\nnetworks:\n  esnet:\n\n`\n```\nThe nodes are as follows:\n\nThank you for helping me",
      "solution": "Looking at the error and reproducing locally, it seems that each es* node is using the `esnet` network to discover the other instances, but is advertizing its address on the ingress network.\nI can't see how to specify to elastic which interface specifically its listening on, but removing the \"ports\" removes the ingress bridge and its interface leaving elastic with a single choice.\nThis deployment is my reproduction that worked:\n`version: \"3.10\"\n\nnetworks:\n  esnet:\n    attachable: true\n\nvolumes:\n  esdata:\n    name: '{{index .Service.Labels \"com.docker.stack.namespace\"}}-esdata0{{.Task.Slot}}'\n\nservices:\n  search:\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.10.4\n    hostname: es0{{.Task.Slot}}\n    environment:\n      - node.name=es0{{.Task.Slot}}\n      - discovery.seed_hosts=es01,es02,es03\n      - cluster.initial_master_nodes=es01,es02,es03\n      - cluster.name=my-cluster\n      - bootstrap.memory_lock=true\n      - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\n      - xpack.security.enabled=false\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    volumes:\n      - esdata:/usr/share/elasticsearch/data\n    networks:\n      - esnet\n    deploy:\n      replicas: 3\n\n  proxy:\n    image: alpine:latest\n    command:\n      - /bin/sh\n      - -c\n      - |\n        apk update\n        apk add socat\n        socat TCP-LISTEN:9200,fork TCP:search:9200\n    ports:\n      - 9200:9200\n    networks:\n      - esnet\n`\nOf course, most use cases of elastic will probably see other services attached to `esnet` connecting to read or write, in which case, except for diagnostics, the proxy to allow ingress is unnecessary.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2023-12-09T15:17:20",
      "url": "https://stackoverflow.com/questions/77631583/elasticsearch-cluster-using-docker-swarm-with-three-nodes-on-three-separate-serv"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 76693841,
      "title": "Is it possible to pass OS variables to docker-compose (or stack)",
      "problem": "I'd like to pass the result of bash-command (or cmd) or OS variable as an env to `docker-compose.yml`, is it possible or not?\nFor example:\n\n`hostname` gives me the current machine name: `ubuntu-machine-01`\n\nand I'd like to pass it as env for docker-compose file:\n`version: '3.8'\nservices:\n  service:\n    image: docker-container:latest\n    environment:\n      - WORKSPACE=${{hostname}}\n`\nP.S. Unfortunately I can't bypass arg during Docker image build phase & unable to call it inside container, so the question is about passing OS env to docker-compose via syntax of the file.",
      "solution": "With regards to `hostname` specifically it is worth mentioning Docker Swarm supports service template parameters.\nSo to follow the example on a swarm, if you needed `WORKSPACE` to reflect the hostname of the node the service is running on (rather than deployed from), the following syntax would apply:\n`version: '3.9'\nservices:\n  service:\n    image: docker-container:latest\n    environment:\n      WORKSPACE: \"{{.Node.Hostname}}\"\n`",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2023-07-15T14:54:50",
      "url": "https://stackoverflow.com/questions/76693841/is-it-possible-to-pass-os-variables-to-docker-compose-or-stack"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 70782445,
      "title": "Docker build is giving me this error &quot; /bin/sh: -c requires an argument , The command &#39;/bin/sh -c&#39; returned a non-zero code: 2&quot;",
      "problem": "I am creating a docker file for my messaging-app, Here is my dockerfile.\n```\n`FROM python:3.9-alpine3.15\nLABEL maintainer=\"Noor Ibrahim\"\nENV PYTHONUNBUFFERED 1\nCOPY ./message /message\nCOPY ./requirements.txt /requirements.txt\nWORKDIR /message\nEXPOSE 8000\nRUN\nRUN python -m venv /py && \\\n    /py/bin/pip install --upgrade pip && \\\n    /py/bin/pip install -r /requirements.txt && \\\n    adduser --disabled-password --no-create-home user\n\nENV PATH=\"/py/bin:$PATH\"\nUSER user\n`\n```\nI am running this command\n```\n`docker build .\n`\n```\nI am getting this error while creating the build. I have tried different things but this error doesn't go away.\n```\n`Sending build context to Docker daemon  67.07kB\nStep 1/11 : FROM python:3.9-alpine3.15\n ---> e49e2f1d4108\nStep 2/11 : LABEL maintainer=\"Noor Ibrahim\"\n ---> Using cache\n ---> 517b54d522ef\nStep 3/11 : ENV PYTHONUNBUFFERED 1\n ---> Running in d8eb9d900584\n ---> 29e281514398\nStep 4/11 : COPY ./message /message\n ---> f84edc508eec\nStep 5/11 : COPY ./requirements.txt /requirements.txt\n ---> 608149cc5c42\nStep 6/11 : WORKDIR /message\n ---> Running in b26ec4b33053\nRemoving intermediate container b26ec4b33053\n ---> c608a04f1993\nStep 7/11 : EXPOSE 8000\n ---> Running in 8b4f7f49a3b8\nRemoving intermediate container 8b4f7f49a3b8\n ---> a18f155d9320\nStep 8/11 : RUN\n ---> Running in 6fa80a39c7d0\n/bin/sh: -c requires an argument\nThe command '/bin/sh -c' returned a non-zero code: 2\n`\n```",
      "solution": "Almost certainly that extra `RUN` line with no command after it\n```\n`EXPOSE 8000\nRUN     Get rid of that and it should be fine.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-01-20T08:49:20",
      "url": "https://stackoverflow.com/questions/70782445/docker-build-is-giving-me-this-error-bin-sh-c-requires-an-argument-the-co"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 67730556,
      "title": "Does docker swarm mode start replicas simultaneously or wait until each one finishes?",
      "problem": "I'm attempting to use `docker swarm` and `docker service` to deploy Confluence Data Center. However, the Atlassian documentation for the clustering part gives this warning:\n\nYou should only start one node at a time. Starting up multiple nodes simultaneously can cause serious failures.\n\nWhat is the default startup policy for `docker swarm` service replicas? Do they all start simultaneously, or one after the other? If one after the other, how does it decide that one is done starting?",
      "solution": "By default docker swarm create the replicas at same time, but update one replica at time (rolling update). As far as I know, you can't control the creation but you can create an application with one replica and then update it:\n```\n`docker service create --replicas 1 --name apache httpd:alpine\nsleep 60 # one minute\ndocker service update --replicas 2 apache\nsleep 60 # one more minute ...\ndocker service update --replicas 3 apache\n`\n```\nThis is safe because the existing replica is untouched if you don't change any parameter other then `--replicas`.\nIf you want to update the replicas one at time you can use `--update-parallelism`, and go even further controlling the time between each update replica with `--update-delay`:\n`docker service update apache \\\n--image nginx:alpine \\\n--update-parallelism 1 \\\n--update-delay 10s\n`",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-05-28T00:07:08",
      "url": "https://stackoverflow.com/questions/67730556/does-docker-swarm-mode-start-replicas-simultaneously-or-wait-until-each-one-fini"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 67708023,
      "title": "Running LINSTOR in Docker Swarm",
      "problem": "I am currently trying out `linstor` in my lab.  I am trying to setup a separation of `compute` and `storage` node.  Storage node that runs linstor whereas Compute node is running Docker Swarm or K8s.  I have setup 1 linstor node and 1 docker swarm node in this testing.  Linstor node is configured successfully.\nLinstor Node\nDRBD `9.1.2`\n```\n`\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u250a StoragePool          \u250a Node       \u250a Driver   \u250a PoolName       \u250a FreeCapacity \u250a TotalCapacity \u250a CanSnapshots \u250a State \u250a SharedName \u250a\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u250a DfltDisklessStorPool \u250a instance-2 \u250a DISKLESS \u250a                \u250a              \u250a               \u250a False        \u250a Ok    \u250a            \u250a\n\u250a pd-std-pool          \u250a instance-2 \u250a LVM_THIN \u250a vg/lvmthinpool \u250a   199.80 GiB \u250a    199.80 GiB \u250a True         \u250a Ok    \u250a            \u250a\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n# linstor node list\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u250a Node       \u250a NodeType \u250a Addresses                \u250a State    \u250a\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u250a instance-2 \u250a COMBINED \u250a 10.100.0.29:3366 (PLAIN) \u250a Online   \u250a\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n`\n```\nDocker Node\nIn another node, I have Docker Swarm running.  This node does not have any tools installed such as drbd, drbdtop, drbdsetup ...etc.  Technically is running a minimal installation that is sufficient to run only Docker to keep it lightweight.  Docker version is `20.10.3`.  I have also installed the linstor docker volume written in golang.\nBelow is my `/etc/linstor/docker-volume.conf` and docker volume plugin installed in my Docker Swarm node\n```\n`$ docker plugin ls\nID             NAME                                  DESCRIPTION                        ENABLED\n6300029b3178   linbit/linstor-docker-volume:latest   Linstor volume plugin for Docker   true\n\n$ cat /etc/linstor/docker-volume.conf\n[global]\ncontrollers = linstor://instance-2\nfs = xfs\n`\n```\nI got an error when trying to use the volume created by linstor. I have confirmed I can ping linstor controller at `instance-2` and have all ports open in the firewall. Here is the error and the step to reproduce\n```\n`$ docker volume create -d linbit/linstor-docker-volume:latest  --name=first --opt size=20 --opt replicas=1 --opt storage-pool=pd-std-pool\n$ docker volume ls\nDRIVER                                VOLUME NAME\nlocal                                 64f864db31990baa6b790dde34513a7f6fc466ca0c5e72ffab7024365a9f45da\nlinbit/linstor-docker-volume:latest   first\n$ docker volume inspect first\n[\n    {\n        \"CreatedAt\": \"0001-01-01T00:00:00Z\",\n        \"Driver\": \"linbit/linstor-docker-volume:latest\",\n        \"Labels\": {},\n        \"Mountpoint\": \"\",\n        \"Name\": \"first\",\n        \"Options\": {\n            \"replicas\": \"1\",\n            \"size\": \"20\",\n            \"storage-pool\": \"pd-std-pool\"\n        },\n        \"Scope\": \"global\"\n    }\n]\n$ docker run --rm -it -v first:/data alpine sh\ndocker: Error response from daemon: error while mounting volume '': VolumeDriver.Mount: 404 Not Found.\nERRO[0000] error waiting for container: context canceled \n`\n```\nQuestions\n\nDo I need to install drbd-utils in my Docker Swarm for it to work?\nWhat is the error `VolumeDriver.Mount 404 Not Found` means ?",
      "solution": "LINSTOR manages storage in a cluster of nodes replicating disk space inside a LVM or ZFS volume (or bare partition I'd say) by using DRDB (Distributed Replicated Block Device) to replicate data across the nodes, as per the official docs:\n\n\"LINSTOR is a configuration management system for storage on Linux systems. It manages LVM logical volumes and/or ZFS ZVOLs on a cluster of nodes. It leverages DRBD for replication between different nodes and to provide block storage devices to users and applications. It manages snapshots, encryption and caching of HDD backed data in SSDs via bcache.\"\n\nSo I'd say yes, you really need to have the driver on every node on which you want to use the driver (I did see Docker's storage plugin try to mount the DRBD volume locally)\nHowever, you do not necessarily need to have the storage space itself on the compute node, since you can mount a diskless DRBD resource from volumes that are replicated on separate nodes so I'd say your idea should work, unless there is some bug in the driver itself I didn't discover yet: your compute node(s) needs to be registered as being a diskless node for all the required pools (I didn't try this but remember reading it's not only possible but recommended for some types of data migrations).\nOf course if you don't have more than 1 storage nodes you don't gain much from using LINSTOR/drbd (node or disk failure will leave you diskless). My use case for it was to have replicated storage across different servers in different datacenters, so that the next time one burns to a crisp \ud83d\ude05 I can have my data and containers running after minutes instead of several days...",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-05-26T17:25:35",
      "url": "https://stackoverflow.com/questions/67708023/running-linstor-in-docker-swarm"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 67134199,
      "title": "Error while creating overlay network for standalone containers",
      "problem": "As per the Docker documentation, overlay network is automatically getting created when we initialise docker swarm. But we can not use that network for individual docker container which not part of swarm resource. So, we need to create overlay network with \"--attachable\" flag.\nI tried to create attachable overlay network but I am getting following error :\n```\n`docker network create -d overlay --attachable my-attachable-overlay \nError response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.\n`\n```\nDo we need to run this command on swarm manager ? Can't we use it directly on low weight container like boot2docker without initialising docker swarm ?",
      "solution": "The swarm scoped overlay network driver does indeed require swarm. If you have a single node, you only need to do `docker swarm init` and then you can create a swarm scoped network. If you are getting this error on a swarm worker node, then you just need to create the network on a manager in the swarm and then it can be used on the worker nodes in that swarm.\nThe whole purpose of the overlay network driver is to enable container-to-container communication between multiple nodes in a swarm. It is not necessary to use the overlay network driver in a single node where you do not intend to use any other swarm features nor communicate with containers on other nodes. Use a local scoped network driver instead like `bridge`.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-04-17T04:45:21",
      "url": "https://stackoverflow.com/questions/67134199/error-while-creating-overlay-network-for-standalone-containers"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 66814699,
      "title": "br_netfilter error when deploying docker containers to swarm on ubuntu 20.04",
      "problem": "I've been struggling to deploy my containers to Docker swarm on Ubuntu server 20.04.\nI'm trying to use Docker swarm on a single VPS host for zero-downtime deployments.\nRunning containers with docker-compose everything works.\nNow trying to deploy the same docker-compose file to docker swarm.\n```\n`# docker swarm init\nSwarm initialized: current node (wlshyv0s1n5c85mao8jt9wo5j) is now a manager.\n\nTo add a worker to this swarm, run the following command:\n...\n\n# docker stack deploy --compose-file docker-compose.yml dash\nIgnoring unsupported options: build\n\nCreating network dash_default\nCreating service dash_db\nCreating service dash_nginx\n...\n\n`\n```\nAfter finishing the deploy command, with `docker ps` i see that there are no running containers.\nNow checking with `docker ps -a` I see a lot of containers and all their statuses say \"Created\".\nNext when i inspect one container, then it's state shows that:\n```\n`\"State\": {\n    \"Status\": \"created\",\n    \"Running\": false,\n    \"Paused\": false,\n    \"Restarting\": false,\n    \"OOMKilled\": false,\n    \"Dead\": false,\n    \"Pid\": 0,\n    \"ExitCode\": 128,\n    \"Error\": \"error creating external connectivity network: cannot restrict inter-container communication: please ensure that br_netfilter kernel module is loaded\",\n    \"StartedAt\": \"0001-01-01T00:00:00Z\",\n    \"FinishedAt\": \"0001-01-01T00:00:00Z\"\n}\n`\n```\nChecking for loaded modules:\n```\n`# lsmod | grep br_netfilter\nbr_netfilter            4242  -2\nbridge                  4242  -2 br_netfilter,ebtable_broute\n`\n```\nAfter running `docker info` i saw 2 warnings:\n```\n`# docker info\nClient:\n Context:    default\n Debug Mode: false\n Plugins:\n  app: Docker App (Docker Inc., v0.9.1-beta3)\n  buildx: Build with BuildKit (Docker Inc., v0.5.1-docker)\n\nServer:\n Containers: 0\n  Running: 0\n  Paused: 0\n  Stopped: 0\n Images: 40\n Server Version: 20.10.5\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Native Overlay Diff: true\n Logging Driver: json-file\n Cgroup Driver: cgroupfs\n Cgroup Version: 1\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\n Swarm: active\n  NodeID: ij25ein3xvcr8p5ky765ol8t0\n  Is Manager: true\n  ClusterID: mdb2r7vnngw62lg8uoj5ef55k\n  Managers: 1\n  Nodes: 1\n  Default Address Pool: 10.0.0.0/8\n  SubnetSize: 24\n  Data Path Port: 4789\n  Orchestration:\n   Task History Retention Limit: 5\n  Raft:\n   Snapshot Interval: 10000\n   Number of Old Snapshots to Retain: 0\n   Heartbeat Tick: 1\n   Election Tick: 10\n  Dispatcher:\n   Heartbeat Period: 5 seconds\n  CA Configuration:\n   Expiry Duration: 3 months\n   Force Rotate: 0\n  Autolock Managers: false\n  Root Rotation In Progress: false\n  ...\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 05f951a3781f4f2c1911b05e61c160e9c30eaa8e\n runc version: 12644e614e25b05da6fd08a38ffa0cfe1903fdec\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: default\n Kernel Version: 5.4.0\n Operating System: Ubuntu 20.04.2 LTS\n OSType: linux\n Architecture: x86_64\n CPUs: 2\n Total Memory: 4GiB\n ...\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Registry: https://index.docker.io/v1/\n Labels:\n Experimental: false\n Insecure Registries:\n  127.0.0.0/8\n Live Restore Enabled: false\n\nWARNING: bridge-nf-call-iptables is disabled\nWARNING: bridge-nf-call-ip6tables is disabled\n\n`\n```\nSearching for a solution I found that I should call the `sysctl` command, but I still get an error.\n```\n`# sysctl net.bridge.bridge-nf-call-ip6tables=1\nsysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-ip6tables: No such file or directory\n`\n```\nNow searching solution for that I found the next command, but that does not work as well.\n```\n`# modprobe br_netfilter\nmodprobe: FATAL: Module br_netfilter not found in directory /lib/modules/5.4.0\n`\n```\nI don't know anymore what to do, to make swarm work.\nEverything works on my Windows machine while using swarm mode.\nAny suggestions on what should I do/check next?",
      "solution": "That problem was in the hosting provider.\nProvider told us that other customers have tried to configure Docker Swarm on their VPS too, but no one has figured out how to get it to work.\nThe provider didn't allow any kernel modification or anything else on the lower level.\nNow we are using another hosting provider and everything works fine.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-03-26T10:59:58",
      "url": "https://stackoverflow.com/questions/66814699/br-netfilter-error-when-deploying-docker-containers-to-swarm-on-ubuntu-20-04"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 65699639,
      "title": "How do I prevent swarm containers from becoming orphans upon removing the stack?",
      "problem": "I run a Docker Swarm instance with the following restart script:\n```\n`#!/usr/bin/env sh\ndocker stack rm owlab\nsleep 10\ndocker stack deploy --compose-file ./docker-compose.yml owlab\n`\n```\ndocker-compose.yml:\n```\n`version: \"3\"\nservices:\n webapp-front:\n  image: \"preprod.thatsowl.com:4200/webapp-front-dev\"\n  ports:\n   - \"80:80\"\n  volumes:\n   - \"../../webapp/frontend:/usr/src/app/\"\n webapp-back:\n  image: \"webapp-back-dev\"\n  ports:\n   - \"4000:4000\"\n  volumes:\n   - ../../webapp/backend/src:/usr/src/app/src\n   - ../../webapp/backend/uploads:/usr/src/app/uploads\n   - ../../webapp/shared:/usr/src/app/shared\n   - ../../webapp/backend/html-minifier.conf:/usr/src/app/html-minifier.conf\n  environment:\n   - HOST_TO=http://localhost\n   - DB_TO=local\n  depends_on:\n    - mongo\n mongo:\n   image: mongo:4.2.8\n   restart: always\n   ports:\n    - 27017:27017\n   environment:\n     MONGO_INITDB_ROOT_USERNAME: root\n     MONGO_INITDB_ROOT_PASSWORD: example\n   volumes:\n    - ~/owlab_volumes/mongo:/data/db\n mongo-express:\n   image: mongo-express\n   restart: always\n   ports:\n    - 7081:8081\n   environment:\n    ME_CONFIG_MONGODB_ADMINUSERNAME: root\n    ME_CONFIG_MONGODB_ADMINPASSWORD: example\n`\n```\nSometimes, when when I run my restart script, some containers stay alive. I would expect all containers to be removed upon removing the stack, as it usually does, but sometimes 1 or more containers stay alive, as if they decided to declare their independence. The frequency at which this happens and the number of containers involved when it happens are apparently random.\n```\n`\u279c  local git:(master) \u2717 docker service ls\nID        NAME      MODE      REPLICAS   IMAGE     PORTS\n\u279c  local git:(master) \u2717 docker ps\nCONTAINER ID   IMAGE                                               COMMAND                  CREATED       STATUS       PORTS       NAMES\n3a75b1930778   mongo:4.2.8                                         \"docker-entrypoint.s\u2026\"   2 hours ago   Up 2 hours   27017/tcp   owlab_mongo.1.q7np7im4sbpbtdxe0l1q989dk\nd086085894d4   3e0babb28f48                                        \"npm run dev\"            2 hours ago   Up 2 hours   4000/tcp    owlab_webapp-back.1.ykpkjb79tjq21dbr3fmhjoa21\n58d178bba35f   preprod.thatsowl.com:4200/webapp-front-dev:latest   \"npm start\"              2 hours ago   Up 2 hours   80/tcp      owlab_webapp-front.1.jam4w1z3py8m52msrgx8k23hc\n`\n```\nI tried to stop the container by hand but it freezes and the container stays alive forever.\n```\n`\u279c  local git:(master) \u2717 docker container stop 3a75b1930778\n`\n```\nI also tried to use an image which I thought might solve my problem, but it freezes too:\n```\n`\u279c  local git:(master) \u2717 docker run --rm -v /var/run/docker/swarm/control.sock:/var/run/swarmd.sock dperny/tasknuke 3a75b1930778\nUnable to find image 'dperny/tasknuke:latest' locally\nlatest: Pulling from dperny/tasknuke\n88286f41530e: Pull complete \n0e61a138cf9f: Pull complete \nDigest: sha256:9e2e81971d201cee98f595f4516793333a4eb21bb9d7f7ca858ad2edb50353ad\nStatus: Downloaded newer image for dperny/tasknuke:latest\n^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C\n`\n```\nNote that this problem prevents me from creating my stack again:\n```\n`\u279c  local git:(master) \u2717 ./restart.sh\nRemoving network owlab_default\nFailed to remove network iykl6khzqe646fra5xvddf9ow: Error response from daemon: network iykl6khzqe646fra5xvddf9ow not foundFailed to remove some resources from stack: owlab\n\nCreating service owlab_webapp-front\nfailed to create service owlab_webapp-front: Error response from daemon: network owlab_default not found\n`\n```\nWhy are some containers randomly (as it seems) staying alive upon removing the stack? How do I prevent this from happening?",
      "solution": "The deployment and removal of a stack is performed asynchronous. What you experience is high likely a race condition.\nMake sure to wait until the objects from the removed stack are gone, before starting it again. I had such race conditions in the past as well.. this approach did the trick for me:\n```\n`stack=owlab\ndocker stack rm ${stack}\ntypes=\"service network config secret\";\nfor type in $types;do \n  until [ -z \"$(docker $type ls --filter label=com.docker.stack.namespace=${stack} -q)\" ];do\n    sleep 1\n  done\ndone\ndocker stack deploy --compose-file ./docker-compose.yml ${stack}\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-01-13T11:04:30",
      "url": "https://stackoverflow.com/questions/65699639/how-do-i-prevent-swarm-containers-from-becoming-orphans-upon-removing-the-stack"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 79210917,
      "title": "Promtail config and setup for Docker Swarm containers",
      "problem": "I have a Promtail and Docker Compose config and setup that works fine but when I try to follow same for Docker Swarm cluster, logs are not showing up.\nI have searched online for a doc on working config and setup for Docker Swarm with Promtail and unfortunately I could not find anything to help.\nHere is error from Promtail container in Docker Swarm\n```\n`level=error ts=2024-11-21T00:52:59.569451224Z caller=client.go:430 component=client host=loki:3100 msg=\"final error sending batch\" status=400 tenant= error=\"server returned HTTP status 400 Bad Request (400): error at least one label pair is required per stream\"\n\nlevel=error ts=2024-11-21T00:53:10.069885261Z caller=client.go:430 component=client host=loki:3100 msg=\"final error sending batch\" status=400 tenant= error=\"server returned HTTP status 400 Bad Request (400): error at least one label pair is required per stream\"\n`\n```\nHere is Promtail config am using\n```\n`server:\n  http_listen_port: 9080\n  grpc_listen_port: 0\n\npositions:\n  filename: /tmp/positions.yaml\n\nclients:\n  - url: http://loki:3100/loki/api/v1/push\n\nscrape_configs:\n  - job_name: system\n    static_configs:\n    - targets:\n        - localhost\n      labels:\n        job: varlogs\n        # __path__: /var/log/*log\n        __path__: /var/log/!(auth.log)*log\n\n  - job_name: flog_scrape \n    docker_sd_configs:\n      - host: unix:///var/run/docker.sock\n        refresh_interval: 5s\n        filters:\n          - name: label\n            values: [\"logging=promtail\"] \n    relabel_configs:\n      - source_labels: ['__meta_docker_container_name']\n        regex: '/(.*)'\n        target_label: 'container'\n      - source_labels: ['__meta_docker_container_log_stream']\n        target_label: 'logstream'\n      - source_labels: ['__meta_docker_container_label_logging_jobname']\n        target_label: 'job'\n    pipeline_stages:\n      - cri: {}\n      - multiline:\n          firstline: ^\\d{4}-\\d{2}-\\d{2} \\d{1,2}:\\d{2}:\\d{2},\\d{3}\n          max_wait_time: 3s\n      # https://grafana.com/docs/loki/latest/clients/promtail/stages/json/\n      - json:\n          expressions:\n            #message: message\n            level: level\n            #output: 'message'\n`\n```\nand here is the Docker Swarm service I have the logging labels attached to\n```\n`x-logging:\n  &default-logging\n  driver: \"json-file\"\n  options:\n    max-size: \"1m\"\n    max-file: \"1\"\n    tag: \"{{.Name}}\"\n\nservices:\n\n  app-1:\n    image: \"app-1:v0.1\"\n    networks:\n      - apps\n    env_file:\n      - ./env/.env.app-1\n    deploy:\n      mode: replicated\n      replicas: 2\n      placement:\n        constraints:\n          - node.labels.node != node-1\n    labels:\n      logging: \"promtail\"\n      logging_jobname: \"containerlogs\"\n    logging: *default-logging\n\nnetworks:\n  apps:\n    external: true\n`\n```\nBut when I check Loki, I do not see the `container` and `logstream` labels in Loki dashboard like it does for Docker Compose one that works with the same above config\nWhat am I doing wrong and what do I need to fix so I can see the `container` and `logstream` labels to filter to the container logs I want to view logs for?\nDocker Swarm labels are not showing up in Loki\n\nDocker Compose works fine and labels show up in Loki",
      "solution": "issue fixed, i needed to deploy promtail on docker swarm cluster as global mode, not on 1 node only\nso updated promtail to be global deploy on all nodes in the cluster so it can scrpae the logs from the nodes\nissue was i only had it on just one node and thus will not work to scrape logs for the other nodes\n```\n`  promtail:\n    image: grafana/promtail:2.9.1\n    command: \"-config.file=/mnt/config/promtail-config.yaml\"\n    volumes:\n      - ./promtail/promtail-config.yaml:/mnt/config/promtail-config.yaml\n      - /var/log:/var/log\n      - /var/run/docker.sock:/var/run/docker.sock\n      - /var/lib/docker/containers:/var/lib/docker/containers:ro\n    deploy:\n      mode: global\n\n`\n```",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2024-11-21T12:24:06",
      "url": "https://stackoverflow.com/questions/79210917/promtail-config-and-setup-for-docker-swarm-containers"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 77410057,
      "title": "How/Where to save credentials to use from Dockerfile",
      "problem": "I have a Dockerfile with the following content\n```\n`FROM eclipse-temurin:17-alpine\nRUN apk update && apk add --no-cache curl gcompat\n\nENV REPO_USERNAME=username\nENV REPO_PASSWORD=password\n\n# Create a directory for Puppeteer\nRUN mkdir -p /puppeteer\n\nRUN curl --user \"$REPO_USERNAME:$REPO_PASSWORD\" -o reqLoader-linux http://10.81.9.1/tools/puppeteer-v13/reqLoader-linux\nRUN mv reqLoader-linux /puppeteer/reqLoader-linux\n`\n```\nIt works fine, as you see I use there username and password,\nI want to save those credentials in a secure place and be able to access them from this script.\nI am running this Dockerfile using .sh script in git bash(win 11)\nThis image will be created on the linux env in production and I want simple/minimal solution to achive this\nI tryied to run `docker secret create my_secret ./credentials` and in the credentials.json I have\n```\n`{\n    \"username\" : \"username\",\n    \"password\" : \"password\"\n}\n`\n```\nIn this case, I get\n```\n`Error response from daemon: This node is not a swarm manager. Use \"docker swarm init\" or \"docker swarm join\" to connect this node to swarm and try again.\n`\n```\nshould I init swarm and go that way?\nanother possible solution I am reading now is compose file\nthe solution that I imagine should be an encrypted file in which stored the credentials and only docker can see it when running Dockerfile",
      "solution": "I was able to solve the problem by doing the following steps\n\nI created .netrc in the same level that my Dockerfile is\nI added the following  content in the .netrc file `machine 10.81.9.1 login testusername password testpassword!`\nI modified the .sh file to run the Dockerfile using this script `DOCKER_BUILDKIT=1 docker build --secret id=netrc,src=./.netrc -t $IMAGE:$TAG .` instead of the old `docker build . -t $IMAGE:$TAG`\nmodified the Dockerfile by adding in the first line `# syntax = docker/dockerfile:1.0-experimental` and then to  download file using this command `RUN --mount=type=secret,id=netrc  curl --netrc-file /run/secrets/netrc --output reqLoaderLinux http://10.81.9.1/tools/puppeteer-v13/reqLoader-linux`\n\nuseful sources\nsolution , about .netrc file",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2023-11-02T14:37:32",
      "url": "https://stackoverflow.com/questions/77410057/how-where-to-save-credentials-to-use-from-dockerfile"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 77261837,
      "title": "Rails 7 Action Cable with Puma, Nginx, Docker Swarm",
      "problem": "I setup a DigitalOcean Ubuntu 22 droplet to host a docker swarm with containers for a rails app, postgress, and redis. I\u2019m not able to get action cable to work with puma and nginx. I see errors in the browser console for:\n`WebSocket connection to wss://mydomain.com/cable failed`\nI followed these guides: Setting up nginx as a reverse proxy: https://www.digitalocean.com/community/tutorials/how-to-configure-nginx-as-a-reverse-proxy-on-ubuntu-22-04\nEncrypting nginx: https://www.digitalocean.com/community/tutorials/how-to-secure-nginx-with-let-s-encrypt-on-ubuntu-20-04\nMy /etc/nginx/sites-available/mydomain.com\n```\n`server {\n    server_name mydomain.com www.mydomain.com;\n\n    location / {\n        proxy_pass http://127.0.0.1:3000;\n        include proxy_params;\n    }\n    \n    listen 443 ssl; # managed by Certbot\n    ssl_certificate /etc/letsencrypt/live/mydomain.com/fullchain.pem; # managed by Certbot\n    ssl_certificate_key /etc/letsencrypt/live/mydomain.com/privkey.pem; # managed by Certbot\n    include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot\n    ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot\n\n}\nserver {\n    if ($host = www.mydomain.com) {\n        return 301 https://$host$request_uri;\n    } # managed by Certbot\n\n    if ($host = mydomain.com) {\n        return 301 https://$host$request_uri;\n    } # managed by Certbot\n\n    listen 80;\n    listen [::]:80;\n    server_name mydomain.com www.mydomain.com;\n    return 404; # managed by Certbot\n\n}\n`\n```\nMy cable.yml\n```\n`development:\n  adapter: redis\n  url: \n  channel_prefix: study_notes_development\n\ntest:\n  adapter: redis\n  url: \n  channel_prefix: study_notes_test\n\nproduction:\n  adapter: redis\n  url: \n  channel_prefix: study_notes\n`\n```\nmy docker-stack.yml\n```\n`version: '3'\n\nservices:\n  db:\n    image: postgres\n    volumes:\n      - db_data:/var/lib/postgresql/data\n    env_file:\n      - .env.production.local\n    deploy:\n      restart_policy:\n        condition: on-failure\n        delay: 5s\n        max_attempts: 3\n        window: 120s\n  redis:\n    image: redis:latest\n    volumes:\n      - redis:/data\n    deploy:\n      restart_policy:\n        condition: on-failure\n        delay: 5s\n        max_attempts: 3\n        window: 120s\n  cron:\n    image: myimage:cron\n    env_file:\n      - .env.production\n      - .env.production.local\n    deploy:\n      restart_policy:\n        condition: on-failure\n        delay: 5s\n        max_attempts: 3\n        window: 120s\n  web:\n    image: myimage:prod\n    env_file:\n      - .env.production\n      - .env.production.local\n    ports:\n      - \"3000:3000\"\n    deploy:\n      restart_policy:\n        condition: on-failure\n        delay: 5s\n        max_attempts: 3\n        window: 120s\n  db_migrator:\n    image: myimage:prod\n    command: [\"./wait-for\",\"--timeout=300\",\"db:5432\",\"--\",\"bin/rails\",\"db:migrate\",\"db:seed\"]\n    env_file:\n      - .env.production\n      - .env.production.local\n    deploy:\n      restart_policy:\n        condition: none\n\nvolumes:\n  db_data:\n  redis:\n`\n```\nI've tried variations on nginx settings I found in other questions, such as the accepted answer for this one: What do I need to do to hook up ActionCable on nginx and puma?",
      "solution": "I updated my /etc/nginx/sites-available/mydomain.com This is in the first server section under server_name\n```\n`location / {\n    proxy_pass http://127.0.0.1:3000;\n    proxy_set_header Host $host;\n    proxy_set_header X-Real-IP $remote_addr;\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    proxy_set_header X-Forwarded-Proto $scheme;\n    proxy_set_header Upgrade $http_upgrade;\n    proxy_set_header Connection \"upgrade\";\n}\n\nlocation /cable {\n    proxy_pass http://127.0.0.1:3000/cable;\n    proxy_http_version 1.1;\n    proxy_set_header Upgrade $http_upgrade;\n    proxy_set_header Connection \"Upgrade\";\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    proxy_set_header Host $http_host;\n    proxy_set_header X-Real-IP $remote_addr;\n    proxy_set_header X-Forwarded-Proto https;\n    proxy_redirect off;\n}\n`\n```\nI also modified my config/production.rb The HOST variable is the domain name of my server.\n```\n`  config.action_cable.url = \"wss://#{ENV['HOST']}/cable\"\n  config.action_cable.allowed_request_origins = [\"https://#{ENV['HOST']}\"]\n`\n```",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2023-10-09T23:21:48",
      "url": "https://stackoverflow.com/questions/77261837/rails-7-action-cable-with-puma-nginx-docker-swarm"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 76463930,
      "title": "Host Network on Swarm: Service Discovery and Communication with other Services",
      "problem": "I\u2019ve been working with Docker Swarm and have run into an issue regarding service discovery. Specifically, one of my services is utilizing the \u201chost network\u201d, and I\u2019ve learned from a discussion on this GitHub issue that I\u2019m unable to simultaneously include my service in the overlay network.\nThis situation has created a significant roadblock for me because it has prevented the use of Docker Swarm\u2019s DNSRR feature. Previously, I leveraged DNSRR for service discovery, particularly for identifying the IPs of active tasks.I am seeking a solution or feature that allows me to query all the tasks currently running under the service, including their private and/or public IPs.\nFurthermore, when my service was attached to the overlay network, I was able to directly access other services using their DNS names. However, now that my service isn\u2019t a part of the overlay network, I am compelled to use private IPs, which isn\u2019t optimal.\nCould anyone point me in the direction of a solution or workaround for these challenges? Any advice or insights would be greatly appreciated.\nThank you in advance for your help.\nI have tried to add my service into the overlay network but it failed",
      "solution": "Since you cannot have both `host` and another network because of a limitation of Docker Swarm.  What you can do is the following assuming you have `traefik`.\nRun two services:\n\n`traefik` for external\na. runs and listens to port 80/443 on host mode networking.\nb. it is only on host network\nc. I'm assuming you'd have a reroute so HTTP goes to HTTPS\nd. Route HTTPS over to the other traefik\n\n`traefik` for Docker swarm overlay\na. runs on overlay network\nb. get the real IP from `external-traefik`\nc. exposed on a different port e.g. `8443`",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2023-06-13T12:25:11",
      "url": "https://stackoverflow.com/questions/76463930/host-network-on-swarm-service-discovery-and-communication-with-other-services"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 75985365,
      "title": "Docker swarm can&#39;t reach services in another host",
      "problem": "Consider the following docker stack\n```\n`version: \"3.2\"\n\nservices:\n  web1:\n    image: nginx\n    ports:\n      - \"8081:80\"\n    deploy:    \n      placement:\n        constraints:\n          - node.role == manager\n\n  web2:\n    image: nginx\n    ports:\n      - \"8082:80\"\n`\n```\nI can easily access both services using my browser with the addresses (http://docker_swarm_manager:8081 and http://docker_swarm_node:8082)\nBut service web1 cannot access web2 and vice versa.\nPing works fine so DNS is working\n```\n`root@dfb7b6a65a2e:/# ping web\nping: web: Temporary failure in name resolution\nroot@dfb7b6a65a2e:/# ping web1\nPING web1 (10.0.5.5) 56(84) bytes of data.\n64 bytes from 10.0.5.5 (10.0.5.5): icmp_seq=1 ttl=64 time=0.072 ms\n64 bytes from 10.0.5.5 (10.0.5.5): icmp_seq=2 ttl=64 time=0.082 ms\n...\n--- web1 ping statistics ---\n7 packets transmitted, 7 received, 0% packet loss, time 6009ms\nrtt min/avg/max/mdev = 0.067/0.073/0.082/0.005 ms\n`\n```\nBut accessing the service does not work\n```\n`root@dfb7b6a65a2e:/# curl -v web2:80\n*   Trying 10.0.5.2:80...\n* connect to 10.0.5.2 port 80 failed: Connection timed out\n* Failed to connect to web2 port 80: Connection timed out\n* Closing connection 0\ncurl: (28) Failed to connect to web2 port 80: Connection timed out\n`\n```\nThere is no IP/network conflict, docker swarm uses default network, the hosts are using 172.X.X.X/24 network.",
      "solution": "Docker swarm is running on a virtual machine (vmware) and vmware use the same port as docker for\nFrom vmware docs\n`Starting with NSX 6.2.3, the default VXLAN port is 4789, the standard port assigned by IANA.`\nSo the solution is to recreate the docker swarm cluster using a different port\n```\n`docker swarm init --data-path-port=7788\n`\n```\nFrom docker swarm docs\n`Port 4789 UDP (configurable) for the container ingress network.`",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2023-04-11T13:30:00",
      "url": "https://stackoverflow.com/questions/75985365/docker-swarm-cant-reach-services-in-another-host"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 75362040,
      "title": "Docker Swarm - don&#39;t restart service on entrypoint success",
      "problem": "When trying to deploy my app on Docker swarm I have two services: NGINX to serve static files and app to compile some static files. To run static files compilation I'm using entrypoint in Compose file.\ndocker-compose.yml:\n```\n`version: \"3.9\"\n\nservices:\n  nginx:\n    image: nginx\n    healthcheck:\n      test: curl --fail -s http://localhost:80/lib/tether/examples/viewport/index.html || exit 1\n      interval: 1m\n      timeout: 5s\n      retries: 3\n    volumes:\n      - /www:/usr/share/nginx/html/\n    deploy:\n      mode: replicated\n      replicas: 1\n      placement:\n        constraints:\n          - node.role == manager\n    ports:\n      - \"8000:80\"\n    depends_on:\n      - client\n\n  client:\n    image: my-client-image:latest\n    restart: \"no\"\n    deploy:\n      mode: replicated\n      replicas: 1\n      placement:\n        constraints:\n          - node.role == manager\n    volumes:\n      - /www:/app/www\n    entrypoint: /entrypoint.sh\n`\n```\nentrypoint.sh\n```\n`./node_modules/.bin/gulp compilescss\n`\n```\nI tried adding `restart: \"no\"` in my service, but service is restarted on entrypoint completion anyway",
      "solution": "Docker 23.0.0 is now out. As such you have two options:\n\nstack files now support swarm jobs. Swarm understands that these run to completion. i.e. `mode: replicated-job`.\nDocker Compose V3 Reference makes it clear that \"restart:\" applies to compose and \"deploy.restart_policy.condition: on-failure\" is the equivalent swarm statement.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2023-02-06T14:39:25",
      "url": "https://stackoverflow.com/questions/75362040/docker-swarm-dont-restart-service-on-entrypoint-success"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 74987323,
      "title": "Docker swarm get access from outside network",
      "problem": "I have 4 Proxmox LXC virtual containers with Ubuntu 22.04.\nThe corresponding docker swarm nodes are placed on them:\n```\n`sudo docker node ls\nID                            HOSTNAME         STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION\nlk0nxl4bh6hbt8v8f30v08y6s *   vm-swarm-1-1   Ready     Active         Leader           20.10.22\n09amzoukr1wpuw7svvic79ai2     vm-swarm-1-2   Ready     Active                          20.10.22\nlgmu00c5cgqw12satg7txb6ba     vm-swarm-1-3   Ready     Active                          20.10.22\npv1v3whrotxduv40cw911qguh     vm-swarm-1-4   Ready     Active                          20.10.22\n`\n```\nMy services are described in a docker-compose.yaml file:\n```\n`version: '3'\nservices:\n    php_test:\n        image: nanoninja/php-fpm:8.1\n        container_name: phpfpm\n        restart: always\n        ports:\n          - \"3030:3030\"\n        command: php -S=\"0.0.0.0:3030\" -t=\"/var/www/html\"\n`\n```\nI deploy the stack with a command:\n```\n`sudo docker stack deploy -c ./docker-compose.yaml php_test\nIgnoring unsupported options: restart\n\nIgnoring deprecated options:\n\ncontainer_name: Setting the container name is not supported.\n\nCreating network php_test_default\nCreating service php_test_php_test\n`\n```\nI check that my stack is working with commands:\n```\n`sudo docker stack ls\nNAME       SERVICES   ORCHESTRATOR\nphp_test   1          Swarm\nprod       1          Swarm\nviz        1          Swarm\n`\n```\nand\n```\n`sudo docker stack services php_test\nID             NAME                MODE         REPLICAS   IMAGE                   PORTS\n6gcfr5zfm5hn   php_test_php_test   replicated   1/1        nanoninja/php-fpm:8.1   *:3030->3030/tcp\n`\n```\nThe docker swarm networks look like this:\n```\n`sudo docker network ls \nNETWORK ID     NAME               DRIVER    SCOPE\nc23af3b0a95b   bridge             bridge    local\n30b132f70f88   docker_gwbridge    bridge    local\n680262af3a30   host               host      local\niu9hug3kt509   ingress            overlay   swarm\n1674e3ce429c   none               null      local\nyfir1163z01i   php_test_default   overlay   swarm\np9h6n1bjlanq   prod_default       overlay   swarm\nkqr5aj2cbe6s   viz_default        overlay   swarm\n`\n```\nUbuntu and Proxmox firewall are disabled.\nIn appearance, the stack is available.\nBut when I try to access the stack, I get denied:\n```\n`curl -i 'http://192.168.0.151:3030'\ncurl: (7) Failed to connect to 192.168.0.151 port 3030 after 0 ms: Connection refused\ncurl -i 'http://192.168.0.152:3030'\ncurl: (7) Failed to connect to 192.168.0.152 port 3030 after 0 ms: Connection refused\ncurl -i 'http://192.168.0.153:3030'\ncurl: (7) Failed to connect to 192.168.0.153 port 3030 after 0 ms: Connection refused\ncurl -i 'http://192.168.0.154:3030'\ncurl: (7) Failed to connect to 192.168.0.154 port 3030 after 0 ms: Connection refused\n`\n```\nQuestion: How to start the PHP stack on docker swarm correctly to get access from outside network?\nNote: If possible, I want to do without the Nginx.",
      "solution": "I spent weeks on it. Finally, I am able to access Docker Swarm on the `Proxmox LXC` by enabling `ip_forward` for ingress_sbox namespace.\nRunning this inside the LXC container\n`nsenter --net=/run/docker/netns/ingress_sbox sysctl -w net.ipv4.ip_forward=1`\nRef: https://discuss.linuxcontainers.org/t/docker-swarm-in-lxd-container/937",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2023-01-02T22:48:29",
      "url": "https://stackoverflow.com/questions/74987323/docker-swarm-get-access-from-outside-network"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 71506572,
      "title": "How to change the container name in Docker Swarm?",
      "problem": "I deployed docker-compose.yml through Docker Swarm.\nThe container-name in docker-compose.yml was ignored.\nI have to use the designated container name, but \"docker container name\" cannot be used.\nIs there any way?\nActually, I needed this task to map the container ip to /etc/hosts.\ndocker-compose's links, depends-on is doesn't work.\nI solved this problem with the \"docker inspect\" command. and volume mount between docker inspect's result and container's /etc/hosts.",
      "solution": "It is expected behavior.\nAs per Official Docker Doc\n```\n`Note when using docker stack deploy\n\nThe [container_name][1] option is ignored when [deploying a stack in swarm mode][1]\n`\n```\nNote: Checkout Other SO Answer",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2022-03-17T03:58:31",
      "url": "https://stackoverflow.com/questions/71506572/how-to-change-the-container-name-in-docker-swarm"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 69745628,
      "title": "Rollback whole swarm when one service fails",
      "problem": "Is it possible to rollback another services when only one fails on update with `docker stack deploy`?\nFor example I have serv1, serv2, serv3. I run `docker stack deploy`,\nserv1 and serv2 update without any problems, but then serv3 fails on healthcheck.  Now I want serv1, serv2 and serv3 to rollback to previous version.\nIs that possible in docker swarm mode?",
      "solution": "You can add pinging each other to each service's health check.\nSo, service 1 health check - ensures service 1 is running, and both service 2 and service 3 is reachable.\nFor service 2 - ping 1st and 3rd services\nFor service 3 - ping 2nd and 3rd.\nAs result, if any of services fails, its restarted, if it fails to ping any of dependent ones, its restarted too.\nI encountered analogous issue, so, we added 4th service - the global healthchecker one, that has docker socket mounted from host, and it pinged all services. It was custom and, quite ugly, nodejs script. It pinged/checked each of services it was watching, and if any of them failed, it send `docker service update --force serv1` or something like this to docker socket bound from host machine.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-10-27T23:21:03",
      "url": "https://stackoverflow.com/questions/69745628/rollback-whole-swarm-when-one-service-fails"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 69548829,
      "title": "Using depend_on with stack",
      "problem": "The documentation for depends_on states:\n\nThere are several things to be aware of when using `depends_on`:\n\n`depends_on` does not wait for `db` and `redis` to be \u201cready\u201d before starting `web` - only until they have been started. If you need to wait\nfor a service to be ready, see Controlling startup order for more on\nthis problem and strategies for solving it.\n\nThe `depends_on` option is ignored when deploying a stack in swarm mode with a version 3 Compose file.\n\nSo, does this mean when I'm using swarm mode and version 3 compose file to deploy it will ignore all conditions under depends_on? For example:\n```\n`version: \"3\"\nservices:\n  web:\n    build: .\n    ports:\n      - \"80:8000\"\n    depends_on:\n      - \"db\"\n    command: [\"./wait-for-it.sh\", \"db:5432\", \"--\", \"python\", \"app.py\"]\n  db:\n    image: postgres\n`\n```",
      "solution": "Pretty correct, `swarm` as a orchestration will ignore the `depends_on`.\nBut for your scenario, `depends_on` is really not necessary as you already have `wait-for-it.sh` which have better control than `depends_on` to assure the start order.\n\nYou can control the order of service startup and shutdown with the depends_on option. Compose always starts and stops containers in dependency order, where dependencies are determined by depends_on, links, volumes_from, and network_mode: \"service:...\".\nHowever, for startup Compose does not wait until a container is \u201cready\u201d (whatever that means for your particular application) - only until it\u2019s running. There\u2019s a good reason for this.\nThere are limitations to this first solution. For example, it doesn\u2019t verify when a specific service is really ready.\nAlternatively, write your own wrapper script to perform a more application-specific health check. For example, you might want to wait until Postgres is ready to accept commands\n\nDetails see Control startup and shutdown order in Compose, so you no need `depends_on` in your solution.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-10-13T03:52:05",
      "url": "https://stackoverflow.com/questions/69548829/using-depend-on-with-stack"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 68526826,
      "title": "How to solve &quot;Couldn&#39;t start vault with IPC_LOCK. Disabling IPC_LOCK&quot; and &quot;Cluster address must be set when using raft storage&quot; errors?",
      "problem": "I use this following stack file to deploy the vault service in docker swarm mode.\nThe stack file:\n```\n`version: '3.8'\nservices:\n\n  faume-vault:\n    image: vault:1.6.0\n    environment:\n      TZ: UTC\n      VAULT_ADDR: 'http://0.0.0.0:8200'\n      VAULT_LOCAL_CONFIG: |-\n        {\n          \"disable_cache\": true,\n          \"disable_mlock\": true,\n          \"ui\": true,\n          \"backend\": {\n            \"raft\": {\n              \"node_id\": \"vault\",\n              \"path\": \"/vault\"\n            }\n          },\n          \"default_lease_ttl\": \"168h\",\n          \"max_lease_ttl\": \"720h\",\n          \"seal\": {\n            \"awskms\": {\n              \"access_key\": \"xxxxxxxxxxxxxxxxxxxxxxx\"\n              \"secret_key\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n              \"kms_key_id\": \"xxxxxxxxxxxxxxxxxxxxxxx\"\n            }\n          },\n          \"listener\": {\n            \"tcp\": {\n              \"address\": \"0.0.0.0:8200\",\n              \"cluster_address\": \"0.0.0.0:8201\",\n              \"tls_disable\": true\n            }\n          },\n          \"cluster_addr\": \"http://vault.faume.local:8201\",\n          \"api_addr\": \"http://vault.faume.local:8200\"\n        }\n    command: server\n    volumes:\n      - 'faume-vault:/vault'\n    ports:\n      - '8200:8200'\n    networks:\n      faume:\n        aliases:\n          - vault.faume.local\n    deploy:\n      mode: replicated\n      replicas: 1\n      restart_policy:\n        condition: on-failure\n      placement:\n        constraints:\n          - node.role==manager\n\nvolumes:\n  faume-vault:\n\nnetworks:\n  faume:\n`\n```\nWhen deploy the stack file, the service is created without errors, but service does not run.\nService logs are as following:\n```\n`faume-vault_faume-vault.1.xztddsvjaa2c@DevOps-Dhanushka    | Couldn't start vault with IPC_LOCK. Disabling IPC_LOCK, please use --privileged or --cap-add IPC_LOCK\nfaume-vault_faume-vault.1.xkh8abjlzyel@DevOps-Dhanushka    | Couldn't start vault with IPC_LOCK. Disabling IPC_LOCK, please use --privileged or --cap-add IPC_LOCK\nfaume-vault_faume-vault.1.xkh8abjlzyel@DevOps-Dhanushka    | 2021-07-26T08:20:15.705Z [INFO]  proxy environment: http_proxy= https_proxy= no_proxy=\nfaume-vault_faume-vault.1.xztddsvjaa2c@DevOps-Dhanushka    | 2021-07-26T08:20:31.553Z [INFO]  proxy environment: http_proxy= https_proxy= no_proxy=\nfaume-vault_faume-vault.1.xkh8abjlzyel@DevOps-Dhanushka    | Cluster address must be set when using raft storage\nfaume-vault_faume-vault.1.xztddsvjaa2c@DevOps-Dhanushka    | Cluster address must be set when using raft storage\nfaume-vault_faume-vault.1.5g7wzqm7fn0f@DevOps-Dhanushka    | Couldn't start vault with IPC_LOCK. Disabling IPC_LOCK, please use --privileged or --cap-add IPC_LOCK\nfaume-vault_faume-vault.1.5g7wzqm7fn0f@DevOps-Dhanushka    | 2021-07-26T08:20:23.070Z [INFO]  proxy environment: http_proxy= https_proxy= no_proxy=\nfaume-vault_faume-vault.1.5g7wzqm7fn0f@DevOps-Dhanushka    | Cluster address must be set when using raft storage\nfaume-vault_faume-vault.1.kf0k9eoou749@DevOps-Dhanushka    | Couldn't start vault with IPC_LOCK. Disabling IPC_LOCK, please use --privileged or --cap-add IPC_LOCK\nfaume-vault_faume-vault.1.kf0k9eoou749@DevOps-Dhanushka    | Cluster address must be set when using raft storage\nfaume-vault_faume-vault.1.kf0k9eoou749@DevOps-Dhanushka    | 2021-07-26T08:20:39.894Z [INFO]  proxy environment: http_proxy= https_proxy= no_proxy=\n`\n```\nIt seems error is \"mlock\". But I have configure \"mlock\" variable successfully. Can you please, make some suggestions?",
      "solution": "You need to run your `faume-vault` container in privileged mode.\nJust add `privileged: true` option.\nThis is supported by docker swarm only in recent releases (see privileged mode in docker compose in a swarm) do please ensure you are running a recent enough version.\nPlease make sure you understand the security concerns involved with running privileged pods.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-07-26T10:40:11",
      "url": "https://stackoverflow.com/questions/68526826/how-to-solve-couldnt-start-vault-with-ipc-lock-disabling-ipc-lock-and-clust"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 68085623,
      "title": "Docker swarm overlay, single node, no connection between services",
      "problem": "I'm trying to make a connection from one service to another, to achieve it I created an overlay network and two services attached to it like so.\n`$ docker network create -d overlay net1\n$ docker service create --name busybox --network net1 busybox sleep 3000\n$ docker service create --name busybox2 --network net1 busybox sleep 3000\n`\nNow I make sure my services are running and both connected to overlay.\n```\n`$ docker ps\nCONTAINER ID   IMAGE            COMMAND        CREATED              STATUS              PORTS     NAMES\necc8dd465cb1   busybox:latest   \"sleep 3000\"   About a minute ago   Up About a minute             busybox2.1.uw597s90tkvbcaisgaq7los2q\nf8cfe793e3d9   busybox:latest   \"sleep 3000\"   About a minute ago   Up About a minute             busybox.1.l5lxp4v0mcbujqh79dne2ds42\n\n$ docker network inspect net1\n[\n    {\n        \"Name\": \"net1\",\n        \"Id\": \"5dksx8hlxh1rbj42pva21obyz\",\n        \"Created\": \"2021-06-22T14:23:43.739770415Z\",\n        \"Scope\": \"swarm\",\n        \"Driver\": \"overlay\",\n        \"EnableIPv6\": false,\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Options\": null,\n            \"Config\": [\n                {\n                    \"Subnet\": \"10.0.4.0/24\",\n                    \"Gateway\": \"10.0.4.1\"\n                }\n            ]\n        },\n        \"Internal\": false,\n        \"Attachable\": false,\n        \"Ingress\": false,\n        \"ConfigFrom\": {\n            \"Network\": \"\"\n        },\n        \"ConfigOnly\": false,\n        \"Containers\": {\n            \"ecc8dd465cb12c622f48b109529534279dddd4fe015a66c848395157fb73bc69\": {\n                \"Name\": \"busybox2.1.uw597s90tkvbcaisgaq7los2q\",\n                \"EndpointID\": \"b666f6374a815341cb8af7642a7523c9bb153f153b688218ad006605edd6e196\",\n                \"MacAddress\": \"02:42:0a:00:04:06\",\n                \"IPv4Address\": \"10.0.4.6/24\",\n                \"IPv6Address\": \"\"\n            },\n            \"f8cfe793e3d97f72393f556c2ae555217e32e35b00306e765489ac33455782aa\": {\n                \"Name\": \"busybox.1.l5lxp4v0mcbujqh79dne2ds42\",\n                \"EndpointID\": \"fff680bd13a235c4bb050ecd8318971612b66954f7bd79ac3ee0799ee18f16bf\",\n                \"MacAddress\": \"02:42:0a:00:04:03\",\n                \"IPv4Address\": \"10.0.4.3/24\",\n                \"IPv6Address\": \"\"\n            },\n            \"lb-net1\": {\n                \"Name\": \"net1-endpoint\",\n                \"EndpointID\": \"2a3b02f66f395e613c6bc88f16d0723762d28488b429a9e50f7df24c04e9f1f0\",\n                \"MacAddress\": \"02:42:0a:00:04:04\",\n                \"IPv4Address\": \"10.0.4.4/24\",\n                \"IPv6Address\": \"\"\n            }\n        },\n        \"Options\": {\n            \"com.docker.network.driver.overlay.vxlanid_list\": \"4101\"\n        },\n        \"Labels\": {},\n        \"Peers\": [\n            {\n                \"Name\": \"e1c2ac76b95b\",\n                \"IP\": \"10.18.0.6\"\n            }\n        ]\n    }\n]\n`\n```\nSo far so good! Next I ssh into one of containers and try to nslookup the second one, but have no luck.\n```\n`$ docker exec -it busybox.1.l5lxp4v0mcbujqh79dne2ds42 sh\n/ # nslookup busybox2\nServer:     127.0.0.11\nAddress:    127.0.0.11:53\n\nNon-authoritative answer:\n*** Can't find busybox2: No answer\n\n*** Can't find busybox2: No answer\n\n/ # nslookup busybox2.1.uw597s90tkvbcaisgaq7los2q\nServer:     127.0.0.11\nAddress:    127.0.0.11:53\n\nNon-authoritative answer:\n*** Can't find busybox2.1.uw597s90tkvbcaisgaq7los2q: No answer\n\n*** Can't find busybox2.1.uw597s90tkvbcaisgaq7los2q: No answer\n`\n```\nI know that `overlay` questions are quite common here, but they are mostly about node to node connections, not single node swarm. Another think to keep in mind is there is no local firewall on that node at all.\nAm I trying to connect in the wrong way or is it a configuration issue?",
      "solution": "The solution was simply adding a `--attachable` flag to `network create` command. After that I could ping my services by name.\nTurns out you need that flag no matter if you are adding stack (in my case I have multiple stacks in the same swarm) or single services.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-06-22T16:40:23",
      "url": "https://stackoverflow.com/questions/68085623/docker-swarm-overlay-single-node-no-connection-between-services"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 67858599,
      "title": "Valid docker-compose file not deploying as stack when using yaml anchors",
      "problem": "I have been refactoring some docker-compose files to try and take advantage of tip #82 and hit a problem I haven't been able to find a solution to; I'm hoping someone can assist.\nUsing the following stripped example `test-compose.yml` file:\n```\n`version: '3'\n\nx-test: &test\n  deploy:\n    mode: replicated\n\nservices:\n  hello-world:\n    Running under `docker-compose` works as expected:\n```\n`root@docker01:~# docker-compose -f test-compose.yml up\nRecreating root_hello-world_1 ... done\nRecreating root_hello-world_2 ... done\nAttaching to root_hello-world_2, root_hello-world_1\nhello-world_1  | PING www.google.com (172.217.16.228): 56 data bytes\nhello-world_1  | 64 bytes from 172.217.16.228: seq=0 ttl=114 time=6.704 ms\nhello-world_2  | PING www.google.com (172.217.16.228): 56 data bytes\nhello-world_2  | 64 bytes from 172.217.16.228: seq=0 ttl=114 time=6.595 ms\n`\n```\nHowever launching the same as a stack, fails:\n```\n`root@docker01:~# docker stack deploy --compose-file test-compose.yml hello-world\n(root) Additional property x-test is not allowed\n`\n```\nIs there a way to get the same extensions (\"x-* properties) working for both docker-compose and stack?",
      "solution": "So, two things are going to bite you here:\nFirst, docker stack deploy is fussy about the version you specify, so you need to strictly specify a valid compose version equal to or higher than the feature you are trying to use. Not sure when anchor support was added, but it definitely works when the version is specified as \"3.9\".\nYour next problem is that merging is shallow. In your example case this isn't a problem because `x-test` contains only one setting which is already on its default value, but more generally, to handle complex cases, something like this is needed:\n`version: \"3.9\"\nx-defaults:\n  service: &service-defaults\n    deploy: &deploy-defaults\n      placement:\n        constraints:\n        - node.role==worker\n\nservices:\n  hello-world:\n    \nAs adding \"deploy\" to the hello-world map completely overrides any entry set by the default-service, it needs its own anchor reference to import sub-settings.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-06-06T13:30:54",
      "url": "https://stackoverflow.com/questions/67858599/valid-docker-compose-file-not-deploying-as-stack-when-using-yaml-anchors"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 67741454,
      "title": "Running docker stack deploy not able to connect to app. Docker-compose works fine",
      "problem": "I am trying to get a very simple docker-swarm going. If I start the container using `docker-compose up -d` I am able to go to locahost and see the 'Hello message'.\nRunning `docker stack deploy -c docker-compose.yml swar` starts the swarm fine.\ndocker ps\n\ndocker service ls\n\nHowever navigating to localhost, 0.0.0.0, 127.0.0.1 or IP of machine doesn't work, it just timesout with 'This site can't be reached. took too long to respond.'\nIve also tried it with another small tutorial from github, which also has same issue.\nAny ideas what is wrong?\nserver.js:\n```\n`const express = require(\"express\");\nconst os = require(\"os\");\n\nconst app = express();\n\napp.get(\"/\", (req, res) => {\n  res.send(\"Hello from Swarm \" + os.hostname());\n});\n\napp.listen(3000, () => {\n  console.log(\"Server is running on port 3000\");\n});\n`\n```\ndocker-compose.yml:\n```\n`version: \"3\"\n\nservices:\n  web:\n    build: .\n    image: takacsmark/swarm-example:1.0\n    ports:\n      - 80:3000\n    networks:\n      - mynet\n\nnetworks:\n  mynet:\n`\n```\nDockerfile:\n```\n`FROM node:11.1.0-alpine\n\nWORKDIR /home/node\n\nCOPY . .\n\nRUN npm install\n\nCMD npm start\n`\n```\nEDIT:\nrunning wget  just hangs.\n```\n`--2021-05-28 16:59:52--  http:///\nConnecting to :80... \n`\n```\ncurl  just hangs no output.\nIve initialised the swarm included advertise ip aswell\n```\n`docker swarm init --advertise-addr  \n`\n```\nAnd still no luck.\nMy docker version is 20.10.5, docker-compose version: 1.25.0",
      "solution": "For anyone ever getting this sort of problem. Problem was the version of docker. Ive removed 20.10.15 and reinstalled a docker 19.03.10. To install custom docker instead of latest follow steps. https://docs.docker.com/engine/install/ubuntu/\nlocahost doesnt work, but run `hostname -I` to get the ip of your machine and paste that in. Will work then.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-05-28T17:13:36",
      "url": "https://stackoverflow.com/questions/67741454/running-docker-stack-deploy-not-able-to-connect-to-app-docker-compose-works-fin"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 67659002,
      "title": "Docker Swarm: React frontend cannot connect to Kong API Gateway",
      "problem": "I am working on a project using microservices and I am trying to connect my frontend in React with my backend which is served through Kong API Gateway. The problem is that whenever I send a request to the server, I get the error `net::ERR_NAME_NOT_RESOLVED`. The frontend service is running in the same network as the API Gateway service, but apparently, the DNS resolution is not working.\nThis is how I build the React frontend Docker image:\n```\n`FROM node:13.12.0-alpine\n\nWORKDIR /app\n\nENV PATH /app/node_modules/.bin:$PATH\n\nCOPY package.json ./\nCOPY package-lock.json ./\nRUN npm install --silent\nRUN npm install react-scripts@3.4.1 -g --silent\n\nCOPY . ./\n\nCMD [\"npm\", \"start\"]\n`\n```\nThis is how I send the request from the frontend:\n```\n`handleLogin(event) {\n    event.preventDefault();\n    console.log(\"Log in \" + JSON.stringify(this.state.loginForm));\n    axios.post(`http://${process.env.API_URL}:${process.env.API_PORT}/api/users/login`, this.state.loginForm)\n      .then((response) => {\n        const model: LoginModel = response.data;\n        this.handleLoginResponse(model);\n      })\n      .catch((errors) => alert(`Error sending login request to server: ${errors}`));\n  }\n`\n```\nThis is my stack.yml configuration file for Docker Swarm deployment:\n```\n`version: \"3.8\"\n\nservices:\n  kong:\n    image: kong:latest\n    hostname: kong\n    environment:\n      KONG_DATABASE: \"off\"\n      KONG_DECLARATIVE_CONFIG: /usr/local/kong/declarative/kong-plugins.yml\n      KONG_PROXY_ACCESS_LOG: /dev/stdout\n      KONG_ADMIN_ACCESS_LOG: /dev/stdout\n      KONG_PROXY_ERROR_LOG: /dev/stderr\n      KONG_ADMIN_ERROR_LOG: /dev/stderr\n      KONG_ADMIN_LISTEN: 0.0.0.0:8001, 0.0.0.0:8444 ssl\n    deploy:\n      placement:\n        constraints: [node.role == manager]\n    ports:\n      - 8000:8000\n      - 8443:8443\n      - 8001:8001\n      - 8444:8444\n    volumes:\n      - ../../Kong:/usr/local/kong/declarative\n    networks:\n      - frontend-network\n      - internal\n      - logging\n\n  frontend-service:\n    image: virusx98/frontend\n    environment:\n      API_URL: kong\n      API_PORT: 8000\n    ports:\n      - \"3001:3000\"\n    networks:\n      - frontend-network\n\n  [...] /* other services */\n\n  \nvolumes:\n  [...] /* volumes for other services */\n\nnetworks:\n  frontend-network:\n  [...] /* other networks */\n\nsecrets:\n  [...] /* secrets for other services */\n`\n```\nThe stack works perfectly on the backend side. If I send requests using Postman, they reach the backend services and a result is returned. But if I send requests from the frontend, the above error is thrown, even though both services are placed in the same network and DNS resolution should work. The environment variables are read correctly by the React service, but it says it cannot resolve hostname `kong`. What am I missing here?",
      "solution": "Edit based on comment\nThe reason you are unable to reach the API is because the Node process is not sending the request. Your browser is sending the request. The API must be accessible from your (and everyone else's) browser. In this case, your URL would need to be `localhost:8000` in your javascript, so that your browser will be able to reach the `kong` container.\nOriginal answer (only helps with env  vars)\nreact-scripts will only load a couple environment variables from Node. From their documentation:\n\nNote: You must create custom environment variables beginning with REACT_APP_. Any other variables except NODE_ENV will be ignored to avoid accidentally exposing a private key on the machine that could have the same name.\n\nThis means you have to update `docker-compose.yml` like so:\n`    environment: \n      REACT_APP_API_URL: kong\n      REACT_APP_API_PORT: 8000\n`\nDoing this, and `console.log(process.env)` yields:\n`{\n  \"NODE_ENV\": \"development\",\n  \"PUBLIC_URL\": \"\",\n  \"FAST_REFRESH\": true,\n  \"REACT_APP_API_PORT\": \"8000\",\n  \"REACT_APP_API_URL\": \"kong\"\n}\n`\nThis being said, you should not use react-scripts start for production purposes. Always make a production build. You should instead add a `.env` file to your project so those variables get injected in the build. You won't be able to inject them via docker-compose.\nReference:\n\nhttps://create-react-app.dev/docs/adding-custom-environment-variables/",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-05-23T13:03:53",
      "url": "https://stackoverflow.com/questions/67659002/docker-swarm-react-frontend-cannot-connect-to-kong-api-gateway"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 67556400,
      "title": "Docker proxy settings not consistent",
      "problem": "I set a proxy on my host machine according to the docker docs in `~/.docker/config.json` (https://docs.docker.com/network/proxy/#configure-the-docker-client):\n```\n`{\n \"proxies\":\n {\n   \"default\":\n   {\n     \"httpProxy\": \"http://127.0.0.1:3001\",\n     \"httpsProxy\": \"http://127.0.0.1:3001\"\n   }\n }\n}\n`\n```\nIf I start a new container, the environment variables seem to be set. If I check it form the host:\n```\n`# docker exec  echo $http_proxy\n# http://127.0.0.1:3001\n`\n```\nHowever, if I enter the container with an interactive shell, the environment variables are not there:\n```\n`# docker exec -it  sh\n# echo $http_proxy\n#\n`\n```\nWhy can't I see it here? I seems like my application cannot see the setting neither. Is the proxy setting just set for different users?",
      "solution": "The two commands are very different, and not caused by docker, but rather your shell on the host. This command:\n```\n`# docker exec  echo $http_proxy\n# http://127.0.0.1:3001\n`\n```\nExpands the variable in your shell on the host, then passes that expanded variable to the docker command that runs inside the container. It's very similar to running:\n```\n`# echo $http_proxy\nhttp://127.0.0.1:3001\n# docker exec  echo http://127.0.0.1:3001\nhttp://127.0.0.1:3001\n`\n```\nThe second command you provided does expand the variable inside the container, so this shows you do not have the setting applied inside your container.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-05-16T13:58:13",
      "url": "https://stackoverflow.com/questions/67556400/docker-proxy-settings-not-consistent"
    },
    {
      "tech": "docker",
      "source": "stackoverflow",
      "tag": "docker-swarm",
      "question_id": 66975507,
      "title": "docker stack deploy does not update config",
      "problem": "Trying to set up a zero-downtime deployment using docker stack deploy, docker swarm one node localhost environment.\nAfter building image `demo:latest`, the first deployment using the command `docker stack deploy --compose-file docker-compose.yml demo` able to see 4 replicas running and can access nginx default home page on port 8080 on my local machine. Now updating `index.html`, building image with the same name and tag running docker stack deplopy command causing below error and changes are not reflected.\nDeleting the deployment and recreating will work, but I am trying to see how can updates rolled in without downtime. Please help here.\nError\n```\n`Updating service demo_demo (id: wh5jcgirsdw27k0v1u5wla0x8)\nimage demo:latest could not be accessed on a registry to record\nits digest. Each node will access demo:latest independently,\npossibly leading to different nodes running different\nversions of the image.\n`\n```\nDockerfile\n```\n`FROM nginx:1.19-alpine\nADD index.html /usr/share/nginx/html/\n`\n```\ndocker-compose.yml\n```\n`version: \"3.7\"\nservices:\n  demo:\n    image: demo:latest\n    ports:\n      - \"8080:80\"\n    deploy:\n      replicas: 4\n      update_config:\n        parallelism: 2\n        order: start-first\n        failure_action: rollback\n        delay: 10s\n      rollback_config:\n        parallelism: 0\n        order: stop-first\n`\n```",
      "solution": "TLDR: push your image to a registry after you build it\nDocker swarm doesn't really work without a public or private docker registry. Basically all the nodes need to get their images from the same place, and the registry is the mechanism by which that information is shared. There are other ways to get images loaded on each node in the swarm, but it involves executing the same commands on every node one at a time to load in the image, which isn't great.\nAlternatively you could use docker configs for your configuration data and not rebuild the image every time. That would work passably well without a registry, and you can swap out the config data with little-no downtime:\nRotate Configs",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-04-06T21:43:37",
      "url": "https://stackoverflow.com/questions/66975507/docker-stack-deploy-does-not-update-config"
    }
  ]
}