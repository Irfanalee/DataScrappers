{
  "tech": "influxdb",
  "count": 96,
  "examples": [
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 68122202,
      "title": "Grafana - InfluxDB 2 - Label/Alias data",
      "problem": "I am in the processing of migrating my panels from using the SQL syntax (from InfluxDB version 1.X) to the new influx syntax (InfluxDB version 2).\nThere is an issue with the labels of the data. It includes the attributes that I used to filter it. For example, if I select data from a range that contains 2 days, it splits the data up. See the screenshot below:\n\nThis completely messes the chart up. The base code looks like this:\n```\n`from(bucket: \"main\")\n  |> range(start: v.timeRangeStart, stop:v.timeRangeStop)\n  |> filter(fn: (r) =>\n      r._measurement == \"POWER\" and\n      r._field == \"value\" and\n      r.device == \"living_room\"\n  )\n  |> aggregateWindow(every: v.windowPeriod, fn: sum)\n`\n```\nIt should obviously just be \"POWER\" and \"CURRENT\".\nI tried a dozen of different approaches, but cannot come up with a working solution.\nFor example, if I do:\n```\n`from(bucket: \"main\")\n  |> range(start: v.timeRangeStart, stop:v.timeRangeStop)\n  |> filter(fn: (r) =>\n      r._measurement == \"POWER\" and\n      r._field == \"value\" and\n      r.device == \"living_room\"\n  )\n  |> aggregateWindow(every: v.windowPeriod, fn: sum)\n  |> map(fn: (r) => ({ POWER: r._value }))\n`\n```\nit says \"Data does not have a time field\".\nI also tried using\n```\n`from(bucket: \"main\")\n  |> range(start: v.timeRangeStart, stop:v.timeRangeStop)\n  |> filter(fn: (r) =>\n      r._measurement == \"POWER\" and\n      r._field == \"value\" and\n      r.device == \"living_room\"\n  )\n  |> aggregateWindow(every: v.windowPeriod, fn: sum)\n  |> yield(name: \"POWER\")\n`\n```\nthat does not work either. I tried many other things without success.\nHow can I fix this?",
      "solution": "After hours of trial and error, I was able to produce a working solution. I imagine that other users may stumble upon the same issue, I will therefore not delete the question and instead provide my solution.\nI basically had to map the required fields and tags and assign the desired label, instead of just mapping the value that should be displayed (because then the date/time data is missing).\nThe solution looks like this:\n```\n`from(bucket: \"main\")\n  |> range(start: v.timeRangeStart, stop:v.timeRangeStop)\n  |> filter(fn: (r) =>\n      r._measurement == \"POWER\" and\n      r._field == \"value\" and\n      r.device == \"living_room\"\n      )\n  |> aggregateWindow(every: v.windowPeriod, fn: max)\n  |> map(fn: (r) => ({ _value:r._value, _time:r._time, _field:\"Power (W)\" }))\n`\n```\nPower (W) is the label/alias that is going to be used.\nI wish that Influx would provide an easier way to alias the desired field. The current approach is not very intuitive.",
      "question_score": 20,
      "answer_score": 27,
      "created_at": "2021-06-24T22:09:23",
      "url": "https://stackoverflow.com/questions/68122202/grafana-influxdb-2-label-alias-data"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 67617181,
      "title": "Why is this InfluxDB Flux query returning 2 tables?",
      "problem": "Obv. I'm new to InfluxDB & the Flux query language so appreciate patience! Happy to be redirected to documentation but I haven't found anything genuinely useful to date.\nI've configured Jenkins (2.277.3) to push build metrics to InfluxDB (Version 2.0.5 ('7c3ead)) using plugin (https://plugins.jenkins.io/influxdb/). No custom metrics at the moment. Data is being successfully sent.\nI'd like to build a simple bar chart to show build times for a specific project. Each \"bar\" would be an individual build (with a distinct build number). Also:\n\nX-axis, date/time of build\nY-axis, duration of build\n(Ideally bars would be green/red to indicate success/anything else and would be labelled with job number. In time I'd like to add an overlay with average build time.)\n\nI'm trying to create the query(ies) to support this view:\n```\n`from(bucket: \"db0\")\n  |> range(start: -2d)\n  |> filter(fn: (r) => r[\"project_name\"] == \"Job2\")\n  |> filter(fn: (r) => r._measurement == \"jenkins_data\" and r._field == \"build_time\" )\n`\n```\nThis results in 2 tables in the Table view, one for build SUCCESS and one for build FAILURE. Can someone explain to be why this is the case, and whether I'm missing some fundamental understanding of how to use the tool?",
      "solution": "Accepting @ditoslav's comment as the answer to my question:\n\nEach flux query returns a stream of tables meaning your query can return multiple tables. Each table is created depending on the grouping. If you change the grouping at the end of your query you could merge these tables into 1. The simples example would be to just add ` |> group()` at the end and see that now you are getting just 1 table.",
      "question_score": 6,
      "answer_score": 20,
      "created_at": "2021-05-20T10:58:35",
      "url": "https://stackoverflow.com/questions/67617181/why-is-this-influxdb-flux-query-returning-2-tables"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 69352264,
      "title": "Problems setting up a docker based InfluxDB/Grafana network",
      "problem": "My intention is to use Windows Docker to deploy an InfluxDB 2.0.8 database and link to it via a self-hosted Grafana instance on the same docker network.\nTo do so, I've done the below steps:\n\nStart the network, InfluxDB and Grafana via below:\n\n```\n`docker network create influxdb\ndocker run -d --net=influxdb --name=grafana -p 3000:3000 grafana/grafana\ndocker run -d --net=influxdb --name=influxdb -p 8086:8086 --volume C:/influxdb:/var/lib/influxdb2 influxdb:2.0.8\n`\n```\n\nOpen http://localhost:8086 to set up the basics. Via the InfluxDB UI get the below details:\n\nOrg ID (from the About page)\nToken (from the Token page, using the already generated one)\nDefault bucket (the bucket I created)\nURL (http://localhost:8086)\n\nI then go to Grafana via localhost:3000 and add an InfluxDB data storage (Flux) and enter the above details. However, when I test it, I get an \"Error reading InfluxDB\". In the console, I get the below error:\n\n```\n`POST http://localhost:3000/api/ds/query 400 (Bad Request)\n{refId: 'test', message: 'Post \"http://localhost:8086/api/v2/query?org=35e0f\u2026l tcp 127.0.0.1:8086: connect: connection refused'}\n`\n```\nAny idea what might be missing in the above?",
      "solution": "I think another way to access influx from grafana would be accessing it from container name:\nsetup influxdb data source as:\ninfluxdb:8086",
      "question_score": 6,
      "answer_score": 6,
      "created_at": "2021-09-27T21:27:16",
      "url": "https://stackoverflow.com/questions/69352264/problems-setting-up-a-docker-based-influxdb-grafana-network"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 68274418,
      "title": "How to get column names of a table in influxdb",
      "problem": "I am trying to work with a database that I barly know and I need to know  column names of a table:\nHere is what I tried:\n`client = DataFrameClient(host, 8086, username, password, \"marketdata\")\nclient.switch_database('marketdata')\nprint(client.query(\"show measurements\"))\n# ResultSet({'('measurements', None)': [{'name': 'bookTicker'}]})\n\nquery = \"SELECT COUNT(DISTINCT bookTicker) FROM information_schema.columns WHERE table_schema = 'marketdata'\"\n\ndataframes = client.query(query)\n`\n`raise InfluxDBClientError(self.error)\ninfluxdb.exceptions.InfluxDBClientError: retention policy not found: information_schema\n`\nI also tried:\n`query = \"select * from bookTicker limit 10\"\n`\n`raise ChunkedEncodingError(e)\nrequests.exceptions.ChunkedEncodingError: (\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read))\n`",
      "solution": "You are trying to use `SQL` knowledge for nonSQL InfluxDB.\n`InfluxDB` doesn't have concept of columns, but tags (in SQL meaning something like indexed column) and fields (something like a column).\nYou can explore them with the queries:\n\nSHOW TAG KEYS [ON ] [FROM_clause] [WHERE   ['' | ]] [LIMIT_clause] [OFFSET_clause]\n\nhttps://docs.influxdata.com/influxdb/v1.7/query_language/schema_exploration/#show-tag-keys\n\nSHOW FIELD KEYS [ON ] [FROM ]\n\nhttps://docs.influxdata.com/influxdb/v1.7/query_language/schema_exploration/#show-field-keys",
      "question_score": 6,
      "answer_score": 4,
      "created_at": "2021-07-06T18:43:15",
      "url": "https://stackoverflow.com/questions/68274418/how-to-get-column-names-of-a-table-in-influxdb"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 70765969,
      "title": "Finding the Sum based on Group using Flux Query - InfluxDB",
      "problem": "I am trying to use flux query language to find the sum of a column based on a person.\nIf I have the following input table:\n\nHow can I use a Flux Query to obtain the following output table:\n\nI have tried something like this so far but I get errors:\n```\n`from: (bucket: \"example\")\n  |> range(start:v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r)=> r[\"_measurement\"] == \"test\")\n  |> group(columns: r[\"person\"])\n  |> reduce( fn: (r, accumulator) => ({sum: r._value + accumulator.sum}), identity: {sum: 0})\n`\n```",
      "solution": "You are almost on the right way. You have already grouped by name and now you need to use function `sum`. Pay attention on last function - `|> group()` it's just for union table to one view.\n```\n`|> group(columns: [\"person\"])\n|> sum(column: \"hoursSpent\")\n|> group()\n`\n```\n\nI provide full of my query for debug:\n```\n`import \"array\"\n\ndata = array.from(rows: [\n  {person: \"John Smith\", sport: \"Cycling\", hoursSpent: 5},\n  {person: \"John Smith\", sport: \"Hiking\", hoursSpent: 6},\n  {person: \"John Smith\", sport: \"Swimming\", hoursSpent: 1},\n  {person: \"John Smith\", sport: \"Dancing\", hoursSpent: 2},\n  {person: \"Nancy Jones\", sport: \"Badminton\", hoursSpent: 10},\n  {person: \"Nancy Jones\", sport: \"Soccer\", hoursSpent: 31},\n  {person: \"Nancy Jones\", sport: \"Basketball\", hoursSpent: 8},\n  {person: \"Trevor John\", sport: \"Baseball\", hoursSpent: 24},\n  {person: \"Trevor John\", sport: \"Water Polo\", hoursSpent: 2},\n])\n\ndata\n |> group(columns: [\"person\"])\n |> sum(column: \"hoursSpent\")\n |> group()\n`\n```",
      "question_score": 5,
      "answer_score": 8,
      "created_at": "2022-01-19T07:09:18",
      "url": "https://stackoverflow.com/questions/70765969/finding-the-sum-based-on-group-using-flux-query-influxdb"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 70291589,
      "title": "Grouping influx data per day using flux",
      "problem": "I have been wrangling with a time grouping issue in influxDB using the flux query language. I can illustrate with generated data, one entry per day from 2021-01-01 to 2021-01-05.\n```\n`import \"generate\"\ndata = generate.from(\n  count: 5,\n  fn: (n) => n + 1,\n  start: 2021-01-01T00:00:00Z,\n  stop: 2021-01-06T00:00:00Z,\n)\n\ndata\n  |> range(start: 2021-01-01T00:00:00Z, stop: 2021-01-05T05:00:00Z) \n`\n```\nthat generates:\n\n_table\n_value\n_start\n_stop\n_time\n\n0\n1\n2021-01-01T00:00:00.000Z\n2021-01-05T23:59:00.000Z\n2021-01-01T00:00:00.000Z\n\n0\n2\n2021-01-01T00:00:00.000Z\n2021-01-05T23:59:00.000Z\n2021-01-02T00:00:00.000Z\n\n0\n3\n2021-01-01T00:00:00.000Z\n2021-01-05T23:59:00.000Z\n2021-01-03T00:00:00.000Z\n\n0\n4\n2021-01-01T00:00:00.000Z\n2021-01-05T23:59:00.000Z\n2021-01-04T00:00:00.000Z\n\n0\n5\n2021-01-01T00:00:00.000Z\n2021-01-05T23:59:00.000Z\n2021-01-05T00:00:00.000Z\n\nnow I query the data and aggregate per day\n```\n`import \"generate\"\ndata = generate.from(\ncount: 5,\n  fn: (n) => n + 1,\n  start: 2021-01-01T00:00:00Z,\n  stop: 2021-01-06T00:00:00Z,\n)\ndata\n  |> range(start: 2021-01-01T00:00:00Z, stop: 2021-01-05T23:59:00Z)\n  |> aggregateWindow(every: 1d, fn: sum, createEmpty: false)\n`\n```\nI get this\n\n_table\n_value\n_start\n_stop\n_time\n\n0\n1\n2021-01-01T00:00:00.000Z\n2021-01-05T23:59:00.000Z\n2021-01-02T00:00:00.000Z\n\n0\n2\n2021-01-01T00:00:00.000Z\n2021-01-05T23:59:00.000Z\n2021-01-03T00:00:00.000Z\n\n0\n3\n2021-01-01T00:00:00.000Z\n2021-01-05T23:59:00.000Z\n2021-01-04T00:00:00.000Z\n\n0\n4\n2021-01-01T00:00:00.000Z\n2021-01-05T23:59:00.000Z\n2021-01-05T00:00:00.000Z\n\n0\n5\n2021-01-01T00:00:00.000Z\n2021-01-05T23:59:00.000Z\n2021-01-05T23:59:00.000Z\n\nthe first time is `2021-01-02T00:00:00.000Z` and not `2021-01-01` and the two last entries cover the same day `2021-01-05`.\nhow can I get the entries per day as below using flux:\n\n2021-01-01 - 1\n2021-01-02 - 2\n2021-01-03 - 3\n2021-01-04 - 4\n2021-01-05 - 5",
      "solution": "You simply need to change/add the `timeSrc` parameter in your `aggregateWindow` window function. By default, Flux will use `_stop` as value of the parameter, creating \"high\" bound windows.\nLet's make your example a bit simpler with the time not being exact at day end/start for better illustration: If for instance, it is 22:20 system time, I issue a query with range \"-1h\" and group by 1h, Flux will create two tumbling time windows: 21:00-22:00 and 22:00-23:00. In fact, Flux detects its range and truncates those global time windows to 21:20-22:00 and 22:00-22:20.\nIf now, aggregateWindow timeSrc is set to _stop, the time column will be 22:00 and 22:20. [the end of each previously mentioned time windows]. If you chose \"_start\", the time column will be 21:20 and 22:00.\nIf you want 21:00 and 22:00 in the above example, I suggest adjusting your range to something like: `range(start: date.truncate(t: -24h, unit: 1h)` which usually also makes more sense that having a first bucket also contains all the data from that bucket.",
      "question_score": 4,
      "answer_score": 5,
      "created_at": "2021-12-09T15:24:43",
      "url": "https://stackoverflow.com/questions/70291589/grouping-influx-data-per-day-using-flux"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 67562098,
      "title": "What does the Angular &quot;strict-origin-when-cross-origin&quot; Error mean?",
      "problem": "From my app I want to reach an API. The curl request is:\n```\n`curl --request POST   https://...    --header 'Authorization: Token ...'   --header 'Accept: application/csv'   --header 'Content-type: application/vnd.flux'   --data '...'\n`\n```\nIt works and returns data from an InfluxDB instance. The problem is when I try everything in Angular: I set up an proxy which redirects the requests correctly (I check this in the terminal). But in the web browser (Firefox Developer Edition) I get:\n```\n`POSThttp://localhost:4200/api/\n[HTTP/1.1 401 Unauthorized 204ms].\n\nStatus 401\nUnauthorized\nVersion HTTP/1.1\n\u00dcbertragen 350 B (55 B Gr\u00f6\u00dfe)\nReferrer Policy strict-origin-when-cross-origin\n`\n```\nThe API URL is a https address, same error happens with http. Same happens in the production app.\nproxy.conf\n```\n`{\n\"/api/*\": {\n\"target\": \"...\",\n\"secure\": false,\n\"logLevel\": \"debug\",\n\"changeOrigin\": true\n}\n}\n`\n```\nTerminal confirmation:\n```\n`[HPM] POST /api/ -> correct url\n`\n```\nRequest in component:\n```\n`apiurl = '/api/';\ndata = JSON.stringify('request');\nheaders:HttpHeaders = new HttpHeaders();;\n\nconstructor(private route: ActivatedRoute,\nprivate httpClient: HttpClient) {\nthis.headers.set('Authorization', 'Token token');\nthis.headers.set('Content-type', 'application/vnd.flux');\n}\n\ngetConfig() {\nreturn this.httpClient.request('post',`${this.apiurl}`,\n    {\n      body: this.data,\n      headers: this.headers\n    }\n );\n}\n\nngOnInit(): void {\n\nthis.getConfig()\n.pipe(first())\n.subscribe(\n data => {\n   console.log(data);\n },\n error => {\n   console.log(error);\n });\n  }\n`\n```\nI would appreciate a look your ideas!",
      "solution": "You missed `\"pathRewrite\": { \"^/api\": \"\" }`\nChange your proxy configuration from this:\n```\n`{\n \"/api/*\": {\n   \"target\": \"...\",\n   \"secure\": false,\n   \"logLevel\": \"debug\",\n   \"changeOrigin\": true\n }\n}\n`\n```\nTo this:\n```\n`{\n \"/api/*\": {\n   \"target\": \"...\",\n   \"secure\": false,\n   \"logLevel\": \"debug\",\n   \"changeOrigin\": true,\n   \"pathRewrite\": { \"^/api\": \"\" }\n }\n}\n`\n```",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2021-05-17T00:34:41",
      "url": "https://stackoverflow.com/questions/67562098/what-does-the-angular-strict-origin-when-cross-origin-error-mean"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 74736334,
      "title": "InfluxDB CLI 401 Unauthorized",
      "problem": "```\n`$ dpkg --print-architecture\namd64\n$ lsb_release -a\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 18.04.6 LTS\nRelease:        18.04\nCodename:       bionic\n`\n```\nInstalled InfluxDB according to the manual\nInstalled InfluxDB CLI according to the manual\nMade the initial configuration in InfluxBD UI(http://localhost:8086/) by setting user, password and operator token.\n`$ influx version`\n`Influx CLI 2.5.0 (git: 3285a03) build_date: 2022-11-01T16:32:06Z`\nProvide required authentication credentials.\n```\n`influx config create --config-name default \\\n  --host-url http://localhost:8086 \\\n  --org myInitialOrg \\\n  --token myOperatorToken \\\n  --active\n\n  Active  Name    URL                     Org\n*       default http://localhost:8086   myInitialOrg\n$ influx config\nActive  Name    URL                     Org\n*       default http://localhost:8086   myInitialOrg\n`\n```\nNext, I get a 401 response code for almost all commands:\n```\n`$ influx user list\nError: failed to list users: 401 Unauthorized: unauthorized access\n$ influx auth list\nError: could not find authorization with given parameters: 401 Unauthorized: unauthorized access\n$ influx org list\nError: failed to list orgs: 401 Unauthorized: unauthorized access\n$ influx dashboards\nError: failed to find dashboards: 401 Unauthorized: unauthorized access\n`\n```\n...\nHere are the configuration files + logs + server status:\n```\n`$ nano ~/.influxdbv2/configs\n\n[default]\n  url = \"http://localhost:8086\"\n  token = \"myOperatorToken\"\n  org = \"myInitialOrg\"\n  active = true\n# \n# [eu-central]\n#   url = \"https://eu-central-1-1.aws.cloud2.influxdata.com\"\n#   token = \"XXX\"\n#   org = \"\"\n# \n# [us-central]\n#   url = \"https://us-central1-1.gcp.cloud2.influxdata.com\"\n#   token = \"XXX\"\n#   org = \"\"\n# \n# [us-west]\n#   url = \"https://us-west-2-1.aws.cloud2.influxdata.com\"\n#   token = \"XXX\"\n#   org = \"\"\n\n$ nano /etc/influxdb/config.toml\n\nbolt-path = \"/var/lib/influxdb/influxd.bolt\"\nengine-path = \"/var/lib/influxdb/engine\"\n\n$ cd /var/log/influxdb/\n$ ls -al\n\ntotal 8\ndrwxr-xr-x  2 influxdb influxdb 4096 \u0433\u0440\u0443  8 21:16 .\ndrwxrwxr-x 22 root     syslog   4096 \u0433\u0440\u0443  8 21:16 ..\n\n$ service influxdb status\n\u25cf influxdb.service - InfluxDB is an open-source, distributed, time series database\n   Loaded: loaded (/lib/systemd/system/influxdb.service; enabled; vendor preset: enabled)\n   Active: active (running) since Thu 2022-12-08 22:15:52 EET; 31min ago\n     Docs: https://docs.influxdata.com/influxdb/\n  Process: 31456 ExecStart=/usr/lib/influxdb/scripts/influxd-systemd-start.sh (code=exited, status=0/SUCCESS)\n Main PID: 31459 (influxd)\n    Tasks: 10 (limit: 4915)\n   CGroup: /system.slice/influxdb.service\n           \u2514\u250031459 /usr/bin/influxd\n\ninfluxd-systemd-start.sh[31456]: ts=2022-12-08T20:46:06.013581Z lvl=info msg=Unauthorized log_id=0edqvnil000 error=\"authorization not found\"\n`\n```\nWhen I want to create an all access token through InfluxDB CLI, it does not see the organization created through InfluxBD UI:\n```\n`$ influx auth create \\\n>   --all-access \\\n>   --host http://localhost:8086 \\\n>   --org myInitialOrg \\\n>   --token myOperatorToken\nError: failed to lookup org with name \"myInitialOrg\": 401 Unauthorized: unauthorized access\n`\n```\nI can create organizations, buckets and such in InfluxBD UI, but I can't do almost anything in CLI. :( I understand that there is some kind of mess with user rights, but I don't understand where to look for the reason.\nI tested installation InfluxBD under root and under a normal user.\nAfter starting the daemon, I get an authorization error:\n```\n`~$ influxd\n2022-12-13T07:44:19.829656Z     info    Welcome to InfluxDB     {\"log_id\": \"0ejbuoTG000\", \"version\": \"v2.5.1\", \"commit\": \"5b6fdbf05d\", \"build_date\": \"2022-11-02T18:06:28Z\", \"log_level\": \"info\"}\n2022-12-13T07:44:19.852907Z     info    Resources opened        {\"log_id\": \"0ejbuoTG000\", \"service\": \"bolt\", \"path\": \"/home/sergey/.influxdbv2/influxd.bolt\"}\n2022-12-13T07:44:19.853282Z     info    Resources opened        {\"log_id\": \"0ejbuoTG000\", \"service\": \"sqlite\", \"path\": \"/home/sergey/.influxdbv2/influxd.sqlite\"}\n2022-12-13T07:44:19.854955Z     info    Bringing up metadata migrations {\"log_id\": \"0ejbuoTG000\", \"service\": \"KV migrations\", \"migration_count\": 20}\n2022-12-13T07:44:20.053738Z     info    Bringing up metadata migrations {\"log_id\": \"0ejbuoTG000\", \"service\": \"SQL migrations\", \"migration_count\": 8}\n2022-12-13T07:44:20.214786Z     info    Using data dir  {\"log_id\": \"0ejbuoTG000\", \"service\": \"storage-engine\", \"service\": \"store\", \"path\": \"/home/sergey/.influxdbv2/engine/data\"}\n2022-12-13T07:44:20.215516Z     info    Compaction settings     {\"log_id\": \"0ejbuoTG000\", \"service\": \"storage-engine\", \"service\": \"store\", \"max_concurrent_compactions\": 2, \"throughput_bytes_per_second\": 50331648, \"throughput_bytes_per_second_burst\": 50331648}\n2022-12-13T07:44:20.215630Z     info    Open store (start)      {\"log_id\": \"0ejbuoTG000\", \"service\": \"storage-engine\", \"service\": \"store\", \"op_name\": \"tsdb_open\", \"op_event\": \"start\"}\n2022-12-13T07:44:20.216077Z     info    Open store (end)        {\"log_id\": \"0ejbuoTG000\", \"service\": \"storage-engine\", \"service\": \"store\", \"op_name\": \"tsdb_open\", \"op_event\": \"end\", \"op_elapsed\": \"0.455ms\"}\n2022-12-13T07:44:20.216250Z     info    Starting retention policy enforcement service   {\"log_id\": \"0ejbuoTG000\", \"service\": \"retention\", \"check_interval\": \"30m\"}\n2022-12-13T07:44:20.216327Z     info    Starting precreation service    {\"log_id\": \"0ejbuoTG000\", \"service\": \"shard-precreation\", \"check_interval\": \"10m\", \"advance_period\": \"30m\"}\n2022-12-13T07:44:20.221739Z     info    Starting query controller       {\"log_id\": \"0ejbuoTG000\", \"service\": \"storage-reads\", \"concurrency_quota\": 1024, \"initial_memory_bytes_quota_per_query\": 9223372036854775807, \"memory_bytes_quota_per_query\": 9223372036854775807, \"max_memory_bytes\": 0, \"queue_size\": 1024}\n2022-12-13T07:44:20.236566Z     info    Configuring InfluxQL statement executor (zeros indicate unlimited).     {\"log_id\": \"0ejbuoTG000\", \"max_select_point\": 0, \"max_select_series\": 0, \"max_select_buckets\": 0}\n2022-12-13T07:44:20.247538Z     info    Starting        {\"log_id\": \"0ejbuoTG000\", \"service\": \"telemetry\", \"interval\": \"8h\"}\n2022-12-13T07:44:20.247735Z     info    Listening       {\"log_id\": \"0ejbuoTG000\", \"service\": \"tcp-listener\", \"transport\": \"http\", \"addr\": \":8086\", \"port\": 8086}\n2022-12-13T07:44:27.087602Z     info    Unauthorized    {\"log_id\": \"0ejbuoTG000\", \"error\": \"authorization not found\"}\n2022-12-13T07:44:37.085461Z     info    Unauthorized    {\"log_id\": \"0ejbuoTG000\", \"error\": \"authorization not found\"}\n2022-12-13T07:44:47.086652Z     info    Unauthorized    {\"log_id\": \"0ejbuoTG000\", \"error\": \"authorization not found\"}\n2022-12-13T07:44:57.088191Z     info    Unauthorized    {\"log_id\": \"0ejbuoTG000\", \"error\": \"authorization not found\"}\n2022-12-13T07:45:07.092043Z     info    Unauthorized    {\"log_id\": \"0ejbuoTG000\", \"error\": \"authorization not found\"}\n`\n```\nThrough the web interface(InfluxBD UI), I created a initial user + operator token + initial organization + initial bucket.\nCommands using the operator token also give an authorization error:\n```\n`~$ influx user list -t web-token\nError: failed to list users: 401 Unauthorized: unauthorized access\n~$ influx auth list -t web-token\nError: could not find authorization with given parameters: 401 Unauthorized: unauthorized access\n~$ influx auth list -u web-user -t \"web-token\"\nError: could not find authorization with given parameters: 401 Unauthorized: unauthorized access\n`\n```",
      "solution": "I understand that this is not a completely correct answer for the situation, but it solved the problem in general. After switching to `Ubuntu 20.04.6 LTS`, absolutely all problems were solved. Aggressive decision :(",
      "question_score": 4,
      "answer_score": 1,
      "created_at": "2022-12-08T22:07:09",
      "url": "https://stackoverflow.com/questions/74736334/influxdb-cli-401-unauthorized"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 73339917,
      "title": "How do I limit records for Influx using flux?",
      "problem": "I'm new with Influx, and although I can get `range`, `filter`, `sort` and `group` to work, I just can't get `limit` to work.\nI'm using Influx OSS 2.3 and I'd assume the query should look like this:\n```\n`from(bucket: \"readings\") \n    |> range(start: - 1d) \n    |> limit(n:10)\n`\n```\nI've tried both through the Data explorer and the C# sdk, but it always returns 400 records.\nIf I use `offset` I'm not getting any records:\n```\n`from(bucket: \"readings\") \n    |> range(start: - 1d) \n    |> limit(n:10, offset:2)\n`\n```\nThanks for the help",
      "solution": "I believe @alespour was on the right track. If you have tags, these will implicitly group the data into independent tables. In flux, each operation is applied to each table independently, so if you have 40 distinct tag values, a call to `limit(n: 10)` would return 400.\nUsing `group()` with no arguments will drop grouping and the number of records you specify will be applied to all results.\n```\n`from(bucket: \"readings\") \n    |> range(start: - 1d) \n    |> group()\n    |> limit(n:10)\n`\n```",
      "question_score": 3,
      "answer_score": 4,
      "created_at": "2022-08-12T23:22:09",
      "url": "https://stackoverflow.com/questions/73339917/how-do-i-limit-records-for-influx-using-flux"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 73664619,
      "title": "How to correctly await a Swift callback closure result",
      "problem": "In the past when I needed to wait for a Swift callback closure (in this case to the InfluxDB Swift API) to produce a result I used a semaphore to signal the completion.\n```\n`    func getChargeList(from: Int, until: Int) async -> [Charge] {\n        let flux =  \"\"\"\n                    from(bucket: \"\\(self.bucket)\")\n                        ...\n                    \"\"\"\n        let queryTask = Task { () -> [Charge] in\n            **let s = DispatchSemaphore(value: 0)**\n            var chargeSessions: [Charge] = []\n            self.client.queryAPI.query(query: flux) { response, error in\n                if let error = error {\n                    print(\"getChargeList error: \\(error)\")\n                }\n                if let response = response {\n                    do {\n                        ...\n                }\n                **s.signal()**\n            }\n            **s.wait()**\n            return chargeSessions\n        }\n        return await queryTask.value\n    }\n`\n```\nAs of Swift 5.7 I get a warning\n`Instance method 'wait' is unavailable from asynchronous contexts; Await a Task handle instead; this is an error in Swift 6`\nso time to look at solutions.\nBeing new to Swift and asynchronous programming I have not come up to a solution to the InfluxDB query API returning immediately and then suspending execution until the query results are returned using the trailing closure. Hopefully just missing something something simple due to my lack of experience so any comments will be appreciated.\nI have opened an issue with the InfluxDB Swift API repo to consider using new async/await standard but a workaround or solution ahead of an updated library would be useful to have.",
      "solution": "You don't need task. Instead, do this:\n\nWrap your query in function with callback:\n\n```\n`func runQuery(from: Int, until: Int, callback: (Result) -> ()) {\n     let flux =  \"\"\"\n                    from(bucket: \"\\(self.bucket)\")\n                        ...\n                    \"\"\"\n     self.client.queryAPI.query(query: flux) { response, error in\n         if let error = error {\n             callback(.failure(error))\n             return\n         }\n         if let response = response {\n             let chargeSessions = ...\n             callback(.success(chargeSessions))\n         }\n    }\n}\n`\n```\n\nCreate an await/async wrapper for this function:\n\n```\n`func getChargeList(from: Int, until: Int) async -> Result {\n    await withCheckedContinuation { continuation in\n        runQuery(from: from, until: until) { result in\n            continuation.resume(returning: result)\n        }\n    }\n}\n`\n```\nThe use of `Result` is optional of course, but it allows to conveniently pack both successful and failed cases.\nAlso you could fit it into 1 function, but then you have 4 level of braces - not a convenient code to read.",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2022-09-09T17:46:53",
      "url": "https://stackoverflow.com/questions/73664619/how-to-correctly-await-a-swift-callback-closure-result"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 68272397,
      "title": "How to make sense of the micrometer metrics using SpringBoot 2, InfluxDB and Grafana?",
      "problem": "I'm trying to configure a SpringBoot application to export metrics to InfluxDB to visualise them using a Grafana dashboard. I'm using this dashboard as an example which uses Prometheus as a backend.\nFor some metrics I have no problem figuring out how to create graphs for them but for some others I don't know how to create the graphs or even if it's possible at all. So I enumerate the things I'm not really sure about in the following points:\n\nIs there any documentation where a value unit is described? The application I'm using as an example doesn't have any load on it so sometimes I don't know whether the value is a bit, a byte, a second, a millisecond, a count, etc.\n\nSome measurements contain the tag 'metric_type = histogram' with fields 'count', 'sum', 'mean' and 'upper'. Again, here I don't know what the value units are, what upper means or how I'm suppose to plot them. Examples of this are 'http_server_requests' or 'jvm_gc_pause'.\n\nFrom what I see in the Grafana dashboard example, it seems I should use these measurements of type histogram to create both a graph with counts and graphs with duration. For example I see I should be able to create a graph with the number of requests and another one with their duration. Or for the garbage collector, I should be able to provide a graph for the number of minor and major GCs and another for their duration.\n\nAs an example of measures I get inserted into InfluxDB:\n```\n`time                 count exception mean     method metric_type outcome status sum      upper    uri\n1625579637946000000  1     None      0.892144 GET    histogram   SUCCESS 200    0.892144 0.892144 /actuator/health\n`\n```\nor\n```\n`time                action          cause                 count   mean  metric_type  sum upper\n1625581132316000000 end of minor    GC Allocation Failure     1      2  histogram    2   2\n`\n```",
      "solution": "I agree the documentation for micrometer is not great. I've had to dig through the code to find answers...\nRegarding your questions about jvm_gc_pause, it is a Timer and the implementation is `AbstractTimer` which is a class that wraps a `Histogram` among other components.  This particular metric is registered by the `JvmGcMetrics` class. The various measurements that are published to InfluxDB are determined by the `InfluxMeterRegistry.writeTimer(Timer timer)` method:\n\nsum: `timer.totalTime(getBaseTimeUnit()) // The total time of recorded events`\ncount: `timer.count() // The number of times stop has been called on the timer`\nmean: `timer.mean(getBaseTimeUnit()) // totalTime()/count()`\nupper: `timer.max(getBaseTimeUnit()) // The max time of a single event`\n\nThe base time unit is milliseconds.\nSimilarly, http_server_requests appears to be a `Timer` as well.\nI believe you are correct that the sensible thing is to chart on two separate Grafana panels: one panel for GC pause seconds using `sum` (or `mean` or `upper`), and one panel for GC events using `count`.",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2021-07-06T16:26:22",
      "url": "https://stackoverflow.com/questions/68272397/how-to-make-sense-of-the-micrometer-metrics-using-springboot-2-influxdb-and-gra"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 79847037,
      "title": "Use _value in custom function in Flux - InfluxDB",
      "problem": "Using Flux, how to add the `_value` in a custom functions ?\nFor example, say I want to divide the obtained `_value` by 4. This does not work:\n```\n`myFunction = (x= x / 4\nfrom(bucket: \"myBucket\")\n  |> range(start: -5m)\n  |> filter(fn: (r) => r._measurement == \"foo\" and r._field == \"bar\")\n  |> myFunction(_value)\n`\n```\nHow to formulate `  |> myFunction(_value)` ?\nI don't have any practical problem. It is just for learning purposes.",
      "solution": "Credit to nextfauzan for helping me to find the right function.\nYou can use the map function:\n```\n`from(bucket: \"myBucket\")\n  |> range(start: -5m)\n  |> filter(fn: (r) => r._measurement == \"foo\" and r._field == \"bar\")\n  |> map(fn: (r) => ({r with _value: r._value / 4}))\n`\n```",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2025-12-14T14:07:31",
      "url": "https://stackoverflow.com/questions/79847037/use-value-in-custom-function-in-flux-influxdb"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 72684205,
      "title": "InfluxDB2 / Grafana : how can we filter a list of tag values with flux",
      "problem": "InfluxDB2 flux language provides a convenient way to get all the tags values for a specific bucket/measurement combination, using the schema.measurementTagValues function.\nThe problem is that the documentation doesn't mention how to filter this list to keep only the tag values that match a certain criteria.\nExample : \nWith the following query, I can get all the transaction tag values :\n```\n`import \"influxdata/influxdb/schema\"\n\nschema.measurementTagValues(\n    bucket: \"jmeter\",\n    measurement: \"jmeter\",\n    tag: \"transaction\",\n)\n`\n```\nThe schema contains another tag named \"application\". I want to get all the transactions for a specific application, not all of them.\nHow can we achieve this with flux?\nThe same request in InfluxQL would be pretty straightforward :\n`SHOW TAG VALUES FROM \"jmeter\" WITH KEY = \"transaction\" WHERE \"application\" = $application`\nThe goal is to create Grafana dynamic dropdown lists like this one :",
      "solution": "If you use `schema.tagValues()` instead of `schema.measurementTagValues()` you can define a predicate function that filters you results. In your example:\n```\n`import \"influxdata/influxdb/schema\"\n\nschema.tagValues(\n    bucket: \"jmeter\",\n    tag: \"transaction\",\n    predicate: (r) => r._measurement == \"jmeter\" and r.application == ${application:doublequote},\n    start: -3000d\n)\n`\n```\nAs you see you need to define a time range for this function. You can use the timerange variables of the grafana dashboard here (`v.timeRangeStart` and `v.timeRangeStop`) if suitable.",
      "question_score": 2,
      "answer_score": 9,
      "created_at": "2022-06-20T10:27:49",
      "url": "https://stackoverflow.com/questions/72684205/influxdb2-grafana-how-can-we-filter-a-list-of-tag-values-with-flux"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 68338777,
      "title": "POST gzip to influxdb results in &quot;unexpected EOF&quot;",
      "problem": "Trying to write a gzip post request to influxdb, but getting \"unexpected EOF\"\nThe following code is in F#, but it is straightforward. You can translate it easily in your mind to C#\n`let gzip(s: string) =\n    let bytes = Encoding.UTF8.GetBytes s\n    use ms = new MemoryStream()\n    use gz = new GZipStream(ms, CompressionLevel.Fastest)\n    (gz.Write(bytes, 0, bytes.Length))\n    ms.ToArray()\nlet toGzipContent(s: string) =\n    let content = new ByteArrayContent(gzip s)\n    content.Headers.ContentEncoding.Add \"gzip\"\n    content.Headers.ContentType  toGzipContent\n    let resp = client.PostAsync(cfg.url, content)\n    resp\n`\nThe influxdb response:\n```\n`  StatusCode: 400, ReasonPhrase: 'Bad Request', Version: 1.1, Content: System.Net.Http.HttpConnectionResponseContent, Headers:\n{\n  Request-Id: a1edc900-e26e-11eb-b78b-244bfe8c4492\n  X-Influxdb-Build: OSS\n  X-Influxdb-Error: unexpected EOF\n  X-Influxdb-Version: 1.8.6\n  X-Request-ID: a1edc900-e26e-11eb-b78b-244bfe8c4492\n  Date: Sun, 11 Jul 2021 17:37:15 GMT\n  Content-Type: application/json\n  Content-Length: 27\n}\n`\n```\nSending plain text works, but gzipped not. What could be wrong here? Already looked into influxdb .NET client implementations and found no issues",
      "solution": "The gzipped `content` is empty, the stream needs to be flushed. Inserting `gz.Flush()` between writing and converting the underlying `ms.ToArray()` should fix this.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-07-11T19:58:49",
      "url": "https://stackoverflow.com/questions/68338777/post-gzip-to-influxdb-results-in-unexpected-eof"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 70171542,
      "title": "Query last value in Flux",
      "problem": "I'm trying to get the last value from some IoT sensors and I actually achieved an intermediary result with the following Flux query:\n```\n`from(bucket:\"mqtt-bucket\")\n|> range(start:-10m )\n|> filter(fn: (r) => r[\"_measurement\"] == \"mqtt_consumer\")\n|> filter(fn: (r) => r[\"thingy\"] == \"things/green-1/shadow/update\"\n                  or r[\"thingy\"] == \"things/green-3/shadow/update\" \n                  or r[\"thingy\"] == \"things/green-2/shadow/update\")\n|> filter(fn: (r) => r[\"_field\"] == \"data\")\n|> filter(fn: (r) => r[\"appId\"] == \"TEMP\" or r[\"appId\"] == \"HUMID\")\n|> toFloat()\n|> last()\n`\n```\nThe problem: I would like to get the last mesured value independently of a time range.\nI saw in the docs that there is no way to unbound the range function. Maybe there is a work around ?",
      "solution": "I just found this:\n```\n`from(bucket: \"stockdata\")\n  |> range(start: 0)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"nasdaq\")\n  |> filter(fn: (r) => r[\"symbol\"] == \"OPEC/ORB\")\n  |> last()\n`\n```",
      "question_score": 2,
      "answer_score": 5,
      "created_at": "2021-11-30T16:04:55",
      "url": "https://stackoverflow.com/questions/70171542/query-last-value-in-flux"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 69774634,
      "title": "Telegraf can not connect to Docker sock",
      "problem": "I try to gather some metrics about my Docker containers using Telegraf. I have mounted the docker sock to it but I still receive an error message. What am I missing here?\n```\n`    volumes:\n      - ./data/telegraf:/etc/telegraf\n      - /var/run/docker.sock:/var/run/docker.sock\n`\n```\n```\n`2021-10-29T20:11:30Z E! [inputs.docker] Error in plugin: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get \"http:///var/run/docker.sock/v1.21/containers/json?filters={\"status\":[\"running\"]}&limit=0\": dial unix /var/run/docker.so\n`\n```\n```\n`[[inputs.docker]]\n  endpoint = \"unix:///var/run/docker.sock\"\n  gather_services = false\n  container_names = []\n  source_tag = false\n  container_name_include = []\n  container_name_exclude = []\n  timeout = \"5s\"\n  perdevice = true\n  total = false\n  docker_label_include = []\n  docker_label_exclude = []\n  tag_env = [\"JAVA_HOME\", \"HEAP_SIZE\"]\n`\n```",
      "solution": "The Telegraf Docker images now run the telegraf process as the `telegraf` user/group and no longer as the `root` user. In order to monitor the docker socket, which is traditionally owned by `root:docker group`, you need to pass the group into the telegraf user.\nThis can be done via:\n```\n`--user telegraf:$(stat -c '%g' /var/run/docker.sock)\n`\n```",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2021-10-29T22:50:04",
      "url": "https://stackoverflow.com/questions/69774634/telegraf-can-not-connect-to-docker-sock"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 68389089,
      "title": "delete data from influxdb 2.0",
      "problem": "I want to delete data from influxdb v2.0:\nI read its doc and I try 2 ways that it says, but I get error.\nhttps://docs.influxdata.com/influxdb/v2.0/write-data/delete-data/\nin cli:\n```\n`influx delete \\\n--host HOST \\\n--org ORG \\\n--token TOKEN \\\n--bucket BUCKET \\\n--start 2021-06-01T00:00:00Z \\\n--stop 2021-06-01T01:00:00Z\n`\n```\nerror:\n```\n`Error: Failed to delete data: Not implemented.\nSee 'influx delete -h' for help\n`\n```\nCan you help me, how can I delete data?",
      "solution": "Delete data from same host:\n```\n`influx delete --bucket example-bucket \\\n  --start 2020-03-01T00:00:00Z \\\n  --stop 2020-11-14T00:00:00Z\n`\n```\nYou can also delete data via Curl\n```\n`curl --request POST https://influxurl/api/v2/delete?org=example-org&bucket=example-bucket \\\n  --header 'Authorization: Token YOUR_API_TOKEN' \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n    \"start\": \"2022-01-19T06:32:00Z\",\n    \"stop\": \"2022-01-19T06:33:00Z\",\n    \"predicate\": \"_measurement=\\\"example-measurement\\\" AND feature=\\\"temp2\\\" \"\n  }'\n`\n```\nPredicate method not working properly. (bug)",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2021-07-15T08:43:11",
      "url": "https://stackoverflow.com/questions/68389089/delete-data-from-influxdb-2-0"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 67869086,
      "title": "How can i use influxdb in django project",
      "problem": "i have some trouble about influxdb+django configurations.\nFirstly let me summarize my situation. I have an influxdb which is already collecting data from endnode(sensors). Data is transfering by LoraWan technology. I can read that datas from terminal by writing flux queries so database is working without any problem.\nNow my second phase of this project is visualizing that datas on an web page. I am using django framework for that i completed the frontend parts nearly. I looked on internet for the configurations for influxdb on django but i couldnt handle it. In django documentation page they are listed some databases like below:\nDjango officially supports the following databases:\nPostgreSQL\nMariaDB\nMySQL\nOracle\nSQLite\nHow will i use/configure and get data from my influxdb ? Is it possible ? What are the alternative solutions.",
      "solution": "Sure, Django doesn't support InfluxDB for its usual models (authentication and what-have-you, and of course your own apps), but you can simply use the InfluxDB Python client library to make a query in a view and e.g. return JSON data.\nAdapting from the readme, you might have a view like\n```\n`from influxdb_client import InfluxDBClient\n\nclient = InfluxDBClient(url=\"http://localhost:8086\", token=\"my-token\", org=\"my-org\")\n\ndef get_data(request):\n    bucket = \"my-bucket\"\n    query_api = client.query_api()\n    result = query_api.query_csv('from(bucket:\"my-bucket\") |> range(start: -10m)')\n    return JSONResponse(result)\n`\n```",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2021-06-07T11:25:51",
      "url": "https://stackoverflow.com/questions/67869086/how-can-i-use-influxdb-in-django-project"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 66727280,
      "title": "InfluxDB and Grafana: InfluxDB Error: Bad Request | Docker",
      "problem": "I am trying to connect Grafana with InfluxDB, but it throws\nInfluxDB Error: Bad Request\nBoth i have in docker, and I am using this tutorial where he wrote download and run\n```\n`docker pull influxdb\n\ndocker run \\\n-d \\\n--name influxdb \\\n-p 8086:8086 \\\n-e INFLUXDB_DB=sensordata \\\n-e INFLUXDB_ADMIN_USER=root \\\n-e INFLUXDB_ADMIN_PASSWORD=toor \\\n-e INFLUXDB_HTTP_AUTH_ENABLED=true \\\ninfluxdb\n`\n```\nand about Grafana\n```\n`docker pull grafana/grafana\n\ndocker run -d --name=grafana -p 3000:3000 grafana/grafana\n`\n```\nIn Grafana setting I wrote all as were show in tutorial\nurl: http://10.0.1.76:8086/\ndatabase: sensordata\nuser: root\npasswd: toor\nCould please somebody help me with this ?  Thank you !",
      "solution": "In the tutorial you pointed out, is using influxdb version prior to 2.0\nTry\ndocker pull influxdb:1.8.4-alpine\nand use this image to start your influxdb container and it should work.\nThanks",
      "question_score": 2,
      "answer_score": 4,
      "created_at": "2021-03-20T23:58:54",
      "url": "https://stackoverflow.com/questions/66727280/influxdb-and-grafana-influxdb-error-bad-request-docker"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 71017908,
      "title": "influx aggregateWindow with continuous counter",
      "problem": "An influx 2 database stores incrementing values from an mechanical counter (gas counter).\nThe goal is to build a query to get the consumption over a certain interval, e.g. one day.\nWith SQL I would group the data in the desired interval and than calculate max() - min()  +1 for that interval.\nWhat is the preferred way to do that with the flux query language?\n```\n`from(bucket: \"Energy\")\n|> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n|> filter(fn: (r) => r[\"_measurement\"] == \"gas\")\n|> filter(fn: (r) => r[\"kind\"] == \"count\")\n|> aggregateWindow(every: 24h, fn: difference, createEmpty: false)\n|> yield(name: \"interval\")\n`\n```\ndoes not work, error @5:6-5:69: missing required argument column (argument fn)",
      "solution": "The solution is to examine difference() before aggregateWindow and as aggregate function to use sum.\n```\n`from(bucket: \"Energy\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"gas\")\n  |> filter(fn: (r) => r[\"_field\"] == \"count\")\n  |> difference()\n  |> aggregateWindow(every: 1h, fn: sum, createEmpty: false)\n  |> yield(name: \"consumption\")\n`\n```",
      "question_score": 2,
      "answer_score": 3,
      "created_at": "2022-02-07T12:36:43",
      "url": "https://stackoverflow.com/questions/71017908/influx-aggregatewindow-with-continuous-counter"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 68385739,
      "title": "Using params in flux queries with Python influxdb_client",
      "problem": "I am trying to update all of our influxdb python queries, so that they are not vulnerable to sql injections.\nTo do this, I have read that you can use params with the `query_api()` and specifically with the `query_data_frame()` (https://medium.com/sekoia-io-blog/avoiding-injections-with-influxdb-bind-parameters-50f67e379abb)\nThe issue I am running into is that I can not figure out how to get my params to be passed into my queries. Below is an example of one of our queries:\n`client = InfluxDBClient(url=\"localhost:5000\", token=\"\", timeout=100000, retries=0, enable_gzip=True, profilers=\"query, operator\")\nquery_api = client.query_api()\n\nver = \"data\" # This variable would actually come from a function\nparams = {\n    \"ver\": ver,\n}\nquery =                      '''from(bucket: \"db\")\n                                |> range(start: -200d)\n                                |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n                                |> filter(fn: (r) => r._measurement == \"test_result\")\n                                |> filter(fn: (r) => r.version == ver)\n                                |> keep(columns: [\"_time\", \"test\", \"run\", \"status_tag\", \"duration_sec\", \"version\"])'''\n\ndf = query_api.query_data_frame(query=query, params=params)\n`\nRunning the above gives me a `HTTP response body: b'{\"error\":\"type error 5:75-5:78: undefined identifier \\\\\"ver\\\\\"\"}\\n'` error.\nDoes anyone know how to inject params correctly into a flux query with Python?\nI also used the following for help:\nhttps://influxdb-client.readthedocs.io/_/downloads/en/stable/pdf/\nUpdate based on user16442705 question\nI tried another variable name within my dict, and it yielded the same result. I also tried using $ in the query which yielded a different error. See the below code with errors:\n`client = InfluxDBClient(url=\"localhost:5000\", token=\"\", timeout=100000, retries=0, enable_gzip=True, profilers=\"query, operator\")\nquery_api = client.query_api()\n\nver = \"data\" # This variable would actually come from a function\nparams = {\n    \"pVersion\": ver,\n}\nquery =                      '''from(bucket: \"db\")\n                                |> range(start: -200d)\n                                |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n                                |> filter(fn: (r) => r._measurement == \"test_result\")\n                                |> filter(fn: (r) => r.version == pVersion)\n                                |> keep(columns: [\"_time\", \"test\", \"run\", \"status_tag\", \"duration_sec\", \"version\"])'''\n\ndf = query_api.query_data_frame(query=query, params=params)\n`\n`HTTP response body: b'{\"error\":\"type error 5:67-5:80: undefined identifier \\\\\"pVersion\\\\\"\"}\\n'`\n`client = InfluxDBClient(url=\"localhost:5000\", token=\"\", timeout=100000, retries=0, enable_gzip=True, profilers=\"query, operator\")\nquery_api = client.query_api()\n\nver = \"data\" # This variable would actually come from a function\nparams = {\n    \"pVersion\": ver,\n}\nquery =                      '''from(bucket: \"db\")\n                                |> range(start: -200d)\n                                |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n                                |> filter(fn: (r) => r._measurement == \"test_result\")\n                                |> filter(fn: (r) => r.version == $pVersion)\n                                |> keep(columns: [\"_time\", \"test\", \"run\", \"status_tag\", \"duration_sec\", \"version\"])'''\n\ndf = query_api.query_data_frame(query=query, params=params)\n`\n`HTTP response body: b'{\"error\":\"loc 0:0-0:0: expected an operator between two expressions\"}\\n'`\nAnother data point to note is that we are using the following versions:\n\nInfluxdb-Version: 1.8.6\ninfluxdb-client: 1.19.0",
      "solution": "The issue is actually with the version of influxdb I was using (1.8.6). The query params is not a feature of Influxdb 1.8.6, and was only introduced into Influxdb 2.0.x\nSee the link below for a question opened with the Influxdb-python-client team.\nhttps://github.com/influxdata/influxdb-client-python/issues/285",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-07-15T00:26:05",
      "url": "https://stackoverflow.com/questions/68385739/using-params-in-flux-queries-with-python-influxdb-client"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 67598729,
      "title": "Counting boolean values in flux query",
      "problem": "I have a bucket where one field is a boolean\nI'd like to count the number of true and the number of false for each hour\n```\n`from(bucket: \"xxx\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> window(every: 1h)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"xxx\")\n  |> filter(fn: (r) => r[\"_field\"] == \"myBoolField\")\n  |> group(columns: [\"_stop\"])\n`\n```\nBecause this is issued from a cron that runs every minute (more or less), this will give something like :\n```\n`table _start              _stop            _time            _value otherfield1 otherfield2\n0     2021-05-18T19:00:00 2021-05-18T20:00 2021-05-18T19:01 false  xxx         xxx\n0     2021-05-18T19:00:00 2021-05-18T20:00 2021-05-18T19:02 true   xxx         xxx\n0     2021-05-18T19:00:00 2021-05-18T20:00 2021-05-18T19:03 true   xxx         xxx\n...\n1     2021-05-18T20:00:00 2021-05-18T21:00 2021-05-18T20:01 false  xxx         xxx\n1     2021-05-18T20:00:00 2021-05-18T21:00 2021-05-18T20:02 false  xxx         xxx\n1     2021-05-18T20:00:00 2021-05-18T21:00 2021-05-18T20:03 false  xxx         xxx\n...\n`\n```\nNow, I'd like to count the total, the number of false and the number of true for each hour (so for each table) but without losing/dropping the other fields\nSo I'd like a structure like\n```\n`table _stop            _value nbFalse nbTrue otherfield1 otherfield2\n0     2021-05-18T20:00 59     1       58     xxx         xxx\n1     2021-05-18T21:00 55     4       51     xxx         xxx\n`\n```\nI've tried many combinations of pivot, count, ... without success\nFrom my understanding, the correct way to do is\n\ndrop _start and _time\n\nduplicate _value into nbTrue and nbFalse\n\nre-aggregate by _stop to keep only true in nbTrue and false in nbFalse\n\ncount the three columns _value, nbTrue and nbFalse\n|> drop(columns: [\"_start\", \"_time\"])\n|> duplicate(column: \"_value\", as: \"nbTrue\")\n|> duplicate(column: \"_value\", as: \"nbFalse\")\n\nbut I am stucked at step 3...",
      "solution": "Didn't test it, but I have something similar to this on my mind:\n```\n`from(bucket: \"xxx\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"xxx\")\n  |> filter(fn: (r) => r[\"_field\"] == \"myBoolField\")\n  |> aggregateWindow(\n        every: 1h,\n        fn: (column, tables= tables |> reduce(\n            identity: {true_count: 0.0},\n            fn: (r, accumulator) => ({\n                true_count:\n                    if r._value == true then accumulator.true_count + 1.0\n                    else accumulator.true_count + 0.0\n            })\n        )\n    )\n`\n```\nI got this from the docs and adjusted it a bit, I think it should get you what you need.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-05-19T09:29:36",
      "url": "https://stackoverflow.com/questions/67598729/counting-boolean-values-in-flux-query"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 79721896,
      "title": "Why does my Influx query produce &quot;6 tables 6 rows&quot; rather than &quot;6 rows&quot;?",
      "problem": "I am trying to develop a Flux query to produce a table visualisation.\nBut first some context. I am pushing data in line protocol format to InfluxDB like so:\n```\n`local_destination_page,country=\u00c4gypten,continent=Afrika,locale=de success=1\n`\n```\nA full run would have around 200 rows. Of course the hope is that all are successful, but occasionally some will fail.\nIn this line I have a table name, `local_destination_page`, three tags/labels, and one field for the success/failure (1 or 0) of an uptime probe. I believe the data is being received succesfully.\nNow here is what I am aiming for. This table could be captioned as \"Most recent failures\", and it shows failures in reverse order. The key thing here is that something is a failure where the most recent instance of the tuple [country, continent, locale] has a field of success=0.\n\nTime\nCountry\nContinent\nSource Locale\n\n2025-07-30 22:19:27\nFrance\nEurope\nen\n\n2025-07-30 21:18:06\n\u00c4gypten\nAfrika\nde\n\nSo now to my query:\n```\n`from(bucket: \"analytics\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r._measurement == \"local_destination_page\")\n  |> filter(fn: (r) => r._field == \"success\")\n  |> keep(columns: [\"country\", \"continent\", \"locale\", \"_value\"])\n  |> group(columns: [\"country\", \"continent\", \"locale\"])\n  |> sort(columns: [\"_time\"], desc: true)\n  |> first(column: \"_value\")\n  |> filter(fn: (r) => r._value == 0)\n`\n```\nHowever it does not seem to have worked in my Grafana dashboard.\nI also realised that rendering this query in Grafana and in InfluxDB was producing rather different results, which complicated the debugging process. I was primarily using Grafana, and I now wonder if the table renderer is doing some kind of unhelpful pre-processing of the data.\nHere is what I see in InfluxDB for that query, using the \"Simple Table\" visualisation:\n\n(I generally avoid screenshots for text information, but I believe it is necessary in this case. I want to show how the screen is structured for each UI.)\nHere is what I see in Grafana in a dashboard panel:\n\nThe reader can see that this produces one row, but there is a chooser, where one can see the rows that were rendered in InfluxDB. Why is the table not just rendered as having six rows, rather than one row and a chooser?\nNow I have spotted that InfluxDB says that this result is 6 tables 6 rows rather than 1 table 6 rows; in other words, I think the problem is that I have 6 tables of 1 row each, and this is why Grafana is rendering it badly.\nIs there an ungroup/unwrap in Grafana that will take the data and convert it to one table?\nWalkthrough of query\nFinally, I want to explain how I think this query is right, and thus why InfluxDB's rendering is correct, and why Grafana's rendering is wrong.\n\n`from`, `range`, `filter`, `filter` - this is just basic stuff to focus on one table\n`keep` - this brings the tags into the current resultset, otherwise we will only see fields\n`group` - we group by the tags first, at this point permitting both success and failure values\n`sort` - this ensures the latest entry for the label combo is first\n`first` - this selects the first row from each group\n`filter` - this removes successful groups, leaving only failures\n\nThis seems to be a really simple requirement, and I've been through several hoops to get here. I am wondering whether I should use MySQL as my data collector, so at least the query language behaves as I'd expect, but I want to make one last push on the existing stack, before I try another thing.",
      "solution": "I finally resolved it. It does not seem to be well-documented that the `group()` function creates a \"table\" for each group. So the Grafana rendering was not wrong, and I guess, neither was the Simple Table in InfluxDB. The latter elected to merge the rows from all resultsets, and the former did not; they are just different design decisions.\nBut it would be common enough to want to merge these tables so they are unambiguously one recordset. To do that, just add `group()` to the end of the query, with no columns.\n```\n`from(bucket: \"analytics\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r._measurement == \"local_destination_page\")\n  |> filter(fn: (r) => r._field == \"success\")\n  |> keep(columns: [\"country\", \"continent\", \"locale\", \"_value\"])\n  |> group(columns: [\"country\", \"continent\", \"locale\"])\n  |> sort(columns: [\"_time\"], desc: true)\n  |> first(column: \"_value\")\n  |> filter(fn: (r) => r._value == 0)\n  |> group()\n`\n```\nThis change allows all table visualisations to render the data in a similar fashion.\nAddendum\nI'll add an observation here that is not entirely germane to this answer. I'm new to the Grafana ecosystem, of which I'd regard InfluxDB as a part. I've gotten no substantive help on this question, and struggled a great deal. AI sent me on several useless wild-goose chases. Moreover Flux is no longer going to be developed. To say that the Grafana ecosystem feels patchily documented and poorly supported would be an understatement. My various questions over at the Grafana Cloud forum also went completely unaddressed.\nShould an InfluxDB expert have some reflections on this complaint, I'd be eager to hear them. I will set up a Grafana stack, but I am still nervous about selling it to colleagues, given that it could still turn into a white elephant.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2025-08-01T00:29:15",
      "url": "https://stackoverflow.com/questions/79721896/why-does-my-influx-query-produce-6-tables-6-rows-rather-than-6-rows"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 75408503,
      "title": "influx write command is throwing error: &quot;unsupported protocol scheme&quot;",
      "problem": "I have exported my data from an InfluxDB bucket with the following command:\n```\n`> influxd inspect export-lp --bucket-id d5f80730ede82d67 --engine-path ~\\.influxdbv2\\engine --output-path ~\\Desktop\\my-data.lp.gz --start 2022-11-01T00:00:00Z --end 2022-12-31T00:00:00Z --compress\n`\n```\nI am following steps from this influxdb document.\n\nThe size of the exported file is ~8MB.\nI use the below command to write the exported file back to my new bucket:\n```\n`> influx write --bucket my-new-bucket --file ~\\Desktop\\my-data.lp.gz\n`\n```\nI am following this InfluxDB document to write my data.\n\nNow when I try to write it back to the DB, I get an Error:\n\nError: failed to write data: Post \"/api/v2/write?bucket=my-new-bucket&org=00ef2f123c4706fd&precision=ns\": unsupported protocol scheme \"\"\n\nI have even tried to export and import without compressing and using .txt format for my line protocol. Still for all my attempts I face this same error.\nI even tried uploading the same exported file through Telegraf > Sources > Line Protocol. But that too fails with an Error:\n\nUnable to Write Data\n\nFailed to write data - invalid line protocol submitted.\n\nI don't know why the file exported from InfluxDB's \"export-lp\" command fails when I try to write it back.",
      "solution": "If it can help , you I have this error. because I no system variable for:\nINFLUX_HOST: InfluxDB host URL.-->export INFLUX_HOST=http://localhost:8086\n[root@Monitor influxdb]# export INFLUX_HOST=\"\"\n[root@Monitor influxdb]# influx write --bucket buck01 --precision s \"home,room=Kitchen temp=22.7,hum=36.5,co=26i 1641067400\"\nError: failed to write data: Post \"/api/v2/write?bucket=buck01&org=xxx&precision=s\": unsupported protocol scheme \"\"\nwith the variables:\ninflux write --bucket buck01 -host \"http://localhost:8086\" --precision s \"home,room=Kitchen temp=22.7,hum=36.5,co=26i 1641067400\"\nOK no error",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2023-02-10T09:19:50",
      "url": "https://stackoverflow.com/questions/75408503/influx-write-command-is-throwing-error-unsupported-protocol-scheme"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 73006654,
      "title": "Telegraf uses timestamp when data sent to Influx, not when data received?",
      "problem": "I have a telegraf instance, which receives it's data solely from MQTT on the same device as telegraf.  It then outputs the data to InfluxDB on a separate server.  The network connection to the InfluxDB server is not reliable and sometimes goes down for an extended period of time.  (Minutes to hours)\nThus, I have configured telegraf to buffer this data using the `metric_buffer_limit` parameter, which does work.\nHOWEVER, it appears that InfluxDB sees these data points as having occurred when it receives them from telegraf, not when telegraf receives the actual data from MQTT.  I was wondering if I need some parameter added to my telegraf configuration to achieve this.\nIs there a way to make these data points be properly associated with the point in time when the information was received?\nMy `telegraf.conf`:\n```\n`[agent]\n  interval = \"10s\"\n  round_interval = true\n  metric_batch_size = 100\n  metric_buffer_limit = 50000\n  collection_jitter = \"0s\"\n  flush_interval = \"10s\"\n  flush_jitter = \"1s\"\n  debug = false\n  quiet = false\n  hostname = \"{snip}\"\n  omit_hostname = false\n\n################################\n#            OUTPUTS           #\n################################\n\n#Sensors DB\n[[outputs.influxdb_v2]]\n  urls = [\"{snip}\"]\n  token = \"{snip}\"\n  organization = \"{snip}\"\n  bucket = \"{snip}\"\n\n################################\n#            INPUTS            #\n#            SENSORS           #\n################################\n\n[[inputs.mqtt_consumer]]\n  servers = [\"tcp://127.0.0.1:1883\"]\n  qos = 0\n  connection_timeout = \"30s\"\n  topics = [ \"#\" ]\n  client_id = \"telegraf\"\n  persistent_session = false\n  data_format = \"json\"\n`\n```",
      "solution": "Do the data points from MQTT have a timestamp themselves? If so, that timestamp can be used to set the data's timestamp rather than the current time.\nIf not, another option would be to use a starlark processor to set the timestamp during processing.\nEdit: @giovanni-luisotto got me to look, and it appears the mqtt input uses a `TrackingAccumulator`. This special accumulator in Telegraf ensures that metrics are captured, queued, and successfully sent to outputs. The timestamp is set when the data is set to outputs, I believe, unless it is already set.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2022-07-16T20:17:23",
      "url": "https://stackoverflow.com/questions/73006654/telegraf-uses-timestamp-when-data-sent-to-influx-not-when-data-received"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 75865285,
      "title": "InfluxDB: Flux vs InfluxQL chart differences",
      "problem": "I am trying to migrate from InfluxQL to Flux but when displayed in a chart I get strange results and I was not able to solve it.\nInfluxQL query and chart:\n```\n`SELECT mean(\"machine_temperature\") FROM \"machine\" WHERE time >= 1677625200000ms and time \nThis is the same basic version in Flux:\n```\n`from(bucket: \"mybucket\")\n  |> range(start: -30d, stop: -26d)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"machine\")\n  |> filter(fn: (r) => r[\"_field\"] == \"machine_temperature\")\n  |> aggregateWindow(every: 1h, fn: mean, createEmpty: false)\n  |> yield(name: \"Mean\")\n`\n```\nBut in the chart I get multiple stacking lines, each for every tag with different colors. There are also \"gaps\" between parts of the chart and it appears broken. Why is it?\n\nI tried reading all the guides about migration from InfluxQL and all the guides about Flux but I was not able to get a clean chart.\nIs there something very basic that I am missing?\nPlease note that the two charts are generated on the same database.",
      "solution": "Flux handles series different than InfluxQL. In flux series are \"separate\" by default. This means that any Flux query that does not specify otherwise will always group by every tag (like adding a `GROUB BY *` in your InfluxQL query).\nIf you want to replicate the InfluxQL query than you should do:\n```\n`from(bucket: \"mybucket\")\n  |> range(start: 1677625200000000000, stop: 1679039209170000000)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"machine\")\n  |> filter(fn: (r) => r[\"_field\"] == \"machine_temperature\")\n  |> group(columns: [])\n  |> aggregateWindow(every: 20m, fn: mean, createEmpty: false)\n  |> yield(name: \"Mean\")\n`\n```\nThe `group` statement is telling flux to not group any of the tags (to replicate the fact that you are grouping by time only in the InfluxQL query).",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2023-03-28T12:48:48",
      "url": "https://stackoverflow.com/questions/75865285/influxdb-flux-vs-influxql-chart-differences"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 72180677,
      "title": "WARN: Write to InfluxDB failed [[RequestTimedOutError]]",
      "problem": "```\n`WARN: Write to InfluxDB failed (attempt: 1). Error: \nat S. (/Users/sathish/Documents/sample/raviAPI/node_modules/core-js/internals/wrap-error-constructor-with-cause.js:37:62)\nat new super (/Users/sathish/Documents/sample/raviAPI/node_modules/core-js/modules/es.error.cause.js:28:43)\nat new S (/Users/sathish/Documents/sample/raviAPI/node_modules/@influxdata/influxdb-client/src/errors.ts:163:5)\nat ClientRequest. (/Users/sathish/Documents/sample/raviAPI/node_modules/@influxdata/influxdb-client/src/impl/node/NodeHttpTransport.ts:346:23)\nat ClientRequest.emit (events.js:314:20)\nat ClientRequest.EventEmitter.emit (domain.js:483:12)\nat Socket.emitRequestTimeout (_http_client.js:715:9)\nat Object.onceWrapper (events.js:420:28)\nat Socket.emit (events.js:326:22)\nat Socket.EventEmitter.emit (domain.js:483:12)\nat Socket._onTimeout (net.js:483:8)\nat listOnTimeout (internal/timers.js:554:17)\nat processTimers (internal/timers.js:497:7) \n`\n```\nname: 'RequestTimedOutError',\nmessage: 'Request timed out'\nI am facing this issue while writing data for each second in influxdb.\nThis is the influxdb client i am using from nodejs @influxdata/influxdb-client",
      "solution": "It would be easier to answer this if you would post some of the code here.\nlooks like classing http request timeout\nhttps://github.com/influxdata/influxdb-client-js/blob/558b3b4f6a5fd1e94c1579bd894cb0ce5e126368/packages/core/src/impl/node/NodeHttpTransport.ts#L345\nwhich comes from regular nodejs http.IncomingMessage\nhttps://github.com/influxdata/influxdb-client-js/blob/558b3b4f6a5fd1e94c1579bd894cb0ce5e126368/packages/core/src/impl/node/NodeHttpTransport.ts#L47\nyou can read more about timeouts here https://nodejs.org/api/http.html#event-timeout\nWhatever timeout setting you are using is too low for the server to respond.\nDefault Agent has timeout options in milliseconds, probably there is a way to configure that in this node module as well.\nhttps://nodejs.org/api/http.html#new-agentoptions\nhttps://github.com/influxdata/influxdb-client-js/blob/558b3b4f6a5fd1e94c1579bd894cb0ce5e126368/packages/core/src/impl/node/NodeHttpTransport.ts#L62",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2022-05-10T06:14:28",
      "url": "https://stackoverflow.com/questions/72180677/warn-write-to-influxdb-failed-requesttimedouterror"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 71396961,
      "title": "How can I use data from more than one measurement in a single Grafana panel?",
      "problem": "I am attempting to create a gauge panel in Grafana (Version 6.6.2 - presume that upgrading is a last resort, but possible if necessary, for the purposes of this problem) that can represent the percentage of total available memory used by the Java Virtual Machine running a process of mine. the problem that I am running into is the following:\n\nI have used Springboot actuator's metrics and imported them into an Influx database with Micrometer, but in the process, it has stored the two values that I would like to use in my calculation into two different measurements. `jvm_memory_used` and `jvm_memory_max`\n\nMy initial Idea was to simply call a `SELECT` on both of the measurements to get the value that I want, and then divide the \"used\" / \"max\" and multiply that value by 100 to get the percentage to display. Unfortunately I run into syntax errors when I try to do this manually, and I am unsure if I can do this using Grafana's query builder.\nI know that the syntax is incorrect, but I am not familiar enough with InfluxQL to know how to properly structure this query. Here is what I had tried:\n```\n`(SELECT last(\"value\")\n    FROM \"jvm_memory_used\" \n    WHERE (\"area\" = 'heap') \n    AND $timeFilter \n    GROUP BY time($__interval) fill(null)\n) /\n(SELECT last(\"value\")\n    FROM \"jvm_memory_max\" \n    WHERE (\"area\" = 'heap') \n    AND $timeFilter \n    GROUP BY time($__interval) fill(null)\n)\n`\n```\n(The `AND` and `GROUP BY` are present as a result of the default values from Grafana's query builder, I am not sure whether they are necessary or not)\nI'm assuming that my parenthesis and division process is illegal, but I am not sure how to resolve it.\n\nHow can I divide these two values from separate tables?\nEDIT: I have gotten slightly further but it seems that there is a new issue. I now have the following query that I am sending in:\n```\n`SELECT 100 * (last(\"used\") / sum(\"max\")) AS \"percentUsed\" \n    FROM(\n        SELECT last(\"value\") AS \"used\" \n            FROM \"jvm_memory_used\" \n            WHERE (\"area\" = 'heap')\n            AND $timeFilter\n    ),(\n        SELECT last(\"value\") AS \"max\" \n            FROM \"jvm_memory_max\" \n            WHERE (\"area\" = 'heap')\n            AND $timeFilter\n    )  \n    GROUP BY time($__interval) fill(null)\n`\n```\nand the result I get is this:\n\nHow can I now get this query to return only one gauge with data, instead of two with nulls?\nI've accepted an answer that works for versions of Grafana after 7. If there are any other answers that arise that do not involve updating the version of Grafana, please provide them as well!",
      "solution": "I am not particulary experienced with Influx, but since your question is how to use/combine two measurements (query results) for a Grafana panel, I can tell you about one approach:\nYou can use a transformation. By that, you can keep two separate queries. With the transformation mode binary operation you can simply divide one of your values by the other one.\nIn your specific case, to display the result as percentage, you can then use Percent (0.0-1.0) as unit and you should have accomplished your goal.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2022-03-08T15:50:28",
      "url": "https://stackoverflow.com/questions/71396961/how-can-i-use-data-from-more-than-one-measurement-in-a-single-grafana-panel"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 74778457,
      "title": "Storing numbers above 2e19",
      "problem": "I am using InfluxDB and need to store very large numbers (`uint256`) with full precision (no floating point).\nI am currently using strings to achieve this, but I thus loose the ability to perform arithmetic operations on these numbers, which is something I need to implement.\nex.\n```\n`{ _measurement=transfer, _field=amount, _value=90000000000000000000001 }\n{ _measurement=transfer, _field=amount, _value=12000000000000000000000 }\n`\n```\n```\n`from(bucket: \"xxx\")\n  |> range(start: 0)\n  |> filter(fn: (r) => r._measurement == \"transfer\" and r._field == \"amount\")\n  |> toUint()\n  |> sum()\n`\n```\nI get the following error: `runtime error @4:6-4:13: toInt: failed to evaluate map function: cannot convert string \"90000000000000000000001\" to int due to invalid syntax`\nIs there a built-in solution like ClickHouse's uint256, or a third-party package to achieve this?",
      "solution": "Unfortunately InfluxDB doesn't support big numbers yet. It stores all integers as signed int64 data types. The minimum and maximum valid values for int64 are -9223372036854775808 and 9223372036854775807. See more info here.",
      "question_score": 1,
      "answer_score": 5,
      "created_at": "2022-12-13T00:33:33",
      "url": "https://stackoverflow.com/questions/74778457/storing-numbers-above-2e19"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 67006998,
      "title": "Influxdb high CPU usage jumping to 80 %?",
      "problem": "I am relatively new to time series db world . I am running a Influxdb 1.8.x as a docker container, and I have configured the influxdb.conf file as a default config. Currently I am facing a issue of high CPU usage by influxdb, the CPU jumps to 80 to 90% and creating a problem for other process running on same machine.\nI have tried a solution given here ->> Influx high CPU issue but unfortunately It did not work? I am unable to understand the reason behind the issue and also struggling to get support in terms of documentation or community help.\nWhat I have tried so far:\n\nupdated the monitor section of influxdb.conf file like this ->> monitor DB\nChecked the series cardinality `SHOW SERIES CARDINALITY` and it looks well within limits--9400(I am also not sure about the ideal number for high cardinality red flag)\n\nI am looking for an approach, which will help me understand this problem the root cause?\nPlease let me know if you need any further information on same.",
      "solution": "After reading about Influxdb debug and CPU profiling HTTP API influxdb I was able to pin-down the issue, the problem was in the way I was making the query, my query involved more complex functions and also `GROUP BAY` tag.I also tried query analysis using `EXPLAIN ANALYZE (query)` command to check how much time a query is taking to execute. I resolved that and noticed a huge Improvement in CPU load.\nBasically I can suggest the following:\n\nRun the CPU profile analysis using influxdb HTTP API with the command `curl -o  http://localhost:8086/debug/pprof/all?cpu=true e` and collect result.\nVisualize the result using tool like Pprof tool and find the problem\nAlso one can run basic commands like `SHOW SERIES CARDINALITY` and `EXPLAIN ANALYZE ` to understand the execution of the query\nBefore designing any schema and Influx client check the hardware recommendation ->> Hardware sizing guidelines",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2021-04-08T17:16:23",
      "url": "https://stackoverflow.com/questions/67006998/influxdb-high-cpu-usage-jumping-to-80"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 66657352,
      "title": "Show second to last item in Influx query (or ignore last)",
      "problem": "I am using Grafana to show the number of entries added to the database every minute, and I would like to display the last recent fully counted value.\nIf I give the following command:\n```\n`SELECT count(\"value\") FROM \"numSv\" GROUP BY time(1m)\n\n1615904700000000000 60\n1615904760000000000 60\n1615904820000000000 60\n1615904880000000000 60\n1615904940000000000 36\n`\n```\nGrafana is going to display the last entry, which is still in the process of counting. How can I display the n[-1] entry, which has been fully counted?\nOtherwise, how do I ask Influx to give me the same results excluding the last dataset?\nP.S.: Using WHERE time > now() - 60s, etc... doesn't work.",
      "solution": "Use \"magic\" Grafana time range math and select dashboard time range from `now-1m/m` to `now-1m/m`. That generates an absolute time range, which refers to last fully counted minute. Query is then standard with `$timeFilter` Grafana macro:\n```\n`SELECT count(\"value\") FROM \"numSv\" WHERE $timeFilter\n`\n```",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2021-03-16T15:35:38",
      "url": "https://stackoverflow.com/questions/66657352/show-second-to-last-item-in-influx-query-or-ignore-last"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 77493199,
      "title": "how to build influxdb rust source?",
      "problem": "I am a bit surprised that InfluxDB official document does not give an step by step guide to build its rust code base. I have found How to compiling InfluxDB from source code. However it's talking about the go code base. As far as I know the latest InfluxDB has switched to rust already.\nI did git clone from InfluxDB at github. I asked an online AI chatbot about it. It suggests the following steps after cloning:\n`cd influxdb_iox\ncargo build\n`\nI am working on a Debian 9 box. It's my first time to deal with rust. I did following to have it installed.\n`sudo apt install rustc\nrustc --version\n\n> rustc 1.34.2\n\ncargo --version\n\n> cargo 1.34.0\n\ncd influxdb_iox\ncargo build\n\n> error: failed to parse manifest at `/home/username/work/influxdb/influxdb_iox/Cargo.toml`\n\n> Caused by:\n>  invalid type: map, expected a sequence for key `package.authors`\n\n`\nMy code tip is:\n`git log\n\n> commit bb6a5c0bf6968117251617cda99cb39a5274b6dd\n> Author: Jamie Strandboge \n> Date:   Thu Nov 2 12:16:13 2023 +0000\n>\n>    chore: ignore Go in .github/dependabot.yml, take 3 (#24439)\n>\n>    Update to use the documented dependency-name: \"*\" methodology rather\n>    than an undocumented example.\n>\n>    References:\n>    - https://docs.github.com/en/code-security/dependabot/dependabot-version-updates/configuration-options-for-the-dependabot.yml-file\n`\nSince I am fresh on rust, I am not sure whether it's a rustc/cargo version problem, since I am using a pretty old Debian?\nI am appreciate anyone can share me their experience or point me to the right document. Also I wonder if there is a build method use traditional makefile?",
      "solution": "`cargo 1.34.0` released in 2019.\nThe project you are trying to build uses specifies `edition = \"2021\"` in it's `Cargo.toml`.\nI therefore suspect that your issue is that your rust toolchain is too old. Consider using `rustup` to get a newer rust toolchain,\nor upgrade to a newer version of Debian.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2023-11-16T09:21:01",
      "url": "https://stackoverflow.com/questions/77493199/how-to-build-influxdb-rust-source"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 77235023,
      "title": "How to get start of week in Influx QL?",
      "problem": "I need to write an flux string in such a way as to get data for a week, but not the `|> range(start: -1w)`, but the calendar week, that is, in my case - 02.10.2023 00:00:00.\nIt would also be interesting to know how to get data from the beginning of a calendar month/quarter/year.\nI tried using\n`|> range(start: date.truncate(t: now(), unit: 1w))`\n`|> range(start: -1w)`\nbut I get either the start of the current day or the start of the day that was 7 days ago.\nI also tried to use `date.add()` in combination with `date.weekDay()`, but I got an error because the `d:` parameter expects Duration, not the number of days.\nI need the calendar start of the week.",
      "solution": "Finally, combination of methods `date.truncate` and `timeShift` helped me to solve my problem.\n```\n`import \"date\"\n   from(bucket: \"training\")\n  |> range(start: date.truncate(t: -1y, unit: 1w))\n  |> aggregateWindow(every: 1w, fn: sum, createEmpty: true)\n  |> timeShift(duration: -3d)\n  |> fill(column: \"_value\", value: 0.0)\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2023-10-05T09:29:01",
      "url": "https://stackoverflow.com/questions/77235023/how-to-get-start-of-week-in-influx-ql"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 75383360,
      "title": "Flux Query : join.inner() returns nothing if I don&#39;t limit my stream used",
      "problem": "I get an issue understanding how to use the join.inner() function.\nIt seems I can only have a result (and the correct one) if I use the limit() function to the stream I want to use the join.inner function with.\nIf don't limit this left stream, I don't get any error but just no result.\nIt is because of how I get my left stream ?\nDo you have any idea what I am doing wrong here ?\nI am pretty new using InfluxDB therefore the flux language so it must be me.\nThank you all for your answers !\n```\n`import \"array\"\nimport \"join\"\n\nleft =\n    from(bucket: \"TestBucket\")\n    |> range(start: 0)\n    |> filter(fn: (r) => r[\"_measurement\"] == \"TestMeasurement\")\n    |> limit(n : 1000000000000000000)\n    |> group()\n     //|> yield(name: \"LEFT\")\n    \n\nright =\n    array.from(\n        rows: [\n            {arrayValue: \"123\", _time: 2023-02-07T12:00:00.000Z}, //This timestamp exists in the left stream\n        ],\n    )\n    //|> yield(name: \"RIGHT\")\n\nresult = join.inner(\n    left: left,\n    right: right,\n    on: (l, r) => l._time == r._time, // I made sure that there is indeed a common time \n    as: (l, r) => ({l with rightValue: r.arrayValue}),\n)\n    |> yield(name: \"RESULT\") \n`\n```",
      "solution": "Ok, the solution was to group by _time column the stream AND the table :\n`|> group(columns: [\"_time\"])`",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2023-02-08T10:00:47",
      "url": "https://stackoverflow.com/questions/75383360/flux-query-join-inner-returns-nothing-if-i-dont-limit-my-stream-used"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 74017261,
      "title": "Finding the Count per field for a Window using Flux Query - InfluxDB",
      "problem": "I am trying to display data where it aggregates by a field and displays count per a window. I'm struggling coming up with the syntax.  Let's assume this is my data in InfluxDb:\n\r\n\r\n`import \"array\"\n\ndata = array.from(rows: [\n  {application: \"ap1\", time: \"2022-10-01T10:10:06.757Z\", message: \"error...\"},\n  {application: \"ap2\", time: \"2022-10-03T15:11:05.757Z\", message: \"error...\"},\n  {application: \"ap1\", time: \"2022-10-02T12:11:08.757Z\", message: \"error...\"},\n  {application: \"ap1\", time: \"2022-10-04T13:13:05.757Z\", message: \"error...\"},\n  {application: \"ap3\", time: \"2022-10-05T10:11:16.757Z\", message: \"error...\"},\n  {application: \"ap3\", time: \"2022-10-06T15:22:05.757Z\", message: \"error...\"},\n])\n\ndata\n |> group(columns: [\"application\", \"time\"])`\r\n\r\n\r\n\nI'd like to group by results like this:\n\nThe window could be...show count per application type...per hour, per day, or per week.",
      "solution": "You could try with the count function.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2022-10-10T17:31:05",
      "url": "https://stackoverflow.com/questions/74017261/finding-the-count-per-field-for-a-window-using-flux-query-influxdb"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 73837715,
      "title": "Reading a influx measurement using an esp32 and Ardino",
      "problem": "I need to read a temperature present in an influx v1.8 database, using esp8266 and esp32 in Arduino.\nI am using the ESP8266 influxdb v3.12.1 library.\nThis query works from a command prompt:\n```\n`curl --insecure -XPOST https://10.1.1.40:8086/api/v2/query -sS \\                                     \n           -H 'Accept:application/csv' \\\n           -H 'Content-type:application/vnd.flux' \\\n           -d 'from(bucket:\"test01\")\n             |> range(start:-5m)\n             |> filter(fn:(r) => r._measurement == \"sens848a6a6\")' \n`\n```\nIn my Arduino code I have:\n```\n`void loop() {\n  String query = \"from(bucket:\\\"test01\\\") |> range(start:-5m) |> filter(fn:(r) => r._measurement == \\\"sens848a6a6\\\")\";\n   \n  // Send query to the server and get result\n  FluxQueryResult result = client.query(query);\n \n  // Check if there was an error\n  if(result.getError() != \"\") {\n    Serial.print(\"Query result error: \");\n    Serial.println(result.getError());\n  }\n  \n  while (result.next()) {\n    // Get typed value for flux result column 'temperature'       \n    String temp = result.getValueByName(\"temperature\").getString();\n    Serial.print(\"temperature: \");\n    Serial.print(temp);\n    // Get converted value for flux result column '_value' where there is temperature value\n    long value = result.getValueByName(\"_value\").getLong();\n    Serial.print(value);\n    Serial.println();\n    }\n}\n`\n```\nBut is not working:\n```\n`Query result error: send header failed\ntemperature: 0\n`\n```\nHelp appreciated!",
      "solution": "That send header failed might give us some hint on the Arduino SDK you are using. InflxuDB instance should be. The issue I assume should be inside the Arduino HTTP client. You should use Postman to mock the behavior of Arduino HTTP client, which set up the HTTP headers and send the Flux query in the body to see whether it works or not.\nBesides, you are using HTTPS, there was an issue of Arduino HTTP client calling HTTPS. You might just try HTTP first and then switch to HTTPS when the client is stable or you could provide the certificates.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2022-09-24T15:48:49",
      "url": "https://stackoverflow.com/questions/73837715/reading-a-influx-measurement-using-an-esp32-and-ardino"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 73139145,
      "title": "Telegraf not writing data to any output type",
      "problem": "I am trying to consume MQTT events and publishing them via Telegraf. Eventually I'll only use InfluxDB as the output but for debugging reasons I'm also writing to a file now.\nThe problem is that the file generated by Telegraf is always empty, and nothing is written to InfluxDB even though MQTT has events being generated.\nAn event published in MQTT looks like this:\n```\n`{\"100001\":\"100\",\"100002\":\"200\"}\n`\n```\nThe telegraf.conf file looks like this:\n```\n`    [[inputs.mqtt_consumer]]\n      servers = [\"tcp://mqtt:1883\"]\n      qos = 0\n      connection_timeout = \"30s\"\n      topics = [\n        \"mytopics/test\",\n      ]\n      persistent_session = true\n      client_id = \"MQTT\"\n      data_format = \"json\"\n      data_type = \"integer\"\n \n    \n    [[outputs.file]]\n      files = [\"stdout\", \"./test.txt\"]\n      data_format = \"json\"\n\n    [[outputs.influxdb]]\n    database = \"testdb\"\n    urls = [\"http://influxdb:8086\"]\n\n    [agent]\n    interval = \"10s\"\n    debug = true\n    quiet = false\n`\n```\nFor example, If I change the `telegraf.conf` file for debugging purposes and remove `data_format = \"json\"`, the following error happens:\n```\n`Error in plugin [inputs.mqtt_consumer]: metric parse error: expected tag at offset 31: \"{\\\"100001\\\":\\\"100\\\",\\\"100002\\\":\\\"200\\\"}\"\n`\n```\nAfter fixing it and adding `data_format = \"json\"`, nothing happens and the messages in Debug Mode only show:\n```\n` Output [file] buffer fullness: 0 / 10000 metrics. \n`\n```\nI am wondering what is wrong with my configuration, as I have tried writing this data to InfluxDB, files and Prometheus using Telegraf without any luck. Also, there are no error messages and everything looks OK.\nAny ideas are welcome!",
      "solution": "The issue is that the mqtt topic is getting read, but telegraf is unable to parse the data that is coming in without some more knowledge of the data.\nWith the data you are receiving from MQTT are you looking to store JSON elements as fields like so: `mqtt 100001=100i,100002=200i`? For example, I can produce a metric like that with the `json_v2` parser:\n`[[inputs.file]]\n  files = [\"data.json\"]\n  data_format = \"json_v2\"\n  [[inputs.file.json_v2]]\n    [[inputs.file.json_v2.field]]\n      path = \"100001\"\n      type = \"int\"\n\n    [[inputs.file.json_v2.field]]\n      path = \"100002\"\n      type = \"int\"\n`\nObviously, you will want to replace the file with mqtt for your config. With the example data you provided, this produces:\n`file 100001=100i,100002=200i 1659117305000000000\n`",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-07-27T15:49:08",
      "url": "https://stackoverflow.com/questions/73139145/telegraf-not-writing-data-to-any-output-type"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 72745478,
      "title": "Calculating average of elapsed time",
      "problem": "I want to calculate the average time duration of my events. After an event starts and ends, it sends a request to my InfluxDB in the Line Protocol Syntax:\n```\n`mes1 id=1,event=\"start\" 1655885442\nmes1 id=1,event=\"end\" 1655885519\nmes1 id=2,event=\"start\" 1655885643\nmes1 id=2,event=\"end\" 1655885914\nmes1 id=3,event=\"start\" 1655886288\nmes1 id=3,event=\"end\" 1655886372\nmes1 id=4,event=\"start\" 1655889323\nmes1 id=4,event=\"end\" 1655889490\n`\n```\nI can query the results like this:\n```\n`from(bucket: \"buck1\") \n  |> range(start: -1w)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"mes1\")\n  |> filter(fn: (r) => r[\"_field\"] == \"event\")\n  |> elapsed()\n`\n```\nResult:\n\nAs you can see, I also get the durations between those events, not only of the events themselves.\nConsequently, when I add the mean() function, I get the mean of ALL elapsed seconds:\n```\n`from(bucket: \"buck1\") \n  |> range(start: -1w)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"mes1\")\n  |> filter(fn: (r) => r[\"_field\"] == \"event\")\n  |> elapsed()\n  |> mean(column: \"elapsed\")\n`\n```\nResult:\n\nHow Can I get the average of only the events, not the time between them?\n\nThe durations of those events are:\n\n77 sec\n271 sec\n84 sec\n167 sec\n\nSo the expected result is 599/4 = 149.75 seconds.\n\nUpdate:\n```\n`from(bucket: \"buck1\") \n  |> range(start: -1w)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"mes1\")\n  |> filter(fn: (r) => r[\"_field\"] == \"event\"  or r[\"_field\"] == \"id\")\n  |> group(columns: [\"id\"])\n  |> elapsed()\n  |> group(columns: [\"_measurement\"])\n  |> mean(column: \"elapsed\")\n`\n```\nResult:\n```\n`runtime error @6:8-6:17: elapsed: schema collision: cannot group string and float types together\n`\n```",
      "solution": "You need to group by `id` and then ungroup via `_measurement`\n```\n`|> group(columns: [\"id\"])\n|> elapsed()\n|> group(columns: [\"_measurement\"])\n|> mean(column: \"elapsed\")\n`\n```\n\nUpdate\nI found another solution. Need to use `difference` instead of `elapsed`\n```\n`|> filter(fn: (r) => r._field == \"id\")\n|> group(columns: [\"_value\"])\n|> difference(columns: [\"_time\"])\n|> group()\n|> mean(column: \"_time\")\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-06-24T16:22:08",
      "url": "https://stackoverflow.com/questions/72745478/calculating-average-of-elapsed-time"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 72122365,
      "title": "Add custom values in the query for timeFilter",
      "problem": "I have below query and need to replace varibale with hard coded values.\nBelow query works fine when given the time range from the dashboard filter\n```\n`SELECT mean(\"count\") / $send_interval FROM \"$measurement_name\" WHERE (\"transaction\" = 'all' AND \"application\" =~ /^$application$/) AND $timeFilter GROUP BY time($__interval) fill(null)\n`\n```\nI eneterd same time range (2022-05-05 12:46:00 - 2022-05-05 12:53:00)  in millisecon as below, However can't see the data in the graph\n```\n`SELECT mean(\"count\") / $send_interval FROM \"$measurement_name\" WHERE (\"transaction\" = 'all' AND \"application\" =~ /^$application$/) AND time > 1651718760000 AND time My version:\n```\n`Grafana v6.4.1\nInfluxdb 1.7.7\n`\n```",
      "solution": "You should use timestamp format, specified here: https://docs.influxdata.com/influxdb/v1.7/query_language/spec/#dates--times\n\nThe date and time literal format is not specified in EBNF like the rest of this document. It is specified using Go\u2019s date / time parsing format, which is a reference date written in the format required by InfluxQL. The reference date time is:\n\nInfluxQL reference date time: January 2nd, 2006 at 3:04:05 PM\n```\n`time_lit            = \"2006-01-02 15:04:05.999999\" | \"2006-01-02\"\n`\n```\nAlternatively you would likely need to use epoch time in nanoseconds, since influx is stored in ns.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-05-05T07:34:23",
      "url": "https://stackoverflow.com/questions/72122365/add-custom-values-in-the-query-for-timefilter"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 71871792,
      "title": "Why does calling `write` in aioinflux with an iterable of a user-defined dataclass seem to only insert the last item?",
      "problem": "I've been trying to insert the data contained in a list, inside a running influxdb server.\nThe list contains items of the following type CoordInfluxData:\n```\n`from aioinflux.serialization.usertype import FLOAT, INT, TIMEINT, lineprotocol\nfrom dataclasses import dataclass\n\n@lineprotocol(\n    schema=dict(lon=INT, lat=INT, time=TIMEINT, humidity=FLOAT, wind_speed=FLOAT)\n)\n@dataclass\nclass CoordInfluxData(dict):\n    lon: int\n    lat: int\n    time: int\n    humidity: float\n    wind_speed: float\n`\n```\nI'm using influxdb1.8 and I can't understand why inserting an iterable of such a user-defined dataclass, not only seems to only insert the last item of the iterable in the db, but even if I explicitly call `write` and provide a `measurement` argument, the measurement does not get created in the db. The only measurement that gets created has the same name as the custom dataclass I attempt to write.\nHere's a sample script\n```\n`import asyncio\nfrom dataclasses import dataclass\n\nfrom aioinflux import InfluxDBClient\nfrom aioinflux.serialization.usertype import FLOAT, INT, TIMEINT, lineprotocol\n\n@lineprotocol(\n    schema=dict(lon=INT, lat=INT, time=TIMEINT, humidity=FLOAT, wind_speed=FLOAT)\n)\n@dataclass\nclass CoordInfluxData(dict):\n    lon: int\n    lat: int\n    time: int\n    humidity: float\n    wind_speed: float\n\nasync def main():\n    db_name = \"coord_data\"\n    data = [\n        CoordInfluxData(\n            lon=164, lat=-15, time=1649938757, humidity=75, wind_speed=5.36\n        ),\n        CoordInfluxData(lon=33, lat=-18, time=1649938757, humidity=73, wind_speed=0.99),\n        CoordInfluxData(\n            lon=139, lat=18, time=1649938757, humidity=86, wind_speed=15.13\n        ),\n    ]\n\n    client = InfluxDBClient(db=db_name)\n    await client.create_database(db_name)\n\n    await client.write(data, \"weather_data\")\n\n    await client.close()\n\n# Call main\nasyncio.run(main())\n\n`\n```",
      "solution": "After exhaustively studying the reported issue and by carefully looking at the provided data I believe that the problem is that `InfluxDB` identifies the provided points as duplicates. As the docs say:\n\nA point is uniquely identified by the measurement name, tag set, and\ntimestamp. If you submit a new point with the same measurement, tag\nset, and timestamp as an existing point, the field set becomes the\nunion of the old field set and the new field set, where any ties go to\nthe new field set",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-04-14T14:40:36",
      "url": "https://stackoverflow.com/questions/71871792/why-does-calling-write-in-aioinflux-with-an-iterable-of-a-user-defined-datacla"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 69653830,
      "title": "@influxdata/giraffe - Table graph not rendering",
      "problem": "I've been trying to integrate giraffe's tableGraph\nin a app I was building but got this error:\n\nTypeError: Cannot read properties of undefined (reading 'data')\n\nHere are the configurations:\n`    const tableLayer = {\n    type: \"table\",\n    timeZone: \"America/Phoenix\",\n    tableTheme: \"light\",\n    properties: {\n      colors: DEFAULT_TABLE_COLORS,\n      tableOptions: {\n        fixFirstColumn: false,\n        verticalTimeAxis: false,\n        sortBy: {\n          internalName: \"_time\",\n          displayName: \"_time\",\n          visible: true,\n        },\n      },\n      fieldOptions: {\n        internalName: \"_time\",\n        displayName: \"_time\",\n        visible: true,\n      },\n      timeFormat: \"YYYY-MM-DD HH:mm:ss ZZ\",\n      decimalPlaces: {\n        isEnforced: true,\n        digits: 3,\n      },\n    },\n  };\n\n  let config = {\n    table: fluxResponse.table,\n    layers: [tableLayer],\n  };\n`\nAnd here is the rendered component:\n`    \n          \n            \n          \n    \n`\nAny ideas on what causes this error?",
      "solution": "According to TCL735 in issue #742, the problem is in your `config`:\nTableGraph doesn't have a property `table`, instead you should use property `fluxResponse`.\n\nThus, you can pass your `fluxResponse` directly into the `fluxResponse` property and this should work.\n\nYou can find out more about it in the documentation on Data Properties: For TableGraph, the property `fluxResponse` is required \"as it is the only property that represents the data\", while property `table` is not used.\n\nSince the naming is confusing, they consider to either rename TableGraph or delete it in favor of Simple Graph.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-10-21T00:43:59",
      "url": "https://stackoverflow.com/questions/69653830/influxdata-giraffe-table-graph-not-rendering"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 68107174,
      "title": "InfluxDB placeholder &quot;Empty bound parameter&quot;",
      "problem": "I'm struggling to format a InfluxDB query using placeholders.\nHere I query with multiple placeholders.\nThe values are defined in the placeholders object, as seen below,\n```\n`let query = `SELECT grid_ref_x, grid_ref_y, label FROM position\nWHERE \"label\" = $ and time >= $ - $`;\n\nconst placeholders = {label: 'person', from: 'now()', interval: '5m'};\nconst resp = await influx.query(query, { placeholders });\n`\n```\nOnce sent, an `error 400 - error parsing query: empty bound parameter`\nIn the error I can see the GET request, where it appears that the Influx library has correctly formatted the placeholders under \"params\".\n```\n`/query?p=root&u=root&db=heatmap&epoch=&q=SELECT grid_ref_x, grid_ref_y, label FROM position WHERE label = $ and time >= $ - $&rp=&params={\"from\":\"now()\",\"interval\":\"5h\",\"label\":\"person\"}\n`\n```\nHow do I correctly format my query?",
      "solution": "Have you tried changing\n```\n`let query = `SELECT grid_ref_x, grid_ref_y, label FROM position\nWHERE \"label\" = $ and time >= $ - $`;\n`\n```\nTO\n```\n`let query = `SELECT grid_ref_x, grid_ref_y, label FROM position\nWHERE \"label\" = $label and time >= $from - $interval`;\n`\n```\nThe difference is that you reference the Placeholders or bind parameters using $variable_name instead of $. Assuming you're using node_influx, you can remove the double quotes on your tags and it'll still work.\nI used this PR as reference https://github.com/node-influx/node-influx/issues/587",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-06-23T23:24:16",
      "url": "https://stackoverflow.com/questions/68107174/influxdb-placeholder-empty-bound-parameter"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 67999381,
      "title": "Create Dockerfile for python application to read InfluxDB",
      "problem": "I am having a simple python script to fetch data from a table in InfluxDB installed in the local system. The deviceStatus.py script is as shown\n```\n`import time\nimport sys\nimport influxdb\nfrom influxdb import InfluxDBClient\n\nclient = InfluxDBClient(host='localhost', port=8086)\nclient.switch_database('deviceConfiguration')\nresults = client.query('SELECT (*) FROM \"autogen\".\"FactoryConfig\"')\npoints = results.get_points()\nfor point in points:\n     print(point['Connection'])\n`\n```\nThis script runs without any error and prints the IP Address (Connection) from the table FactoryConfig.\nNow I want to create a docker image out of it. I wrote a Dockerfile that looks like this\n```\n`FROM python:3.10.0b2-buster\n\nWORKDIR /usr/src/app\n\nCOPY deviceStatus.py .\n\nRUN pip install influxdb\n\nCMD [\"python\", \"./deviceStatus.py\"]\n`\n```\nThis file compiles and creates a docker image named devicestatus. Now when I try to run the image with\n```\n`sudo docker run devicestatus\n`\n```\nit shows me an error on line 8 and complains that it cannot establish a new connection: [Errno 111] Connection refused\n```\n`File \"/usr/src/app/./deviceStatus.py\", line 8, in \n    results= client.query('SELECT (*) FROM \"autogen\".\"FactoryConfig\"')\n`\n```\nI suppose it\u2019s something to do with the port. I am not able to understand how can I expose the port if this is the problem. I need help regarding this issue.\nThanks in advance.\nCheers,\nSD",
      "solution": "client = InfluxDBClient(host='localhost', port=8086)\n\nWhen running in container, `localhost` means the container itself, not the host machine. So you have 2 solutions to choose:\n\nChange `localhost` to the ip of your host pc.\nWhen docker run, add `--net=host` to let container directly use the host network.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-06-16T10:53:03",
      "url": "https://stackoverflow.com/questions/67999381/create-dockerfile-for-python-application-to-read-influxdb"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 67925175,
      "title": "InfluxQL: tag called Group causing grief",
      "problem": "I have to work with an influxDB database and one of the tags is called Group. For example the query `select * from Testing limit 10` returns\n```\n`time          Expiry     Group Parameter Type     value\n----          ------     ----- --------- ----     -----\n1623325196515 2021-06-18 GA011 Aleft     Combined -21.739\n1623325196515 2021-06-18 GA011 Aright    Combined 0.933\n1623325196515 2021-06-18 GA011 Bleft     Combined 43.097\n1623325196515 2021-06-18 GA011 Bright    Combined 2.582\n1623325196515 2021-06-18 GA011 Cleft     Combined 24.841\n1623325196515 2021-06-18 GA011 Cright    Combined -3.444\n1623325196515 2021-06-18 GA011 Dleft     Combined 12.012\n1623325196515 2021-06-18 GA011 Dright    Combined 2.574\n1623325196515 2021-06-18 GA011 Eleft     Combined 1.232\n1623325196515 2021-06-18 GA011 Eright    Combined 15.764\n`\n```\nThe problem is when I try to have a rule based on group, e.g.\n```\n`select * from Testing where Group='GA011'\n`\n```\nI get an error that says\n\nERR: error parsing query: found GROUP, expected identifier, string, number, bool at line 1, char 29.\n\nBasically it treats `Group` as a function not as a tag. Is there any way to escape the keyword `Group` and query the database.\nPS. I didn't have to do anything with the design and it cannot be changed now.",
      "solution": "you can escape it  using double quote:\n```\n`select * from Testing where \"Group\"='GA011'\n`\n```",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-06-10T18:33:03",
      "url": "https://stackoverflow.com/questions/67925175/influxql-tag-called-group-causing-grief"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 67038206,
      "title": "How to get a query variable on InfluxDB 2.0 dashboard?",
      "problem": "I read the documentation https://docs.influxdata.com/influxdb/v2.0/visualize-data/variables/\nI thought great that will be a piece of cake.\nI take a look at an existing query variable named `bucket`:\n```\n`buckets()\n  |> filter(fn: (r) => r.name !~ /^_/)\n  |> rename(columns: {name: \"_value\"})\n  |> keep(columns: [\"_value\"])\n`\n```\nIt returns this data:\n```\n`#group,false,false,false\n#datatype,string,long,string\n#default,_result,,\n,result,table,_value\n,,0,pool\n,,0,test\n`\n```\nThe `bucket` variable works and I can refer to it as `v.bucket` in the cell queries of any dashboard.\nBuilding on this example I craft the following query:\n```\n`from(bucket: \"pool\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r._measurement == \"minerstat\")\n  |> keep(columns: [\"account\"])\n  |> distinct(column: \"account\")\n  |> keep(columns: [\"_value\"])\n`\n```\nThat returns this data:\n```\n`#group,false,false,false\n#datatype,string,long,string\n#default,_result,,\n,result,table,_value\n,,0,0x04ff4e0c05c0feacccf93251c52a78639e0abef4\n,,0,0x201f1a58f31801dcd09dc75616fa40e07a70467f\n,,0,0x80475710b08ef41f5361e07ad5a815eb3b11ed7b\n,,0,0xa68a71f0529a864319082c2475cb4e495a5580fd\n`\n```\nAnd I save it as a query variable with the name `account`.\nThen I use it in a dashboard cell query like this:\n```\n`from(bucket: \"pool\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"minerstat\")\n  |> filter(fn: (r) => r[\"account\"] == v.account)\n  |> filter(fn: (r) => r[\"_field\"] == \"currentHashrate\" or r[\"_field\"] == \"hashrate\")\n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)\n  |> yield(name: \"last\")\n`\n```\nBut this returns no data.  And the dropdown menu for the account variable on the dashboard view is empty.\nIf I replace `v.account` above with one of the value returned by the query behind the variable:\n```\n`from(bucket: \"pool\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"minerstat\")\n  |> filter(fn: (r) => r[\"account\"] == \"0x04ff4e0c05c0feacccf93251c52a78639e0abef4\")\n  |> filter(fn: (r) => r[\"_field\"] == \"currentHashrate\" or r[\"_field\"] == \"hashrate\")\n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)\n  |> yield(name: \"last\")\n`\n```\nThat works as intended and display a nice graph.\nWhat am I missing here?\nSOLUTION: you cannot use variables inside the definition of a variable.\nI replaced\n`start: v.timeRangeStart, stop: v.timeRangeStop`\nwith\n`start: -1d`\nin the variable definition:\n```\n`from(bucket: \"pool\")\n  |> range(start: -1d)\n  |> filter(fn: (r) => r._measurement == \"minerstat\")\n  |> keep(columns: [\"account\"])\n  |> distinct(column: \"account\")\n  |> keep(columns: [\"_value\"])\n`\n```",
      "solution": "I don't think you can use variables within variables, so things like v.timeRangeStart that you can use in a dashboard query can't be used to define another dashboard variable.\nYou can use duration literals though, like -5d or -2h in your range() call though.",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2021-04-10T21:03:27",
      "url": "https://stackoverflow.com/questions/67038206/how-to-get-a-query-variable-on-influxdb-2-0-dashboard"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 78571836,
      "title": "(InfluxDB + SQL) How to count number of events in a time window using date_bin, and fill empty windows with zero?",
      "problem": "(Using InfluxDB 3.0 cloud serverless, and SQL as the query language).\nI have a simple table that captures that a named event occurred (with no data attached to it). I simply want to count the number of occurrences in a given time window. I can do that using the `date_bin` function. However, my problem is that if no events occurred within a certain time window, no records are present in the result set.\nInstead, I want the result set to include rows for every time window in the overall time range, and show `0` if no events occurred within it.\nHere's my current query:\n```\n`SELECT date_bin('1 minute', time) AS time, Count(\"Event\")\nFROM \"CW\" \nWHERE $__timeFilter(time) \nAND \"Event\" = 'thing_xyz_happened'\nGROUP BY 1\nORDER BY \"time\"\n`\n```\nSo for example, consider my overall query time range being 5 minutes. In the first minute, 1 \"thing_xyz_happened\" event occurred, and in the second minute, it happened twice. In minutes 3, 4 and 5, no such event happened and therefore no records exist in the database for those minutes. Currently here is what that query returns:\n\nTime\nCount\n\n2024-06-03 13:37\n1\n\n2024-06-03 13:38\n2\n\nNote that 13:39, :40 and :41 are absent. This messes up my bar chart graphs. I want to show an actual \"0\" for those last 3 minutes:\n\nTime\nCount\n\n2024-06-03 13:37\n1\n\n2024-06-03 13:38\n2\n\n2024-06-03 13:39\n0\n\n2024-06-03 13:40\n0\n\n2024-06-03 13:41\n0\n\nI've looked at date_bin_gapfill, but the only options available are interpolation or carrying-forward of previous data. Neither of those will give me the `0` I want.",
      "solution": "try with `generate_series` function:\n```\n`WITH time_series AS (\n  SELECT generate_series(start_time, end_time, 1m) AS time\n  FROM (\n    SELECT date_trunc('minute', $__timeFrom()) AS start_time,\n           date_trunc('minute', $__timeTo()) AS end_time\n  )\n)\nSELECT date_bin('1 minute', ts.time) AS time,\n       COUNT(\"Event\") AS count\nFROM time_series ts\nLEFT JOIN \"CW\" c ON date_bin('1 minute', c.time) = date_bin('1 minute', ts.time) AND c.\"Event\" = 'thing_xyz_happened'\nGROUP BY 1\nORDER BY time;\n`\n```",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2024-06-03T19:54:27",
      "url": "https://stackoverflow.com/questions/78571836/influxdb-sql-how-to-count-number-of-events-in-a-time-window-using-date-bin"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 78040253,
      "title": "&quot;i.micrometer.influx.InfluxMeterRegistry : unable to create database &#39;mydb&#39;: {&quot;code&quot;:&quot;unauthorized&quot;,&quot;message&quot;:&quot;unauthorized access&quot;}&quot;",
      "problem": "I'm trying out Influxdb for the first and I want to use it to save and fetch the metrics of my application. I have version v2.7.5 installed, I have created an organization, bucket and generated an API token with all access on Influx UI. In my application, I set the MicrometerMetricsOptions, InfluxMeterRegistry, and a Metrics controller but when I call the endpoint I get an error message i.micrometer.influx.InfluxMeterRegistry  : unable to create database 'mydb': {\"code\":\"unauthorized\",\"message\":\"unauthorized access\"} . on the UI there's no place to create a db and even when I setDb method on my configuration it's still the same issue. Below is my configuration\n```\n`@Configuration\npublic class VertxConfig {\n\n    @Value(\"${influxdb.api.token}\")\n    public String authToken;\n\n    @Bean\n    @Autowired\n    public Vertx vertx(VerticleFactory verticleFactory) {\n        MicrometerMetricsOptions metricsOptions = new MicrometerMetricsOptions()\n                .setInfluxDbOptions(new VertxInfluxDbOptions()\n                        .setUri(\"http://localhost:8086\")\n                        .setOrg(\"Technologies\")\n                        .setDb(\"life\")\n                        .setUserName(\"life\")\n                        .setPassword(\"1234\")\n                        .setBucket(\"life\")\n                        .setToken(authToken)\n                        .setEnabled(true))\n                .setEnabled(true);\n\n        VertxOptions vertxOptions = new VertxOptions()\n                .setMetricsOptions(metricsOptions)\n                .setWorkerPoolSize(35);\n        Vertx vertx = Vertx.vertx(vertxOptions);\n        vertx.registerVerticleFactory(verticleFactory);\n        return vertx;\n    }\n}\n`\n```\n```\n`@SpringBootApplication\npublic class MetricsApplication {\n\n    public static void main(String[] args) {\n        SpringApplication springApplication = new SpringApplication(MetricsApplication.class);\n        springApplication.setWebApplicationType(WebApplicationType.NONE);\n        springApplication.setAdditionalProfiles(\"prod\");\n        springApplication.run(args);\n\n        InfluxConfig config = new InfluxConfig() {\n            @Override\n            public String get(String key) {\n                return switch (key) {\n                    case \"influxdb.uri\" -> \"http://localhost:8086\";\n//                  case \"influxdb.db\" -> \"life\";\n                    case \"influxdb.userName\" -> \"life\";\n                    case \"influxdb.password\" -> \"1234\";\n                    default -> null;\n                };\n            }\n        };\n\n        InfluxMeterRegistry registry = InfluxMeterRegistry.builder(config)\n                .clock(Clock.SYSTEM)\n                .build();\n\n        registry.config().meterFilter(new MeterFilter() {\n            @Override\n            public DistributionStatisticConfig configure(Meter.Id id, DistributionStatisticConfig config) {\n                return DistributionStatisticConfig.builder()\n                        .percentilesHistogram(true)\n                        .build()\n                        .merge(config);\n            }\n        });\n    }\n\n}\n`\n```",
      "solution": "I figured the problem was the InfluxMeterRegistry I implemented in the main method. That's Influxdb v1, once I removed it the error was gone. The InfluxMeterRegistry is configured in v2. What's needed for Influxdb V2 configuration is Uri, organization or organizationId, bucket, and token. Then you can visualize metrics stored in Influxdb with tools like Grafana",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2024-02-22T11:38:35",
      "url": "https://stackoverflow.com/questions/78040253/i-micrometer-influx-influxmeterregistry-unable-to-create-database-mydb-c"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 76762446,
      "title": "Undefined identifier in InfluxDB query, invalid binary operator INVALID_OP",
      "problem": "I am querying InfluxDB by bucket name and with a range and I got this error: `\"code\":\"invalid\",\"message\":\"error @1:56-1:57: undefined identifier r\\\\n\\\\nerror @1:53-1:57: invalid binary operator \\\\u003cINVALID_OP\\\\u003e\"}'` from the following request:\n```\n`client.query_api().query(query='from(bucket:\"bucket_name\") |> range(start: -1hr)')\n`\n```",
      "solution": "Turns out that the response error is a roundabout way of saying that you should use `1h` instead of `1hr`. Be careful with your time range formats and it will hopefully save you some time",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2023-07-25T13:54:38",
      "url": "https://stackoverflow.com/questions/76762446/undefined-identifier-in-influxdb-query-invalid-binary-operator-invalid-op"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 76411498,
      "title": "Python unittest mocking influxdb",
      "problem": "I want to test a class which passes data to an influx-database.\nFor that purpose I use influxdb.InfluxDBClient. For the unit-tests I want to mock influxdb.InfluxDBClient, but it seems that this is not working. I always get timeouts, so the mocking does not work at all.\nThis is the class I want to test:\n`''' Client for influxDB1. '''\nfrom influxdb import InfluxDBClient\n\nclass Influx1Writer:\n''' Client to access influxDB1. '''\n        \n    def __init__(self, host, port, user, password, dbname, usessl) -> None:\n    ''' Initializer, will be called from constructor. '''\n        self.client = InfluxDBClient(\n            host=host,\n            port=port,\n            username=user,\n            password=password,\n            database=dbname,\n            ssl=usessl)\n        self.client.create_database(dbname)\n        self.client.switch_database(dbname)\n        self.client.switch_user(user, password)\n`\nThis is the test case\n`''' Unittest writer for influxdb, version 1. '''\nimport unittest\nfrom unittest.mock import patch, MagicMock\n\nfrom influxdb import InfluxDBClient\nfrom influx.influx_writer_1 import Influx1Writer\n\nclass TestInflux1Writer(unittest.TestCase):\n    ''' Unittests of Influx1Writer. '''\n\n    host = \"host\"\n    port = 8080\n    user = \"user\"\n    password = \"password\"\n    dbname = \"dbname\"\n    usessl = False\n\n    def setUp(self) -> None:\n        ''' Setup test fixture. '''\n        mock_influx = MagicMock('influxdb.InfluxDBClient')\n        self.handler = Influx1Writer(self.host, \\\n                                     self.port, \\\n                                     self.user, \\\n                                     self.password, \\\n                                     self.dbname, \\\n                                     self.usessl)\n        mock_influx.assert_called_with(self.host, \\\n                                        self.port, \\\n                                        self.user, \\\n                                        self.password, \\\n                                        self.dbname, \\\n                                        self.usessl)\n\n        super().setUp()\n\n    def tearDown(self) -> None:\n        ''' Tear down test fixture. '''\n        self.handler = None\n        super().tearDown()\n\n    ###\n\n    @patch('influxdb.InfluxDBClient')\n    @patch('influxdb.InfluxDBClient.create_database')\n    @patch('influxdb.InfluxDBClient.switch_database')\n    @patch('influxdb.InfluxDBClient.switch_user')\n    def test_01a_init_called(self, mock_client, mock_create_database, mock_switch_database, mock_switch_user) -> None:\n        ''' Check InfluxDBClient is called with parameters. '''\n        mock_client.__init__.assert_called_with(self.host, self.port, self.user, self.password, self.dbname, self.usessl)\n        mock_create_database.assert_called_with(self.dbname)\n        mock_switch_database.assert_called_with(self.dbname)\n        mock_switch_user.assert_called_with(self.dbname)\n        self.assertEqual(type(self.handler), Influx1Writer)\n`\nWhen the test is executed, a timeout will occur. This means that influxdb.InfluxDBClient is not mocked.\nWhat am I doing wrong here?\n\nFound the solution, thank you @blhsing\n`\n    def setUp(self) -> None:\n        ''' Setup test fixture. '''\n        self.patch_client = patch('influx.influx_writer_1.InfluxDBClient')\n        mock_influx = self.patch_client.start()\n        self.client = Influx1Writer(self.host, \\\n                                    self.port, \\\n                                    self.user, \\\n                                    self.password, \\\n                                    self.dbname, \\\n                                    self.usessl)\n        mock_influx.assert_called_with( host=self.host, \\\n                                        port=self.port, \\\n                                        username=self.user, \\\n                                        password=self.password, \\\n                                        database=self.dbname, \\\n                                        ssl=self.usessl)\n        super().setUp()\n`",
      "solution": "By the time you patch `influxdb.InfluxDBClient`, it is already imported into the namespace of the module `influx.influx_writer_1` so the module keeps a reference to the original `InfluxDBClient` even after `influxdb.InfluxDBClient` is patched. Instead, you should patch `influx.influx_writer_1.InfluxDBClient` so the name `InfluxDBClient` within the module `influxdb.InfluxDBClient` can actually be reassigned.\nAlso, you should use `patch` instead of `MagicMock` to patch an object. To patch an object for the tests in a `TestCase`, call the `start` method of the patcher in the `setUp` method, and then restore the original object by calling the `stop` method of the patcher in the `tearDown` method of the `TestCase`:\n```\n`def setUp(self) -> None:\n    ''' Setup test fixture. '''\n    self.patcher = patch('influx.influx_writer_1.InfluxDBClient')\n    self.patcher.start()\n    self.handler = Influx1Writer(self.host, \\\n                                 self.port, \\\n                                 self.user, \\\n                                 self.password, \\\n                                 self.dbname, \\\n                                 self.usessl)\n    Influx1Writer.assert_called_with(self.host, \\\n                                    self.port, \\\n                                    self.user, \\\n                                    self.password, \\\n                                    self.dbname, \\\n                                    self.usessl)\n\n    super().setUp()\n\ndef tearDown(self) -> None:\n    ''' Tear down test fixture. '''\n    self.patcher.stop()\n    self.handler = None\n    super().tearDown()\n`\n```",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2023-06-06T06:43:32",
      "url": "https://stackoverflow.com/questions/76411498/python-unittest-mocking-influxdb"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 74611153,
      "title": "InfluxDB 2 / influxdb-client-java: Long data is written but String value is read. ClassCastException occurred",
      "problem": "InfluxDB v2.5.1\ninfluxdb-client-java 6.7.0\nJava POJO\n`@Data\n@Builder\n@NoArgsConstructor\n@AllArgsConstructor\n@Measurement(name = \"my_measurement\")\npublic class MyMeasurement {\n\n    @Column(timestamp = true)\n    private Instant time;\n\n    @Column(name = \"my_tag\", tag = true)\n    private Long myTag;\n\n    @Column(name = \"value\")\n    private Integer value;\n\n}\n`\nWrite\n`influxDBClient.getWriteApiBlocking().writeMeasurements(WritePrecision.NS,\n        IntStream.range(0, 1000)\n                .mapToObj(i -> MyMeasurement.builder()\n                        .time(Instant.now())\n                        .myTag(Math.abs(new Random().nextLong()))\n                        .value(Math.abs(new Random().nextInt()))\n                        .build())\n                .collect(Collectors.toList()));\n`\nRead\n`influxDBClient.getQueryApi()\n    .query(\"from(bucket: \\\"mybucket\\\") |> range(start: -1d) |> filter(fn: (r) => r[\\\"_measurement\\\"] == \\\"my_measurement\\\")\", MyMeasurement.class);\n`\nThe reading statement throws `InfluxException`\n```\n`com.influxdb.exceptions.InfluxException: Class 'MyMeasurement' field 'myTag' was defined with a different field type and caused a ClassCastException. The correct type is 'java.lang.String' (current field value: '1000816197908726879').\n`\n```",
      "solution": "First of all tag in influxdb always is string link\nSecondly when you get it from influx using influx-client it get it from influxdb as a String and cannot cast to Long.\n(because under hood java influx client uses `long.class.isAssignableFrom(fieldType))` and String is not assiganble for Long.\nPossible solution is to use `String` for tags in your POJO.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2022-11-29T09:36:09",
      "url": "https://stackoverflow.com/questions/74611153/influxdb-2-influxdb-client-java-long-data-is-written-but-string-value-is-read"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 74083933,
      "title": "Telegraf json_v2 parser error: Unable to convert field to type int. strconv.ParseInt: parsing invalid syntax",
      "problem": "Good day!\nI have built a small IoT device that monitors the conditions inside a specific enclosure using an ESP32 and a couple of sensors. I want to monitor that data by publishing it to the ThingSpeak cloud, then writing it to InfluxDB with Telegraf and finally using the InfluxDB data source in Grafana to visualize it.\nSo far I have made everything work flawlessly, but with one small exception.\nWhich is: One of the plugins in my telegraf config fails with the error:\n\nparsing metrics failed: Unable to convert field 'temperature' to type int: strconv.ParseInt: parsing \"15.4\": invalid syntax\n\nThe plugins are `[inputs.http]]` and `[[inputs.http.json_v2]]` and what I am doing with them is authenticating against my ThingSpeak API and parsing the `json` output of my fields. Then in my `/etc/telegraf/telegraf.conf` under `[[inputs.http.json_v2.field]]` I have added `type = int` as otherwise telegraf writes my metrics as Strings in InfluxDB and the only way to visualize them is using either a table or a single stat, because the rest of the flux queries fail with the error `unsupported input type for mean aggregate: string`. However, when I change to `type = float` in the config file I get a different error:\n\nunprocessable entity: failure writing points to database: partial write: field type conflict: input field \"temperature\" on measurement \"sensorData\" is type float, already exists as type string dropped=1\n\nI have a suspicion that I have misconfigured the parser plugin, however after hours of debugging I couldn't come up with a solution.\nSome information that might be of use:\nTelegraf version: `Telegraf 1.24.2`\nInfluxdb version: `InfluxDB v2.4.0`\nPlease see below for my `telegraf.conf` as well as the error messages.\nAny help would be highly appreciated! (:\n```\n`[agent]\n  interval = \"10s\"\n  round_interval = true\n  metric_batch_size = 1000\n  metric_buffer_limit = 1000\n  collection_jitter = \"0s\"\n  flush_interval = \"10s\"\n  flush_jitter = \"0s\"\n  precision = \"\"\n  hostname = \"\"\n  omit_hostname = false\n\n[[outputs.influxdb_v2]]\n  urls = [\"http://localhost:8086\"]\n  token = \"XXXXXXXX\"\n  organization = \"XXXXXXXXX\"\n  bucket = \"sensor\"\n\n[[inputs.http]]\n    urls = [\n\"https://api.thingspeak.com/channels/XXXXX/feeds.json?api_key=XXXXXXXXXX&results=2\"\n]\n  name_override = \"sensorData\"\n  tagexclude = [\"url\", \"host\"]\n  data_format = \"json_v2\"\n\n  ## HTTP method\n  method = \"GET\"\n\n[[inputs.http.json_v2]]\n\n  [[inputs.http.json_v2.field]]\n       path = \"feeds.1.field1\"\n       rename = \"temperature\"\n       type = \"int\"        #Error message 1 \n       #type = \"float\"     #Error message 2\n`\n```\nError when type = \"float\":\n```\n`me@myserver:/etc/telegraf$ telegraf -config telegraf.conf --debug\n2022-10-16T00:31:43Z I! Starting Telegraf 1.24.2\n2022-10-16T00:31:43Z I! Available plugins: 222 inputs, 9 aggregators, 26 processors, 20 \nparsers, 57 outputs\n2022-10-16T00:31:43Z I! Loaded inputs: http\n2022-10-16T00:31:43Z I! Loaded aggregators:\n2022-10-16T00:31:43Z I! Loaded processors:\n2022-10-16T00:31:43Z I! Loaded outputs: influxdb_v2\n2022-10-16T00:31:43Z I! Tags enabled: host=myserver\n2022-10-16T00:31:43Z I! [agent] Config: Interval:10s, Quiet:false, Hostname:\"myserver\", \nFlush Interval:10s\n2022-10-16T00:31:43Z D! [agent] Initializing plugins\n2022-10-16T00:31:43Z D! [agent] Connecting outputs\n2022-10-16T00:31:43Z D! [agent] Attempting connection to [outputs.influxdb_v2]\n2022-10-16T00:31:43Z D! [agent] Successfully connected to outputs.influxdb_v2\n2022-10-16T00:31:43Z D! [agent] Starting service inputs\n2022-10-16T00:31:53Z E! [outputs.influxdb_v2] Failed to write metric to sensor (will be \ndropped: 422 Unprocessable Entity): unprocessable entity: failure writing points to \ndatabase: partial write: field type conflict: input field \"temperature\" on measurement \n\"sensorData\" is type float, already exists as type string dropped=1\n2022-10-16T00:31:53Z D! [outputs.influxdb_v2] Wrote batch of 1 metrics in 8.9558ms\n2022-10-16T00:31:53Z D! [outputs.influxdb_v2] Buffer fullness: 0 / 10000 metrics\n`\n```\nError when type = \"int\"\n```\n`me@myserver:/etc/telegraf$ telegraf -config telegraf.conf --debug\n2022-10-16T00:37:05Z I! Starting Telegraf 1.24.2\n2022-10-16T00:37:05Z I! Available plugins: 222 inputs, 9 aggregators, 26 processors, 20 \nparsers, 57 outputs\n2022-10-16T00:37:05Z I! Loaded inputs: http\n2022-10-16T00:37:05Z I! Loaded aggregators:\n2022-10-16T00:37:05Z I! Loaded processors:\n2022-10-16T00:37:05Z I! Loaded outputs: influxdb_v2\n2022-10-16T00:37:05Z I! Tags enabled: host=myserver\n2022-10-16T00:37:05Z I! [agent] Config: Interval:10s, Quiet:false, Hostname:\"myserver\", \nFlush Interval:10s\n2022-10-16T00:37:05Z D! [agent] Initializing plugins\n2022-10-16T00:37:05Z D! [agent] Connecting outputs\n2022-10-16T00:37:05Z D! [agent] Attempting connection to [outputs.influxdb_v2]\n2022-10-16T00:37:05Z D! [agent] Successfully connected to outputs.influxdb_v2\n2022-10-16T00:37:05Z D! [agent] Starting service inputs\n2022-10-16T00:37:10Z E! [inputs.http] Error in plugin: \n[url=https://api.thingspeak.com/channels/XXXXXX/feeds.json? \napi_key=XXXXXXX&results=2]: parsing metrics failed: Unable to convert field \n'temperature' to type int: strconv.ParseInt: parsing \"15.3\": invalid syntax\n`\n```",
      "solution": "Fixed it by leaving `type = float` under `[[inputs.http.json_v2.field]]` in `telegraf.conf` and creating a NEW bucket with a new API key in Influx.\nThe issue was that the bucket `sensor` that I had previously defined in my `telegraf.conf` already had the field temperature created in my influx database from previous tries with its type set as `last` (aka: `String`) which could not be overwritten with the new type `mean` (aka: `float`).\nAs soon as I deleted all pre existing buckets everything started working as expected.\nInfluxDB dashboard",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2022-10-16T02:46:49",
      "url": "https://stackoverflow.com/questions/74083933/telegraf-json-v2-parser-error-unable-to-convert-field-to-type-int-strconv-pars"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 73244302,
      "title": "Access InfluxDB 2 through Traefik via subdomain",
      "problem": "I've installed Traefik and InfluxDB on docker, and am trying to configure Traefik to act as a reverse proxy for all InfluxDB requests via a subdomain e.g. https://influxdb.mydomain.com.\nThe InfluxDB labels I have are:\n```\n`- traefik.enable=true\n- traefik.http.routers.influxdb.entryPoints=websecure\n- traefik.http.routers.influxdb.rule=Host(`influxdb.mydomain.com`)\n- traefik.http.routers.influxdb.tls=true\n- traefik.http.routers.influxdb.tls.certresolver=cloudflare\n- traefik.http.services.influxdb.loadbalancer.server.port=8086\n`\n```\nWhen accessing InfluxDB, I can see the page loading, but get a `404` on `/api/v2/setup` (when I inspect the network requests), so it doesn't load completely. All other resource load correctly e.g. `/6588f709b0.js`, `/26.c9f12339d6.js`, etc.\nI can access InfluxDB via port `8086` without issues `http://influxdb.mydomain.com:8086`.\nAny idea what I might be missing to allow access to `/api` via Traefik?",
      "solution": "Just posting a solution to my issue in case anyone else runs into a similar problem.\nI had setup Traefik to route all `/api` requests to itself, since Traefik also has the `/api` endpoint, without specifying the `Host`.  After specifying the `Host` for the Traefik container, `/api` requests get routed correctly to the `influx` sub-domain.  Traefik Dashboard and InfluxDB are on different sub-domains.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2022-08-05T05:19:51",
      "url": "https://stackoverflow.com/questions/73244302/access-influxdb-2-through-traefik-via-subdomain"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 72897615,
      "title": "Influxdb 2.0.8 CLI remote access with basic auth",
      "problem": "Is there a way to connect to an Influx DB 2.0.8 remotely from Windows using the `influx` cli tool with basic auth (username and password)?\nCannot find anything in the documentation, it either requires a token or assumes the CLI is run on the same machine with InfluxDB.",
      "solution": "The CLI for InfluxDB 2.x uses organization and token for authentication, rather than username and password.\nSee `influx config create --help` for information on how to set that up.\nYou can also add multiple credentials to the 2.x CLI, to allow switching back and forth between development and production, or whatever, using the `influx config` command.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2022-07-07T14:12:33",
      "url": "https://stackoverflow.com/questions/72897615/influxdb-2-0-8-cli-remote-access-with-basic-auth"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 72750170,
      "title": "Create Influx query as a function in R",
      "problem": "I am trying to turn the Influx query into a function in R so I can change the fields as I see fit. Here is an example of the code I am running\n```\n`my_bucket  range(start:',start,'stop:,'stop')',sep = \"\")\ndata This particular method uses paste() and it keeps the escape character \\ in the query. I would like to get rid of that . I have tried using cat() but that is for printing to the console and also have tried capture.output() of the cat() string which still captures the escape characters.\nWhat I would like to see and be stored as an object is the output below. I used cat() to show you exactly what I need (I know I can't use it to store things).\n```\n`cat('\\'from(bucket:\\\"',my_bucket,'\\\")|> range(start:',start,',stop:,',stop,')\\'', sep = \"\")\n>'from(bucket:\"my_bucket\")|> range(start:start_time,stop:,stop_time)'\n`\n```\nNote the single quotes around the query beginning at from and ending after the parantheses after stop_time. In addtion the double quotes must be present around the bucket I call to. This is required syntax for the query from R.",
      "solution": "I would suggest you try to use `sprintf`, I find it much easier to properly format the query.\n```\n`q  range(start: %s, stop: %s)', my_bucket, start, stop)\n`\n```\nAnyway, the same can be done with `paste`:\n```\n`q  range(start: ',start,',stop: ',stop,')',sep = \"\")\n`\n```",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2022-06-25T01:12:41",
      "url": "https://stackoverflow.com/questions/72750170/create-influx-query-as-a-function-in-r"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 71842922,
      "title": "Store datapoints persistently before writing to database",
      "problem": "I have a Django app that receives sensor data. This data is then processed and written to `influxDB` using the `influxdb-client-python` library. I would like to write the data in an asynchronous manner and thus return a response to the producer of the data before it is actually written to the database.\nHowever, once I send this response I can no longer afford to lose this data. Since I can never be sure that the server will in fact be able to write the data to `influxDB`, I was thinking about first writing it to a file and returning a response after this is successful (similar to a WAL). This introduces new problems like making sure the WAL is actually written to a file and most importantly, making sure it is thread-safe since the server may be handling multiple requests concurrently.\nIs there any better way of doing this, maybe built-in in `Django`?",
      "solution": "That sounds like a queue, background tasks, etc.  You are right, it just displaces the issue, but queues are highly reliable as well.\nThe standard libs for doing this with Django and/or Rest Framework are:\n\nCelery - Full featured, mature, pluggable queue backends\nDjango RQ - Simpler, uses only redis\n\nCelery is probably the right starting point here, since it lets you use a \"real\" queue backend, but Django RQ+redis will also work if there isn't a ton of load.\nWithout knowing anything more about your app or architecture, its hard to say more.  There are a lot of queuing systems (Rabbit, ZeroMQ, AWS SQS, Google's, etc).  You can also look into building the queue+processor using, for example, AWS SQS and AWS Lambda Functions (or google versions).",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2022-04-12T14:40:57",
      "url": "https://stackoverflow.com/questions/71842922/store-datapoints-persistently-before-writing-to-database"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 69948238,
      "title": "Calculating amount of time per day a sensor is in a specific state",
      "problem": "I have several sensors which frequently report their state to InfluxDB. I now would like to get the amount of hours per day the sensor's state is ON (1). The graph looks like this:\n\nMy Python script looks like this:\n`from influxdb_client import InfluxDBClient\nimport pandas as pd\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_colwidth', None)\n\ntoken = 'XXX'\norg = 'XXX'\nbucket = 'XXX'\nquery= '''\nfrom(bucket: \"XXX\")\n  |> range(start:-6h)\n  |> filter(fn: (r) => r[\"name\"] == \"sensor1\")\n  |> filter(fn: (r) => r[\"_measurement\"] == \"undefined\")\n  |> filter(fn: (r) => r[\"_field\"] == \"STATE\")\n  |> toInt()\n'''\n\nclient = InfluxDBClient(url='https://westeurope-1.azure.cloud2.influxdata.com', token=token, org=org, debug=False)\ndf = client.query_api().query_data_frame(org=org, query=query)\ndf = df.reindex(columns=['_value', '_time'])\ndisplay(df)\n`\n```\n`_value  _time\n0   1   2021-11-12 13:22:32+00:00\n1   0   2021-11-12 13:22:34+00:00\n2   0   2021-11-12 13:26:32+00:00\n3   0   2021-11-12 13:26:34+00:00\n4   1   2021-11-12 13:28:05+00:00\n5   0   2021-11-12 13:28:06+00:00\n6   0   2021-11-12 13:29:04+00:00\n7   0   2021-11-12 13:29:06+00:00\n8   1   2021-11-12 13:39:04+00:00\n9   0   2021-11-12 13:39:06+00:00\n10  0   2021-11-12 13:41:09+00:00\n11  0   2021-11-12 13:41:10+00:00\n12  1   2021-11-12 13:52:33+00:00\n13  0   2021-11-12 13:52:34+00:00\n14  0   2021-11-12 14:00:08+00:00\n15  0   2021-11-12 14:00:10+00:00\n16  1   2021-11-12 14:07:32+00:00\n17  0   2021-11-12 14:07:34+00:00\n18  1   2021-11-12 14:17:52+00:00\n19  0   2021-11-12 14:17:53+00:00\n20  0   2021-11-12 14:18:45+00:00\n21  0   2021-11-12 14:18:46+00:00\n22  1   2021-11-12 14:22:33+00:00\n23  0   2021-11-12 14:22:34+00:00\n24  0   2021-11-12 14:32:05+00:00\n25  0   2021-11-12 14:32:06+00:00\n26  0   2021-11-12 14:33:06+00:00\n27  0   2021-11-12 14:33:45+00:00\n28  0   2021-11-12 14:33:46+00:00\n29  1   2021-11-12 14:37:33+00:00\n30  0   2021-11-12 14:37:34+00:00\n31  0   2021-11-12 14:48:20+00:00\n32  0   2021-11-12 14:48:21+00:00\n33  1   2021-11-12 14:52:33+00:00\n34  0   2021-11-12 14:52:34+00:00\n35  0   2021-11-12 14:56:04+00:00\n36  0   2021-11-12 14:56:06+00:00\n37  0   2021-11-12 14:57:55+00:00\n38  0   2021-11-12 14:57:56+00:00\n39  1   2021-11-12 15:22:33+00:00\n40  0   2021-11-12 15:22:34+00:00\n41  0   2021-11-12 15:23:04+00:00\n42  0   2021-11-12 15:23:06+00:00\n43  1   2021-11-12 15:26:04+00:00\n44  0   2021-11-12 15:26:06+00:00\n45  0   2021-11-12 15:26:33+00:00\n46  0   2021-11-12 15:26:34+00:00\n47  1   2021-11-12 15:37:32+00:00\n48  0   2021-11-12 15:37:34+00:00\n49  0   2021-11-12 15:41:05+00:00\n50  0   2021-11-12 15:41:06+00:00\n51  1   2021-11-12 15:52:32+00:00\n52  0   2021-11-12 15:52:34+00:00\n53  1   2021-11-12 16:03:13+00:00\n54  0   2021-11-12 16:03:14+00:00\n55  0   2021-11-12 16:20:04+00:00\n56  0   2021-11-12 16:20:06+00:00\n57  1   2021-11-12 16:22:05+00:00\n58  0   2021-11-12 16:37:04+00:00\n59  0   2021-11-12 16:37:06+00:00\n60  1   2021-11-12 16:37:33+00:00\n61  0   2021-11-12 16:37:34+00:00\n62  0   2021-11-12 16:43:05+00:00\n63  0   2021-11-12 16:43:06+00:00\n64  0   2021-11-12 16:43:51+00:00\n65  0   2021-11-12 16:43:53+00:00\n66  1   2021-11-12 16:56:05+00:00\n67  0   2021-11-12 16:56:06+00:00\n68  0   2021-11-12 16:56:32+00:00\n69  0   2021-11-12 16:56:34+00:00\n70  1   2021-11-12 17:07:33+00:00\n71  1   2021-11-12 17:07:34+00:00\n72  0   2021-11-12 17:08:05+00:00\n73  0   2021-11-12 17:08:06+00:00\n74  1   2021-11-12 17:10:05+00:00\n75  0   2021-11-12 17:10:06+00:00\n76  0   2021-11-12 17:11:09+00:00\n77  0   2021-11-12 17:11:10+00:00\n78  1   2021-11-12 17:22:32+00:00\n79  0   2021-11-12 17:22:33+00:00\n80  1   2021-11-12 17:22:45+00:00\n81  1   2021-11-12 17:24:08+00:00\n82  0   2021-11-12 17:24:13+00:00\n83  0   2021-11-12 17:24:14+00:00\n84  1   2021-11-12 17:25:05+00:00\n85  0   2021-11-12 17:25:06+00:00\n86  0   2021-11-12 18:14:37+00:00\n87  0   2021-11-12 18:14:38+00:00\n88  0   2021-11-12 18:45:44+00:00\n89  0   2021-11-12 18:45:46+00:00\n`\n```\n`df['end'] = df['_time'].shift(-1)\ndf['duration'] = df['end'] - df['_time']\ndisplay(df)\n`\n```\n`_value  _time   end duration\n0   1   2021-11-12 13:22:32+00:00   2021-11-12 13:22:34+00:00   0 days 00:00:02\n1   0   2021-11-12 13:22:34+00:00   2021-11-12 13:26:32+00:00   0 days 00:03:58\n2   0   2021-11-12 13:26:32+00:00   2021-11-12 13:26:34+00:00   0 days 00:00:02\n3   0   2021-11-12 13:26:34+00:00   2021-11-12 13:28:05+00:00   0 days 00:01:31\n4   1   2021-11-12 13:28:05+00:00   2021-11-12 13:28:06+00:00   0 days 00:00:01\n5   0   2021-11-12 13:28:06+00:00   2021-11-12 13:29:04+00:00   0 days 00:00:58\n6   0   2021-11-12 13:29:04+00:00   2021-11-12 13:29:06+00:00   0 days 00:00:02\n7   0   2021-11-12 13:29:06+00:00   2021-11-12 13:39:04+00:00   0 days 00:09:58\n8   1   2021-11-12 13:39:04+00:00   2021-11-12 13:39:06+00:00   0 days 00:00:02\n9   0   2021-11-12 13:39:06+00:00   2021-11-12 13:41:09+00:00   0 days 00:02:03\n10  0   2021-11-12 13:41:09+00:00   2021-11-12 13:41:10+00:00   0 days 00:00:01\n11  0   2021-11-12 13:41:10+00:00   2021-11-12 13:52:33+00:00   0 days 00:11:23\n12  1   2021-11-12 13:52:33+00:00   2021-11-12 13:52:34+00:00   0 days 00:00:01\n13  0   2021-11-12 13:52:34+00:00   2021-11-12 14:00:08+00:00   0 days 00:07:34\n14  0   2021-11-12 14:00:08+00:00   2021-11-12 14:00:10+00:00   0 days 00:00:02\n15  0   2021-11-12 14:00:10+00:00   2021-11-12 14:07:32+00:00   0 days 00:07:22\n16  1   2021-11-12 14:07:32+00:00   2021-11-12 14:07:34+00:00   0 days 00:00:02\n17  0   2021-11-12 14:07:34+00:00   2021-11-12 14:17:52+00:00   0 days 00:10:18\n18  1   2021-11-12 14:17:52+00:00   2021-11-12 14:17:53+00:00   0 days 00:00:01\n19  0   2021-11-12 14:17:53+00:00   2021-11-12 14:18:45+00:00   0 days 00:00:52\n20  0   2021-11-12 14:18:45+00:00   2021-11-12 14:18:46+00:00   0 days 00:00:01\n21  0   2021-11-12 14:18:46+00:00   2021-11-12 14:22:33+00:00   0 days 00:03:47\n22  1   2021-11-12 14:22:33+00:00   2021-11-12 14:22:34+00:00   0 days 00:00:01\n23  0   2021-11-12 14:22:34+00:00   2021-11-12 14:32:05+00:00   0 days 00:09:31\n24  0   2021-11-12 14:32:05+00:00   2021-11-12 14:32:06+00:00   0 days 00:00:01\n25  0   2021-11-12 14:32:06+00:00   2021-11-12 14:33:06+00:00   0 days 00:01:00\n26  0   2021-11-12 14:33:06+00:00   2021-11-12 14:33:45+00:00   0 days 00:00:39\n27  0   2021-11-12 14:33:45+00:00   2021-11-12 14:33:46+00:00   0 days 00:00:01\n28  0   2021-11-12 14:33:46+00:00   2021-11-12 14:37:33+00:00   0 days 00:03:47\n29  1   2021-11-12 14:37:33+00:00   2021-11-12 14:37:34+00:00   0 days 00:00:01\n30  0   2021-11-12 14:37:34+00:00   2021-11-12 14:48:20+00:00   0 days 00:10:46\n31  0   2021-11-12 14:48:20+00:00   2021-11-12 14:48:21+00:00   0 days 00:00:01\n32  0   2021-11-12 14:48:21+00:00   2021-11-12 14:52:33+00:00   0 days 00:04:12\n33  1   2021-11-12 14:52:33+00:00   2021-11-12 14:52:34+00:00   0 days 00:00:01\n34  0   2021-11-12 14:52:34+00:00   2021-11-12 14:56:04+00:00   0 days 00:03:30\n35  0   2021-11-12 14:56:04+00:00   2021-11-12 14:56:06+00:00   0 days 00:00:02\n36  0   2021-11-12 14:56:06+00:00   2021-11-12 14:57:55+00:00   0 days 00:01:49\n37  0   2021-11-12 14:57:55+00:00   2021-11-12 14:57:56+00:00   0 days 00:00:01\n38  0   2021-11-12 14:57:56+00:00   2021-11-12 15:22:33+00:00   0 days 00:24:37\n39  1   2021-11-12 15:22:33+00:00   2021-11-12 15:22:34+00:00   0 days 00:00:01\n40  0   2021-11-12 15:22:34+00:00   2021-11-12 15:23:04+00:00   0 days 00:00:30\n41  0   2021-11-12 15:23:04+00:00   2021-11-12 15:23:06+00:00   0 days 00:00:02\n42  0   2021-11-12 15:23:06+00:00   2021-11-12 15:26:04+00:00   0 days 00:02:58\n43  1   2021-11-12 15:26:04+00:00   2021-11-12 15:26:06+00:00   0 days 00:00:02\n44  0   2021-11-12 15:26:06+00:00   2021-11-12 15:26:33+00:00   0 days 00:00:27\n45  0   2021-11-12 15:26:33+00:00   2021-11-12 15:26:34+00:00   0 days 00:00:01\n46  0   2021-11-12 15:26:34+00:00   2021-11-12 15:37:32+00:00   0 days 00:10:58\n47  1   2021-11-12 15:37:32+00:00   2021-11-12 15:37:34+00:00   0 days 00:00:02\n48  0   2021-11-12 15:37:34+00:00   2021-11-12 15:41:05+00:00   0 days 00:03:31\n49  0   2021-11-12 15:41:05+00:00   2021-11-12 15:41:06+00:00   0 days 00:00:01\n50  0   2021-11-12 15:41:06+00:00   2021-11-12 15:52:32+00:00   0 days 00:11:26\n51  1   2021-11-12 15:52:32+00:00   2021-11-12 15:52:34+00:00   0 days 00:00:02\n52  0   2021-11-12 15:52:34+00:00   2021-11-12 16:03:13+00:00   0 days 00:10:39\n53  1   2021-11-12 16:03:13+00:00   2021-11-12 16:03:14+00:00   0 days 00:00:01\n54  0   2021-11-12 16:03:14+00:00   2021-11-12 16:20:04+00:00   0 days 00:16:50\n55  0   2021-11-12 16:20:04+00:00   2021-11-12 16:20:06+00:00   0 days 00:00:02\n56  0   2021-11-12 16:20:06+00:00   2021-11-12 16:22:05+00:00   0 days 00:01:59\n57  1   2021-11-12 16:22:05+00:00   2021-11-12 16:37:04+00:00   0 days 00:14:59\n58  0   2021-11-12 16:37:04+00:00   2021-11-12 16:37:06+00:00   0 days 00:00:02\n59  0   2021-11-12 16:37:06+00:00   2021-11-12 16:37:33+00:00   0 days 00:00:27\n60  1   2021-11-12 16:37:33+00:00   2021-11-12 16:37:34+00:00   0 days 00:00:01\n61  0   2021-11-12 16:37:34+00:00   2021-11-12 16:43:05+00:00   0 days 00:05:31\n62  0   2021-11-12 16:43:05+00:00   2021-11-12 16:43:06+00:00   0 days 00:00:01\n63  0   2021-11-12 16:43:06+00:00   2021-11-12 16:43:51+00:00   0 days 00:00:45\n64  0   2021-11-12 16:43:51+00:00   2021-11-12 16:43:53+00:00   0 days 00:00:02\n65  0   2021-11-12 16:43:53+00:00   2021-11-12 16:56:05+00:00   0 days 00:12:12\n66  1   2021-11-12 16:56:05+00:00   2021-11-12 16:56:06+00:00   0 days 00:00:01\n67  0   2021-11-12 16:56:06+00:00   2021-11-12 16:56:32+00:00   0 days 00:00:26\n68  0   2021-11-12 16:56:32+00:00   2021-11-12 16:56:34+00:00   0 days 00:00:02\n69  0   2021-11-12 16:56:34+00:00   2021-11-12 17:07:33+00:00   0 days 00:10:59\n70  1   2021-11-12 17:07:33+00:00   2021-11-12 17:07:34+00:00   0 days 00:00:01\n71  1   2021-11-12 17:07:34+00:00   2021-11-12 17:08:05+00:00   0 days 00:00:31\n72  0   2021-11-12 17:08:05+00:00   2021-11-12 17:08:06+00:00   0 days 00:00:01\n73  0   2021-11-12 17:08:06+00:00   2021-11-12 17:10:05+00:00   0 days 00:01:59\n74  1   2021-11-12 17:10:05+00:00   2021-11-12 17:10:06+00:00   0 days 00:00:01\n75  0   2021-11-12 17:10:06+00:00   2021-11-12 17:11:09+00:00   0 days 00:01:03\n76  0   2021-11-12 17:11:09+00:00   2021-11-12 17:11:10+00:00   0 days 00:00:01\n77  0   2021-11-12 17:11:10+00:00   2021-11-12 17:22:32+00:00   0 days 00:11:22\n78  1   2021-11-12 17:22:32+00:00   2021-11-12 17:22:33+00:00   0 days 00:00:01\n79  0   2021-11-12 17:22:33+00:00   2021-11-12 17:22:45+00:00   0 days 00:00:12\n80  1   2021-11-12 17:22:45+00:00   2021-11-12 17:24:08+00:00   0 days 00:01:23\n81  1   2021-11-12 17:24:08+00:00   2021-11-12 17:24:13+00:00   0 days 00:00:05\n82  0   2021-11-12 17:24:13+00:00   2021-11-12 17:24:14+00:00   0 days 00:00:01\n83  0   2021-11-12 17:24:14+00:00   2021-11-12 17:25:05+00:00   0 days 00:00:51\n84  1   2021-11-12 17:25:05+00:00   2021-11-12 17:25:06+00:00   0 days 00:00:01\n85  0   2021-11-12 17:25:06+00:00   2021-11-12 18:14:37+00:00   0 days 00:49:31\n86  0   2021-11-12 18:14:37+00:00   2021-11-12 18:14:38+00:00   0 days 00:00:01\n87  0   2021-11-12 18:14:38+00:00   2021-11-12 18:45:44+00:00   0 days 00:31:06\n88  0   2021-11-12 18:45:44+00:00   2021-11-12 18:45:46+00:00   0 days 00:00:02\n89  0   2021-11-12 18:45:46+00:00   NaT NaT\n`\n```\n`print('ON State: {}'.format(df.query('_value == 1')['duration'].sum()))\nprint('OFF State: {}'.format(df.query('_value == 0')['duration'].sum()))\n`\n```\n`ON State: 0 days 00:17:24\nOFF State: 0 days 05:05:50\n`\n```\nAs you can see above the sensor was ON for about 17 minutes in the last 6 hours, which is clearly wrong as you can see in the graph. Why is this the case? Can someone help me finding the issue?\nThanks a lot!",
      "solution": "I suspect you're shifting the wrong way:\n`# instead of .shift(1), .shift(-1)\n# and change name appropriately\ndf['_start'] = df._time.shift(1)\n# not really necessary to explicitly call pd.to_timedelta\n# but makes it clear what's happening\ndf['_duration'] = pd.to_timedelta(df._time - df._start)\nprint(f'ON:  {df.loc[df._value.eq(1)]._duration.sum()}')\nprint(f'OFF: {df.loc[df._value.eq(0)]._duration.sum()}')\n`\ngives\n```\n`ON:  0 days 02:34:21\nOFF: 0 days 02:48:53\n`\n```",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-11-12T20:36:15",
      "url": "https://stackoverflow.com/questions/69948238/calculating-amount-of-time-per-day-a-sensor-is-in-a-specific-state"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 69561884,
      "title": "Why won&#39;t my custom Dockerfile connect over the docker-compose network when other services will?",
      "problem": "The problem\nI am attempting to create a docker-compose file that will host three services. InfluxDB, Grafana, and a custom script in a customer Dockerfile that fills the database. I am having networking issues, and the custom script is not able to connect to the InfluxDB due to a connection refused error (shown below).\nWhat is working so far\nThe interesting thing is, that when I remove the custom script service (called ads_agent) from my docker-compose file and either run that script from the localhost or even build and run that Dockerfile in its own container, it connects just fine.\nWhat's the difference between the two\nMy script reads an environment variable called KTS_TELEMETRY_INFLUXDB_URL which is used for the InfluxDB client's URL to connect to. I can use \"http://localhost:8086\" for the URL when I run just from my command line, that works. I use my local machine's LAN IP address when I wrap the script in a Docker container because to it, localhost would be just the container. But nonetheless, this works just fine.\nFrom within my docker-compose, since all three services are on the same network, I'm using \"http://influxdb:8086\" since that host name should be bound to that service's network interface. And indeed it is, because Grafana is connecting just fine using that URL. Sadly, when I try this with the script, I get connection refused.\nThe error\n```\n`urllib3.exceptions.NewConnectionError: : Failed to establish a new connection: [Errno 111] Connection refused\n`\n```\nMy code\nThis is my docker-compose.yaml\n```\n`version: \"3\"\nservices:\n  influxdb:\n    container_name: influxdb\n    image: influxdb:2.0.9-alpine # influxdb:latest\n    networks:\n      - telemetry_network\n    ports:\n      - 8086:8086\n    volumes:\n      - influxdb-storage:/var/lib/influxdb2\n    restart: always\n    environment:\n      - DOCKER_INFLUXDB_INIT_MODE=setup\n      - DOCKER_INFLUXDB_INIT_USERNAME=$KTS_TELEMETRY_INFLUXDB_USERNAME\n      - DOCKER_INFLUXDB_INIT_PASSWORD=$KTS_TELEMETRY_INFLUXDB_PASSWORD\n      - DOCKER_INFLUXDB_INIT_ORG=$KTS_TELEMETRY_INFLUXDB_ORG\n      - DOCKER_INFLUXDB_INIT_BUCKET=$KTS_TELEMETRY_INFLUXDB_BUCKET\n      - DOCKER_INFLUXDB_INIT_RETENTION=$KTS_TELEMETRY_INFLUXDB_RETENTION\n      - DOCKER_INFLUXDB_INIT_ADMIN_TOKEN=$KTS_TELEMETRY_INFLUXDB_TOKEN\n  grafana:\n    container_name: grafana\n    image: grafana/grafana:8.1.7 # grafana/grafana:latest\n    networks:\n      - telemetry_network\n    ports:\n      - 3000:3000\n    volumes:\n      - grafana-storage:/var/lib/grafana\n    restart: always\n    depends_on:\n      - influxdb\n  ads_agent:\n    container_name: ads_agent\n    build: ./ads_agent\n    networks:\n      - telemetry_network\n    restart: always\n    depends_on:\n      - influxdb\n    environment:\n      - KTS_TELEMETRY_INFLUXDB_URL=http://influxdb:8086\n      - KTS_TELEMETRY_INFLUXDB_TOKEN=$KTS_TELEMETRY_INFLUXDB_TOKEN\n      - KTS_TELEMETRY_INFLUXDB_ORG=$KTS_TELEMETRY_INFLUXDB_ORG\n      - KTS_TELEMETRY_INFLUXDB_BUCKET=$KTS_TELEMETRY_INFLUXDB_BUCKET\n\nnetworks:\n  telemetry_network:\n\nvolumes:\n  influxdb-storage:\n  grafana-storage:\n`\n```\nThis is my ads_agent/Dockerfile\n```\n`FROM python:3.9\nCOPY requirements.txt .\nRUN pip install --upgrade pip\nRUN pip install -r /requirements.txt\nCOPY main.py .\nENTRYPOINT /usr/local/bin/python3 /main.py\n`\n```\nads_agent/requirements.txt just has the influxdb-client and this is my ads/main.py\n```\n`import os\nfrom influxdb_client import InfluxDBClient, Point, WritePrecision\nfrom influxdb_client.client.write_api import SYNCHRONOUS\nfrom datetime import datetime\nimport random\nimport time\n\ntoken = os.environ[\"KTS_TELEMETRY_INFLUXDB_TOKEN\"]\norg = os.environ[\"KTS_TELEMETRY_INFLUXDB_ORG\"]\nbucket = os.environ[\"KTS_TELEMETRY_INFLUXDB_BUCKET\"]\nurl = os.environ[\"KTS_TELEMETRY_INFLUXDB_URL\"]\n\nclient = InfluxDBClient(url=url, token=token)\ndbh = client.write_api(write_options=SYNCHRONOUS)\n\nwhile True:\n    symbol_name = 'rand_num'\n    value = random.random()\n    timestamp = datetime.utcnow()\n    print(timestamp, symbol_name, value)\n    point = Point(\"mem\") \\\n        .field(symbol_name, value) \\\n        .time(timestamp, WritePrecision.NS)\n    dbh.write(bucket, org, point)\n    time.sleep(1)\n`\n```",
      "solution": "Your problem not related to `network connectivity`, just related to `startup order`. Although you define `depends_on - influxdb` for `ads_agent`, still will have chance\nthat when your script try to connect the influxdb, the influx db still not finish up.\nThis is why you can success if you do it manually, as there is time delay there for your manual operation, at that time, the db already ready.\nReason see this:\n\n`depends_on` does not wait for db and redis to be \u201cready\u201d before starting web - only until they have been started. If you need to wait for a service to be ready.\n)\n\nTo assure your db really up before your script starts, you need to refers to Control startup and shutdown order in Compose:\n\nTo handle this, design your application to attempt to re-establish a connection to the database after a failure. If the application retries the connection, it can eventually connect to the database.\nThe best solution is to perform this check in your application code, both at startup and whenever a connection is lost for any reason. However, if you don\u2019t need this level of resilience, you can work around the problem with a wrapper script:\n\nUse a tool such as wait-for-it, dockerize, sh-compatible wait-for, or RelayAndContainers template. These are small wrapper scripts which you can include in your application\u2019s image to poll a given host and port until it\u2019s accepting TCP connections.\nFor example, to use wait-for-it.sh or wait-for to wrap your service\u2019s command:\n```\n`version: \"2\"\nservices:\n  web:\n    build: .\n    ports:\n      - \"80:8000\"\n    depends_on:\n      - \"db\"\n    command: [\"./wait-for-it.sh\", \"db:5432\", \"--\", \"python\", \"app.py\"]\n  db:\n    image: postgres\n`\n```\n\nAlternatively, write your own wrapper script to perform a more application-specific health check.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-10-13T22:24:11",
      "url": "https://stackoverflow.com/questions/69561884/why-wont-my-custom-dockerfile-connect-over-the-docker-compose-network-when-othe"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 69536469,
      "title": "How to use a database in influx with a dot in its name?",
      "problem": "When I try to reach my database, 'EUNL.F' there only comes back an error.\n```\n`use exampledb\n`\n```\nWorks fine\nbut\n```\n`use EUNL.F\n`\n```\nreturns: EUNL doesn't exist.\nHow do I include the dot to the name?",
      "solution": "For influxdb, you should escape `dot` character with `\\`\n```\n`use EUNL\\.F\nuse \"EUNL\\.F\"\n`\n```",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-10-12T09:15:01",
      "url": "https://stackoverflow.com/questions/69536469/how-to-use-a-database-in-influx-with-a-dot-in-its-name"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb",
      "question_id": 67712041,
      "title": "InfluxDB 2.0 unexpected token trying to group by time",
      "problem": "So I have data in a bucket in InfluxDB 2.0\nI'm in the \"Data Explorer\" and I'm trying to write a query to see the data that I have inserted.\nI'm trying a simple query, but I can't seem to make it work, even if I took an example from the documentation.\n```\n`SELECT COUNT(driverNo) FROM \"csvDataTest\" WHERE time >= \"2021-01-18T00:06:00Z\" AND time It keeps saying\n```\n`unexpected token for property key: DURATION (12m)\n`\n```\nAll examples I've seen in the documentation group by time this way.\nWhat am I doing wrong? Can anyone help me?",
      "solution": "Oh, I think I have figured it out. I think I was looking at the documentation from version 1.7 instead of version 2.0\nIt works when I try the syntax from the 2.0 documentation.\nhttps://docs.influxdata.com/influxdb/v2.0/query-data/flux/group-data/\nThe query looks something like this instead (I removed the count, still working on query syntax but the group by clause works now).\n```\n`from(bucket: \"csvDataTest\")\n|> range(start: 1900-01-10T00:00:00Z, stop: 2021-04-23T00:00:00Z)\n|> filter(fn: (r) => r._measurement == \"channels\")\n|> filter(fn: (r) => r.driverNo == \"12345\")\n|> group(columns: [\"_time\"])\n`\n```",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-05-26T22:08:14",
      "url": "https://stackoverflow.com/questions/67712041/influxdb-2-0-unexpected-token-trying-to-group-by-time"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb-2",
      "question_id": 69664307,
      "title": "Python InfluxDB2 - write_api.write(...) How to check for success?",
      "problem": "I need to write historic data into InfluxDB (I'm using Python, which is not a must in this case, so I maybe willing to accept non-Python solutions). I set up the write API like this\n```\n`write_api = client.write_api(write_options=ASYNCHRONOUS)\n`\n```\nThe Data comes from a DataFrame with a timestamp as key, so I write it to the database like this\n```\n`result = write_api.write(bucket=bucket, data_frame_measurement_name=field_key, record=a_data_frame)\n`\n```\nThis call does not throw an exception, even if the InfluxDB server is down. `result` has a protected attribute `_success` that is a boolean in debugging, but I cannot access it from the code.\nHow do I check if the write was a success?",
      "solution": "`write_api.write()` returns a `multiprocessing.pool.AsyncResult` or `multiprocessing.pool.AsyncResult` (both are the same).\nWith this return object you can check on the asynchronous request in a couple of ways. See here: https://docs.python.org/2/library/multiprocessing.html#multiprocessing.pool.AsyncResult\nIf you can use a blocking request, then `write_api = client.write_api(write_options=SYNCRONOUS)` can be used.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-10-21T16:59:32",
      "url": "https://stackoverflow.com/questions/69664307/python-influxdb2-write-api-write-how-to-check-for-success"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "influxdb-2",
      "question_id": 69203698,
      "title": "InfluxDB V2 OSS Alerts : capturing the metric value in notification message",
      "problem": "We are monitoring a bunch of IoT Devices using TIG stack and are trying to configure alerts using InfluxDB V2's inbuilt Alerts feature. We have configured the slack notification channel and we would like to capture the metric value as part of the message. Here is a sample status message template:\n```\n`The memory utilization is *${ r._level }* \nOn Device Name: *${ r.host }*\nPolled At: *${ r._time}*\n`\n```\nThis shows the alert like this:\n```\n`Memory Alert  The memory utilization is crit\nOn Device Name: Device-dev-02\nPolled At: 2021-09-16T06:41:15.000000000Z\n`\n```\nThe actual value is in the field `_value`, when I try to capture this as part of the message\n```\n`The memory utilization is *${ r._level }* \nOn Device Name: *${ r.host }*\nPolled At: *${ r._time}*\nCurrent Memory: *${ r._value}*\n`\n```\nThe notification check fails. I read in the documentation that Flux only interpolates string values, I tried to convert the _value to string\n```\n`The memory utilization is *${ r._level }* \nOn Device Name: *${ r.host }*\nPolled At: *${ r._time}*\nCurrent Memory: *${string(v: r._value)}*\n`\n```\nStill the notification check fails.\nAre we missing something?",
      "solution": "I was able to get this question answered, thanks to Jay from Influxdata community. Posting the answer here, hopefully someone might find it useful.\nWhen creating a check the _value field is automatically pivoted. In short this means instead of using the `_value` field you should use the `_fieldname`.\nFor example:\n```\n`Check: ${ r._check_name } is: ${ r._level } ${string(v: r.used_percent)} \n`\n```\nWhere by `used_percent` was the _field name.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-09-16T08:57:18",
      "url": "https://stackoverflow.com/questions/69203698/influxdb-v2-oss-alerts-capturing-the-metric-value-in-notification-message"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "telegraf",
      "question_id": 69053003,
      "title": "telegraf and script command to send to grafana",
      "problem": "I have the following inputs in my telegraf.conf that go to Grafana. I can get the simpler first 3 examples to work but cannot get the 4th to work.\n```\n`[[inputs.exec]]\ncommands = [\"sh -c 'grep -i DatasetVersion /etc/aaa/systemnameX/configfile | cut -d'=' -f2'\"]\nname_override = \"systemnameX\"\ntimeout = \"5s\"\ndata_format = \"value\"\ndata_type = \"string\"\n[inputs.exec.tags]\ntype = \"dataset_version\"\n\n[[inputs.exec]]\ncommands = [\"sh -c 'cat /root/config/OS_Version.txt | tr -d C'\"]\nname_override = \"systemnameX\"\ntimeout = \"5s\"\ndata_format = \"value\"\ndata_type = \"string\"\n[inputs.exec.tags]\ntype = \"os_version\"\n\n[[inputs.exec]]\ncommands = [\"sh -c 'grep -i VERSION= /usr/bin/sw_install | head -1 | cut -d'=' -f2 '\"]\nname_override = \"systemnameX\"\ntimeout = \"5s\"\ndata_format = \"value\"\ndata_type = \"string\"\n[inputs.exec.tags]\ntype = \"sw_version\"\n\n[[inputs.exec]]\ncommands = [\"/sbin/ifconfig eth0 | grep 'inet' | cut -d: -f2 | awk '{print $2}'\"]\nname_override = \"systemnameX\"\ntimeout = \"5s\"\ndata_format = \"value\"\ndata_type = \"string\"\n[inputs.exec.tags]\ntype = \"ip_address\"\n`\n```\nThis is the error when I run the telegraf test\n```\n`2021-09-04T06:36:51Z E! Error in plugin [inputs.exec]: exec: exit status 1 for command '/sbin/ifconfig eth0 | grep 'inet' | cut -d: -f2 | awk '{print $2}''\n`\n```\nand when I put the exact command into a command line I get the following error:\n```\n`awk: cmd. line:1: {print\nawk: cmd. line:1:       ^ unexpected newline or end of string\n`\n```\nAny help about how to fix the awk part would be greatly appreciated.\ncheers,\nTara",
      "solution": "I have tried running different permutations and combinations of your `/sbin/ifconfig` command but its giving me also same error. But I have read the telegraf manual and come up with following approach/steps.\nI have tested this in Linux with Telegraf's `1.19.3` version and it worked fine for me.\nSteps:\n\nFirst thing first, don't waste your time(until someone posts an answer here), when we can create a script from your used `commands` in `inputs.exec` module of telegraf.\nSo in spite of running direct command in telegraf conf file create a script(eg: `script.bash` in my tested case) and place your command there like as follows:\n\n```\n`cat /etc/telegraf/script.bash\n#!/bin/bash\n/sbin/ifconfig eth0 | grep 'inet' | cut -d: -f2 | awk '{print $2}'\n`\n```\n\nOR you can also change your `/sbin/ifconfig` command to following in a single `awk` command:\n```\n`/sbin/ifconfig eth0 | awk '/inet/{print $2}'\n`\n```\n\nNow make following entry in your conf file:\n\n```\n`[[inputs.exec]]\ncommands = [\"/etc/telegraf/script.bash\" ]\nname_override = \"systemnameX\"\ntimeout = \"5s\"\ndata_format = \"value\"\ndata_type = \"string\"\n[inputs.exec.tags]\ntype = \"ip_address\"\n`\n```\n\nRestart your telegraf services once and test your status if your telegraf is operating fine or not by following command:\n\n```\n`sudo -u telegraf telegraf -test -config /etc/telegraf/telegraf.conf\n`\n```\nNOTE: I have created test script in `/etc/telegraf/script.bash` you can create it wherever you want to, but make sure you are giving absolute complete and correct path in conf file of telegraf.",
      "question_score": 4,
      "answer_score": 3,
      "created_at": "2021-09-04T08:41:26",
      "url": "https://stackoverflow.com/questions/69053003/telegraf-and-script-command-to-send-to-grafana"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "telegraf",
      "question_id": 79114536,
      "title": "AWK or alternative way to join (and occasionally correct) two fields in csv",
      "problem": "A little while I ago I started writing something to parse some application logs and thought everything was well but today I noticed some data missing and it's because of a timestamp issue. The log structure is like this:\n```\n`f_timestamp,f_timestamp2,f_date\n1729448207,303701614,2024/10/20 19:16:47 303701614\n1729415974,96090458,2024/10/20 19:16:47 096090458\n`\n```\nI need nano second precision, what I was doing was crudely gluing `f_stampstamp` and `f_timestamp2` together\n```\n`awk -F ',' '{print $0 FS $1$2}' filein.csv > fileout.csv\n`\n```\nProblematic result:\n```\n`f_timestampf_timestamp2\n1729448207303701614\n172941597496090458\n`\n```\nProblem is that `f_timestamp2` is sometimes 8 digits - the leading 0 means my dates are months out\nExpected result is\n```\n`f_timestampf_timestamp2\n1729448207303701614\n1729415974096090458\n`\n```\nThere are two ways I know of out of this problem\n\nFind a way to add the leading 0 when necessary i.e. `f_timestamp2` \nConvert `f_date` to `YYYY-MM-DD hh:mm:ss.sssssssss` format since this is what my database expects for timestamps and forget about the unix timestamps. The format of `f_date` differs in later versions but I can work around that.",
      "solution": "Append new column (`f_timestamp` + 0-padded `f_timestamp2`)\nOne `awk` idea using `sprintf(\"%09d\", ...)` to left pad `f_timestamp2 / $2` with zero's:\n```\n`awk '\nBEGIN { FS = OFS = \",\" }\n      { print $0, $1 (NR==1 ? $2 : sprintf(\"%09d\", $2)) }\n' filein.csv\n`\n```\nAnother `awk` idea using a dual input delimiter (command and space) so we can make use of the last field which is already 0-padded:\n```\n`awk '\nBEGIN { FS = \"[, ]\"; OFS = \",\" }\n      { print $0, $1 (NR==1 ? $2 : $NF) }\n' filein.csv\n`\n```\nBoth of these generate:\n```\n`f_timestamp,f_timestamp2,f_date,f_timestampf_date\n1729448207,303701614,2024/10/20 19:16:47 303701614,1729448207303701614\n1729415974,96090458,2024/10/20 19:16:47 096090458,1729415974096090458\n`\n```\n\nReformat `f_date` to `YYYY-MM-DD hh:mm:ss.sssssssss`\nOne idea using `sed` to make the edits:\n```\n`$ sed 's#/#-#g; s# #.#2' filein.csv\n`\n```\nWhere:\n\nsince our data includes the forward slash (`/`) we'll use `#` as our `sed` script delimiter\n`s#/#-#g` - replace all forward slashes (`/`) with hyphens (`-`)\n`s# #.#2` - replace 2nd occurrence of a space with a period\n\nThis generates:\n```\n`f_timestamp,f_timestamp2,f_date\n1729448207,303701614,2024-10-20 19:16:47.303701614\n1729415974,96090458,2024-10-20 19:16:47.096090458\n`\n```",
      "question_score": 3,
      "answer_score": 2,
      "created_at": "2024-10-22T16:25:27",
      "url": "https://stackoverflow.com/questions/79114536/awk-or-alternative-way-to-join-and-occasionally-correct-two-fields-in-csv"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "telegraf",
      "question_id": 65702577,
      "title": "Telegraf connection to Mosquitto using TLS",
      "problem": "In my system (with raspberry) I have some sensors that publish data to Mosquitto, I'm using Telegraf to transfer the data do an influxDB database and I'm using Grafana to show the data.\nDuring the test without TLS connection (in mosquittos) everything works correctly but when I activated the TLS I start to have a problem with Telegraf.\nThe sensor are sending the data to the broker using the client.key, client.crt and ca.crt.\nIn the broker I can see the data from the sensor. So I think the problem in not in this.\nIn telegraf (I suppose it works as client) I tried to configure the TLS connection.\nLooking at the telegraf.service status , it is active and running. Looking at the journal I don't see errors in the connection but I can't see any data from the broker.\nIn Telegraf.conf I set the certificate as you can see here below. Instead using pem file I used the file that I use for the sensor or other client connected to the system: the extension is different and I don't know if the problem is here.\nHere the configuration of Telegraf (mqtt_consumer)\n```\n`# # Read metrics from MQTT topic(s)\n [[inputs.mqtt_consumer]]\n#   ## Broker URLs for the MQTT server or cluster.  To connect to multiple\n#   ## clusters or standalone servers, use a seperate plugin instance.\n#   ##   example: servers = [\"tcp://localhost:1883\"]\n#   ##            servers = [\"ssl://localhost:1883\"]\n#   ##            servers = [\"ws://localhost:1883\"]\n   servers = [\"tcp://192.168.1.58:8883\"]\n#\n#   ## Topics that will be subscribed to.\n   topics = [\n     \"sensors/#\"\n   ]\n#\n#   ## The message topic will be stored in a tag specified by this value.  If set\n#   ## to the empty string no topic tag will be created.\n#   # topic_tag = \"topic\"\n#\n#   ## QoS policy for messages\n#   ##   0 = at most once\n#   ##   1 = at least once\n#   ##   2 = exactly once\n#   ##\n#   ## When using a QoS of 1 or 2, you should enable persistent_session to allow\n#   ## resuming unacknowledged messages.\n#   # qos = 0\n#\n#   ## Connection timeout for initial connection in seconds\n#   # connection_timeout = \"30s\"\n#\n#   ## Maximum messages to read from the broker that have not been written by an\n#   ## output.  For best throughput set based on the number of metrics within\n#   ## each message and the size of the output's metric_batch_size.\n#   ##\n#   ## For example, if each message from the queue contains 10 metrics and the\n#   ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n#   ## full batch is collected and the write is triggered immediately without\n#   ## waiting until the next flush_interval.\n#   # max_undelivered_messages = 1000\n#\n#   ## Persistent session disables clearing of the client session on connection.\n#   ## In order for this option to work you must also set client_id to identify\n#   ## the client.  To receive messages that arrived while the client is offline,\n#   ## also set the qos option to 1 or 2 and don't forget to also set the QoS when\n#   ## publishing.\n#   # persistent_session = false\n#\n#   ## If unset, a random client ID will be generated.\n    client_id = \"\"\n#\n#   ## Username and password to connect MQTT server.\n    #username = \"\"\n    #password = \"\"\n#\n#   ## Optional TLS Config\n    tls_ca   = \"/etc/telegraf/ca.crt\"\n    tls_cert = \"/etc/telegraf/client.crt\"\n    tls_key  = \"/etc/telegraf/client.key\"\n#   ## Use TLS but skip chain & host verification\n#    insecure_skip_verify = false\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n    data_format = \"influx\"\n`\n```\nHow can I check the connection to the broker in Telegraf? Is it correct the configuration or I should use only .pem file?",
      "solution": "Your MQTT URL starts with `tcp://` but it should start with `ssl://` for a MQTT over SSL connection.",
      "question_score": 2,
      "answer_score": 6,
      "created_at": "2021-01-13T14:06:09",
      "url": "https://stackoverflow.com/questions/65702577/telegraf-connection-to-mosquitto-using-tls"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "telegraf",
      "question_id": 71396884,
      "title": "Error on Telegraf Helm Chart update: Error parsing data",
      "problem": "Im trying to deploy telegraf helm chart on kubernetes.\n\nhelm upgrade --install telegraf-instance -f values.yaml influxdata/telegraf\n\nWhen I add modbus input plugin with holding_register i get error\n\n[telegraf] Error running agent: Error loading config file /etc/telegraf/telegraf.conf: Error parsing data: line 49: key `name\u2019 is in conflict with line 2fd\n\nmy values.yaml like below\n```\n`## Default values.yaml for Telegraf\n## This is a YAML-formatted file.\n## ref: https://hub.docker.com/r/library/telegraf/tags/\n\nreplicaCount: 1\n\nimage:\n  repo: \"telegraf\"\n  tag: \"1.21.4\"\n  pullPolicy: IfNotPresent\n\npodAnnotations: {}\n\npodLabels: {}\n\nimagePullSecrets: []\n\nargs: []\n\nenv:\n  - name: HOSTNAME\n    value: \"telegraf-polling-service\"\n\nresources: {}\n\nnodeSelector: {}\n\naffinity: {}\n\ntolerations: []\n\nservice:\n  enabled: true\n  type: ClusterIP\n  annotations: {}\n\nrbac:\n  create: true\n  clusterWide: false\n  rules: []\n\nserviceAccount:\n  create: false\n  name:\n  annotations: {}\n\nconfig:\n  agent:\n    interval: 60s\n    round_interval: true\n    metric_batch_size: 1000000\n    metric_buffer_limit: 100000000\n    collection_jitter: 0s\n    flush_interval: 60s\n    flush_jitter: 0s\n    precision: ''\n    hostname: '9825128'\n    omit_hostname: false\n  processors:\n    - enum:\n        mapping:\n          field: \"status\"\n          dest: \"status_code\"\n          value_mappings:\n            healthy: 1\n            problem: 2\n            critical: 3\n  inputs:\n    - modbus:\n        name: \"PS MAIN ENGINE\"\n        controller: 'tcp://192.168.0.101:502'\n        slave_id: 1\n        holding_registers: \n          - name: \"Coolant Level\"\n            byte_order: CDAB\n            data_type: FLOAT32\n            scale: 0.001\n            address: [51410, 51411]\n    - modbus:\n        name: \"SB MAIN ENGINE\"\n        controller: 'tcp://192.168.0.102:502'\n        slave_id: 1\n        holding_registers: \n          - name: \"Coolant Level\"\n            byte_order: CDAB\n            data_type: FLOAT32\n            scale: 0.001\n            address: [51410, 51411]\n  outputs:\n    - influxdb_v2:\n        token: token\n        organization: organisation\n        bucket: bucket\n        urls:\n          - \"url\"\n\nmetrics:\n  health:\n    enabled: true\n    service_address: \"http://:8888\"\n    threshold: 5000.0\n  internal:\n    enabled: true\n    collect_memstats: false\n\npdb:\n  create: true\n  minAvailable: 1\n`\n```",
      "solution": "Problem resolved by doing the following steps\n\ndeleted config section of my values.yaml\nadded my telegraf.conf to /additional_config path\nadded configmap to kubernetes with the following command\n\n```\n`  kubectl create configmap external-config --from-file=/additional_config\n`\n```\n\nadded the following command to values.yaml\n\n```\n`    volumes:\n      - name: my-config\n        configMap:\n          name: external-config\n    volumeMounts:\n      - name: my-config\n        mountPath: /additional_config\n    args:\n      - \"--config=/etc/telegraf/telegraf.conf\"\n      - \"--config-directory=/additional_config\"\n`\n```",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2022-03-08T15:45:34",
      "url": "https://stackoverflow.com/questions/71396884/error-on-telegraf-helm-chart-update-error-parsing-data"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "telegraf",
      "question_id": 68021560,
      "title": "How i can request the user&#39;s location in inline keyboard within telegram?",
      "problem": "hi everyone I'm kinda new in telegram bot coding and I'm building a Geodata collector bot. I want to retrieve the location of the user in a button using the inline keyboard. there is just request_location param that can be used in the keyboard, not in the inline keyboard. I want also to use a callback for one of my buttons to gets back to the previous menu and I wasn't able to find a correct callback query for the keyboard. can sb help me that how I would be able to request the location in an inline keyboard?\n//my buttons for acquiring the location\n```\n`bot.action(\"POS-PCdF\", (ctx) => {\n  ctx.deleteMessage();\n  console.log(ctx.from);\n  bot.telegram.sendMessage(ctx.chat.id, \"Can we access your location?\", {\n    reply_markup: {\n      keyboard: [\n        [\n          {\n            text: \"Access my location\",\n            request_location: true,\n            callback_data: \"Access_my_loc\",\n          },\n          { text: \"Cancel\", callback_data: \"PCdF\" },\n        ],\n      ],\n    },\n  });\n});\n`\n```\n//the menu that I want to use in my callback\n```\n`bot.action(\"PCdF\", (ctx) => {\n  ctx.deleteMessage();\n  ctx.telegram.sendMessage(\n    ctx.chat.id,\n    \"Share with us the problems about *******!\",\n    {\n      reply_markup: {\n        inline_keyboard: [\n          [\n            { text: \"Pictures\", callback_data: \"PIC-PCdF\" },\n            { text: \"Location\", callback_data: \"POS-PCdF\" },\n          ],\n          [\n            { text: \"write to us\", callback_data: \"TEXT-PCdF\" },\n            { text: \"Go back Home\", callback_data: \"go-back\" },\n          ],\n        ],\n      },\n    }\n  );\n});\n`\n```\nthanks in advance.",
      "solution": "As explained in the Official Telegram Bot API, `InlineKeyboardMarkupButton` should contain exactly one of the following optional feilds: `callback_data`, `login_url`, `switch_inline_query`, `switch_inline_query_current_chat`, `callback_game`, `pay`\nThis means that an inline keyboard cannot have a button requesting for user's location AKA `request_location`.\nAlso, there's no built in button or solution for returning to the last keyboard (whether it's a `ReplyKeyboard` or a `InlineKeyboard`). You have to implement a return button with a proper `text` (`callback_data` if it's an `InlineKeyboardButton`) specifically declared so that your code can recognize it and return to the last keyboard.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-06-17T16:40:40",
      "url": "https://stackoverflow.com/questions/68021560/how-i-can-request-the-users-location-in-inline-keyboard-within-telegram"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "telegraf",
      "question_id": 70845713,
      "title": "Telegram Bot how to send HTML file directly?",
      "problem": "I created a game using HTML, now I am trying to import it in my js file, and send to user.\nThis is my code so far\n```\n`const { Telegraf } = require('telegraf')\nconst bot = new Telegraf(process.env.BOT_TOKEN);\nconst test = require('./index.html');\n\nbot.hears('?', ctx => {\n//   console.log(ctx.from)\n  let msg = `Spin the wheel`;\n  ctx.deleteMessage();\n  bot.telegram.sendMessage(ctx.chat.id, msg, {\n      reply_markup: {\n          inline_keyboard: [\n              [{\n                      text: \"SPIN\",\n                      callback_data: 'wheeloffortune'\n                  }\n              ],\n          ]\n      }\n  })\n})\n\nbot.action('wheeloffortune', ctx => {\n    bot.telegram.sendMessage(ctx.chat.id, test, {parse_mode: 'HTML'});\n})\n\n`\n```\nbut I am getting this error\n```\n`\n^\n\nSyntaxError: Unexpected token 'Any advice is appreciated",
      "solution": "As mentioned on Telegram BOT API's documentation, `parse_mode` of type `html` only supports a few tags. Therefore, you can't send your game's html file to the chat for the user with `parse_mode` set to `html`.\nHere's what the docs say:\nTo use this mode, pass HTML in the parse_mode field. The following tags are currently supported:\n```\n`bold, bold\nitalic, italic\nunderline, underline\nstrikethrough, strikethrough, strikethrough\nspoiler, spoiler\nbold italic bold italic bold strikethrough italic bold strikethrough spoiler underline italic bold bold\ninline URL\ninline mention of a user\ninline fixed-width code`\npre-formatted fixed-width code block\n```\n```\npre-formatted fixed-width code block written in the Python programming language\n```\n\nOnly the tags mentioned above are currently supported.\nAll  and & symbols that are not a part of a tag or an HTML entity must be replaced with the corresponding HTML entities ( with > and & with &).\nAll numerical HTML entities are supported.\nThe API currently supports only the following named HTML entities: , & and \".\nUse nested pre and code tags, to define programming language for pre entity.\nProgramming language can't be specified for standalone code tags.",
      "question_score": 1,
      "answer_score": 4,
      "created_at": "2022-01-25T09:46:26",
      "url": "https://stackoverflow.com/questions/70845713/telegram-bot-how-to-send-html-file-directly"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "telegraf",
      "question_id": 78281823,
      "title": "Why bot.reaction() by Telegraf.js for Telegram Bot doesn&#39;t work?",
      "problem": "Trying to develop a telegram bot with Telegraf.js when I try to listen message reactions the code doesn't work.\nThe code appears the same as Telegraf 4.16.0 Release Notes - Working with Reactions\nMy demo code:\n```\n`const {Telegraf} = require('telegraf');\n\nconst dotenv = require('dotenv');\n\n// Load DOTENV\ndotenv.config();\n// Config\nconst BOT_TOKEN = process.env.BOT_TOKEN || '';\n// Init bot\nconst bot = new Telegraf(BOT_TOKEN);\n\nbot.reaction(\"\ud83d\udc4d\", (ctx) => {\n    // user added a \ud83d\udc4d reaction\n    console.log(\"added\")\n});\n\nbot.launch();\nconsole.log(`Launch local with token: ${BOT_TOKEN}`)\n`\n```\npackage.json - dependencies\n```\n`\"dependencies\": {\n    \"dayjs\": \"^1.11.7\",\n    \"dotenv\": \"^16.0.3\",\n    \"mongodb\": \"^4.13.0\",\n    \"rss-parser\": \"^3.12.0\",\n    \"telegraf\": \"^4.16.3\"\n  }\n`\n```\nI have tried with specific version 4.16.0 and other ways like `bot.on('message_reaction')` but anything happened\nThanks for the help.",
      "solution": "Was having same problem here, turns out that reactions are not send by Telegram by default, you have to enable it.\n`// enable updates from reactions\nbot.launch({ allowedUpdates: [ \"message\", \"message_reaction\" ] });\n`",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2024-04-05T20:38:51",
      "url": "https://stackoverflow.com/questions/78281823/why-bot-reaction-by-telegraf-js-for-telegram-bot-doesnt-work"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "telegraf",
      "question_id": 68458508,
      "title": "Callback for user&#39;s responses in a telegram bot (Telegraf)",
      "problem": "I'm coding a telegram bot with Telegraf on node.js and I need to get some information about users. For example, the bot is asking them where they are from. Then, I want to wait for the user's response and store it in my db. However, I don't understand how to achieve that since I can't just use a `bot.on('text', ...)` because I'm asking multiple questions and need to differenciate responses afterwards. I found out that there are callbacks but all that I found is for buttons and keyboards. Thanks!",
      "solution": "Oh, I found the solution, you need to use scenes to achieve that\nhttps://github.com/telegraf/telegraf/issues/705",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2021-07-20T18:42:07",
      "url": "https://stackoverflow.com/questions/68458508/callback-for-users-responses-in-a-telegram-bot-telegraf"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "telegraf",
      "question_id": 72964916,
      "title": "How to change the interval of a plugin in telegraf?",
      "problem": "Using: `telegraf version 1.23.1`\nThats the workflow `Telegraf => Influx => Grafana`.\nI am using telegraf to check my metrics on a shared server. So far so good, i already could initalize the Telegraf `uWSGI` Plugin and display the data of my running django projects in grafana.\n\nProblem\nNow i wanted to check some folder size too with the `[[inputs.filecount]]` Telegraf Plugin and this works also well. However i do not need Metrics for every `10s` for this plugin. So i change the interval like mentioned in the Documentation in the `[[inputs.filecount]]` Plugin.\n`telegraf.conf`\n```\n`[agent] \n  interval = \"10s\"\n  round_interval = true\n  metric_batch_size = 1000\n  metric_buffer_limit = 10000\n  collection_jitter = \"5s\"\n  flush_interval = \"10s\"\n  flush_jitter = \"0s\"\n\n#... PLUGIN\n[[inputs.filecount]]\n  # set different interval for this input plugin every 10min\n  interval=\u201c600s\u201d\n  collection_jitter=\u201c20s\u201d\n  # Default from Doc =>\n  directories = [\"/home/myserver/logs\", \"/home/someName/growingData, ]\n  name = \"*\"\n  recursive = true\n  regular_only = false\n  follow_symlinks = false\n  size = \"0B\" \n  mtime = \"0s\"\n`\n```\nAfter restarting Telegram with Supervisor it crashed because it could not parse the new lines.\n`supervisor.log`\n```\n` Error running agent: Error loading config file /home/user/etc/telegraf/telegraf.conf: Error parsing data: line 208: invalid TOML syntax\n`\n```\nSo that are these lines i added because i thought that is how the Doc it mention it.\n`telegraf.conf`\n```\n`  # set different interval for this input plugin every 10min\n  interval=\u201c600s\u201d\n  collection_jitter=\u201c20s\u201d\n`\n```\n\nQuestion\nSo my question is. How can i change or setup the interval for a single input plugin in telegraf?\nOr do i have to apply a different TOML syntax like `[[inputs.filecount.agent]]` or so?\nI assume that i do not have to change any output interval also? Because i assume even though its currently 10s, if this input plugin only pulls/inputs data every 600s it should not matter, some flush cycle will push the Data to influx .",
      "solution": "How can i change or setup the interval for a single input plugin in telegraf?\n\nAs the link you pointed to shows, individual inputs can set the `interval` and `collection_jitter` options. There is no difference in the TOML syntax for example I can do the following for the memory input plugin:\n`[[inputs.mem]]\n  interval=\"600s\"\n  collection_jitter=\"20s\"\n`\n\nI assume that i do not have to change any output interval also?\n\nCorrect, these are independent of each other.\n\nline 208: invalid TOML syntax\n\nKnowing what exactly is on line 208 and around that line will hopefully resolve your issue and get you going again. Also make sure your quotes that you used are correct. Sometimes when people copy and paste quotes they get `\u201d` vs `\"` which can cause issues!",
      "question_score": 1,
      "answer_score": 2,
      "created_at": "2022-07-13T12:39:49",
      "url": "https://stackoverflow.com/questions/72964916/how-to-change-the-interval-of-a-plugin-in-telegraf"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "telegraf",
      "question_id": 73050860,
      "title": "Telegraf.js bot create a user link without mentioning a user",
      "problem": "I am trying to send message via bot that contains a link to user profile, but problem is, that this mesage makes an annoying notification for user, with an '@' icon in chat group, is there a way to avoid that?\n```\n`        const msg = data?.sort((a, b) => b.rating - a.rating)\n      .reduce((acc, item, index) => {\n        if (index > 14) return acc;\n\n        acc += `\ud83c\udf1f ${item.user_name} - ${item.rating}\\n`;\n  \n        return acc;\n      }, '');\n    ctx.replyWithHTML(`Top 15 gamers:\n${msg}`, { disable_notification: true }).catch((err) => console.error(err));\n`\n```\nthat icon",
      "solution": "```\n`\"\u2705  {$name}\"\n`\n```\nfigured it out 1 year later",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2022-07-20T13:30:34",
      "url": "https://stackoverflow.com/questions/73050860/telegraf-js-bot-create-a-user-link-without-mentioning-a-user"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "telegraf",
      "question_id": 70810080,
      "title": "chat_member not getting invocked",
      "problem": "I am using `TelegrafJS` and `nodejs` for developing a `telegram` bot and I want to use \"chat_member\" because it returns the invite link used by the user to join the telegram group. but the problem is it is not getting invoked. when new members are joined. my primary goal is to get the invite link used by the member to join the telegram group.\n```\n`const { Router, Markup, Telegraf } = require('telegraf');\n\n//require .ENV\nconst path = require('path');\nrequire('dotenv').config({ path: path.resolve(__dirname + './../.env') });\n\n//creating Bot\nconst bot = new Telegraf(process.env.TELEGRAM_TOKEN);\n\nbot.on(\"chat_member\", async (ctx) => {\n    console.log(ctx);\n})\n\nbot.on(\"group_chat_created\", async (ctx) => {\n    console.log(\"group_chat_created\")\n})\n\n//launching bot\nbot.launch()\n    .then(() => {\n        console.log(\"Bot Running\");\n    })\n    .catch((err) => {\n        console.log(`Error Running Bot: ${err}`);\n    })\n\n`\n```\nAll other are working like \"group_chat_created\",\"new_chat_members\",\"left_chat_member\".\nThe documentation says I need to add it to allowed_updates but how to add it.",
      "solution": "I got the solution the thing I needed to add was the launch options in the bot.launch method\n```\n`\nvar options2 =\n{\n    allowedUpdates: ['chat_member']\n}\n//launching bot\nbot.launch(options2)\n    .then(() => {\n        console.log(\"Bot Running\");\n    })\n    .catch((err) => {\n        console.log(`Error Running Bot: ${err}`);\n    })\n\n`\n```\nyou will not get update about this method directly you need to add it to allowed_updates",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2022-01-22T05:48:37",
      "url": "https://stackoverflow.com/questions/70810080/chat-member-not-getting-invocked"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "telegraf",
      "question_id": 68883437,
      "title": "How to save telegram data in a google spreedsheet?",
      "problem": "I have built a simple telegram bot using telegraf and used this code to log a specific information that I need :\n```\n`bot.on('text', (ctx, next) => {\n  console.log(`[text] ${ ctx.message.chat.id } ${ ctx.from.username } ${ ctx.message.chat.first_name+ \" \" + ctx.message.chat.last_name } ${ ctx.message.text }`);\n   return next();\n});\n`\n```\nand as result, the log be something like this\n\n[text] 563789142 xMA3x Mohammed Abbas /start\n\nNow I want to save that information in a google spreadsheet, I had followed this Tutorial and was able to push a quotation marked values into the spreadsheet only, but i don't know how to push the console.log result into the spreddsheet\nanyway, here is my code\n```\n`const { Telegraf } = require('telegraf');\nconst bot = new Telegraf(\"xyz\")\nconst { google } = require(\"googleapis\");\nconst keys = require(\"./Keys.json\")\n\nbot.on('text', (ctx, next) => {\n  console.log(`[text] ${ ctx.message.chat.id } ${ ctx.from.username } ${ ctx.message.chat.first_name+ \" \" + ctx.message.chat.last_name } ${ ctx.message.text }`);\n   return next();\n});\n\nbot.start((ctx) => ctx.reply('Welcome'))\nbot.help((ctx) => ctx.reply('Send me a sticker'))\nbot.on('sticker', (ctx) => ctx.reply('\ud83d\udc4d'))\nbot.hears('hi', (ctx) =>  ctx.reply('Hey there'))\n\nconst client = new google.auth.JWT(\n  keys.client_email, \n  null, \n  keys.private_key, \n  [\"https://www.googleapis.com/auth/spreadsheets\"]\n);\n\nclient.authorize(function(err){\n\n    if(err){\n      console.log(err);\n      return;\n    } else {\n      console.log(\"connected\");\n      gsrun(client);\n    }\n\n});\n\nasync function gsrun(cl){\n\n  const gsapi = google.sheets({version:\"v4\", auth: cl});\n    \n  const updateOptions = {\n    spreadsheetId: \"xyz\",\n    range: \"Sheet1\",\n    valueInputOption: \"RAW\",\n    insertDataOption: \"INSERT_ROWS\",\n    resource: {\n      values:[\n        [\"this is working\"]\n      ]}\n  };\n let res = await gsapi.spreadsheets.values.append(updateOptions);\n console.log(res);\n}\nbot.launch()\n`\n```\nso as you see the \"this is working\" is pushed successfully in the spreadsheet, but when I try to add another value like ctx.message.chat.id it give me ReferenceError: ctx is not defined\nso how I can make the google sheet API recognize the telegraf commands? or to be more general, how I can save the *ctx.message.chat.id,ctx.from.username..etc * info (that come form the telegram) into the spreedhsset ?",
      "solution": "`ctx` lives within your bot hooks, so to save the information to the sheet, you have to call your googlesheets function inside the relevant hook.\nPossible updates:\n```\n`const { Telegraf } = require('telegraf');\nconst bot = new Telegraf(\"xyz\")\nconst { google } = require(\"googleapis\");\nconst keys = require(\"./Keys.json\")\n\nconst client = new google.auth.JWT(\n  keys.client_email, \n  null, \n  keys.private_key, \n  [\"https://www.googleapis.com/auth/spreadsheets\"]\n);\n\nasync function gsrun(cl, data){\n\n    const gsapi = google.sheets({version:\"v4\", auth: cl});\n      \n    const updateOptions = {\n      spreadsheetId: \"xyz\",\n      range: \"Sheet1\",\n      valueInputOption: \"RAW\",\n      insertDataOption: \"INSERT_ROWS\",\n      resource: {\n        values:[\n          [data]\n        ]}\n    };\n   let res = await gsapi.spreadsheets.values.append(updateOptions);\n   console.log(res);\n  }\n\nconst saveMetadataToSheets = (data) => {\n    client.authorize(function(err){\n        if(err){\n          console.log(err);\n          return;\n        } else {\n          console.log(\"connected\");\n          gsrun(client, data);\n        }\n    });\n}\n\nbot.on('text', (ctx, next) => {\n    const data = `[text] ${ ctx.message.chat.id } ${ ctx.from.username } ${ ctx.message.chat.first_name+ \" \" + ctx.message.chat.last_name } ${ ctx.message.text }`\n    console.log(data);\n    // pass any data that you need to save to the sheets\n    saveMetadataToSheets(data)\n     return next();\n  });\n  \nbot.start((ctx) => ctx.reply('Welcome'))\nbot.help((ctx) => ctx.reply('Send me a sticker'))\nbot.on('sticker', (ctx) => ctx.reply('\ud83d\udc4d'))\nbot.hears('hi', (ctx) =>  ctx.reply('Hey there'))\nbot.launch()\n`\n```",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-08-22T19:02:22",
      "url": "https://stackoverflow.com/questions/68883437/how-to-save-telegram-data-in-a-google-spreedsheet"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "telegraf",
      "question_id": 66123009,
      "title": "parsing pattern using logparse gork not working",
      "problem": "I have a log file with a specific pattern format and I want to extract some field using a pattern but still not able to retrieve the correct value :\nThis's a line of my log file :\n```\n`2021-02-08 09:09:38,111 INFO  [stdout] (default task-26) Payload: {\"rqUID\":\"12345678-abdcABCD-09876543-abcdefgh\",\"addUsrIdentif\":{\"userId\":\"string\",\"requestDate\":\"string\",\"userLanguage\":\"string\",\"financialInstitution\":\"string\",\"products\":[\"string\"],\"providerLogin\":\"string\",\"providerPasswd\":\"string\"},\"customerInformation\":{\"customerId\":\"string\",\"orgId\":\"string\",\"orgDepth\":12,\"contractId\":\"string\"},\"merchantInformation\":{\"chainId\":\"string\",\"merchantId\":\"string\",\"posId\":\"string\"},\"agentInformation\":{\"oedInstitutionId\":\"string\",\"branchId\":\"string\",\"agentId\":\"string\"},\"caseReference\":12}\n`\n```\nI want to extract the field rqUID value using this pattern I get this result :\n```\n`[[inputs.logparser]]\n    files = [\"D:\\\\server.log\"]\n    from_beginning = true\n    [inputs.logparser.grok]\n        measurement = \"PWCAPI\"\n        patterns = [\"%{LOG_PATTERN}\"]\n        custom_patterns = '''\n        LOG_PATTERN %{WORD:rqUID}\n'''\n`\n```\nResults :\n{\n\"rqUID\": [\n[\n\"2021\"\n]\n]\n}\nMy purpose is the get : `rqUID = 12345678-abdcABCD-09876543-abcdefgh`\nI tested the pattern using : https://grokdebug.herokuapp.com/\nCan someone help ,thanks in advance :)",
      "solution": "You can use a named capturing group here with a customized pattern:\n`\"rqUID\":\"(?[^\"]+)\n`\nDetails:\n\n`\"rqUID\":\"` - a literal substring\n`(?[^\"]+)` - a named capturing group `rqUID` that captures any one or more chars other than a `\"` char.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-02-09T17:27:19",
      "url": "https://stackoverflow.com/questions/66123009/parsing-pattern-using-logparse-gork-not-working"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "flux",
      "question_id": 71465522,
      "title": "Subscribe to flux from inside subscribe in Spring webFlux java",
      "problem": "I have written a logic using spring reactor library to get all operators and then all devices for each operator (paginated) in async mode.\nCreated a flux to get all operator and then subscribing to it.\n```\n`    final Flux> operatorDetailsFlux = reactiveResourceProvider.getOperators();\n    operatorDetailsFlux\n        .subscribe(operatorDetailsList -> {\n          for (final OperatorDetails operatorDetails : operatorDetailsList) {\n            getAndCacheDevicesForOperator(operatorDetails.getId());\n          }\n        });\n`\n```\nNow, for each operator I'm fetching the devices which requires multiple subscriptions to get device mono which gets all pages async by subscribing to the MONO.\n```\n`private void getAndCacheDevicesForOperator(final int operatorId) {\n    Mono deviceListResponseEntityMono = reactiveResourceProvider.getConnectedDeviceMonoWithRetryAndErrorSpec(\n        operatorId, 0);\n\n    deviceListResponseEntityMono.subscribe(deviceListResponseEntity -> {\n      final PaginatedResponseEntity PaginatedResponseEntity = deviceListResponseEntity.getData();\n      final long totalDevicesInOperator = PaginatedResponseEntity.getTotalCount();\n\n      int deviceCount = PaginatedResponseEntity.getCount();\n      while (deviceCount  deviceListResponseEntityPageMono = reactiveResourceProvider.getConnectedDeviceMonoWithRetryAndErrorSpec(\n            operatorId, deviceCount);\n\n        deviceListResponseEntityPageMono.subscribe(deviceListResponseEntityPage -> {\n          final List deviceDetailsList = deviceListResponseEntityPage.getData()\n              .getItems();\n          // work on devices\n        });\n\n        deviceCount += DEVICE_PAGE_SIZE;\n      }\n    });\n  }\n`\n```\nThis code works fine. But my question is it a good idea to subscribe to mono from inside subscribe?",
      "solution": "I broke it down to two flows 1st getting all operators and then getting all devices for each operator.\nFor pagination I'm using `Flux.expand` to extract all pages.\n```\n`public Flux getAllOperators() {\n  return getOperatorsMonoWithRetryAndErrorSpec(0)\n      .expand(paginatedResponse -> {\n        final PaginatedEntity operatorDetailsPage = paginatedResponse.getData();\n        if (morePagesAvailable(operatorDetailsPage) {\n          return getOperatorsMonoWithRetryAndErrorSpec(operatorDetailsPage.getOffset() + operatorDetailsPage.getCount());\n        }\n        return Mono.empty();\n      })\n      .flatMap(responseEntity -> fromIterable(responseEntity.getData().getItems()))\n      .subscribeOn(apiScheduler);\n}\n\n`\n```\n```\n`public Flux getAllDevices(final int opId, final int offset) {\n  return getConnectedDeviceMonoWithRetryAndErrorSpec(opId, offset)\n      .expand(paginatedResponse -> {\n        final PaginatedEntity deviceDetailsPage = paginatedResponse.getData();\n        if (morePagesAvailabile(deviceDetailsPage)) {\n          return getConnectedDeviceMonoWithRetryAndErrorSpec(opId,\n              deviceDetailsPage.getOffset() + deviceDetailsPage.getCount());\n        }\n        return Mono.empty();\n      })\n      .flatMap(responseEntity -> fromIterable(responseEntity.getData().getItems()))\n      .subscribeOn(apiScheduler);\n}\n\n`\n```\nFinally I'm creating a pipeline and subscribing to it to trigger the pipeline.\n```\n`operatorDetailsFlux\n    .flatMap(operatorDetails -> {\n        return reactiveResourceProvider.getAllDevices(operatorDetails.getId(), 0);\n    })\n    .subscribe(deviceDetails -> {\n      // act on devices\n    });\n\n`\n```",
      "question_score": 5,
      "answer_score": 3,
      "created_at": "2022-03-14T10:32:16",
      "url": "https://stackoverflow.com/questions/71465522/subscribe-to-flux-from-inside-subscribe-in-spring-webflux-java"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "flux",
      "question_id": 72529044,
      "title": "Why doesn&#39;t Schedulers.newParallel() from project Reactor stop running after Flux is done emitting elements?",
      "problem": "I have a primitive `Flux` of `String`s, and run this code in the `main()` method.\n`package com.example;\n    \nimport reactor.core.publisher.Flux;\nimport reactor.core.scheduler.Schedulers;\nimport reactor.util.Logger;\nimport reactor.util.Loggers;\n    \nimport java.util.Arrays;\nimport java.util.List;\n    \npublic class Parallel {\n    \n  private static final Logger log = Loggers.getLogger(Parallel.class.getName());\n\n  private static List COLORS = Arrays.asList(\"red\", \"white\", \"blue\");\n\n  public static void main(String[] args) throws InterruptedException {\n    Flux flux = Flux.fromIterable(COLORS);\n    flux\n      .log()\n      .map(String::toUpperCase)\n      .subscribeOn(Schedulers.newParallel(\"sub\"))\n      .publishOn(Schedulers.newParallel(\"pub\", 1))\n      .subscribe(value -> {\n        log.info(\"==============Consumed: \" + value);\n      });\n  }\n}\n`\nIf you try to run this code the app never stops running and you need to stop it manually.\nIf I replace `.newParallel()` with `.parallel()` everything works as expected and the app finishes normally.\nWhy can't it finish running on its own? Why does it hang?\nWhat is the reason for this behavior?\nIf you run this code as a JUnit test it works fine and it doesn't hang.",
      "solution": "`Scheduler` instances that you create yourself with `newXxx` factory methods are created in non-daemon mode by default, which means that it can prevent the JVM from exiting.\nJUnit calls `System.exit()` when all the tests have run, which explains why the test scenario doesn't hang.\nIn this context, `Schedulers.newSingle()` and `Schedulers.newParallel()` variants are the worst \"offenders\", because the threads created are not culled after an inactivity timeout, unlike with `Schedulers.newBoundedElastic()`.\nIf in a real world scenario you have a well defined application lifecycle, you could store the `Scheduler` instances somewhere (eg. as beans) and ensure each `Scheduler#dispose()` is called at the end of the application lifecycle.\nEasier solution: create the `Schedulers` explicitly with `daemon == true` using the relevant factory overload.",
      "question_score": 4,
      "answer_score": 5,
      "created_at": "2022-06-07T11:41:58",
      "url": "https://stackoverflow.com/questions/72529044/why-doesnt-schedulers-newparallel-from-project-reactor-stop-running-after-flu"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "flux",
      "question_id": 70605765,
      "title": "How to return Flux&lt;String&gt; in ServerResponse in WebFlux",
      "problem": "I have a Jdbc Layer which is returning Flux. While returning the data, the fromPublisher method, it's accepting other Serializable classes, but the method is not accepting Flux.\nApproach 1\n```\n`public Mono getNames(final ServerRequest request) {\n               Flux strings = Flux.just(\"a\", \"b\", \"c\"); \n        return ServerResponse.ok().contentType(APPLICATION_JSON)\n                .body(fromPublisher(response), String.class);\n    }\n`\n```\nAbove approach is returning abc combined as a Single String.\nI tried this,\n```\n`return ServerResponse.ok()\n                .contentType(APPLICATION_JSON)\n                .body(BodyInserters.fromValue(response), List.class);\n`\n```\nI tried this aswell.\n```\n` Mono> mono = response.collectList();\nServerResponse.ok()\n                .contentType(APPLICATION_JSON)\n                .body(fromPublisher(mono, String.class));\n`\n```\nbut this is also giving a Runtime error of\n```\n`> body' should be an object, for reactive types use a variant specifying\n> a publisher/producer and its related element type\n`\n```",
      "solution": "This is working.\n```\n`Mono> strings = Flux.just(\"a\", \"b\", \"c\").collectList();\n \nreturn strings.flatMap(string -> ServerResponse.ok()\n                        .contentType(APPLICATION_JSON)\n                        .bodyValue(string));\n`\n```",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2022-01-06T11:31:10",
      "url": "https://stackoverflow.com/questions/70605765/how-to-return-fluxstring-in-serverresponse-in-webflux"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "flux",
      "question_id": 79629110,
      "title": "Livewire Flux and Tailwind list items",
      "problem": "I'm having an issue and I think it might be with Tailwind4, I haven't really used Tailwind before. But now since I am using Livewire Flux, I have an issue with the Flux editor, or at least, with displaying the output. Here's an image of how I enter it in my editor:\n\nSo I expect the output to be the same, but it is being displayed like so:\n\nThe HTML looks like this:\n\n`\n\n  Hello, this is a test:\n  \n    First\n    Second\n    Third\n  \n  And this is also a test:\n  \n    First\n    Second\n    Third\n  \n  Does it work?\n`\n\nSo, two questions:\n\nWhy are the lists not being displayed properly?\nWhy are there no line breaks as expected? Or am I missing something here?",
      "solution": "TailwindCSS is working correctly. What you're missing is the default CSS styling. Disable Tailwind Preflight when import TailwindCSS v4.\n\nPreflight - TailwindCSS v4 Docs\n\nThe `@import \"tailwindcss\"` statement imports Preflight, which is designed to remove all default CSS styles from elements. If you want to skip this, you can do so by manually importing the individual parts of TailwindCSS and omitting Preflight:\n\nTailwindCSS v4 Playground\n\n`\n\n@layer theme, base, components, utilities;\n@import \"tailwindcss/theme.css\" layer(theme);\n@import \"tailwindcss/utilities.css\" layer(utilities);\n\n  Hello, this is a test:\n  \n    First\n    Second\n    Third\n  \n  And this is also a test:\n  \n    First\n    Second\n    Third\n  \n  Does it work?\n`\n\nOr restore the style of `ol` and `ul` lists like this:\n\nTailwindCSS v4 Playground\n\n`\n\n@import \"tailwindcss\";\n\n@layer base {\n  ul,\n  ol {\n    list-style: revert;\n    margin: revert; \n    padding-inline-start: revert;\n  }\n\n  li {\n    margin: revert;\n    padding: revert;\n    display: list-item;\n  }\n}\n\n  Hello, this is a test:\n  \n    First\n    Second\n    Third\n  \n  And this is also a test:\n  \n    First\n    Second\n    Third\n  \n  Does it work?\n`\n\nOr you can use TailwindCSS list classes (note that padding and margin are reset in this case, so you may want to restore them):\n\nTypography: `list-style-type` - TailwindCSS v4 Docs\nTailwindCSS v4 Playground\n\n`\n\n@import \"tailwindcss\";\n\n  Hello, this is a test:\n  \n    First\n    Second\n    Third\n  \n  And this is also a test:\n  \n    First\n    Second\n    Third\n  \n  Does it work?\n`",
      "question_score": 3,
      "answer_score": 3,
      "created_at": "2025-05-19T18:07:51",
      "url": "https://stackoverflow.com/questions/79629110/livewire-flux-and-tailwind-list-items"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "flux",
      "question_id": 75416440,
      "title": "Bug when training a custom model using Flux",
      "problem": "I am pretty new to Julia and Flux. I am trying to build a simple neural network but using an attention layer. I wrote the code as follows, which works fine in the inference(feed-forward) mode:\n```\n`using Flux\n\nstruct Attention\n    W\n    v\nend\n\nAttention(vehile_embedding_dim::Integer) = Attention(\n    Dense(vehile_embedding_dim => vehile_embedding_dim, tanh),\n    Dense(vehile_embedding_dim, 1, bias=false, init=Flux.zeros32)\n)\n\nfunction (a::Attention)(inputs)\n    alphas = [a.v(e) for e in a.W.(inputs)]\n    alphas = sigmoid.(alphas)\n    output = sum([alpha.*input for (alpha, input) in zip(alphas, inputs)])\n    return output\nend\n\nFlux.@functor Attention\n\nstruct AttentionNet \n    embedding\n    attention\n    fc_output\n    vehicle_num::Integer\n    vehicle_dim::Integer\nend\n\nAttentionNet(vehicle_num::Integer, vehicle_dim::Integer, embedding_dim::Integer) = AttentionNet(\n    Dense(vehicle_dim+1 => embedding_dim, relu),\n    Attention(embedding_dim),\n    Dense(1+embedding_dim => 1),\n    vehicle_num,\n    vehicle_dim\n)\n\nfunction (a_net::AttentionNet)(x)\n    time_idx = x[[1], :]\n    vehicle_states = [x[2+a_net.vehicle_dim*(i-1):2+a_net.vehicle_dim*i-1, :] for i in 1:a_net.vehicle_num]\n    vehicle_states = [vcat(time_idx, vehicle_state) for vehicle_state in vehicle_states]\n\n    vehicle_embeddings = a_net.embedding.(vehicle_states)\n    attention_output = a_net.attention(vehicle_embeddings)\n    \n    x = a_net.fc_output(vcat(time_idx, attention_output))\n    return x\nend\n\nFlux.@functor AttentionNet\nFlux.trainable(a_net::AttentionNet) = (a_net.embedding, a_net.attention, a_net.fc_output,)\n\nfake_inputs = rand(22, 640)\nfake_outputs = rand(1, 640)\na_net = AttentionNet(3, 7, 64)|> gpu\nopt = Adam(.01)\nopt_state = Flux.setup(opt, a_net)\n\ndata = Flux.DataLoader((fake_inputs, fake_outputs)|>gpu, batchsize=32, shuffle=true)\n\nFlux.train!(a_net, data, opt_state) do m, x, y\n    Flux.mse(m(x), y)\nend\n`\n```\nBut when I trained it, I got the following error message and a warning:\n```\n`\u250c Warning: trainable(x) should now return a NamedTuple with the field names, not a Tuple\n\u2514 @ Optimisers C:\\Users\\Herr LU\\.julia\\packages\\Optimisers\\SoKJO\\src\\interface.jl:164\nERROR: MethodError: no method matching +(::Base.RefValue{Any}, ::NamedTuple{(:contents,), Tuple{CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}}})\nClosest candidates are:\n  +(::Any, ::Any, ::Any, ::Any...) at operators.jl:591\n  +(::Union{InitialValues.NonspecificInitialValue, InitialValues.SpecificInitialValue{typeof(+)}}, ::Any) at C:\\Users\\Herr LU\\.julia\\packages\\InitialValues\\OWP8V\\src\\InitialValues.jl:154\n  +(::ChainRulesCore.Tangent{P}, ::P) where P at C:\\Users\\Herr LU\\.julia\\packages\\ChainRulesCore\\C73ay\\src\\tangent_arithmetic.jl:146\n  ...\nStacktrace:\n  [1] accum(x::Base.RefValue{Any}, y::NamedTuple{(:contents,), Tuple{CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}}})\n    @ Zygote C:\\Users\\Herr LU\\.julia\\packages\\Zygote\\SmJK6\\src\\lib\\lib.jl:17\n  [2] accum(x::Base.RefValue{Any}, y::NamedTuple{(:contents,), Tuple{CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}}}, zs::Base.RefValue{Any}) (repeats 2 times)\n    @ Zygote C:\\Users\\Herr LU\\.julia\\packages\\Zygote\\SmJK6\\src\\lib\\lib.jl:22\n  [3] Pullback\n    @ e:\\Master Thesis\\lu_jizhou\\toy exmaple\\dqn_model.jl:39 [inlined]\n  [4] (::typeof(\u2202(\u03bb)))(\u0394::CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer})\n    @ Zygote C:\\Users\\Herr LU\\.julia\\packages\\Zygote\\SmJK6\\src\\compiler\\interface2.jl:0\n  [5] Pullback\n    @ e:\\Master Thesis\\lu_jizhou\\toy exmaple\\dqn_model.jl:62 [inlined]\n  [6] #208\n    @ C:\\Users\\Herr LU\\.julia\\packages\\Zygote\\SmJK6\\src\\lib\\lib.jl:206 [inlined]\n  [7] #2066#back\n    @ C:\\Users\\Herr LU\\.julia\\packages\\ZygoteRules\\AIbCs\\src\\adjoint.jl:67 [inlined]\n  [8] Pullback\n    @ C:\\Users\\Herr LU\\.julia\\packages\\Flux\\ZdbJr\\src\\train.jl:102 [inlined]\n  [9] (::typeof(\u2202(\u03bb)))(\u0394::Float32)\n    @ Zygote C:\\Users\\Herr LU\\.julia\\packages\\Zygote\\SmJK6\\src\\compiler\\interface2.jl:0\n [10] (::Zygote.var\"#60#61\"{typeof(\u2202(\u03bb))})(\u0394::Float32)\n    @ Zygote C:\\Users\\Herr LU\\.julia\\packages\\Zygote\\SmJK6\\src\\compiler\\interface.jl:45\n [11] withgradient(f::Function, args::AttentionNet)\n    @ Zygote C:\\Users\\Herr LU\\.julia\\packages\\Zygote\\SmJK6\\src\\compiler\\interface.jl:133\n [12] macro expansion\n    @ C:\\Users\\Herr LU\\.julia\\packages\\Flux\\ZdbJr\\src\\train.jl:102 [inlined]\n [13] macro expansion\n    @ C:\\Users\\Herr LU\\.julia\\packages\\ProgressLogging\\6KXlp\\src\\ProgressLogging.jl:328 [inlined]\n [14] train!(loss::Function, model::AttentionNet, data::MLUtils.DataLoader{Tuple{CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}}, Random._GLOBAL_RNG, Val{nothing}}, opt::Named\nTuple{(:embedding, :attention, :fc_output, :vehicle_num, :vehicle_dim), Tuple{NamedTuple{(:weight, :bias, :\u03c3), Tuple{Optimisers.Leaf{Optimisers.Adam{Float64}, Tuple{CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, CUDA.CuArr\nay{Float32, 2, CUDA.Mem.DeviceBuffer}, Tuple{Float64, Float64}}}, Optimisers.Leaf{Optimisers.Adam{Float64}, Tuple{CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Tuple{Float64\n, Float64}}}, Tuple{}}}, NamedTuple{(:W, :v), Tuple{NamedTuple{(:weight, :bias, :\u03c3), Tuple{Optimisers.Leaf{Optimisers.Adam{Float64}, Tuple{CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 2, CUDA.Mem.De\nviceBuffer}, Tuple{Float64, Float64}}}, Optimisers.Leaf{Optimisers.Adam{Float64}, Tuple{CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Tuple{Float64, Float64}}}, Tuple{}}}, N\namedTuple{(:weight, :bias, :\u03c3), Tuple{Optimisers.Leaf{Optimisers.Adam{Float64}, Tuple{CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, Tuple{Float64, Float64}}}, Tuple{}, Tupl\ne{}}}}}, NamedTuple{(:weight, :bias, :\u03c3), Tuple{Optimisers.Leaf{Optimisers.Adam{Float64}, Tuple{CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, Tuple{Float64, Float64}}}, Opt\nimisers.Leaf{Optimisers.Adam{Float64}, Tuple{CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Tuple{Float64, Float64}}}, Tuple{}}}, Tuple{}, Tuple{}}}; cb::Nothing)\n    @ Flux.Train C:\\Users\\Herr LU\\.julia\\packages\\Flux\\ZdbJr\\src\\train.jl:100\n [15] train!(loss::Function, model::AttentionNet, data::MLUtils.DataLoader{Tuple{CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}}, Random._GLOBAL_RNG, Val{nothing}}, opt::Named\nTuple{(:embedding, :attention, :fc_output, :vehicle_num, :vehicle_dim), Tuple{NamedTuple{(:weight, :bias, :\u03c3), Tuple{Optimisers.Leaf{Optimisers.Adam{Float64}, Tuple{CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, CUDA.CuArr\nay{Float32, 2, CUDA.Mem.DeviceBuffer}, Tuple{Float64, Float64}}}, Optimisers.Leaf{Optimisers.Adam{Float64}, Tuple{CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Tuple{Float64\n, Float64}}}, Tuple{}}}, NamedTuple{(:W, :v), Tuple{NamedTuple{(:weight, :bias, :\u03c3), Tuple{Optimisers.Leaf{Optimisers.Adam{Float64}, Tuple{CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 2, CUDA.Mem.De\nviceBuffer}, Tuple{Float64, Float64}}}, Optimisers.Leaf{Optimisers.Adam{Float64}, Tuple{CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Tuple{Float64, Float64}}}, Tuple{}}}, N\namedTuple{(:weight, :bias, :\u03c3), Tuple{Optimisers.Leaf{Optimisers.Adam{Float64}, Tuple{CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, Tuple{Float64, Float64}}}, Tuple{}, Tupl\ne{}}}}}, NamedTuple{(:weight, :bias, :\u03c3), Tuple{Optimisers.Leaf{Optimisers.Adam{Float64}, Tuple{CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, Tuple{Float64, Float64}}}, Opt\nimisers.Leaf{Optimisers.Adam{Float64}, Tuple{CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Tuple{Float64, Float64}}}, Tuple{}}}, Tuple{}, Tuple{}}})\n    @ Flux.Train C:\\Users\\Herr LU\\.julia\\packages\\Flux\\ZdbJr\\src\\train.jl:97\n [16] top-level scope\n    @ e:\\Master Thesis\\lu_jizhou\\toy exmaple\\dqn_model.jl:61\n`\n```\nI followed the instruction from the official tutorial on custom layers, but it doesn\u2019t specify how to get custom layers properly trained. Could someone help me out?",
      "solution": "For anyone who is interested, this problem is well solved by @ToucheSir on this GitHub thread.",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2023-02-10T22:51:54",
      "url": "https://stackoverflow.com/questions/75416440/bug-when-training-a-custom-model-using-flux"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "flux",
      "question_id": 72249536,
      "title": "Is there a name to this common state manegement pattern with rxjs? Does it have limitations?",
      "problem": "In Angular applications, we typically do something like this to manage shared states:\n```\n`import { BehaviorSubject } from 'rxjs';\n\ninterface User {\n  id: number;\n}\n\nclass UserService {\n  private _users$ = new BehaviorSubject([]);\n\n  users$ = this._users$.asObservable();\n\n  deleteUser(user: User) {\n    const users = this._users$.getValue();\n    this._users$.next(users.filter(user => user.id !== user.id));\n  }\n\n  addUser(user: User) {\n    const users = this._users$.getValue();\n    this._users$.next([...users, user]);\n  }\n\n  updateUser(updatedUser: User) {\n    const users = this._users$.getValue();\n    this._users$.next(users.map(user => user.id === updatedUser.id ? updatedUser : user));\n  }\n}\n`\n```\nThis basically solves the same problem that, fundamentally, Flux-based and Redux-based patterns (like ngRx and redux itself) try to solve: how can we update shared state so that view components can act reactively to changes and will aways be able to show the true current state. But it is simpler than those patterns.\nMy question is: does this pattern have a known name? Do we have any advantage in using libraries like ngRx(or even redux in react) that would cover any limitation of this pattern?",
      "solution": "That's a valid pattern, and that's why we also have introduced ngrx/component-store (https://ngrx.io/guide/component-store).\nThe docs also provide pro/cons of both \"ways\": https://ngrx.io/guide/component-store/comparison",
      "question_score": 3,
      "answer_score": 1,
      "created_at": "2022-05-15T17:10:29",
      "url": "https://stackoverflow.com/questions/72249536/is-there-a-name-to-this-common-state-manegement-pattern-with-rxjs-does-it-have"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "flux",
      "question_id": 70050658,
      "title": "How to delay emitting each item in iterable Spring Boot Flux",
      "problem": "My problem is slightly different, but I can describe the issue in the following manner.\nWhat I would like is some code that emits an item every delay period (3 seconds). But when I hit the `/flux` URL, the page waits for 3 seconds and gives all 4 items. That means it emits all the items after 3 seconds instead of one item every 3 seconds.\n```\n`@RestController\n@RequestMapping(\"/flux\")\npublic class MyController {\n\n    List items = Arrays.asList(\n            new Item(\"name1\",\"description1\"),\n            new Item(\"name2\",\"description2\"),\n            new Item(\"name3\",\"description3\"),\n            new Item(\"name4\",\"description4\"));\n    @GetMapping(produces = MediaType.TEXT_EVENT_STREAM_VALUE)\n    Flux getItems(){\n        return Flux.fromIterable(items)\n                .delayElements(Duration.ofSeconds(3));\n    }\n}\n@Data\n@AllArgsConstructor\nclass Item{\n    String name;\n    String description;\n}\n`\n```\nUpdate\nI saw this post to explain how to do this in RxJava, so I tried this. But with `ZipWith` the results are worse. Now the page waits 12 seconds. That means the browser response is sent only when Flux is completed... Not Sure why.\n```\n`Flux getItems(){\n            return Flux.fromIterable(items)\n           .zipWith(Flux.interval(Duration.ofSeconds(3)),(item,time)->item);\n}\n`\n```\np.s. Using Spring WebFlux dependency, so local started up with Netty not Tomcat.",
      "solution": "Problem is not in the code, problem is in the browser.\nIf we use Chrome, Both the code mentioned above works as expected. But does not work with safari browser",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-11-21T00:23:36",
      "url": "https://stackoverflow.com/questions/70050658/how-to-delay-emitting-each-item-in-iterable-spring-boot-flux"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "flux",
      "question_id": 77876955,
      "title": "Why doesn&#39;t the loss calculated by Flux `withgradient` match what I have calculated?",
      "problem": "I'm trying to train a simple CNN with Flux and running into a weird issue...during training the loss appears to go down (indicating that it's working) but despite what the loss curve suggested the \"trained\" model output was very bad, and when I calculated the loss by hand I noticed that it differed from what the training indicated it should be (it was acting like it hadn't been trained at all).\nI then started calculating the loss returned inside the gradient vs. outside, and after a lot of digging I think the problem is related to the `BatchNorm` layer. Consider the following minimum example:\n```\n`using Flux\nx = rand(100,100,1,1) #say a greyscale image 100x100 with 1 channel (greyscale) and 1 batch\ny = @. 5*x + 3 #output image, some relationship to the input values (doesn't matter for this)\nm = Chain(BatchNorm(1),Conv((1,1),1=>1)) #very simple model (doesn't really do anything but illustrates the problem)\nl_init = Flux.mse(m(x),y) #initial loss after model creation\nl_grad, grad = Flux.withgradient(m -> Flux.mse(m(x),y), m) #loss calculated by gradient\nl_final = Flux.mse(m(x),y) #loss calculated again using the model (no parameters have been updated)\nprintln(\"initial loss: $l_init\")\nprintln(\"loss calculated in withgradient: $l_grad\")\nprintln(\"final loss: $l_final\")\n`\n```\nAll of the losses above will be different, sometimes pretty drastically (when running just now I got 22.6, 30.7, and 23.0), when I think they should all be the same?\nInterestingly if I remove the `BatchNorm` layer, the outputs are all the same, i.e. running:\n```\n`using Flux\nx = rand(100,100,1,1) #say a greyscale image 100x100 with 1 channel (greyscale) and 1 batch\ny = @. 5*x + 3 #output image\nm = Chain(Conv((1,1),1=>1))\nl_init = Flux.mse(m(x),y) #initial loss after model creation\nl_grad, grad = Flux.withgradient(m -> Flux.mse(m(x),y), m)\nl_final = Flux.mse(m(x),y)\nprintln(\"initial loss: $l_init\")\nprintln(\"loss calculated in withgradient: $l_grad\")\nprintln(\"final loss: $l_final\")\n`\n```\nProduces the same number for each loss calculation.\nWhy does including the `BatchNorm` layer change the value of the loss like this?\nMy (limited) understanding was that this was just supposed to normalize the input values, which I understand could affect the loss between the unormalized and normalized case, but I don't understand why it would produce different values of the loss for the same input values on the same model without any of the parameters of said model being updated?",
      "solution": "Look at the documentation of `BatchNorm`\n```\n`BatchNorm(channels::Integer, \u03bb=identity;\n            init\u03b2=zeros32, init\u03b3=ones32,\n            affine=true, track_stats=true, active=nothing,\n            eps=1f-5, momentum= 0.1f0)\n\n  Batch Normalization (https://arxiv.org/abs/1502.03167) layer. channels should\n  be the size of the channel dimension in your data (see below).\n\n  Given an array with N dimensions, call the N-1th the channel dimension. For a\n  batch of feature vectors this is just the data dimension, for WHCN images it's\n  the usual channel dimension.\n\n  BatchNorm computes the mean and variance for each D_1\u00d7...\u00d7D_{N-2}\u00d71\u00d7D_N input\n  slice and normalises the input accordingly.\n\n  If affine=true, it also applies a shift and a rescale to the input through to\n  learnable per-channel bias \u03b2 and scale \u03b3 parameters.\n\n  After normalisation, elementwise activation \u03bb is applied.\n\n  If track_stats=true, accumulates mean and var statistics in training phase that\n  will be used to renormalize the input in test phase.\n\n  Use testmode! during inference.\n\n  Examples\n  \u2261\u2261\u2261\u2261\u2261\u2261\u2261\u2261\u2261\u2261\n\n  julia> using Statistics\n  \n  julia> xs = rand(3, 3, 3, 2);  # a batch of 2 images, each having 3 channels\n  \n  julia> m = BatchNorm(3);\n  \n  julia> Flux.trainmode!(m);\n  \n  julia> isapprox(std(m(xs)), 1, atol=0.1) && std(xs) != std(m(xs))\n  true\n`\n```\nThe key bit here is that per default `track_stats=true`. This leads to the changing inputs. If you don't want to have this behaviour, initialise your model with\n```\n`m = Chain(BatchNorm(1, track_state=false),Conv((1,1),1=>1)) #very simple model (doesn't really do anything but illustrates the problem)\n`\n```\nand you'll get identical outputs as in your second example.\nThe `BatchNorm` is initialised with zero mean and unit std, and your input data isn't, that's why you'll get the changing output even with repeated identical input in the case that `track_state=true`, as far as I can see it (quickly).",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2024-01-25T01:30:43",
      "url": "https://stackoverflow.com/questions/77876955/why-doesnt-the-loss-calculated-by-flux-withgradient-match-what-i-have-calcula"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "flux",
      "question_id": 75626811,
      "title": "Error training using Flux.train! in Julia",
      "problem": "Problem\nI have the below code trying to train a neural network in Julia but I am getting an error whenever I try using Flux.train!\n```\n`using Flux, CSV, DataFrames, Random, Statistics, Plots\n\n# Load the data\ndata = CSV.read(\"daily_stock_returns.csv\")\n\n# Split the data into training and testing sets\ntrain_data = data[1:800, :]\ntest_data = data[801:end, :]\n\n# Define the input and output variables\ninputs = Matrix(train_data[:, 2:end])\noutputs = Matrix(train_data[:, 1])\n\n# Define the neural network architecture\nn_inputs = size(inputs, 2)\nn_hidden = 10\nn_outputs = 1\nmodel = Chain(\n    Dense(n_inputs, n_hidden, relu),\n    Dense(n_hidden, n_outputs)\n)\n\n# Define the loss function\nloss(x, y) = Flux.mse(model(x), y)\n\n# Define the optimizer\noptimizer = ADAM()\n\n# Train the model\nn_epochs = 100\nfor epoch in 1:n_epochs\n    Flux.train!(loss, params(model), [(inputs, outputs)], optimizer)\nend\n\n# Test the model\ntest_inputs = Matrix(test_data[:, 2:end])\ntest_outputs = Matrix(test_data[:, 1])\npredictions = Flux.predict(model, test_inputs)\ntest_loss = Flux.mse(predictions, test_outputs)\n\n# Print the test loss\nprintln(\"Test loss: $test_loss\")\n\n`\n```\nError\nWhen I run this cell of code:\n```\n`# Train the model\nn_epochs = 100\nfor epoch in 1:n_epochs\n    Flux.train!(loss, params(model), [(inputs, outputs)], optimizer)\nend\n`\n```\nI get the below error.\n```\n`UndefVarError: params not defined\n\nStacktrace:\n [1] top-level scope\n   @ .\\In[16]:4\n [2] eval\n   @ .\\boot.jl:368 [inlined]\n [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n   @ Base .\\loading.jl:1428\n`\n```\nI have tried uninstalling and installing with no improvement. How do I fix this error?",
      "solution": "It's because `Fux.params` (see the docs) isn't exported by default. You either replace `params` by `Flux.params` (which is how it's often done in the docs, as far as I recall), or replace `using Flux` by `using Flux: params` (the second option being probably less ideal because it adds a name that is really generic to your namespace).",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2023-03-03T13:01:40",
      "url": "https://stackoverflow.com/questions/75626811/error-training-using-flux-train-in-julia"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "flux",
      "question_id": 66189472,
      "title": "How to close client sse connection in spring boot application",
      "problem": "I have a spring boot application and I have to connect to some outside service using SSE. WebClient establishes the connection and then I'm using Flux for reading responses. Everything works ok, but the problem is that the connection stays open, because the process is not designed to reach the finish point every time in that 3rd party service. I would like to close the connection manually as a client since I know when this connection should finish. How can I do that?\nEstablishing connection:\n```\n`private Flux> connect(String accessToken) {\n    TcpClient timeoutClient = createTimeoutClient();\n    ReactorClientHttpConnector reactorClientHttpConnector = new ReactorClientHttpConnector(HttpClient.from(timeoutClient));\n    String url = npzServerBaseUrl+uniqueCodePath;\n    WebClient client = WebClient\n            .builder()\n            .clientConnector(reactorClientHttpConnector)\n            .defaultHeader(HttpHeaders.AUTHORIZATION, Naming.TOKEN_PREFIX + accessToken)\n            .baseUrl(url)\n            .build();\n\n    ParameterizedTypeReference> type\n            = new ParameterizedTypeReference>() {};\n    return client.get()\n            .retrieve()\n            .onStatus(HttpStatus::is4xxClientError, clientResponse -> {\n                String msg = \"Error from server: \"+clientResponse.statusCode().toString();\n                        //invalidate access token\n                        if (clientResponse.statusCode().value()==401) {\n                            //remove invalid token and connect again\n                            loginContext.invalidToken(accessToken);\n                            return Mono.error(new InvalidNpzToken(msg));\n                        }\n                        return Mono.error(new IllegalStateException(msg));\n                    }\n            )\n            .onStatus(HttpStatus::is5xxServerError, clientResponse ->\n                    Mono.error(new IllegalStateException(\"Error from server: \"+clientResponse.statusCode().toString()))\n            )\n            .bodyToFlux(type);\n}\n\nprivate TcpClient createTimeoutClient() {\n    return TcpClient.create()\n            .option(ChannelOption.CONNECT_TIMEOUT_MILLIS, SECONDS*1000)\n            .option(EpollChannelOption.TCP_USER_TIMEOUT, SECONDS*1000)\n            .doOnConnected(\n                    c -> c.addHandlerLast(new ReadTimeoutHandler(SECONDS))\n                            .addHandlerLast(new WriteTimeoutHandler(SECONDS)));\n}\n`\n```\nHandling content:\n```\n`Flux> eventStream = connect(accessToken);\n\n    eventStream.subscribe(\n            content -> {\n                log.info(\"Time: {} - event: name[{}], id [{}], content[{}] \",\n                    LocalTime.now(), content.event(), content.id(), content.data());\n                if (\"uuid\".equals(content.event().trim())) {\n                    listener.receivedUniqueCode(content.data().trim());\n                } else if (\"code\".equals(content.event().trim())) {\n                    listener.receivedCode(content.data().trim());\n                }\n            },\n            (Throwable error) -> {\n                if (error instanceof InvalidToken) {\n                    log.error(\"Error receiving SSE\", error);\n                    //let's retry connection as token has expired\n                    getCode(request, listener);\n                }\n            },\n            () -> log.info(\"Connection closed!\"));\n`\n```\nWhat I expect is that I can call connection.close() or something like that and connection will be closed.\nThanks\nSome more information if needed.\nImports:\n```\n`import io.netty.channel.ChannelOption;\nimport io.netty.channel.epoll.EpollChannelOption;\nimport io.netty.handler.timeout.ReadTimeoutHandler;\nimport io.netty.handler.timeout.WriteTimeoutHandler;\nimport lombok.extern.slf4j.Slf4j;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.beans.factory.annotation.Value;\nimport org.springframework.core.ParameterizedTypeReference;\nimport org.springframework.http.HttpHeaders;\nimport org.springframework.http.HttpStatus;\nimport org.springframework.http.client.reactive.ReactorClientHttpConnector;\nimport org.springframework.http.codec.ServerSentEvent;\nimport org.springframework.stereotype.Component;\nimport org.springframework.web.reactive.function.client.WebClient;\nimport reactor.core.publisher.Flux;\nimport reactor.core.publisher.Mono;\nimport reactor.netty.http.client.HttpClient;\nimport reactor.netty.tcp.TcpClient;\n`\n```\nSpring boot:\n```\n`org.springframework.boot\nspring-boot-starter-parent\n2.1.1.RELEASE\n`\n```",
      "solution": "`eventStream.subscribe()` returns a `reactor.core.Disposable`\nYou can call `dispose()` on it to cancel the subscription and the underlying resources.",
      "question_score": 2,
      "answer_score": 2,
      "created_at": "2021-02-13T21:16:27",
      "url": "https://stackoverflow.com/questions/66189472/how-to-close-client-sse-connection-in-spring-boot-application"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "flux",
      "question_id": 78490672,
      "title": "Error in webflux webclient with response 200",
      "problem": "I am creating an example usisng spring boot webflux that is super simple. Two services and one of them call reactively two times the service of the other.\nBascially one controller of the flights \"microservice\" can register Flights with departure airport code and landing airport code and it will call a Mono endpoint of the airports service to know if the airport code exists.\nAll code can be found here:\nhttps://github.com/Gaboxondo/springbootwebfluxdemo/blob/main/error.log\nJust for explaining it with some code that you can find on the github repo:\n```\n`@Service\n@AllArgsConstructor\n@Slf4j\npublic class FlightsServiceImpl implements FlightsService {\n\n    private final FlightRepository flightRepository;\n    private final FlightFactory flightfactory;\n    private final AirportsService airportsService;\n\n    @Override\n    public Mono createFlight(String departureAirportCode, String landingAirportCode, Double price) {\n        Mono departCodeMonoValid = airportsService.airportCodeIsValid( departureAirportCode );\n        Mono landingCodeMonoValid = airportsService.airportCodeIsValid( landingAirportCode );\n        return Mono.zip(departCodeMonoValid,landingCodeMonoValid )\n            .flatMap( data-> {\n                if(!(data.getT1() && data.getT2())){\n                    throw new ControlledErrorException( \"error.code.09\",\"the airports codes are not valid\" );\n                }\n            return flightRepository.saveFlight( flightfactory.createFlight( departureAirportCode,landingAirportCode,price ) );\n        });\n    }\n`\n```\nThe client to call the other service is the following:\n```\n`@Component\n@AllArgsConstructor\npublic class AirportsWebClient {\n\n    //I know all is hardcoded and should be configurable with some @ConfigurationProperties, this is just for testing\n    public Mono validateAirportCode(String airportCode){\n        WebClient airportsWebClient = WebClient.builder().baseUrl( \"http://localhost:8082\" ).build();\n        return airportsWebClient.get().uri( \"/v1/airports/validate/\" + airportCode ).accept(\n                MediaType.valueOf( MediaType.TEXT_EVENT_STREAM_VALUE ) )\n            .retrieve().bodyToMono( Boolean.class );\n    }\n}\n`\n```\nand the endpoint in the airport service is this one:\n```\n`@RestController\n@RequestMapping(\"/v1/airports\")\n@AllArgsConstructor\npublic class AirportsController {\n\n    private static final List validCodes = List.of(\"MLG\",\"ROM\");\n\n    @GetMapping(value = \"/validate/{airportCode}\",produces = MediaType.TEXT_EVENT_STREAM_VALUE)\n    @Operation(description = \"Indicates if the airport code is valid or not\")\n    @ResponseStatus(HttpStatus.OK)\n    public Mono validateAirportCode(@PathVariable String airportCode){\n        if(validCodes.contains( airportCode )){\n            return Mono.just( true ).log();\n        }else {\n            return Mono.just( false ).log();\n        }\n    }\n}\n`\n```\nWhen I invoque the service to create a flight that also calls the other \"microservice\" for validating the airpotrs code I get the following error:\n```\n`2024-05-14 11:05:53,612 service=flights-service ERROR 664329712a5442f77e1ce9dc12f43b11 class=reactor.Mono.MapFuseable.1 \norg.springframework.web.reactive.function.client.WebClientResponseException: 200 OK from GET http://localhost:8082/v1/airports/validate/ROM\nCaused by: java.lang.UnsupportedOperationException: ServerSentEventHttpMessageReader only supports reading stream of events as a Flux\n    at org.springframework.http.codec.ServerSentEventHttpMessageReader.readMono(ServerSentEventHttpMessageReader.java:218)\n    Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: \nError has been observed at the following site(s):\n    *__checkpoint \u21e2 Body from GET http://localhost:8082/v1/airports/validate/ROM [DefaultClientResponse]\n`\n```\nI also added the log in the repo:\nhttps://github.com/Gaboxondo/springbootwebfluxdemo/blob/main/error.log\nI do not really understand what I am doing wrong, because the two reactives calls are returning 200 with a Mono Boolean but the error say something about that can only be flux.\nYou can also can run the two services without any special configuration just running the docker compose to create the mongoDb server that already creates the two databases.\nThen in postman just call the service:\n\nor with curl\n```\n`curl --location 'http://localhost:8081/v1/flights' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"departureAirportCode\" : \"MLG\",\n    \"landingAirportCode\": \"ROM\",\n    \"price\": 100\n}'\n`\n```\nThanks in advance",
      "solution": "Why are you using `MediaType.TEXT_EVENT_STREAM_VALUE` in both `com.mashosoft.airportsService.interfaces.web.AirportsController#validateAirportCode` and `com.mashosoft.flightsService.infrastructure.airports.client.AirportsWebClient#validateAirportCode` ?\nDoing this makes your controller switch to SSE mode which indeed sends a `Flux` and not a `Mono`.\nA quick fix could be to simply switch to the following code:\n`  @GetMapping(value = \"/validate/{airportCode}\", produces = MediaType.APPLICATION_JSON_VALUE)\n  @Operation(description = \"Indicates if the airport code is valid or not\")\n  @ResponseStatus(HttpStatus.OK)\n  public Mono validateAirportCode(@PathVariable String airportCode) {\n    if (validCodes.contains(airportCode)) {\n      return Mono.just(true).log();\n    } else {\n      return Mono.just(false).log();\n    }\n  }\n`\nand\n`    public Mono validateAirportCode(String airportCode){\n        WebClient airportsWebClient = WebClient.builder().baseUrl( \"http://localhost:8082\" ).build();\n        return airportsWebClient.get().uri( \"/v1/airports/validate/\" + airportCode )\n            .retrieve().bodyToMono( Boolean.class );\n    }\n`\nRunning the `curl` then gives:\n`\u279c  ~ curl -X POST --location \"http://127.0.0.1:8081/v1/flights\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Accept: application/x-ndjson\" \\\n    -d '{\n          \"departureAirportCode\": \"ROM\",\n          \"landingAirportCode\": \"MLG\",\n          \"price\": 100\n        }'\n\n{\"id\":\"99465ee5-2879-4f03-923c-1acf825adf50\",\"departureAirportCode\":\"ROM\",\"landingAirportCode\":\"MLG\",\"price\":100.0}\n`",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2024-05-16T16:36:06",
      "url": "https://stackoverflow.com/questions/78490672/error-in-webflux-webclient-with-response-200"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "flux",
      "question_id": 67344519,
      "title": "Can I have Flux as a field of ServerResponse body?",
      "problem": "I'm new to Spring Reactive Web, and i met the following issue. I want to create a microservice A with an endpoint which accepts a number N, sends N requests to microservice B (which returns a string for each request), wraps the strings into objects, combines them into a List/Flux (?) and returns a JSON with those objects, like:\n`{\n  \"number\": 4,\n  \"objects\": [\n    {\n       \"name\": \"first\"\n    },\n    {\n       \"name\": \"second\"\n    },\n    {\n       \"name\": \"third\"\n    },\n    {\n       \"name\": \"fourth\"\n    }\n  ]\n}\n`\nI want to use a functional endpoint for this. So i tried the following (made my best to simplify it):\n`public class MyObject {\n    private String name; // here should be a value received from B\n    // ...\n}\n`\n`public class MyResponse {\n    private int number;\n    private Flux objects; // or List?\n    // ...\n}\n`\n`@Component\n@RequiredArgsConstructor\npublic class MyHandler {\n\n    private final MyClient client;\n\n    public Mono generate(ServerRequest serverRequest) {\n        return serverRequest.bodyToMono(MyRequestBody.class)\n                .flatMap(request -> buildServerResponse(HttpStatus.OK, buildResponseBody(request)));\n    }\n\n    private Mono buildServerResponse(HttpStatus status, Mono responseBody) {\n        return ServerResponse.status(status)\n                .contentType(MediaType.APPLICATION_JSON)\n                .body(responseBody, MyResponse.class);\n    }\n\n    private Mono buildResponseBody(MyRequestBody request) {\n        return Mono.just(MyResponse.builder()\n                .number(request.getNumber())\n                .objects(getObjects(request.getNumber())\n                .build());\n    }\n\n    private Flux getObjects(int n) {\n        // how to receive n strings from MyClient, make MyObject from each of them and then combine them together to a Flux/List?\n    }\n`\n`public class MyClient {\n    public Mono getName() {\n        WebClient client = WebClient.builder().baseUrl(getUrl()).build();\n\n        return client.get()\n                // ...\n                .retrieve()\n                .bodyToMono(String.class);\n    }\n\n    private String getUrl() {\n        // ...\n    }\n}\n`\nSo, if i use Flux in MyResponse, i receive a response like:\n`{\n  \"number\": 4,\n  \"objects\": {\n    \"prefetch\": 2147483647,\n    \"scanAvailable\": true\n  }\n}\n`\non the other hand, if i try to use a List, it seems to require blocking at some point, and i receive errors related to it. So, how do i do it?\nThanks in advance!\n\nUPDATE: if i use `collectList().block()` to make a List out of Flux, i receive this:\n```\n`java.lang.IllegalStateException: block()/blockFirst()/blockLast() are blocking, which is not supported in thread \n`\n```\nAs I understand from answers to this question, i should never block when my method returns `Mono`/`Flux`. Exctracting the `block()` call to a separate method which is called from the one returning `Mono`/`Flux` doesn't help. If i use `share()` before `block()`, then my request just executes forever, for some reason which i don't understand yet.",
      "solution": "Alright, i made it.\n`Flux` as a field doesn't work in a desired way, so i need a `List`.\n`public class MyResponse {\n    private int number;\n    private List objects;\n    // ...\n}\n`\nNow i need a way to make a `List` out of multiple `Mono`s where each `MyObject` has a `String` field.\nThe thing is that we don't ever get rid out of `Mono` or `Flux`, so we go for `Flux` first.\n`private Flux getObjects(int n) {\n    return Flux.range(0, n) // Flux\n            .map(i -> myClient.getName()) // Flux\n            .map(name -> new MyObject(name)); // Flux\n}\n`\nand then we make Flux:\n`private Mono buildResponseBody(MyRequestBody request) {\n    return getObjects(request.getNumber()) // Flux\n            .collectList() // Mono>\n            .map(objects -> MyResponse.builder() // Mono\n                    .number(request.getNumber())\n                    .objects(objects)\n                    .build()));\n}\n`\nThis way it works, as we don't have to block anything.\nThe problem only appears when we want to get rid of `Mono`/`Flux` at some point, like if we want a pure `List`. But as long as we have a `Mono` and/or `Flux` as both input and output, we can do all the manipulations with the methods of these classes, preserving `Mono` or `Flux` at each stage.",
      "question_score": 2,
      "answer_score": 1,
      "created_at": "2021-05-01T11:01:30",
      "url": "https://stackoverflow.com/questions/67344519/can-i-have-flux-as-a-field-of-serverresponse-body"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "flux",
      "question_id": 73725389,
      "title": "Flux not decrypting using SOPS",
      "problem": "I have configured Flux to use SOPS to decrypt. Here's a brief highlight of what I did. In the `gotk-sync.yaml` file I have added the `decryption` property.\n`apiVersion: kustomize.toolkit.fluxcd.io/v1beta2\nkind: Kustomization\nmetadata:\n  name: flux-system\n  namespace: flux-system\nspec:\n  interval: 10m0s\n  path: ./clusters/my-cluster\n  prune: true\n  sourceRef:\n    kind: GitRepository\n    name: flux-system\n  decryption:\n    provider: sops\n    secretRef:\n      name: my-private-key\n`\nThe secret `my-private-key` is created correctly and has the private key.\nI have pushed the file and the change has taken effect.\nIn my application repo I have a secret file.\n`apiVersion: v1\nkind: Secret\nmetadata:\n    name: mysqlcreds\ntype: Opaque\ndata: null\nstringData:\n    DB_USER: bugs\n    DB_PASSWORD: bunny\n`\nI'm encrypting this file with SOPS and pushing it. Flux picks up the change and reconciles. But the `stringData` values remain encrypted. My application gets these values from the environment variable and they show up encrypted like this:\n```\n`ENC[AES256_GCM,data:PdU1ex4H,iv:p5u11vsmHc/tBVGV2g9kTsMSFvQDiYNEwFVeEeMg/pY=,tag:/JTTNNRnYh076EPAd8c/LA==,type:str]\n`\n```\nI can't figure out why Flux is not decrypting the data. How do I debug this? `flux logs` shows nothing wrong.",
      "solution": "I was enabling SOPS for the wrong Git repo. I had to do that for my application git repo's Kustomization.\n`apiVersion: kustomize.toolkit.fluxcd.io/v1beta2\nkind: Kustomization\nmetadata:\n  name: my-demo-webapp\n  namespace: flux-system\nspec:\n  interval: 5m0s\n  path: ./flux-config\n  prune: true\n  sourceRef:\n    kind: GitRepository\n    name: my-demo-webapp\n  targetNamespace: default\n  decryption:\n    provider: sops\n    secretRef:\n      name: my-private-key\n`\nAfter that decryption worked fine.",
      "question_score": 1,
      "answer_score": 4,
      "created_at": "2022-09-15T05:41:21",
      "url": "https://stackoverflow.com/questions/73725389/flux-not-decrypting-using-sops"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "flux",
      "question_id": 67245223,
      "title": "The component styled.p with the id of &quot;sc-iseJRi&quot; has been created dynamically",
      "problem": "Good evening everyone, I need some help.\nI can't solve a warning:\n\nKeyframes.js:20 The component styled.p with the id of \"sc-iseJRi\" has\nbeen created dynamically. You may see this warning because you've\ncalled styled inside another component. To resolve this only create\nnew StyledComponents outside of any render method and function\ncomponent.\n\nIn this link ( https://pastebin.com/a0kMztfD ) is an example of how I use the styled-component.\nIn a checkboxes file I have all the functions I use for the styled-component rules, which I then call in the App.js file to assign them to a const variable to use in the return()\nHow could I solve this problem? It doesn't create any errors for me but of course it creates a thousand warnings.\nI also put the code in addition to the link put previously:\nIn cards.js:\n```\n`export function getCard(Card) {\n \n    let fillMode = (Card.style === null) ? 'both' : Card.style.fillMode\n    let duration = (Card.style === null) ? '1s' : Card.style.duration\n \n    const tmp = keyframes`\n \n                      from,to {\n                                    width: ${Card.width};\n                                    height: ${Card.height};\n                                    background-color: ${Card.colorCard};\n                                    background: linear-gradient(${Card.colorCard2}, ${Card.colorCard});\n                                    box-shadow: 0 16px 16px -8px rgba(0,0,0,0.4);\n                                    border-radius: 6px;\n                                    overflow: hidden;\n                                    position: relative;\n                                    margin: ${Card.marginCard}; \n                      }\n    `;\n \n    const CardFinal = styled.div`\n          animation: ${duration} ${tmp} ${fillMode};\n    `;\n \n    return CardFinal\n}\n`\n```\nIn App.js:\n```\n`Const CardContainer = getCard(card1)\n \nreturn (\n    \n);\n`\n```",
      "solution": "The problem is that you're creating a `styled.div` inside your `getCard` function.\nThe way you get rid of this warning is to move the creation of `CardFinal` outside of getCard and use `getCard` function to return whatever css you want to generate and pass them as props later on. Here's how you can pass props with styled-components.\nThis is how it would look like for your code\n```\n`const CardFinal = styled.div`\n  ${getAnimation}\n`;\n\nexport function getCardProps(Card) {\n  const fillMode = Card.style === null ? \"both\" : Card.style.fillMode;\n  const duration = Card.style === null ? \"1s\" : Card.style.duration;\n\n  const tmp = keyframes`\n    from,to {\n      width: ${Card.width};\n      height: ${Card.height};\n      background-color: ${Card.colorCard};\n      background: linear-gradient(${Card.colorCard2}, ${Card.colorCard});\n      box-shadow: 0 16px 16px -8px rgba(0,0,0,0.4);\n      border-radius: 6px;\n      overflow: hidden;\n      position: relative;\n      margin: ${Card.marginCard}; \n    }\n  `;\n\n  return { fillMode, duration, tmp };\n}\n\nconst getAnimation = ({ duration, tmp, fillMode }) => {\n  return css`\n    animation: ${duration} ${tmp} ${fillMode};\n  `;\n};\n`\n```\nNow you'll just use the `getCardProps` function to the props that CardFinal expects from the `getCardProps`.\n```\n`export default function App() {\n  const cardProps = getCardProps({\n    style: null,\n    weight: \"100px\",\n    height: \"100px\",\n    colorCard: \"grey\",\n    marginCard: \"10px\"\n  });\n\n  return (\n    \n      YO\n    \n  );\n}\n`\n```\nHere's a codesandbox link of where you can try & play around to see how it works.\nYou can also try to un-comment a `// const WarningDiv`, that basically replicates the warnings you've been encountering with just a basic function that returns an empty `styled.div`",
      "question_score": 1,
      "answer_score": 4,
      "created_at": "2021-04-24T18:47:53",
      "url": "https://stackoverflow.com/questions/67245223/the-component-styled-p-with-the-id-of-sc-isejri-has-been-created-dynamically"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "flux",
      "question_id": 75857635,
      "title": "Spring WebClient to send Flux to ElasticSearch using /_bulk api",
      "problem": "What I would like to achieve:\n\nUse Spring WebClient to send a Flux to ElasticSearch leveraging the `/_bulk` api.\n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html\nWhat I have tried:\nUsing this code:\n```\n`public class BulkInsertOfFluxUsingWebClientBulkRestApi {\n\n    public static void main(String[] args) throws InterruptedException {\n        WebClient client = WebClient.create(\"http://127.0.0.1:9200/\").mutate().clientConnector(new ReactorClientHttpConnector(HttpClient.create().wiretap(true))).build();\n        Flux createCommandFlux = Flux.interval(Duration.ofMillis(100))\n                .map(i -> {\n                    try {\n                        Foo onePojo = new Foo(LocalDateTime.now().toString(), String.valueOf(i));\n                        String jsonStringOfOnePojo = new ObjectMapper().writeValueAsString(onePojo);\n                        String bulkCreateCommande = \"{ \\\"create\\\" : {} }\\n\" + jsonStringOfOnePojo + \"\\n\";\n                        return bulkCreateCommande;\n                    } catch (Exception e) {\n                        e.printStackTrace();\n                        return \"\";\n                    }\n                });\n        \n        Disposable disposable = createCommandFlux\n                .window(100) \n                .flatMap(windowFlux -> client\n                        .post()\n                        .uri(\"my_index/_bulk\")\n                        .contentType(MediaType.APPLICATION_NDJSON)\n                        .body(windowFlux, Foo.class)\n                        .exchange()\n                        .doOnNext(response -> System.out.println(response))\n                        .flatMap(clientResponse -> clientResponse.bodyToMono(String.class)))\n                .subscribe();\n        Thread.sleep(1000000);\n        disposable.dispose();\n    }\n`\n```\nNote:\n\nThis is just using reactive Spring WebClient, not another http client, not the ElsaticSearch java client etc.\nIt is trying to save a Flux (which can be infinite) inside ElasticSearch\nI would like to avoid making one http request per object, therefore, \"grouping\" them and sending them as bulk leveraging the `/_bulk` api.\n\nIssue:\nUnfortunately, this code is currently returning http 400 bad request.",
      "solution": "`\n\n    @Bean\n    WebClient elasticWebClient() {\n        return WebClient.builder()\n                .baseUrl(\"http://localhost:9200\")\n                .defaultHeader(HttpHeaders.CONTENT_TYPE, MediaType.APPLICATION_JSON_VALUE)\n                .build();\n    }\n\n  @Autowired ObjectMapper mapper;\n    @GetMapping(\"write\")\n    Mono write() throws IOException {\n\n        Function mapToString = new Function() {\n            @Override\n            public String apply(Object o) {\n                try {\n                    return mapper.writeValueAsString(o);\n                } catch (JsonProcessingException e) {\n                    throw new RuntimeException(e);\n                }\n            }\n        };\n\n        final Map> headingJson = Map.of(\"index\", Map.of(\"_index\", \"new_index\"));\n        final Map value1 = Map.of(\"name\", \"name1\", \"description\", \"description1\");\n        final Map value2 = Map.of(\"name\", \"name2\", \"description\", \"description2\");\n        final Map value3 = Map.of(\"name\", \"name3\", \"description\", \"description3\");\n        final Map value4 = Map.of(\"name\", \"name4\", \"description\", \"description4\");\n        final Map value5 = Map.of(\"name\", \"name5\", \"description\", \"description5\");\n        final Map value6 = Map.of(\"name\", \"name6\", \"description\", \"description6\");\n\n        final String requestBody = Stream.of(value1, value2, value3, value4, value5, value6)\n                .flatMap(val -> Stream.of(headingJson, val))\n                .map(mapToString)\n                .collect(Collectors.joining(\"\\n\", \"\", \"\\n\"));\n\n        return client\n                .post()\n                .uri(u -> u.path(\"_bulk\").build())\n                .bodyValue(requestBody)\n                .exchangeToMono(clientResponse -> clientResponse.toEntity(Map.class))\n                .map(HttpEntity::getBody);\n`",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2023-03-27T17:33:01",
      "url": "https://stackoverflow.com/questions/75857635/spring-webclient-to-send-flux-to-elasticsearch-using-bulk-api"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "flux",
      "question_id": 69496951,
      "title": "Mono - Flux switchIfEmpty and onErrorResume",
      "problem": "In project reactor is it possible to implement a stream with `switchIfEmpty` and `onErrorResume` at the same time?\n```\n`infoRepository.findById(id); //returns Mono\n`\n```\nin case of `empty or error then switch to the same backup stream`?",
      "solution": "There's no single operator that does these things together, but you can trivially switch to an empty publisher on an error, then handle both cases through `switchIfEmpty` like:\n```\n`infoRepository.findById(id)\n              .onErrorResume(e -> Mono.empty())\n              .switchIfEmpty(newPublisher);\n`\n```",
      "question_score": 1,
      "answer_score": 3,
      "created_at": "2021-10-08T15:50:11",
      "url": "https://stackoverflow.com/questions/69496951/mono-flux-switchifempty-and-onerrorresume"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "flux",
      "question_id": 79375111,
      "title": "Join sorted Flux Producers efficiently",
      "problem": "I'm need to merge large data streams available over flux which each contain a timestamp and a value. If the timestamps match, then the values need to be summed up. The data in the flux is sorted by the timestamp in ascending order.\nFor smaller streams I would utilize the `groupBy` function, but since the flux hold many entries this isn't efficient.\nI would like to exploit the fact that the entries in the flux are ordered but I can't find the right construct. What are the tools to achieve something like this. Below is some sudo code on what I want to do:\n```\n`    var flux1 = Flux.just(\n            new Data(ZonedDateTime.parse(\"2025-01-01T00:00:00\"), 1.0),\n            new Data(ZonedDateTime.parse(\"2025-03-01T00:00:00\"), 1.0)\n    );\n\n    var flux2 = Flux.just(\n            new Data(ZonedDateTime.parse(\"2025-02-01T00:00:00\"), 2.0),\n            new Data(ZonedDateTime.parse(\"2025-03-01T00:00:00\"), 2.0),\n            new Data(ZonedDateTime.parse(\"2025-04-01T00:00:00\"), 2.0)\n    );\n\n    var flux3 = Flux.just(\n            new Data(ZonedDateTime.parse(\"2025-02-01T00:00:00\"), 5.0)\n    );\n\n    var input = List.of(flux1, flux2, flux3);\n\n    var output = Flux.create(sink -> {\n        List nextEntries = input.stream().map(Flux::next).toList();\n\n        do {\n            ZonedDateTime nextTimestamp = nextEntries.stream().map(Data::getTimestamp).min(ZonedDateTime::compareTo).get();\n            List affectedStreams = IntStream.range(0, input.size()).filter(i -> nextTimestamp == nextEntries[i].getTimestamp()).toList();\n            double nextOutput = affectedStreams.stream().mapToDouble(i -> nextEntries[i].getValue()).sum();\n            sink.next(new Data(nextTimestamp, nextOutput));\n            affectedStreams.forEach(i -> nextEntries[i] = input.get(i).next());\n        } while (!allFluxAreConsumed);\n    });\n\n    // expected output:\n    // [\n    //      Data(ZonedDateTime.parse(\"2025-01-01T00:00:00\"), 1.0),\n    //      Data(ZonedDateTime.parse(\"2025-02-01T00:00:00\"), 7.0),\n    //      Data(ZonedDateTime.parse(\"2025-03-01T00:00:00\"), 3.0),\n    //      Data(ZonedDateTime.parse(\"2025-05-01T00:00:00\"), 2.0)\n    // ]\n`\n```",
      "solution": "You can get the expected result by stacking the following operators :\n\nCombine input fluxes preserving overall date ordering using mergeComparing operator. WARNING: if one the input flux is slow, it will slow down the entire downstream pipeline\nUsing windowUntilChanged to group adjacent records that use the same timestamp\nUse `reduce` on each window to merge records as you wish.\n\nWhich gives something like this :\n`Flux.mergeComparing(Comparator.comparing(Data::datetime), flux1, flux2, flux3)\n    .windowUntilChanged(Data::datetime)\n    .flatMap(window -> window.reduce((d1, d2) -> new Data(d1.datetime(), d1.value() + d2.value())));\n`\nYou can test it with a unit test like so :\n`import org.junit.jupiter.api.Test;\nimport reactor.core.publisher.Flux;\nimport reactor.test.StepVerifier;\n\nimport java.time.LocalDateTime;\nimport java.util.Comparator;\n\npublic class MergeSorted {\n\n    record Data(LocalDateTime datetime, Integer value) {}\n\n    @Test\n    public void test() {\n\n        var flux1 = Flux.just(\n                new Data(LocalDateTime.parse(\"2025-01-01T00:00:00\"), 1),\n                new Data(LocalDateTime.parse(\"2025-03-01T00:00:00\"), 1)\n        );\n\n        var flux2 = Flux.just(\n                new Data(LocalDateTime.parse(\"2025-02-01T00:00:00\"), 2),\n                new Data(LocalDateTime.parse(\"2025-03-01T00:00:00\"), 2),\n                new Data(LocalDateTime.parse(\"2025-05-01T00:00:00\"), 2)\n        );\n\n        var flux3 = Flux.just(\n                new Data(LocalDateTime.parse(\"2025-02-01T00:00:00\"), 5)\n        );\n\n        var mergeSum = Flux.mergeComparing(Comparator.comparing(Data::datetime), flux1, flux2, flux3)\n                           .windowUntilChanged(Data::datetime)\n                           .flatMap(window -> window.reduce((d1, d2) -> new Data(d1.datetime(), d1.value() + d2.value())));\n\n        StepVerifier.create(mergeSum)\n                .expectNext(\n                              new Data(LocalDateTime.parse(\"2025-01-01T00:00:00\"), 1),\n                              new Data(LocalDateTime.parse(\"2025-02-01T00:00:00\"), 7),\n                              new Data(LocalDateTime.parse(\"2025-03-01T00:00:00\"), 3),\n                              new Data(LocalDateTime.parse(\"2025-05-01T00:00:00\"), 2)\n                )\n                .verifyComplete();\n    }\n}\n`",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2025-01-21T17:06:12",
      "url": "https://stackoverflow.com/questions/79375111/join-sorted-flux-producers-efficiently"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "flux",
      "question_id": 76027735,
      "title": "How to adjust the learning rate of the optimiser in Flux Julia",
      "problem": "As mentioned I tried to implement a learning rate decay for my neural network. I set up the model as follows:\n```\n`nn = Chain(Dense(10,5),Dense(5,1))\nopt = Adam(0.01)\nopt_state = setup(opt, nn)\n`\n```\nI tried to adjust the learning rate directly from `opt_state`. Then I found that `Optimisers.Adam` is an immutable struct in the `opt_state` but if I changed the `eta` in `opt` it worked fine.\n```\n`opt_state.layers[1][:weight].rule.eta = 0.001 # ERROR: setfield!: immutable struct of type Adam cannot be changed\nopt.eta = 0.001 # no error\n`\n```\nFurthermore, accessing the learning rate like this doesn't look great. Is it possible to modify the learning rate of the `opt` without setting up a new `opt_state`?",
      "solution": "You're looking for `adjust!(opt_state, 0.001)` described here: https://fluxml.ai/Optimisers.jl/dev/#Adjusting-Hyperparameters",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2023-04-16T14:23:59",
      "url": "https://stackoverflow.com/questions/76027735/how-to-adjust-the-learning-rate-of-the-optimiser-in-flux-julia"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "flux",
      "question_id": 75682083,
      "title": "How to collect Flux of Map into Mono of Map (Flux&lt;Map&lt;String,String&gt;&gt; into Mono&lt;Map&lt;String,String&gt;&gt;)?",
      "problem": "I have found an extensive thread of how to merge maps in java8 streams, but I cannot quite translate that to Webflux.\nI have two Fluxes of the form:\n`Flux>> fluxOne = Flux.fromIterable(\n    List.of(\n        Collections.singletonMap(\"one\", List.of(\"one\",\"two\")),\n        Collections.singletonMap(\"two\", List.of(\"one\",\"two\"))));\nFlux> fluxTwo = Flux.fromIterable(\n    List.of(\n        Collections.singletonMap(\"one\", \"two\"),\n        Collections.singletonMap(\"one\", \"two\")));\n`\nI'd like to transform that into Mono's by collecting it's values:\n`Mono>> monoOne = fluxOne.collect(...);\nMono> monoTwo = fluxTwo.collect(...);\n`\nSo my question, how do I do that?",
      "solution": "Perhaps this is what you are looking for:\n```\n`Mono>> monoOne = fluxOne.collect(HashMap::new, Map::putAll);\nMono> monoTwo = fluxTwo.collect(HashMap::new, Map::putAll);\n`\n```",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2023-03-09T09:32:39",
      "url": "https://stackoverflow.com/questions/75682083/how-to-collect-flux-of-map-into-mono-of-map-fluxmapstring-string-into-mono"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "flux",
      "question_id": 74110297,
      "title": "Send/Update reactor Mono Object to different attributes of a Flux.concat()",
      "problem": "I have a `Flux.concat()` operation which takes 3 checks like this:\n```\n`\npublic Mono checkSleep(Student std) \n{\n\nFlux.concat(isHealthy(std), isWealthy(std, sleep), isWise(std, sleep))\n\n.filter(result -> !result)\n            .next()\n            .flatMap(result -> Mono.just(false)) //returns false if any one is false\n            .switchIfEmpty(Mono.just(true)); // returns true if all are true\n\n}\n\n`\n```\neach of this methods has a common external api call `extService.getSleep(Student std)` to get Sleep Object `Mono` for its flow.\nI want to call `extService.getSleep(Student std)` only once in the entire flow,\nideally in the first check `isHealthy(std)` and pass the object `Mono` to the next 2 checks as well.\nI am not understanding how to make this call as `Flux.concat` does not allow a `Mono` to be added in the prefix.\nEach of the checks have similar body like this:\n```\n`Mono isHealthy(Student std) \n  {\n    \n    return Mono.just(std)\n                 .flatMap(std->extService.getSleep(std)).map(sleep-> sleep.isValid()); \n  }\n`\n```\nin the next check I want to pass `sleep` object from previous method,\n`isWealthy(Student std, Sleep sleep)`\nI do not want to call `extService.getSleep(std))` once again,\nI thought of creating a variable outside these 3 methods and update it when the api returns a something,\nit throws error saying `\"Variable used in lambda expression should be final or effectively final\"`\nLet me know if there is a better way to handle this scenario.\nI am new to reactive spring programming, any help is appreciated.\nthanks in advance.",
      "solution": "Your line of thinking was not far off!\nWhenever you need to \"reach outside\" a publisher, consider using `AtomicBoolean`, `AtomicInteger`, etc. or the parameterized `AtomicReference` to get around the final or effectively final compiler warning. However, it should be noted that asynchronous operations like `flatMap` may not have the correct value when they get the wrapped values from these, so it's best to get around the problem in a different way.\nFortunately, Reactor has a myriad of useful methods on its publishers!\nIf I understand correctly, the `checkSleep` function should resolve to true if all three of `isHealthy`, `isWealthy` and `isWise` also resolve to true - false if even one of them resolve to false.\nI have created a simple simulation of this scenario:\n`private Mono checkSleep(Student std) {\n    return getSleep(std)\n        .flatMapMany(sleep -> Flux.merge(isHealthy(std, sleep), isWealthy(std, sleep), isWise(std, sleep)))\n        .all(result -> result);\n}\n\nprivate Mono getSleep(Student std) {\n    return Mono.just(new Sleep(8));\n}\n\nprivate Mono isHealthy(Student std, Sleep sleep) {\n    return Mono.just(true);\n}\n\nprivate Mono isWealthy(Student std, Sleep sleep) {\n    return Mono.just(true);\n}\n\nprivate Mono isWise(Student std, Sleep sleep) {\n    return Mono.just(true);\n}\n`\nThis way, getSleep is only called once, and is used to flat map the emitted value into the three booleans you're looking for. The `Flux::all` method then ensures that the returned Mono will wrap true only if all three inners have emitted true.\nAnother thing to note is that I've replaced `Flux::concat` with `Flux::merge`. The former goes sequentially, subscribing, waiting for result, then repeat. These three publishers seem to be independent of one another, so replacing concat with merge allows all three to be subscribed to at the same time, thereby reducing time wasted with waiting.",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2022-10-18T13:35:17",
      "url": "https://stackoverflow.com/questions/74110297/send-update-reactor-mono-object-to-different-attributes-of-a-flux-concat"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "flux",
      "question_id": 70288948,
      "title": "Flux V2 is unable to clone a repo",
      "problem": "I'm trying to use flux v2 for continuous deployments on my K8 cluster. But when I try to create my flux source is gives the following error\n```\n`\u271a generating GitRepository source\n\u25ba applying GitRepository source\n\u2714 GitRepository source created\n\u25ce waiting for GitRepository source reconciliation\n\u2717 unable to clone: remote authentication required but no callback set\n`\n```\nI'm trying to connect my azure repo here and I have obtained a PAT from azure and has applied it here\nCan anyone please help me out?\nUPDATE\nI have managed to get it running with SSH keys from Azure",
      "solution": "The actual issue was that the internet drop occasionally in the environment. what happens then is the pods in the flux namespace lose connection to your image repository. Then what you must do is restart the pods in the flux namespace. Then you can execute the reconcile commands on the image automation controller and the source controller..\n```\n`flux reconcile source git \n\nflux reconcile image update \n`\n```",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-12-09T12:15:22",
      "url": "https://stackoverflow.com/questions/70288948/flux-v2-is-unable-to-clone-a-repo"
    },
    {
      "tech": "influxdb",
      "source": "stackoverflow",
      "tag": "flux",
      "question_id": 68462882,
      "title": "Is there a FluxCD equivalent to &quot;argocd app wait&quot; or &quot;helm upgrade --wait&quot;?",
      "problem": "I did the following to deploy a helm chart (you can copy-and-paste my sequence of commands to reproduce this error).\n```\n`$ flux --version\nflux version 0.16.1\n\n$ kubectl create ns traefik\n\n$ flux create source helm traefik --url https://helm.traefik.io/traefik --namespace traefik\n\n$ cat values-6666.yaml\nports:\n  traefik:\n    healthchecksPort: 6666   # !!! Deliberately wrong port number!!!\n\n$ flux create helmrelease my-traefik --chart traefik --source HelmRepository/traefik --chart-version 9.18.2 --namespace traefik --values=./values-6666.yaml\n\u271a generating HelmRelease\n\u25ba applying HelmRelease\n\u2714 HelmRelease created\n\u25ce waiting for HelmRelease reconciliation\n\u2714 HelmRelease my-traefik is ready\n\u2714 applied revision 9.18.2\n\n`\n```\nSo Flux reports it as a success, and can be confirmed like this:\n```\n`$ flux get helmrelease --namespace traefik\nNAME        READY   MESSAGE                             REVISION    SUSPENDED\nmy-traefik  True    Release reconciliation succeeded    9.18.2      False\n`\n```\nBut in fact, as shown above, `values-6666.yaml` contains a deliberately wrong port number 6666 for pod's readiness probe (as well as liveness probe):\n```\n`$ kubectl -n traefik describe pod my-traefik-8488cc49b8-qf5zz\n  ...\n  Type     Reason    ... From     Message\n  ----     ------    ... ----     -------\n  Warning  Unhealthy ... kubelet  Liveness  probe failed: Get \"http://172.31.61.133:6666/ping\": dial tcp 172.31.61.133:6666: connect: connection refused\n  Warning  Unhealthy ... kubelet  Readiness probe failed: Get \"http://172.31.61.133:6666/ping\": dial tcp 172.31.61.133:6666: connect: connection refused\n  Warning  BackOff   ... kubelet  Back-off restarting failed container\n`\n```\nMy goal is to have FluxCD automatically detect the above error. But, as shown above, FluxCD deems it a success.\nEither of the following deployment methods would have detected that failure:\n```\n`$ helm upgrade --wait ...\n`\n```\nor\n```\n`$ argocd app sync ... && argocd app wait ...\n`\n```\nSo, is there something similar in FluxCD to achieve the same effect?\n====================================================================\nP.S. Flux docs here seems to suggest that the equivalent to `helm --wait` is already the default behaviour in FluxCD. My test above shows that it isn't. Furthermore, in the following example, I explicitly set it to `disableWait: false` but the result is the same.\n```\n`$ cat helmrelease.yaml\n---\napiVersion: helm.toolkit.fluxcd.io/v2beta1\nkind: HelmRelease\nmetadata:\n  name: my-traefik\n  namespace: traefik\nspec:\n  chart:\n    spec:\n      chart: traefik\n      sourceRef:\n        kind: HelmRepository\n        name: traefik\n      version: 9.18.2\n  install:\n    disableWait: false      # !!! Explicitly set this flag !!!\n  interval: 1m0s\n  values:\n    ports:\n      traefik:\n        healthchecksPort: 6666\n\n$ kubectl -n traefik create -f helmrelease.yaml\nhelmrelease.helm.toolkit.fluxcd.io/my-traefik created\n\n  ## Again, Flux deems it a success:\n$ flux get hr -n traefik\nNAME        READY   MESSAGE                             REVISION    SUSPENDED\nmy-traefik  True    Release reconciliation succeeded    9.18.2      False\n\n  ## Again, the pod actually failed:\n$ kubectl -n traefik describe pod my-traefik-8488cc49b8-bmxnv\n... // Same error as earlier\n`\n```",
      "solution": "Helm considers a deployment with one replica and strategy rollingUpdate with maxUnavailable of 1 to be ready when it has been deployed and there is 1 unavailable pod. If you test Helm itself, I believe you will find the same behavior exists in the Helm CLI / Helm SDK package upstream.\n(Even if the deployment's one and only pod has entered CrashLoopBackOff and readiness and liveness checks have all failed... with maxUnavailable of 1 and replicas of 1, the deployment technically has no more than the allowed number of unavailable pods, so it is considered ready.)\nThis question was re-raised recently at: https://github.com/fluxcd/helm-controller/issues/355 and I provided more in-depth feedback there.\nAnyway, as for the source of this behavior which is seemingly/clearly not what the user wanted (even if it appears to be specifically what the user has asked for, which is perhaps debatable):\nAs for Helm, this appears to be the same issue reported at GitHub here:\n\nhelm install --wait does not wait for deployment pod readiness properly - (helm/helm#3173)\nand resurrected here:\nhelm upgrade --wait does not wait on newer versions - (helm/helm#10061)",
      "question_score": 1,
      "answer_score": 1,
      "created_at": "2021-07-21T03:03:13",
      "url": "https://stackoverflow.com/questions/68462882/is-there-a-fluxcd-equivalent-to-argocd-app-wait-or-helm-upgrade-wait"
    }
  ]
}